{"home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_ckpts.save_checkpoint": [[35, 195], ["dict", "os.path.split", "os.path.join", "torch.save", "print", "time.time", "ckpt_dict.keys", "ckpt_dict.keys", "os.path.exists", "os.makedirs", "os.path.exists", "os.path.join", "torch.load", "len", "kept_ckpts.sort", "range", "open", "json.dump", "open", "json.load", "os.listdir", "permanent_ckpts.append", "kept_ckpts.append", "len", "permanent_ckpts.sort", "kept_ckpts.sort", "enumerate", "print", "os.remove", "os.path.isfile", "f.startswith", "print", "torch.load", "torch.save", "print", "len", "fname.startswith", "os.path.join"], "function", ["None"], ["def", "save_checkpoint", "(", "ckpt_dict", ",", "file_path", ",", "performance_score", ",", "train_iter", "=", "None", ",", "\n", "max_ckpts_to_keep", "=", "5", ",", "keep_cktp_every", "=", "2", ",", "timestamp", "=", "None", ")", ":", "\n", "    ", "\"\"\"Save checkpoint to file.\n\n    Example:\n        .. code-block:: python\n\n            save_checkpoint({\n                'state_dict': net.state_dict(),\n                'train_iter': curr_iteration\n            }, 'ckpts/my_net', current_test_accuracy)\n\n    Args:\n        ckpt_dict: A dict with mostly arbitrary content. Though, most important,\n                   it needs to include the state dict and should also include\n                   the current training iteration.\n        file_path: Where to store the checkpoint. Note, the filepath should\n                   not change. Instead, 'train_iter' should be provided,\n                   such that this method can handle the filenames by itself.\n        performance_score: A score that expresses the performance of the\n                           current network state, e.g., accuracy for a\n                           classification task. This score is used to\n                           maintain the list of kept checkpoints during\n                           training.\n        train_iter (optional): If given, it will be added to the filename.\n            Otherwise, existing checkpoints are simply overwritten.\n        max_ckpts_to_keep: The maximum number of checkpoints to\n            keep. This will use the performance score to determine the n-1\n            checkoints not to be deleted (where n is the number of\n            checkpoints to keep). The current checkpoint will always be saved.\n        keep_cktp_every: If this option is not :code:`None`,\n            then every n hours one checkpoint will be permanently saved, i.e.,\n            this checkpoint will not be maintained by 'max_ckpts_to_keep'\n            anymore. The checkpoint to be kept will be the best one from the\n            time window that spans the last n hours.\n        timestamp (optional): The timestamp of this checkpoint. If not given,\n            a current timestamp will be used. This option is useful when one\n            aims to synchronize checkpoint savings from multiple networks.\n    \"\"\"", "\n", "if", "timestamp", "is", "None", ":", "\n", "        ", "ts", "=", "time", ".", "time", "(", ")", "# timestamp", "\n", "", "else", ":", "\n", "        ", "ts", "=", "timestamp", "\n", "\n", "", "assert", "(", "'state_dict'", "in", "ckpt_dict", ".", "keys", "(", ")", ")", "\n", "# We need to store internal (checkpoint maintenance related) information in", "\n", "# each checkpoint.", "\n", "internal_key", "=", "_INTERNAL_KEY", "\n", "assert", "(", "internal_key", "not", "in", "ckpt_dict", ".", "keys", "(", ")", ")", "\n", "ckpt_dict", "[", "internal_key", "]", "=", "dict", "(", ")", "\n", "ckpt_dict", "[", "internal_key", "]", "[", "'permanent'", "]", "=", "False", "\n", "ckpt_dict", "[", "internal_key", "]", "[", "'score'", "]", "=", "performance_score", "\n", "ckpt_dict", "[", "internal_key", "]", "[", "'ts'", "]", "=", "ts", "\n", "\n", "# FIXME We currently don't care about file extensions.", "\n", "dname", ",", "fname", "=", "os", ".", "path", ".", "split", "(", "file_path", ")", "\n", "# Where do we store meta data, needed for maintenance.", "\n", "meta_fn", "=", "(", "'.'", "if", "not", "fname", ".", "startswith", "(", "'.'", ")", "else", "''", ")", "+", "fname", "+", "'_meta'", "\n", "meta_fn", "=", "os", ".", "path", ".", "join", "(", "dname", ",", "meta_fn", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "dname", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "dname", ")", "\n", "\n", "# Needed for option 'keep_cktp_every'. When was the first ckpt stored?", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_fn", ")", ":", "\n", "        ", "with", "open", "(", "meta_fn", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "{", "'init_ts'", ":", "ts", "}", ",", "f", ")", "\n", "", "init_ts", "=", "ts", "\n", "", "else", ":", "\n", "        ", "with", "open", "(", "meta_fn", ")", "as", "f", ":", "\n", "            ", "meta_dict", "=", "json", ".", "load", "(", "f", ")", "\n", "", "init_ts", "=", "meta_dict", "[", "'init_ts'", "]", "\n", "\n", "", "hrs_passed", "=", "(", "ts", "-", "init_ts", ")", "/", "(", "60", "*", "60", ")", "\n", "\n", "### Iterate all existing checkpoints to determine which we remove.", "\n", "ckpt_fns", "=", "[", "os", ".", "path", ".", "join", "(", "dname", ",", "f", ")", "for", "f", "in", "os", ".", "listdir", "(", "dname", ")", "if", "\n", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "dname", ",", "f", ")", ")", "and", "\n", "f", ".", "startswith", "(", "fname", ")", "]", "\n", "\n", "kept_ckpts", "=", "[", "]", "\n", "permanent_ckpts", "=", "[", "]", "\n", "\n", "for", "fn", "in", "ckpt_fns", ":", "\n", "# FIXME loading all checkpoints is expensive.", "\n", "        ", "ckpt", "=", "torch", ".", "load", "(", "fn", ")", "\n", "\n", "if", "not", "internal_key", "in", "ckpt", ":", "\n", "            ", "continue", "\n", "\n", "", "if", "ckpt", "[", "internal_key", "]", "[", "'permanent'", "]", ":", "\n", "            ", "permanent_ckpts", ".", "append", "(", "(", "fn", ",", "ckpt", "[", "internal_key", "]", "[", "'ts'", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "kept_ckpts", ".", "append", "(", "(", "fn", ",", "ckpt", "[", "internal_key", "]", "[", "'ts'", "]", ",", "\n", "ckpt", "[", "internal_key", "]", "[", "'score'", "]", ")", ")", "\n", "\n", "## Decide, whether a new permanent checkpoint should be saved.", "\n", "", "", "if", "keep_cktp_every", "is", "not", "None", "and", "hrs_passed", ">=", "keep_cktp_every", ":", "\n", "        ", "perm_ckpt_needed", "=", "True", "\n", "\n", "num_wins", "=", "hrs_passed", "//", "keep_cktp_every", "\n", "win_start", "=", "(", "num_wins", "-", "1", ")", "*", "keep_cktp_every", "\n", "\n", "# Check whether a permanent checkpoint for the current window already", "\n", "# exists.", "\n", "if", "len", "(", "permanent_ckpts", ")", ">", "0", ":", "\n", "            ", "permanent_ckpts", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "tup", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "ts_last_perm", "=", "permanent_ckpts", "[", "0", "]", "[", "1", "]", "\n", "if", "(", "(", "ts_last_perm", "-", "init_ts", ")", "/", "(", "60", "*", "60", ")", ")", ">=", "win_start", ":", "\n", "                ", "perm_ckpt_needed", "=", "False", "\n", "\n", "", "", "if", "perm_ckpt_needed", ":", "\n", "# Choose the checkpoint with the best score in the current window", "\n", "# as next permanent checkpoint.", "\n", "            ", "kept_ckpts", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "tup", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "max_score", "=", "-", "1", "\n", "max_ind", "=", "-", "1", "\n", "\n", "for", "i", ",", "tup", "in", "enumerate", "(", "kept_ckpts", ")", ":", "\n", "                ", "if", "(", "(", "tup", "[", "1", "]", "-", "init_ts", ")", "/", "(", "60", "*", "60", ")", ")", "<", "win_start", ":", "\n", "                    ", "break", "\n", "\n", "", "if", "max_ind", "==", "-", "1", "or", "max_score", "<", "tup", "[", "2", "]", ":", "\n", "                    ", "max_ind", "=", "i", "\n", "max_score", "=", "tup", "[", "2", "]", "\n", "\n", "", "", "if", "max_ind", "!=", "-", "1", "and", "max_score", ">", "performance_score", ":", "\n", "# Transform an existing checkpoint into a permanent one.", "\n", "                ", "ckpt_tup", "=", "kept_ckpts", "[", "max_ind", "]", "\n", "# Important, we need to remove this item from the kept_ckpts,", "\n", "# as this list is used in the next step to determine which", "\n", "# checkpoints are removed.", "\n", "del", "kept_ckpts", "[", "max_ind", "]", "\n", "print", "(", "'Checkpoint %s will be kept permanently.'", "%", "ckpt_tup", "[", "0", "]", ")", "\n", "\n", "# FIXME: We might need the device here as in the load method.", "\n", "ckpt", "=", "torch", ".", "load", "(", "ckpt_tup", "[", "0", "]", ")", "\n", "ckpt", "[", "internal_key", "]", "[", "'permanent'", "]", "=", "True", "\n", "torch", ".", "save", "(", "ckpt", ",", "ckpt_tup", "[", "0", "]", ")", "\n", "\n", "", "else", ":", "\n", "                ", "print", "(", "'New checkpoint will be kept permanently.'", ")", "\n", "ckpt_dict", "[", "internal_key", "]", "[", "'permanent'", "]", "=", "True", "\n", "\n", "## Decide, whether a checkpoint has to be deleted.", "\n", "", "", "", "if", "len", "(", "kept_ckpts", ")", ">=", "max_ckpts_to_keep", ":", "\n", "        ", "kept_ckpts", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "tup", "[", "2", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "kept_ckpts", ")", "-", "(", "max_ckpts_to_keep", "-", "1", ")", ")", ":", "\n", "            ", "fn", "=", "kept_ckpts", "[", "i", "]", "[", "0", "]", "\n", "print", "(", "'Deleting old checkpoint: %s.'", "%", "fn", ")", "\n", "os", ".", "remove", "(", "fn", ")", "\n", "\n", "### Save new checkpoint.", "\n", "", "", "if", "train_iter", "is", "not", "None", ":", "\n", "        ", "file_path", "+=", "'_%d'", "%", "train_iter", "\n", "\n", "", "torch", ".", "save", "(", "ckpt_dict", ",", "file_path", ")", "\n", "print", "(", "'Checkpoint saved to %s'", "%", "file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_ckpts.load_checkpoint": [[196, 234], ["net.load_state_dict", "torch.load", "torch.load"], "function", ["None"], ["", "def", "load_checkpoint", "(", "ckpt_path", ",", "net", ",", "device", "=", "None", ",", "ret_performance_score", "=", "False", ")", ":", "\n", "    ", "\"\"\"Load a checkpoint from file.\n\n    Args:\n        ckpt_path: Path to checkpoint.\n        net: The network, that should load the state dict saved in this\n             checkpoint.\n        device (optional): The device currently used by the model. Can help to\n                           speed up loading the checkpoint.\n        ret_performance_score: If True, the score associated with this\n            checkpoint will be returned as well. See argument\n            \"performance_score\" of method \"save_ckecpoint\".\n\n    Returns:\n        The loaded checkpoint. Note, the state_dict is already applied to the\n        network. However, there might be other important dict elements.\n    \"\"\"", "\n", "# See here for details on how to load the checkpoint:", "\n", "# https://blog.floydhub.com/checkpointing-tutorial-for-tensorflow-keras-and-pytorch/", "\n", "if", "device", "is", "not", "None", "and", "device", ".", "type", "==", "'cuda'", ":", "\n", "        ", "ckpt", "=", "torch", ".", "load", "(", "ckpt_path", ")", "\n", "", "else", ":", "\n", "# Load GPU model on CPU", "\n", "        ", "ckpt", "=", "torch", ".", "load", "(", "ckpt_path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "\n", "", "net", ".", "load_state_dict", "(", "ckpt", "[", "'state_dict'", "]", ")", "\n", "\n", "if", "ret_performance_score", ":", "\n", "        ", "score", "=", "ckpt", "[", "_INTERNAL_KEY", "]", "[", "'score'", "]", "\n", "\n", "# That key was added for maintenance reasons in the method save_checkpoint.", "\n", "", "if", "_INTERNAL_KEY", "in", "ckpt", ":", "\n", "        ", "del", "ckpt", "[", "_INTERNAL_KEY", "]", "\n", "\n", "", "if", "ret_performance_score", ":", "\n", "        ", "return", "ckpt", ",", "score", "\n", "\n", "", "return", "ckpt", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_ckpts.make_ckpt_list": [[235, 270], ["os.path.split", "os.path.exists", "ckpts.sort", "torch.load", "ckpts.append", "open", "os.path.join", "os.listdir", "os.path.join", "f.write", "os.path.isfile", "f.startswith", "os.path.join"], "function", ["None"], ["", "def", "make_ckpt_list", "(", "file_path", ")", ":", "\n", "    ", "\"\"\"Creates a file that lists all checkpoints together with there scores,\n    such that one can easily find the checkpoint associated with the maximum\n    score.\n\n    Args:\n        file_path: See method :func:`save_checkpoints`.\n    \"\"\"", "\n", "internal_key", "=", "_INTERNAL_KEY", "\n", "\n", "dname", ",", "fname", "=", "os", ".", "path", ".", "split", "(", "file_path", ")", "\n", "\n", "assert", "(", "os", ".", "path", ".", "exists", "(", "dname", ")", ")", "\n", "\n", "ckpt_fns", "=", "[", "(", "f", ",", "os", ".", "path", ".", "join", "(", "dname", ",", "f", ")", ")", "for", "f", "in", "os", ".", "listdir", "(", "dname", ")", "if", "\n", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "dname", ",", "f", ")", ")", "and", "\n", "f", ".", "startswith", "(", "fname", ")", "]", "\n", "\n", "ckpts", "=", "[", "]", "\n", "\n", "for", "fn", ",", "fpath", "in", "ckpt_fns", ":", "\n", "        ", "ckpt", "=", "torch", ".", "load", "(", "fpath", ")", "\n", "\n", "if", "not", "internal_key", "in", "ckpt", ":", "\n", "            ", "continue", "\n", "\n", "", "score", "=", "ckpt", "[", "internal_key", "]", "[", "'score'", "]", "\n", "\n", "ckpts", ".", "append", "(", "(", "fn", ",", "score", ")", ")", "\n", "\n", "", "ckpts", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "tup", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "dname", ",", "'score_list_'", "+", "fname", "+", "'.txt'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "tup", "in", "ckpts", ":", "\n", "            ", "f", ".", "write", "(", "'%s, %f\\n'", "%", "(", "tup", "[", "0", "]", ",", "tup", "[", "1", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_ckpts.get_best_ckpt_path": [[271, 309], ["os.path.split", "os.path.exists", "os.path.join", "os.path.exists", "os.path.join", "torch.load", "open", "os.path.join", "os.listdir", "os.path.isfile", "f.startswith", "f.readline().split", "os.path.join", "f.readline"], "function", ["None"], ["", "", "", "def", "get_best_ckpt_path", "(", "file_path", ")", ":", "\n", "    ", "\"\"\"Returns the path to the checkpoint with the highest score.\n\n    Args:\n        file_path: See method :func:`save_checkpoints`.\n    \"\"\"", "\n", "dname", ",", "fname", "=", "os", ".", "path", ".", "split", "(", "file_path", ")", "\n", "assert", "(", "os", ".", "path", ".", "exists", "(", "dname", ")", ")", "\n", "\n", "# See method make_ckpt_list.", "\n", "ckpt_list_fn", "=", "os", ".", "path", ".", "join", "(", "dname", ",", "'score_list_'", "+", "fname", "+", "'.txt'", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "ckpt_list_fn", ")", ":", "\n", "        ", "with", "open", "(", "ckpt_list_fn", ",", "'r'", ")", "as", "f", ":", "\n", "# Get first word from file. Note, the filename ends with a comma.", "\n", "            ", "best_ckpt_fname", "=", "f", ".", "readline", "(", ")", ".", "split", "(", "None", ",", "1", ")", "[", "0", "]", "[", ":", "-", "1", "]", "\n", "\n", "", "return", "os", ".", "path", ".", "join", "(", "dname", ",", "best_ckpt_fname", ")", "\n", "\n", "# Go through each checkpoint and evaluate the score achieved.", "\n", "", "ckpt_fns", "=", "[", "(", "f", ",", "os", ".", "path", ".", "join", "(", "dname", ",", "f", ")", ")", "for", "f", "in", "os", ".", "listdir", "(", "dname", ")", "if", "\n", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "dname", ",", "f", ")", ")", "and", "\n", "f", ".", "startswith", "(", "fname", ")", "]", "\n", "\n", "best_ckpt_path", "=", "None", "\n", "best_score", "=", "-", "1", "\n", "\n", "for", "fn", ",", "fpath", "in", "ckpt_fns", ":", "\n", "        ", "ckpt", "=", "torch", ".", "load", "(", "fpath", ")", "\n", "\n", "if", "not", "_INTERNAL_KEY", "in", "ckpt", ":", "\n", "            ", "continue", "\n", "\n", "", "score", "=", "ckpt", "[", "_INTERNAL_KEY", "]", "[", "'score'", "]", "\n", "if", "score", ">", "best_score", ":", "\n", "            ", "best_score", "=", "score", "\n", "best_ckpt_path", "=", "fpath", "\n", "\n", "", "", "return", "best_ckpt_path", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.gan_helpers.dis_loss": [[29, 83], ["torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.binary_cross_entropy", "torch.binary_cross_entropy", "torch.zeros_like", "torch.zeros_like", "torch.mse_loss", "torch.mse_loss", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.mse_loss", "torch.mse_loss", "logit_fake.mean", "torch.ones_like", "torch.ones_like", "logit_real.mean", "torch.ones_like", "torch.ones_like"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["def", "dis_loss", "(", "logit_real", ",", "logit_fake", ",", "loss_choice", ")", ":", "\n", "    ", "\"\"\"Compute the loss for the discriminator.\n\n    Note, only the discriminator weights should be updated using this loss.\n\n    Args:\n        logit_real: Outputs of the discriminator after seeing real samples.\n\n            .. note::\n                We assume a linear output layer.\n        logit_fake: Outputs of the discriminator after seeing fake samples.\n\n            .. note::\n                We assume a linear output layer.\n        loss_choice (int): Define what loss function is used to train the GAN.\n            Note, the choice of loss function also influences how the output\n            of the discriminator network  if reinterpreted or squashed (either\n            between ``[0,1]`` or an arbitrary real number).\n\n            The following choices are available.\n\n            - ``0``: Vanilla GAN (Goodfellow et al., 2014). Non-saturating\n              loss version. Note, we additionally apply one-sided label\n              smoothing for this loss.\n            - ``1``: Traditional LSGAN (Mao et al., 2018). See eq. 14 of\n              the paper. This loss corresponds to a parameter\n              choice :math:`a=0`, :math:`b=1` and :math:`c=1`.\n            - ``2``: Pearson Chi^2 LSGAN (Mao et al., 2018). See eq. 13.\n              Parameter choice: :math:`a=-1`, :math:`b=1` and :math:`c=0`.\n            - ``3``: Wasserstein GAN (Arjovski et al., 2017).\n\n    Returns:\n        The discriminator loss.\n    \"\"\"", "\n", "if", "loss_choice", "==", "0", ":", "# Vanilla GAN", "\n", "# We use the binary cross entropy.", "\n", "# Note, we use one-sided label-smoothing.", "\n", "        ", "fake", "=", "torch", ".", "sigmoid", "(", "logit_fake", ")", "\n", "real", "=", "torch", ".", "sigmoid", "(", "logit_real", ")", "\n", "r_loss", "=", "F", ".", "binary_cross_entropy", "(", "real", ",", "0.9", "*", "torch", ".", "ones_like", "(", "real", ")", ")", "\n", "f_loss", "=", "F", ".", "binary_cross_entropy", "(", "fake", ",", "torch", ".", "zeros_like", "(", "fake", ")", ")", "\n", "\n", "", "elif", "loss_choice", "==", "1", ":", "# Traditional LSGAN", "\n", "        ", "r_loss", "=", "F", ".", "mse_loss", "(", "logit_real", ",", "torch", ".", "ones_like", "(", "logit_real", ")", ")", "\n", "f_loss", "=", "F", ".", "mse_loss", "(", "logit_fake", ",", "torch", ".", "zeros_like", "(", "logit_fake", ")", ")", "\n", "\n", "", "elif", "loss_choice", "==", "2", ":", "# Pearson Chi^2 LSGAN", "\n", "        ", "r_loss", "=", "F", ".", "mse_loss", "(", "logit_real", ",", "torch", ".", "ones_like", "(", "logit_real", ")", ")", "\n", "f_loss", "=", "F", ".", "mse_loss", "(", "logit_fake", ",", "-", "torch", ".", "ones_like", "(", "logit_fake", ")", ")", "\n", "", "else", ":", "# WGAN", "\n", "        ", "r_loss", "=", "-", "logit_real", ".", "mean", "(", ")", "\n", "f_loss", "=", "logit_fake", ".", "mean", "(", ")", "\n", "\n", "", "return", "(", "r_loss", "+", "f_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.gan_helpers.gen_loss": [[84, 106], ["torch.sigmoid", "torch.sigmoid", "torch.binary_cross_entropy", "torch.ones_like", "torch.ones_like", "torch.mse_loss", "torch.ones_like", "torch.ones_like", "torch.mse_loss", "torch.zeros_like", "torch.zeros_like", "logit_fake.mean"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["", "def", "gen_loss", "(", "logit_fake", ",", "loss_choice", ")", ":", "\n", "    ", "\"\"\"Compute the loss for the generator.\n\n    Args:\n        (....): See docstring of function :func:`dis_loss`.\n\n    Returns:\n        The generator loss.\n    \"\"\"", "\n", "if", "loss_choice", "==", "0", ":", "# Vanilla GAN", "\n", "# We use the -log(D) trick.", "\n", "        ", "fake", "=", "torch", ".", "sigmoid", "(", "logit_fake", ")", "\n", "return", "F", ".", "binary_cross_entropy", "(", "fake", ",", "torch", ".", "ones_like", "(", "fake", ")", ")", "\n", "\n", "", "elif", "loss_choice", "==", "1", ":", "# Traditional LSGAN", "\n", "        ", "return", "F", ".", "mse_loss", "(", "logit_fake", ",", "torch", ".", "ones_like", "(", "logit_fake", ")", ")", "\n", "\n", "", "elif", "loss_choice", "==", "2", ":", "# Pearson Chi^2 LSGAN", "\n", "        ", "return", "F", ".", "mse_loss", "(", "logit_fake", ",", "torch", ".", "zeros_like", "(", "logit_fake", ")", ")", "\n", "\n", "", "else", ":", "# WGAN", "\n", "        ", "return", "-", "logit_fake", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.gan_helpers.accuracy": [[107, 140], ["logit_real.numel", "logit_fake.numel"], "function", ["None"], ["", "", "def", "accuracy", "(", "logit_real", ",", "logit_fake", ",", "loss_choice", ")", ":", "\n", "    ", "\"\"\"The accuracy of the discriminator.\n\n    It is computed based on the assumption that values greater than a threshold\n    are classified as real.\n\n    Note, the accuracy measure is only well defined for the Vanilla GAN.\n    Though, we just look at generally preferred value ranges and generalize\n    the concept of accuracy to the other GAN formulations using the\n    following thresholds:\n\n    - ``0.5`` for Vanilla GAN and Traditional LSGAN\n    - ``0`` for Pearson Chi^2 LSGAN and WGAN.\n\n    Args:\n        (....): See docstring of function :func:`dis_loss`.\n\n    Returns:\n        The relative accuracy of the discriminator.\n    \"\"\"", "\n", "T", "=", "0.5", "if", "loss_choice", "<", "2", "else", "0.0", "\n", "\n", "#if loss_choice == 0:", "\n", "#    fake = torch.sigmoid(logit_fake)", "\n", "#    real = torch.sigmoid(logit_real)", "\n", "\n", "# Note, values above 0 will be above 0.5 after being passed  through a", "\n", "# softmax. Therefore, we take the threshold 0 for logit activations, if the", "\n", "# logits are supposed to be passed through a softmax.", "\n", "T", "=", "0", "if", "loss_choice", "==", "0", "else", "T", "\n", "\n", "n_correct", "=", "(", "logit_real", ">", "T", ")", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "(", "logit_fake", "<=", "T", ")", ".", "float", "(", ")", ".", "sum", "(", ")", "\n", "return", "n_correct", "/", "(", "logit_real", ".", "numel", "(", ")", "+", "logit_fake", ".", "numel", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.gan_helpers.concat_mean_stats": [[141, 161], ["torch.mean", "torch.mean", "stats.expand.expand", "torch.cat", "torch.cat", "inputs.size"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["", "def", "concat_mean_stats", "(", "inputs", ")", ":", "\n", "    ", "\"\"\"Add mean statistics to discriminator input.\n\n    GANs often run into mode collapse since the discriminator sees every\n    sample in isolation. I.e., it cannot detect whether all samples in a batch\n    do look alike.\n\n    A simple way to allow the discriminator to have access to batch statistics\n    is to simply concatenate the mean (across batch dimension) of all\n    discriminator samples to each sample.\n\n    Args:\n        inputs: The input batch to the discriminator.\n\n    Returns:\n        The modified input batch.\n    \"\"\"", "\n", "stats", "=", "torch", ".", "mean", "(", "inputs", ",", "0", ",", "keepdim", "=", "True", ")", "\n", "stats", "=", "stats", ".", "expand", "(", "inputs", ".", "size", "(", ")", ")", "\n", "return", "torch", ".", "cat", "(", "[", "stats", ",", "inputs", "]", ",", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.hypernet_args": [[43, 187], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["def", "hypernet_args", "(", "parser", ",", "dhyper_chunks", "=", "11010", ",", "dhnet_arch", "=", "'50,50,50'", ",", "\n", "dtemb_size", "=", "32", ",", "demb_size", "=", "32", ",", "dhnet_act", "=", "'relu'", ",", "prefix", "=", "None", ",", "\n", "pf_name", "=", "None", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method `parse_cmd_arguments` to add\n    an argument group for hypernetwork-only arguments.\n\n    Arguments specified in this function:\n        - `hyper_chunks`\n        - `hnet_arch`\n        - `hnet_act`\n        - `temb_size`\n        - `emb_size`\n        - `hnet_noise_dim`\n        - `hnet_dropout_rate`\n        - `temb_std`\n        - `sa_hnet_num_layers`\n        - `sa_hnet_filters`\n        - `sa_hnet_kernels`\n        - `sa_hnet_attention_layers`\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n        dhyper_chunks: Default value of option \"hyper_chunks\".\n        dhnet_arch: Default value of option \"hnet_arch\".\n        dtemb_size: Default value of option \"temb_size\".\n        demb_size: Default value of option \"emb_size\".\n        dhnet_act: Default value of option \"hnet_act\".\n        prefix (optional): If arguments should be instantiated with a certain\n            prefix. E.g., a setup requires several hypernetworks, that may need\n            different settings. For instance: :code:`prefix='gen_'`.\n        pf_name (optional): A name of type of hypernetwork for which that prefix\n            is needed. For instance: :code:`prefix='generator'`.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "assert", "(", "prefix", "is", "None", "or", "pf_name", "is", "not", "None", ")", "\n", "\n", "heading", "=", "'Hypernet options'", "\n", "\n", "if", "prefix", "is", "None", ":", "\n", "        ", "prefix", "=", "''", "\n", "pf_name", "=", "''", "\n", "", "else", ":", "\n", "        ", "heading", "=", "'Hypernet options for %s network'", "%", "pf_name", "\n", "pf_name", "+=", "' '", "\n", "\n", "# Abbreviations.", "\n", "", "p", "=", "prefix", "\n", "n", "=", "pf_name", "\n", "\n", "### CHypernet options", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "heading", ")", "\n", "agroup", ".", "add_argument", "(", "'--%shyper_chunks'", "%", "p", ",", "type", "=", "str", ",", "default", "=", "dhyper_chunks", ",", "\n", "help", "=", "'The output size of the %shypernet. If -1, '", "%", "n", "+", "\n", "'then the hypernet output dimensionality is the '", "+", "\n", "'number of weights in the main network. '", "+", "\n", "'If it is a positive integer, the weights are '", "+", "\n", "'split into chunks and the hypernet gets as '", "+", "\n", "'additional input a trainable chunk-specific '", "+", "\n", "'embedding (see class '", "+", "\n", "'ChunkedHyperNetworkHandler for details). '", "+", "\n", "'If a string of two or three comma-separated '", "+", "\n", "'integers is provided, then a chunk of weights '", "+", "\n", "'is generated by a transpose convolutional '", "+", "\n", "'network with self-attention layers (see '", "+", "\n", "'class SAHyperNetwork for details). Default: '", "+", "\n", "'%(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%shnet_arch'", "%", "p", ",", "type", "=", "str", ",", "default", "=", "dhnet_arch", ",", "\n", "help", "=", "'A string of comma-separated integers, each '", "+", "\n", "'denoting the size of a hidden layer of the '", "+", "\n", "'%shypernetwork. This option is discarded '", "%", "n", "+", "\n", "'when using SAHyperNetwork class. Default: '", "+", "\n", "'%(default)s.'", ")", "\n", "### We decided to discard remaining weights rather", "\n", "### than generating them by a seperate network.", "\n", "#'Note, this option ' +", "\n", "#'also determines the architecture of the ' +", "\n", "#'\"remaining weight generator\" (see constructor ' +", "\n", "#'argument \"rem_layers\" of class SAHyperNetwork ' +", "\n", "#'for details). The option does not apply for a ' +", "\n", "#'full hypernetwork!')", "\n", "agroup", ".", "add_argument", "(", "'--%shnet_act'", "%", "p", ",", "type", "=", "str", ",", "default", "=", "dhnet_act", ",", "\n", "help", "=", "'Activation function used in the hypernetwork. '", "+", "\n", "'If \"linear\", no activation function is used. '", "+", "\n", "'Default: %(default)s.'", ",", "\n", "choices", "=", "[", "'linear'", ",", "'sigmoid'", ",", "'relu'", ",", "'elu'", "]", ")", "\n", "agroup", ".", "add_argument", "(", "'--%stemb_size'", "%", "p", ",", "type", "=", "int", ",", "default", "=", "dtemb_size", ",", "\n", "help", "=", "'Size of the task embedding space (input to '", "+", "\n", "'hypernet). Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%semb_size'", "%", "p", ",", "type", "=", "int", ",", "default", "=", "demb_size", ",", "\n", "help", "=", "'If using a hypernetwork that utilizes chunking'", "+", "\n", "', then this option defines the size of the '", "+", "\n", "'chunk embeddings. Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%shnet_noise_dim'", "%", "p", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'During training, a zero-mean noise vector will '", "+", "\n", "'be concatenated to the task embeddings to help '", "+", "\n", "'regularize the task embedding space and the '", "+", "\n", "'hypernetwork itself. During testing, zeros '", "+", "\n", "'will be concatenated. Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%shnet_dropout_rate'", "%", "p", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'Use dropout in the hypernet with the given '", "+", "\n", "'dropout probability (dropout is deactivated '", "+", "\n", "'for a rate of -1). Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%stemb_std'", "%", "p", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'If not -1, then this number will be '", "+", "\n", "'interpreted as the std of zero-mean Gaussian '", "+", "\n", "'noise that is used to perturb task embeddings '", "+", "\n", "'during training (as a regularization '", "+", "\n", "'technique). Default: %(default)s.'", ")", "\n", "# Specific to self-attention network!", "\n", "agroup", ".", "add_argument", "(", "'--%ssa_hnet_num_layers'", "%", "p", ",", "type", "=", "int", ",", "default", "=", "5", ",", "\n", "help", "=", "'Number of layers in the self-attention '", "+", "\n", "'hypernet. '", "+", "\n", "'See constructor argument \"num_layers\" of '", "+", "\n", "'class SAHyperNetwork for details. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%ssa_hnet_filters'", "%", "p", ",", "type", "=", "str", ",", "\n", "default", "=", "'128,512,256,128'", ",", "\n", "help", "=", "'A string of comma-separated integers, each '", "+", "\n", "'indicating the number of output channels for a '", "+", "\n", "'layer in the self-attention hypernet. '", "+", "\n", "'See constructor argument \"num_filters\" of '", "+", "\n", "'class SAHyperNetwork for details. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%ssa_hnet_kernels'", "%", "p", ",", "type", "=", "str", ",", "default", "=", "5", ",", "\n", "help", "=", "'A string of comma-separated integers, '", "+", "\n", "'indicating kernel sizes in the self-attention '", "+", "\n", "'hypernet. Note, to specify a distinct kernel '", "+", "\n", "'size per dimension of each layer, just enter a '", "+", "\n", "'list with twice the number of elements as '", "+", "\n", "'convolutional layers in the hypernet. '", "+", "\n", "'See constructor argument \"kernel_size\" of '", "+", "\n", "'class SAHyperNetwork for details. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%ssa_hnet_attention_layers'", "%", "p", ",", "type", "=", "str", ",", "\n", "default", "=", "'1,3'", ",", "\n", "help", "=", "'A string of comma-separated integers, '", "+", "\n", "'indicating after which layers of the hypernet'", "+", "\n", "'a self-attention unit should be added. '", "+", "\n", "'See constructor argument \"sa_units\" of '", "+", "\n", "'class SAHyperNetwork for details. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.main_net_args": [[190, 393], ["parser.add_argument_group", "warnings.warn", "ValueError", "len", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "len", "warnings.warn"], "function", ["None"], ["", "def", "main_net_args", "(", "parser", ",", "allowed_nets", "=", "[", "'fc'", "]", ",", "dfc_arch", "=", "'100,100'", ",", "\n", "dmlp_arch", "=", "'100,100'", ",", "show_net_act", "=", "True", ",", "dnet_act", "=", "'relu'", ",", "\n", "show_no_bias", "=", "False", ",", "show_dropout_rate", "=", "True", ",", "\n", "ddropout_rate", "=", "-", "1", ",", "show_specnorm", "=", "True", ",", "show_batchnorm", "=", "True", ",", "\n", "show_no_batchnorm", "=", "False", ",", "show_bn_no_running_stats", "=", "False", ",", "\n", "show_bn_distill_stats", "=", "False", ",", "\n", "show_bn_no_stats_checkpointing", "=", "False", ",", "\n", "prefix", "=", "None", ",", "pf_name", "=", "None", ")", ":", "\n", "    ", "\"\"\"This is a helper function for the function `parse_cmd_arguments` to add\n    an argument group for options to a main network.\n\n    Arguments specified in this function:\n        - `net_type`\n        - `fc_arch`\n        - `mlp_arch`\n        - `net_act`\n        - `no_bias`\n        - `dropout_rate`\n        - `specnorm`\n        - `batchnorm`\n        - `no_batchnorm`\n        - `bn_no_running_stats`\n        - `bn_distill_stats`\n        - `bn_no_stats_checkpointing`\n\n    Args:\n        parser (:class:`argparse.ArgumentParser`): The argument parser to which\n            the argument group should be added.\n        allowed_nets (list): List of allowed network identifiers. The following\n            identifiers are considered (note, we also reference the network that\n            each network type targets):\n\n            - ``mlp``: :class:`mnets.mlp.MLP`\n            - ``resnet``: :class:`mnets.resnet.ResNet`\n            - ``zenke``: :class:`mnets.zenkenet.ZenkeNet`\n            - ``bio_conv_net``: :class:`mnets.bio_conv_net.BioConvNet`\n            - ``fc``: :class:`mnets.mlp.MLP`\n\n              .. deprecated:: 1.0\n                  Please use network type ``mlp`` instead of ``fc``. Network\n                  type ``fc`` will be removed in the future.\n\n            .. warning::\n                Default value of ``allowed_nets`` is going to change to\n                :code:`['mlp']` in the future.\n        dfc_arch: Default value of option `fc_arch`.\n\n            .. deprecated:: 1.0\n                  Please use network type ``mlp`` and argument ``dfc_arch``\n                  instead.\n        dmlp_arch: Default value of option `mlp_arch`.\n        show_net_act (bool): Whether the option `net_act` should be provided.\n        dnet_act: Default value of option `net_act`.\n        show_no_bias (bool): Whether the option `no_bias` should be provided.\n        show_dropout_rate (bool): Whether the option `dropout_rate` should be\n            provided.\n        ddropout_rate: Default value of option ``dropout_rate``.\n        show_specnorm (bool): Whether the option `specnorm` should be provided.\n        show_batchnorm (bool): Whether the option `batchnorm` should be\n            provided.\n        show_no_batchnorm (bool): Whether the option `no_batchnorm` should be\n            provided.\n        show_bn_no_running_stats (bool): Whether the option\n            `bn_no_running_stats` should be provided.\n        show_bn_distill_stats (bool): Whether the option `bn_distill_stats`\n            should be provided.\n        show_bn_no_stats_checkpointing (bool): Whether the option\n            `bn_no_stats_checkpointing` should be provided.\n        prefix (optional): If arguments should be instantiated with a certain\n            prefix. E.g., a setup requires several main network, that may need\n            different settings. For instance: prefix=:code:`prefix='gen_'`.\n        pf_name (optional): A name of the type of main net for which that prefix\n            is needed. For instance: prefix=:code:`'generator'`.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "assert", "(", "prefix", "is", "None", "or", "pf_name", "is", "not", "None", ")", "\n", "\n", "# TODO Delete 'fc' from list.", "\n", "for", "nt", "in", "allowed_nets", ":", "\n", "        ", "assert", "(", "nt", "in", "[", "'fc'", ",", "'mlp'", ",", "'resnet'", ",", "'zenke'", ",", "'bio_conv_net'", "]", ")", "\n", "\n", "", "assert", "(", "not", "show_batchnorm", "or", "not", "show_no_batchnorm", ")", "\n", "\n", "# TODO 'fc' should be renamed to 'mlp'.", "\n", "if", "'fc'", "in", "allowed_nets", "and", "len", "(", "allowed_nets", ")", "==", "1", ":", "\n", "        ", "warn", "(", "'Network type \"fc\" is deprecated. Default value of argument '", "+", "\n", "'\"allowed_nets\" will be changed to [\\'mlp\\'] in the future!'", ",", "\n", "DeprecationWarning", ")", "\n", "", "elif", "'fc'", "in", "allowed_nets", ":", "\n", "# TODO change warning into error at some point.", "\n", "        ", "warn", "(", "'Network type \"fc\" is deprecated! Use \"mlp\" instead.'", ",", "\n", "DeprecationWarning", ")", "\n", "", "if", "'fc'", "in", "allowed_nets", "and", "'mlp'", "in", "allowed_nets", ":", "\n", "# Doesn't make sense to have both.", "\n", "        ", "raise", "ValueError", "(", "'Network type names \"fc\" and \"mlp\" refer to the '", "+", "\n", "'same network type! Note, \"fc\" is deprecated.'", ")", "\n", "\n", "", "heading", "=", "'Main network options'", "\n", "\n", "if", "prefix", "is", "None", ":", "\n", "        ", "prefix", "=", "''", "\n", "pf_name", "=", "''", "\n", "", "else", ":", "\n", "        ", "heading", "=", "'Main network options for %s network'", "%", "pf_name", "\n", "pf_name", "+=", "' '", "\n", "\n", "# Abbreviations.", "\n", "", "p", "=", "prefix", "\n", "n", "=", "pf_name", "\n", "\n", "### Main network options.", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "heading", ")", "\n", "\n", "if", "len", "(", "allowed_nets", ")", ">", "1", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%snet_type'", "%", "p", ",", "type", "=", "str", ",", "\n", "default", "=", "allowed_nets", "[", "0", "]", ",", "\n", "help", "=", "'Type of network to be used for this %s '", "%", "n", "+", "\n", "'network. Default: %(default)s.'", ",", "\n", "choices", "=", "allowed_nets", ")", "\n", "\n", "# DELETEME once we delete option 'fc'.", "\n", "", "if", "'fc'", "in", "allowed_nets", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%sfc_arch'", "%", "p", ",", "type", "=", "str", ",", "default", "=", "dfc_arch", ",", "\n", "help", "=", "'If using a \"fc\" %s network, this will '", "%", "n", "+", "\n", "'specify the hidden layers. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "\n", "", "if", "'mlp'", "in", "allowed_nets", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%smlp_arch'", "%", "p", ",", "type", "=", "str", ",", "default", "=", "dmlp_arch", ",", "\n", "help", "=", "'If using a \"mlp\" %s network, this will '", "%", "n", "+", "\n", "'specify the hidden layers. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "\n", "# Note, if you want to add more activation function choices here, you have", "\n", "# to add them to the corresponding function `utils.misc.str_to_act` as well!", "\n", "", "if", "show_net_act", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%snet_act'", "%", "p", ",", "type", "=", "str", ",", "default", "=", "dnet_act", ",", "\n", "help", "=", "'Activation function used in the %s network.'", "%", "n", "+", "\n", "'If \"linear\", no activation function is used. '", "+", "\n", "'Default: %(default)s.'", ",", "\n", "choices", "=", "[", "'linear'", ",", "'sigmoid'", ",", "'relu'", ",", "'elu'", "]", ")", "\n", "\n", "", "if", "show_no_bias", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%sno_bias'", "%", "p", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'No biases will be used in the %s network. '", "%", "n", "+", "\n", "'Note, does not affect normalization (like '", "+", "\n", "'batchnorm).'", ")", "\n", "\n", "", "if", "show_dropout_rate", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%sdropout_rate'", "%", "p", ",", "type", "=", "float", ",", "\n", "default", "=", "ddropout_rate", ",", "\n", "help", "=", "'Use dropout in the %s network with the '", "%", "n", "+", "\n", "'given dropout probability (dropout is '", "+", "\n", "'deactivated for a rate of -1). Default: '", "+", "\n", "'%(default)s.'", ")", "\n", "\n", "", "if", "show_specnorm", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%sspecnorm'", "%", "p", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable spectral normalization in the '", "+", "\n", "'%s network.'", "%", "n", ")", "\n", "\n", "### Batchnorm related options.", "\n", "", "if", "show_batchnorm", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%sbatchnorm'", "%", "p", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable batchnorm in the %s network.'", "%", "n", ")", "\n", "", "if", "show_no_batchnorm", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%sno_batchnorm'", "%", "p", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Disable batchnorm in the %s network.'", "%", "n", ")", "\n", "\n", "", "if", "show_bn_no_running_stats", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%sbn_no_running_stats'", "%", "p", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'If batch normalization is used, then this '", "+", "\n", "'option will deactivate the tracking '", "+", "\n", "'of running statistics. Hence, statistics '", "+", "\n", "'computed per batch will be used during '", "+", "\n", "'evaluation.'", ")", "\n", "\n", "", "if", "show_bn_distill_stats", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%sbn_distill_stats'", "%", "p", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'If batch normalization is used, '", "+", "\n", "'then usually the running statistics are '", "+", "\n", "'checkpointed for every task (e.g., in '", "+", "\n", "'continual learning), which has linearly '", "+", "\n", "'increasing memory requirements. If '", "+", "\n", "'this option is activated, the running '", "+", "\n", "'statistics will be distilled into the '", "+", "\n", "'hypernetwork after training each task, '", "+", "\n", "'such that only the statistics of the '", "+", "\n", "'current and previous task have to be '", "+", "\n", "'explicitly kept in  memory'", ")", "\n", "\n", "", "if", "show_bn_no_stats_checkpointing", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%sbn_no_stats_checkpointing'", "%", "p", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "'If batch normalization is used, then'", "+", "\n", "'this option will prevent the checkpointing'", "+", "\n", "'of batchnorm statistics for every task.'", "+", "\n", "'In this case, one set of statistics is '", "+", "\n", "'used for all tasks.'", ")", "\n", "\n", "", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.init_args": [[394, 446], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "init_args", "(", "parser", ",", "custom_option", "=", "True", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method `parse_cmd_arguments` to add\n    an argument group for options regarding network initialization.\n\n    Arguments specified in this function:\n        - `custom_network_init`\n        - `normal_init`\n        - `std_normal_init`\n        - `std_normal_temb`\n        - `std_normal_emb`\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n        custom_option: Whether the option `custom_network_init` should be\n            provided.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "### Weight initialization.", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Network initialization options'", ")", "\n", "if", "custom_option", ":", "\n", "# This option becomes important if posthoc custom init is not that", "\n", "# trivial anymore (e.g., if networks use batchnorm). Then, the network", "\n", "# init must be customized for each such network.", "\n", "        ", "agroup", ".", "add_argument", "(", "'--custom_network_init'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether network parameters should be '", "+", "\n", "'initialized in a custom way. If this flag '", "+", "\n", "'is set, then Xavier initialization is '", "+", "\n", "'applied to weight tensors (zero '", "+", "\n", "'initialization for bias vectors). The '", "+", "\n", "'initialization of chunk and task '", "+", "\n", "'embeddings is independent of this option.'", ")", "\n", "", "agroup", ".", "add_argument", "(", "'--normal_init'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use weight initialization from a zero-mean '", "+", "\n", "'normal with std defined by the argument '", "+", "\n", "'\\'std_normal_init\\'. Otherwise, Xavier '", "+", "\n", "'initialization is used. Biases are '", "+", "\n", "'initialized to zero.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--std_normal_init'", ",", "type", "=", "float", ",", "default", "=", "0.02", ",", "\n", "help", "=", "'If normal initialization is used, this will '", "+", "\n", "'be the standard deviation used. Default: '", "+", "\n", "'%(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--std_normal_temb'", ",", "type", "=", "float", ",", "default", "=", "1.", ",", "\n", "help", "=", "'Std when initializing task embeddings. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--std_normal_emb'", ",", "type", "=", "float", ",", "default", "=", "1.", ",", "\n", "help", "=", "'If a chunked hypernetwork is used (including '", "+", "\n", "'self-attention hypernet), then this will be '", "+", "\n", "'the std of their initialization. Default: '", "+", "\n", "'%(default)s.'", ")", "\n", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.miscellaneous_args": [[447, 525], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["None"], ["", "def", "miscellaneous_args", "(", "parser", ",", "big_data", "=", "True", ",", "synthetic_data", "=", "False", ",", "\n", "show_plots", "=", "False", ",", "no_cuda", "=", "False", ",", "dout_dir", "=", "None", ",", "\n", "show_publication_style", "=", "False", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method `parse_cmd_arguments` to add\n    an argument group for miscellaneous arguments.\n\n    Arguments specified in this function:\n        - `num_workers`\n        - `out_dir`\n        - `use_cuda`\n        - `no_cuda`\n        - `loglevel_info`\n        - `deterministic_run`\n        - `publication_style`\n        - `show_plots`\n        - `data_random_seed`\n        - `random_seed`\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n        big_data: If the program processes big datasets that need to be loaded\n            from disk on the fly. In this case, more options are provided.\n        synthetic_data: If data is randomly generated, then we want to decouple\n            this randomness from the training randomness.\n        show_plots: Whether the option `show_plots` should be provided.\n        no_cuda: If True, the user has to explicitly set the flag `--use_cuda`\n            rather than using CUDA by default.\n        dout_dir (optional): Default value of option `out_dir`. If :code:`None`,\n            the default value will be `./out/run_<YY>-<MM>-<DD>_<hh>-<mm>-<ss>`\n            that contains the current date and time.\n        show_publication_style: Whether the option `publication_style` should be\n            provided.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "if", "dout_dir", "is", "None", ":", "\n", "        ", "dout_dir", "=", "'./out/run_'", "+", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S'", ")", "\n", "\n", "### Miscellaneous arguments", "\n", "", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Miscellaneous options'", ")", "\n", "if", "big_data", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--num_workers'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "8", ",", "\n", "help", "=", "'Number of workers per dataset loader. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "", "agroup", ".", "add_argument", "(", "'--out_dir'", ",", "type", "=", "str", ",", "default", "=", "dout_dir", ",", "\n", "help", "=", "'Where to store the outputs of this simulation.'", ")", "\n", "if", "no_cuda", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--use_cuda'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Flag to enable GPU usage.'", ")", "\n", "", "else", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--no_cuda'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Flag to disable GPU usage.'", ")", "\n", "", "agroup", ".", "add_argument", "(", "'--loglevel_info'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'If the console log level should be raised '", "+", "\n", "'from DEBUG to INFO.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--deterministic_run'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable deterministic CuDNN behavior. Note, that'", "+", "\n", "'CuDNN algorithms are not deterministic by '", "+", "\n", "'default and results might not be reproducible '", "+", "\n", "'unless this option is activated. Note, that '", "+", "\n", "'this may slow down training significantly!'", ")", "\n", "if", "show_publication_style", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--publication_style'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether plots should be publication-ready.'", ")", "\n", "", "if", "show_plots", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--show_plots'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Whether plots should be shown.'", ")", "\n", "", "if", "synthetic_data", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--data_random_seed'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "'The data is randomly generated at every '", "+", "\n", "'run. This seed ensures that the randomness '", "+", "\n", "'during data generation is decoupled from the '", "+", "\n", "'training randomness. Default: %(default)s.'", ")", "\n", "", "agroup", ".", "add_argument", "(", "'--random_seed'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "42", ",", "\n", "help", "=", "'Random seed. Default: %(default)s.'", ")", "\n", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.eval_args": [[526, 558], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "eval_args", "(", "parser", ",", "dval_iter", "=", "500", ",", "show_val_batch_size", "=", "False", ",", "\n", "dval_batch_size", "=", "256", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method `parse_cmd_arguments` to add\n    an argument group for validation and testing options.\n\n    Arguments specified in this function:\n        - `val_iter`\n        - `val_batch_size`\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n        dval_iter: Default value of argument `val_iter`.\n        show_val_batch_size: Whether the `val_batch_size` argument should be\n            shown.\n        dval_batch_size: Default value of argument `val_batch_size`.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "### Eval arguments", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Evaluation options'", ")", "\n", "agroup", ".", "add_argument", "(", "'--val_iter'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "dval_iter", ",", "\n", "help", "=", "'How often the validation should be performed '", "+", "\n", "'during training. Default: %(default)s.'", ")", "\n", "\n", "if", "show_val_batch_size", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--val_batch_size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "default", "=", "dval_batch_size", ",", "\n", "help", "=", "'Batch size during validation/testing. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "\n", "", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.train_args": [[559, 665], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "train_args", "(", "parser", ",", "show_lr", "=", "False", ",", "dlr", "=", "0.1", ",", "show_epochs", "=", "False", ",", "depochs", "=", "-", "1", ",", "\n", "dbatch_size", "=", "32", ",", "dn_iter", "=", "100001", ",", "show_use_adam", "=", "False", ",", "\n", "dadam_beta1", "=", "0.9", ",", "show_use_rmsprop", "=", "False", ",", "show_use_adadelta", "=", "False", ",", "\n", "show_use_adagrad", "=", "False", ",", "show_clip_grad_value", "=", "False", ",", "\n", "show_clip_grad_norm", "=", "False", ",", "show_adam_beta1", "=", "False", ",", "\n", "show_momentum", "=", "True", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method `parse_cmd_arguments` to add\n    an argument group for options to configure network training.\n\n    Arguments specified in this function:\n        - `batch_size`\n        - `n_iter`\n        - `epochs`\n        - `lr`\n        - `momentum`\n        - `weight_decay`\n        - `use_adam`\n        - `adam_beta1`\n        - `use_rmsprop`\n        - `use_adadelta`\n        - `use_adagrad`\n        - `clip_grad_value`\n        - `clip_grad_norm`\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n        show_lr: Whether the `lr` - learning rate - argument should be shown.\n            Might not be desired if individual learning rates per optimizer\n            should be specified.\n        dlr: Default value for option `lr`.\n        show_epochs: Whether the `epochs` argument should be shown.\n        depochs: Default value for option `epochs`.\n        dbatch_size: Default value for option `batch_size`.\n        dn_iter: Default value for option `n_iter`.\n        show_use_adam: Whether the `use_adam` argument should be shown. Will\n            also show the `adam_beta1` argument.\n        dadam_beta1: Default value for option `adam_beta1`.\n        show_use_rmsprop: Whether the `use_rmsprop` argument should be shown.\n        show_use_adadelta: Whether the `use_adadelta` argument should be shown.\n        show_use_adagrad: Whether the `use_adagrad` argument should be shown.\n        show_clip_grad_value: Whether the `clip_grad_value` argument should be\n            shown.\n        show_clip_grad_norm: Whether the `clip_grad_norm` argument should be\n            shown.\n        show_adam_beta1: Whether the `adam_beta1` argument should be\n            shown. Note, this argument is also shown when ``show_use_adam`` is\n            ``True``.\n        show_momentum: Whether the `momentum` argument should be\n            shown.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "### Training options.", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Training options'", ")", "\n", "agroup", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "default", "=", "dbatch_size", ",", "\n", "help", "=", "'Training batch size. Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--n_iter'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "dn_iter", ",", "\n", "help", "=", "'Number of training iterations per task. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "if", "show_epochs", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "depochs", ",", "\n", "help", "=", "'Number of epochs per task. If -1, \"n_iter\" '", "+", "\n", "'is used instead. Default: %(default)s.'", ")", "\n", "", "if", "show_lr", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "dlr", ",", "\n", "help", "=", "'Learning rate of optimizer(s). Default: '", "+", "\n", "'%(default)s.'", ")", "\n", "", "if", "show_momentum", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--momentum'", ",", "type", "=", "float", ",", "default", "=", "0.0", ",", "\n", "help", "=", "'Momentum of the optimizer (only used in '", "+", "\n", "'SGD and RMSprop). Default: %(default)s.'", ")", "\n", "", "agroup", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "'Weight decay of the optimizer(s). Default: '", "+", "\n", "'%(default)s.'", ")", "\n", "if", "show_use_adam", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--use_adam'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use Adam rather than SGD optimizer.'", ")", "\n", "", "if", "show_use_adam", "or", "show_adam_beta1", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--adam_beta1'", ",", "type", "=", "float", ",", "default", "=", "dadam_beta1", ",", "\n", "help", "=", "'The \"beta1\" parameter when using torch.optim.'", "+", "\n", "'Adam as optimizer. Default: %(default)s.'", ")", "\n", "", "if", "show_use_rmsprop", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--use_rmsprop'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use RMSprop rather than SGD optimizer.'", ")", "\n", "", "if", "show_use_adadelta", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--use_adadelta'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use Adadelta rather than SGD optimizer.'", ")", "\n", "", "if", "show_use_adagrad", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--use_adagrad'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use Adagrad rather than SGD optimizer.'", ")", "\n", "\n", "", "if", "show_clip_grad_value", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--clip_grad_value'", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'If not \"-1\", gradients will be clipped using '", "+", "\n", "'\"torch.nn.utils.clip_grad_value_\". Default: '", "+", "\n", "'%(default)s.'", ")", "\n", "\n", "", "if", "show_clip_grad_norm", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--clip_grad_norm'", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'If not \"-1\", gradient norms will be clipped '", "+", "\n", "'using \"torch.nn.utils.clip_grad_norm_\". '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "\n", "", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.cl_args": [[666, 744], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "cl_args", "(", "parser", ",", "show_beta", "=", "True", ",", "dbeta", "=", "0.01", ",", "show_from_scratch", "=", "False", ",", "\n", "show_multi_head", "=", "False", ",", "show_cl_scenario", "=", "False", ",", "\n", "show_split_head_cl3", "=", "True", ",", "dcl_scenario", "=", "1", ",", "\n", "show_num_tasks", "=", "False", ",", "dnum_tasks", "=", "1", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method `parse_cmd_arguments` to add\n    an argument group for typical continual learning arguments.\n\n    Arguments specified in this function:\n        - `beta`\n        - `train_from_scratch`\n        - `multi_head`\n        - `cl_scenario`\n        - `split_head_cl3`\n        - `num_tasks`\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n        show_beta: Whether option `beta` should be shown.\n        dbeta: Default value of option `beta`.\n        show_from_scratch: Whether option `train_from_scratch` should be shown.\n        show_multi_head: Whether option `multi_head` should be shown.\n        show_cl_scenario: Whether option `cl_scenario` should be shown.\n        show_split_head_cl3: Whether option `split_head_cl3` should be shown.\n            Only has an effect if ``show_cl_scenario`` is ``True``.\n        dcl_scenario: Default value of option `cl_scenario`.\n        show_num_tasks: Whether option `num_tasks` should be shown.\n        dnum_tasks: Default value of option `num_tasks`.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "### Continual learning options.", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Continual learning options'", ")", "\n", "\n", "if", "show_beta", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--beta'", ",", "type", "=", "float", ",", "default", "=", "dbeta", ",", "\n", "help", "=", "'Trade-off for the CL regularizer. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "\n", "", "if", "show_from_scratch", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--train_from_scratch'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'If set, all networks are recreated after '", "+", "\n", "'training on each task. Hence, training starts '", "+", "\n", "'from scratch.'", ")", "\n", "\n", "", "if", "show_multi_head", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--multi_head'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use a multihead setting, where each task has '", "+", "\n", "'its own output head.'", ")", "\n", "\n", "", "if", "show_cl_scenario", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--cl_scenario'", ",", "type", "=", "int", ",", "default", "=", "dcl_scenario", ",", "\n", "help", "=", "'Continual learning scenarios according to '", "+", "\n", "'https://arxiv.org/pdf/1809.10635.pdf. '", "+", "\n", "'\"1\" - Task-incremental learning; '", "+", "\n", "'\"2\" - Domain-incremental learning; '", "+", "\n", "'\"3\" - Class-incremental learning. '", "+", "\n", "'Default: %(default)s.'", ",", "\n", "choices", "=", "[", "1", ",", "2", ",", "3", "]", ")", "\n", "\n", "", "if", "show_cl_scenario", "and", "show_split_head_cl3", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--split_head_cl3'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'CL scenario 3 (CL3, cmp. \"cl_scenario\") '", "+", "\n", "'originally requires to compute the softmax '", "+", "\n", "'across all output neurons. Though, if a '", "+", "\n", "'task-conditioned hypernetwork is used, the '", "+", "\n", "'task identity had to be inferred a priori. '", "+", "\n", "'Hence, in CL2 and CL3 we always know the '", "+", "\n", "'task identity, which is why we can also '", "+", "\n", "'compute the softmax over single output '", "+", "\n", "'heads in CL3 using this option.'", ")", "\n", "\n", "", "if", "show_num_tasks", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--num_tasks'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "default", "=", "dnum_tasks", ",", "\n", "help", "=", "'Number of tasks. Default: %(default)s.'", ")", "\n", "\n", "", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.gan_args": [[745, 796], ["warnings.warn", "parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "gan_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method `parse_cmd_arguments` to add\n    an argument group for options to configure the generator and discriminator\n    network.\n\n    .. deprecated:: 1.0\n        Please use method :func:`main_net_args` and :func:`generator_args`\n        instead.\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "warn", "(", "'Please use method \"main_net_args\" and \"generator_args\" instead.'", ",", "\n", "DeprecationWarning", ")", "\n", "\n", "### Training options.", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Generator and discriminator network '", "+", "\n", "'options'", ")", "\n", "for", "n", ",", "m", "in", "[", "(", "'gen'", ",", "'generator'", ")", ",", "(", "'dis'", ",", "'discriminator'", ")", "]", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%s_net_type'", "%", "n", ",", "type", "=", "str", ",", "default", "=", "'fc'", ",", "\n", "help", "=", "'Type of network to be used for the %s '", "%", "m", "+", "\n", "'network. Default: %(default)s.'", ",", "\n", "choices", "=", "[", "'fc'", "]", ")", "\n", "agroup", ".", "add_argument", "(", "'--%s_fc_arch'", "%", "n", ",", "type", "=", "str", ",", "default", "=", "'100,100'", ",", "\n", "help", "=", "'If using a \"fc\" %s network, this will '", "%", "m", "+", "\n", "'specify the hidden layers. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%s_net_act'", "%", "n", ",", "type", "=", "str", ",", "default", "=", "'relu'", ",", "\n", "help", "=", "'Activation function used in the %s network.'", "%", "m", "+", "\n", "'If \"linear\", no activation function is used. '", "+", "\n", "'Default: %(default)s.'", ",", "\n", "choices", "=", "[", "'linear'", ",", "'sigmoid'", ",", "'relu'", ",", "'elu'", "]", ")", "\n", "agroup", ".", "add_argument", "(", "'--%s_dropout_rate'", "%", "n", ",", "type", "=", "float", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'Use dropout in the %s with the given '", "%", "m", "+", "\n", "'dropout probability (dropout is deactivated '", "+", "\n", "'for a rate of -1). Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--%s_batchnorm'", "%", "n", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable batchnorm in the %s.'", "%", "m", ")", "\n", "agroup", ".", "add_argument", "(", "'--%s_specnorm'", "%", "n", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable spectral normalization in the %s.'", "%", "m", ")", "\n", "\n", "", "agroup", ".", "add_argument", "(", "'--latent_dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "default", "=", "3", ",", "\n", "help", "=", "'Dimensionality of the latent vector (noise '", "+", "\n", "'input to the generator. Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--latent_std'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Standard deviation of the latent space. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.generator_args": [[797, 820], ["agroup.add_argument", "agroup.add_argument"], "function", ["None"], ["", "def", "generator_args", "(", "agroup", ",", "dlatent_dim", "=", "3", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method `parse_cmd_arguments` (or more\n    specifically an auxillary method to :func:`train_args`) to add arguments to\n    an argument group for options specific to a main network that should act as\n    a generator.\n\n    Arguments specified in this function:\n        - `latent_dim`\n        - `latent_std`\n\n    Args:\n        agroup: The argument group returned by, for instance, function\n            :func:`main_net_args`.\n        dlatent_dim: Default value of option `latent_dim`.\n    \"\"\"", "\n", "### Generator options.", "\n", "agroup", ".", "add_argument", "(", "'--latent_dim'", ",", "type", "=", "int", ",", "metavar", "=", "'N'", ",", "\n", "default", "=", "dlatent_dim", ",", "\n", "help", "=", "'Dimensionality of the latent vector (noise '", "+", "\n", "'input to the generator. Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--latent_std'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Standard deviation of the latent space. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.data_args": [[821, 850], ["parser.add_argument_group", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "data_args", "(", "parser", ",", "show_disable_data_augmentation", "=", "False", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the function `parse_cmd_arguments` to add\n    an argument group for typical dataset related options.\n\n    Arguments specified in this function:\n        - `disable_data_augment`\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n        show_disable_data_augmentation: Whether option\n            `disable_data_augmentation` should be shown.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "### Continual learning options.", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Data-specific options'", ")", "\n", "\n", "# FIXME At the moment, this is the only argument added by this function!", "\n", "assert", "(", "show_disable_data_augmentation", ")", "\n", "\n", "if", "show_disable_data_augmentation", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--disable_data_augmentation'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'If activated, no data augmentation will be '", "+", "\n", "'applied. Note, this option only affects '", "+", "\n", "'datasets that have preprocessing implemented '", "+", "\n", "'(such CIFAR-10).'", ")", "\n", "\n", "", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.check_invalid_argument_usage": [[851, 973], ["enumerate", "hasattr", "enumerate", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "ValueError", "hasattr", "hasattr", "ValueError", "hasattr", "hasattr", "ValueError", "hasattr", "ValueError", "ValueError", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "getattr", "getattr", "ValueError", "hasattr"], "function", ["None"], ["", "def", "check_invalid_argument_usage", "(", "args", ")", ":", "\n", "    ", "\"\"\"This method checks for common conflicts when using the arguments defined\n    by methods in this module.\n\n    The following things will be checked:\n\n        - Based on the optimizer choices specified in :func:`train_args`, we\n          assert here that only one optimizer is selected at a time.\n        - Assert that `clip_grad_value` and `clip_grad_norm` are not set at the\n          same time.\n        - Assert that `split_head_cl3` is only set for `cl_scenario=3`\n        - Assert that the arguments specified in function :func:`main_net_args`\n          are correctly used.\n\n          .. note::\n              The checks can't handle prefixes yet.\n\n    Args:\n        args: The parsed command-line arguments, i.e., the output of method\n            :meth:`argparse.ArgumentParser.parse_args`.\n\n    Raises:\n        ValueError: If invalid argument combinations are used.\n    \"\"\"", "\n", "optim_args", "=", "[", "'use_adam'", ",", "'use_rmsprop'", ",", "'use_adadelta'", ",", "'use_adagrad'", "]", "\n", "for", "i", ",", "o1", "in", "enumerate", "(", "optim_args", ")", ":", "\n", "        ", "if", "not", "hasattr", "(", "args", ",", "o1", ")", ":", "\n", "            ", "continue", "\n", "\n", "", "for", "j", ",", "o2", "in", "enumerate", "(", "optim_args", ")", ":", "\n", "            ", "if", "i", "==", "j", "or", "not", "hasattr", "(", "args", ",", "o2", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "getattr", "(", "args", ",", "o1", ")", "and", "getattr", "(", "args", ",", "o2", ")", ":", "\n", "                ", "raise", "ValueError", "(", "'Cannot simultaneously use 2 optimizers '", "+", "\n", "'(arguments \"%s\" and \"%s\").'", "%", "(", "o1", ",", "o2", ")", ")", "\n", "\n", "", "", "", "if", "hasattr", "(", "args", ",", "'clip_grad_value'", ")", "and", "hasattr", "(", "args", ",", "'clip_grad_norm'", ")", ":", "\n", "        ", "if", "args", ".", "clip_grad_value", "!=", "-", "1", "and", "args", ".", "clip_grad_norm", "!=", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "'Cannot simultaneously clip gradiant values and '", "+", "\n", "'gradient norm.'", ")", "\n", "\n", "", "", "if", "hasattr", "(", "args", ",", "'cl_scenario'", ")", "and", "hasattr", "(", "args", ",", "'split_head_cl3'", ")", ":", "\n", "        ", "if", "args", ".", "cl_scenario", "!=", "3", "and", "args", ".", "split_head_cl3", ":", "\n", "            ", "raise", "ValueError", "(", "'Flag \"split_head_cl3\" may only be set when '", "+", "\n", "'running CL scenario 3 (CL3)!'", ")", "\n", "\n", "# TODO if `custom_network_init` is used but deactivated, then the other init", "\n", "# options have no effect -> user should be warned.", "\n", "\n", "### Check consistent use of arguments from `main_net_args`.", "\n", "# FIXME These checks don't deal with prefixes yet!", "\n", "", "", "if", "hasattr", "(", "args", ",", "'net_type'", ")", "and", "hasattr", "(", "args", ",", "'dropout_rate'", ")", ":", "\n", "        ", "if", "args", ".", "net_type", "in", "[", "'resnet'", ",", "'bio_conv_net'", "]", "and", "args", ".", "dropout_rate", "!=", "-", "1", ":", "\n", "            ", "warn", "(", "'Dropout is not implement for network %s.'", "%", "args", ".", "net_type", ")", "\n", "\n", "", "", "if", "hasattr", "(", "args", ",", "'net_type'", ")", "and", "hasattr", "(", "args", ",", "'specnorm'", ")", ":", "\n", "        ", "if", "args", ".", "net_type", "in", "[", "'resnet'", ",", "'zenke'", ",", "'bio_conv_net'", "]", "and", "args", ".", "specnorm", ":", "\n", "            ", "warn", "(", "'Spectral Normalization is not implement for network %s.'", "\n", "%", "args", ".", "net_type", ")", "\n", "\n", "", "", "if", "hasattr", "(", "args", ",", "'net_type'", ")", "and", "hasattr", "(", "args", ".", "net_act", ")", ":", "\n", "        ", "if", "args", ".", "net_type", "in", "[", "'resnet'", ",", "'zenke'", "]", "and", "args", ".", "net_act", "!=", "'relu'", ":", "\n", "            ", "warn", "(", "'%s network uses ReLU activation functions. '", "%", "args", ".", "net_type", "+", "\n", "'Ignoring option \"net_act\".'", ")", "\n", "\n", "", "if", "args", ".", "net_type", "in", "[", "'bio_conv_net'", "]", ":", "# and args.net_act != 'tanh':", "\n", "            ", "warn", "(", "'%s network uses Tanh activation functions. '", "%", "args", ".", "net_type", "+", "\n", "'Ignoring option \"net_act\".'", ")", "\n", "\n", "", "", "if", "hasattr", "(", "args", ",", "'net_type'", ")", "and", "hasattr", "(", "args", ".", "no_bias", ")", ":", "\n", "# FIXME Should be configurable for resnet in future!", "\n", "        ", "if", "args", ".", "net_type", "in", "[", "'resnet'", ",", "'zenke'", ",", "'bio_conv_net'", "]", "and", "args", ".", "no_bias", ":", "\n", "            ", "warn", "(", "'%s network always uses biases!'", "%", "args", ".", "net_type", ")", "\n", "\n", "", "", "bn_used", "=", "False", "\n", "if", "hasattr", "(", "args", ",", "'batchnorm'", ")", ":", "\n", "        ", "bn_used", "=", "args", ".", "batchnorm", "\n", "", "elif", "hasattr", "(", "args", ",", "'no_batchnorm'", ")", ":", "\n", "        ", "bn_used", "=", "not", "args", ".", "no_batchnorm", "\n", "", "else", ":", "\n", "# We don't know whether it is used.", "\n", "        ", "bn_used", "=", "None", "\n", "\n", "", "if", "bn_used", "is", "not", "None", "and", "bn_used", "and", "hasattr", "(", "args", ",", "'net_type'", ")", ":", "\n", "        ", "if", "args", ".", "net_type", "in", "[", "'zenke'", ",", "'bio_conv_net'", "]", ":", "\n", "            ", "warn", "(", "'Batch Normalization is not implemented for network %s.'", "\n", "%", "args", ".", "net_type", ")", "\n", "\n", "", "", "if", "bn_used", "is", "not", "None", "and", "hasattr", "(", "args", ",", "'bn_no_running_stats'", ")", ":", "\n", "        ", "if", "not", "bn_used", "and", "args", ".", "bn_no_running_stats", ":", "\n", "            ", "warn", "(", "'Option \"bn_no_running_stats\" has no effect if batch '", "+", "\n", "'normalization not activated.'", ")", "\n", "\n", "", "", "if", "bn_used", "is", "not", "None", "and", "hasattr", "(", "args", ",", "'bn_distill_stats'", ")", ":", "\n", "        ", "if", "not", "bn_used", "and", "args", ".", "bn_distill_stats", ":", "\n", "            ", "warn", "(", "'Option \"bn_distill_stats\" has no effect if batch '", "+", "\n", "'normalization not activated.'", ")", "\n", "\n", "", "", "if", "bn_used", "is", "not", "None", "and", "hasattr", "(", "args", ",", "'bn_no_stats_checkpointing'", ")", ":", "\n", "        ", "if", "not", "bn_used", "and", "args", ".", "bn_no_stats_checkpointing", ":", "\n", "            ", "warn", "(", "'Option \"bn_no_stats_checkpointing\" has no effect if batch '", "+", "\n", "'normalization not activated.'", ")", "\n", "\n", "", "", "if", "hasattr", "(", "args", ",", "'bn_no_stats_checkpointing'", ")", "and", "hasattr", "(", "args", ",", "'bn_no_running_stats'", ")", "and", "args", ".", "bn_no_stats_checkpointing", "and", "args", ".", "bn_no_running_stats", ":", "\n", "        ", "raise", "ValueError", "(", "'Options \"bn_no_stats_checkpointing\" and '", "+", "\n", "'\"bn_no_running_stats\" are not compatible'", ")", "\n", "", "if", "hasattr", "(", "args", ",", "'bn_no_stats_checkpointing'", ")", "and", "hasattr", "(", "args", ",", "'bn_no_running_stats'", ")", "and", "args", ".", "bn_no_stats_checkpointing", "and", "args", ".", "bn_distill_stats", ":", "\n", "        ", "raise", "ValueError", "(", "'Options \"bn_no_running_stats\" and '", "+", "\n", "'\"bn_distill_stats\" are not compatible'", ")", "\n", "", "if", "hasattr", "(", "args", ",", "'bn_no_running_stats'", ")", "and", "hasattr", "(", "args", ",", "'bn_distill_stats'", ")", "and", "args", ".", "bn_no_stats_checkpointing", "and", "args", ".", "bn_distill_stats", ":", "\n", "        ", "raise", "ValueError", "(", "'Options \"bn_no_running_stats\" and '", "+", "\n", "'\"bn_distill_stats\" are not compatible'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.context_mod_layer.ContextModLayer.__init__": [[129, 134], ["torch.Module.__init__", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ",", "num_features", ",", "no_weights", "=", "False", ",", "no_gains", "=", "False", ",", "\n", "no_shifts", "=", "False", ",", "apply_gain_offset", "=", "False", ")", ":", "\n", "        ", "super", "(", "ContextModLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "raise", "NotImplementedError", "(", "'Implementation not publicly available yet!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.calc_delta_theta": [[98, 147], ["isinstance", "isinstance", "isinstance", "optim_step.sgd_step", "optim_step.adam_step", "optim_step.rmsprop_step", "NotImplementedError", "ret.append", "ret.append", "p.grad.detach().clone", "p.grad.clone", "p.grad.detach"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.sgd_step", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.adam_step", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.rmsprop_step"], ["def", "calc_delta_theta", "(", "optimizer", ",", "use_sgd_change", ",", "lr", "=", "None", ",", "detach_dt", "=", "True", ")", ":", "\n", "    ", "r\"\"\"Calculate :math:`\\Delta\\theta`, i.e., the change in trainable parameters\n    (:math:`\\theta`) in order to minimize the task-specific loss.\n\n    **Note**, one has to call :func:`torch.autograd.backward` on a\n    desired loss before calling this function, otherwise there are no gradients\n    to compute the weight change that the optimizer would cause. Hence, this\n    method is called in between :func:`torch.autograd.backward` and\n    :meth:`torch.optim.Optimizer.step`.\n\n    Note, by default, gradients are detached from the computational graph.\n\n    Args:\n        optimizer: The optimizer that will be used to change :math:`\\theta`.\n        use_sgd_change: If :code:`True`, then we won't calculate the actual step\n            done by the current optimizer, but the one that would be done by a\n            simple SGD optimizer.\n        lr: Has to be specified if `use_sgd_change` is :code:`True`. The\n            learning rate if the optimizer.\n        detach_dt: Whether :math:`\\Delta\\theta` should be detached from the\n            computational graph. Note, in order to backprop through\n            :math:`\\Delta\\theta`, you have to call\n            :func:`torch.autograd.backward` with `create_graph` set to\n            :code:`True` before calling this method.\n\n    Returns:\n        :math:`\\Delta\\theta`\n    \"\"\"", "\n", "assert", "(", "not", "use_sgd_change", "or", "lr", "is", "not", "None", ")", "\n", "\n", "if", "use_sgd_change", ":", "\n", "        ", "ret", "=", "[", "]", "\n", "for", "g", "in", "optimizer", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "g", "[", "'params'", "]", ":", "\n", "                ", "if", "detach_dt", ":", "\n", "                    ", "ret", ".", "append", "(", "-", "lr", "*", "p", ".", "grad", ".", "detach", "(", ")", ".", "clone", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "ret", ".", "append", "(", "-", "lr", "*", "p", ".", "grad", ".", "clone", "(", ")", ")", "\n", "", "", "", "return", "ret", "\n", "", "else", ":", "\n", "        ", "if", "isinstance", "(", "optimizer", ",", "optim", ".", "SGD", ")", ":", "\n", "            ", "return", "sgd_step", "(", "optimizer", ",", "detach_dp", "=", "detach_dt", ")", "\n", "", "if", "isinstance", "(", "optimizer", ",", "optim", ".", "Adam", ")", ":", "\n", "            ", "return", "adam_step", "(", "optimizer", ",", "detach_dp", "=", "detach_dt", ")", "\n", "", "if", "isinstance", "(", "optimizer", ",", "optim", ".", "RMSprop", ")", ":", "\n", "            ", "return", "rmsprop_step", "(", "optimizer", ",", "detach_dp", "=", "detach_dt", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Not implemented for optimizer %s'", "%", "\n", "optimizer", ".", "type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.sgd_step": [[148, 213], ["isinstance", "d_ps.append", "p.grad.detach().clone", "p.grad.clone", "d_p.add.add_", "dict", "dict", "orig_state[].clone", "torch.clone().detach", "torch.clone().detach.mul_().add_", "d_p.add.add", "p.grad.detach", "torch.clone", "torch.clone().detach.mul_"], "function", ["None"], ["", "", "", "def", "sgd_step", "(", "optimizer", ",", "detach_dp", "=", "True", ")", ":", "\n", "    ", "\"\"\"Performs a single optimization step using the SGD optimizer. The code\n    has been copied from:\n\n        https://git.io/fjYit\n\n    Note, this function does not change the inner state of the given\n    optimizer object.\n\n    Note, gradients are cloned and detached by default.\n\n    Args:\n        optimizer: An instance of class :class:`torch.optim.SGD`.\n        detach_dp: Whether gradients are detached from the computational\n            graph. Note, :code:`False` only makes sense if\n            func:`torch.autograd.backward` was called with the argument\n            `create_graph` set to :code:`True`.\n\n    Returns:\n        A list of gradient changes `d_p` that would be applied by this\n        optimizer to all parameters when calling :meth:`torch.optim.SGD.step`.\n    \"\"\"", "\n", "assert", "(", "isinstance", "(", "optimizer", ",", "optim", ".", "SGD", ")", ")", "\n", "\n", "d_ps", "=", "[", "]", "\n", "\n", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "weight_decay", "=", "group", "[", "'weight_decay'", "]", "\n", "momentum", "=", "group", "[", "'momentum'", "]", "\n", "dampening", "=", "group", "[", "'dampening'", "]", "\n", "nesterov", "=", "group", "[", "'nesterov'", "]", "\n", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "            ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "detach_dp", ":", "\n", "                ", "d_p", "=", "p", ".", "grad", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "                ", "d_p", "=", "p", ".", "grad", ".", "clone", "(", ")", "\n", "\n", "", "if", "weight_decay", "!=", "0", ":", "\n", "                ", "d_p", ".", "add_", "(", "weight_decay", ",", "p", ".", "data", ")", "\n", "", "if", "momentum", "!=", "0", ":", "\n", "                ", "orig_state", "=", "dict", "(", "optimizer", ".", "state", "[", "p", "]", ")", "\n", "param_state", "=", "dict", "(", ")", "\n", "\n", "if", "'momentum_buffer'", "in", "orig_state", ":", "\n", "                    ", "param_state", "[", "'momentum_buffer'", "]", "=", "orig_state", "[", "'momentum_buffer'", "]", ".", "clone", "(", ")", "\n", "\n", "", "if", "'momentum_buffer'", "not", "in", "param_state", ":", "\n", "                    ", "buf", "=", "torch", ".", "clone", "(", "d_p", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "                    ", "buf", "=", "param_state", "[", "'momentum_buffer'", "]", "\n", "buf", ".", "mul_", "(", "momentum", ")", ".", "add_", "(", "1", "-", "dampening", ",", "d_p", ")", "\n", "#buf = buf.mul(momentum).add(1 - dampening, d_p)", "\n", "", "if", "nesterov", ":", "\n", "                    ", "d_p", "=", "d_p", ".", "add", "(", "momentum", ",", "buf", ")", "\n", "", "else", ":", "\n", "                    ", "d_p", "=", "buf", "\n", "\n", "", "", "d_ps", ".", "append", "(", "-", "group", "[", "'lr'", "]", "*", "d_p", ")", "\n", "\n", "", "", "return", "d_ps", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.adam_step": [[214, 315], ["isinstance", "dict", "dict", "int", "orig_state[].clone", "orig_state[].clone", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "d_ps.append", "p.grad.detach().clone", "p.grad.clone", "RuntimeError", "ValueError", "len", "torch.zeros_like", "torch.zeros_like", "orig_state[].clone", "p.grad.clone.add", "torch.max", "max_exp_avg_sq.sqrt().add_", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "exp_avg_sq.sqrt", "math.sqrt", "p.grad.detach", "max_exp_avg_sq.sqrt"], "function", ["None"], ["", "def", "adam_step", "(", "optimizer", ",", "detach_dp", "=", "True", ")", ":", "\n", "    ", "\"\"\"Performs a single optimization step using the Adam optimizer. The code\n    has been copied from:\n\n        https://git.io/fjYP3\n\n    Note, this function does not change the inner state of the given\n    optimizer object.\n\n    Note, gradients are cloned and detached by default.\n\n    Args:\n        optimizer: An instance of class :class:`torch.optim.Adam`.\n        detach_dp: Whether gradients are detached from the computational\n            graph. Note, :code:`False` only makes sense if\n            func:`torch.autograd.backward` was called with the argument\n            `create_graph` set to :code:`True`.\n\n    Returns:\n        A list of gradient changes `d_p` that would be applied by this\n        optimizer to all parameters when calling :meth:`torch.optim.Adam.step`.\n    \"\"\"", "\n", "assert", "(", "isinstance", "(", "optimizer", ",", "optim", ".", "Adam", ")", ")", "\n", "\n", "d_ps", "=", "[", "]", "\n", "\n", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "            ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "detach_dp", ":", "\n", "                ", "grad", "=", "p", ".", "grad", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "                ", "grad", "=", "p", ".", "grad", ".", "clone", "(", ")", "\n", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "if", "amsgrad", "and", "not", "detach_dp", ":", "\n", "                ", "raise", "ValueError", "(", "'Cannot backprop through optimizer step if '", "+", "\n", "'\"amsgrad\" is enabled.'", ")", "\n", "\n", "", "orig_state", "=", "dict", "(", "optimizer", ".", "state", "[", "p", "]", ")", "\n", "state", "=", "dict", "(", ")", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "orig_state", ")", "==", "0", ":", "\n", "                ", "orig_state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "orig_state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "orig_state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                    ", "orig_state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "# Copy original state.", "\n", "", "", "state", "[", "'step'", "]", "=", "int", "(", "orig_state", "[", "'step'", "]", ")", "\n", "state", "[", "'exp_avg'", "]", "=", "orig_state", "[", "'exp_avg'", "]", ".", "clone", "(", ")", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "orig_state", "[", "'exp_avg_sq'", "]", ".", "clone", "(", ")", "\n", "if", "amsgrad", ":", "\n", "                ", "state", "[", "'max_exp_avg_sq'", "]", "=", "orig_state", "[", "'max_exp_avg_sq'", "]", ".", "clone", "(", ")", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "#grad.add_(group['weight_decay'], p.data)", "\n", "                ", "grad", ".", "add", "(", "group", "[", "'weight_decay'", "]", ",", "p", ".", "data", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "#exp_avg.mul_(beta1)", "\n", "#exp_avg += (1 - beta1) * grad", "\n", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "#exp_avg_sq.mul_(beta2)", "\n", "#exp_avg_sq = torch.addcmul(exp_avg_sq, 1 - beta2, grad, grad)", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "#denom = exp_avg_sq.sqrt().add_(group['eps'])", "\n", "                ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", "+", "group", "[", "'eps'", "]", "\n", "\n", "", "bias_correction1", "=", "1", "-", "beta1", "**", "state", "[", "'step'", "]", "\n", "bias_correction2", "=", "1", "-", "beta2", "**", "state", "[", "'step'", "]", "\n", "step_size", "=", "group", "[", "'lr'", "]", "*", "math", ".", "sqrt", "(", "\n", "bias_correction2", ")", "/", "bias_correction1", "\n", "\n", "d_ps", ".", "append", "(", "-", "step_size", "*", "(", "exp_avg", "/", "denom", ")", ")", "\n", "\n", "", "", "return", "d_ps", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.rmsprop_step": [[316, 407], ["isinstance", "dict", "dict", "int", "orig_state[].clone", "square_avg.mul().addcmul.mul().addcmul", "p.grad.detach().clone", "p.grad.clone", "RuntimeError", "len", "torch.zeros_like", "orig_state[].clone", "orig_state[].clone", "grad.add.add", "grad_avg.mul_().add_", "square_avg.mul().addcmul.addcmul().sqrt().add", "square_avg.mul().addcmul.sqrt().add", "d_ps.append", "d_ps.append", "torch.zeros_like", "torch.zeros_like", "square_avg.mul().addcmul.mul", "buf.mul", "p.grad.detach", "grad_avg.mul_", "square_avg.mul().addcmul.addcmul().sqrt", "square_avg.mul().addcmul.sqrt", "square_avg.mul().addcmul.addcmul"], "function", ["None"], ["", "def", "rmsprop_step", "(", "optimizer", ",", "detach_dp", "=", "True", ")", ":", "\n", "    ", "\"\"\"Performs a single optimization step using the RMSprop optimizer. The code\n    has been copied from:\n\n        https://git.io/fjurp\n\n    Note, this function does not change the inner state of the given\n    optimizer object.\n\n    Note, gradients are cloned and detached by default.\n\n    Args:\n        optimizer: An instance of class :class:`torch.optim.Adam`.\n        detach_dp: Whether gradients are detached from the computational\n            graph. Note, :code:`False` only makes sense if\n            func:`torch.autograd.backward` was called with the argument\n            `create_graph` set to :code:`True`.\n\n    Returns:\n        A list of gradient changes `d_p` that would be applied by this\n        optimizer to all parameters when calling\n        :meth:`torch.optim.RMSprop.step`.\n    \"\"\"", "\n", "assert", "(", "isinstance", "(", "optimizer", ",", "optim", ".", "RMSprop", ")", ")", "\n", "\n", "d_ps", "=", "[", "]", "\n", "\n", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "            ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                ", "continue", "\n", "\n", "", "if", "detach_dp", ":", "\n", "                ", "grad", "=", "p", ".", "grad", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "                ", "grad", "=", "p", ".", "grad", ".", "clone", "(", ")", "\n", "\n", "", "if", "grad", ".", "is_sparse", ":", "\n", "                ", "raise", "RuntimeError", "(", "'RMSprop does not support sparse gradients'", ")", "\n", "\n", "", "orig_state", "=", "dict", "(", "optimizer", ".", "state", "[", "p", "]", ")", "\n", "state", "=", "dict", "(", ")", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "orig_state", ")", "==", "0", ":", "\n", "                ", "orig_state", "[", "'step'", "]", "=", "0", "\n", "orig_state", "[", "'square_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "group", "[", "'momentum'", "]", ">", "0", ":", "\n", "                    ", "orig_state", "[", "'momentum_buffer'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "if", "group", "[", "'centered'", "]", ":", "\n", "                    ", "orig_state", "[", "'grad_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "# Copy original state.", "\n", "", "", "state", "[", "'step'", "]", "=", "int", "(", "orig_state", "[", "'step'", "]", ")", "\n", "state", "[", "'square_avg'", "]", "=", "orig_state", "[", "'square_avg'", "]", ".", "clone", "(", ")", "\n", "if", "group", "[", "'momentum'", "]", ">", "0", ":", "\n", "                ", "state", "[", "'momentum_buffer'", "]", "=", "orig_state", "[", "'momentum_buffer'", "]", ".", "clone", "(", ")", "\n", "", "if", "group", "[", "'centered'", "]", ":", "\n", "                ", "state", "[", "'grad_avg'", "]", "=", "orig_state", "[", "'grad_avg'", "]", ".", "clone", "(", ")", "\n", "\n", "", "square_avg", "=", "state", "[", "'square_avg'", "]", "\n", "alpha", "=", "group", "[", "'alpha'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                ", "grad", "=", "grad", ".", "add", "(", "group", "[", "'weight_decay'", "]", ",", "p", ".", "data", ")", "\n", "\n", "#square_avg.mul_(alpha).addcmul_(1 - alpha, grad, grad)", "\n", "", "square_avg", "=", "square_avg", ".", "mul", "(", "alpha", ")", ".", "addcmul", "(", "1", "-", "alpha", ",", "grad", ",", "grad", ")", "\n", "\n", "if", "group", "[", "'centered'", "]", ":", "\n", "                ", "grad_avg", "=", "state", "[", "'grad_avg'", "]", "\n", "grad_avg", ".", "mul_", "(", "alpha", ")", ".", "add_", "(", "1", "-", "alpha", ",", "grad", ")", "\n", "#avg = square_avg.addcmul(-1, grad_avg, grad_avg).sqrt().add_(group['eps'])", "\n", "avg", "=", "square_avg", ".", "addcmul", "(", "-", "1", ",", "grad_avg", ",", "grad_avg", ")", ".", "sqrt", "(", ")", ".", "add", "(", "group", "[", "'eps'", "]", ")", "\n", "", "else", ":", "\n", "#avg = square_avg.sqrt().add_(group['eps'])", "\n", "                ", "avg", "=", "square_avg", ".", "sqrt", "(", ")", ".", "add", "(", "group", "[", "'eps'", "]", ")", "\n", "\n", "", "if", "group", "[", "'momentum'", "]", ">", "0", ":", "\n", "                ", "buf", "=", "state", "[", "'momentum_buffer'", "]", "\n", "#buf.mul_(group['momentum']).addcdiv_(grad, avg)", "\n", "buf", "=", "buf", ".", "mul", "(", "group", "[", "'momentum'", "]", ")", "+", "grad", "/", "avg", "\n", "\n", "d_ps", ".", "append", "(", "-", "group", "[", "'lr'", "]", "*", "buf", ")", "\n", "", "else", ":", "\n", "                ", "d_ps", ".", "append", "(", "-", "group", "[", "'lr'", "]", "*", "(", "grad", "/", "avg", ")", ")", "\n", "\n", "", "", "", "return", "d_ps", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.self_attention_layer.SelfAttnLayer.__init__": [[40, 76], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.utils.spectral_norm", "torch.utils.spectral_norm", "torch.utils.spectral_norm", "torch.utils.spectral_norm", "torch.utils.spectral_norm", "torch.utils.spectral_norm", "torch.utils.spectral_norm", "torch.utils.spectral_norm", "torch.utils.spectral_norm"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ",", "in_dim", ",", "use_spectral_norm", ")", ":", "\n", "        ", "\"\"\"Initialize self-attention layer.\n\n        Args:\n            in_dim: Number of input channels (C).\n            use_spectral_norm: Enable spectral normalization for all 1x1 conv.\n                layers.\n        \"\"\"", "\n", "super", "(", "SelfAttnLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "channel_in", "=", "in_dim", "\n", "\n", "# 1x1 convolution to generate f(x).", "\n", "self", ".", "query_conv", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "in_dim", ",", "\n", "out_channels", "=", "in_dim", "//", "8", ",", "kernel_size", "=", "1", ")", "\n", "# 1x1 convolution to generate g(x).", "\n", "self", ".", "key_conv", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "in_dim", ",", "out_channels", "=", "in_dim", "//", "8", ",", "\n", "kernel_size", "=", "1", ")", "\n", "# 1x1 convolution to generate h(x).", "\n", "self", ".", "value_conv", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "in_dim", ",", "out_channels", "=", "in_dim", ",", "\n", "kernel_size", "=", "1", ")", "\n", "# This parameter is on purpose initialized to be zero as described in", "\n", "# the paper.", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ")", ")", "\n", "\n", "# Spectral normalization is used in the original implementation:", "\n", "#   https://github.com/brain-research/self-attention-gan", "\n", "# Note, the original implementation also appears to use an additional", "\n", "# (fourth) 1x1 convolution to postprocess h(x) * beta before adding it", "\n", "# to the input tensor. Though, the reason is not fully obvious to me and", "\n", "# seems to be not mentioned in the paper.", "\n", "if", "use_spectral_norm", ":", "\n", "            ", "self", ".", "query_conv", "=", "nn", ".", "utils", ".", "spectral_norm", "(", "self", ".", "query_conv", ")", "\n", "self", ".", "key_conv", "=", "nn", ".", "utils", ".", "spectral_norm", "(", "self", ".", "key_conv", ")", "\n", "self", ".", "value_conv", "=", "nn", ".", "utils", ".", "spectral_norm", "(", "self", ".", "value_conv", ")", "\n", "\n", "", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.self_attention_layer.SelfAttnLayer.forward": [[77, 117], ["x.size", "self_attention_layer.SelfAttnLayer.SelfAttnLayer.query_conv().view().permute", "self_attention_layer.SelfAttnLayer.SelfAttnLayer.key_conv().view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "self_attention_layer.SelfAttnLayer.SelfAttnLayer.softmax", "self_attention_layer.SelfAttnLayer.SelfAttnLayer.value_conv().view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "out.view.view.view", "self_attention_layer.SelfAttnLayer.SelfAttnLayer.permute", "self_attention_layer.SelfAttnLayer.SelfAttnLayer.query_conv().view", "self_attention_layer.SelfAttnLayer.SelfAttnLayer.key_conv", "self_attention_layer.SelfAttnLayer.SelfAttnLayer.value_conv", "self_attention_layer.SelfAttnLayer.SelfAttnLayer.query_conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "ret_attention", "=", "False", ")", ":", "\n", "        ", "\"\"\"Compute and apply attention map to mix global information into local\n        features.\n\n        Args:\n            x: Input feature maps (shape: B x C x W x H).\n            ret_attention (optional): If the attention map should be returned\n                as an additional return value.\n\n        Returns:\n            (tuple): Tuple (if ``ret_attention`` is ``True``) containing:\n\n            - **out**: gamma * (self-)attention features + input features.\n            - **attention**: Attention map, shape: B X N X N (N = W * H).\n        \"\"\"", "\n", "m_batchsize", ",", "C", ",", "width", ",", "height", "=", "x", ".", "size", "(", ")", "\n", "\n", "# Compute f(x)^T, shape: B x N x C//8.", "\n", "proj_query", "=", "self", ".", "query_conv", "(", "x", ")", ".", "view", "(", "m_batchsize", ",", "-", "1", ",", "width", "*", "height", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "# Compute g(x), shape: B x C//8 x N.", "\n", "proj_key", "=", "self", ".", "key_conv", "(", "x", ")", ".", "view", "(", "m_batchsize", ",", "-", "1", ",", "width", "*", "height", ")", "\n", "energy", "=", "torch", ".", "bmm", "(", "proj_query", ",", "proj_key", ")", "# f(x)^T g(x)", "\n", "# We compute the softmax per column of \"energy\" -> columns should sum", "\n", "# up to 1.", "\n", "attention", "=", "self", ".", "softmax", "(", "energy", ")", "# shape: B x N x N", "\n", "# Compute h(x), shape: B x C x N.", "\n", "proj_value", "=", "self", ".", "value_conv", "(", "x", ")", ".", "view", "(", "m_batchsize", ",", "-", "1", ",", "width", "*", "height", ")", "\n", "\n", "# Compute h(x) * beta (equation 2 in the paper).", "\n", "# FIXME I am sure that taking the tranpose of \"attention\" is wrong, as", "\n", "# the columns (not rows) of \"attention\" sum to 1.", "\n", "out", "=", "torch", ".", "bmm", "(", "proj_value", ",", "attention", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "\n", "out", "=", "out", ".", "view", "(", "m_batchsize", ",", "C", ",", "width", ",", "height", ")", "\n", "\n", "out", "=", "self", ".", "gamma", "*", "out", "+", "x", "\n", "\n", "if", "ret_attention", ":", "\n", "            ", "return", "out", ",", "attention", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.self_attention_layer.SelfAttnLayerV2.__init__": [[139, 205], ["torch.Module.__init__", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "enumerate", "NotImplementedError", "self_attention_layer.SelfAttnLayerV2.SelfAttnLayerV2._weights.append", "range", "range", "torch.init.constant_", "torch.init.constant_", "torch.init.constant_", "torch.Parameter", "torch.Parameter", "torch.Parameter", "len", "len", "len", "numpy.all", "utils.misc.init_params", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "numpy.equal", "len", "list", "list"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params"], ["def", "__init__", "(", "self", ",", "in_dim", ",", "use_spectral_norm", ",", "no_weights", "=", "False", ",", "\n", "init_weights", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initialize self-attention layer.\n\n        Args:\n            in_dim: Number of input channels (C).\n            use_spectral_norm: Enable spectral normalization for all 1x1 conv.\n                layers.\n            no_weights: If set to True, no trainable parameters will be\n                constructed, i.e., weights are assumed to be produced ad-hoc\n                by a hypernetwork and passed to the forward function.\n            init_weights (optional): This option is for convinience reasons.\n                The option expects a list of parameter values that are used to\n                initialize the network weights. As such, it provides a\n                convinient way of initializing a network with a weight draw\n                produced by the hypernetwork.\n                See attribute \"weight_shapes\" for the format in which parameters\n                should be passed.\n        \"\"\"", "\n", "super", "(", "SelfAttnLayerV2", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "not", "no_weights", "or", "init_weights", "is", "None", ")", "\n", "if", "use_spectral_norm", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Spectral norm not yet implemented '", "+", "\n", "'for this layer type.'", ")", "\n", "\n", "", "self", ".", "channel_in", "=", "in_dim", "\n", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "\n", "# 1x1 convolution to generate f(x).", "\n", "query_dim", "=", "[", "in_dim", "//", "8", ",", "in_dim", ",", "1", ",", "1", "]", "\n", "# 1x1 convolution to generate g(x).", "\n", "key_dim", "=", "[", "in_dim", "//", "8", ",", "in_dim", ",", "1", ",", "1", "]", "\n", "# 1x1 convolution to generate h(x).", "\n", "value_dim", "=", "[", "in_dim", ",", "in_dim", ",", "1", ",", "1", "]", "\n", "gamma_dim", "=", "[", "1", "]", "\n", "self", ".", "_weight_shapes", "=", "[", "query_dim", ",", "[", "query_dim", "[", "0", "]", "]", ",", "\n", "key_dim", ",", "[", "key_dim", "[", "0", "]", "]", ",", "\n", "value_dim", ",", "[", "value_dim", "[", "0", "]", "]", ",", "\n", "gamma_dim", "\n", "]", "\n", "\n", "if", "no_weights", ":", "\n", "            ", "self", ".", "_weights", "=", "None", "\n", "return", "\n", "\n", "### Define and initialize network weights.", "\n", "", "self", ".", "_weights", "=", "nn", ".", "ParameterList", "(", ")", "\n", "\n", "for", "i", ",", "dims", "in", "enumerate", "(", "self", ".", "_weight_shapes", ")", ":", "\n", "            ", "self", ".", "_weights", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "*", "dims", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "\n", "", "if", "init_weights", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "init_weights", ")", "==", "len", "(", "self", ".", "_weight_shapes", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "init_weights", ")", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "i", "]", ".", "shape", ")", ",", "\n", "list", "(", "self", ".", "_weights", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "self", ".", "_weights", "[", "i", "]", ".", "data", "=", "init_weights", "[", "i", "]", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "self", ".", "_weights", ")", "-", "1", ",", "2", ")", ":", "\n", "                ", "init_params", "(", "self", ".", "_weights", "[", "i", "]", ",", "self", ".", "_weights", "[", "i", "+", "1", "]", ")", "\n", "# This gamma parameter is on purpose initialized to be zero as", "\n", "# described in the paper.", "\n", "", "nn", ".", "init", ".", "constant_", "(", "self", ".", "_weights", "[", "-", "1", "]", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.self_attention_layer.SelfAttnLayerV2.weight_shapes": [[206, 210], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "weight_shapes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute weight_shapes.\"\"\"", "\n", "return", "self", ".", "_weight_shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.self_attention_layer.SelfAttnLayerV2.weights": [[211, 219], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute weights.\n\n        Returns:\n            A torch.nn.ParameterList or None, if this network has no weights.\n        \"\"\"", "\n", "return", "self", ".", "_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.self_attention_layer.SelfAttnLayerV2.forward": [[220, 286], ["x.size", "torch.conv2d().view().permute", "torch.conv2d().view().permute", "torch.conv2d().view().permute", "torch.conv2d().view", "torch.conv2d().view", "torch.conv2d().view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "self_attention_layer.SelfAttnLayerV2.SelfAttnLayerV2.softmax", "torch.conv2d().view", "torch.conv2d().view", "torch.conv2d().view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "out.view.view.view", "Exception", "enumerate", "self_attention_layer.SelfAttnLayerV2.SelfAttnLayerV2.permute", "len", "len", "len", "len", "new_weights.append", "torch.conv2d().view", "torch.conv2d().view", "torch.conv2d().view", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.conv2d"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "ret_attention", "=", "False", ",", "weights", "=", "None", ",", "dWeights", "=", "None", ")", ":", "\n", "        ", "\"\"\"Compute and apply attention map to mix global information into local\n        features.\n\n        Args:\n            x: Input feature maps (shape: B x C x W x H).\n            ret_attention (optional): If the attention map should be returned\n                as an additional return value.\n            weights: List of weight tensors, that are used as layer parameters.\n                If \"no_weights\" was set in the constructor, then this parameter\n                is mandatory.\n                Note, when provided, internal parameters are not used.\n            dWeights: List of weight tensors, that are added to \"weights\" (the\n                internal list of parameters or the one given via the option\n                \"weights\"), when computing the output of this network.\n\n        Returns:\n            (tuple): Tuple (if ``ret_attention`` is ``True``) containing:\n\n            - **out**: gamma * (self-)attention features + input features.\n            - **attention**: Attention map, shape: B X N X N (N = W * H).\n        \"\"\"", "\n", "if", "self", ".", "_weights", "is", "None", "and", "weights", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Layer was generated without internal weights. '", "+", "\n", "'Hence, \"weights\" option may not be None.'", ")", "\n", "\n", "", "if", "weights", "is", "None", ":", "\n", "            ", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "            ", "assert", "(", "len", "(", "weights", ")", "==", "len", "(", "self", ".", "weight_shapes", ")", ")", "\n", "\n", "", "if", "dWeights", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "dWeights", ")", "==", "len", "(", "self", ".", "weight_shapes", ")", ")", "\n", "\n", "new_weights", "=", "[", "]", "\n", "for", "i", ",", "w", "in", "enumerate", "(", "weights", ")", ":", "\n", "                ", "new_weights", ".", "append", "(", "w", "+", "dWeights", "[", "i", "]", ")", "\n", "", "weights", "=", "new_weights", "\n", "\n", "", "m_batchsize", ",", "C", ",", "width", ",", "height", "=", "x", ".", "size", "(", ")", "\n", "\n", "# Compute f(x)^T, shape: B x N x C//8.", "\n", "proj_query", "=", "F", ".", "conv2d", "(", "x", ",", "weights", "[", "0", "]", ",", "bias", "=", "weights", "[", "1", "]", ")", ".", "view", "(", "m_batchsize", ",", "-", "1", ",", "width", "*", "height", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "# Compute g(x), shape: B x C//8 x N.", "\n", "proj_key", "=", "F", ".", "conv2d", "(", "x", ",", "weights", "[", "2", "]", ",", "bias", "=", "weights", "[", "3", "]", ")", ".", "view", "(", "m_batchsize", ",", "-", "1", ",", "width", "*", "height", ")", "\n", "energy", "=", "torch", ".", "bmm", "(", "proj_query", ",", "proj_key", ")", "# f(x)^T g(x)", "\n", "# We compute the softmax per column of \"energy\" -> columns should sum", "\n", "# up to 1.", "\n", "attention", "=", "self", ".", "softmax", "(", "energy", ")", "# shape: B x N x N", "\n", "# Compute h(x), shape: B x C x N.", "\n", "proj_value", "=", "F", ".", "conv2d", "(", "x", ",", "weights", "[", "4", "]", ",", "bias", "=", "weights", "[", "5", "]", ")", ".", "view", "(", "m_batchsize", ",", "-", "1", ",", "width", "*", "height", ")", "\n", "\n", "# Compute h(x) * beta (equation 2 in the paper).", "\n", "# FIXME I am sure that taking the tranpose of \"attention\" is wrong, as", "\n", "# the columns (not rows) of \"attention\" sum to 1.", "\n", "out", "=", "torch", ".", "bmm", "(", "proj_value", ",", "attention", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "\n", "out", "=", "out", ".", "view", "(", "m_batchsize", ",", "C", ",", "width", ",", "height", ")", "\n", "\n", "out", "=", "weights", "[", "6", "]", "*", "out", "+", "x", "\n", "\n", "if", "ret_attention", ":", "\n", "            ", "return", "out", ",", "attention", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.init_utils.xavier_fan_in_": [[34, 51], ["torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init._no_grad_uniform_", "math.sqrt", "math.sqrt"], "function", ["None"], ["def", "xavier_fan_in_", "(", "tensor", ")", ":", "\n", "    ", "\"\"\"Initialize the given weight tensor with Xavier fan-in init.\n\n    Unfortunately, :func:`torch.nn.init.xavier_uniform_` doesn't give\n    us the choice to use fan-in init (always uses the harmonic mean).\n    Therefore, we provide our own implementation.\n\n    Args:\n        tensor (torch.Tensor): Weight tensor that will be modified\n            (initialized) in-place.\n    \"\"\"", "\n", "fan_in", ",", "_", "=", "torch", ".", "nn", ".", "init", ".", "_calculate_fan_in_and_fan_out", "(", "tensor", ")", "\n", "std", "=", "1.", "/", "math", ".", "sqrt", "(", "fan_in", ")", "\n", "# Note, std(Unif(-a, a)) = a / sqrt(3)", "\n", "a", "=", "math", ".", "sqrt", "(", "3.0", ")", "*", "std", "\n", "\n", "torch", ".", "nn", ".", "init", ".", "_no_grad_uniform_", "(", "tensor", ",", "-", "a", ",", "a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.init_utils.calc_fan_in_and_out": [[52, 81], ["len", "len", "int", "numpy.prod"], "function", ["None"], ["", "def", "calc_fan_in_and_out", "(", "shapes", ")", ":", "\n", "    ", "\"\"\"Calculate fan-in and fan-out.\n\n    Note:\n        This function expects the shapes of an at least 2D tensor.\n\n    Args:\n        shapes (list): List of integers.\n\n    Returns:\n        (tuple) Tuple containing:\n\n        - **fan_in**\n        - **fan_out**\n    \"\"\"", "\n", "assert", "len", "(", "shapes", ")", ">", "1", "\n", "\n", "fan_in", "=", "shapes", "[", "1", "]", "\n", "fan_out", "=", "shapes", "[", "0", "]", "\n", "\n", "if", "len", "(", "shapes", ")", ">", "2", ":", "\n", "        ", "receptive_field_size", "=", "int", "(", "np", ".", "prod", "(", "shapes", "[", "2", ":", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "receptive_field_size", "=", "1", "\n", "\n", "", "fan_in", "*=", "receptive_field_size", "\n", "fan_out", "*=", "receptive_field_size", "\n", "\n", "return", "fan_in", ",", "fan_out", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.init_params": [[33, 59], ["warnings.warn", "torch.nn.init.kaiming_uniform_", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init.uniform_", "math.sqrt", "math.sqrt"], "function", ["None"], ["def", "init_params", "(", "weights", ",", "bias", "=", "None", ")", ":", "\n", "    ", "\"\"\"Initialize the weights and biases of a linear or (transpose) conv layer.\n\n    Note, the implementation is based on the method \"reset_parameters()\",\n    that defines the original PyTorch initialization for a linear or\n    convolutional layer, resp. The implementations can be found here:\n\n        https://git.io/fhnxV\n\n        https://git.io/fhnx2\n\n    .. deprecated:: 1.0\n        Please use function :func:`utils.torch_utils.init_params` instead.\n\n    Args:\n        weights: The weight tensor to be initialized.\n        bias (optional): The bias tensor to be initialized.\n    \"\"\"", "\n", "warn", "(", "'Function is deprecated. Use \"utils.torch_utils.init_params\" instead.'", ",", "\n", "DeprecationWarning", ")", "\n", "\n", "nn", ".", "init", ".", "kaiming_uniform_", "(", "weights", ",", "a", "=", "math", ".", "sqrt", "(", "5", ")", ")", "\n", "if", "bias", "is", "not", "None", ":", "\n", "        ", "fan_in", ",", "_", "=", "nn", ".", "init", ".", "_calculate_fan_in_and_fan_out", "(", "weights", ")", "\n", "bound", "=", "1", "/", "math", ".", "sqrt", "(", "fan_in", ")", "\n", "nn", ".", "init", ".", "uniform_", "(", "bias", ",", "-", "bound", ",", "bound", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints": [[60, 79], ["isinstance", "len", "int", "str_arg.split"], "function", ["None"], ["", "", "def", "str_to_ints", "(", "str_arg", ")", ":", "\n", "    ", "\"\"\"Helper function to convert a list of comma separated strings into\n    integers.\n\n    Args:\n        str_arg: String containing list of comma-separated ints. For convenience\n            reasons, we allow the user to also pass single integers that a put\n            into a list of length 1 by this function.\n\n    Returns:\n        List of integers.\n    \"\"\"", "\n", "if", "isinstance", "(", "str_arg", ",", "int", ")", ":", "\n", "        ", "return", "[", "str_arg", "]", "\n", "\n", "", "if", "len", "(", "str_arg", ")", ">", "0", ":", "\n", "        ", "return", "[", "int", "(", "s", ")", "for", "s", "in", "str_arg", ".", "split", "(", "','", ")", "]", "\n", "", "else", ":", "\n", "        ", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.list_to_str": [[80, 96], ["enumerate", "str"], "function", ["None"], ["", "", "def", "list_to_str", "(", "list_arg", ",", "delim", "=", "' '", ")", ":", "\n", "    ", "\"\"\"Convert a list of numbers into a string.\n\n    Args:\n        list_arg: List of numbers.\n        delim (optional): Delimiter between numbers.\n\n    Returns:\n        List converted to string.\n    \"\"\"", "\n", "ret", "=", "''", "\n", "for", "i", ",", "e", "in", "enumerate", "(", "list_arg", ")", ":", "\n", "        ", "if", "i", ">", "0", ":", "\n", "            ", "ret", "+=", "delim", "\n", "", "ret", "+=", "str", "(", "e", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_act": [[97, 119], ["torch.nn.Sigmoid", "torch.nn.ReLU", "torch.nn.ELU", "Exception"], "function", ["None"], ["", "def", "str_to_act", "(", "act_str", ")", ":", "\n", "    ", "\"\"\"Convert the name of an activation function into the actual PyTorch\n    activation function.\n\n    Args:\n        act_str: Name of activation function (as defined by command-line\n            arguments).\n\n    Returns:\n        Torch activation function instance or ``None``, if ``linear`` is given.\n    \"\"\"", "\n", "if", "act_str", "==", "'linear'", ":", "\n", "        ", "act", "=", "None", "\n", "", "elif", "act_str", "==", "'sigmoid'", ":", "\n", "        ", "act", "=", "torch", ".", "nn", ".", "Sigmoid", "(", ")", "\n", "", "elif", "act_str", "==", "'relu'", ":", "\n", "        ", "act", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", "\n", "", "elif", "act_str", "==", "'elu'", ":", "\n", "        ", "act", "=", "torch", ".", "nn", ".", "ELU", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Activation function %s unknown.'", "%", "act_str", ")", "\n", "", "return", "act", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.configure_matplotlib_params": [[120, 146], ["matplotlib.rcParams.update", "matplotlib.rcParams.update"], "function", ["None"], ["", "def", "configure_matplotlib_params", "(", "fig_size", "=", "[", "6.4", ",", "4.8", "]", ",", "two_axes", "=", "True", ",", "\n", "font_size", "=", "8", ")", ":", "\n", "    ", "\"\"\"Helper function to configure default matplotlib parameters.\n\n    Args:\n        fig_size: Figure size (width, height) in inches.\n    \"\"\"", "\n", "params", "=", "{", "\n", "'axes.labelsize'", ":", "font_size", ",", "\n", "'font.size'", ":", "font_size", ",", "\n", "'font.sans-serif'", ":", "[", "'Arial'", "]", ",", "\n", "'text.usetex'", ":", "True", ",", "\n", "'text.latex.preamble'", ":", "[", "r'\\usepackage[scaled]{helvet}'", ",", "\n", "r'\\usepackage{sfmath}'", "]", ",", "\n", "'font.family'", ":", "'sans-serif'", ",", "\n", "'legend.fontsize'", ":", "font_size", ",", "\n", "'xtick.labelsize'", ":", "font_size", ",", "\n", "'ytick.labelsize'", ":", "font_size", ",", "\n", "'axes.titlesize'", ":", "font_size", ",", "\n", "'axes.spines.right'", ":", "not", "two_axes", ",", "\n", "'axes.spines.top'", ":", "not", "two_axes", ",", "\n", "'figure.figsize'", ":", "fig_size", ",", "\n", "'legend.handlelength'", ":", "0.5", "\n", "}", "\n", "\n", "matplotlib", ".", "rcParams", ".", "update", "(", "params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.get_colorbrewer2_colors": [[147, 194], ["None"], "function", ["None"], ["", "def", "get_colorbrewer2_colors", "(", "family", "=", "'Set2'", ")", ":", "\n", "    ", "\"\"\"Helper function that returns a list of color combinations\n    extracted from colorbrewer2.org.\n\n    Args:\n        type: the color family from colorbrewer2.org to use.\n    \"\"\"", "\n", "if", "family", "==", "'Set2'", ":", "\n", "        ", "return", "[", "\n", "'#e41a1c'", ",", "\n", "'#377eb8'", ",", "\n", "'#4daf4a'", ",", "\n", "'#984ea3'", ",", "\n", "'#ff7f00'", ",", "\n", "'#ffff33'", ",", "\n", "'#a65628'", ",", "\n", "'#b3de69'", "\n", "]", "\n", "", "if", "family", "==", "'Set3'", ":", "\n", "        ", "return", "[", "\n", "'#8dd3c7'", ",", "\n", "'#ffffb3'", ",", "\n", "'#bebada'", ",", "\n", "'#fb8072'", ",", "\n", "'#80b1d3'", ",", "\n", "'#fdb462'", ",", "\n", "''", "\n", "]", "\n", "", "elif", "family", "==", "'Dark2'", ":", "\n", "        ", "return", "[", "\n", "'#1b9e77'", ",", "\n", "'#d95f02'", ",", "\n", "'#7570b3'", ",", "\n", "'#e7298a'", ",", "\n", "'#66a61e'", ",", "\n", "'#e6ab02'", ",", "\n", "'#a6761d'", "\n", "]", "\n", "", "elif", "family", "==", "'Pastel'", ":", "\n", "        ", "return", "[", "\n", "'#fbb4ae'", ",", "\n", "'#b3cde3'", ",", "\n", "'#ccebc5'", ",", "\n", "'#decbe4'", ",", "\n", "'#fed9a6'", ",", "\n", "'#ffffcc'", ",", "\n", "'#e5d8bd'", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.repair_canvas_and_show_fig": [[196, 215], ["matplotlib.figure", "fig.set_canvas", "matplotlib.close", "matplotlib.figure", "matplotlib.show", "matplotlib.close"], "function", ["None"], ["", "", "def", "repair_canvas_and_show_fig", "(", "fig", ",", "close", "=", "True", ")", ":", "\n", "    ", "\"\"\"If writing a figure to tensorboard via \"add_figure\" it might change the\n    canvas, such that our backend doesn't allow to show the figure anymore.\n    This method will generate a new canvas and replace the old one of the\n    given figure.\n\n    Args:\n        fig: The figure to be shown.\n        close: Whether the figure should be closed after it has been shown.\n    \"\"\"", "\n", "tmp_fig", "=", "plt", ".", "figure", "(", ")", "\n", "tmp_manager", "=", "tmp_fig", ".", "canvas", ".", "manager", "\n", "tmp_manager", ".", "canvas", ".", "figure", "=", "fig", "\n", "fig", ".", "set_canvas", "(", "tmp_manager", ".", "canvas", ")", "\n", "plt", ".", "close", "(", "tmp_fig", ".", "number", ")", "\n", "plt", ".", "figure", "(", "fig", ".", "number", ")", "\n", "plt", ".", "show", "(", ")", "\n", "if", "close", ":", "\n", "        ", "plt", ".", "close", "(", "fig", ".", "number", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.sim_utils.setup_environment": [[52, 152], ["os.path.exists", "utils.logger_config.config_logger", "torch.manual_seed", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed", "hasattr", "torch.device", "logger_config.config_logger.info", "input", "shutil.rmtree", "os.makedirs", "print", "os.makedirs", "print", "open", "pickle.dump", "open", "json.dump", "os.path.join", "hasattr", "hasattr", "hasattr", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "Exception", "os.path.join", "os.path.join", "vars", "int", "hasattr", "logger_config.config_logger.warning", "hasattr", "hasattr", "torch.cuda.is_available", "torch.cuda.is_available", "str", "os.path.join", "os.path.join", "time.time"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.logger_config.config_logger"], ["def", "setup_environment", "(", "config", ",", "logger_name", "=", "'hnet_sim_logger'", ")", ":", "\n", "    ", "\"\"\"Setup the general environment for training.\n\n    This function should be called at the beginning of a simulation script\n    (right after the command-line arguments have been parsed). The setup will\n    incorporate:\n\n        - creating the output folder\n        - initializing logger\n        - making computation deterministic (depending on config)\n        - selecting the torch device\n        - creating the Tensorboard writer\n\n    Args:\n        config (argparse.Namespace): Command-line arguments.\n\n            .. note::\n                The function expects command-line arguments available according\n                to the function :func:`utils.cli_args.miscellaneous_args`.\n        logger_name (str): Name of the logger to be created (time stamp will be\n            appended to this name).\n\n    Returns:\n        (tuple): Tuple containing:\n\n        - **device**: Torch device to be used.\n        - **writer**: Tensorboard writer. Note, you still have to close the\n          writer manually!\n        - **logger**: Console (and file) logger.\n    \"\"\"", "\n", "### Output folder.", "\n", "if", "os", ".", "path", ".", "exists", "(", "config", ".", "out_dir", ")", ":", "\n", "# TODO allow continuing from an old checkpoint.", "\n", "        ", "response", "=", "input", "(", "'The output folder %s already exists. '", "%", "(", "config", ".", "out_dir", ")", "+", "'Do you want us to delete it? [y/n]'", ")", "\n", "if", "response", "!=", "'y'", ":", "\n", "            ", "raise", "Exception", "(", "'Could not delete output folder!'", ")", "\n", "", "shutil", ".", "rmtree", "(", "config", ".", "out_dir", ")", "\n", "\n", "os", ".", "makedirs", "(", "config", ".", "out_dir", ")", "\n", "print", "(", "\"Created output folder %s.\"", "%", "(", "config", ".", "out_dir", ")", ")", "\n", "\n", "", "else", ":", "\n", "        ", "os", ".", "makedirs", "(", "config", ".", "out_dir", ")", "\n", "print", "(", "\"Created output folder %s.\"", "%", "(", "config", ".", "out_dir", ")", ")", "\n", "\n", "# Save user configs to ensure reproducibility of this experiment.", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "config", ".", "out_dir", ",", "'config.pickle'", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "config", ",", "f", ")", "\n", "# A JSON file is easier to read for a human.", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "config", ".", "out_dir", ",", "'config.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "vars", "(", "config", ")", ",", "f", ")", "\n", "\n", "### Initialize logger.", "\n", "", "logger_name", "=", "'%s_%d'", "%", "(", "logger_name", ",", "int", "(", "time", "(", ")", "*", "1000", ")", ")", "\n", "logger", "=", "logger_config", ".", "config_logger", "(", "logger_name", ",", "\n", "os", ".", "path", ".", "join", "(", "config", ".", "out_dir", ",", "'logfile.txt'", ")", ",", "\n", "logging", ".", "DEBUG", ",", "logging", ".", "INFO", "if", "config", ".", "loglevel_info", "else", "logging", ".", "DEBUG", ")", "\n", "# FIXME If we don't disable this, then the multiprocessing from the data", "\n", "# loader causes all messages to be logged twice. I could not find the cause", "\n", "# of this problem, but this simple switch fixes it.", "\n", "logger", ".", "propagate", "=", "False", "\n", "\n", "### Deterministic computation.", "\n", "torch", ".", "manual_seed", "(", "config", ".", "random_seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "config", ".", "random_seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "config", ".", "random_seed", ")", "\n", "random", ".", "seed", "(", "config", ".", "random_seed", ")", "\n", "\n", "# Ensure that runs are reproducible. Note, this slows down training!", "\n", "# https://pytorch.org/docs/stable/notes/randomness.html", "\n", "if", "config", ".", "deterministic_run", ":", "\n", "        ", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "\n", "if", "hasattr", "(", "config", ",", "'num_workers'", ")", "and", "config", ".", "num_workers", ">", "1", ":", "\n", "            ", "logger", ".", "warning", "(", "'Deterministic run desired but not possible with '", "+", "\n", "'more than 1 worker (see \"num_workers\").'", ")", "\n", "\n", "### Select torch device.", "\n", "", "", "assert", "(", "hasattr", "(", "config", ",", "'no_cuda'", ")", "or", "hasattr", "(", "config", ",", "'use_cuda'", ")", ")", "\n", "assert", "(", "not", "hasattr", "(", "config", ",", "'no_cuda'", ")", "or", "not", "hasattr", "(", "config", ",", "'use_cuda'", ")", ")", "\n", "\n", "if", "hasattr", "(", "config", ",", "'no_cuda'", ")", ":", "\n", "        ", "use_cuda", "=", "not", "config", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "", "else", ":", "\n", "        ", "use_cuda", "=", "config", ".", "use_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "use_cuda", "else", "\"cpu\"", ")", "\n", "logger", ".", "info", "(", "'Using cuda: '", "+", "str", "(", "use_cuda", ")", ")", "\n", "\n", "### Initialize summary writer.", "\n", "# Flushes every 120 secs by default.", "\n", "# DELETEME Ensure downwards compatibility.", "\n", "if", "not", "hasattr", "(", "tensorboardX", ",", "'__version__'", ")", ":", "\n", "        ", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "os", ".", "path", ".", "join", "(", "config", ".", "out_dir", ",", "'summary'", ")", ")", "\n", "", "else", ":", "\n", "        ", "writer", "=", "SummaryWriter", "(", "logdir", "=", "os", ".", "path", ".", "join", "(", "config", ".", "out_dir", ",", "'summary'", ")", ")", "\n", "\n", "", "return", "device", ",", "writer", ",", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.sim_utils.get_mnet_model": [[153, 301], ["sim_utils.get_mnet_model.hc"], "function", ["None"], ["", "def", "get_mnet_model", "(", "config", ",", "net_type", ",", "in_shape", ",", "out_shape", ",", "device", ",", "cprefix", "=", "None", ",", "\n", "no_weights", "=", "False", ")", ":", "\n", "    ", "\"\"\"Generate a main network instance.\n\n    A helper to generate a main network according to the given the user\n    configurations.\n\n    .. note::\n        Generation of networks with context-modulation is not yet supported,\n        since there is no global argument set in :mod:`utils.cli_args` yet.\n\n    Args:\n        config (argparse.Namespace): Command-line arguments.\n\n            .. note::\n                The function expects command-line arguments available according\n                to the function :func:`utils.cli_args.main_net_args`.\n        net_type (str): The type of network. The following options are\n            available:\n            \n            - ``mlp``: :class:`mnets.mlp.MLP`\n            - ``resnet``: :class:`mnets.resnet.ResNet`\n            - ``zenke``: :class:`mnets.zenkenet.ZenkeNet`\n            - ``bio_conv_net``: :class:`mnets.bio_conv_net.BioConvNet`\n        in_shape (list): Shape of network inputs. Can be ``None`` if not\n            required by network type.\n\n            For instance: For an MLP network :class:`mnets.mlp.MLP` with 100\n            input neurons it should be :code:`in_shape=[100]`.\n        out_shape (list): Shape of network outputs. See ``in_shape`` for more\n            details.\n        device: PyTorch device.\n        cprefix (str, optional): A prefix of the config names. It might be, that\n            the config names used in this method are prefixed, since several\n            main networks should be generated (e.g., :code:`cprefix='gen_'` or\n            ``'dis_'`` when training a GAN).\n\n            Also see docstring of parameter ``prefix`` in function\n            :func:`utils.cli_args.main_net_args`.\n        no_weights (bool): Whether the main network should be generated without\n            weights.\n\n    Returns:\n        The created main network model.\n    \"\"\"", "\n", "assert", "(", "net_type", "in", "[", "'mlp'", ",", "'resnet'", ",", "'zenke'", ",", "'bio_conv_net'", "]", ")", "\n", "\n", "if", "cprefix", "is", "None", ":", "\n", "        ", "cprefix", "=", "''", "\n", "\n", "", "def", "gc", "(", "name", ")", ":", "\n", "        ", "\"\"\"Get config value with that name.\"\"\"", "\n", "return", "getattr", "(", "config", ",", "'%s%s'", "%", "(", "cprefix", ",", "name", ")", ")", "\n", "", "def", "hc", "(", "name", ")", ":", "\n", "        ", "\"\"\"Check whether config exists.\"\"\"", "\n", "return", "hasattr", "(", "config", ",", "'%s%s'", "%", "(", "cprefix", ",", "name", ")", ")", "\n", "\n", "", "mnet", "=", "None", "\n", "\n", "if", "hc", "(", "'net_act'", ")", ":", "\n", "        ", "net_act", "=", "gc", "(", "'net_act'", ")", "\n", "net_act", "=", "misc", ".", "str_to_act", "(", "net_act", ")", "\n", "", "else", ":", "\n", "        ", "net_act", "=", "None", "\n", "\n", "", "def", "get_val", "(", "name", ")", ":", "\n", "        ", "ret", "=", "None", "\n", "if", "hc", "(", "name", ")", ":", "\n", "            ", "ret", "=", "gc", "(", "name", ")", "\n", "", "return", "ret", "\n", "\n", "", "no_bias", "=", "get_val", "(", "'no_bias'", ")", "\n", "dropout_rate", "=", "get_val", "(", "'dropout_rate'", ")", "\n", "specnorm", "=", "get_val", "(", "'specnorm'", ")", "\n", "batchnorm", "=", "get_val", "(", "'batchnorm'", ")", "\n", "no_batchnorm", "=", "get_val", "(", "'no_batchnorm'", ")", "\n", "bn_no_running_stats", "=", "get_val", "(", "'bn_no_running_stats'", ")", "\n", "bn_distill_stats", "=", "get_val", "(", "'bn_distill_stats'", ")", "\n", "#bn_no_stats_checkpointing = get_val('bn_no_stats_checkpointing')", "\n", "\n", "use_bn", "=", "None", "\n", "if", "batchnorm", "is", "not", "None", ":", "\n", "        ", "use_bn", "=", "batchnorm", "\n", "", "elif", "no_batchnorm", "is", "not", "None", ":", "\n", "        ", "use_bn", "=", "not", "no_batchnorm", "\n", "\n", "# FIXME if an argument wasn't specified, then we use the default value that", "\n", "# is currently (at time of implementation) in the constructor.", "\n", "", "assign", "=", "lambda", "x", ",", "y", ":", "y", "if", "x", "is", "None", "else", "x", "\n", "\n", "if", "net_type", "==", "'mlp'", ":", "\n", "        ", "assert", "(", "hc", "(", "'mlp_arch'", ")", ")", "\n", "assert", "(", "len", "(", "in_shape", ")", "==", "1", "and", "len", "(", "out_shape", ")", "==", "1", ")", "\n", "\n", "mnet", "=", "MLP", "(", "n_in", "=", "in_shape", "[", "0", "]", ",", "n_out", "=", "out_shape", "[", "0", "]", ",", "\n", "hidden_layers", "=", "misc", ".", "str_to_ints", "(", "gc", "(", "'mlp_arch'", ")", ")", ",", "\n", "activation_fn", "=", "assign", "(", "net_act", ",", "torch", ".", "nn", ".", "ReLU", "(", ")", ")", ",", "\n", "use_bias", "=", "not", "assign", "(", "no_bias", ",", "False", ")", ",", "\n", "no_weights", "=", "no_weights", ",", "\n", "#init_weights=None,", "\n", "dropout_rate", "=", "assign", "(", "dropout_rate", ",", "-", "1", ")", ",", "\n", "use_spectral_norm", "=", "assign", "(", "specnorm", ",", "False", ")", ",", "\n", "use_batch_norm", "=", "assign", "(", "use_bn", ",", "False", ")", ",", "\n", "bn_track_stats", "=", "assign", "(", "not", "bn_no_running_stats", ",", "True", ")", ",", "\n", "distill_bn_stats", "=", "assign", "(", "bn_distill_stats", ",", "False", ")", ",", "\n", "#use_context_mod=False,", "\n", "#context_mod_inputs=False,", "\n", "#no_last_layer_context_mod=False,", "\n", "#context_mod_no_weights=False,", "\n", "#context_mod_post_activation=False,", "\n", "#context_mod_gain_offset=False,", "\n", "#out_fn=None,", "\n", "verbose", "=", "True", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "elif", "net_type", "==", "'resnet'", ":", "\n", "        ", "assert", "(", "len", "(", "out_shape", ")", "==", "1", ")", "\n", "\n", "mnet", "=", "ResNet", "(", "in_shape", "=", "in_shape", ",", "num_classes", "=", "out_shape", "[", "0", "]", ",", "\n", "verbose", "=", "True", ",", "#n=5,", "\n", "no_weights", "=", "no_weights", ",", "\n", "#init_weights=None,", "\n", "use_batch_norm", "=", "assign", "(", "use_bn", ",", "True", ")", ",", "\n", "bn_track_stats", "=", "assign", "(", "not", "bn_no_running_stats", ",", "True", ")", ",", "\n", "distill_bn_stats", "=", "assign", "(", "bn_distill_stats", ",", "False", ")", ",", "\n", "#use_context_mod=False,", "\n", "#context_mod_inputs=False,", "\n", "#no_last_layer_context_mod=False,", "\n", "#context_mod_no_weights=False,", "\n", "#context_mod_post_activation=False,", "\n", "#context_mod_gain_offset=False,", "\n", "#context_mod_apply_pixel_wise=False", "\n", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "elif", "net_type", "==", "'zenke'", ":", "\n", "        ", "assert", "(", "len", "(", "out_shape", ")", "==", "1", ")", "\n", "\n", "mnet", "=", "ZenkeNet", "(", "in_shape", "=", "in_shape", ",", "num_classes", "=", "out_shape", "[", "0", "]", ",", "\n", "verbose", "=", "True", ",", "#arch='cifar',", "\n", "no_weights", "=", "no_weights", ",", "\n", "#init_weights=None,", "\n", "dropout_rate", "=", "assign", "(", "dropout_rate", ",", "0.25", ")", ")", ".", "to", "(", "device", ")", "\n", "", "else", ":", "\n", "        ", "assert", "(", "net_type", "==", "'bio_conv_net'", ")", "\n", "assert", "(", "len", "(", "out_shape", ")", "==", "1", ")", "\n", "\n", "raise", "NotImplementedError", "(", "'Implementation not publicly available!'", ")", "\n", "\n", "", "return", "mnet", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.sim_utils.get_hnet_model": [[302, 408], ["utils.misc.str_to_ints", "utils.misc.str_to_ints", "utils.misc.str_to_ints", "utils.misc.str_to_ints", "utils.misc.str_to_ints", "utils.misc.str_to_act", "isinstance", "getattr", "sim_utils.get_mnet_model.gc"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_act"], ["", "def", "get_hnet_model", "(", "config", ",", "num_tasks", ",", "device", ",", "mnet_shapes", ",", "cprefix", "=", "None", ")", ":", "\n", "    ", "\"\"\"Generate a hypernetwork instance.\n\n    A helper to generate the hypernetwork according to the given the user\n    configurations.\n\n    Args:\n        config (argparse.Namespace): Command-line arguments.\n\n            .. note::\n                The function expects command-line arguments available according\n                to the function :func:`utils.cli_args.hypernet_args`.\n        num_tasks (int): The number of task embeddings the hypernetwork should\n            have.\n        device: PyTorch device.\n        mnet_shapes: Dimensions of the weight tensors of the main network.\n            See main net argument\n            :attr:`mnets.mnet_interface.MainNetInterface.param_shapes`.\n        cprefix (str, optional): A prefix of the config names. It might be, that\n            the config names used in this method are prefixed, since several\n            hypernetworks should be generated (e.g., :code:`cprefix='gen_'` or\n            ``'dis_'`` when training a GAN).\n\n            Also see docstring of parameter ``prefix`` in function\n            :func:`utils.cli_args.hypernet_args`.\n\n    Returns:\n        The created hypernet model.\n    \"\"\"", "\n", "if", "cprefix", "is", "None", ":", "\n", "        ", "cprefix", "=", "''", "\n", "\n", "", "def", "gc", "(", "name", ")", ":", "\n", "        ", "\"\"\"Get config value with that name.\"\"\"", "\n", "return", "getattr", "(", "config", ",", "'%s%s'", "%", "(", "cprefix", ",", "name", ")", ")", "\n", "\n", "", "hyper_chunks", "=", "misc", ".", "str_to_ints", "(", "gc", "(", "'hyper_chunks'", ")", ")", "\n", "assert", "(", "len", "(", "hyper_chunks", ")", "in", "[", "1", ",", "2", ",", "3", "]", ")", "\n", "if", "len", "(", "hyper_chunks", ")", "==", "1", ":", "\n", "        ", "hyper_chunks", "=", "hyper_chunks", "[", "0", "]", "\n", "\n", "", "hnet_arch", "=", "misc", ".", "str_to_ints", "(", "gc", "(", "'hnet_arch'", ")", ")", "\n", "sa_hnet_filters", "=", "misc", ".", "str_to_ints", "(", "gc", "(", "'sa_hnet_filters'", ")", ")", "\n", "sa_hnet_kernels", "=", "misc", ".", "str_to_ints", "(", "gc", "(", "'sa_hnet_kernels'", ")", ")", "\n", "sa_hnet_attention_layers", "=", "misc", ".", "str_to_ints", "(", "gc", "(", "'sa_hnet_attention_layers'", ")", ")", "\n", "\n", "hnet_act", "=", "misc", ".", "str_to_act", "(", "gc", "(", "'hnet_act'", ")", ")", "\n", "\n", "if", "isinstance", "(", "hyper_chunks", ",", "list", ")", ":", "# Chunked self-attention hypernet", "\n", "        ", "if", "len", "(", "sa_hnet_kernels", ")", "==", "1", ":", "\n", "            ", "sa_hnet_kernels", "=", "sa_hnet_kernels", "[", "0", "]", "\n", "# Note, that the user can specify the kernel size for each dimension and", "\n", "# layer separately.", "\n", "", "elif", "len", "(", "sa_hnet_kernels", ")", ">", "2", "and", "len", "(", "sa_hnet_kernels", ")", "==", "gc", "(", "'sa_hnet_num_layers'", ")", "*", "2", ":", "\n", "            ", "tmp", "=", "sa_hnet_kernels", "\n", "sa_hnet_kernels", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "tmp", ")", ",", "2", ")", ":", "\n", "                ", "sa_hnet_kernels", ".", "append", "(", "[", "tmp", "[", "i", "]", ",", "tmp", "[", "i", "+", "1", "]", "]", ")", "\n", "\n", "", "", "if", "gc", "(", "'hnet_dropout_rate'", ")", "!=", "-", "1", ":", "\n", "            ", "warn", "(", "'SA-Hypernet doesn\\'t use dropout. Dropout rate will be '", "+", "\n", "'ignored.'", ")", "\n", "", "if", "gc", "(", "'hnet_act'", ")", "!=", "'relu'", ":", "\n", "            ", "warn", "(", "'SA-Hypernet doesn\\'t support the other non-linearities '", "+", "\n", "'than ReLUs yet. Option \"%shnet_act\" (%s) will be ignored.'", "\n", "%", "(", "cprefix", ",", "gc", "(", "'hnet_act'", ")", ")", ")", "\n", "\n", "", "hnet", "=", "SAHyperNetwork", "(", "mnet_shapes", ",", "num_tasks", ",", "\n", "out_size", "=", "hyper_chunks", ",", "\n", "num_layers", "=", "gc", "(", "'sa_hnet_num_layers'", ")", ",", "\n", "num_filters", "=", "sa_hnet_filters", ",", "\n", "kernel_size", "=", "sa_hnet_kernels", ",", "\n", "sa_units", "=", "sa_hnet_attention_layers", ",", "\n", "# Note, we don't use an additional hypernet for the remaining", "\n", "# weights!", "\n", "#rem_layers=hnet_arch,", "\n", "te_dim", "=", "gc", "(", "'temb_size'", ")", ",", "\n", "ce_dim", "=", "gc", "(", "'emb_size'", ")", ",", "\n", "no_theta", "=", "False", ",", "\n", "# Batchnorm and spectral norma are not yet implemented.", "\n", "#use_batch_norm=gc('hnet_batchnorm'),", "\n", "#use_spectral_norm=gc('hnet_specnorm'),", "\n", "# Droput would only be used for the additional network, which we", "\n", "# don't use.", "\n", "#dropout_rate=gc('hnet_dropout_rate'),", "\n", "discard_remainder", "=", "True", ",", "\n", "noise_dim", "=", "gc", "(", "'hnet_noise_dim'", ")", ",", "\n", "temb_std", "=", "gc", "(", "'temb_std'", ")", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "elif", "hyper_chunks", "!=", "-", "1", ":", "# Chunked fully-connected hypernet", "\n", "        ", "hnet", "=", "ChunkedHyperNetworkHandler", "(", "mnet_shapes", ",", "num_tasks", ",", "\n", "chunk_dim", "=", "hyper_chunks", ",", "layers", "=", "hnet_arch", ",", "\n", "activation_fn", "=", "hnet_act", ",", "te_dim", "=", "gc", "(", "'temb_size'", ")", ",", "\n", "ce_dim", "=", "gc", "(", "'emb_size'", ")", ",", "dropout_rate", "=", "gc", "(", "'hnet_dropout_rate'", ")", ",", "\n", "noise_dim", "=", "gc", "(", "'hnet_noise_dim'", ")", ",", "\n", "temb_std", "=", "gc", "(", "'temb_std'", ")", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "else", ":", "# Fully-connected hypernet.", "\n", "        ", "hnet", "=", "HyperNetwork", "(", "mnet_shapes", ",", "num_tasks", ",", "layers", "=", "hnet_arch", ",", "\n", "te_dim", "=", "gc", "(", "'temb_size'", ")", ",", "activation_fn", "=", "hnet_act", ",", "\n", "dropout_rate", "=", "gc", "(", "'hnet_dropout_rate'", ")", ",", "\n", "noise_dim", "=", "gc", "(", "'hnet_noise_dim'", ")", ",", "\n", "temb_std", "=", "gc", "(", "'temb_std'", ")", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "return", "hnet", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.logger_config.config_logger": [[31, 82], ["logging.Formatter", "logging.Formatter", "logging.StreamHandler", "logging.StreamHandler.setFormatter", "logging.StreamHandler.setLevel", "logging.getLogger", "logging.getLogger.setLevel", "logging.getLogger.addHandler", "os.path.dirname", "os.path.exists", "logging.FileHandler", "logging.FileHandler.setFormatter", "logging.FileHandler.setLevel", "logging.getLogger.addHandler", "os.mkdir", "os.remove", "os.path.isdir"], "function", ["None"], ["def", "config_logger", "(", "name", ",", "log_file", ",", "file_level", ",", "console_level", ")", ":", "\n", "    ", "\"\"\"Configure the logger that should be used by all modules in this\n    package.\n    This method sets up a logger, such that all messages are written to console\n    and to an extra logging file. Both outputs will be the same, except that\n    a message logged to file contains the module name, where the message comes\n    from.\n\n    The implementation is based on an earlier implementation of a function I\n    used in another project:\n\n        https://git.io/fNDZJ\n\n    Args:\n        name: The name of the created logger.\n        log_file: Path of the log file. If None, no logfile will be generated.\n            If the logfile already exists, it will be overwritten.\n        file_level: Log level for logging to log file.\n        console_level: Log level for logging to console.\n\n    Returns:\n        The configured logger.\n    \"\"\"", "\n", "file_formatter", "=", "logging", ".", "Formatter", "(", "fmt", "=", "'%(asctime)s - %(levelname)s'", "+", "' - %(module)s - %(message)s'", ",", "datefmt", "=", "'%m/%d/%Y %I:%M:%S %p'", ")", "\n", "stream_formatter", "=", "logging", ".", "Formatter", "(", "fmt", "=", "'%(asctime)s - %(levelname)s'", "+", "' - %(message)s'", ",", "datefmt", "=", "'%m/%d/%Y %I:%M:%S %p'", ")", "\n", "\n", "if", "log_file", "is", "not", "None", ":", "\n", "        ", "log_dir", "=", "os", ".", "path", ".", "dirname", "(", "log_file", ")", "\n", "if", "log_dir", "!=", "''", "and", "not", "os", ".", "path", ".", "isdir", "(", "log_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "log_dir", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "log_file", ")", ":", "\n", "            ", "os", ".", "remove", "(", "log_file", ")", "\n", "", "file_handler", "=", "logging", ".", "FileHandler", "(", "log_file", ")", "\n", "file_handler", ".", "setFormatter", "(", "file_formatter", ")", "\n", "file_handler", ".", "setLevel", "(", "file_level", ")", "\n", "\n", "", "stream_handler", "=", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "\n", "stream_handler", ".", "setFormatter", "(", "stream_formatter", ")", "\n", "stream_handler", ".", "setLevel", "(", "console_level", ")", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", "name", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "if", "log_file", "is", "not", "None", ":", "\n", "        ", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "", "logger", ".", "addHandler", "(", "stream_handler", ")", "\n", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.__init__": [[120, 232], ["torch.Module.__init__", "batchnorm_layer.BatchNormLayer.register_buffer", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "NotImplementedError", "NotImplementedError", "ValueError", "ValueError", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "batchnorm_layer.BatchNormLayer.register_parameter", "batchnorm_layer.BatchNormLayer.register_parameter", "batchnorm_layer.BatchNormLayer._weights.append", "batchnorm_layer.BatchNormLayer._weights.append", "torch.init.ones_", "torch.init.ones_", "torch.init.ones_", "torch.init.zeros_", "torch.init.zeros_", "torch.init.zeros_", "NotImplementedError", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "batchnorm_layer.BatchNormLayer.checkpoint_stats", "batchnorm_layer.BatchNormLayer._stats_names", "batchnorm_layer.BatchNormLayer.register_buffer", "batchnorm_layer.BatchNormLayer.register_buffer", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.checkpoint_stats", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer._stats_names"], ["def", "__init__", "(", "self", ",", "num_features", ",", "momentum", "=", "0.1", ",", "affine", "=", "True", ",", "\n", "track_running_stats", "=", "True", ",", "frozen_stats", "=", "False", ",", "\n", "learnable_stats", "=", "False", ")", ":", "\n", "        ", "r\"\"\"\n        Args:\n            num_features: See argument ``num_features``, for instance, of class\n                :class:`torch.nn.BatchNorm1d`.\n            momentum: See argument ``momentum`` of class\n                :class:`torch.nn.BatchNorm1d`.\n            affine: See argument ``affine`` of class\n                :class:`torch.nn.BatchNorm1d`. If set to :code:`False`, the\n                input activity will simply be \"whitened\" according to the\n                applied layer statistics (except if gain :math:`\\gamma` and\n                offset :math:`\\beta` are passed to the :meth:`forward` method).\n\n                Note, if ``learnable_stats`` is :code:`False`, then setting\n                ``affine`` to :code:`False` results in no learnable weights for\n                this layer (running stats might still be updated, but not via\n                gradient descent).\n\n                Note, even if this option is ``False``, one may still pass a\n                gain :math:`\\gamma` and offset :math:`\\beta` to the\n                :meth:`forward` method.\n            track_running_stats: See argument ``track_running_stats`` of class\n                :class:`torch.nn.BatchNorm1d`.\n            frozen_stats: If ``True``, the layer statistics are frozen at their\n                initial values of :math:`\\gamma = 1` and :math:`\\beta = 0`,\n                i.e., layer activity will not be whitened.\n\n                Note, this option requires ``track_running_stats`` to be set to\n                ``False``.\n            learnable_stats: If ``True``, the layer statistics are initialized\n                as learnable parameters (:code:`requires_grad=True`).\n\n                Note, these extra parameters will be maintained internally and\n                not added to the :attr:`weights`. Statistics can always be\n                maintained externally and passed to the :meth:`forward` method.\n\n                Note, this option requires ``track_running_stats`` to be set to\n                ``False``.\n        \"\"\"", "\n", "super", "(", "BatchNormLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "learnable_stats", ":", "\n", "# FIXME We need our custom stats computation for this.", "\n", "# The running stats updated by `torch.nn.functional.batch_norm` do", "\n", "# not allow backpropagation.", "\n", "# See here on how they are computed:", "\n", "# https://github.com/pytorch/pytorch/blob/96fe2b4ecbbd02143d95f467655a2d697282ac32/aten/src/ATen/native/Normalization.cpp#L137", "\n", "            ", "raise", "NotImplementedError", "(", "'Option \"learnable_stats\" has not been '", "+", "\n", "'implemented yet!'", ")", "\n", "\n", "", "if", "momentum", "is", "None", ":", "\n", "# If one wants to implement this, then please note that the", "\n", "# attribute `num_batches_tracked` has to be added. Also, note the", "\n", "# extra code for computing the momentum value in the forward method", "\n", "# of class `_BatchNorm`:", "\n", "# https://pytorch.org/docs/stable/_modules/torch/nn/modules/batchnorm.html#_BatchNorm", "\n", "            ", "raise", "NotImplementedError", "(", "'This reimplementation of PyTorch its '", "+", "\n", "'batchnorm layer does not support '", "+", "\n", "'setting \"momentum\" to None.'", ")", "\n", "\n", "", "if", "learnable_stats", "and", "track_running_stats", ":", "\n", "            ", "raise", "ValueError", "(", "'Option \"track_running_stats\" must be set to '", "+", "\n", "'False when enabling \"learnable_stats\".'", ")", "\n", "\n", "", "if", "frozen_stats", "and", "track_running_stats", ":", "\n", "            ", "raise", "ValueError", "(", "'Option \"track_running_stats\" must be set to '", "+", "\n", "'False when enabling \"frozen_stats\".'", ")", "\n", "\n", "", "self", ".", "_num_features", "=", "num_features", "\n", "self", ".", "_momentum", "=", "momentum", "\n", "self", ".", "_affine", "=", "affine", "\n", "self", ".", "_track_running_stats", "=", "track_running_stats", "\n", "self", ".", "_frozen_stats", "=", "frozen_stats", "\n", "self", ".", "_learnable_stats", "=", "learnable_stats", "\n", "\n", "self", ".", "register_buffer", "(", "'_num_stats'", ",", "torch", ".", "tensor", "(", "0", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "\n", "self", ".", "_weights", "=", "nn", ".", "ParameterList", "(", ")", "\n", "self", ".", "_param_shapes", "=", "[", "[", "num_features", "]", ",", "[", "num_features", "]", "]", "\n", "\n", "if", "affine", ":", "\n", "# Gamma", "\n", "            ", "self", ".", "register_parameter", "(", "'scale'", ",", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_features", ")", ",", "requires_grad", "=", "True", ")", ")", "\n", "# Beta", "\n", "self", ".", "register_parameter", "(", "'bias'", ",", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_features", ")", ",", "requires_grad", "=", "True", ")", ")", "\n", "\n", "self", ".", "_weights", ".", "append", "(", "self", ".", "scale", ")", "\n", "self", ".", "_weights", ".", "append", "(", "self", ".", "bias", ")", "\n", "\n", "nn", ".", "init", ".", "ones_", "(", "self", ".", "scale", ")", "\n", "nn", ".", "init", ".", "zeros_", "(", "self", ".", "bias", ")", "\n", "\n", "", "elif", "not", "learnable_stats", ":", "\n", "            ", "self", ".", "_weights", "=", "None", "\n", "\n", "", "if", "learnable_stats", ":", "\n", "# Don't forget to add the new params to `self._weights`.", "\n", "# Don't forget to add shapes to `self._param_shapes`.", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "\n", "", "elif", "track_running_stats", "or", "frozen_stats", ":", "\n", "# Note, in case of frozen stats, we just don't update the stats", "\n", "# initialized here later on.", "\n", "            ", "self", ".", "checkpoint_stats", "(", ")", "\n", "", "else", ":", "\n", "            ", "mname", ",", "vname", "=", "self", ".", "_stats_names", "(", "0", ")", "\n", "self", ".", "register_buffer", "(", "mname", ",", "None", ")", "\n", "self", ".", "register_buffer", "(", "vname", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.weights": [[233, 242], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`weights`.\n\n        Returns:\n            A :class:`torch.nn.ParameterList` or ``None``, if no parameters are\n            internally maintained.\n        \"\"\"", "\n", "return", "self", ".", "_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.param_shapes": [[243, 251], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "param_shapes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`param_shapes`.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "return", "self", ".", "_param_shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.hyper_shapes": [[252, 263], ["NotImplementedError"], "methods", ["None"], ["", "@", "property", "\n", "def", "hyper_shapes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`hyper_shapes`.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "# FIXME not implemented attribute. Do we even need the attribute, given", "\n", "# that all components are individually passed to the forward method?", "\n", "raise", "NotImplementedError", "(", "'Not implemented yet!'", ")", "\n", "return", "self", ".", "_hyper_shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.num_stats": [[264, 272], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_stats", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`num_stats`.\n\n        Returns:\n            (int)\n        \"\"\"", "\n", "return", "self", ".", "_num_stats", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.forward": [[273, 459], ["torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "NotImplementedError", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "torch.batch_norm", "ValueError", "ValueError", "warnings.warn", "batchnorm_layer.BatchNormLayer.get_stats"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.get_stats"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "running_mean", "=", "None", ",", "running_var", "=", "None", ",", "weight", "=", "None", ",", "\n", "bias", "=", "None", ",", "stats_id", "=", "None", ")", ":", "\n", "        ", "r\"\"\"Apply batch normalization to given layer activations.\n\n        Based on the state if this module (attribute :attr:`training`), the\n        configuration of this layer and the parameters currently passed, the\n        behavior of this function will be different.\n\n        The core of this method still relies on the function\n        :func:`torch.nn.functional.batch_norm`. In the following we list the\n        different behaviors of this method based on the context.\n\n        **In training mode:**\n\n        We first consider the case that this module is in training mode, i.e.,\n        :meth:`torch.nn.Module.train` has been called.\n\n        Usually, during training, the running statistics are not used when\n        computing the output, instead the statistics computed on the current\n        batch are used (denoted by *use batch stats* in the table below).\n        However, the batch statistics are typically updated during training\n        (denoted by *update running stats* in the table below).\n\n        The above described scenario would correspond to passing batch\n        statistics to the function :func:`torch.nn.functional.batch_norm` and\n        setting the parameter ``training`` to ``True``.\n\n        +----------------------+---------------------+-------------------------+\n        | **training mode**    | **use batch stats** | **update running stats**|\n        +----------------------+---------------------+-------------------------+\n        | given stats          | Yes                 | Yes                     |\n        +----------------------+---------------------+-------------------------+\n        | track running stats  | Yes                 | Yes                     |\n        +----------------------+---------------------+-------------------------+\n        | frozen stats         | No                  | No                      |\n        +----------------------+---------------------+-------------------------+\n        | learnable stats      | Yes                 | Yes [1]_                |\n        +----------------------+---------------------+-------------------------+\n        |no track running stats| Yes                 | No                      |\n        +----------------------+---------------------+-------------------------+\n\n        The meaning of each row in this table is as follows:\n\n            - **given stats**: External stats are provided via the parameters\n              ``running_mean`` and ``running_var``.\n            - **track running stats**: If ``track_running_stats`` was set to\n              ``True`` in the constructor and no stats were given.\n            - **frozen stats**: If ``frozen_stats`` was set to ``True`` in the\n              constructor and no stats were given.\n            - **learnable stats**: If ``learnable_stats`` was set to ``True`` in\n              the constructor and no stats were given.\n            - **no track running stats**: If none of the above options apply,\n              then the statistics will always be computed from the current batch\n              (also in eval mode).\n\n        .. note::\n            If provided, running stats specified via ``running_mean`` and\n            ``running_var`` always have priority.\n\n        .. [1] We use a custom implementation to update the running statistics,\n           that is compatible with backpropagation.\n\n        **In evaluation mode:**\n\n        We now consider the case that this module is in evaluation mode, i.e.,\n        :meth:`torch.nn.Module.eval` has been called.\n\n        Here is the same table as above just for the evaluation mode.\n\n        +----------------------+---------------------+-------------------------+\n        | **evaluation mode**  | **use batch stats** | **update running stats**|\n        +----------------------+---------------------+-------------------------+\n        | track running stats  | No                  | No                      |\n        +----------------------+---------------------+-------------------------+\n        | frozen stats         | No                  | No                      |\n        +----------------------+---------------------+-------------------------+\n        | learnable stats      | No                  | No                      |\n        +----------------------+---------------------+-------------------------+\n        | given stats          | No                  | No                      |\n        +----------------------+---------------------+-------------------------+\n        |no track running stats| Yes                 | No                      |\n        +----------------------+---------------------+-------------------------+\n\n        Args:\n            inputs: The inputs to the batchnorm layer.\n            running_mean (optional): Running mean stats\n                :math:`m_{\\text{stats}}`. This option has priority, i.e., any\n                internally maintained statistics are ignored if given.\n\n                .. note::\n                    If specified, then ``running_var`` also has to be specified.\n            running_var (optional): Similar to option ``running_mean``, but for\n                the running variance stats :math:`v_{\\text{stats}}`\n\n                .. note::\n                    If specified, then ``running_mean`` also has to be\n                    specified.\n            weight (optional): The gain factors :math:`\\gamma`. If given, any\n                internal gains are ignored. If option ``affine`` was set to\n                ``False`` in the constructor and this option remains ``None``,\n                then no gains are multiplied to the \"whitened\" inputs.\n            bias (optional): The behavior of this option is similar to option\n                ``weight``, except that this option represents the offsets\n                :math:`\\beta`.\n            stats_id: This argument is optional except if multiple running\n                stats checkpoints exist (i.e., attribute :attr:`num_stats` is\n                greater than 1) and no running stats have been provided to this\n                method.\n\n                .. note::\n                    This argument is ignored if running stats have been passed.\n\n        Returns:\n            The layer activation ``inputs`` after batch-norm has been applied.\n        \"\"\"", "\n", "assert", "(", "running_mean", "is", "None", "and", "running_var", "is", "None", "or", "running_mean", "is", "not", "None", "and", "running_var", "is", "not", "None", ")", "\n", "\n", "if", "not", "self", ".", "_affine", ":", "\n", "            ", "if", "weight", "is", "None", "or", "bias", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "'Layer was generated in non-affine mode. '", "+", "\n", "'Therefore, arguments \"weight\" and \"bias\" '", "+", "\n", "'may not be None.'", ")", "\n", "\n", "# No gains given but we have internal gains.", "\n", "# Otherwise, if no gains are given we leave `weight` as None.", "\n", "", "", "if", "weight", "is", "None", "and", "self", ".", "_affine", ":", "\n", "            ", "weight", "=", "self", ".", "scale", "\n", "", "if", "bias", "is", "None", "and", "self", ".", "_affine", ":", "\n", "            ", "bias", "=", "self", ".", "bias", "\n", "\n", "", "stats_given", "=", "running_mean", "is", "not", "None", "\n", "\n", "if", "(", "running_mean", "is", "None", "or", "running_var", "is", "None", ")", ":", "\n", "            ", "if", "stats_id", "is", "None", "and", "self", ".", "num_stats", ">", "1", ":", "\n", "                ", "raise", "ValueError", "(", "'Parameter \"stats_id\" is not defined but '", "+", "\n", "'multiple running stats are available.'", ")", "\n", "", "elif", "self", ".", "_track_running_stats", ":", "\n", "                ", "if", "stats_id", "is", "None", ":", "\n", "                    ", "stats_id", "=", "0", "\n", "", "assert", "(", "stats_id", "<", "self", ".", "num_stats", ")", "\n", "\n", "rm", ",", "rv", "=", "self", ".", "get_stats", "(", "stats_id", ")", "\n", "\n", "if", "running_mean", "is", "None", ":", "\n", "                    ", "running_mean", "=", "rm", "\n", "", "if", "running_var", "is", "None", ":", "\n", "                    ", "running_var", "=", "rv", "\n", "", "", "", "elif", "stats_id", "is", "not", "None", ":", "\n", "            ", "warn", "(", "'Parameter \"stats_id\" is ignored since running stats have '", "+", "\n", "'been provided.'", ")", "\n", "\n", "", "momentum", "=", "self", ".", "_momentum", "\n", "\n", "if", "stats_given", "or", "self", ".", "_track_running_stats", ":", "\n", "            ", "return", "F", ".", "batch_norm", "(", "inputs", ",", "running_mean", ",", "running_var", ",", "\n", "weight", "=", "weight", ",", "bias", "=", "bias", ",", "\n", "training", "=", "self", ".", "training", ",", "momentum", "=", "momentum", ")", "\n", "\n", "", "if", "self", ".", "_learnable_stats", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "\n", "", "if", "self", ".", "_frozen_stats", ":", "\n", "            ", "return", "F", ".", "batch_norm", "(", "inputs", ",", "running_mean", ",", "running_var", ",", "\n", "weight", "=", "weight", ",", "bias", "=", "bias", ",", "training", "=", "False", ")", "\n", "\n", "# TODO implement scale and shift here. Note, that `running_mean` and", "\n", "# `running_var` are always 0 and 1, resp. Therefore, the call to", "\n", "# `F.batch_norm` is a waste of computation.", "\n", "#ret = inputs", "\n", "#if weight is not None:", "\n", "#    # Multiply `ret` with `weight` such that dimensions are", "\n", "#    # respected.", "\n", "#    pass", "\n", "#if bias is not None:", "\n", "#    # Add `bias` to modified `ret` such that dimensions are", "\n", "#    # respected.", "\n", "#    pass", "\n", "#return ret", "\n", "\n", "", "else", ":", "\n", "            ", "assert", "(", "not", "self", ".", "_track_running_stats", ")", "\n", "\n", "# Always compute statistics based on current batch.", "\n", "return", "F", ".", "batch_norm", "(", "inputs", ",", "None", ",", "None", ",", "weight", "=", "weight", ",", "bias", "=", "bias", ",", "\n", "training", "=", "True", ",", "momentum", "=", "momentum", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.checkpoint_stats": [[460, 489], ["batchnorm_layer.BatchNormLayer._stats_names", "batchnorm_layer.BatchNormLayer.register_buffer", "batchnorm_layer.BatchNormLayer.register_buffer", "NotImplementedError", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "batchnorm_layer.BatchNormLayer._stats_names", "getattr"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer._stats_names", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer._stats_names"], ["", "", "def", "checkpoint_stats", "(", "self", ",", "device", "=", "None", ")", ":", "\n", "        ", "\"\"\"Buffers for a new set of running stats will be registered.\n\n        Calling this function will also increment the attribute\n        :attr:`num_stats`.\n\n        Args:\n            device (optional): If not provided, the newly created statistics\n                will either be moved to the device of the most recent statistics\n                or to CPU if no prior statistics exist.\n        \"\"\"", "\n", "assert", "(", "self", ".", "_track_running_stats", "or", "self", ".", "_frozen_stats", "and", "self", ".", "_num_stats", "==", "0", ")", "\n", "\n", "if", "device", "is", "None", ":", "\n", "            ", "if", "self", ".", "num_stats", ">", "0", ":", "\n", "                ", "mname_old", ",", "_", "=", "self", ".", "_stats_names", "(", "self", ".", "_num_stats", "-", "1", ")", "\n", "device", "=", "getattr", "(", "self", ",", "mname_old", ")", ".", "device", "\n", "\n", "", "", "if", "self", ".", "_learnable_stats", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "\n", "", "mname", ",", "vname", "=", "self", ".", "_stats_names", "(", "self", ".", "_num_stats", ")", "\n", "self", ".", "_num_stats", "+=", "1", "\n", "\n", "self", ".", "register_buffer", "(", "mname", ",", "torch", ".", "zeros", "(", "self", ".", "_num_features", ",", "\n", "device", "=", "device", ")", ")", "\n", "self", ".", "register_buffer", "(", "vname", ",", "torch", ".", "ones", "(", "self", ".", "_num_features", ",", "\n", "device", "=", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.get_stats": [[490, 513], ["batchnorm_layer.BatchNormLayer._stats_names", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer._stats_names"], ["", "def", "get_stats", "(", "self", ",", "stats_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get a set of running statistics (means and variances).\n\n        Args:\n            stats_id (optional): ID of stats. If not provided, the most recent\n                stats are returned.\n\n        Returns:\n            (tuple): Tuple containing:\n\n            - **running_mean**\n            - **running_var**\n        \"\"\"", "\n", "if", "stats_id", "is", "None", ":", "\n", "            ", "stats_id", "=", "self", ".", "num_stats", "-", "1", "\n", "", "assert", "(", "stats_id", "<", "self", ".", "num_stats", ")", "\n", "\n", "mname", ",", "vname", "=", "self", ".", "_stats_names", "(", "stats_id", ")", "\n", "\n", "running_mean", "=", "getattr", "(", "self", ",", "mname", ")", "\n", "running_var", "=", "getattr", "(", "self", ",", "vname", ")", "\n", "\n", "return", "running_mean", ",", "running_var", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer._stats_names": [[515, 532], ["None"], "methods", ["None"], ["", "def", "_stats_names", "(", "self", ",", "stats_id", ")", ":", "\n", "        ", "\"\"\"Get the buffer names for mean and variance statistics depending on\n        the ``stats_id``, i.e., the ID of the stats checkpoint.\n\n        Args:\n            stats_id: ID of stats.\n\n        Returns:\n            (tuple): Tuple containing:\n\n            - **mean_name**\n            - **var_name**\n        \"\"\"", "\n", "mean_name", "=", "'mean_%d'", "%", "stats_id", "\n", "var_name", "=", "'var_%d'", "%", "stats_id", "\n", "\n", "return", "mean_name", ",", "var_name", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.__init__": [[59, 77], ["abc.ABC.__init__", "warnings.warn"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize the network.\"\"\"", "\n", "super", "(", "CLHyperNetInterface", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "warn", "(", "'Please use class \"hnets.hnet_interface.CLHyperNetInterface\" '", "+", "\n", "'instead.'", ",", "DeprecationWarning", ")", "\n", "\n", "# The following member variables have to be set by all classes that", "\n", "# implement this interface.", "\n", "self", ".", "_theta", "=", "None", "\n", "self", ".", "_task_embs", "=", "None", "\n", "self", ".", "_theta_shapes", "=", "None", "\n", "# Task embedding weights + theta weights.", "\n", "self", ".", "_num_weights", "=", "None", "\n", "self", ".", "_num_outputs", "=", "None", "\n", "# If an external input is required, this may not be None.", "\n", "self", ".", "_size_ext_input", "=", "None", "\n", "self", ".", "_target_shapes", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface._is_properly_setup": [[78, 87], ["None"], "methods", ["None"], ["", "def", "_is_properly_setup", "(", "self", ")", ":", "\n", "        ", "\"\"\"This method can be used by classes that implement this interface to\n        check whether all required properties have been set.\"\"\"", "\n", "#assert(self._theta is not None)", "\n", "#assert(self._task_embs is not None)", "\n", "assert", "(", "self", ".", "_theta_shapes", "is", "not", "None", ")", "\n", "assert", "(", "self", ".", "_num_weights", "is", "not", "None", ")", "\n", "assert", "(", "self", ".", "_num_outputs", "is", "not", "None", ")", "\n", "assert", "(", "self", ".", "_target_shapes", "is", "not", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.theta": [[88, 102], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "theta", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute theta.\n\n        Theta are all learnable parameters of the hypernet except the task\n        embeddings, i.e., theta comprises all parameters that should be\n        regularized in order to avoid catastrophic forgetting when training\n        the hypernetwork in a Continual Learning setting.\n\n        Returns:\n            A :class:`torch.nn.ParameterList` or None, if this network has no\n            weights.\n        \"\"\"", "\n", "return", "self", ".", "_theta", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.num_outputs": [[103, 107], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_outputs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for the attribute num_outputs.\"\"\"", "\n", "return", "self", ".", "_num_outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.num_weights": [[108, 112], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute num_weights.\"\"\"", "\n", "return", "self", ".", "_num_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.has_theta": [[113, 117], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_theta", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute has_theta.\"\"\"", "\n", "return", "self", ".", "_theta", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.theta_shapes": [[118, 126], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "theta_shapes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute theta_shapes.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "return", "self", ".", "_theta_shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.has_task_embs": [[127, 131], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_task_embs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute has_task_embs.\"\"\"", "\n", "return", "self", ".", "_task_embs", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.num_task_embs": [[132, 137], ["len"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_task_embs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute num_task_embs.\"\"\"", "\n", "assert", "(", "self", ".", "has_task_embs", ")", "\n", "return", "len", "(", "self", ".", "_task_embs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.requires_ext_input": [[138, 142], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "requires_ext_input", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute requires_ext_input.\"\"\"", "\n", "return", "self", ".", "_size_ext_input", "is", "not", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.target_shapes": [[143, 151], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "target_shapes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute target_shapes.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "return", "self", ".", "_target_shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.get_task_embs": [[152, 160], ["None"], "methods", ["None"], ["", "def", "get_task_embs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return a list of all task embeddings.\n\n        Returns:\n            A list of Parameter tensors.\n        \"\"\"", "\n", "assert", "(", "self", ".", "has_task_embs", ")", "\n", "return", "self", ".", "_task_embs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.get_task_emb": [[161, 173], ["None"], "methods", ["None"], ["", "def", "get_task_emb", "(", "self", ",", "task_id", ")", ":", "\n", "        ", "\"\"\"Return the task embedding corresponding to a given task id.\n\n        Args:\n            task_id: Determines the task for which the embedding should be\n                returned.\n\n        Returns:\n            A list of Parameter tensors.\n        \"\"\"", "\n", "assert", "(", "self", ".", "has_task_embs", ")", "\n", "return", "self", ".", "_task_embs", "[", "task_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.CLHyperNetInterface.forward": [[174, 206], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "forward", "(", "self", ",", "task_id", "=", "None", ",", "theta", "=", "None", ",", "dTheta", "=", "None", ",", "task_emb", "=", "None", ",", "\n", "ext_inputs", "=", "None", ",", "squeeze", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute all HyperWeights.\n\n        Args:\n            task_id: The index of the task for which the network should\n                produce weights. The corresponding internal task embedding will\n                be selected as input. Only one integer can be given!\n            theta: List of weight tensors, that are used as network parameters.\n                If \"has_theta\" is False, then this parameter is mandatory.\n                Note, when provided, internal parameters (theta) are not used.\n            dTheta: List of weight tensors, that are added to \"theta\" (the\n                internal list of parameters or the one given via the option\n                \"theta\"), when computing the output of this network.\n            task_emb: If \"has_task_embs\" is False, then one has to provide the\n                task embedding as additional input via this option.\n            ext_inputs: If \"requires_ext_input\" is True, then one has to provide\n                the additional embeddings as input here. Note, one might provide\n                a batch of embeddings (see option \"squeeze\" for details).\n            squeeze: If a batch of inputs is given, the first dimension of the\n                resulting weight tensors will have as first dimension the batch\n                dimension. Though, the main network expects this dimension to\n                be squeezed away. This will be done automatically if this\n                option is enabled (hence, it only has an effect for a batch\n                size of 1).\n\n        Returns:\n            A list of weights. Two consecutive entries always correspond to a\n            weight matrix followed by a bias vector.\n        \"\"\"", "\n", "pass", "# TODO implement", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.MainNetInterface.__init__": [[239, 258], ["abc.ABC.__init__", "warnings.warn"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize the network.\n\n        Args:\n\n        \"\"\"", "\n", "super", "(", "MainNetInterface", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "warn", "(", "'Please use class \"mnets.mnet_interface.MainNetInterface\" '", "+", "\n", "'instead.'", ",", "DeprecationWarning", ")", "\n", "\n", "# The following member variables have to be set by all classes that", "\n", "# implement this interface.", "\n", "self", ".", "_weights", "=", "None", "\n", "self", ".", "_all_shapes", "=", "None", "\n", "self", ".", "_hyper_shapes", "=", "None", "\n", "self", ".", "_num_params", "=", "None", "\n", "self", ".", "_has_bias", "=", "None", "\n", "self", ".", "_has_fc_out", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.MainNetInterface._is_properly_setup": [[259, 273], ["isinstance", "isinstance", "len", "len", "len", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "_is_properly_setup", "(", "self", ")", ":", "\n", "        ", "\"\"\"This method can be used by classes that implement this interface to\n        check whether all required properties have been set.\"\"\"", "\n", "assert", "(", "self", ".", "_weights", "is", "not", "None", "or", "self", ".", "_hyper_shapes", "is", "not", "None", ")", "\n", "if", "self", ".", "_weights", "is", "not", "None", "and", "self", ".", "_hyper_shapes", "is", "not", "None", ":", "\n", "            ", "assert", "(", "(", "len", "(", "self", ".", "_weights", ")", "+", "len", "(", "self", ".", "_hyper_shapes", ")", ")", "==", "len", "(", "self", ".", "_all_shapes", ")", ")", "\n", "", "elif", "self", ".", "_weights", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "self", ".", "_weights", ")", "==", "len", "(", "self", ".", "_all_shapes", ")", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "len", "(", "self", ".", "_hyper_shapes", ")", "==", "len", "(", "self", ".", "_all_shapes", ")", ")", "\n", "", "assert", "(", "self", ".", "_all_shapes", "is", "not", "None", ")", "\n", "assert", "(", "isinstance", "(", "self", ".", "_has_bias", ",", "bool", ")", ")", "\n", "assert", "(", "isinstance", "(", "self", ".", "_has_fc_out", ",", "bool", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.MainNetInterface.weights": [[274, 283], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute weights.\n\n        Returns:\n            A :class:`torch.nn.ParameterList` or None, if no parameters are\n            internally maintained.\n        \"\"\"", "\n", "return", "self", ".", "_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.MainNetInterface.param_shapes": [[284, 292], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "param_shapes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute param_shapes.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "return", "self", ".", "_all_shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.MainNetInterface.hyper_shapes": [[293, 301], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "hyper_shapes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute hyper_shapes.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "return", "self", ".", "_hyper_shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.MainNetInterface.has_bias": [[302, 306], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_bias", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute has_bias.\"\"\"", "\n", "return", "self", ".", "_has_bias", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.MainNetInterface.has_fc_out": [[307, 311], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_fc_out", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute has_fc_out.\"\"\"", "\n", "return", "self", ".", "_has_fc_out", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.module_wrappers.MainNetInterface.num_params": [[312, 323], ["int", "numpy.sum", "numpy.prod"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute num_params.\n\n        Returns:\n            Total number of parameters in the network.\n        \"\"\"", "\n", "if", "self", ".", "_num_params", "is", "None", ":", "\n", "            ", "self", ".", "_num_params", "=", "int", "(", "np", ".", "sum", "(", "[", "np", ".", "prod", "(", "l", ")", "for", "l", "in", "\n", "self", ".", "param_shapes", "]", ")", ")", "\n", "", "return", "self", ".", "_num_params", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer.compute_fisher": [[40, 423], ["isinstance", "mnet.eval", "data.reset_batch_generator", "range", "range", "enumerate", "mnet.train", "isinstance", "isinstance", "isinstance", "NotImplementedError", "min", "hnet.eval", "fisher.append", "data.next_train_batch", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "mnet.zero_grad", "torch.autograd.backward", "enumerate", "len", "ewc_regularizer._ewc_buffer_names", "net.register_buffer", "net.register_buffer", "hnet.train", "torch.zeros_like", "ewc_regularizer.cognet_mse_nll.custom_nll", "hnet.zero_grad", "torch.pow", "p.detach().clone", "getattr", "fisher[].detach().clone", "mnet.forward", "custom_forward", "hnet.forward", "mnet.forward", "custom_forward", "len", "len", "p.grad.detach", "NotImplementedError", "p.detach", "fisher[].detach", "len", "len", "torch.argmax", "torch.argmax", "torch.nn.functional.nll_loss", "torch.nn.functional.log_softmax", "ll.permute.permute", "torch.nn.functional.nll_loss", "nll.mean().sum.mean().sum", "torch.nn.functional.log_softmax", "torch.tensor().to", "len", "len", "nll.mean().sum.mean", "torch.tensor"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.reset_batch_generator", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer._ewc_buffer_names", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["def", "compute_fisher", "(", "task_id", ",", "data", ",", "params", ",", "device", ",", "mnet", ",", "hnet", "=", "None", ",", "\n", "empirical_fisher", "=", "True", ",", "online", "=", "False", ",", "gamma", "=", "1.", ",", "n_max", "=", "-", "1", ",", "\n", "regression", "=", "False", ",", "time_series", "=", "False", ",", "\n", "allowed_outputs", "=", "None", ",", "custom_forward", "=", "None", ",", "custom_nll", "=", "None", ")", ":", "\n", "    ", "r\"\"\"Compute estimates of the diagonal elements of the Fisher information\n    matrix, as needed as importance-weights by elastic weight consolidation\n    (EWC).\n\n    Note, this method registers buffers in the given module (storing the\n    current parameters and the estimate of the Fisher diagonal elements), i.e.,\n    the \"mnet\" if \"hnet\" is None, otherwise the \"hnet\".\n\n    Args:\n        task_id: The ID of the current task, needed to store the computed\n            tensors with a unique name. When \"hnet\" is given, it is used as\n            input to the \"hnet\" forward method to select the current task\n            embedding.\n        data: A data handler. We will compute the Fisher estimate across the\n            whole training set (except ``n_max`` is specified).\n        params: A list of parameter tensors from the module of which we aim to\n            compute the Fisher for. If ``hnet`` is given, then these are assumed\n            to be the \"theta\" parameters, that we pass to the forward function\n            of the hypernetwork. Otherwise, these are the \"weights\" passed to\n            the forward method of the main network.\n            Note, they might not be detached from their original parameters,\n            because we use ``backward()`` on the computational graph to read out\n            the ``.grad`` variable.\n            Note, the order in which these parameters are passed to this method\n            and the corresponding EWC loss function must not change, because\n            the index within the \"params\" list will be used as unique\n            identifier.\n        device: Current PyTorch device.\n        mnet: The main network. If ``hnet`` is ``None``, then ``params`` are\n            assumed to belong to this network. The fisher estimate will be\n            computed accordingly.\n            Note, ``params`` might be the output of the hypernetwork, i.e.,\n            weights for a specific task. In this case, \"online\"-EWC doesn't make\n            much sense, as we don't follow the Bayesian view of using the old\n            task weights as prior for the current once. Instead, we have a new\n            set of weights for all tasks.\n        hnet (optional): If given, ``params`` is assumed to correspond to the\n            weights \"theta\" (which does not include task embeddings) of the\n            hypernetwork. In this case, the diagonal Fisher entries belong to\n            weights of the hypernetwork. The Fisher will then be computed based\n            on the probability :math:`p(y \\mid x, \\text{task\\_id})`, where\n            ``task_id`` is just a constant input (actually the corresponding\n            task embedding) in addition to the training samples :math:`x`.\n        empirical_fisher: If ``True``, we compute the fisher based on training\n            targets. Note, this has to be ``True`` if ``regression`` is set,\n            otherwise the squared norm between main network output and\n            \"most likely\" output is always zero, as they are identical.\n        online: If ``True``, then we use online EWC, hence, there is only one\n            diagonal Fisher approximation and one target parameter value stored\n            at the time, rather than for all previous tasks.\n        gamma: The gamma parameter for online EWC, controlling the gradual decay\n            of previous tasks.\n        n_max (optional): If not ``-1``, this will be the maximum amount of\n            training samples considered for estimating the Fisher.\n        regression: Whether the task at hand is a classification or regression\n            task. If ``True``, a regression task is assumed. For simplicity, we\n            assume the following probabilistic model\n            :math:`p(y \\mid x) = \\mathcal{N}\\big(f(x), I\\big)` with :math:`I`\n            being the identity matrix. In this case, the only terms of the log\n            probability that influence the gradient are:\n            :math:`\\log p(y \\mid x) = \\lVert f(x) - y \\rVert^2`\n        time_series (bool): If ``True``, the output of the main network\n            ``mnet`` is expected to be a time series. In particular, we\n            assume that the output is a tensor of shape ``[S, N, F]``,\n            where ``S`` is the length of the time series, ``N`` is the batch\n            size and ``F`` is the size of each feature vector (e.g., in\n            classification, ``F`` would be the number of classes).\n\n            Let :math:`\\mathbf{y} = (\\mathbf{y}_1, \\dots \\mathbf{y}_S)` be the\n            output of the main network. We denote the parameters ``params`` by\n            :math:`\\theta` and the input by :math:`\\mathbf{x}` (which we do not\n            consider as random). We use the following decomposition of the\n            likelihood\n            \n            .. math::\n                \n                p(\\mathbf{y} \\mid \\theta; \\mathbf{x}) =\n                \\prod_{i=1}^S p(\\mathbf{y}_i \\mid \\mathbf{y}_1, \\dots,\n                \\mathbf{y}_{i-1}, \\theta; \\mathbf{x}_i)\n\n            **Classification:** If\n            :math:`f(\\mathbf{x}_i, \\mathbf{h}_{i-1}, \\theta)` denotes the output\n            of the main network ``mnet`` for timestep :math:`i` (assuming\n            :math:`\\mathbf{h}_{i-1}` is the most recent hidden state), we assume\n\n            .. math::\n\n                p(\\mathbf{y}_i \\mid \\mathbf{y}_1, \\dots, \\mathbf{y}_{i-1},\n                \\theta; \\mathbf{x}_i) \\equiv \\text{softmax} \\big(\n                f(\\mathbf{x}_i, \\mathbf{h}_{i-1}, \\theta) \\big)\n\n            Hence, we assume that we can write the negative log-likelihood (NLL)\n            as follows given a label :math:`t \\in [1, \\dots, F]^S`:\n\n            .. math::\n\n                \\text{NLL} &= - \\log p(Y = t \\mid \\theta; \\mathbf{x}) \\\\\n                &= \\sum_{i=1}^S - \\text{softmax} \\big(\n                f(\\mathbf{x}_i, \\mathbf{h}_{i-1}, \\theta)_{t_i} \\big) \\\\\n                &= \\sum_{i=1}^S \\text{cross\\_entropy} \\big(\n                f(\\mathbf{x}_i, \\mathbf{h}_{i-1}, \\theta), t_i \\big)\n\n            Thus, we simply sum the cross-entropy losses per time-step to\n            estimate the NLL, which we then backpropagate through in order to\n            compute the diagonal Fisher elements.\n        allowed_outputs (optional): A list of indices, indicating which output\n            neurons of the main network should be taken into account when\n            computing the log probability. If not specified, all output neurons\n            are considered.\n        custom_forward (optional): A function handle that can replace the\n            default procedure of forwarding samples through the given\n            network(s).\n\n            The default forward procedure if ``hnet`` is ``None`` is\n\n            .. code:: python\n\n                Y = mnet.forward(X, weights=params)\n\n            Otherwise, the default forward procedure is\n\n            .. code:: python\n\n                weights = hnet.forward(task_id, theta=params)\n                Y = mnet.forward(X, weights=weights)\n\n            The signature of this function should be as follows.\n                - ``hnet`` is ``None``: :code:`@fun(mnet, params, X)`\n                - ``hnet`` is not ``None``:\n                  :code:`@fun(mnet, hnet, task_id, params, X)`\n\n            where :code:`X` denotes the input batch to the main network (usually\n            consisting of a single sample).\n\n            Example:\n                Imagine a situation where the main network uses context-\n                dependent modulation (cmp.\n                :class:`utils.context_mod_layer.ContextModLayer`) and the\n                parameters of these context-mod layers are produced by the\n                hypernetwork ``hnet``, whereas the remaining weights of the\n                main network ``mnet`` are maintained internally and passed as\n                argument ``params`` to this method.\n\n                In particular, we look at a main network that is an instance\n                of class :class:`mnets.mlp.MLP`. The forward pass through this\n                combination of networks should be handled as follows in order\n                to compute the correct fisher matrix:\n\n                .. code:: python\n\n                    def custom_forward(mnet, hnet, task_id, params, X):\n                        mod_weights = hnet.forward(task_id)\n                        weights = {\n                            'mod_weights': mod_weights,\n                            'internal_weights': params\n                        }\n                        Y = mnet.forward(X, weights=weights)\n                        return Y\n        custom_nll (optional): A function handle that can replace the default \n            procedure of computing the negative-log-likelihood (NLL), which is\n            required to compute the Fisher.\n\n            The signature of this function should be as follows:\n                :code:`@fun(Y, T, data, allowed_outputs, empirical_fisher)`\n\n            where ``Y`` are the outputs of the main network. Note,\n            ``allowed_outputs`` have already been applied to ``Y``, if given.\n            ``T`` is the target provided by the dataset ``data``, transformed as\n            follows:\n\n            .. code:: python\n\n                T = data.output_to_torch_tensor(batch[1], device,\n                                                mode='inference')\n\n            The arguments ``data``, ``allowed_outputs`` and ``empirical_fisher``\n            are only passed for convinience (e.g., to apply simple sanity checks\n            using assertions).\n\n            The output of the function handle should be the NLL for the given\n            sample.\n    \"\"\"", "\n", "# Note, this function makes some assumptions about how to use either of", "\n", "# these networks. Before adding new main or hypernetwork classes to the", "\n", "# assertions, please ensure that this network uses the \"forward\" functions", "\n", "# correctly.", "\n", "# If your network does not provide the capability to pass its weights to the", "\n", "# forward method, it might be cleaner to implement a separate method,", "\n", "# similar to:", "\n", "#   https://git.io/fjcnL", "\n", "\n", "# FIXME The `mnet` should be a subclass of the interface", "\n", "# `MainNetInterface`. Though, to ensure downwards compatibility, we allow", "\n", "# the deprecated class `MainNetwork` as well for now. However, this class", "\n", "# doesn't allow us to check for compatibility (e.g., ensuring that network", "\n", "# output is linear).", "\n", "assert", "(", "isinstance", "(", "mnet", ",", "MainNetInterface", ")", "or", "isinstance", "(", "mnet", ",", "MainNetwork", ")", ")", "\n", "assert", "(", "hnet", "is", "None", "or", "isinstance", "(", "hnet", ",", "HyperNetwork", ")", ")", "\n", "if", "isinstance", "(", "mnet", ",", "MainNetInterface", ")", ":", "\n", "        ", "assert", "(", "mnet", ".", "has_linear_out", ")", "\n", "\n", "# FIXME The above assertions are not necessary with the new network", "\n", "# interfaces, that clearly specify how to use the `forward` methods and how", "\n", "# to check the output non-linearity. But someone should carefully check the", "\n", "# implementation of this method before adapting the assertions.", "\n", "# I just wanna point out that we may wanna provide downwards compatibility", "\n", "# as follows as long as not all network types are migrated to the new", "\n", "# interface.", "\n", "#if not hasattr(mnet, 'has_linear_out'):", "\n", "#    pass # TODO new interface not yet available for network type", "\n", "#else:", "\n", "#    # Knowing the type of output non-linearity gives us a clear way of", "\n", "#    # computing the loss for classification tasks.", "\n", "#    assert(mnet.has_linear_out)", "\n", "\n", "", "assert", "(", "hnet", "is", "None", "or", "task_id", "is", "not", "None", ")", "\n", "assert", "(", "regression", "is", "False", "or", "empirical_fisher", ")", "\n", "assert", "(", "not", "online", "or", "(", "gamma", ">=", "0.", "and", "gamma", "<=", "1.", ")", ")", "\n", "assert", "(", "n_max", "is", "-", "1", "or", "n_max", ">", "0", ")", "\n", "\n", "if", "time_series", "and", "regression", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'Computing the Fisher for a recurrent '", "+", "\n", "'regression task is not yet implemented.'", ")", "\n", "\n", "", "n_samples", "=", "data", ".", "num_train_samples", "\n", "if", "n_max", "!=", "-", "1", ":", "\n", "        ", "n_samples", "=", "min", "(", "n_samples", ",", "n_max", ")", "\n", "\n", "", "mnet_mode", "=", "mnet", ".", "training", "\n", "mnet", ".", "eval", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "hnet_mode", "=", "hnet", ".", "training", "\n", "hnet", ".", "eval", "(", ")", "\n", "\n", "", "fisher", "=", "[", "]", "\n", "for", "p", "in", "params", ":", "\n", "        ", "fisher", ".", "append", "(", "torch", ".", "zeros_like", "(", "p", ")", ")", "\n", "\n", "assert", "(", "p", ".", "requires_grad", ")", "# Otherwise, we can't compute the Fisher.", "\n", "\n", "# Ensure, that we go through all training samples (note, that training", "\n", "# samples are always randomly shuffled when using \"next_train_batch\", but", "\n", "# we always go though the complete batch before reshuffling the samples.)", "\n", "# If n_max was specified, we always go through a different random subsample", "\n", "# of the training set.", "\n", "", "data", ".", "reset_batch_generator", "(", "train", "=", "True", ",", "test", "=", "False", ",", "val", "=", "False", ")", "\n", "\n", "# Since the PyTorch grad function accumulates gradients, we have to go", "\n", "# through single training samples.", "\n", "for", "s", "in", "range", "(", "n_samples", ")", ":", "\n", "        ", "batch", "=", "data", ".", "next_train_batch", "(", "1", ")", "\n", "X", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ",", "mode", "=", "'inference'", ")", "\n", "T", "=", "data", ".", "output_to_torch_tensor", "(", "batch", "[", "1", "]", ",", "device", ",", "mode", "=", "'inference'", ")", "\n", "\n", "if", "hnet", "is", "None", ":", "\n", "            ", "if", "custom_forward", "is", "None", ":", "\n", "                ", "Y", "=", "mnet", ".", "forward", "(", "X", ",", "weights", "=", "params", ")", "\n", "", "else", ":", "\n", "                ", "Y", "=", "custom_forward", "(", "mnet", ",", "params", ",", "X", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "custom_forward", "is", "None", ":", "\n", "                ", "weights", "=", "hnet", ".", "forward", "(", "task_id", ",", "theta", "=", "params", ")", "\n", "Y", "=", "mnet", ".", "forward", "(", "X", ",", "weights", "=", "weights", ")", "\n", "", "else", ":", "\n", "                ", "Y", "=", "custom_forward", "(", "mnet", ",", "hnet", ",", "task_id", ",", "params", ",", "X", ")", "\n", "\n", "", "", "if", "not", "time_series", ":", "\n", "            ", "assert", "(", "len", "(", "Y", ".", "shape", ")", "==", "2", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "len", "(", "Y", ".", "shape", ")", "==", "3", ")", "\n", "\n", "", "if", "allowed_outputs", "is", "not", "None", ":", "\n", "            ", "if", "not", "time_series", ":", "\n", "                ", "Y", "=", "Y", "[", ":", ",", "allowed_outputs", "]", "\n", "", "else", ":", "\n", "                ", "Y", "=", "Y", "[", ":", ",", ":", ",", "allowed_outputs", "]", "\n", "\n", "### Compute negative log-likelihood.", "\n", "", "", "if", "custom_nll", "is", "not", "None", ":", "\n", "            ", "nll", "=", "custom_nll", "(", "Y", ",", "T", ",", "data", ",", "allowed_outputs", ",", "empirical_fisher", ")", "\n", "\n", "", "elif", "regression", ":", "\n", "# Note, if regression, we don't have to modify the targets.", "\n", "# Thus, through \"allowed_outputs\" Y has been brought into the same", "\n", "# shape as T.", "\n", "\n", "# The term that doesn't vanish in the gradient of the log", "\n", "# probability is the squared L2 norm between Y and T.", "\n", "            ", "nll", "=", "0.5", "*", "(", "Y", "-", "T", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", "\n", "\n", "", "else", ":", "\n", "# Note, we assume the output of the main network is linear, such", "\n", "# that we can compute the log probabilities by applying the log-", "\n", "# softmax to these outputs.", "\n", "\n", "            ", "assert", "(", "data", ".", "classification", "and", "len", "(", "data", ".", "out_shape", ")", "==", "1", ")", "\n", "if", "allowed_outputs", "is", "not", "None", ":", "\n", "                ", "assert", "(", "len", "(", "allowed_outputs", ")", "==", "data", ".", "num_classes", ")", "\n", "assert", "(", "Y", ".", "shape", "[", "2", "if", "time_series", "else", "1", "]", "==", "data", ".", "num_classes", ")", "\n", "\n", "# Targets might be labels or one-hot encodings.", "\n", "", "if", "data", ".", "is_one_hot", ":", "\n", "                ", "assert", "(", "data", ".", "out_shape", "[", "0", "]", "==", "data", ".", "num_classes", ")", "\n", "if", "time_series", ":", "\n", "                    ", "assert", "(", "len", "(", "T", ".", "shape", ")", "==", "3", "and", "T", ".", "shape", "[", "2", "]", "==", "data", ".", "num_classes", ")", "\n", "T", "=", "torch", ".", "argmax", "(", "T", ",", "2", ")", "\n", "", "else", ":", "\n", "# Note, this function processes always one sample at a time", "\n", "# (batchsize=1), so `T` contains a single number.", "\n", "                    ", "T", "=", "torch", ".", "argmax", "(", "T", ")", "\n", "\n", "# Important, distinguish between empiricial and normal fisher!", "\n", "", "", "if", "empirical_fisher", ":", "\n", "                ", "if", "not", "time_series", ":", "\n", "# For classification, only the loss associated with the", "\n", "# target unit is taken into consideration.", "\n", "                    ", "nll", "=", "F", ".", "nll_loss", "(", "F", ".", "log_softmax", "(", "Y", ",", "dim", "=", "1", ")", ",", "\n", "torch", ".", "tensor", "(", "[", "T", "]", ")", ".", "to", "(", "device", ")", ")", "\n", "", "else", ":", "\n", "                    ", "ll", "=", "F", ".", "log_softmax", "(", "Y", ",", "dim", "=", "2", ")", "# log likelihood for all labels", "\n", "# We need to swap dimenstions from [S, N, F] to [S, F, N].", "\n", "# See documentation of method `nll_loss`.", "\n", "ll", "=", "ll", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "\n", "nll", "=", "F", ".", "nll_loss", "(", "ll", ",", "T", ",", "reduction", "=", "'none'", ")", "\n", "# Mean across batch dimension, but sum across time-series", "\n", "# dimension.", "\n", "assert", "(", "len", "(", "nll", ".", "shape", ")", "==", "2", ")", "\n", "nll", "=", "nll", ".", "mean", "(", "dim", "=", "1", ")", ".", "sum", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Only empirical Fisher is '", "+", "\n", "'implemented so far!'", ")", "\n", "\n", "### Compute gradient of negative log likelihood to estimate Fisher", "\n", "", "", "mnet", ".", "zero_grad", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "            ", "hnet", ".", "zero_grad", "(", ")", "\n", "", "torch", ".", "autograd", ".", "backward", "(", "nll", ",", "retain_graph", "=", "False", ",", "create_graph", "=", "False", ")", "\n", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "params", ")", ":", "\n", "            ", "fisher", "[", "i", "]", "+=", "torch", ".", "pow", "(", "p", ".", "grad", ".", "detach", "(", ")", ",", "2", ")", "\n", "\n", "# This version would not require use to call zero_grad and hence, we", "\n", "# wouldn't fiddle with internal variables, but it would require us to", "\n", "# loop over tensors and retain the graph in between.", "\n", "#for p in params:", "\n", "#    g = torch.autograd.grad(nll, p, grad_outputs=None,", "\n", "#                retain_graph=True, create_graph=False,", "\n", "#                only_inputs=True)[0]", "\n", "#    fisher[i] += torch.pow(g.detach(), 2)", "\n", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "params", ")", ")", ":", "\n", "        ", "fisher", "[", "i", "]", "/=", "n_samples", "\n", "\n", "### Register buffers to store current task weights as well as the Fisher.", "\n", "", "net", "=", "mnet", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "net", "=", "hnet", "\n", "", "for", "i", ",", "p", "in", "enumerate", "(", "params", ")", ":", "\n", "        ", "buff_w_name", ",", "buff_f_name", "=", "_ewc_buffer_names", "(", "task_id", ",", "i", ",", "online", ")", "\n", "\n", "# We use registered buffers rather than class members to ensure that", "\n", "# these variables appear in the state_dict and are thus written into", "\n", "# checkpoints.", "\n", "net", ".", "register_buffer", "(", "buff_w_name", ",", "p", ".", "detach", "(", ")", ".", "clone", "(", ")", ")", "\n", "\n", "# In the \"online\" case, the old fisher estimate buffer will be", "\n", "# overwritten.", "\n", "if", "online", "and", "task_id", ">", "0", ":", "\n", "            ", "prev_fisher_est", "=", "getattr", "(", "net", ",", "buff_f_name", ")", "\n", "\n", "# Decay of previous fisher.", "\n", "fisher", "[", "i", "]", "+=", "gamma", "*", "prev_fisher_est", "\n", "\n", "", "net", ".", "register_buffer", "(", "buff_f_name", ",", "fisher", "[", "i", "]", ".", "detach", "(", ")", ".", "clone", "(", ")", ")", "\n", "\n", "", "mnet", ".", "train", "(", "mode", "=", "mnet_mode", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "hnet", ".", "train", "(", "mode", "=", "hnet_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer.ewc_regularizer": [[424, 476], ["range", "enumerate", "ewc_regularizer._ewc_buffer_names", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer._ewc_buffer_names"], ["", "", "def", "ewc_regularizer", "(", "task_id", ",", "params", ",", "mnet", ",", "hnet", "=", "None", ",", "\n", "online", "=", "False", ",", "gamma", "=", "1.", ")", ":", "\n", "    ", "\"\"\"Compute the EWC regularizer, that can be added to the remaining loss.\n    Note, the hyperparameter, that trades-off the regularization strength is\n    not yet multiplied by the loss.\n\n    This loss assumes an appropriate use of the method \"compute_fisher\". Note,\n    for the current task \"compute_fisher\" has to be called after calling this\n    method.\n\n    If `online` is False, this method implements the loss proposed in eq. (3) in\n    [EWC2017]_, except for the missing hyperparameter `lambda`.\n    \n    The online EWC implementation follows eq. (8) from [OnEWC2018]_ (note, that\n    lambda does not appear in this equation, but it was used in their\n    experiments).\n\n    .. [EWC2017] https://arxiv.org/abs/1612.00796\n    .. [OnEWC2018] https://arxiv.org/abs/1805.06370\n\n    Args:\n        (....): See docstring of method :func:`compute_fisher`.\n\n    Returns:\n        EWC regularizer.\n    \"\"\"", "\n", "assert", "(", "task_id", ">", "0", ")", "\n", "\n", "net", "=", "mnet", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "net", "=", "hnet", "\n", "\n", "", "ewc_reg", "=", "0", "\n", "\n", "num_prev_tasks", "=", "1", "if", "online", "else", "task_id", "\n", "for", "t", "in", "range", "(", "num_prev_tasks", ")", ":", "\n", "        ", "for", "i", ",", "p", "in", "enumerate", "(", "params", ")", ":", "\n", "            ", "buff_w_name", ",", "buff_f_name", "=", "_ewc_buffer_names", "(", "t", ",", "i", ",", "online", ")", "\n", "\n", "prev_weights", "=", "getattr", "(", "net", ",", "buff_w_name", ")", "\n", "fisher_est", "=", "getattr", "(", "net", ",", "buff_f_name", ")", "\n", "# Note, since we haven't called \"compute_fisher\" yet, the forgetting", "\n", "# scalar has been multiplied yet.", "\n", "if", "online", ":", "\n", "                ", "fisher_est", "*=", "gamma", "\n", "\n", "", "ewc_reg", "+=", "(", "fisher_est", "*", "(", "p", "-", "prev_weights", ")", ".", "pow", "(", "2", ")", ")", ".", "sum", "(", ")", "\n", "\n", "# Note, the loss proposed in the original paper is not normalized by the", "\n", "# number of tasks", "\n", "#return ewc_reg / num_prev_tasks / 2.", "\n", "", "", "return", "ewc_reg", "/", "2.", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer._ewc_buffer_names": [[477, 497], ["None"], "function", ["None"], ["", "def", "_ewc_buffer_names", "(", "task_id", ",", "param_id", ",", "online", ")", ":", "\n", "    ", "\"\"\"The names of the buffers used to store EWC variables.\n\n    Args:\n        task_id: ID of task (only used of `online` is False).\n        param_id: Identifier of parameter tensor.\n        online: Whether the online EWC algorithm is used.\n\n    Returns:\n        (tuple): Tuple containing:\n\n        - **weight_buffer_name**\n        - **fisher_estimate_buffer_name**\n    \"\"\"", "\n", "task_ident", "=", "''", "if", "online", "else", "'_task_%d'", "%", "task_id", "\n", "\n", "weight_name", "=", "'ewc_prev{}_weights_{}'", ".", "format", "(", "task_ident", ",", "param_id", ")", "\n", "fisher_name", "=", "'ewc_fisher_estimate{}_weights_{}'", ".", "format", "(", "task_ident", ",", "\n", "param_id", ")", "\n", "return", "weight_name", ",", "fisher_name", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer.context_mod_forward": [[498, 543], ["hnet.forward", "mnet.forward", "mnet.forward"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "context_mod_forward", "(", "mod_weights", "=", "None", ")", ":", "\n", "    ", "\"\"\"Create a custom forward function for function :func:`compute_fisher`.\n\n    See argument ``custom_forward`` of function :func:`compute_fisher` for more\n    details.\n\n    This is a helper method to quickly retrieve a function handle that manages\n    the forward pass for a context-modulated main network.\n\n    We assume that the interface of the main network is similar to the one of\n    :meth:`mnets.mlp.MLP.forward`.\n\n    Args:\n        mod_weights (optional): If provided, it is assumed that\n            :func:`compute_fisher` is called with ``hnet`` set to ``None``.\n            Hence, the returned function handle will have the given\n            context-modulation pattern hard-coded.\n            If left unspecified, it is assumed that a ``hnet`` is passed to\n            :func:`compute_fisher` and that this ``hnet`` computes only the\n            parameters of all context-mod layers.\n\n    Returns:\n        A function handle.\n    \"\"\"", "\n", "def", "hnet_forward", "(", "mnet", ",", "hnet", ",", "task_id", ",", "params", ",", "X", ")", ":", "\n", "        ", "mod_weights", "=", "hnet", ".", "forward", "(", "task_id", ")", "\n", "weights", "=", "{", "\n", "'mod_weights'", ":", "mod_weights", ",", "\n", "'internal_weights'", ":", "params", "\n", "}", "\n", "Y", "=", "mnet", ".", "forward", "(", "X", ",", "weights", "=", "weights", ")", "\n", "return", "Y", "\n", "\n", "", "def", "mnet_only_forward", "(", "mnet", ",", "params", ",", "X", ")", ":", "\n", "        ", "weights", "=", "{", "\n", "'mod_weights'", ":", "mod_weights", ",", "\n", "'internal_weights'", ":", "params", "\n", "}", "\n", "Y", "=", "mnet", ".", "forward", "(", "X", ",", "weights", "=", "weights", ")", "\n", "return", "Y", "\n", "\n", "", "if", "mod_weights", "is", "None", ":", "\n", "        ", "return", "hnet_forward", "\n", "", "else", ":", "\n", "        ", "return", "mnet_only_forward", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer.cognet_mse_nll": [[544, 652], ["numpy.all", "torch.argmax", "numpy.equal", "torch.ones", "torch.ones_like.view", "torch.ones_like", "labels.expand.view", "labels.expand.expand", "list", "list", "torch.ones_like.size"], "function", ["None"], ["", "", "def", "cognet_mse_nll", "(", "no_fix_unit_amplification", "=", "False", ")", ":", "\n", "    ", "r\"\"\"Create a custom NLL function for function\n    :func:`utils.ewc_regularizer.compute_fisher`.\n\n    Here, we consider a set of cognitive tasks as suggested by\n\n        https://www.nature.com/articles/s41593-018-0310-2\n\n    We assume the network loss is computed as described in section *Training\n    procedure* on pg. 12 of the paper lined above.\n\n    Thus the network has an output shape of ``[S, N, F]``, where ``S`` is the\n    length of a time sequence, ``N`` is the batch size (we can assume ``N`` is 1\n    in function :func:`utils.ewc_regularizer.compute_fisher`) and ``F`` is the\n    number of output classes. The network is trained using masked MSE loss.\n\n    Note, there are 9 output classes (8 *output ring units* and 1 *fixation\n    output unit*), where the last class (the *fixation output unit*) might be\n    treated differently.\n\n    The first 10 timesteps (100 ms) are ignored. The fixation period is defined\n    by the timesteps that are associated with label 8.\n\n    During the fixation period, the MSE between ring units and targets\n    (which will be zero) will be weighted by 1, whereas the MSE between the\n    fixation unit and its target (which will be 1) is weighted by 2.\n\n    During the response period, the weighting will change to 5 for ring units\n    and 10 for fixation units.\n\n    Similar to function :func:`utils.ewc_regularizer.compute_fisher` (cmp.\n    argument ``time_series``), we adopt the following decomposition of the joint\n\n    .. math::\n\n        p(\\mathbf{y} \\mid \\theta; \\mathbf{x}) =\n        \\prod_{i=1}^S p(\\mathbf{y}_i \\mid \\mathbf{y}_1, \\dots,\n        \\mathbf{y}_{i-1}, \\theta; \\mathbf{x}_i)\n\n    Since the loss is a masked MSE loss, we assume the predictive distribution\n    per time step is a Gaussian\n    :math:`\\mathcal{N}(\\mathbf{\\mu}, I \\mathbf{\\sigma}^2)` with diagonal\n    covariance matrix.\n\n    Hence, we can write the NLL for the :math:`i`-th output as follows, assuming\n    :math:`\\textbf{t}_i` is the corresponding 1-hot target:\n\n    .. math::\n\n        \\text{NLL}_i &= - \\log p(\\mathbf{y}_i = \\mathbf{t}_i\n        \\mid \\mathbf{y}_1, \\dots, \\mathbf{y}_{i-1}, \\theta; \\mathbf{x}_i)\\\\\n        &= \\text{const.} + \\frac{1}{2} \\sum_{j=0}^8 \\frac{1}{\\sigma_{i,j}^2}\n        \\big(f(\\mathbf{x}_i, \\mathbf{h}_{i-1}, \\theta)_j - t_{i,j} \\big)^2\\\\\n        &= \\text{const.} + \\sum_{j=0}^8 \\tau_{i,j}\n        \\big(f(\\mathbf{x}_i, \\mathbf{h}_{i-1}, \\theta)_j - t_{i,j} \\big)^2\n\n    where we defined :math:`\\tau_{i,j} \\equiv \\frac{1}{2 \\sigma_{i,j}^2}`.\n    Based on the time step :math:`i` and the output unit :math:`j`, we can set\n    the variance such that :math:`\\tau_{i,j}` corresponds to the masking value\n    as defined above.\n\n    The complete NLL over all timesteps is then simply:\n\n    .. math::\n\n        \\text{NLL} &= \\sum_{i=1}^S \\text{NLL}_i \\\\\n        &= \\text{const.} + \\sum_{i=1}^S \\sum_{j=0}^8 \\tau_{i,j}\n        \\big(f(\\mathbf{x}_i, \\mathbf{h}_{i-1}, \\theta)_j - t_{i,j} \\big)^2\n\n    Note, a mask value of zero (:math:`\\tau_{i,j} = 0`) corresponds to infinite\n    variance.\n\n    Args:\n        no_fix_unit_amplification (bool): If ``True``, then the masking for\n            the fixation unit is not amplified (by a factor of 2) as described\n            above. Instead, fixation and ring units are treated equally.\n    \"\"\"", "\n", "def", "custom_nll", "(", "Y", ",", "T", ",", "data", ",", "allowed_outputs", ",", "empirical_fisher", ")", ":", "\n", "# We expect targets to be given as 1-hot encodings.", "\n", "        ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "Y", ".", "shape", ")", ",", "list", "(", "T", ".", "shape", ")", ")", ")", ")", "\n", "\n", "# Fixation period is defined by timesteps having label 8.", "\n", "labels", "=", "torch", ".", "argmax", "(", "T", ",", "2", ")", "\n", "\n", "if", "no_fix_unit_amplification", ":", "\n", "            ", "mask", "=", "torch", ".", "ones", "(", "T", ".", "shape", "[", "0", "]", ",", "T", ".", "shape", "[", "1", "]", ")", "\n", "mask", "[", "labels", "!=", "8", "]", "=", "5", "\n", "mask", "[", "0", ":", "10", ",", ":", "]", "=", "0", "\n", "\n", "# Make sure that `mask` are broadcastable wrt `Y` and `T`.", "\n", "mask", "=", "mask", ".", "view", "(", "T", ".", "shape", "[", "0", "]", ",", "T", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "\n", "", "else", ":", "\n", "            ", "mask", "=", "torch", ".", "ones_like", "(", "T", ")", "\n", "# Make sure that `labels` can be used to index `mask`.", "\n", "labels", "=", "labels", ".", "view", "(", "T", ".", "shape", "[", "0", "]", ",", "T", ".", "shape", "[", "1", "]", ",", "1", ")", "\n", "labels", "=", "labels", ".", "expand", "(", "mask", ".", "size", "(", ")", ")", "\n", "\n", "mask", "[", "labels", "!=", "8", "]", "=", "5", "\n", "mask", "[", "0", ":", "10", ",", ":", ",", ":", "]", "=", "0", "\n", "\n", "mask", "[", ":", ",", ":", ",", "8", "]", "=", "2", "*", "mask", "[", ":", ",", ":", ",", "8", "]", "\n", "\n", "", "nll", "=", "(", "mask", "*", "(", "Y", "-", "T", ")", "**", "2", ")", ".", "sum", "(", ")", "\n", "\n", "return", "nll", "\n", "\n", "", "return", "custom_nll", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params": [[30, 50], ["torch.nn.init.kaiming_uniform_", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init.uniform_", "math.sqrt", "math.sqrt"], "function", ["None"], ["def", "init_params", "(", "weights", ",", "bias", "=", "None", ")", ":", "\n", "    ", "\"\"\"Initialize the weights and biases of a linear or (transpose) conv layer.\n\n    Note, the implementation is based on the method \"reset_parameters()\",\n    that defines the original PyTorch initialization for a linear or\n    convolutional layer, resp. The implementations can be found here:\n\n        https://git.io/fhnxV\n\n        https://git.io/fhnx2\n\n    Args:\n        weights: The weight tensor to be initialized.\n        bias (optional): The bias tensor to be initialized.\n    \"\"\"", "\n", "nn", ".", "init", ".", "kaiming_uniform_", "(", "weights", ",", "a", "=", "math", ".", "sqrt", "(", "5", ")", ")", "\n", "if", "bias", "is", "not", "None", ":", "\n", "        ", "fan_in", ",", "_", "=", "nn", ".", "init", ".", "_calculate_fan_in_and_fan_out", "(", "weights", ")", "\n", "bound", "=", "1", "/", "math", ".", "sqrt", "(", "fan_in", ")", "\n", "nn", ".", "init", ".", "uniform_", "(", "bias", ",", "-", "bound", ",", "bound", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.get_optimizer": [[51, 92], ["torch.optim.Adam", "torch.optim.RMSprop", "torch.optim.Adadelta", "torch.optim.Adagrad", "torch.optim.SGD"], "function", ["None"], ["", "", "def", "get_optimizer", "(", "params", ",", "lr", ",", "momentum", "=", "0", ",", "weight_decay", "=", "0", ",", "use_adam", "=", "False", ",", "\n", "adam_beta1", "=", "0.9", ",", "use_rmsprop", "=", "False", ",", "use_adadelta", "=", "False", ",", "\n", "use_adagrad", "=", "False", ")", ":", "\n", "    ", "\"\"\"Create an optimizer instance for the given set of parameters. Default\n    optimizer is :class:`torch.optim.SGD`.\n\n    Args:\n        params: The parameters passed to the optimizer.\n        lr: Learning rate.\n        momentum (optional): Momentum (only applicable to\n            :class:`torch.optim.SGD` and :class:`torch.optim.RMSprop`.\n        weight_decay (optional): L2 penalty.\n        use_adam: Use :class:`torch.optim.Adam` optimizer.\n        adam_beta1: First parameter in the `betas` tuple that is passed to the\n            optimizer :class:`torch.optim.Adam`:\n            :code:`betas=(adam_beta1, 0.999)`.\n        use_rmsprop: Use :class:`torch.optim.RMSprop` optimizer.\n        use_adadelta: Use :class:`torch.optim.Adadelta` optimizer.\n        use_adagrad: Use :class:`torch.optim.Adagrad` optimizer.\n\n    Returns:\n        Optimizer instance.\n    \"\"\"", "\n", "if", "use_adam", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "params", ",", "lr", "=", "lr", ",", "betas", "=", "[", "adam_beta1", ",", "0.999", "]", ",", "\n", "weight_decay", "=", "weight_decay", ")", "\n", "", "elif", "use_rmsprop", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "RMSprop", "(", "params", ",", "lr", "=", "lr", ",", "\n", "weight_decay", "=", "weight_decay", ",", "\n", "momentum", "=", "momentum", ")", "\n", "", "elif", "use_adadelta", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adadelta", "(", "params", ",", "lr", "=", "lr", ",", "\n", "weight_decay", "=", "weight_decay", ")", "\n", "", "elif", "use_adagrad", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adagrad", "(", "params", ",", "lr", "=", "lr", ",", "\n", "weight_decay", "=", "weight_decay", ")", "\n", "", "else", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "params", ",", "lr", "=", "lr", ",", "momentum", "=", "momentum", ",", "\n", "weight_decay", "=", "weight_decay", ")", "\n", "\n", "", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.get_current_targets": [[32, 73], ["hnet.eval", "hnet.train", "torch.no_grad", "range", "hnet.forward", "ret.append", "d.detach().clone", "d.detach"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["def", "get_current_targets", "(", "task_id", ",", "hnet", ")", ":", "\n", "    ", "r\"\"\"For all :math:`j < \\text{task\\_id}`, compute the output of the\n    hypernetwork. This output will be detached from the graph and cloned before\n    being added to the return list of this function.\n\n    Note, if these targets don't change during training, it would be more memory\n    efficient to store the weights :math:`\\theta^*` of the hypernetwork (which\n    is a fixed amount of memory compared to the variable number of tasks).\n    Though, it is more computationally expensive to recompute\n    :math:`h(c_j, \\theta^*)` for all :math:`j < \\text{task\\_id}` everytime the\n    target is needed.\n\n    Note, this function sets the hypernet temporarily in eval mode. No gradients\n    are computed.\n\n    Args:\n        task_id: The ID of the current task.\n        hnet: An instance of the hypernetwork before learning a new task\n            (i.e., the hypernetwork has the weights :math:`\\theta^*` necessary\n            to compute the targets).\n\n    Returns:\n        An empty list, if `task_id` is 0. Otherwise, a list of `task_id`-1\n        targets. These targets can be passed to the method\n        :func:`calc_fix_target_reg` while training on the new task.\n    \"\"\"", "\n", "# We temporarily switch to eval mode for target computation (e.g., to get", "\n", "# rid of training stochasticities such as dropout).", "\n", "hnet_mode", "=", "hnet", ".", "training", "\n", "hnet", ".", "eval", "(", ")", "\n", "\n", "ret", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "task_id", ")", ":", "\n", "            ", "W", "=", "hnet", ".", "forward", "(", "task_id", "=", "j", ")", "\n", "ret", ".", "append", "(", "[", "d", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "d", "in", "W", "]", ")", "\n", "\n", "", "", "hnet", ".", "train", "(", "mode", "=", "hnet_mode", ")", "\n", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_jac_reguarizer": [[74, 143], ["range", "torch.cat", "enumerate", "enumerate", "torch.pow", "w.view", "torch.mul().sum", "hnet.forward", "torch.autograd.grad", "torch.mul"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "calc_jac_reguarizer", "(", "hnet", ",", "task_id", ",", "dTheta", ",", "device", ")", ":", "\n", "    ", "r\"\"\"Compute the CL regularzier, which is a sum over all previous task\n    ID's, that enforces that the norm of the matrix product of hypernet\n    Jacobian and dTheta is small.\n\n    I.e., for all :math:`j < \\text{task\\_id}` minimize the following norm:\n\n    .. math::\n        \\lVert J_h(c_j, \\theta) \\Delta \\theta \\rVert^2\n\n    where :math:`\\theta` (and :math:`\\Delta\\theta`) is assumed to be vectorized.\n\n    This regularizer origins in a first-order Taylor approximation of the\n    regularizer:\n\n    .. math::\n        \\lVert h(c_j, \\theta) - h(c_j, \\theta + \\Delta\\theta) \\rVert^2\n\n    Args:\n        (....): See docstring of method :func:`calc_fix_target_reg`.\n        device: Current PyTorch device.\n\n    Returns:\n        The value of the regularizer.\n    \"\"\"", "\n", "assert", "(", "task_id", ">", "0", ")", "\n", "assert", "(", "hnet", ".", "has_theta", ")", "# We need parameters to be regularized.", "\n", "\n", "reg", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "task_id", ")", ":", "# For all previous tasks.", "\n", "        ", "W", "=", "torch", ".", "cat", "(", "[", "w", ".", "view", "(", "-", "1", ")", "for", "w", "in", "hnet", ".", "forward", "(", "task_id", "=", "i", ")", "]", ")", "\n", "\n", "# Problem, autograd.grad() accumulates all gradients with", "\n", "# respect to each w in \"W\". Hence, we don't get a Jacobian but a", "\n", "# tensor of the same size as \"t\", where the partials of all W's", "\n", "# with respect to this \"t\" are summed.", "\n", "#J = torch.autograd.grad(W, t,", "\n", "#    grad_outputs=torch.ones(W.size()).to(device),", "\n", "#    retain_graph=True, create_graph=True, only_inputs=True)[0]", "\n", "#", "\n", "# Hence, to get the actual jacobian, we have to iterate over all", "\n", "# individual outputs of the hypernet.", "\n", "for", "wind", ",", "w", "in", "enumerate", "(", "W", ")", ":", "\n", "            ", "tmp", "=", "0", "\n", "# FIXME `torch.autograd.grad` can also take a list of tensors as", "\n", "# input (and returns a corresponding tuple). Thus, this second for", "\n", "# loop is unneccessary. It should be much more efficient to get rid", "\n", "# of it (`grad` may also reuse gradients in this case).", "\n", "for", "tind", ",", "t", "in", "enumerate", "(", "hnet", ".", "theta", ")", ":", "\n", "                ", "partial", "=", "torch", ".", "autograd", ".", "grad", "(", "w", ",", "t", ",", "grad_outputs", "=", "None", ",", "\n", "retain_graph", "=", "True", ",", "create_graph", "=", "True", ",", "\n", "only_inputs", "=", "True", ")", "[", "0", "]", "\n", "\n", "# Intuitively, \"partial\" represents part of a row in the", "\n", "# Jacobian (if dTheta would have been linearized to a", "\n", "# vector). To compute the matrix vector product (Jacobian", "\n", "# times dTheta), we have to sum over the element-wise", "\n", "# product of rows from the Jacobian with dTheta.", "\n", "tmp", "+=", "torch", ".", "mul", "(", "partial", ",", "dTheta", "[", "tind", "]", ")", ".", "sum", "(", ")", "\n", "\n", "# Since we are interested in computing the squared L2 norm of", "\n", "# the matrix vector product: Jacobian times dTheta,", "\n", "# we have to simply sum the sqaured dot products between all", "\n", "# rows of the Jacobian with dTheta.", "\n", "", "reg", "+=", "torch", ".", "pow", "(", "tmp", ",", "2", ")", "\n", "\n", "# Normalize by the number of tasks.", "\n", "", "", "return", "reg", "/", "task_id", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_value_preserving_reg": [[144, 170], ["range", "torch.cat", "torch.cat", "w.view", "w.view", "hnet.forward", "hnet.forward"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "calc_value_preserving_reg", "(", "hnet", ",", "task_id", ",", "dTheta", ")", ":", "\n", "    ", "r\"\"\"This regularizer simply restricts a change in output-mapping for\n    previous task embeddings. I.e., for all j < task_id minimize:\n\n    .. math::\n        \\lVert h(c_j, \\theta) - h(c_j, \\theta + \\Delta\\theta) \\rVert^2\n\n    Args:\n        (....): See docstring of method :func:`calc_fix_target_reg`.\n\n    Returns:\n        The value of the regularizer.\n    \"\"\"", "\n", "assert", "(", "task_id", ">", "0", ")", "\n", "assert", "(", "hnet", ".", "has_theta", ")", "# We need parameters to be regularized.", "\n", "\n", "reg", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "task_id", ")", ":", "# For all previous tasks.", "\n", "        ", "W_prev", "=", "torch", ".", "cat", "(", "[", "w", ".", "view", "(", "-", "1", ")", "for", "w", "in", "hnet", ".", "forward", "(", "task_id", "=", "i", ")", "]", ")", "\n", "W_new", "=", "torch", ".", "cat", "(", "[", "w", ".", "view", "(", "-", "1", ")", "for", "w", "in", "hnet", ".", "forward", "(", "task_id", "=", "i", ",", "\n", "dTheta", "=", "dTheta", ")", "]", ")", "\n", "\n", "reg", "+=", "(", "W_prev", "-", "W_new", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", "\n", "\n", "", "return", "reg", "/", "task_id", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_fix_target_reg": [[171, 313], ["isinstance", "list", "range", "len", "len", "len", "len", "len", "numpy.random.choice().tolist", "hnet.forward", "hnet.forward", "hnet.eval", "hnet.train", "hnet_regularizer.flatten_and_remove_out_heads", "hnet_regularizer.flatten_and_remove_out_heads", "torch.cat", "torch.cat", "hnet_regularizer._assert_shape_equality", "torch.cat", "hnet.get_task_emb", "torch.no_grad", "hnet.forward", "d.detach().clone", "numpy.random.choice", "w.view", "w.view", "w.view", "d.detach"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.flatten_and_remove_out_heads", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.flatten_and_remove_out_heads", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer._assert_shape_equality", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "calc_fix_target_reg", "(", "hnet", ",", "task_id", ",", "targets", "=", "None", ",", "dTheta", "=", "None", ",", "dTembs", "=", "None", ",", "\n", "mnet", "=", "None", ",", "inds_of_out_heads", "=", "None", ",", "\n", "fisher_estimates", "=", "None", ",", "prev_theta", "=", "None", ",", "\n", "prev_task_embs", "=", "None", ",", "batch_size", "=", "None", ",", "reg_scaling", "=", "None", ")", ":", "\n", "    ", "r\"\"\"This regularizer simply restricts the output-mapping for previous\n    task embeddings. I.e., for all :math:`j < \\text{task\\_id}` minimize:\n\n    .. math::\n        \\lVert \\text{target}_j - h(c_j, \\theta + \\Delta\\theta) \\rVert^2\n\n    where :math:`c_j` is the current task embedding for task :math:`j` (and we\n    assumed that `dTheta` was passed).\n\n    Args:\n        hnet: The hypernetwork whose output should be regularized. Has to\n            implement the interface CLHyperNetInterface.\n        task_id: The ID of the current task (the one that is used to\n            compute dTheta.\n        targets: A list of outputs of the hypernetwork. Each list entry must\n            have the output shape as returned by the forward method of this\n            class. Note, this method doesn't detach targets. If desired,\n            that should be done before calling this method.\n        dTheta (optional): The current direction of weight change for the\n            internal weights of the hypernetwork evaluated on the task-specific\n            loss, i.e., the weight change that would be applied to theta. This\n            regularizer aims to modify this direction, such that the hypernet\n            output for embeddings of previous tasks remains unaffected.\n            Note, this function does not detach dTheta. It is up to the\n            user to decide whether dTheta should be a constant vector or\n            might depend on parameters of the hypernet.\n        dTembs (optional): The current direction of weight change for the task\n            embeddings of all tasks been learned already.\n            See dTheta for details.\n        mnet: Instance of the main network. Has to be given if\n            `inds_of_out_heads` are specified.\n        inds_of_out_heads: (optional): List of lists of integers, denoting which\n            output neurons of the fully-connected output layer of the main\n            network are used for predictions of the corresponding previous task.\n            This will ensure that only weights of output neurons involved in\n            solving a task are regularized.\n            Note, this may only be used for main networks that have a fully-\n            connected output layer.\n        fisher_estimates (optional): A list of list of tensors, containing\n            estimates of the Fisher Information matrix for each weight\n            tensor in the main network and each task.\n            Note, that :code:`len(fisher_estimates) == task_id`.\n            The Fisher estimates are used as importance weights for single\n            weights when computing the regularizer.\n        prev_theta (optional): If given, `prev_task_embs` but not `targets`\n            has to be specified. `prev_theta` is expected to be the internal\n            weights theta prior to learning the current task. Hence, it can be\n            used to compute the targets on the fly (which is more memory\n            efficient (constant memory), but more computationally demanding).\n            The computed targets will be detached from the computational graph.\n            Independent of the current hypernet mode, the targets are computed\n            in \"eval\" mode.\n        prev_task_embs (optional): If given, `prev_theta` but not `targets`\n            has to be specified. \"prev_task_embs\" are the task embeddings \n            learned prior to learning the current task. It is sufficient to\n            only pass the task embeddings for tasks with ID smaller than the\n            current one (only those tasks that are regularized).\n            See docstring of \"prev_theta\" for more details.\n        batch_size (optional): If specified, only a random subset of previous\n            task mappings is regularized. If the given number is bigger than the\n            number of previous tasks, all previous tasks are regularized.\n        reg_scaling (optional): If specified, the regulariation terms for the \n            different tasks are scaled arcording to the entries of this list.\n    Returns:\n        The value of the regularizer.\n    \"\"\"", "\n", "assert", "(", "isinstance", "(", "hnet", ",", "CLHyperNetInterface", ")", ")", "\n", "assert", "(", "task_id", ">", "0", ")", "\n", "assert", "(", "hnet", ".", "has_theta", ")", "# We need parameters to be regularized.", "\n", "assert", "(", "targets", "is", "None", "or", "len", "(", "targets", ")", "==", "task_id", ")", "\n", "assert", "(", "inds_of_out_heads", "is", "None", "or", "mnet", "is", "not", "None", ")", "\n", "assert", "(", "inds_of_out_heads", "is", "None", "or", "len", "(", "inds_of_out_heads", ")", ">=", "task_id", ")", "\n", "assert", "(", "targets", "is", "None", "or", "(", "prev_theta", "is", "None", "and", "prev_task_embs", "is", "None", ")", ")", "\n", "assert", "(", "prev_theta", "is", "None", "or", "prev_task_embs", "is", "not", "None", ")", "\n", "assert", "(", "prev_task_embs", "is", "None", "or", "len", "(", "prev_task_embs", ")", ">=", "task_id", ")", "\n", "assert", "(", "dTembs", "is", "None", "or", "len", "(", "dTembs", ")", ">=", "task_id", ")", "\n", "assert", "(", "reg_scaling", "is", "None", "or", "len", "(", "reg_scaling", ")", ">=", "task_id", ")", "\n", "\n", "# Number of tasks to be regularized.", "\n", "num_regs", "=", "task_id", "\n", "ids_to_reg", "=", "list", "(", "range", "(", "num_regs", ")", ")", "\n", "if", "batch_size", "is", "not", "None", ":", "\n", "        ", "if", "num_regs", ">", "batch_size", ":", "\n", "            ", "ids_to_reg", "=", "np", ".", "random", ".", "choice", "(", "num_regs", ",", "size", "=", "batch_size", ",", "\n", "replace", "=", "False", ")", ".", "tolist", "(", ")", "\n", "num_regs", "=", "batch_size", "\n", "\n", "", "", "reg", "=", "0", "\n", "\n", "for", "i", "in", "ids_to_reg", ":", "\n", "        ", "if", "dTembs", "is", "None", ":", "\n", "            ", "weights_predicted", "=", "hnet", ".", "forward", "(", "task_id", "=", "i", ",", "dTheta", "=", "dTheta", ")", "\n", "", "else", ":", "\n", "            ", "temb", "=", "hnet", ".", "get_task_emb", "(", "i", ")", "+", "dTembs", "[", "i", "]", "\n", "weights_predicted", "=", "hnet", ".", "forward", "(", "dTheta", "=", "dTheta", ",", "task_emb", "=", "temb", ")", "\n", "\n", "", "if", "targets", "is", "not", "None", ":", "\n", "            ", "target", "=", "targets", "[", "i", "]", "\n", "", "else", ":", "\n", "# Compute targets in eval mode!", "\n", "            ", "hnet_mode", "=", "hnet", ".", "training", "\n", "hnet", ".", "eval", "(", ")", "\n", "\n", "# Compute target on the fly using previous hnet.", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "target", "=", "hnet", ".", "forward", "(", "theta", "=", "prev_theta", ",", "\n", "task_emb", "=", "prev_task_embs", "[", "i", "]", ")", "\n", "", "target", "=", "[", "d", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "d", "in", "target", "]", "\n", "\n", "hnet", ".", "train", "(", "mode", "=", "hnet_mode", ")", "\n", "\n", "", "if", "inds_of_out_heads", "is", "not", "None", ":", "\n", "# Regularize all weights of the main network except for the weights", "\n", "# belonging to output heads of the target network other than the", "\n", "# current one (defined by task id).", "\n", "            ", "W_target", "=", "flatten_and_remove_out_heads", "(", "mnet", ",", "target", ",", "\n", "inds_of_out_heads", "[", "i", "]", ")", "\n", "W_predicted", "=", "flatten_and_remove_out_heads", "(", "mnet", ",", "weights_predicted", ",", "\n", "inds_of_out_heads", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "# Regularize all weights of the main network.", "\n", "            ", "W_target", "=", "torch", ".", "cat", "(", "[", "w", ".", "view", "(", "-", "1", ")", "for", "w", "in", "target", "]", ")", "\n", "W_predicted", "=", "torch", ".", "cat", "(", "[", "w", ".", "view", "(", "-", "1", ")", "for", "w", "in", "weights_predicted", "]", ")", "\n", "\n", "", "if", "fisher_estimates", "is", "not", "None", ":", "\n", "            ", "_assert_shape_equality", "(", "weights_predicted", ",", "fisher_estimates", "[", "i", "]", ")", "\n", "FI", "=", "torch", ".", "cat", "(", "[", "w", ".", "view", "(", "-", "1", ")", "for", "w", "in", "fisher_estimates", "[", "i", "]", "]", ")", "\n", "\n", "reg_i", "=", "(", "FI", "*", "(", "W_target", "-", "W_predicted", ")", ".", "pow", "(", "2", ")", ")", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "            ", "reg_i", "=", "(", "W_target", "-", "W_predicted", ")", ".", "pow", "(", "2", ")", ".", "sum", "(", ")", "\n", "\n", "", "if", "reg_scaling", "is", "not", "None", ":", "\n", "            ", "reg", "+=", "reg_scaling", "[", "i", "]", "*", "reg_i", "\n", "", "else", ":", "\n", "            ", "reg", "+=", "reg_i", "\n", "\n", "", "", "return", "reg", "/", "num_regs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer._assert_shape_equality": [[314, 319], ["range", "len", "len", "len", "numpy.all", "numpy.equal", "list", "list"], "function", ["None"], ["", "def", "_assert_shape_equality", "(", "list1", ",", "list2", ")", ":", "\n", "    ", "\"\"\"Ensure that 2 lists of tensors have the same shape.\"\"\"", "\n", "assert", "(", "len", "(", "list1", ")", "==", "len", "(", "list2", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "list1", ")", ")", ":", "\n", "        ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "list1", "[", "i", "]", ".", "shape", ")", ",", "list", "(", "list2", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.flatten_and_remove_out_heads": [[320, 361], ["enumerate", "torch.cat", "hasattr", "len", "len", "len", "ret.append", "ret.append", "ret.append", "w[].view", "w.view"], "function", ["None"], ["", "", "def", "flatten_and_remove_out_heads", "(", "mnet", ",", "weights", ",", "allowed_outputs", ")", ":", "\n", "    ", "\"\"\"Flatten a list of target network tensors to a single vector, such that\n    output neurons that belong to other than the current output head are\n    dropped.\n\n    Note, this method assumes that the main network has a fully-connected output\n    layer.\n\n    Args:\n        mnet: Main network instance.\n        weights: A list of weight tensors of the main network (must adhere the\n            corresponding weight shapes).\n        allowed_outputs: List of integers, denoting which output neurons of\n            the fully-connected output layer belong to the current head.\n\n    Returns:\n        The flattened weights with those output weights not belonging to the\n        current head being removed.\n    \"\"\"", "\n", "# FIXME the option `mask_fc_out` did not exist in a previous version of the", "\n", "# main network interface, which is why we need to ensure downwards", "\n", "# compatibility.", "\n", "# Previously, it was assumed sufficient for masking if `has_fc_out` was set", "\n", "# to True.", "\n", "assert", "(", "mnet", ".", "has_fc_out", ")", "\n", "assert", "(", "not", "hasattr", "(", "mnet", ",", "'mask_fc_out'", ")", "or", "(", "mnet", ".", "has_fc_out", "and", "mnet", ".", "mask_fc_out", ")", ")", "\n", "\n", "obias_ind", "=", "len", "(", "weights", ")", "-", "1", "if", "mnet", ".", "has_bias", "else", "-", "1", "\n", "oweights_ind", "=", "len", "(", "weights", ")", "-", "2", "if", "mnet", ".", "has_bias", "else", "len", "(", "weights", ")", "-", "1", "\n", "\n", "ret", "=", "[", "]", "\n", "for", "i", ",", "w", "in", "enumerate", "(", "weights", ")", ":", "\n", "        ", "if", "i", "==", "obias_ind", ":", "# Output bias", "\n", "            ", "ret", ".", "append", "(", "w", "[", "allowed_outputs", "]", ")", "\n", "", "elif", "i", "==", "oweights_ind", ":", "# Output weights", "\n", "            ", "ret", ".", "append", "(", "w", "[", "allowed_outputs", ",", ":", "]", ".", "view", "(", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "ret", ".", "append", "(", "w", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "", "return", "torch", ".", "cat", "(", "ret", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train.test": [[47, 234], ["mnet.eval", "hnet.eval", "logger.info", "logger.info", "torch.no_grad", "torch.no_grad", "numpy.empty", "numpy.empty", "numpy.empty", "logger.info", "NotImplementedError", "data.reset_batch_generator", "logger.info", "numpy.ones", "data.next_test_batch", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "mnet.forward", "torch.nn.functional.softmax().cpu().numpy", "data.output_to_torch_tensor.argmax().cpu().numpy", "F.softmax().cpu().numpy.argmax", "mnets.classifier_interface.Classifier.logit_cross_entropy_loss", "writer.add_scalar", "NotImplementedError", "writer.add_scalar", "logger.warning", "NotImplementedError", "NotImplementedError", "torch.nn.functional.softmax().cpu", "data.output_to_torch_tensor.argmax().cpu", "hnet.forward", "hnet.forward", "torch.nn.functional.softmax", "data.output_to_torch_tensor.argmax"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.reset_batch_generator", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_test_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.logit_cross_entropy_loss", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["def", "test", "(", "task_id", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "config", ",", "writer", ",", "logger", ",", "\n", "train_iter", "=", "None", ",", "task_emb", "=", "None", ",", "cl_scenario", "=", "None", ",", "test_size", "=", "None", ")", ":", "\n", "    ", "\"\"\"Evaluate the current performance using the test set.\n\n    Note:\n        The hypernetwork ``hnet`` may be ``None``, in which case it is assumed\n        that the main network ``mnet`` has internal weights.\n\n    Args:\n        (....): See docstring of function :func:`train`.\n        train_iter (int, optional): The current training iteration. If given, it\n            is used for tensorboard logging.\n        task_emb (torch.Tensor, optional): Task embedding. If given, no task ID\n            will be provided to the hypernetwork. This might be useful if the\n            performance of other than the trained task embeddings should be\n            tested.\n\n            .. note::\n                This option may only be used for ``cl_scenario=1``. It doesn't\n                make sense if the task ID has to be inferred.\n        cl_scenario (int, optional): In case the system should be tested on\n            another CL scenario than the one user-defined in ``config``.\n            \n            .. note::\n                It is up to the user to ensure that the CL scnearios are\n                compatible in this implementation.\n        test_size (int, optional): In case the testing shouldn't be performed\n            on the entire test set, this option can be used to specify the\n            number of test samples to be used.\n\n    Returns:\n        (tuple): Tuple containing:\n\n        - **test_acc**: Test accuracy on classification task.\n        - **task_acc**: Task prediction accuracy (always 100% for **CL1**).\n    \"\"\"", "\n", "if", "cl_scenario", "is", "None", ":", "\n", "        ", "cl_scenario", "=", "config", ".", "cl_scenario", "\n", "", "else", ":", "\n", "        ", "assert", "cl_scenario", "in", "[", "1", ",", "2", ",", "3", "]", "\n", "\n", "# `task_emb` ignored for other cl scenarios!", "\n", "", "assert", "task_emb", "is", "None", "or", "cl_scenario", "==", "1", ",", "'\"task_emb\" may only be specified for CL1, as we infer the '", "+", "'embedding for other scenarios.'", "\n", "\n", "mnet", ".", "eval", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "hnet", ".", "eval", "(", ")", "\n", "\n", "", "if", "train_iter", "is", "None", ":", "\n", "        ", "logger", ".", "info", "(", "'### Test run ...'", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "'# Testing network before running training step %d ...'", "%", "train_iter", ")", "\n", "\n", "# We need to tell the main network, which batch statistics to use, in case", "\n", "# batchnorm is used and we checkpoint the batchnorm stats.", "\n", "", "mnet_kwargs", "=", "{", "}", "\n", "if", "mnet", ".", "batchnorm_layers", "is", "not", "None", ":", "\n", "        ", "if", "config", ".", "bn_distill_stats", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "", "elif", "not", "config", ".", "bn_no_running_stats", "and", "not", "config", ".", "bn_no_stats_checkpointing", ":", "\n", "# Specify current task as condition to select correct", "\n", "# running stats.", "\n", "            ", "mnet_kwargs", "[", "'condition'", "]", "=", "task_id", "\n", "\n", "if", "task_emb", "is", "not", "None", ":", "\n", "# NOTE `task_emb` might have nothing to do with `task_id`.", "\n", "                ", "logger", ".", "warning", "(", "'Using batch statistics accumulated for task '", "+", "\n", "'%d for batchnorm, but testing is '", "%", "task_id", "+", "\n", "'performed using a given task embedding.'", ")", "\n", "\n", "", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "batch_size", "=", "config", ".", "val_batch_size", "\n", "# FIXME Assuming all output heads have the same size.", "\n", "n_head", "=", "data", ".", "num_classes", "\n", "\n", "if", "test_size", "is", "None", "or", "test_size", ">=", "data", ".", "num_test_samples", ":", "\n", "            ", "test_size", "=", "data", ".", "num_test_samples", "\n", "", "else", ":", "\n", "# Make sure that we always use the same test samples.", "\n", "            ", "data", ".", "reset_batch_generator", "(", "train", "=", "False", ",", "test", "=", "True", ",", "val", "=", "False", ")", "\n", "logger", ".", "info", "(", "'Note, only part of test set is used for this test '", "+", "\n", "'run!'", ")", "\n", "\n", "", "test_loss", "=", "0.0", "\n", "\n", "# We store all predicted labels and tasks while going over individual", "\n", "# test batches.", "\n", "correct_labels", "=", "np", ".", "empty", "(", "test_size", ",", "np", ".", "int", ")", "\n", "pred_labels", "=", "np", ".", "empty", "(", "test_size", ",", "np", ".", "int", ")", "\n", "correct_tasks", "=", "np", ".", "ones", "(", "test_size", ",", "np", ".", "int", ")", "*", "task_id", "\n", "pred_tasks", "=", "np", ".", "empty", "(", "test_size", ",", "np", ".", "int", ")", "\n", "\n", "curr_bs", "=", "batch_size", "\n", "N_processed", "=", "0", "\n", "\n", "# Sweep through the test set.", "\n", "while", "N_processed", "<", "test_size", ":", "\n", "            ", "if", "N_processed", "+", "curr_bs", ">", "test_size", ":", "\n", "                ", "curr_bs", "=", "test_size", "-", "N_processed", "\n", "", "N_processed", "+=", "curr_bs", "\n", "\n", "batch", "=", "data", ".", "next_test_batch", "(", "curr_bs", ")", "\n", "X", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ")", "\n", "T", "=", "data", ".", "output_to_torch_tensor", "(", "batch", "[", "1", "]", ",", "device", ")", "\n", "\n", "############################", "\n", "### Get main net weights ###", "\n", "############################", "\n", "if", "hnet", "is", "None", ":", "\n", "                ", "weights", "=", "None", "\n", "", "elif", "cl_scenario", ">", "1", ":", "\n", "                ", "raise", "NotImplementedError", "(", ")", "\n", "", "elif", "task_emb", "is", "not", "None", ":", "\n", "                ", "weights", "=", "hnet", ".", "forward", "(", "task_emb", "=", "task_emb", ")", "\n", "", "else", ":", "\n", "                ", "weights", "=", "hnet", ".", "forward", "(", "task_id", "=", "task_id", ")", "\n", "\n", "#######################", "\n", "### Get predictions ###", "\n", "#######################", "\n", "", "Y_hat_logits", "=", "mnet", ".", "forward", "(", "X", ",", "weights", "=", "weights", ",", "**", "mnet_kwargs", ")", "\n", "\n", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "# Select current head.", "\n", "                ", "task_out", "=", "[", "task_id", "*", "n_head", ",", "(", "task_id", "+", "1", ")", "*", "n_head", "]", "\n", "", "elif", "config", ".", "cl_scenario", "==", "2", ":", "\n", "# Only 1 output head.", "\n", "                ", "task_out", "=", "[", "0", ",", "n_head", "]", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", ")", "\n", "# TODO Choose the predicted output head per sample.", "\n", "#task_out = [predicted_task_id[0]*n_head,", "\n", "#            (predicted_task_id[0]+1)*n_head]", "\n", "\n", "", "Y_hat_logits", "=", "Y_hat_logits", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "# We take the softmax after the output neurons are chosen.", "\n", "Y_hat", "=", "F", ".", "softmax", "(", "Y_hat_logits", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "correct_labels", "[", "N_processed", "-", "curr_bs", ":", "N_processed", "]", "=", "T", ".", "argmax", "(", "dim", "=", "1", ",", "keepdim", "=", "False", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "pred_labels", "[", "N_processed", "-", "curr_bs", ":", "N_processed", "]", "=", "Y_hat", ".", "argmax", "(", "axis", "=", "1", ")", "\n", "\n", "# Set task prediction to 100% if we do not infer it.", "\n", "if", "cl_scenario", ">", "1", ":", "\n", "                ", "raise", "NotImplementedError", "(", ")", "\n", "#pred_tasks[N_processed-curr_bs:N_processed] = \\", "\n", "#    predicted_task_id.cpu().numpy()", "\n", "", "else", ":", "\n", "                ", "pred_tasks", "[", "N_processed", "-", "curr_bs", ":", "N_processed", "]", "=", "task_id", "\n", "\n", "# Note, targets are 1-hot encoded.", "\n", "", "test_loss", "+=", "Classifier", ".", "logit_cross_entropy_loss", "(", "Y_hat_logits", ",", "T", ",", "\n", "reduction", "=", "'sum'", ")", "\n", "\n", "", "class_n_correct", "=", "(", "correct_labels", "==", "pred_labels", ")", ".", "sum", "(", ")", "\n", "test_acc", "=", "100.0", "*", "class_n_correct", "/", "test_size", "\n", "\n", "task_n_correct", "=", "(", "correct_tasks", "==", "pred_tasks", ")", ".", "sum", "(", ")", "\n", "task_acc", "=", "100.0", "*", "task_n_correct", "/", "test_size", "\n", "\n", "test_loss", "/=", "test_size", "\n", "\n", "msg", "=", "'### Test accuracy of task %d'", "%", "(", "task_id", "+", "1", ")", "+", "(", "' (before training iteration %d)'", "%", "train_iter", "if", "train_iter", "is", "not", "None", "else", "''", ")", "+", "': %.3f'", "%", "(", "test_acc", ")", "+", "(", "' (using a given task embedding)'", "if", "task_emb", "is", "not", "None", "else", "''", ")", "+", "(", "' - task prediction accuracy: %.3f'", "%", "task_acc", "if", "cl_scenario", ">", "1", "else", "''", ")", "\n", "logger", ".", "info", "(", "msg", ")", "\n", "\n", "if", "train_iter", "is", "not", "None", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'test/task_%d/class_accuracy'", "%", "task_id", ",", "\n", "test_acc", ",", "train_iter", ")", "\n", "\n", "if", "config", ".", "cl_scenario", ">", "1", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "'test/task_%d/task_pred_accuracy'", "%", "task_id", ",", "task_acc", ",", "train_iter", ")", "\n", "\n", "", "", "return", "test_acc", ",", "task_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train.train": [[235, 590], ["time.time", "logger.info", "mnet.train", "utils.torch_utils.get_optimizer", "range", "logger.info", "logger.info", "hnet.train", "list", "utils.torch_utils.get_optimizer", "torch.lr_scheduler.ReduceLROnPlateau", "torch.lr_scheduler.LambdaLR", "int", "time.time", "utils.torch_utils.get_optimizer.zero_grad", "data.next_train_batch", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "mnet.forward", "Classifier.logit_cross_entropy_loss.backward", "utils.torch_utils.get_optimizer.step", "torch.nn.functional.softmax", "time.time", "range", "torch.lr_scheduler.ReduceLROnPlateau", "torch.lr_scheduler.LambdaLR", "utils.get_current_targets", "NotImplementedError", "numpy.ceil", "train.test", "mnet.train", "logger.info", "utils.torch_utils.get_optimizer.zero_grad", "hnet.forward", "torch.where", "torch.where", "soft_targets.to.to", "mnets.classifier_interface.Classifier.softmax_and_cross_entropy", "mnets.classifier_interface.Classifier.logit_cross_entropy_loss", "utils.torch_utils.get_optimizer.step", "utils.calc_fix_target_reg", "hreg.calc_fix_target_reg.backward", "mnets.classifier_interface.Classifier.accuracy", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "logger.debug", "logger.info", "list.append", "hnet.get_task_emb", "numpy.sqrt", "p.detach().clone", "p.detach().clone", "list", "hnet.train", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "utils.calc_delta_theta", "logger.info", "train.test", "mnet.train", "optim.lr_scheduler.ReduceLROnPlateau.step", "logger.info", "optim.lr_scheduler.LambdaLR.step", "msg.format", "bn_layer.checkpoint_stats", "hnet.get_task_emb", "numpy.sqrt", "hnet.get_task_embs", "range", "range", "hnet.train", "optim.lr_scheduler.ReduceLROnPlateau.step", "optim.lr_scheduler.LambdaLR.step", "time.time", "p.detach", "p.detach"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.get_optimizer", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.get_optimizer", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.get_current_targets", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.softmax_and_cross_entropy", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.logit_cross_entropy_loss", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_fix_target_reg", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.accuracy", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.calc_delta_theta", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.checkpoint_stats", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train"], ["", "", "def", "train", "(", "task_id", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "logger", ")", ":", "\n", "    ", "\"\"\"Train the hyper network using the task-specific loss plus a regularizer\n    that should overcome catastrophic forgetting.\n\n    :code:`loss = task_loss + beta * regularizer`.\n\n    Args:\n        task_id: The index of the task on which we train.\n        data: The dataset handler.\n        mnet: The model of the main network.\n        hnet: The model of the hyper network. May be ``None``.\n        device: Torch device (cpu or gpu).\n        config: The command line arguments.\n        shared (argparse.Namespace): Set of variables shared between functions.\n        writer: The tensorboard summary writer.\n        logger: The logger that should be used rather than the print method.\n    \"\"\"", "\n", "start_time", "=", "time", "(", ")", "\n", "\n", "logger", ".", "info", "(", "'Training network ...'", ")", "\n", "\n", "mnet", ".", "train", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "hnet", ".", "train", "(", ")", "\n", "\n", "#################", "\n", "### Optimizer ###", "\n", "#################", "\n", "# Define the optimizers used to train main network and hypernet.", "\n", "", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "theta_params", "=", "list", "(", "hnet", ".", "theta", ")", "\n", "if", "config", ".", "continue_emb_training", ":", "\n", "            ", "for", "i", "in", "range", "(", "task_id", ")", ":", "# for all previous task embeddings", "\n", "                ", "theta_params", ".", "append", "(", "hnet", ".", "get_task_emb", "(", "i", ")", ")", "\n", "\n", "# Only for the current task embedding.", "\n", "# Important that this embedding is in a different optimizer in case", "\n", "# we use the lookahead.", "\n", "", "", "emb_optimizer", "=", "get_optimizer", "(", "[", "hnet", ".", "get_task_emb", "(", "task_id", ")", "]", ",", "\n", "config", ".", "lr", ",", "momentum", "=", "config", ".", "momentum", ",", "\n", "weight_decay", "=", "config", ".", "weight_decay", ",", "use_adam", "=", "config", ".", "use_adam", ",", "\n", "adam_beta1", "=", "config", ".", "adam_beta1", ",", "use_rmsprop", "=", "config", ".", "use_rmsprop", ")", "\n", "", "else", ":", "\n", "        ", "theta_params", "=", "mnet", ".", "weights", "\n", "emb_optimizer", "=", "None", "\n", "\n", "", "theta_optimizer", "=", "get_optimizer", "(", "theta_params", ",", "config", ".", "lr", ",", "\n", "momentum", "=", "config", ".", "momentum", ",", "weight_decay", "=", "config", ".", "weight_decay", ",", "\n", "use_adam", "=", "config", ".", "use_adam", ",", "adam_beta1", "=", "config", ".", "adam_beta1", ",", "\n", "use_rmsprop", "=", "config", ".", "use_rmsprop", ")", "\n", "\n", "################################", "\n", "### Learning rate schedulers ###", "\n", "################################", "\n", "if", "config", ".", "plateau_lr_scheduler", ":", "\n", "        ", "assert", "(", "config", ".", "epochs", "!=", "-", "1", ")", "\n", "# The scheduler config has been taken from here:", "\n", "# https://keras.io/examples/cifar10_resnet/", "\n", "# Note, we use 'max' instead of 'min' as we look at accuracy rather", "\n", "# than validation loss!", "\n", "plateau_scheduler_theta", "=", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "theta_optimizer", ",", "'max'", ",", "factor", "=", "np", ".", "sqrt", "(", "0.1", ")", ",", "patience", "=", "5", ",", "\n", "min_lr", "=", "0.5e-6", ",", "cooldown", "=", "0", ")", "\n", "plateau_scheduler_emb", "=", "None", "\n", "if", "emb_optimizer", "is", "not", "None", ":", "\n", "            ", "plateau_scheduler_emb", "=", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "emb_optimizer", ",", "'max'", ",", "factor", "=", "np", ".", "sqrt", "(", "0.1", ")", ",", "patience", "=", "5", ",", "\n", "min_lr", "=", "0.5e-6", ",", "cooldown", "=", "0", ")", "\n", "\n", "", "", "if", "config", ".", "lambda_lr_scheduler", ":", "\n", "        ", "assert", "(", "config", ".", "epochs", "!=", "-", "1", ")", "\n", "\n", "def", "lambda_lr", "(", "epoch", ")", ":", "\n", "            ", "\"\"\"Multiplicative Factor for Learning Rate Schedule.\n\n            Computes a multiplicative factor for the initial learning rate based\n            on the current epoch. This method can be used as argument\n            ``lr_lambda`` of class :class:`torch.optim.lr_scheduler.LambdaLR`.\n\n            The schedule is inspired by the Resnet CIFAR-10 schedule suggested\n            here https://keras.io/examples/cifar10_resnet/.\n\n            Args:\n                epoch (int): The number of epochs\n\n            Returns:\n                lr_scale (float32): learning rate scale\n            \"\"\"", "\n", "lr_scale", "=", "1.", "\n", "if", "epoch", ">", "180", ":", "\n", "                ", "lr_scale", "=", "0.5e-3", "\n", "", "elif", "epoch", ">", "160", ":", "\n", "                ", "lr_scale", "=", "1e-3", "\n", "", "elif", "epoch", ">", "120", ":", "\n", "                ", "lr_scale", "=", "1e-2", "\n", "", "elif", "epoch", ">", "80", ":", "\n", "                ", "lr_scale", "=", "1e-1", "\n", "", "return", "lr_scale", "\n", "\n", "", "lambda_scheduler_theta", "=", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "theta_optimizer", ",", "\n", "lambda_lr", ")", "\n", "lambda_scheduler_emb", "=", "None", "\n", "if", "emb_optimizer", "is", "not", "None", ":", "\n", "            ", "lambda_scheduler_emb", "=", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "emb_optimizer", ",", "\n", "lambda_lr", ")", "\n", "\n", "##############################", "\n", "### Prepare CL Regularizer ###", "\n", "##############################", "\n", "# Whether we will calculate the regularizer.", "\n", "", "", "calc_reg", "=", "task_id", ">", "0", "and", "not", "config", ".", "mnet_only", "and", "config", ".", "beta", ">", "0", "and", "not", "config", ".", "train_from_scratch", "\n", "\n", "# Compute targets when the reg is activated and we are not training", "\n", "# the first task", "\n", "if", "calc_reg", ":", "\n", "        ", "if", "config", ".", "online_target_computation", ":", "\n", "# Compute targets for the regularizer whenever they are needed.", "\n", "# -> Computationally expensive.", "\n", "            ", "targets_hypernet", "=", "None", "\n", "prev_theta", "=", "[", "p", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "p", "in", "hnet", ".", "theta", "]", "\n", "prev_task_embs", "=", "[", "p", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "p", "in", "hnet", ".", "get_task_embs", "(", ")", "]", "\n", "", "else", ":", "\n", "# Compute targets for the regularizer once and keep them all in", "\n", "# memory -> Memory expensive.", "\n", "            ", "targets_hypernet", "=", "hreg", ".", "get_current_targets", "(", "task_id", ",", "hnet", ")", "\n", "prev_theta", "=", "None", "\n", "prev_task_embs", "=", "None", "\n", "\n", "# If we do not want to regularize all outputs (in a multi-head setup).", "\n", "# Note, we don't care whether output heads other than the current one", "\n", "# change.", "\n", "", "regged_outputs", "=", "None", "\n", "if", "config", ".", "cl_scenario", "!=", "2", ":", "\n", "# FIXME We assume here that all tasks have the same output size.", "\n", "            ", "n_y", "=", "data", ".", "num_classes", "\n", "regged_outputs", "=", "[", "list", "(", "range", "(", "i", "*", "n_y", ",", "(", "i", "+", "1", ")", "*", "n_y", ")", ")", "for", "i", "in", "\n", "range", "(", "task_id", ")", "]", "\n", "\n", "# We need to tell the main network, which batch statistics to use, in case", "\n", "# batchnorm is used and we checkpoint the batchnorm stats.", "\n", "", "", "mnet_kwargs", "=", "{", "}", "\n", "if", "mnet", ".", "batchnorm_layers", "is", "not", "None", ":", "\n", "        ", "if", "config", ".", "bn_distill_stats", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "", "elif", "not", "config", ".", "bn_no_running_stats", "and", "not", "config", ".", "bn_no_stats_checkpointing", ":", "\n", "# Specify current task as condition to select correct", "\n", "# running stats.", "\n", "            ", "mnet_kwargs", "[", "'condition'", "]", "=", "task_id", "\n", "\n", "######################", "\n", "### Start training ###", "\n", "######################", "\n", "\n", "", "", "iter_per_epoch", "=", "-", "1", "\n", "if", "config", ".", "epochs", "==", "-", "1", ":", "\n", "        ", "training_iterations", "=", "config", ".", "n_iter", "\n", "", "else", ":", "\n", "        ", "assert", "(", "config", ".", "epochs", ">", "0", ")", "\n", "iter_per_epoch", "=", "int", "(", "np", ".", "ceil", "(", "data", ".", "num_train_samples", "/", "config", ".", "batch_size", ")", ")", "\n", "training_iterations", "=", "config", ".", "epochs", "*", "iter_per_epoch", "\n", "\n", "", "summed_iter_runtime", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "training_iterations", ")", ":", "\n", "### Evaluate network.", "\n", "# We test the network before we run the training iteration.", "\n", "# That way, we can see the initial performance of the untrained network.", "\n", "        ", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "test", "(", "task_id", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "config", ",", "writer", ",", "\n", "logger", ",", "train_iter", "=", "i", ")", "\n", "mnet", ".", "train", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "                ", "hnet", ".", "train", "(", ")", "\n", "\n", "", "", "if", "i", "%", "200", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Training step: %d ...'", "%", "i", ")", "\n", "\n", "", "iter_start_time", "=", "time", "(", ")", "\n", "\n", "theta_optimizer", ".", "zero_grad", "(", ")", "\n", "if", "emb_optimizer", "is", "not", "None", ":", "\n", "            ", "emb_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "#######################################", "\n", "### Data for current task and batch ###", "\n", "#######################################", "\n", "", "batch", "=", "data", ".", "next_train_batch", "(", "config", ".", "batch_size", ")", "\n", "X", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ",", "mode", "=", "'train'", ")", "\n", "T", "=", "data", ".", "output_to_torch_tensor", "(", "batch", "[", "1", "]", ",", "device", ",", "mode", "=", "'train'", ")", "\n", "\n", "# Get the output neurons depending on the continual learning scenario.", "\n", "n_y", "=", "data", ".", "num_classes", "\n", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "# Choose current head.", "\n", "            ", "task_out", "=", "[", "task_id", "*", "n_y", ",", "(", "task_id", "+", "1", ")", "*", "n_y", "]", "\n", "", "elif", "config", ".", "cl_scenario", "==", "2", ":", "\n", "# Always all output neurons, only one head is used.", "\n", "            ", "task_out", "=", "[", "0", ",", "n_y", "]", "\n", "", "else", ":", "\n", "# Choose current head, which will be inferred during inference.", "\n", "            ", "task_out", "=", "[", "task_id", "*", "n_y", ",", "(", "task_id", "+", "1", ")", "*", "n_y", "]", "\n", "\n", "########################", "\n", "### Loss computation ###", "\n", "########################", "\n", "", "if", "config", ".", "mnet_only", ":", "\n", "            ", "weights", "=", "None", "\n", "", "else", ":", "\n", "            ", "weights", "=", "hnet", ".", "forward", "(", "task_id", "=", "task_id", ")", "\n", "", "Y_hat_logits", "=", "mnet", ".", "forward", "(", "X", ",", "weights", ",", "**", "mnet_kwargs", ")", "\n", "\n", "# Restrict output neurons", "\n", "Y_hat_logits", "=", "Y_hat_logits", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "assert", "(", "T", ".", "shape", "[", "1", "]", "==", "Y_hat_logits", ".", "shape", "[", "1", "]", ")", "\n", "# compute loss on task and compute gradients", "\n", "if", "config", ".", "soft_targets", ":", "\n", "            ", "soft_label", "=", "0.95", "\n", "num_classes", "=", "data", ".", "num_classes", "\n", "soft_targets", "=", "torch", ".", "where", "(", "T", "==", "1", ",", "\n", "torch", ".", "Tensor", "(", "[", "soft_label", "]", ")", ",", "\n", "torch", ".", "Tensor", "(", "[", "(", "1", "-", "soft_label", ")", "/", "(", "num_classes", "-", "1", ")", "]", ")", ")", "\n", "soft_targets", "=", "soft_targets", ".", "to", "(", "device", ")", "\n", "loss_task", "=", "Classifier", ".", "softmax_and_cross_entropy", "(", "Y_hat_logits", ",", "\n", "soft_targets", ")", "\n", "", "else", ":", "\n", "            ", "loss_task", "=", "Classifier", ".", "logit_cross_entropy_loss", "(", "Y_hat_logits", ",", "T", ")", "\n", "\n", "# Compute gradients based on task loss (those might be used in the CL", "\n", "# regularizer).", "\n", "", "loss_task", ".", "backward", "(", "retain_graph", "=", "calc_reg", ",", "create_graph", "=", "calc_reg", "and", "config", ".", "backprop_dt", ")", "\n", "\n", "# The current task embedding only depends in the task loss, so we can", "\n", "# update it already.", "\n", "if", "emb_optimizer", "is", "not", "None", ":", "\n", "            ", "emb_optimizer", ".", "step", "(", ")", "\n", "\n", "#############################", "\n", "### CL (HNET) Regularizer ###", "\n", "#############################", "\n", "", "loss_reg", "=", "0", "\n", "dTheta", "=", "None", "\n", "\n", "if", "calc_reg", ":", "\n", "            ", "if", "config", ".", "no_lookahead", ":", "\n", "                ", "dTembs", "=", "None", "\n", "dTheta", "=", "None", "\n", "", "else", ":", "\n", "                ", "dTheta", "=", "opstep", ".", "calc_delta_theta", "(", "theta_optimizer", ",", "False", ",", "\n", "lr", "=", "config", ".", "lr", ",", "detach_dt", "=", "not", "config", ".", "backprop_dt", ")", "\n", "\n", "if", "config", ".", "continue_emb_training", ":", "\n", "                    ", "dTembs", "=", "dTheta", "[", "-", "task_id", ":", "]", "\n", "dTheta", "=", "dTheta", "[", ":", "-", "task_id", "]", "\n", "", "else", ":", "\n", "                    ", "dTembs", "=", "None", "\n", "\n", "", "", "loss_reg", "=", "hreg", ".", "calc_fix_target_reg", "(", "hnet", ",", "task_id", ",", "\n", "targets", "=", "targets_hypernet", ",", "dTheta", "=", "dTheta", ",", "dTembs", "=", "dTembs", ",", "\n", "mnet", "=", "mnet", ",", "inds_of_out_heads", "=", "regged_outputs", ",", "\n", "prev_theta", "=", "prev_theta", ",", "prev_task_embs", "=", "prev_task_embs", ",", "\n", "batch_size", "=", "config", ".", "cl_reg_batch_size", ")", "\n", "\n", "loss_reg", "*=", "config", ".", "beta", "\n", "\n", "loss_reg", ".", "backward", "(", ")", "\n", "\n", "# Now, that we computed the regularizer, we can use the accumulated", "\n", "# gradients and update the hnet (or mnet) parameters.", "\n", "", "theta_optimizer", ".", "step", "(", ")", "\n", "\n", "Y_hat", "=", "F", ".", "softmax", "(", "Y_hat_logits", ",", "dim", "=", "1", ")", "\n", "classifier_accuracy", "=", "Classifier", ".", "accuracy", "(", "Y_hat", ",", "T", ")", "*", "100.0", "\n", "\n", "#########################", "\n", "# Learning rate scheduler", "\n", "#########################", "\n", "if", "config", ".", "plateau_lr_scheduler", ":", "\n", "            ", "assert", "(", "iter_per_epoch", "!=", "-", "1", ")", "\n", "if", "i", "%", "iter_per_epoch", "==", "0", "and", "i", ">", "0", ":", "\n", "                ", "curr_epoch", "=", "i", "//", "iter_per_epoch", "\n", "logger", ".", "info", "(", "'Computing test accuracy for plateau LR '", "+", "\n", "'scheduler (epoch %d).'", "%", "curr_epoch", ")", "\n", "# We need a validation quantity for the plateau LR scheduler.", "\n", "# FIXME we should use an actual validation set rather than the", "\n", "# test set.", "\n", "# Note, https://keras.io/examples/cifar10_resnet/ uses the test", "\n", "# set to compute the validation loss. We use the \"validation\"", "\n", "# accuracy instead.", "\n", "# FIXME We increase `train_iter` as the print messages in the", "\n", "# test method suggest that the testing has been executed before", "\n", "test_acc", ",", "_", "=", "test", "(", "task_id", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "\n", "config", ",", "writer", ",", "logger", ",", "train_iter", "=", "i", "+", "1", ")", "\n", "mnet", ".", "train", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "                    ", "hnet", ".", "train", "(", ")", "\n", "\n", "", "plateau_scheduler_theta", ".", "step", "(", "test_acc", ")", "\n", "if", "plateau_scheduler_emb", "is", "not", "None", ":", "\n", "                    ", "plateau_scheduler_emb", ".", "step", "(", "test_acc", ")", "\n", "\n", "", "", "", "if", "config", ".", "lambda_lr_scheduler", ":", "\n", "            ", "assert", "(", "iter_per_epoch", "!=", "-", "1", ")", "\n", "if", "i", "%", "iter_per_epoch", "==", "0", "and", "i", ">", "0", ":", "\n", "                ", "curr_epoch", "=", "i", "//", "iter_per_epoch", "\n", "logger", ".", "info", "(", "'Applying Lambda LR scheduler (epoch %d).'", "\n", "%", "curr_epoch", ")", "\n", "\n", "lambda_scheduler_theta", ".", "step", "(", ")", "\n", "if", "lambda_scheduler_emb", "is", "not", "None", ":", "\n", "                        ", "lambda_scheduler_emb", ".", "step", "(", ")", "\n", "\n", "###########################", "\n", "### Tensorboard summary ###", "\n", "###########################", "\n", "# We don't wanna slow down training by having too much output.", "\n", "", "", "", "if", "i", "%", "50", "==", "0", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'train/task_%d/class_accuracy'", "%", "task_id", ",", "\n", "classifier_accuracy", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/loss_task'", "%", "task_id", ",", "loss_task", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/loss_reg'", "%", "task_id", ",", "loss_reg", ",", "i", ")", "\n", "\n", "### Show the current training progress to the user.", "\n", "", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "msg", "=", "'Training step {}: Classifier Accuracy: {:.3f} '", "+", "'(on current training batch).'", "\n", "logger", ".", "debug", "(", "msg", ".", "format", "(", "i", ",", "classifier_accuracy", ")", ")", "\n", "\n", "", "iter_end_time", "=", "time", "(", ")", "\n", "summed_iter_runtime", "+=", "(", "iter_end_time", "-", "iter_start_time", ")", "\n", "\n", "if", "i", "%", "200", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Training step: %d ... Done -- (runtime: %f sec)'", "%", "(", "i", ",", "iter_end_time", "-", "iter_start_time", ")", ")", "\n", "\n", "\n", "", "", "if", "mnet", ".", "batchnorm_layers", "is", "not", "None", ":", "\n", "        ", "if", "not", "config", ".", "bn_distill_stats", "and", "not", "config", ".", "bn_no_running_stats", "and", "not", "config", ".", "bn_no_stats_checkpointing", ":", "\n", "# Checkpoint the current running statistics (that have been", "\n", "# estimated while training the current task).", "\n", "            ", "for", "bn_layer", "in", "mnet", ".", "batchnorm_layers", ":", "\n", "                ", "assert", "(", "bn_layer", ".", "num_stats", "==", "task_id", "+", "1", ")", "\n", "bn_layer", ".", "checkpoint_stats", "(", ")", "\n", "\n", "", "", "", "avg_iter_time", "=", "summed_iter_runtime", "/", "config", ".", "n_iter", "\n", "logger", ".", "info", "(", "'Average runtime per training iteration: %f sec.'", "%", "avg_iter_time", ")", "\n", "\n", "logger", ".", "info", "(", "'Elapsed time for training task %d: %f sec.'", "%", "(", "task_id", "+", "1", ",", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train.test_multiple": [[591, 669], ["len", "logger.info", "range", "numpy.mean", "logger.info", "writer.add_scalar", "NotImplementedError", "range", "numpy.mean", "logger.info", "writer.add_scalar", "train.test", "class_accs.append", "logger.info", "logger.info", "train.test", "class_accs.append", "task_accs.append", "logger.info", "writer.add_scalar", "logger.info", "writer.add_scalar", "numpy.mean", "str", "str", "numpy.mean", "str", "str"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["", "def", "test_multiple", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "\n", "logger", ")", ":", "\n", "    ", "\"\"\"Method to test continual learning experiment accuracy\n\n    Args:\n        (....): See docstring of function :func:`train`.\n        dhandlers (list): List of data handlers. The accuracy of each task in\n            this list will be computed using function :func:`test`. The index\n            within the list will be considered as task ID.\n    \"\"\"", "\n", "class_accs", "=", "[", "]", "\n", "task_accs", "=", "[", "]", "\n", "\n", "num_tasks", "=", "len", "(", "dhandlers", ")", "\n", "\n", "### Task-incremental learning", "\n", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "        ", "logger", ".", "info", "(", "'### Testing task-incremental learning scenario'", ")", "\n", "# Iterate through learned embeddings and tasks and compute test acc.", "\n", "for", "j", "in", "range", "(", "num_tasks", ")", ":", "\n", "            ", "data", "=", "dhandlers", "[", "j", "]", "\n", "\n", "test_acc", ",", "_", "=", "test", "(", "j", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "\n", "config", ",", "writer", ",", "logger", ")", "\n", "\n", "class_accs", ".", "append", "(", "test_acc", ")", "\n", "shared", ".", "summary", "[", "'acc_final'", "]", "[", "j", "]", "=", "test_acc", "\n", "\n", "", "shared", ".", "summary", "[", "'acc_avg_final'", "]", "=", "np", ".", "mean", "(", "class_accs", ")", "\n", "logger", ".", "info", "(", "'### Task-incremental learning scenario accuracies: %s '", "%", "(", "str", "(", "class_accs", ")", ")", "+", "'(avg: %.3f)'", "\n", "%", "(", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "'final/task_incremental'", ",", "\n", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", "\n", "\n", "### Domain-incremental learning & class-incremental learning", "\n", "", "if", "config", ".", "cl_scenario", "==", "2", "or", "config", ".", "cl_scenario", "==", "3", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n", "if", "config", ".", "cl_scenario", "==", "2", ":", "\n", "            ", "logger", ".", "info", "(", "'### Testing domain-incremental learning scenario'", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "'### Testing class-incrementa learning scenario'", ")", "\n", "\n", "\n", "", "for", "j", "in", "range", "(", "num_tasks", ")", ":", "\n", "            ", "data", "=", "dhandlers", "[", "j", "]", "\n", "\n", "test_acc", ",", "task_acc", "=", "test", "(", "j", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "\n", "config", ",", "writer", ",", "logger", ")", "\n", "\n", "class_accs", ".", "append", "(", "test_acc", ")", "\n", "task_accs", ".", "append", "(", "task_acc", ")", "\n", "\n", "shared", ".", "summary", "[", "'acc_final'", "]", "[", "j", "]", "=", "test_acc", "\n", "\n", "", "shared", ".", "summary", "[", "'acc_avg_final'", "]", "=", "np", ".", "mean", "(", "class_accs", ")", "\n", "\n", "if", "config", ".", "cl_scenario", "==", "2", ":", "\n", "            ", "logger", ".", "info", "(", "'### Domain-incremental learning scenario '", "+", "\n", "'accuracies: %s '", "%", "(", "str", "(", "class_accs", ")", ")", "+", "'(avg: %.3f)'", "\n", "%", "(", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'final/domain_incremental'", ",", "\n", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "'### Class-incremental learning scenario '", "+", "\n", "'accuracies: %s '", "%", "(", "str", "(", "class_accs", ")", ")", "+", "'(avg: %.3f)'", "\n", "%", "(", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'final/class_incremental'", ",", "\n", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", "\n", "\n", "", "logger", ".", "info", "(", "'### Task-inference accuracies: %s '", "%", "(", "str", "(", "task_accs", ")", ")", "+", "'(avg: %.3f)'", "\n", "%", "(", "np", ".", "mean", "(", "task_accs", ")", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'final/task_inference_acc'", ",", "np", ".", "mean", "(", "task_accs", ")", ")", "\n", "\n", "", "return", "task_accs", ",", "class_accs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train.analysis": [[670, 717], ["mnet.eval", "hnet.eval", "len", "range", "range", "hnet.forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "logger.info", "range", "hnet.forward", "hnet.forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "logger.info", "a.detach().clone().cpu().flatten", "a.flatten", "a.detach().clone().flatten", "a.detach().clone().flatten", "a.detach().clone().cpu", "torch.sqrt", "torch.sqrt", "torch.sum", "torch.sum", "a.detach().clone", "a.detach().clone", "torch.sqrt", "torch.sqrt", "a.detach().clone", "torch.sum", "torch.sum", "a.detach", "a.detach", "a.detach"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "analysis", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "logger", ",", "\n", "during_weights", ")", ":", "\n", "    ", "\"\"\"A function to do some post-hoc analysis on the hypernetwork.\n\n    Specifically, this function does the following:\n        - Computing and logging statistics on how the weights changed since a\n          task has been learned.\n        - Assessing the diversity of ``hnet`` outputs, i.e., how close are the\n          ``hnet`` outputs for different tasks.\n\n    Args:\n        (....): See docstring of function :func:`test_multiple`.\n        during_weights (list): List of flattened ``hnet`` outputs right after\n            training on each task.\n    \"\"\"", "\n", "assert", "hnet", "is", "not", "None", "\n", "mnet", ".", "eval", "(", ")", "\n", "hnet", ".", "eval", "(", ")", "\n", "\n", "num_tasks", "=", "len", "(", "dhandlers", ")", "\n", "\n", "# Test how much the weights of each task have changed during training the", "\n", "# remaining tasks.", "\n", "for", "j", "in", "range", "(", "num_tasks", ")", ":", "\n", "        ", "cur_weights", "=", "hnet", ".", "forward", "(", "j", ")", "\n", "cur_weights", "=", "torch", ".", "cat", "(", "[", "a", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "cpu", "(", ")", ".", "flatten", "(", ")", "\n", "for", "a", "in", "cur_weights", "]", ")", "\n", "aft_weights", "=", "torch", ".", "cat", "(", "[", "a", ".", "flatten", "(", ")", "for", "a", "in", "during_weights", "[", "j", "]", "]", ")", "\n", "\n", "logger", ".", "info", "(", "'### Euclidean distance of current hnet output to '", "+", "\n", "'original one for task %d: %f'", "%", "(", "j", ",", "torch", ".", "sqrt", "(", "torch", ".", "sum", "(", "(", "aft_weights", "-", "cur_weights", ")", "**", "2", ")", ")", ")", ")", "\n", "\n", "# FIXME Inefficient, we already computed all hnet outputs above.", "\n", "", "for", "j", "in", "range", "(", "num_tasks", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "num_tasks", ")", ":", "\n", "            ", "if", "i", "<=", "j", ":", "\n", "                ", "continue", "\n", "", "weights_1", "=", "hnet", ".", "forward", "(", "j", ")", "\n", "weights_2", "=", "hnet", ".", "forward", "(", "i", ")", "\n", "weights_1", "=", "torch", ".", "cat", "(", "[", "a", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "flatten", "(", ")", "for", "a", "in", "weights_1", "]", ")", "\n", "weights_2", "=", "torch", ".", "cat", "(", "[", "a", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "flatten", "(", ")", "for", "a", "in", "weights_2", "]", ")", "\n", "logger", ".", "info", "(", "'### Euclidean distance between '", "+", "\n", "'task %d and task %d: %f'", "%", "(", "j", ",", "i", ",", "\n", "torch", ".", "sqrt", "(", "torch", ".", "sum", "(", "(", "weights_1", "-", "weights_2", ")", "**", "2", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train.run": [[718, 859], ["time.time", "utils.sim_utils.setup_environment", "argparse.Namespace", "cifar.train_utils.load_datasets", "cifar.train_utils.get_main_model", "cifar.train_utils.setup_summary_dict", "writer.add_hparams", "range", "numpy.mean", "logger.info", "logger.info", "writer.add_scalar", "train.test_multiple", "cifar.train_utils.save_summary_dict", "writer.close", "logger.info", "cifar.train_utils.get_hnet_model", "logger.info", "train.train", "train.test", "logger.info", "logger.info", "cifar.train_utils.save_summary_dict", "train.analysis", "tutils.get_hnet_model.get_task_emb().detach().clone", "tutils.get_hnet_model.forward", "weights_after_training.append", "str", "vars", "tutils.get_hnet_model.get_task_emb", "logger.info", "cifar.train_utils.get_main_model", "logger.info", "cifar.train_utils.get_hnet_model", "w.detach().clone().cpu", "time.time", "tutils.get_hnet_model.get_task_emb().detach", "w.detach().clone", "tutils.get_hnet_model.get_task_emb", "w.detach"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.sim_utils.setup_environment", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.load_datasets", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.get_main_model", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.setup_summary_dict", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train.test_multiple", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.save_summary_dict", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.get_hnet_model", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.save_summary_dict", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train.analysis", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.get_main_model", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.get_hnet_model", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb"], ["", "", "", "def", "run", "(", "config", ",", "experiment", "=", "'resnet'", ")", ":", "\n", "    ", "\"\"\"Run the training.\n\n    Args:\n        config (argparse.Namespace): Command-line arguments.\n        experiment (str): Which kind of experiment should be performed?\n\n            - ``resnet``: CIFAR-10/100 with Resnet-32.\n            - ``zenke``: CIFAR-10/100 with Zenkenet.\n    \"\"\"", "\n", "assert", "(", "experiment", "in", "[", "'resnet'", ",", "'zenke'", "]", ")", "\n", "\n", "script_start", "=", "time", "(", ")", "\n", "\n", "device", ",", "writer", ",", "logger", "=", "sutils", ".", "setup_environment", "(", "config", ",", "\n", "logger_name", "=", "'det_cl_cifar_%s'", "%", "experiment", ")", "\n", "# TODO Adapt script to allow checkpointing of models using", "\n", "# `utils.torch_ckpts` (i.e., we should be able to continue training or just", "\n", "# test an existing checkpoint).", "\n", "#config.ckpt_dir = os.path.join(config.out_dir, 'checkpoints')", "\n", "\n", "# Container for variables shared across function.", "\n", "shared", "=", "Namespace", "(", ")", "\n", "shared", ".", "experiment", "=", "experiment", "\n", "\n", "### Load datasets (i.e., create tasks).", "\n", "dhandlers", "=", "tutils", ".", "load_datasets", "(", "config", ",", "shared", ",", "logger", ",", "\n", "data_dir", "=", "'../datasets'", ")", "\n", "\n", "### Create main network.", "\n", "# TODO Allow main net only training.", "\n", "mnet", "=", "tutils", ".", "get_main_model", "(", "config", ",", "shared", ",", "logger", ",", "device", ",", "\n", "no_weights", "=", "not", "config", ".", "mnet_only", ")", "\n", "\n", "### Create the hypernetwork.", "\n", "if", "config", ".", "mnet_only", ":", "\n", "        ", "hnet", "=", "None", "\n", "", "else", ":", "\n", "        ", "hnet", "=", "tutils", ".", "get_hnet_model", "(", "config", ",", "mnet", ",", "logger", ",", "device", ")", "\n", "\n", "### Initialize the performance measures, that should be tracked during", "\n", "### training.", "\n", "", "tutils", ".", "setup_summary_dict", "(", "config", ",", "shared", ",", "mnet", ",", "hnet", "=", "hnet", ")", "\n", "\n", "# Add hparams to tensorboard, such that the identification of runs is", "\n", "# easier.", "\n", "writer", ".", "add_hparams", "(", "hparam_dict", "=", "{", "**", "vars", "(", "config", ")", ",", "**", "{", "\n", "'num_weights_main'", ":", "shared", ".", "summary", "[", "'num_weights_main'", "]", ",", "\n", "'num_weights_hyper'", ":", "shared", ".", "summary", "[", "'num_weights_hyper'", "]", ",", "\n", "'num_weights_ratio'", ":", "shared", ".", "summary", "[", "'num_weights_ratio'", "]", ",", "\n", "}", "}", ",", "metric_dict", "=", "{", "}", ")", "\n", "\n", "# FIXME: Method \"calc_fix_target_reg\" expects a None value.", "\n", "# But `writer.add_hparams` can't deal with `None` values.", "\n", "if", "config", ".", "cl_reg_batch_size", "==", "-", "1", ":", "\n", "        ", "config", ".", "cl_reg_batch_size", "=", "None", "\n", "\n", "# We keep the hnet output right after training to measure forgetting.", "\n", "", "weights_after_training", "=", "[", "]", "\n", "\n", "######################", "\n", "### Start Training ###", "\n", "######################", "\n", "for", "j", "in", "range", "(", "config", ".", "num_tasks", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Starting training of task %d ...'", "%", "(", "j", "+", "1", ")", ")", "\n", "\n", "data", "=", "dhandlers", "[", "j", "]", "\n", "\n", "# It might be that tasks are very similar and we can transfer knowledge", "\n", "# form the previous solution.", "\n", "if", "hnet", "is", "not", "None", "and", "config", ".", "init_with_prev_emb", "and", "j", ">", "0", ":", "\n", "            ", "last_emb", "=", "hnet", ".", "get_task_emb", "(", "j", "-", "1", ")", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "hnet", ".", "get_task_emb", "(", "j", ")", ".", "data", "=", "last_emb", "\n", "\n", "# Training from scratch -- create new network instance!", "\n", "# -> No transfer possible.", "\n", "", "if", "j", ">", "0", "and", "config", ".", "train_from_scratch", ":", "\n", "# FIXME Since we simply override the current network, future testing", "\n", "# on this new network for old tasks doesn't make sense. So we", "\n", "# shouldn't report `final` accuracies.", "\n", "            ", "if", "config", ".", "mnet_only", ":", "\n", "                ", "logger", ".", "info", "(", "'From scratch training: Creating new main network.'", ")", "\n", "mnet", "=", "tutils", ".", "get_main_model", "(", "config", ",", "shared", ",", "logger", ",", "device", ",", "\n", "no_weights", "=", "not", "config", ".", "mnet_only", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "'From scratch training: Creating new hypernetwork.'", ")", "\n", "hnet", "=", "tutils", ".", "get_hnet_model", "(", "config", ",", "mnet", ",", "logger", ",", "device", ")", "\n", "\n", "################################", "\n", "### Train and test on task j ###", "\n", "################################", "\n", "", "", "train", "(", "j", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "logger", ")", "\n", "\n", "### Final test run.", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "            ", "weights", "=", "hnet", ".", "forward", "(", "j", ")", "\n", "# Push to CPU to avoid growing GPU memory when solving very long", "\n", "# task sequences.", "\n", "weights", "=", "[", "w", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "cpu", "(", ")", "for", "w", "in", "weights", "]", "\n", "weights_after_training", ".", "append", "(", "weights", ")", "\n", "\n", "", "test_acc", ",", "_", "=", "test", "(", "j", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "config", ",", "writer", ",", "\n", "logger", ")", "\n", "\n", "logger", ".", "info", "(", "'### Accuracy of task %d / %d:  %.3f'", "%", "(", "j", "+", "1", ",", "config", ".", "num_tasks", ",", "test_acc", ")", ")", "\n", "logger", ".", "info", "(", "'### Finished training task: %d'", "%", "(", "j", "+", "1", ")", ")", "\n", "shared", ".", "summary", "[", "'acc_during'", "]", "[", "j", "]", "=", "test_acc", "\n", "\n", "# Backup results so far.", "\n", "tutils", ".", "save_summary_dict", "(", "config", ",", "shared", ",", "experiment", ")", "\n", "\n", "", "shared", ".", "summary", "[", "'acc_avg_during'", "]", "=", "np", ".", "mean", "(", "shared", ".", "summary", "[", "'acc_during'", "]", ")", "\n", "\n", "logger", ".", "info", "(", "'### Accuracy of individual tasks after training %s'", "%", "(", "str", "(", "shared", ".", "summary", "[", "'acc_during'", "]", ")", ")", ")", "\n", "logger", ".", "info", "(", "'### Average of these accuracies  %.2f'", "%", "(", "shared", ".", "summary", "[", "'acc_avg_during'", "]", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'final/during_acc_avg'", ",", "shared", ".", "summary", "[", "'acc_avg_during'", "]", ")", "\n", "\n", "#########################################", "\n", "### Test continual learning scenarios ###", "\n", "#########################################", "\n", "test_multiple", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "\n", "logger", ")", "\n", "\n", "#########################", "\n", "### Run some analysis ###", "\n", "#########################", "\n", "if", "not", "config", ".", "mnet_only", ":", "\n", "        ", "analysis", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "logger", ",", "\n", "weights_after_training", ")", "\n", "\n", "### Write final summary.", "\n", "", "shared", ".", "summary", "[", "'finished'", "]", "=", "1", "\n", "tutils", ".", "save_summary_dict", "(", "config", ",", "shared", ",", "experiment", ")", "\n", "\n", "writer", ".", "close", "(", ")", "\n", "\n", "logger", ".", "info", "(", "'Program finished successfully in %f sec.'", "\n", "%", "(", "time", "(", ")", "-", "script_start", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_args.parse_cmd_arguments": [[35, 150], ["argparse.ArgumentParser", "train_args.general_options", "train_args.special_cl_options", "train_args.special_train_options", "utils.init_args", "train_args.special_init_options", "utils.eval_args", "utils.miscellaneous_args", "argparse.ArgumentParser.parse_args", "utils.check_invalid_argument_usage", "utils.cl_args", "utils.main_net_args", "utils.hypernet_args", "utils.data_args", "utils.train_args", "ValueError", "NotImplementedError", "ValueError", "ValueError", "ValueError", "ValueError", "datetime.datetime.now().strftime", "utils.cl_args", "utils.main_net_args", "utils.hypernet_args", "utils.data_args", "utils.train_args", "warnings.warn", "datetime.datetime.now().strftime", "datetime.datetime.now", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_args.general_options", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_args.special_cl_options", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_args.special_train_options", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.init_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_args.special_init_options", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.eval_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.miscellaneous_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.check_invalid_argument_usage", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.cl_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.main_net_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.hypernet_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.data_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.train_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.cl_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.main_net_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.hypernet_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.data_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.train_args"], ["def", "parse_cmd_arguments", "(", "mode", "=", "'resnet_cifar'", ",", "default", "=", "False", ",", "argv", "=", "None", ")", ":", "\n", "    ", "\"\"\"Parse command-line arguments.\n\n    Args:\n        mode (str): For what script should the parser assemble the set of\n            command-line parameters? Options:\n\n                - ``resnet_cifar``\n                - ``zenke_cifar``\n\n        default (bool, optional): If ``True``, command-line arguments will be\n            ignored and only the default values will be parsed.\n        argv (list, optional): If provided, it will be treated as a list of\n            command- line argument that is passed to the parser in place of\n            :code:`sys.argv`.\n\n    Returns:\n        (argparse.Namespace): The Namespace object containing argument names and\n            values.\n    \"\"\"", "\n", "if", "mode", "==", "'resnet_cifar'", ":", "\n", "        ", "description", "=", "'CIFAR-10/100 CL experiment using a Resnet-32'", "\n", "", "elif", "mode", "==", "'zenke_cifar'", ":", "\n", "        ", "description", "=", "'CIFAR-10/100 CL experiment using the Zenkenet'", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Mode \"%s\" unknown.'", "%", "(", "mode", ")", ")", "\n", "\n", "", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "description", ")", "\n", "\n", "general_options", "(", "parser", ")", "\n", "\n", "if", "mode", "==", "'resnet_cifar'", ":", "\n", "        ", "dout_dir", "=", "'./out_resnet/run_'", "+", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S'", ")", "\n", "\n", "cl_group", "=", "cli", ".", "cl_args", "(", "parser", ",", "show_beta", "=", "True", ",", "dbeta", "=", "0.05", ",", "\n", "show_from_scratch", "=", "True", ",", "show_multi_head", "=", "False", ",", "\n", "show_cl_scenario", "=", "True", ",", "show_split_head_cl3", "=", "False", ",", "\n", "show_num_tasks", "=", "True", ",", "dnum_tasks", "=", "6", ")", "\n", "cli", ".", "main_net_args", "(", "parser", ",", "allowed_nets", "=", "[", "'resnet'", "]", ",", "show_batchnorm", "=", "False", ",", "\n", "show_no_batchnorm", "=", "True", ",", "show_bn_no_running_stats", "=", "True", ",", "\n", "show_bn_distill_stats", "=", "True", ",", "show_bn_no_stats_checkpointing", "=", "True", ",", "\n", "show_specnorm", "=", "False", ",", "show_dropout_rate", "=", "False", ",", "show_net_act", "=", "False", ")", "\n", "cli", ".", "hypernet_args", "(", "parser", ",", "dhyper_chunks", "=", "7000", ",", "dhnet_arch", "=", "''", ",", "\n", "dtemb_size", "=", "32", ",", "demb_size", "=", "32", ")", "\n", "cli", ".", "data_args", "(", "parser", ",", "show_disable_data_augmentation", "=", "True", ")", "\n", "train_agroup", "=", "cli", ".", "train_args", "(", "parser", ",", "show_lr", "=", "True", ",", "dlr", "=", "0.001", ",", "\n", "show_epochs", "=", "True", ",", "depochs", "=", "200", ",", "dbatch_size", "=", "32", ",", "\n", "dn_iter", "=", "2000", ",", "show_use_adam", "=", "True", ",", "show_use_rmsprop", "=", "True", ",", "\n", "show_use_adadelta", "=", "False", ",", "show_use_adagrad", "=", "False", ",", "\n", "show_clip_grad_value", "=", "False", ",", "show_clip_grad_norm", "=", "False", ")", "\n", "\n", "", "elif", "mode", "==", "'zenke_cifar'", ":", "\n", "        ", "dout_dir", "=", "'./out_zenke/run_'", "+", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S'", ")", "\n", "\n", "cl_group", "=", "cli", ".", "cl_args", "(", "parser", ",", "show_beta", "=", "True", ",", "dbeta", "=", "0.01", ",", "\n", "show_from_scratch", "=", "True", ",", "show_multi_head", "=", "False", ",", "\n", "show_cl_scenario", "=", "True", ",", "show_split_head_cl3", "=", "False", ",", "\n", "show_num_tasks", "=", "True", ",", "dnum_tasks", "=", "6", ")", "\n", "cli", ".", "main_net_args", "(", "parser", ",", "allowed_nets", "=", "[", "'zenke'", "]", ",", "show_batchnorm", "=", "False", ",", "\n", "show_no_batchnorm", "=", "False", ",", "show_dropout_rate", "=", "True", ",", "ddropout_rate", "=", "0.25", ",", "\n", "show_specnorm", "=", "False", ",", "show_net_act", "=", "False", ")", "\n", "cli", ".", "hypernet_args", "(", "parser", ",", "dhyper_chunks", "=", "5500", ",", "dhnet_arch", "=", "'100,150,200'", ",", "\n", "dtemb_size", "=", "48", ",", "demb_size", "=", "80", ")", "\n", "cli", ".", "data_args", "(", "parser", ",", "show_disable_data_augmentation", "=", "True", ")", "\n", "train_agroup", "=", "cli", ".", "train_args", "(", "parser", ",", "show_lr", "=", "True", ",", "dlr", "=", "0.0001", ",", "\n", "show_epochs", "=", "True", ",", "depochs", "=", "80", ",", "dbatch_size", "=", "256", ",", "\n", "dn_iter", "=", "2000", ",", "show_use_adam", "=", "True", ",", "\n", "dadam_beta1", "=", "0.5", ",", "show_use_rmsprop", "=", "True", ",", "\n", "show_use_adadelta", "=", "False", ",", "show_use_adagrad", "=", "False", ",", "\n", "show_clip_grad_value", "=", "False", ",", "show_clip_grad_norm", "=", "False", ")", "\n", "\n", "", "special_cl_options", "(", "cl_group", ")", "\n", "special_train_options", "(", "train_agroup", ")", "\n", "init_group", "=", "cli", ".", "init_args", "(", "parser", ",", "custom_option", "=", "True", ")", "\n", "special_init_options", "(", "init_group", ")", "\n", "cli", ".", "eval_args", "(", "parser", ",", "show_val_batch_size", "=", "True", ",", "dval_batch_size", "=", "1000", ")", "\n", "cli", ".", "miscellaneous_args", "(", "parser", ",", "big_data", "=", "False", ",", "synthetic_data", "=", "False", ",", "\n", "show_plots", "=", "False", ",", "no_cuda", "=", "False", ",", "dout_dir", "=", "dout_dir", ")", "\n", "\n", "args", "=", "None", "\n", "if", "argv", "is", "not", "None", ":", "\n", "        ", "if", "default", ":", "\n", "            ", "warnings", ".", "warn", "(", "'Provided \"argv\" will be ignored since \"default\" '", "+", "\n", "'option was turned on.'", ")", "\n", "", "args", "=", "argv", "\n", "", "if", "default", ":", "\n", "        ", "args", "=", "[", "]", "\n", "", "config", "=", "parser", ".", "parse_args", "(", "args", "=", "args", ")", "\n", "\n", "### Check argument values!", "\n", "cli", ".", "check_invalid_argument_usage", "(", "config", ")", "\n", "\n", "### ... insert additional checks if necessary", "\n", "if", "config", ".", "num_tasks", "<", "1", "or", "config", ".", "num_tasks", ">", "11", ":", "\n", "        ", "raise", "ValueError", "(", "'Argument \"num_tasks\" must be between 1 and 11!'", ")", "\n", "\n", "", "if", "config", ".", "cl_scenario", "!=", "1", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'CIFAR experiments are currently only '", "+", "\n", "'implemented for CL1.'", ")", "\n", "\n", "", "if", "config", ".", "plateau_lr_scheduler", "and", "config", ".", "epochs", "==", "-", "1", ":", "\n", "        ", "raise", "ValueError", "(", "'Flag \"plateau_lr_scheduler\" can only be used if '", "+", "\n", "'\"epochs\" was set.'", ")", "\n", "\n", "", "if", "config", ".", "lambda_lr_scheduler", "and", "config", ".", "epochs", "==", "-", "1", ":", "\n", "        ", "raise", "ValueError", "(", "'Flag \"lambda_lr_scheduler\" can only be used if '", "+", "\n", "'\"epochs\" was set.'", ")", "\n", "\n", "", "if", "config", ".", "no_lookahead", "and", "config", ".", "backprop_dt", ":", "\n", "        ", "raise", "ValueError", "(", "'Can\\'t activate \"no_lookahead\" and \"backprop_dt\" '", "+", "\n", "'simultaneously.'", ")", "\n", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_args.general_options": [[151, 168], ["parser.add_argument_group", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "general_options", "(", "parser", ")", ":", "\n", "    ", "\"\"\"This is a helper function of the function `parse_cmd_arguments` to create\n    an argument group for general stuff important for the types of experiments\n    conducted here.\n\n    Args:\n        parser (:class:`argparse.ArgumentParser`): The argument parser to which\n            the argument group should be added.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'General options'", ")", "\n", "\n", "agroup", ".", "add_argument", "(", "'--mnet_only'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Train the main network without a hypernetwork. '", "+", "\n", "'No continual learning support!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_args.special_init_options": [[169, 181], ["agroup.add_argument"], "function", ["None"], ["", "def", "special_init_options", "(", "agroup", ")", ":", "\n", "    ", "\"\"\"This is a helper function of the function `parse_cmd_arguments` to add\n    arguments to the `initialization` argument group.\n\n    Args:\n        agroup: The argument group returned by method\n            :func:`utils.cli_args.init_args`.\n    \"\"\"", "\n", "agroup", ".", "add_argument", "(", "'--hnet_init_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Shift the initial hnet output such that it '", "+", "\n", "'resembles a xavier or normal init for the '", "+", "\n", "'target network.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_args.special_cl_options": [[182, 226], ["agroup.add_argument", "agroup.add_argument", "agroup.add_argument", "agroup.add_argument", "agroup.add_argument", "agroup.add_argument"], "function", ["None"], ["", "def", "special_cl_options", "(", "agroup", ")", ":", "\n", "    ", "\"\"\"This is a helper function of the function `parse_cmd_arguments` to add\n    arguments to the `continual learning` argument group.\n\n    Args:\n        agroup: The argument group returned by method\n            :func:`utils.cli_args.cl_args`.\n    \"\"\"", "\n", "agroup", ".", "add_argument", "(", "'--init_with_prev_emb'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Initialize embeddings of new tasks with the '", "+", "\n", "'embedding of the most recent task.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--continue_emb_training'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Continue the training of task embeddings for '", "+", "\n", "'old tasks. This will give further flexibility '", "+", "\n", "'to the hypernet in terms of finding a '", "+", "\n", "'configuration that preserves previous outputs '", "+", "\n", "'and generates a suitable new output.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--online_target_computation'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'For our CL regularizer, this option will '", "+", "\n", "'ensure that the targets are computed on the '", "+", "\n", "'fly, using the hypernet weights acquired after '", "+", "\n", "'learning the previous task. Note, this '", "+", "\n", "'option ensures that there is alsmost no memory '", "+", "\n", "'grow with an increasing number of tasks '", "+", "\n", "'(except from an increasing number of task '", "+", "\n", "'embeddings). If this option is deactivated, '", "+", "\n", "'the more computationally efficient way is '", "+", "\n", "'chosen of computing all main network weight '", "+", "\n", "'targets (from all previous tasks) once before '", "+", "\n", "'learning a new task.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--cl_reg_batch_size'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'If not \"-1\", then this number will determine '", "+", "\n", "'the maximum number of previous tasks that are '", "+", "\n", "'are considered when computing the regularizer. '", "+", "\n", "'Hence, if the number of previous tasks is '", "+", "\n", "'than this number, then the regularizer will be '", "+", "\n", "'computed only over a random subset of previous '", "+", "\n", "'tasks. Default: %(default)s.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--no_lookahead'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use a simplified version of our regularizer, '", "+", "\n", "'that doesn\\'t use the theta lookahead.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--backprop_dt'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Allow backpropagation through \"delta theta\" in '", "+", "\n", "'the regularizer.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_args.special_train_options": [[227, 250], ["agroup.add_argument", "agroup.add_argument", "agroup.add_argument"], "function", ["None"], ["", "def", "special_train_options", "(", "agroup", ")", ":", "\n", "    ", "\"\"\"This is a helper function of the function `parse_cmd_arguments` to add\n    arguments to the `training` argument group.\n\n    Args:\n        agroup: The argument group returned by method\n            :func:`utils.cli_args.train_args`.\n    \"\"\"", "\n", "agroup", ".", "add_argument", "(", "'--plateau_lr_scheduler'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Will enable the usage of the learning rate '", "+", "\n", "'scheduler torch.optim.lr_scheduler.'", "+", "\n", "'ReduceLROnPlateau. Note, this option requires '", "+", "\n", "'that the argument \"epochs\" has been set.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--lambda_lr_scheduler'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Will enable the usage of the learning rate '", "+", "\n", "'scheduler torch.optim.lr_scheduler.'", "+", "\n", "'LambdaLR. Note, this option requires '", "+", "\n", "'that the argument \"epochs\" has been set. '", "+", "\n", "'The scheduler will behave as specified by '", "+", "\n", "'the function \"lr_schedule\" in '", "+", "\n", "'https://keras.io/examples/cifar10_resnet/.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--soft_targets'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use soft targets for classification.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.sa_hyper_model.SAHnetPart.__init__": [[109, 334], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "utils.module_wrappers.CLHyperNetInterface.__init__", "numpy.prod", "num_filters.append", "print", "range", "enumerate", "numpy.prod", "theta_shapes_internal.extend", "print", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "range", "numpy.sum", "print", "sa_hyper_model.SAHnetPart._is_properly_setup", "NotImplementedError", "NotImplementedError", "numpy.power().tolist", "isinstance", "len", "enumerate", "sa_hyper_model.SAHnetPart.__init__.compute_pads"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup"], ["def", "__init__", "(", "self", ",", "out_size", ",", "num_layers", ",", "num_filters", ",", "kernel_size", ",", "sa_units", ",", "\n", "input_dim", ",", "use_batch_norm", ",", "use_spectral_norm", ",", "no_theta", ",", "\n", "init_theta", ")", ":", "\n", "# FIXME find a way using super to handle multiple inheritence.", "\n", "#super(SAHnetPart, self).__init__()", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "CLHyperNetInterface", ".", "__init__", "(", "self", ")", "\n", "\n", "assert", "(", "init_theta", "is", "None", "or", "not", "no_theta", ")", "\n", "\n", "if", "use_spectral_norm", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Spectral normalization not yet '", "+", "\n", "'implemented for this hypernetwork type.'", ")", "\n", "", "if", "use_batch_norm", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Batch normalization not yet '", "+", "\n", "'implemented for this hypernetwork type.'", ")", "\n", "\n", "# FIXME task embeddings are currently maintained outside of this class.", "\n", "", "self", ".", "_target_shapes", "=", "out_size", "\n", "self", ".", "_task_embs", "=", "None", "\n", "self", ".", "_size_ext_input", "=", "input_dim", "\n", "self", ".", "_num_outputs", "=", "np", ".", "prod", "(", "out_size", ")", "\n", "\n", "if", "sa_units", "is", "None", ":", "\n", "            ", "sa_units", "=", "[", "]", "\n", "\n", "", "self", ".", "_sa_units_inds", "=", "sa_units", "\n", "self", ".", "_use_batch_norm", "=", "use_batch_norm", "\n", "\n", "assert", "(", "num_layers", ">", "0", ")", "# Initial fully-connected layer must exist.", "\n", "assert", "(", "num_filters", "is", "None", "or", "len", "(", "num_filters", ")", "==", "num_layers", "-", "1", ")", "\n", "assert", "(", "len", "(", "out_size", ")", "==", "2", "or", "len", "(", "out_size", ")", "==", "3", ")", "\n", "#assert(num_layers-1 not in sa_units)", "\n", "assert", "(", "len", "(", "sa_units", ")", "==", "0", "or", "np", ".", "max", "(", "sa_units", ")", "<", "num_layers", "-", "1", ")", "\n", "\n", "out_channels", "=", "1", "if", "len", "(", "out_size", ")", "==", "2", "else", "out_size", "[", "2", "]", "\n", "\n", "if", "num_filters", "is", "None", ":", "\n", "            ", "num_filters", "=", "[", "128", "]", "*", "(", "num_layers", "-", "1", ")", "\n", "multipliers", "=", "np", ".", "power", "(", "2", ",", "range", "(", "num_layers", "-", "2", ",", "-", "1", ",", "-", "1", ")", ")", ".", "tolist", "(", ")", "\n", "num_filters", "=", "[", "e1", "*", "e2", "for", "e1", ",", "e2", "in", "zip", "(", "num_filters", ",", "multipliers", ")", "]", "\n", "", "num_filters", ".", "append", "(", "out_channels", ")", "\n", "\n", "if", "kernel_size", "is", "None", ":", "\n", "            ", "kernel_size", "=", "5", "\n", "", "if", "not", "isinstance", "(", "kernel_size", ",", "list", ")", ":", "\n", "            ", "kernel_size", "=", "[", "kernel_size", ",", "kernel_size", "]", "\n", "", "if", "len", "(", "kernel_size", ")", "==", "2", ":", "\n", "            ", "kernel_size", "=", "[", "kernel_size", "]", "*", "(", "num_layers", "-", "1", ")", "\n", "", "else", ":", "\n", "            ", "for", "i", ",", "tup", "in", "enumerate", "(", "kernel_size", ")", ":", "\n", "                ", "if", "not", "isinstance", "(", "tup", ",", "list", ")", ":", "\n", "                    ", "kernel_size", "[", "i", "]", "=", "[", "tup", ",", "tup", "]", "\n", "\n", "", "", "", "print", "(", "'Building a self-attention generator with %d layers and an '", "%", "(", "num_layers", ")", "+", "'output shape of %s.'", "%", "str", "(", "out_size", ")", ")", "\n", "\n", "### Compute strides and pads of all transpose conv layers.", "\n", "# Keep in mind the formula:", "\n", "# W_o = S * (W_i - 1) - 2 * P + K + P_o", "\n", "# S - Strides", "\n", "# P - Padding", "\n", "# P_o - Output padding", "\n", "# K - Kernel size", "\n", "strides", "=", "[", "[", "2", ",", "2", "]", "for", "_", "in", "range", "(", "num_layers", "-", "1", ")", "]", "\n", "pads", "=", "[", "[", "0", ",", "0", "]", "for", "_", "in", "range", "(", "num_layers", "-", "1", ")", "]", "\n", "out_pads", "=", "[", "[", "0", ",", "0", "]", "for", "_", "in", "range", "(", "num_layers", "-", "1", ")", "]", "\n", "# Layer sizes.", "\n", "sizes", "=", "[", "[", "out_size", "[", "0", "]", ",", "out_size", "[", "1", "]", "]", "]", "*", "(", "num_layers", "-", "1", ")", "\n", "\n", "w", "=", "out_size", "[", "0", "]", "\n", "h", "=", "out_size", "[", "1", "]", "\n", "\n", "def", "compute_pads", "(", "w", ",", "k", ",", "s", ")", ":", "\n", "            ", "\"\"\"Compute paddings. Given the equation\n                W_o = S * (W_i - 1) - 2 * P + K + P_o\n            Paddings and output paddings are chosen such that it holds:\n                W_o = S * W_i\n\n            Args:\n                w: Size of output dimension.\n                k: Kernel size.\n                s: Stride.\n\n            Returns:\n                Padding, output padding.\n            \"\"\"", "\n", "offset", "=", "s", "\n", "if", "s", "==", "2", "and", "(", "w", "%", "2", ")", "==", "1", ":", "\n", "                ", "offset", "=", "3", "\n", "", "if", "(", "(", "k", "-", "offset", ")", "%", "2", ")", "==", "0", ":", "\n", "                ", "p", "=", "(", "k", "-", "offset", ")", "//", "2", "\n", "p_out", "=", "0", "\n", "", "else", ":", "\n", "                ", "p", "=", "int", "(", "np", ".", "ceil", "(", "(", "k", "-", "offset", ")", "/", "2", ")", ")", "\n", "p_out", "=", "-", "(", "k", "-", "offset", "-", "2", "*", "p", ")", "\n", "\n", "", "return", "p", ",", "p_out", "\n", "\n", "", "for", "i", "in", "range", "(", "num_layers", "-", "2", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "sizes", "[", "i", "]", "=", "[", "w", ",", "h", "]", "\n", "\n", "# This is a condition we set.", "\n", "# If one of the sizes is too small, we just keep the layer size.", "\n", "if", "w", "<=", "4", ":", "\n", "                ", "strides", "[", "i", "]", "[", "0", "]", "=", "1", "\n", "", "if", "h", "<=", "4", ":", "\n", "                ", "strides", "[", "i", "]", "[", "1", "]", "=", "1", "\n", "\n", "", "pads", "[", "i", "]", "[", "0", "]", ",", "out_pads", "[", "i", "]", "[", "0", "]", "=", "compute_pads", "(", "w", ",", "kernel_size", "[", "i", "]", "[", "0", "]", ",", "\n", "strides", "[", "i", "]", "[", "0", "]", ")", "\n", "pads", "[", "i", "]", "[", "1", "]", ",", "out_pads", "[", "i", "]", "[", "1", "]", "=", "compute_pads", "(", "h", ",", "kernel_size", "[", "i", "]", "[", "1", "]", ",", "\n", "strides", "[", "i", "]", "[", "1", "]", ")", "\n", "\n", "w", "=", "w", "if", "strides", "[", "i", "]", "[", "0", "]", "==", "1", "else", "w", "//", "2", "\n", "h", "=", "h", "if", "strides", "[", "i", "]", "[", "1", "]", "==", "1", "else", "h", "//", "2", "\n", "\n", "", "self", ".", "_fc_out_shape", "=", "[", "num_filters", "[", "0", "]", ",", "w", ",", "h", "]", "\n", "if", "num_layers", ">", "1", ":", "\n", "            ", "num_filters", "=", "num_filters", "[", "1", ":", "]", "\n", "\n", "# Just a sanity check.", "\n", "", "for", "i", ",", "s", "in", "enumerate", "(", "strides", ")", ":", "\n", "            ", "w", "=", "s", "[", "0", "]", "*", "(", "w", "-", "1", ")", "+", "kernel_size", "[", "i", "]", "[", "0", "]", "-", "2", "*", "pads", "[", "i", "]", "[", "0", "]", "+", "out_pads", "[", "i", "]", "[", "0", "]", "\n", "h", "=", "s", "[", "1", "]", "*", "(", "h", "-", "1", ")", "+", "kernel_size", "[", "i", "]", "[", "1", "]", "-", "2", "*", "pads", "[", "i", "]", "[", "1", "]", "+", "out_pads", "[", "i", "]", "[", "1", "]", "\n", "", "assert", "(", "w", "==", "out_size", "[", "0", "]", "and", "h", "==", "out_size", "[", "1", "]", ")", "\n", "\n", "# For shapes of self-maintained parameters (underlying modules, like", "\n", "# self-attention layers, maintain their own weights).", "\n", "theta_shapes_internal", "=", "[", "]", "\n", "if", "no_theta", ":", "\n", "            ", "self", ".", "_theta", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "_theta", "=", "nn", ".", "ParameterList", "(", ")", "\n", "\n", "if", "init_theta", "is", "not", "None", "and", "len", "(", "sa_units", ")", ">", "0", ":", "\n", "                ", "num_p", "=", "7", "# Number of param tensors per self-attention layer.", "\n", "num_sa_p", "=", "len", "(", "sa_units", ")", "*", "num_p", "\n", "\n", "sind", "=", "len", "(", "init_theta", ")", "-", "num_sa_p", "\n", "\n", "sa_init_weights", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "sa_units", ")", ")", ":", "\n", "                    ", "sa_init_weights", ".", "append", "(", "init_theta", "[", "sind", "+", "i", "*", "num_p", ":", "sind", "+", "(", "i", "+", "1", ")", "*", "num_p", "]", ")", "\n", "\n", "", "init_theta", "=", "init_theta", "[", ":", "sind", "]", "\n", "\n", "### Initial fully-connected layer.", "\n", "", "", "num_units", "=", "np", ".", "prod", "(", "self", ".", "_fc_out_shape", ")", "\n", "theta_shapes_internal", ".", "extend", "(", "[", "[", "num_units", ",", "input_dim", "]", ",", "[", "num_units", "]", "]", ")", "\n", "\n", "print", "(", "'The output shape of the fully-connected layer will be %s'", "%", "\n", "(", "str", "(", "self", ".", "_fc_out_shape", ")", ")", ")", "\n", "\n", "### Transpose Convolutional Layers.", "\n", "self", ".", "_sa_units", "=", "torch", ".", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "prev_nfilters", "=", "self", ".", "_fc_out_shape", "[", "0", "]", "\n", "\n", "sa_ind", "=", "0", "\n", "if", "0", "in", "sa_units", ":", "\n", "            ", "print", "(", "'A self-attention unit is added after the initial fc layer.'", ")", "\n", "w_init", "=", "None", "\n", "if", "init_theta", "is", "not", "None", ":", "\n", "                ", "w_init", "=", "sa_init_weights", "[", "sa_ind", "]", "\n", "", "self", ".", "_sa_units", ".", "append", "(", "SelfAttnLayerV2", "(", "prev_nfilters", ",", "\n", "use_spectral_norm", ",", "no_weights", "=", "no_theta", ",", "init_weights", "=", "w_init", ")", ")", "\n", "\n", "sa_ind", "+=", "1", "\n", "\n", "# Needed to setup transpose convolutional layers in forward method.", "\n", "", "self", ".", "_strides", "=", "strides", "\n", "self", ".", "_pads", "=", "pads", "\n", "self", ".", "_out_pads", "=", "out_pads", "\n", "\n", "for", "i", "in", "range", "(", "num_layers", "-", "1", ")", ":", "\n", "            ", "theta_shapes_internal", ".", "extend", "(", "[", "\n", "[", "prev_nfilters", ",", "num_filters", "[", "i", "]", ",", "*", "kernel_size", "[", "i", "]", "]", ",", "\n", "[", "num_filters", "[", "i", "]", "]", "\n", "]", ")", "\n", "prev_nfilters", "=", "num_filters", "[", "i", "]", "\n", "\n", "msg", "=", "'Transpose convolutional layer %d will have output '", "+", "'shape %s. It uses strides=%s, padding=%s and '", "'output_padding=%s. The kernel size is %s.'", "\n", "print", "(", "msg", "%", "(", "i", ",", "str", "(", "[", "num_filters", "[", "i", "]", ",", "*", "sizes", "[", "i", "]", "]", ")", ",", "str", "(", "strides", "[", "i", "]", ")", ",", "\n", "str", "(", "pads", "[", "i", "]", ")", ",", "str", "(", "out_pads", "[", "i", "]", ")", ",", "str", "(", "kernel_size", "[", "i", "]", ")", ")", ")", "\n", "\n", "if", "(", "i", "+", "1", ")", "in", "sa_units", ":", "\n", "                ", "print", "(", "'A self-attention unit is added after transpose conv '", "+", "'layer %d.'", "%", "i", ")", "\n", "w_init", "=", "None", "\n", "if", "init_theta", "is", "not", "None", ":", "\n", "                    ", "w_init", "=", "sa_init_weights", "[", "sa_ind", "]", "\n", "", "self", ".", "_sa_units", ".", "append", "(", "SelfAttnLayerV2", "(", "num_filters", "[", "i", "]", ",", "\n", "use_spectral_norm", ",", "no_weights", "=", "no_theta", ",", "\n", "init_weights", "=", "w_init", ")", ")", "\n", "\n", "sa_ind", "+=", "1", "\n", "\n", "", "", "if", "not", "no_theta", ":", "\n", "            ", "for", "i", ",", "dims", "in", "enumerate", "(", "theta_shapes_internal", ")", ":", "\n", "                ", "self", ".", "_theta", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "*", "dims", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "\n", "", "if", "init_theta", "is", "not", "None", ":", "\n", "                ", "assert", "(", "len", "(", "init_theta", ")", "==", "len", "(", "theta_shapes_internal", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "init_theta", ")", ")", ":", "\n", "                    ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_theta", "[", "i", "]", ".", "shape", ")", ",", "\n", "list", "(", "self", ".", "_theta", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "self", ".", "_theta", "[", "i", "]", ".", "data", "=", "init_theta", "[", "i", "]", "\n", "", "", "else", ":", "\n", "                ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "self", ".", "_theta", ")", ",", "2", ")", ":", "\n", "                    ", "init_params", "(", "self", ".", "_theta", "[", "i", "]", ",", "self", ".", "_theta", "[", "i", "+", "1", "]", ")", "\n", "\n", "", "", "", "self", ".", "_theta_shapes", "=", "theta_shapes_internal", "\n", "for", "unit", "in", "self", ".", "_sa_units", ":", "\n", "            ", "self", ".", "_theta_shapes", ".", "extend", "(", "unit", ".", "weight_shapes", ")", "\n", "\n", "", "self", ".", "_num_weights", "=", "np", ".", "sum", "(", "[", "np", ".", "prod", "(", "s", ")", "for", "s", "in", "self", ".", "_theta_shapes", "]", ")", "\n", "print", "(", "'Total number of parameters in the self-attention generator: %d'", "%", "\n", "self", ".", "_num_weights", ")", "\n", "\n", "self", ".", "_is_properly_setup", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.sa_hyper_model.SAHnetPart.forward": [[336, 425], ["torch.relu", "torch.relu", "torch.relu", "sa_hyper_model.SAHnetPart.view", "len", "range", "Exception", "ValueError", "Exception", "enumerate", "len", "len", "range", "enumerate", "torch.linear", "torch.linear", "torch.linear", "NotImplementedError", "sa_hyper_model.SAHnetPart._sa_units[].forward", "torch.conv_transpose2d", "torch.conv_transpose2d", "torch.conv_transpose2d", "len", "len", "numpy.all", "len", "len", "len", "len", "len", "sa_weights.append", "weights.append", "torch.relu", "torch.relu", "torch.relu", "sa_hyper_model.SAHnetPart._sa_units[].forward", "numpy.equal", "sa_dWeights.append", "sa_dWeights.append", "NotImplementedError", "list"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "forward", "(", "self", ",", "task_id", "=", "None", ",", "theta", "=", "None", ",", "dTheta", "=", "None", ",", "task_emb", "=", "None", ",", "\n", "ext_inputs", "=", "None", ",", "squeeze", "=", "True", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract super class method.\n        \n        Note, we currently assume that task embeddings have been concatenated\n        to ``ext_inputs`` as this class doesn't maintain any class embeddings!\n        \"\"\"", "\n", "if", "task_id", "is", "not", "None", "or", "task_emb", "is", "not", "None", ":", "\n", "            ", "raise", "Exception", "(", "'This hypernet does not support task embeddings, '", "+", "\n", "'please concatenate them to the external input.'", ")", "\n", "\n", "", "if", "ext_inputs", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'This hypernet type always expects an external '", "+", "\n", "'input (\"ext_inputs\" must be set).'", ")", "\n", "\n", "", "if", "not", "self", ".", "has_theta", "and", "theta", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Network was generated without internal weights. '", "+", "\n", "'Hence, \"theta\" option may not be None.'", ")", "\n", "\n", "", "if", "theta", "is", "None", ":", "\n", "            ", "theta", "=", "self", ".", "theta", "\n", "", "else", ":", "\n", "            ", "assert", "(", "len", "(", "theta", ")", "==", "len", "(", "self", ".", "theta_shapes", ")", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "self", ".", "theta_shapes", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "s", ",", "list", "(", "theta", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "", "", "if", "dTheta", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "dTheta", ")", "==", "len", "(", "self", ".", "theta_shapes", ")", ")", "\n", "\n", "", "if", "len", "(", "self", ".", "_sa_units", ")", ">", "0", ":", "\n", "            ", "num_p", "=", "len", "(", "self", ".", "_sa_units", "[", "0", "]", ".", "weight_shapes", ")", "\n", "num_sa_p", "=", "len", "(", "self", ".", "_sa_units", ")", "*", "num_p", "\n", "\n", "sind", "=", "len", "(", "theta", ")", "-", "num_sa_p", "\n", "\n", "sa_weights", "=", "[", "]", "\n", "sa_dWeights", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_sa_units", ")", ")", ":", "\n", "                ", "sa_weights", ".", "append", "(", "theta", "[", "sind", "+", "i", "*", "num_p", ":", "sind", "+", "(", "i", "+", "1", ")", "*", "num_p", "]", ")", "\n", "if", "dTheta", "is", "not", "None", ":", "\n", "                    ", "sa_dWeights", ".", "append", "(", "dTheta", "[", "sind", "+", "i", "*", "num_p", ":", "sind", "+", "(", "i", "+", "1", ")", "*", "num_p", "]", ")", "\n", "", "else", ":", "\n", "                    ", "sa_dWeights", ".", "append", "(", "None", ")", "\n", "\n", "", "", "theta", "=", "theta", "[", ":", "sind", "]", "\n", "if", "dTheta", "is", "not", "None", ":", "\n", "                ", "dTheta", "=", "dTheta", "[", ":", "sind", "]", "\n", "\n", "", "", "if", "dTheta", "is", "not", "None", ":", "\n", "            ", "weights", "=", "[", "]", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "theta", ")", ":", "\n", "                ", "weights", ".", "append", "(", "t", "+", "dTheta", "[", "i", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "weights", "=", "theta", "\n", "\n", "### Initial fully-connected layer.", "\n", "", "h", "=", "ext_inputs", "\n", "h", "=", "F", ".", "relu", "(", "F", ".", "linear", "(", "h", ",", "weights", "[", "0", "]", ",", "bias", "=", "weights", "[", "1", "]", ")", ")", "\n", "if", "self", ".", "_use_batch_norm", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "#h = F.batch_norm(h, bn_stats[ii], bn_stats[ii+1],", "\n", "#                 weight=bn_weights[ii], bias=bn_weights[ii+1],", "\n", "#                 training=self.training)", "\n", "", "h", "=", "h", ".", "view", "(", "[", "-", "1", ",", "*", "self", ".", "_fc_out_shape", "]", ")", "\n", "\n", "### Transpose Convolutional Layers.", "\n", "sa_ind", "=", "0", "\n", "if", "0", "in", "self", ".", "_sa_units_inds", ":", "\n", "            ", "h", "=", "self", ".", "_sa_units", "[", "sa_ind", "]", ".", "forward", "(", "h", ",", "weights", "=", "sa_weights", "[", "sa_ind", "]", ",", "\n", "dWeights", "=", "sa_dWeights", "[", "sa_ind", "]", ")", "\n", "sa_ind", "+=", "1", "\n", "\n", "", "num_tc_layers", "=", "len", "(", "self", ".", "_strides", ")", "\n", "for", "i", "in", "range", "(", "num_tc_layers", ")", ":", "\n", "            ", "h", "=", "F", ".", "conv_transpose2d", "(", "h", ",", "weights", "[", "2", "+", "2", "*", "i", "]", ",", "bias", "=", "weights", "[", "3", "+", "2", "*", "i", "]", ",", "\n", "stride", "=", "self", ".", "_strides", "[", "i", "]", ",", "padding", "=", "self", ".", "_pads", "[", "i", "]", ",", "\n", "output_padding", "=", "self", ".", "_out_pads", "[", "i", "]", ")", "\n", "# No activation function and no batchnorm in the last layer.", "\n", "if", "i", "<", "num_tc_layers", "-", "1", ":", "\n", "                ", "h", "=", "F", ".", "relu", "(", "h", ")", "\n", "if", "self", ".", "_use_batch_norm", ":", "\n", "                    ", "raise", "NotImplementedError", "(", ")", "\n", "\n", "", "", "if", "(", "i", "+", "1", ")", "in", "self", ".", "_sa_units_inds", ":", "\n", "                ", "h", "=", "self", ".", "_sa_units", "[", "sa_ind", "]", ".", "forward", "(", "h", ",", "\n", "weights", "=", "sa_weights", "[", "sa_ind", "]", ",", "dWeights", "=", "sa_dWeights", "[", "sa_ind", "]", ")", "\n", "sa_ind", "+=", "1", "\n", "\n", "", "", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.sa_hyper_model.SAHnetPart.theta": [[427, 445], ["torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList.extend", "torch.ParameterList.extend"], "methods", ["None"], ["", "@", "property", "\n", "def", "theta", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute ``theta``.\n\n        Returns:\n            A :class:`torch.nn.ParameterList` or ``None``, if this network has\n            no weights.\n        \"\"\"", "\n", "if", "self", ".", "_theta", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "ret", "=", "nn", ".", "ParameterList", "(", ")", "\n", "ret", ".", "extend", "(", "self", ".", "_theta", ")", "\n", "# Self-attention units need to be appended to the parameter list.", "\n", "for", "unit", "in", "self", ".", "_sa_units", ":", "\n", "            ", "ret", ".", "extend", "(", "unit", ".", "weights", ")", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.sa_hyper_model.SAHyperNetwork.__init__": [[542, 647], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "utils.module_wrappers.CLHyperNetInterface.__init__", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "print", "numpy.prod", "sa_hyper_model.SAHnetPart", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "range", "list", "print", "NotImplementedError", "len", "print", "print", "toy_example.hyper_model.HyperNetwork", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "sa_hyper_model.SAHyperNetwork._task_embs.append", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "sa_hyper_model.SAHyperNetwork.parameters", "numpy.prod", "print", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights"], ["def", "__init__", "(", "self", ",", "main_dims", ",", "num_tasks", ",", "out_size", "=", "[", "64", ",", "64", "]", ",", "num_layers", "=", "5", ",", "\n", "num_filters", "=", "None", ",", "kernel_size", "=", "5", ",", "sa_units", "=", "[", "1", ",", "3", "]", ",", "\n", "rem_layers", "=", "[", "50", ",", "50", ",", "50", "]", ",", "te_dim", "=", "8", ",", "ce_dim", "=", "8", ",", "\n", "no_theta", "=", "False", ",", "init_theta", "=", "None", ",", "use_batch_norm", "=", "False", ",", "\n", "use_spectral_norm", "=", "False", ",", "dropout_rate", "=", "-", "1", ",", "\n", "discard_remainder", "=", "False", ",", "noise_dim", "=", "-", "1", ",", "temb_std", "=", "-", "1", ")", ":", "\n", "# FIXME find a way using super to handle multiple inheritence.", "\n", "#super(SAHyperNetwork, self).__init__()", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "CLHyperNetInterface", ".", "__init__", "(", "self", ")", "\n", "\n", "if", "init_theta", "is", "not", "None", ":", "\n", "# FIXME I would need to know the number of parameter tensors in each", "\n", "# hypernet before creating them to split the list init_theta.", "\n", "            ", "raise", "NotImplementedError", "(", "'Argument \"init_theta\" not implemented '", "+", "\n", "'yet!'", ")", "\n", "\n", "", "assert", "(", "init_theta", "is", "None", "or", "no_theta", "is", "False", ")", "\n", "self", ".", "_no_theta", "=", "no_theta", "\n", "self", ".", "_te_dim", "=", "te_dim", "\n", "self", ".", "_discard_remainder", "=", "discard_remainder", "\n", "\n", "self", ".", "_target_shapes", "=", "main_dims", "\n", "self", ".", "_num_outputs", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "main_dims", ")", "\n", "print", "(", "'Building a self-attention hypernet for a network with %d '", "%", "self", ".", "_num_outputs", "+", "'weights.'", ")", "\n", "assert", "(", "len", "(", "out_size", ")", "in", "[", "2", ",", "3", "]", ")", "\n", "self", ".", "_out_size", "=", "out_size", "\n", "num_outs", "=", "np", ".", "prod", "(", "out_size", ")", "\n", "assert", "(", "num_outs", "<=", "self", ".", "_num_outputs", ")", "\n", "self", ".", "_noise_dim", "=", "noise_dim", "\n", "self", ".", "_temb_std", "=", "temb_std", "\n", "\n", "num_embs", "=", "self", ".", "_num_outputs", "//", "num_outs", "\n", "rem_weights", "=", "self", ".", "_num_outputs", "%", "num_outs", "\n", "\n", "if", "rem_weights", ">", "0", "and", "not", "discard_remainder", ":", "\n", "            ", "print", "(", "'%d remaining weights (%.2f%%) are generated by a fully-'", "%", "(", "rem_weights", ",", "100.0", "*", "rem_weights", "/", "self", ".", "_num_outputs", ")", "+", "'connected hypernetwork.'", ")", "\n", "", "elif", "rem_weights", ">", "0", ":", "\n", "            ", "num_embs", "+=", "1", "\n", "\n", "print", "(", "'%d weights generated by the last chunk of the self-'", "\n", "%", "(", "num_outs", "-", "rem_weights", ")", "+", "'attention hypernet will be '", "+", "\n", "'discarded.'", ")", "\n", "\n", "", "self", ".", "_num_embs", "=", "num_embs", "\n", "\n", "### Generate Hypernet.", "\n", "self", ".", "_hypernet", "=", "SAHnetPart", "(", "out_size", "=", "out_size", ",", "num_layers", "=", "num_layers", ",", "\n", "num_filters", "=", "num_filters", ",", "kernel_size", "=", "kernel_size", ",", "sa_units", "=", "sa_units", ",", "\n", "input_dim", "=", "te_dim", "+", "ce_dim", "+", "(", "noise_dim", "if", "noise_dim", "!=", "-", "1", "else", "0", ")", ",", "\n", "use_batch_norm", "=", "use_batch_norm", ",", "use_spectral_norm", "=", "use_spectral_norm", ",", "\n", "no_theta", "=", "no_theta", ",", "init_theta", "=", "None", ")", "\n", "\n", "self", ".", "_rem_hypernet", "=", "None", "\n", "self", ".", "_remainder", "=", "rem_weights", "\n", "if", "rem_weights", ">", "0", "and", "not", "discard_remainder", ":", "\n", "            ", "print", "(", "'A second hypernet for the remainder of the weights has '", "+", "\n", "'to be created, as %d is not dividable by %d '", "%", "\n", "(", "self", ".", "_num_outputs", ",", "num_outs", ")", "+", "'(remaidner %d)'", "%", "\n", "rem_weights", ")", "\n", "self", ".", "_rem_hypernet", "=", "HyperNetwork", "(", "[", "[", "rem_weights", "]", "]", ",", "None", ",", "\n", "layers", "=", "rem_layers", ",", "te_dim", "=", "te_dim", ",", "no_te_embs", "=", "True", ",", "\n", "no_weights", "=", "no_theta", ",", "\n", "ce_dim", "=", "(", "noise_dim", "if", "noise_dim", "!=", "-", "1", "else", "None", ")", ",", "\n", "dropout_rate", "=", "dropout_rate", ",", "use_batch_norm", "=", "use_batch_norm", ",", "\n", "use_spectral_norm", "=", "use_spectral_norm", ",", "noise_dim", "=", "-", "1", ",", "\n", "temb_std", "=", "None", ")", "\n", "\n", "### Generate embeddings for all weight chunks.", "\n", "", "if", "no_theta", ":", "\n", "            ", "self", ".", "_embs", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "_embs", "=", "nn", ".", "Parameter", "(", "data", "=", "torch", ".", "Tensor", "(", "num_embs", ",", "ce_dim", ")", ",", "\n", "requires_grad", "=", "True", ")", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "self", ".", "_embs", ",", "mean", "=", "0.", ",", "std", "=", "1.", ")", "\n", "\n", "# There is no need for a chunk embedding, as this network always", "\n", "# produces the same chunk.", "\n", "#if self._remainder > 0  and not discard_remainder:", "\n", "#    self._rem_emb = nn.Parameter(data=torch.Tensor(1, ce_dim),", "\n", "#                                 requires_grad=True)", "\n", "#    torch.nn.init.normal_(self._rem_emb, mean=0., std=1.)", "\n", "\n", "### Generate task embeddings.", "\n", "", "self", ".", "_task_embs", "=", "nn", ".", "ParameterList", "(", ")", "\n", "# We store individual task embeddings as it makes it easier to pass", "\n", "# only subsets of task embeddings to an optimizer.", "\n", "for", "_", "in", "range", "(", "num_tasks", ")", ":", "\n", "            ", "self", ".", "_task_embs", ".", "append", "(", "nn", ".", "Parameter", "(", "data", "=", "torch", ".", "Tensor", "(", "te_dim", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "self", ".", "_task_embs", "[", "-", "1", "]", ",", "mean", "=", "0.", ",", "std", "=", "1.", ")", "\n", "\n", "", "self", ".", "_num_weights", "=", "0", "\n", "for", "p", "in", "list", "(", "self", ".", "parameters", "(", ")", ")", ":", "\n", "            ", "self", ".", "_num_weights", "+=", "np", ".", "prod", "(", "p", ".", "shape", ")", "\n", "", "print", "(", "'Total number of parameters in the hypernetwork: %d'", "%", "\n", "self", ".", "_num_weights", ")", "\n", "\n", "self", ".", "_theta_shapes", "=", "[", "[", "num_embs", ",", "ce_dim", "]", "]", "+", "self", ".", "_hypernet", ".", "theta_shapes", "\n", "if", "self", ".", "_rem_hypernet", "is", "not", "None", ":", "\n", "            ", "self", ".", "_theta_shapes", "+=", "self", ".", "_rem_hypernet", ".", "theta_shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.sa_hyper_model.SAHyperNetwork.forward": [[649, 763], ["len", "sa_hyper_model.SAHyperNetwork._hypernet.forward", "torch.cat.view", "torch.cat.view", "torch.cat.view", "Exception", "Exception", "NotImplementedError", "numpy.all", "task_emb.add", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.expand", "torch.cat.expand", "torch.cat.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "task_emb.view().expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "sa_hyper_model.SAHyperNetwork._rem_hypernet.forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "int", "ret.append", "len", "len", "numpy.equal", "len", "len", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "eps.to.to.to", "numpy.prod", "W.view", "list", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "sa_hyper_model.SAHyperNetwork._embs.get_device", "task_emb.view", "task_emb.view", "rem_weights[].view"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "", "def", "forward", "(", "self", ",", "task_id", "=", "None", ",", "theta", "=", "None", ",", "dTheta", "=", "None", ",", "task_emb", "=", "None", ",", "\n", "ext_inputs", "=", "None", ",", "squeeze", "=", "True", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract super class method.\n\n        Note, this methods can't handle external inputs yet!\n\n        The method will iterate through the set of internal chunk embeddings,\n        calling the internally maintained transpose conv. hypernetwork\n        (potentially with self-attention layers). If necessary, a small portion\n        of the chunks will be created by an additional fully-connected network.\n        \"\"\"", "\n", "if", "task_id", "is", "None", "and", "task_emb", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'The hyper network has to get either a task ID'", "+", "\n", "'to choose the learned embedding or directly '", "+", "\n", "'get an embedding as input (e.g. from a task '", "+", "\n", "'recognition model).'", ")", "\n", "\n", "", "if", "not", "self", ".", "has_theta", "and", "theta", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Network was generated without internal weights. '", "+", "\n", "'Hence, \"theta\" option may not be None.'", ")", "\n", "\n", "", "if", "ext_inputs", "is", "not", "None", ":", "\n", "# FIXME If this will be implemented, please consider:", "\n", "# * batch size will have to be multiplied based on num chunk", "\n", "#   embeddings and the number of external inputs -> large batches", "\n", "# * noise dim must adhere correct behavior (different noise per", "\n", "#   external input).", "\n", "            ", "raise", "NotImplementedError", "(", "'This hypernetwork implementation does '", "+", "\n", "'not yet support the passing of external inputs.'", ")", "\n", "\n", "", "if", "theta", "is", "None", ":", "\n", "            ", "theta", "=", "self", ".", "theta", "\n", "", "else", ":", "\n", "            ", "assert", "(", "len", "(", "theta", ")", "==", "len", "(", "self", ".", "theta_shapes", ")", ")", "\n", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "self", ".", "_embs", ".", "shape", ",", "list", "(", "theta", "[", "0", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "", "nhnet_shapes", "=", "len", "(", "self", ".", "_hypernet", ".", "theta_shapes", ")", "\n", "\n", "chunk_embs", "=", "theta", "[", "0", "]", "\n", "hnet_theta", "=", "theta", "[", "1", ":", "1", "+", "nhnet_shapes", "]", "\n", "if", "self", ".", "_rem_hypernet", "is", "not", "None", ":", "\n", "            ", "rem_hnet_theta", "=", "theta", "[", "1", "+", "nhnet_shapes", ":", "]", "\n", "\n", "", "if", "dTheta", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "dTheta", ")", "==", "len", "(", "self", ".", "theta_shapes", ")", ")", "\n", "\n", "chunk_embs", "=", "chunk_embs", "+", "dTheta", "[", "0", "]", "\n", "hnet_dTheta", "=", "dTheta", "[", "1", ":", "1", "+", "nhnet_shapes", "]", "\n", "if", "self", ".", "_rem_hypernet", "is", "not", "None", ":", "\n", "                ", "rem_hnet_dTheta", "=", "dTheta", "[", "1", "+", "nhnet_shapes", ":", "]", "\n", "", "", "else", ":", "\n", "            ", "hnet_dTheta", "=", "None", "\n", "rem_hnet_dTheta", "=", "None", "\n", "\n", "# Currently, there is no option in the constructor to not generate", "\n", "# task embeddings, that is why the code below is commented out.", "\n", "# Select task embeddings.", "\n", "#if not self.has_task_embs and task_emb is None:", "\n", "#    raise Exception('The network was created with no internal task ' +", "\n", "#                    'embeddings, thus parameter \"task_emb\" has to ' +", "\n", "#                    'be specified.')", "\n", "\n", "", "if", "task_emb", "is", "None", ":", "\n", "            ", "task_emb", "=", "self", ".", "_task_embs", "[", "task_id", "]", "\n", "", "if", "self", ".", "training", "and", "self", ".", "_temb_std", "!=", "-", "1", ":", "\n", "            ", "task_emb", ".", "add", "(", "torch", ".", "randn_like", "(", "task_emb", ")", "*", "self", ".", "_temb_std", ")", "\n", "\n", "# Concatenate the same noise to all chunks, such that it can be", "\n", "# viewed as if it were an external input.", "\n", "", "if", "self", ".", "_noise_dim", "!=", "-", "1", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "eps", "=", "torch", ".", "randn", "(", "(", "1", ",", "self", ".", "_noise_dim", ")", ")", "\n", "", "else", ":", "\n", "                ", "eps", "=", "torch", ".", "zeros", "(", "(", "1", ",", "self", ".", "_noise_dim", ")", ")", "\n", "", "if", "self", ".", "_embs", ".", "is_cuda", ":", "\n", "                ", "eps", "=", "eps", ".", "to", "(", "self", ".", "_embs", ".", "get_device", "(", ")", ")", "\n", "\n", "# The hypernet input is a concatenation of the task embedding with", "\n", "# the noise vector and each chunk embedding.", "\n", "", "hnet_input", "=", "torch", ".", "cat", "(", "[", "task_emb", ".", "view", "(", "1", ",", "-", "1", ")", ",", "eps", "]", ",", "dim", "=", "1", ")", "\n", "hnet_input", "=", "hnet_input", ".", "expand", "(", "self", ".", "_num_embs", ",", "\n", "self", ".", "_te_dim", "+", "self", ".", "_noise_dim", ")", "\n", "hnet_input", "=", "torch", ".", "cat", "(", "[", "chunk_embs", ",", "hnet_input", "]", ",", "dim", "=", "1", ")", "\n", "\n", "", "else", ":", "\n", "            ", "eps", "=", "None", "\n", "\n", "# The hypernet input is a concatenation of the task embedding with", "\n", "# each chunk embedding.", "\n", "hnet_input", "=", "task_emb", ".", "view", "(", "1", ",", "-", "1", ")", ".", "expand", "(", "self", ".", "_num_embs", ",", "\n", "self", ".", "_te_dim", ")", "\n", "hnet_input", "=", "torch", ".", "cat", "(", "[", "chunk_embs", ",", "hnet_input", "]", ",", "dim", "=", "1", ")", "\n", "\n", "### Gather all generated weights.", "\n", "", "weights", "=", "self", ".", "_hypernet", ".", "forward", "(", "task_id", "=", "None", ",", "theta", "=", "hnet_theta", ",", "\n", "dTheta", "=", "hnet_dTheta", ",", "task_emb", "=", "None", ",", "ext_inputs", "=", "hnet_input", ")", "\n", "weights", "=", "weights", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "\n", "if", "self", ".", "_rem_hypernet", "is", "not", "None", ":", "\n", "            ", "rem_weights", "=", "self", ".", "_rem_hypernet", ".", "forward", "(", "theta", "=", "rem_hnet_theta", ",", "\n", "dTheta", "=", "rem_hnet_dTheta", ",", "task_emb", "=", "task_emb", ",", "ext_inputs", "=", "eps", ")", "\n", "weights", "=", "torch", ".", "cat", "(", "[", "weights", ",", "rem_weights", "[", "0", "]", ".", "view", "(", "1", ",", "-", "1", ")", "]", ",", "dim", "=", "1", ")", "\n", "\n", "### Reshape weights.", "\n", "", "ind", "=", "0", "\n", "ret", "=", "[", "]", "\n", "\n", "for", "s", "in", "self", ".", "target_shapes", ":", "\n", "            ", "num", "=", "int", "(", "np", ".", "prod", "(", "s", ")", ")", "\n", "W", "=", "weights", "[", "0", "]", "[", "ind", ":", "ind", "+", "num", "]", "\n", "ind", "+=", "num", "\n", "ret", ".", "append", "(", "W", ".", "view", "(", "*", "s", ")", ")", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.sa_hyper_model.SAHyperNetwork.chunk_embeddings": [[764, 773], ["list", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split"], "methods", ["None"], ["", "@", "property", "\n", "def", "chunk_embeddings", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute chunk_embeddings.\n\n    Returns:\n        A list of all chunk embedding vectors.\n    \"\"\"", "\n", "# Note, the remainder network has no chunk embedding.", "\n", "return", "list", "(", "torch", ".", "split", "(", "self", ".", "_embs", ",", "1", ",", "dim", "=", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.sa_hyper_model.SAHyperNetwork.theta": [[775, 797], ["list", "list"], "methods", ["None"], ["", "@", "property", "\n", "def", "theta", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute ``theta``.\n\n        Theta are all learnable parameters of the chunked hypernet including\n        the chunk embeddings that need to be learned.\n        Not included are the task embeddings.\n\n        .. note::\n            Chunk embeddings are prepended to the list of weights ``theta`` from\n            the internal SA hypernetwork (if existing, ``theta`` from the\n            remainder network will be appended).\n\n        Returns:\n            A list of tensors or ``None``, if ``no_theta`` was set to ``True``\n            in the constructor of this class.\n        \"\"\"", "\n", "theta", "=", "[", "self", ".", "_embs", "]", "+", "list", "(", "self", ".", "_hypernet", ".", "theta", ")", "\n", "if", "self", ".", "_rem_hypernet", "is", "not", "None", ":", "\n", "            ", "theta", "+=", "list", "(", "self", ".", "_rem_hypernet", ".", "theta", ")", "\n", "\n", "", "return", "theta", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.sa_hyper_model.SAHyperNetwork.has_theta": [[799, 803], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_theta", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute ``has_theta``.\"\"\"", "\n", "return", "not", "self", ".", "_no_theta", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.load_datasets": [[41, 77], ["logger.info", "data.special.split_cifar.get_split_cifar_handlers", "logger.info", "logger.info", "len"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_cifar.get_split_cifar_handlers"], ["def", "load_datasets", "(", "config", ",", "shared", ",", "logger", ",", "data_dir", "=", "'../datasets'", ")", ":", "\n", "    ", "\"\"\"Create a data handler per task.\n\n    Note:\n        Datasets are generated with targets being 1-hot encoded.\n\n    Args:\n        config (argparse.Namespace): Command-line arguments.\n        shared (argparse.Namespace): Object for sharing data between functions.\n            Contains the type of experiment.\n        logger: Logger object.\n        data_dir (str): From where to load (or to where to download) the\n            datasets?\n\n    Returns:\n        (list) A list of data handlers (i.e., objects of class\n        :class:`data.dataset.Dataset`.\n    \"\"\"", "\n", "augment_data", "=", "not", "config", ".", "disable_data_augmentation", "\n", "#if shared.experiment == 'zenke':", "\n", "#    augment_data = False", "\n", "#    # To be comparable to previous results. Note, Zenke et al. didn't", "\n", "#    # utilize any data augmentation as far as I know.", "\n", "#    logger.warning('Data augmentation disabled for Zenkenet.')", "\n", "if", "augment_data", ":", "\n", "        ", "logger", ".", "info", "(", "'Data augmentation will be used.'", ")", "\n", "\n", "", "assert", "(", "config", ".", "num_tasks", "<=", "11", ")", "\n", "logger", ".", "info", "(", "'Loading CIFAR datasets ...'", ")", "\n", "dhandlers", "=", "get_split_cifar_handlers", "(", "data_dir", ",", "use_one_hot", "=", "True", ",", "\n", "use_data_augmentation", "=", "augment_data", ",", "num_tasks", "=", "config", ".", "num_tasks", ")", "\n", "assert", "(", "len", "(", "dhandlers", ")", "==", "config", ".", "num_tasks", ")", "\n", "\n", "logger", ".", "info", "(", "'Loaded %d CIFAR task(s) into memory.'", "%", "config", ".", "num_tasks", ")", "\n", "\n", "return", "dhandlers", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.get_main_model": [[78, 120], ["logger.info", "utils.sim_utils.get_mnet_model", "train_utils.init_network_weights", "logger.info", "logger.info"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.sim_utils.get_mnet_model", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.init_network_weights"], ["", "def", "get_main_model", "(", "config", ",", "shared", ",", "logger", ",", "device", ",", "no_weights", "=", "False", ")", ":", "\n", "    ", "\"\"\"Helper function to generate the main network.\n\n    This function uses :func:`utils.sim_utils.get_mnet_model` to generate the\n    main network.\n\n    The function also takes care of weight initialization, if configured.\n\n    Args:\n        (....): See docstring of function :func:`load_datasets`.\n        device: The PyTorch device.\n        no_weights (bool): If ``True``, the main network is generated without\n            internal weights.\n\n    Returns:\n        The main network.\n    \"\"\"", "\n", "if", "shared", ".", "experiment", "==", "'zenke'", ":", "\n", "        ", "net_type", "=", "'zenke'", "\n", "logger", ".", "info", "(", "'Building a ZenkeNet ...'", ")", "\n", "\n", "", "else", ":", "\n", "        ", "net_type", "=", "'resnet'", "\n", "logger", ".", "info", "(", "'Building a ResNet ...'", ")", "\n", "\n", "", "num_outputs", "=", "10", "\n", "\n", "if", "config", ".", "cl_scenario", "==", "1", "or", "config", ".", "cl_scenario", "==", "3", ":", "\n", "        ", "num_outputs", "*=", "config", ".", "num_tasks", "\n", "\n", "", "logger", ".", "info", "(", "'The network will have %d output neurons.'", "%", "num_outputs", ")", "\n", "\n", "in_shape", "=", "[", "32", ",", "32", ",", "3", "]", "\n", "out_shape", "=", "[", "num_outputs", "]", "\n", "\n", "# TODO Allow main net only training.", "\n", "mnet", "=", "sutils", ".", "get_mnet_model", "(", "config", ",", "net_type", ",", "in_shape", ",", "out_shape", ",", "device", ",", "\n", "no_weights", "=", "no_weights", ")", "\n", "\n", "init_network_weights", "(", "mnet", ".", "weights", ",", "config", ",", "logger", ",", "net", "=", "mnet", ")", "\n", "\n", "return", "mnet", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.get_hnet_model": [[121, 155], ["logger.info", "utils.sim_utils.get_hnet_model", "hasattr", "train_utils.init_network_weights", "sutils.get_hnet_model.parameters", "train_utils.hnet_init_shift", "sutils.get_hnet_model.get_task_embs"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.get_hnet_model", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.init_network_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.hnet_init_shift", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs"], ["", "def", "get_hnet_model", "(", "config", ",", "mnet", ",", "logger", ",", "device", ")", ":", "\n", "    ", "\"\"\"Generate the hypernetwork.\n\n    This function uses :func:`utils.sim_utils.get_hnet_model` to generate the\n    hypernetwork.\n\n    The function also takes care of weight initialization, if configured.\n\n    Args:\n        (....): See docstring of function :func:`get_main_model`.\n        mnet: The main network.\n\n    Returns:\n        The hypernetwork or ``None`` if no hypernet is needed.\n    \n    \"\"\"", "\n", "logger", ".", "info", "(", "'Creating hypernetwork ...'", ")", "\n", "hnet", "=", "sutils", ".", "get_hnet_model", "(", "config", ",", "config", ".", "num_tasks", ",", "device", ",", "\n", "mnet", ".", "param_shapes", ")", "\n", "# FIXME There should be a nicer way of initializing hypernets in the", "\n", "# future.", "\n", "chunk_embs", "=", "None", "\n", "if", "hasattr", "(", "hnet", ",", "'chunk_embeddings'", ")", ":", "\n", "        ", "chunk_embs", "=", "hnet", ".", "chunk_embeddings", "\n", "", "init_network_weights", "(", "hnet", ".", "parameters", "(", ")", ",", "config", ",", "logger", ",", "\n", "chunk_embs", "=", "chunk_embs", ",", "task_embs", "=", "hnet", ".", "get_task_embs", "(", ")", ",", "net", "=", "hnet", ")", "\n", "if", "config", ".", "hnet_init_shift", ":", "\n", "        ", "hnet_init_shift", "(", "hnet", ",", "mnet", ",", "config", ",", "logger", ",", "device", ")", "\n", "\n", "# TODO Incorporate hyperchunk init.", "\n", "#if isinstance(hnet, ChunkedHyperNetworkHandler):", "\n", "#    hnet.apply_chunked_hyperfan_init(temb_var=config.std_normal_temb**2)", "\n", "\n", "", "return", "hnet", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.init_network_weights": [[156, 226], ["isinstance", "logger.info", "net.custom_init", "logger.warning", "torch.nn.init.normal_", "torch.nn.init.normal_", "W.ndimension", "torch.nn.init.constant_", "torch.nn.init.normal_", "torch.nn.init.xavier_uniform_"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.custom_init"], ["", "def", "init_network_weights", "(", "all_params", ",", "config", ",", "logger", ",", "chunk_embs", "=", "None", ",", "\n", "task_embs", "=", "None", ",", "net", "=", "None", ")", ":", "\n", "    ", "\"\"\"Initialize a given set of weight tensors according to the user\n    configuration.\n\n    Warning:\n        This method is agnostic to where the weights stem from and is\n        therefore slightly dangerous. Use with care.\n\n    Note:\n        The method only exists as at the time of implementation the package\n        :mod:`hnets` wasn't available yet. In the future, initialization should\n        be part of the network implementation (e.g., via method\n        :meth:`mnets.mnet_interface.MainNetInterface.custom_init`).\n\n    Note:\n        If the given network implements interface\n        :class:`mnets.mnet_interface.MainNetInterface`, then the corresponding\n        method :meth:`mnets.mnet_interface.MainNetInterface.custom_init` is\n        used.\n\n    Note:\n        Papers like the following show that hypernets should get a special\n        init. This function does not take this into consideration.\n\n            https://openreview.net/forum?id=H1lma24tPB\n\n    Args:\n        all_params: A list of weight tensors to be initialized.\n        config: Command-line arguments.\n        logger: Logger.\n        chunk_embs (optional): A list of chunk embeddings.\n        task_embs (optional): A list of task embeddings.\n        net (optional): The network from which the parameters stem come from.\n            Can be used to implement network specific initializations (e.g.,\n            batch-norm weights).\n    \"\"\"", "\n", "if", "config", ".", "custom_network_init", ":", "\n", "        ", "if", "net", "is", "not", "None", "and", "isinstance", "(", "net", ",", "MainNetInterface", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'Applying custom initialization to network ...'", ")", "\n", "net", ".", "custom_init", "(", "normal_init", "=", "config", ".", "normal_init", ",", "\n", "normal_std", "=", "config", ".", "std_normal_init", ",", "zero_bias", "=", "True", ")", "\n", "\n", "", "else", ":", "\n", "            ", "logger", ".", "warning", "(", "'Custom weight initialization is applied to all '", "+", "\n", "'network parameters. Note, the current '", "+", "\n", "'implementation might be agnostic to special '", "+", "\n", "'network parameters.'", ")", "\n", "for", "W", "in", "all_params", ":", "\n", "# FIXME not all 1D vectors are bias vectors.", "\n", "# Examples of parameters that are 1D and not bias vectors:", "\n", "# * batchnorm weights", "\n", "# * embedding vectors", "\n", "                ", "if", "W", ".", "ndimension", "(", ")", "==", "1", ":", "# Bias vector.", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "W", ",", "0", ")", "\n", "", "elif", "config", ".", "normal_init", ":", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "W", ",", "mean", "=", "0", ",", "std", "=", "config", ".", "std_normal_init", ")", "\n", "", "else", ":", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "W", ")", "\n", "\n", "\n", "# Note, the embedding vectors inside \"all_params\" have been considered", "\n", "# as bias vectors and thus initialized to zero.", "\n", "", "", "", "", "if", "chunk_embs", "is", "not", "None", ":", "\n", "        ", "for", "emb", "in", "chunk_embs", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "emb", ",", "mean", "=", "0", ",", "std", "=", "config", ".", "std_normal_emb", ")", "\n", "\n", "", "", "if", "task_embs", "is", "not", "None", ":", "\n", "        ", "for", "temb", "in", "task_embs", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "temb", ",", "mean", "=", "0.", ",", "std", "=", "config", ".", "std_normal_temb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.hnet_init_shift": [[227, 272], ["logger.warning", "hnet.forward", "train_utils.init_network_weights", "enumerate", "hasattr", "o.detach().clone", "torch.zeros_like", "o_shift.append", "o.detach"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.init_network_weights"], ["", "", "", "def", "hnet_init_shift", "(", "hnet", ",", "mnet", ",", "config", ",", "logger", ",", "device", ")", ":", "\n", "    ", "\"\"\"Init the hypernet ``hnet`` such that the weights of the main network \n    ``mnet`` are initialised as if there would be no hypernetwork i.e. the first\n    hypernetwork output is a standard init (for now normal or Xavier\n    are implemented).\n\n    Note:\n        This function is only meant for exploratory purposes. It does not\n        provide a proper weight initialization as for instance\n\n            https://openreview.net/forum?id=H1lma24tPB\n\n        Though, it is independent of the hypernet type/architecture.\n\n    Warning:\n        Not all hypernets support this quick-fix.\n\n    Args:\n        hnet: The model of the hyper network.\n        mnet: The main model.\n        config: The command line arguments.\n        device: Torch device (cpu or gpu).\n    \"\"\"", "\n", "logger", ".", "warning", "(", "'Config \"hnet_init_shift\" is just a temporary test and '", "+", "\n", "'should be used with care.'", ")", "\n", "\n", "# Get the current output, this should be normal or xavier or ...", "\n", "hnet_outputs", "=", "hnet", ".", "forward", "(", "0", ")", "\n", "orig_output", "=", "[", "o", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "o", "in", "hnet_outputs", "]", "\n", "mnet_init", "=", "[", "torch", ".", "zeros_like", "(", "o", ")", "for", "o", "in", "hnet_outputs", "]", "\n", "\n", "tmp", "=", "config", ".", "custom_network_init", "\n", "config", ".", "custom_network_init", "=", "True", "\n", "init_network_weights", "(", "mnet_init", ",", "config", ",", "logger", ",", "net", "=", "mnet", ")", "\n", "config", ".", "custom_network_init", "=", "tmp", "\n", "\n", "# The shift of the hypernetwork outputs will be computed by subtracting the", "\n", "# current output and adding the new desired output.", "\n", "o_shift", "=", "[", "]", "\n", "for", "i", ",", "o", "in", "enumerate", "(", "orig_output", ")", ":", "\n", "        ", "o_shift", ".", "append", "(", "-", "o", "+", "mnet_init", "[", "i", "]", ")", "\n", "\n", "# set the shifts", "\n", "", "assert", "(", "hasattr", "(", "hnet", ",", "'_shifts'", ")", ")", "# Only temporarily added to some hnets.", "\n", "hnet", ".", "_shifts", "=", "o_shift", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.setup_summary_dict": [[273, 312], ["hasattr", "dict"], "function", ["None"], ["", "def", "setup_summary_dict", "(", "config", ",", "shared", ",", "mnet", ",", "hnet", "=", "None", ")", ":", "\n", "    ", "\"\"\"Setup the summary dictionary that is written to the performance\n    summary file (in the result folder).\n\n    This method adds the keyword ``summary`` to ``shared``.\n\n    Args:\n        config (argparse.Namespace): Command-line arguments.\n        shared (argparse.Namespace): Miscellaneous data shared among training\n            functions (summary dict will be added to this object).\n        mnet: Main network.\n        hnet (optional): Hypernetwork.\n    \"\"\"", "\n", "assert", "(", "hasattr", "(", "shared", ",", "'experiment'", ")", ")", "\n", "\n", "summary", "=", "dict", "(", ")", "\n", "\n", "if", "hnet", "is", "None", ":", "\n", "        ", "num", "=", "mnet", ".", "num_params", "\n", "hnum", "=", "-", "1", "\n", "ratio", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "num", "=", "hnet", ".", "num_outputs", "\n", "hnum", "=", "hnet", ".", "num_weights", "\n", "ratio", "=", "hnum", "/", "num", "\n", "\n", "# FIXME keywords should be cross-checked with those specified in the", "\n", "# corresponding `_SUMMARY_KEYWORDS` of the hyperparam search.", "\n", "\n", "", "summary", "[", "'acc_final'", "]", "=", "[", "-", "1", "]", "*", "config", ".", "num_tasks", "\n", "summary", "[", "'acc_during'", "]", "=", "[", "-", "1", "]", "*", "config", ".", "num_tasks", "\n", "summary", "[", "'acc_avg_final'", "]", "=", "-", "1", "\n", "summary", "[", "'acc_avg_during'", "]", "=", "-", "1", "\n", "summary", "[", "'num_weights_main'", "]", "=", "num", "\n", "summary", "[", "'num_weights_hyper'", "]", "=", "hnum", "\n", "summary", "[", "'num_weights_ratio'", "]", "=", "ratio", "\n", "summary", "[", "'finished'", "]", "=", "0", "\n", "\n", "shared", ".", "summary", "=", "summary", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.save_summary_dict": [[313, 334], ["hasattr", "open", "shared.summary.items", "os.path.join", "isinstance", "f.write", "isinstance", "f.write", "f.write", "utils.misc.list_to_str"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.list_to_str"], ["", "def", "save_summary_dict", "(", "config", ",", "shared", ",", "experiment", ")", ":", "\n", "    ", "\"\"\"Write a text file in the result folder that gives a quick\n    overview over the results achieved so far.\n\n    Args:\n        (....): See docstring of function :func:`setup_summary_dict`.\n    \"\"\"", "\n", "# \"setup_summary_dict\" must be called first.", "\n", "assert", "(", "hasattr", "(", "shared", ",", "'summary'", ")", ")", "\n", "\n", "summary_fn", "=", "'performance_summary.txt'", "\n", "#summary_fn = hpperm._SUMMARY_FILENAME", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "config", ".", "out_dir", ",", "summary_fn", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "k", ",", "v", "in", "shared", ".", "summary", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "list", ")", ":", "\n", "                ", "f", ".", "write", "(", "'%s %s\\n'", "%", "(", "k", ",", "misc", ".", "list_to_str", "(", "v", ")", ")", ")", "\n", "", "elif", "isinstance", "(", "v", ",", "float", ")", ":", "\n", "                ", "f", ".", "write", "(", "'%s %f\\n'", "%", "(", "k", ",", "v", ")", ")", "\n", "", "else", ":", "\n", "                ", "f", ".", "write", "(", "'%s %d\\n'", "%", "(", "k", ",", "v", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._viz_init": [[39, 94], ["dhandlers[].next_test_batch", "dhandlers[].input_to_torch_tensor", "plotting._plotImages", "writer.add_figure", "dec_hnet.get_task_embs", "numpy.asarray", "enc_hnet.get_task_embs", "numpy.asarray", "plotting._scatterPlotData", "writer.add_figure", "plotting._scatterPlotData", "writer.add_figure", "np.asarray.append", "np.asarray.append", "np.asarray.append", "np.asarray.append", "emb.cpu().detach().numpy", "emb.detach().numpy", "emb.cpu().detach().numpy", "emb.detach().numpy", "emb.cpu().detach", "emb.detach", "emb.cpu().detach", "emb.detach", "emb.cpu", "emb.cpu"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_test_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._plotImages", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._scatterPlotData", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._scatterPlotData"], ["def", "_viz_init", "(", "dhandlers", ",", "enc_hnet", ",", "dec_hnet", ",", "writer", ",", "config", ",", "lims", "=", "8", ")", ":", "\n", "    ", "\"\"\" Initial visualization of hypernetwork embeddings and datasets.\n        \n        Args:\n            dhandlers: data handlers\n            enc_hnet: encoder hypernetwork\n            dec_hnet: decoder hypernetwork\n            writer: tensorboard writer\n            config: global config file\n            lims: x/y plot limits for scatterplots\n\n        Returns:\n            enc_embeddings: intiial encoder embeddings for further plotting\n            dec_embeddings: initial decoder embeddings for further plotting\n    \"\"\"", "\n", "\n", "if", "dec_hnet", "is", "not", "None", ":", "\n", "        ", "dec_embeddings", "=", "[", "]", "\n", "for", "emb", "in", "dec_hnet", ".", "get_task_embs", "(", ")", ":", "\n", "            ", "if", "(", "not", "config", ".", "no_cuda", ")", ":", "\n", "                ", "dec_embeddings", ".", "append", "(", "emb", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "dec_embeddings", ".", "append", "(", "emb", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "", "dec_embeddings", "=", "np", ".", "asarray", "(", "dec_embeddings", ")", "\n", "", "else", ":", "\n", "        ", "dec_embeddings", "=", "None", "\n", "\n", "", "if", "enc_hnet", "is", "not", "None", ":", "\n", "        ", "enc_embeddings", "=", "[", "]", "\n", "for", "emb", "in", "enc_hnet", ".", "get_task_embs", "(", ")", ":", "\n", "            ", "if", "(", "not", "config", ".", "no_cuda", ")", ":", "\n", "                ", "enc_embeddings", ".", "append", "(", "emb", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "enc_embeddings", ".", "append", "(", "emb", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "", "enc_embeddings", "=", "np", ".", "asarray", "(", "enc_embeddings", ")", "\n", "", "else", ":", "\n", "        ", "enc_embeddings", "=", "None", "\n", "\n", "\n", "", "data_batch", "=", "dhandlers", "[", "0", "]", ".", "next_test_batch", "(", "config", ".", "batch_size", ")", "\n", "X", "=", "dhandlers", "[", "0", "]", ".", "input_to_torch_tensor", "(", "data_batch", "[", "0", "]", ",", "'cpu'", ",", "\n", "mode", "=", "'inference'", ")", "\n", "figure", "=", "_plotImages", "(", "X", ",", "config", ")", "\n", "\n", "writer", ".", "add_figure", "(", "'overall'", ",", "figure", ")", "\n", "\n", "if", "dec_hnet", "is", "not", "None", ":", "\n", "        ", "fig", "=", "_scatterPlotData", "(", "[", "dec_embeddings", "]", ",", "config", ",", "[", "'blue'", "]", ")", "\n", "writer", ".", "add_figure", "(", "'decoder embeddings'", ",", "fig", ")", "\n", "\n", "", "if", "enc_hnet", "is", "not", "None", ":", "\n", "        ", "fig", "=", "_scatterPlotData", "(", "[", "enc_embeddings", "]", ",", "config", ",", "[", "'blue'", "]", ")", "\n", "writer", ".", "add_figure", "(", "'encoder embeddings'", ",", "fig", ")", "\n", "\n", "", "return", "enc_embeddings", ",", "dec_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._plotImages": [[95, 122], ["matplotlib.subplots", "range", "X.reshape.cpu().detach().numpy", "X.reshape.detach().numpy", "X.reshape.reshape", "X.reshape.reshape", "range", "ax[].imshow", "ax[].axis", "X.reshape.cpu().detach", "X.reshape.detach", "X.reshape.cpu"], "function", ["None"], ["", "def", "_plotImages", "(", "X", ",", "config", ",", "cur_bs", "=", "None", ")", ":", "\n", "    ", "\"\"\" Helper function to plot MNIST images.\n\n        Args:\n            X: Images to plot (can be fake or real).\n            config: The command line arguments.\n    \"\"\"", "\n", "\n", "add", "=", "config", ".", "padding", "*", "2", "\n", "\n", "if", "not", "config", ".", "no_cuda", ":", "\n", "        ", "X", "=", "X", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "        ", "X", "=", "X", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "", "if", "cur_bs", "is", "None", ":", "\n", "        ", "X", "=", "X", ".", "reshape", "(", "config", ".", "batch_size", ",", "28", "+", "add", ",", "28", "+", "add", ")", "\n", "", "else", ":", "\n", "        ", "X", "=", "X", ".", "reshape", "(", "cur_bs", ",", "28", "+", "add", ",", "28", "+", "add", ")", "\n", "", "num_plots", "=", "4", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "3", ",", "3", ")", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "3", ")", ":", "\n", "            ", "ax", "[", "i", ",", "j", "]", ".", "imshow", "(", "X", "[", "i", "*", "num_plots", "+", "j", "]", ",", "\n", "interpolation", "=", "'nearest'", ",", "cmap", "=", "'gray_r'", ")", "\n", "ax", "[", "i", ",", "j", "]", ".", "axis", "(", "'off'", ")", "\n", "", "", "return", "fig", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._scatterPlotData": [[123, 149], ["matplotlib.subplots", "enumerate", "ax.set_xlim", "ax.set_ylim", "type", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.scatter", "dataset.cpu().detach().numpy", "dataset.cpu().detach().numpy", "dataset.detach().numpy", "dataset.detach().numpy", "dataset.cpu().detach", "dataset.cpu().detach", "dataset.detach", "dataset.detach", "dataset.cpu", "dataset.cpu"], "function", ["None"], ["", "def", "_scatterPlotData", "(", "datasets", ",", "config", ",", "colors", ",", "lims", "=", "None", ")", ":", "\n", "    ", "\"\"\" Helper function to generate Scatterplots.\n\n        Args:\n            datasets: datasets to be scatter-plotted\n            config: The command line arguments.\n            colors: colors for each dataset to be plotted, order with\n                respect to datasets\n            lims: x/y plot limits for scatterplots\n    \"\"\"", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "if", "(", "lims", ")", ":", "\n", "        ", "ax", ".", "set_xlim", "(", "[", "-", "lims", ",", "lims", "]", ")", "\n", "ax", ".", "set_ylim", "(", "[", "-", "lims", ",", "lims", "]", ")", "\n", "# add colors", "\n", "", "for", "idx", ",", "dataset", "in", "enumerate", "(", "datasets", ")", ":", "\n", "        ", "if", "(", "type", "(", "dataset", ")", "==", "np", ".", "ndarray", ")", ":", "\n", "            ", "plt", ".", "scatter", "(", "dataset", "[", ":", ",", "0", "]", ",", "\n", "dataset", "[", ":", ",", "1", "]", ",", "color", "=", "colors", "[", "idx", "]", ")", "\n", "", "elif", "(", "not", "config", ".", "no_cuda", ")", ":", "\n", "            ", "plt", ".", "scatter", "(", "dataset", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "0", "]", ",", "\n", "dataset", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "1", "]", ",", "color", "=", "colors", "[", "idx", "]", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "scatter", "(", "dataset", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "0", "]", ",", "\n", "dataset", ".", "detach", "(", ")", ".", "numpy", "(", ")", "[", ":", ",", "1", "]", ",", "color", "=", "colors", "[", "idx", "]", ")", "\n", "", "", "return", "fig", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._viz_training": [[150, 202], ["plotting._plotImages", "writer.add_figure", "plotting._plotImages", "writer.add_figure", "matplotlib.subplots", "writer.add_figure", "matplotlib.subplots", "writer.add_figure", "matplotlib.scatter", "numpy.log", "matplotlib.scatter", "matplotlib.scatter", "numpy.log", "matplotlib.scatter", "numpy.arange", "numpy.arange", "len", "len"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._plotImages", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._plotImages"], ["", "def", "_viz_training", "(", "data_real", ",", "x_fake", ",", "enc_embeddings", ",", "dec_embeddings", ",", "\n", "enc_embedding_history", ",", "dec_embedding_history", ",", "\n", "writer", ",", "step", ",", "config", ",", "lims", "=", "8", ",", "title", "=", "'train'", ")", ":", "\n", "    ", "\"\"\" Visualize the trianing process.\n\n        Args:\n            data_real: real data\n            x_fake: g generated fake data\n            enc_embeddings: current encoder embedding\n            dec_embeddings: current decoder embedding\n            enc_embedding_history: history of embeddings of the encoder\n            dec_embedding_history: history of embeddings of the decoder\n            writer: tensorboard writer\n            step: current step/iteration\n            config: global config\n            lims: x/y plot limits for scatter plots\n    \"\"\"", "\n", "\n", "# plot fake data", "\n", "fig", "=", "_plotImages", "(", "data_real", ",", "config", ")", "\n", "writer", ".", "add_figure", "(", "title", "+", "'_real'", ",", "fig", ",", "global_step", "=", "step", ")", "\n", "\n", "# plot real data", "\n", "fig", "=", "_plotImages", "(", "x_fake", ",", "config", ")", "\n", "writer", ".", "add_figure", "(", "title", "+", "'_fake'", ",", "fig", ",", "global_step", "=", "step", ")", "\n", "\n", "# plot embedding history with current embedding", "\n", "#TODO: restart scale for each mode", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "if", "dec_embeddings", "is", "not", "None", ":", "\n", "        ", "plt", ".", "scatter", "(", "dec_embeddings", "[", ":", ",", "0", "]", ",", "\n", "dec_embeddings", "[", ":", ",", "1", "]", ",", "color", "=", "'red'", ")", "\n", "\n", "", "if", "dec_embedding_history", "is", "not", "None", ":", "\n", "        ", "t", "=", "np", ".", "log", "(", "np", ".", "arange", "(", "1", ",", "len", "(", "dec_embedding_history", ")", "+", "1", ")", ")", "\n", "plt", ".", "scatter", "(", "dec_embedding_history", "[", ":", ",", "0", "]", ",", "dec_embedding_history", "[", ":", ",", "1", "]", ",", "c", "=", "t", ")", "\n", "\n", "", "writer", ".", "add_figure", "(", "title", "[", ":", "5", "]", "+", "'_decoder_embeddings'", ",", "\n", "fig", ",", "global_step", "=", "step", ")", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "if", "enc_embeddings", "is", "not", "None", ":", "\n", "        ", "plt", ".", "scatter", "(", "enc_embeddings", "[", ":", ",", "0", "]", ",", "\n", "enc_embeddings", "[", ":", ",", "1", "]", ",", "color", "=", "'red'", ")", "\n", "\n", "", "if", "enc_embedding_history", "is", "not", "None", ":", "\n", "        ", "t", "=", "np", ".", "log", "(", "np", ".", "arange", "(", "1", ",", "len", "(", "enc_embedding_history", ")", "+", "1", ")", ")", "\n", "plt", ".", "scatter", "(", "enc_embedding_history", "[", ":", ",", "0", "]", ",", "enc_embedding_history", "[", ":", ",", "1", "]", ",", "c", "=", "t", ")", "\n", "\n", "", "writer", ".", "add_figure", "(", "title", "[", ":", "5", "]", "+", "'_encoder_embeddings'", ",", "\n", "fig", ",", "global_step", "=", "step", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST._save_performance_summary": [[66, 118], ["dict", "utils.misc.list_to_str", "utils.misc.list_to_str", "utils.misc.list_to_str", "sum", "open", "os.path.join", "f.write", "f.write", "f.write"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.list_to_str", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.list_to_str", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.list_to_str"], ["def", "_save_performance_summary", "(", "config", ",", "train_iter", "=", "None", ")", ":", "\n", "    ", "\"\"\"Save a summary of the test results achieved so far in a easy to parse\n    file (for humans and subsequent programs).\n\n    Args:\n        config: Command-line arguments.\n        train_iter:  (optional) The current training iteration. \n            Though, the results written in the file correspond have there own \n            training iteration assigned.\n    \"\"\"", "\n", "if", "train_iter", "is", "None", ":", "\n", "        ", "train_iter", "=", "config", ".", "n_iter", "\n", "\n", "", "tp", "=", "dict", "(", ")", "\n", "\n", "if", "config", ".", "upper_bound", "or", "(", "config", ".", "infer_task_id", "and", "config", ".", "cl_scenario", "==", "1", ")", ":", "\n", "        ", "config", ".", "num_weights_rp_net", "=", "0", "\n", "config", ".", "num_weights_rp_hyper_net", "=", "0", "\n", "config", ".", "compression_ratio_rp", "=", "0", "\n", "\n", "", "tp", "[", "\"acc_after_list\"", "]", "=", "misc", ".", "list_to_str", "(", "config", ".", "overall_acc_list", ")", "\n", "tp", "[", "\"acc_during_list\"", "]", "=", "misc", ".", "list_to_str", "(", "config", ".", "during_accs_final", ")", "\n", "tp", "[", "\"acc_after_mean\"", "]", "=", "config", ".", "acc_mean", "\n", "tp", "[", "\"acc_during_mean\"", "]", "=", "sum", "(", "config", ".", "during_accs_final", ")", "/", "config", ".", "num_tasks", "\n", "tp", "[", "\"num_weights_class_net\"", "]", "=", "config", ".", "num_weights_class_net", "\n", "tp", "[", "\"num_weights_rp_net\"", "]", "=", "config", ".", "num_weights_rp_net", "\n", "tp", "[", "\"num_weights_rp_hyper_net\"", "]", "=", "config", ".", "num_weights_rp_hyper_net", "\n", "tp", "[", "\"num_weights_class_hyper_net\"", "]", "=", "config", ".", "num_weights_class_hyper_net", "\n", "tp", "[", "\"compression_ratio_rp\"", "]", "=", "config", ".", "compression_ratio_rp", "\n", "tp", "[", "\"compression_ratio_class\"", "]", "=", "config", ".", "compression_ratio_class", "\n", "tp", "[", "\"overall_task_infer_accuracy_list\"", "]", "=", "misc", ".", "list_to_str", "(", "config", ".", "overall_task_infer_accuracy_list", ")", "\n", "\n", "tp", "[", "\"acc_task_infer_mean\"", "]", "=", "config", ".", "acc_task_infer_mean", "\n", "# Note, the keywords of this dictionary are defined by the array:", "\n", "#   hpsearch._SUMMARY_KEYWORDS", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "config", ".", "out_dir", ",", "\n", "hpsearch", ".", "_SUMMARY_FILENAME", ")", ",", "'w'", ")", "as", "f", ":", "\n", "\n", "        ", "assert", "(", "'num_train_iter'", "in", "hpsearch", ".", "_SUMMARY_KEYWORDS", ")", "\n", "\n", "for", "kw", "in", "hpsearch", ".", "_SUMMARY_KEYWORDS", ":", "\n", "            ", "if", "kw", "==", "'num_train_iter'", ":", "\n", "                ", "f", ".", "write", "(", "'%s %d\\n'", "%", "(", "'num_train_iter'", ",", "train_iter", ")", ")", "\n", "continue", "\n", "", "if", "kw", "==", "'finished'", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "                ", "try", ":", "\n", "                    ", "f", ".", "write", "(", "'%s %f\\n'", "%", "(", "kw", ",", "tp", "[", "kw", "]", ")", ")", "\n", "", "except", ":", "\n", "                    ", "f", ".", "write", "(", "'%s %s\\n'", "%", "(", "kw", ",", "tp", "[", "kw", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.test": [[119, 381], ["infer_net.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "c_net.eval", "c_net_hnet.eval", "range", "range", "print", "overall_task_infer_accuracy_list.append", "print", "dhandler.next_test_batch", "dhandler.input_to_torch_tensor", "dhandler.output_to_torch_tensor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "print", "overall_acc_list.append", "print", "print", "print", "infer_net.forward", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.softmax", "torch.cat.append", "torch.cat.append", "torch.cat.argmax", "mnets.classifier_interface.Classifier.accuracy", "c_net_hnet.forward", "c_net.forward", "c_net.forward", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.cat", "torch.cat", "torch.cat", "infer_net.forward", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.max", "torch.max", "torch.max", "c_net_hnet.forward", "c_net.forward", "c_net.forward", "torch.zeros", "torch.zeros", "torch.zeros", "torch.argmin", "torch.argmin", "torch.argmin", "F.softmax.argmax", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "range", "range", "c_net_hnet.forward", "c_net.forward", "torch.softmax", "entropies.append", "torch.stack", "torch.stack", "torch.stack", "torch.sum", "torch.sum", "torch.sum", "torch.log", "torch.log", "torch.log"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_test_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.accuracy", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "", "", "", "", "def", "test", "(", "dhandlers", ",", "class_nets", ",", "infer_net", ",", "device", ",", "config", ",", "writer", ",", "\n", "task_id", "=", "None", ")", ":", "\n", "    ", "\"\"\" Test continual learning experiments on MNIST dataset. This can either \n    be splitMNIST or permutedMNIST. \n    Depending on the method and cl scenario used, this methods manages\n    to measure the test accuracy of a given task or all tasks after \n    training. In order to do so, correct targets need to be constructed \n    and output heads need to be set (or inferred). \n    Furthermore, this method distinguises between classification accuracy\n    on a task or on the accuracy to infer task id's if applicable. \n\n    Args:\n        (....): See docstring of function :func:`train_tasks`.\n        task_id: (optional) If not None, the method will compute and return \n                   test acc for the the given task id, not all tasks.\n    \n    Returns:\n        Scalar represting the test accuracy for the given task id.\n        If ``task_id`` is None, the accuracy of the last task of the cl \n        experiment is returned. \n    \"\"\"", "\n", "\n", "# get hnet if this option is given", "\n", "if", "class_nets", "is", "not", "None", ":", "\n", "        ", "if", "config", ".", "training_with_hnet", ":", "\n", "            ", "c_net_hnet", "=", "class_nets", "[", "1", "]", "\n", "c_net", "=", "class_nets", "[", "0", "]", "\n", "c_net", ".", "eval", "(", ")", "\n", "c_net_hnet", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "c_net", "=", "class_nets", "\n", "\n", "", "", "if", "infer_net", "is", "not", "None", ":", "\n", "        ", "infer_net", ".", "eval", "(", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "        ", "overall_acc", "=", "0", "\n", "overall_acc_list", "=", "[", "]", "\n", "overall_task_infer_accuracy", "=", "0", "\n", "overall_task_infer_accuracy_list", "=", "[", "]", "\n", "\n", "# choose tasks to test", "\n", "if", "task_id", "is", "not", "None", ":", "\n", "            ", "task_range", "=", "range", "(", "task_id", ",", "task_id", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "task_range", "=", "range", "(", "config", ".", "num_tasks", ")", "\n", "\n", "# iterate through all old tasks", "\n", "", "for", "t", "in", "task_range", ":", "\n", "            ", "print", "(", "\"Testing task: \"", ",", "t", ")", "\n", "# reset data", "\n", "if", "task_id", "is", "not", "None", ":", "\n", "                ", "dhandler", "=", "dhandlers", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "dhandler", "=", "dhandlers", "[", "t", "]", "\n", "\n", "# create some variables", "\n", "", "N_processed", "=", "0", "\n", "test_size", "=", "dhandler", ".", "num_test_samples", "\n", "\n", "# is task id has to be inferred, for every x we have to do that", "\n", "# and therefore have one h(e) = W per data point - this is only ", "\n", "# possible with batch size one, for now", "\n", "if", "(", "config", ".", "infer_task_id", "and", "infer_net", "is", "not", "None", ")", "or", "config", ".", "infer_with_entropy", ":", "\n", "                ", "curr_bs", "=", "1", "\n", "", "else", ":", "\n", "                ", "curr_bs", "=", "config", ".", "test_batch_size", "\n", "\n", "", "classifier_accuracy", "=", "0", "\n", "task_infer_accuracy", "=", "0", "\n", "Y_hat_all", "=", "[", "]", "\n", "T_all", "=", "[", "]", "\n", "\n", "# go through test set", "\n", "while", "N_processed", "<", "test_size", ":", "\n", "# test size of tasks might be \"arbitrary\"", "\n", "                ", "if", "N_processed", "+", "curr_bs", ">", "test_size", ":", "\n", "                    ", "curr_bs", "=", "test_size", "-", "N_processed", "\n", "", "N_processed", "+=", "curr_bs", "\n", "\n", "# get data", "\n", "real_batch", "=", "dhandler", ".", "next_test_batch", "(", "curr_bs", ")", "\n", "X_real", "=", "dhandler", ".", "input_to_torch_tensor", "(", "real_batch", "[", "0", "]", ",", "device", ",", "\n", "mode", "=", "'inference'", ")", "\n", "T_real", "=", "dhandler", ".", "output_to_torch_tensor", "(", "real_batch", "[", "1", "]", ",", "device", ",", "\n", "mode", "=", "'inference'", ")", "\n", "\n", "# get short version of output dim", "\n", "od", "=", "config", ".", "out_dim", "\n", "\n", "#######################################", "\n", "# SET THE OUTPUT HEAD / COMPUTE TARGETS", "\n", "#######################################", "\n", "\n", "# get dummy for easy access to the  output dim of our main ", "\n", "# network as a dummy, only needed for the first iteration", "\n", "if", "class_nets", "is", "not", "None", ":", "\n", "                    ", "if", "config", ".", "training_with_hnet", ":", "\n", "                        ", "weights_dummy", "=", "c_net_hnet", ".", "forward", "(", "0", ")", "\n", "Y_dummies", "=", "c_net", ".", "forward", "(", "X_real", ",", "weights_dummy", ")", "\n", "", "else", ":", "\n", "                        ", "Y_dummies", "=", "c_net", ".", "forward", "(", "X_real", ")", "\n", "", "", "else", ":", "\n", "                    ", "Y_dummies", "=", "infer_net", ".", "forward", "(", "X_real", ")", "\n", "\n", "# build one hots if this option was chosen", "\n", "# here we build targets if only have one neuron per task ", "\n", "# which we set to 1", "\n", "", "if", "config", ".", "class_incremental", ":", "\n", "                    ", "task_out", "=", "[", "0", ",", "config", ".", "num_tasks", "]", "\n", "T_real", "=", "torch", ".", "zeros", "(", "(", "Y_dummies", ".", "shape", "[", "0", "]", ",", "\n", "config", ".", "num_tasks", ")", ")", ".", "to", "(", "device", ")", "\n", "T_real", "[", ":", ",", "t", "]", "=", "1", "\n", "\n", "# compute targets - this is a bit unelegant, cl 3 requires hacks", "\n", "", "elif", "config", ".", "cl_scenario", "==", "1", "or", "config", ".", "cl_scenario", "==", "2", ":", "\n", "                    ", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "# take the task specific output neuron", "\n", "                        ", "task_out", "=", "[", "t", "*", "od", ",", "t", "*", "od", "+", "od", "]", "\n", "", "else", ":", "\n", "# always all output neurons (only one head is used)", "\n", "                        ", "task_out", "=", "[", "0", ",", "od", "]", "\n", "", "", "else", ":", "\n", "# This here is the classic CL 3 scenario", "\n", "# first we get the predictions, this is over all neurons", "\n", "                    ", "task_out", "=", "[", "0", ",", "config", ".", "num_tasks", "*", "od", "]", "\n", "# Here we build the targets, this is zero everywhere ", "\n", "# except for the current task - here the correct target", "\n", "# is inserted", "\n", "\n", "# build the two zero tensors that surround the targets", "\n", "zeros1", "=", "torch", ".", "zeros", "(", "Y_dummies", "[", ":", ",", "0", ":", "t", "*", "od", "]", ".", "shape", ")", ".", "to", "(", "device", ")", "\n", "zeros2", "=", "torch", ".", "zeros", "(", "Y_dummies", "[", ":", ",", "0", ":", "(", "config", ".", "num_tasks", "-", "1", "-", "t", ")", "*", "od", "]", ".", "shape", ")", ".", "to", "(", "device", ")", "\n", "T_real", "=", "torch", ".", "cat", "(", "[", "zeros1", ",", "T_real", ",", "zeros2", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n", "#################", "\n", "# TASK PREDICTION", "\n", "#################", "\n", "\n", "# get task predictions", "\n", "", "if", "config", ".", "cl_scenario", "!=", "1", ":", "\n", "                    ", "if", "infer_net", "is", "not", "None", ":", "\n", "# get infer net to predict the apparent task id ", "\n", "                        ", "task_pred", "=", "infer_net", ".", "forward", "(", "X_real", ")", "\n", "task_pred", "=", "task_pred", "[", ":", ",", "0", ":", "config", ".", "num_tasks", "]", "\n", "task_pred", "=", "torch", ".", "sigmoid", "(", "task_pred", ")", "\n", "_", ",", "inf_task_id", "=", "torch", ".", "max", "(", "task_pred", ",", "1", ")", "\n", "\n", "# measure acc of prediction", "\n", "task_infer_accuracy", "+=", "(", "inf_task_id", "==", "t", ")", ".", "float", "(", ")", "\n", "\n", "", "elif", "config", ".", "infer_with_entropy", "and", "class_nets", "is", "not", "None", "and", "config", ".", "training_with_hnet", ":", "\n", "                        ", "entropies", "=", "[", "]", "\n", "if", "task_id", "is", "not", "None", ":", "\n", "                            ", "entrop_to_test", "=", "range", "(", "0", ",", "task_id", "+", "1", ")", "\n", "", "else", ":", "\n", "                            ", "entrop_to_test", "=", "range", "(", "config", ".", "num_tasks", ")", "\n", "# infer task id through entropy of softmax outputs of ", "\n", "# different models", "\n", "", "for", "e", "in", "entrop_to_test", ":", "\n", "                            ", "weights_c", "=", "c_net_hnet", ".", "forward", "(", "e", ")", "\n", "Y_hat_logits", "=", "c_net", ".", "forward", "(", "X_real", ",", "weights_c", ")", "\n", "if", "config", ".", "cl_scenario", "==", "2", ":", "\n", "                                ", "task_out", "=", "[", "0", ",", "od", "]", "\n", "", "else", ":", "\n", "                                ", "task_out", "=", "[", "e", "*", "od", ",", "e", "*", "od", "+", "od", "]", "\n", "", "Y_hat", "=", "F", ".", "softmax", "(", "Y_hat_logits", "[", ":", ",", "\n", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "/", "config", ".", "soft_temp", ",", "-", "1", ")", "\n", "entropy", "=", "-", "1", "*", "torch", ".", "sum", "(", "Y_hat", "*", "torch", ".", "log", "(", "Y_hat", ")", ")", "\n", "entropies", ".", "append", "(", "entropy", ")", "\n", "", "inf_task_id", "=", "torch", ".", "argmin", "(", "torch", ".", "stack", "(", "entropies", ")", ")", "\n", "task_infer_accuracy", "+=", "(", "inf_task_id", "==", "t", ")", ".", "float", "(", ")", "\n", "\n", "", "if", "config", ".", "cl_scenario", "==", "3", "and", "config", ".", "infer_output_head", ":", "\n", "                        ", "task_out", "=", "[", "inf_task_id", "*", "od", ",", "inf_task_id", "*", "od", "+", "od", "]", "\n", "", "", "else", ":", "\n", "# if task id is known, task inference acc is 100%", "\n", "                    ", "task_infer_accuracy", "+=", "1", "\n", "inf_task_id", "=", "t", "\n", "\n", "", "if", "class_nets", "is", "not", "None", ":", "\n", "# from the given inf_task_id we try to produce the ", "\n", "# correct model for that tasks", "\n", "                    ", "if", "config", ".", "training_with_hnet", ":", "\n", "                        ", "weights_c", "=", "c_net_hnet", ".", "forward", "(", "inf_task_id", ")", "\n", "Y_hat_logits", "=", "c_net", ".", "forward", "(", "X_real", ",", "weights_c", ")", "\n", "", "else", ":", "\n", "                        ", "Y_hat_logits", "=", "c_net", ".", "forward", "(", "X_real", ")", "\n", "\n", "#################", "\n", "# CLASSIFICATION", "\n", "#################", "\n", "", "", "if", "class_nets", "is", "not", "None", ":", "\n", "# save predictions of current batch", "\n", "                    ", "Y_hat_logits", "=", "Y_hat_logits", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "Y_hat", "=", "F", ".", "softmax", "(", "Y_hat_logits", ",", "dim", "=", "1", ")", "\n", "if", "config", ".", "cl_scenario", "==", "3", "and", "config", ".", "infer_output_head", ":", "\n", "# this is the special case where the output head is ", "\n", "# inferred. Here we compute the argmax of the single ", "\n", "# head and add the number of previous neurons such that", "\n", "# it coincides with the argmax of a hot enc target   ", "\n", "# that is build for all heads. Example: we detect that", "\n", "# task 3 is present, and every task consist of two", "\n", "# classes. The argmax of Y_hat will either give us 0", "\n", "# or 1, since Y_hat_logits was already cut to two ", "\n", "# dimensions. Now we have to add 3*2 to the argmax ", "\n", "# of Y_hat to get a prediction between class 0 and ", "\n", "# num_tasks*class_per_task.", "\n", "\n", "                        ", "Y_hat", "=", "Y_hat", ".", "argmax", "(", "dim", "=", "1", ",", "keepdim", "=", "False", ")", "+", "inf_task_id", "*", "od", "\n", "", "Y_hat_all", ".", "append", "(", "Y_hat", ")", "\n", "T_all", ".", "append", "(", "T_real", ")", "\n", "\n", "", "", "if", "class_nets", "is", "not", "None", ":", "\n", "# append predictions", "\n", "                ", "Y_hat_all", "=", "torch", ".", "cat", "(", "Y_hat_all", ")", "\n", "T_all", "=", "torch", ".", "cat", "(", "T_all", ")", "\n", "# check if all test samples are used", "\n", "assert", "(", "Y_hat_all", ".", "shape", "[", "0", "]", "==", "dhandler", ".", "num_test_samples", ")", "\n", "\n", "# compute class acc's", "\n", "if", "config", ".", "cl_scenario", "==", "3", "and", "class_nets", "is", "not", "None", "and", "config", ".", "infer_output_head", ":", "\n", "# this is a special case, we compare the", "\n", "                    ", "targets", "=", "T_all", ".", "argmax", "(", "dim", "=", "1", ",", "keepdim", "=", "False", ")", "\n", "classifier_accuracy", "=", "(", "Y_hat_all", "==", "targets", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "                    ", "classifier_accuracy", "=", "Classifier", ".", "accuracy", "(", "Y_hat_all", ",", "T_all", ")", "\n", "\n", "", "classifier_accuracy", "*=", "100.", "\n", "print", "(", "\"Accuracy of task: \"", ",", "t", ",", "\" % \"", ",", "classifier_accuracy", ")", "\n", "overall_acc_list", ".", "append", "(", "classifier_accuracy", ")", "\n", "overall_acc", "+=", "classifier_accuracy", "\n", "\n", "# compute task inference acc\"s", "\n", "", "ti_accuracy", "=", "task_infer_accuracy", "/", "dhandler", ".", "num_test_samples", "*", "100.", "\n", "if", "config", ".", "training_task_infer", "or", "config", ".", "infer_with_entropy", ":", "\n", "                ", "print", "(", "\"Accuracy of task inference: \"", ",", "t", ",", "\" % \"", ",", "ti_accuracy", ")", "\n", "", "overall_task_infer_accuracy", "+=", "ti_accuracy", "\n", "overall_task_infer_accuracy_list", ".", "append", "(", "ti_accuracy", ")", "\n", "\n", "# testing all tasks", "\n", "", "if", "task_id", "is", "None", ":", "\n", "            ", "if", "class_nets", "is", "not", "None", ":", "\n", "                ", "print", "(", "\"Overall mean acc: \"", ",", "overall_acc", "/", "config", ".", "num_tasks", ")", "\n", "", "if", "config", ".", "training_task_infer", "or", "config", ".", "infer_with_entropy", ":", "\n", "                ", "print", "(", "\"Overall task inf acc: \"", ",", "overall_task_infer_accuracy", "/", "config", ".", "num_tasks", ")", "\n", "", "config", ".", "overall_acc_list", "=", "overall_acc_list", "\n", "config", ".", "acc_mean", "=", "overall_acc", "/", "config", ".", "num_tasks", "\n", "config", ".", "overall_task_infer_accuracy_list", "=", "overall_task_infer_accuracy_list", "\n", "config", ".", "acc_task_infer_mean", "=", "overall_task_infer_accuracy", "/", "config", ".", "num_tasks", "\n", "print", "(", "config", ".", "overall_task_infer_accuracy_list", ",", "config", ".", "acc_task_infer_mean", ")", "\n", "", "", "return", "classifier_accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.get_fake_data_loss": [[382, 541], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "int", "numpy.random.randint", "range", "net_copy.forward().detach", "net.forward", "torch.cat.append", "all_Y_hat_ls.append", "print", "mnets.classifier_interface.Classifier.logit_cross_entropy_loss", "mnets.classifier_interface.Classifier.knowledge_distillation_loss", "numpy.ceil", "dhandlers_rp[].next_train_batch", "dhandlers_rp[].input_to_torch_tensor", "mnist.replay.train_replay.sample.detach", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "mnets.classifier_interface.Classifier.accuracy", "msg.format", "mnist.replay.train_gan.sample", "mnist.replay.train_replay.sample", "mnist.plotting._plotImages", "writer.add_figure", "net_copy.forward", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.zeros", "torch.zeros", "torch.zeros", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.max", "torch.max", "torch.max", "torch.zeros().to.scatter_", "torch.max", "torch.max", "torch.max", "torch.zeros().to.scatter_", "argmax.view", "torch.zeros", "torch.zeros", "torch.zeros", "argmax.view", "str", "torch.zeros", "torch.zeros", "torch.zeros"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.logit_cross_entropy_loss", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.knowledge_distillation_loss", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.accuracy", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.sample", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.sample", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._plotImages", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "get_fake_data_loss", "(", "dhandlers_rp", ",", "net", ",", "dec", ",", "d_hnet", ",", "device", ",", "config", ",", "writer", ",", "\n", "t", ",", "i", ",", "net_copy", ")", ":", "\n", "    ", "\"\"\" Sample fake data from generator for tasks up to t and compute a loss\n    compared to predictions of a checkpointed network.\n    \n    We must take caution when considering the different learning scenarios\n    and methods and training stages, see detailed comments in the code.\n    \n    In general, we build a batch of replayed data from all previous tasks.\n    Since we do not know the labels of the replayed data, we consider the\n    output of the checkpointed network as ground thruth i.e. we must compute\n    a loss between two logits.See :class:`mnets.classifier_interface.Classifier`\n    for a detailed describtion of the different loss functions.\n        \n    Args:\n        (....): See docstring of function :func:`train_tasks`.\n        t: Task id.\n        i: Current training iteration.\n        net_copy: Copy/checkpoint of the classifier network before \n            learning task ``t``.\n    Returns:\n        The loss between predictions and predictions of a \n        checkpointed network or replayed data.\n    \n    \"\"\"", "\n", "\n", "all_Y_hat_ls", "=", "[", "]", "\n", "all_targets", "=", "[", "]", "\n", "\n", "# we have to choose from which embeddings (multiple?!) to sample from ", "\n", "if", "config", ".", "class_incremental", "or", "config", ".", "single_class_replay", ":", "\n", "# if we trained every class with a different generator", "\n", "        ", "emb_num", "=", "t", "*", "config", ".", "out_dim", "\n", "", "else", ":", "\n", "# here samples from the whole task come from one generator", "\n", "        ", "emb_num", "=", "t", "\n", "# we have to choose from which embeddings to sample from ", "\n", "\n", "", "if", "config", ".", "fake_data_full_range", ":", "\n", "        ", "ran", "=", "range", "(", "0", ",", "emb_num", ")", "\n", "bs_per_task", "=", "int", "(", "np", ".", "ceil", "(", "config", ".", "batch_size", "/", "emb_num", ")", ")", "\n", "", "else", ":", "\n", "        ", "random_t", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "emb_num", ")", "\n", "ran", "=", "range", "(", "random_t", ",", "random_t", "+", "1", ")", "\n", "bs_per_task", "=", "config", ".", "batch_size", "\n", "\n", "", "for", "re", "in", "ran", ":", "\n", "\n", "# exchange replay data with real data to compute upper bounds ", "\n", "        ", "if", "config", ".", "upper_bound", ":", "\n", "            ", "real_batch", "=", "dhandlers_rp", "[", "re", "]", ".", "next_train_batch", "(", "bs_per_task", ")", "\n", "X_fake", "=", "dhandlers_rp", "[", "re", "]", ".", "input_to_torch_tensor", "(", "real_batch", "[", "0", "]", ",", "\n", "device", ",", "mode", "=", "'train'", ")", "\n", "", "else", ":", "\n", "# get fake data", "\n", "            ", "if", "config", ".", "replay_method", "==", "'gan'", ":", "\n", "                ", "X_fake", "=", "sample_gan", "(", "dec", ",", "d_hnet", ",", "config", ",", "re", ",", "device", ",", "\n", "bs", "=", "bs_per_task", ")", "\n", "", "else", ":", "\n", "                ", "X_fake", "=", "sample_vae", "(", "dec", ",", "d_hnet", ",", "config", ",", "re", ",", "device", ",", "\n", "bs", "=", "bs_per_task", ")", "\n", "\n", "# save some fake data to the writer", "\n", "", "", "if", "i", "%", "100", "==", "0", ":", "\n", "            ", "if", "X_fake", ".", "shape", "[", "0", "]", ">=", "15", ":", "\n", "                ", "fig_fake", "=", "_plotImages", "(", "X_fake", ",", "config", ",", "bs_per_task", ")", "\n", "writer", ".", "add_figure", "(", "'train_class_'", "+", "str", "(", "re", ")", "+", "'_fake'", ",", "\n", "fig_fake", ",", "global_step", "=", "i", ")", "\n", "\n", "# compute soft targets with copied network", "\n", "", "", "target_logits", "=", "net_copy", ".", "forward", "(", "X_fake", ")", ".", "detach", "(", ")", "\n", "Y_hat_ls", "=", "net", ".", "forward", "(", "X_fake", ".", "detach", "(", ")", ")", "\n", "\n", "###############", "\n", "# BUILD TARGETS", "\n", "###############", "\n", "od", "=", "config", ".", "out_dim", "\n", "\n", "if", "config", ".", "class_incremental", "or", "config", ".", "training_task_infer", ":", "\n", "# This is a bit complicated: If we train class/task incrementally", "\n", "# we skip thraining the classifier on the first task. ", "\n", "# So when starting to train the classifier on task 2, we have to", "\n", "# build a hard target for this first output neuron trained by", "\n", "# replay data. A soft target (on an untrained output) would not ", "\n", "# make sense.", "\n", "\n", "# output head over all output neurons already available", "\n", "            ", "task_out", "=", "[", "0", ",", "(", "t", "+", "1", ")", "*", "od", "]", "\n", "# create target with zero everywhere except from the current re", "\n", "zeros", "=", "torch", ".", "zeros", "(", "target_logits", "[", ":", ",", "0", ":", "(", "t", "+", "1", ")", "*", "od", "]", ".", "shape", ")", ".", "to", "(", "device", ")", "\n", "\n", "if", "config", ".", "hard_targets", "or", "(", "t", "==", "1", "and", "re", "==", "0", ")", ":", "\n", "                ", "zeros", "[", ":", ",", "re", "]", "=", "1", "\n", "", "else", ":", "\n", "                ", "zeros", "[", ":", ",", "0", ":", "t", "*", "od", "]", "=", "target_logits", "[", ":", ",", "0", ":", "t", "*", "od", "]", "\n", "\n", "", "targets", "=", "zeros", "\n", "Y_hat_ls", "=", "Y_hat_ls", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "\n", "", "elif", "config", ".", "cl_scenario", "==", "1", "or", "config", ".", "cl_scenario", "==", "2", ":", "\n", "            ", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "# take the task specific output neuron", "\n", "                ", "task_out", "=", "[", "re", "*", "od", ",", "re", "*", "od", "+", "od", "]", "\n", "", "else", ":", "\n", "# always all output neurons, only one head is used", "\n", "                ", "task_out", "=", "[", "0", ",", "od", "]", "\n", "\n", "", "Y_hat_ls", "=", "Y_hat_ls", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "target_logits", "=", "target_logits", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "# build hard targets i.e. one hots if this option is chosen", "\n", "if", "config", ".", "hard_targets", ":", "\n", "                ", "soft_targets", "=", "torch", ".", "sigmoid", "(", "target_logits", ")", "\n", "zeros", "=", "torch", ".", "zeros", "(", "Y_hat_ls", ".", "shape", ")", ".", "to", "(", "device", ")", "\n", "_", ",", "argmax", "=", "torch", ".", "max", "(", "soft_targets", ",", "1", ")", "\n", "targets", "=", "zeros", ".", "scatter_", "(", "1", ",", "argmax", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "# loss expects logits", "\n", "                ", "targets", "=", "target_logits", "\n", "", "", "else", ":", "\n", "# take all neurons used up until now", "\n", "\n", "# output head over all output neurons already available", "\n", "            ", "task_out", "=", "[", "0", ",", "(", "t", "+", "1", ")", "*", "od", "]", "\n", "# create target with zero everywhere except from the current re", "\n", "zeros", "=", "torch", ".", "zeros", "(", "target_logits", "[", ":", ",", "0", ":", "(", "t", "+", "1", ")", "*", "od", "]", ".", "shape", ")", ".", "to", "(", "device", ")", "\n", "\n", "# sigmoid over the output head(s) from all previous task", "\n", "soft_targets", "=", "torch", ".", "sigmoid", "(", "target_logits", "[", ":", ",", "0", ":", "t", "*", "od", "]", ")", "\n", "\n", "# compute one hots", "\n", "if", "config", ".", "hard_targets", ":", "\n", "                ", "_", ",", "argmax", "=", "torch", ".", "max", "(", "soft_targets", ",", "1", ")", "\n", "zeros", ".", "scatter_", "(", "1", ",", "argmax", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", ")", "\n", "", "else", ":", "\n", "# loss expects logits", "\n", "                ", "zeros", "[", ":", ",", "0", ":", "t", "*", "od", "]", "=", "target_logits", "[", ":", ",", "0", ":", "t", "*", "od", "]", "\n", "", "targets", "=", "zeros", "\n", "# choose the correct output size for the actual ", "\n", "Y_hat_ls", "=", "Y_hat_ls", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "\n", "# add to list", "\n", "", "all_targets", ".", "append", "(", "targets", ")", "\n", "all_Y_hat_ls", ".", "append", "(", "Y_hat_ls", ")", "\n", "\n", "# cat to one tensor", "\n", "", "all_targets", "=", "torch", ".", "cat", "(", "all_targets", ")", "\n", "Y_hat_ls", "=", "torch", ".", "cat", "(", "all_Y_hat_ls", ")", "\n", "\n", "if", "i", "%", "200", "==", "0", ":", "\n", "        ", "classifier_accuracy", "=", "Classifier", ".", "accuracy", "(", "Y_hat_ls", ",", "all_targets", ")", "*", "100.0", "\n", "msg", "=", "'Training step {}: Classifier Accuracy: {:.3f} '", "+", "'(on current FAKE DATA training batch).'", "\n", "print", "(", "msg", ".", "format", "(", "i", ",", "classifier_accuracy", ")", ")", "\n", "\n", "# dependent on the target softness, the loss function is chosen", "\n", "", "if", "config", ".", "hard_targets", "or", "(", "config", ".", "class_incremental", "and", "t", "==", "1", ")", ":", "\n", "        ", "return", "Classifier", ".", "logit_cross_entropy_loss", "(", "Y_hat_ls", ",", "all_targets", ")", "\n", "", "else", ":", "\n", "        ", "return", "Classifier", ".", "knowledge_distillation_loss", "(", "Y_hat_ls", ",", "all_targets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.train_class_one_t": [[542, 784], ["dhandler_class.reset_batch_generator", "range", "net.train", "net_hnet.train", "list", "torch.Adam", "torch.Adam", "net.train", "torch.Adam", "dec.eval", "d_hnet.eval", "copy.deepcopy", "int", "optim.Adam.zero_grad", "dhandler_class.next_train_batch", "dhandler_class.input_to_torch_tensor", "dhandler_class.output_to_torch_tensor", "net.forward", "Classifier.softmax_and_cross_entropy.backward", "net.parameters", "utils.get_current_targets", "int", "optim.Adam.zero_grad", "mnist.plotting._plotImages", "writer.add_figure", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "net_hnet.forward", "torch.where", "torch.where", "torch.where", "soft_targets.to.to", "mnets.classifier_interface.Classifier.softmax_and_cross_entropy", "mnets.classifier_interface.Classifier.softmax_and_cross_entropy", "train_splitMNIST.get_fake_data_loss", "loss_reg.backward", "optim.Adam.step", "optim.Adam.step", "net.forward", "torch.softmax", "writer.add_scalar", "writer.add_scalar", "print", "net_hnet.get_task_emb", "p.detach().clone", "p.detach().clone", "numpy.ceil", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "utils.calc_delta_theta", "utils.calc_fix_target_reg", "mnets.classifier_interface.Classifier.accuracy", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "msg.format", "net_hnet.get_task_embs", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "net.parameters", "p.grad.data.norm", "net_hnet.get_task_emb", "p.grad.data.norm", "p.detach", "p.detach", "str", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "p.grad.data.norm.item", "p.grad.data.norm.item", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.reset_batch_generator", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.get_current_targets", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._plotImages", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.softmax_and_cross_entropy", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.softmax_and_cross_entropy", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.get_fake_data_loss", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.calc_delta_theta", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_fix_target_reg", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.accuracy", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb"], ["", "", "def", "train_class_one_t", "(", "dhandler_class", ",", "dhandlers_rp", ",", "dec", ",", "d_hnet", ",", "net", ",", "\n", "device", ",", "config", ",", "writer", ",", "t", ")", ":", "\n", "    ", "\"\"\"Train continual learning experiments on MNIST dataset for one task.\n    In this function the main training logic is implemented. \n    After setting the optimizers for the network and hypernetwork if \n    applicable, the training is structured as follows: \n    First, we get the a training batch of the current task. Depending on \n    the learning scenario, we choose output heads and build targets \n    accordingly. \n    Second, if ``t`` is greater than 1, we add a loss term concerning \n    predictions of replayed data. See :func:`get_fake_data_loss` for \n    details. Third, to protect the hypernetwork from forgetting, we add an \n    additional L2 loss term namely the difference between its current output \n    given an embedding and checkpointed targets.\n    Finally, we track some training statistics.\n\n    Args:\n        (....): See docstring of function :func:`train_tasks`.\n        t: Task id.\n    \"\"\"", "\n", "\n", "# if cl with task inference we have the classifier empowered with a hnet ", "\n", "if", "config", ".", "training_with_hnet", ":", "\n", "        ", "net_hnet", "=", "net", "[", "1", "]", "\n", "net", "=", "net", "[", "0", "]", "\n", "net", ".", "train", "(", ")", "\n", "net_hnet", ".", "train", "(", ")", "\n", "params_to_regularize", "=", "list", "(", "net_hnet", ".", "theta", ")", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "params_to_regularize", ",", "\n", "lr", "=", "config", ".", "class_lr", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ")", "\n", "\n", "c_emb_optimizer", "=", "optim", ".", "Adam", "(", "[", "net_hnet", ".", "get_task_emb", "(", "t", ")", "]", ",", "\n", "lr", "=", "config", ".", "class_lr_emb", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ")", "\n", "", "else", ":", "\n", "        ", "net", ".", "train", "(", ")", "\n", "net_hnet", "=", "None", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "net", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "config", ".", "class_lr", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ")", "\n", "\n", "# dont train the replay model if available", "\n", "", "if", "dec", "is", "not", "None", ":", "\n", "        ", "dec", ".", "eval", "(", ")", "\n", "", "if", "d_hnet", "is", "not", "None", ":", "\n", "        ", "d_hnet", ".", "eval", "(", ")", "\n", "\n", "# compute targets if classifier is trained with hnet", "\n", "", "if", "t", ">", "0", "and", "config", ".", "training_with_hnet", ":", "\n", "        ", "if", "config", ".", "online_target_computation", ":", "\n", "# Compute targets for the regularizer whenever they are needed.", "\n", "# -> Computationally expensive.", "\n", "            ", "targets_C", "=", "None", "\n", "prev_theta", "=", "[", "p", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "p", "in", "net_hnet", ".", "theta", "]", "\n", "prev_task_embs", "=", "[", "p", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "p", "in", "net_hnet", ".", "get_task_embs", "(", ")", "]", "\n", "", "else", ":", "\n", "# Compute targets for the regularizer once and keep them all in", "\n", "# memory -> Memory expensive.", "\n", "            ", "targets_C", "=", "hreg", ".", "get_current_targets", "(", "t", ",", "net_hnet", ")", "\n", "prev_theta", "=", "None", "\n", "prev_task_embs", "=", "None", "\n", "\n", "\n", "", "", "dhandler_class", ".", "reset_batch_generator", "(", ")", "\n", "\n", "# make copy of network", "\n", "if", "t", ">=", "1", ":", "\n", "        ", "net_copy", "=", "copy", ".", "deepcopy", "(", "net", ")", "\n", "\n", "# set training_iterations if epochs are set", "\n", "", "if", "config", ".", "epochs", "==", "-", "1", ":", "\n", "        ", "training_iterations", "=", "config", ".", "n_iter", "\n", "", "else", ":", "\n", "        ", "assert", "(", "config", ".", "epochs", ">", "0", ")", "\n", "training_iterations", "=", "config", ".", "epochs", "*", "int", "(", "np", ".", "ceil", "(", "dhandler_class", ".", "num_train_samples", "/", "config", ".", "batch_size", ")", ")", "\n", "\n", "", "if", "config", ".", "class_incremental", ":", "\n", "        ", "training_iterations", "=", "int", "(", "training_iterations", "/", "config", ".", "out_dim", ")", "\n", "\n", "# Whether we will calculate the regularizer.", "\n", "", "calc_reg", "=", "t", ">", "0", "and", "config", ".", "class_beta", ">", "0", "and", "config", ".", "training_with_hnet", "\n", "\n", "# set if we want the reg only computed for a subset of the  previous tasks", "\n", "if", "config", ".", "hnet_reg_batch_size", "!=", "-", "1", ":", "\n", "        ", "hnet_reg_batch_size", "=", "config", ".", "hnet_reg_batch_size", "\n", "", "else", ":", "\n", "        ", "hnet_reg_batch_size", "=", "None", "\n", "\n", "", "for", "i", "in", "range", "(", "training_iterations", ")", ":", "\n", "\n", "# set optimizer to zero", "\n", "        ", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "net_hnet", "is", "not", "None", ":", "\n", "            ", "c_emb_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# Get real data", "\n", "", "real_batch", "=", "dhandler_class", ".", "next_train_batch", "(", "config", ".", "batch_size", ")", "\n", "X_real", "=", "dhandler_class", ".", "input_to_torch_tensor", "(", "real_batch", "[", "0", "]", ",", "device", ",", "\n", "mode", "=", "'train'", ")", "\n", "T_real", "=", "dhandler_class", ".", "output_to_torch_tensor", "(", "real_batch", "[", "1", "]", ",", "device", ",", "\n", "mode", "=", "'train'", ")", "\n", "\n", "if", "i", "%", "100", "==", "0", "and", "config", ".", "show_plots", ":", "\n", "            ", "fig_real", "=", "_plotImages", "(", "X_real", ",", "config", ")", "\n", "writer", ".", "add_figure", "(", "'train_class_'", "+", "str", "(", "t", ")", "+", "'_real'", ",", "\n", "fig_real", ",", "global_step", "=", "i", ")", "\n", "\n", "#################################################", "\n", "# Choosing output heads and constructing targets", "\n", "################################################# ", "\n", "\n", "# If we train a task inference net or class incremental learning we ", "\n", "# we construct a target for every single class/task", "\n", "", "if", "config", ".", "class_incremental", "or", "config", ".", "training_task_infer", ":", "\n", "# in the beginning of training, we look at two output neuron", "\n", "            ", "task_out", "=", "[", "0", ",", "t", "+", "1", "]", "\n", "T_real", "=", "torch", ".", "zeros", "(", "(", "config", ".", "batch_size", ",", "task_out", "[", "1", "]", ")", ")", ".", "to", "(", "device", ")", "\n", "T_real", "[", ":", ",", "task_out", "[", "1", "]", "-", "1", "]", "=", "1", "\n", "\n", "", "elif", "config", ".", "cl_scenario", "==", "1", "or", "config", ".", "cl_scenario", "==", "2", ":", "\n", "            ", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "# take the task specific output neuron", "\n", "                ", "task_out", "=", "[", "t", "*", "config", ".", "out_dim", ",", "t", "*", "config", ".", "out_dim", "+", "config", ".", "out_dim", "]", "\n", "", "else", ":", "\n", "# always all output neurons, only one head is used", "\n", "                ", "task_out", "=", "[", "0", ",", "config", ".", "out_dim", "]", "\n", "", "", "else", ":", "\n", "# The number of output neurons is generic and can grow i.e. we", "\n", "# do not have to know the number of tasks before we start ", "\n", "# learning.", "\n", "            ", "if", "not", "config", ".", "infer_output_head", ":", "\n", "                ", "task_out", "=", "[", "0", ",", "(", "t", "+", "1", ")", "*", "config", ".", "out_dim", "]", "\n", "T_real", "=", "torch", ".", "cat", "(", "(", "torch", ".", "zeros", "(", "(", "config", ".", "batch_size", ",", "\n", "t", "*", "config", ".", "out_dim", ")", ")", ".", "to", "(", "device", ")", ",", "\n", "T_real", ")", ",", "dim", "=", "1", ")", "\n", "# this is a special case where we will infer the task id by another ", "\n", "# neural network so we can train on the correct output head direclty", "\n", "# and use the infered output head to compute the prediction", "\n", "", "else", ":", "\n", "                ", "task_out", "=", "[", "t", "*", "config", ".", "out_dim", ",", "t", "*", "config", ".", "out_dim", "+", "config", ".", "out_dim", "]", "\n", "\n", "# compute loss of current data", "\n", "", "", "if", "config", ".", "training_with_hnet", ":", "\n", "            ", "weights_c", "=", "net_hnet", ".", "forward", "(", "t", ")", "\n", "", "else", ":", "\n", "            ", "weights_c", "=", "None", "\n", "\n", "", "Y_hat_logits", "=", "net", ".", "forward", "(", "X_real", ",", "weights_c", ")", "\n", "Y_hat_logits", "=", "Y_hat_logits", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "\n", "if", "config", ".", "soft_targets", ":", "\n", "            ", "soft_label", "=", "0.95", "\n", "num_classes", "=", "T_real", ".", "shape", "[", "1", "]", "\n", "soft_targets", "=", "torch", ".", "where", "(", "T_real", "==", "1", ",", "\n", "torch", ".", "Tensor", "(", "[", "soft_label", "]", ")", ".", "to", "(", "device", ")", ",", "\n", "torch", ".", "Tensor", "(", "[", "(", "1", "-", "soft_label", ")", "/", "(", "num_classes", "-", "1", ")", "]", ")", ".", "to", "(", "device", ")", ")", "\n", "soft_targets", "=", "soft_targets", ".", "to", "(", "device", ")", "\n", "loss_task", "=", "Classifier", ".", "softmax_and_cross_entropy", "(", "Y_hat_logits", ",", "\n", "soft_targets", ")", "\n", "", "else", ":", "\n", "            ", "loss_task", "=", "Classifier", ".", "softmax_and_cross_entropy", "(", "Y_hat_logits", ",", "T_real", ")", "\n", "\n", "############################", "\n", "# compute loss for fake data", "\n", "############################", "\n", "\n", "# Get fake data (of all tasks up until now and merge into list)", "\n", "", "if", "t", ">=", "1", "and", "not", "config", ".", "training_with_hnet", ":", "\n", "            ", "fake_loss", "=", "get_fake_data_loss", "(", "dhandlers_rp", ",", "net", ",", "dec", ",", "d_hnet", ",", "device", ",", "\n", "config", ",", "writer", ",", "t", ",", "i", ",", "net_copy", ")", "\n", "loss_task", "=", "(", "1", "-", "config", ".", "l_rew", ")", "*", "loss_task", "+", "config", ".", "l_rew", "*", "fake_loss", "\n", "\n", "\n", "", "loss_task", ".", "backward", "(", "retain_graph", "=", "calc_reg", ",", "create_graph", "=", "calc_reg", "and", "config", ".", "backprop_dt", ")", "\n", "\n", "# compute hypernet loss and fix embedding -> change current embs", "\n", "if", "calc_reg", ":", "\n", "            ", "if", "config", ".", "no_lookahead", ":", "\n", "                ", "dTheta", "=", "None", "\n", "", "else", ":", "\n", "                ", "dTheta", "=", "opstep", ".", "calc_delta_theta", "(", "optimizer", ",", "\n", "config", ".", "use_sgd_change", ",", "lr", "=", "config", ".", "class_lr", ",", "\n", "detach_dt", "=", "not", "config", ".", "backprop_dt", ")", "\n", "", "loss_reg", "=", "config", ".", "class_beta", "*", "hreg", ".", "calc_fix_target_reg", "(", "net_hnet", ",", "t", ",", "\n", "targets", "=", "targets_C", ",", "mnet", "=", "net", ",", "dTheta", "=", "dTheta", ",", "dTembs", "=", "None", ",", "\n", "prev_theta", "=", "prev_theta", ",", "prev_task_embs", "=", "prev_task_embs", ",", "\n", "batch_size", "=", "hnet_reg_batch_size", ")", "\n", "loss_reg", ".", "backward", "(", ")", "\n", "\n", "# compute backward passloss_task.backward()", "\n", "", "if", "not", "config", ".", "dont_train_main_model", ":", "\n", "            ", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "if", "net_hnet", "is", "not", "None", "and", "config", ".", "train_class_embeddings", ":", "\n", "            ", "c_emb_optimizer", ".", "step", "(", ")", "\n", "\n", "# same stats saving", "\n", "", "if", "i", "%", "50", "==", "0", ":", "\n", "# compute accuracies for tracking", "\n", "            ", "Y_hat_logits", "=", "net", ".", "forward", "(", "X_real", ",", "weights_c", ")", "\n", "Y_hat_logits", "=", "Y_hat_logits", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "Y_hat", "=", "F", ".", "softmax", "(", "Y_hat_logits", ",", "dim", "=", "1", ")", "\n", "classifier_accuracy", "=", "Classifier", ".", "accuracy", "(", "Y_hat", ",", "T_real", ")", "*", "100.0", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/class_accuracy'", "%", "t", ",", "\n", "classifier_accuracy", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/loss_task'", "%", "t", ",", "\n", "loss_task", ",", "i", ")", "\n", "if", "t", ">=", "1", "and", "not", "config", ".", "training_with_hnet", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "'train/task_%d/fake_loss'", "%", "t", ",", "\n", "fake_loss", ",", "i", ")", "\n", "\n", "# plot some gradient statistics", "\n", "", "", "if", "i", "%", "200", "==", "0", ":", "\n", "            ", "if", "not", "config", ".", "dont_train_main_model", ":", "\n", "                ", "total_norm", "=", "0", "\n", "if", "config", ".", "training_with_hnet", ":", "\n", "                    ", "params", "=", "net_hnet", ".", "theta", "\n", "", "else", ":", "\n", "                    ", "params", "=", "net", ".", "parameters", "(", ")", "\n", "\n", "", "for", "p", "in", "params", ":", "\n", "                    ", "param_norm", "=", "p", ".", "grad", ".", "data", ".", "norm", "(", "2", ")", "\n", "total_norm", "+=", "param_norm", ".", "item", "(", ")", "**", "2", "\n", "", "total_norm", "=", "total_norm", "**", "(", "1.", "/", "2", ")", "\n", "# TODO write gradient histograms?", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/main_params_grad_norms'", "%", "t", ",", "\n", "total_norm", ",", "i", ")", "\n", "\n", "", "if", "net_hnet", "is", "not", "None", "and", "config", ".", "train_class_embeddings", ":", "\n", "                    ", "total_norm", "=", "0", "\n", "for", "p", "in", "[", "net_hnet", ".", "get_task_emb", "(", "t", ")", "]", ":", "\n", "                        ", "param_norm", "=", "p", ".", "grad", ".", "data", ".", "norm", "(", "2", ")", "\n", "total_norm", "+=", "param_norm", ".", "item", "(", ")", "**", "2", "\n", "", "total_norm", "=", "total_norm", "**", "(", "1.", "/", "2", ")", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/hnet_emb_grad_norms'", "%", "t", ",", "\n", "total_norm", ",", "i", ")", "\n", "\n", "", "", "if", "i", "%", "200", "==", "0", ":", "\n", "            ", "msg", "=", "'Training step {}: Classifier Accuracy: {:.3f} '", "+", "'(on current training batch).'", "\n", "print", "(", "msg", ".", "format", "(", "i", ",", "classifier_accuracy", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.train_tasks": [[785, 850], ["print", "range", "mnist.replay.train_replay.init_plotting_embedding", "train_splitMNIST.test", "during_accs.append", "print", "train_splitMNIST.train_class_one_t", "print", "train_splitMNIST.train_class_one_t", "mnist.replay.train_gan.train_gan_one_t", "mnist.replay.train_replay.train_vae_one_t"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.init_plotting_embedding", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.train_class_one_t", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.train_class_one_t", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_gan.train_gan_one_t", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train_vae_one_t"], ["", "", "", "def", "train_tasks", "(", "dhandlers_class", ",", "dhandlers_rp", ",", "enc", ",", "dec", ",", "d_hnet", ",", "class_net", ",", "\n", "device", ",", "config", ",", "writer", ",", "infer_net", "=", "None", ")", ":", "\n", "    ", "\"\"\" Train continual learning experiments on MNIST dataset.\n    This is a helper function that loops over the range of tasks and \n    iteratively starts training the classifier and the replay model \n    on new tasks. Additionally, we save the task performace just after \n    training which can later be compared to the performance after training \n    on all tasks.\n\n    Args:\n        dhandlers_class: The dataset handlers for classification.\n        dhandlers_rp: The dataset handlers from the replay.\n        enc: The model of the encoder network.\n        dec: The model of the decoder network.\n        d_hnet. The model of the decoder hyper network.\n        class_net: The model of the classifier.\n        device: Torch device (cpu or gpu).\n        config: The command line arguments.\n        writer: The tensorboard summary writer.\n        infer_net: (optional) Task inference net, only used for testing.\n\n    Returns:\n        A list of test accuracies of all tasks directly after training.\n    \"\"\"", "\n", "\n", "print", "(", "'Training MNIST (task inference) classifier ...'", ")", "\n", "\n", "if", "not", "(", "config", ".", "upper_bound", "or", "(", "config", ".", "infer_task_id", "and", "\n", "config", ".", "cl_scenario", "==", "1", ")", ")", ":", "\n", "            ", "if", "not", "config", ".", "trained_replay_model", ":", "\n", "                ", "embd_list", "=", "init_plotting_embedding", "(", "dhandlers_rp", ",", "\n", "d_hnet", ",", "writer", ",", "config", ")", "\n", "\n", "", "", "during_accs", "=", "[", "]", "\n", "# Begin training loop for the single tasks", "\n", "for", "t", "in", "range", "(", "0", ",", "config", ".", "num_tasks", ")", ":", "\n", "        ", "dhandler", "=", "dhandlers_class", "[", "t", "]", "\n", "if", "class_net", "is", "not", "None", ":", "\n", "            ", "if", "not", "(", "config", ".", "class_incremental", "and", "t", "==", "0", ")", ":", "\n", "                ", "print", "(", "\"Training classifier on data handler: \"", ",", "t", ")", "\n", "train_class_one_t", "(", "dhandler", ",", "dhandlers_rp", ",", "dec", ",", "\n", "d_hnet", ",", "class_net", ",", "device", ",", "config", ",", "writer", ",", "t", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "t", ">", "0", ":", "\n", "                ", "print", "(", "\"Training task inference system on data handler: \"", ",", "t", ")", "\n", "train_class_one_t", "(", "dhandler", ",", "dhandlers_rp", ",", "dec", ",", "\n", "d_hnet", ",", "infer_net", ",", "device", ",", "config", ",", "writer", ",", "t", ")", "\n", "\n", "", "", "if", "not", "(", "t", "==", "0", "and", "class_net", "is", "None", ")", ":", "\n", "            ", "durring_cc", "=", "test", "(", "[", "dhandler", "]", ",", "class_net", ",", "infer_net", ",", "device", ",", "\n", "config", ",", "writer", ",", "task_id", "=", "t", ")", "\n", "during_accs", ".", "append", "(", "durring_cc", ")", "\n", "\n", "", "if", "not", "(", "config", ".", "upper_bound", "or", "(", "config", ".", "infer_task_id", "and", "\n", "config", ".", "cl_scenario", "==", "1", ")", ")", ":", "\n", "\n", "            ", "if", "not", "config", ".", "trained_replay_model", "and", "t", "<", "config", ".", "num_tasks", "-", "1", ":", "\n", "                ", "if", "config", ".", "replay_method", "==", "'gan'", ":", "\n", "                    ", "train_gan_one_t", "(", "dhandlers_rp", "[", "t", "]", ",", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "\n", "config", ",", "writer", ",", "embd_list", ",", "t", ")", "\n", "", "else", ":", "\n", "                    ", "train_vae_one_t", "(", "dhandlers_rp", "[", "t", "]", ",", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "\n", "config", ",", "writer", ",", "embd_list", ",", "t", ")", "\n", "\n", "", "", "", "", "return", "during_accs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.run": [[851, 988], ["mnist.train_args.parse_cmd_arguments", "mnist.train_utils._generate_tasks", "mnist.replay.train_replay.run", "print", "mnist.train_utils.generate_classifier", "train_splitMNIST.train_tasks", "print", "train_splitMNIST.test", "train_splitMNIST._save_performance_summary", "writer.close", "print", "mnist.train_args_default._set_default", "print", "print", "mnist.train_utils.generate_classifier", "train_splitMNIST.train_tasks", "print", "train_splitMNIST.test"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils.parse_cmd_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_tasks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.run", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_utils.generate_classifier", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.train_tasks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST._save_performance_summary", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args_default._set_default", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_utils.generate_classifier", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_splitMNIST.train_tasks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test"], ["", "def", "run", "(", "mode", "=", "'split'", ")", ":", "\n", "\n", "    ", "\"\"\" Method to start MNIST experiments. \n    Depending on the configurations, here we control the creation and \n    training of the different (replay) modules for classification or \n    task inference build out of standart neural networks and their \n    corresponding hypernetworks.\n\n    Args:\n        mode (str): Training mode defines which experiments and default values \n        are loaded. Options are splitMNIST or permutedMNIST:\n\n                - ``split``\n                - ``perm``\n    \"\"\"", "\n", "\n", "### Get command line arguments.", "\n", "config", "=", "train_args", ".", "parse_cmd_arguments", "(", "mode", "=", "mode", ")", "\n", "\n", "assert", "(", "config", ".", "experiment", "==", "\"splitMNIST\"", "or", "config", ".", "experiment", "==", "\"permutedMNIST\"", ")", "\n", "if", "not", "config", ".", "dont_set_default", ":", "\n", "        ", "config", "=", "_set_default", "(", "config", ")", "\n", "\n", "", "if", "config", ".", "infer_output_head", ":", "\n", "        ", "assert", "(", "config", ".", "infer_task_id", "==", "True", ")", "\n", "\n", "", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "        ", "assert", "(", "config", ".", "class_incremental", "==", "False", ")", "\n", "assert", "(", "config", ".", "single_class_replay", "==", "False", ")", "\n", "\n", "", "if", "config", ".", "infer_with_entropy", ":", "\n", "        ", "assert", "(", "config", ".", "infer_task_id", "==", "True", ")", "\n", "# single class only implemented for splitMNIST", "\n", "", "if", "config", ".", "single_class_replay", "or", "config", ".", "class_incremental", ":", "\n", "        ", "assert", "(", "config", ".", "experiment", "==", "\"splitMNIST\"", ")", "\n", "\n", "# check range of number of tasks", "\n", "", "assert", "(", "config", ".", "num_tasks", ">", "0", ")", "\n", "if", "config", ".", "experiment", "==", "\"splitMNIST\"", ":", "\n", "        ", "if", "config", ".", "class_incremental", ":", "\n", "            ", "assert", "(", "config", ".", "num_tasks", "<=", "10", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "config", ".", "num_tasks", "<=", "5", ")", "\n", "\n", "# the following combination is not supported ", "\n", "", "", "if", "config", ".", "infer_task_id", ":", "\n", "        ", "assert", "(", "config", ".", "class_incremental", "==", "False", ")", "\n", "\n", "# enforce correct cl scenario", "\n", "", "if", "config", ".", "class_incremental", ":", "\n", "        ", "config", ".", "single_class_replay", "=", "1", "\n", "config", ".", "cl_scenario", "=", "3", "\n", "print", "(", "\"Attention: Cl scenario 3 is enforced!\"", ")", "\n", "steps", "=", "1", "\n", "", "else", ":", "\n", "        ", "steps", "=", "2", "\n", "\n", "#### Get data handlers", "\n", "", "dhandlers_class", "=", "train_utils", ".", "_generate_tasks", "(", "config", ",", "steps", ")", "\n", "\n", "# decide if you want to train a replay model", "\n", "# in the case where you only want a classifier and you know the task id", "\n", "# we only train a classifier + hnet. Upper bound considers the replay case ", "\n", "# but you replay real data as if the replayu model would be \"perfect\".", "\n", "if", "config", ".", "upper_bound", "or", "(", "config", ".", "infer_task_id", "and", "config", ".", "cl_scenario", "==", "1", ")", ":", "\n", "        ", "train_rp", "=", "False", "\n", "", "else", ":", "\n", "         ", "train_rp", "=", "True", "\n", "\n", "### Get replay model trained continually with hnet.", "\n", "", "dec", ",", "d_hnet", ",", "enc", ",", "dhandlers_rp", ",", "device", ",", "writer", ",", "config", "=", "replay_model", "(", "config", ",", "train_rp", ")", "\n", "\n", "# if we have a replay model trained, we now train a classifier", "\n", "# that either solves a task directly (HNET+replay) or we train a model", "\n", "# that infers the task from input.", "\n", "\n", "###############################", "\n", "# Train task inference network", "\n", "###############################", "\n", "\n", "if", "config", ".", "infer_task_id", "and", "not", "config", ".", "cl_scenario", "==", "1", "and", "not", "config", ".", "infer_with_entropy", ":", "\n", "        ", "print", "(", "\"Training task inference model ...\"", ")", "\n", "config", ".", "trained_replay_model", "=", "False", "\n", "config", ".", "training_task_infer", "=", "True", "\n", "config", ".", "training_with_hnet", "=", "False", "\n", "### Generate task inference network.", "\n", "infer_net", "=", "train_utils", ".", "generate_classifier", "(", "config", ",", "\n", "dhandlers_class", ",", "device", ")", "\n", "\n", "### Train the task inference network.", "\n", "config", ".", "during_accs_inference", "=", "train_tasks", "(", "dhandlers_class", ",", "\n", "dhandlers_rp", ",", "enc", ",", "dec", ",", "d_hnet", ",", "None", ",", "\n", "device", ",", "config", ",", "writer", ",", "infer_net", "=", "infer_net", ")", "\n", "### Test network.", "\n", "print", "(", "\"Testing task inference model ...\"", ")", "\n", "test", "(", "dhandlers_class", ",", "None", ",", "infer_net", ",", "device", ",", "config", ",", "writer", ")", "\n", "config", ".", "training_with_hnet", "=", "True", "\n", "config", ".", "trained_replay_model", "=", "True", "\n", "", "else", ":", "\n", "# if we do not train an inference network we just train a model ", "\n", "# that knows it all and not", "\n", "        ", "infer_net", "=", "None", "\n", "if", "config", ".", "infer_with_entropy", ":", "\n", "            ", "config", ".", "trained_replay_model", "=", "True", "\n", "", "else", ":", "\n", "            ", "config", ".", "trained_replay_model", "=", "False", "\n", "\n", "", "", "if", "config", ".", "infer_task_id", ":", "\n", "        ", "config", ".", "training_with_hnet", "=", "True", "\n", "", "else", ":", "\n", "        ", "config", ".", "training_with_hnet", "=", "False", "\n", "\n", "###################", "\n", "# Train classifier", "\n", "###################", "\n", "\n", "", "config", ".", "training_task_infer", "=", "False", "\n", "\n", "print", "(", "\"Training final classifier ...\"", ")", "\n", "### Generate another classifier network.", "\n", "class_nets", "=", "train_utils", ".", "generate_classifier", "(", "config", ",", "\n", "dhandlers_class", ",", "device", ")", "\n", "### Train the network.", "\n", "config", ".", "during_accs_final", "=", "train_tasks", "(", "dhandlers_class", ",", "dhandlers_rp", ",", "enc", ",", "\n", "dec", ",", "d_hnet", ",", "class_nets", ",", "device", ",", "config", ",", "writer", ",", "infer_net", ")", "\n", "\n", "print", "(", "\"Testing final classifier ...\"", ")", "\n", "### Test network.", "\n", "test", "(", "dhandlers_class", ",", "class_nets", ",", "infer_net", ",", "device", ",", "config", ",", "writer", ")", "\n", "\n", "_save_performance_summary", "(", "config", ")", "\n", "writer", ".", "close", "(", ")", "\n", "\n", "print", "(", "'Program finished successfully.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.__init__": [[78, 149], ["torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.Module.__init__", "torch.Module.__init__", "utils.module_wrappers.CLHyperNetInterface.__init__", "toy_example.hyper_model.HyperNetwork", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "int", "int", "print", "print", "chunked_hyper_model.ChunkedHyperNetworkHandler._is_properly_setup", "len", "warnings.warn", "numpy.ceil", "torch.Parameter", "torch.Parameter", "torch.init.normal_", "torch.init.normal_", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "numpy.sum", "chunked_hyper_model.ChunkedHyperNetworkHandler._embs.numel", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "t.numel", "chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs"], ["def", "__init__", "(", "self", ",", "target_shapes", ",", "num_tasks", ",", "chunk_dim", "=", "2586", ",", "\n", "layers", "=", "[", "50", ",", "100", "]", ",", "te_dim", "=", "8", ",", "activation_fn", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "use_bias", "=", "True", ",", "no_weights", "=", "False", ",", "ce_dim", "=", "None", ",", "\n", "init_weights", "=", "None", ",", "dropout_rate", "=", "-", "1", ",", "noise_dim", "=", "-", "1", ",", "\n", "temb_std", "=", "-", "1", ")", ":", "\n", "# FIXME find a way using super to handle multiple inheritence.", "\n", "#super(ChunkedHyperNetworkHandler, self).__init__()", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "CLHyperNetInterface", ".", "__init__", "(", "self", ")", "\n", "\n", "assert", "(", "len", "(", "target_shapes", ")", ">", "0", ")", "\n", "assert", "(", "init_weights", "is", "None", "or", "no_weights", "is", "False", ")", "\n", "assert", "(", "ce_dim", "is", "not", "None", ")", "\n", "self", ".", "_target_shapes", "=", "target_shapes", "\n", "self", ".", "_num_tasks", "=", "num_tasks", "\n", "self", ".", "_ce_dim", "=", "ce_dim", "\n", "self", ".", "_chunk_dim", "=", "chunk_dim", "\n", "self", ".", "_layers", "=", "layers", "\n", "self", ".", "_use_bias", "=", "use_bias", "\n", "self", ".", "_act_fn", "=", "activation_fn", "\n", "self", ".", "_init_weights", "=", "init_weights", "\n", "self", ".", "_no_weights", "=", "no_weights", "\n", "self", ".", "_te_dim", "=", "te_dim", "\n", "self", ".", "_noise_dim", "=", "noise_dim", "\n", "self", ".", "_temb_std", "=", "temb_std", "\n", "self", ".", "_shifts", "=", "None", "# FIXME temporary test.", "\n", "\n", "# FIXME: weights should incorporate chunk embeddings as they are part of", "\n", "# theta.", "\n", "if", "init_weights", "is", "not", "None", ":", "\n", "            ", "warn", "(", "'Argument \"init_weights\" does not yet allow initialization '", "+", "\n", "'of chunk embeddings.'", ")", "\n", "\n", "### Generate Hypernet with chunk_dim output.", "\n", "# Note, we can safely pass \"temb_std\" to the full hypernetwork, as we", "\n", "# process all chunks in one big batch and the hypernet will use the same", "\n", "# perturbed task embeddings for that reason (i.e., noise is shared).", "\n", "", "self", ".", "_hypernet", "=", "HyperNetwork", "(", "[", "[", "chunk_dim", "]", "]", ",", "num_tasks", ",", "verbose", "=", "False", ",", "\n", "layers", "=", "layers", ",", "te_dim", "=", "te_dim", ",", "activation_fn", "=", "activation_fn", ",", "\n", "use_bias", "=", "use_bias", ",", "no_weights", "=", "no_weights", ",", "init_weights", "=", "init_weights", ",", "\n", "ce_dim", "=", "ce_dim", "+", "(", "noise_dim", "if", "noise_dim", "!=", "-", "1", "else", "0", ")", ",", "\n", "dropout_rate", "=", "dropout_rate", ",", "noise_dim", "=", "-", "1", ",", "temb_std", "=", "temb_std", ")", "\n", "\n", "self", ".", "_num_outputs", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "self", ".", "_target_shapes", ")", "\n", "### Generate embeddings for all weight chunks.", "\n", "self", ".", "_num_chunks", "=", "int", "(", "np", ".", "ceil", "(", "self", ".", "_num_outputs", "/", "chunk_dim", ")", ")", "\n", "if", "no_weights", ":", "\n", "            ", "self", ".", "_embs", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "_embs", "=", "nn", ".", "Parameter", "(", "data", "=", "torch", ".", "Tensor", "(", "self", ".", "_num_chunks", ",", "\n", "ce_dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "self", ".", "_embs", ",", "mean", "=", "0.", ",", "std", "=", "1.", ")", "\n", "\n", "# Note, the chunk embeddings are part of theta.", "\n", "", "hdims", "=", "self", ".", "_hypernet", ".", "theta_shapes", "\n", "ntheta", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "hdims", ")", "+", "(", "self", ".", "_embs", ".", "numel", "(", ")", "if", "not", "no_weights", "else", "0", ")", "\n", "\n", "ntembs", "=", "int", "(", "np", ".", "sum", "(", "[", "t", ".", "numel", "(", ")", "for", "t", "in", "self", ".", "get_task_embs", "(", ")", "]", ")", ")", "\n", "self", ".", "_num_weights", "=", "ntheta", "+", "ntembs", "\n", "print", "(", "'Constructed hypernetwork with %d parameters '", "%", "(", "ntheta", "+", "ntembs", ")", "+", "'(%d network weights + %d task embedding weights).'", "\n", "%", "(", "ntheta", ",", "ntembs", ")", ")", "\n", "\n", "print", "(", "'The hypernetwork has a total of %d outputs.'", "%", "self", ".", "_num_outputs", ")", "\n", "\n", "self", ".", "_theta_shapes", "=", "[", "[", "self", ".", "_num_chunks", ",", "ce_dim", "]", "]", "+", "self", ".", "_hypernet", ".", "theta_shapes", "\n", "\n", "self", ".", "_is_properly_setup", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.chunk_embeddings": [[150, 161], ["list", "torch.split", "torch.split", "torch.split", "torch.split"], "methods", ["None"], ["", "@", "property", "\n", "def", "chunk_embeddings", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`chunk_embeddings`.\n\n        Get the chunk embeddings used to produce a full set of main network\n        weights with the underlying (small) hypernetwork.\n\n        Returns:\n            A list of all chunk embedding vectors.\n        \"\"\"", "\n", "return", "list", "(", "torch", ".", "split", "(", "self", ".", "_embs", ",", "1", ",", "dim", "=", "0", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.forward": [[163, 240], ["chunked_hyper_model.ChunkedHyperNetworkHandler._hypernet.forward", "weights[].view", "enumerate", "Exception", "Exception", "NotImplementedError", "numpy.all", "eps.to.to.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "int", "W.view.view.view", "ret.append", "len", "len", "numpy.equal", "len", "len", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "eps.to.to.to", "numpy.prod", "list", "chunked_hyper_model.ChunkedHyperNetworkHandler._embs.get_device"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "forward", "(", "self", ",", "task_id", "=", "None", ",", "theta", "=", "None", ",", "dTheta", "=", "None", ",", "task_emb", "=", "None", ",", "\n", "ext_inputs", "=", "None", ",", "squeeze", "=", "True", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract super class method.\n\n        Note:\n            This methods can't handle external inputs yet!\n\n        The method will iterate through the set of internal chunk embeddings,\n        calling the internally maintained (small) full hypernetwork for each,\n        in order to generate a full set of main network weights.\n        \"\"\"", "\n", "if", "task_id", "is", "None", "and", "task_emb", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'The hyper network has to get either a task ID'", "+", "\n", "'to choose the learned embedding or directly '", "+", "\n", "'get an embedding as input (e.g. from a task '", "+", "\n", "'recognition model).'", ")", "\n", "\n", "", "if", "not", "self", ".", "has_theta", "and", "theta", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Network was generated without internal weights. '", "+", "\n", "'Hence, \"theta\" option may not be None.'", ")", "\n", "\n", "", "if", "ext_inputs", "is", "not", "None", ":", "\n", "# FIXME If this will be implemented, please consider:", "\n", "# * batch size will have to be multiplied based on num chunk", "\n", "#   embeddings and the number of external inputs -> large batches", "\n", "# * noise dim must adhere correct behavior (different noise per", "\n", "#   external input).", "\n", "            ", "raise", "NotImplementedError", "(", "'This hypernetwork implementation does '", "+", "\n", "'not yet support the passing of external inputs.'", ")", "\n", "\n", "", "if", "theta", "is", "None", ":", "\n", "            ", "theta", "=", "self", ".", "theta", "\n", "", "else", ":", "\n", "            ", "assert", "(", "len", "(", "theta", ")", "==", "len", "(", "self", ".", "theta_shapes", ")", ")", "\n", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "self", ".", "_embs", ".", "shape", ",", "list", "(", "theta", "[", "0", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "", "chunk_embs", "=", "theta", "[", "0", "]", "\n", "hnet_theta", "=", "theta", "[", "1", ":", "]", "\n", "\n", "if", "dTheta", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "dTheta", ")", "==", "len", "(", "self", ".", "theta_shapes", ")", ")", "\n", "\n", "chunk_embs", "=", "chunk_embs", "+", "dTheta", "[", "0", "]", "\n", "hnet_dTheta", "=", "dTheta", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "hnet_dTheta", "=", "None", "\n", "\n", "# Concatenate the same noise to all chunks, such that it can be", "\n", "# viewed as if it were an external input.", "\n", "", "if", "self", ".", "_noise_dim", "!=", "-", "1", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "eps", "=", "torch", ".", "randn", "(", "(", "1", ",", "self", ".", "_noise_dim", ")", ")", "\n", "", "else", ":", "\n", "                ", "eps", "=", "torch", ".", "zeros", "(", "(", "1", ",", "self", ".", "_noise_dim", ")", ")", "\n", "", "if", "self", ".", "_embs", ".", "is_cuda", ":", "\n", "                ", "eps", "=", "eps", ".", "to", "(", "self", ".", "_embs", ".", "get_device", "(", ")", ")", "\n", "\n", "", "eps", "=", "eps", ".", "expand", "(", "self", ".", "_num_chunks", ",", "self", ".", "_noise_dim", ")", "\n", "chunk_embs", "=", "torch", ".", "cat", "(", "[", "chunk_embs", ",", "eps", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# get chunked weights from HyperNet", "\n", "", "weights", "=", "self", ".", "_hypernet", ".", "forward", "(", "task_id", "=", "task_id", ",", "theta", "=", "hnet_theta", ",", "\n", "dTheta", "=", "hnet_dTheta", ",", "task_emb", "=", "task_emb", ",", "ext_inputs", "=", "chunk_embs", ")", "\n", "weights", "=", "weights", "[", "0", "]", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "\n", "### Reshape weights dependent on the main networks architecture.", "\n", "ind", "=", "0", "\n", "ret", "=", "[", "]", "\n", "for", "j", ",", "s", "in", "enumerate", "(", "self", ".", "target_shapes", ")", ":", "\n", "            ", "num", "=", "int", "(", "np", ".", "prod", "(", "s", ")", ")", "\n", "W", "=", "weights", "[", "0", "]", "[", "ind", ":", "ind", "+", "num", "]", "\n", "ind", "+=", "num", "\n", "W", "=", "W", ".", "view", "(", "*", "s", ")", "\n", "if", "self", ".", "_shifts", "is", "not", "None", ":", "# FIXME temporary test!", "\n", "                ", "W", "+=", "self", ".", "_shifts", "[", "j", "]", "\n", "", "ret", ".", "append", "(", "W", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.theta": [[242, 262], ["list"], "methods", ["None"], ["", "@", "property", "\n", "def", "theta", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute ``theta``.\n\n        ``theta`` are all learnable parameters of the chunked hypernet including\n        the chunk embeddings that need to be learned.\n        Not included are the task embeddings, i.e., ``theta`` comprises\n        all parameters that should be regularized in order to avoid\n        catastrophic forgetting when training the hypernetwork in a Continual\n        Learning setting.\n\n        Note:\n            Chunk embeddings are prepended to the list of weights ``theta`` from\n            the internal full hypernetwork.\n\n        Returns:\n            A list of tensors or ``None``, if ``no_weights`` was set to ``True``\n            in the constructor of this class.\n        \"\"\"", "\n", "return", "[", "self", ".", "_embs", "]", "+", "list", "(", "self", ".", "_hypernet", ".", "theta", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs": [[264, 267], ["chunked_hyper_model.ChunkedHyperNetworkHandler._hypernet.get_task_embs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs"], ["", "def", "get_task_embs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Overriden super class method.\"\"\"", "\n", "return", "self", ".", "_hypernet", ".", "get_task_embs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb": [[269, 272], ["chunked_hyper_model.ChunkedHyperNetworkHandler._hypernet.get_task_emb"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb"], ["", "def", "get_task_emb", "(", "self", ",", "task_id", ")", ":", "\n", "        ", "\"\"\"Overriden super class method.\"\"\"", "\n", "return", "self", ".", "_hypernet", ".", "get_task_emb", "(", "task_id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.has_theta": [[274, 278], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_theta", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute ``has_theta``.\"\"\"", "\n", "return", "not", "self", ".", "_no_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.has_task_embs": [[280, 284], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_task_embs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute ``has_task_embs``.\"\"\"", "\n", "return", "self", ".", "_hypernet", ".", "has_task_embs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.num_task_embs": [[286, 290], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_task_embs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute ``num_task_embs``.\"\"\"", "\n", "return", "self", ".", "_hypernet", ".", "num_task_embs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.apply_chunked_hyperfan_init": [[291, 576], ["enumerate", "numpy.prod", "range", "math.sqrt", "print", "enumerate", "enumerate", "enumerate", "ValueError", "ValueError", "chunk_vars.append", "min", "warnings.warn", "c_vars.append", "warnings.warn", "math.sqrt", "len", "target_vars.append", "utils.init_utils.calc_fan_in_and_out", "target_vars.append", "max", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "utils.init_utils.calc_fan_in_and_out", "min", "math.sqrt", "utils.init_utils.xavier_fan_in_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "len", "len", "numpy.prod", "len", "len"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.init_utils.calc_fan_in_and_out", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.init_utils.calc_fan_in_and_out", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.init_utils.xavier_fan_in_"], ["", "def", "apply_chunked_hyperfan_init", "(", "self", ",", "method", "=", "'in'", ",", "use_xavier", "=", "False", ",", "\n", "temb_var", "=", "1.", ",", "ext_inp_var", "=", "1.", ",", "eps", "=", "1e-5", ",", "\n", "cemb_normal_init", "=", "False", ")", ":", "\n", "        ", "r\"\"\"Initialize the network using a chunked hyperfan init.\n\n        Inspired by the method\n        `Hyperfan Init <https://openreview.net/forum?id=H1lma24tPB>`__ which we\n        implemented for the full hypernetwork in method\n        :meth:`toy_example.hyper_model.HyperNetwork.apply_hyperfan_init`, we\n        heuristically developed a better initialization method for chunked\n        hypernetworks.\n\n        Unfortunately, the `Hyperfan Init` method does not apply to this kind of\n        hypernetwork, since we reuse the same hypernet output head for the whole\n        main network.\n\n        Luckily, we can provide a simple heuristic. Similar to\n        `Meyerson & Miikkulainen <https://arxiv.org/abs/1906.00097>`__ we play\n        with the variance of the input embeddings to affect the variance of the\n        output weights.\n\n        In a chunked hypernetwork, the input for each chunk is identical except\n        for the chunk embeddings :math:`\\mathbf{c}`. Let :math:`\\mathbf{e}`\n        denote the remaining inputs to the hypernetwork, which are identical\n        for all chunks. Then, assuming the hypernetwork was initialized via\n        fan-in init, the variance of the hypernetwork output :math:`\\mathbf{v}`\n        can be written as follows (see documentation of method\n        :meth:`toy_example.hyper_model.HyperNetwork.apply_hyperfan_init`):\n\n        .. math::\n\n            \\text{Var}(v) = \\frac{n_e}{n_e+n_c} \\text{Var}(e) + \\\n                \\frac{n_c}{n_e+n_c} \\text{Var}(c)\n\n        Hence, we can achieve a desired output variance :math:`\\text{Var}(v)`\n        by initializing the chunk embeddinggs :math:`\\mathbf{c}` via the\n        following variance:\n\n        .. math::\n\n            \\text{Var}(c) = \\max \\Big\\{ 0, \\\n                \\frac{1}{n_c} \\big[ (n_e+n_c) \\text{Var}(v) - \\\n                n_e \\text{Var}(e) \\big] \\Big\\}\n\n        Now, one important question remains. How do we pick a desired output\n        variance :math:`\\text{Var}(v)` for a chunk?\n\n        Note, a chunk may include weights from several layers. The likelihood\n        for this to happen depends on the main net architecture and the chunk\n        size (see constructor argument ``chunk_dim``). The smaller the chunk\n        size, the less likely it is that a chunk will contain elements from\n        multiple main net weight tensors.\n\n        In case each chunk would contain only weights from one main net weight\n        tensor, we could simply pick the variance :math:`\\text{Var}(v)` that\n        would have been chosen by a main net initialization method (such as\n        Xavier).\n\n        In case a chunk contains contributions from several main net weight\n        tensors, we apply the following heuristic. If a chunk contains\n        contributions of a set of main network weight tensors\n        :math:`W_1, \\dots, W_K` with relative contribution sizes\\\n        :math:`n_1, \\dots, n_K` such that :math:`n_1 + \\dots + n_K = n_v` where\n        :math:`n_v` denotes the chunk size and if the corresponding main network\n        initialization method would require init varainces\n        :math:`\\text{Var}(w_1), \\dots, \\text{Var}(w_K)`, then we simply request\n        a weighted average as follow:\n\n        .. math::\n\n            \\text{Var}(v) = \\frac{1}{n_v} \\sum_{k=1}^K n_k \\text{Var}(w_k)\n\n        What about bias vectors? Usually, the variance analysis applied to\n        Xavier or Kaiming init assumes that biases are initialized to zero. This\n        is not possible in this setting, as it would require assigning a\n        negative variance to :math:`\\mathbf{c}`. Instead, we follow the default\n        PyTorch initialization (e.g., see method ``reset_parameters`` in class\n        :class:`torch.nn.Linear`). There, bias vectors are initialized uniformly\n        within a range of :math:`\\pm \\frac{1}{\\sqrt{f_{\\text{in}}}}` where\n        :math:`f_{\\text{in}}` refers to the fan-in of the layer. This type of\n        initialization corresponds to a variance of\n        :math:`\\text{Var}(v) = \\frac{1}{3 f_{\\text{in}}}`.\n\n        Warning:\n            Note, in order to compute the fan-in of layers with bias vectors, we\n            need access to the corresponding weight tensor in the same layer.\n            Since there is no clean way of matching a bias shape to its\n            corresponging weight tensor shape we use the following heuristic,\n            which should be correct for most main networks. We assume that the\n            shape directly preceding a bias shape in the constructor argument\n            ``target_shapes`` is the corresponding weight tensor.\n\n        Note:\n            Constructor argument ``noise_dim`` is automatically considered by\n            this method.\n\n        Note:\n            We hypernet inputs should be zero mean.\n\n        Warning:\n            This method considers all 1D target weight tensors as bias vectors.\n\n        Note:\n            To avoid that the variances with which chunks are initialized\n            have to be clipped (because they are too small or even negative),\n            the variance of the remaining hypernet inputs should be properly\n            scaled. In general, one should adhere the following rule\n\n            .. math::\n\n                \\text{Var}(e) < \\frac{n_e+n_c}{n_e} \\text{Var}(v)\n\n            This method will calculate and print the maximum value that should\n            be chosen for :math:`\\text{Var}(e)` and will print warnings if\n            variances have to be clipped.\n\n        Args:\n            method (str): The type of initialization that should be applied.\n                Possible options are:\n\n                - ``in``: Use `Chunked Hyperfan-in`, i.e., rather the output\n                  variances of the hypernetwork should correspond to fan-in\n                  variances.\n                - ``out``: Use `Chunked Hyperfan-out`, i.e., rather the output\n                  variances of the hypernetwork should correspond to fan-out\n                  variances.\n                - ``harmonic``: Use the harmonic mean of the fan-in and fan-out\n                  variance as target variance of the hypernetwork output.\n            use_xavier (bool): Whether Kaiming (``False``) or Xavier (``True``)\n                init should be used.\n            temb_var (float): The initial variance of the task embeddings.\n\n                .. note::\n                    If ``temb_std`` was set in the constructor, then this method\n                    will automatically correct the provided ``temb_var`` as \n                    follows: :code:`temb_var += temb_std**2`.\n            ext_inp_var (float): The initial variance of the external input.\n                Only needs to be specified if external inputs are provided.\n                \n                .. note::\n                    Not supported yet by this hypernetwork type, but should soon\n                    be included as a feature.\n            eps (float): The minimum variance with which a chunk embedding is\n                initialized.\n            cemb_normal_init (bool): Use normal init for chunk embeedings\n                rather than uniform init.\n        \"\"\"", "\n", "if", "method", "not", "in", "[", "'in'", ",", "'out'", ",", "'harmonic'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid value for argument \"method\".'", ")", "\n", "", "if", "not", "self", ".", "has_theta", ":", "\n", "            ", "raise", "ValueError", "(", "'Hypernet without internal weights can\\'t be '", "+", "\n", "'initialized.'", ")", "\n", "\n", "### Compute input variance ###", "\n", "# The input variance does not include the variance of chunk embeddings!", "\n", "# Instead, it is the varaince of the inputs that are shared across all", "\n", "# chunks.", "\n", "", "if", "self", ".", "_temb_std", "!=", "-", "1", ":", "\n", "# Sum of uncorrelated variables.", "\n", "            ", "temb_var", "+=", "self", ".", "_temb_std", "**", "2", "\n", "\n", "", "assert", "self", ".", "_noise_dim", "==", "-", "1", "or", "self", ".", "_noise_dim", ">", "0", "\n", "\n", "# TODO external inputs are not yet considered.", "\n", "inp_dim", "=", "self", ".", "_te_dim", "+", "(", "self", ".", "_noise_dim", "if", "self", ".", "_noise_dim", "!=", "-", "1", "else", "0", ")", "\n", "#(self._size_ext_input if self._size_ext_input is not None else 0) \\", "\n", "\n", "inp_var", "=", "(", "self", ".", "_te_dim", "/", "inp_dim", ")", "*", "temb_var", "\n", "#if self._size_ext_input is not None:", "\n", "#    inp_var += (self._size_ext_input  / inp_dim) * ext_inp_var", "\n", "if", "self", ".", "_noise_dim", "!=", "-", "1", ":", "\n", "            ", "inp_var", "+=", "(", "self", ".", "_noise_dim", "/", "inp_dim", ")", "*", "1.", "\n", "\n", "", "c_dim", "=", "self", ".", "_ce_dim", "\n", "\n", "### Compute target variance of each output tensor ###", "\n", "target_vars", "=", "[", "]", "\n", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "self", ".", "target_shapes", ")", ":", "\n", "# FIXME 1D shape is not necessarily bias vector.", "\n", "            ", "if", "len", "(", "s", ")", "==", "1", ":", "# Assume it's a bias vector", "\n", "# Assume that last shape has been the corresponding weight", "\n", "# tensor.", "\n", "                ", "if", "i", ">", "0", "and", "len", "(", "self", ".", "target_shapes", "[", "i", "-", "1", "]", ")", ">", "1", ":", "\n", "                    ", "fan_in", ",", "_", "=", "iutils", ".", "calc_fan_in_and_out", "(", "self", ".", "target_shapes", "[", "i", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "# FIXME Quick-fix, use fan-out instead.", "\n", "                    ", "fan_in", "=", "s", "[", "0", "]", "\n", "\n", "", "target_vars", ".", "append", "(", "1.", "/", "(", "3.", "*", "fan_in", ")", ")", "\n", "\n", "", "else", ":", "\n", "                ", "fan_in", ",", "fan_out", "=", "iutils", ".", "calc_fan_in_and_out", "(", "s", ")", "\n", "\n", "c_relu", "=", "1", "if", "use_xavier", "else", "2", "\n", "\n", "var_in", "=", "c_relu", "/", "fan_in", "\n", "var_out", "=", "c_relu", "/", "fan_out", "\n", "\n", "if", "method", "==", "'in'", ":", "\n", "                    ", "var", "=", "var_in", "\n", "", "elif", "method", "==", "'out'", ":", "\n", "                    ", "var", "=", "var_out", "\n", "", "else", ":", "\n", "                    ", "var", "=", "2", "*", "(", "1.", "/", "var_in", "+", "1.", "/", "var_out", ")", "\n", "\n", "", "target_vars", ".", "append", "(", "var", ")", "\n", "\n", "### Target variance per chunk ###", "\n", "", "", "chunk_vars", "=", "[", "]", "\n", "i", "=", "0", "\n", "n", "=", "np", ".", "prod", "(", "self", ".", "target_shapes", "[", "i", "]", ")", "\n", "\n", "for", "j", "in", "range", "(", "self", ".", "_num_chunks", ")", ":", "\n", "            ", "m", "=", "self", ".", "_chunk_dim", "\n", "var", "=", "0", "\n", "\n", "while", "m", ">", "0", ":", "\n", "# Special treatment to fill up last chunk.", "\n", "                ", "if", "j", "==", "self", ".", "_num_chunks", "-", "1", "and", "i", "==", "len", "(", "target_vars", ")", "-", "1", ":", "\n", "                    ", "assert", "n", "<=", "m", "\n", "o", "=", "m", "\n", "", "else", ":", "\n", "                    ", "o", "=", "min", "(", "m", ",", "n", ")", "\n", "\n", "", "var", "+=", "o", "/", "self", ".", "_chunk_dim", "*", "target_vars", "[", "i", "]", "\n", "m", "-=", "o", "\n", "n", "-=", "o", "\n", "\n", "if", "n", "==", "0", ":", "\n", "                    ", "i", "+=", "1", "\n", "if", "i", "<", "len", "(", "target_vars", ")", ":", "\n", "                        ", "n", "=", "np", ".", "prod", "(", "self", ".", "target_shapes", "[", "i", "]", ")", "\n", "\n", "", "", "", "chunk_vars", ".", "append", "(", "var", ")", "\n", "\n", "", "max_inp_var", "=", "(", "inp_dim", "+", "c_dim", ")", "/", "inp_dim", "*", "min", "(", "chunk_vars", ")", "\n", "max_inp_std", "=", "math", ".", "sqrt", "(", "max_inp_var", ")", "\n", "print", "(", "'Initializing hypernet with Chunked Hyperfan Init ...'", ")", "\n", "if", "inp_var", ">=", "max_inp_var", ":", "\n", "            ", "warn", "(", "'Note, hypernetwork inputs should have an initial total '", "+", "\n", "'variance (std) smaller than %f (%f) in order for this '", "%", "(", "max_inp_var", ",", "max_inp_std", ")", "+", "'method to work properly.'", ")", "\n", "\n", "### Compute variances of chunk embeddings ###", "\n", "# We could have done that in the previous loop. But I think the code is", "\n", "# more readible this way.", "\n", "", "c_vars", "=", "[", "]", "\n", "n_clipped", "=", "0", "\n", "for", "i", ",", "var", "in", "enumerate", "(", "chunk_vars", ")", ":", "\n", "            ", "c_var", "=", "1.", "/", "c_dim", "*", "(", "(", "inp_dim", "+", "c_dim", ")", "*", "var", "-", "inp_dim", "*", "inp_var", ")", "\n", "if", "c_var", "<", "eps", ":", "\n", "                ", "n_clipped", "+=", "1", "\n", "#warn('Initial variance of chunk embedding %d has to ' % i + \\", "\n", "#     'be clipped.')", "\n", "\n", "", "c_vars", ".", "append", "(", "max", "(", "eps", ",", "c_var", ")", ")", "\n", "\n", "", "if", "n_clipped", ">", "0", ":", "\n", "            ", "warn", "(", "'Initial variance of %d/%d '", "%", "(", "n_clipped", ",", "len", "(", "chunk_vars", ")", ")", "+", "'chunk embeddings had to be clipped.'", ")", "\n", "\n", "### Initialize chunk embeddings ###", "\n", "", "for", "i", ",", "c_emb", "in", "enumerate", "(", "self", ".", "chunk_embeddings", ")", ":", "\n", "            ", "c_std", "=", "math", ".", "sqrt", "(", "c_vars", "[", "i", "]", ")", "\n", "if", "cemb_normal_init", ":", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "c_emb", ",", "mean", "=", "0", ",", "std", "=", "c_std", ")", "\n", "", "else", ":", "\n", "                ", "a", "=", "math", ".", "sqrt", "(", "3.0", ")", "*", "c_std", "\n", "torch", ".", "nn", ".", "init", ".", "_no_grad_uniform_", "(", "c_emb", ",", "-", "a", ",", "a", ")", "\n", "\n", "### Initialize hypernet with fan-in init ###", "\n", "", "", "for", "i", ",", "w", "in", "enumerate", "(", "self", ".", "_hypernet", ".", "theta", ")", ":", "\n", "            ", "if", "w", ".", "ndim", "==", "1", ":", "# bias", "\n", "                ", "assert", "i", "%", "2", "==", "1", "\n", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "w", ",", "0", ")", "\n", "\n", "", "else", ":", "\n", "                ", "if", "use_xavier", ":", "\n", "                    ", "iutils", ".", "xavier_fan_in_", "(", "w", ")", "\n", "", "else", ":", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "kaiming_uniform_", "(", "w", ",", "mode", "=", "'fan_in'", ",", "\n", "nonlinearity", "=", "'relu'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hp_search_splitMNIST._get_performance_summary": [[144, 155], ["None"], "function", ["None"], ["def", "_get_performance_summary", "(", "out_dir", ",", "cmd_ident", ")", ":", "\n", "    ", "\"\"\"See docstring of method\n    :func:`hpsearch.hpsearch._get_performance_summary`.\n\n    You only need to implement this function, if the default parser in module\n    :func:`hpsearch.hpsearch` is not sufficient for your purposes.\n\n    In case you would like to use a custom parser, you have to set the\n    attribute :attr:`_SUMMARY_PARSER_HANDLER` correctly.\n    \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hp_search_splitMNIST._performance_criteria": [[163, 184], ["NotImplementedError"], "function", ["None"], ["def", "_performance_criteria", "(", "summary_dict", ",", "performance_criteria", ")", ":", "\n", "    ", "\"\"\"Evaluate whether a run meets a given performance criteria.\n\n    This function is needed to decide whether the output directory of a run is\n    deleted or kept.\n\n    Args:\n        summary_dict: The performance summary dictionary as returned by\n            :attr:`_SUMMARY_PARSER_HANDLE`.\n        performance_criteria (float): The performance criteria. E.g., see\n            command-line option `performance_criteria` of script\n            :mod:`hpsearch.hpsearch_postprocessing`.\n\n    Returns:\n        bool: If :code:`True`, the result folder will be kept as the performance\n        criteria is assumed to be met.\n    \"\"\"", "\n", "### Example:", "\n", "# return summary_dict['performance_measure1'] > performance_criteria", "\n", "\n", "raise", "NotImplementedError", "(", "'TODO implement'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args.parse_cmd_arguments": [[39, 103], ["mnist.replay.train_args_replay.collect_rp_cmd_arguments", "train_args.cl_arguments_general", "train_args.cl_arguments_classificiation", "mnist.replay.train_args_replay.collect_rp_cmd_arguments.parse_args", "utils.check_invalid_argument_usage", "utils.main_net_args", "utils.hypernet_args", "mnist.replay.train_args_replay.train_args_replay", "Exception", "utils.main_net_args", "utils.hypernet_args", "mnist.replay.train_args_replay.train_args_replay", "warnings.warn", "ValueError"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.collect_rp_cmd_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.cl_arguments_general", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args.cl_arguments_classificiation", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.check_invalid_argument_usage", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.main_net_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.hypernet_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.train_args_replay", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.main_net_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.hypernet_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.train_args_replay"], ["\n", "if", "mode", "==", "'resnet_cifar'", ":", "\n", "        ", "description", "=", "'CIFAR-10/100 CL experiment using a Resnet-32'", "\n", "", "elif", "mode", "==", "'zenke_cifar'", ":", "\n", "        ", "description", "=", "'CIFAR-10/100 CL experiment using the Zenkenet'", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Mode \"%s\" unknown.'", "%", "(", "mode", ")", ")", "\n", "\n", "", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "description", ")", "\n", "\n", "general_options", "(", "parser", ")", "\n", "\n", "if", "mode", "==", "'resnet_cifar'", ":", "\n", "        ", "dout_dir", "=", "'./out_resnet/run_'", "+", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S'", ")", "\n", "\n", "cl_group", "=", "cli", ".", "cl_args", "(", "parser", ",", "show_beta", "=", "True", ",", "dbeta", "=", "0.05", ",", "\n", "show_from_scratch", "=", "True", ",", "show_multi_head", "=", "False", ",", "\n", "show_cl_scenario", "=", "True", ",", "show_split_head_cl3", "=", "False", ",", "\n", "show_num_tasks", "=", "True", ",", "dnum_tasks", "=", "6", ")", "\n", "cli", ".", "main_net_args", "(", "parser", ",", "allowed_nets", "=", "[", "'resnet'", "]", ",", "show_batchnorm", "=", "False", ",", "\n", "show_no_batchnorm", "=", "True", ",", "show_bn_no_running_stats", "=", "True", ",", "\n", "show_bn_distill_stats", "=", "True", ",", "show_bn_no_stats_checkpointing", "=", "True", ",", "\n", "show_specnorm", "=", "False", ",", "show_dropout_rate", "=", "False", ",", "show_net_act", "=", "False", ")", "\n", "cli", ".", "hypernet_args", "(", "parser", ",", "dhyper_chunks", "=", "7000", ",", "dhnet_arch", "=", "''", ",", "\n", "dtemb_size", "=", "32", ",", "demb_size", "=", "32", ")", "\n", "cli", ".", "data_args", "(", "parser", ",", "show_disable_data_augmentation", "=", "True", ")", "\n", "train_agroup", "=", "cli", ".", "train_args", "(", "parser", ",", "show_lr", "=", "True", ",", "dlr", "=", "0.001", ",", "\n", "show_epochs", "=", "True", ",", "depochs", "=", "200", ",", "dbatch_size", "=", "32", ",", "\n", "dn_iter", "=", "2000", ",", "show_use_adam", "=", "True", ",", "show_use_rmsprop", "=", "True", ",", "\n", "show_use_adadelta", "=", "False", ",", "show_use_adagrad", "=", "False", ",", "\n", "show_clip_grad_value", "=", "False", ",", "show_clip_grad_norm", "=", "False", ")", "\n", "\n", "", "elif", "mode", "==", "'zenke_cifar'", ":", "\n", "        ", "dout_dir", "=", "'./out_zenke/run_'", "+", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S'", ")", "\n", "\n", "cl_group", "=", "cli", ".", "cl_args", "(", "parser", ",", "show_beta", "=", "True", ",", "dbeta", "=", "0.01", ",", "\n", "show_from_scratch", "=", "True", ",", "show_multi_head", "=", "False", ",", "\n", "show_cl_scenario", "=", "True", ",", "show_split_head_cl3", "=", "False", ",", "\n", "show_num_tasks", "=", "True", ",", "dnum_tasks", "=", "6", ")", "\n", "cli", ".", "main_net_args", "(", "parser", ",", "allowed_nets", "=", "[", "'zenke'", "]", ",", "show_batchnorm", "=", "False", ",", "\n", "show_no_batchnorm", "=", "False", ",", "show_dropout_rate", "=", "True", ",", "ddropout_rate", "=", "0.25", ",", "\n", "show_specnorm", "=", "False", ",", "show_net_act", "=", "False", ")", "\n", "cli", ".", "hypernet_args", "(", "parser", ",", "dhyper_chunks", "=", "5500", ",", "dhnet_arch", "=", "'100,150,200'", ",", "\n", "dtemb_size", "=", "48", ",", "demb_size", "=", "80", ")", "\n", "cli", ".", "data_args", "(", "parser", ",", "show_disable_data_augmentation", "=", "True", ")", "\n", "train_agroup", "=", "cli", ".", "train_args", "(", "parser", ",", "show_lr", "=", "True", ",", "dlr", "=", "0.0001", ",", "\n", "show_epochs", "=", "True", ",", "depochs", "=", "80", ",", "dbatch_size", "=", "256", ",", "\n", "dn_iter", "=", "2000", ",", "show_use_adam", "=", "True", ",", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args.cl_arguments_general": [[104, 121], ["parser.add_argument", "parser.add_argument"], "function", ["None"], ["dadam_beta1", "=", "0.5", ",", "show_use_rmsprop", "=", "True", ",", "\n", "show_use_adadelta", "=", "False", ",", "show_use_adagrad", "=", "False", ",", "\n", "show_clip_grad_value", "=", "False", ",", "show_clip_grad_norm", "=", "False", ")", "\n", "\n", "", "special_cl_options", "(", "cl_group", ")", "\n", "special_train_options", "(", "train_agroup", ")", "\n", "init_group", "=", "cli", ".", "init_args", "(", "parser", ",", "custom_option", "=", "True", ")", "\n", "special_init_options", "(", "init_group", ")", "\n", "cli", ".", "eval_args", "(", "parser", ",", "show_val_batch_size", "=", "True", ",", "dval_batch_size", "=", "1000", ")", "\n", "cli", ".", "miscellaneous_args", "(", "parser", ",", "big_data", "=", "False", ",", "synthetic_data", "=", "False", ",", "\n", "show_plots", "=", "False", ",", "no_cuda", "=", "False", ",", "dout_dir", "=", "dout_dir", ")", "\n", "\n", "args", "=", "None", "\n", "if", "argv", "is", "not", "None", ":", "\n", "        ", "if", "default", ":", "\n", "            ", "warnings", ".", "warn", "(", "'Provided \"argv\" will be ignored since \"default\" '", "+", "\n", "'option was turned on.'", ")", "\n", "", "args", "=", "argv", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args.cl_arguments_classificiation": [[122, 190], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "if", "default", ":", "\n", "        ", "args", "=", "[", "]", "\n", "", "config", "=", "parser", ".", "parse_args", "(", "args", "=", "args", ")", "\n", "\n", "### Check argument values!", "\n", "cli", ".", "check_invalid_argument_usage", "(", "config", ")", "\n", "\n", "### ... insert additional checks if necessary", "\n", "if", "config", ".", "num_tasks", "<", "1", "or", "config", ".", "num_tasks", ">", "11", ":", "\n", "        ", "raise", "ValueError", "(", "'Argument \"num_tasks\" must be between 1 and 11!'", ")", "\n", "\n", "", "if", "config", ".", "cl_scenario", "!=", "1", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'CIFAR experiments are currently only '", "+", "\n", "'implemented for CL1.'", ")", "\n", "\n", "", "if", "config", ".", "plateau_lr_scheduler", "and", "config", ".", "epochs", "==", "-", "1", ":", "\n", "        ", "raise", "ValueError", "(", "'Flag \"plateau_lr_scheduler\" can only be used if '", "+", "\n", "'\"epochs\" was set.'", ")", "\n", "\n", "", "if", "config", ".", "lambda_lr_scheduler", "and", "config", ".", "epochs", "==", "-", "1", ":", "\n", "        ", "raise", "ValueError", "(", "'Flag \"lambda_lr_scheduler\" can only be used if '", "+", "\n", "'\"epochs\" was set.'", ")", "\n", "\n", "", "if", "config", ".", "no_lookahead", "and", "config", ".", "backprop_dt", ":", "\n", "        ", "raise", "ValueError", "(", "'Can\\'t activate \"no_lookahead\" and \"backprop_dt\" '", "+", "\n", "'simultaneously.'", ")", "\n", "\n", "", "return", "config", "\n", "\n", "", "def", "general_options", "(", "parser", ")", ":", "\n", "    ", "\"\"\"This is a helper function of the function `parse_cmd_arguments` to create\n    an argument group for general stuff important for the types of experiments\n    conducted here.\n\n    Args:\n        parser (:class:`argparse.ArgumentParser`): The argument parser to which\n            the argument group should be added.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'General options'", ")", "\n", "\n", "agroup", ".", "add_argument", "(", "'--mnet_only'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Train the main network without a hypernetwork. '", "+", "\n", "'No continual learning support!'", ")", "\n", "\n", "", "def", "special_init_options", "(", "agroup", ")", ":", "\n", "    ", "\"\"\"This is a helper function of the function `parse_cmd_arguments` to add\n    arguments to the `initialization` argument group.\n\n    Args:\n        agroup: The argument group returned by method\n            :func:`utils.cli_args.init_args`.\n    \"\"\"", "\n", "agroup", ".", "add_argument", "(", "'--hnet_init_shift'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Shift the initial hnet output such that it '", "+", "\n", "'resembles a xavier or normal init for the '", "+", "\n", "'target network.'", ")", "\n", "\n", "", "def", "special_cl_options", "(", "agroup", ")", ":", "\n", "    ", "\"\"\"This is a helper function of the function `parse_cmd_arguments` to add\n    arguments to the `continual learning` argument group.\n\n    Args:\n        agroup: The argument group returned by method\n            :func:`utils.cli_args.cl_args`.\n    \"\"\"", "\n", "agroup", ".", "add_argument", "(", "'--init_with_prev_emb'", ",", "action", "=", "'store_true'", ",", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_utils._setup_environment": [[55, 116], ["torch.manual_seed", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed", "os.path.exists", "torch.device", "print", "input", "shutil.rmtree", "os.makedirs", "print", "os.makedirs", "print", "open", "pickle.dump", "torch.cuda.is_available", "hasattr", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "Exception", "os.path.join", "str", "os.path.join", "os.path.join"], "function", ["None"], ["\n", "augment_data", "=", "not", "config", ".", "disable_data_augmentation", "\n", "#if shared.experiment == 'zenke':", "\n", "#    augment_data = False", "\n", "#    # To be comparable to previous results. Note, Zenke et al. didn't", "\n", "#    # utilize any data augmentation as far as I know.", "\n", "#    logger.warning('Data augmentation disabled for Zenkenet.')", "\n", "if", "augment_data", ":", "\n", "        ", "logger", ".", "info", "(", "'Data augmentation will be used.'", ")", "\n", "\n", "", "assert", "(", "config", ".", "num_tasks", "<=", "11", ")", "\n", "logger", ".", "info", "(", "'Loading CIFAR datasets ...'", ")", "\n", "dhandlers", "=", "get_split_cifar_handlers", "(", "data_dir", ",", "use_one_hot", "=", "True", ",", "\n", "use_data_augmentation", "=", "augment_data", ",", "num_tasks", "=", "config", ".", "num_tasks", ")", "\n", "assert", "(", "len", "(", "dhandlers", ")", "==", "config", ".", "num_tasks", ")", "\n", "\n", "logger", ".", "info", "(", "'Loaded %d CIFAR task(s) into memory.'", "%", "config", ".", "num_tasks", ")", "\n", "\n", "return", "dhandlers", "\n", "\n", "", "def", "get_main_model", "(", "config", ",", "shared", ",", "logger", ",", "device", ",", "no_weights", "=", "False", ")", ":", "\n", "    ", "\"\"\"Helper function to generate the main network.\n\n    This function uses :func:`utils.sim_utils.get_mnet_model` to generate the\n    main network.\n\n    The function also takes care of weight initialization, if configured.\n\n    Args:\n        (....): See docstring of function :func:`load_datasets`.\n        device: The PyTorch device.\n        no_weights (bool): If ``True``, the main network is generated without\n            internal weights.\n\n    Returns:\n        The main network.\n    \"\"\"", "\n", "if", "shared", ".", "experiment", "==", "'zenke'", ":", "\n", "        ", "net_type", "=", "'zenke'", "\n", "logger", ".", "info", "(", "'Building a ZenkeNet ...'", ")", "\n", "\n", "", "else", ":", "\n", "        ", "net_type", "=", "'resnet'", "\n", "logger", ".", "info", "(", "'Building a ResNet ...'", ")", "\n", "\n", "", "num_outputs", "=", "10", "\n", "\n", "if", "config", ".", "cl_scenario", "==", "1", "or", "config", ".", "cl_scenario", "==", "3", ":", "\n", "        ", "num_outputs", "*=", "config", ".", "num_tasks", "\n", "\n", "", "logger", ".", "info", "(", "'The network will have %d output neurons.'", "%", "num_outputs", ")", "\n", "\n", "in_shape", "=", "[", "32", ",", "32", ",", "3", "]", "\n", "out_shape", "=", "[", "num_outputs", "]", "\n", "\n", "# TODO Allow main net only training.", "\n", "mnet", "=", "sutils", ".", "get_mnet_model", "(", "config", ",", "net_type", ",", "in_shape", ",", "out_shape", ",", "device", ",", "\n", "no_weights", "=", "no_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_utils._generate_tasks": [[117, 150], ["data.special.split_mnist.get_split_MNIST_handlers", "numpy.random.RandomState", "ValueError", "data.special.permuted_mnist.PermutedMNISTList", "np.random.RandomState.permutation", "PermutedMNIST", "range"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_mnist.get_split_MNIST_handlers", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.permutation"], ["init_network_weights", "(", "mnet", ".", "weights", ",", "config", ",", "logger", ",", "net", "=", "mnet", ")", "\n", "\n", "return", "mnet", "\n", "\n", "", "def", "get_hnet_model", "(", "config", ",", "mnet", ",", "logger", ",", "device", ")", ":", "\n", "    ", "\"\"\"Generate the hypernetwork.\n\n    This function uses :func:`utils.sim_utils.get_hnet_model` to generate the\n    hypernetwork.\n\n    The function also takes care of weight initialization, if configured.\n\n    Args:\n        (....): See docstring of function :func:`get_main_model`.\n        mnet: The main network.\n\n    Returns:\n        The hypernetwork or ``None`` if no hypernet is needed.\n    \n    \"\"\"", "\n", "logger", ".", "info", "(", "'Creating hypernetwork ...'", ")", "\n", "hnet", "=", "sutils", ".", "get_hnet_model", "(", "config", ",", "config", ".", "num_tasks", ",", "device", ",", "\n", "mnet", ".", "param_shapes", ")", "\n", "# FIXME There should be a nicer way of initializing hypernets in the", "\n", "# future.", "\n", "chunk_embs", "=", "None", "\n", "if", "hasattr", "(", "hnet", ",", "'chunk_embeddings'", ")", ":", "\n", "        ", "chunk_embs", "=", "hnet", ".", "chunk_embeddings", "\n", "", "init_network_weights", "(", "hnet", ".", "parameters", "(", ")", ",", "config", ",", "logger", ",", "\n", "chunk_embs", "=", "chunk_embs", ",", "task_embs", "=", "hnet", ".", "get_task_embs", "(", ")", ",", "net", "=", "hnet", ")", "\n", "if", "config", ".", "hnet_init_shift", ":", "\n", "        ", "hnet_init_shift", "(", "hnet", ",", "mnet", ",", "config", ",", "logger", ",", "device", ")", "\n", "\n", "# TODO Incorporate hyperchunk init.", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_utils.generate_classifier": [[151, 275], ["print", "utils.str_to_ints", "mnets.mlp.MLP().to", "print", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "hasattr", "utils.sim_utils.get_hnet_model", "list", "sum", "print", "list", "sim_utils.get_hnet_model.get_task_embs", "mnets.mlp.MLP", "sim_utils.get_hnet_model.parameters", "print", "MLP().to.parameters", "W.ndimension", "torch.nn.init.constant_", "torch.nn.init.xavier_uniform_", "torch.nn.init.normal_", "torch.nn.init.normal_", "p.numel", "utils.str_to_act", "sim_utils.get_hnet_model.parameters"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.get_hnet_model", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_act"], ["#if isinstance(hnet, ChunkedHyperNetworkHandler):", "\n", "#    hnet.apply_chunked_hyperfan_init(temb_var=config.std_normal_temb**2)", "\n", "\n", "", "return", "hnet", "\n", "\n", "", "def", "init_network_weights", "(", "all_params", ",", "config", ",", "logger", ",", "chunk_embs", "=", "None", ",", "\n", "task_embs", "=", "None", ",", "net", "=", "None", ")", ":", "\n", "    ", "\"\"\"Initialize a given set of weight tensors according to the user\n    configuration.\n\n    Warning:\n        This method is agnostic to where the weights stem from and is\n        therefore slightly dangerous. Use with care.\n\n    Note:\n        The method only exists as at the time of implementation the package\n        :mod:`hnets` wasn't available yet. In the future, initialization should\n        be part of the network implementation (e.g., via method\n        :meth:`mnets.mnet_interface.MainNetInterface.custom_init`).\n\n    Note:\n        If the given network implements interface\n        :class:`mnets.mnet_interface.MainNetInterface`, then the corresponding\n        method :meth:`mnets.mnet_interface.MainNetInterface.custom_init` is\n        used.\n\n    Note:\n        Papers like the following show that hypernets should get a special\n        init. This function does not take this into consideration.\n\n            https://openreview.net/forum?id=H1lma24tPB\n\n    Args:\n        all_params: A list of weight tensors to be initialized.\n        config: Command-line arguments.\n        logger: Logger.\n        chunk_embs (optional): A list of chunk embeddings.\n        task_embs (optional): A list of task embeddings.\n        net (optional): The network from which the parameters stem come from.\n            Can be used to implement network specific initializations (e.g.,\n            batch-norm weights).\n    \"\"\"", "\n", "if", "config", ".", "custom_network_init", ":", "\n", "        ", "if", "net", "is", "not", "None", "and", "isinstance", "(", "net", ",", "MainNetInterface", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'Applying custom initialization to network ...'", ")", "\n", "net", ".", "custom_init", "(", "normal_init", "=", "config", ".", "normal_init", ",", "\n", "normal_std", "=", "config", ".", "std_normal_init", ",", "zero_bias", "=", "True", ")", "\n", "\n", "", "else", ":", "\n", "            ", "logger", ".", "warning", "(", "'Custom weight initialization is applied to all '", "+", "\n", "'network parameters. Note, the current '", "+", "\n", "'implementation might be agnostic to special '", "+", "\n", "'network parameters.'", ")", "\n", "for", "W", "in", "all_params", ":", "\n", "# FIXME not all 1D vectors are bias vectors.", "\n", "# Examples of parameters that are 1D and not bias vectors:", "\n", "# * batchnorm weights", "\n", "# * embedding vectors", "\n", "                ", "if", "W", ".", "ndimension", "(", ")", "==", "1", ":", "# Bias vector.", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "W", ",", "0", ")", "\n", "", "elif", "config", ".", "normal_init", ":", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "W", ",", "mean", "=", "0", ",", "std", "=", "config", ".", "std_normal_init", ")", "\n", "", "else", ":", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "W", ")", "\n", "\n", "\n", "# Note, the embedding vectors inside \"all_params\" have been considered", "\n", "# as bias vectors and thus initialized to zero.", "\n", "", "", "", "", "if", "chunk_embs", "is", "not", "None", ":", "\n", "        ", "for", "emb", "in", "chunk_embs", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "emb", ",", "mean", "=", "0", ",", "std", "=", "config", ".", "std_normal_emb", ")", "\n", "\n", "", "", "if", "task_embs", "is", "not", "None", ":", "\n", "        ", "for", "temb", "in", "task_embs", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "temb", ",", "mean", "=", "0.", ",", "std", "=", "config", ".", "std_normal_temb", ")", "\n", "\n", "", "", "", "def", "hnet_init_shift", "(", "hnet", ",", "mnet", ",", "config", ",", "logger", ",", "device", ")", ":", "\n", "    ", "\"\"\"Init the hypernet ``hnet`` such that the weights of the main network \n    ``mnet`` are initialised as if there would be no hypernetwork i.e. the first\n    hypernetwork output is a standard init (for now normal or Xavier\n    are implemented).\n\n    Note:\n        This function is only meant for exploratory purposes. It does not\n        provide a proper weight initialization as for instance\n\n            https://openreview.net/forum?id=H1lma24tPB\n\n        Though, it is independent of the hypernet type/architecture.\n\n    Warning:\n        Not all hypernets support this quick-fix.\n\n    Args:\n        hnet: The model of the hyper network.\n        mnet: The main model.\n        config: The command line arguments.\n        device: Torch device (cpu or gpu).\n    \"\"\"", "\n", "logger", ".", "warning", "(", "'Config \"hnet_init_shift\" is just a temporary test and '", "+", "\n", "'should be used with care.'", ")", "\n", "\n", "# Get the current output, this should be normal or xavier or ...", "\n", "hnet_outputs", "=", "hnet", ".", "forward", "(", "0", ")", "\n", "orig_output", "=", "[", "o", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "o", "in", "hnet_outputs", "]", "\n", "mnet_init", "=", "[", "torch", ".", "zeros_like", "(", "o", ")", "for", "o", "in", "hnet_outputs", "]", "\n", "\n", "tmp", "=", "config", ".", "custom_network_init", "\n", "config", ".", "custom_network_init", "=", "True", "\n", "init_network_weights", "(", "mnet_init", ",", "config", ",", "logger", ",", "net", "=", "mnet", ")", "\n", "config", ".", "custom_network_init", "=", "tmp", "\n", "\n", "# The shift of the hypernetwork outputs will be computed by subtracting the", "\n", "# current output and adding the new desired output.", "\n", "o_shift", "=", "[", "]", "\n", "for", "i", ",", "o", "in", "enumerate", "(", "orig_output", ")", ":", "\n", "        ", "o_shift", ".", "append", "(", "-", "o", "+", "mnet_init", "[", "i", "]", ")", "\n", "\n", "# set the shifts", "\n", "", "assert", "(", "hasattr", "(", "hnet", ",", "'_shifts'", ")", ")", "# Only temporarily added to some hnets.", "\n", "hnet", ".", "_shifts", "=", "o_shift", "\n", "\n", "", "def", "setup_summary_dict", "(", "config", ",", "shared", ",", "mnet", ",", "hnet", "=", "None", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args_default._set_default": [[32, 59], ["train_args_default._set_default_split", "train_args_default._set_default_permuted", "train_args_default._set_default_gan"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args_default._set_default_split", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args_default._set_default_permuted", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args_default._set_default_gan"], ["def", "_set_default", "(", "config", ")", ":", "\n", "    ", "\"\"\"Overwrite default configs.\n\n    Args:\n        config: The command line arguments.\n    Returns:\n        Altered configs to reproduce results reported in the paper.\n    \"\"\"", "\n", "config", ".", "latent_dim", "=", "100", "\n", "config", ".", "conditional_replay", "=", "True", "\n", "config", ".", "fake_data_full_range", "=", "True", "\n", "config", ".", "show_plots", "=", "True", "\n", "config", ".", "train_class_embeddings", "=", "True", "\n", "\n", "if", "config", ".", "experiment", "==", "\"splitMNIST\"", ":", "\n", "        ", "config", "=", "_set_default_split", "(", "config", ")", "\n", "", "else", ":", "\n", "        ", "config", "=", "_set_default_permuted", "(", "config", ")", "\n", "\n", "", "config", ".", "infer_output_head", "=", "False", "\n", "if", "config", ".", "cl_scenario", "==", "3", "and", "config", ".", "infer_task_id", ":", "\n", "        ", "config", ".", "infer_output_head", "=", "True", "\n", "\n", "", "if", "config", ".", "replay_method", "==", "\"gan\"", ":", "\n", "        ", "config", "=", "_set_default_gan", "(", "config", ")", "\n", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args_default._set_default_split": [[60, 109], ["None"], "function", ["None"], ["", "def", "_set_default_split", "(", "config", ")", ":", "\n", "    ", "\"\"\"Overwrite default configs for splitMNIST.\n\n    Args:\n        config: The command line arguments.\n    Returns:\n        Altered configs to reproduce results reported in the paper.\n    \"\"\"", "\n", "\n", "# General setup", "\n", "config", ".", "enc_fc_arch", "=", "'400,400'", "\n", "config", ".", "dec_fc_arch", "=", "'400,400'", "\n", "config", ".", "class_fc_arch", "=", "'400,400'", "\n", "config", ".", "enc_lr", ",", "config", ".", "dec_lr", ",", "config", ".", "dec_lr_emb", "=", "0.001", ",", "0.001", ",", "0.001", "\n", "config", ".", "class_lr", ",", "config", ".", "class_lr_emb", "=", "0.001", ",", "0.001", "\n", "config", ".", "n_iter", "=", "2000", "\n", "config", ".", "batch_size", "=", "128", "\n", "config", ".", "data_dir", "=", "'../datasets'", "\n", "config", ".", "num_tasks", "=", "5", "\n", "config", ".", "padding", "=", "0", "\n", "config", ".", "no_lookahead", "=", "False", "\n", "\n", "# VAE hnet", "\n", "config", ".", "rp_temb_size", "=", "96", "\n", "config", ".", "rp_emb_size", "=", "96", "\n", "config", ".", "rp_hnet_act", "=", "\"elu\"", "\n", "config", ".", "rp_hyper_chunks", "=", "50000", "\n", "config", ".", "rp_hnet_arch", "=", "'10,10'", "\n", "config", ".", "rp_beta", "=", "0.01", "\n", "\n", "# Classifier hnet", "\n", "config", ".", "class_temb_size", "=", "96", "\n", "config", ".", "class_emb_size", "=", "96", "\n", "config", ".", "class_hnet_act", "=", "\"relu\"", "\n", "config", ".", "class_hyper_chunks", "=", "42000", "\n", "config", ".", "class_hnet_arch", "=", "'10,10'", "\n", "config", ".", "class_beta", "=", "0.01", "\n", "\n", "#HNET+TIR", "\n", "if", "config", ".", "infer_task_id", ":", "\n", "        ", "config", ".", "hard_targets", "=", "True", "\n", "config", ".", "dec_fc_arch", "=", "'50,150'", "\n", "\n", "#HNET+R", "\n", "", "else", ":", "\n", "        ", "config", ".", "hard_targets", "=", "False", "\n", "config", ".", "dec_fc_arch", "=", "'250,350'", "\n", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args_default._set_default_permuted": [[110, 192], ["warnings.warn", "print"], "function", ["None"], ["", "def", "_set_default_permuted", "(", "config", ")", ":", "\n", "    ", "\"\"\"Overwrite default configs for permutedMNIST.\n\n    Args:\n        config: The command line arguments.\n    Returns:\n        Altered configs to reproduce results reported in the paper.\n    \"\"\"", "\n", "if", "config", ".", "num_tasks", "<", "10", ":", "\n", "        ", "warn", "(", "'Training permuted with num tasks = %d. '", "%", "(", "config", ".", "num_tasks", ")", ")", "\n", "\n", "# General setup", "\n", "", "config", ".", "enc_fc_arch", "=", "'1000,1000'", "\n", "config", ".", "dec_fc_arch", "=", "'1000,1000'", "\n", "config", ".", "class_fc_arch", "=", "'1000,1000'", "\n", "config", ".", "enc_lr", ",", "config", ".", "dec_lr", ",", "config", ".", "dec_lr_emb", "=", "0.0001", ",", "0.0001", ",", "0.0001", "\n", "config", ".", "class_lr", ",", "config", ".", "class_lr_emb", "=", "0.0001", ",", "0.0001", "\n", "config", ".", "n_iter", "=", "5000", "\n", "config", ".", "batch_size", "=", "128", "\n", "config", ".", "data_dir", "=", "'../datasets'", "\n", "config", ".", "padding", "=", "2", "\n", "config", ".", "no_lookahead", "=", "False", "\n", "\n", "# VAE hnet", "\n", "config", ".", "rp_temb_size", "=", "24", "\n", "config", ".", "rp_emb_size", "=", "8", "\n", "config", ".", "rp_hnet_act", "=", "\"elu\"", "\n", "config", ".", "rp_hyper_chunks", "=", "85000", "\n", "config", ".", "rp_hnet_arch", "=", "'25,25'", "\n", "config", ".", "rp_beta", "=", "0.1", "\n", "\n", "# Classifier hnet", "\n", "config", ".", "class_temb_size", "=", "24", "\n", "config", ".", "class_emb_size", "=", "8", "\n", "config", ".", "class_hnet_act", "=", "\"elu\"", "\n", "config", ".", "class_hyper_chunks", "=", "78000", "\n", "config", ".", "class_hnet_arch", "=", "'25,25'", "\n", "config", ".", "class_beta", "=", "0.1", "\n", "\n", "# small capacity", "\n", "if", "config", ".", "class_fc_arch", "==", "'100,100'", ":", "\n", "# Classifier hnet", "\n", "        ", "config", ".", "class_temb_size", "=", "64", "\n", "config", ".", "class_emb_size", "=", "12", "\n", "config", ".", "class_hyper_chunks", "=", "2000", "\n", "config", ".", "class_beta", "=", "0.05", "\n", "config", ".", "class_hnet_arch", "=", "'100,75,50'", "\n", "\n", "#perm100 classifier config", "\n", "", "if", "config", ".", "num_tasks", ">=", "100", ":", "\n", "        ", "config", ".", "class_temb_size", "=", "128", "\n", "config", ".", "class_emb_size", "=", "12", "\n", "config", ".", "class_hnet_act", "=", "\"relu\"", "\n", "config", ".", "class_hnet_arch", "=", "'200,250,350'", "\n", "config", ".", "class_hyper_chunks", "=", "7500", "\n", "config", ".", "class_beta", "=", "0.01", "\n", "\n", "#perm250", "\n", "", "if", "config", ".", "num_tasks", ">=", "250", ":", "\n", "        ", "print", "(", "\"Attention: permuted250 not tested after the code migration.\"", ")", "\n", "config", ".", "class_hyper_chunks", "=", "12000", "\n", "config", ".", "hnet_reg_batch_size", "=", "32", "\n", "config", ".", "online_target_computation", "=", "True", "\n", "\n", "#HNET+TIR", "\n", "", "if", "config", ".", "infer_task_id", ":", "\n", "        ", "config", ".", "hard_targets", "=", "True", "\n", "#perm100 replay config", "\n", "if", "config", ".", "num_tasks", ">=", "100", ":", "\n", "            ", "config", ".", "dec_fc_arch", "=", "'400,600'", "\n", "config", ".", "rp_hnet_arch", "=", "'100'", "\n", "config", ".", "rp_hyper_chunks", "=", "20000", "\n", "config", ".", "rp_temb_size", "=", "128", "\n", "config", ".", "rp_emb_size", "=", "12", "\n", "if", "config", ".", "cl_scenario", "==", "2", ":", "\n", "                ", "config", ".", "hnet_reg_batch_size", "=", "20", "\n", "#HNET+R", "\n", "", "", "", "else", ":", "\n", "        ", "config", ".", "dec_fc_arch", "=", "'400,400'", "\n", "config", ".", "hard_targets", "=", "False", "\n", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.train_args_default._set_default_gan": [[193, 205], ["None"], "function", ["None"], ["", "def", "_set_default_gan", "(", "config", ")", ":", "\n", "    ", "\"\"\"Overwrite default configs for GAN training.\n\n    Args:\n        config: The command line arguments.\n    Returns:\n        Altered configs to reproduce results reported in the paper.\n    \"\"\"", "\n", "\n", "#TODO", "\n", "\n", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hp_search_permutedMNIST._get_performance_summary": [[145, 156], ["None"], "function", ["None"], ["def", "_get_performance_summary", "(", "out_dir", ",", "cmd_ident", ")", ":", "\n", "    ", "\"\"\"See docstring of method\n    :func:`hpsearch.hpsearch._get_performance_summary`.\n\n    You only need to implement this function, if the default parser in module\n    :func:`hpsearch.hpsearch` is not sufficient for your purposes.\n\n    In case you would like to use a custom parser, you have to set the\n    attribute :attr:`_SUMMARY_PARSER_HANDLER` correctly.\n    \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hp_search_permutedMNIST._performance_criteria": [[164, 185], ["NotImplementedError"], "function", ["None"], ["def", "_performance_criteria", "(", "summary_dict", ",", "performance_criteria", ")", ":", "\n", "    ", "\"\"\"Evaluate whether a run meets a given performance criteria.\n\n    This function is needed to decide whether the output directory of a run is\n    deleted or kept.\n\n    Args:\n        summary_dict: The performance summary dictionary as returned by\n            :attr:`_SUMMARY_PARSER_HANDLE`.\n        performance_criteria (float): The performance criteria. E.g., see\n            command-line option `performance_criteria` of script\n            :mod:`hpsearch.hpsearch_postprocessing`.\n\n    Returns:\n        bool: If :code:`True`, the result folder will be kept as the performance\n        criteria is assumed to be met.\n    \"\"\"", "\n", "### Example:", "\n", "# return summary_dict['performance_measure1'] > performance_criteria", "\n", "\n", "raise", "NotImplementedError", "(", "'TODO implement'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._grid_to_commands": [[101, 136], ["list", "grid_dict.keys", "len", "dict", "enumerate", "commands.append", "range", "len", "len"], "function", ["None"], ["def", "_grid_to_commands", "(", "grid_dict", ")", ":", "\n", "    ", "\"\"\"Translate a dictionary of parameter values into a list of commands.\n\n    Args:\n        grid_dict: A dictionary of argument names to lists, where each list\n            contains possible values for this argument.\n\n    Returns:\n        A list of dictionaries. Each key is an argument name that maps onto a\n        single value.\n    \"\"\"", "\n", "# We build a list of dictionaries with key value pairs.", "\n", "commands", "=", "[", "]", "\n", "\n", "# We need track of the index within each value array.", "\n", "gkeys", "=", "list", "(", "grid_dict", ".", "keys", "(", ")", ")", "\n", "indices", "=", "[", "0", "]", "*", "len", "(", "gkeys", ")", "\n", "\n", "stopping_criteria", "=", "False", "\n", "while", "not", "stopping_criteria", ":", "\n", "\n", "        ", "cmd", "=", "dict", "(", ")", "\n", "for", "i", ",", "k", "in", "enumerate", "(", "gkeys", ")", ":", "\n", "            ", "v", "=", "grid_dict", "[", "k", "]", "[", "indices", "[", "i", "]", "]", "\n", "cmd", "[", "k", "]", "=", "v", "\n", "", "commands", ".", "append", "(", "cmd", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "indices", ")", "-", "1", ",", "-", "1", ",", "-", "1", ")", ":", "\n", "            ", "indices", "[", "i", "]", "=", "(", "indices", "[", "i", "]", "+", "1", ")", "%", "len", "(", "grid_dict", "[", "gkeys", "[", "i", "]", "]", ")", "\n", "if", "indices", "[", "i", "]", "==", "0", "and", "i", "==", "0", ":", "\n", "                ", "stopping_criteria", "=", "True", "\n", "", "elif", "indices", "[", "i", "]", "!=", "0", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "return", "commands", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._args_to_cmd_str": [[137, 162], ["cmd_dict.items", "type", "str"], "function", ["None"], ["", "def", "_args_to_cmd_str", "(", "cmd_dict", ",", "out_dir", "=", "None", ")", ":", "\n", "    ", "\"\"\"Translate a dictionary of argument names to values into a string that\n    can be typed into a console.\n\n    Args:\n        cmd_dict: Dictionary with argument names as keys, that map to a value.\n        out_dir (optional): The output directory that should be passed to the\n            command. No output directory will be passed if not specified.\n\n    Returns:\n        A string of the form:\n            python3 train.py --out_dir=OUT_DIR --ARG1=VAL1 ...\n    \"\"\"", "\n", "cmd_str", "=", "'python3 %s'", "%", "_SCRIPT_NAME", "\n", "\n", "if", "out_dir", "is", "not", "None", ":", "\n", "        ", "cmd_str", "+=", "' --%s=%s'", "%", "(", "_OUT_ARG", ",", "out_dir", ")", "\n", "\n", "", "for", "k", ",", "v", "in", "cmd_dict", ".", "items", "(", ")", ":", "\n", "        ", "if", "type", "(", "v", ")", "==", "bool", ":", "\n", "            ", "cmd_str", "+=", "' --%s'", "%", "k", "if", "v", "else", "''", "\n", "", "else", ":", "\n", "            ", "cmd_str", "+=", "' --%s=%s'", "%", "(", "k", ",", "str", "(", "v", ")", ")", "\n", "\n", "", "", "return", "cmd_str", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._get_performance_summary": [[163, 223], ["os.path.join", "dict", "zip", "os.path.exists", "IOError", "open", "f.readlines", "line.split", "re.findall", "line.startswith", "ValueError", "len", "utils.misc.list_to_str"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.list_to_str"], ["", "def", "_get_performance_summary", "(", "out_dir", ",", "cmd_ident", ")", ":", "\n", "    ", "\"\"\"Parse the performance summary file of a simulation.\n\n    This is a very primitive parser, that expects that each line of the\n    result file :code:`os.path.join(out_dir, _SUMMARY_FILENAME)` is a\n    keyword-value pair. The keyword is taken from the :code:`_SUMMARY_KEYWORDS`\n    list. **They must appear in the correct order.**\n    The value can either be a single number or a list of numbers. A list of\n    numbers will be converted into a string, such that it appears in a single\n    cell under the given keyword when opening the result CSV file with a\n    spreadsheet.\n\n    Args:\n        out_dir: The output directory of the simulation.\n        cmd_ident (int): Identifier of this command (needed for informative\n            error messages).\n\n    Raises:\n        IOError: If performance summary file does not exist.\n        ValueError: If a summary key is not at the expected position in the\n            result file.\n\n    Returns:\n        A dictionary containing strings as keywords. Note, the values may not be\n        lists, and strings need to be wrapped into an extra layer of double\n        quotes such that the spreadsheet interprets them as a single entity.\n    \"\"\"", "\n", "# Get training results.", "\n", "result_summary_fn", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "_SUMMARY_FILENAME", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "result_summary_fn", ")", ":", "\n", "        ", "raise", "IOError", "(", "'Training run %d did not finish. No results!'", "%", "(", "cmd_ident", "+", "1", ")", ")", "\n", "\n", "", "with", "open", "(", "result_summary_fn", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "result_summary", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "# Ensure downwards compatibility!", "\n", "", "summary_keys", "=", "_SUMMARY_KEYWORDS", "\n", "\n", "performance_dict", "=", "dict", "(", ")", "\n", "for", "line", ",", "key", "in", "zip", "(", "result_summary", ",", "summary_keys", ")", ":", "\n", "        ", "if", "not", "line", ".", "startswith", "(", "key", ")", ":", "\n", "            ", "raise", "ValueError", "(", "'Key %s does not appear in performance '", "\n", "%", "(", "key", ")", "+", "'summary where it is expected.'", ")", "\n", "# Parse the lines of the result file.", "\n", "# Crop keyword to retrieve only the value.", "\n", "", "_", ",", "line", "=", "line", ".", "split", "(", "' '", ",", "maxsplit", "=", "1", ")", "\n", "# https://stackoverflow.com/questions/4703390/how-to-extract-a-floating-number-from-a-string", "\n", "line_nums", "=", "re", ".", "findall", "(", "r\"[-+]?\\d*\\.\\d+|\\d+\"", ",", "line", ")", "\n", "if", "len", "(", "line_nums", ")", "==", "1", ":", "# Single number", "\n", "            ", "performance_dict", "[", "key", "]", "=", "[", "line_nums", "[", "0", "]", "]", "\n", "", "else", ":", "# List of numbers", "\n", "# Convert list to a string for the resulting CSV file. Note, the", "\n", "# quotes are needed that the list will be written into a single cell", "\n", "# when opening the csv file (note, every key can have exactly one", "\n", "# value).", "\n", "            ", "performance_dict", "[", "key", "]", "=", "[", "'\"'", "+", "misc", ".", "list_to_str", "(", "line_nums", ",", "delim", "=", "','", ")", "+", "'\"'", "]", "\n", "\n", "", "", "return", "performance_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._run_cmds_on_single_machine": [[224, 296], ["enumerate", "os.path.join", "os.path.exists", "hpsearch._args_to_cmd_str", "print", "subprocess.call", "print", "time.sleep", "os.path.join", "os.path.exists", "_SUMMARY_PARSER_HANDLE", "_SUMMARY_PARSER_HANDLE.items", "pandas.DataFrame.from_dict", "os.path.isfile", "pandas.concat.to_csv", "int", "traceback.print_exc", "warnings.warn", "datetime.datetime.now().strftime", "open", "f.write", "f.write", "pandas.read_csv", "pandas.concat", "traceback.print_exc", "warnings.warn", "datetime.datetime.now().strftime", "len", "os.path.join", "datetime.datetime.now", "hpsearch._args_to_cmd_str", "len", "hpsearch._args_to_cmd_str", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._args_to_cmd_str", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._args_to_cmd_str", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._args_to_cmd_str"], ["", "def", "_run_cmds_on_single_machine", "(", "args", ",", "commands", ",", "out_dir", ",", "results_file", ")", ":", "\n", "    ", "\"\"\"Method to run the jobs sequentially on a single machine.\n\n    Args:\n        args: Command-line arguments.\n        commands: List of command dictionaries.\n        out_dir: Output directory.\n        results_file: CSV file to store summary.\n    \"\"\"", "\n", "# FIXME: The code in this function is mostly a dublicate of the code in", "\n", "# function _run_cmds_on_cluster.", "\n", "\n", "for", "i", ",", "cmd_dict", "in", "enumerate", "(", "commands", ")", ":", "\n", "# A call might fail for several reasons. We don't want the whole search", "\n", "# to fail.", "\n", "        ", "try", ":", "\n", "# FIXME quick and dirty solution.", "\n", "            ", "cmd_out_dir", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\n", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S-%f'", ")", "[", ":", "-", "3", "]", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "cmd_out_dir", ")", ":", "\n", "                ", "time", ".", "sleep", "(", "1.1", ")", "\n", "cmd_out_dir", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "\n", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S-%f'", ")", "[", ":", "-", "3", "]", ")", "\n", "", "assert", "(", "not", "os", ".", "path", ".", "exists", "(", "cmd_out_dir", ")", ")", "\n", "\n", "cmd_str", "=", "_args_to_cmd_str", "(", "cmd_dict", ",", "out_dir", "=", "cmd_out_dir", ")", "\n", "cmd_dict", "[", "_OUT_ARG", "]", "=", "cmd_out_dir", "\n", "\n", "# Execute the program.", "\n", "print", "(", "'Starting training run %d/%d -- \"%s\"'", "%", "(", "i", "+", "1", ",", "len", "(", "commands", ")", ",", "\n", "cmd_str", ")", ")", "\n", "ret", "=", "call", "(", "cmd_str", ",", "shell", "=", "True", ")", "\n", "print", "(", "'Call finished with return code %d.'", "%", "ret", ")", "\n", "\n", "try", ":", "\n", "# We store the command used for execution. This might be helpful", "\n", "# for the user in case he wants to manually continue the", "\n", "# simulation.", "\n", "                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "cmd_out_dir", ",", "'hpsearch_command.sh'", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "'#!/bin/sh\\n'", ")", "\n", "f", ".", "write", "(", "'%s'", "%", "(", "_args_to_cmd_str", "(", "cmd_dict", ")", ")", ")", "\n", "\n", "# Get training results.", "\n", "", "performance_dict", "=", "_SUMMARY_PARSER_HANDLE", "(", "cmd_out_dir", ",", "i", ")", "\n", "for", "k", ",", "v", "in", "performance_dict", ".", "items", "(", ")", ":", "\n", "                    ", "cmd_dict", "[", "k", "]", "=", "v", "\n", "\n", "# Create or update the CSV file summarizing all runs.", "\n", "", "panda_frame", "=", "pandas", ".", "DataFrame", ".", "from_dict", "(", "cmd_dict", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "results_file", ")", ":", "\n", "                    ", "old_frame", "=", "pandas", ".", "read_csv", "(", "results_file", ",", "sep", "=", "';'", ")", "\n", "panda_frame", "=", "pandas", ".", "concat", "(", "[", "old_frame", ",", "panda_frame", "]", ",", "\n", "sort", "=", "True", ")", "\n", "", "panda_frame", ".", "to_csv", "(", "results_file", ",", "sep", "=", "';'", ",", "index", "=", "False", ")", "\n", "\n", "# Check whether simulation has finished successfully.", "\n", "has_finished", "=", "int", "(", "cmd_dict", "[", "'finished'", "]", "[", "0", "]", ")", "\n", "if", "has_finished", "==", "1", ":", "\n", "                    ", "_CMD_FINISHED", "[", "i", "]", "=", "True", "\n", "", "else", ":", "\n", "                    ", "_CMD_FINISHED", "[", "i", "]", "=", "False", "\n", "\n", "", "", "except", "Exception", ":", "\n", "                ", "traceback", ".", "print_exc", "(", "file", "=", "sys", ".", "stdout", ")", "\n", "warnings", ".", "warn", "(", "'Could not assess whether run %d has been '", "%", "(", "i", "+", "1", ")", "+", "'completed.'", ")", "\n", "\n", "", "", "except", "Exception", ":", "\n", "            ", "traceback", ".", "print_exc", "(", "file", "=", "sys", ".", "stdout", ")", "\n", "warnings", ".", "warn", "(", "'Call %d/%d failed -- \"%s\".'", "%", "(", "i", "+", "1", ",", "len", "(", "commands", ")", ",", "_args_to_cmd_str", "(", "cmd_dict", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._run_cmds_on_cluster": [[297, 399], ["bsub.running_jobs", "len", "hpsearch._run_cmds_on_cluster.check_running"], "function", ["None"], ["", "", "", "def", "_run_cmds_on_cluster", "(", "args", ",", "commands", ",", "out_dir", ",", "results_file", ")", ":", "\n", "    ", "\"\"\"This method will submit a certain number of jobs onto an LSF cluster and\n    wait for these jobs to complete before starting new jobs. This allows to\n    run several jobs in parallel.\n\n    Args:\n        args: Command-line arguments.\n        commands: List of command dictionaries.\n        out_dir: Output directory.\n        results_file: CSV file to store summary.\n    \"\"\"", "\n", "from", "bsub", "import", "bsub", "\n", "\n", "def", "check_running", "(", "jobs", ")", ":", "\n", "        ", "rjobs", "=", "bsub", ".", "running_jobs", "(", ")", "\n", "tmp_jobs", "=", "jobs", "\n", "jobs", "=", "[", "]", "\n", "for", "job", ",", "cmd_dict", ",", "ind", "in", "tmp_jobs", ":", "\n", "            ", "if", "job", ".", "job_id", "in", "rjobs", ":", "\n", "                ", "jobs", ".", "append", "(", "(", "job", ",", "cmd_dict", ",", "ind", ")", ")", "\n", "continue", "\n", "\n", "", "print", "(", "'Job %d finished.'", "%", "ind", ")", "\n", "cmd_out_dir", "=", "cmd_dict", "[", "_OUT_ARG", "]", "\n", "\n", "try", ":", "\n", "# We store the command used for execution. This might be helpful", "\n", "# for the user in case he wants to manually continue the", "\n", "# simulation.", "\n", "                ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "cmd_out_dir", ",", "'hpsearch_command.sh'", ")", ",", "\n", "'w'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "'#!/bin/sh\\n'", ")", "\n", "f", ".", "write", "(", "'%s'", "%", "(", "_args_to_cmd_str", "(", "cmd_dict", ")", ")", ")", "\n", "\n", "# Get training results.", "\n", "", "performance_dict", "=", "_SUMMARY_PARSER_HANDLE", "(", "cmd_out_dir", ",", "i", ")", "\n", "for", "k", ",", "v", "in", "performance_dict", ".", "items", "(", ")", ":", "\n", "                    ", "cmd_dict", "[", "k", "]", "=", "v", "\n", "\n", "# Create or update the CSV file summarizing all runs.", "\n", "", "panda_frame", "=", "pandas", ".", "DataFrame", ".", "from_dict", "(", "cmd_dict", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "results_file", ")", ":", "\n", "                    ", "old_frame", "=", "pandas", ".", "read_csv", "(", "results_file", ",", "sep", "=", "';'", ")", "\n", "panda_frame", "=", "pandas", ".", "concat", "(", "[", "old_frame", ",", "panda_frame", "]", ",", "\n", "sort", "=", "True", ")", "\n", "", "panda_frame", ".", "to_csv", "(", "results_file", ",", "sep", "=", "';'", ",", "index", "=", "False", ")", "\n", "\n", "# Check whether simulation has finished successfully.", "\n", "has_finished", "=", "int", "(", "cmd_dict", "[", "'finished'", "]", "[", "0", "]", ")", "\n", "if", "has_finished", "==", "1", ":", "\n", "                    ", "_CMD_FINISHED", "[", "ind", "]", "=", "True", "\n", "", "else", ":", "\n", "                    ", "_CMD_FINISHED", "[", "ind", "]", "=", "False", "\n", "\n", "", "", "except", "Exception", ":", "\n", "                ", "traceback", ".", "print_exc", "(", "file", "=", "sys", ".", "stdout", ")", "\n", "warnings", ".", "warn", "(", "'Could not assess whether run %d has been '", "%", "(", "ind", "+", "1", ")", "+", "'completed.'", ")", "\n", "\n", "", "", "return", "jobs", "\n", "\n", "", "jobs", "=", "[", "]", "\n", "i", "=", "-", "1", "\n", "while", "len", "(", "commands", ")", ">", "0", ":", "\n", "        ", "jobs", "=", "check_running", "(", "jobs", ")", "\n", "while", "len", "(", "jobs", ")", ">=", "args", ".", "num_jobs", ":", "\n", "            ", "time", ".", "sleep", "(", "10", ")", "\n", "jobs", "=", "check_running", "(", "jobs", ")", "\n", "\n", "", "cmd_dict", "=", "commands", ".", "pop", "(", ")", "\n", "i", "+=", "1", "\n", "\n", "# FIXME quick and dirty solution.", "\n", "folder_name", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S-%f'", ")", "[", ":", "-", "3", "]", "\n", "if", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "out_dir", ",", "folder_name", ")", ")", ":", "\n", "            ", "time", ".", "sleep", "(", "1.1", ")", "\n", "folder_name", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S-%f'", ")", "[", ":", "-", "3", "]", "\n", "", "cmd_out_dir", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "folder_name", ")", "\n", "assert", "(", "not", "os", ".", "path", ".", "exists", "(", "cmd_out_dir", ")", ")", "\n", "\n", "cmd_str", "=", "_args_to_cmd_str", "(", "cmd_dict", ",", "out_dir", "=", "cmd_out_dir", ")", "\n", "cmd_dict", "[", "_OUT_ARG", "]", "=", "cmd_out_dir", "\n", "\n", "# Execute the program.", "\n", "print", "(", "'Starting training run %d/%d -- \"%s\"'", "%", "(", "i", "+", "1", ",", "len", "(", "commands", ")", ",", "\n", "cmd_str", ")", ")", "\n", "\n", "job_name", "=", "'job_%s'", "%", "folder_name", "\n", "# FIXME the bsub module ignores the pathnames we set. Hence, all output", "\n", "# files are simply stored in the local directory. For now, we will", "\n", "# capture this in the postprocessing script.", "\n", "job_error_file", "=", "os", ".", "path", ".", "join", "(", "cmd_out_dir", ",", "job_name", "+", "'.err'", ")", "\n", "job_out_file", "=", "os", ".", "path", ".", "join", "(", "cmd_out_dir", ",", "job_name", "+", "'.out'", ")", "\n", "sub", "=", "bsub", "(", "job_name", ",", "R", "=", "args", ".", "resources", ",", "n", "=", "1", ",", "W", "=", "'%d:00'", "%", "args", ".", "num_hours", ",", "\n", "e", "=", "job_error_file", ",", "o", "=", "job_out_file", ",", "verbose", "=", "True", ")", "\n", "sub", "(", "cmd_str", ")", "\n", "jobs", ".", "append", "(", "(", "sub", ",", "cmd_dict", ",", "i", ")", ")", "\n", "\n", "# Wait for all jobs to complete.", "\n", "", "while", "len", "(", "jobs", ")", ">", "0", ":", "\n", "        ", "time", ".", "sleep", "(", "10", ")", "\n", "jobs", "=", "check_running", "(", "jobs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._backup_commands": [[400, 428], ["os.path.join", "os.path.join", "open", "pickle.dump", "open", "f.write", "f.write", "f.write", "f.write", "hpsearch._args_to_cmd_str", "len"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._args_to_cmd_str"], ["", "", "def", "_backup_commands", "(", "commands", ",", "out_dir", ")", ":", "\n", "    ", "\"\"\"This function will generate a bash script that resembles the order in\n    which the individual commands have been executed. This is important, as the\n    order might be random. This script is just another helper for the user to\n    follow the execution order. Additionally, this file save the commands as\n    pickle. This is a backup for future usage (i.e., maybe a continue search\n    option will be build in at some point).\n\n    Args:\n        commands: List of command dictionaries.\n        out_dir: Output directory.\n    \"\"\"", "\n", "fn_script", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'commands.sh'", ")", "\n", "fn_pickle", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'commands.pickle'", ")", "\n", "\n", "with", "open", "(", "fn_pickle", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "commands", ",", "f", ")", "\n", "\n", "", "with", "open", "(", "fn_script", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "'#!/bin/sh\\n'", ")", "\n", "f", ".", "write", "(", "'# This script contains all %d commands that are planned '", "%", "(", "len", "(", "commands", ")", ")", "+", "'to be executed during this '", "+", "\n", "'hyperparameter search. The order of execution is preserved '", "+", "\n", "'in this script.\\n'", ")", "\n", "f", ".", "write", "(", "'# Note, output directories are not yet specified in the '", "+", "\n", "'commands listed here!\\n'", ")", "\n", "for", "cmd", "in", "commands", ":", "\n", "            ", "f", ".", "write", "(", "'%s\\n\\n'", "%", "(", "_args_to_cmd_str", "(", "cmd", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._store_incomplete": [[429, 451], ["enumerate", "warnings.warn", "os.path.join", "len", "open", "pickle.dump", "incomplete.append", "len"], "function", ["None"], ["", "", "", "def", "_store_incomplete", "(", "commands", ",", "out_dir", ")", ":", "\n", "    ", "\"\"\"This function will pickle all command dictionaries of commands that have\n    not been completed. This might be used to just continue an interrupted\n    hyperparameter search.\n\n    Args:\n        commands: List of command dictionaries.\n        out_dir: Output directory.\n    \"\"\"", "\n", "incomplete", "=", "[", "]", "\n", "\n", "for", "i", ",", "cmd", "in", "enumerate", "(", "commands", ")", ":", "\n", "        ", "if", "not", "_CMD_FINISHED", "[", "i", "]", ":", "\n", "            ", "incomplete", ".", "append", "(", "cmd", ")", "\n", "\n", "", "", "if", "len", "(", "incomplete", ")", "==", "0", ":", "\n", "        ", "return", "\n", "\n", "", "warnings", ".", "warn", "(", "'%d runs have not been completed.'", "%", "(", "len", "(", "incomplete", ")", ")", ")", "\n", "fn_pickle", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'not_completed.pickle'", ")", "\n", "with", "open", "(", "fn_pickle", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "incomplete", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.hpsearch._read_config": [[452, 511], ["hasattr", "hasattr", "hasattr", "hasattr", "hasattr", "globals", "globals", "globals", "hasattr", "hasattr", "globals", "globals", "globals", "hasattr", "globals", "warnings.warn", "globals", "globals", "globals", "hasattr", "globals", "hasattr"], "function", ["None"], ["", "", "def", "_read_config", "(", "config_mod", ",", "require_perf_eval_handle", "=", "False", ",", "\n", "require_argparse_handle", "=", "False", ")", ":", "\n", "    ", "\"\"\"Parse the configuration module and check whether all attributes are set\n    correctly.\n\n    This function will set the corresponding global variables from this script\n    appropriately.\n\n    Args:\n        config_mod: The implemented configuration template\n            :mod:`hpsearch.hpsearch_postprocessing`.\n        require_perf_eval_handle: Whether :attr:`_PERFORMANCE_EVAL_HANDLE` has\n            to be specified in the config file.\n        require_argparse_handle: Whether :attr:`_ARGPARSE_HANDLE` has to be\n            specified in the config file.\n    \"\"\"", "\n", "assert", "(", "hasattr", "(", "config_mod", ",", "'_SCRIPT_NAME'", ")", ")", "\n", "assert", "(", "hasattr", "(", "config_mod", ",", "'_SUMMARY_FILENAME'", ")", ")", "\n", "assert", "(", "hasattr", "(", "config_mod", ",", "'_SUMMARY_KEYWORDS'", ")", "and", "'finished'", "in", "config_mod", ".", "_SUMMARY_KEYWORDS", ")", "\n", "globals", "(", ")", "[", "'_SCRIPT_NAME'", "]", "=", "config_mod", ".", "_SCRIPT_NAME", "\n", "globals", "(", ")", "[", "'_SUMMARY_FILENAME'", "]", "=", "config_mod", ".", "_SUMMARY_FILENAME", "\n", "globals", "(", ")", "[", "'_SUMMARY_KEYWORDS'", "]", "=", "config_mod", ".", "_SUMMARY_KEYWORDS", "\n", "\n", "# Ensure downwards compatibility -- attributes did not exist previously.", "\n", "if", "hasattr", "(", "config_mod", ",", "'_OUT_ARG'", ")", ":", "\n", "        ", "globals", "(", ")", "[", "'_OUT_ARG'", "]", "=", "config_mod", ".", "_OUT_ARG", "\n", "\n", "", "if", "hasattr", "(", "config_mod", ",", "'_SUMMARY_PARSER_HANDLE'", ")", "and", "config_mod", ".", "_SUMMARY_PARSER_HANDLE", "is", "not", "None", ":", "\n", "        ", "globals", "(", ")", "[", "'_SUMMARY_PARSER_HANDLE'", "]", "=", "config_mod", ".", "_SUMMARY_PARSER_HANDLE", "\n", "", "else", ":", "\n", "        ", "globals", "(", ")", "[", "'_SUMMARY_PARSER_HANDLE'", "]", "=", "_get_performance_summary", "\n", "\n", "", "if", "require_perf_eval_handle", ":", "\n", "        ", "assert", "(", "hasattr", "(", "config_mod", ",", "'_PERFORMANCE_EVAL_HANDLE'", ")", "and", "config_mod", ".", "_PERFORMANCE_EVAL_HANDLE", "is", "not", "None", ")", "\n", "globals", "(", ")", "[", "'_PERFORMANCE_EVAL_HANDLE'", "]", "=", "config_mod", ".", "_PERFORMANCE_EVAL_HANDLE", "\n", "", "else", ":", "\n", "        ", "if", "not", "hasattr", "(", "config_mod", ",", "'_PERFORMANCE_EVAL_HANDLE'", ")", "or", "config_mod", ".", "_PERFORMANCE_EVAL_HANDLE", "is", "None", ":", "\n", "            ", "warnings", ".", "warn", "(", "'Attribute \"_PERFORMANCE_EVAL_HANDLE\" not defined '", "+", "\n", "'in configuration file but might be required in '", "+", "\n", "'future releases.'", ")", "\n", "\n", "", "", "if", "hasattr", "(", "config_mod", ",", "'_PERFORMANCE_KEY'", ")", "and", "config_mod", ".", "_PERFORMANCE_KEY", "is", "not", "None", ":", "\n", "        ", "globals", "(", ")", "[", "'_PERFORMANCE_KEY'", "]", "=", "config_mod", ".", "_PERFORMANCE_KEY", "\n", "", "else", ":", "\n", "        ", "globals", "(", ")", "[", "'_PERFORMANCE_KEY'", "]", "=", "config_mod", ".", "_SUMMARY_KEYWORDS", "[", "0", "]", "\n", "\n", "", "if", "hasattr", "(", "config_mod", ",", "'_PERFORMANCE_SORT_ASC'", ")", ":", "\n", "        ", "globals", "(", ")", "[", "'_PERFORMANCE_SORT_ASC'", "]", "=", "config_mod", ".", "_PERFORMANCE_SORT_ASC", "\n", "\n", "", "if", "require_argparse_handle", ":", "\n", "        ", "assert", "(", "hasattr", "(", "config_mod", ",", "'_ARGPARSE_HANDLE'", ")", "and", "config_mod", ".", "_ARGPARSE_HANDLE", "is", "not", "None", ")", "\n", "globals", "(", ")", "[", "'_ARGPARSE_HANDLE'", "]", "=", "config_mod", ".", "_ARGPARSE_HANDLE", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_utils_replay.generate_replay_networks": [[53, 207], ["print", "utils.str_to_ints", "mnets.mlp.MLP().to", "print", "list", "print", "utils.str_to_ints", "mnets.mlp.MLP().to", "print", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "hasattr", "MLP().to.parameters", "print", "utils.sim_utils.get_hnet_model", "list", "sum", "print", "list", "sim_utils.get_hnet_model.get_task_embs", "mnets.mlp.MLP", "mnets.mlp.MLP", "sim_utils.get_hnet_model.parameters", "print", "MLP().to.parameters", "W.ndimension", "torch.nn.init.constant_", "torch.nn.init.xavier_uniform_", "torch.nn.init.normal_", "torch.nn.init.normal_", "p.numel", "utils.str_to_act", "utils.str_to_act", "sim_utils.get_hnet_model.parameters"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.cifar.train_utils.get_hnet_model", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_act", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_act"], ["def", "generate_replay_networks", "(", "config", ",", "data_handlers", ",", "device", ",", "\n", "create_rp_hnet", "=", "True", ",", "only_train_replay", "=", "False", ")", ":", "\n", "    ", "\"\"\"Create a replay model that consists of either a encoder/decoder or\n    a discriminator/generator pair. Additionally, this method manages the \n    creation of a hypernetwork for the generator/decoder. \n    Following important configurations will be determined in order to create\n    the replay model: \n    * in- and output and hidden layer dimensions of the encoder/decoder. \n    * architecture, chunk- and task-embedding details of decoder's hypernetwork. \n\n    .. note::\n        This module also handles the initialisation of the weights of either \n        the classifier or its hypernetwork. This will change in the near future.\n\n    Args:\n        config: Command-line arguments.\n        data_handlers: List of data handlers, one for each task. Needed to\n            extract the number of inputs/outputs of the main network. And to\n            infer the number of tasks.\n        device: Torch device..\n        create_rp_hnet: Whether a hypernetwork for the replay should be \n            constructed. If not, the decoder/generator will have \n            trainable weights on its own.\n        only_train_replay: We normally do not train on the last task since we do \n            not need to replay this last tasks data. But if we want a replay \n            method to be able to generate data from all tasks then we set this \n            option to true.\n\n    Returns:\n        (tuple): Tuple containing:\n\n        - **enc**: Encoder/discriminator network instance.\n        - **dec**: Decoder/generator networkinstance.\n        - **dec_hnet**: Hypernetwork instance for the decoder/generator. This \n            return value is None if no hypernetwork should be constructed.\n    \"\"\"", "\n", "\n", "if", "config", ".", "replay_method", "==", "'gan'", ":", "\n", "        ", "n_out", "=", "1", "\n", "", "else", ":", "\n", "        ", "n_out", "=", "config", ".", "latent_dim", "*", "2", "\n", "\n", "", "n_in", "=", "data_handlers", "[", "0", "]", ".", "in_shape", "[", "0", "]", "\n", "pd", "=", "config", ".", "padding", "*", "2", "\n", "if", "config", ".", "experiment", "==", "\"splitMNIST\"", ":", "\n", "        ", "n_in", "=", "n_in", "*", "n_in", "\n", "", "else", ":", "# permutedMNIST", "\n", "        ", "n_in", "=", "(", "n_in", "+", "pd", ")", "*", "(", "n_in", "+", "pd", ")", "\n", "\n", "", "config", ".", "input_dim", "=", "n_in", "\n", "if", "config", ".", "experiment", "==", "\"splitMNIST\"", ":", "\n", "        ", "if", "config", ".", "single_class_replay", ":", "\n", "            ", "config", ".", "out_dim", "=", "1", "\n", "", "else", ":", "\n", "            ", "config", ".", "out_dim", "=", "2", "\n", "", "", "else", ":", "# permutedMNIST", "\n", "        ", "config", ".", "out_dim", "=", "10", "\n", "\n", "", "if", "config", ".", "infer_task_id", ":", "\n", "# task inference network", "\n", "        ", "config", ".", "out_dim", "=", "1", "\n", "\n", "# builld encoder", "\n", "", "print", "(", "'For the replay encoder/discriminator: '", ")", "\n", "enc_arch", "=", "misc", ".", "str_to_ints", "(", "config", ".", "enc_fc_arch", ")", "\n", "enc", "=", "MLP", "(", "n_in", "=", "n_in", ",", "n_out", "=", "n_out", ",", "hidden_layers", "=", "enc_arch", ",", "\n", "activation_fn", "=", "misc", ".", "str_to_act", "(", "config", ".", "enc_net_act", ")", ",", "\n", "dropout_rate", "=", "config", ".", "enc_dropout_rate", ",", "\n", "no_weights", "=", "False", ")", ".", "to", "(", "device", ")", "\n", "print", "(", "'Constructed MLP with shapes: '", ",", "enc", ".", "param_shapes", ")", "\n", "init_params", "=", "list", "(", "enc", ".", "parameters", "(", ")", ")", "\n", "# builld decoder", "\n", "print", "(", "'For the replay decoder/generator: '", ")", "\n", "dec_arch", "=", "misc", ".", "str_to_ints", "(", "config", ".", "dec_fc_arch", ")", "\n", "# add dimensions for conditional input", "\n", "n_out", "=", "config", ".", "latent_dim", "\n", "\n", "if", "config", ".", "conditional_replay", ":", "\n", "        ", "n_out", "+=", "config", ".", "conditional_dim", "\n", "\n", "", "dec", "=", "MLP", "(", "n_in", "=", "n_out", ",", "n_out", "=", "n_in", ",", "hidden_layers", "=", "dec_arch", ",", "\n", "activation_fn", "=", "misc", ".", "str_to_act", "(", "config", ".", "dec_net_act", ")", ",", "\n", "use_bias", "=", "True", ",", "\n", "no_weights", "=", "config", ".", "rp_beta", ">", "0", ",", "\n", "dropout_rate", "=", "config", ".", "dec_dropout_rate", ")", ".", "to", "(", "device", ")", "\n", "\n", "print", "(", "'Constructed MLP with shapes: '", ",", "dec", ".", "param_shapes", ")", "\n", "config", ".", "num_weights_enc", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "enc", ".", "param_shapes", ")", "\n", "\n", "config", ".", "num_weights_dec", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "dec", ".", "param_shapes", ")", "\n", "config", ".", "num_weights_rp_net", "=", "config", ".", "num_weights_enc", "+", "config", ".", "num_weights_dec", "\n", "# we do not need a replay model for the last task", "\n", "\n", "# train on last task or not", "\n", "if", "only_train_replay", ":", "\n", "        ", "subtr", "=", "0", "\n", "", "else", ":", "\n", "        ", "subtr", "=", "1", "\n", "\n", "", "num_embeddings", "=", "config", ".", "num_tasks", "-", "subtr", "if", "config", ".", "num_tasks", ">", "1", "else", "1", "\n", "\n", "if", "config", ".", "single_class_replay", ":", "\n", "# we do not need a replay model for the last task", "\n", "        ", "if", "config", ".", "num_tasks", ">", "1", ":", "\n", "            ", "num_embeddings", "=", "config", ".", "out_dim", "*", "(", "config", ".", "num_tasks", "-", "subtr", ")", "\n", "", "else", ":", "\n", "            ", "num_embeddings", "=", "config", ".", "out_dim", "*", "(", "config", ".", "num_tasks", ")", "\n", "\n", "", "", "config", ".", "num_embeddings", "=", "num_embeddings", "\n", "# build decoder hnet", "\n", "if", "create_rp_hnet", ":", "\n", "        ", "print", "(", "'For the decoder/generator hypernetwork: '", ")", "\n", "d_hnet", "=", "sim_utils", ".", "get_hnet_model", "(", "config", ",", "num_embeddings", ",", "\n", "device", ",", "dec", ".", "hyper_shapes_learned", ",", "cprefix", "=", "'rp_'", ")", "\n", "\n", "init_params", "+=", "list", "(", "d_hnet", ".", "parameters", "(", ")", ")", "\n", "\n", "config", ".", "num_weights_rp_hyper_net", "=", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "\n", "d_hnet", ".", "parameters", "(", ")", "if", "p", ".", "requires_grad", ")", "\n", "config", ".", "compression_ratio_rp", "=", "config", ".", "num_weights_rp_hyper_net", "/", "config", ".", "num_weights_dec", "\n", "print", "(", "'Created replay hypernetwork with ratio: '", ",", "\n", "config", ".", "compression_ratio_rp", ")", "\n", "if", "config", ".", "compression_ratio_rp", ">", "1", ":", "\n", "            ", "print", "(", "'Note that the compression ratio is computed compared to '", "+", "\n", "'current target network,\\nthis might not be directly '", "+", "\n", "'comparable with the number of parameters of methods we '", "+", "\n", "'compare against.'", ")", "\n", "", "", "else", ":", "\n", "        ", "num_embeddings", "=", "config", ".", "num_tasks", "-", "subtr", "\n", "d_hnet", "=", "None", "\n", "init_params", "+=", "list", "(", "dec", ".", "parameters", "(", ")", ")", "\n", "config", ".", "num_weights_rp_hyper_net", "=", "0", "\n", "config", ".", "compression_ratio_rp", "=", "0", "\n", "\n", "### Initialize network weights.", "\n", "", "for", "W", "in", "init_params", ":", "\n", "        ", "if", "W", ".", "ndimension", "(", ")", "==", "1", ":", "# Bias vector.", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "W", ",", "0", ")", "\n", "", "else", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "W", ")", "\n", "\n", "# The task embeddings are initialized differently.", "\n", "", "", "if", "create_rp_hnet", ":", "\n", "        ", "for", "temb", "in", "d_hnet", ".", "get_task_embs", "(", ")", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "temb", ",", "mean", "=", "0.", ",", "std", "=", "config", ".", "std_normal_temb", ")", "\n", "\n", "", "", "if", "hasattr", "(", "d_hnet", ",", "'chunk_embeddings'", ")", ":", "\n", "        ", "for", "emb", "in", "d_hnet", ".", "chunk_embeddings", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "emb", ",", "mean", "=", "0", ",", "std", "=", "config", ".", "std_normal_emb", ")", "\n", "\n", "", "", "return", "enc", ",", "dec", ",", "d_hnet", "", "", ""]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.collect_rp_cmd_arguments": [[36, 109], ["argparse.ArgumentParser", "train_args_replay.cl_arguments_replay", "train_args_replay.cl_arguments_general", "train_args_replay.data_args", "utils.cl_args", "utils.train_args", "utils.main_net_args", "utils.main_net_args", "utils.hypernet_args", "utils.init_args", "utils.miscellaneous_args", "utils.generator_args", "utils.eval_args", "train_args_replay.train_args_replay", "train_args_replay.train_args_replay", "train_args_replay.split_args", "datetime.datetime.now().strftime", "utils.cl_args", "utils.train_args", "utils.main_net_args", "utils.main_net_args", "utils.hypernet_args", "utils.init_args", "utils.miscellaneous_args", "utils.generator_args", "utils.eval_args", "train_args_replay.train_args_replay", "train_args_replay.train_args_replay", "train_args_replay.perm_args", "datetime.datetime.now().strftime", "datetime.datetime.now", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.cl_arguments_replay", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.cl_arguments_general", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.data_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.cl_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.train_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.main_net_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.main_net_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.hypernet_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.init_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.miscellaneous_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.generator_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.eval_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.train_args_replay", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.train_args_replay", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.split_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.cl_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.train_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.main_net_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.main_net_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.hypernet_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.init_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.miscellaneous_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.generator_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.eval_args", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.train_args_replay", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.train_args_replay", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.perm_args"], ["def", "collect_rp_cmd_arguments", "(", "mode", "=", "'split'", ",", "description", "=", "\"\"", ")", ":", "\n", "    ", "\"\"\"Collect command-line arguments.\n\n    Args:\n        mode: For what script should the parser assemble the set of command-line\n            parameters? Options:\n\n                - \"split\"\n                - \"perm\"\n    Returns:\n        The Namespace object containing argument names and values.\n    \"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "description", ")", "\n", "\n", "# If needed, add additional parameters.", "\n", "if", "mode", "==", "'split'", ":", "\n", "        ", "dout_dir", "=", "'./out_split/run_'", "+", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S'", ")", "\n", "\n", "cl_argroup", "=", "cli", ".", "cl_args", "(", "parser", ",", "show_beta", "=", "False", ",", "\n", "show_from_scratch", "=", "True", ",", "show_multi_head", "=", "False", ",", "\n", "show_cl_scenario", "=", "True", ",", "show_num_tasks", "=", "True", ",", "dnum_tasks", "=", "5", ")", "\n", "train_argroup", "=", "cli", ".", "train_args", "(", "parser", ",", "show_lr", "=", "False", ",", "dbatch_size", "=", "128", ",", "\n", "dn_iter", "=", "2000", ",", "show_epochs", "=", "True", ")", "\n", "cli", ".", "main_net_args", "(", "parser", ",", "allowed_nets", "=", "[", "'fc'", "]", ",", "dfc_arch", "=", "'400,400'", ",", "\n", "dnet_act", "=", "'relu'", ",", "prefix", "=", "'enc_'", ",", "pf_name", "=", "'encoder'", ")", "\n", "cli", ".", "main_net_args", "(", "parser", ",", "allowed_nets", "=", "[", "'fc'", "]", ",", "dfc_arch", "=", "'400,400'", ",", "\n", "dnet_act", "=", "'relu'", ",", "prefix", "=", "'dec_'", ",", "pf_name", "=", "'decoder'", ")", "\n", "cli", ".", "hypernet_args", "(", "parser", ",", "dhyper_chunks", "=", "50000", ",", "dhnet_arch", "=", "'10,10'", ",", "\n", "dtemb_size", "=", "96", ",", "demb_size", "=", "96", ",", "prefix", "=", "'rp_'", ",", "\n", "pf_name", "=", "'replay'", ",", "dhnet_act", "=", "'elu'", ")", "\n", "cli", ".", "init_args", "(", "parser", ",", "custom_option", "=", "False", ")", "\n", "cli", ".", "miscellaneous_args", "(", "parser", ",", "big_data", "=", "False", ",", "synthetic_data", "=", "False", ",", "\n", "show_plots", "=", "True", ",", "no_cuda", "=", "False", ",", "dout_dir", "=", "dout_dir", ")", "\n", "cli", ".", "generator_args", "(", "parser", ",", "dlatent_dim", "=", "100", ")", "\n", "cli", ".", "eval_args", "(", "parser", ",", "dval_iter", "=", "1000", ")", "\n", "\n", "train_args_replay", "(", "parser", ",", "prefix", "=", "'enc_'", ",", "pf_name", "=", "'encoder'", ")", "\n", "train_args_replay", "(", "parser", ",", "show_emb_lr", "=", "True", ",", "prefix", "=", "'dec_'", ",", "\n", "pf_name", "=", "'decoder'", ")", "\n", "split_args", "(", "parser", ")", "\n", "\n", "", "elif", "mode", "==", "'perm'", ":", "\n", "        ", "dout_dir", "=", "'./out_permuted/run_'", "+", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S'", ")", "\n", "cl_argroup", "=", "cli", ".", "cl_args", "(", "parser", ",", "show_beta", "=", "False", ",", "\n", "show_from_scratch", "=", "True", ",", "show_multi_head", "=", "False", ",", "\n", "show_cl_scenario", "=", "True", ",", "show_num_tasks", "=", "True", ",", "dnum_tasks", "=", "10", ")", "\n", "train_argroup", "=", "cli", ".", "train_args", "(", "parser", ",", "show_lr", "=", "False", ",", "dbatch_size", "=", "128", ",", "\n", "dn_iter", "=", "5000", ",", "show_epochs", "=", "True", ")", "\n", "cli", ".", "main_net_args", "(", "parser", ",", "allowed_nets", "=", "[", "'fc'", "]", ",", "dfc_arch", "=", "'1000,1000'", ",", "\n", "dnet_act", "=", "'relu'", ",", "prefix", "=", "'enc_'", ",", "pf_name", "=", "'encoder'", ")", "\n", "cli", ".", "main_net_args", "(", "parser", ",", "allowed_nets", "=", "[", "'fc'", "]", ",", "dfc_arch", "=", "'1000,1000'", ",", "\n", "dnet_act", "=", "'relu'", ",", "prefix", "=", "'dec_'", ",", "pf_name", "=", "'decoder'", ")", "\n", "cli", ".", "hypernet_args", "(", "parser", ",", "dhyper_chunks", "=", "85000", ",", "dhnet_arch", "=", "'25,25'", ",", "\n", "dtemb_size", "=", "24", ",", "demb_size", "=", "8", ",", "prefix", "=", "'rp_'", ",", "\n", "pf_name", "=", "'replay'", ",", "dhnet_act", "=", "'elu'", ")", "\n", "cli", ".", "init_args", "(", "parser", ",", "custom_option", "=", "False", ")", "\n", "cli", ".", "miscellaneous_args", "(", "parser", ",", "big_data", "=", "False", ",", "synthetic_data", "=", "True", ",", "\n", "show_plots", "=", "True", ",", "no_cuda", "=", "False", ",", "dout_dir", "=", "dout_dir", ")", "\n", "cli", ".", "generator_args", "(", "parser", ",", "dlatent_dim", "=", "100", ")", "\n", "cli", ".", "eval_args", "(", "parser", ",", "dval_iter", "=", "1000", ")", "\n", "train_args_replay", "(", "parser", ",", "prefix", "=", "'enc_'", ",", "pf_name", "=", "'Encoder'", ",", "dlr", "=", "0.0001", ")", "\n", "train_args_replay", "(", "parser", ",", "show_emb_lr", "=", "True", ",", "prefix", "=", "'dec_'", ",", "dlr", "=", "0.0001", ",", "\n", "dlr_emb", "=", "0.0001", ",", "pf_name", "=", "'Decoder'", ")", "\n", "\n", "perm_args", "(", "parser", ")", "\n", "\n", "", "cl_arguments_replay", "(", "parser", ")", "\n", "cl_arguments_general", "(", "parser", ")", "\n", "data_args", "(", "parser", ")", "\n", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.parse_rp_cmd_arguments": [[110, 150], ["train_args_replay.collect_rp_cmd_arguments", "collect_rp_cmd_arguments.parse_args", "utils.check_invalid_argument_usage", "Exception", "warnings.warn", "ValueError"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.collect_rp_cmd_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.cli_args.check_invalid_argument_usage"], ["", "def", "parse_rp_cmd_arguments", "(", "mode", "=", "'split'", ",", "default", "=", "False", ",", "argv", "=", "None", ")", ":", "\n", "    ", "\"\"\"Parse command-line arguments.\n\n    Args:\n        See docstring of method collect_cmd_arguments.\n        default (optional): If True, command-line arguments will be ignored and\n            only the default values will be parsed.\n        argv (optional): If provided, it will be treated as a list of command-\n            line argument that is passed to the parser in place of sys.argv.\n    Returns:\n        The Namespace object containing argument names and values.\n    \"\"\"", "\n", "\n", "if", "mode", "==", "'split'", ":", "\n", "        ", "description", "=", "'Training replay model sequentially on splitMNIST'", "\n", "", "elif", "mode", "==", "'perm'", ":", "\n", "        ", "description", "=", "'Training replay model sequentially on permutedMNIST'", "\n", "", "else", ":", "\n", "        ", "raise", "Exception", "(", "'Mode \"%s\" unknown.'", "%", "(", "mode", ")", ")", "\n", "\n", "", "parser", "=", "collect_rp_cmd_arguments", "(", "mode", "=", "mode", ",", "description", "=", "description", ")", "\n", "\n", "args", "=", "None", "\n", "if", "argv", "is", "not", "None", ":", "\n", "        ", "if", "default", ":", "\n", "            ", "warnings", ".", "warn", "(", "'Provided \"argv\" will be ignored since \"default\" '", "+", "\n", "'option was turned on.'", ")", "\n", "", "args", "=", "argv", "\n", "", "if", "default", ":", "\n", "        ", "args", "=", "[", "]", "\n", "\n", "", "config", "=", "parser", ".", "parse_args", "(", "args", "=", "args", ")", "\n", "\n", "### Check argument values!", "\n", "cli", ".", "check_invalid_argument_usage", "(", "config", ")", "\n", "if", "mode", "==", "'split'", ":", "\n", "        ", "if", "config", ".", "num_tasks", ">", "5", ":", "\n", "            ", "raise", "ValueError", "(", "'SplitMNIST may have maximally 5 tasks.'", ")", "\n", "\n", "", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.data_args": [[151, 166], ["parser.add_argument_group", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "data_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method parse_cmd_arguments to add\n    an argument group for special options regarding the datasets.\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "### Dataset options", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Dataset options'", ")", "\n", "agroup", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "'../data/'", ",", "\n", "help", "=", "'Directory where the data is sotred.'", ")", "\n", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.perm_args": [[167, 188], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "perm_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method parse_cmd_arguments to add\n    an argument group for special options regarding the Permuted MNIST experime.\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "### Permuted MNIST Options", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Permuted MNIST Options'", ")", "\n", "agroup", ".", "add_argument", "(", "'--experiment'", ",", "type", "=", "str", ",", "default", "=", "\"permutedMNIST\"", ",", "\n", "help", "=", "'Argument specifying the dataset used.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--padding'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'Padding the images with zeros for the'", "+", "\n", "'permutation experiments. This is done to '", "+", "\n", "'relate to results from '", "+", "\n", "'arxiv.org/pdf/1809.10635.pdf. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.split_args": [[189, 210], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "split_args", "(", "parser", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method parse_cmd_arguments to add\n    an argument group for special options regarding the splitMNIST experiment.\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "### splitMNIST Options", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'splitMNIST Options'", ")", "\n", "agroup", ".", "add_argument", "(", "'--experiment'", ",", "type", "=", "str", ",", "default", "=", "\"splitMNIST\"", ",", "\n", "help", "=", "'Argument specifying the dataset used.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--padding'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Padding the images with zeros for the'", "+", "\n", "'permutation experiments. This is done to '", "+", "\n", "'relate to results from '", "+", "\n", "'arxiv.org/pdf/1809.10635.pdf. '", "+", "\n", "'Default: %(default)s.'", ")", "\n", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.train_args_replay": [[211, 259], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument"], "function", ["None"], ["", "def", "train_args_replay", "(", "parser", ",", "dlr", "=", "0.001", ",", "show_emb_lr", "=", "False", ",", "\n", "dlr_emb", "=", "0.001", ",", "prefix", "=", "None", ",", "pf_name", "=", "None", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method parse_cmd_arguments to add\n    arguments to the parser that are specific to the training of the \n    replay model.\n\n    Arguments specified in this function:\n        - `lr`\n        - `emb_lr`\n\n    Args:\n        dlr: Default learning rate for the optimizer. \n        show_emb_lr: Whether the option `lr_emb` should be provided.\n        dlr_emb: Default learning rate for the embedding parameters of a\n            hypernetwork. Provieded if show_emb_lr is set to True.\n        prefix (optional): If arguments should be instantiated with a certain\n            prefix. E.g., a setup requires several main network, that may need\n            different settings. For instance: prefix=:code:`prefix='gen_'`.\n        pf_name (optional): A name of the type of main net for which that prefix\n            is needed. For instance: prefix=:code:`'generator'`.\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "\n", "assert", "(", "prefix", "is", "None", "or", "pf_name", "is", "not", "None", ")", "\n", "\n", "heading", "=", "'Replay network training options'", "\n", "\n", "if", "prefix", "is", "None", ":", "\n", "        ", "prefix", "=", "''", "\n", "pf_name", "=", "''", "\n", "", "else", ":", "\n", "        ", "heading", "=", "'Replay network training options for %s network'", "%", "pf_name", "\n", "pf_name", "+=", "''", "\n", "\n", "# Abbreviations.", "\n", "", "p", "=", "prefix", "\n", "n", "=", "pf_name", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "heading", ")", "\n", "agroup", ".", "add_argument", "(", "'--%slr'", "%", "p", ",", "type", "=", "float", ",", "default", "=", "dlr", ",", "\n", "help", "=", "'Learning rate of optimizer(s) for %s '", "%", "n", "+", "\n", "'. Default: %(default)s.'", ")", "\n", "if", "show_emb_lr", ":", "\n", "        ", "agroup", ".", "add_argument", "(", "'--%slr_emb'", "%", "p", ",", "type", "=", "float", ",", "default", "=", "dlr_emb", ",", "\n", "help", "=", "'Learning rate of optimizer(s) for embeddings'", "+", "\n", "' of hypernetwork for %s '", "%", "n", "+", "\n", "'. Default: '", "+", "'%(default)s.'", ")", "\n", "", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.cl_arguments_general": [[261, 293], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "cl_arguments_general", "(", "parser", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method parse_cmd_arguments to add\n    arguments to the parser that are specific to the general cl setup.\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n    \"\"\"", "\n", "\n", "parser", ".", "add_argument", "(", "'--cl_reg_batch_size'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "\n", "help", "=", "'If not \"-1\", then this number will determine '", "+", "\n", "'the maximum number of previous tasks that are '", "+", "\n", "'are considered when computing the regularizer. '", "+", "\n", "'Hence, if the number of previous tasks is '", "+", "\n", "'than this number, then the regularizer will be '", "+", "\n", "'computed only over a random subset of previous '", "+", "\n", "'tasks.'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_lookahead'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Use a simplified version of our regularizer, '", "+", "\n", "'that doesn\\'t use the theta lookahead.'", ")", "\n", "parser", ".", "add_argument", "(", "'--backprop_dt'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Allow backpropagation through delta theta in '", "+", "\n", "'the regularizer.'", ")", "\n", "parser", ".", "add_argument", "(", "'--use_sgd_change'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'This argument decides how delta theta (the '", "+", "\n", "'difference of the hypernet weights when taking '", "+", "\n", "'a step in optimizing the task-specific loss) '", "+", "\n", "'is computed. Note, delta theta is needed to '", "+", "\n", "'compute the CL regularizer. If this option is '", "+", "\n", "'True, then we approximate delta theta by its '", "+", "\n", "'SGD version: - alpha * grad, where alpha '", "+", "\n", "'represents the learning rate. This version is '", "+", "\n", "'computationally cheaper.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_args_replay.cl_arguments_replay": [[294, 350], ["parser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "def", "cl_arguments_replay", "(", "parser", ")", ":", "\n", "    ", "\"\"\"This is a helper method of the method parse_cmd_arguments to add\n    arguments to the parser that are specific to the continual replay setup.\n\n    Args:\n        parser: Object of class :class:`argparse.ArgumentParser`.\n\n    Returns:\n        The created argument group, in case more options should be added.\n    \"\"\"", "\n", "### ", "\n", "agroup", "=", "parser", ".", "add_argument_group", "(", "'Replay continual learning options.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--rp_beta'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'Trade-off for the CL regularizer for the hnet '", "+", "\n", "'in the replay model.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--dont_train_rp_embeddings'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Train embeddings of discriminator hnet.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--replay_method'", ",", "type", "=", "str", ",", "default", "=", "'vae'", ",", "\n", "help", "=", "'String depicting which replay method to use.'", "+", "\n", "'Options are \"gan\" or \"vae\" which is as default.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--infer_task_id'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Train a system to infer the task id. Otherwise '", "+", "\n", "'we learn a model to replay and another model '", "+", "\n", "'to classifier. This is HNET+TIR else HNET+R.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--single_class_replay'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Weather or not we want the replay moderl to '", "+", "\n", "'learn each class in every task sequentially '", "+", "\n", "'or one task (with multiple classes) at a time. '", "+", "\n", "'Note the difference to class_incremental '", "+", "\n", "'learning where the new task consists of a '", "+", "\n", "'task.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--embedding_reset'", ",", "type", "=", "str", ",", "default", "=", "'normal'", ",", "\n", "help", "=", "'How to reset hypernet embedding after training'", "+", "\n", "' a task? Possible choices are:'", "+", "\n", "'\"normal\" - sample from a Normal Distribution'", "+", "\n", "'\"old_embedding\" - embedding from previous task'", ")", "\n", "agroup", ".", "add_argument", "(", "'--conditional_replay'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Have a task specific input to the replay model.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--conditional_dim'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'Specifies the dim of the task specific input.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--not_conditional_hot_enc'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'If conditions should be one-hot, if not they '", "+", "\n", "'are drawn from a Gaussian.'", ")", "\n", "agroup", ".", "add_argument", "(", "'--conditional_prior'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Have a task specific prior mean of the replay '", "+", "\n", "'model latent space.'", ")", "\n", "parser", ".", "add_argument", "(", "'--plot_update_steps'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "\n", "help", "=", "'How often to plot.'", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_fun'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'If we train a replay GAN, we can specifiy the '", "+", "\n", "'GAN loss. The following options are available:'", "+", "\n", "'0: Vanilla GAN (Goodfellow et al., 2014).'", "+", "\n", "'1: Traditional LSGAN (Mao et al., 2018).'", "+", "\n", "'2: Pearson Chi^2 LSGAN (Mao et al., 2018).'", "+", "\n", "'3: Wasserstein distance(Arjovsky et al., 2017).'", ")", "\n", "return", "agroup", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_gan.test": [[46, 91], ["dis.eval", "gen.eval", "print", "print", "g_hnet.eval", "torch.no_grad", "torch.no_grad", "range", "train_gan.sample", "mnist.plotting._plotImages", "writer.add_figure", "writer.add_figure", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.sample", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._plotImages"], ["def", "test", "(", "dis", ",", "gen", ",", "g_hnet", ",", "device", ",", "config", ",", "writer", ",", "train_iter", "=", "None", ",", "\n", "condition", "=", "None", ")", ":", "\n", "    ", "\"\"\" Test the MNIST GAN - here we only sample from a fixed noise to compare\n    images qualitatively. One should also keep track of the GAN loss \n    error of e.g. a test set.\n\n    Args:\n        (....): See docstring of function \n            :func:`mnist.replay.train_replay.train`.\n        train_iter: The current training iteration.\n        condition: Condition (class/task) we are currently training.\n    \"\"\"", "\n", "\n", "if", "train_iter", "is", "None", ":", "\n", "        ", "print", "(", "'### Final test run ...'", ")", "\n", "train_iter", "=", "config", ".", "n_iter", "\n", "", "else", ":", "\n", "        ", "print", "(", "'# Testing network before running training step %d ...'", "%", "train_iter", ")", "\n", "# if no condition is given, we iterate over all (trained) embeddings", "\n", "", "if", "condition", "is", "None", ":", "\n", "        ", "condition", "=", "config", ".", "num_embeddings", "-", "1", "\n", "\n", "# eval all nets", "\n", "", "dis", ".", "eval", "(", ")", "\n", "gen", ".", "eval", "(", ")", "\n", "if", "g_hnet", "is", "not", "None", ":", "\n", "        ", "g_hnet", ".", "eval", "(", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# iterate over all conditions", "\n", "        ", "for", "m", "in", "range", "(", "condition", "+", "1", ")", ":", "\n", "# Get pre training saved noise", "\n", "            ", "z", "=", "config", ".", "test_z", "[", "m", "]", "\n", "X_fake", "=", "sample", "(", "gen", ",", "g_hnet", ",", "config", ",", "m", ",", "device", ",", "z", "=", "z", ")", "\n", "X_fake", "=", "X_fake", "*", "2", "-", "1", "\n", "if", "config", ".", "show_plots", ":", "\n", "                ", "fig_real", "=", "_plotImages", "(", "X_fake", ",", "config", ")", "\n", "writer", ".", "add_figure", "(", "'test_cond_'", "+", "str", "(", "m", ")", "+", "\n", "'_sampled_after_'", "+", "str", "(", "condition", ")", ",", "fig_real", ",", "\n", "global_step", "=", "train_iter", ")", "\n", "if", "train_iter", "==", "config", ".", "n_iter", ":", "\n", "                    ", "writer", ".", "add_figure", "(", "'test_cond_final_'", "+", "str", "(", "m", ")", "+", "\n", "'_sampled_after_'", "+", "str", "(", "condition", ")", ",", "fig_real", ",", "\n", "global_step", "=", "train_iter", ")", "\n", "# TODO test GAN loss           ", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_gan.sample": [[93, 139], ["gen.forward", "torch.tanh", "torch.tanh", "torch.randn_like", "torch.randn_like", "torch.cat", "torch.cat", "g_hnet.forward", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "", "", "", "", "def", "sample", "(", "gen", ",", "g_hnet", ",", "config", ",", "condition", ",", "device", ",", "z", "=", "None", ",", "bs", "=", "None", ")", ":", "\n", "    ", "\"\"\"Sample from the generator. Given a certain condition (the task id),\n    we sample from the generator model a batch of replay data. This input of the \n    generator will be a noise vector (optional with a specific mean) and/or and\n    additional task specific input. \n\n    Args:\n        (....): See docstring of funct :func:`mnist.replay.train_replay.train`.\n        condition: Condition (class/task) we want to sample from. Not to be \n        confused with the additional option that one can input a task specific \n        condition the replay model. \n\n    Returns:\n        Batch of replay data from the generator, given a certain \n        condition / task id.\n    \"\"\"", "\n", "\n", "if", "z", "is", "None", ":", "\n", "# get the prior mean  ", "\n", "        ", "if", "config", ".", "conditional_prior", ":", "\n", "            ", "cur_prior", "=", "config", ".", "priors", "[", "condition", "]", "\n", "", "else", ":", "\n", "            ", "cur_prior", "=", "torch", ".", "zeros", "(", "(", "config", ".", "batch_size", ",", "\n", "config", ".", "latent_dim", ")", ")", ".", "to", "(", "device", ")", "\n", "\n", "# sample normal gaussian and build noise vector", "\n", "", "eps", "=", "torch", ".", "randn_like", "(", "cur_prior", ")", "\n", "z", "=", "cur_prior", "+", "eps", "\n", "\n", "# get condition if given", "\n", "", "if", "config", ".", "conditional_replay", ":", "\n", "        ", "z", "=", "torch", ".", "cat", "(", "[", "z", ",", "config", ".", "vae_conds", "[", "condition", "]", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# cut for replay when we need the X_fake from all previous tasks need to sum", "\n", "# up the given batch_size such that batch_size(X_fake) == batch_size(X_real) ", "\n", "", "if", "bs", "is", "not", "None", ":", "\n", "        ", "z", "=", "z", "[", ":", "bs", ",", ":", "]", "\n", "\n", "# get weights from hnet", "\n", "", "if", "g_hnet", "is", "not", "None", ":", "\n", "        ", "weights_d", "=", "g_hnet", ".", "forward", "(", "condition", ")", "\n", "", "else", ":", "\n", "        ", "weights_d", "=", "None", "\n", "\n", "", "samples", "=", "gen", ".", "forward", "(", "z", ",", "weights_d", ")", "\n", "return", "torch", ".", "tanh", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_gan.train_gan_one_t": [[140, 347], ["print", "dis.parameters", "torch.Adam", "torch.Adam", "range", "train_gan.test", "int", "list", "gen.parameters", "dhandler.next_train_batch", "dhandler.input_to_torch_tensor", "optim.Adam.zero_grad", "optim.Adam.zero_grad", "train_gan.sample", "dis.forward", "dis.forward", "utils.gan_helpers.dis_loss", "gan_helpers.dis_loss.backward", "optim.Adam.step", "optim.Adam.zero_grad", "optim.Adam.zero_grad", "train_gan.sample", "dis.forward", "utils.gan_helpers.gen_loss", "gan_helpers.gen_loss.backward", "optim.Adam.step", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "int", "[].detach().clone", "utils.get_current_targets", "torch.Adam", "train_gan.test", "gen.train", "dis.train", "print", "optim.Adam.zero_grad", "optim.Adam.zero_grad", "gloss_reg.backward", "optim.Adam.step", "mnist.plotting._viz_training", "utils.gan_helpers.accuracy", "writer.add_scalar", "writer.add_scalar", "numpy.ceil", "g_hnet.train", "utils.calc_delta_theta", "utils.calc_fix_target_reg", "numpy.asarray", "numpy.asarray", "[].detach", "g_hnet.get_task_embs", "g_hnet.get_task_emb", "g_embedding_history.append", "g_embedding_history.append", "g_hnet.get_task_emb().clone().detach().cpu().numpy", "g_hnet.get_task_emb().clone().detach().numpy", "str", "g_hnet.get_task_embs", "g_hnet.get_task_emb().clone().detach().cpu", "g_hnet.get_task_emb().clone().detach", "g_hnet.get_task_emb().clone().detach", "g_hnet.get_task_emb().clone", "g_hnet.get_task_emb().clone", "g_hnet.get_task_emb", "g_hnet.get_task_emb"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.sample", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.gan_helpers.dis_loss", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.sample", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.gan_helpers.gen_loss", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.get_current_targets", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._viz_training", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.accuracy", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.calc_delta_theta", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_fix_target_reg", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb"], ["", "def", "train_gan_one_t", "(", "dhandler", ",", "dis", ",", "gen", ",", "g_hnet", ",", "device", ",", "config", ",", "writer", ",", "\n", "embd_list", ",", "t", ")", ":", "\n", "    ", "\"\"\" Train the conditional MNIST GAN for one task.\n    In this function the main training logic for this replay model is \n    implemented. After setting the optimizers for the discriminator/generator \n    and it's hypernetwork if applicable, a standart variational autoencoder \n    training scheme is implemented. To prevent the generator (its hypernetwork) \n    from forgetting, we add our hypernetwork regularisation term for all tasks \n    seen before ``t`` to the vae loss. \n\n    Args:\n        (....): See docstring of function \n            :func:`mnist.replay.train_replay.train`.\n        embd_list: Helper list of lists for embedding plotting.\n        t: Task id to train.\n    \"\"\"", "\n", "\n", "print", "(", "\"Training GAN on data handler: \"", ",", "t", ")", "\n", "\n", "# get lists for plotting embeddings", "\n", "d_embeddings", ",", "g_embeddings", ",", "d_embedding_history", ",", "g_embedding_history", "=", "embd_list", "[", ":", "]", "\n", "# set training_iterations if epochs are set", "\n", "if", "config", ".", "epochs", "==", "-", "1", ":", "\n", "        ", "training_iterations", "=", "config", ".", "n_iter", "\n", "", "else", ":", "\n", "        ", "assert", "(", "config", ".", "epochs", ">", "0", ")", "\n", "training_iterations", "=", "config", ".", "epochs", "*", "int", "(", "np", ".", "ceil", "(", "dhandler", ".", "num_train_samples", "/", "config", ".", "batch_size", ")", ")", "\n", "\n", "# Here we adjust the number of training iterations when we train our replay ", "\n", "# method to replay every single class in a task given that condition. ", "\n", "# We need to adjust the training iterations such that we train every ", "\n", "# class in the task only a portion of the time we are given for the ", "\n", "# whole task:", "\n", "# Training_time_per_class = training_time_per_task / num_class_per_task", "\n", "# This is important to compare to related work, as they set the training ", "\n", "# time per task which we now have to split up.", "\n", "\n", "", "if", "config", ".", "single_class_replay", ":", "\n", "        ", "training_iterations", "=", "int", "(", "training_iterations", "/", "config", ".", "out_dim", ")", "\n", "\n", "# if we want to start training the new task with the weights of the previous", "\n", "# task we have to set the start embedding for the new task to the embedding", "\n", "# of the previous task. ", "\n", "", "if", "config", ".", "embedding_reset", "==", "\"old_embedding\"", "and", "t", ">", "0", ":", "\n", "        ", "if", "g_hnet", "is", "not", "None", ":", "\n", "            ", "last_emb", "=", "g_hnet", ".", "get_task_embs", "(", ")", "[", "t", "-", "1", "]", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "g_hnet", ".", "get_task_embs", "(", ")", "[", "t", "]", ".", "data", "=", "last_emb", "\n", "\n", "# Compute targets for the hnet before training. ", "\n", "", "", "if", "t", ">", "0", ":", "\n", "        ", "if", "config", ".", "rp_beta", ">", "0", "and", "g_hnet", "is", "not", "None", ":", "\n", "            ", "targets_G", "=", "hreg", ".", "get_current_targets", "(", "t", ",", "g_hnet", ")", "\n", "", "else", ":", "\n", "            ", "targets_G", "=", "None", "\n", "\n", "############", "\n", "# OPTIMIZERS ", "\n", "############", "\n", "\n", "# discriminator optimizer", "\n", "", "", "dis_paras", "=", "dis", ".", "parameters", "(", ")", "\n", "doptimizer", "=", "optim", ".", "Adam", "(", "dis_paras", ",", "lr", "=", "config", ".", "enc_lr", ",", "\n", "betas", "=", "(", "0.9", ",", "0.999", ")", ")", "\n", "\n", "# discriminator optimizer (hnet or weights directly)", "\n", "if", "g_hnet", "is", "not", "None", ":", "\n", "        ", "g_paras", "=", "list", "(", "g_hnet", ".", "theta", ")", "\n", "if", "not", "config", ".", "dont_train_rp_embeddings", ":", "\n", "# Set the embedding optimizer only for the current task embedding.", "\n", "# Note that we could here continue training the old embeddings.", "\n", "            ", "g_emb_optimizer", "=", "optim", ".", "Adam", "(", "[", "g_hnet", ".", "get_task_emb", "(", "t", ")", "]", ",", "\n", "lr", "=", "config", ".", "dec_lr_emb", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ")", "\n", "", "else", ":", "\n", "            ", "g_emb_optimizer", "=", "None", "\n", "", "", "else", ":", "\n", "        ", "g_emb_optimizer", "=", "None", "\n", "g_paras", "=", "gen", ".", "parameters", "(", ")", "\n", "\n", "", "goptimizer", "=", "optim", ".", "Adam", "(", "g_paras", ",", "lr", "=", "config", ".", "dec_lr", ",", "\n", "betas", "=", "(", "0.9", ",", "0.999", ")", ")", "\n", "\n", "calc_reg", "=", "config", ".", "rp_beta", ">", "0", "and", "t", ">", "0", "and", "g_hnet", "is", "not", "None", "\n", "\n", "for", "i", "in", "range", "(", "training_iterations", ")", ":", "\n", "### Test network.", "\n", "# We test the network before we run the training iteration.", "\n", "# That way, we can see the initial performance of the untrained net.", "\n", "        ", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "test", "(", "dis", ",", "gen", ",", "g_hnet", ",", "device", ",", "config", ",", "writer", ",", "i", ",", "t", ")", "\n", "gen", ".", "train", "(", ")", "\n", "dis", ".", "train", "(", ")", "\n", "if", "g_hnet", "is", "not", "None", ":", "\n", "                ", "g_hnet", ".", "train", "(", ")", "\n", "\n", "", "", "if", "i", "%", "100", "==", "0", ":", "\n", "            ", "print", "(", "'Training iteration: %d.'", "%", "i", ")", "\n", "\n", "", "if", "config", ".", "show_plots", ":", "\n", "            ", "if", "g_hnet", "is", "not", "None", ":", "\n", "                ", "if", "(", "not", "config", ".", "no_cuda", ")", ":", "\n", "                    ", "g_embedding_history", ".", "append", "(", "g_hnet", ".", "get_task_emb", "(", "t", ")", ".", "\n", "clone", "(", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "g_embedding_history", ".", "append", "(", "g_hnet", ".", "get_task_emb", "(", "t", ")", ".", "\n", "clone", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "#######", "\n", "# DATA ", "\n", "#######", "\n", "", "", "", "real_batch", "=", "dhandler", ".", "next_train_batch", "(", "config", ".", "batch_size", ")", "\n", "X_real", "=", "dhandler", ".", "input_to_torch_tensor", "(", "real_batch", "[", "0", "]", ",", "\n", "device", ",", "mode", "=", "'train'", ")", "\n", "#shift data in range [-1, 1] so we can tanh the output of G", "\n", "X_real", "=", "X_real", "*", "2", "-", "1.0", "\n", "\n", "######################", "\n", "# TRAIN DISCRIMINATOR", "\n", "######################", "\n", "\n", "# set gradients again to zero", "\n", "doptimizer", ".", "zero_grad", "(", ")", "\n", "goptimizer", ".", "zero_grad", "(", ")", "\n", "if", "g_emb_optimizer", "is", "not", "None", ":", "\n", "            ", "g_emb_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "# Note that X_fake is not normalize between 0 and 1", "\n", "# but like in in https://github.com/Zackory/Kera", "\n", "# s-MNIST-GAN/blob/master/mnist_gan.py", "\n", "# inputs are shiftet between [-1, 1] and X_fake is put through tanh", "\n", "#X_fake = torch.tanh(X_fake)                        ", "\n", "", "X_fake", "=", "sample", "(", "gen", ",", "g_hnet", ",", "config", ",", "t", ",", "device", ")", "\n", "\n", "fake", "=", "dis", ".", "forward", "(", "X_fake", ")", "\n", "real", "=", "dis", ".", "forward", "(", "X_real", ")", "\n", "\n", "# compute discriminator loss", "\n", "dloss", "=", "gan_helpers", ".", "dis_loss", "(", "real", ",", "fake", ",", "config", ".", "loss_fun", ")", "\n", "\n", "# compute gradients for discriminator and take gradient step", "\n", "dloss", ".", "backward", "(", ")", "\n", "doptimizer", ".", "step", "(", ")", "\n", "\n", "######################", "\n", "# TRAIN GENERATOR", "\n", "######################", "\n", "\n", "# set gradients again to zero", "\n", "goptimizer", ".", "zero_grad", "(", ")", "\n", "doptimizer", ".", "zero_grad", "(", ")", "\n", "if", "g_emb_optimizer", "is", "not", "None", ":", "\n", "            ", "g_emb_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "X_fake", "=", "sample", "(", "gen", ",", "g_hnet", ",", "config", ",", "t", ",", "device", ")", "\n", "fake", "=", "dis", ".", "forward", "(", "X_fake", ")", "\n", "\n", "# compute generator loss", "\n", "gloss", "=", "gan_helpers", ".", "gen_loss", "(", "fake", ",", "config", ".", "loss_fun", ")", "\n", "\n", "gloss", ".", "backward", "(", "retain_graph", "=", "calc_reg", ",", "create_graph", "=", "calc_reg", "and", "config", ".", "backprop_dt", ")", "\n", "\n", "# compute hypernet reg loss and fix embedding->change current embs", "\n", "if", "calc_reg", ":", "\n", "            ", "if", "config", ".", "no_lookahead", ":", "\n", "                ", "dTheta", "=", "None", "\n", "", "else", ":", "\n", "                ", "dTheta", "=", "opstep", ".", "calc_delta_theta", "(", "goptimizer", ",", "\n", "config", ".", "use_sgd_change", ",", "lr", "=", "config", ".", "dec_lr", ",", "\n", "detach_dt", "=", "not", "config", ".", "backprop_dt", ")", "\n", "\n", "", "gloss_reg", "=", "config", ".", "rp_beta", "*", "hreg", ".", "calc_fix_target_reg", "(", "g_hnet", ",", "t", ",", "\n", "targets", "=", "targets_G", ",", "mnet", "=", "gen", ",", "dTheta", "=", "dTheta", ",", "dTembs", "=", "None", ")", "\n", "gloss_reg", ".", "backward", "(", ")", "\n", "", "else", ":", "\n", "            ", "gloss_reg", "=", "0", "\n", "\n", "# compute gradients for generator and take gradient step", "\n", "", "goptimizer", ".", "step", "(", ")", "\n", "if", "g_hnet", "is", "not", "None", "and", "not", "config", ".", "dont_train_rp_embeddings", ":", "\n", "            ", "g_emb_optimizer", ".", "step", "(", ")", "\n", "\n", "# Visualization of current progress in tensorboard", "\n", "", "if", "i", "%", "config", ".", "plot_update_steps", "==", "0", "and", "i", ">", "0", "and", "config", ".", "show_plots", ":", "\n", "            ", "if", "d_embedding_history", "is", "not", "None", ":", "\n", "                ", "d_embedding_cut", "=", "np", ".", "asarray", "(", "d_embedding_history", "[", "2", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "d_embedding_cut", "=", "None", "\n", "", "if", "g_embedding_history", "is", "not", "None", ":", "\n", "                ", "g_embedding_cut", "=", "np", ".", "asarray", "(", "g_embedding_history", "[", "2", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "g_embedding_cut", "=", "None", "\n", "", "_viz_training", "(", "X_real", ",", "X_fake", ",", "g_embeddings", ",", "d_embeddings", ",", "\n", "g_embedding_cut", ",", "d_embedding_cut", ",", "\n", "writer", ",", "i", ",", "config", ",", "title", "=", "\"train_cond_\"", "+", "str", "(", "t", ")", ")", "\n", "\n", "# track some training statistics", "\n", "", "writer", ".", "add_scalar", "(", "'train/gen_loss_%d'", "%", "(", "t", ")", ",", "gloss", "+", "gloss_reg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/dloss_all_%d'", "%", "(", "t", ")", ",", "dloss", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/dis_accuracy_%d'", "%", "(", "t", ")", ",", "\n", "gan_helpers", ".", "accuracy", "(", "real", ",", "fake", ",", "config", ".", "loss_fun", ")", ",", "i", ")", "\n", "if", "config", ".", "rp_beta", ">", "0", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'train/g_hnet_loss_reg_%d'", "%", "(", "t", ")", ",", "gloss_reg", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/g_loss_only_%d'", "%", "(", "t", ")", ",", "gloss", ",", "i", ")", "\n", "\n", "", "", "test", "(", "dis", ",", "gen", ",", "g_hnet", ",", "device", ",", "config", ",", "writer", ",", "config", ".", "n_iter", ",", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.test": [[51, 92], ["enc.eval", "dec.eval", "print", "print", "d_hnet.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "range", "train_replay.sample", "mnist.plotting._plotImages", "writer.add_figure", "writer.add_figure", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.sample", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._plotImages"], ["def", "test", "(", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "config", ",", "writer", ",", "train_iter", "=", "None", ",", "\n", "condition", "=", "None", ")", ":", "\n", "    ", "\"\"\" Test the MNIST VAE - here we only sample from a fixed noise to compare\n    images qualitatively. One should also keep track of the reconstruction \n    error of e.g. a test set.\n\n    Args:\n        (....): See docstring of function :func:`train`.\n        train_iter: The current training iteration.\n        condition: Condition (class/task) we are currently training.\n    \"\"\"", "\n", "if", "train_iter", "is", "None", ":", "\n", "        ", "print", "(", "'### Final test run ...'", ")", "\n", "train_iter", "=", "config", ".", "n_iter", "\n", "", "else", ":", "\n", "        ", "print", "(", "'# Testing network before running training step %d ...'", "%", "train_iter", ")", "\n", "# if no condition is given, we iterate over all (trained) embeddings", "\n", "", "if", "condition", "is", "None", ":", "\n", "        ", "condition", "=", "config", ".", "num_embeddings", "-", "1", "\n", "# eval all nets", "\n", "", "enc", ".", "eval", "(", ")", "\n", "dec", ".", "eval", "(", ")", "\n", "if", "d_hnet", "is", "not", "None", ":", "\n", "        ", "d_hnet", ".", "eval", "(", ")", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# iterate over all conditions", "\n", "        ", "for", "m", "in", "range", "(", "condition", "+", "1", ")", ":", "\n", "# Get pre training saved noise", "\n", "            ", "z", "=", "config", ".", "test_z", "[", "m", "]", "\n", "reconstructions", "=", "sample", "(", "dec", ",", "d_hnet", ",", "config", ",", "m", ",", "device", ",", "z", "=", "z", ")", "\n", "if", "config", ".", "show_plots", ":", "\n", "                ", "fig_real", "=", "_plotImages", "(", "reconstructions", ",", "config", ")", "\n", "writer", ".", "add_figure", "(", "'test_cond_'", "+", "str", "(", "m", ")", "+", "\n", "'_sampled_after_'", "+", "str", "(", "condition", ")", ",", "fig_real", ",", "\n", "global_step", "=", "train_iter", ")", "\n", "if", "train_iter", "==", "config", ".", "n_iter", ":", "\n", "                    ", "writer", ".", "add_figure", "(", "'test_cond_final_'", "+", "str", "(", "m", ")", "+", "\n", "'_sampled_after_'", "+", "str", "(", "condition", ")", ",", "fig_real", ",", "\n", "global_step", "=", "train_iter", ")", "\n", "# TODO write test reconstrunction error           ", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.sample": [[94, 140], ["dec.forward", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.cat", "torch.cat", "torch.cat", "d_hnet.forward", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "", "", "", "", "def", "sample", "(", "dec", ",", "d_hnet", ",", "config", ",", "condition", ",", "device", ",", "z", "=", "None", ",", "bs", "=", "None", ")", ":", "\n", "    ", "\"\"\"Sample from the decoder. Given a certain condition (the task id),\n    we sample from the decoder model a batch of replay data. This input of the \n    decoder will be a noise vector (optional with a specific mean) and/or and\n    additional task specific input. \n\n    Args:\n        (....): See docstring of function :func:`train`.\n        condition: Condition (class/task) we want to sample from. Not to be \n        confused with the additional option that one can input a task specific \n        condition the replay model. \n\n    Returns:\n        Batch of replay data from the decoder, given a certain \n        condition / task id.\n    \"\"\"", "\n", "\n", "if", "z", "is", "None", ":", "\n", "# get the prior mean  ", "\n", "        ", "if", "config", ".", "conditional_prior", ":", "\n", "            ", "cur_prior", "=", "config", ".", "priors", "[", "condition", "]", "\n", "", "else", ":", "\n", "            ", "cur_prior", "=", "torch", ".", "zeros", "(", "(", "config", ".", "batch_size", ",", "\n", "config", ".", "latent_dim", ")", ")", ".", "to", "(", "device", ")", "\n", "\n", "# sample normal gaussian and build noise vector", "\n", "", "eps", "=", "torch", ".", "randn_like", "(", "cur_prior", ")", "\n", "z", "=", "cur_prior", "+", "eps", "\n", "\n", "# get condition if given", "\n", "", "if", "config", ".", "conditional_replay", ":", "\n", "        ", "z", "=", "torch", ".", "cat", "(", "[", "z", ",", "config", ".", "vae_conds", "[", "condition", "]", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# cut for replay when we need the X_fake from all previous tasks need to sum", "\n", "# up the given batch_size such that batch_size(X_fake) == batch_size(X_real) ", "\n", "", "if", "bs", "is", "not", "None", ":", "\n", "        ", "z", "=", "z", "[", ":", "bs", ",", ":", "]", "\n", "\n", "# get weights from hnet", "\n", "", "if", "d_hnet", "is", "not", "None", ":", "\n", "        ", "weights_d", "=", "d_hnet", ".", "forward", "(", "condition", ")", "\n", "", "else", ":", "\n", "        ", "weights_d", "=", "None", "\n", "", "samples", "=", "dec", ".", "forward", "(", "z", ",", "weights_d", ")", "\n", "\n", "return", "torch", ".", "sigmoid", "(", "samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.init_plotting_embedding": [[141, 167], ["mnist.plotting._viz_init", "dec_embs_history.append", "dec_embs_history.append", "d_hnet.get_task_emb().cpu().detach().numpy", "d_hnet.get_task_emb().detach().numpy", "d_hnet.get_task_emb().cpu().detach", "d_hnet.get_task_emb().detach", "d_hnet.get_task_emb().cpu", "d_hnet.get_task_emb", "d_hnet.get_task_emb"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._viz_init", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb"], ["", "def", "init_plotting_embedding", "(", "dhandlers", ",", "d_hnet", ",", "writer", ",", "config", ")", ":", "\n", "    ", "\"\"\" This is a helper function to get lists to plot embedding histories.\n\n    Args:\n        (....): See docstring of function :func:`train`.\n    Returns:\n        List of lists for embedding plots during training.\n    \"\"\"", "\n", "\n", "# initial visualization and setting up training viz", "\n", "if", "config", ".", "show_plots", ":", "\n", "        ", "_", ",", "dec_embs", "=", "_viz_init", "(", "dhandlers", ",", "None", ",", "d_hnet", ",", "writer", ",", "config", ")", "\n", "if", "d_hnet", "is", "not", "None", ":", "\n", "            ", "dec_embs_history", "=", "[", "]", "\n", "if", "(", "not", "config", ".", "no_cuda", ")", ":", "\n", "                ", "dec_embs_history", ".", "append", "(", "d_hnet", ".", "get_task_emb", "(", "0", ")", ".", "\n", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "dec_embs_history", ".", "append", "(", "d_hnet", ".", "get_task_emb", "(", "0", ")", ".", "\n", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "dec_embs_history", "=", "None", "\n", "\n", "", "return", "[", "None", ",", "dec_embs", ",", "None", ",", "dec_embs_history", "]", "\n", "", "else", ":", "\n", "        ", "return", "[", "None", ",", "None", ",", "None", ",", "None", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.reparameterize": [[168, 184], ["torch.exp", "torch.exp", "torch.exp", "torch.randn_like", "torch.randn_like", "torch.randn_like"], "function", ["None"], ["", "", "def", "reparameterize", "(", "mu", ",", "logvar", ")", ":", "\n", "    ", "\"\"\"Reparameterize encoder output for vae loss. Code from\n        https://github.com/pytorch/examples/blob/master/vae/main.py#L48\n\n    Args:\n        mu: Output of encoder parameterising the mean of the Gaussian.\n        logvar: Output of the encoder that get transformed into the\n            variance to be used for the reparameterization trick below.\n        eps: Use epsilon already drawn to reduce variance\n\n    Returns:\n        Sample from the Gaussian through the reparameterization trick.\n    \"\"\"", "\n", "std", "=", "torch", ".", "exp", "(", "0.5", "*", "logvar", ")", "\n", "eps", "=", "torch", ".", "randn_like", "(", "std", ")", "\n", "return", "mu", "+", "eps", "*", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.compute_kld": [[185, 214], ["torch.sum", "torch.sum", "torch.sum", "torch.mean", "torch.mean", "torch.mean", "logvar.exp"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["", "def", "compute_kld", "(", "mu", ",", "logvar", ",", "config", ",", "t", ")", ":", "\n", "    ", "\"\"\"Compute the kullback-leibler divergence between normal gaussian around\n    zero or mu_prior and a gaussian with parameters mu, logvar.\n\n    Args:\n        mu: Outputs of the encoder, mean of the VAE latent Gaussian.\n        logvar: Outputs of the encoder, logvar of the VAE latent Gaussian.\n        config: Command-line arguments.\n        t: task id.            \n    Returns:\n        LKD between gausian with parameters by encoder and prior.\n    \"\"\"", "\n", "\n", "# see Appendix B from VAE paper:", "\n", "# Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014", "\n", "# https://arxiv.org/abs/1312.6114", "\n", "# 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)", "\n", "# add prior matching loss", "\n", "if", "config", ".", "conditional_prior", ":", "\n", "        ", "cur_prior", "=", "config", ".", "priors", "[", "t", "]", "\n", "", "else", ":", "\n", "        ", "cur_prior", "=", "0", "\n", "", "kld", "=", "-", "0.5", "*", "torch", ".", "sum", "(", "1", "+", "logvar", "-", "(", "mu", "-", "cur_prior", ")", ".", "pow", "(", "2", ")", "-", "logvar", ".", "exp", "(", ")", ",", "dim", "=", "1", ")", "\n", "# average kl by input dim (to compare to related work, see", "\n", "# https://github.com/GMvandeVen/continual-learning/blob/master/train.py)", "\n", "\n", "kld", "=", "torch", ".", "mean", "(", "kld", ")", "/", "config", ".", "input_dim", "\n", "return", "kld", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train_vae_one_t": [[215, 423], ["enc.train", "dec.train", "print", "enc.parameters", "torch.Adam", "torch.Adam", "range", "train_replay.test", "d_hnet.train", "int", "list", "dec.parameters", "dhandler.next_train_batch", "dhandler.input_to_torch_tensor", "optim.Adam.zero_grad", "optim.Adam.zero_grad", "enc.forward", "train_replay.compute_kld", "train_replay.reparameterize", "train_replay.sample", "torch.binary_cross_entropy", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "loss.backward", "optim.Adam.step", "optim.Adam.step", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "int", "[].detach().clone", "utils.get_current_targets", "torch.Adam", "train_replay.test", "enc.train", "dec.train", "print", "optim.Adam.zero_grad", "dloss_reg.backward", "optim.Adam.step", "mnist.plotting._viz_training", "writer.add_scalar", "numpy.ceil", "d_hnet.train", "utils.calc_delta_theta", "utils.calc_fix_target_reg", "numpy.asarray", "numpy.asarray", "[].detach", "d_hnet.get_task_embs", "d_hnet.get_task_emb", "dec_embs_history.append", "dec_embs_history.append", "d_hnet.get_task_emb().clone().detach().cpu().numpy", "d_hnet.get_task_emb().clone().detach().numpy", "str", "d_hnet.get_task_embs", "d_hnet.get_task_emb().clone().detach().cpu", "d_hnet.get_task_emb().clone().detach", "d_hnet.get_task_emb().clone().detach", "d_hnet.get_task_emb().clone", "d_hnet.get_task_emb().clone", "d_hnet.get_task_emb", "d_hnet.get_task_emb"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.compute_kld", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.reparameterize", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.sample", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.get_current_targets", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.plotting._viz_training", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.calc_delta_theta", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_fix_target_reg", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb"], ["", "def", "train_vae_one_t", "(", "dhandler", ",", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "config", ",", "writer", ",", "\n", "embd_list", ",", "t", ")", ":", "\n", "    ", "\"\"\" Train the conditional MNIST VAE for one task.\n    In this function the main training logic for this replay model is \n    implemented. After setting the optimizers for the encoder/decoder and it's\n    hypernetwork if applicable, a standart variational autoencoder training\n    scheme is implemented. To prevent the decoder (its hypernetwork) from \n    forgetting, we add our hypernetwork regularisation term for all tasks \n    seen before ``t`` to the vae loss. \n\n    Args:\n        (....): See docstring of function :func:`train`.\n        embd_list: Helper list of lists for embedding plotting.\n        t: Task id that will be trained.\n\n    \"\"\"", "\n", "\n", "# set to training mode ", "\n", "enc", ".", "train", "(", ")", "\n", "dec", ".", "train", "(", ")", "\n", "if", "d_hnet", "is", "not", "None", ":", "\n", "        ", "d_hnet", ".", "train", "(", ")", "\n", "\n", "# reset data handler", "\n", "", "print", "(", "\"Training VAE on data handler: \"", ",", "t", ")", "\n", "\n", "# get lists for plotting embeddings", "\n", "enc_embs", ",", "dec_embs", ",", "enc_embs_history", ",", "dec_embs_history", "=", "embd_list", "[", ":", "]", "\n", "# set training_iterations if epochs are set", "\n", "if", "config", ".", "epochs", "==", "-", "1", ":", "\n", "        ", "training_iterations", "=", "config", ".", "n_iter", "\n", "", "else", ":", "\n", "        ", "assert", "(", "config", ".", "epochs", ">", "0", ")", "\n", "training_iterations", "=", "config", ".", "epochs", "*", "int", "(", "np", ".", "ceil", "(", "dhandler", ".", "num_train_samples", "/", "config", ".", "batch_size", ")", ")", "\n", "\n", "# Here we adjust the number of training iterations when we train our replay ", "\n", "# method to replay every single class in a task given that condition. ", "\n", "# We need to adjust the training iterations such that we train every ", "\n", "# class in the task only a portion of the time we are given for the ", "\n", "# whole task:", "\n", "# Training_time_per_class = training_time_per_task / num_class_per_task", "\n", "# This is important to compare to related work, as they set the training ", "\n", "# time per task which we now have to split up.", "\n", "\n", "", "if", "config", ".", "single_class_replay", ":", "\n", "        ", "training_iterations", "=", "int", "(", "training_iterations", "/", "config", ".", "out_dim", ")", "\n", "\n", "# if we want to start training the new task with the weights of the previous", "\n", "# task we have to set the start embedding for the new task to the embedding", "\n", "# of the previous task. ", "\n", "", "if", "config", ".", "embedding_reset", "==", "\"old_embedding\"", "and", "t", ">", "0", ":", "\n", "        ", "if", "d_hnet", "is", "not", "None", ":", "\n", "            ", "last_emb", "=", "d_hnet", ".", "get_task_embs", "(", ")", "[", "t", "-", "1", "]", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "d_hnet", ".", "get_task_embs", "(", ")", "[", "t", "]", ".", "data", "=", "last_emb", "\n", "\n", "# Compute targets for the hnet before training. ", "\n", "", "", "if", "t", ">", "0", ":", "\n", "        ", "if", "config", ".", "rp_beta", ">", "0", "and", "d_hnet", "is", "not", "None", ":", "\n", "            ", "targets_D", "=", "hreg", ".", "get_current_targets", "(", "t", ",", "d_hnet", ")", "\n", "", "else", ":", "\n", "            ", "targets_D", "=", "None", "\n", "\n", "############", "\n", "# OPTIMIZERS ", "\n", "############", "\n", "\n", "# encoder optimizer", "\n", "", "", "e_paras", "=", "enc", ".", "parameters", "(", ")", "\n", "eoptimizer", "=", "optim", ".", "Adam", "(", "e_paras", ",", "lr", "=", "config", ".", "enc_lr", ",", "\n", "betas", "=", "(", "0.9", ",", "0.999", ")", ")", "\n", "\n", "# decoder optimizer (hnet or weights directly)", "\n", "if", "d_hnet", "is", "not", "None", ":", "\n", "        ", "d_paras", "=", "list", "(", "d_hnet", ".", "theta", ")", "\n", "if", "not", "config", ".", "dont_train_rp_embeddings", ":", "\n", "# Set the embedding optimizer only for the current task embedding.", "\n", "# Note that we could here continue training the old embeddings.", "\n", "            ", "d_emb_optimizer", "=", "optim", ".", "Adam", "(", "[", "d_hnet", ".", "get_task_emb", "(", "t", ")", "]", ",", "\n", "lr", "=", "config", ".", "dec_lr_emb", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ")", "\n", "", "else", ":", "\n", "            ", "d_emb_optimizer", "=", "None", "\n", "", "", "else", ":", "\n", "        ", "d_emb_optimizer", "=", "None", "\n", "d_paras", "=", "dec", ".", "parameters", "(", ")", "\n", "\n", "", "doptimizer", "=", "optim", ".", "Adam", "(", "d_paras", ",", "lr", "=", "config", ".", "dec_lr", ",", "\n", "betas", "=", "(", "0.9", ",", "0.999", ")", ")", "\n", "\n", "calc_reg", "=", "config", ".", "rp_beta", ">", "0", "and", "t", ">", "0", "and", "d_hnet", "is", "not", "None", "\n", "\n", "###########", "\n", "# TRAINING ", "\n", "###########", "\n", "\n", "for", "i", "in", "range", "(", "training_iterations", ")", ":", "\n", "### Test network.", "\n", "# We test the network before we run the training iteration.", "\n", "# That way, we can see the initial performance of the untrained net.", "\n", "        ", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "test", "(", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "config", ",", "writer", ",", "i", ",", "t", ")", "\n", "enc", ".", "train", "(", ")", "\n", "dec", ".", "train", "(", ")", "\n", "if", "d_hnet", "is", "not", "None", ":", "\n", "                ", "d_hnet", ".", "train", "(", ")", "\n", "\n", "", "", "if", "i", "%", "100", "==", "0", ":", "\n", "            ", "print", "(", "'Training iteration: %d.'", "%", "i", ")", "\n", "\n", "# Some code for plotting. ", "\n", "# We want to visualize the hnet embedding trajectories. ", "\n", "", "if", "config", ".", "show_plots", ":", "\n", "            ", "if", "d_hnet", "is", "not", "None", ":", "\n", "                ", "if", "(", "not", "config", ".", "no_cuda", ")", ":", "\n", "                    ", "dec_embs_history", ".", "append", "(", "d_hnet", ".", "get_task_emb", "(", "t", ")", ".", "\n", "clone", "(", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "else", ":", "\n", "                    ", "dec_embs_history", ".", "append", "(", "d_hnet", ".", "get_task_emb", "(", "t", ")", ".", "\n", "clone", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "#######", "\n", "# DATA ", "\n", "#######", "\n", "", "", "", "real_batch", "=", "dhandler", ".", "next_train_batch", "(", "config", ".", "batch_size", ")", "\n", "X_real", "=", "dhandler", ".", "input_to_torch_tensor", "(", "real_batch", "[", "0", "]", ",", "device", ",", "\n", "mode", "=", "'train'", ")", "\n", "\n", "# set gradients again to zero", "\n", "eoptimizer", ".", "zero_grad", "(", ")", "\n", "doptimizer", ".", "zero_grad", "(", ")", "\n", "if", "d_emb_optimizer", "is", "not", "None", ":", "\n", "            ", "d_emb_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "############################", "\n", "# KLD + RECONSTRUCTION ", "\n", "############################", "\n", "\n", "# feed data through encoder", "\n", "", "mu_var", "=", "enc", ".", "forward", "(", "X_real", ")", "\n", "mu", "=", "mu_var", "[", ":", ",", "0", ":", "config", ".", "latent_dim", "]", "\n", "logvar", "=", "mu_var", "[", ":", ",", "config", ".", "latent_dim", ":", "2", "*", "config", ".", "latent_dim", "]", "\n", "\n", "# compute KLD", "\n", "kld", "=", "compute_kld", "(", "mu", ",", "logvar", ",", "config", ",", "t", ")", "\n", "\n", "# sample from encoder gaussian distribution", "\n", "dec_input", "=", "reparameterize", "(", "mu", ",", "logvar", ")", "\n", "reconstructions", "=", "sample", "(", "dec", ",", "d_hnet", ",", "config", ",", "t", ",", "device", ",", "z", "=", "dec_input", ")", "\n", "# average reconstruction error like this to compare to related work, see", "\n", "# https://github.com/GMvandeVen/continual-learning/blob/master/train.py", "\n", "\n", "x_rec_loss", "=", "F", ".", "binary_cross_entropy", "(", "reconstructions", ",", "\n", "X_real", ",", "reduction", "=", "'none'", ")", "\n", "x_rec_loss", "=", "torch", ".", "mean", "(", "x_rec_loss", ",", "dim", "=", "1", ")", "\n", "x_rec_loss", "=", "torch", ".", "mean", "(", "x_rec_loss", ")", "\n", "\n", "loss", "=", "x_rec_loss", "+", "kld", "\n", "\n", "######################################################", "\n", "# HYPERNET REGULARISATION - CONTINUAL LEARNING METHOD", "\n", "######################################################", "\n", "\n", "loss", ".", "backward", "(", "retain_graph", "=", "calc_reg", ",", "create_graph", "=", "calc_reg", "and", "config", ".", "backprop_dt", ")", "\n", "\n", "# compute hypernet loss and fix embedding -> change current embs", "\n", "if", "calc_reg", ":", "\n", "            ", "if", "config", ".", "no_lookahead", ":", "\n", "                ", "dTheta", "=", "None", "\n", "", "else", ":", "\n", "                ", "dTheta", "=", "opstep", ".", "calc_delta_theta", "(", "doptimizer", ",", "\n", "config", ".", "use_sgd_change", ",", "lr", "=", "config", ".", "dec_lr", ",", "\n", "detach_dt", "=", "not", "config", ".", "backprop_dt", ")", "\n", "", "dloss_reg", "=", "config", ".", "rp_beta", "*", "hreg", ".", "calc_fix_target_reg", "(", "d_hnet", ",", "t", ",", "\n", "targets", "=", "targets_D", ",", "\n", "mnet", "=", "dec", ",", "dTheta", "=", "dTheta", ",", "dTembs", "=", "None", ")", "\n", "dloss_reg", ".", "backward", "(", ")", "\n", "", "else", ":", "\n", "            ", "dloss_reg", "=", "0", "\n", "\n", "# compute gradients for generator and take gradient step", "\n", "", "doptimizer", ".", "step", "(", ")", "\n", "eoptimizer", ".", "step", "(", ")", "\n", "if", "d_hnet", "is", "not", "None", "and", "not", "config", ".", "dont_train_rp_embeddings", ":", "\n", "            ", "d_emb_optimizer", ".", "step", "(", ")", "\n", "\n", "# Visualization of current progress in tensorboard", "\n", "", "if", "(", "i", "%", "config", ".", "plot_update_steps", "==", "0", "and", "i", ">", "0", "and", "config", ".", "show_plots", ")", ":", "\n", "            ", "if", "dec_embs_history", "is", "not", "None", ":", "\n", "                ", "dec_embedding_cut", "=", "np", ".", "asarray", "(", "dec_embs_history", "[", "2", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "dec_embedding_cut", "=", "None", "\n", "", "if", "enc_embs_history", "is", "not", "None", ":", "\n", "                ", "enc_embedding_cut", "=", "np", ".", "asarray", "(", "enc_embs_history", "[", "2", ":", "]", ")", "\n", "", "else", ":", "\n", "                ", "enc_embedding_cut", "=", "None", "\n", "", "_viz_training", "(", "X_real", ",", "reconstructions", ",", "enc_embs", ",", "\n", "dec_embs", ",", "enc_embedding_cut", ",", "dec_embedding_cut", ",", "\n", "writer", ",", "i", ",", "config", ",", "title", "=", "\"train_cond_\"", "+", "str", "(", "t", ")", ")", "\n", "\n", "# track some training statistics", "\n", "", "writer", ".", "add_scalar", "(", "'train/kld_%d'", "%", "(", "t", ")", ",", "kld", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/reconstruction_%d'", "%", "(", "t", ")", ",", "x_rec_loss", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/all_loss_%d'", "%", "(", "t", ")", ",", "loss", "+", "dloss_reg", ",", "i", ")", "\n", "if", "config", ".", "rp_beta", ">", "0", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'train/d_hnet_loss_reg_%d'", "%", "(", "t", ")", ",", "dloss_reg", ",", "i", ")", "\n", "\n", "", "", "test", "(", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "config", ",", "writer", ",", "config", ".", "n_iter", ",", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train": [[424, 452], ["print", "train_replay.init_plotting_embedding", "range", "mnist.replay.train_gan.train_gan_one_t", "train_replay.train_vae_one_t"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.init_plotting_embedding", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_gan.train_gan_one_t", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train_vae_one_t"], ["", "def", "train", "(", "dhandlers", ",", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "config", ",", "writer", ")", ":", "\n", "    ", "\"\"\" Train replay model in continual fashion on MNIST dataset.\n    This is a helper function that loops over the range of tasks and \n    iteratively starts training the replay model on new tasks. \n\n    Args:\n        dhandlers: The dataset handlers.\n        enc: The model of the encoder network.\n        dec. The model of the decoder network.\n        d_hnet. The model of the decoder hyper network.\n        device: Torch device (cpu or gpu).\n        latent_sampler: An initialized distribution, we can sample from.\n        config: The command line arguments.\n        writer: The tensorboard summary writer.\n    \"\"\"", "\n", "\n", "print", "(", "'Training the MNIST replay model ...'", ")", "\n", "\n", "# get embedding lists for plotting", "\n", "embd_list", "=", "init_plotting_embedding", "(", "dhandlers", ",", "d_hnet", ",", "writer", ",", "config", ")", "\n", "# train the replay model task by task", "\n", "for", "t", "in", "range", "(", "config", ".", "num_embeddings", ")", ":", "\n", "        ", "if", "config", ".", "replay_method", "==", "'gan'", ":", "\n", "            ", "train_gan_one_t", "(", "dhandlers", "[", "t", "]", ",", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "\n", "config", ",", "writer", ",", "embd_list", ",", "t", ")", "\n", "", "else", ":", "\n", "            ", "train_vae_one_t", "(", "dhandlers", "[", "t", "]", ",", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "\n", "config", ",", "writer", ",", "embd_list", ",", "t", ")", "\n", "", "", "", "def", "run", "(", "config", ",", "train_system", "=", "True", ",", "only_train_replay", "=", "False", ",", "train_tandem", "=", "True", ")", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.run": [[452, 562], ["mnist.train_utils._setup_environment", "print", "mnist.train_utils._generate_tasks", "mnist.replay.train_utils_replay.generate_replay_networks", "range", "torch.randn_like", "torch.randn_like", "torch.randn_like", "test_z.append", "train_replay.train", "train_replay.test", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.nn.init.normal_", "torch.stack", "torch.stack", "torch.stack", "priors.append", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "priors.append", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.stack", "torch.stack", "torch.stack", "vae_conds.append", "torch.nn.init.normal_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._setup_environment", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_tasks", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_utils_replay.generate_replay_networks", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test"], ["", "", "", "def", "run", "(", "config", ",", "train_system", "=", "True", ",", "only_train_replay", "=", "False", ",", "train_tandem", "=", "True", ")", ":", "\n", "    ", "\"\"\" Method to start training MNIST replay model. \n    Depending on the configurations, here we control the creation and \n    training of the different replay modules with their corresponding \n    hypernetworks.\n        \n    Args:\n        config: The command line arguments.\n        train_system: (optional) Set to false if we want this function \n            only to create config, networks and data_handlers for future \n            training. See :func:`mnist.train_splitMNIST.run` for a use case.\n        only_train_replay: (optional) If this script will only be used to \n            train a replay model. Normally, we use this script in tandem \n            with an additional classifier that uses this replay model to \n            replay old tasks data.\n        train_tandem: (optional) If we will use this script to train in \n            tandem i.e. in an alternating fashion with a classifier.\n    Returns:\n        (tuple): Tuple containing:\n        (....): See docstring of function :func:`train`.\n    \"\"\"", "\n", "\n", "# if we want to train a classifier on single classes then we need a single", "\n", "# class replay method. This need not be the case otherwise i.e. we can ", "\n", "# have a single class replay method but train our classifier on the ", "\n", "# replay data (build out of multiple replayed conidtions) and the current", "\n", "# data at once.", "\n", "# single class replay only implemented for splitMNIST", "\n", "if", "config", ".", "single_class_replay", ":", "\n", "        ", "assert", "(", "config", ".", "experiment", "==", "\"splitMNIST\"", ")", "\n", "\n", "", "if", "config", ".", "num_tasks", ">", "100", "and", "config", ".", "cl_scenario", "!=", "1", ":", "\n", "        ", "print", "(", "\"Attention: Replay model not tested for num tasks > 100\"", ")", "\n", "\n", "### Setup environment", "\n", "", "device", ",", "writer", "=", "train_utils", ".", "_setup_environment", "(", "config", ")", "\n", "\n", "### Create tasks for split MNIST", "\n", "if", "config", ".", "single_class_replay", ":", "\n", "        ", "steps", "=", "1", "\n", "", "else", ":", "\n", "        ", "steps", "=", "2", "\n", "\n", "### Create tasks for split MNIST", "\n", "", "if", "train_system", "==", "False", "and", "config", ".", "upper_bound", "==", "False", ":", "\n", "        ", "dhandlers", "=", "None", "\n", "", "else", ":", "\n", "        ", "dhandlers", "=", "train_utils", ".", "_generate_tasks", "(", "config", ",", "steps", ")", "\n", "\n", "### Generate networks.", "\n", "", "if", "train_system", "==", "False", ":", "\n", "        ", "enc", ",", "dec", ",", "d_hnet", "=", "None", ",", "None", ",", "None", "\n", "", "else", ":", "\n", "        ", "if", "config", ".", "rp_beta", ">", "0", ":", "\n", "            ", "create_rp_hnet", "=", "True", "\n", "", "else", ":", "\n", "            ", "create_rp_hnet", "=", "False", "\n", "", "enc", ",", "dec", ",", "d_hnet", "=", "train_utils_replay", ".", "generate_replay_networks", "(", "config", ",", "\n", "dhandlers", ",", "device", ",", "create_rp_hnet", ",", "\n", "only_train_replay", "=", "only_train_replay", ")", "\n", "### Generate task prioirs for latent space.", "\n", "priors", "=", "[", "]", "\n", "test_z", "=", "[", "]", "\n", "vae_conds", "=", "[", "]", "\n", "\n", "### Save some noise vectors for testing", "\n", "\n", "for", "t", "in", "range", "(", "config", ".", "num_embeddings", ")", ":", "\n", "# if conditional prior create some task priors and save them", "\n", "            ", "if", "config", ".", "conditional_prior", ":", "\n", "                ", "mu", "=", "torch", ".", "zeros", "(", "(", "config", ".", "latent_dim", ")", ")", ".", "to", "(", "device", ")", "\n", "nn", ".", "init", ".", "normal_", "(", "mu", ",", "mean", "=", "0", ",", "std", "=", "1.", ")", "\n", "mu", "=", "torch", ".", "stack", "(", "[", "mu", "]", "*", "config", ".", "batch_size", ")", "\n", "mu", ".", "requires_grad", "=", "False", "\n", "priors", ".", "append", "(", "mu", ")", "\n", "", "else", ":", "\n", "                ", "mu", "=", "torch", ".", "zeros", "(", "(", "config", ".", "batch_size", ",", "\n", "config", ".", "latent_dim", ")", ")", ".", "to", "(", "device", ")", "\n", "priors", ".", "append", "(", "None", ")", "\n", "\n", "### Generate sampler for latent space.", "\n", "", "eps", "=", "torch", ".", "randn_like", "(", "mu", ")", "\n", "\n", "sample", "=", "mu", "+", "eps", "\n", "sample", ".", "requires_grad", "=", "False", "\n", "test_z", ".", "append", "(", "sample", ")", "\n", "\n", "# if vae has some conditional input, then either save hot-encodings", "\n", "# or some conditions from a gaussian", "\n", "if", "config", ".", "conditional_replay", ":", "\n", "                ", "vae_c", "=", "torch", ".", "zeros", "(", "(", "config", ".", "conditional_dim", ")", ")", ".", "to", "(", "device", ")", "\n", "if", "not", "config", ".", "not_conditional_hot_enc", ":", "\n", "                    ", "vae_c", "[", "t", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "nn", ".", "init", ".", "normal_", "(", "vae_c", ",", "mean", "=", "0", ",", "std", "=", "1.", ")", "\n", "", "vae_c", "=", "torch", ".", "stack", "(", "[", "vae_c", "]", "*", "config", ".", "batch_size", ")", "\n", "vae_c", ".", "requires_grad", "=", "False", "\n", "vae_conds", ".", "append", "(", "vae_c", ")", "\n", "\n", "", "", "config", ".", "test_z", "=", "test_z", "\n", "config", ".", "priors", "=", "priors", "\n", "config", ".", "vae_conds", "=", "vae_conds", "\n", "if", "not", "train_tandem", ":", "\n", "### Train the network.", "\n", "            ", "train", "(", "dhandlers", ",", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "config", ",", "writer", ")", "\n", "\n", "### Test network.", "\n", "test", "(", "enc", ",", "dec", ",", "d_hnet", ",", "device", ",", "config", ",", "writer", ")", "\n", "\n", "", "", "return", "dec", ",", "d_hnet", ",", "enc", ",", "dhandlers", ",", "device", ",", "writer", ",", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.tests.test_utils.nostdout": [[33, 56], ["io.BytesIO"], "function", ["None"], ["@", "contextlib", ".", "contextmanager", "\n", "def", "nostdout", "(", ")", ":", "\n", "    ", "\"\"\"A context that can be used to surpress all std outputs of a function.\n\n    The code from this method has been copied from (accessed: 08/14/2019):\n        https://stackoverflow.com/questions/2828953/silence-the-stdout-of-a-function-in-python-without-trashing-sys-stdout-and-resto\n\n    NOTE Our copyright and license does not apply for this function.\n    We use this code WITHOUT ANY WARRANTIES.\n\n    Instead, the code in this method is licensed under CC BY-SA 3.0:\n        https://creativecommons.org/licenses/by-sa/3.0/\n\n    The code stems from an answer by Alex Martelli:\n        https://stackoverflow.com/users/95810/alex-martelli\n\n    The answer has been editted by Nick T:\n        https://stackoverflow.com/users/194586/nick-t\n    \"\"\"", "\n", "save_stdout", "=", "sys", ".", "stdout", "\n", "sys", ".", "stdout", "=", "io", ".", "BytesIO", "(", ")", "\n", "yield", "\n", "sys", ".", "stdout", "=", "save_stdout", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.tests.test_utils.unittest_verbosity": [[57, 80], ["inspect.currentframe", "inspect.currentframe.f_locals.get", "isinstance"], "function", ["None"], ["", "def", "unittest_verbosity", "(", ")", ":", "\n", "    ", "\"\"\"Return the verbosity setting of the currently running unittest\n    program, or 0 if none is running.\n    \n    The code from this method has been copied from (accessed: 08/14/2019):\n        https://stackoverflow.com/questions/13761697/how-to-access-the-unittest-mainverbosity-setting-in-a-unittest-testcase\n\n    NOTE Our copyright and license does not apply for this function.\n    We use this code WITHOUT ANY WARRANTIES.\n\n    Instead, the code in this method is licensed under CC BY-SA 3.0:\n        https://creativecommons.org/licenses/by-sa/3.0/\n\n    The code stems from an answer by Gareth Rees:\n        https://stackoverflow.com/users/68063/gareth-rees\n    \"\"\"", "\n", "frame", "=", "inspect", ".", "currentframe", "(", ")", "\n", "while", "frame", ":", "\n", "        ", "self", "=", "frame", ".", "f_locals", ".", "get", "(", "'self'", ")", "\n", "if", "isinstance", "(", "self", ",", "unittest", ".", "TestProgram", ")", ":", "\n", "            ", "return", "self", ".", "verbosity", "\n", "", "frame", "=", "frame", ".", "f_back", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.test_toy_example.test_train.TrainTestCase.setUp": [[42, 44], ["None"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "pass", "# Nothing to setup.", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.test_toy_example.test_train.TrainTestCase.test_cl_hnet_setup": [[45, 78], ["tests.test_utils.unittest_verbosity", "int", "os.path.join", "list", "os.path.exists", "shutil.rmtree", "test_train.TrainTestCase.assertEqual", "range", "tempfile.gettempdir", "shutil.rmtree", "toy_example.train.run", "len", "len", "len", "test_train.TrainTestCase.assertAlmostEqual", "time.time", "open", "contextlib.redirect_stdout", "toy_example.train.run"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.tests.test_utils.unittest_verbosity", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.run", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.run"], ["", "def", "test_cl_hnet_setup", "(", "self", ")", ":", "\n", "        ", "\"\"\"This method tests whether the CL capabilities of the 3 polynomials\n        toy regression remain as reported in the readme of the corresponding\n        folder.\"\"\"", "\n", "verbosity_level", "=", "unittest_verbosity", "(", ")", "\n", "targets", "=", "[", "0.004187723621726036", ",", "0.002387890825048089", ",", "\n", "0.006071540527045727", "]", "\n", "\n", "# Without timestamp, test would get stuck/fail if someone mistakenly", "\n", "# starts the test case twice.", "\n", "timestamp", "=", "int", "(", "time", ".", "time", "(", ")", "*", "1000", ")", "\n", "out_dir", "=", "os", ".", "path", ".", "join", "(", "tempfile", ".", "gettempdir", "(", ")", ",", "\n", "'test_cl_hnet_setup_%d'", "%", "timestamp", ")", "\n", "my_argv", "=", "[", "'foo'", ",", "'--no_plots'", ",", "'--no_cuda'", ",", "'--beta=0.005'", ",", "\n", "'--emb_size=2'", ",", "'--n_iter=4001'", ",", "'--lr_hyper=1e-2'", ",", "\n", "'--data_random_seed=42'", ",", "'--out_dir=%s'", "%", "out_dir", "]", "\n", "sys", ".", "argv", "=", "list", "(", "my_argv", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "out_dir", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "out_dir", ")", "\n", "\n", "", "if", "verbosity_level", "==", "2", ":", "\n", "            ", "fmse", ",", "_", ",", "_", "=", "train", ".", "run", "(", ")", "\n", "", "else", ":", "\n", "#with nostdout():", "\n", "            ", "with", "open", "(", "os", ".", "devnull", ",", "'w'", ")", "as", "devnull", ":", "\n", "                ", "with", "contextlib", ".", "redirect_stdout", "(", "devnull", ")", ":", "\n", "                    ", "fmse", ",", "_", ",", "_", "=", "train", ".", "run", "(", ")", "\n", "", "", "", "shutil", ".", "rmtree", "(", "out_dir", ")", "\n", "\n", "self", ".", "assertEqual", "(", "len", "(", "fmse", ")", ",", "len", "(", "targets", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "fmse", ")", ")", ":", "\n", "            ", "self", ".", "assertAlmostEqual", "(", "fmse", "[", "i", "]", ",", "targets", "[", "i", "]", ",", "places", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.test_toy_example.test_train.TrainTestCase.tearDown": [[79, 81], ["None"], "methods", ["None"], ["", "", "def", "tearDown", "(", "self", ")", ":", "\n", "        ", "pass", "# Nothing to clean up.", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.__init__": [[120, 152], ["abc.ABC.__init__"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize the network.\n\n        Args:\n\n        \"\"\"", "\n", "super", "(", "MainNetInterface", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# The following member variables have to be set by all classes that", "\n", "# implement this interface.", "\n", "self", ".", "_weights", "=", "None", "\n", "self", ".", "_param_shapes", "=", "None", "\n", "self", ".", "_hyper_shapes_learned", "=", "None", "\n", "self", ".", "_hyper_shapes_distilled", "=", "None", "\n", "self", ".", "_has_bias", "=", "None", "\n", "self", ".", "_has_fc_out", "=", "None", "\n", "self", ".", "_mask_fc_out", "=", "None", "\n", "self", ".", "_has_linear_out", "=", "None", "\n", "self", ".", "_layer_weight_tensors", "=", "None", "\n", "self", ".", "_layer_bias_vectors", "=", "None", "\n", "self", ".", "_batchnorm_layers", "=", "None", "\n", "self", ".", "_context_mod_layers", "=", "None", "\n", "\n", "# This will be set automatically based on attribute `_param_shapes`.", "\n", "self", ".", "_num_params", "=", "None", "\n", "# This will be set automatically based on attribute `_weights`.", "\n", "self", ".", "_num_internal_params", "=", "None", "\n", "\n", "# Deprecated, use `_hyper_shapes_learned` instead.", "\n", "self", ".", "_hyper_shapes", "=", "None", "\n", "# Deprecated, use `_param_shapes` instead.", "\n", "self", ".", "_all_shapes", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup": [[153, 195], ["isinstance", "isinstance", "isinstance", "isinstance", "warnings.warn", "warnings.warn", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "_is_properly_setup", "(", "self", ")", ":", "\n", "        ", "\"\"\"This method can be used by classes that implement this interface to\n        check whether all required properties have been set.\"\"\"", "\n", "assert", "(", "self", ".", "_param_shapes", "is", "not", "None", "or", "self", ".", "_all_shapes", "is", "not", "None", ")", "\n", "if", "self", ".", "_param_shapes", "is", "None", ":", "\n", "            ", "warn", "(", "'Private member \"_param_shapes\" should be specified in each '", "+", "\n", "'sublcass that implements this interface, since private '", "+", "\n", "'member \"_all_shapes\" is deprecated.'", ",", "DeprecationWarning", ")", "\n", "self", ".", "_param_shapes", "=", "self", ".", "_all_shapes", "\n", "\n", "", "if", "self", ".", "_hyper_shapes", "is", "not", "None", "or", "self", ".", "_hyper_shapes_learned", "is", "not", "None", ":", "\n", "            ", "if", "self", ".", "_hyper_shapes_learned", "is", "None", ":", "\n", "                ", "warn", "(", "'Private member \"_hyper_shapes_learned\" should be '", "+", "\n", "'specified in each sublcass that implements this '", "+", "\n", "'interface, since private member \"_hyper_shapes\" is '", "+", "\n", "'deprecated.'", ",", "DeprecationWarning", ")", "\n", "self", ".", "_hyper_shapes_learned", "=", "self", ".", "_hyper_shapes", "\n", "# FIXME we should actually assert equality if", "\n", "# `_hyper_shapes_learned` was not None.", "\n", "", "self", ".", "_hyper_shapes", "=", "self", ".", "_hyper_shapes_learned", "\n", "\n", "", "assert", "(", "self", ".", "_weights", "is", "not", "None", "or", "self", ".", "_hyper_shapes_learned", "is", "not", "None", ")", "\n", "\n", "if", "self", ".", "_hyper_shapes_learned", "is", "None", "and", "self", ".", "hyper_shapes_distilled", "is", "None", ":", "\n", "# Note, `weights` should only contain trainable weights and not", "\n", "# other things like running statistics. Thus, things that are passed", "\n", "# to an optimizer.", "\n", "            ", "assert", "(", "len", "(", "self", ".", "_weights", ")", "==", "len", "(", "self", ".", "_param_shapes", ")", ")", "\n", "\n", "", "assert", "(", "isinstance", "(", "self", ".", "_has_bias", ",", "bool", ")", ")", "\n", "assert", "(", "isinstance", "(", "self", ".", "_has_fc_out", ",", "bool", ")", ")", "\n", "assert", "(", "isinstance", "(", "self", ".", "_mask_fc_out", ",", "bool", ")", ")", "\n", "assert", "(", "isinstance", "(", "self", ".", "_has_linear_out", ",", "bool", ")", ")", "\n", "\n", "assert", "(", "self", ".", "_layer_weight_tensors", "is", "not", "None", ")", "\n", "assert", "(", "self", ".", "_layer_bias_vectors", "is", "not", "None", ")", "\n", "if", "self", ".", "_has_bias", ":", "\n", "            ", "assert", "(", "len", "(", "self", ".", "_layer_weight_tensors", ")", "==", "len", "(", "self", ".", "_layer_bias_vectors", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.weights": [[196, 205], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`weights`.\n\n        Returns:\n            A :class:`torch.nn.ParameterList` or ``None``, if no parameters are\n            internally maintained.\n        \"\"\"", "\n", "return", "self", ".", "_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.param_shapes": [[206, 214], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "param_shapes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`param_shapes`.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "return", "self", ".", "_param_shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.hyper_shapes": [[215, 229], ["warnings.warn"], "methods", ["None"], ["", "@", "property", "\n", "def", "hyper_shapes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`hyper_shapes`.\n\n        .. deprecated:: 1.0\n            This attribute has been renamed to :attr:`hyper_shapes_learned`.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "warn", "(", "'Use atrtibute \"hyper_shapes_learned\" instead.'", ",", "\n", "DeprecationWarning", ")", "\n", "\n", "return", "self", ".", "hyper_shapes_learned", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.hyper_shapes_learned": [[230, 238], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "hyper_shapes_learned", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`hyper_shapes_learned`.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "return", "self", ".", "_hyper_shapes_learned", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.hyper_shapes_distilled": [[239, 247], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "hyper_shapes_distilled", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`hyper_shapes_distilled`.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "return", "self", ".", "_hyper_shapes_distilled", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.has_bias": [[248, 252], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_bias", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`has_bias`.\"\"\"", "\n", "return", "self", ".", "_has_bias", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.has_fc_out": [[253, 257], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_fc_out", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`has_fc_out`.\"\"\"", "\n", "return", "self", ".", "_has_fc_out", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.mask_fc_out": [[258, 262], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "mask_fc_out", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`mask_fc_out`.\"\"\"", "\n", "return", "self", ".", "_mask_fc_out", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.has_linear_out": [[263, 267], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "has_linear_out", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`has_linear_out`.\"\"\"", "\n", "return", "self", ".", "_has_linear_out", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.num_params": [[268, 279], ["int", "numpy.sum", "numpy.prod"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`num_params`.\n\n        Returns:\n            (int): Total number of parameters in the network.\n        \"\"\"", "\n", "if", "self", ".", "_num_params", "is", "None", ":", "\n", "            ", "self", ".", "_num_params", "=", "int", "(", "np", ".", "sum", "(", "[", "np", ".", "prod", "(", "l", ")", "for", "l", "in", "\n", "self", ".", "param_shapes", "]", ")", ")", "\n", "", "return", "self", ".", "_num_params", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.num_internal_params": [[280, 297], ["int", "sum", "p.numel"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_internal_params", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`num_internal_params`.\n\n        Returns:\n            (int): Total number of parameters currently maintained by this\n            network instance.\n        \"\"\"", "\n", "if", "self", ".", "_num_internal_params", "is", "None", ":", "\n", "            ", "if", "self", ".", "weights", "is", "None", ":", "\n", "                ", "self", ".", "_num_internal_params", "=", "0", "\n", "", "else", ":", "\n", "# FIXME should we distinguish between trainable and", "\n", "# non-trainable parameters (`p.requires_grad`)?", "\n", "                ", "self", ".", "_num_internal_params", "=", "int", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "self", ".", "weights", ")", ")", "\n", "", "", "return", "self", ".", "_num_internal_params", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.layer_weight_tensors": [[298, 306], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "layer_weight_tensors", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`layer_weight_tensors`.\n\n        Returns:\n            A list (e.g., an instance of class :class:`torch.nn.ParameterList`).\n        \"\"\"", "\n", "return", "self", ".", "_layer_weight_tensors", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.layer_bias_vectors": [[307, 315], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "layer_bias_vectors", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`layer_bias_vectors`.\n\n        Returns:\n            A list (e.g., an instance of class :class:`torch.nn.ParameterList`).\n        \"\"\"", "\n", "return", "self", ".", "_layer_bias_vectors", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.batchnorm_layers": [[316, 326], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "batchnorm_layers", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`batchnorm_layers`.\n\n        Returns:\n            (:class:`torch.nn.ModuleList`): A list of\n            :class:`utils.batchnorm_layer.BatchNormLayer` instances, if batch\n            normalization is used.\n        \"\"\"", "\n", "return", "self", ".", "_batchnorm_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.context_mod_layers": [[327, 337], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "context_mod_layers", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`context_mod_layers`.\n\n        Returns:\n            (:class:`torch.nn.ModuleList`): A list of\n            :class:`utils.context_mod_layer.ContextModLayer` instances, if these\n            layers are in use.\n        \"\"\"", "\n", "return", "self", ".", "_context_mod_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.distillation_targets": [[338, 362], ["NotImplementedError"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "distillation_targets", "(", "self", ")", ":", "\n", "        ", "\"\"\"Targets to be distilled after training.\n\n        If :attr:`hyper_shapes_distilled` is not ``None``, then this method\n        can be used to retrieve the targets that should be distilled into an\n        external hypernetwork after training.\n\n        The shapes of the returned tensors have to match the shapes specified in\n        :attr:`hyper_shapes_distilled`.\n\n        Example:\n\n            Assume a continual learning scenario with a main network that uses\n            batch normalization (and tracks running statistics). Then this\n            method should be called right after training on a task in order to\n            retrieve the running statistics, such that they can be distilled\n            into a hypernetwork.\n\n        Returns:\n            The target tensors corresponding to the shapes specified in\n            attribute :attr:`hyper_shapes_distilled`.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "'TODO implement function'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.forward": [[363, 402], ["NotImplementedError"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "forward", "(", "self", ",", "x", ",", "weights", "=", "None", ",", "distilled_params", "=", "None", ",", "condition", "=", "None", ")", ":", "\n", "        ", "\"\"\"Compute the output :math:`y` of this network given the input\n        :math:`x`.\n\n        Args:\n            x: The inputs :math:`x` to the network.\n            weights (optional): List of weight tensors, that are used as network\n                parameters. If attribute :attr:`hyper_shapes_learned` is not\n                ``None``, then this argument is non-optional and the shapes\n                of the weight tensors have to be as specified by\n                :attr:`hyper_shapes_learned`.\n\n                Otherwise, this option might still be set but the weight tensors\n                must follow the shapes specified by attribute\n                :attr:`param_shapes`.\n            distilled_params (optional): May only be passed if attribute\n                :attr:`hyper_shapes_distilled` is not ``None``.\n\n                If not passed but the network relies on those parameters\n                (e.g., batchnorm running statistics), then this method simply\n                chooses the current internal representation of these parameters\n                as returned by :meth:`distillation_targets`.\n            condition (optional): Sometimes, the network will have to be\n                conditioned on contextual information, which can be passed via\n                this argument and depends on the actual implementation of this\n                interface.\n\n                For instance, when using batch normalization in a continual\n                learning scenario, where running statistics have been\n                checkpointed for every task, then this ``condition`` might be\n                the actual task ID, that is passed as the argument ``stats_id``\n                of the method\n                :meth:`utils.batchnorm_layer.BatchNormLayer.forward`.\n\n        Returns:\n            The output :math:`y` of the network.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "'TODO implement function'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights": [[403, 416], ["numpy.sum", "numpy.prod"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "shapes_to_num_weights", "(", "dims", ")", ":", "\n", "        ", "\"\"\"The number of parameters contained in a list of tensors with the\n        given shapes.\n\n        Args:\n            dims: List of tensor shapes. For instance, the attribute\n                :attr:`hyper_shapes_learned`.\n\n        Returns:\n            (int)\n        \"\"\"", "\n", "return", "np", ".", "sum", "(", "[", "np", ".", "prod", "(", "l", ")", "for", "l", "in", "dims", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.custom_init": [[417, 444], ["torch.nn.init.normal_", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "custom_init", "(", "self", ",", "normal_init", "=", "False", ",", "normal_std", "=", "0.02", ",", "zero_bias", "=", "True", ")", ":", "\n", "        ", "\"\"\"Initialize weight tensors in attribute :attr:`layer_weight_tensors`\n        using Xavier initialization and set bias vectors to 0.\n\n        Note:\n            This method will override the default initialization of the network,\n            which is often based on :func:`torch.nn.init.kaiming_uniform_`\n            for weight tensors (i.e., attribute :attr:`layer_weight_tensors`)\n            and a uniform init based on fan-in/fan-out for bias vectors\n            (i.e., attribute :attr:`layer_bias_vectors`).\n\n        Args:\n            normal_init (bool): Use normal initialization rather than Xavier.\n            normal_std (float): The standard deviation when choosing\n                ``normal_init``.\n            zero_bias (bool): Whether bias vectors should be initialized to\n                zero. If ``False``, then bias vectors are left untouched.\n        \"\"\"", "\n", "for", "w", "in", "self", ".", "layer_weight_tensors", ":", "\n", "            ", "if", "normal_init", ":", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "w", ",", "mean", "=", "0", ",", "std", "=", "normal_std", ")", "\n", "", "else", ":", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "w", ")", "\n", "\n", "", "", "if", "zero_bias", ":", "\n", "            ", "for", "b", "in", "self", ".", "layer_bias_vectors", ":", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "b", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.zenkenet.ZenkeNet.__init__": [[93, 170], ["mnets.classifier_interface.Classifier.__init__", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "enumerate", "zenkenet.ZenkeNet._is_properly_setup", "ZenkeNet._architectures.keys", "print", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "zenkenet.ZenkeNet._is_properly_setup", "zenkenet.ZenkeNet._weights.append", "range", "range", "ValueError", "torch.Parameter", "torch.Parameter", "torch.Parameter", "zenkenet.ZenkeNet._layer_weight_tensors.append", "zenkenet.ZenkeNet._layer_bias_vectors.append", "len", "len", "len", "numpy.all", "len", "utils.misc.init_params", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "len", "numpy.equal", "list", "list"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params"], ["def", "__init__", "(", "self", ",", "in_shape", "=", "[", "32", ",", "32", ",", "3", "]", ",", "\n", "num_classes", "=", "10", ",", "verbose", "=", "True", ",", "arch", "=", "'cifar'", ",", "no_weights", "=", "False", ",", "\n", "init_weights", "=", "None", ",", "dropout_rate", "=", "0.25", ")", ":", "\n", "        ", "super", "(", "ZenkeNet", ",", "self", ")", ".", "__init__", "(", "num_classes", ",", "verbose", ")", "\n", "\n", "assert", "(", "in_shape", "[", "0", "]", "==", "32", "and", "in_shape", "[", "1", "]", "==", "32", ")", "\n", "self", ".", "_in_shape", "=", "in_shape", "\n", "\n", "assert", "(", "arch", "in", "ZenkeNet", ".", "_architectures", ".", "keys", "(", ")", ")", "\n", "self", ".", "_param_shapes", "=", "ZenkeNet", ".", "_architectures", "[", "arch", "]", "\n", "self", ".", "_param_shapes", "[", "-", "2", "]", "[", "0", "]", "=", "num_classes", "\n", "self", ".", "_param_shapes", "[", "-", "1", "]", "[", "0", "]", "=", "num_classes", "\n", "\n", "assert", "(", "init_weights", "is", "None", "or", "no_weights", "is", "False", ")", "\n", "self", ".", "_no_weights", "=", "no_weights", "\n", "\n", "self", ".", "_use_dropout", "=", "dropout_rate", "!=", "-", "1", "\n", "\n", "self", ".", "_has_bias", "=", "True", "\n", "self", ".", "_has_fc_out", "=", "True", "\n", "# We need to make sure that the last 2 entries of `weights` correspond", "\n", "# to the weight matrix and bias vector of the last layer.", "\n", "self", ".", "_mask_fc_out", "=", "True", "\n", "# We don't use any output non-linearity.", "\n", "self", ".", "_has_linear_out", "=", "True", "\n", "\n", "self", ".", "_num_weights", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "self", ".", "_param_shapes", ")", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'Creating a ZenkeNet with %d weights'", "%", "(", "self", ".", "_num_weights", ")", "\n", "+", "(", "', that uses dropout.'", "if", "self", ".", "_use_dropout", "else", "'.'", ")", ")", "\n", "\n", "", "if", "self", ".", "_use_dropout", ":", "\n", "            ", "if", "dropout_rate", ">", "0.5", ":", "\n", "# FIXME not a pretty solution, but we aim to follow the original", "\n", "# paper.", "\n", "                ", "raise", "ValueError", "(", "'Dropout rate must be smaller equal 0.5.'", ")", "\n", "", "self", ".", "_drop_conv", "=", "nn", ".", "Dropout2d", "(", "p", "=", "dropout_rate", ")", "\n", "self", ".", "_drop_fc1", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout_rate", "*", "2.", ")", "\n", "\n", "", "self", ".", "_layer_weight_tensors", "=", "nn", ".", "ParameterList", "(", ")", "\n", "self", ".", "_layer_bias_vectors", "=", "nn", ".", "ParameterList", "(", ")", "\n", "\n", "if", "no_weights", ":", "\n", "            ", "self", ".", "_weights", "=", "None", "\n", "self", ".", "_hyper_shapes_learned", "=", "self", ".", "_param_shapes", "\n", "self", ".", "_is_properly_setup", "(", ")", "\n", "return", "\n", "\n", "### Define and initialize network weights.", "\n", "# Each odd entry of this list will contain a weight Tensor and each", "\n", "# even entry a bias vector.", "\n", "", "self", ".", "_weights", "=", "nn", ".", "ParameterList", "(", ")", "\n", "\n", "for", "i", ",", "dims", "in", "enumerate", "(", "self", ".", "_param_shapes", ")", ":", "\n", "            ", "self", ".", "_weights", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "*", "dims", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "\n", "if", "i", "%", "2", "==", "0", ":", "\n", "                ", "self", ".", "_layer_weight_tensors", ".", "append", "(", "self", ".", "_weights", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "assert", "(", "len", "(", "dims", ")", "==", "1", ")", "\n", "self", ".", "_layer_bias_vectors", ".", "append", "(", "self", ".", "_weights", "[", "i", "]", ")", "\n", "\n", "", "", "if", "init_weights", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "init_weights", ")", "==", "len", "(", "self", ".", "_param_shapes", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "init_weights", ")", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "i", "]", ".", "shape", ")", ",", "\n", "list", "(", "self", ".", "_weights", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "self", ".", "_weights", "[", "i", "]", ".", "data", "=", "init_weights", "[", "i", "]", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_layer_weight_tensors", ")", ")", ":", "\n", "                ", "init_params", "(", "self", ".", "_layer_weight_tensors", "[", "i", "]", ",", "\n", "self", ".", "_layer_bias_vectors", "[", "i", "]", ")", "\n", "\n", "", "", "self", ".", "_is_properly_setup", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.zenkenet.ZenkeNet.forward": [[171, 238], ["x.permute.permute.view", "x.permute.permute.permute", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.relu", "torch.relu", "torch.relu", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.relu", "torch.relu", "torch.relu", "torch.conv2d", "torch.conv2d", "torch.conv2d", "torch.max_pool2d", "torch.max_pool2d", "torch.max_pool2d", "zenkenet.ZenkeNet.view", "torch.relu", "torch.relu", "torch.relu", "torch.linear", "torch.linear", "torch.linear", "ValueError", "ValueError", "Exception", "enumerate", "torch.relu", "torch.relu", "torch.relu", "zenkenet.ZenkeNet._drop_conv", "torch.relu", "torch.relu", "torch.relu", "zenkenet.ZenkeNet._drop_conv", "torch.linear", "torch.linear", "torch.linear", "zenkenet.ZenkeNet._drop_fc1", "len", "len", "numpy.all", "weights[].size", "numpy.equal", "list"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "weights", "=", "None", ",", "distilled_params", "=", "None", ",", "condition", "=", "None", ")", ":", "\n", "        ", "\"\"\"Compute the output :math:`y` of this network given the input\n        :math:`x`.\n\n\n        Args:\n            (....): See docstring of method\n                :meth:`mnets.mnet_interface.MainNetInterface.forward`. We\n                provide some more specific information below.\n            x: Input image.\n\n                .. note::\n                    We assume the Tensorflow format, where the last entry\n                    denotes the number of channels.\n\n        Returns:\n            y: The output of the network.\n        \"\"\"", "\n", "if", "distilled_params", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Parameter \"distilled_params\" has no '", "+", "\n", "'implementation for this network!'", ")", "\n", "\n", "", "if", "condition", "is", "not", "None", ":", "\n", "            ", "raise", "ValueError", "(", "'Parameter \"condition\" has no '", "+", "\n", "'implementation for this network!'", ")", "\n", "\n", "", "if", "self", ".", "_no_weights", "and", "weights", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Network was generated without weights. '", "+", "\n", "'Hence, \"weights\" option may not be None.'", ")", "\n", "\n", "", "if", "weights", "is", "None", ":", "\n", "            ", "weights", "=", "self", ".", "_weights", "\n", "", "else", ":", "\n", "            ", "shapes", "=", "self", ".", "param_shapes", "\n", "assert", "(", "len", "(", "weights", ")", "==", "len", "(", "shapes", ")", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "shapes", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "s", ",", "list", "(", "weights", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "# Note, implementation aims to follow:", "\n", "#     https://git.io/fj8xP", "\n", "\n", "# first block", "\n", "", "", "x", "=", "x", ".", "view", "(", "*", "(", "[", "-", "1", "]", "+", "self", ".", "_in_shape", ")", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "h", "=", "F", ".", "conv2d", "(", "x", ",", "weights", "[", "0", "]", ",", "bias", "=", "weights", "[", "1", "]", ",", "padding", "=", "1", ")", "# 'SAME'", "\n", "h", "=", "F", ".", "relu", "(", "h", ")", "\n", "h", "=", "F", ".", "conv2d", "(", "h", ",", "weights", "[", "2", "]", ",", "bias", "=", "weights", "[", "3", "]", ",", "padding", "=", "0", ")", "# 'VALID'", "\n", "h", "=", "F", ".", "max_pool2d", "(", "F", ".", "relu", "(", "h", ")", ",", "2", ")", "\n", "if", "self", ".", "_use_dropout", ":", "\n", "            ", "h", "=", "self", ".", "_drop_conv", "(", "h", ")", "\n", "\n", "# second block", "\n", "", "h", "=", "F", ".", "conv2d", "(", "h", ",", "weights", "[", "4", "]", ",", "bias", "=", "weights", "[", "5", "]", ",", "padding", "=", "1", ")", "# 'SAME'", "\n", "h", "=", "F", ".", "relu", "(", "h", ")", "\n", "h", "=", "F", ".", "conv2d", "(", "h", ",", "weights", "[", "6", "]", ",", "bias", "=", "weights", "[", "7", "]", ",", "padding", "=", "0", ")", "# 'VALID'", "\n", "h", "=", "F", ".", "max_pool2d", "(", "F", ".", "relu", "(", "h", ")", ",", "2", ")", "\n", "if", "self", ".", "_use_dropout", ":", "\n", "            ", "h", "=", "self", ".", "_drop_conv", "(", "h", ")", "\n", "\n", "# last fully connected layers", "\n", "", "h", "=", "h", ".", "view", "(", "-", "1", ",", "weights", "[", "8", "]", ".", "size", "(", ")", "[", "1", "]", ")", "\n", "h", "=", "F", ".", "relu", "(", "F", ".", "linear", "(", "h", ",", "weights", "[", "8", "]", ",", "bias", "=", "weights", "[", "9", "]", ")", ")", "\n", "if", "self", ".", "_use_dropout", ":", "\n", "            ", "h", "=", "self", ".", "_drop_fc1", "(", "h", ")", "\n", "", "h", "=", "F", ".", "linear", "(", "h", ",", "weights", "[", "10", "]", ",", "bias", "=", "weights", "[", "11", "]", ")", "\n", "\n", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.zenkenet.ZenkeNet.distillation_targets": [[239, 251], ["None"], "methods", ["None"], ["", "def", "distillation_targets", "(", "self", ")", ":", "\n", "        ", "\"\"\"Targets to be distilled after training.\n\n        See docstring of abstract super method\n        :meth:`mnets.mnet_interface.MainNetInterface.distillation_targets`.\n\n        This network does not have any distillation targets.\n\n        Returns:\n            ``None``\n        \"\"\"", "\n", "return", "None", "\n", "", "", ""]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.__init__": [[41, 58], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "mnets.mnet_interface.MainNetInterface.__init__"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ",", "num_classes", ",", "verbose", ")", ":", "\n", "        ", "\"\"\"Initialize the network.\n\n        Args:\n            num_classes: The number of output neurons.\n            verbose: Allow printing of general information about the generated\n                network (such as number of weights).\n        \"\"\"", "\n", "# FIXME find a way using super to handle multiple inheritence.", "\n", "#super(Classifier, self).__init__()", "\n", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "MainNetInterface", ".", "__init__", "(", "self", ")", "\n", "\n", "assert", "(", "num_classes", ">", "0", ")", "\n", "self", ".", "_num_classes", "=", "num_classes", "\n", "\n", "self", ".", "_verbose", "=", "verbose", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.num_classes": [[59, 63], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_classes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute num_classes.\"\"\"", "\n", "return", "self", ".", "_num_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.logit_cross_entropy_loss": [[64, 84], ["t.argmax", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "logit_cross_entropy_loss", "(", "h", ",", "t", ",", "reduction", "=", "'mean'", ")", ":", "\n", "        ", "\"\"\"Compute cross-entropy loss for given predictions and targets.\n        Note, we assume that the argmax of the target vectors results in the\n        correct label.\n\n        Args:\n            h: Unscaled outputs from the main network, i.e., activations of the\n                last hidden layer (unscaled logits).\n            t: Targets in form os soft labels or 1-hot encodings.\n            reduction (str): The reduction method to be passed to\n                :func:`torch.nn.functional.cross_entropy`.\n\n        Returns:\n            Cross-entropy loss computed on logits h and labels extracted\n            from target vector t.\n        \"\"\"", "\n", "assert", "(", "t", ".", "shape", "[", "1", "]", "==", "h", ".", "shape", "[", "1", "]", ")", "\n", "targets", "=", "t", ".", "argmax", "(", "dim", "=", "1", ",", "keepdim", "=", "False", ")", "\n", "return", "F", ".", "cross_entropy", "(", "h", ",", "targets", ",", "reduction", "=", "reduction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.knowledge_distillation_loss": [[85, 135], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.zeros_like().to", "torch.zeros_like().to", "torch.zeros_like().to", "torch.zeros_like().to", "torch.zeros_like().to", "torch.zeros_like().to", "torch.zeros_like().to", "torch.zeros_like().to", "torch.zeros_like().to", "ValueError", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "knowledge_distillation_loss", "(", "logits", ",", "target_logits", ",", "target_mapping", "=", "None", ",", "\n", "device", "=", "None", ",", "T", "=", "2.", ")", ":", "\n", "        ", "\"\"\"Compute the knowledge distillation loss as proposed by\n\n            Hinton et al., \"Distilling the Knowledge in a Neural Network\",\n            NIPS Deep Learning and Representation Learning Workshop, 2015.\n            http://arxiv.org/abs/1503.02531\n\n        Args:\n            logits: Unscaled outputs from the main network, i.e., activations of\n                the last hidden layer (unscaled logits).\n            target_logits: Target logits, i.e., activations of the last hidden\n                layer (unscaled logits) from the target model.\n                Note, we won't detach \"target_logits\" from the graph. Make sure,\n                that you do this before calling this method.\n            target_mapping: In continual learning, it might be that the output\n                layer size of a model is growing. Thus, it could be that the\n                model providing the \"target_logits\" has a smaller output size\n                than the current model providing the \"logits\". Therefore, one\n                has to provide a mapping, which is a list of indices for\n                \"logits\" that state which activations in \"logits\" have a\n                corresponding target in \"target_logits\".\n                For instance, if the output layer size just increased by 1\n                through appending a new output neuron to the current model, the\n                mapping would simply be:\n                :code:`target_mapping = list(range(target_logits.shape[1]))`.\n            device: Current PyTorch device. Only needs to be specified if\n                \"target_mapping\" is given.\n            T: Softmax temperature.\n\n        Returns:\n            Knowledge Distillation (KD) loss.\n        \"\"\"", "\n", "assert", "(", "target_mapping", "is", "None", "or", "device", "is", "not", "None", ")", "\n", "targets", "=", "F", ".", "softmax", "(", "target_logits", "/", "T", ",", "dim", "=", "1", ")", "\n", "n_classes", "=", "logits", ".", "shape", "[", "1", "]", "\n", "n_targets", "=", "targets", ".", "shape", "[", "1", "]", "\n", "\n", "if", "target_mapping", "is", "None", ":", "\n", "            ", "if", "n_classes", "!=", "n_targets", ":", "\n", "                ", "raise", "ValueError", "(", "'If sizes of \"logits\" and \"target_logits\" '", "+", "\n", "'differ, \"target_mapping\" must be specified.'", ")", "\n", "", "", "else", ":", "\n", "            ", "new_targets", "=", "torch", ".", "zeros_like", "(", "logits", ")", ".", "to", "(", "device", ")", "\n", "new_targets", "[", ":", ",", "target_mapping", "]", "=", "targets", "\n", "targets", "=", "new_targets", "\n", "\n", "", "return", "-", "(", "targets", "*", "F", ".", "log_softmax", "(", "logits", "/", "T", ",", "dim", "=", "1", ")", ")", ".", "sum", "(", "dim", "=", "1", ")", ".", "mean", "(", ")", "*", "T", "**", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.softmax_and_cross_entropy": [[136, 157], ["loss.sum", "loss.mean", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["", "@", "staticmethod", "\n", "def", "softmax_and_cross_entropy", "(", "h", ",", "t", ",", "reduction_sum", "=", "False", ")", ":", "\n", "        ", "\"\"\"Compute the cross entropy from logits, allowing smoothed labels\n        (i.e., this function does not require 1-hot targets).\n\n        Args:\n            h: Unscaled outputs from the main network, i.e., activations of the\n                last hidden layer (unscaled logits).\n            t: Targets in form os soft labels or 1-hot encodings.\n\n        Returns:\n            Cross-entropy loss computed on logits h and given targets t.\n        \"\"\"", "\n", "assert", "(", "t", ".", "shape", "[", "1", "]", "==", "h", ".", "shape", "[", "1", "]", ")", "\n", "\n", "loss", "=", "-", "(", "t", "*", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "h", ",", "dim", "=", "1", ")", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "\n", "if", "reduction_sum", ":", "\n", "            ", "return", "loss", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "loss", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.accuracy": [[158, 176], ["y.argmax", "t.argmax"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "accuracy", "(", "y", ",", "t", ")", ":", "\n", "        ", "\"\"\"Computing the accuracy between predictions y and targets t. We\n        assume that the argmax of t results in labels as described in the\n        docstring of method \"cross_entropy_loss\".\n\n        Args:\n            y: Outputs from the main network.\n            t: Targets in form of soft labels or 1-hot encodings.\n\n        Returns:\n            Relative prediction accuracy on the given batch.\n        \"\"\"", "\n", "assert", "(", "t", ".", "shape", "[", "1", "]", "==", "y", ".", "shape", "[", "1", "]", ")", "\n", "predictions", "=", "y", ".", "argmax", "(", "dim", "=", "1", ",", "keepdim", "=", "False", ")", "\n", "targets", "=", "t", ".", "argmax", "(", "dim", "=", "1", ",", "keepdim", "=", "False", ")", "\n", "\n", "return", "(", "predictions", "==", "targets", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.classifier_interface.Classifier.num_hyper_weights": [[177, 196], ["warnings.warn", "numpy.sum", "numpy.prod"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "num_hyper_weights", "(", "dims", ")", ":", "\n", "        ", "\"\"\"The number of weights that have to be predicted by a hypernetwork.\n\n        .. deprecated:: 1.0\n            Please use method\n            :meth:`mnets.mnet_interface.MainNetInterface.shapes_to_num_weights`\n            instead.\n\n        Args:\n            dims: For instance, the attribute :attr:`hyper_shapes`.\n\n        Returns:\n            (int)\n        \"\"\"", "\n", "warn", "(", "'Please use class \"mnets.mnet_interface.MainNetInterface.'", "+", "\n", "'shapes_to_num_weights\" instead.'", ",", "DeprecationWarning", ")", "\n", "\n", "return", "np", ".", "sum", "(", "[", "np", ".", "prod", "(", "l", ")", "for", "l", "in", "dims", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.resnet.ResNet.__init__": [[166, 415], ["mnets.classifier_interface.Classifier.__init__", "resnet.ResNet._param_shapes.extend", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "resnet.ResNet._layer_weight_tensors.append", "resnet.ResNet._layer_bias_vectors.append", "range", "resnet.ResNet._layer_weight_tensors.append", "resnet.ResNet._layer_bias_vectors.append", "range", "resnet.ResNet._is_properly_setup", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "resnet.ResNet._compute_layer_out_sizes", "enumerate", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "print", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "resnet.ResNet._compute_hyper_shapes", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "resnet.ResNet._is_properly_setup", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "len", "range", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "len", "resnet.ResNet._weights.append", "resnet.ResNet._weights.append", "range", "range", "cm_shapes.append", "cm_shapes.extend", "cm_shapes.extend", "enumerate", "utils.context_mod_layer.ContextModLayer", "resnet.ResNet._context_mod_layers.append", "resnet.ResNet.param_shapes.extend", "resnet.ResNet._context_mod_shapes.extend", "range", "resnet.ResNet._compute_hyper_shapes", "resnet.ResNet._hyper_shapes_learned.extend", "resnet.ResNet._weights.extend", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "resnet.ResNet._layer_weight_tensors.append", "resnet.ResNet._layer_bias_vectors.append", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "len", "len", "len", "numpy.all", "len", "utils.torch_utils.init_params", "resnet.ResNet._hyper_shapes_learned.extend", "resnet.ResNet._weights.extend", "range", "numpy.prod", "utils.batchnorm_layer.BatchNormLayer", "resnet.ResNet._batchnorm_layers.append", "resnet.ResNet._compute_hyper_shapes", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "numpy.equal", "len", "len", "numpy.all", "numpy.sum", "numpy.sum", "numpy.sum", "resnet.ResNet._hyper_shapes_distilled.extend", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "list", "list", "numpy.equal", "len", "numpy.all", "list", "list", "list", "numpy.equal", "range", "range", "range", "utils.batchnorm_layer.BatchNormLayer.get_stats", "list", "list"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.resnet.ResNet._compute_layer_out_sizes", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.resnet.ResNet._compute_hyper_shapes", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.resnet.ResNet._compute_hyper_shapes", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.resnet.ResNet._compute_hyper_shapes", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.get_stats"], ["def", "__init__", "(", "self", ",", "in_shape", "=", "[", "32", ",", "32", ",", "3", "]", ",", "\n", "num_classes", "=", "10", ",", "verbose", "=", "True", ",", "n", "=", "5", ",", "no_weights", "=", "False", ",", "\n", "init_weights", "=", "None", ",", "use_batch_norm", "=", "True", ",", "\n", "bn_track_stats", "=", "True", ",", "distill_bn_stats", "=", "False", ",", "\n", "use_context_mod", "=", "False", ",", "context_mod_inputs", "=", "False", ",", "\n", "no_last_layer_context_mod", "=", "False", ",", "context_mod_no_weights", "=", "False", ",", "\n", "context_mod_post_activation", "=", "False", ",", "\n", "context_mod_gain_offset", "=", "False", ",", "\n", "context_mod_apply_pixel_wise", "=", "False", ")", ":", "\n", "        ", "super", "(", "ResNet", ",", "self", ")", ".", "__init__", "(", "num_classes", ",", "verbose", ")", "\n", "\n", "self", ".", "_in_shape", "=", "in_shape", "\n", "self", ".", "_n", "=", "n", "\n", "\n", "assert", "(", "init_weights", "is", "None", "or", "(", "not", "no_weights", "or", "not", "context_mod_no_weights", ")", ")", "\n", "self", ".", "_no_weights", "=", "no_weights", "\n", "\n", "assert", "(", "not", "use_batch_norm", "or", "(", "not", "distill_bn_stats", "or", "bn_track_stats", ")", ")", "\n", "\n", "self", ".", "_use_batch_norm", "=", "use_batch_norm", "\n", "self", ".", "_bn_track_stats", "=", "bn_track_stats", "\n", "self", ".", "_distill_bn_stats", "=", "distill_bn_stats", "and", "use_batch_norm", "\n", "\n", "self", ".", "_use_context_mod", "=", "use_context_mod", "\n", "self", ".", "_context_mod_inputs", "=", "context_mod_inputs", "\n", "self", ".", "_no_last_layer_context_mod", "=", "no_last_layer_context_mod", "\n", "self", ".", "_context_mod_no_weights", "=", "context_mod_no_weights", "\n", "self", ".", "_context_mod_post_activation", "=", "context_mod_post_activation", "\n", "self", ".", "_context_mod_gain_offset", "=", "context_mod_gain_offset", "\n", "self", ".", "_context_mod_apply_pixel_wise", "=", "context_mod_apply_pixel_wise", "\n", "\n", "self", ".", "_kernel_size", "=", "[", "3", ",", "3", "]", "\n", "self", ".", "_filter_sizes", "=", "[", "16", ",", "16", ",", "32", ",", "64", "]", "\n", "\n", "self", ".", "_has_bias", "=", "True", "\n", "self", ".", "_has_fc_out", "=", "True", "\n", "# We need to make sure that the last 2 entries of `weights` correspond", "\n", "# to the weight matrix and bias vector of the last layer.", "\n", "self", ".", "_mask_fc_out", "=", "True", "\n", "# We don't use any output non-linearity.", "\n", "self", ".", "_has_linear_out", "=", "True", "\n", "\n", "self", ".", "_param_shapes", "=", "[", "]", "\n", "self", ".", "_weights", "=", "None", "if", "no_weights", "and", "context_mod_no_weights", "else", "nn", ".", "ParameterList", "(", ")", "\n", "self", ".", "_hyper_shapes_learned", "=", "None", "if", "not", "no_weights", "and", "not", "context_mod_no_weights", "else", "[", "]", "\n", "\n", "#################################################", "\n", "### Define and initialize context mod weights ###", "\n", "#################################################", "\n", "self", ".", "_context_mod_layers", "=", "nn", ".", "ModuleList", "(", ")", "if", "use_context_mod", "else", "None", "\n", "self", ".", "_context_mod_shapes", "=", "[", "]", "if", "use_context_mod", "else", "None", "\n", "\n", "if", "use_context_mod", ":", "\n", "            ", "cm_ind", "=", "0", "\n", "cm_shapes", "=", "[", "]", "# Output shape of all layers.", "\n", "if", "context_mod_inputs", ":", "\n", "                ", "cm_shapes", ".", "append", "(", "[", "in_shape", "[", "2", "]", ",", "*", "in_shape", "[", ":", "2", "]", "]", ")", "\n", "", "layer_out_shapes", "=", "self", ".", "_compute_layer_out_sizes", "(", ")", "\n", "if", "no_last_layer_context_mod", ":", "\n", "                ", "cm_shapes", ".", "extend", "(", "layer_out_shapes", "[", ":", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "cm_shapes", ".", "extend", "(", "layer_out_shapes", ")", "\n", "\n", "", "if", "not", "context_mod_apply_pixel_wise", ":", "\n", "# Only scalar gain and shift per feature map!", "\n", "                ", "for", "i", ",", "s", "in", "enumerate", "(", "cm_shapes", ")", ":", "\n", "                    ", "if", "len", "(", "s", ")", "==", "3", ":", "\n", "                        ", "cm_shapes", "[", "i", "]", "=", "[", "s", "[", "0", "]", ",", "1", ",", "1", "]", "\n", "\n", "", "", "", "for", "i", ",", "s", "in", "enumerate", "(", "cm_shapes", ")", ":", "\n", "                ", "cmod_layer", "=", "ContextModLayer", "(", "s", ",", "\n", "no_weights", "=", "context_mod_no_weights", ",", "\n", "apply_gain_offset", "=", "context_mod_gain_offset", ")", "\n", "self", ".", "_context_mod_layers", ".", "append", "(", "cmod_layer", ")", "\n", "\n", "self", ".", "param_shapes", ".", "extend", "(", "cmod_layer", ".", "param_shapes", ")", "\n", "self", ".", "_context_mod_shapes", ".", "extend", "(", "cmod_layer", ".", "param_shapes", ")", "\n", "if", "context_mod_no_weights", ":", "\n", "                    ", "self", ".", "_hyper_shapes_learned", ".", "extend", "(", "cmod_layer", ".", "param_shapes", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_weights", ".", "extend", "(", "cmod_layer", ".", "weights", ")", "\n", "\n", "# FIXME ugly code. Move initialization somewhere else.", "\n", "", "if", "not", "context_mod_no_weights", "and", "init_weights", "is", "not", "None", ":", "\n", "                    ", "assert", "(", "len", "(", "cmod_layer", ".", "weights", ")", "==", "2", ")", "\n", "for", "ii", "in", "range", "(", "2", ")", ":", "\n", "                        ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "cm_ind", "]", ".", "shape", ")", ",", "\n", "list", "(", "cm_ind", ".", "weights", "[", "ii", "]", ".", "shape", ")", ")", ")", ")", "\n", "cmod_layer", ".", "weights", "[", "ii", "]", ".", "data", "=", "init_weights", "[", "cm_ind", "]", "\n", "cm_ind", "+=", "1", "\n", "\n", "", "", "", "if", "init_weights", "is", "not", "None", ":", "\n", "                ", "init_weights", "=", "init_weights", "[", "cm_ind", ":", "]", "\n", "\n", "###########################", "\n", "### Print infos to user ###", "\n", "###########################", "\n", "# Compute the total number of weights in this network and display", "\n", "# them to the user.", "\n", "# Note, this complicated calculation is not necessary as we can simply", "\n", "# count the number of weights afterwards. But it's an additional sanity", "\n", "# check for us.", "\n", "", "", "fs", "=", "self", ".", "_filter_sizes", "\n", "num_weights", "=", "np", ".", "prod", "(", "self", ".", "_kernel_size", ")", "*", "(", "in_shape", "[", "2", "]", "*", "fs", "[", "0", "]", "+", "np", ".", "sum", "(", "[", "fs", "[", "i", "]", "*", "fs", "[", "i", "+", "1", "]", "+", "(", "2", "*", "n", "-", "1", ")", "*", "fs", "[", "i", "+", "1", "]", "**", "2", "for", "i", "in", "range", "(", "3", ")", "]", ")", ")", "+", "(", "fs", "[", "0", "]", "+", "2", "*", "n", "*", "np", ".", "sum", "(", "[", "fs", "[", "i", "]", "for", "i", "in", "range", "(", "1", ",", "4", ")", "]", ")", ")", "+", "(", "fs", "[", "-", "1", "]", "*", "num_classes", "+", "num_classes", ")", "\n", "\n", "cm_num_weights", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "self", ".", "_context_mod_shapes", ")", "if", "use_context_mod", "else", "0", "\n", "num_weights", "+=", "cm_num_weights", "\n", "\n", "if", "use_batch_norm", ":", "\n", "# The gamma and beta parameters of a batch norm layer are", "\n", "# learned as well.", "\n", "            ", "num_weights", "+=", "2", "*", "(", "fs", "[", "0", "]", "+", "2", "*", "n", "*", "np", ".", "sum", "(", "[", "fs", "[", "i", "]", "for", "i", "in", "range", "(", "1", ",", "4", ")", "]", ")", ")", "\n", "\n", "", "if", "verbose", ":", "\n", "            ", "print", "(", "'A ResNet with %d layers and %d weights is created'", "%", "(", "6", "*", "n", "+", "2", ",", "num_weights", ")", "\n", "+", "(", "' (including %d context-mod weights).'", "%", "cm_num_weights", "if", "cm_num_weights", ">", "0", "else", "'.'", ")", ")", "\n", "\n", "################################################", "\n", "### Define and initialize batch norm weights ###", "\n", "################################################", "\n", "", "self", ".", "_batchnorm_layers", "=", "nn", ".", "ModuleList", "(", ")", "if", "use_batch_norm", "else", "None", "\n", "\n", "if", "use_batch_norm", ":", "\n", "            ", "if", "distill_bn_stats", ":", "\n", "                ", "self", ".", "_hyper_shapes_distilled", "=", "[", "]", "\n", "\n", "", "bn_ind", "=", "0", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "self", ".", "_filter_sizes", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "num", "=", "1", "\n", "", "else", ":", "\n", "                    ", "num", "=", "2", "*", "n", "\n", "\n", "", "for", "j", "in", "range", "(", "num", ")", ":", "\n", "                    ", "bn_layer", "=", "BatchNormLayer", "(", "s", ",", "affine", "=", "not", "no_weights", ",", "\n", "track_running_stats", "=", "bn_track_stats", ")", "\n", "self", ".", "_batchnorm_layers", ".", "append", "(", "bn_layer", ")", "\n", "\n", "if", "distill_bn_stats", ":", "\n", "                        ", "self", ".", "_hyper_shapes_distilled", ".", "extend", "(", "[", "list", "(", "p", ".", "shape", ")", "for", "p", "in", "bn_layer", ".", "get_stats", "(", "0", ")", "]", ")", "\n", "\n", "", "if", "not", "no_weights", "and", "init_weights", "is", "not", "None", ":", "\n", "                        ", "assert", "(", "len", "(", "bn_layer", ".", "weights", ")", "==", "2", ")", "\n", "for", "ii", "in", "range", "(", "2", ")", ":", "\n", "                            ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "bn_ind", "]", ".", "shape", ")", ",", "\n", "list", "(", "bn_layer", ".", "weights", "[", "ii", "]", ".", "shape", ")", ")", ")", ")", "\n", "bn_layer", ".", "weights", "[", "ii", "]", ".", "data", "=", "init_weights", "[", "bn_ind", "]", "\n", "bn_ind", "+=", "1", "\n", "\n", "", "", "", "", "if", "init_weights", "is", "not", "None", ":", "\n", "                ", "init_weights", "=", "init_weights", "[", "bn_ind", ":", "]", "\n", "\n", "# Note, method `_compute_hyper_shapes` doesn't take context-mod into", "\n", "# consideration.", "\n", "", "", "self", ".", "_param_shapes", ".", "extend", "(", "self", ".", "_compute_hyper_shapes", "(", "no_weights", "=", "True", ")", ")", "\n", "assert", "(", "num_weights", "==", "MainNetInterface", ".", "shapes_to_num_weights", "(", "self", ".", "_param_shapes", ")", ")", "\n", "\n", "self", ".", "_layer_weight_tensors", "=", "nn", ".", "ParameterList", "(", ")", "\n", "self", ".", "_layer_bias_vectors", "=", "nn", ".", "ParameterList", "(", ")", "\n", "\n", "if", "no_weights", ":", "\n", "            ", "if", "self", ".", "_hyper_shapes_learned", "is", "None", ":", "\n", "                ", "self", ".", "_hyper_shapes_learned", "=", "self", ".", "_compute_hyper_shapes", "(", ")", "\n", "", "else", ":", "\n", "# Context-mod weights are already included.", "\n", "                ", "self", ".", "_hyper_shapes_learned", ".", "extend", "(", "self", ".", "_compute_hyper_shapes", "(", ")", ")", "\n", "\n", "", "self", ".", "_is_properly_setup", "(", ")", "\n", "return", "\n", "\n", "", "if", "use_batch_norm", ":", "\n", "            ", "for", "bn_layer", "in", "self", ".", "_batchnorm_layers", ":", "\n", "                ", "self", ".", "_weights", ".", "extend", "(", "bn_layer", ".", "weights", ")", "\n", "\n", "############################################", "\n", "### Define and initialize layer weights ###", "\n", "###########################################", "\n", "### Does not include context-mod or batchnorm weights.", "\n", "# First layer.", "\n", "", "", "self", ".", "_layer_weight_tensors", ".", "append", "(", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "self", ".", "_filter_sizes", "[", "0", "]", ",", "self", ".", "_in_shape", "[", "2", "]", ",", "\n", "*", "self", ".", "_kernel_size", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "self", ".", "_layer_bias_vectors", ".", "append", "(", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "self", ".", "_filter_sizes", "[", "0", "]", ")", ",", "requires_grad", "=", "True", ")", ")", "\n", "\n", "# Each block consists of 2n layers.", "\n", "for", "i", "in", "range", "(", "1", ",", "len", "(", "self", ".", "_filter_sizes", ")", ")", ":", "\n", "            ", "in_filters", "=", "self", ".", "_filter_sizes", "[", "i", "-", "1", "]", "\n", "out_filters", "=", "self", ".", "_filter_sizes", "[", "i", "]", "\n", "\n", "for", "_", "in", "range", "(", "2", "*", "n", ")", ":", "\n", "                ", "self", ".", "_layer_weight_tensors", ".", "append", "(", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "out_filters", ",", "in_filters", ",", "*", "self", ".", "_kernel_size", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "self", ".", "_layer_bias_vectors", ".", "append", "(", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "out_filters", ")", ",", "requires_grad", "=", "True", ")", ")", "\n", "# Note, that the first layer in this block has potentially a", "\n", "# different number of input filters.", "\n", "in_filters", "=", "out_filters", "\n", "\n", "# After the average pooling, there is one more dense layer.", "\n", "", "", "self", ".", "_layer_weight_tensors", ".", "append", "(", "nn", ".", "Parameter", "(", "\n", "torch", ".", "Tensor", "(", "num_classes", ",", "self", ".", "_filter_sizes", "[", "-", "1", "]", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "self", ".", "_layer_bias_vectors", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "num_classes", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "\n", "# We add the weights interleaved, such that there are always consecutive", "\n", "# weight tensor and bias vector per layer. This fulfils the requirements", "\n", "# of attribute `mask_fc_out`.", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_layer_weight_tensors", ")", ")", ":", "\n", "            ", "self", ".", "_weights", ".", "append", "(", "self", ".", "_layer_weight_tensors", "[", "i", "]", ")", "\n", "self", ".", "_weights", ".", "append", "(", "self", ".", "_layer_bias_vectors", "[", "i", "]", ")", "\n", "\n", "### Initialize weights.", "\n", "", "if", "init_weights", "is", "not", "None", ":", "\n", "            ", "num_layers", "=", "6", "*", "n", "+", "2", "\n", "assert", "(", "len", "(", "init_weights", ")", "==", "2", "*", "num_layers", ")", "\n", "offset", "=", "0", "\n", "if", "use_batch_norm", ":", "\n", "                ", "offset", "=", "2", "*", "(", "6", "*", "n", "+", "1", ")", "\n", "", "assert", "(", "len", "(", "self", ".", "_weights", ")", "==", "offset", "+", "2", "*", "num_layers", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "init_weights", ")", ")", ":", "\n", "                ", "j", "=", "offset", "+", "i", "\n", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "i", "]", ".", "shape", ")", ",", "\n", "list", "(", "self", ".", "_weights", "[", "j", "]", ".", "shape", ")", ")", ")", ")", "\n", "self", ".", "_weights", "[", "j", "]", ".", "data", "=", "init_weights", "[", "i", "]", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_layer_weight_tensors", ")", ")", ":", "\n", "                ", "init_params", "(", "self", ".", "_layer_weight_tensors", "[", "i", "]", ",", "\n", "self", ".", "_layer_bias_vectors", "[", "i", "]", ")", "\n", "\n", "", "", "self", ".", "_is_properly_setup", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.resnet.ResNet.forward": [[416, 727], ["enumerate", "x.permute.permute.view", "x.permute.permute.permute", "resnet.ResNet.forward.conv_layer"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "weights", "=", "None", ",", "distilled_params", "=", "None", ",", "condition", "=", "None", ")", ":", "\n", "        ", "\"\"\"Compute the output :math:`y` of this network given the input\n        :math:`x`.\n\n        Args:\n            (....): See docstring of method\n                :meth:`mnets.mnet_interface.MainNetInterface.forward`. We\n                provide some more specific information below.\n            x: Input image.\n\n                .. note::\n                    We assume the Tensorflow format, where the last entry\n                    denotes the number of channels.\n            weights (list or dict): If a list of parameter tensors is given and\n                context modulation is used (see argument ``use_context_mod`` in\n                constructor), then these parameters are interpreted as context-\n                modulation parameters if the length of ``weights`` equals\n                :code:`2*len(net.context_mod_layers)`. Otherwise, the length is\n                expected to be equal to the length of the attribute\n                :attr:`mnets.mnet_interface.MainNetInterface.param_shapes`.\n\n                Alternatively, a dictionary can be passed with the possible\n                keywords ``internal_weights`` and ``mod_weights``. Each keyword\n                is expected to map onto a list of tensors.\n                The keyword ``internal_weights`` refers to all weights of this\n                network except for the weights of the context-modulation layers.\n                The keyword ``mod_weights``, on the other hand, refers\n                specifically to the weights of the context-modulation layers.\n                It is not necessary to specify both keywords.\n            distilled_params: Will be passed as ``running_mean`` and\n                ``running_var`` arguments of method\n                :meth:`utils.batchnorm_layer.BatchNormLayer.forward` if\n                batch normalization is used.\n            condition (optional, int or dict): If ``int`` is provided, then this\n                argument will be passed as argument ``stats_id`` to the method\n                :meth:`utils.batchnorm_layer.BatchNormLayer.forward` if\n                batch normalization is used.\n\n                If a ``dict`` is provided instead, the following keywords are\n                allowed:\n\n                    - ``bn_stats_id``: Will be handled as ``stats_id`` of the\n                      batchnorm layers as described above.\n                    - ``cmod_ckpt_id``: Will be passed as argument ``ckpt_id``\n                      to the method\n                      :meth:`utils.context_mod_layer.ContextModLayer.forward`.\n\n        Returns:\n            y: The output of the network.\n        \"\"\"", "\n", "if", "(", "(", "not", "self", ".", "_use_context_mod", "and", "self", ".", "_no_weights", ")", "or", "(", "self", ".", "_no_weights", "or", "self", ".", "_context_mod_no_weights", ")", ")", "and", "weights", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Network was generated without weights. '", "+", "\n", "'Hence, \"weights\" option may not be None.'", ")", "\n", "\n", "############################################", "\n", "### Extract which weights should be used ###", "\n", "############################################", "\n", "# I.e., are we using internally maintained weights or externally given", "\n", "# ones or are we even mixing between these groups.", "\n", "# FIXME code mostly copied from MLP forward method.", "\n", "", "n_cm", "=", "0", "if", "self", ".", "context_mod_layers", "is", "None", "else", "2", "*", "len", "(", "self", ".", "context_mod_layers", ")", "\n", "\n", "if", "weights", "is", "None", ":", "\n", "            ", "weights", "=", "self", ".", "weights", "\n", "\n", "if", "self", ".", "_use_context_mod", ":", "\n", "                ", "cm_weights", "=", "weights", "[", ":", "n_cm", "]", "\n", "int_weights", "=", "weights", "[", "n_cm", ":", "]", "\n", "", "else", ":", "\n", "                ", "int_weights", "=", "weights", "\n", "", "", "else", ":", "\n", "            ", "int_weights", "=", "None", "\n", "cm_weights", "=", "None", "\n", "\n", "if", "isinstance", "(", "weights", ",", "dict", ")", ":", "\n", "                ", "assert", "(", "'internal_weights'", "in", "weights", ".", "keys", "(", ")", "or", "'mod_weights'", "in", "weights", ".", "keys", "(", ")", ")", "\n", "if", "'internal_weights'", "in", "weights", ".", "keys", "(", ")", ":", "\n", "                    ", "int_weights", "=", "weights", "[", "'internal_weights'", "]", "\n", "", "if", "'mod_weights'", "in", "weights", ".", "keys", "(", ")", ":", "\n", "                    ", "cm_weights", "=", "weights", "[", "'mod_weights'", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "self", ".", "_use_context_mod", "and", "len", "(", "weights", ")", "==", "n_cm", ":", "\n", "                    ", "cm_weights", "=", "weights", "\n", "", "else", ":", "\n", "                    ", "assert", "(", "len", "(", "weights", ")", "==", "len", "(", "self", ".", "param_shapes", ")", ")", "\n", "if", "self", ".", "_use_context_mod", ":", "\n", "                        ", "cm_weights", "=", "weights", "[", ":", "n_cm", "]", "\n", "int_weights", "=", "weights", "[", "n_cm", ":", "]", "\n", "", "else", ":", "\n", "                        ", "int_weights", "=", "weights", "\n", "\n", "", "", "", "if", "self", ".", "_use_context_mod", "and", "cm_weights", "is", "None", ":", "\n", "                ", "if", "self", ".", "_context_mod_no_weights", ":", "\n", "                    ", "raise", "Exception", "(", "'Network was generated without weights '", "+", "\n", "'for context-mod layers. Hence, they must be passed '", "+", "\n", "'via the \"weights\" option.'", ")", "\n", "", "cm_weights", "=", "self", ".", "weights", "[", ":", "n_cm", "]", "\n", "", "if", "int_weights", "is", "None", ":", "\n", "                ", "if", "self", ".", "_no_weights", ":", "\n", "                    ", "raise", "Exception", "(", "'Network was generated without internal '", "+", "\n", "'weights. Hence, they must be passed via the '", "+", "\n", "'\"weights\" option.'", ")", "\n", "", "if", "self", ".", "_context_mod_no_weights", ":", "\n", "                    ", "int_weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                    ", "int_weights", "=", "self", ".", "weights", "[", "n_cm", ":", "]", "\n", "\n", "# Note, context-mod weights might have different shapes, as they", "\n", "# may be parametrized on a per-sample basis.", "\n", "", "", "if", "self", ".", "_use_context_mod", ":", "\n", "                ", "assert", "(", "len", "(", "cm_weights", ")", "==", "len", "(", "self", ".", "_context_mod_shapes", ")", ")", "\n", "", "int_shapes", "=", "self", ".", "param_shapes", "[", "n_cm", ":", "]", "\n", "assert", "(", "len", "(", "int_weights", ")", "==", "len", "(", "int_shapes", ")", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "int_shapes", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "s", ",", "list", "(", "int_weights", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "########################", "\n", "### Parse condition ###", "\n", "#######################", "\n", "\n", "", "", "bn_cond", "=", "None", "\n", "cmod_cond", "=", "None", "\n", "\n", "if", "condition", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "condition", ",", "dict", ")", ":", "\n", "                ", "assert", "(", "'bn_stats_id'", "in", "condition", ".", "keys", "(", ")", "or", "'cmod_ckpt_id'", "in", "condition", ".", "keys", "(", ")", ")", "\n", "if", "'bn_stats_id'", "in", "condition", ".", "keys", "(", ")", ":", "\n", "                    ", "bn_cond", "=", "condition", "[", "'bn_stats_id'", "]", "\n", "", "if", "'cmod_ckpt_id'", "in", "condition", ".", "keys", "(", ")", ":", "\n", "                    ", "cmod_cond", "=", "condition", "[", "'cmod_ckpt_id'", "]", "\n", "", "", "else", ":", "\n", "                ", "bn_cond", "=", "condition", "\n", "\n", "######################################", "\n", "### Select batchnorm running stats ###", "\n", "######################################", "\n", "", "", "if", "self", ".", "_use_batch_norm", ":", "\n", "# There are 6*n+1 layers that use batch normalization.", "\n", "            ", "lbw", "=", "2", "*", "(", "6", "*", "self", ".", "_n", "+", "1", ")", "\n", "\n", "bn_weights", "=", "int_weights", "[", ":", "lbw", "]", "\n", "layer_weights", "=", "int_weights", "[", "lbw", ":", "]", "\n", "\n", "nn", "=", "len", "(", "self", ".", "_batchnorm_layers", ")", "\n", "running_means", "=", "[", "None", "]", "*", "nn", "\n", "running_vars", "=", "[", "None", "]", "*", "nn", "\n", "", "else", ":", "\n", "            ", "layer_weights", "=", "int_weights", "\n", "\n", "", "if", "distilled_params", "is", "not", "None", ":", "\n", "            ", "if", "not", "self", ".", "_distill_bn_stats", ":", "\n", "                ", "raise", "ValueError", "(", "'Argument \"distilled_params\" can only be '", "+", "\n", "'provided if the return value of '", "+", "\n", "'method \"distillation_targets()\" is not None.'", ")", "\n", "", "shapes", "=", "self", ".", "hyper_shapes_distilled", "\n", "assert", "(", "len", "(", "distilled_params", ")", "==", "len", "(", "shapes", ")", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "shapes", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "s", ",", "list", "(", "distilled_params", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "# Extract batchnorm stats from distilled_params", "\n", "", "for", "i", "in", "range", "(", "0", ",", "len", "(", "distilled_params", ")", ",", "2", ")", ":", "\n", "                ", "running_means", "[", "i", "//", "2", "]", "=", "distilled_params", "[", "i", "]", "\n", "running_vars", "[", "i", "//", "2", "]", "=", "distilled_params", "[", "i", "+", "1", "]", "\n", "\n", "", "", "elif", "self", ".", "_use_batch_norm", "and", "self", ".", "_bn_track_stats", "and", "bn_cond", "is", "None", ":", "\n", "            ", "for", "i", ",", "bn_layer", "in", "enumerate", "(", "self", ".", "_batchnorm_layers", ")", ":", "\n", "                ", "running_means", "[", "i", "]", ",", "running_vars", "[", "i", "]", "=", "bn_layer", ".", "get_stats", "(", ")", "\n", "\n", "###############################################", "\n", "### Extract weight tensors and bias vectors ###", "\n", "###############################################", "\n", "", "", "assert", "(", "self", ".", "has_bias", ")", "\n", "w_weights", "=", "[", "]", "\n", "b_weights", "=", "[", "]", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "layer_weights", ")", ":", "\n", "            ", "if", "i", "%", "2", "==", "1", ":", "\n", "                ", "b_weights", ".", "append", "(", "p", ")", "\n", "", "else", ":", "\n", "                ", "w_weights", ".", "append", "(", "p", ")", "\n", "\n", "###########################", "\n", "### Forward Computation ###", "\n", "###########################", "\n", "", "", "cm_ind", "=", "0", "\n", "bn_ind", "=", "0", "\n", "layer_ind", "=", "0", "\n", "\n", "### Helper function to process convolutional layers.", "\n", "def", "conv_layer", "(", "h", ",", "stride", ",", "shortcut", "=", "None", ")", ":", "\n", "            ", "\"\"\"Compute the output of a resnet conv layer including batchnorm,\n            context-mod, non-linearity and shortcut.\n\n            The order if the following:\n\n            conv-layer -> context-mod (if pre-activation) -> batch-norm ->\n            shortcut -> non-linearity -> context-mod (if post-activation)\n\n            This method increments the indices ``layer_ind``, ``cm_ind`` and\n            ``bn_ind``.\n\n            Args:\n                h: Input activity.\n                stride: Stride of conv. layer (padding is set to 1).\n                shortcut: Is set, this tensor will be added to the activation\n                    before the non-linearity is applied.\n\n            Returns:\n                Output of layer.\n            \"\"\"", "\n", "nonlocal", "layer_ind", ",", "cm_ind", ",", "bn_ind", "\n", "\n", "h", "=", "F", ".", "conv2d", "(", "h", ",", "w_weights", "[", "layer_ind", "]", ",", "bias", "=", "b_weights", "[", "layer_ind", "]", ",", "\n", "stride", "=", "stride", ",", "padding", "=", "1", ")", "\n", "layer_ind", "+=", "1", "\n", "\n", "# Context-dependent modulation (pre-activation).", "\n", "if", "self", ".", "_use_context_mod", "and", "not", "self", ".", "_context_mod_post_activation", ":", "\n", "                ", "h", "=", "self", ".", "_context_mod_layers", "[", "cm_ind", "]", ".", "forward", "(", "h", ",", "\n", "weights", "=", "cm_weights", "[", "2", "*", "cm_ind", ":", "2", "*", "cm_ind", "+", "2", "]", ",", "\n", "ckpt_id", "=", "cmod_cond", ")", "\n", "cm_ind", "+=", "1", "\n", "\n", "# Batch-norm", "\n", "", "if", "self", ".", "_use_batch_norm", ":", "\n", "                ", "h", "=", "self", ".", "_batchnorm_layers", "[", "bn_ind", "]", ".", "forward", "(", "h", ",", "\n", "running_mean", "=", "running_means", "[", "bn_ind", "]", ",", "\n", "running_var", "=", "running_vars", "[", "bn_ind", "]", ",", "\n", "weight", "=", "bn_weights", "[", "2", "*", "bn_ind", "]", ",", "\n", "bias", "=", "bn_weights", "[", "2", "*", "bn_ind", "+", "1", "]", ",", "stats_id", "=", "bn_cond", ")", "\n", "bn_ind", "+=", "1", "\n", "\n", "# Note, as can be seen in figure 5 of the original paper, the", "\n", "# shortcut is performed before the ReLU is applied.", "\n", "", "if", "shortcut", "is", "not", "None", ":", "\n", "                ", "h", "+=", "shortcut", "\n", "\n", "# Non-linearity", "\n", "", "h", "=", "F", ".", "relu", "(", "h", ")", "\n", "\n", "# Context-dependent modulation (post-activation).", "\n", "if", "self", ".", "_use_context_mod", "and", "self", ".", "_context_mod_post_activation", ":", "\n", "                ", "h", "=", "self", ".", "_context_mod_layers", "[", "cm_ind", "]", ".", "forward", "(", "h", ",", "\n", "weights", "=", "cm_weights", "[", "2", "*", "cm_ind", ":", "2", "*", "cm_ind", "+", "2", "]", ",", "\n", "ckpt_id", "=", "cmod_cond", ")", "\n", "cm_ind", "+=", "1", "\n", "\n", "", "return", "h", "\n", "\n", "", "x", "=", "x", ".", "view", "(", "*", "(", "[", "-", "1", "]", "+", "self", ".", "_in_shape", ")", ")", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "\n", "h", "=", "x", "\n", "\n", "# Context-dependent modulation of inputs directly.", "\n", "if", "self", ".", "_use_context_mod", "and", "self", ".", "_context_mod_inputs", ":", "\n", "            ", "h", "=", "self", ".", "_context_mod_layers", "[", "cm_ind", "]", ".", "forward", "(", "h", ",", "\n", "weights", "=", "cm_weights", "[", "2", "*", "cm_ind", ":", "2", "*", "cm_ind", "+", "2", "]", ",", "ckpt_id", "=", "cmod_cond", ")", "\n", "cm_ind", "+=", "1", "\n", "\n", "### Initial convolutional layer.", "\n", "", "h", "=", "conv_layer", "(", "h", ",", "1", ",", "shortcut", "=", "None", ")", "\n", "\n", "### Three blocks, each containing n resnet units.", "\n", "for", "i", "in", "range", "(", "3", ")", ":", "\n", "# Only the first layer in a block may be a strided convolution.", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "stride", "=", "1", "\n", "", "else", ":", "\n", "                ", "stride", "=", "2", "\n", "\n", "# For each resnet unit. A resnet unit consists of 2 convolutional", "\n", "# layers.", "\n", "", "for", "j", "in", "range", "(", "self", ".", "_n", ")", ":", "\n", "                ", "if", "stride", "==", "1", ":", "\n", "                    ", "shortcut_h", "=", "h", "\n", "", "else", ":", "\n", "# The original paper uses zero padding for added output", "\n", "# feature dimensions. Since we apply a strided conv, we", "\n", "# additionally have to subsample the input.", "\n", "# This implementation is motivated by", "\n", "#    https://git.io/fhcfk", "\n", "                    ", "fs", "=", "self", ".", "_filter_sizes", "[", "i", "+", "1", "]", "\n", "shortcut_h", "=", "F", ".", "pad", "(", "h", "[", ":", ",", ":", ",", ":", ":", "2", ",", ":", ":", "2", "]", ",", "\n", "(", "0", ",", "0", ",", "0", ",", "0", ",", "fs", "//", "4", ",", "fs", "//", "4", ")", ",", "\"constant\"", ",", "0", ")", "\n", "\n", "", "h", "=", "conv_layer", "(", "h", ",", "stride", ",", "shortcut", "=", "None", ")", "\n", "\n", "stride", "=", "1", "\n", "\n", "h", "=", "conv_layer", "(", "h", ",", "stride", ",", "shortcut", "=", "shortcut_h", ")", "\n", "\n", "### Average pool all activities within a feature map.", "\n", "", "", "h", "=", "F", ".", "avg_pool2d", "(", "h", ",", "[", "h", ".", "size", "(", ")", "[", "2", "]", ",", "h", ".", "size", "(", ")", "[", "3", "]", "]", ")", "\n", "h", "=", "h", ".", "view", "(", "h", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n", "### Apply final fully-connected layer and compute outputs.", "\n", "h", "=", "F", ".", "linear", "(", "h", ",", "w_weights", "[", "layer_ind", "]", ",", "bias", "=", "b_weights", "[", "layer_ind", "]", ")", "\n", "\n", "# Context-dependent modulation in output layer.", "\n", "if", "self", ".", "_use_context_mod", "and", "not", "self", ".", "_no_last_layer_context_mod", ":", "\n", "            ", "h", "=", "self", ".", "_context_mod_layers", "[", "cm_ind", "]", ".", "forward", "(", "h", ",", "\n", "weights", "=", "cm_weights", "[", "2", "*", "cm_ind", ":", "2", "*", "cm_ind", "+", "2", "]", ",", "ckpt_id", "=", "cmod_cond", ")", "\n", "\n", "", "return", "h", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.resnet.ResNet._compute_hyper_shapes": [[728, 793], ["enumerate", "ret.append", "ret.append", "enumerate", "range", "range", "ret.append", "ret.append", "ret.append"], "methods", ["None"], ["", "def", "_compute_hyper_shapes", "(", "self", ",", "no_weights", "=", "None", ")", ":", "\n", "        ", "r\"\"\"Helper function to compute weight shapes of this network for\n        externally maintained weights.\n\n        Returns a list of lists of integers denoting the shape of every\n        weight tensor that is not a trainable parameter of this network (i.e.,\n        those weight tensors whose shapes are specified in\n        :attr:`mnets.mnet_interface.MainNetInterface.hyper_shapes_distilled`).\n\n        If batchnorm layers are used, then the first :math:`2 * (6n+1)` lists\n        will denote the shapes of the batchnorm weights\n        :math:`[\\gamma_1, \\beta_1, \\gamma_2, ..., \\beta_{6n+1}]`.\n\n        The remaining :math:`2 * (6n+2)` entries are weight tensors and bias\n        vectors of each convolutional or fully-connected (last two entries)\n        layer in this network.\n\n        Args:\n            no_weights (optional): If specified, it will overwrite the private\n                member :code:`self._no_weights`.\n\n                If set to ``True``, then all weight shapes of the network\n                are computed independent of whether they are maintained\n                internally or externally.\n\n        Returns:\n            A list of lists of integers.\n        \"\"\"", "\n", "if", "no_weights", "is", "None", ":", "\n", "            ", "no_weights", "=", "self", ".", "_no_weights", "\n", "\n", "", "ret", "=", "[", "]", "\n", "if", "no_weights", "is", "False", ":", "\n", "            ", "return", "ret", "\n", "\n", "", "fs", "=", "self", ".", "_filter_sizes", "\n", "ks", "=", "self", ".", "_kernel_size", "\n", "n", "=", "self", ".", "_n", "\n", "\n", "if", "self", ".", "_use_batch_norm", ":", "\n", "            ", "for", "i", ",", "s", "in", "enumerate", "(", "fs", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "num", "=", "1", "\n", "", "else", ":", "\n", "                    ", "num", "=", "2", "*", "n", "\n", "\n", "", "for", "_", "in", "range", "(", "2", "*", "num", ")", ":", "\n", "                    ", "ret", ".", "append", "(", "[", "s", "]", ")", "\n", "\n", "", "", "", "f_in", "=", "self", ".", "_in_shape", "[", "-", "1", "]", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "fs", ")", ":", "\n", "            ", "f_out", "=", "s", "\n", "if", "i", "==", "0", ":", "\n", "                ", "num", "=", "1", "\n", "", "else", ":", "\n", "                ", "num", "=", "2", "*", "n", "\n", "\n", "", "for", "_", "in", "range", "(", "num", ")", ":", "\n", "                ", "ret", ".", "append", "(", "[", "f_out", ",", "f_in", ",", "*", "ks", "]", ")", "\n", "ret", ".", "append", "(", "[", "f_out", "]", ")", "\n", "f_in", "=", "f_out", "\n", "", "", "ret", ".", "append", "(", "[", "self", ".", "_num_classes", ",", "fs", "[", "-", "1", "]", "]", ")", "\n", "ret", ".", "append", "(", "[", "self", ".", "_num_classes", "]", ")", "\n", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.resnet.ResNet.distillation_targets": [[794, 816], ["ret.extend", "bn_layer.get_stats"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.get_stats"], ["", "def", "distillation_targets", "(", "self", ")", ":", "\n", "        ", "\"\"\"Targets to be distilled after training.\n\n        See docstring of abstract super method\n        :meth:`mnets.mnet_interface.MainNetInterface.distillation_targets`.\n\n        This method will return the current batch statistics of all batch\n        normalization layers if ``distill_bn_stats`` and ``use_batch_norm``\n        was set to ``True`` in the constructor.\n\n        Returns:\n            The target tensors corresponding to the shapes specified in\n            attribute :attr:`hyper_shapes_distilled`.\n        \"\"\"", "\n", "if", "self", ".", "hyper_shapes_distilled", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "ret", "=", "[", "]", "\n", "for", "bn_layer", "in", "self", ".", "_batchnorm_layers", ":", "\n", "            ", "ret", ".", "extend", "(", "bn_layer", ".", "get_stats", "(", ")", ")", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.resnet.ResNet._compute_layer_out_sizes": [[817, 883], ["ret.append", "ret.extend", "ret.extend", "ret.extend", "ret.append", "len", "len", "len", "len"], "methods", ["None"], ["", "def", "_compute_layer_out_sizes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Compute the output shapes of all layers in this network.\n\n        This method will compute the output shape of each layer in this network,\n        including the output layer, which just corresponds to the number of\n        classes.\n\n        Returns:\n            (list): A list of shapes (lists of integers). The first entry will\n            correspond to the shape of the output of the first convolutional\n            layer. The last entry will correspond to the output shape.\n\n            .. note:\n                Output shapes of convolutional layers will adhere PyTorch\n                convention, i.e., ``[C, H, W]``, where ``C`` denotes the channel\n                dimension.\n        \"\"\"", "\n", "in_shape", "=", "self", ".", "_in_shape", "\n", "fs", "=", "self", ".", "_filter_sizes", "\n", "ks", "=", "self", ".", "_kernel_size", "\n", "pd", "=", "1", "# all paddings are 1.", "\n", "assert", "(", "len", "(", "ks", ")", "==", "2", ")", "\n", "assert", "(", "len", "(", "fs", ")", "==", "4", ")", "\n", "n", "=", "self", ".", "_n", "\n", "\n", "# Note, `in_shape` is in Tensorflow layout.", "\n", "assert", "(", "len", "(", "in_shape", ")", "==", "3", ")", "\n", "in_shape", "=", "[", "in_shape", "[", "2", "]", ",", "*", "in_shape", "[", ":", "2", "]", "]", "\n", "\n", "ret", "=", "[", "]", "\n", "\n", "C", ",", "H", ",", "W", "=", "in_shape", "\n", "\n", "# Recall the formular for convolutional layers:", "\n", "# W_new = (W - K + 2P) // S + 1", "\n", "\n", "# First conv layer (stride 1).", "\n", "C", "=", "fs", "[", "0", "]", "\n", "H", "=", "(", "H", "-", "ks", "[", "0", "]", "+", "2", "*", "pd", ")", "//", "1", "+", "1", "\n", "W", "=", "(", "W", "-", "ks", "[", "1", "]", "+", "2", "*", "pd", ")", "//", "1", "+", "1", "\n", "ret", ".", "append", "(", "[", "C", ",", "H", ",", "W", "]", ")", "\n", "\n", "# First block (no strides).", "\n", "C", "=", "fs", "[", "1", "]", "\n", "H", "=", "(", "H", "-", "ks", "[", "0", "]", "+", "2", "*", "pd", ")", "//", "1", "+", "1", "\n", "W", "=", "(", "W", "-", "ks", "[", "1", "]", "+", "2", "*", "pd", ")", "//", "1", "+", "1", "\n", "ret", ".", "extend", "(", "[", "[", "C", ",", "H", ",", "W", "]", "]", "*", "(", "2", "*", "n", ")", ")", "\n", "\n", "# Second block (first layer has stride 2).", "\n", "C", "=", "fs", "[", "2", "]", "\n", "H", "=", "(", "H", "-", "ks", "[", "0", "]", "+", "2", "*", "pd", ")", "//", "2", "+", "1", "\n", "W", "=", "(", "W", "-", "ks", "[", "1", "]", "+", "2", "*", "pd", ")", "//", "2", "+", "1", "\n", "ret", ".", "extend", "(", "[", "[", "C", ",", "H", ",", "W", "]", "]", "*", "(", "2", "*", "n", ")", ")", "\n", "\n", "# Third block (first layer has stride 2).", "\n", "C", "=", "fs", "[", "3", "]", "\n", "H", "=", "(", "H", "-", "ks", "[", "0", "]", "+", "2", "*", "pd", ")", "//", "2", "+", "1", "\n", "W", "=", "(", "W", "-", "ks", "[", "1", "]", "+", "2", "*", "pd", ")", "//", "2", "+", "1", "\n", "ret", ".", "extend", "(", "[", "[", "C", ",", "H", ",", "W", "]", "]", "*", "(", "2", "*", "n", ")", ")", "\n", "\n", "# Final fully-connected layer (after avg pooling), i.e., output size.", "\n", "ret", ".", "append", "(", "[", "self", ".", "_num_classes", "]", ")", "\n", "\n", "assert", "(", "len", "(", "ret", ")", "==", "6", "*", "n", "+", "2", ")", "\n", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mlp.MLP.__init__": [[163, 365], ["torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "mnets.mnet_interface.MainNetInterface.__init__", "mlp.MLP.weight_shapes", "mlp.MLP._param_shapes.extend", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "enumerate", "mlp.MLP._is_properly_setup", "NotImplementedError", "ValueError", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "cm_sizes.extend", "enumerate", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "enumerate", "print", "mlp.MLP._hyper_shapes_learned.extend", "mlp.MLP._is_properly_setup", "mlp.MLP._weights.append", "range", "range", "cm_sizes.append", "cm_sizes.append", "utils.context_mod_layer.ContextModLayer", "mlp.MLP._context_mod_layers.append", "mlp.MLP.param_shapes.extend", "mlp.MLP._context_mod_shapes.extend", "utils.batchnorm_layer.BatchNormLayer", "mlp.MLP._batchnorm_layers.append", "mlp.MLP._param_shapes.extend", "torch.Parameter", "torch.Parameter", "torch.Parameter", "len", "mlp.MLP._layer_bias_vectors.append", "mlp.MLP._layer_weight_tensors.append", "len", "len", "len", "numpy.all", "len", "mlp.MLP._hyper_shapes_learned.extend", "mlp.MLP._weights.extend", "range", "mlp.MLP._hyper_shapes_learned.extend", "mlp.MLP._weights.extend", "mlp.MLP._hyper_shapes_distilled.extend", "range", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "numpy.equal", "utils.torch_utils.init_params", "utils.torch_utils.init_params", "len", "numpy.all", "len", "numpy.all", "list", "numpy.equal", "list", "numpy.equal", "list", "list", "utils.batchnorm_layer.BatchNormLayer.get_stats", "list", "list"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.weight_shapes", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.get_stats"], ["def", "__init__", "(", "self", ",", "n_in", "=", "1", ",", "n_out", "=", "1", ",", "hidden_layers", "=", "[", "10", ",", "10", "]", ",", "\n", "activation_fn", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "use_bias", "=", "True", ",", "no_weights", "=", "False", ",", "\n", "init_weights", "=", "None", ",", "dropout_rate", "=", "-", "1", ",", "use_spectral_norm", "=", "False", ",", "\n", "use_batch_norm", "=", "False", ",", "bn_track_stats", "=", "True", ",", "\n", "distill_bn_stats", "=", "False", ",", "use_context_mod", "=", "False", ",", "\n", "context_mod_inputs", "=", "False", ",", "no_last_layer_context_mod", "=", "False", ",", "\n", "context_mod_no_weights", "=", "False", ",", "\n", "context_mod_post_activation", "=", "False", ",", "\n", "context_mod_gain_offset", "=", "False", ",", "out_fn", "=", "None", ",", "verbose", "=", "True", ")", ":", "\n", "# FIXME find a way using super to handle multiple inheritence.", "\n", "#super(MainNetwork, self).__init__()", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "MainNetInterface", ".", "__init__", "(", "self", ")", "\n", "\n", "if", "use_spectral_norm", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Spectral normalization not yet '", "+", "\n", "'implemented for this network.'", ")", "\n", "\n", "", "if", "use_batch_norm", "and", "use_context_mod", ":", "\n", "# FIXME Does it make sense to have both enabled?", "\n", "# I.e., should we produce a warning or error?", "\n", "            ", "pass", "\n", "\n", "", "self", ".", "_a_fun", "=", "activation_fn", "\n", "assert", "(", "init_weights", "is", "None", "or", "(", "not", "no_weights", "or", "not", "context_mod_no_weights", ")", ")", "\n", "self", ".", "_no_weights", "=", "no_weights", "\n", "self", ".", "_dropout_rate", "=", "dropout_rate", "\n", "#self._use_spectral_norm = use_spectral_norm", "\n", "self", ".", "_use_batch_norm", "=", "use_batch_norm", "\n", "self", ".", "_bn_track_stats", "=", "bn_track_stats", "\n", "self", ".", "_distill_bn_stats", "=", "distill_bn_stats", "and", "use_batch_norm", "\n", "self", ".", "_use_context_mod", "=", "use_context_mod", "\n", "self", ".", "_context_mod_inputs", "=", "context_mod_inputs", "\n", "self", ".", "_no_last_layer_context_mod", "=", "no_last_layer_context_mod", "\n", "self", ".", "_context_mod_no_weights", "=", "context_mod_no_weights", "\n", "self", ".", "_context_mod_post_activation", "=", "context_mod_post_activation", "\n", "self", ".", "_context_mod_gain_offset", "=", "context_mod_gain_offset", "\n", "self", ".", "_out_fn", "=", "out_fn", "\n", "\n", "self", ".", "_has_bias", "=", "use_bias", "\n", "self", ".", "_has_fc_out", "=", "True", "\n", "# We need to make sure that the last 2 entries of `weights` correspond", "\n", "# to the weight matrix and bias vector of the last layer.", "\n", "self", ".", "_mask_fc_out", "=", "True", "\n", "self", ".", "_has_linear_out", "=", "True", "if", "out_fn", "is", "None", "else", "False", "\n", "\n", "if", "use_spectral_norm", "and", "no_weights", ":", "\n", "            ", "raise", "ValueError", "(", "'Cannot use spectral norm in a network without '", "+", "\n", "'parameters.'", ")", "\n", "\n", "# FIXME make sure that this implementation is correct in all situations", "\n", "# (e.g., what to do if weights are passed to the forward method?).", "\n", "", "if", "use_spectral_norm", ":", "\n", "            ", "self", ".", "_spec_norm", "=", "nn", ".", "utils", ".", "spectral_norm", "\n", "", "else", ":", "\n", "            ", "self", ".", "_spec_norm", "=", "lambda", "x", ":", "x", "# identity", "\n", "\n", "", "self", ".", "_param_shapes", "=", "[", "]", "\n", "self", ".", "_weights", "=", "None", "if", "no_weights", "and", "context_mod_no_weights", "else", "nn", ".", "ParameterList", "(", ")", "\n", "self", ".", "_hyper_shapes_learned", "=", "None", "if", "not", "no_weights", "and", "not", "context_mod_no_weights", "else", "[", "]", "\n", "\n", "if", "dropout_rate", "!=", "-", "1", ":", "\n", "            ", "assert", "(", "dropout_rate", ">=", "0.", "and", "dropout_rate", "<=", "1.", ")", "\n", "self", ".", "_dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout_rate", ")", "\n", "\n", "### Define and initialize context mod weights.", "\n", "", "self", ".", "_context_mod_layers", "=", "nn", ".", "ModuleList", "(", ")", "if", "use_context_mod", "else", "None", "\n", "self", ".", "_context_mod_shapes", "=", "[", "]", "if", "use_context_mod", "else", "None", "\n", "\n", "if", "use_context_mod", ":", "\n", "            ", "cm_ind", "=", "0", "\n", "cm_sizes", "=", "[", "]", "\n", "if", "context_mod_inputs", ":", "\n", "                ", "cm_sizes", ".", "append", "(", "n_in", ")", "\n", "", "cm_sizes", ".", "extend", "(", "hidden_layers", ")", "\n", "if", "not", "no_last_layer_context_mod", ":", "\n", "                ", "cm_sizes", ".", "append", "(", "n_out", ")", "\n", "\n", "", "for", "i", ",", "n", "in", "enumerate", "(", "cm_sizes", ")", ":", "\n", "                ", "cmod_layer", "=", "ContextModLayer", "(", "n", ",", "\n", "no_weights", "=", "context_mod_no_weights", ",", "\n", "apply_gain_offset", "=", "context_mod_gain_offset", ")", "\n", "self", ".", "_context_mod_layers", ".", "append", "(", "cmod_layer", ")", "\n", "\n", "self", ".", "param_shapes", ".", "extend", "(", "cmod_layer", ".", "param_shapes", ")", "\n", "self", ".", "_context_mod_shapes", ".", "extend", "(", "cmod_layer", ".", "param_shapes", ")", "\n", "if", "context_mod_no_weights", ":", "\n", "                    ", "self", ".", "_hyper_shapes_learned", ".", "extend", "(", "cmod_layer", ".", "param_shapes", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_weights", ".", "extend", "(", "cmod_layer", ".", "weights", ")", "\n", "\n", "# FIXME ugly code. Move initialization somewhere else.", "\n", "", "if", "not", "context_mod_no_weights", "and", "init_weights", "is", "not", "None", ":", "\n", "                    ", "assert", "(", "len", "(", "cmod_layer", ".", "weights", ")", "==", "2", ")", "\n", "for", "ii", "in", "range", "(", "2", ")", ":", "\n", "                        ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "cm_ind", "]", ".", "shape", ")", ",", "\n", "list", "(", "cm_ind", ".", "weights", "[", "ii", "]", ".", "shape", ")", ")", ")", ")", "\n", "cmod_layer", ".", "weights", "[", "ii", "]", ".", "data", "=", "init_weights", "[", "cm_ind", "]", "\n", "cm_ind", "+=", "1", "\n", "\n", "", "", "", "if", "init_weights", "is", "not", "None", ":", "\n", "                ", "init_weights", "=", "init_weights", "[", "cm_ind", ":", "]", "\n", "\n", "### Define and initialize batch norm weights.", "\n", "", "", "self", ".", "_batchnorm_layers", "=", "nn", ".", "ModuleList", "(", ")", "if", "use_batch_norm", "else", "None", "\n", "\n", "if", "use_batch_norm", ":", "\n", "            ", "if", "distill_bn_stats", ":", "\n", "                ", "self", ".", "_hyper_shapes_distilled", "=", "[", "]", "\n", "\n", "", "bn_ind", "=", "0", "\n", "for", "i", ",", "n", "in", "enumerate", "(", "hidden_layers", ")", ":", "\n", "                ", "bn_layer", "=", "BatchNormLayer", "(", "n", ",", "affine", "=", "not", "no_weights", ",", "\n", "track_running_stats", "=", "bn_track_stats", ")", "\n", "self", ".", "_batchnorm_layers", ".", "append", "(", "bn_layer", ")", "\n", "self", ".", "_param_shapes", ".", "extend", "(", "bn_layer", ".", "param_shapes", ")", "\n", "\n", "if", "no_weights", ":", "\n", "                    ", "self", ".", "_hyper_shapes_learned", ".", "extend", "(", "bn_layer", ".", "param_shapes", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "_weights", ".", "extend", "(", "bn_layer", ".", "weights", ")", "\n", "\n", "", "if", "distill_bn_stats", ":", "\n", "                    ", "self", ".", "_hyper_shapes_distilled", ".", "extend", "(", "[", "list", "(", "p", ".", "shape", ")", "for", "p", "in", "bn_layer", ".", "get_stats", "(", "0", ")", "]", ")", "\n", "\n", "# FIXME ugly code. Move initialization somewhere else.", "\n", "", "if", "not", "no_weights", "and", "init_weights", "is", "not", "None", ":", "\n", "                    ", "assert", "(", "len", "(", "bn_layer", ".", "weights", ")", "==", "2", ")", "\n", "for", "ii", "in", "range", "(", "2", ")", ":", "\n", "                        ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "bn_ind", "]", ".", "shape", ")", ",", "\n", "list", "(", "bn_layer", ".", "weights", "[", "ii", "]", ".", "shape", ")", ")", ")", ")", "\n", "bn_layer", ".", "weights", "[", "ii", "]", ".", "data", "=", "init_weights", "[", "bn_ind", "]", "\n", "bn_ind", "+=", "1", "\n", "\n", "", "", "", "if", "init_weights", "is", "not", "None", ":", "\n", "                ", "init_weights", "=", "init_weights", "[", "bn_ind", ":", "]", "\n", "\n", "# Compute shapes of linear layers.", "\n", "", "", "linear_shapes", "=", "MLP", ".", "weight_shapes", "(", "n_in", "=", "n_in", ",", "n_out", "=", "n_out", ",", "\n", "hidden_layers", "=", "hidden_layers", ",", "use_bias", "=", "use_bias", ")", "\n", "self", ".", "_param_shapes", ".", "extend", "(", "linear_shapes", ")", "\n", "\n", "num_weights", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "self", ".", "_param_shapes", ")", "\n", "\n", "if", "verbose", ":", "\n", "            ", "if", "use_context_mod", ":", "\n", "                ", "cm_num_weights", "=", "0", "\n", "for", "cm_layer", "in", "self", ".", "_context_mod_layers", ":", "\n", "                    ", "cm_num_weights", "+=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "cm_layer", ".", "param_shapes", ")", "\n", "\n", "", "", "print", "(", "'Creating an MLP with %d weights'", "%", "num_weights", "\n", "+", "(", "' (including %d weights associated with-'", "%", "cm_num_weights", "\n", "+", "'context modulation)'", "if", "use_context_mod", "else", "''", ")", "\n", "+", "'.'", "\n", "+", "(", "' The network uses dropout.'", "if", "dropout_rate", "!=", "-", "1", "else", "''", ")", "\n", "+", "(", "' The network uses batchnorm.'", "if", "use_batch_norm", "else", "''", ")", ")", "\n", "\n", "", "self", ".", "_layer_weight_tensors", "=", "nn", ".", "ParameterList", "(", ")", "\n", "self", ".", "_layer_bias_vectors", "=", "nn", ".", "ParameterList", "(", ")", "\n", "\n", "if", "no_weights", ":", "\n", "            ", "self", ".", "_hyper_shapes_learned", ".", "extend", "(", "linear_shapes", ")", "\n", "self", ".", "_is_properly_setup", "(", ")", "\n", "return", "\n", "\n", "### Define and initialize linear weights.", "\n", "", "for", "i", ",", "dims", "in", "enumerate", "(", "linear_shapes", ")", ":", "\n", "            ", "self", ".", "_weights", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "*", "dims", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "if", "len", "(", "dims", ")", "==", "1", ":", "\n", "                ", "self", ".", "_layer_bias_vectors", ".", "append", "(", "self", ".", "_weights", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "_layer_weight_tensors", ".", "append", "(", "self", ".", "_weights", "[", "-", "1", "]", ")", "\n", "\n", "", "", "if", "init_weights", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "init_weights", ")", "==", "len", "(", "linear_shapes", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "init_weights", ")", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "i", "]", ".", "shape", ")", ",", "\n", "linear_shapes", "[", "i", "]", ")", ")", ")", "\n", "if", "use_bias", ":", "\n", "                    ", "if", "i", "%", "2", "==", "0", ":", "\n", "                        ", "self", ".", "_layer_weight_tensors", "[", "i", "//", "2", "]", ".", "data", "=", "init_weights", "[", "i", "]", "\n", "", "else", ":", "\n", "                        ", "self", ".", "_layer_bias_vectors", "[", "i", "//", "2", "]", ".", "data", "=", "init_weights", "[", "i", "]", "\n", "", "", "else", ":", "\n", "                    ", "self", ".", "_layer_weight_tensors", "[", "i", "]", ".", "data", "=", "init_weights", "[", "i", "]", "\n", "", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_layer_weight_tensors", ")", ")", ":", "\n", "                ", "if", "use_bias", ":", "\n", "                    ", "init_params", "(", "self", ".", "_layer_weight_tensors", "[", "i", "]", ",", "\n", "self", ".", "_layer_bias_vectors", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                    ", "init_params", "(", "self", ".", "_layer_weight_tensors", "[", "i", "]", ")", "\n", "\n", "", "", "", "self", ".", "_is_properly_setup", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mlp.MLP.forward": [[366, 614], ["enumerate", "range", "Exception", "isinstance", "enumerate", "isinstance", "len", "enumerate", "range", "mlp.MLP._context_mod_layers[].forward", "len", "mlp.MLP._spec_norm", "mlp.MLP._context_mod_layers[].forward", "len", "len", "len", "numpy.all", "len", "b_weights.append", "w_weights.append", "ValueError", "len", "len", "numpy.all", "len", "enumerate", "torch.linear", "torch.linear", "torch.linear", "mlp.MLP._out_fn", "weights.keys", "weights.keys", "Exception", "Exception", "len", "len", "numpy.equal", "condition.keys", "condition.keys", "numpy.equal", "bn_layer.get_stats", "len", "mlp.MLP._context_mod_layers[].forward", "mlp.MLP._batchnorm_layers[].forward", "mlp.MLP._dropout", "mlp.MLP._a_fun", "mlp.MLP._context_mod_layers[].forward", "weights.keys", "weights.keys", "len", "len", "len", "list", "condition.keys", "condition.keys", "list"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.get_stats", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "forward", "(", "self", ",", "x", ",", "weights", "=", "None", ",", "distilled_params", "=", "None", ",", "condition", "=", "None", ")", ":", "\n", "        ", "\"\"\"Compute the output :math:`y` of this network given the input\n        :math:`x`.\n\n        Args:\n            (....): See docstring of method\n                :meth:`mnets.mnet_interface.MainNetInterface.forward`. We\n                provide some more specific information below.\n            weights (list or dict): If a list of parameter tensors is given and\n                context modulation is used (see argument ``use_context_mod`` in\n                constructor), then these parameters are interpreted as context-\n                modulation parameters if the length of ``weights`` equals\n                :code:`2*len(net.context_mod_layers)`. Otherwise, the length is\n                expected to be equal to the length of the attribute\n                :attr:`mnets.mnet_interface.MainNetInterface.param_shapes`.\n\n                Alternatively, a dictionary can be passed with the possible\n                keywords ``internal_weights`` and ``mod_weights``. Each keyword\n                is expected to map onto a list of tensors.\n                The keyword ``internal_weights`` refers to all weights of this\n                network except for the weights of the context-modulation layers.\n                The keyword ``mod_weights``, on the other hand, refers\n                specifically to the weights of the context-modulation layers.\n                It is not necessary to specify both keywords.\n            distilled_params: Will be passed as ``running_mean`` and\n                ``running_var`` arguments of method\n                :meth:`utils.batchnorm_layer.BatchNormLayer.forward` if\n                batch normalization is used.\n            condition (optional, int or dict): If ``int`` is provided, then this\n                argument will be passed as argument ``stats_id`` to the method\n                :meth:`utils.batchnorm_layer.BatchNormLayer.forward` if\n                batch normalization is used.\n\n                If a ``dict`` is provided instead, the following keywords are\n                allowed:\n\n                    - ``bn_stats_id``: Will be handled as ``stats_id`` of the\n                      batchnorm layers as described above.\n                    - ``cmod_ckpt_id``: Will be passed as argument ``ckpt_id``\n                      to the method\n                      :meth:`utils.context_mod_layer.ContextModLayer.forward`.\n\n        Returns:\n            (tuple): Tuple containing:\n\n            - **y**: The output of the network.\n            - **h_y** (optional): If ``out_fn`` was specified in the\n              constructor, then this value will be returned. It is the last\n              hidden activation (before the ``out_fn`` has been applied).\n        \"\"\"", "\n", "if", "(", "(", "not", "self", ".", "_use_context_mod", "and", "self", ".", "_no_weights", ")", "or", "(", "self", ".", "_no_weights", "or", "self", ".", "_context_mod_no_weights", ")", ")", "and", "weights", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Network was generated without weights. '", "+", "\n", "'Hence, \"weights\" option may not be None.'", ")", "\n", "\n", "############################################", "\n", "### Extract which weights should be used ###", "\n", "############################################", "\n", "# I.e., are we using internally maintained weights or externally given", "\n", "# ones or are we even mixing between these groups.", "\n", "", "n_cm", "=", "0", "if", "self", ".", "context_mod_layers", "is", "None", "else", "2", "*", "len", "(", "self", ".", "context_mod_layers", ")", "\n", "\n", "if", "weights", "is", "None", ":", "\n", "            ", "weights", "=", "self", ".", "weights", "\n", "\n", "if", "self", ".", "_use_context_mod", ":", "\n", "                ", "cm_weights", "=", "weights", "[", ":", "n_cm", "]", "\n", "int_weights", "=", "weights", "[", "n_cm", ":", "]", "\n", "", "else", ":", "\n", "                ", "int_weights", "=", "weights", "\n", "", "", "else", ":", "\n", "            ", "int_weights", "=", "None", "\n", "cm_weights", "=", "None", "\n", "\n", "if", "isinstance", "(", "weights", ",", "dict", ")", ":", "\n", "                ", "assert", "(", "'internal_weights'", "in", "weights", ".", "keys", "(", ")", "or", "'mod_weights'", "in", "weights", ".", "keys", "(", ")", ")", "\n", "if", "'internal_weights'", "in", "weights", ".", "keys", "(", ")", ":", "\n", "                    ", "int_weights", "=", "weights", "[", "'internal_weights'", "]", "\n", "", "if", "'mod_weights'", "in", "weights", ".", "keys", "(", ")", ":", "\n", "                    ", "cm_weights", "=", "weights", "[", "'mod_weights'", "]", "\n", "", "", "else", ":", "\n", "                ", "if", "self", ".", "_use_context_mod", "and", "len", "(", "weights", ")", "==", "n_cm", ":", "\n", "                    ", "cm_weights", "=", "weights", "\n", "", "else", ":", "\n", "                    ", "assert", "(", "len", "(", "weights", ")", "==", "len", "(", "self", ".", "param_shapes", ")", ")", "\n", "if", "self", ".", "_use_context_mod", ":", "\n", "                        ", "cm_weights", "=", "weights", "[", ":", "n_cm", "]", "\n", "int_weights", "=", "weights", "[", "n_cm", ":", "]", "\n", "", "else", ":", "\n", "                        ", "int_weights", "=", "weights", "\n", "\n", "", "", "", "if", "self", ".", "_use_context_mod", "and", "cm_weights", "is", "None", ":", "\n", "                ", "if", "self", ".", "_context_mod_no_weights", ":", "\n", "                    ", "raise", "Exception", "(", "'Network was generated without weights '", "+", "\n", "'for context-mod layers. Hence, they must be passed '", "+", "\n", "'via the \"weights\" option.'", ")", "\n", "", "cm_weights", "=", "self", ".", "weights", "[", ":", "n_cm", "]", "\n", "", "if", "int_weights", "is", "None", ":", "\n", "                ", "if", "self", ".", "_no_weights", ":", "\n", "                    ", "raise", "Exception", "(", "'Network was generated without internal '", "+", "\n", "'weights. Hence, they must be passed via the '", "+", "\n", "'\"weights\" option.'", ")", "\n", "", "if", "self", ".", "_context_mod_no_weights", ":", "\n", "                    ", "int_weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "                    ", "int_weights", "=", "self", ".", "weights", "[", "n_cm", ":", "]", "\n", "\n", "# Note, context-mod weights might have different shapes, as they", "\n", "# may be parametrized on a per-sample basis.", "\n", "", "", "if", "self", ".", "_use_context_mod", ":", "\n", "                ", "assert", "(", "len", "(", "cm_weights", ")", "==", "len", "(", "self", ".", "_context_mod_shapes", ")", ")", "\n", "", "int_shapes", "=", "self", ".", "param_shapes", "[", "n_cm", ":", "]", "\n", "assert", "(", "len", "(", "int_weights", ")", "==", "len", "(", "int_shapes", ")", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "int_shapes", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "s", ",", "list", "(", "int_weights", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "", "", "cm_ind", "=", "0", "\n", "bn_ind", "=", "0", "\n", "\n", "if", "self", ".", "_use_batch_norm", ":", "\n", "            ", "n_bn", "=", "2", "*", "len", "(", "self", ".", "batchnorm_layers", ")", "\n", "bn_weights", "=", "int_weights", "[", ":", "n_bn", "]", "\n", "layer_weights", "=", "int_weights", "[", "n_bn", ":", "]", "\n", "", "else", ":", "\n", "            ", "layer_weights", "=", "int_weights", "\n", "\n", "", "w_weights", "=", "[", "]", "\n", "b_weights", "=", "[", "]", "\n", "for", "i", ",", "p", "in", "enumerate", "(", "layer_weights", ")", ":", "\n", "            ", "if", "self", ".", "has_bias", "and", "i", "%", "2", "==", "1", ":", "\n", "                ", "b_weights", ".", "append", "(", "p", ")", "\n", "", "else", ":", "\n", "                ", "w_weights", ".", "append", "(", "p", ")", "\n", "\n", "########################", "\n", "### Parse condition ###", "\n", "#######################", "\n", "\n", "", "", "bn_cond", "=", "None", "\n", "cmod_cond", "=", "None", "\n", "\n", "if", "condition", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "condition", ",", "dict", ")", ":", "\n", "                ", "assert", "(", "'bn_stats_id'", "in", "condition", ".", "keys", "(", ")", "or", "'cmod_ckpt_id'", "in", "condition", ".", "keys", "(", ")", ")", "\n", "if", "'bn_stats_id'", "in", "condition", ".", "keys", "(", ")", ":", "\n", "                    ", "bn_cond", "=", "condition", "[", "'bn_stats_id'", "]", "\n", "", "if", "'cmod_ckpt_id'", "in", "condition", ".", "keys", "(", ")", ":", "\n", "                    ", "cmod_cond", "=", "condition", "[", "'cmod_ckpt_id'", "]", "\n", "", "", "else", ":", "\n", "                ", "bn_cond", "=", "condition", "\n", "\n", "######################################", "\n", "### Select batchnorm running stats ###", "\n", "######################################", "\n", "", "", "if", "self", ".", "_use_batch_norm", ":", "\n", "            ", "nn", "=", "len", "(", "self", ".", "_batchnorm_layers", ")", "\n", "running_means", "=", "[", "None", "]", "*", "nn", "\n", "running_vars", "=", "[", "None", "]", "*", "nn", "\n", "\n", "", "if", "distilled_params", "is", "not", "None", ":", "\n", "            ", "if", "not", "self", ".", "_distill_bn_stats", ":", "\n", "                ", "raise", "ValueError", "(", "'Argument \"distilled_params\" can only be '", "+", "\n", "'provided if the return value of '", "+", "\n", "'method \"distillation_targets()\" is not None.'", ")", "\n", "", "shapes", "=", "self", ".", "hyper_shapes_distilled", "\n", "assert", "(", "len", "(", "distilled_params", ")", "==", "len", "(", "shapes", ")", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "shapes", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "s", ",", "list", "(", "distilled_params", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "# Extract batchnorm stats from distilled_params", "\n", "", "for", "i", "in", "range", "(", "0", ",", "len", "(", "distilled_params", ")", ",", "2", ")", ":", "\n", "                ", "running_means", "[", "i", "//", "2", "]", "=", "distilled_params", "[", "i", "]", "\n", "running_vars", "[", "i", "//", "2", "]", "=", "distilled_params", "[", "i", "+", "1", "]", "\n", "\n", "", "", "elif", "self", ".", "_use_batch_norm", "and", "self", ".", "_bn_track_stats", "and", "bn_cond", "is", "None", ":", "\n", "            ", "for", "i", ",", "bn_layer", "in", "enumerate", "(", "self", ".", "_batchnorm_layers", ")", ":", "\n", "                ", "running_means", "[", "i", "]", ",", "running_vars", "[", "i", "]", "=", "bn_layer", ".", "get_stats", "(", ")", "\n", "\n", "###########################", "\n", "### Forward Computation ###", "\n", "###########################", "\n", "", "", "hidden", "=", "x", "\n", "\n", "# Context-dependent modulation of inputs directly.", "\n", "if", "self", ".", "_use_context_mod", "and", "self", ".", "_context_mod_inputs", ":", "\n", "            ", "hidden", "=", "self", ".", "_context_mod_layers", "[", "cm_ind", "]", ".", "forward", "(", "hidden", ",", "\n", "weights", "=", "cm_weights", "[", "2", "*", "cm_ind", ":", "2", "*", "cm_ind", "+", "2", "]", ",", "ckpt_id", "=", "cmod_cond", ")", "\n", "cm_ind", "+=", "1", "\n", "\n", "", "for", "l", "in", "range", "(", "len", "(", "w_weights", ")", ")", ":", "\n", "            ", "W", "=", "w_weights", "[", "l", "]", "\n", "if", "self", ".", "has_bias", ":", "\n", "                ", "b", "=", "b_weights", "[", "l", "]", "\n", "", "else", ":", "\n", "                ", "b", "=", "None", "\n", "\n", "# Linear layer.", "\n", "", "hidden", "=", "self", ".", "_spec_norm", "(", "F", ".", "linear", "(", "hidden", ",", "W", ",", "bias", "=", "b", ")", ")", "\n", "\n", "# Only for hidden layers.", "\n", "if", "l", "<", "len", "(", "w_weights", ")", "-", "1", ":", "\n", "# Context-dependent modulation (pre-activation).", "\n", "                ", "if", "self", ".", "_use_context_mod", "and", "not", "self", ".", "_context_mod_post_activation", ":", "\n", "                    ", "hidden", "=", "self", ".", "_context_mod_layers", "[", "cm_ind", "]", ".", "forward", "(", "hidden", ",", "\n", "weights", "=", "cm_weights", "[", "2", "*", "cm_ind", ":", "2", "*", "cm_ind", "+", "2", "]", ",", "\n", "ckpt_id", "=", "cmod_cond", ")", "\n", "cm_ind", "+=", "1", "\n", "\n", "# Batch norm", "\n", "", "if", "self", ".", "_use_batch_norm", ":", "\n", "                    ", "hidden", "=", "self", ".", "_batchnorm_layers", "[", "bn_ind", "]", ".", "forward", "(", "hidden", ",", "\n", "running_mean", "=", "running_means", "[", "bn_ind", "]", ",", "\n", "running_var", "=", "running_vars", "[", "bn_ind", "]", ",", "\n", "weight", "=", "bn_weights", "[", "2", "*", "bn_ind", "]", ",", "\n", "bias", "=", "bn_weights", "[", "2", "*", "bn_ind", "+", "1", "]", ",", "stats_id", "=", "bn_cond", ")", "\n", "bn_ind", "+=", "1", "\n", "\n", "# Dropout", "\n", "", "if", "self", ".", "_dropout_rate", "!=", "-", "1", ":", "\n", "                    ", "hidden", "=", "self", ".", "_dropout", "(", "hidden", ")", "\n", "\n", "# Non-linearity", "\n", "", "if", "self", ".", "_a_fun", "is", "not", "None", ":", "\n", "                    ", "hidden", "=", "self", ".", "_a_fun", "(", "hidden", ")", "\n", "\n", "# Context-dependent modulation (post-activation).", "\n", "", "if", "self", ".", "_use_context_mod", "and", "self", ".", "_context_mod_post_activation", ":", "\n", "                    ", "hidden", "=", "self", ".", "_context_mod_layers", "[", "cm_ind", "]", ".", "forward", "(", "hidden", ",", "\n", "weights", "=", "cm_weights", "[", "2", "*", "cm_ind", ":", "2", "*", "cm_ind", "+", "2", "]", ",", "\n", "ckpt_id", "=", "cmod_cond", ")", "\n", "cm_ind", "+=", "1", "\n", "\n", "# Context-dependent modulation in output layer.", "\n", "", "", "", "if", "self", ".", "_use_context_mod", "and", "not", "self", ".", "_no_last_layer_context_mod", ":", "\n", "            ", "hidden", "=", "self", ".", "_context_mod_layers", "[", "cm_ind", "]", ".", "forward", "(", "hidden", ",", "\n", "weights", "=", "cm_weights", "[", "2", "*", "cm_ind", ":", "2", "*", "cm_ind", "+", "2", "]", ",", "ckpt_id", "=", "cmod_cond", ")", "\n", "\n", "", "if", "self", ".", "_out_fn", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_out_fn", "(", "hidden", ")", ",", "hidden", "\n", "\n", "", "return", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mlp.MLP.distillation_targets": [[615, 637], ["ret.extend", "bn_layer.get_stats"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.batchnorm_layer.BatchNormLayer.get_stats"], ["", "def", "distillation_targets", "(", "self", ")", ":", "\n", "        ", "\"\"\"Targets to be distilled after training.\n\n        See docstring of abstract super method\n        :meth:`mnets.mnet_interface.MainNetInterface.distillation_targets`.\n\n        This method will return the current batch statistics of all batch\n        normalization layers if ``distill_bn_stats`` and ``use_batch_norm``\n        was set to ``True`` in the constructor.\n\n        Returns:\n            The target tensors corresponding to the shapes specified in\n            attribute :attr:`hyper_shapes_distilled`.\n        \"\"\"", "\n", "if", "self", ".", "hyper_shapes_distilled", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "ret", "=", "[", "]", "\n", "for", "bn_layer", "in", "self", ".", "_batchnorm_layers", ":", "\n", "            ", "ret", ".", "extend", "(", "bn_layer", ".", "get_stats", "(", ")", ")", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mlp.MLP.weight_shapes": [[638, 665], ["enumerate", "shapes.append", "shapes.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "weight_shapes", "(", "n_in", "=", "1", ",", "n_out", "=", "1", ",", "hidden_layers", "=", "[", "10", ",", "10", "]", ",", "use_bias", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the tensor shapes of all parameters in a fully-connected\n        network.\n\n        Args:\n            n_in: Number of inputs.\n            n_out: Number of output units.\n            hidden_layers: A list of ints, each number denoting the size of a\n                hidden layer.\n            use_bias: Whether the FC layers should have biases.\n\n        Returns:\n            A list of list of integers, denoting the shapes of the individual\n            parameter tensors.\n        \"\"\"", "\n", "shapes", "=", "[", "]", "\n", "\n", "prev_dim", "=", "n_in", "\n", "layer_out_sizes", "=", "hidden_layers", "+", "[", "n_out", "]", "\n", "for", "i", ",", "size", "in", "enumerate", "(", "layer_out_sizes", ")", ":", "\n", "            ", "shapes", ".", "append", "(", "[", "size", ",", "prev_dim", "]", ")", "\n", "if", "use_bias", ":", "\n", "                ", "shapes", ".", "append", "(", "[", "size", "]", ")", "\n", "", "prev_dim", "=", "size", "\n", "\n", "", "return", "shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_ewc.train_ewc": [[40, 108], ["print", "mnet.train", "torch.Adam", "range", "utils.compute_fisher", "print", "list", "mnet.parameters", "optim.Adam.zero_grad", "data.next_train_batch", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "mnet.forward", "torch.mse_loss", "loss.backward", "optim.Adam.step", "range", "toy_example.evaluate", "mnet.train", "print", "utils.ewc_regularizer", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer.compute_fisher", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.evaluate", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer.ewc_regularizer"], ["def", "train_ewc", "(", "task_id", ",", "data", ",", "mnet", ",", "device", ",", "config", ",", "writer", ")", ":", "\n", "    ", "\"\"\"Train the main network in a continual learning setup using the EWC\n    regularizer to prevent catastrophic forgetting.\n\n    loss = task_loss + beta * ewc_regularizer.\n\n    Args:\n        (....): See docstring of method :func:`toy_example.train.train_reg`.\n    \"\"\"", "\n", "print", "(", "'Training network ...'", ")", "\n", "\n", "mnet", ".", "train", "(", ")", "\n", "\n", "allowed_outputs", "=", "None", "\n", "if", "config", ".", "multi_head", ":", "\n", "        ", "n_y", "=", "data", ".", "out_shape", "[", "0", "]", "\n", "allowed_outputs", "=", "list", "(", "range", "(", "task_id", "*", "n_y", ",", "(", "task_id", "+", "1", ")", "*", "n_y", ")", ")", "\n", "\n", "", "optimizer", "=", "optim", ".", "Adam", "(", "mnet", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "lr_hyper", ")", "\n", "\n", "for", "i", "in", "range", "(", "config", ".", "n_iter", ")", ":", "\n", "### Evaluate network.", "\n", "# We test the network before we run the training iteration.", "\n", "# That way, we can see the initial performance of the untrained network.", "\n", "        ", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "train_cl", ".", "evaluate", "(", "task_id", ",", "data", ",", "mnet", ",", "None", ",", "device", ",", "config", ",", "writer", ",", "\n", "i", ")", "\n", "mnet", ".", "train", "(", ")", "\n", "\n", "", "if", "i", "%", "100", "==", "0", ":", "\n", "            ", "print", "(", "'Training iteration: %d.'", "%", "i", ")", "\n", "\n", "### Train theta.", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "batch", "=", "data", ".", "next_train_batch", "(", "config", ".", "batch_size", ")", "\n", "X", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ",", "mode", "=", "'train'", ")", "\n", "T", "=", "data", ".", "output_to_torch_tensor", "(", "batch", "[", "1", "]", ",", "device", ",", "mode", "=", "'train'", ")", "\n", "\n", "Y", "=", "mnet", ".", "forward", "(", "X", ")", "\n", "if", "config", ".", "multi_head", ":", "\n", "            ", "Y", "=", "Y", "[", ":", ",", "allowed_outputs", "]", "\n", "\n", "# Task-specific loss.", "\n", "", "loss_task", "=", "F", ".", "mse_loss", "(", "Y", ",", "T", ")", "\n", "\n", "loss_reg", "=", "0", "\n", "\n", "if", "task_id", ">", "0", "and", "config", ".", "beta", ">", "0", ":", "\n", "            ", "loss_reg", "=", "ewc", ".", "ewc_regularizer", "(", "task_id", ",", "mnet", ".", "weights", ",", "mnet", ",", "\n", "online", "=", "config", ".", "online_ewc", ",", "gamma", "=", "config", ".", "gamma", ")", "\n", "\n", "", "loss", "=", "loss_task", "+", "config", ".", "beta", "*", "loss_reg", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "i", "%", "10", "==", "0", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'train/task_%d/mse_loss'", "%", "task_id", ",", "loss_task", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/regularizer'", "%", "task_id", ",", "loss_reg", ",", "\n", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/full_loss'", "%", "task_id", ",", "loss", ",", "i", ")", "\n", "\n", "## Estimate diagonal Fisher elements.", "\n", "", "", "ewc", ".", "compute_fisher", "(", "task_id", ",", "data", ",", "mnet", ".", "weights", ",", "device", ",", "mnet", ",", "\n", "empirical_fisher", "=", "True", ",", "online", "=", "config", ".", "online_ewc", ",", "gamma", "=", "config", ".", "gamma", ",", "\n", "n_max", "=", "config", ".", "n_fisher", ",", "regression", "=", "True", ",", "allowed_outputs", "=", "allowed_outputs", ")", "\n", "\n", "print", "(", "'Training network ... Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_ewc.run": [[109, 157], ["toy_example.train_utils.parse_cmd_arguments", "toy_example.train_utils._setup_environment", "toy_example.train_utils._generate_tasks", "toy_example.train_utils._generate_networks", "range", "print", "print", "print", "writer.close", "print", "numpy.ones", "print", "train_ewc.train_ewc", "toy_example.test", "toy_example.train_utils._generate_networks", "numpy.array2string", "numpy.array2string", "current_mse.mean", "current_mse.std"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils.parse_cmd_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._setup_environment", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_tasks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_networks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_ewc.train_ewc", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_networks", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["", "def", "run", "(", ")", ":", "\n", "    ", "\"\"\"Run the script\n\n    Returns:\n        final_mse: Final MSE for each task.\n        immediate_mse: MSE achieved directly after training on each task.\n    \"\"\"", "\n", "config", "=", "train_utils", ".", "parse_cmd_arguments", "(", "mode", "=", "'train_ewc_regression'", ")", "\n", "\n", "device", ",", "writer", "=", "train_utils", ".", "_setup_environment", "(", "config", ")", "\n", "\n", "### Create tasks.", "\n", "dhandlers", ",", "num_tasks", "=", "train_utils", ".", "_generate_tasks", "(", "config", ")", "\n", "\n", "### Generate networks.", "\n", "mnet", ",", "_", ",", "_", "=", "train_utils", ".", "_generate_networks", "(", "config", ",", "dhandlers", ",", "\n", "device", ",", "create_hnet", "=", "False", ",", "create_rnet", "=", "False", ")", "\n", "\n", "### Train on tasks sequentially.", "\n", "immediate_mse", "=", "np", ".", "ones", "(", "num_tasks", ")", "*", "-", "1.", "\n", "\n", "for", "i", "in", "range", "(", "num_tasks", ")", ":", "\n", "        ", "print", "(", "'### Training on task %d ###'", "%", "(", "i", "+", "1", ")", ")", "\n", "data", "=", "dhandlers", "[", "i", "]", "\n", "# Train the network.", "\n", "train_ewc", "(", "i", ",", "data", ",", "mnet", ",", "device", ",", "config", ",", "writer", ")", "\n", "\n", "### Test networks.", "\n", "current_mse", ",", "immediate_mse", ",", "_", "=", "train_cl", ".", "test", "(", "dhandlers", "[", ":", "(", "i", "+", "1", ")", "]", ",", "mnet", ",", "None", ",", "device", ",", "config", ",", "writer", ",", "rnet", "=", "None", ",", "\n", "immediate_mse", "=", "immediate_mse", ")", "\n", "\n", "if", "config", ".", "train_from_scratch", ":", "\n", "            ", "mnet", ",", "_", ",", "_", "=", "train_utils", ".", "_generate_networks", "(", "config", ",", "dhandlers", ",", "\n", "device", ",", "create_hnet", "=", "False", ",", "create_rnet", "=", "False", ")", "\n", "\n", "", "", "print", "(", "'Immediate MSE values after training each task: %s'", "%", "np", ".", "array2string", "(", "immediate_mse", ",", "precision", "=", "5", ",", "separator", "=", "','", ")", ")", "\n", "print", "(", "'Final MSE values after training on all tasks: %s'", "%", "np", ".", "array2string", "(", "current_mse", ",", "precision", "=", "5", ",", "separator", "=", "','", ")", ")", "\n", "print", "(", "'Final MSE mean %.4f (std %.4f).'", "%", "(", "current_mse", ".", "mean", "(", ")", ",", "\n", "current_mse", ".", "std", "(", ")", ")", ")", "\n", "\n", "writer", ".", "close", "(", ")", "\n", "\n", "print", "(", "'Program finished successfully.'", ")", "\n", "\n", "return", "current_mse", ",", "immediate_mse", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData.__init__": [[120, 181], ["numpy.array", "data.dataset.Dataset.__init__", "warnings.warn", "numpy.random.RandomState.multivariate_normal", "numpy.random.RandomState.multivariate_normal", "numpy.vstack", "numpy.vstack", "numpy.arange", "numpy.arange", "numpy.eye", "numpy.random.RandomState", "map_function", "map_function", "map_function", "map_function", "scipy.stats.multivariate_normal.pdf().reshape", "scipy.stats.multivariate_normal.pdf"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ",", "mean", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", ",", "cov", "=", "0.05", "**", "2", "*", "np", ".", "eye", "(", "2", ")", ",", "\n", "num_train", "=", "100", ",", "num_test", "=", "100", ",", "map_function", "=", "None", ",", "rseed", "=", "None", ")", ":", "\n", "        ", "\"\"\"Generate a new dataset.\n\n        The input data x for train and test samples will be drawn iid from the\n        given Gaussian. Per default, the map function is the probability\n        density of the given Gaussian: y = f(x) = p(x).\n\n        Args:\n            mean: The mean of the Gaussian.\n            cov: The covariance of the Gaussian.\n            num_train: Number of training samples.\n            num_test: Number of test samples.\n            map_function (optional): A function handle that receives input\n                samples and maps them to output samples. If not specified, the\n                density function will be used as map function.\n            rseed: If None, the current random state of numpy is used to\n                generate the data. Otherwise, a new random state with the given\n                seed is generated.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "warn", "(", "'Please use class '", "+", "\n", "'\"data.special.gaussian_mixture_data.GaussianData\" instead.'", ",", "\n", "DeprecationWarning", ")", "\n", "\n", "if", "rseed", "is", "None", ":", "\n", "            ", "rand", "=", "np", ".", "random", "\n", "", "else", ":", "\n", "            ", "rand", "=", "np", ".", "random", ".", "RandomState", "(", "rseed", ")", "\n", "\n", "", "n_x", "=", "mean", ".", "size", "\n", "assert", "(", "n_x", "==", "2", ")", "# Only required when using plotting functions.", "\n", "\n", "train_x", "=", "rand", ".", "multivariate_normal", "(", "mean", ",", "cov", ",", "size", "=", "num_train", ")", "\n", "test_x", "=", "rand", ".", "multivariate_normal", "(", "mean", ",", "cov", ",", "size", "=", "num_test", ")", "\n", "\n", "if", "map_function", "is", "None", ":", "\n", "            ", "map_function", "=", "lambda", "x", ":", "multivariate_normal", ".", "pdf", "(", "x", ",", "mean", ",", "cov", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "\n", "# f(x) = p(x)", "\n", "train_y", "=", "map_function", "(", "train_x", ")", "\n", "test_y", "=", "map_function", "(", "test_x", ")", "\n", "", "else", ":", "\n", "            ", "train_y", "=", "map_function", "(", "train_x", ")", "\n", "test_y", "=", "map_function", "(", "test_x", ")", "\n", "\n", "# Specify internal data structure.", "\n", "", "self", ".", "_data", "[", "'classification'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'in_data'", "]", "=", "np", ".", "vstack", "(", "[", "train_x", ",", "test_x", "]", ")", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "n_x", "]", "\n", "self", ".", "_data", "[", "'out_data'", "]", "=", "np", ".", "vstack", "(", "[", "train_y", ",", "test_y", "]", ")", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "1", "]", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ")", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ",", "num_train", "+", "num_test", ")", "\n", "\n", "self", ".", "_mean", "=", "mean", "\n", "self", ".", "_cov", "=", "cov", "\n", "self", ".", "_map", "=", "map_function", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData.mean": [[182, 186], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "mean", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute mean.\"\"\"", "\n", "return", "self", ".", "_mean", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData.cov": [[187, 191], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "cov", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute cov.\"\"\"", "\n", "return", "self", ".", "_cov", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData.get_identifier": [[192, 195], ["None"], "methods", ["None"], ["", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'GaussianInputData'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData.plot_samples": [[196, 253], ["matplotlib.figure", "matplotlib.title", "gaussian_mixture_data.GaussianData._get_function_vals", "matplotlib.contourf", "matplotlib.colorbar", "matplotlib.legend", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.ion", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.savefig", "matplotlib.show", "matplotlib.contourf.cmap", "matplotlib.contourf.cmap", "matplotlib.contourf.norm", "matplotlib.contourf.norm", "outputs.squeeze", "predictions.squeeze"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals"], ["", "def", "plot_samples", "(", "self", ",", "title", ",", "inputs", ",", "outputs", "=", "None", ",", "predictions", "=", "None", ",", "\n", "num_samples_per_row", "=", "4", ",", "show", "=", "True", ",", "filename", "=", "None", ",", "\n", "interactive", "=", "False", ",", "figsize", "=", "(", "10", ",", "6", ")", ")", ":", "\n", "        ", "\"\"\"Plot samples belonging to this dataset.\n\n        Note, either \"outputs\" or \"predictions\" must be not None!\n\n        Args:\n            title: The title of the whole figure.\n            inputs: A 2D numpy array, where each row is an input sample.\n            outputs (optional): A 2D numpy array of actual dataset targets.\n            predictions (optional): A 2D numpy array of predicted output\n                samples (i.e., output predicted by a neural network).\n            num_samples_per_row: Maximum number of samples plotted\n                per row in the generated figure.\n            show: Whether the plot should be shown.\n            filename (optional): If provided, the figure will be stored under\n                this filename.\n            interactive: Turn on interactive mode. We mainly\n                use this option to ensure that the program will run in\n                background while figure is displayed. The figure will be\n                displayed until another one is displayed, the user closes it or\n                the program has terminated. If this option is deactivated, the\n                program will freeze until the user closes the figure.\n                Note, if using the iPython inline backend, this option has no\n                effect.\n            figsize: A tuple, determining the size of the\n                figure in inches.\n        \"\"\"", "\n", "assert", "(", "outputs", "is", "not", "None", "or", "predictions", "is", "not", "None", ")", "\n", "\n", "plt", ".", "figure", "(", "figsize", "=", "figsize", ")", "\n", "plt", ".", "title", "(", "title", ",", "size", "=", "20", ")", "\n", "if", "interactive", ":", "\n", "            ", "plt", ".", "ion", "(", ")", "\n", "\n", "", "X1", ",", "X2", ",", "Y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "f", "=", "plt", ".", "contourf", "(", "X1", ",", "X2", ",", "Y", ")", "\n", "plt", ".", "colorbar", "(", "f", ")", "\n", "\n", "if", "outputs", "is", "not", "None", ":", "\n", "            ", "plt", ".", "scatter", "(", "inputs", "[", ":", ",", "0", "]", ",", "inputs", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'b'", ",", "\n", "label", "=", "'Targets'", ",", "\n", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "outputs", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "", "if", "predictions", "is", "not", "None", ":", "\n", "            ", "plt", ".", "scatter", "(", "inputs", "[", ":", ",", "0", "]", ",", "inputs", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'r'", ",", "\n", "label", "=", "'Predictions'", ",", "\n", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "predictions", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "xlabel", "(", "'x1'", ")", "\n", "plt", ".", "ylabel", "(", "'x2'", ")", "\n", "\n", "if", "filename", "is", "not", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "filename", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n", "", "if", "show", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData._plot_sample": [[254, 261], ["NotImplementedError"], "methods", ["None"], ["", "", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Not implemented\"\"\"", "\n", "# We overwrote the plot_samples method, so there is no need to ever call", "\n", "# this method (it's just here because the baseclass requires its", "\n", "# existence).", "\n", "raise", "NotImplementedError", "(", "'TODO implement'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData.plot_dataset": [[262, 295], ["matplotlib.subplots", "gaussian_mixture_data.GaussianData.get_train_inputs", "gaussian_mixture_data.GaussianData.get_train_outputs().squeeze", "gaussian_mixture_data.GaussianData.get_test_inputs", "gaussian_mixture_data.GaussianData.get_test_outputs().squeeze", "gaussian_mixture_data.GaussianData._get_function_vals", "matplotlib.contourf", "matplotlib.colorbar", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.legend", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "gaussian_mixture_data.GaussianData.get_train_outputs", "gaussian_mixture_data.GaussianData.get_test_outputs", "matplotlib.contourf.cmap", "matplotlib.contourf.cmap", "matplotlib.contourf.norm", "matplotlib.contourf.norm"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs"], ["", "def", "plot_dataset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Plot the whole dataset.\"\"\"", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", "\n", "train_y", "=", "self", ".", "get_train_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", "\n", "test_y", "=", "self", ".", "get_test_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "X1", ",", "X2", ",", "Y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "heatmap", "=", "plt", ".", "contourf", "(", "X1", ",", "X2", ",", "Y", ")", "\n", "plt", ".", "colorbar", "(", "heatmap", ")", "\n", "\n", "#plt.scatter(train_x[:, 0], train_x[:, 1], edgecolors='r', label='Train',", "\n", "#            facecolors='none')", "\n", "#plt.scatter(test_x[:, 0], test_x[:, 1], edgecolors='b', label='Test',", "\n", "#            facecolors='none')", "\n", "\n", "# In case outputs might be noisy, we draw facecolors to match the", "\n", "# output value rather than drawing circles with no fill.", "\n", "plt", ".", "scatter", "(", "train_x", "[", ":", ",", "0", "]", ",", "train_x", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'r'", ",", "label", "=", "'train'", ",", "\n", "facecolor", "=", "heatmap", ".", "cmap", "(", "heatmap", ".", "norm", "(", "train_y", ")", ")", ")", "\n", "plt", ".", "scatter", "(", "test_x", "[", ":", ",", "0", "]", ",", "test_x", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'b'", ",", "label", "=", "'test'", ",", "\n", "facecolor", "=", "heatmap", ".", "cmap", "(", "heatmap", ".", "norm", "(", "test_y", ")", ")", ")", "\n", "\n", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "title", "(", "'Gaussian Input Dataset'", ")", "\n", "plt", ".", "xlabel", "(", "'x1'", ")", "\n", "plt", ".", "ylabel", "(", "'x2'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "return", "fig", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData._get_function_vals": [[296, 327], ["gaussian_mixture_data.GaussianData.get_train_inputs", "gaussian_mixture_data.GaussianData.get_test_inputs", "max", "numpy.linspace", "numpy.linspace", "numpy.meshgrid", "gaussian_mixture_data.GaussianData._map().reshape", "numpy.abs().max", "numpy.abs().max", "numpy.vstack", "gaussian_mixture_data.GaussianData._map", "numpy.abs", "numpy.abs", "X1.ravel", "X2.ravel"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs"], ["", "def", "_get_function_vals", "(", "self", ",", "grid_size", "=", "100", ")", ":", "\n", "        ", "\"\"\"Get real function values for a grid of equidistant x values in a\n        range that covers the test and training data. These values can be used\n        to plot the ground truth function.\n\n        Args:\n            grid_size: Width (or height) of the quadratic grid.\n\n        Returns:\n            X1, X2, Y: Three numpy arrays (2d) containing the corresponding x\n                and y values. X1 and X2 follow the np.meshgrid definition.\n        \"\"\"", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", "\n", "\n", "mu", "=", "self", ".", "_mean", "\n", "\n", "dx", "=", "max", "(", "np", ".", "abs", "(", "train_x", "-", "mu", "[", "None", ",", ":", "]", ")", ".", "max", "(", ")", ",", "\n", "np", ".", "abs", "(", "test_x", "-", "mu", "[", "None", ",", ":", "]", ")", ".", "max", "(", ")", ")", "\n", "dx", "=", "1.05", "*", "dx", "\n", "\n", "x1", "=", "np", ".", "linspace", "(", "start", "=", "mu", "[", "0", "]", "-", "dx", ",", "stop", "=", "mu", "[", "0", "]", "+", "dx", ",", "num", "=", "grid_size", ")", "\n", "x2", "=", "np", ".", "linspace", "(", "start", "=", "mu", "[", "1", "]", "-", "dx", ",", "stop", "=", "mu", "[", "1", "]", "+", "dx", ",", "num", "=", "grid_size", ")", "\n", "\n", "X1", ",", "X2", "=", "np", ".", "meshgrid", "(", "x1", ",", "x2", ")", "\n", "\n", "X", "=", "np", ".", "vstack", "(", "[", "X1", ".", "ravel", "(", ")", ",", "X2", ".", "ravel", "(", ")", "]", ")", ".", "T", "\n", "\n", "Y", "=", "self", ".", "_map", "(", "X", ")", ".", "reshape", "(", "X1", ".", "shape", ")", "\n", "\n", "return", "X1", ",", "X2", ",", "Y", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData.plot_predictions": [[328, 363], ["gaussian_mixture_data.GaussianData.get_train_inputs", "gaussian_mixture_data.GaussianData.get_train_outputs().squeeze", "gaussian_mixture_data.GaussianData.get_test_inputs", "gaussian_mixture_data.GaussianData.get_test_outputs().squeeze", "gaussian_mixture_data.GaussianData._get_function_vals", "matplotlib.contourf", "matplotlib.colorbar", "matplotlib.scatter", "matplotlib.legend", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "matplotlib.scatter", "matplotlib.scatter", "gaussian_mixture_data.GaussianData.get_train_outputs", "gaussian_mixture_data.GaussianData.get_test_outputs", "matplotlib.contourf.cmap", "matplotlib.contourf.cmap", "matplotlib.contourf.cmap", "matplotlib.contourf.norm", "matplotlib.contourf.norm", "matplotlib.contourf.norm", "predictions[].squeeze", "gaussian_mixture_data.GaussianData.squeeze", "gaussian_mixture_data.GaussianData.squeeze"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs"], ["", "def", "plot_predictions", "(", "self", ",", "predictions", ",", "label", "=", "'Pred'", ",", "show_train", "=", "True", ",", "\n", "show_test", "=", "True", ")", ":", "\n", "        ", "\"\"\"Plot the dataset as well as predictions.\n\n        Args:\n            predictions: A tuple of x and y values, where the y values are\n                computed by a trained regression network. Note, that x is\n                supposed to be 2D numpy array, whereas y is a 1D numpy array.\n            label: Label of the predicted values as shown in the legend.\n            show_train: Show train samples.\n            show_test: Show test samples.\n        \"\"\"", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", "\n", "train_y", "=", "self", ".", "get_train_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", "\n", "test_y", "=", "self", ".", "get_test_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "X1", ",", "X2", ",", "Y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "f", "=", "plt", ".", "contourf", "(", "X1", ",", "X2", ",", "Y", ")", "\n", "plt", ".", "colorbar", "(", "f", ")", "\n", "\n", "if", "show_train", ":", "\n", "            ", "plt", ".", "scatter", "(", "train_x", "[", ":", ",", "0", "]", ",", "train_x", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'r'", ",", "\n", "label", "=", "'Train'", ",", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "train_y", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "", "if", "show_test", ":", "\n", "            ", "plt", ".", "scatter", "(", "test_x", "[", ":", ",", "0", "]", ",", "test_x", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'b'", ",", "\n", "label", "=", "'Test'", ",", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "test_y", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "", "plt", ".", "scatter", "(", "predictions", "[", "0", "]", "[", ":", ",", "0", "]", ",", "predictions", "[", "0", "]", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'g'", ",", "\n", "label", "=", "label", ",", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "predictions", "[", "1", "]", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "title", "(", "'Gaussian Input Dataset'", ")", "\n", "plt", ".", "xlabel", "(", "'x1'", ")", "\n", "plt", ".", "ylabel", "(", "'x2'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.GaussianData.plot_datasets": [[364, 486], ["len", "matplotlib.subplots", "matplotlib.title", "numpy.zeros", "numpy.zeros", "range", "min_x.min.min.min", "max_x.max.max.max", "numpy.linspace", "numpy.linspace", "numpy.meshgrid", "scipy.spatial.cKDTree", "scipy.spatial.cKDTree.query", "numpy.empty", "range", "Y.reshape.reshape.reshape", "matplotlib.contourf", "matplotlib.colorbar", "matplotlib.pyplot.cm.rainbow", "enumerate", "matplotlib.legend", "matplotlib.xlabel", "matplotlib.ylabel", "data.get_train_inputs", "data.get_test_inputs", "max", "numpy.vstack", "data_handlers[]._map().squeeze", "numpy.linspace", "matplotlib.savefig", "matplotlib.show", "numpy.abs().max", "numpy.abs().max", "matplotlib.scatter", "phandlers.append", "plabels.append", "len", "len", "len", "X1.ravel", "X2.ravel", "data_handlers[]._map", "numpy.abs", "numpy.abs", "matplotlib.contourf.cmap", "matplotlib.contourf.norm", "predictions[].squeeze"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs"], ["", "@", "staticmethod", "\n", "def", "plot_datasets", "(", "data_handlers", ",", "inputs", "=", "None", ",", "predictions", "=", "None", ",", "labels", "=", "None", ",", "\n", "show", "=", "True", ",", "filename", "=", "None", ",", "figsize", "=", "(", "10", ",", "6", ")", ")", ":", "\n", "        ", "\"\"\"Plot several datasets of this class in one plot.\n\n        Args:\n            data_handlers: A list of GaussianData objects.\n            inputs (optional): A list of numpy arrays representing inputs for\n                each dataset.\n            predictions (optional): A list of numpy arrays containing the\n                predicted output values for the given input values.\n            labels (optional): A label for each dataset.\n            show: Whether the plot should be shown.\n            filename (optional): If provided, the figure will be stored under\n                this filename.\n            figsize: A tuple, determining the size of the\n                figure in inches.\n        \"\"\"", "\n", "n", "=", "len", "(", "data_handlers", ")", "\n", "assert", "(", "(", "inputs", "is", "None", "and", "predictions", "is", "None", ")", "or", "(", "inputs", "is", "not", "None", "and", "predictions", "is", "not", "None", ")", ")", "\n", "assert", "(", "(", "inputs", "is", "None", "or", "len", "(", "inputs", ")", "==", "n", ")", "and", "(", "predictions", "is", "None", "or", "len", "(", "predictions", ")", "==", "n", ")", "and", "(", "labels", "is", "None", "or", "len", "(", "labels", ")", "==", "n", ")", ")", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "figsize", ")", "\n", "#plt.figure(figsize=figsize)", "\n", "plt", ".", "title", "(", "'GaussianMixture tasks'", ",", "size", "=", "20", ")", "\n", "\n", "# We need to produce a heatmap that spans all tasks.", "\n", "min_x", "=", "np", ".", "zeros", "(", "(", "2", ",", "n", ")", ")", "\n", "max_x", "=", "np", ".", "zeros", "(", "(", "2", ",", "n", ")", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "data", "=", "data_handlers", "[", "i", "]", "\n", "\n", "train_x", "=", "data", ".", "get_train_inputs", "(", ")", "\n", "test_x", "=", "data", ".", "get_test_inputs", "(", ")", "\n", "mu", "=", "data", ".", "_mean", "\n", "\n", "#dx = np.abs(np.vstack([train_x, test_x]) - mu[None, :]).max(axis=0)", "\n", "dx", "=", "max", "(", "np", ".", "abs", "(", "train_x", "-", "mu", "[", "None", ",", ":", "]", ")", ".", "max", "(", ")", ",", "\n", "np", ".", "abs", "(", "test_x", "-", "mu", "[", "None", ",", ":", "]", ")", ".", "max", "(", ")", ")", "\n", "\n", "min_x", "[", ":", ",", "i", "]", "=", "mu", "-", "dx", "\n", "max_x", "[", ":", ",", "i", "]", "=", "mu", "+", "dx", "\n", "\n", "", "min_x", "=", "min_x", ".", "min", "(", "axis", "=", "1", ")", "\n", "max_x", "=", "max_x", ".", "max", "(", "axis", "=", "1", ")", "\n", "\n", "slack", "=", "(", "max_x", "-", "min_x", ")", "*", "0.02", "\n", "min_x", "-=", "slack", "\n", "max_x", "+=", "slack", "\n", "\n", "grid_size", "=", "1000", "\n", "x1", "=", "np", ".", "linspace", "(", "start", "=", "min_x", "[", "0", "]", ",", "stop", "=", "max_x", "[", "0", "]", ",", "num", "=", "grid_size", ")", "\n", "x2", "=", "np", ".", "linspace", "(", "start", "=", "min_x", "[", "1", "]", ",", "stop", "=", "max_x", "[", "1", "]", ",", "num", "=", "grid_size", ")", "\n", "\n", "X1", ",", "X2", "=", "np", ".", "meshgrid", "(", "x1", ",", "x2", ")", "\n", "X", "=", "np", ".", "vstack", "(", "[", "X1", ".", "ravel", "(", ")", ",", "X2", ".", "ravel", "(", ")", "]", ")", ".", "T", "\n", "\n", "# Problem: Now that we have the underlying X mesh, how do we compute the", "\n", "# heatmap. Since every Gaussian has full support, we can't draw a", "\n", "# heatmap that displays all tasks with their correct Y value.", "\n", "# One options would be to just add all heat maps. For small variances", "\n", "# this would look \"almost\" correct.", "\n", "# Another option is to compute Voronoi cells for all tasks and compute", "\n", "# at each mesh position the y value corresponding to the task with the", "\n", "# nearest mean.", "\n", "\n", "# We decide here to compute y based on the nearest neighor, as this", "\n", "# seems to be the \"most correct\" plotting option.", "\n", "\n", "means", "=", "[", "d", ".", "_mean", "for", "d", "in", "data_handlers", "]", "\n", "\n", "# Plot Voronoi diagram for debugging.", "\n", "#from scipy.spatial import Voronoi, voronoi_plot_2d", "\n", "#vor = Voronoi(means)", "\n", "#voronoi_plot_2d(vor)", "\n", "\n", "vor_tree", "=", "cKDTree", "(", "means", ")", "\n", "_", ",", "minds", "=", "vor_tree", ".", "query", "(", "X", ")", "\n", "\n", "Y", "=", "np", ".", "empty", "(", "X", ".", "shape", "[", "0", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "mask", "=", "minds", "==", "i", "\n", "Y", "[", "mask", "]", "=", "data_handlers", "[", "i", "]", ".", "_map", "(", "X", "[", "mask", ",", ":", "]", ")", ".", "squeeze", "(", ")", "\n", "\n", "", "Y", "=", "Y", ".", "reshape", "(", "X1", ".", "shape", ")", "\n", "\n", "f", "=", "plt", ".", "contourf", "(", "X1", ",", "X2", ",", "Y", ")", "\n", "plt", ".", "colorbar", "(", "f", ")", "\n", "\n", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "n", ")", ")", "\n", "\n", "phandlers", "=", "[", "]", "\n", "plabels", "=", "[", "]", "\n", "\n", "for", "i", ",", "data", "in", "enumerate", "(", "data_handlers", ")", ":", "\n", "            ", "if", "labels", "is", "not", "None", ":", "\n", "                ", "lbl", "=", "labels", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "lbl", "=", "'Predictions %d'", "%", "i", "\n", "\n", "", "if", "inputs", "is", "not", "None", ":", "\n", "                ", "p", "=", "plt", ".", "scatter", "(", "inputs", "[", "i", "]", "[", ":", ",", "0", "]", ",", "inputs", "[", "i", "]", "[", ":", ",", "1", "]", ",", "\n", "edgecolors", "=", "colors", "[", "i", "]", ",", "\n", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "predictions", "[", "i", "]", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "phandlers", ".", "append", "(", "p", ")", "\n", "plabels", ".", "append", "(", "lbl", ")", "\n", "\n", "", "", "plt", ".", "legend", "(", "phandlers", ",", "plabels", ")", "\n", "plt", ".", "xlabel", "(", "'x1'", ")", "\n", "plt", ".", "ylabel", "(", "'x2'", ")", "\n", "\n", "if", "filename", "is", "not", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "filename", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n", "", "if", "show", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n", "", "return", "fig", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.gaussian_mixture_data.get_gmm_tasks": [[65, 104], ["warnings.warn", "range", "len", "len", "len", "ret.append", "len", "len", "len", "gaussian_mixture_data.GaussianData"], "function", ["None"], ["def", "get_gmm_tasks", "(", "means", "=", "DEFAULT_MEANS", ",", "covs", "=", "DEFAULT_VARIANCES", ",", "num_train", "=", "100", ",", "\n", "num_test", "=", "100", ",", "map_functions", "=", "None", ",", "rseed", "=", "None", ")", ":", "\n", "    ", "\"\"\"Generate a set of data handlers (one for each task) of class\n    \"GaussianData\".\n\n    .. deprecated:: 1.0\n        Please use function\n        :func:`data.special.gaussian_mixture_data.get_gmm_tasks` instead.\n\n    Args:\n        means: The mean of each Gaussian.\n        covs: The covariance matrix of each Gaussian.\n        num_train: Number of training samples per task.\n        num_test: Number of test samples per task.\n        map_functions (optional): A list of \"map_function\"s, one for each task.\n        rseed: If None, the current random state of numpy is used to generate\n            the data. Otherwise, a new random state with the given seed is\n            generated.\n\n    Returns:\n        A list of objects of class \"GaussianData\".\n    \"\"\"", "\n", "warn", "(", "'Please use function '", "+", "\n", "'\"data.special.gaussian_mixture_data.get_gmm_tasks\" instead.'", ",", "\n", "DeprecationWarning", ")", "\n", "\n", "assert", "(", "len", "(", "means", ")", "==", "len", "(", "covs", ")", ")", "\n", "\n", "if", "map_functions", "is", "None", ":", "\n", "        ", "map_functions", "=", "[", "None", "]", "*", "len", "(", "means", ")", "\n", "", "else", ":", "\n", "        ", "assert", "(", "len", "(", "map_functions", ")", "==", "len", "(", "means", ")", ")", "\n", "\n", "", "ret", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "means", ")", ")", ":", "\n", "        ", "ret", ".", "append", "(", "GaussianData", "(", "mean", "=", "means", "[", "i", "]", ",", "cov", "=", "covs", "[", "i", "]", ",", "num_train", "=", "num_train", ",", "\n", "num_test", "=", "num_test", ",", "map_function", "=", "map_functions", "[", "i", "]", ",", "rseed", "=", "rseed", ")", ")", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_multitask.train_with_embs": [[39, 150], ["print", "mnet.train", "hnet.train", "len", "torch.Adam", "range", "print", "hnet.parameters", "optim.Adam.zero_grad", "enumerate", "loss.backward", "optim.Adam.step", "enumerate", "range", "toy_example.test", "mnet.train", "hnet.train", "print", "numpy.random.permutation", "data.next_train_batch", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "hnet.forward", "mnet.forward", "torch.mse_loss", "writer.add_scalar", "toy_example.evaluate", "list", "int", "range", "numpy.random.randint"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.permutation", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.evaluate"], ["def", "train_with_embs", "(", "data_handlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "writer", ",", "\n", "mixed_gradient", "=", "True", ")", ":", "\n", "    ", "\"\"\"Train our hypernet - main-net combination on all tasks at once (multi-\n    task learning).\n\n    Note, there are several ways to do this. There are methods that ignore the\n    task identity of samples, e.g.,\n\n        - training the pure main network on a combined dataset (which is the\n          typical baseline in the literature)\n        - training a combination of main net and hypernet with a single task\n          embeddings, such that the hypernetwork is expected to output a set\n          of weights that solves all tasks equally good.\n\n    The above baseline methods have arguably less capacity, which is why one\n    would expect that they are no fair comparison to our method.\n\n    An alternative strategy is to keep the task identities (i.e., a task\n    embedding for each task) and train on a combined dataset. The pitfall\n    with this method is, that using a hypernet with minibatches is not\n    readily parallelizable. Hence, in an efficient implementation one would\n    select a random task_id per mini-batch and only process samples from this\n    task-specific dataset.\n\n    The drawback of this method is, that the gradient would always be computed\n    based on a single task (no influence of mixed task signals on the gradient).\n\n    To overcome this problem, one would need to loop over task-embeddings to\n    compute the weights for different tasks, which results in linearized\n    (inefficient) computation.\n\n    This method allows both types of baseline computation, which keep the task\n    identity.\n\n    Args:\n        (....): See docstring of method :func:`toy_example.train.train_reg`.\n        data_handlers: A list of dataset handlers, one for each task.\n        mixed_gradient: If False,in each training iteration a random task will\n            be selected and the training step is computed based on samples\n            from this task only. If True, random samples from all tasks are\n            selected, which means that one has to loop over the generation of\n            weights for different tasks (computationally inefficient).\n    \"\"\"", "\n", "print", "(", "'Training network ...'", ")", "\n", "\n", "mnet", ".", "train", "(", ")", "\n", "hnet", ".", "train", "(", ")", "\n", "\n", "n", "=", "len", "(", "data_handlers", ")", "\n", "\n", "# Number of samples per task per mini-batch if \"mixed_gradient\" activated.", "\n", "sample_size", "=", "[", "config", ".", "batch_size", "//", "n", "]", "*", "n", "\n", "sample_size", "=", "[", "s", "+", "(", "1", "if", "i", "<", "config", ".", "batch_size", "%", "n", "else", "0", ")", "for", "i", ",", "s", "in", "\n", "enumerate", "(", "sample_size", ")", "]", "\n", "\n", "# Since we have no regularizers, we can optimize all parameters at once.", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "hnet", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "lr_hyper", ")", "\n", "\n", "for", "i", "in", "range", "(", "config", ".", "n_iter", "*", "n", ")", ":", "\n", "### Evaluate network.", "\n", "# We test the network before we run the training iteration.", "\n", "# That way, we can see the initial performance of the untrained network.", "\n", "        ", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "for", "t", "in", "range", "(", "n", ")", ":", "\n", "                ", "train_cl", ".", "evaluate", "(", "t", ",", "data_handlers", "[", "t", "]", ",", "mnet", ",", "hnet", ",", "device", ",", "\n", "config", ",", "writer", ",", "i", ")", "\n", "", "train_cl", ".", "test", "(", "data_handlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "writer", ")", "\n", "mnet", ".", "train", "(", ")", "\n", "hnet", ".", "train", "(", ")", "\n", "\n", "", "if", "i", "%", "100", "==", "0", ":", "\n", "            ", "print", "(", "'Training iteration: %d.'", "%", "i", ")", "\n", "\n", "# Choose number of samples for each task:", "\n", "", "if", "mixed_gradient", ":", "\n", "            ", "n_samples", "=", "np", ".", "random", ".", "permutation", "(", "sample_size", ")", "\n", "", "else", ":", "\n", "            ", "n_samples", "=", "[", "0", "]", "*", "n", "\n", "n_samples", "[", "int", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "n", ")", ")", "]", "=", "config", ".", "batch_size", "\n", "\n", "### Train hypernet.", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "loss", "=", "0", "\n", "for", "t", ",", "bs", "in", "enumerate", "(", "n_samples", ")", ":", "\n", "            ", "if", "bs", "==", "0", ":", "\n", "                ", "continue", "\n", "", "data", "=", "data_handlers", "[", "t", "]", "\n", "\n", "batch", "=", "data", ".", "next_train_batch", "(", "bs", ")", "\n", "X", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ",", "mode", "=", "'train'", ")", "\n", "T", "=", "data", ".", "output_to_torch_tensor", "(", "batch", "[", "1", "]", ",", "device", ",", "mode", "=", "'train'", ")", "\n", "\n", "weights", "=", "hnet", ".", "forward", "(", "t", ")", "\n", "Y", "=", "mnet", ".", "forward", "(", "X", ",", "weights", ")", "\n", "\n", "if", "config", ".", "multi_head", ":", "\n", "                ", "n_y", "=", "data", ".", "out_shape", "[", "0", "]", "\n", "allowed_outputs", "=", "list", "(", "range", "(", "t", "*", "n_y", ",", "(", "t", "+", "1", ")", "*", "n_y", ")", ")", "\n", "Y", "=", "Y", "[", ":", ",", "allowed_outputs", "]", "\n", "\n", "# Task-specific loss.", "\n", "", "loss", "+=", "F", ".", "mse_loss", "(", "Y", ",", "T", ")", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "i", "%", "10", "==", "0", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'train/mse_loss'", ",", "loss", ",", "i", ")", "\n", "\n", "", "", "print", "(", "'Training network ... Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_multitask.train_main_only": [[151, 229], ["print", "mnet.train", "len", "torch.Adam", "range", "print", "mnet.parameters", "numpy.unique", "optim.Adam.zero_grad", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "enumerate", "mnet.forward", "torch.mse_loss", "F.mse_loss.backward", "optim.Adam.step", "range", "toy_example.test", "mnet.train", "print", "numpy.random.randint", "data.next_train_batch", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "torch.empty", "torch.empty", "torch.empty", "enumerate", "writer.add_scalar", "toy_example.evaluate", "list", "range"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.evaluate"], ["", "def", "train_main_only", "(", "data_handlers", ",", "mnet", ",", "device", ",", "config", ",", "writer", ")", ":", "\n", "    ", "\"\"\"Train main network on all tasks at once (multi-task learning). Note, no\n    hypernetwork is used.\n\n    Args:\n        (....): See docstring of method :func:`toy_example.train.train_reg`.\n        data_handlers: A list of dataset handlers, one for each task.\n    \"\"\"", "\n", "print", "(", "'Training network ...'", ")", "\n", "\n", "mnet", ".", "train", "(", ")", "\n", "\n", "n", "=", "len", "(", "data_handlers", ")", "\n", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "mnet", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "lr_hyper", ")", "\n", "\n", "for", "i", "in", "range", "(", "config", ".", "n_iter", "*", "n", ")", ":", "\n", "### Evaluate network.", "\n", "# We test the network before we run the training iteration.", "\n", "# That way, we can see the initial performance of the untrained network.", "\n", "        ", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "for", "t", "in", "range", "(", "n", ")", ":", "\n", "                ", "train_cl", ".", "evaluate", "(", "t", ",", "data_handlers", "[", "t", "]", ",", "mnet", ",", "None", ",", "device", ",", "\n", "config", ",", "writer", ",", "i", ")", "\n", "", "train_cl", ".", "test", "(", "data_handlers", ",", "mnet", ",", "None", ",", "device", ",", "config", ",", "writer", ")", "\n", "mnet", ".", "train", "(", ")", "\n", "\n", "", "if", "i", "%", "100", "==", "0", ":", "\n", "            ", "print", "(", "'Training iteration: %d.'", "%", "i", ")", "\n", "\n", "# Choose number of samples for each task:", "\n", "", "_", ",", "n_samples", "=", "np", ".", "unique", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "high", "=", "n", ",", "\n", "size", "=", "config", ".", "batch_size", ")", ",", "return_counts", "=", "True", ")", "\n", "\n", "### Train main network.", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "X", "=", "torch", ".", "empty", "(", "(", "config", ".", "batch_size", ",", "*", "data_handlers", "[", "0", "]", ".", "in_shape", ")", ")", "\n", "T", "=", "torch", ".", "empty", "(", "(", "config", ".", "batch_size", ",", "*", "data_handlers", "[", "0", "]", ".", "out_shape", ")", ")", "\n", "m", "=", "0", "\n", "for", "t", ",", "bs", "in", "enumerate", "(", "n_samples", ")", ":", "\n", "            ", "if", "bs", "==", "0", ":", "\n", "                ", "continue", "\n", "", "data", "=", "data_handlers", "[", "t", "]", "\n", "\n", "batch", "=", "data", ".", "next_train_batch", "(", "bs", ")", "\n", "X", "[", "m", ":", "m", "+", "bs", ",", ":", "]", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ",", "\n", "mode", "=", "'train'", ")", "\n", "T", "[", "m", ":", "m", "+", "bs", ",", ":", "]", "=", "data", ".", "output_to_torch_tensor", "(", "batch", "[", "1", "]", ",", "device", ",", "\n", "mode", "=", "'train'", ")", "\n", "\n", "m", "+=", "bs", "\n", "\n", "", "Y", "=", "mnet", ".", "forward", "(", "X", ")", "\n", "\n", "if", "config", ".", "multi_head", ":", "\n", "            ", "n_y", "=", "data", ".", "out_shape", "[", "0", "]", "\n", "\n", "Y_full", "=", "Y", "\n", "Y", "=", "torch", ".", "empty", "(", "(", "Y_full", ".", "shape", "[", "0", "]", ",", "n_y", ")", ")", "\n", "\n", "m", "=", "0", "\n", "for", "t", ",", "bs", "in", "enumerate", "(", "n_samples", ")", ":", "\n", "                ", "allowed_outputs", "=", "list", "(", "range", "(", "t", "*", "n_y", ",", "(", "t", "+", "1", ")", "*", "n_y", ")", ")", "\n", "Y", "[", "m", ":", "m", "+", "bs", ",", ":", "]", "=", "Y_full", "[", "m", ":", "m", "+", "bs", ",", "allowed_outputs", "]", "\n", "\n", "m", "+=", "bs", "\n", "\n", "# Task-specific loss.", "\n", "", "", "loss", "=", "F", ".", "mse_loss", "(", "Y", ",", "T", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "i", "%", "10", "==", "0", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'train/mse_loss'", ",", "loss", ",", "i", ")", "\n", "\n", "", "", "print", "(", "'Training network ... Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_multitask.train_without_embs": [[230, 319], ["print", "mnet.train", "hnet.train", "len", "torch.Adam", "range", "print", "hnet.parameters", "numpy.unique", "optim.Adam.zero_grad", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "enumerate", "hnet.forward", "mnet.forward", "torch.mse_loss", "F.mse_loss.backward", "optim.Adam.step", "hnet.get_task_embs", "range", "range", "toy_example.test", "mnet.train", "hnet.train", "print", "numpy.random.randint", "data.next_train_batch", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "torch.empty", "torch.empty", "torch.empty", "enumerate", "len", "writer.add_scalar", "toy_example.evaluate", "list", "range"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.evaluate"], ["", "def", "train_without_embs", "(", "data_handlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "writer", ")", ":", "\n", "    ", "\"\"\"Train our hypernet - main-net combination on all tasks at once (multi-\n    task learning), using only one task embedding. Hence, the hypernetwork has\n    to learn to produce a set of weights, that solves all tasks at once.\n\n\n    Args:\n        (....): See docstring of method :func:`toy_example.train.train_reg`.\n        data_handlers: A list of dataset handlers, one for each task.\n    \"\"\"", "\n", "print", "(", "'Training network ...'", ")", "\n", "\n", "mnet", ".", "train", "(", ")", "\n", "hnet", ".", "train", "(", ")", "\n", "\n", "n", "=", "len", "(", "data_handlers", ")", "\n", "\n", "# Since we have no regularizers, we can optimize all parameters at once.", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "hnet", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "lr_hyper", ")", "\n", "\n", "for", "i", "in", "range", "(", "config", ".", "n_iter", "*", "n", ")", ":", "\n", "### Evaluate network.", "\n", "# We test the network before we run the training iteration.", "\n", "# That way, we can see the initial performance of the untrained network.", "\n", "        ", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "for", "t", "in", "range", "(", "n", ")", ":", "\n", "                ", "train_cl", ".", "evaluate", "(", "t", ",", "data_handlers", "[", "t", "]", ",", "mnet", ",", "hnet", ",", "device", ",", "\n", "config", ",", "writer", ",", "i", ")", "\n", "", "train_cl", ".", "test", "(", "data_handlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "writer", ")", "\n", "mnet", ".", "train", "(", ")", "\n", "hnet", ".", "train", "(", ")", "\n", "\n", "", "if", "i", "%", "100", "==", "0", ":", "\n", "            ", "print", "(", "'Training iteration: %d.'", "%", "i", ")", "\n", "\n", "# Choose number of samples for each task:", "\n", "", "_", ",", "n_samples", "=", "np", ".", "unique", "(", "np", ".", "random", ".", "randint", "(", "0", ",", "high", "=", "n", ",", "\n", "size", "=", "config", ".", "batch_size", ")", ",", "return_counts", "=", "True", ")", "\n", "\n", "### Train hypernet.", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "X", "=", "torch", ".", "empty", "(", "(", "config", ".", "batch_size", ",", "*", "data_handlers", "[", "0", "]", ".", "in_shape", ")", ")", "\n", "T", "=", "torch", ".", "empty", "(", "(", "config", ".", "batch_size", ",", "*", "data_handlers", "[", "0", "]", ".", "out_shape", ")", ")", "\n", "m", "=", "0", "\n", "for", "t", ",", "bs", "in", "enumerate", "(", "n_samples", ")", ":", "\n", "            ", "if", "bs", "==", "0", ":", "\n", "                ", "continue", "\n", "", "data", "=", "data_handlers", "[", "t", "]", "\n", "\n", "batch", "=", "data", ".", "next_train_batch", "(", "bs", ")", "\n", "X", "[", "m", ":", "m", "+", "bs", ",", ":", "]", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ",", "\n", "mode", "=", "'train'", ")", "\n", "T", "[", "m", ":", "m", "+", "bs", ",", ":", "]", "=", "data", ".", "output_to_torch_tensor", "(", "batch", "[", "1", "]", ",", "device", ",", "\n", "mode", "=", "'train'", ")", "\n", "\n", "m", "+=", "bs", "\n", "\n", "", "weights", "=", "hnet", ".", "forward", "(", "0", ")", "\n", "Y", "=", "mnet", ".", "forward", "(", "X", ",", "weights", ")", "\n", "\n", "if", "config", ".", "multi_head", ":", "\n", "            ", "n_y", "=", "data", ".", "out_shape", "[", "0", "]", "\n", "\n", "Y_full", "=", "Y", "\n", "Y", "=", "torch", ".", "empty", "(", "(", "Y_full", ".", "shape", "[", "0", "]", ",", "n_y", ")", ")", "\n", "\n", "m", "=", "0", "\n", "for", "t", ",", "bs", "in", "enumerate", "(", "n_samples", ")", ":", "\n", "                ", "allowed_outputs", "=", "list", "(", "range", "(", "t", "*", "n_y", ",", "(", "t", "+", "1", ")", "*", "n_y", ")", ")", "\n", "Y", "[", "m", ":", "m", "+", "bs", ",", ":", "]", "=", "Y_full", "[", "m", ":", "m", "+", "bs", ",", "allowed_outputs", "]", "\n", "\n", "m", "+=", "bs", "\n", "\n", "", "", "loss", "=", "F", ".", "mse_loss", "(", "Y", ",", "T", ")", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "# Ensure that all embeddings are the same (note, that the testing and", "\n", "# evaluation methods require an embedding for each task).", "\n", "tembs", "=", "hnet", ".", "get_task_embs", "(", ")", "\n", "for", "t", "in", "range", "(", "1", ",", "len", "(", "tembs", ")", ")", ":", "\n", "            ", "tembs", "[", "t", "]", ".", "data", "=", "tembs", "[", "0", "]", ".", "data", "\n", "\n", "", "if", "i", "%", "10", "==", "0", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'train/mse_loss'", ",", "loss", ",", "i", ")", "\n", "\n", "", "", "print", "(", "'Training network ... Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_multitask.train_rnet": [[320, 385], ["print", "len", "rnet.train", "torch.Adam", "range", "print", "rnet.parameters", "optim.Adam.zero_grad", "range", "loss.backward", "optim.Adam.step", "toy_example.test", "print", "rnet.train", "print", "data.next_train_batch", "data.input_to_torch_tensor", "rnet.forward", "toy_example.task_recognition_model.RecognitionNet.task_cross_entropy", "alpha.argmax", "writer.add_scalar", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.log_softmax", "writer.add_scalar", "str", "torch.ones", "torch.ones", "torch.ones"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.task_cross_entropy"], ["", "def", "train_rnet", "(", "data_handlers", ",", "mnet", ",", "hnet", ",", "rnet", ",", "device", ",", "config", ",", "writer", ")", ":", "\n", "    ", "\"\"\"Train the recognition network. However, for multitask learning, there is\n    no need to prevent catastophic forgetting in the task recognition network,\n    hence, no replay network (decoder) is needed.\n\n    Note, mnet and hnet are only needed for testing purposes.\n\n    Args:\n        (....): See docstring of method :func:`toy_example.train.train_reg`.\n        data_handlers: A list of dataset handlers, one for each task.\n        rnet: The recognition network.\n    \"\"\"", "\n", "print", "(", "'Training recognition network ...'", ")", "\n", "\n", "n", "=", "len", "(", "data_handlers", ")", "\n", "\n", "rnet", ".", "train", "(", ")", "\n", "\n", "optimizer", "=", "optim", ".", "Adam", "(", "rnet", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "lr_ae", ")", "\n", "\n", "for", "i", "in", "range", "(", "config", ".", "n_iter_ae", "*", "n", ")", ":", "\n", "### Evaluate network.", "\n", "# We test the network before we run the training iteration.", "\n", "# That way, we can see the initial performance of the untrained network.", "\n", "        ", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "test_mse", ",", "_", ",", "_", "=", "train_cl", ".", "test", "(", "data_handlers", ",", "mnet", ",", "hnet", ",", "device", ",", "\n", "config", ",", "writer", ",", "rnet", "=", "rnet", ")", "\n", "print", "(", "'Current MSE values using task recognition %s'", "%", "\n", "str", "(", "test_mse", ")", ")", "\n", "rnet", ".", "train", "(", ")", "\n", "\n", "", "if", "i", "%", "100", "==", "0", ":", "\n", "            ", "print", "(", "'Training iteration: %d.'", "%", "i", ")", "\n", "\n", "### Train theta.", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "loss", "=", "0", "\n", "\n", "for", "t", "in", "range", "(", "n", ")", ":", "\n", "# Task recognition targets:", "\n", "            ", "T", "=", "torch", ".", "ones", "(", "config", ".", "batch_size", ",", "dtype", "=", "torch", ".", "int64", ")", ".", "to", "(", "device", ")", "*", "t", "\n", "\n", "data", "=", "data_handlers", "[", "t", "]", "\n", "batch", "=", "data", ".", "next_train_batch", "(", "config", ".", "batch_size", ")", "\n", "X", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ",", "mode", "=", "'train'", ")", "\n", "\n", "alpha", ",", "h_alpha", "=", "rnet", ".", "forward", "(", "X", ")", "\n", "\n", "loss", "+=", "RecognitionNet", ".", "task_cross_entropy", "(", "F", ".", "log_softmax", "(", "h_alpha", ",", "\n", "dim", "=", "1", ")", ",", "T", ")", "\n", "\n", "task_preds", "=", "alpha", ".", "argmax", "(", "dim", "=", "1", ",", "keepdim", "=", "False", ")", "\n", "acc", "=", "(", "task_preds", "==", "T", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "*", "100.", "\n", "\n", "if", "i", "%", "10", "==", "0", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "'train_ae/accuracy_%d'", "%", "(", "t", ")", ",", "acc", ",", "i", ")", "\n", "\n", "", "", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "i", "%", "10", "==", "0", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'train_ae/loss'", ",", "loss", ",", "i", ")", "\n", "\n", "", "", "print", "(", "'Training recognition network ... Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_multitask.run": [[386, 449], ["toy_example.train_utils.parse_cmd_arguments", "toy_example.train_utils._setup_environment", "toy_example.train_utils._generate_tasks", "toy_example.train_utils._generate_networks", "toy_example.test", "print", "print", "writer.close", "print", "range", "ValueError", "train_multitask.train_main_only", "train_multitask.train_with_embs", "numpy.array2string", "train_multitask.train_rnet", "toy_example.test", "print", "train_multitask.train_without_embs", "current_mse.mean", "current_mse.std", "str"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils.parse_cmd_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._setup_environment", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_tasks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_networks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_multitask.train_main_only", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_multitask.train_with_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.train_rnet", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_multitask.train_without_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["", "def", "run", "(", ")", ":", "\n", "    ", "\"\"\"Run the script\n\n    Returns:\n        (tuple): Tuple containing:\n\n        - **final_mse**: Final MSE for each task.\n        - **final_rnet_mse** (optional): Final MSE for each task, when using\n          recognition model to infer task identity during testing.\n    \"\"\"", "\n", "config", "=", "train_utils", ".", "parse_cmd_arguments", "(", "mode", "=", "'train_mt_regression'", ")", "\n", "assert", "(", "config", ".", "method", "in", "range", "(", "3", ")", ")", "\n", "\n", "device", ",", "writer", "=", "train_utils", ".", "_setup_environment", "(", "config", ")", "\n", "\n", "if", "config", ".", "method", "in", "[", "0", ",", "2", "]", "and", "config", ".", "use_task_detection", ":", "\n", "        ", "raise", "ValueError", "(", "'The selected multitask method is not conditioned '", "+", "\n", "'on a task. Hence, a recognition model is not '", "+", "\n", "'required.'", ")", "\n", "\n", "### Create tasks.", "\n", "", "dhandlers", ",", "num_tasks", "=", "train_utils", ".", "_generate_tasks", "(", "config", ")", "\n", "\n", "### Generate networks.", "\n", "# Note, if we use a recognition network, then we use a custom one, as we", "\n", "# don't need an autoencoder (no replay needed in multitask learning).", "\n", "mnet", ",", "hnet", ",", "rnet", "=", "train_utils", ".", "_generate_networks", "(", "config", ",", "dhandlers", ",", "device", ",", "\n", "create_hnet", "=", "config", ".", "method", "!=", "0", ",", "create_rnet", "=", "config", ".", "use_task_detection", ",", "\n", "no_replay", "=", "True", ")", "\n", "\n", "### Train on tasks in parallel.", "\n", "current_rnet_mse", "=", "None", "\n", "\n", "if", "config", ".", "method", "==", "0", ":", "\n", "        ", "train_main_only", "(", "dhandlers", ",", "mnet", ",", "device", ",", "config", ",", "writer", ")", "\n", "", "elif", "config", ".", "method", "==", "1", ":", "\n", "        ", "train_with_embs", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "writer", ")", "\n", "\n", "if", "config", ".", "use_task_detection", ":", "\n", "            ", "train_rnet", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "rnet", ",", "device", ",", "config", ",", "writer", ")", "\n", "\n", "current_rnet_mse", ",", "_", ",", "_", "=", "train_cl", ".", "test", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "\n", "device", ",", "config", ",", "writer", ",", "rnet", "=", "rnet", ",", "save_fig", "=", "False", ")", "\n", "print", "(", "'Final MSE values after training on all tasks using '", "+", "\n", "'task recognition %s'", "%", "str", "(", "current_rnet_mse", ")", ")", "\n", "\n", "", "", "elif", "config", ".", "method", "==", "2", ":", "\n", "        ", "train_without_embs", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "writer", ")", "\n", "\n", "### Test networks.", "\n", "", "current_mse", ",", "_", ",", "_", "=", "train_cl", ".", "test", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "\n", "writer", ")", "\n", "\n", "print", "(", "'Final MSE values after training on all tasks: %s'", "%", "np", ".", "array2string", "(", "current_mse", ",", "precision", "=", "5", ",", "separator", "=", "','", ")", ")", "\n", "print", "(", "'Final MSE mean %.4f (std %.4f).'", "%", "(", "current_mse", ".", "mean", "(", ")", ",", "\n", "current_mse", ".", "std", "(", ")", ")", ")", "\n", "\n", "writer", ".", "close", "(", ")", "\n", "\n", "print", "(", "'Program finished successfully.'", ")", "\n", "\n", "return", "current_mse", ",", "current_rnet_mse", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.hyper_model.HyperNetwork.__init__": [[68, 219], ["torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "utils.module_wrappers.CLHyperNetInterface.__init__", "hyper_model.HyperNetwork._gen_layers", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "hyper_model.HyperNetwork._is_properly_setup", "NotImplementedError", "NotImplementedError", "len", "hyper_model.HyperNetwork._create_feedback_matrix", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "range", "int", "print", "print", "ValueError", "hyper_model.HyperNetwork._task_embs.append", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.normal_", "numpy.sum", "torch.Parameter", "torch.Parameter", "torch.Parameter", "t.numel", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.hyper_model.HyperNetwork._gen_layers", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.hyper_model.HyperNetwork._create_feedback_matrix"], ["def", "__init__", "(", "self", ",", "target_shapes", ",", "num_tasks", ",", "layers", "=", "[", "50", ",", "100", "]", ",", "verbose", "=", "True", ",", "\n", "te_dim", "=", "8", ",", "no_te_embs", "=", "False", ",", "activation_fn", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "use_bias", "=", "True", ",", "no_weights", "=", "False", ",", "init_weights", "=", "None", ",", "\n", "ce_dim", "=", "None", ",", "dropout_rate", "=", "-", "1", ",", "use_spectral_norm", "=", "False", ",", "\n", "create_feedback_matrix", "=", "False", ",", "target_net_out_dim", "=", "10", ",", "\n", "random_scale_feedback_matrix", "=", "1.", ",", "\n", "use_batch_norm", "=", "False", ",", "noise_dim", "=", "-", "1", ",", "temb_std", "=", "-", "1", ")", ":", "\n", "        ", "\"\"\"Build the network. The network will consist of several hidden layers\n        and a dedicated output layer for each weight matrix/bias vector.\n\n        The input to the network will be a learned task embedding.\n\n        Args:\n            target_shapes: A list of list of integers, denoting the shape of\n                each parameter tensor in the main network (hence, determining\n                the output of this network).\n            num_tasks: The number of task embeddings needed.\n            layers: A list of integers, each indicating the size of a hidden\n                    layer in this network.\n            te_dim: The dimensionality of the task embeddings.\n            no_te_embs: If this option is True, no class internal task\n                embeddings are constructed and are instead expected to be\n                provided to the forward method.\n            activation_fn: The nonlinearity used in hidden layers. If None, no\n                nonlinearity will be applied.\n            use_bias: Whether layers may have bias terms.\n            no_weights: If set to True, no trainable parameters will be\n                constructed, i.e., weights are assumed to be produced ad-hoc\n                by a hypernetwork and passed to the forward function.\n                Does not affect task embeddings.\n            init_weights (optional): This option is for convenience reasons.\n                The option expects a list of parameter values that are used to\n                initialize the network weights. As such, it provides a\n                convenient way of initializing a network with, for instance, a\n                weight draw produced by the hypernetwork.\n                Does not affect task embeddings.\n            ce_dim (optional): The dimensionality of any additional embeddings,\n                (in addition to the task embedding) that will be used as input\n                to the hypernetwork. If this option is ``None``, no additional\n                input is expected. Otherwise, an additional embedding has to be\n                passed to the :meth:`forward` method (see argument\n                ``ext_inputs``).\n                A typical usecase would be a chunk embedding.\n            dropout_rate (optional): If -1, no dropout will be applied.\n                Otherwise a number between 0 and 1 is expected, denoting the\n                dropout of hidden layers.\n            use_spectral_norm: Enable spectral normalization for all layers.\n            create_feedback_matrix: A feedback matrix for credit assignment in\n                the main network will be created. See attribute\n                :attr:`feedback_matrix`.\n            target_net_out_dim: Target network output dimension. We need this\n                information to create feedback matrices that can be used in\n                conjunction with Direct Feedback Alignment. Only needs to be\n                specified when enabling ``create_feedback_matrix``.\n            random_scale_feedback_matrix: Scale of uniform distribution used\n                to create the feedback matrix. Only needs to be specified when\n                enabling ``create_feedback_matrix``.\n            use_batch_norm: If True, batchnorm will be applied to all hidden\n                layers.\n            noise_dim (optional): If -1, no noise will be applied.\n                Otherwise the hypernetwork will receive as additional input\n                zero-mean Gaussian noise with unit variance during training\n                (zeroes will be inputted during eval-mode). Note, if a batch of\n                inputs is given, then a different noise vector is generated for\n                every sample in the batch.\n            temb_std (optional): If not -1, the task embeddings will be\n                perturbed by zero-mean Gaussian noise with the given std\n                (additive noise). The perturbation is only applied if the\n                network is in training mode. Note, per batch of external inputs,\n                the perturbation of the task embedding will be shared.\n        \"\"\"", "\n", "# FIXME find a way using super to handle multiple inheritence.", "\n", "#super(HyperNetwork, self).__init__()", "\n", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "CLHyperNetInterface", ".", "__init__", "(", "self", ")", "\n", "\n", "if", "use_spectral_norm", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Spectral normalization not yet '", "+", "\n", "'implemented for this hypernetwork type.'", ")", "\n", "", "if", "use_batch_norm", ":", "\n", "# Note, batch normalization only makes sense when batch processing", "\n", "# is applied during training (i.e., batch size > 1).", "\n", "# As long as we only support processing of 1 task embedding, that", "\n", "# means that external inputs are required.", "\n", "            ", "if", "ce_dim", "is", "None", ":", "\n", "                ", "raise", "ValueError", "(", "'Can\\'t use batchnorm as long as '", "+", "\n", "'hypernetwork process more than 1 sample '", "+", "\n", "'(\"ce_dim\" must be specified).'", ")", "\n", "", "raise", "NotImplementedError", "(", "'Batch normalization not yet '", "+", "\n", "'implemented for this hypernetwork type.'", ")", "\n", "\n", "", "assert", "(", "len", "(", "target_shapes", ")", ">", "0", ")", "\n", "assert", "(", "no_te_embs", "or", "num_tasks", ">", "0", ")", "\n", "self", ".", "_num_tasks", "=", "num_tasks", "\n", "\n", "assert", "(", "init_weights", "is", "None", "or", "no_weights", "is", "False", ")", "\n", "self", ".", "_no_weights", "=", "no_weights", "\n", "self", ".", "_no_te_embs", "=", "no_te_embs", "\n", "self", ".", "_te_dim", "=", "te_dim", "\n", "self", ".", "_size_ext_input", "=", "ce_dim", "\n", "self", ".", "_layers", "=", "layers", "\n", "self", ".", "_target_shapes", "=", "target_shapes", "\n", "self", ".", "_use_bias", "=", "use_bias", "\n", "self", ".", "_act_fn", "=", "activation_fn", "\n", "self", ".", "_init_weights", "=", "init_weights", "\n", "self", ".", "_dropout_rate", "=", "dropout_rate", "\n", "self", ".", "_noise_dim", "=", "noise_dim", "\n", "self", ".", "_temb_std", "=", "temb_std", "\n", "self", ".", "_shifts", "=", "None", "# FIXME temporary test.", "\n", "\n", "### Hidden layers", "\n", "self", ".", "_gen_layers", "(", "layers", ",", "te_dim", ",", "use_bias", ",", "no_weights", ",", "init_weights", ",", "\n", "ce_dim", ",", "noise_dim", ")", "\n", "\n", "if", "create_feedback_matrix", ":", "\n", "            ", "self", ".", "_create_feedback_matrix", "(", "target_shapes", ",", "target_net_out_dim", ",", "\n", "random_scale_feedback_matrix", ")", "\n", "\n", "", "self", ".", "_dropout", "=", "None", "\n", "if", "dropout_rate", "!=", "-", "1", ":", "\n", "            ", "assert", "(", "dropout_rate", ">=", "0", "and", "dropout_rate", "<=", "1", ")", "\n", "self", ".", "_dropout", "=", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n", "# Task embeddings.", "\n", "", "if", "no_te_embs", ":", "\n", "            ", "self", ".", "_task_embs", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "_task_embs", "=", "nn", ".", "ParameterList", "(", ")", "\n", "for", "_", "in", "range", "(", "num_tasks", ")", ":", "\n", "                ", "self", ".", "_task_embs", ".", "append", "(", "nn", ".", "Parameter", "(", "data", "=", "torch", ".", "Tensor", "(", "te_dim", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "self", ".", "_task_embs", "[", "-", "1", "]", ",", "mean", "=", "0.", ",", "std", "=", "1.", ")", "\n", "\n", "", "", "self", ".", "_theta_shapes", "=", "self", ".", "_hidden_dims", "+", "self", ".", "_out_dims", "\n", "\n", "ntheta", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "self", ".", "_theta_shapes", ")", "\n", "ntembs", "=", "int", "(", "np", ".", "sum", "(", "[", "t", ".", "numel", "(", ")", "for", "t", "in", "self", ".", "_task_embs", "]", ")", ")", "if", "not", "no_te_embs", "else", "0", "\n", "self", ".", "_num_weights", "=", "ntheta", "+", "ntembs", "\n", "\n", "self", ".", "_num_outputs", "=", "MainNetInterface", ".", "shapes_to_num_weights", "(", "self", ".", "target_shapes", ")", "\n", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "'Constructed hypernetwork with %d parameters ('", "%", "(", "ntheta", "+", "ntembs", ")", "+", "'%d network weights + %d task embedding weights).'", "\n", "%", "(", "ntheta", ",", "ntembs", ")", ")", "\n", "print", "(", "'The hypernetwork has a total of %d outputs.'", "\n", "%", "self", ".", "_num_outputs", ")", "\n", "\n", "", "self", ".", "_is_properly_setup", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.hyper_model.HyperNetwork._create_feedback_matrix": [[220, 240], ["hyper_model.HyperNetwork._feedback_matrix.append", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty().uniform_", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty", "torch.empty"], "methods", ["None"], ["", "def", "_create_feedback_matrix", "(", "self", ",", "target_shapes", ",", "target_net_out_dim", ",", "\n", "random_scale_feedback_matrix", ")", ":", "\n", "        ", "\"\"\"Create a feeback matrix for credit assignment as an alternative\n        to backprop.\n\n        The matrix will be of dimension:\n        ``[target network output dim x hypernetwork output]``.\n\n        Note:\n            For now, this method only generates a feedback matrix appropriate\n            for Direct Feedback Alignment.\n\n        Args:\n            (....): See constructor docstring.\n        \"\"\"", "\n", "s", "=", "random_scale_feedback_matrix", "\n", "self", ".", "_feedback_matrix", "=", "[", "]", "\n", "for", "k", "in", "target_shapes", ":", "\n", "            ", "dims", "=", "[", "target_net_out_dim", "]", "+", "k", "\n", "self", ".", "_feedback_matrix", ".", "append", "(", "torch", ".", "empty", "(", "dims", ")", ".", "uniform_", "(", "-", "s", ",", "s", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.hyper_model.HyperNetwork.feedback_matrix": [[241, 254], ["None"], "methods", ["None"], ["", "", "@", "property", "\n", "def", "feedback_matrix", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`feedback_matrix`.\n\n        The matrix will be of dimension:\n        ``[target network output dim x hypernetwork output]``.\n\n        Return:\n            (list): Feeback matrix for credit assignment, which is represented\n            as a list of torch tensors.\n        \"\"\"", "\n", "\n", "return", "self", ".", "_feedback_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.hyper_model.HyperNetwork._gen_layers": [[255, 315], ["enumerate", "enumerate", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "enumerate", "hyper_model.HyperNetwork._hidden_dims.append", "numpy.prod", "hyper_model.HyperNetwork._out_dims.append", "hyper_model.HyperNetwork._theta.append", "range", "range", "hyper_model.HyperNetwork._hidden_dims.append", "hyper_model.HyperNetwork._out_dims.append", "torch.Parameter", "torch.Parameter", "torch.Parameter", "len", "len", "len", "numpy.all", "len", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "numpy.equal", "utils.torch_utils.init_params", "utils.torch_utils.init_params", "list", "list"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params"], ["", "def", "_gen_layers", "(", "self", ",", "layers", ",", "te_dim", ",", "use_bias", ",", "no_weights", ",", "init_weights", ",", "\n", "ce_dim", ",", "noise_dim", ")", ":", "\n", "        ", "\"\"\"Generate all layers of this network. This method will create\n        the parameters of each layer. Note, this method should only be\n        called by the constructor.\n\n        This method will add the attributes \"_hidden_dims\" and \"_out_dims\".\n        If \"no_weights\" is False, it will also create an attribute \"_weights\"\n        and initialize all parameters. Otherwise, _weights\" is set to None.\n\n        Args:\n            See constructur arguments.\n        \"\"\"", "\n", "### Compute the shapes of all parameters.", "\n", "# Hidden layers.", "\n", "self", ".", "_hidden_dims", "=", "[", "]", "\n", "prev_dim", "=", "te_dim", "\n", "if", "ce_dim", "is", "not", "None", ":", "\n", "            ", "prev_dim", "+=", "ce_dim", "\n", "", "if", "noise_dim", "!=", "-", "1", ":", "\n", "            ", "prev_dim", "+=", "noise_dim", "\n", "", "for", "i", ",", "size", "in", "enumerate", "(", "layers", ")", ":", "\n", "            ", "self", ".", "_hidden_dims", ".", "append", "(", "[", "size", ",", "prev_dim", "]", ")", "\n", "if", "use_bias", ":", "\n", "                ", "self", ".", "_hidden_dims", ".", "append", "(", "[", "size", "]", ")", "\n", "", "prev_dim", "=", "size", "\n", "", "self", ".", "_last_hidden_size", "=", "prev_dim", "\n", "\n", "# Output layers.", "\n", "self", ".", "_out_dims", "=", "[", "]", "\n", "for", "i", ",", "dims", "in", "enumerate", "(", "self", ".", "target_shapes", ")", ":", "\n", "            ", "nouts", "=", "np", ".", "prod", "(", "dims", ")", "\n", "self", ".", "_out_dims", ".", "append", "(", "[", "nouts", ",", "self", ".", "_last_hidden_size", "]", ")", "\n", "if", "use_bias", ":", "\n", "                ", "self", ".", "_out_dims", ".", "append", "(", "[", "nouts", "]", ")", "\n", "", "", "if", "no_weights", ":", "\n", "            ", "self", ".", "_theta", "=", "None", "\n", "return", "\n", "\n", "### Create parameter tensors.", "\n", "# If \"use_bias\" is True, then each odd entry of this list will contain", "\n", "# a weight matrix and each even entry a bias vector. Otherwise,", "\n", "# it only contains a weight matrix per layer.", "\n", "", "self", ".", "_theta", "=", "nn", ".", "ParameterList", "(", ")", "\n", "for", "i", ",", "dims", "in", "enumerate", "(", "self", ".", "_hidden_dims", "+", "self", ".", "_out_dims", ")", ":", "\n", "            ", "self", ".", "_theta", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "*", "dims", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "\n", "", "if", "init_weights", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "init_weights", ")", "==", "len", "(", "self", ".", "_theta", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "init_weights", ")", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "i", "]", ".", "shape", ")", ",", "\n", "list", "(", "self", ".", "_theta", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "self", ".", "_theta", "[", "i", "]", ".", "data", "=", "init_weights", "[", "i", "]", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "self", ".", "_theta", ")", ",", "2", "if", "use_bias", "else", "1", ")", ":", "\n", "                ", "if", "use_bias", ":", "\n", "                    ", "init_params", "(", "self", ".", "_theta", "[", "i", "]", ",", "self", ".", "_theta", "[", "i", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "                    ", "init_params", "(", "self", ".", "_theta", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.hyper_model.HyperNetwork.forward": [[317, 413], ["range", "range", "Exception", "Exception", "enumerate", "enumerate", "Exception", "task_emb.expand.expand.add", "Exception", "task_emb.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "task_emb.expand.expand.expand", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "torch.linear", "torch.linear", "torch.linear", "len", "len", "torch.linear", "torch.linear", "torch.linear", "torch.squeeze.view", "torch.squeeze.view", "torch.squeeze.view", "outputs.append", "len", "len", "numpy.all", "len", "len", "weights.append", "Exception", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "eps.to.to.to", "hyper_model.HyperNetwork._act_fn", "hyper_model.HyperNetwork._dropout", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "numpy.equal", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "hyper_model.HyperNetwork.get_device", "list"], "methods", ["None"], ["", "", "", "", "def", "forward", "(", "self", ",", "task_id", "=", "None", ",", "theta", "=", "None", ",", "dTheta", "=", "None", ",", "task_emb", "=", "None", ",", "\n", "ext_inputs", "=", "None", ",", "squeeze", "=", "True", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract super class method.\"\"\"", "\n", "if", "task_id", "is", "None", "and", "task_emb", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'The hyper network has to get either a task ID'", "+", "\n", "'to choose the learned embedding or directly '", "+", "\n", "'get an embedding as input (e.g. from a task '", "+", "\n", "'recognition model).'", ")", "\n", "\n", "", "if", "not", "self", ".", "has_theta", "and", "theta", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Network was generated without internal weights. '", "+", "\n", "'Hence, \"theta\" option may not be None.'", ")", "\n", "\n", "", "if", "theta", "is", "None", ":", "\n", "            ", "theta", "=", "self", ".", "theta", "\n", "", "else", ":", "\n", "            ", "assert", "(", "len", "(", "theta", ")", "==", "len", "(", "self", ".", "theta_shapes", ")", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "self", ".", "theta_shapes", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "s", ",", "list", "(", "theta", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "", "", "if", "dTheta", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "dTheta", ")", "==", "len", "(", "self", ".", "theta_shapes", ")", ")", "\n", "\n", "weights", "=", "[", "]", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "theta", ")", ":", "\n", "                ", "weights", ".", "append", "(", "t", "+", "dTheta", "[", "i", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "weights", "=", "theta", "\n", "\n", "# Select task embeddings.", "\n", "", "if", "not", "self", ".", "has_task_embs", "and", "task_emb", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'The network was created with no internal task '", "+", "\n", "'embeddings, thus parameter \"task_emb\" has to '", "+", "\n", "'be specified.'", ")", "\n", "\n", "", "if", "task_emb", "is", "None", ":", "\n", "            ", "task_emb", "=", "self", ".", "_task_embs", "[", "task_id", "]", "\n", "", "if", "self", ".", "training", "and", "self", ".", "_temb_std", "!=", "-", "1", ":", "\n", "            ", "task_emb", ".", "add", "(", "torch", ".", "randn_like", "(", "task_emb", ")", "*", "self", ".", "_temb_std", ")", "\n", "\n", "# Concatenate additional embeddings to task embedding, if given.", "\n", "", "if", "self", ".", "requires_ext_input", "and", "ext_inputs", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'The network was created to expect additional '", "+", "\n", "'inputs, thus parameter \"ext_inputs\" has to '", "+", "\n", "'be specified.'", ")", "\n", "", "elif", "not", "self", ".", "requires_ext_input", "and", "ext_inputs", "is", "not", "None", ":", "\n", "            ", "raise", "Exception", "(", "'The network was created to not expect '", "+", "\n", "'additional embeddings, thus parameter '", "+", "\n", "'\"ext_inputs\" cannot be specified.'", ")", "\n", "\n", "", "if", "ext_inputs", "is", "not", "None", ":", "\n", "# FIXME at the moment, we only process one task embedding at a time,", "\n", "# thus additional embeddings define the batch size.", "\n", "            ", "batch_size", "=", "ext_inputs", ".", "shape", "[", "0", "]", "\n", "task_emb", "=", "task_emb", ".", "expand", "(", "batch_size", ",", "self", ".", "_te_dim", ")", "\n", "h", "=", "torch", ".", "cat", "(", "[", "task_emb", ",", "ext_inputs", "]", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "1", "\n", "h", "=", "task_emb", ".", "expand", "(", "batch_size", ",", "self", ".", "_te_dim", ")", "\n", "\n", "", "if", "self", ".", "_noise_dim", "!=", "-", "1", ":", "\n", "            ", "if", "self", ".", "training", ":", "\n", "                ", "eps", "=", "torch", ".", "randn", "(", "(", "batch_size", ",", "self", ".", "_noise_dim", ")", ")", "\n", "", "else", ":", "\n", "                ", "eps", "=", "torch", ".", "zeros", "(", "(", "batch_size", ",", "self", ".", "_noise_dim", ")", ")", "\n", "", "if", "h", ".", "is_cuda", ":", "\n", "                ", "eps", "=", "eps", ".", "to", "(", "h", ".", "get_device", "(", ")", ")", "\n", "", "h", "=", "torch", ".", "cat", "(", "[", "h", ",", "eps", "]", ",", "dim", "=", "1", ")", "\n", "\n", "# Hidden activations.", "\n", "", "for", "i", "in", "range", "(", "0", ",", "len", "(", "self", ".", "_hidden_dims", ")", ",", "2", "if", "self", ".", "_use_bias", "else", "1", ")", ":", "\n", "            ", "b", "=", "None", "\n", "if", "self", ".", "_use_bias", ":", "\n", "                ", "b", "=", "weights", "[", "i", "+", "1", "]", "\n", "", "h", "=", "F", ".", "linear", "(", "h", ",", "weights", "[", "i", "]", ",", "bias", "=", "b", ")", "\n", "if", "self", ".", "_act_fn", "is", "not", "None", ":", "\n", "                ", "h", "=", "self", ".", "_act_fn", "(", "h", ")", "\n", "", "if", "self", ".", "_dropout", "is", "not", "None", ":", "\n", "                ", "h", "=", "self", ".", "_dropout", "(", "h", ")", "\n", "", "", "outputs", "=", "[", "]", "\n", "j", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_hidden_dims", ")", ",", "len", "(", "self", ".", "_theta_shapes", ")", ",", "\n", "2", "if", "self", ".", "_use_bias", "else", "1", ")", ":", "\n", "            ", "b", "=", "None", "\n", "if", "self", ".", "_use_bias", ":", "\n", "                ", "b", "=", "weights", "[", "i", "+", "1", "]", "\n", "", "W", "=", "F", ".", "linear", "(", "h", ",", "weights", "[", "i", "]", ",", "bias", "=", "b", ")", "\n", "W", "=", "W", ".", "view", "(", "batch_size", ",", "*", "self", ".", "target_shapes", "[", "j", "]", ")", "\n", "if", "squeeze", ":", "\n", "                ", "W", "=", "torch", ".", "squeeze", "(", "W", ",", "dim", "=", "0", ")", "\n", "", "if", "self", ".", "_shifts", "is", "not", "None", ":", "# FIXME temporary test!", "\n", "                ", "W", "+=", "self", ".", "_shifts", "[", "j", "]", "\n", "", "outputs", ".", "append", "(", "W", ")", "\n", "j", "+=", "1", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.hyper_model.HyperNetwork.apply_hyperfan_init": [[414, 669], ["range", "range", "ValueError", "ValueError", "len", "len", "len", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init._calculate_fan_in_and_fan_out", "torch.nn.init._calculate_fan_in_and_fan_out", "math.sqrt", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "torch.nn.init._no_grad_uniform_", "utils.init_utils.xavier_fan_in_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.kaiming_uniform_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "len", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.constant_", "len", "max", "utils.init_utils.calc_fan_in_and_out", "math.sqrt", "utils.init_utils.calc_fan_in_and_out", "len", "ValueError"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.init_utils.xavier_fan_in_", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.init_utils.calc_fan_in_and_out", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.init_utils.calc_fan_in_and_out"], ["", "def", "apply_hyperfan_init", "(", "self", ",", "method", "=", "'in'", ",", "use_xavier", "=", "False", ",", "\n", "temb_var", "=", "1.", ",", "ext_inp_var", "=", "1.", ")", ":", "\n", "        ", "r\"\"\"Initialize the network using hyperfan init.\n\n        Hyperfan initialization was developed in the following paper for this\n        kind of hypernetwork\n\n            \"Principled Weight Initialization for Hypernetworks\"\n            https://openreview.net/forum?id=H1lma24tPB\n\n        The initialization is based on the following idea: When the main network\n        would be initialized using Xavier or Kaiming init, then variance of\n        activations (fan-in) or gradients (fan-out) would be preserved by using\n        a proper variance for the initial weight distribution (assuming certain\n        assumptions hold at initialization, which are different for Xavier and\n        Kaiming).\n\n        When using these kind of initializations in the hypernetwork, then the\n        variance of the initial main net weight distribution would simply equal\n        the variance of the input embeddings (which can lead to exploding\n        activations, e.g., for fan-in inits).\n\n        The above mentioned paper proposes a quick fix for the type of hypernets\n        which have a separate output head per weight tensor in the main network\n        (which is the case for this hypernetwork class).\n\n        Assuming that input embeddings are initialized with a certain variance\n        (e.g., 1) and we use Xavier or Kaiming init for the hypernet, then the\n        variance of the last hidden activation will also be 1.\n\n        Then, we can modify the variance of the weights of each output head in\n        the hypernet to obtain the variance for the main net weight tensors that\n        we would typically obtain when applying Xavier or Kaiming to the main\n        network directly.\n\n        Warning:\n            This method currently assumes that 1D target tensors (cmp.\n            constructor argument ``target_shapes``) are bias vectors in the\n            main network.\n\n        Warning:\n            To compute the hyperfan-out initialization of bias vectors, we need\n            access to the fan-in of the layer, which we can only compute based\n            on the corresponding weight tensor in the same layer. Since there is\n            no clean way of matching a bias shape to its corresponging weight\n            tensor shape we use the following heuristic, which should be correct\n            for most main networks. We assume that the shape directly preceding\n            a bias shape in the constructor argument ``target_shapes`` is the\n            corresponding weight tensor.\n\n        **Variance of the hypernet input**\n\n        In general, the input to the hypernetwork can be a concatenation of\n        multiple embeddings (see description of arguments ``temb_var`` and\n        ``ext_inp_var``).\n\n        Let's denote the complete hypernetwork input by\n        :math:`\\mathbf{x} \\in \\mathbb{R}^n`, which consists of a task embedding\n        :math:`\\mathbf{e} \\in \\mathbb{R}^{n_e}` and an external input\n        :math:`\\mathbf{c} \\in \\mathbb{R}^{n_c}`, i.e.,\n\n        .. math::\n\n            \\mathbf{x} = \\begin{bmatrix} \\\n            \\mathbf{e} \\\\ \\\n            \\mathbf{c} \\\n            \\end{bmatrix}\n\n        We simply define the variance of an input :math:`\\text{Var}(x_j)` as\n        the weighted average of the individual variances, i.e.,\n\n        .. math::\n\n            \\text{Var}(x_j) \\equiv \\frac{n_e}{n_e+n_c} \\text{Var}(e) + \\\n                \\frac{n_c}{n_e+n_c} \\text{Var}(c)\n\n        To see that this is correct, consider a linear layer\n        :math:`\\mathbf{y} = W \\mathbf{x}` or\n\n        .. math::\n\n            y_i &= \\sum_j w_{ij} x_j \\\\ \\\n                &= \\sum_{j=1}^{n_e} w_{ij} e_j + \\\n                   \\sum_{j=n_e+1}^{n_e+n_c} w_{ij} c_{j-n_e}\n\n        Hence, we can compute the variance of :math:`y_i` as follows (assuming\n        the typical Xavier assumptions):\n\n        .. math::\n\n            \\text{Var}(y) &= n_e \\text{Var}(w) \\text{Var}(e) + \\\n                             n_c \\text{Var}(w) \\text{Var}(c) \\\\ \\\n                          &= \\frac{n_e}{n_e+n_c} \\text{Var}(e) + \\\n                             \\frac{n_c}{n_e+n_c} \\text{Var}(c)\n\n        Note, that Xavier would have initialized :math:`W` using\n        :math:`\\text{Var}(w) = \\frac{1}{n} = \\frac{1}{n_e+n_c}`.\n\n        Note:\n            This method will automatically incorporate the noise embedding that\n            is inputted into the network if constructor argument ``noise_dim``\n            was set.\n\n        Note:\n            All hypernet inputs should be zero mean.\n\n        Args:\n            method (str): The type of initialization that should be applied.\n                Possible options are:\n\n                - ``in``: Use `Hyperfan-in`.\n                - ``out``: Use `Hyperfan-out`.\n                - ``harmonic``: Use the harmonic mean of the `Hyperfan-in` and\n                  `Hyperfan-out` init.\n            use_xavier (bool): Whether Kaiming (``False``) or Xavier (``True``)\n                init should be used.\n            temb_var (float): The initial variance of the task embeddings.\n\n                .. note::\n                    If ``temb_std`` was set in the constructor, then this method\n                    will automatically correct the provided ``temb_var`` as \n                    follows: :code:`temb_var += temb_std**2`.\n            ext_inp_var (float): The initial variance of the external input.\n                Only needs to be specified if external inputs are provided\n                (see argument ``ce_dim`` of constructor).\n        \"\"\"", "\n", "# FIXME If the network has external inputs and task embeddings, then", "\n", "# both these inputs might have different variances. Thus, a single", "\n", "# parameter `input_variance` might not be sufficient.", "\n", "# Now, we assume that the user provides a proper variance. We could", "\n", "# simplify the job for him by providing multiple arguments and compute", "\n", "# the weighting ourselves.", "\n", "\n", "# FIXME Handle constructor arguments `noise_dim` and `temb_std`.", "\n", "# Note, we would jost need to add `temb_std**2` to the variance of", "\n", "# task embeddings, since the variance of a sum of uncorrelated RVs is", "\n", "# just the sum of the individual variances.", "\n", "\n", "if", "method", "not", "in", "[", "'in'", ",", "'out'", ",", "'harmonic'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid value for argument \"method\".'", ")", "\n", "", "if", "not", "self", ".", "has_theta", ":", "\n", "            ", "raise", "ValueError", "(", "'Hypernet without internal weights can\\'t be '", "+", "\n", "'initialized.'", ")", "\n", "\n", "### Compute input variance ###", "\n", "", "if", "self", ".", "_temb_std", "!=", "-", "1", ":", "\n", "# Sum of uncorrelated variables.", "\n", "            ", "temb_var", "+=", "self", ".", "_temb_std", "**", "2", "\n", "\n", "", "assert", "self", ".", "_size_ext_input", "is", "None", "or", "self", ".", "_size_ext_input", ">", "0", "\n", "assert", "self", ".", "_noise_dim", "==", "-", "1", "or", "self", ".", "_noise_dim", ">", "0", "\n", "\n", "inp_dim", "=", "self", ".", "_te_dim", "+", "(", "self", ".", "_size_ext_input", "if", "self", ".", "_size_ext_input", "is", "not", "None", "else", "0", ")", "+", "(", "self", ".", "_noise_dim", "if", "self", ".", "_noise_dim", "!=", "-", "1", "else", "0", ")", "\n", "\n", "input_variance", "=", "(", "self", ".", "_te_dim", "/", "inp_dim", ")", "*", "temb_var", "\n", "if", "self", ".", "_size_ext_input", "is", "not", "None", ":", "\n", "            ", "input_variance", "+=", "(", "self", ".", "_size_ext_input", "/", "inp_dim", ")", "*", "ext_inp_var", "\n", "", "if", "self", ".", "_noise_dim", "!=", "-", "1", ":", "\n", "            ", "input_variance", "+=", "(", "self", ".", "_noise_dim", "/", "inp_dim", ")", "*", "1.", "\n", "\n", "### Initialize hidden layers to preserve variance ###", "\n", "# We initialize biases with 0 (see Xavier assumption 4 in the Hyperfan", "\n", "# paper). Otherwise, we couldn't ignore the biases when computing the", "\n", "# output variance of a layer.", "\n", "# Note, we have to use fan-in init for the hidden layer to ensure the", "\n", "# property, that we preserve the input variance.", "\n", "\n", "", "for", "i", "in", "range", "(", "0", ",", "len", "(", "self", ".", "_hidden_dims", ")", ",", "2", "if", "self", ".", "_use_bias", "else", "1", ")", ":", "\n", "#W = self.theta[i]", "\n", "            ", "if", "use_xavier", ":", "\n", "                ", "iutils", ".", "xavier_fan_in_", "(", "self", ".", "theta", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "kaiming_uniform_", "(", "self", ".", "theta", "[", "i", "]", ",", "mode", "=", "'fan_in'", ",", "\n", "nonlinearity", "=", "'relu'", ")", "\n", "\n", "", "if", "self", ".", "_use_bias", ":", "\n", "#b = self.theta[i+1]", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "self", ".", "theta", "[", "i", "+", "1", "]", ",", "0", ")", "\n", "\n", "### Initialize output heads ###", "\n", "", "", "c_relu", "=", "1", "if", "use_xavier", "else", "2", "\n", "# FIXME Not a proper way to figure out whether the hnet produces", "\n", "# bias vectors in the mnet.", "\n", "c_bias", "=", "1", "\n", "for", "s", "in", "self", ".", "target_shapes", ":", "\n", "            ", "if", "len", "(", "s", ")", "==", "1", ":", "\n", "                ", "c_bias", "=", "2", "\n", "break", "\n", "# This is how we should do it instead.", "\n", "#c_bias = 2 if mnet.has_bias else 1", "\n", "\n", "", "", "j", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "_hidden_dims", ")", ",", "len", "(", "self", ".", "_theta_shapes", ")", ",", "\n", "2", "if", "self", ".", "_use_bias", "else", "1", ")", ":", "\n", "\n", "# All output heads are linear layers. The biases of these linear", "\n", "# layers (called gamma and beta in the paper) are simply initialized", "\n", "# to zero.", "\n", "            ", "if", "self", ".", "_use_bias", ":", "\n", "#b = self.theta[i+1]", "\n", "                ", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "self", ".", "theta", "[", "i", "+", "1", "]", ",", "0", ")", "\n", "\n", "# We are not interested in the fan-out, since the fan-out is just", "\n", "# the number of elements in the corresponding main network tensor.", "\n", "# `fan-in` is called `d_k` in the paper and is just the size of the", "\n", "# last hidden layer.", "\n", "", "fan_in", ",", "_", "=", "torch", ".", "nn", ".", "init", ".", "_calculate_fan_in_and_fan_out", "(", "self", ".", "theta", "[", "i", "]", ")", "\n", "\n", "out_shape", "=", "self", ".", "target_shapes", "[", "j", "]", "\n", "\n", "# FIXME 1D output tensors don't need to be bias vectors. They can", "\n", "# be arbitrary embeddings or, for instance, batchnorm weights.", "\n", "if", "len", "(", "out_shape", ")", "==", "1", ":", "# Assume output is bias vector.", "\n", "                ", "m_fan_out", "=", "out_shape", "[", "0", "]", "\n", "\n", "# NOTE For the hyperfan-out init, we also need to know the", "\n", "# fan-in of the layer.", "\n", "# FIXME We have no proper way at the moment to get the correct", "\n", "# fan-in of the layer this bias vector belongs to.", "\n", "if", "j", ">", "0", "and", "len", "(", "self", ".", "target_shapes", "[", "j", "-", "1", "]", ")", ">", "1", ":", "\n", "                    ", "m_fan_in", ",", "_", "=", "iutils", ".", "calc_fan_in_and_out", "(", "self", ".", "target_shapes", "[", "j", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "# FIXME Quick-fix.", "\n", "                    ", "m_fan_in", "=", "m_fan_out", "\n", "\n", "", "var_in", "=", "c_relu", "/", "(", "2.", "*", "fan_in", "*", "input_variance", ")", "\n", "num", "=", "c_relu", "*", "(", "1.", "-", "m_fan_in", "/", "m_fan_out", ")", "\n", "denom", "=", "fan_in", "*", "input_variance", "\n", "var_out", "=", "max", "(", "0", ",", "num", "/", "denom", ")", "\n", "\n", "", "else", ":", "\n", "                ", "m_fan_in", ",", "m_fan_out", "=", "iutils", ".", "calc_fan_in_and_out", "(", "out_shape", ")", "\n", "\n", "var_in", "=", "c_relu", "/", "(", "c_bias", "*", "m_fan_in", "*", "fan_in", "*", "input_variance", ")", "\n", "var_out", "=", "c_relu", "/", "(", "m_fan_out", "*", "fan_in", "*", "input_variance", ")", "\n", "\n", "", "if", "method", "==", "'in'", ":", "\n", "                ", "var", "=", "var_in", "\n", "", "elif", "method", "==", "'out'", ":", "\n", "                ", "var", "=", "var_out", "\n", "", "elif", "method", "==", "'harmonic'", ":", "\n", "                ", "var", "=", "2", "*", "(", "1.", "/", "var_in", "+", "1.", "/", "var_out", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'Method %s invalid.'", "%", "method", ")", "\n", "\n", "# Initialize output head weight tensor using `var`.", "\n", "", "std", "=", "math", ".", "sqrt", "(", "var", ")", "\n", "a", "=", "math", ".", "sqrt", "(", "3.0", ")", "*", "std", "\n", "torch", ".", "nn", ".", "init", ".", "_no_grad_uniform_", "(", "self", ".", "theta", "[", "i", "]", ",", "-", "a", ",", "a", ")", "\n", "\n", "j", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.__init__": [[66, 114], ["torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.Module.__init__", "toy_example.main_model.MainNetwork.weight_shapes", "toy_example.main_model.MainNetwork", "toy_example.main_model.MainNetwork.weight_shapes", "toy_example.main_model.MainNetwork", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "numpy.sum", "print", "list", "numpy.prod", "reversed", "task_recognition_model.RecognitionNet.parameters"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.weight_shapes", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.weight_shapes"], ["def", "__init__", "(", "self", ",", "n_in", ",", "n_tasks", ",", "dim_z", "=", "8", ",", "enc_layers", "=", "[", "10", ",", "10", "]", ",", "\n", "activation_fn", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "use_bias", "=", "True", ")", ":", "\n", "        ", "\"\"\"Initialize the network.\n\n        Args:\n            n_in: Input size (input dim of encoder and output dim of decoder).\n            n_tasks: The maximum number of tasks to be detected (size of\n                softmax layer).\n            dim_z: Dimensionality of latent space z.\n            enc_layers: A list of integers, each denoting the size of a hidden\n                layer in the encoder. The decoder will have layer sizes in\n                reverse order.\n            activation_fn: The nonlinearity used in hidden layers. If None, no\n                nonlinearity will be applied.\n            use_bias: Whether layers may have bias terms.\n        \"\"\"", "\n", "super", "(", "RecognitionNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_n_alpha", "=", "n_tasks", "\n", "self", ".", "_n_nu_z", "=", "2", "*", "dim_z", "\n", "self", ".", "_n_z", "=", "dim_z", "\n", "\n", "## Enoder", "\n", "encoder_shapes", "=", "MainNetwork", ".", "weight_shapes", "(", "n_in", "=", "n_in", ",", "\n", "n_out", "=", "self", ".", "_n_alpha", "+", "self", ".", "_n_nu_z", ",", "hidden_layers", "=", "enc_layers", ",", "\n", "use_bias", "=", "use_bias", ")", "\n", "self", ".", "_encoder", "=", "MainNetwork", "(", "encoder_shapes", ",", "activation_fn", "=", "activation_fn", ",", "\n", "use_bias", "=", "use_bias", ",", "no_weights", "=", "False", ",", "\n", "dropout_rate", "=", "-", "1", ",", "verbose", "=", "False", ")", "\n", "self", ".", "_weights_enc", "=", "self", ".", "_encoder", ".", "weights", "\n", "\n", "## Decoder", "\n", "decoder_shapes", "=", "MainNetwork", ".", "weight_shapes", "(", "n_in", "=", "self", ".", "_n_alpha", "+", "self", ".", "_n_z", ",", "\n", "n_out", "=", "n_in", ",", "hidden_layers", "=", "list", "(", "reversed", "(", "enc_layers", ")", ")", ",", "\n", "use_bias", "=", "use_bias", ")", "\n", "self", ".", "_decoder", "=", "MainNetwork", "(", "decoder_shapes", ",", "activation_fn", "=", "activation_fn", ",", "\n", "use_bias", "=", "use_bias", ",", "no_weights", "=", "False", ",", "\n", "dropout_rate", "=", "-", "1", ",", "verbose", "=", "False", ")", "\n", "self", ".", "_weights_dec", "=", "self", ".", "_decoder", ".", "weights", "\n", "\n", "## Prior", "\n", "# Note, when changing the prior, one has to change the method", "\n", "# \"prior_matching\".", "\n", "self", ".", "_mu_z", "=", "torch", ".", "zeros", "(", "dim_z", ")", "\n", "self", ".", "_sigma_z", "=", "torch", ".", "ones", "(", "dim_z", ")", "\n", "\n", "n_params", "=", "np", ".", "sum", "(", "[", "np", ".", "prod", "(", "p", ".", "shape", ")", "for", "p", "in", "self", ".", "parameters", "(", ")", "]", ")", "\n", "print", "(", "'Constructed recognition model with %d parameters.'", "%", "n_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.dim_alpha": [[115, 123], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dim_alpha", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute dim_alpha.\n\n        Returns:\n            Size of alpha layer.\n        \"\"\"", "\n", "return", "self", ".", "_n_alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.dim_z": [[124, 132], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "dim_z", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute dim_z.\n\n        Returns:\n            Size of z layer.\n        \"\"\"", "\n", "return", "self", ".", "_n_z", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.encoder_weights": [[133, 141], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "encoder_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute encoder_weights.\n\n        Returns:\n            A torch.nn.ParameterList.\n        \"\"\"", "\n", "return", "self", ".", "_weights_enc", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.decoder_weights": [[142, 150], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "decoder_weights", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute decoder_weights.\n\n        Returns:\n            A torch.nn.ParameterList.\n        \"\"\"", "\n", "return", "self", ".", "_weights_dec", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.forward": [[151, 167], ["task_recognition_model.RecognitionNet.encode", "task_recognition_model.RecognitionNet.decode"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.encode", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.decode"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"This function computes\n            x_rec = decode(encode(x))\n\n        Note, the function utilizes the class members \"encode\" and \"decode\".\n\n        Args:\n            x: The input to the \"autoencoder\".\n\n        Returns:\n            x_rec: The reconstruction of the input.\n        \"\"\"", "\n", "alpha", ",", "_", ",", "z", "=", "self", ".", "encode", "(", "x", ")", "\n", "x_rec", "=", "self", ".", "decode", "(", "alpha", ",", "z", ")", "\n", "\n", "return", "x_rec", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.encode": [[168, 211], ["task_recognition_model.RecognitionNet._encoder.forward", "torch.softmax", "torch.softmax", "torch.softmax", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.distributions.Normal().rsample", "torch.distributions.Normal().rsample", "torch.distributions.Normal().rsample", "torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "encode", "(", "self", ",", "x", ",", "ret_log_alpha", "=", "False", ",", "encoder_weights", "=", "None", ")", ":", "\n", "        ", "\"\"\"Encode a sample x -> \"recognize the task of x\".\n        \n        Args:\n            x: An input sample (from which a task should be inferred).\n            ret_log_alpha (optional): Whether the log-softmax distribution of\n                the output layer alpha should be returned as well.\n            encoder_weights (optional): If given, these will be the parameters\n                used in the encoder rather than the ones maintained object\n                internally.\n\n        Returns:\n            (tuple): Tuple containing:\n\n            - **alpha**: The softmax output (task classification output).\n            - **nu_z**: The parameters of the latent distribution from which \"z\" is\n              sampled (i.e., the actual output of the encoder besides alpha).\n              Note, that these parameters are the cooncatenated means and\n              log-variances of the latent distribution.\n            - **z**: A latent space embedding retrieved via the\n              reparametrization trick.\n            - **log_alpha** (optional): The log softmax activity of alpha.\n        \"\"\"", "\n", "phi_e", "=", "None", "\n", "if", "encoder_weights", "is", "not", "None", ":", "\n", "            ", "phi_e", "=", "encoder_weights", "\n", "\n", "", "h", "=", "self", ".", "_encoder", ".", "forward", "(", "x", ",", "weights", "=", "phi_e", ")", "\n", "\n", "h_alpha", "=", "h", "[", ":", ",", ":", "self", ".", "_n_alpha", "]", "\n", "alpha", "=", "F", ".", "softmax", "(", "h_alpha", ",", "dim", "=", "1", ")", "\n", "\n", "params_z", "=", "h", "[", ":", ",", "self", ".", "_n_alpha", ":", "]", "\n", "mu_z", "=", "params_z", "[", ":", ",", ":", "self", ".", "_n_z", "]", "\n", "logvar_z", "=", "params_z", "[", ":", ",", "self", ".", "_n_z", ":", "]", "\n", "\n", "std_z", "=", "torch", ".", "exp", "(", "0.5", "*", "logvar_z", ")", "\n", "z", "=", "Normal", "(", "mu_z", ",", "std_z", ")", ".", "rsample", "(", ")", "\n", "\n", "if", "ret_log_alpha", ":", "\n", "            ", "return", "alpha", ",", "params_z", ",", "z", ",", "F", ".", "log_softmax", "(", "h_alpha", ",", "dim", "=", "1", ")", "\n", "\n", "", "return", "alpha", ",", "params_z", ",", "z", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.decode": [[212, 235], ["task_recognition_model.RecognitionNet._decoder.forward", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["", "def", "decode", "(", "self", ",", "alpha", ",", "z", ",", "decoder_weights", "=", "None", ")", ":", "\n", "        ", "\"\"\"Decode a latent representation back to a sample.\n        If alpha is a 1-hot encoding denoting a specific task and z are latent\n        space samples, the decoding can be seen as \"replay\" of task samples.\n\n        Args:\n            alpha: See return value of method \"encode\".\n            z: See return value of method \"encode\".\n            decoder_weights (optional): If given, these will be the parameters\n                used in the decoder rather than the ones maintained object\n                internally.\n\n        Returns:\n            x_dec: The decoded sample.\n        \"\"\"", "\n", "phi_d", "=", "None", "\n", "if", "decoder_weights", "is", "not", "None", ":", "\n", "            ", "phi_d", "=", "decoder_weights", "\n", "\n", "", "x_dec", "=", "self", ".", "_decoder", ".", "forward", "(", "torch", ".", "cat", "(", "[", "alpha", ",", "z", "]", ",", "dim", "=", "1", ")", ",", "\n", "weights", "=", "phi_d", ")", "\n", "\n", "return", "x_dec", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.prior_samples": [[236, 246], ["torch.distributions.Normal().rsample", "torch.distributions.Normal().rsample", "torch.distributions.Normal().rsample", "torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal"], "methods", ["None"], ["", "def", "prior_samples", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"Obtain a batch of samples from the prior for the latent space z.\n\n        Args:\n            batch_size: Number of samples to acquire.\n\n        Returns:\n            A torch tensor of samples.\n        \"\"\"", "\n", "return", "Normal", "(", "self", ".", "_mu_z", ",", "self", ".", "_sigma_z", ")", ".", "rsample", "(", "[", "batch_size", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.prior_matching": [[247, 263], ["logvar_z.exp", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "mu_z.pow"], "methods", ["None"], ["", "def", "prior_matching", "(", "self", ",", "nu_z", ")", ":", "\n", "        ", "\"\"\"Compute the prior matching term between the Gaussian described by\n        the parameters \"nu_z\" and a standard normal distribution N(0, I).\n\n        Args:\n            nu_z: Part of the encoder output.\n\n        Returns:\n            The value of the prior matching loss.\n        \"\"\"", "\n", "mu_z", "=", "nu_z", "[", ":", ",", ":", "self", ".", "_n_z", "]", "\n", "logvar_z", "=", "nu_z", "[", ":", ",", "self", ".", "_n_z", ":", "]", "\n", "\n", "var_z", "=", "logvar_z", ".", "exp", "(", ")", "\n", "\n", "return", "-", "0.5", "*", "torch", ".", "sum", "(", "1", "+", "logvar_z", "-", "mu_z", ".", "pow", "(", "2", ")", "-", "var_z", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.task_cross_entropy": [[264, 276], ["torch.nll_loss", "torch.nll_loss", "torch.nll_loss"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "task_cross_entropy", "(", "log_alpha", ",", "target", ")", ":", "\n", "        ", "\"\"\"A call to pytorch \"nll_loss\".\n\n        Args:\n            log_alpha: The log softmax activity of the alpha layer.\n            target: A vector of task ids.\n\n        Returns:\n            Cross-entropy loss\n        \"\"\"", "\n", "return", "F", ".", "nll_loss", "(", "log_alpha", ",", "target", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.reconstruction_loss": [[277, 290], ["torch.mse_loss", "torch.mse_loss", "torch.mse_loss"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "reconstruction_loss", "(", "x", ",", "x_rec", ")", ":", "\n", "        ", "\"\"\"A call to pytorch \"mse_loss\"\n\n        Args:\n            x: An input sample.\n            x_rec: The reconstruction provided by the recognition AE when seeing\n                input \"x\".\n\n        Returns:\n            The MSE loss between x and x_rec.\n        \"\"\"", "\n", "return", "F", ".", "mse_loss", "(", "x", ",", "x_rec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test": [[47, 210], ["print", "mnet.eval", "data_handlers[].plot_datasets", "matplotlib.figure", "matplotlib.title", "matplotlib.scatter", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.xticks", "matplotlib.legend", "print", "print", "hnet.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "len", "numpy.zeros", "range", "os.path.join", "numpy.arange", "matplotlib.scatter", "baseline_mse.items", "numpy.arange", "matplotlib.savefig", "matplotlib.show", "matplotlib.figure", "matplotlib.title", "matplotlib.scatter", "matplotlib.ylabel", "matplotlib.xlabel", "matplotlib.xticks", "matplotlib.show", "print", "numpy.zeros", "data.get_test_inputs", "data.get_test_outputs", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "torch.mse_loss", "inputs.append", "predictions.append", "numpy.arange", "matplotlib.scatter", "os.path.join", "numpy.arange", "numpy.arange", "mnet.forward", "hnet.forward", "list", "data.input_to_torch_tensor.data.cpu().numpy", "torch.empty_like.data.cpu().numpy", "torch.cat", "torch.cat", "torch.cat", "numpy.arange", "np.zeros.mean", "np.zeros.std", "mnet.forward", "torch.empty_like", "torch.empty_like", "torch.empty_like", "torch.empty", "torch.empty", "torch.empty", "range", "acc.cpu().numpy", "range", "type", "torch.norm", "torch.norm", "torch.norm", "print", "X[].view", "isinstance", "alpha.argmax", "hnet.forward", "mnet.forward", "data.input_to_torch_tensor.data.cpu", "torch.empty_like.data.cpu", "d.clone().view", "rnet.encode", "rnet.forward", "acc.cpu", "d.clone"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.plot_datasets", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.encode", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward"], ["def", "test", "(", "task_id", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "config", ",", "writer", ",", "logger", ",", "\n", "train_iter", "=", "None", ",", "task_emb", "=", "None", ",", "cl_scenario", "=", "None", ",", "test_size", "=", "None", ")", ":", "\n", "    ", "\"\"\"Evaluate the current performance using the test set.\n\n    Note:\n        The hypernetwork ``hnet`` may be ``None``, in which case it is assumed\n        that the main network ``mnet`` has internal weights.\n\n    Args:\n        (....): See docstring of function :func:`train`.\n        train_iter (int, optional): The current training iteration. If given, it\n            is used for tensorboard logging.\n        task_emb (torch.Tensor, optional): Task embedding. If given, no task ID\n            will be provided to the hypernetwork. This might be useful if the\n            performance of other than the trained task embeddings should be\n            tested.\n\n            .. note::\n                This option may only be used for ``cl_scenario=1``. It doesn't\n                make sense if the task ID has to be inferred.\n        cl_scenario (int, optional): In case the system should be tested on\n            another CL scenario than the one user-defined in ``config``.\n            \n            .. note::\n                It is up to the user to ensure that the CL scnearios are\n                compatible in this implementation.\n        test_size (int, optional): In case the testing shouldn't be performed\n            on the entire test set, this option can be used to specify the\n            number of test samples to be used.\n\n    Returns:\n        (tuple): Tuple containing:\n\n        - **test_acc**: Test accuracy on classification task.\n        - **task_acc**: Task prediction accuracy (always 100% for **CL1**).\n    \"\"\"", "\n", "if", "cl_scenario", "is", "None", ":", "\n", "        ", "cl_scenario", "=", "config", ".", "cl_scenario", "\n", "", "else", ":", "\n", "        ", "assert", "cl_scenario", "in", "[", "1", ",", "2", ",", "3", "]", "\n", "\n", "# `task_emb` ignored for other cl scenarios!", "\n", "", "assert", "task_emb", "is", "None", "or", "cl_scenario", "==", "1", ",", "'\"task_emb\" may only be specified for CL1, as we infer the '", "+", "'embedding for other scenarios.'", "\n", "\n", "mnet", ".", "eval", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "hnet", ".", "eval", "(", ")", "\n", "\n", "", "if", "train_iter", "is", "None", ":", "\n", "        ", "logger", ".", "info", "(", "'### Test run ...'", ")", "\n", "", "else", ":", "\n", "        ", "logger", ".", "info", "(", "'# Testing network before running training step %d ...'", "%", "train_iter", ")", "\n", "\n", "# We need to tell the main network, which batch statistics to use, in case", "\n", "# batchnorm is used and we checkpoint the batchnorm stats.", "\n", "", "mnet_kwargs", "=", "{", "}", "\n", "if", "mnet", ".", "batchnorm_layers", "is", "not", "None", ":", "\n", "        ", "if", "config", ".", "bn_distill_stats", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "", "elif", "not", "config", ".", "bn_no_running_stats", "and", "not", "config", ".", "bn_no_stats_checkpointing", ":", "\n", "# Specify current task as condition to select correct", "\n", "# running stats.", "\n", "            ", "mnet_kwargs", "[", "'condition'", "]", "=", "task_id", "\n", "\n", "if", "task_emb", "is", "not", "None", ":", "\n", "# NOTE `task_emb` might have nothing to do with `task_id`.", "\n", "                ", "logger", ".", "warning", "(", "'Using batch statistics accumulated for task '", "+", "\n", "'%d for batchnorm, but testing is '", "%", "task_id", "+", "\n", "'performed using a given task embedding.'", ")", "\n", "\n", "", "", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "batch_size", "=", "config", ".", "val_batch_size", "\n", "# FIXME Assuming all output heads have the same size.", "\n", "n_head", "=", "data", ".", "num_classes", "\n", "\n", "if", "test_size", "is", "None", "or", "test_size", ">=", "data", ".", "num_test_samples", ":", "\n", "            ", "test_size", "=", "data", ".", "num_test_samples", "\n", "", "else", ":", "\n", "# Make sure that we always use the same test samples.", "\n", "            ", "data", ".", "reset_batch_generator", "(", "train", "=", "False", ",", "test", "=", "True", ",", "val", "=", "False", ")", "\n", "logger", ".", "info", "(", "'Note, only part of test set is used for this test '", "+", "\n", "'run!'", ")", "\n", "\n", "", "test_loss", "=", "0.0", "\n", "\n", "# We store all predicted labels and tasks while going over individual", "\n", "# test batches.", "\n", "correct_labels", "=", "np", ".", "empty", "(", "test_size", ",", "np", ".", "int", ")", "\n", "pred_labels", "=", "np", ".", "empty", "(", "test_size", ",", "np", ".", "int", ")", "\n", "correct_tasks", "=", "np", ".", "ones", "(", "test_size", ",", "np", ".", "int", ")", "*", "task_id", "\n", "pred_tasks", "=", "np", ".", "empty", "(", "test_size", ",", "np", ".", "int", ")", "\n", "\n", "curr_bs", "=", "batch_size", "\n", "N_processed", "=", "0", "\n", "\n", "# Sweep through the test set.", "\n", "while", "N_processed", "<", "test_size", ":", "\n", "            ", "if", "N_processed", "+", "curr_bs", ">", "test_size", ":", "\n", "                ", "curr_bs", "=", "test_size", "-", "N_processed", "\n", "", "N_processed", "+=", "curr_bs", "\n", "\n", "batch", "=", "data", ".", "next_test_batch", "(", "curr_bs", ")", "\n", "X", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ")", "\n", "T", "=", "data", ".", "output_to_torch_tensor", "(", "batch", "[", "1", "]", ",", "device", ")", "\n", "\n", "############################", "\n", "### Get main net weights ###", "\n", "############################", "\n", "if", "hnet", "is", "None", ":", "\n", "                ", "weights", "=", "None", "\n", "", "elif", "cl_scenario", ">", "1", ":", "\n", "                ", "raise", "NotImplementedError", "(", ")", "\n", "", "elif", "task_emb", "is", "not", "None", ":", "\n", "                ", "weights", "=", "hnet", ".", "forward", "(", "task_emb", "=", "task_emb", ")", "\n", "", "else", ":", "\n", "                ", "weights", "=", "hnet", ".", "forward", "(", "task_id", "=", "task_id", ")", "\n", "\n", "#######################", "\n", "### Get predictions ###", "\n", "#######################", "\n", "", "Y_hat_logits", "=", "mnet", ".", "forward", "(", "X", ",", "weights", "=", "weights", ",", "**", "mnet_kwargs", ")", "\n", "\n", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "# Select current head.", "\n", "                ", "task_out", "=", "[", "task_id", "*", "n_head", ",", "(", "task_id", "+", "1", ")", "*", "n_head", "]", "\n", "", "elif", "config", ".", "cl_scenario", "==", "2", ":", "\n", "# Only 1 output head.", "\n", "                ", "task_out", "=", "[", "0", ",", "n_head", "]", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", ")", "\n", "# TODO Choose the predicted output head per sample.", "\n", "#task_out = [predicted_task_id[0]*n_head,", "\n", "#            (predicted_task_id[0]+1)*n_head]", "\n", "\n", "", "Y_hat_logits", "=", "Y_hat_logits", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "# We take the softmax after the output neurons are chosen.", "\n", "Y_hat", "=", "F", ".", "softmax", "(", "Y_hat_logits", ",", "dim", "=", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "correct_labels", "[", "N_processed", "-", "curr_bs", ":", "N_processed", "]", "=", "T", ".", "argmax", "(", "dim", "=", "1", ",", "keepdim", "=", "False", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "pred_labels", "[", "N_processed", "-", "curr_bs", ":", "N_processed", "]", "=", "Y_hat", ".", "argmax", "(", "axis", "=", "1", ")", "\n", "\n", "# Set task prediction to 100% if we do not infer it.", "\n", "if", "cl_scenario", ">", "1", ":", "\n", "                ", "raise", "NotImplementedError", "(", ")", "\n", "#pred_tasks[N_processed-curr_bs:N_processed] = \\", "\n", "#    predicted_task_id.cpu().numpy()", "\n", "", "else", ":", "\n", "                ", "pred_tasks", "[", "N_processed", "-", "curr_bs", ":", "N_processed", "]", "=", "task_id", "\n", "\n", "# Note, targets are 1-hot encoded.", "\n", "", "test_loss", "+=", "Classifier", ".", "logit_cross_entropy_loss", "(", "Y_hat_logits", ",", "T", ",", "\n", "reduction", "=", "'sum'", ")", "\n", "\n", "", "class_n_correct", "=", "(", "correct_labels", "==", "pred_labels", ")", ".", "sum", "(", ")", "\n", "test_acc", "=", "100.0", "*", "class_n_correct", "/", "test_size", "\n", "\n", "task_n_correct", "=", "(", "correct_tasks", "==", "pred_tasks", ")", ".", "sum", "(", ")", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.evaluate": [[211, 265], ["mnet.eval", "print", "print", "hnet.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "torch.mse_loss", "print", "data.get_test_inputs", "data.get_test_outputs", "data.get_val_inputs", "data.get_val_outputs", "mnet.forward", "hnet.forward", "mnet.forward", "list", "data.plot_predictions", "range", "data.get_val_inputs.data.cpu().numpy", "mnet.forward.data.cpu().numpy", "data.get_val_inputs.data.cpu", "mnet.forward.data.cpu"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_val_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_val_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.plot_predictions"], ["task_acc", "=", "100.0", "*", "task_n_correct", "/", "test_size", "\n", "\n", "test_loss", "/=", "test_size", "\n", "\n", "msg", "=", "'### Test accuracy of task %d'", "%", "(", "task_id", "+", "1", ")", "+", "(", "' (before training iteration %d)'", "%", "train_iter", "if", "train_iter", "is", "not", "None", "else", "''", ")", "+", "': %.3f'", "%", "(", "test_acc", ")", "+", "(", "' (using a given task embedding)'", "if", "task_emb", "is", "not", "None", "else", "''", ")", "+", "(", "' - task prediction accuracy: %.3f'", "%", "task_acc", "if", "cl_scenario", ">", "1", "else", "''", ")", "\n", "logger", ".", "info", "(", "msg", ")", "\n", "\n", "if", "train_iter", "is", "not", "None", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'test/task_%d/class_accuracy'", "%", "task_id", ",", "\n", "test_acc", ",", "train_iter", ")", "\n", "\n", "if", "config", ".", "cl_scenario", ">", "1", ":", "\n", "                ", "writer", ".", "add_scalar", "(", "'test/task_%d/task_pred_accuracy'", "%", "task_id", ",", "task_acc", ",", "train_iter", ")", "\n", "\n", "", "", "return", "test_acc", ",", "task_acc", "\n", "\n", "", "", "def", "train", "(", "task_id", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "logger", ")", ":", "\n", "    ", "\"\"\"Train the hyper network using the task-specific loss plus a regularizer\n    that should overcome catastrophic forgetting.\n\n    :code:`loss = task_loss + beta * regularizer`.\n\n    Args:\n        task_id: The index of the task on which we train.\n        data: The dataset handler.\n        mnet: The model of the main network.\n        hnet: The model of the hyper network. May be ``None``.\n        device: Torch device (cpu or gpu).\n        config: The command line arguments.\n        shared (argparse.Namespace): Set of variables shared between functions.\n        writer: The tensorboard summary writer.\n        logger: The logger that should be used rather than the print method.\n    \"\"\"", "\n", "start_time", "=", "time", "(", ")", "\n", "\n", "logger", ".", "info", "(", "'Training network ...'", ")", "\n", "\n", "mnet", ".", "train", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "hnet", ".", "train", "(", ")", "\n", "\n", "#################", "\n", "### Optimizer ###", "\n", "#################", "\n", "# Define the optimizers used to train main network and hypernet.", "\n", "", "if", "hnet", "is", "not", "None", ":", "\n", "        ", "theta_params", "=", "list", "(", "hnet", ".", "theta", ")", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.evaluate_rnet": [[266, 305], ["print", "rnet.eval", "torch.no_grad", "torch.no_grad", "torch.no_grad", "data.input_to_torch_tensor", "rnet.encode", "rnet.decode", "alpha.argmax", "rnet.reconstruction_loss", "print", "writer.add_scalar", "writer.add_scalar", "data.get_test_inputs", "data.get_val_inputs", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones", "torch.ones", "torch.ones"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.encode", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.decode", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.reconstruction_loss", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_val_inputs"], ["if", "config", ".", "continue_emb_training", ":", "\n", "            ", "for", "i", "in", "range", "(", "task_id", ")", ":", "# for all previous task embeddings", "\n", "                ", "theta_params", ".", "append", "(", "hnet", ".", "get_task_emb", "(", "i", ")", ")", "\n", "\n", "# Only for the current task embedding.", "\n", "# Important that this embedding is in a different optimizer in case", "\n", "# we use the lookahead.", "\n", "", "", "emb_optimizer", "=", "get_optimizer", "(", "[", "hnet", ".", "get_task_emb", "(", "task_id", ")", "]", ",", "\n", "config", ".", "lr", ",", "momentum", "=", "config", ".", "momentum", ",", "\n", "weight_decay", "=", "config", ".", "weight_decay", ",", "use_adam", "=", "config", ".", "use_adam", ",", "\n", "adam_beta1", "=", "config", ".", "adam_beta1", ",", "use_rmsprop", "=", "config", ".", "use_rmsprop", ")", "\n", "", "else", ":", "\n", "        ", "theta_params", "=", "mnet", ".", "weights", "\n", "emb_optimizer", "=", "None", "\n", "\n", "", "theta_optimizer", "=", "get_optimizer", "(", "theta_params", ",", "config", ".", "lr", ",", "\n", "momentum", "=", "config", ".", "momentum", ",", "weight_decay", "=", "config", ".", "weight_decay", ",", "\n", "use_adam", "=", "config", ".", "use_adam", ",", "adam_beta1", "=", "config", ".", "adam_beta1", ",", "\n", "use_rmsprop", "=", "config", ".", "use_rmsprop", ")", "\n", "\n", "################################", "\n", "### Learning rate schedulers ###", "\n", "################################", "\n", "if", "config", ".", "plateau_lr_scheduler", ":", "\n", "        ", "assert", "(", "config", ".", "epochs", "!=", "-", "1", ")", "\n", "# The scheduler config has been taken from here:", "\n", "# https://keras.io/examples/cifar10_resnet/", "\n", "# Note, we use 'max' instead of 'min' as we look at accuracy rather", "\n", "# than validation loss!", "\n", "plateau_scheduler_theta", "=", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "theta_optimizer", ",", "'max'", ",", "factor", "=", "np", ".", "sqrt", "(", "0.1", ")", ",", "patience", "=", "5", ",", "\n", "min_lr", "=", "0.5e-6", ",", "cooldown", "=", "0", ")", "\n", "plateau_scheduler_emb", "=", "None", "\n", "if", "emb_optimizer", "is", "not", "None", ":", "\n", "            ", "plateau_scheduler_emb", "=", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "emb_optimizer", ",", "'max'", ",", "factor", "=", "np", ".", "sqrt", "(", "0.1", ")", ",", "patience", "=", "5", ",", "\n", "min_lr", "=", "0.5e-6", ",", "cooldown", "=", "0", ")", "\n", "\n", "", "", "if", "config", ".", "lambda_lr_scheduler", ":", "\n", "        ", "assert", "(", "config", ".", "epochs", "!=", "-", "1", ")", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.calc_reg_masks": [[306, 338], ["warnings.warn", "enumerate", "list", "masks.append", "range", "toy_example.main_model.MainNetwork.get_reg_masks"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.get_reg_masks"], ["\n", "def", "lambda_lr", "(", "epoch", ")", ":", "\n", "            ", "\"\"\"Multiplicative Factor for Learning Rate Schedule.\n\n            Computes a multiplicative factor for the initial learning rate based\n            on the current epoch. This method can be used as argument\n            ``lr_lambda`` of class :class:`torch.optim.lr_scheduler.LambdaLR`.\n\n            The schedule is inspired by the Resnet CIFAR-10 schedule suggested\n            here https://keras.io/examples/cifar10_resnet/.\n\n            Args:\n                epoch (int): The number of epochs\n\n            Returns:\n                lr_scale (float32): learning rate scale\n            \"\"\"", "\n", "lr_scale", "=", "1.", "\n", "if", "epoch", ">", "180", ":", "\n", "                ", "lr_scale", "=", "0.5e-3", "\n", "", "elif", "epoch", ">", "160", ":", "\n", "                ", "lr_scale", "=", "1e-3", "\n", "", "elif", "epoch", ">", "120", ":", "\n", "                ", "lr_scale", "=", "1e-2", "\n", "", "elif", "epoch", ">", "80", ":", "\n", "                ", "lr_scale", "=", "1e-1", "\n", "", "return", "lr_scale", "\n", "\n", "", "lambda_scheduler_theta", "=", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "theta_optimizer", ",", "\n", "lambda_lr", ")", "\n", "lambda_scheduler_emb", "=", "None", "\n", "if", "emb_optimizer", "is", "not", "None", ":", "\n", "            ", "lambda_scheduler_emb", "=", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "emb_optimizer", ",", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.train_reg": [[339, 536], ["print", "mnet.train", "hnet.train", "list", "torch.Adam", "torch.Adam", "range", "print", "len", "range", "utils.get_current_targets", "range", "optim.Adam.zero_grad", "optim.Adam.zero_grad", "data.next_train_batch", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "hnet.forward", "mnet.forward", "torch.mse_loss", "F.mse_loss.backward", "optim.Adam.step", "optim.Adam.step", "utils.ewc_regularizer.compute_fisher", "hnet.forward", "torch.nn.ParameterList", "enumerate", "utils.ewc_regularizer.compute_fisher", "list", "range", "fisher_ests.append", "list.append", "hnet.get_task_emb", "train.evaluate", "mnet.train", "hnet.train", "print", "utils.calc_delta_theta", "ewc.ewc_regularizer.backward", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "nn.ParameterList.append", "range", "range", "utils.ewc_regularizer._ewc_buffer_names", "ff.append", "hnet.get_task_emb", "torch.cat", "torch.cat", "torch.cat", "utils.calc_fix_target_reg", "torch.cat", "torch.cat", "torch.cat", "torch.norm", "torch.norm", "torch.norm", "torch.cat", "torch.cat", "torch.cat", "torch.cosine_similarity", "torch.norm", "torch.norm", "torch.norm", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "torch.nn.Parameter", "getattr", "utils.calc_value_preserving_reg", "grad_diff.view", "torch.cat.view", "torch.cat", "torch.cat", "torch.cat", "torch.norm", "torch.norm", "torch.norm", "torch.Tensor", "torch.Tensor", "torch.Tensor", "d.grad.clone().view", "utils.calc_jac_reguarizer", "d.grad.view", "d.view().clone", "utils.ewc_regularizer.ewc_regularizer", "d.view", "d.grad.clone", "d.view"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.get_current_targets", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer.compute_fisher", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer.compute_fisher", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.evaluate", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.optim_step.calc_delta_theta", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer._ewc_buffer_names", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_fix_target_reg", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_value_preserving_reg", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_jac_reguarizer", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.ewc_regularizer.ewc_regularizer"], ["lambda_lr", ")", "\n", "\n", "##############################", "\n", "### Prepare CL Regularizer ###", "\n", "##############################", "\n", "# Whether we will calculate the regularizer.", "\n", "", "", "calc_reg", "=", "task_id", ">", "0", "and", "not", "config", ".", "mnet_only", "and", "config", ".", "beta", ">", "0", "and", "not", "config", ".", "train_from_scratch", "\n", "\n", "# Compute targets when the reg is activated and we are not training", "\n", "# the first task", "\n", "if", "calc_reg", ":", "\n", "        ", "if", "config", ".", "online_target_computation", ":", "\n", "# Compute targets for the regularizer whenever they are needed.", "\n", "# -> Computationally expensive.", "\n", "            ", "targets_hypernet", "=", "None", "\n", "prev_theta", "=", "[", "p", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "p", "in", "hnet", ".", "theta", "]", "\n", "prev_task_embs", "=", "[", "p", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "p", "in", "hnet", ".", "get_task_embs", "(", ")", "]", "\n", "", "else", ":", "\n", "# Compute targets for the regularizer once and keep them all in", "\n", "# memory -> Memory expensive.", "\n", "            ", "targets_hypernet", "=", "hreg", ".", "get_current_targets", "(", "task_id", ",", "hnet", ")", "\n", "prev_theta", "=", "None", "\n", "prev_task_embs", "=", "None", "\n", "\n", "# If we do not want to regularize all outputs (in a multi-head setup).", "\n", "# Note, we don't care whether output heads other than the current one", "\n", "# change.", "\n", "", "regged_outputs", "=", "None", "\n", "if", "config", ".", "cl_scenario", "!=", "2", ":", "\n", "# FIXME We assume here that all tasks have the same output size.", "\n", "            ", "n_y", "=", "data", ".", "num_classes", "\n", "regged_outputs", "=", "[", "list", "(", "range", "(", "i", "*", "n_y", ",", "(", "i", "+", "1", ")", "*", "n_y", ")", ")", "for", "i", "in", "\n", "range", "(", "task_id", ")", "]", "\n", "\n", "# We need to tell the main network, which batch statistics to use, in case", "\n", "# batchnorm is used and we checkpoint the batchnorm stats.", "\n", "", "", "mnet_kwargs", "=", "{", "}", "\n", "if", "mnet", ".", "batchnorm_layers", "is", "not", "None", ":", "\n", "        ", "if", "config", ".", "bn_distill_stats", ":", "\n", "            ", "raise", "NotImplementedError", "(", ")", "\n", "", "elif", "not", "config", ".", "bn_no_running_stats", "and", "not", "config", ".", "bn_no_stats_checkpointing", ":", "\n", "# Specify current task as condition to select correct", "\n", "# running stats.", "\n", "            ", "mnet_kwargs", "[", "'condition'", "]", "=", "task_id", "\n", "\n", "######################", "\n", "### Start training ###", "\n", "######################", "\n", "\n", "", "", "iter_per_epoch", "=", "-", "1", "\n", "if", "config", ".", "epochs", "==", "-", "1", ":", "\n", "        ", "training_iterations", "=", "config", ".", "n_iter", "\n", "", "else", ":", "\n", "        ", "assert", "(", "config", ".", "epochs", ">", "0", ")", "\n", "iter_per_epoch", "=", "int", "(", "np", ".", "ceil", "(", "data", ".", "num_train_samples", "/", "config", ".", "batch_size", ")", ")", "\n", "training_iterations", "=", "config", ".", "epochs", "*", "iter_per_epoch", "\n", "\n", "", "summed_iter_runtime", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "training_iterations", ")", ":", "\n", "### Evaluate network.", "\n", "# We test the network before we run the training iteration.", "\n", "# That way, we can see the initial performance of the untrained network.", "\n", "        ", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "test", "(", "task_id", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "config", ",", "writer", ",", "\n", "logger", ",", "train_iter", "=", "i", ")", "\n", "mnet", ".", "train", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "                ", "hnet", ".", "train", "(", ")", "\n", "\n", "", "", "if", "i", "%", "200", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Training step: %d ...'", "%", "i", ")", "\n", "\n", "", "iter_start_time", "=", "time", "(", ")", "\n", "\n", "theta_optimizer", ".", "zero_grad", "(", ")", "\n", "if", "emb_optimizer", "is", "not", "None", ":", "\n", "            ", "emb_optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "#######################################", "\n", "### Data for current task and batch ###", "\n", "#######################################", "\n", "", "batch", "=", "data", ".", "next_train_batch", "(", "config", ".", "batch_size", ")", "\n", "X", "=", "data", ".", "input_to_torch_tensor", "(", "batch", "[", "0", "]", ",", "device", ",", "mode", "=", "'train'", ")", "\n", "T", "=", "data", ".", "output_to_torch_tensor", "(", "batch", "[", "1", "]", ",", "device", ",", "mode", "=", "'train'", ")", "\n", "\n", "# Get the output neurons depending on the continual learning scenario.", "\n", "n_y", "=", "data", ".", "num_classes", "\n", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "# Choose current head.", "\n", "            ", "task_out", "=", "[", "task_id", "*", "n_y", ",", "(", "task_id", "+", "1", ")", "*", "n_y", "]", "\n", "", "elif", "config", ".", "cl_scenario", "==", "2", ":", "\n", "# Always all output neurons, only one head is used.", "\n", "            ", "task_out", "=", "[", "0", ",", "n_y", "]", "\n", "", "else", ":", "\n", "# Choose current head, which will be inferred during inference.", "\n", "            ", "task_out", "=", "[", "task_id", "*", "n_y", ",", "(", "task_id", "+", "1", ")", "*", "n_y", "]", "\n", "\n", "########################", "\n", "### Loss computation ###", "\n", "########################", "\n", "", "if", "config", ".", "mnet_only", ":", "\n", "            ", "weights", "=", "None", "\n", "", "else", ":", "\n", "            ", "weights", "=", "hnet", ".", "forward", "(", "task_id", "=", "task_id", ")", "\n", "", "Y_hat_logits", "=", "mnet", ".", "forward", "(", "X", ",", "weights", ",", "**", "mnet_kwargs", ")", "\n", "\n", "# Restrict output neurons", "\n", "Y_hat_logits", "=", "Y_hat_logits", "[", ":", ",", "task_out", "[", "0", "]", ":", "task_out", "[", "1", "]", "]", "\n", "assert", "(", "T", ".", "shape", "[", "1", "]", "==", "Y_hat_logits", ".", "shape", "[", "1", "]", ")", "\n", "# compute loss on task and compute gradients", "\n", "if", "config", ".", "soft_targets", ":", "\n", "            ", "soft_label", "=", "0.95", "\n", "num_classes", "=", "data", ".", "num_classes", "\n", "soft_targets", "=", "torch", ".", "where", "(", "T", "==", "1", ",", "\n", "torch", ".", "Tensor", "(", "[", "soft_label", "]", ")", ",", "\n", "torch", ".", "Tensor", "(", "[", "(", "1", "-", "soft_label", ")", "/", "(", "num_classes", "-", "1", ")", "]", ")", ")", "\n", "soft_targets", "=", "soft_targets", ".", "to", "(", "device", ")", "\n", "loss_task", "=", "Classifier", ".", "softmax_and_cross_entropy", "(", "Y_hat_logits", ",", "\n", "soft_targets", ")", "\n", "", "else", ":", "\n", "            ", "loss_task", "=", "Classifier", ".", "logit_cross_entropy_loss", "(", "Y_hat_logits", ",", "T", ")", "\n", "\n", "# Compute gradients based on task loss (those might be used in the CL", "\n", "# regularizer).", "\n", "", "loss_task", ".", "backward", "(", "retain_graph", "=", "calc_reg", ",", "create_graph", "=", "calc_reg", "and", "config", ".", "backprop_dt", ")", "\n", "\n", "# The current task embedding only depends in the task loss, so we can", "\n", "# update it already.", "\n", "if", "emb_optimizer", "is", "not", "None", ":", "\n", "            ", "emb_optimizer", ".", "step", "(", ")", "\n", "\n", "#############################", "\n", "### CL (HNET) Regularizer ###", "\n", "#############################", "\n", "", "loss_reg", "=", "0", "\n", "dTheta", "=", "None", "\n", "\n", "if", "calc_reg", ":", "\n", "            ", "if", "config", ".", "no_lookahead", ":", "\n", "                ", "dTembs", "=", "None", "\n", "dTheta", "=", "None", "\n", "", "else", ":", "\n", "                ", "dTheta", "=", "opstep", ".", "calc_delta_theta", "(", "theta_optimizer", ",", "False", ",", "\n", "lr", "=", "config", ".", "lr", ",", "detach_dt", "=", "not", "config", ".", "backprop_dt", ")", "\n", "\n", "if", "config", ".", "continue_emb_training", ":", "\n", "                    ", "dTembs", "=", "dTheta", "[", "-", "task_id", ":", "]", "\n", "dTheta", "=", "dTheta", "[", ":", "-", "task_id", "]", "\n", "", "else", ":", "\n", "                    ", "dTembs", "=", "None", "\n", "\n", "", "", "loss_reg", "=", "hreg", ".", "calc_fix_target_reg", "(", "hnet", ",", "task_id", ",", "\n", "targets", "=", "targets_hypernet", ",", "dTheta", "=", "dTheta", ",", "dTembs", "=", "dTembs", ",", "\n", "mnet", "=", "mnet", ",", "inds_of_out_heads", "=", "regged_outputs", ",", "\n", "prev_theta", "=", "prev_theta", ",", "prev_task_embs", "=", "prev_task_embs", ",", "\n", "batch_size", "=", "config", ".", "cl_reg_batch_size", ")", "\n", "\n", "loss_reg", "*=", "config", ".", "beta", "\n", "\n", "loss_reg", ".", "backward", "(", ")", "\n", "\n", "# Now, that we computed the regularizer, we can use the accumulated", "\n", "# gradients and update the hnet (or mnet) parameters.", "\n", "", "theta_optimizer", ".", "step", "(", ")", "\n", "\n", "Y_hat", "=", "F", ".", "softmax", "(", "Y_hat_logits", ",", "dim", "=", "1", ")", "\n", "classifier_accuracy", "=", "Classifier", ".", "accuracy", "(", "Y_hat", ",", "T", ")", "*", "100.0", "\n", "\n", "#########################", "\n", "# Learning rate scheduler", "\n", "#########################", "\n", "if", "config", ".", "plateau_lr_scheduler", ":", "\n", "            ", "assert", "(", "iter_per_epoch", "!=", "-", "1", ")", "\n", "if", "i", "%", "iter_per_epoch", "==", "0", "and", "i", ">", "0", ":", "\n", "                ", "curr_epoch", "=", "i", "//", "iter_per_epoch", "\n", "logger", ".", "info", "(", "'Computing test accuracy for plateau LR '", "+", "\n", "'scheduler (epoch %d).'", "%", "curr_epoch", ")", "\n", "# We need a validation quantity for the plateau LR scheduler.", "\n", "# FIXME we should use an actual validation set rather than the", "\n", "# test set.", "\n", "# Note, https://keras.io/examples/cifar10_resnet/ uses the test", "\n", "# set to compute the validation loss. We use the \"validation\"", "\n", "# accuracy instead.", "\n", "# FIXME We increase `train_iter` as the print messages in the", "\n", "# test method suggest that the testing has been executed before", "\n", "test_acc", ",", "_", "=", "test", "(", "task_id", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "\n", "config", ",", "writer", ",", "logger", ",", "train_iter", "=", "i", "+", "1", ")", "\n", "mnet", ".", "train", "(", ")", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "                    ", "hnet", ".", "train", "(", ")", "\n", "\n", "", "plateau_scheduler_theta", ".", "step", "(", "test_acc", ")", "\n", "if", "plateau_scheduler_emb", "is", "not", "None", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.train_proximal": [[537, 706], ["print", "mnet.train", "hnet.train", "torch.nn.ParameterList", "dTheta.to.to", "torch.Adam", "torch.Adam", "range", "print", "NotImplementedError", "NotImplementedError", "utils.get_current_targets", "dTheta.to.append", "dt.data.zero_", "data.next_train_batch", "data.input_to_torch_tensor", "data.output_to_torch_tensor", "range", "enumerate", "optim.Adam.zero_grad", "hnet.forward", "mnet.forward", "torch.mse_loss", "F.mse_loss.backward", "optim.Adam.step", "list", "torch.nn.Parameter", "hnet.get_task_emb", "train.evaluate", "mnet.train", "hnet.train", "print", "dt.data.zero_", "optim.Adam.zero_grad", "hnet.forward", "mnet.forward", "torch.mse_loss", "torch.norm", "torch.norm", "torch.norm", "torch.zeros", "torch.zeros", "torch.zeros", "loss.backward", "optim.Adam.step", "dT_loss_vals.append", "writer.add_scalar", "torch.norm", "torch.norm", "torch.norm", "writer.add_scalar", "range", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "utils.calc_fix_target_reg", "l.data.cpu().numpy", "d.view", "utils.calc_value_preserving_reg", "d.view", "utils.calc_jac_reguarizer", "l.data.cpu"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.get_current_targets", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_emb", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.evaluate", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_fix_target_reg", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_value_preserving_reg", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.hnet_regularizer.calc_jac_reguarizer"], ["                    ", "plateau_scheduler_emb", ".", "step", "(", "test_acc", ")", "\n", "\n", "", "", "", "if", "config", ".", "lambda_lr_scheduler", ":", "\n", "            ", "assert", "(", "iter_per_epoch", "!=", "-", "1", ")", "\n", "if", "i", "%", "iter_per_epoch", "==", "0", "and", "i", ">", "0", ":", "\n", "                ", "curr_epoch", "=", "i", "//", "iter_per_epoch", "\n", "logger", ".", "info", "(", "'Applying Lambda LR scheduler (epoch %d).'", "\n", "%", "curr_epoch", ")", "\n", "\n", "lambda_scheduler_theta", ".", "step", "(", ")", "\n", "if", "lambda_scheduler_emb", "is", "not", "None", ":", "\n", "                        ", "lambda_scheduler_emb", ".", "step", "(", ")", "\n", "\n", "###########################", "\n", "### Tensorboard summary ###", "\n", "###########################", "\n", "# We don't wanna slow down training by having too much output.", "\n", "", "", "", "if", "i", "%", "50", "==", "0", ":", "\n", "            ", "writer", ".", "add_scalar", "(", "'train/task_%d/class_accuracy'", "%", "task_id", ",", "\n", "classifier_accuracy", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/loss_task'", "%", "task_id", ",", "loss_task", ",", "i", ")", "\n", "writer", ".", "add_scalar", "(", "'train/task_%d/loss_reg'", "%", "task_id", ",", "loss_reg", ",", "i", ")", "\n", "\n", "### Show the current training progress to the user.", "\n", "", "if", "i", "%", "config", ".", "val_iter", "==", "0", ":", "\n", "            ", "msg", "=", "'Training step {}: Classifier Accuracy: {:.3f} '", "+", "'(on current training batch).'", "\n", "logger", ".", "debug", "(", "msg", ".", "format", "(", "i", ",", "classifier_accuracy", ")", ")", "\n", "\n", "", "iter_end_time", "=", "time", "(", ")", "\n", "summed_iter_runtime", "+=", "(", "iter_end_time", "-", "iter_start_time", ")", "\n", "\n", "if", "i", "%", "200", "==", "0", ":", "\n", "            ", "logger", ".", "info", "(", "'Training step: %d ... Done -- (runtime: %f sec)'", "%", "(", "i", ",", "iter_end_time", "-", "iter_start_time", ")", ")", "\n", "\n", "\n", "", "", "if", "mnet", ".", "batchnorm_layers", "is", "not", "None", ":", "\n", "        ", "if", "not", "config", ".", "bn_distill_stats", "and", "not", "config", ".", "bn_no_running_stats", "and", "not", "config", ".", "bn_no_stats_checkpointing", ":", "\n", "# Checkpoint the current running statistics (that have been", "\n", "# estimated while training the current task).", "\n", "            ", "for", "bn_layer", "in", "mnet", ".", "batchnorm_layers", ":", "\n", "                ", "assert", "(", "bn_layer", ".", "num_stats", "==", "task_id", "+", "1", ")", "\n", "bn_layer", ".", "checkpoint_stats", "(", ")", "\n", "\n", "", "", "", "avg_iter_time", "=", "summed_iter_runtime", "/", "config", ".", "n_iter", "\n", "logger", ".", "info", "(", "'Average runtime per training iteration: %f sec.'", "%", "avg_iter_time", ")", "\n", "\n", "logger", ".", "info", "(", "'Elapsed time for training task %d: %f sec.'", "%", "(", "task_id", "+", "1", ",", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n", "", "def", "test_multiple", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "\n", "logger", ")", ":", "\n", "    ", "\"\"\"Method to test continual learning experiment accuracy\n\n    Args:\n        (....): See docstring of function :func:`train`.\n        dhandlers (list): List of data handlers. The accuracy of each task in\n            this list will be computed using function :func:`test`. The index\n            within the list will be considered as task ID.\n    \"\"\"", "\n", "class_accs", "=", "[", "]", "\n", "task_accs", "=", "[", "]", "\n", "\n", "num_tasks", "=", "len", "(", "dhandlers", ")", "\n", "\n", "### Task-incremental learning", "\n", "if", "config", ".", "cl_scenario", "==", "1", ":", "\n", "        ", "logger", ".", "info", "(", "'### Testing task-incremental learning scenario'", ")", "\n", "# Iterate through learned embeddings and tasks and compute test acc.", "\n", "for", "j", "in", "range", "(", "num_tasks", ")", ":", "\n", "            ", "data", "=", "dhandlers", "[", "j", "]", "\n", "\n", "test_acc", ",", "_", "=", "test", "(", "j", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "\n", "config", ",", "writer", ",", "logger", ")", "\n", "\n", "class_accs", ".", "append", "(", "test_acc", ")", "\n", "shared", ".", "summary", "[", "'acc_final'", "]", "[", "j", "]", "=", "test_acc", "\n", "\n", "", "shared", ".", "summary", "[", "'acc_avg_final'", "]", "=", "np", ".", "mean", "(", "class_accs", ")", "\n", "logger", ".", "info", "(", "'### Task-incremental learning scenario accuracies: %s '", "%", "(", "str", "(", "class_accs", ")", ")", "+", "'(avg: %.3f)'", "\n", "%", "(", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", ")", "\n", "\n", "writer", ".", "add_scalar", "(", "'final/task_incremental'", ",", "\n", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", "\n", "\n", "### Domain-incremental learning & class-incremental learning", "\n", "", "if", "config", ".", "cl_scenario", "==", "2", "or", "config", ".", "cl_scenario", "==", "3", ":", "\n", "        ", "raise", "NotImplementedError", "(", ")", "\n", "\n", "if", "config", ".", "cl_scenario", "==", "2", ":", "\n", "            ", "logger", ".", "info", "(", "'### Testing domain-incremental learning scenario'", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "'### Testing class-incrementa learning scenario'", ")", "\n", "\n", "\n", "", "for", "j", "in", "range", "(", "num_tasks", ")", ":", "\n", "            ", "data", "=", "dhandlers", "[", "j", "]", "\n", "\n", "test_acc", ",", "task_acc", "=", "test", "(", "j", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "\n", "config", ",", "writer", ",", "logger", ")", "\n", "\n", "class_accs", ".", "append", "(", "test_acc", ")", "\n", "task_accs", ".", "append", "(", "task_acc", ")", "\n", "\n", "shared", ".", "summary", "[", "'acc_final'", "]", "[", "j", "]", "=", "test_acc", "\n", "\n", "", "shared", ".", "summary", "[", "'acc_avg_final'", "]", "=", "np", ".", "mean", "(", "class_accs", ")", "\n", "\n", "if", "config", ".", "cl_scenario", "==", "2", ":", "\n", "            ", "logger", ".", "info", "(", "'### Domain-incremental learning scenario '", "+", "\n", "'accuracies: %s '", "%", "(", "str", "(", "class_accs", ")", ")", "+", "'(avg: %.3f)'", "\n", "%", "(", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'final/domain_incremental'", ",", "\n", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "'### Class-incremental learning scenario '", "+", "\n", "'accuracies: %s '", "%", "(", "str", "(", "class_accs", ")", ")", "+", "'(avg: %.3f)'", "\n", "%", "(", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'final/class_incremental'", ",", "\n", "shared", ".", "summary", "[", "'acc_avg_final'", "]", ")", "\n", "\n", "", "logger", ".", "info", "(", "'### Task-inference accuracies: %s '", "%", "(", "str", "(", "task_accs", ")", ")", "+", "'(avg: %.3f)'", "\n", "%", "(", "np", ".", "mean", "(", "task_accs", ")", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'final/task_inference_acc'", ",", "np", ".", "mean", "(", "task_accs", ")", ")", "\n", "\n", "", "return", "task_accs", ",", "class_accs", "\n", "\n", "", "def", "analysis", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "logger", ",", "\n", "during_weights", ")", ":", "\n", "    ", "\"\"\"A function to do some post-hoc analysis on the hypernetwork.\n\n    Specifically, this function does the following:\n        - Computing and logging statistics on how the weights changed since a\n          task has been learned.\n        - Assessing the diversity of ``hnet`` outputs, i.e., how close are the\n          ``hnet`` outputs for different tasks.\n\n    Args:\n        (....): See docstring of function :func:`test_multiple`.\n        during_weights (list): List of flattened ``hnet`` outputs right after\n            training on each task.\n    \"\"\"", "\n", "assert", "hnet", "is", "not", "None", "\n", "mnet", ".", "eval", "(", ")", "\n", "hnet", ".", "eval", "(", ")", "\n", "\n", "num_tasks", "=", "len", "(", "dhandlers", ")", "\n", "\n", "# Test how much the weights of each task have changed during training the", "\n", "# remaining tasks.", "\n", "for", "j", "in", "range", "(", "num_tasks", ")", ":", "\n", "        ", "cur_weights", "=", "hnet", ".", "forward", "(", "j", ")", "\n", "cur_weights", "=", "torch", ".", "cat", "(", "[", "a", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "cpu", "(", ")", ".", "flatten", "(", ")", "\n", "for", "a", "in", "cur_weights", "]", ")", "\n", "aft_weights", "=", "torch", ".", "cat", "(", "[", "a", ".", "flatten", "(", ")", "for", "a", "in", "during_weights", "[", "j", "]", "]", ")", "\n", "\n", "logger", ".", "info", "(", "'### Euclidean distance of current hnet output to '", "+", "\n", "'original one for task %d: %f'", "%", "(", "j", ",", "torch", ".", "sqrt", "(", "torch", ".", "sum", "(", "(", "aft_weights", "-", "cur_weights", ")", "**", "2", ")", ")", ")", ")", "\n", "\n", "# FIXME Inefficient, we already computed all hnet outputs above.", "\n", "", "for", "j", "in", "range", "(", "num_tasks", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "num_tasks", ")", ":", "\n", "            ", "if", "i", "<=", "j", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.train_rnet": [[707, 810], ["print", "rnet.train", "torch.Adam", "range", "print", "p.detach().clone", "rnet.parameters", "optim.Adam.zero_grad", "range", "loss.backward", "optim.Adam.step", "train.evaluate_rnet", "rnet.train", "print", "rnet.encode", "rnet.decode", "rnet.reconstruction_loss", "rnet.task_cross_entropy", "rnet.prior_matching", "alpha.argmax", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "p.detach", "torch.ones().to", "torch.ones().to", "torch.ones().to", "data.next_train_batch", "data.input_to_torch_tensor", "rnet.prior_samples().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "rnet.decode().detach().clone", "writer.add_scalar", "writer.add_histogram", "torch.ones", "torch.ones", "torch.ones", "rnet.prior_samples", "torch.zeros", "torch.zeros", "torch.zeros", "rnet.decode().detach", "rnet.decode"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.evaluate_rnet", "home.repos.pwc.inspect_result.chrhenning_hypercl.replay.train_replay.train", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.encode", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.decode", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.reconstruction_loss", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.task_cross_entropy", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.prior_matching", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.prior_samples", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.task_recognition_model.RecognitionNet.decode"], ["                ", "continue", "\n", "", "weights_1", "=", "hnet", ".", "forward", "(", "j", ")", "\n", "weights_2", "=", "hnet", ".", "forward", "(", "i", ")", "\n", "weights_1", "=", "torch", ".", "cat", "(", "[", "a", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "flatten", "(", ")", "for", "a", "in", "weights_1", "]", ")", "\n", "weights_2", "=", "torch", ".", "cat", "(", "[", "a", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "flatten", "(", ")", "for", "a", "in", "weights_2", "]", ")", "\n", "logger", ".", "info", "(", "'### Euclidean distance between '", "+", "\n", "'task %d and task %d: %f'", "%", "(", "j", ",", "i", ",", "\n", "torch", ".", "sqrt", "(", "torch", ".", "sum", "(", "(", "weights_1", "-", "weights_2", ")", "**", "2", ")", ")", ")", ")", "\n", "\n", "", "", "", "def", "run", "(", "config", ",", "experiment", "=", "'resnet'", ")", ":", "\n", "    ", "\"\"\"Run the training.\n\n    Args:\n        config (argparse.Namespace): Command-line arguments.\n        experiment (str): Which kind of experiment should be performed?\n\n            - ``resnet``: CIFAR-10/100 with Resnet-32.\n            - ``zenke``: CIFAR-10/100 with Zenkenet.\n    \"\"\"", "\n", "assert", "(", "experiment", "in", "[", "'resnet'", ",", "'zenke'", "]", ")", "\n", "\n", "script_start", "=", "time", "(", ")", "\n", "\n", "device", ",", "writer", ",", "logger", "=", "sutils", ".", "setup_environment", "(", "config", ",", "\n", "logger_name", "=", "'det_cl_cifar_%s'", "%", "experiment", ")", "\n", "# TODO Adapt script to allow checkpointing of models using", "\n", "# `utils.torch_ckpts` (i.e., we should be able to continue training or just", "\n", "# test an existing checkpoint).", "\n", "#config.ckpt_dir = os.path.join(config.out_dir, 'checkpoints')", "\n", "\n", "# Container for variables shared across function.", "\n", "shared", "=", "Namespace", "(", ")", "\n", "shared", ".", "experiment", "=", "experiment", "\n", "\n", "### Load datasets (i.e., create tasks).", "\n", "dhandlers", "=", "tutils", ".", "load_datasets", "(", "config", ",", "shared", ",", "logger", ",", "\n", "data_dir", "=", "'../datasets'", ")", "\n", "\n", "### Create main network.", "\n", "# TODO Allow main net only training.", "\n", "mnet", "=", "tutils", ".", "get_main_model", "(", "config", ",", "shared", ",", "logger", ",", "device", ",", "\n", "no_weights", "=", "not", "config", ".", "mnet_only", ")", "\n", "\n", "### Create the hypernetwork.", "\n", "if", "config", ".", "mnet_only", ":", "\n", "        ", "hnet", "=", "None", "\n", "", "else", ":", "\n", "        ", "hnet", "=", "tutils", ".", "get_hnet_model", "(", "config", ",", "mnet", ",", "logger", ",", "device", ")", "\n", "\n", "### Initialize the performance measures, that should be tracked during", "\n", "### training.", "\n", "", "tutils", ".", "setup_summary_dict", "(", "config", ",", "shared", ",", "mnet", ",", "hnet", "=", "hnet", ")", "\n", "\n", "# Add hparams to tensorboard, such that the identification of runs is", "\n", "# easier.", "\n", "writer", ".", "add_hparams", "(", "hparam_dict", "=", "{", "**", "vars", "(", "config", ")", ",", "**", "{", "\n", "'num_weights_main'", ":", "shared", ".", "summary", "[", "'num_weights_main'", "]", ",", "\n", "'num_weights_hyper'", ":", "shared", ".", "summary", "[", "'num_weights_hyper'", "]", ",", "\n", "'num_weights_ratio'", ":", "shared", ".", "summary", "[", "'num_weights_ratio'", "]", ",", "\n", "}", "}", ",", "metric_dict", "=", "{", "}", ")", "\n", "\n", "# FIXME: Method \"calc_fix_target_reg\" expects a None value.", "\n", "# But `writer.add_hparams` can't deal with `None` values.", "\n", "if", "config", ".", "cl_reg_batch_size", "==", "-", "1", ":", "\n", "        ", "config", ".", "cl_reg_batch_size", "=", "None", "\n", "\n", "# We keep the hnet output right after training to measure forgetting.", "\n", "", "weights_after_training", "=", "[", "]", "\n", "\n", "######################", "\n", "### Start Training ###", "\n", "######################", "\n", "for", "j", "in", "range", "(", "config", ".", "num_tasks", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Starting training of task %d ...'", "%", "(", "j", "+", "1", ")", ")", "\n", "\n", "data", "=", "dhandlers", "[", "j", "]", "\n", "\n", "# It might be that tasks are very similar and we can transfer knowledge", "\n", "# form the previous solution.", "\n", "if", "hnet", "is", "not", "None", "and", "config", ".", "init_with_prev_emb", "and", "j", ">", "0", ":", "\n", "            ", "last_emb", "=", "hnet", ".", "get_task_emb", "(", "j", "-", "1", ")", ".", "detach", "(", ")", ".", "clone", "(", ")", "\n", "hnet", ".", "get_task_emb", "(", "j", ")", ".", "data", "=", "last_emb", "\n", "\n", "# Training from scratch -- create new network instance!", "\n", "# -> No transfer possible.", "\n", "", "if", "j", ">", "0", "and", "config", ".", "train_from_scratch", ":", "\n", "# FIXME Since we simply override the current network, future testing", "\n", "# on this new network for old tasks doesn't make sense. So we", "\n", "# shouldn't report `final` accuracies.", "\n", "            ", "if", "config", ".", "mnet_only", ":", "\n", "                ", "logger", ".", "info", "(", "'From scratch training: Creating new main network.'", ")", "\n", "mnet", "=", "tutils", ".", "get_main_model", "(", "config", ",", "shared", ",", "logger", ",", "device", ",", "\n", "no_weights", "=", "not", "config", ".", "mnet_only", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "'From scratch training: Creating new hypernetwork.'", ")", "\n", "hnet", "=", "tutils", ".", "get_hnet_model", "(", "config", ",", "mnet", ",", "logger", ",", "device", ")", "\n", "\n", "################################", "\n", "### Train and test on task j ###", "\n", "################################", "\n", "", "", "train", "(", "j", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "logger", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.run": [[811, 902], ["toy_example.train_utils.parse_cmd_arguments", "toy_example.train_utils._setup_environment", "toy_example.train_utils._generate_tasks", "toy_example.train_utils._generate_networks", "dict", "range", "print", "print", "print", "writer.close", "print", "numpy.ones", "print", "train.test", "train.train_proximal", "train.train_reg", "print", "train.train_rnet", "train.test", "toy_example.train_utils._generate_networks", "numpy.array2string", "numpy.array2string", "print", "current_mse.mean", "current_mse.std", "str"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils.parse_cmd_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._setup_environment", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_tasks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_networks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.train_proximal", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.train_reg", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.train_rnet", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train.test", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_networks", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean"], ["### Final test run.", "\n", "if", "hnet", "is", "not", "None", ":", "\n", "            ", "weights", "=", "hnet", ".", "forward", "(", "j", ")", "\n", "# Push to CPU to avoid growing GPU memory when solving very long", "\n", "# task sequences.", "\n", "weights", "=", "[", "w", ".", "detach", "(", ")", ".", "clone", "(", ")", ".", "cpu", "(", ")", "for", "w", "in", "weights", "]", "\n", "weights_after_training", ".", "append", "(", "weights", ")", "\n", "\n", "", "test_acc", ",", "_", "=", "test", "(", "j", ",", "data", ",", "mnet", ",", "hnet", ",", "device", ",", "shared", ",", "config", ",", "writer", ",", "\n", "logger", ")", "\n", "\n", "logger", ".", "info", "(", "'### Accuracy of task %d / %d:  %.3f'", "%", "(", "j", "+", "1", ",", "config", ".", "num_tasks", ",", "test_acc", ")", ")", "\n", "logger", ".", "info", "(", "'### Finished training task: %d'", "%", "(", "j", "+", "1", ")", ")", "\n", "shared", ".", "summary", "[", "'acc_during'", "]", "[", "j", "]", "=", "test_acc", "\n", "\n", "# Backup results so far.", "\n", "tutils", ".", "save_summary_dict", "(", "config", ",", "shared", ",", "experiment", ")", "\n", "\n", "", "shared", ".", "summary", "[", "'acc_avg_during'", "]", "=", "np", ".", "mean", "(", "shared", ".", "summary", "[", "'acc_during'", "]", ")", "\n", "\n", "logger", ".", "info", "(", "'### Accuracy of individual tasks after training %s'", "%", "(", "str", "(", "shared", ".", "summary", "[", "'acc_during'", "]", ")", ")", ")", "\n", "logger", ".", "info", "(", "'### Average of these accuracies  %.2f'", "%", "(", "shared", ".", "summary", "[", "'acc_avg_during'", "]", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'final/during_acc_avg'", ",", "shared", ".", "summary", "[", "'acc_avg_during'", "]", ")", "\n", "\n", "#########################################", "\n", "### Test continual learning scenarios ###", "\n", "#########################################", "\n", "test_multiple", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "\n", "logger", ")", "\n", "\n", "#########################", "\n", "### Run some analysis ###", "\n", "#########################", "\n", "if", "not", "config", ".", "mnet_only", ":", "\n", "        ", "analysis", "(", "dhandlers", ",", "mnet", ",", "hnet", ",", "device", ",", "config", ",", "shared", ",", "writer", ",", "logger", ",", "\n", "weights_after_training", ")", "\n", "\n", "### Write final summary.", "\n", "", "shared", ".", "summary", "[", "'finished'", "]", "=", "1", "\n", "tutils", ".", "save_summary_dict", "(", "config", ",", "shared", ",", "experiment", ")", "\n", "\n", "writer", ".", "close", "(", ")", "\n", "\n", "logger", ".", "info", "(", "'Program finished successfully in %f sec.'", "\n", "%", "(", "time", "(", ")", "-", "script_start", ")", ")", "\n", "\n", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "raise", "Exception", "(", "'Script is not executable!'", ")", "\n", "\n", "", ""]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils.parse_cmd_arguments": [[48, 197], ["argparse.ArgumentParser", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.add_argument", "_ae_arguments.parse_args", "train_utils._hnet_arguments", "train_utils._cl_arguments_general", "train_utils._cl_arguments_ours", "train_utils._cl_arguments_ewc", "train_utils._ae_arguments", "range", "train_utils._cl_arguments_general", "train_utils._cl_arguments_ewc", "warnings.warn", "ValueError", "Exception", "datetime.datetime.now().strftime", "train_utils._hnet_arguments", "train_utils._mt_arguments", "train_utils._ae_arguments", "range", "ValueError", "range", "ValueError", "NotImplementedError", "ValueError", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._hnet_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._cl_arguments_general", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._cl_arguments_ours", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._cl_arguments_ewc", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._ae_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._cl_arguments_general", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._cl_arguments_ewc", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._hnet_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._mt_arguments", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._ae_arguments"], ["\n", "augment_data", "=", "not", "config", ".", "disable_data_augmentation", "\n", "#if shared.experiment == 'zenke':", "\n", "#    augment_data = False", "\n", "#    # To be comparable to previous results. Note, Zenke et al. didn't", "\n", "#    # utilize any data augmentation as far as I know.", "\n", "#    logger.warning('Data augmentation disabled for Zenkenet.')", "\n", "if", "augment_data", ":", "\n", "        ", "logger", ".", "info", "(", "'Data augmentation will be used.'", ")", "\n", "\n", "", "assert", "(", "config", ".", "num_tasks", "<=", "11", ")", "\n", "logger", ".", "info", "(", "'Loading CIFAR datasets ...'", ")", "\n", "dhandlers", "=", "get_split_cifar_handlers", "(", "data_dir", ",", "use_one_hot", "=", "True", ",", "\n", "use_data_augmentation", "=", "augment_data", ",", "num_tasks", "=", "config", ".", "num_tasks", ")", "\n", "assert", "(", "len", "(", "dhandlers", ")", "==", "config", ".", "num_tasks", ")", "\n", "\n", "logger", ".", "info", "(", "'Loaded %d CIFAR task(s) into memory.'", "%", "config", ".", "num_tasks", ")", "\n", "\n", "return", "dhandlers", "\n", "\n", "", "def", "get_main_model", "(", "config", ",", "shared", ",", "logger", ",", "device", ",", "no_weights", "=", "False", ")", ":", "\n", "    ", "\"\"\"Helper function to generate the main network.\n\n    This function uses :func:`utils.sim_utils.get_mnet_model` to generate the\n    main network.\n\n    The function also takes care of weight initialization, if configured.\n\n    Args:\n        (....): See docstring of function :func:`load_datasets`.\n        device: The PyTorch device.\n        no_weights (bool): If ``True``, the main network is generated without\n            internal weights.\n\n    Returns:\n        The main network.\n    \"\"\"", "\n", "if", "shared", ".", "experiment", "==", "'zenke'", ":", "\n", "        ", "net_type", "=", "'zenke'", "\n", "logger", ".", "info", "(", "'Building a ZenkeNet ...'", ")", "\n", "\n", "", "else", ":", "\n", "        ", "net_type", "=", "'resnet'", "\n", "logger", ".", "info", "(", "'Building a ResNet ...'", ")", "\n", "\n", "", "num_outputs", "=", "10", "\n", "\n", "if", "config", ".", "cl_scenario", "==", "1", "or", "config", ".", "cl_scenario", "==", "3", ":", "\n", "        ", "num_outputs", "*=", "config", ".", "num_tasks", "\n", "\n", "", "logger", ".", "info", "(", "'The network will have %d output neurons.'", "%", "num_outputs", ")", "\n", "\n", "in_shape", "=", "[", "32", ",", "32", ",", "3", "]", "\n", "out_shape", "=", "[", "num_outputs", "]", "\n", "\n", "# TODO Allow main net only training.", "\n", "mnet", "=", "sutils", ".", "get_mnet_model", "(", "config", ",", "net_type", ",", "in_shape", ",", "out_shape", ",", "device", ",", "\n", "no_weights", "=", "no_weights", ")", "\n", "\n", "init_network_weights", "(", "mnet", ".", "weights", ",", "config", ",", "logger", ",", "net", "=", "mnet", ")", "\n", "\n", "return", "mnet", "\n", "\n", "", "def", "get_hnet_model", "(", "config", ",", "mnet", ",", "logger", ",", "device", ")", ":", "\n", "    ", "\"\"\"Generate the hypernetwork.\n\n    This function uses :func:`utils.sim_utils.get_hnet_model` to generate the\n    hypernetwork.\n\n    The function also takes care of weight initialization, if configured.\n\n    Args:\n        (....): See docstring of function :func:`get_main_model`.\n        mnet: The main network.\n\n    Returns:\n        The hypernetwork or ``None`` if no hypernet is needed.\n    \n    \"\"\"", "\n", "logger", ".", "info", "(", "'Creating hypernetwork ...'", ")", "\n", "hnet", "=", "sutils", ".", "get_hnet_model", "(", "config", ",", "config", ".", "num_tasks", ",", "device", ",", "\n", "mnet", ".", "param_shapes", ")", "\n", "# FIXME There should be a nicer way of initializing hypernets in the", "\n", "# future.", "\n", "chunk_embs", "=", "None", "\n", "if", "hasattr", "(", "hnet", ",", "'chunk_embeddings'", ")", ":", "\n", "        ", "chunk_embs", "=", "hnet", ".", "chunk_embeddings", "\n", "", "init_network_weights", "(", "hnet", ".", "parameters", "(", ")", ",", "config", ",", "logger", ",", "\n", "chunk_embs", "=", "chunk_embs", ",", "task_embs", "=", "hnet", ".", "get_task_embs", "(", ")", ",", "net", "=", "hnet", ")", "\n", "if", "config", ".", "hnet_init_shift", ":", "\n", "        ", "hnet_init_shift", "(", "hnet", ",", "mnet", ",", "config", ",", "logger", ",", "device", ")", "\n", "\n", "# TODO Incorporate hyperchunk init.", "\n", "#if isinstance(hnet, ChunkedHyperNetworkHandler):", "\n", "#    hnet.apply_chunked_hyperfan_init(temb_var=config.std_normal_temb**2)", "\n", "\n", "", "return", "hnet", "\n", "\n", "", "def", "init_network_weights", "(", "all_params", ",", "config", ",", "logger", ",", "chunk_embs", "=", "None", ",", "\n", "task_embs", "=", "None", ",", "net", "=", "None", ")", ":", "\n", "    ", "\"\"\"Initialize a given set of weight tensors according to the user\n    configuration.\n\n    Warning:\n        This method is agnostic to where the weights stem from and is\n        therefore slightly dangerous. Use with care.\n\n    Note:\n        The method only exists as at the time of implementation the package\n        :mod:`hnets` wasn't available yet. In the future, initialization should\n        be part of the network implementation (e.g., via method\n        :meth:`mnets.mnet_interface.MainNetInterface.custom_init`).\n\n    Note:\n        If the given network implements interface\n        :class:`mnets.mnet_interface.MainNetInterface`, then the corresponding\n        method :meth:`mnets.mnet_interface.MainNetInterface.custom_init` is\n        used.\n\n    Note:\n        Papers like the following show that hypernets should get a special\n        init. This function does not take this into consideration.\n\n            https://openreview.net/forum?id=H1lma24tPB\n\n    Args:\n        all_params: A list of weight tensors to be initialized.\n        config: Command-line arguments.\n        logger: Logger.\n        chunk_embs (optional): A list of chunk embeddings.\n        task_embs (optional): A list of task embeddings.\n        net (optional): The network from which the parameters stem come from.\n            Can be used to implement network specific initializations (e.g.,\n            batch-norm weights).\n    \"\"\"", "\n", "if", "config", ".", "custom_network_init", ":", "\n", "        ", "if", "net", "is", "not", "None", "and", "isinstance", "(", "net", ",", "MainNetInterface", ")", ":", "\n", "            ", "logger", ".", "info", "(", "'Applying custom initialization to network ...'", ")", "\n", "net", ".", "custom_init", "(", "normal_init", "=", "config", ".", "normal_init", ",", "\n", "normal_std", "=", "config", ".", "std_normal_init", ",", "zero_bias", "=", "True", ")", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._cl_arguments_general": [[198, 218], ["parser.add_argument", "parser.add_argument"], "function", ["None"], ["\n", "", "else", ":", "\n", "            ", "logger", ".", "warning", "(", "'Custom weight initialization is applied to all '", "+", "\n", "'network parameters. Note, the current '", "+", "\n", "'implementation might be agnostic to special '", "+", "\n", "'network parameters.'", ")", "\n", "for", "W", "in", "all_params", ":", "\n", "# FIXME not all 1D vectors are bias vectors.", "\n", "# Examples of parameters that are 1D and not bias vectors:", "\n", "# * batchnorm weights", "\n", "# * embedding vectors", "\n", "                ", "if", "W", ".", "ndimension", "(", ")", "==", "1", ":", "# Bias vector.", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "W", ",", "0", ")", "\n", "", "elif", "config", ".", "normal_init", ":", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "W", ",", "mean", "=", "0", ",", "std", "=", "config", ".", "std_normal_init", ")", "\n", "", "else", ":", "\n", "                    ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "W", ")", "\n", "\n", "\n", "# Note, the embedding vectors inside \"all_params\" have been considered", "\n", "# as bias vectors and thus initialized to zero.", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._cl_arguments_ours": [[219, 301], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "", "", "", "if", "chunk_embs", "is", "not", "None", ":", "\n", "        ", "for", "emb", "in", "chunk_embs", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "emb", ",", "mean", "=", "0", ",", "std", "=", "config", ".", "std_normal_emb", ")", "\n", "\n", "", "", "if", "task_embs", "is", "not", "None", ":", "\n", "        ", "for", "temb", "in", "task_embs", ":", "\n", "            ", "torch", ".", "nn", ".", "init", ".", "normal_", "(", "temb", ",", "mean", "=", "0.", ",", "std", "=", "config", ".", "std_normal_temb", ")", "\n", "\n", "", "", "", "def", "hnet_init_shift", "(", "hnet", ",", "mnet", ",", "config", ",", "logger", ",", "device", ")", ":", "\n", "    ", "\"\"\"Init the hypernet ``hnet`` such that the weights of the main network \n    ``mnet`` are initialised as if there would be no hypernetwork i.e. the first\n    hypernetwork output is a standard init (for now normal or Xavier\n    are implemented).\n\n    Note:\n        This function is only meant for exploratory purposes. It does not\n        provide a proper weight initialization as for instance\n\n            https://openreview.net/forum?id=H1lma24tPB\n\n        Though, it is independent of the hypernet type/architecture.\n\n    Warning:\n        Not all hypernets support this quick-fix.\n\n    Args:\n        hnet: The model of the hyper network.\n        mnet: The main model.\n        config: The command line arguments.\n        device: Torch device (cpu or gpu).\n    \"\"\"", "\n", "logger", ".", "warning", "(", "'Config \"hnet_init_shift\" is just a temporary test and '", "+", "\n", "'should be used with care.'", ")", "\n", "\n", "# Get the current output, this should be normal or xavier or ...", "\n", "hnet_outputs", "=", "hnet", ".", "forward", "(", "0", ")", "\n", "orig_output", "=", "[", "o", ".", "detach", "(", ")", ".", "clone", "(", ")", "for", "o", "in", "hnet_outputs", "]", "\n", "mnet_init", "=", "[", "torch", ".", "zeros_like", "(", "o", ")", "for", "o", "in", "hnet_outputs", "]", "\n", "\n", "tmp", "=", "config", ".", "custom_network_init", "\n", "config", ".", "custom_network_init", "=", "True", "\n", "init_network_weights", "(", "mnet_init", ",", "config", ",", "logger", ",", "net", "=", "mnet", ")", "\n", "config", ".", "custom_network_init", "=", "tmp", "\n", "\n", "# The shift of the hypernetwork outputs will be computed by subtracting the", "\n", "# current output and adding the new desired output.", "\n", "o_shift", "=", "[", "]", "\n", "for", "i", ",", "o", "in", "enumerate", "(", "orig_output", ")", ":", "\n", "        ", "o_shift", ".", "append", "(", "-", "o", "+", "mnet_init", "[", "i", "]", ")", "\n", "\n", "# set the shifts", "\n", "", "assert", "(", "hasattr", "(", "hnet", ",", "'_shifts'", ")", ")", "# Only temporarily added to some hnets.", "\n", "hnet", ".", "_shifts", "=", "o_shift", "\n", "\n", "", "def", "setup_summary_dict", "(", "config", ",", "shared", ",", "mnet", ",", "hnet", "=", "None", ")", ":", "\n", "    ", "\"\"\"Setup the summary dictionary that is written to the performance\n    summary file (in the result folder).\n\n    This method adds the keyword ``summary`` to ``shared``.\n\n    Args:\n        config (argparse.Namespace): Command-line arguments.\n        shared (argparse.Namespace): Miscellaneous data shared among training\n            functions (summary dict will be added to this object).\n        mnet: Main network.\n        hnet (optional): Hypernetwork.\n    \"\"\"", "\n", "assert", "(", "hasattr", "(", "shared", ",", "'experiment'", ")", ")", "\n", "\n", "summary", "=", "dict", "(", ")", "\n", "\n", "if", "hnet", "is", "None", ":", "\n", "        ", "num", "=", "mnet", ".", "num_params", "\n", "hnum", "=", "-", "1", "\n", "ratio", "=", "-", "1", "\n", "", "else", ":", "\n", "        ", "num", "=", "hnet", ".", "num_outputs", "\n", "hnum", "=", "hnet", ".", "num_weights", "\n", "ratio", "=", "hnum", "/", "num", "\n", "\n", "# FIXME keywords should be cross-checked with those specified in the", "\n", "# corresponding `_SUMMARY_KEYWORDS` of the hyperparam search.", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._cl_arguments_ewc": [[302, 340], ["parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], ["", "summary", "[", "'acc_final'", "]", "=", "[", "-", "1", "]", "*", "config", ".", "num_tasks", "\n", "summary", "[", "'acc_during'", "]", "=", "[", "-", "1", "]", "*", "config", ".", "num_tasks", "\n", "summary", "[", "'acc_avg_final'", "]", "=", "-", "1", "\n", "summary", "[", "'acc_avg_during'", "]", "=", "-", "1", "\n", "summary", "[", "'num_weights_main'", "]", "=", "num", "\n", "summary", "[", "'num_weights_hyper'", "]", "=", "hnum", "\n", "summary", "[", "'num_weights_ratio'", "]", "=", "ratio", "\n", "summary", "[", "'finished'", "]", "=", "0", "\n", "\n", "shared", ".", "summary", "=", "summary", "\n", "\n", "", "def", "save_summary_dict", "(", "config", ",", "shared", ",", "experiment", ")", ":", "\n", "    ", "\"\"\"Write a text file in the result folder that gives a quick\n    overview over the results achieved so far.\n\n    Args:\n        (....): See docstring of function :func:`setup_summary_dict`.\n    \"\"\"", "\n", "# \"setup_summary_dict\" must be called first.", "\n", "assert", "(", "hasattr", "(", "shared", ",", "'summary'", ")", ")", "\n", "\n", "summary_fn", "=", "'performance_summary.txt'", "\n", "#summary_fn = hpperm._SUMMARY_FILENAME", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "config", ".", "out_dir", ",", "summary_fn", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "for", "k", ",", "v", "in", "shared", ".", "summary", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "v", ",", "list", ")", ":", "\n", "                ", "f", ".", "write", "(", "'%s %s\\n'", "%", "(", "k", ",", "misc", ".", "list_to_str", "(", "v", ")", ")", ")", "\n", "", "elif", "isinstance", "(", "v", ",", "float", ")", ":", "\n", "                ", "f", ".", "write", "(", "'%s %f\\n'", "%", "(", "k", ",", "v", ")", ")", "\n", "", "else", ":", "\n", "                ", "f", ".", "write", "(", "'%s %d\\n'", "%", "(", "k", ",", "v", ")", ")", "\n", "\n", "", "", "", "", "if", "__name__", "==", "'__main__'", ":", "\n", "    ", "pass", "\n", "", ""]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._ae_arguments": [[341, 383], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], []], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._mt_arguments": [[384, 408], ["parser.add_argument"], "function", ["None"], []], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._hnet_arguments": [[409, 435], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "function", ["None"], []], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._setup_environment": [[436, 494], ["torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed", "os.path.exists", "torch.device", "torch.device", "print", "input", "shutil.rmtree", "os.makedirs", "print", "os.makedirs", "print", "open", "pickle.dump", "torch.cuda.is_available", "torch.cuda.is_available", "hasattr", "tensorboardX.SummaryWriter", "tensorboardX.SummaryWriter", "Exception", "os.path.join", "str", "os.path.join", "os.path.join"], "function", ["None"], []], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_tasks": [[495, 512], ["train_utils._generate_1d_tasks", "train_utils._generate_gmm_tasks", "ValueError"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_1d_tasks", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_gmm_tasks"], []], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_1d_tasks": [[513, 615], ["range", "numpy.random.uniform", "range", "len", "print", "dhandlers.append", "numpy.random.randint", "range", "numpy.polyval", "map_funcs.append", "toy_example.regression1d_data.ToyRegression", "dhandlers[].plot_dataset", "writer.add_figure", "numpy.max", "numpy.power", "matplotlib.gcf", "utils.repair_canvas_and_show_fig", "dhandlers[].plot_dataset", "numpy.abs", "numpy.polyval", "matplotlib.gcf", "numpy.power"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.plot_dataset", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.repair_canvas_and_show_fig", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.plot_dataset"], []], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_gmm_tasks": [[616, 651], ["toy_example.get_gmm_tasks", "len", "toy_example.GaussianData.plot_datasets", "numpy.array", "numpy.eye", "itertools.product", "len", "range", "range"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.get_gmm_tasks", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.plot_datasets"], []], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.train_utils._generate_networks": [[652, 735], ["len", "utils.str_to_ints", "toy_example.main_model.MainNetwork.weight_shapes", "toy_example.main_model.MainNetwork().to", "utils.str_to_ints", "toy_example.hyper_model.HyperNetwork().to", "list", "list", "utils.str_to_ints", "list", "HyperNetwork().to.get_task_embs", "HyperNetwork().to.apply_hyperfan_init", "toy_example.main_model.MainNetwork", "HyperNetwork().to.parameters", "MainNetwork().to.parameters", "toy_example.main_model.MainNetwork.weight_shapes", "toy_example.main_model.MainNetwork", "toy_example.task_recognition_model.RecognitionNet().to", "RecognitionNet().to.parameters", "W.ndimension", "torch.nn.init.constant_", "torch.nn.init.constant_", "torch.nn.init.normal_", "torch.nn.init.normal_", "toy_example.hyper_model.HyperNetwork", "torch.nn.init.normal_", "torch.nn.init.normal_", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_", "utils.str_to_act", "utils.str_to_act", "toy_example.task_recognition_model.RecognitionNet", "utils.str_to_act", "torch.softmax", "utils.str_to_act"], "function", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.weight_shapes", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_ints", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnist.chunked_hyper_model.ChunkedHyperNetworkHandler.get_task_embs", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.hyper_model.HyperNetwork.apply_hyperfan_init", "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.weight_shapes", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_act", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_act", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_act", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.str_to_act"], []], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression.__init__": [[47, 133], ["data.dataset.Dataset.__init__", "warnings.warn", "numpy.random.RandomState.uniform", "numpy.linspace().reshape", "map_function", "map_function", "numpy.arange", "numpy.arange", "numpy.random.RandomState", "numpy.random.RandomState.normal", "numpy.linspace().reshape", "map_function", "numpy.vstack", "numpy.vstack", "numpy.vstack", "numpy.vstack", "numpy.arange", "numpy.linspace", "numpy.linspace"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ",", "train_inter", "=", "[", "-", "10", ",", "10", "]", ",", "num_train", "=", "20", ",", "\n", "test_inter", "=", "[", "-", "10", ",", "10", "]", ",", "num_test", "=", "80", ",", "val_inter", "=", "None", ",", "\n", "num_val", "=", "None", ",", "map_function", "=", "lambda", "x", ":", "x", ",", "std", "=", "0", ",", "rseed", "=", "None", ")", ":", "\n", "        ", "\"\"\"Generate a new dataset.\n\n        The input data x will be uniformly drawn for train samples and\n        equidistant for test samples. The user has to specify a function that\n        will map this random input data onto output samples y.\n\n        Args:\n            train_inter: A tuple, representing the interval from which x\n                samples are drawn in the training set. Note, this range will\n                apply to all input dimensions.\n            num_train: Number of training samples.\n            test_inter: A tuple, representing the interval from which x\n                samples are drawn in the test set. Note, this range will\n                apply to all input dimensions.\n            num_test: Number of test samples.\n            val_inter (optional): See parameter `test_inter`. If set, this\n                argument leads to the construction of a validation set. Note,\n                option `num_val` need to be specified as well.\n            num_val (optional): Number of validation samples.\n            map_function: A function handle that receives input\n                samples and maps them to output samples.\n            std: If not zero, Gaussian white noise with this std will be added\n                to the training outputs.\n            rseed: If None, the current random state of numpy is used to\n                   generate the data. Otherwise, a new random state with the\n                   given seed is generated.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "warn", "(", "'Please use class \"data.special.regression1d_data.ToyRegression\"'", "+", "\n", "' instead.'", ",", "DeprecationWarning", ")", "\n", "\n", "assert", "(", "val_inter", "is", "None", "and", "num_val", "is", "None", "or", "val_inter", "is", "not", "None", "and", "num_val", "is", "not", "None", ")", "\n", "\n", "if", "rseed", "is", "None", ":", "\n", "            ", "rand", "=", "np", ".", "random", "\n", "", "else", ":", "\n", "            ", "rand", "=", "np", ".", "random", ".", "RandomState", "(", "rseed", ")", "\n", "\n", "", "train_x", "=", "rand", ".", "uniform", "(", "low", "=", "train_inter", "[", "0", "]", ",", "high", "=", "train_inter", "[", "1", "]", ",", "\n", "size", "=", "(", "num_train", ",", "1", ")", ")", "\n", "test_x", "=", "np", ".", "linspace", "(", "start", "=", "test_inter", "[", "0", "]", ",", "stop", "=", "test_inter", "[", "1", "]", ",", "\n", "num", "=", "num_test", ")", ".", "reshape", "(", "(", "num_test", ",", "1", ")", ")", "\n", "\n", "train_y", "=", "map_function", "(", "train_x", ")", "\n", "test_y", "=", "map_function", "(", "test_x", ")", "\n", "\n", "# Perturb training outputs.", "\n", "if", "std", ">", "0", ":", "\n", "            ", "train_eps", "=", "rand", ".", "normal", "(", "loc", "=", "0.0", ",", "scale", "=", "std", ",", "size", "=", "(", "num_train", ",", "1", ")", ")", "\n", "train_y", "+=", "train_eps", "\n", "\n", "# Create validation data if requested.", "\n", "", "if", "num_val", "is", "not", "None", ":", "\n", "            ", "val_x", "=", "np", ".", "linspace", "(", "start", "=", "val_inter", "[", "0", "]", ",", "stop", "=", "val_inter", "[", "1", "]", ",", "\n", "num", "=", "num_val", ")", ".", "reshape", "(", "(", "num_val", ",", "1", ")", ")", "\n", "val_y", "=", "map_function", "(", "val_x", ")", "\n", "\n", "in_data", "=", "np", ".", "vstack", "(", "[", "train_x", ",", "test_x", ",", "val_x", "]", ")", "\n", "out_data", "=", "np", ".", "vstack", "(", "[", "train_y", ",", "test_y", ",", "val_y", "]", ")", "\n", "", "else", ":", "\n", "            ", "in_data", "=", "np", ".", "vstack", "(", "[", "train_x", ",", "test_x", "]", ")", "\n", "out_data", "=", "np", ".", "vstack", "(", "[", "train_y", ",", "test_y", "]", ")", "\n", "\n", "# Specify internal data structure.", "\n", "", "self", ".", "_data", "[", "'classification'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'in_data'", "]", "=", "in_data", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "1", "]", "\n", "self", ".", "_data", "[", "'out_data'", "]", "=", "out_data", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "1", "]", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ")", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ",", "num_train", "+", "num_test", ")", "\n", "\n", "if", "num_val", "is", "not", "None", ":", "\n", "            ", "n_start", "=", "num_train", "+", "num_test", "\n", "self", ".", "_data", "[", "'val_inds'", "]", "=", "np", ".", "arange", "(", "n_start", ",", "n_start", "+", "num_val", ")", "\n", "\n", "", "self", ".", "_map", "=", "map_function", "\n", "self", ".", "_train_inter", "=", "train_inter", "\n", "self", ".", "_test_inter", "=", "test_inter", "\n", "self", ".", "_val_inter", "=", "val_inter", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression.train_x_range": [[134, 138], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "train_x_range", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute train_x_range.\"\"\"", "\n", "return", "self", ".", "_train_inter", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression.test_x_range": [[139, 143], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "test_x_range", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute test_x_range.\"\"\"", "\n", "return", "self", ".", "_test_inter", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression.val_x_range": [[144, 148], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "val_x_range", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute val_x_range.\"\"\"", "\n", "return", "self", ".", "_val_inter", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression._get_function_vals": [[149, 179], ["numpy.linspace().reshape", "regression1d_data.ToyRegression._map", "min", "max", "min", "max", "numpy.linspace"], "methods", ["None"], ["", "def", "_get_function_vals", "(", "self", ",", "num_samples", "=", "100", ",", "x_range", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get real function values for equidistant x values in a range that\n        covers the test and training data. These values can be used to plot the\n        ground truth function.\n\n        Args:\n            num_samples: Number of samples to be produced.\n            x_range: If a specific range should be used to gather function\n                values.\n\n        Returns:\n            x, y: Two numpy arrays containing the corresponding x and y values.\n        \"\"\"", "\n", "if", "x_range", "is", "None", ":", "\n", "            ", "min_x", "=", "min", "(", "self", ".", "_train_inter", "[", "0", "]", ",", "self", ".", "_test_inter", "[", "0", "]", ")", "\n", "max_x", "=", "max", "(", "self", ".", "_train_inter", "[", "1", "]", ",", "self", ".", "_test_inter", "[", "1", "]", ")", "\n", "if", "self", ".", "num_val_samples", ">", "0", ":", "\n", "                ", "min_x", "=", "min", "(", "min_x", ",", "self", ".", "_val_inter", "[", "0", "]", ")", "\n", "max_x", "=", "max", "(", "max_x", ",", "self", ".", "_val_inter", "[", "1", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "min_x", "=", "x_range", "[", "0", "]", "\n", "max_x", "=", "x_range", "[", "1", "]", "\n", "\n", "", "slack_x", "=", "0.05", "*", "(", "max_x", "-", "min_x", ")", "\n", "\n", "sample_x", "=", "np", ".", "linspace", "(", "start", "=", "min_x", "-", "slack_x", ",", "stop", "=", "max_x", "+", "slack_x", ",", "\n", "num", "=", "num_samples", ")", ".", "reshape", "(", "(", "num_samples", ",", "1", ")", ")", "\n", "sample_y", "=", "self", ".", "_map", "(", "sample_x", ")", "\n", "\n", "return", "sample_x", ",", "sample_y", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression.plot_dataset": [[180, 216], ["regression1d_data.ToyRegression.get_train_inputs().squeeze", "regression1d_data.ToyRegression.get_train_outputs().squeeze", "regression1d_data.ToyRegression.get_test_inputs().squeeze", "regression1d_data.ToyRegression.get_test_outputs().squeeze", "regression1d_data.ToyRegression._get_function_vals", "matplotlib.locator_params", "matplotlib.locator_params", "matplotlib.plot", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.legend", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "regression1d_data.ToyRegression.get_val_inputs().squeeze", "regression1d_data.ToyRegression.get_val_outputs().squeeze", "matplotlib.scatter", "matplotlib.show", "regression1d_data.ToyRegression.get_train_inputs", "regression1d_data.ToyRegression.get_train_outputs", "regression1d_data.ToyRegression.get_test_inputs", "regression1d_data.ToyRegression.get_test_outputs", "regression1d_data.ToyRegression.get_val_inputs", "regression1d_data.ToyRegression.get_val_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_val_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_val_outputs"], ["", "def", "plot_dataset", "(", "self", ",", "show", "=", "True", ")", ":", "\n", "        ", "\"\"\"Plot the whole dataset.\n\n        Args:\n            show: Whether the plot should be shown.\n        \"\"\"", "\n", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "train_y", "=", "self", ".", "get_train_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "test_y", "=", "self", ".", "get_test_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "if", "self", ".", "num_val_samples", ">", "0", ":", "\n", "            ", "val_x", "=", "self", ".", "get_val_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "val_y", "=", "self", ".", "get_val_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "", "sample_x", ",", "sample_y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "\n", "# The default matplotlib setting is usually too high for most plots.", "\n", "plt", ".", "locator_params", "(", "axis", "=", "'y'", ",", "nbins", "=", "2", ")", "\n", "plt", ".", "locator_params", "(", "axis", "=", "'x'", ",", "nbins", "=", "6", ")", "\n", "\n", "plt", ".", "plot", "(", "sample_x", ",", "sample_y", ",", "color", "=", "'k'", ",", "label", "=", "'f(x)'", ",", "\n", "linestyle", "=", "'dashed'", ",", "linewidth", "=", ".5", ")", "\n", "plt", ".", "scatter", "(", "train_x", ",", "train_y", ",", "color", "=", "'r'", ",", "label", "=", "'Train'", ")", "\n", "plt", ".", "scatter", "(", "test_x", ",", "test_y", ",", "color", "=", "'b'", ",", "label", "=", "'Test'", ",", "alpha", "=", "0.8", ")", "\n", "if", "self", ".", "num_val_samples", ">", "0", ":", "\n", "            ", "plt", ".", "scatter", "(", "val_x", ",", "val_y", ",", "color", "=", "'g'", ",", "label", "=", "'Val'", ",", "alpha", "=", "0.5", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "title", "(", "'1D-Regression Dataset'", ")", "\n", "plt", ".", "xlabel", "(", "'$x$'", ")", "\n", "plt", ".", "ylabel", "(", "'$y$'", ")", "\n", "\n", "if", "show", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression.plot_predictions": [[217, 248], ["regression1d_data.ToyRegression.get_train_inputs().squeeze", "regression1d_data.ToyRegression.get_train_outputs().squeeze", "regression1d_data.ToyRegression.get_test_inputs().squeeze", "regression1d_data.ToyRegression.get_test_outputs().squeeze", "regression1d_data.ToyRegression._get_function_vals", "matplotlib.plot", "matplotlib.scatter", "matplotlib.legend", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "matplotlib.scatter", "matplotlib.scatter", "regression1d_data.ToyRegression.get_train_inputs", "regression1d_data.ToyRegression.get_train_outputs", "regression1d_data.ToyRegression.get_test_inputs", "regression1d_data.ToyRegression.get_test_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs"], ["", "", "def", "plot_predictions", "(", "self", ",", "predictions", ",", "label", "=", "'Pred'", ",", "show_train", "=", "True", ",", "\n", "show_test", "=", "True", ")", ":", "\n", "        ", "\"\"\"Plot the dataset as well as predictions.\n\n        Args:\n            predictions: A tuple of x and y values, where the y values are\n                         computed by a trained regression network.\n                         Note, that we assume the x values to be sorted.\n            label: Label of the predicted values as shown in the legend.\n            show_train: Show train samples.\n            show_test: Show test samples.\n        \"\"\"", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "train_y", "=", "self", ".", "get_train_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "test_y", "=", "self", ".", "get_test_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "sample_x", ",", "sample_y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "plt", ".", "plot", "(", "sample_x", ",", "sample_y", ",", "color", "=", "'k'", ",", "label", "=", "'f(x)'", ",", "\n", "linestyle", "=", "'dashed'", ",", "linewidth", "=", ".5", ")", "\n", "if", "show_train", ":", "\n", "            ", "plt", ".", "scatter", "(", "train_x", ",", "train_y", ",", "color", "=", "'r'", ",", "label", "=", "'Train'", ")", "\n", "", "if", "show_test", ":", "\n", "            ", "plt", ".", "scatter", "(", "test_x", ",", "test_y", ",", "color", "=", "'b'", ",", "label", "=", "'Test'", ")", "\n", "", "plt", ".", "scatter", "(", "predictions", "[", "0", "]", ",", "predictions", "[", "1", "]", ",", "color", "=", "'g'", ",", "label", "=", "label", ")", "\n", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "title", "(", "'1D-Regression Dataset'", ")", "\n", "plt", ".", "xlabel", "(", "'$x$'", ")", "\n", "plt", ".", "ylabel", "(", "'$y$'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression.get_identifier": [[249, 252], ["None"], "methods", ["None"], ["", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'1DRegression'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression.plot_samples": [[253, 305], ["matplotlib.figure", "matplotlib.title", "regression1d_data.ToyRegression._get_function_vals", "matplotlib.plot", "matplotlib.legend", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.ion", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.savefig", "matplotlib.show"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals"], ["", "def", "plot_samples", "(", "self", ",", "title", ",", "inputs", ",", "outputs", "=", "None", ",", "predictions", "=", "None", ",", "\n", "num_samples_per_row", "=", "4", ",", "show", "=", "True", ",", "filename", "=", "None", ",", "\n", "interactive", "=", "False", ",", "figsize", "=", "(", "10", ",", "6", ")", ")", ":", "\n", "        ", "\"\"\"Plot samples belonging to this dataset.\n\n        Note, either \"outputs\" or \"predictions\" must be not None!\n\n        Args:\n            title: The title of the whole figure.\n            inputs: A 2D numpy array, where each row is an input sample.\n            outputs (optional): A 2D numpy array of actual dataset targets.\n            predictions (optional): A 2D numpy array of predicted output\n                samples (i.e., output predicted by a neural network).\n            num_samples_per_row: Maximum number of samples plotted\n                per row in the generated figure.\n            show: Whether the plot should be shown.\n            filename (optional): If provided, the figure will be stored under\n                this filename.\n            interactive: Turn on interactive mode. We mainly\n                use this option to ensure that the program will run in\n                background while figure is displayed. The figure will be\n                displayed until another one is displayed, the user closes it or\n                the program has terminated. If this option is deactivated, the\n                program will freeze until the user closes the figure.\n                Note, if using the iPython inline backend, this option has no\n                effect.\n            figsize: A tuple, determining the size of the\n                figure in inches.\n        \"\"\"", "\n", "assert", "(", "outputs", "is", "not", "None", "or", "predictions", "is", "not", "None", ")", "\n", "\n", "plt", ".", "figure", "(", "figsize", "=", "figsize", ")", "\n", "plt", ".", "title", "(", "title", ",", "size", "=", "20", ")", "\n", "if", "interactive", ":", "\n", "            ", "plt", ".", "ion", "(", ")", "\n", "\n", "", "sample_x", ",", "sample_y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "plt", ".", "plot", "(", "sample_x", ",", "sample_y", ",", "color", "=", "'k'", ",", "label", "=", "'f(x)'", ",", "\n", "linestyle", "=", "'dashed'", ",", "linewidth", "=", ".5", ")", "\n", "if", "outputs", "is", "not", "None", ":", "\n", "            ", "plt", ".", "scatter", "(", "inputs", ",", "outputs", ",", "color", "=", "'b'", ",", "label", "=", "'Targets'", ")", "\n", "", "if", "predictions", "is", "not", "None", ":", "\n", "            ", "plt", ".", "scatter", "(", "inputs", ",", "predictions", ",", "color", "=", "'r'", ",", "label", "=", "'Predictions'", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "xlabel", "(", "'$x$'", ")", "\n", "plt", ".", "ylabel", "(", "'$y$'", ")", "\n", "\n", "if", "filename", "is", "not", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "filename", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n", "", "if", "show", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression._plot_sample": [[306, 313], ["NotImplementedError"], "methods", ["None"], ["", "", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Not implemented\"\"\"", "\n", "# We overwrote the plot_samples method, so there is no need to ever call", "\n", "# this method (it's just here because the baseclass requires its", "\n", "# existence).", "\n", "raise", "NotImplementedError", "(", "'TODO implement'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.regression1d_data.ToyRegression.plot_datasets": [[314, 414], ["len", "utils.misc.get_colorbrewer2_colors", "matplotlib.subplots", "matplotlib.title", "enumerate", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.tight_layout", "len", "warnings.warn", "matplotlib.pyplot.cm.rainbow", "data._get_function_vals", "matplotlib.plot", "phandlers.append", "plabels.append", "axes.grid", "axes.set_facecolor", "axes.axhline", "axes.axvline", "axes.tick_params", "matplotlib.legend", "matplotlib.savefig", "matplotlib.show", "len", "numpy.linspace", "matplotlib.scatter", "phandlers.append", "plabels.append", "len", "matplotlib.yticks", "matplotlib.xticks", "axes.yaxis.get_major_ticks", "axes.xaxis.get_major_ticks", "len", "len", "len", "tick.label.set_fontsize", "tick.label.set_fontsize", "axes.get_ylim", "axes.get_xlim"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.get_colorbrewer2_colors", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals"], ["", "@", "staticmethod", "\n", "def", "plot_datasets", "(", "data_handlers", ",", "inputs", "=", "None", ",", "predictions", "=", "None", ",", "labels", "=", "None", ",", "\n", "fun_xranges", "=", "None", ",", "show", "=", "True", ",", "filename", "=", "None", ",", "\n", "figsize", "=", "(", "10", ",", "6", ")", ",", "publication_style", "=", "False", ")", ":", "\n", "        ", "\"\"\"Plot several datasets of this class in one plot.\n\n        Args:\n            data_handlers: A list of ToyRegression objects.\n            inputs (optional): A list of numpy arrays representing inputs for\n                each dataset.\n            predictions (optional): A list of numpy arrays containing the\n                predicted output values for the given input values.\n            labels (optional): A label for each dataset.\n            fun_xranges (optional): List of x ranges in which the true\n                underlying function per dataset should be sketched.\n            show: Whether the plot should be shown.\n            filename (optional): If provided, the figure will be stored under\n                this filename.\n            figsize: A tuple, determining the size of the figure in inches.\n            publication_style: whether the plots should be in publication style\n        \"\"\"", "\n", "n", "=", "len", "(", "data_handlers", ")", "\n", "assert", "(", "(", "inputs", "is", "None", "and", "predictions", "is", "None", ")", "or", "(", "inputs", "is", "not", "None", "and", "predictions", "is", "not", "None", ")", ")", "\n", "assert", "(", "(", "inputs", "is", "None", "or", "len", "(", "inputs", ")", "==", "n", ")", "and", "(", "predictions", "is", "None", "or", "len", "(", "predictions", ")", "==", "n", ")", "and", "(", "labels", "is", "None", "or", "len", "(", "labels", ")", "==", "n", ")", ")", "\n", "assert", "(", "fun_xranges", "is", "None", "or", "len", "(", "fun_xranges", ")", "==", "n", ")", "\n", "\n", "# Set-up matplotlib to adhere to our graphical conventions.", "\n", "#misc.configure_matplotlib_params(fig_size=1.2*np.array([1.6, 1]),", "\n", "#                                 font_size=8)", "\n", "\n", "# Get a colorscheme from colorbrewer2.org.", "\n", "colors", "=", "misc", ".", "get_colorbrewer2_colors", "(", "family", "=", "'Dark2'", ")", "\n", "if", "n", ">", "len", "(", "colors", ")", ":", "\n", "            ", "warn", "(", "'Changing to automatic color scheme as we don\\'t have '", "+", "\n", "'as many manual colors as tasks.'", ")", "\n", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "n", ")", ")", "\n", "\n", "", "if", "publication_style", ":", "\n", "            ", "ts", ",", "lw", ",", "ms", "=", "60", ",", "15", ",", "140", "# text fontsize, line width, marker size", "\n", "figsize", "=", "(", "12", ",", "6", ")", "\n", "", "else", ":", "\n", "            ", "ts", ",", "lw", ",", "ms", "=", "12", ",", "2", ",", "15", "\n", "\n", "", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "figsize", "=", "figsize", ")", "\n", "plt", ".", "title", "(", "'1D regression'", ",", "size", "=", "ts", ",", "pad", "=", "ts", ")", "\n", "\n", "phandlers", "=", "[", "]", "\n", "plabels", "=", "[", "]", "\n", "\n", "for", "i", ",", "data", "in", "enumerate", "(", "data_handlers", ")", ":", "\n", "            ", "if", "labels", "is", "not", "None", ":", "\n", "                ", "lbl", "=", "labels", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "lbl", "=", "'Function %d'", "%", "i", "\n", "\n", "", "fun_xrange", "=", "None", "\n", "if", "fun_xranges", "is", "not", "None", ":", "\n", "                ", "fun_xrange", "=", "fun_xranges", "[", "i", "]", "\n", "", "sample_x", ",", "sample_y", "=", "data", ".", "_get_function_vals", "(", "x_range", "=", "fun_xrange", ")", "\n", "p", ",", "=", "plt", ".", "plot", "(", "sample_x", ",", "sample_y", ",", "color", "=", "colors", "[", "i", "]", ",", "\n", "linestyle", "=", "'dashed'", ",", "linewidth", "=", "lw", "/", "3", ")", "\n", "\n", "phandlers", ".", "append", "(", "p", ")", "\n", "plabels", ".", "append", "(", "lbl", ")", "\n", "if", "inputs", "is", "not", "None", ":", "\n", "                ", "p", "=", "plt", ".", "scatter", "(", "inputs", "[", "i", "]", ",", "predictions", "[", "i", "]", ",", "color", "=", "colors", "[", "i", "]", ",", "\n", "s", "=", "ms", ")", "\n", "phandlers", ".", "append", "(", "p", ")", "\n", "plabels", ".", "append", "(", "'Predictions'", ")", "\n", "\n", "", "", "if", "publication_style", ":", "\n", "            ", "axes", ".", "grid", "(", "False", ")", "\n", "axes", ".", "set_facecolor", "(", "'w'", ")", "\n", "axes", ".", "axhline", "(", "y", "=", "axes", ".", "get_ylim", "(", ")", "[", "0", "]", ",", "color", "=", "'k'", ",", "lw", "=", "lw", ")", "\n", "axes", ".", "axvline", "(", "x", "=", "axes", ".", "get_xlim", "(", ")", "[", "0", "]", ",", "color", "=", "'k'", ",", "lw", "=", "lw", ")", "\n", "if", "len", "(", "data_handlers", ")", "==", "3", ":", "\n", "                ", "plt", ".", "yticks", "(", "[", "-", "1", ",", "0", ",", "1", "]", ",", "fontsize", "=", "ts", ")", "\n", "plt", ".", "xticks", "(", "[", "-", "2.5", ",", "0", ",", "2.5", "]", ",", "fontsize", "=", "ts", ")", "\n", "", "else", ":", "\n", "                ", "for", "tick", "in", "axes", ".", "yaxis", ".", "get_major_ticks", "(", ")", ":", "\n", "                    ", "tick", ".", "label", ".", "set_fontsize", "(", "ts", ")", "\n", "", "for", "tick", "in", "axes", ".", "xaxis", ".", "get_major_ticks", "(", ")", ":", "\n", "                    ", "tick", ".", "label", ".", "set_fontsize", "(", "ts", ")", "\n", "", "", "axes", ".", "tick_params", "(", "axis", "=", "'both'", ",", "length", "=", "lw", ",", "direction", "=", "'out'", ",", "width", "=", "lw", "/", "2.", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "legend", "(", "phandlers", ",", "plabels", ")", "\n", "\n", "", "plt", ".", "xlabel", "(", "'$x$'", ",", "fontsize", "=", "ts", ")", "\n", "plt", ".", "ylabel", "(", "'$y$'", ",", "fontsize", "=", "ts", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "\n", "if", "filename", "is", "not", "None", ":", "\n", "#plt.savefig(filename + '.pdf', bbox_inches='tight')", "\n", "            ", "plt", ".", "savefig", "(", "filename", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n", "", "if", "show", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.__init__": [[57, 163], ["torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "mnets.mnet_interface.MainNetInterface.__init__", "warnings.warn", "torch.ParameterList", "torch.ParameterList", "torch.ParameterList", "enumerate", "main_model.MainNetwork._is_properly_setup", "NotImplementedError", "NotImplementedError", "len", "ValueError", "print", "torch.Dropout", "torch.Dropout", "torch.Dropout", "main_model.MainNetwork._is_properly_setup", "main_model.MainNetwork._weights.append", "range", "range", "torch.Parameter", "torch.Parameter", "torch.Parameter", "len", "len", "len", "numpy.all", "len", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "numpy.equal", "utils.torch_utils.init_params", "utils.torch_utils.init_params", "mnets.mnet_interface.MainNetInterface.shapes_to_num_weights", "list", "list"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface._is_properly_setup", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params", "home.repos.pwc.inspect_result.chrhenning_hypercl.utils.torch_utils.init_params", "home.repos.pwc.inspect_result.chrhenning_hypercl.mnets.mnet_interface.MainNetInterface.shapes_to_num_weights"], ["def", "__init__", "(", "self", ",", "weight_shapes", ",", "activation_fn", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "use_bias", "=", "True", ",", "no_weights", "=", "False", ",", "init_weights", "=", "None", ",", "\n", "dropout_rate", "=", "-", "1", ",", "out_fn", "=", "None", ",", "verbose", "=", "True", ",", "\n", "use_spectral_norm", "=", "False", ",", "use_batch_norm", "=", "False", ")", ":", "\n", "        ", "\"\"\"Initialize the network.\n\n        Args:\n            weight_shapes: A list of list of integers, denoting the shape of\n                each parameter tensor in this network. Note, this parameter only\n                has an effect on the construction of this network, if\n                \"no_weights\" is False. Otherwise, it is just used to check the\n                shapes of the input to the network in the forward method.\n            activation_fn: The nonlinearity used in hidden layers. If None, no\n                nonlinearity will be applied.\n            use_bias: Whether layers may have bias terms.\n            no_weights: If set to True, no trainable parameters will be\n                constructed, i.e., weights are assumed to be produced ad-hoc\n                by a hypernetwork and passed to the forward function.\n            init_weights (optional): This option is for convinience reasons.\n                The option expects a list of parameter values that are used to\n                initialize the network weights. As such, it provides a\n                convinient way of initializing a network with a weight draw\n                produced by the hypernetwork.\n            dropout_rate: If -1, no dropout will be applied. Otherwise a number\n                between 0 and 1 is expected, denoting the dropout rate of hidden\n                layers.\n            out_fn (optional): If provided, this function will be applied to the\n                output neurons of the network. Note, this changes the output\n                of the forward method.\n            verbose: Whether to print the number of weights in the network.\n            use_spectral_norm: Use spectral normalization for training.\n            use_batch_norm: Whether batch normalization should be used.\n        \"\"\"", "\n", "# FIXME find a way using super to handle multiple inheritence.", "\n", "#super(MainNetwork, self).__init__()", "\n", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "MainNetInterface", ".", "__init__", "(", "self", ")", "\n", "\n", "warn", "(", "'Please use class \"mnets.mlp.MLP\" instead.'", ",", "DeprecationWarning", ")", "\n", "\n", "if", "use_spectral_norm", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Spectral normalization not yet '", "+", "\n", "'implemented for this network.'", ")", "\n", "", "if", "use_batch_norm", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Batch normalization not yet '", "+", "\n", "'implemented for this network.'", ")", "\n", "\n", "", "assert", "(", "len", "(", "weight_shapes", ")", ">", "0", ")", "\n", "self", ".", "_all_shapes", "=", "weight_shapes", "\n", "self", ".", "_has_bias", "=", "use_bias", "\n", "self", ".", "_a_fun", "=", "activation_fn", "\n", "assert", "(", "init_weights", "is", "None", "or", "no_weights", "is", "False", ")", "\n", "self", ".", "_no_weights", "=", "no_weights", "\n", "self", ".", "_dropout_rate", "=", "dropout_rate", "\n", "self", ".", "_out_fn", "=", "out_fn", "\n", "\n", "self", ".", "_has_fc_out", "=", "True", "\n", "\n", "if", "use_spectral_norm", "and", "no_weights", ":", "\n", "            ", "raise", "ValueError", "(", "'Cannot use spectral norm in a network without '", "+", "\n", "'parameters.'", ")", "\n", "\n", "", "if", "use_spectral_norm", ":", "\n", "            ", "self", ".", "_spec_norm", "=", "nn", ".", "utils", ".", "spectral_norm", "\n", "", "else", ":", "\n", "            ", "self", ".", "_spec_norm", "=", "lambda", "x", ":", "x", "# identity", "\n", "\n", "\n", "", "if", "verbose", ":", "\n", "            ", "print", "(", "'Creating an MLP with %d weights'", "%", "(", "MnetAPIV2", ".", "shapes_to_num_weights", "(", "self", ".", "_all_shapes", ")", ")", "\n", "+", "(", "', that uses dropout.'", "if", "dropout_rate", "!=", "-", "1", "else", "'.'", ")", ")", "\n", "\n", "", "if", "dropout_rate", "!=", "-", "1", ":", "\n", "            ", "assert", "(", "dropout_rate", ">=", "0.", "and", "dropout_rate", "<=", "1.", ")", "\n", "self", ".", "_dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout_rate", ")", "\n", "\n", "", "self", ".", "_weights", "=", "None", "\n", "if", "no_weights", ":", "\n", "            ", "self", ".", "_hyper_shapes", "=", "self", ".", "_all_shapes", "\n", "self", ".", "_is_properly_setup", "(", ")", "\n", "return", "\n", "\n", "### Define and initialize network weights.", "\n", "# Each odd entry of this list will contain a weight Tensor and each", "\n", "# even entry a bias vector.", "\n", "", "self", ".", "_weights", "=", "nn", ".", "ParameterList", "(", ")", "\n", "\n", "for", "i", ",", "dims", "in", "enumerate", "(", "self", ".", "_all_shapes", ")", ":", "\n", "            ", "self", ".", "_weights", ".", "append", "(", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "*", "dims", ")", ",", "\n", "requires_grad", "=", "True", ")", ")", "\n", "\n", "", "if", "init_weights", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "init_weights", ")", "==", "len", "(", "self", ".", "_all_shapes", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "init_weights", ")", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "list", "(", "init_weights", "[", "i", "]", ".", "shape", ")", ",", "\n", "list", "(", "self", ".", "_weights", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "self", ".", "_weights", "[", "i", "]", ".", "data", "=", "init_weights", "[", "i", "]", "\n", "", "", "else", ":", "\n", "            ", "for", "i", "in", "range", "(", "0", ",", "len", "(", "self", ".", "_weights", ")", ",", "2", "if", "use_bias", "else", "1", ")", ":", "\n", "                ", "if", "use_bias", ":", "\n", "                    ", "init_params", "(", "self", ".", "_weights", "[", "i", "]", ",", "self", ".", "_weights", "[", "i", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "                    ", "init_params", "(", "self", ".", "_weights", "[", "i", "]", ")", "\n", "\n", "", "", "", "self", ".", "_is_properly_setup", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.forward": [[164, 226], ["range", "Exception", "enumerate", "len", "len", "main_model.MainNetwork._spec_norm", "len", "len", "numpy.all", "len", "torch.linear", "torch.linear", "torch.linear", "main_model.MainNetwork._out_fn", "numpy.equal", "main_model.MainNetwork._dropout", "main_model.MainNetwork._a_fun", "list"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "weights", "=", "None", ")", ":", "\n", "        ", "\"\"\"Predict the output y given the input x, that is propagated through a\n        fully-connected network (using the given weights).\n\n        Args:\n            x: The input to the network.\n            weights: A list of torch parameter tensors has to be provided, where\n                each tensor has the shape as specified by the list\n                \"weight_shapes\" provided to the constructor.\n                If \"no_weights\" was set in the constructor, then this parameter\n                is mandatory.\n                Note, when provided, internal parameters are not used.\n\n        Returns:\n            (tuple): Tuple containing:\n    \n            - **y**: The output of the network.\n            - **h_y** (optional): If `out_fn` was specified in the constructor,\n              then this value will be returned. It is the last hidden activation\n              (before the `out_fn` has been applied).\n        \"\"\"", "\n", "if", "self", ".", "_no_weights", "and", "weights", "is", "None", ":", "\n", "            ", "raise", "Exception", "(", "'Network was generated without weights. '", "+", "\n", "'Hence, \"weights\" option may not be None.'", ")", "\n", "\n", "", "if", "weights", "is", "None", ":", "\n", "            ", "weights", "=", "self", ".", "weights", "\n", "", "else", ":", "\n", "            ", "shapes", "=", "self", ".", "param_shapes", "\n", "assert", "(", "len", "(", "weights", ")", "==", "len", "(", "shapes", ")", ")", "\n", "for", "i", ",", "s", "in", "enumerate", "(", "shapes", ")", ":", "\n", "                ", "assert", "(", "np", ".", "all", "(", "np", ".", "equal", "(", "s", ",", "list", "(", "weights", "[", "i", "]", ".", "shape", ")", ")", ")", ")", "\n", "\n", "", "", "hidden", "=", "x", "\n", "\n", "if", "self", ".", "has_bias", ":", "\n", "            ", "num_layers", "=", "len", "(", "weights", ")", "//", "2", "\n", "step_size", "=", "2", "\n", "", "else", ":", "\n", "            ", "num_layers", "=", "len", "(", "weights", ")", "\n", "step_size", "=", "1", "\n", "\n", "", "for", "l", "in", "range", "(", "0", ",", "len", "(", "weights", ")", ",", "step_size", ")", ":", "\n", "            ", "W", "=", "weights", "[", "l", "]", "\n", "if", "self", ".", "has_bias", ":", "\n", "                ", "b", "=", "weights", "[", "l", "+", "1", "]", "\n", "", "else", ":", "\n", "                ", "b", "=", "None", "\n", "\n", "", "hidden", "=", "self", ".", "_spec_norm", "(", "F", ".", "linear", "(", "hidden", ",", "W", ",", "bias", "=", "b", ")", ")", "\n", "\n", "# Only for hidden layers.", "\n", "if", "l", "/", "step_size", "+", "1", "<", "num_layers", ":", "\n", "                ", "if", "self", ".", "_dropout_rate", "!=", "-", "1", ":", "\n", "                    ", "hidden", "=", "self", ".", "_dropout", "(", "hidden", ")", "\n", "", "if", "self", ".", "_a_fun", "is", "not", "None", ":", "\n", "                    ", "hidden", "=", "self", ".", "_a_fun", "(", "hidden", ")", "\n", "\n", "", "", "", "if", "self", ".", "_out_fn", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_out_fn", "(", "hidden", ")", ",", "hidden", "\n", "\n", "", "return", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.mse": [[227, 241], ["warnings.warn", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "mse", "(", "predictions", ",", "targets", ")", ":", "\n", "        ", "\"\"\"Compute the mean squared error.\n\n        Args:\n            predictions: Outputs y from the network.\n            targets: Targets corresponding to the predictions.\n\n        Returns:\n            MSE between predictions and targets.\n        \"\"\"", "\n", "warn", "(", "'Use torch.nn.functional.mse_loss instead.'", ",", "DeprecationWarning", ")", "\n", "\n", "return", "F", ".", "mse_loss", "(", "predictions", ",", "targets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.weight_shapes": [[242, 269], ["enumerate", "shapes.append", "shapes.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "weight_shapes", "(", "n_in", "=", "1", ",", "n_out", "=", "1", ",", "hidden_layers", "=", "[", "10", ",", "10", "]", ",", "use_bias", "=", "True", ")", ":", "\n", "        ", "\"\"\"Compute the tensor shapes of all parameters in a fully-connected\n        network.\n\n        Args:\n            n_in: Number of inputs.\n            n_out: Number of output units.\n            hidden_layers: A list of ints, each number denoting the size of a\n                hidden layer.\n            use_bias: Whether the FC layers should have biases.\n\n        Returns:\n            A list of list of integers, denoting the shapes of the individual\n            parameter tensors.\n        \"\"\"", "\n", "shapes", "=", "[", "]", "\n", "\n", "prev_dim", "=", "n_in", "\n", "layer_out_sizes", "=", "hidden_layers", "+", "[", "n_out", "]", "\n", "for", "i", ",", "size", "in", "enumerate", "(", "layer_out_sizes", ")", ":", "\n", "            ", "shapes", ".", "append", "(", "[", "size", ",", "prev_dim", "]", ")", "\n", "if", "use_bias", ":", "\n", "                ", "shapes", ".", "append", "(", "[", "size", "]", ")", "\n", "", "prev_dim", "=", "size", "\n", "\n", "", "return", "shapes", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.toy_example.main_model.MainNetwork.get_reg_masks": [[270, 328], ["torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_reg_masks", "(", "weight_shapes", ",", "allowed_outputs", ",", "device", ",", "\n", "use_bias", "=", "True", ")", ":", "\n", "        ", "\"\"\"Get a binary mask for a weight preserving regularizer (e.g., via a\n        quadratic penalty) for a network with a multi-head output.\n        When regularizing the weights for task i, all heads j != i don't\n        influence that task performance. Hence, the weights connecting to output\n        heads j are arbitrary (with respect to the current task) and don't\n        have to be regularized.\n\n        Note, this function only applies to networks that have a fully-connected\n        layer as output layer.\n\n        Note, it would only be sensible to include such a mask in the\n        regularizer. For instance, in EWC the diagonal Fisher estimate is\n        naturally zero for unused weights.\n\n        Args:\n            weight_shapes: The shapes of all parameter tensors in the network to\n                be regularized (e.g., as returned by the method\n                \"weight_shapes\").\n                Note, the last two entries in this list are expected to be the\n                weights of a final fully-connected layer (a weight matrix and\n                a bias vector). Only these tensors can be masked out by this\n                function.\n                If not \"use_bias\", then only the last parameter tensor is\n                considered (the weight matrix of the final fully-connected\n                layer).\n            allowed_outputs: A list of indices, denoting the output neurons\n                (in the final layer) that belong to the current task. Only the\n                weight connecting to these neurons may be regularized. All other\n                output weights will be zeroed-out by the returned mask.\n            device: PyTorch device to move the masks to.\n            use_bias: Whether the final FC layer uses a bias vector (see\n                argument \"weight_shapes\" for more details).\n\n        Returns:\n            A list of binary tensors, where each tensor has a shape\n            corresponding to \"weight_shapes\". All values are 1 except for the\n            masks corresponding to the output weights, which are only 1 for the\n            weights connecting to the output weights.\n        \"\"\"", "\n", "reg_mask", "=", "[", "torch", ".", "ones", "(", "s", ")", ".", "to", "(", "device", ")", "for", "s", "in", "weight_shapes", "]", "\n", "\n", "reg_mask", "[", "-", "1", "]", "=", "torch", ".", "zeros", "(", "weight_shapes", "[", "-", "1", "]", ")", ".", "to", "(", "device", ")", "\n", "if", "use_bias", ":", "\n", "            ", "reg_mask", "[", "-", "2", "]", "=", "torch", ".", "zeros", "(", "weight_shapes", "[", "-", "2", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "if", "use_bias", ":", "\n", "            ", "bias_mask", "=", "reg_mask", "[", "-", "1", "]", "# Output bias", "\n", "weight_mask", "=", "reg_mask", "[", "-", "2", "]", "# Output weight", "\n", "\n", "bias_mask", "[", "allowed_outputs", "]", "=", "1", "\n", "", "else", ":", "\n", "            ", "weight_mask", "=", "reg_mask", "[", "-", "1", "]", "\n", "", "weight_mask", "[", "allowed_outputs", ",", ":", "]", "=", "1", "\n", "\n", "return", "reg_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.ilsvrc2012_data.ILSVRC2012Data.__init__": [[142, 234], ["data.large_img_dataset.LargeImgDataset.__init__", "time.time", "print", "os.path.join", "os.path.join", "os.path.join", "dict", "ilsvrc2012_data.ILSVRC2012Data._read_meta", "ilsvrc2012_data.ILSVRC2012Data._process_dataset", "len", "len", "len", "numpy.chararray", "enumerate", "numpy.array().reshape", "numpy.arange", "numpy.arange", "print", "time.time", "print", "os.path.exists", "FileNotFoundError", "len", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "warnings.warn", "ilsvrc2012_data.ILSVRC2012Data._to_one_hot", "numpy.arange", "os.path.exists", "FileNotFoundError", "max", "numpy.array", "os.path.exists", "FileNotFoundError", "len"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._read_meta", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.ilsvrc2012_data.ILSVRC2012Data._process_dataset", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot"], ["def", "__init__", "(", "self", ",", "data_path", ",", "use_one_hot", "=", "False", ",", "num_val_per_class", "=", "0", ")", ":", "\n", "# 732 is the minimum number of training samples per class in", "\n", "# ILSVRC2012.", "\n", "        ", "assert", "(", "num_val_per_class", "<", "732", ")", "\n", "# We keep the full path to each image in memory, so we don't need to", "\n", "# tell the super class the root path to each image (i.e., samples", "\n", "# contain absolute not relative paths).", "\n", "super", "(", ")", ".", "__init__", "(", "''", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "'Reading ILSVRC2012 dataset ...'", ")", "\n", "\n", "meta_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "ILSVRC2012Data", ".", "_META_FILE", ")", "\n", "train_dir", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "ILSVRC2012Data", ".", "_TRAIN_FOLDER", ")", "\n", "val_dir", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "ILSVRC2012Data", ".", "_VAL_FOLDER", ")", "\n", "\n", "err_msg", "=", "'Please follow the steps described in the file '", "+", "'data/README.md to download and extract the data.'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_dir", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Training images not found in directory '", "+", "\n", "train_dir", "+", "'.\\n'", "+", "err_msg", ")", "\n", "", "elif", "not", "os", ".", "path", ".", "exists", "(", "val_dir", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Validation images not found in '", "+", "\n", "'directory '", "+", "val_dir", "+", "'.\\n'", "+", "err_msg", ")", "\n", "", "elif", "not", "os", ".", "path", ".", "exists", "(", "meta_fn", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Meta file not found: '", "+", "\n", "meta_fn", "+", "'.\\n'", "+", "err_msg", ")", "\n", "\n", "# Read meta file.", "\n", "", "self", ".", "_data", "[", "'meta'", "]", "=", "dict", "(", ")", "\n", "self", ".", "_read_meta", "(", "meta_fn", ")", "\n", "\n", "# Read dataset.", "\n", "self", ".", "_process_dataset", "(", "train_dir", ",", "val_dir", ",", "use_one_hot", ",", "\n", "num_val_per_class", ")", "\n", "\n", "# Translate everything into the internal structure of this class.", "\n", "num_train", "=", "len", "(", "self", ".", "_torch_ds_train", ")", "\n", "num_test", "=", "len", "(", "self", ".", "_torch_ds_test", ")", "\n", "num_val", "=", "0", "if", "self", ".", "_torch_ds_val", "is", "None", "else", "len", "(", "self", ".", "_torch_ds_val", ")", "\n", "num_samples", "=", "num_train", "+", "num_test", "+", "num_val", "\n", "# Just a sanity check, as these numbers should be fixed whenever the", "\n", "# full dataset is loaded.", "\n", "if", "num_test", "!=", "50000", ":", "\n", "            ", "warnings", ".", "warn", "(", "'ILSVRC2012 should contain 50,000 test samples, '", "+", "\n", "'but %d samples were found!'", "%", "num_test", ")", "\n", "", "if", "num_train", "+", "num_val", "!=", "1281167", ":", "\n", "            ", "warnings", ".", "warn", "(", "'ILSVRC2012 should contain 1,281,167 training '", "+", "\n", "'samples, but %d samples were found!'", "\n", "%", "(", "num_train", "+", "num_val", ")", ")", "\n", "\n", "# Maximum string length of an image path.", "\n", "", "max_path_len", "=", "len", "(", "max", "(", "self", ".", "_torch_ds_train", ".", "samples", "+", "\n", "(", "[", "]", "if", "num_val", "==", "0", "else", "self", ".", "_torch_ds_val", ".", "samples", ")", "+", "\n", "self", ".", "_torch_ds_test", ".", "samples", ",", "key", "=", "lambda", "t", ":", "len", "(", "t", "[", "0", "]", ")", ")", "[", "0", "]", ")", "\n", "\n", "self", ".", "_data", "[", "'classification'", "]", "=", "True", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'num_classes'", "]", "=", "1000", "\n", "self", ".", "_data", "[", "'is_one_hot'", "]", "=", "use_one_hot", "\n", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "224", ",", "224", ",", "3", "]", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "1000", "if", "use_one_hot", "else", "1", "]", "\n", "\n", "self", ".", "_data", "[", "'in_data'", "]", "=", "np", ".", "chararray", "(", "[", "num_samples", ",", "1", "]", ",", "\n", "itemsize", "=", "max_path_len", ",", "unicode", "=", "True", ")", "\n", "for", "i", ",", "(", "img_path", ",", "_", ")", "in", "enumerate", "(", "self", ".", "_torch_ds_train", ".", "samples", "+", "\n", "(", "[", "]", "if", "num_val", "==", "0", "else", "self", ".", "_torch_ds_val", ".", "samples", ")", "+", "\n", "self", ".", "_torch_ds_test", ".", "samples", ")", ":", "\n", "            ", "self", ".", "_data", "[", "'in_data'", "]", "[", "i", ",", ":", "]", "=", "img_path", "\n", "\n", "", "labels", "=", "np", ".", "array", "(", "self", ".", "_torch_ds_train", ".", "targets", "+", "\n", "(", "[", "]", "if", "num_val", "==", "0", "else", "self", ".", "_torch_ds_val", ".", "targets", ")", "+", "\n", "self", ".", "_torch_ds_test", ".", "targets", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "if", "use_one_hot", ":", "\n", "            ", "labels", "=", "self", ".", "_to_one_hot", "(", "labels", ")", "\n", "", "self", ".", "_data", "[", "'out_data'", "]", "=", "labels", "\n", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ")", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "np", ".", "arange", "(", "num_train", "+", "num_val", ",", "num_samples", ")", "\n", "if", "num_val", "==", "0", ":", "\n", "            ", "self", ".", "_data", "[", "'val_inds'", "]", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "_data", "[", "'val_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ",", "num_train", "+", "num_val", ")", "\n", "\n", "", "print", "(", "'Dataset consists of %d training, %d validation and %d test '", "\n", "%", "(", "num_train", ",", "num_val", ",", "num_test", ")", "+", "'samples.'", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'Elapsed time to read dataset: %f sec'", "%", "(", "end", "-", "start", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.ilsvrc2012_data.ILSVRC2012Data.tf_input_map": [[235, 240], ["NotImplementedError"], "methods", ["None"], ["", "def", "tf_input_map", "(", "self", ",", "mode", "=", "'inference'", ")", ":", "\n", "        ", "\"\"\"Not impemented.\"\"\"", "\n", "# Confirm, whether you wanna process data as in the baseclass or", "\n", "# implement a new image loader.", "\n", "raise", "NotImplementedError", "(", "'Not implemented yet!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.ilsvrc2012_data.ILSVRC2012Data.get_identifier": [[241, 244], ["None"], "methods", ["None"], ["", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'ILSVRC2012'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.ilsvrc2012_data.ILSVRC2012Data._read_meta": [[245, 288], ["dict", "dict", "range", "scipy.io.loadmat", "len", "len", "dict.keys", "dict.items", "wnid2imid.keys"], "methods", ["None"], ["", "def", "_read_meta", "(", "self", ",", "meta_fn", ")", ":", "\n", "        ", "\"\"\"Read the meta file to know how to translate WNID to ILSVRC2012_ID.\n\n        The following attributes are added (dictionaries):\n            _imid_to_wnid: ILSVRC2012_ID to WNID.\n            _wnid_to_imid: WNID to ILSVRC2012_ID.\n            _imid_to_words: ILSVRC2012_ID to set of words (textual description\n                of label).\n\n        Args:\n            meta_fn: Path to meta file.\n        \"\"\"", "\n", "meta", "=", "loadmat", "(", "meta_fn", ")", "[", "'synsets'", "]", "\n", "\n", "# ILSVRC2012_ID -> WNID", "\n", "imid2wnid", "=", "dict", "(", ")", "\n", "# ILSVRC2012_ID -> Example Words", "\n", "imid2words", "=", "dict", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "meta", ".", "size", ")", ":", "\n", "            ", "imid", "=", "meta", "[", "i", "]", "[", "0", "]", "[", "0", "]", "[", "0", "]", "[", "0", "]", "# ILSVRC2012_ID", "\n", "wnid", "=", "meta", "[", "i", "]", "[", "0", "]", "[", "1", "]", "[", "0", "]", "# WNID", "\n", "words", "=", "meta", "[", "i", "]", "[", "0", "]", "[", "2", "]", "[", "0", "]", "# words", "\n", "num_children", "=", "meta", "[", "i", "]", "[", "0", "]", "[", "4", "]", "[", "0", "]", "[", "0", "]", "\n", "if", "num_children", "!=", "0", ":", "\n", "# We don't care about non-leaf nodes.", "\n", "                ", "assert", "(", "imid", ">=", "1000", ")", "\n", "continue", "\n", "", "assert", "(", "imid", ">=", "1", "and", "imid", "<=", "1000", ")", "\n", "\n", "# NOTE internally, we subtract 1 from all ILSVRC2012_ID to have", "\n", "# labels between 0 and 999.", "\n", "imid2wnid", "[", "imid", "-", "1", "]", "=", "wnid", "\n", "imid2words", "[", "imid", "-", "1", "]", "=", "words", "\n", "\n", "", "assert", "(", "len", "(", "imid2wnid", ".", "keys", "(", ")", ")", "==", "1000", ")", "\n", "\n", "wnid2imid", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "imid2wnid", ".", "items", "(", ")", "}", "\n", "assert", "(", "len", "(", "wnid2imid", ".", "keys", "(", ")", ")", "==", "1000", ")", "\n", "\n", "self", ".", "_imid_to_wnid", "=", "imid2wnid", "\n", "self", ".", "_wnid_to_imid", "=", "wnid2imid", "\n", "self", ".", "_imid_to_words", "=", "imid2words", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.ilsvrc2012_data.ILSVRC2012Data._process_dataset": [[289, 395], ["ilsvrc2012_data.ILSVRC2012Data.torch_input_transforms", "torchvision.ImageFolder", "torchvision.ImageFolder", "torchvision.ImageFolder", "torchvision.ImageFolder", "torchvision.ImageFolder", "torchvision.ImageFolder", "wnid2lbl.keys", "len", "len", "range", "copy.deepcopy", "len", "numpy.zeros", "wnid2lbl.keys", "torchvision.ImageFolder.class_to_idx.keys", "torchvision.ImageFolder.class_to_idx.keys", "wnid2lbl.items", "len", "ilsvrc2012_data.ILSVRC2012Data._imid_to_wnid.keys", "torchvision.ImageFolder.samples.append", "copy.deepcopy.samples.append", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.torch_input_transforms"], ["", "def", "_process_dataset", "(", "self", ",", "train_dir", ",", "val_dir", ",", "use_one_hot", ",", "\n", "num_val_per_class", ")", ":", "\n", "        ", "\"\"\"Read and process the datasets using PyTorch its ImageFolder class.\n\n        The labels used by the ImageFolder class are changed to match the\n        ILSVRC2012_ID labels (where 1 is subtracted to get labels between 0 and\n        999).\n\n        Additionally, this method splits the Imagenet training set into train\n        and validation set. The original ImageNet validation set is used as test\n        set.\n\n        The following attributes are added to the class:\n            _torch_ds_train: A PyTorch Dataset class representing the training\n                set.\n            _torch_ds_test: A PyTorch Dataset class representing the validation\n                set (corresponds to the dataset in \"val_dir\").\n            _torch_ds_val: A PyTorch Dataset class representing the validation\n                set (A subset of the training set).\n            _wnid_to_clbl: A dictionary translating WNID to the \"common label\",\n                that is used by data loaders that simply use the \"ImageFolder\"\n                class. For instance, the pretrained ImageNet classifiers in\n                the the PyTorch model zoo:\n                    https://pytorch.org/docs/stable/torchvision/models.html\n\n        Args:\n            See docstring of constructor.\n            train_dir: Path to ILSVRC2012 training images.\n            val_dir: Path to ILSVRC2012 validation images.\n        \"\"\"", "\n", "# Read raw dataset using the PyTorch ImageFolder class.", "\n", "train_transform", ",", "test_transform", "=", "ILSVRC2012Data", ".", "torch_input_transforms", "(", ")", "\n", "ds_train", "=", "datasets", ".", "ImageFolder", "(", "train_dir", ",", "train_transform", ")", "\n", "ds_test", "=", "datasets", ".", "ImageFolder", "(", "val_dir", ",", "test_transform", ")", "\n", "ds_val", "=", "None", "\n", "\n", "### Translate targets to ILSVRC2012_ID labels.", "\n", "wnid2lbl", "=", "ds_train", ".", "class_to_idx", "\n", "# Sanity check.", "\n", "assert", "(", "len", "(", "wnid2lbl", ".", "keys", "(", ")", ")", "==", "len", "(", "ds_test", ".", "class_to_idx", ".", "keys", "(", ")", ")", ")", "\n", "for", "k", "in", "wnid2lbl", ".", "keys", "(", ")", ":", "\n", "            ", "assert", "(", "k", "in", "ds_test", ".", "class_to_idx", ".", "keys", "(", ")", ")", "\n", "assert", "(", "wnid2lbl", "[", "k", "]", "==", "ds_test", ".", "class_to_idx", "[", "k", "]", ")", "\n", "\n", "", "lbl2wnid", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "wnid2lbl", ".", "items", "(", ")", "}", "\n", "\n", "for", "ds_obj", "in", "[", "ds_train", ",", "ds_test", "]", ":", "\n", "            ", "for", "s", "in", "range", "(", "len", "(", "ds_obj", ".", "samples", ")", ")", ":", "\n", "                ", "img_path", ",", "lbl", "=", "ds_obj", ".", "samples", "[", "s", "]", "\n", "assert", "(", "ds_obj", ".", "targets", "[", "s", "]", "==", "lbl", ")", "\n", "\n", "wnid", "=", "lbl2wnid", "[", "lbl", "]", "\n", "# We assume a folder structure where images are stored under", "\n", "# their corresponding WNID.", "\n", "assert", "(", "wnid", "in", "img_path", ")", "\n", "\n", "imid", "=", "self", ".", "_wnid_to_imid", "[", "wnid", "]", "\n", "\n", "ds_obj", ".", "samples", "[", "s", "]", "=", "(", "img_path", ",", "imid", ")", "\n", "ds_obj", ".", "targets", "[", "s", "]", "=", "imid", "\n", "\n", "assert", "(", "ds_obj", ".", "imgs", "[", "s", "]", "[", "1", "]", "==", "imid", ")", "\n", "\n", "# The mapping from class name (WNID) to label has changed!", "\n", "", "ds_obj", ".", "class_to_idx", "=", "self", ".", "_wnid_to_imid", "\n", "\n", "### Split training set into train/val set.", "\n", "", "if", "num_val_per_class", ">", "0", ":", "\n", "            ", "orig_samples", "=", "ds_train", ".", "samples", "\n", "ds_train", ".", "samples", "=", "None", "\n", "ds_train", ".", "imgs", "=", "None", "\n", "ds_train", ".", "targets", "=", "None", "\n", "\n", "ds_val", "=", "deepcopy", "(", "ds_train", ")", "\n", "ds_val", ".", "transform", "=", "test_transform", "\n", "assert", "(", "ds_val", ".", "target_transform", "is", "None", ")", "\n", "\n", "num_classes", "=", "len", "(", "self", ".", "_imid_to_wnid", ".", "keys", "(", ")", ")", "\n", "assert", "(", "num_classes", "==", "1000", ")", "\n", "val_counts", "=", "np", ".", "zeros", "(", "num_classes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "ds_train", ".", "samples", "=", "[", "]", "\n", "ds_train", ".", "imgs", "=", "ds_train", ".", "samples", "\n", "ds_val", ".", "samples", "=", "[", "]", "\n", "ds_val", ".", "imgs", "=", "ds_val", ".", "samples", "\n", "\n", "for", "img_path", ",", "img_lbl", "in", "orig_samples", ":", "\n", "                ", "if", "val_counts", "[", "img_lbl", "]", ">=", "num_val_per_class", ":", "# train sample", "\n", "                    ", "ds_train", ".", "samples", ".", "append", "(", "(", "img_path", ",", "img_lbl", ")", ")", "\n", "", "else", ":", "# validation sample", "\n", "                    ", "val_counts", "[", "img_lbl", "]", "+=", "1", "\n", "ds_val", ".", "samples", ".", "append", "(", "(", "img_path", ",", "img_lbl", ")", ")", "\n", "\n", "", "", "ds_train", ".", "targets", "=", "[", "s", "[", "1", "]", "for", "s", "in", "ds_train", ".", "samples", "]", "\n", "ds_val", ".", "targets", "=", "[", "s", "[", "1", "]", "for", "s", "in", "ds_val", ".", "samples", "]", "\n", "\n", "for", "ds_obj", "in", "[", "ds_train", ",", "ds_val", "]", ":", "\n", "                ", "assert", "(", "len", "(", "ds_obj", ".", "samples", ")", "==", "len", "(", "ds_obj", ".", "imgs", ")", "and", "len", "(", "ds_obj", ".", "samples", ")", "==", "len", "(", "ds_obj", ".", "targets", ")", ")", "\n", "\n", "", "", "self", ".", "_torch_ds_train", "=", "ds_train", "\n", "self", ".", "_torch_ds_test", "=", "ds_test", "\n", "self", ".", "_torch_ds_val", "=", "ds_val", "\n", "\n", "self", ".", "_wnid_to_clbl", "=", "wnid2lbl", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.ilsvrc2012_data.ILSVRC2012Data.to_common_labels": [[396, 439], ["isinstance", "range", "numpy.copy", "isinstance", "ilsvrc2012_data.ILSVRC2012Data.clone", "ilsvrc2012_data.ILSVRC2012Data._to_one_hot", "ilsvrc2012_data.ILSVRC2012Data._to_one_hot", "len", "NotImplementedError", "int"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot"], ["", "def", "to_common_labels", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"Translate between label conventions.\n\n        Translate a given set of labels (that correspond to the\n        ``ILSVRC2012_ID`` (minus one) of their images) back to the labels\n        provided by the :class:`torchvision.datasets.ImageFolder` class.\n\n        Note:\n            This would be the label convention for ImageNet used by\n            PyTorch examples.\n\n        Args:\n            outputs: Targets (as integers or 1-hot encodings).\n\n        Returns:\n            The translated targets (if the targets where given as 1-hot\n            encodings, then this method also returns 1-hot encodings).\n        \"\"\"", "\n", "is_np", "=", "False", "\n", "# We don't want to do inplace modifications.", "\n", "if", "isinstance", "(", "outputs", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "is_np", "=", "True", "\n", "outputs", "=", "np", ".", "copy", "(", "outputs", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "isinstance", "(", "outputs", ",", "torch", ".", "Tensor", ")", ")", "\n", "outputs", "=", "outputs", ".", "clone", "(", ")", "\n", "\n", "", "is_1_hot", "=", "False", "\n", "if", "len", "(", "outputs", ".", "shape", ")", "==", "2", "and", "outputs", ".", "shape", "[", "1", "]", "==", "self", ".", "num_classes", ":", "\n", "            ", "if", "not", "is_np", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'Method can\\'t deal with 1-hot '", "+", "\n", "'encodings provided as Torch tensors yet!'", ")", "\n", "", "is_1_hot", "=", "True", "\n", "outputs", "=", "self", ".", "_to_one_hot", "(", "outputs", ",", "reverse", "=", "True", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "outputs", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "wnid", "=", "self", ".", "_imid_to_wnid", "[", "int", "(", "outputs", "[", "i", "]", ")", "]", "\n", "outputs", "[", "i", "]", "=", "self", ".", "_wnid_to_clbl", "[", "wnid", "]", "\n", "\n", "", "if", "is_1_hot", ":", "\n", "            ", "outputs", "=", "self", ".", "_to_one_hot", "(", "outputs", ",", "reverse", "=", "False", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.ilsvrc2012_data.ILSVRC2012Data.torch_input_transforms": [[440, 474], ["torchvision.Normalize", "torchvision.Normalize", "torchvision.Normalize", "torchvision.Compose", "torchvision.Compose", "torchvision.Compose", "torchvision.Compose", "torchvision.Compose", "torchvision.Compose", "torchvision.RandomResizedCrop", "torchvision.RandomResizedCrop", "torchvision.RandomResizedCrop", "torchvision.RandomHorizontalFlip", "torchvision.RandomHorizontalFlip", "torchvision.RandomHorizontalFlip", "torchvision.ToTensor", "torchvision.ToTensor", "torchvision.ToTensor", "torchvision.Resize", "torchvision.Resize", "torchvision.Resize", "torchvision.CenterCrop", "torchvision.CenterCrop", "torchvision.CenterCrop", "torchvision.ToTensor", "torchvision.ToTensor", "torchvision.ToTensor"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "torch_input_transforms", "(", ")", ":", "\n", "        ", "\"\"\"Get data augmentation pipelines for ILSVRC2012 inputs.\n\n        Note, the augmentation is inspired by the augmentation proposed in:\n            https://git.io/fjWPZ\n\n        Returns:\n            (tuple): Tuple containing:\n\n                - **train_transform**: A transforms pipeline that applies random\n                  transformations, normalizes the image and resizes/crops it\n                  to a final size of 224 x 224 pixels.\n                - **test_transform**: Similar to train_transform, but no random\n                  transformations are applied.\n        \"\"\"", "\n", "normalize", "=", "transforms", ".", "Normalize", "(", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "\n", "train_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "RandomResizedCrop", "(", "224", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "normalize", ",", "\n", "]", ")", "\n", "\n", "test_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "Resize", "(", "256", ")", ",", "\n", "transforms", ".", "CenterCrop", "(", "224", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "normalize", ",", "\n", "]", ")", "\n", "\n", "return", "train_transform", ",", "test_transform", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.ilsvrc2012_data.ILSVRC2012Data._plot_sample": [[475, 513], ["matplotlib.Subplot", "matplotlib.Subplot.set_axis_off", "matplotlib.Subplot.imshow", "fig.add_subplot", "matplotlib.Subplot.set_title", "numpy.asscalar", "ilsvrc2012_data.ILSVRC2012Data.read_images", "numpy.squeeze", "numpy.size", "matplotlib.Subplot.set_title", "matplotlib.Subplot.set_title", "numpy.reshape", "numpy.size", "numpy.argmax", "numpy.asscalar"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.read_images"], ["", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract method\n        :meth:`data.dataset.Dataset._plot_sample`.\n        \n        Note, label ID in the plot correspond to ``ILSVRC2012_ID`` minus 1.\n        \"\"\"", "\n", "ax", "=", "plt", ".", "Subplot", "(", "fig", ",", "inner_grid", "[", "0", "]", ")", "\n", "\n", "if", "outputs", "is", "None", ":", "\n", "            ", "ax", ".", "set_title", "(", "\"ILSVRC2012 Sample\"", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "np", ".", "size", "(", "outputs", ")", "==", "1", ")", "\n", "label", "=", "np", ".", "asscalar", "(", "outputs", ")", "\n", "label_name", "=", "self", ".", "_imid_to_words", "[", "label", "]", "\n", "\n", "if", "predictions", "is", "None", ":", "\n", "                ", "ax", ".", "set_title", "(", "'Label of shown sample:\\n%s (%d)'", "%", "(", "label_name", ",", "label", ")", ")", "\n", "", "else", ":", "\n", "                ", "if", "np", ".", "size", "(", "predictions", ")", "==", "self", ".", "num_classes", ":", "\n", "                    ", "pred_label", "=", "np", ".", "argmax", "(", "predictions", ")", "\n", "", "else", ":", "\n", "                    ", "pred_label", "=", "np", ".", "asscalar", "(", "predictions", ")", "\n", "", "pred_label_name", "=", "self", ".", "_imid_to_words", "[", "pred_label", "]", "\n", "\n", "ax", ".", "set_title", "(", "'Label of shown sample:\\n%s (%d)'", "%", "(", "label_name", ",", "label", ")", "+", "'\\nPrediction: %s (%d)'", "%", "(", "pred_label_name", ",", "pred_label", ")", ")", "\n", "\n", "", "", "if", "inputs", ".", "size", "==", "1", ":", "\n", "            ", "img", "=", "self", ".", "read_images", "(", "inputs", ")", "\n", "", "else", ":", "\n", "            ", "img", "=", "inputs", "\n", "\n", "", "ax", ".", "set_axis_off", "(", ")", "\n", "ax", ".", "imshow", "(", "np", ".", "squeeze", "(", "np", ".", "reshape", "(", "img", ",", "self", ".", "in_shape", ")", ")", ")", "\n", "fig", ".", "add_subplot", "(", "ax", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar100_data.CIFAR100Data.__init__": [[73, 136], ["data.dataset.Dataset.__init__", "time.time", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "cifar100_data.CIFAR100Data._read_meta", "cifar100_data.CIFAR100Data._read_batches", "time.time", "print", "os.path.exists", "print", "os.makedirs", "os.path.exists", "print", "urllib.request.urlretrieve", "tarfile.open", "tarfile.open.extractall", "tarfile.open.close", "os.remove", "os.path.exists", "os.path.exists", "os.path.exists", "data.cifar10_data.CIFAR10Data.torch_input_transforms"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._read_meta", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._read_batches", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.torch_input_transforms"], ["def", "__init__", "(", "self", ",", "data_path", ",", "use_one_hot", "=", "False", ",", "\n", "use_data_augmentation", "=", "False", ",", "validation_size", "=", "5000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "'Reading CIFAR-100 dataset ...'", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_path", ")", ":", "\n", "            ", "print", "(", "'Creating directory \"%s\" ...'", "%", "(", "data_path", ")", ")", "\n", "os", ".", "makedirs", "(", "data_path", ")", "\n", "\n", "", "extracted_data_dir", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "\n", "CIFAR100Data", ".", "_EXTRACTED_FOLDER", ")", "\n", "\n", "archive_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CIFAR100Data", ".", "_DOWNLOAD_FILE", ")", "\n", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "extracted_data_dir", ")", ":", "\n", "            ", "print", "(", "'Downloading dataset ...'", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "CIFAR100Data", ".", "_DOWNLOAD_PATH", "+", "CIFAR100Data", ".", "_DOWNLOAD_FILE", ",", "archive_fn", ")", "\n", "\n", "# Extract downloaded dataset.", "\n", "tar", "=", "tarfile", ".", "open", "(", "archive_fn", ",", "\"r:gz\"", ")", "\n", "tar", ".", "extractall", "(", "path", "=", "data_path", ")", "\n", "tar", ".", "close", "(", ")", "\n", "\n", "os", ".", "remove", "(", "archive_fn", ")", "\n", "\n", "", "train_batch_fn", "=", "os", ".", "path", ".", "join", "(", "extracted_data_dir", ",", "\n", "CIFAR100Data", ".", "_TRAIN_BATCH_FN", ")", "\n", "test_batch_fn", "=", "os", ".", "path", ".", "join", "(", "extracted_data_dir", ",", "\n", "CIFAR100Data", ".", "_TEST_BATCH_FN", ")", "\n", "meta_fn", "=", "os", ".", "path", ".", "join", "(", "extracted_data_dir", ",", "\n", "CIFAR100Data", ".", "_META_DATA_FN", ")", "\n", "\n", "assert", "(", "os", ".", "path", ".", "exists", "(", "train_batch_fn", ")", "and", "\n", "os", ".", "path", ".", "exists", "(", "test_batch_fn", ")", "and", "os", ".", "path", ".", "exists", "(", "meta_fn", ")", ")", "\n", "\n", "self", ".", "_data", "[", "'classification'", "]", "=", "True", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'num_classes'", "]", "=", "100", "\n", "self", ".", "_data", "[", "'is_one_hot'", "]", "=", "use_one_hot", "\n", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "32", ",", "32", ",", "3", "]", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "100", "if", "use_one_hot", "else", "1", "]", "\n", "\n", "# Fill the remaining _data fields with the information read from", "\n", "# the downloaded files.", "\n", "self", ".", "_read_meta", "(", "meta_fn", ")", "\n", "self", ".", "_read_batches", "(", "train_batch_fn", ",", "test_batch_fn", ",", "validation_size", ")", "\n", "\n", "# Initialize PyTorch data augmentation.", "\n", "self", ".", "_augment_inputs", "=", "False", "\n", "if", "use_data_augmentation", ":", "\n", "            ", "self", ".", "_augment_inputs", "=", "True", "\n", "self", ".", "_train_transform", ",", "self", ".", "_test_transform", "=", "CIFAR10Data", ".", "torch_input_transforms", "(", ")", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'Elapsed time to read dataset: %f sec'", "%", "(", "end", "-", "start", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar100_data.CIFAR100Data._read_meta": [[137, 159], ["dict", "open", "_pickle.load"], "methods", ["None"], ["", "def", "_read_meta", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Read the meta data file.\n\n        This method will add an additional field to the _data attribute named\n        \"cifar100\". This dictionary will be filled with two members:\n            * \"fine_label_names\": The names of the associated categorical class\n                labels.\n            * \"coarse_label_names\": The names of the 20 coarse labels that are\n                associated to each sample.\n\n        Args:\n            filename: The path to the meta data file.\n        \"\"\"", "\n", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "meta_data", "=", "pickle", ".", "load", "(", "f", ",", "encoding", "=", "'UTF-8'", ")", "\n", "\n", "", "self", ".", "_data", "[", "'cifar100'", "]", "=", "dict", "(", ")", "\n", "\n", "self", ".", "_data", "[", "'cifar100'", "]", "[", "'fine_label_names'", "]", "=", "meta_data", "[", "'fine_label_names'", "]", "\n", "self", ".", "_data", "[", "'cifar100'", "]", "[", "'coarse_label_names'", "]", "=", "meta_data", "[", "'coarse_label_names'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar100_data.CIFAR100Data._read_batches": [[160, 226], ["numpy.array", "numpy.array", "numpy.arange", "numpy.concatenate", "numpy.reshape", "numpy.concatenate", "numpy.reshape", "numpy.rollaxis", "numpy.reshape", "open", "_pickle.load", "open", "_pickle.load", "numpy.arange", "numpy.arange", "numpy.arange", "cifar100_data.CIFAR100Data._to_one_hot"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot"], ["", "def", "_read_batches", "(", "self", ",", "train_fn", ",", "test_fn", ",", "validation_size", ")", ":", "\n", "        ", "\"\"\"Read training and testing batch from files.\n\n        The method fills the remaining mandatory fields of the _data attribute,\n        that have not been set yet in the constructor.\n\n        The images are converted to match the output shape (32, 32, 3) and\n        scaled to have values between 0 and 1. For labels, the correct encoding\n        is enforced.\n\n        Args:\n            train_fn: Filepath of the train batch.\n            test_fn: Filepath of the test batch.\n            validation_size: Number of validation samples.\n        \"\"\"", "\n", "# Read test batch.", "\n", "with", "open", "(", "test_fn", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "test_batch", "=", "pickle", ".", "load", "(", "f", ",", "encoding", "=", "'bytes'", ")", "\n", "\n", "# Note, that we ignore the two keys: \"batch_label\", \"coarse_labels\" and", "\n", "# \"filenames\".", "\n", "", "test_labels", "=", "np", ".", "array", "(", "test_batch", "[", "'fine_labels'", ".", "encode", "(", ")", "]", ")", "\n", "test_samples", "=", "test_batch", "[", "'data'", ".", "encode", "(", ")", "]", "\n", "\n", "# Read test batch.", "\n", "with", "open", "(", "train_fn", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "train_batch", "=", "pickle", ".", "load", "(", "f", ",", "encoding", "=", "'bytes'", ")", "\n", "\n", "", "train_labels", "=", "np", ".", "array", "(", "train_batch", "[", "'fine_labels'", ".", "encode", "(", ")", "]", ")", "\n", "train_samples", "=", "train_batch", "[", "'data'", ".", "encode", "(", ")", "]", "\n", "\n", "if", "validation_size", ">", "0", ":", "\n", "            ", "assert", "(", "validation_size", "<", "train_labels", ".", "shape", "[", "0", "]", ")", "\n", "val_inds", "=", "np", ".", "arange", "(", "validation_size", ")", "\n", "train_inds", "=", "np", ".", "arange", "(", "validation_size", ",", "train_labels", ".", "size", ")", "\n", "\n", "", "else", ":", "\n", "            ", "train_inds", "=", "np", ".", "arange", "(", "train_labels", ".", "size", ")", "\n", "\n", "", "test_inds", "=", "np", ".", "arange", "(", "train_labels", ".", "size", ",", "\n", "train_labels", ".", "size", "+", "test_labels", ".", "size", ")", "\n", "\n", "labels", "=", "np", ".", "concatenate", "(", "[", "train_labels", ",", "test_labels", "]", ")", "\n", "labels", "=", "np", ".", "reshape", "(", "labels", ",", "(", "-", "1", ",", "1", ")", ")", "\n", "\n", "images", "=", "np", ".", "concatenate", "(", "[", "train_samples", ",", "test_samples", "]", ",", "axis", "=", "0", ")", "\n", "\n", "# Note, images are currently encoded in a way, that there shape", "\n", "# corresponds to (3, 32, 32). For consistency reasons, we would like to", "\n", "# change that to (32, 32, 3).", "\n", "images", "=", "np", ".", "reshape", "(", "images", ",", "(", "-", "1", ",", "3", ",", "32", ",", "32", ")", ")", "\n", "images", "=", "np", ".", "rollaxis", "(", "images", ",", "1", ",", "4", ")", "\n", "images", "=", "np", ".", "reshape", "(", "images", ",", "(", "-", "1", ",", "32", "*", "32", "*", "3", ")", ")", "\n", "# Scale images into a range between 0 and 1.", "\n", "images", "=", "images", "/", "255", "\n", "\n", "self", ".", "_data", "[", "'in_data'", "]", "=", "images", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "train_inds", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "test_inds", "\n", "if", "validation_size", ">", "0", ":", "\n", "                ", "self", ".", "_data", "[", "'val_inds'", "]", "=", "val_inds", "\n", "\n", "", "if", "self", ".", "_data", "[", "'is_one_hot'", "]", ":", "\n", "            ", "labels", "=", "self", ".", "_to_one_hot", "(", "labels", ")", "\n", "\n", "", "self", ".", "_data", "[", "'out_data'", "]", "=", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar100_data.CIFAR100Data.get_identifier": [[227, 230], ["None"], "methods", ["None"], ["", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'CIFAR-100'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar100_data.CIFAR100Data.input_to_torch_tensor": [[231, 263], ["data.cifar10_data.CIFAR10Data.torch_augment_images", "data.dataset.Dataset.input_to_torch_tensor", "ValueError"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data.torch_augment_images", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor"], ["", "def", "input_to_torch_tensor", "(", "self", ",", "x", ",", "device", ",", "mode", "=", "'inference'", ",", "\n", "force_no_preprocessing", "=", "False", ")", ":", "\n", "        ", "\"\"\"This method can be used to map the internal numpy arrays to PyTorch\n        tensors.\n\n        Note, this method has been overwritten from the base class.\n\n        The input images are preprocessed if data augmentation is enabled.\n        Preprocessing involves normalization and (for training mode) random\n        perturbations.\n\n        Args:\n            (....): See docstring of method\n                :meth:`data.dataset.Dataset.input_to_torch_tensor`.\n\n        Returns:\n            (torch.Tensor): The given input ``x`` as PyTorch tensor.\n        \"\"\"", "\n", "if", "self", ".", "_augment_inputs", "and", "not", "force_no_preprocessing", ":", "\n", "            ", "if", "mode", "==", "'inference'", ":", "\n", "                ", "transform", "=", "self", ".", "_test_transform", "\n", "", "elif", "mode", "==", "'train'", ":", "\n", "                ", "transform", "=", "self", ".", "_train_transform", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'\"%s\" not a valid value for argument \"mode\".'", "\n", "%", "mode", ")", "\n", "\n", "", "return", "CIFAR10Data", ".", "torch_augment_images", "(", "x", ",", "device", ",", "transform", ")", "\n", "\n", "", "else", ":", "\n", "            ", "return", "Dataset", ".", "input_to_torch_tensor", "(", "self", ",", "x", ",", "device", ",", "\n", "mode", "=", "mode", ",", "force_no_preprocessing", "=", "force_no_preprocessing", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar100_data.CIFAR100Data._plot_sample": [[264, 305], ["matplotlib.Subplot", "matplotlib.Subplot.set_axis_off", "matplotlib.Subplot.imshow", "fig.add_subplot", "matplotlib.Subplot.set_title", "numpy.asscalar", "numpy.squeeze", "matplotlib.Subplot", "matplotlib.Subplot.set_title", "matplotlib.Subplot.bar", "matplotlib.Subplot.set_xticks", "fig.add_subplot", "numpy.size", "matplotlib.Subplot.set_title", "matplotlib.Subplot.set_title", "numpy.reshape", "range", "numpy.squeeze", "range", "bars[].set_color", "numpy.size", "numpy.argmax", "numpy.asscalar", "int"], "methods", ["None"], ["", "", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract method\n        :meth:`data.dataset.Dataset._plot_sample`.\n        \"\"\"", "\n", "ax", "=", "plt", ".", "Subplot", "(", "fig", ",", "inner_grid", "[", "0", "]", ")", "\n", "\n", "if", "outputs", "is", "None", ":", "\n", "            ", "ax", ".", "set_title", "(", "\"CIFAR-100 Sample\"", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "np", ".", "size", "(", "outputs", ")", "==", "1", ")", "\n", "label", "=", "np", ".", "asscalar", "(", "outputs", ")", "\n", "label_name", "=", "self", ".", "_data", "[", "'cifar100'", "]", "[", "'fine_label_names'", "]", "[", "label", "]", "\n", "\n", "if", "predictions", "is", "None", ":", "\n", "                ", "ax", ".", "set_title", "(", "'Label of shown sample:\\n%s (%d)'", "%", "(", "label_name", ",", "label", ")", ")", "\n", "", "else", ":", "\n", "                ", "if", "np", ".", "size", "(", "predictions", ")", "==", "self", ".", "num_classes", ":", "\n", "                    ", "pred_label", "=", "np", ".", "argmax", "(", "predictions", ")", "\n", "", "else", ":", "\n", "                    ", "pred_label", "=", "np", ".", "asscalar", "(", "predictions", ")", "\n", "", "pred_label_name", "=", "self", ".", "_data", "[", "'cifar100'", "]", "[", "'fine_label_names'", "]", "[", "pred_label", "]", "\n", "\n", "ax", ".", "set_title", "(", "'Label of shown sample:\\n%s (%d)'", "%", "(", "label_name", ",", "label", ")", "+", "'\\nPrediction: %s (%d)'", "%", "(", "pred_label_name", ",", "pred_label", ")", ")", "\n", "\n", "", "", "ax", ".", "set_axis_off", "(", ")", "\n", "ax", ".", "imshow", "(", "np", ".", "squeeze", "(", "np", ".", "reshape", "(", "inputs", ",", "self", ".", "in_shape", ")", ")", ")", "\n", "fig", ".", "add_subplot", "(", "ax", ")", "\n", "\n", "if", "num_inner_plots", "==", "2", ":", "\n", "            ", "ax", "=", "plt", ".", "Subplot", "(", "fig", ",", "inner_grid", "[", "1", "]", ")", "\n", "ax", ".", "set_title", "(", "'Predictions'", ")", "\n", "bars", "=", "ax", ".", "bar", "(", "range", "(", "self", ".", "num_classes", ")", ",", "np", ".", "squeeze", "(", "predictions", ")", ")", "\n", "ax", ".", "set_xticks", "(", "range", "(", "self", ".", "num_classes", ")", ")", "\n", "if", "outputs", "is", "not", "None", ":", "\n", "                ", "bars", "[", "int", "(", "label", ")", "]", ".", "set_color", "(", "'r'", ")", "\n", "", "fig", ".", "add_subplot", "(", "ax", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar100_data.CIFAR100Data._plot_config": [[306, 325], ["super()._plot_config", "numpy.shape"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._plot_config"], ["", "", "def", "_plot_config", "(", "self", ",", "inputs", ",", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Re-Implementation of method\n        :meth:`data.dataset.Dataset._plot_config`.\n\n        This method has been overriden to ensure, that there are 2 subplots,\n        in case the predictions are given.\n        \"\"\"", "\n", "plot_configs", "=", "super", "(", ")", ".", "_plot_config", "(", "inputs", ",", "outputs", "=", "outputs", ",", "\n", "predictions", "=", "predictions", ")", "\n", "\n", "if", "predictions", "is", "not", "None", "and", "np", ".", "shape", "(", "predictions", ")", "[", "1", "]", "==", "self", ".", "num_classes", ":", "\n", "            ", "plot_configs", "[", "'outer_hspace'", "]", "=", "0.6", "\n", "plot_configs", "[", "'inner_hspace'", "]", "=", "0.4", "\n", "plot_configs", "[", "'num_inner_rows'", "]", "=", "2", "\n", "#plot_configs['num_inner_cols'] = 1", "\n", "plot_configs", "[", "'num_inner_plots'", "]", "=", "2", "\n", "\n", "", "return", "plot_configs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cub_200_2011_data.CUB2002011.__init__": [[130, 317], ["data.large_img_dataset.LargeImgDataset.__init__", "time.time", "print", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "data.ilsvrc2012_data.ILSVRC2012Data.torch_input_transforms", "torchvision.ImageFolder", "torchvision.ImageFolder", "pandas.read_csv", "dict", "dict.keys", "pandas.read_csv", "dict", "dict.keys", "pandas.read_csv", "dict", "pandas.read_csv", "dict", "enumerate", "copy.deepcopy", "len", "numpy.zeros", "len", "len", "len", "numpy.chararray", "enumerate", "numpy.array().reshape", "numpy.arange", "numpy.arange", "print", "time.time", "print", "os.path.exists", "print", "os.makedirs", "print", "os.path.join", "urllib.request.urlretrieve", "tarfile.open", "tarfile.open.extractall", "tarfile.open.close", "os.remove", "zip", "os.path.join", "zip", "zip", "zip", "copy.deepcopy", "dict.keys", "len", "cub_200_2011_data.CUB2002011._to_one_hot", "numpy.arange", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "os.path.exists", "list", "list", "dict.items", "list", "list", "list", "list", "dict.items", "list", "list", "copy.deepcopy.samples.append", "max", "numpy.array", "torchvision.ImageFolder.samples.append", "copy.deepcopy.samples.append", "len", "len", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.torch_input_transforms", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot"], ["def", "__init__", "(", "self", ",", "data_path", ",", "use_one_hot", "=", "False", ",", "num_val_per_class", "=", "0", ")", ":", "\n", "# We keep the full path to each image in memory, so we don't need to", "\n", "# tell the super class the root path to each image (i.e., samples", "\n", "# contain absolute not relative paths).", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "''", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "'Reading CUB-200-2011 dataset ...'", ")", "\n", "\n", "# Actual data path", "\n", "data_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CUB2002011", ".", "_SUBFOLDER", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_path", ")", ":", "\n", "            ", "print", "(", "'Creating directory \"%s\" ...'", "%", "(", "data_path", ")", ")", "\n", "os", ".", "makedirs", "(", "data_path", ")", "\n", "\n", "", "full_data_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CUB2002011", ".", "_REL_BASE", ")", "\n", "image_dir", "=", "os", ".", "path", ".", "join", "(", "full_data_path", ",", "CUB2002011", ".", "_IMG_DIR", ")", "\n", "classes_fn", "=", "os", ".", "path", ".", "join", "(", "full_data_path", ",", "CUB2002011", ".", "_CLASSES_FILE", ")", "\n", "img_class_fn", "=", "os", ".", "path", ".", "join", "(", "full_data_path", ",", "\n", "CUB2002011", ".", "_IMG_CLASS_LBLS_FILE", ")", "\n", "image_fn", "=", "os", ".", "path", ".", "join", "(", "full_data_path", ",", "CUB2002011", ".", "_IMG_FILE", ")", "\n", "train_test_split_fn", "=", "os", ".", "path", ".", "join", "(", "full_data_path", ",", "\n", "CUB2002011", ".", "_TRAIN_TEST_SPLIT_FILE", ")", "\n", "\n", "########################", "\n", "### Download dataset ###", "\n", "########################", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "image_dir", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "classes_fn", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "img_class_fn", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "image_fn", ")", "or", "not", "os", ".", "path", ".", "exists", "(", "train_test_split_fn", ")", ":", "\n", "            ", "print", "(", "'Downloading dataset ...'", ")", "\n", "archive_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CUB2002011", ".", "_IMG_ANNO_FILE", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "CUB2002011", ".", "_DOWNLOAD_PATH", "+", "CUB2002011", ".", "_IMG_ANNO_FILE", ",", "archive_fn", ")", "\n", "# Extract downloaded dataset.", "\n", "tar", "=", "tarfile", ".", "open", "(", "archive_fn", ",", "\"r:gz\"", ")", "\n", "tar", ".", "extractall", "(", "path", "=", "data_path", ")", "\n", "tar", ".", "close", "(", ")", "\n", "\n", "os", ".", "remove", "(", "archive_fn", ")", "\n", "\n", "####################", "\n", "### Read dataset ###", "\n", "####################", "\n", "# We use the same transforms as ", "\n", "", "train_transform", ",", "test_transform", "=", "ILSVRC2012Data", ".", "torch_input_transforms", "(", ")", "\n", "# Consider all images as training images. We split the dataset later.", "\n", "ds_train", "=", "datasets", ".", "ImageFolder", "(", "image_dir", ",", "train_transform", ")", "\n", "\n", "# Ability to translate image IDs into image paths and back.", "\n", "image_ids_csv", "=", "pandas", ".", "read_csv", "(", "image_fn", ",", "sep", "=", "' '", ",", "\n", "names", "=", "[", "'img_id'", ",", "'img_path'", "]", ")", "\n", "id2img", "=", "dict", "(", "zip", "(", "list", "(", "image_ids_csv", "[", "'img_id'", "]", ")", ",", "\n", "list", "(", "image_ids_csv", "[", "'img_path'", "]", ")", ")", ")", "\n", "# Since the ImageFolder class uses absolute paths, we have to change", "\n", "# the just read relative paths.", "\n", "for", "iid", "in", "id2img", ".", "keys", "(", ")", ":", "\n", "            ", "id2img", "[", "iid", "]", "=", "os", ".", "path", ".", "join", "(", "image_dir", ",", "id2img", "[", "iid", "]", ")", "\n", "", "img2id", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "id2img", ".", "items", "(", ")", "}", "\n", "\n", "# Image ID to label.", "\n", "img_lbl_csv", "=", "pandas", ".", "read_csv", "(", "img_class_fn", ",", "sep", "=", "' '", ",", "\n", "names", "=", "[", "'img_id'", ",", "'label'", "]", ")", "\n", "id2lbl", "=", "dict", "(", "zip", "(", "list", "(", "img_lbl_csv", "[", "'img_id'", "]", ")", ",", "\n", "list", "(", "img_lbl_csv", "[", "'label'", "]", ")", ")", ")", "\n", "# Note, categories go from 1-200. We change them to go from 0 - 199.", "\n", "for", "iid", "in", "id2lbl", ".", "keys", "(", ")", ":", "\n", "            ", "id2lbl", "[", "iid", "]", "=", "id2lbl", "[", "iid", "]", "-", "1", "\n", "\n", "# Image ID to label name.", "\n", "", "img_lbl_name_csv", "=", "pandas", ".", "read_csv", "(", "classes_fn", ",", "sep", "=", "' '", ",", "\n", "names", "=", "[", "'label'", ",", "'label_name'", "]", ")", "\n", "lbl2lbl_name_tmp", "=", "dict", "(", "zip", "(", "list", "(", "img_lbl_name_csv", "[", "'label'", "]", ")", ",", "\n", "list", "(", "img_lbl_name_csv", "[", "'label_name'", "]", ")", ")", ")", "\n", "# Here, we also have to modify the labels to be within 0-199.", "\n", "lbl2lbl_name", "=", "{", "k", "-", "1", ":", "v", "for", "k", ",", "v", "in", "lbl2lbl_name_tmp", ".", "items", "(", ")", "}", "\n", "\n", "# Train-test-split.", "\n", "train_test_csv", "=", "pandas", ".", "read_csv", "(", "train_test_split_fn", ",", "sep", "=", "' '", ",", "\n", "names", "=", "[", "'img_id'", ",", "'is_train'", "]", ")", "\n", "id2train", "=", "dict", "(", "zip", "(", "list", "(", "train_test_csv", "[", "'img_id'", "]", ")", ",", "\n", "list", "(", "train_test_csv", "[", "'is_train'", "]", ")", ")", ")", "\n", "\n", "self", ".", "_label_to_name", "=", "lbl2lbl_name", "\n", "\n", "####################", "\n", "### Sanity check ###", "\n", "####################", "\n", "for", "i", ",", "(", "img_path", ",", "lbl", ")", "in", "enumerate", "(", "ds_train", ".", "samples", ")", ":", "\n", "            ", "iid", "=", "img2id", "[", "img_path", "]", "\n", "assert", "(", "id2img", "[", "iid", "]", "==", "img_path", ")", "\n", "assert", "(", "lbl", "==", "id2lbl", "[", "iid", "]", ")", "\n", "\n", "################################", "\n", "### Train / val / test split ###", "\n", "################################", "\n", "", "orig_samples", "=", "ds_train", ".", "samples", "\n", "ds_train", ".", "samples", "=", "[", "]", "\n", "ds_train", ".", "imgs", "=", "ds_train", ".", "samples", "\n", "ds_train", ".", "targets", "=", "[", "]", "\n", "\n", "ds_test", "=", "deepcopy", "(", "ds_train", ")", "\n", "ds_test", ".", "transform", "=", "test_transform", "\n", "assert", "(", "ds_test", ".", "target_transform", "is", "None", ")", "\n", "if", "num_val_per_class", ">", "0", ":", "\n", "            ", "ds_val", "=", "deepcopy", "(", "ds_train", ")", "\n", "# NOTE we use test input transforms for the validation set.", "\n", "ds_val", ".", "transform", "=", "test_transform", "\n", "", "else", ":", "\n", "            ", "ds_val", "=", "None", "\n", "\n", "", "num_classes", "=", "len", "(", "lbl2lbl_name_tmp", ".", "keys", "(", ")", ")", "\n", "assert", "(", "num_classes", "==", "200", ")", "\n", "val_counts", "=", "np", ".", "zeros", "(", "num_classes", ",", "dtype", "=", "np", ".", "int", ")", "\n", "\n", "for", "img_path", ",", "img_lbl", "in", "orig_samples", ":", "\n", "            ", "iid", "=", "img2id", "[", "img_path", "]", "\n", "if", "id2train", "[", "iid", "]", "==", "1", ":", "# In train split.", "\n", "                ", "if", "val_counts", "[", "img_lbl", "]", ">=", "num_val_per_class", ":", "# train sample", "\n", "                    ", "ds_train", ".", "samples", ".", "append", "(", "(", "img_path", ",", "img_lbl", ")", ")", "\n", "", "else", ":", "# validation sample", "\n", "                    ", "val_counts", "[", "img_lbl", "]", "+=", "1", "\n", "ds_val", ".", "samples", ".", "append", "(", "(", "img_path", ",", "img_lbl", ")", ")", "\n", "", "", "else", ":", "# In test split.", "\n", "                ", "ds_test", ".", "samples", ".", "append", "(", "(", "img_path", ",", "img_lbl", ")", ")", "\n", "\n", "", "", "for", "ds_obj", "in", "[", "ds_train", ",", "ds_test", "]", "+", "(", "[", "ds_val", "]", "if", "num_val_per_class", ">", "0", "else", "[", "]", ")", ":", "\n", "            ", "ds_obj", ".", "targets", "=", "[", "s", "[", "1", "]", "for", "s", "in", "ds_obj", ".", "samples", "]", "\n", "assert", "(", "len", "(", "ds_obj", ".", "samples", ")", "==", "len", "(", "ds_obj", ".", "imgs", ")", "and", "len", "(", "ds_obj", ".", "samples", ")", "==", "len", "(", "ds_obj", ".", "targets", ")", ")", "\n", "\n", "", "self", ".", "_torch_ds_train", "=", "ds_train", "\n", "self", ".", "_torch_ds_test", "=", "ds_test", "\n", "self", ".", "_torch_ds_val", "=", "ds_val", "\n", "\n", "#####################################", "\n", "### Build internal data structure ###", "\n", "#####################################", "\n", "num_train", "=", "len", "(", "self", ".", "_torch_ds_train", ")", "\n", "num_test", "=", "len", "(", "self", ".", "_torch_ds_test", ")", "\n", "num_val", "=", "0", "if", "self", ".", "_torch_ds_val", "is", "None", "else", "len", "(", "self", ".", "_torch_ds_val", ")", "\n", "num_samples", "=", "num_train", "+", "num_test", "+", "num_val", "\n", "\n", "max_path_len", "=", "len", "(", "max", "(", "orig_samples", ",", "key", "=", "lambda", "t", ":", "len", "(", "t", "[", "0", "]", ")", ")", "[", "0", "]", ")", "\n", "\n", "self", ".", "_data", "[", "'classification'", "]", "=", "True", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'num_classes'", "]", "=", "200", "\n", "self", ".", "_data", "[", "'is_one_hot'", "]", "=", "use_one_hot", "\n", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "224", ",", "224", ",", "3", "]", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "200", "if", "use_one_hot", "else", "1", "]", "\n", "\n", "self", ".", "_data", "[", "'in_data'", "]", "=", "np", ".", "chararray", "(", "[", "num_samples", ",", "1", "]", ",", "\n", "itemsize", "=", "max_path_len", ",", "unicode", "=", "True", ")", "\n", "for", "i", ",", "(", "img_path", ",", "_", ")", "in", "enumerate", "(", "ds_train", ".", "samples", "+", "\n", "(", "[", "]", "if", "num_val", "==", "0", "else", "ds_val", ".", "samples", ")", "+", "\n", "ds_test", ".", "samples", ")", ":", "\n", "            ", "self", ".", "_data", "[", "'in_data'", "]", "[", "i", ",", ":", "]", "=", "img_path", "\n", "\n", "", "labels", "=", "np", ".", "array", "(", "ds_train", ".", "targets", "+", "\n", "(", "[", "]", "if", "num_val", "==", "0", "else", "ds_val", ".", "targets", ")", "+", "\n", "ds_test", ".", "targets", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "if", "use_one_hot", ":", "\n", "            ", "labels", "=", "self", ".", "_to_one_hot", "(", "labels", ")", "\n", "", "self", ".", "_data", "[", "'out_data'", "]", "=", "labels", "\n", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ")", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "np", ".", "arange", "(", "num_train", "+", "num_val", ",", "num_samples", ")", "\n", "if", "num_val", "==", "0", ":", "\n", "            ", "self", ".", "_data", "[", "'val_inds'", "]", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "_data", "[", "'val_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ",", "num_train", "+", "num_val", ")", "\n", "\n", "", "print", "(", "'Dataset consists of %d training, %d validation and %d test '", "\n", "%", "(", "num_train", ",", "num_val", ",", "num_test", ")", "+", "'samples.'", ")", "\n", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'Elapsed time to read dataset: %f sec'", "%", "(", "end", "-", "start", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cub_200_2011_data.CUB2002011.get_identifier": [[318, 321], ["None"], "methods", ["None"], ["", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'CUB-200-2011'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cub_200_2011_data.CUB2002011.tf_input_map": [[322, 327], ["NotImplementedError"], "methods", ["None"], ["", "def", "tf_input_map", "(", "self", ",", "mode", "=", "'inference'", ")", ":", "\n", "        ", "\"\"\"Not impemented.\"\"\"", "\n", "# Confirm, whether you wanna process data as in the baseclass or", "\n", "# implement a new image loader.", "\n", "raise", "NotImplementedError", "(", "'Not implemented yet!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cub_200_2011_data.CUB2002011._plot_sample": [[328, 364], ["matplotlib.Subplot", "matplotlib.Subplot.set_axis_off", "matplotlib.Subplot.imshow", "fig.add_subplot", "matplotlib.Subplot.set_title", "numpy.asscalar", "cub_200_2011_data.CUB2002011.read_images", "numpy.squeeze", "numpy.size", "matplotlib.Subplot.set_title", "matplotlib.Subplot.set_title", "numpy.reshape", "numpy.size", "numpy.argmax", "numpy.asscalar"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.read_images"], ["", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract method\n        :meth:`data.dataset.Dataset._plot_sample`.\n        \"\"\"", "\n", "ax", "=", "plt", ".", "Subplot", "(", "fig", ",", "inner_grid", "[", "0", "]", ")", "\n", "\n", "if", "outputs", "is", "None", ":", "\n", "            ", "ax", ".", "set_title", "(", "\"CUB-200-2011 Sample\"", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "np", ".", "size", "(", "outputs", ")", "==", "1", ")", "\n", "label", "=", "np", ".", "asscalar", "(", "outputs", ")", "\n", "label_name", "=", "self", ".", "_label_to_name", "[", "label", "]", "\n", "\n", "if", "predictions", "is", "None", ":", "\n", "                ", "ax", ".", "set_title", "(", "'Label of shown sample:\\n%s (%d)'", "%", "(", "label_name", ",", "label", ")", ")", "\n", "", "else", ":", "\n", "                ", "if", "np", ".", "size", "(", "predictions", ")", "==", "self", ".", "num_classes", ":", "\n", "                    ", "pred_label", "=", "np", ".", "argmax", "(", "predictions", ")", "\n", "", "else", ":", "\n", "                    ", "pred_label", "=", "np", ".", "asscalar", "(", "predictions", ")", "\n", "", "pred_label_name", "=", "self", ".", "_label_to_name", "[", "pred_label", "]", "\n", "\n", "ax", ".", "set_title", "(", "'Label of shown sample:\\n%s (%d)'", "%", "(", "label_name", ",", "label", ")", "+", "'\\nPrediction: %s (%d)'", "%", "(", "pred_label_name", ",", "pred_label", ")", ")", "\n", "\n", "", "", "if", "inputs", ".", "size", "==", "1", ":", "\n", "            ", "img", "=", "self", ".", "read_images", "(", "inputs", ")", "\n", "", "else", ":", "\n", "            ", "img", "=", "inputs", "\n", "\n", "", "ax", ".", "set_axis_off", "(", ")", "\n", "ax", ".", "imshow", "(", "np", ".", "squeeze", "(", "np", ".", "reshape", "(", "img", ",", "self", ".", "in_shape", ")", ")", ")", "\n", "fig", ".", "add_subplot", "(", "ax", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData.__init__": [[77, 212], ["data.dataset.Dataset.__init__", "time.time", "print", "os.path.join", "os.path.join", "os.path.isfile", "time.time", "print", "os.path.exists", "print", "os.makedirs", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "mnist_data.MNISTData._read_labels", "mnist_data.MNISTData._read_labels", "mnist_data.MNISTData._read_images", "mnist_data.MNISTData._read_images", "numpy.arange", "numpy.arange", "numpy.concatenate", "numpy.concatenate", "numpy.reshape", "numpy.arange", "numpy.arange", "open", "_pickle.load", "os.path.exists", "print", "urllib.request.urlretrieve", "os.path.exists", "print", "urllib.request.urlretrieve", "os.path.exists", "print", "urllib.request.urlretrieve", "os.path.exists", "print", "urllib.request.urlretrieve", "mnist_data.MNISTData._to_one_hot", "open", "_pickle.dump", "mnist_data.MNISTData._to_one_hot"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData._read_labels", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData._read_labels", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData._read_images", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData._read_images", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot"], ["def", "__init__", "(", "self", ",", "data_path", ",", "use_one_hot", "=", "False", ",", "validation_size", "=", "5000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "'Reading MNIST dataset ...'", ")", "\n", "\n", "# Actual data path", "\n", "data_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "MNISTData", ".", "_SUBFOLDER", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_path", ")", ":", "\n", "            ", "print", "(", "'Creating directory \"%s\" ...'", "%", "(", "data_path", ")", ")", "\n", "os", ".", "makedirs", "(", "data_path", ")", "\n", "\n", "# If data has been processed before.", "\n", "", "build_from_scratch", "=", "True", "\n", "dump_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "MNISTData", ".", "_MNIST_DATA_DUMP", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "dump_fn", ")", ":", "\n", "            ", "build_from_scratch", "=", "False", "\n", "\n", "with", "open", "(", "dump_fn", ",", "'rb'", ")", "as", "f", ":", "\n", "                ", "self", ".", "_data", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "if", "self", ".", "_data", "[", "'is_one_hot'", "]", "!=", "use_one_hot", ":", "\n", "                    ", "reverse", "=", "True", "\n", "if", "use_one_hot", ":", "\n", "                        ", "reverse", "=", "False", "\n", "\n", "", "self", ".", "_data", "[", "'is_one_hot'", "]", "=", "use_one_hot", "\n", "self", ".", "_data", "[", "'out_data'", "]", "=", "self", ".", "_to_one_hot", "(", "\n", "self", ".", "_data", "[", "'out_data'", "]", ",", "reverse", "=", "reverse", ")", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "self", ".", "_data", "[", "'out_data'", "]", ".", "shape", "[", "1", "]", "]", "\n", "\n", "# DELETEME A previous version of the dataloader stored the", "\n", "# validation set in the pickle file. Hence, this line ensures", "\n", "# downwards compatibility.", "\n", "", "if", "self", ".", "num_val_samples", "!=", "0", ":", "\n", "                    ", "build_from_scratch", "=", "True", "\n", "self", ".", "_data", "[", "'val_inds'", "]", "=", "None", "\n", "\n", "\n", "", "", "", "if", "build_from_scratch", ":", "\n", "            ", "train_images_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "MNISTData", ".", "_TRAIN_IMGS_FN", ")", "\n", "train_labels_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "MNISTData", ".", "_TRAIN_LBLS_FN", ")", "\n", "test_images_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "MNISTData", ".", "_TEST_IMGS_FN", ")", "\n", "test_labels_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "MNISTData", ".", "_TEST_LBLS_FN", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "train_images_fn", ")", ":", "\n", "                ", "print", "(", "'Downloading training images ...'", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MNISTData", ".", "_DOWNLOAD_PATH", "+", "MNISTData", ".", "_TRAIN_IMGS_FN", ",", "train_images_fn", ")", "\n", "\n", "## Extract downloaded images.", "\n", "#with gzip.open(train_images_fn, 'rb') as f_in:", "\n", "#     with open(os.path.splitext(train_images_fn)[0], \\", "\n", "#               'wb') as f_out:", "\n", "#         shutil.copyfileobj(f_in, f_out)", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "train_labels_fn", ")", ":", "\n", "                ", "print", "(", "'Downloading training labels ...'", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MNISTData", ".", "_DOWNLOAD_PATH", "+", "MNISTData", ".", "_TRAIN_LBLS_FN", ",", "train_labels_fn", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "test_images_fn", ")", ":", "\n", "                ", "print", "(", "'Downloading test images ...'", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MNISTData", ".", "_DOWNLOAD_PATH", "+", "MNISTData", ".", "_TEST_IMGS_FN", ",", "test_images_fn", ")", "\n", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "test_labels_fn", ")", ":", "\n", "                ", "print", "(", "'Downloading test labels ...'", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MNISTData", ".", "_DOWNLOAD_PATH", "+", "MNISTData", ".", "_TEST_LBLS_FN", ",", "test_labels_fn", ")", "\n", "\n", "# read labels", "\n", "", "train_labels", "=", "MNISTData", ".", "_read_labels", "(", "train_labels_fn", ")", "\n", "test_labels", "=", "MNISTData", ".", "_read_labels", "(", "test_labels_fn", ")", "\n", "\n", "# read images", "\n", "train_inputs", "=", "MNISTData", ".", "_read_images", "(", "train_images_fn", ")", "\n", "test_inputs", "=", "MNISTData", ".", "_read_images", "(", "test_images_fn", ")", "\n", "\n", "assert", "(", "train_labels", ".", "shape", "[", "0", "]", "==", "train_inputs", ".", "shape", "[", "0", "]", ")", "\n", "assert", "(", "test_labels", ".", "shape", "[", "0", "]", "==", "test_inputs", ".", "shape", "[", "0", "]", ")", "\n", "\n", "# Note, we ignore a possible validation set here on purpose, as it", "\n", "# should not be part of the pickle (see below).", "\n", "train_inds", "=", "np", ".", "arange", "(", "train_labels", ".", "size", ")", "\n", "test_inds", "=", "np", ".", "arange", "(", "train_labels", ".", "size", ",", "\n", "train_labels", ".", "size", "+", "test_labels", ".", "size", ")", "\n", "\n", "labels", "=", "np", ".", "concatenate", "(", "[", "train_labels", ",", "test_labels", "]", ")", "\n", "images", "=", "np", ".", "concatenate", "(", "[", "train_inputs", ",", "test_inputs", "]", ",", "axis", "=", "0", ")", "\n", "\n", "labels", "=", "np", ".", "reshape", "(", "labels", ",", "(", "-", "1", ",", "1", ")", ")", "\n", "# Scale images into a range between 0 and 1.", "\n", "images", "=", "images", "/", "255", "\n", "\n", "# Bring these raw readings into the internal structure of the", "\n", "# Dataset class", "\n", "self", ".", "_data", "[", "'classification'", "]", "=", "True", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'num_classes'", "]", "=", "10", "\n", "self", ".", "_data", "[", "'is_one_hot'", "]", "=", "use_one_hot", "\n", "self", ".", "_data", "[", "'in_data'", "]", "=", "images", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "28", ",", "28", ",", "1", "]", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "10", "if", "use_one_hot", "else", "1", "]", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "train_inds", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "test_inds", "\n", "\n", "if", "use_one_hot", ":", "\n", "                ", "labels", "=", "self", ".", "_to_one_hot", "(", "labels", ")", "\n", "\n", "", "self", ".", "_data", "[", "'out_data'", "]", "=", "labels", "\n", "\n", "# Save read dataset to allow faster reading in future.", "\n", "with", "open", "(", "dump_fn", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "self", ".", "_data", ",", "f", ")", "\n", "\n", "# After writing the pickle, correct train and validation set indices.", "\n", "", "", "if", "validation_size", ">", "0", ":", "\n", "            ", "train_inds_orig", "=", "self", ".", "_data", "[", "'train_inds'", "]", "\n", "assert", "(", "validation_size", "<", "train_inds_orig", ".", "size", ")", "\n", "\n", "val_inds", "=", "np", ".", "arange", "(", "validation_size", ")", "\n", "train_inds", "=", "np", ".", "arange", "(", "validation_size", ",", "train_inds_orig", ".", "size", ")", "\n", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "train_inds", "\n", "self", ".", "_data", "[", "'val_inds'", "]", "=", "val_inds", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'Elapsed time to read dataset: %f sec'", "%", "(", "end", "-", "start", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData._read_labels": [[213, 237], ["os.path.isfile", "print", "gzip.open", "f.read", "int.from_bytes", "print", "numpy.array", "f.read", "struct.unpack", "f.read"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_read_labels", "(", "filename", ")", ":", "\n", "        ", "\"\"\"Reading a set of labels from a file.\n\n        Args:\n            filename: Path and name of the byte file that contains the labels.\n\n        Returns:\n            The labels as a 1D numpy array.\n        \"\"\"", "\n", "assert", "(", "os", ".", "path", ".", "isfile", "(", "filename", ")", ")", "\n", "\n", "print", "(", "'Reading labels from %s.'", "%", "filename", ")", "\n", "with", "gzip", ".", "open", "(", "filename", ",", "\"rb\"", ")", "as", "f", ":", "\n", "# Skip magic number.", "\n", "            ", "f", ".", "read", "(", "4", ")", "\n", "# Get number of labels in this file.", "\n", "num", "=", "int", ".", "from_bytes", "(", "f", ".", "read", "(", "4", ")", ",", "byteorder", "=", "'big'", ")", "\n", "print", "(", "'Number of labels in current file: %d'", "%", "num", ")", "\n", "\n", "# The rest of the file are \"num\" bytes, each byte encoding a label.", "\n", "labels", "=", "np", ".", "array", "(", "struct", ".", "unpack", "(", "'%dB'", "%", "num", ",", "f", ".", "read", "(", "num", ")", ")", ")", "\n", "\n", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData._read_images": [[238, 270], ["os.path.isfile", "print", "gzip.open", "f.read", "int.from_bytes", "print", "int.from_bytes", "int.from_bytes", "numpy.array", "numpy.reshape", "f.read", "f.read", "f.read", "struct.unpack", "f.read"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_read_images", "(", "filename", ")", ":", "\n", "        ", "\"\"\"Reading a set of images from a file.\n\n        Args:\n            filename: Path and name of the byte file that contains the images.\n\n        Returns:\n            The images stacked in a 2D array, where each row is one image.\n        \"\"\"", "\n", "assert", "(", "os", ".", "path", ".", "isfile", "(", "filename", ")", ")", "\n", "\n", "print", "(", "'Reading images from %s.'", "%", "filename", ")", "\n", "with", "gzip", ".", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "# Skip magic number", "\n", "            ", "f", ".", "read", "(", "4", ")", "\n", "# Get number of images in this file.", "\n", "num", "=", "int", ".", "from_bytes", "(", "f", ".", "read", "(", "4", ")", ",", "byteorder", "=", "'big'", ")", "\n", "print", "(", "'Number of images in current file: %d'", "%", "num", ")", "\n", "# Get number of rows and columns.", "\n", "rows", "=", "int", ".", "from_bytes", "(", "f", ".", "read", "(", "4", ")", ",", "byteorder", "=", "'big'", ")", "\n", "cols", "=", "int", ".", "from_bytes", "(", "f", ".", "read", "(", "4", ")", ",", "byteorder", "=", "'big'", ")", "\n", "\n", "# The rest of the file consists of pure image data, each pixel", "\n", "# value encoded as a byte.", "\n", "num_rem_bytes", "=", "num", "*", "rows", "*", "cols", "\n", "images", "=", "np", ".", "array", "(", "struct", ".", "unpack", "(", "'%dB'", "%", "num_rem_bytes", ",", "\n", "f", ".", "read", "(", "num_rem_bytes", ")", ")", ")", "\n", "\n", "images", "=", "np", ".", "reshape", "(", "images", ",", "(", "-", "1", ",", "rows", "*", "cols", ")", ")", "\n", "\n", "return", "images", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData.plot_sample": [[271, 306], ["warnings.warn", "matplotlib.axis", "matplotlib.imshow", "matplotlib.title", "matplotlib.title", "matplotlib.ion", "numpy.reshape", "matplotlib.savefig", "matplotlib.show"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "plot_sample", "(", "image", ",", "label", "=", "None", ",", "interactive", "=", "False", ",", "file_name", "=", "None", ")", ":", "\n", "        ", "\"\"\"Plot a single MNIST sample.\n\n        This method is thought to be helpful for evaluation and debugging\n        purposes.\n\n        .. deprecated:: 1.0\n            Please use method :meth:`data.dataset.Dataset.plot_samples` instead.\n\n        Args:\n            image: A single MNIST image (given as 1D vector).\n            label: The label of the given image.\n            interactive: Turn on interactive mode. Thus program will run in\n                background while figure is displayed. The figure will be\n                displayed until another one is displayed, the user closes it or\n                the program has terminated. If this option is deactivated, the\n                program will freeze until the user closes the figure.\n            file_name: (optional) If a file name is provided, then the image\n                will be written into a file instead of plotted to the screen.\n        \"\"\"", "\n", "warn", "(", "'Please use method \"plot_samples\" instead.'", ",", "DeprecationWarning", ")", "\n", "\n", "if", "label", "is", "None", ":", "\n", "            ", "plt", ".", "title", "(", "\"MNIST Sample\"", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "title", "(", "'Label of shown sample: %d'", "%", "label", ")", "\n", "", "plt", ".", "axis", "(", "'off'", ")", "\n", "if", "interactive", ":", "\n", "            ", "plt", ".", "ion", "(", ")", "\n", "", "plt", ".", "imshow", "(", "np", ".", "reshape", "(", "image", ",", "(", "28", ",", "28", ")", ")", ")", "\n", "if", "file_name", "is", "not", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "file_name", ",", "bbox_inches", "=", "'tight'", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData.get_identifier": [[307, 310], ["None"], "methods", ["None"], ["", "", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'MNIST'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData._plot_sample": [[311, 349], ["matplotlib.Subplot", "matplotlib.Subplot.set_axis_off", "matplotlib.Subplot.imshow", "fig.add_subplot", "matplotlib.Subplot.set_title", "numpy.asscalar", "numpy.squeeze", "matplotlib.Subplot", "matplotlib.Subplot.set_title", "matplotlib.Subplot.bar", "matplotlib.Subplot.set_xticks", "fig.add_subplot", "numpy.size", "matplotlib.Subplot.set_title", "matplotlib.Subplot.set_title", "numpy.reshape", "range", "numpy.squeeze", "range", "bars[].set_color", "numpy.size", "numpy.argmax", "numpy.asscalar", "int"], "methods", ["None"], ["", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract method\n        :meth:`data.dataset.Dataset._plot_sample`.\n        \"\"\"", "\n", "ax", "=", "plt", ".", "Subplot", "(", "fig", ",", "inner_grid", "[", "0", "]", ")", "\n", "\n", "if", "outputs", "is", "None", ":", "\n", "            ", "ax", ".", "set_title", "(", "\"MNIST Sample\"", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "np", ".", "size", "(", "outputs", ")", "==", "1", ")", "\n", "label", "=", "np", ".", "asscalar", "(", "outputs", ")", "\n", "\n", "if", "predictions", "is", "None", ":", "\n", "                ", "ax", ".", "set_title", "(", "'MNIST sample with\\nlabel: %d'", "%", "label", ")", "\n", "", "else", ":", "\n", "                ", "if", "np", ".", "size", "(", "predictions", ")", "==", "self", ".", "num_classes", ":", "\n", "                    ", "pred_label", "=", "np", ".", "argmax", "(", "predictions", ")", "\n", "", "else", ":", "\n", "                    ", "pred_label", "=", "np", ".", "asscalar", "(", "predictions", ")", "\n", "\n", "", "ax", ".", "set_title", "(", "'MNIST sample with\\nlabel: %d (prediction: %d)'", "%", "\n", "(", "label", ",", "pred_label", ")", ")", "\n", "\n", "#plt.subplots_adjust(wspace=0.5, hspace=0.4)", "\n", "\n", "", "", "ax", ".", "set_axis_off", "(", ")", "\n", "ax", ".", "imshow", "(", "np", ".", "squeeze", "(", "np", ".", "reshape", "(", "inputs", ",", "self", ".", "in_shape", ")", ")", ")", "\n", "fig", ".", "add_subplot", "(", "ax", ")", "\n", "\n", "if", "num_inner_plots", "==", "2", ":", "\n", "            ", "ax", "=", "plt", ".", "Subplot", "(", "fig", ",", "inner_grid", "[", "1", "]", ")", "\n", "ax", ".", "set_title", "(", "'Predictions'", ")", "\n", "bars", "=", "ax", ".", "bar", "(", "range", "(", "self", ".", "num_classes", ")", ",", "np", ".", "squeeze", "(", "predictions", ")", ")", "\n", "ax", ".", "set_xticks", "(", "range", "(", "self", ".", "num_classes", ")", ")", "\n", "if", "outputs", "is", "not", "None", ":", "\n", "                ", "bars", "[", "int", "(", "label", ")", "]", ".", "set_color", "(", "'r'", ")", "\n", "", "fig", ".", "add_subplot", "(", "ax", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.mnist_data.MNISTData._plot_config": [[350, 369], ["super()._plot_config", "numpy.shape"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._plot_config"], ["", "", "def", "_plot_config", "(", "self", ",", "inputs", ",", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Re-Implementation of method\n        :meth:`data.dataset.Dataset._plot_config`.\n\n        This method has been overriden to ensure, that there are 2 subplots,\n        in case the predictions are given.\n        \"\"\"", "\n", "plot_configs", "=", "super", "(", ")", ".", "_plot_config", "(", "inputs", ",", "outputs", "=", "outputs", ",", "\n", "predictions", "=", "predictions", ")", "\n", "\n", "if", "predictions", "is", "not", "None", "and", "np", ".", "shape", "(", "predictions", ")", "[", "1", "]", "==", "self", ".", "num_classes", ":", "\n", "            ", "plot_configs", "[", "'outer_hspace'", "]", "=", "0.6", "\n", "plot_configs", "[", "'inner_hspace'", "]", "=", "0.4", "\n", "plot_configs", "[", "'num_inner_rows'", "]", "=", "2", "\n", "#plot_configs['num_inner_cols'] = 1", "\n", "plot_configs", "[", "'num_inner_plots'", "]", "=", "2", "\n", "\n", "", "return", "plot_configs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.__init__": [[109, 159], ["data.setdefault", "data.setdefault", "data.setdefault", "data.setdefault", "data.setdefault", "data.setdefault", "data.setdefault", "data.setdefault", "data.setdefault", "data.setdefault", "data.setdefault"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "# Internally, everything is stored in a certain structure, such that it", "\n", "# can easily be backuped (for instance via pickle).", "\n", "        ", "data", "=", "{", "}", "\n", "\n", "# Boolean: See attribute \"classification\".", "\n", "data", ".", "setdefault", "(", "'classification'", ",", "None", ")", "\n", "# Boolean: See attribute \"sequence\".", "\n", "data", ".", "setdefault", "(", "'sequence'", ",", "None", ")", "\n", "\n", "# Integer: See attribute \"num_classes\".", "\n", "data", ".", "setdefault", "(", "'num_classes'", ",", "None", ")", "\n", "# Integer: See attribute \"is_one_hot\".", "\n", "data", ".", "setdefault", "(", "'is_one_hot'", ",", "None", ")", "\n", "\n", "# A 2D numpy array, containing a sample input in each row (all samples", "\n", "# are encoded as single vectors.)", "\n", "data", ".", "setdefault", "(", "'in_data'", ",", "None", ")", "\n", "# A 2D numpy array, containing a sample output in each row (all samples", "\n", "# are encoded as single vectors.)", "\n", "data", ".", "setdefault", "(", "'out_data'", ",", "None", ")", "\n", "\n", "# List or numpy array: See attribute \"in_shape\".", "\n", "data", ".", "setdefault", "(", "'in_shape'", ",", "[", "]", ")", "\n", "# List or numpy array: See attribute \"in_shape\".", "\n", "data", ".", "setdefault", "(", "'out_shape'", ",", "[", "]", ")", "\n", "\n", "# List or numpy array: All row indices of \"in_data\" or \"out_data\", that", "\n", "# correspond to samples belonging to the training set.", "\n", "data", ".", "setdefault", "(", "'train_inds'", ",", "[", "]", ")", "\n", "# List or numpy array: All row indices of \"in_data\" or \"out_data\", that", "\n", "# correspond to samples belonging to the test set.", "\n", "data", ".", "setdefault", "(", "'test_inds'", ",", "[", "]", ")", "\n", "# List or numpy array: All row indices of \"in_data\" or \"out_data\", that", "\n", "# correspond to samples belonging to the validation set.", "\n", "data", ".", "setdefault", "(", "'val_inds'", ",", "None", ")", "\n", "\n", "self", ".", "_data", "=", "data", "\n", "\n", "# These are other private attributes, that are not in the data dict", "\n", "# as there would be no reason to pickle them.", "\n", "self", ".", "_batch_gen_train", "=", "None", "\n", "self", ".", "_batch_gen_test", "=", "None", "\n", "self", ".", "_batch_gen_val", "=", "None", "\n", "\n", "# We only need to fit the one-hot encoder for this dataset once.", "\n", "self", ".", "_one_hot_encoder", "=", "None", "\n", "\n", "self", ".", "_shuffle_test_samples", "=", "True", "\n", "self", ".", "_shuffle_val_samples", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.classification": [[162, 166], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "classification", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`classification`.\"\"\"", "\n", "return", "self", ".", "_data", "[", "'classification'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.sequence": [[167, 171], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "sequence", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`sequence`.\"\"\"", "\n", "return", "self", ".", "_data", "[", "'sequence'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.num_classes": [[172, 176], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_classes", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`num_classes`.\"\"\"", "\n", "return", "self", ".", "_data", "[", "'num_classes'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.is_one_hot": [[177, 181], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "is_one_hot", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`is_one_hot`.\"\"\"", "\n", "return", "self", ".", "_data", "[", "'is_one_hot'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.in_shape": [[182, 186], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "in_shape", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`in_shape`.\"\"\"", "\n", "return", "self", ".", "_data", "[", "'in_shape'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.out_shape": [[187, 191], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "out_shape", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`out_shape`.\"\"\"", "\n", "return", "self", ".", "_data", "[", "'out_shape'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.num_train_samples": [[192, 196], ["numpy.size", "numpy.size"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_train_samples", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`num_train_samples`.\"\"\"", "\n", "return", "np", ".", "size", "(", "self", ".", "_data", "[", "'train_inds'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.num_test_samples": [[197, 201], ["numpy.size", "numpy.size"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_test_samples", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`num_test_samples`.\"\"\"", "\n", "return", "np", ".", "size", "(", "self", ".", "_data", "[", "'test_inds'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.num_val_samples": [[202, 208], ["numpy.size", "numpy.size"], "methods", ["None"], ["", "@", "property", "\n", "def", "num_val_samples", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`num_val_samples`.\"\"\"", "\n", "if", "self", ".", "_data", "[", "'val_inds'", "]", "is", "None", ":", "\n", "            ", "return", "0", "\n", "", "return", "np", ".", "size", "(", "self", ".", "_data", "[", "'val_inds'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.shuffle_test_samples": [[214, 224], ["None"], "methods", ["None"], ["", "@", "shuffle_test_samples", ".", "setter", "\n", "def", "shuffle_test_samples", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"Setter for the attribute :attr:`shuffle_test_samples`..\n\n        Note, a call to this method will reset the current generator, such that\n        the next call to the method :meth:`next_test_batch` results in starting\n        a sweep through a new epoch (full batch).\n        \"\"\"", "\n", "self", ".", "_shuffle_test_samples", "=", "value", "\n", "self", ".", "_batch_gen_test", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.shuffle_val_samples": [[230, 238], ["None"], "methods", ["None"], ["", "@", "shuffle_val_samples", ".", "setter", "\n", "def", "shuffle_val_samples", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"Setter for the attribute :attr:`shuffle_val_samples`.\n\n        See documentation of setter for attribute :attr:`shuffle_test_samples`.\n        \"\"\"", "\n", "self", ".", "_shuffle_val_samples", "=", "value", "\n", "self", ".", "_batch_gen_val", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_inputs": [[239, 251], ["None"], "methods", ["None"], ["", "def", "get_train_inputs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the inputs of all training samples.\n        \n        Note, that each sample is encoded as a single vector. One may use the\n        attribute :attr:`in_shape` to decode the actual shape of an input\n        sample.\n        \n        Returns:\n            (numpy.ndarray): A 2D numpy array, where each row encodes a training\n            sample.\n        \"\"\"", "\n", "return", "self", ".", "_data", "[", "'in_data'", "]", "[", "self", ".", "_data", "[", "'train_inds'", "]", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_inputs": [[252, 261], ["None"], "methods", ["None"], ["", "def", "get_test_inputs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the inputs of all test samples.\n        \n        See documentation of method \"get_train_inputs\" for details.\n        \n        Returns:\n            (numpy.ndarray): A 2D numpy array.\n        \"\"\"", "\n", "return", "self", ".", "_data", "[", "'in_data'", "]", "[", "self", ".", "_data", "[", "'test_inds'", "]", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_val_inputs": [[262, 274], ["None"], "methods", ["None"], ["", "def", "get_val_inputs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the inputs of all validation samples.\n\n        See documentation of method :meth:`get_train_inputs` for details.\n\n        Returns:\n            (numpy.ndarray): A 2D numpy array. Returns ``None`` if no validation\n            set exists.\n        \"\"\"", "\n", "if", "self", ".", "_data", "[", "'val_inds'", "]", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "return", "self", ".", "_data", "[", "'in_data'", "]", "[", "self", ".", "_data", "[", "'val_inds'", "]", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs": [[275, 295], ["dataset.Dataset._get_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_outputs"], ["", "def", "get_train_outputs", "(", "self", ",", "use_one_hot", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get the outputs (targets) of all training samples.\n\n        Note, that each sample is encoded as a single vector. One may use the\n        attribute :attr:`out_shape` to decode the actual shape of an output\n        sample. Keep in mind, that classification samples might be one-hot\n        encoded.\n\n        Args:\n            use_one_hot (bool): For classification samples, the encoding of the\n                returned samples can be either \"one-hot\" or \"class index\". This\n                option is ignored for datasets other than classification sets.\n                If ``None``, the dataset its default encoding is returned.\n\n        Returns:\n            (numpy.ndarray): A 2D numpy array, where each row encodes a training\n            target.\n        \"\"\"", "\n", "out_data", "=", "self", ".", "_data", "[", "'out_data'", "]", "[", "self", ".", "_data", "[", "'train_inds'", "]", ",", ":", "]", "\n", "return", "self", ".", "_get_outputs", "(", "out_data", ",", "use_one_hot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs": [[296, 309], ["dataset.Dataset._get_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_outputs"], ["", "def", "get_test_outputs", "(", "self", ",", "use_one_hot", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get the outputs (targets) of all test samples.\n\n        See documentation of method :meth:`get_train_outputs` for details.\n\n        Args:\n            (....): See docstring of method :meth:`get_train_outputs`.\n\n        Returns:\n            (numpy.ndarray): A 2D numpy array.\n        \"\"\"", "\n", "out_data", "=", "self", ".", "_data", "[", "'out_data'", "]", "[", "self", ".", "_data", "[", "'test_inds'", "]", ",", ":", "]", "\n", "return", "self", ".", "_get_outputs", "(", "out_data", ",", "use_one_hot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_val_outputs": [[310, 326], ["dataset.Dataset._get_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_outputs"], ["", "def", "get_val_outputs", "(", "self", ",", "use_one_hot", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get the outputs (targets) of all validation samples.\n\n        See documentation of method :meth:`get_train_outputs` for details.\n\n        Args:\n            (....): See docstring of method :meth:`get_train_outputs`.\n\n        Returns:\n            (numpy.ndarray): A 2D numpy array. Returns None if no validation set\n            exists.\n        \"\"\"", "\n", "if", "self", ".", "_data", "[", "'val_inds'", "]", "is", "None", ":", "\n", "            ", "return", "None", "\n", "", "out_data", "=", "self", ".", "_data", "[", "'out_data'", "]", "[", "self", ".", "_data", "[", "'val_inds'", "]", ",", ":", "]", "\n", "return", "self", ".", "_get_outputs", "(", "out_data", ",", "use_one_hot", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_train_batch": [[327, 353], ["numpy.fromiter", "numpy.fromiter", "dataset.Dataset.reset_batch_generator", "dataset.Dataset._get_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.reset_batch_generator", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_outputs"], ["", "def", "next_train_batch", "(", "self", ",", "batch_size", ",", "use_one_hot", "=", "None", ")", ":", "\n", "        ", "\"\"\"Return the next random training batch.\n\n        If the behavior of this method should be reproducible, please define a\n        numpy random seed.\n\n        Args:\n            (....): See docstring of method :meth:`get_train_outputs`.\n            batch_size (int): The size of the returned batch.\n\n        Returns:\n            (tuple): Tuple containing the following 2D numpy arrays:\n\n            - **batch_inputs**: The inputs of the samples belonging to the\n              batch.\n            - **batch_outputs**: The outputs of the samples belonging to the\n              batch.\n        \"\"\"", "\n", "if", "self", ".", "_batch_gen_train", "is", "None", ":", "\n", "            ", "self", ".", "reset_batch_generator", "(", "train", "=", "True", ",", "test", "=", "False", ",", "val", "=", "False", ")", "\n", "\n", "", "batch_inds", "=", "np", ".", "fromiter", "(", "self", ".", "_batch_gen_train", ",", "np", ".", "int", ",", "\n", "count", "=", "batch_size", ")", "\n", "return", "[", "self", ".", "_data", "[", "'in_data'", "]", "[", "batch_inds", ",", ":", "]", ",", "\n", "self", ".", "_get_outputs", "(", "self", ".", "_data", "[", "'out_data'", "]", "[", "batch_inds", ",", ":", "]", ",", "\n", "use_one_hot", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_test_batch": [[354, 376], ["numpy.fromiter", "numpy.fromiter", "dataset.Dataset.reset_batch_generator", "dataset.Dataset._get_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.reset_batch_generator", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_outputs"], ["", "def", "next_test_batch", "(", "self", ",", "batch_size", ",", "use_one_hot", "=", "None", ")", ":", "\n", "        ", "\"\"\"Return the next random test batch.\n\n        See documentation of method :meth:`next_train_batch` for details.\n\n        Args:\n            (....): See docstring of method :meth:`next_train_batch`.\n\n        Returns:\n            (tuple): Tuple containing the following 2D numpy arrays:\n\n            - **batch_inputs**\n            - **batch_outputs**\n        \"\"\"", "\n", "if", "self", ".", "_batch_gen_test", "is", "None", ":", "\n", "            ", "self", ".", "reset_batch_generator", "(", "train", "=", "False", ",", "test", "=", "True", ",", "val", "=", "False", ")", "\n", "\n", "", "batch_inds", "=", "np", ".", "fromiter", "(", "self", ".", "_batch_gen_test", ",", "np", ".", "int", ",", "\n", "count", "=", "batch_size", ")", "\n", "return", "[", "self", ".", "_data", "[", "'in_data'", "]", "[", "batch_inds", ",", ":", "]", ",", "\n", "self", ".", "_get_outputs", "(", "self", ".", "_data", "[", "'out_data'", "]", "[", "batch_inds", ",", ":", "]", ",", "\n", "use_one_hot", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.next_val_batch": [[377, 404], ["numpy.fromiter", "numpy.fromiter", "dataset.Dataset.reset_batch_generator", "dataset.Dataset._get_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.reset_batch_generator", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_outputs"], ["", "def", "next_val_batch", "(", "self", ",", "batch_size", ",", "use_one_hot", "=", "None", ")", ":", "\n", "        ", "\"\"\"Return the next random validation batch.\n\n        See documentation of method :meth:`next_train_batch` for details.\n\n        Args:\n            (....): See docstring of method :meth:`next_train_batch`.\n\n        Returns:\n            (tuple): Tuple containing the following 2D numpy arrays:\n\n            - **batch_inputs**\n            - **batch_outputs**\n\n            Returns ``None`` if no validation set exists.\n        \"\"\"", "\n", "if", "self", ".", "_data", "[", "'val_inds'", "]", "is", "None", ":", "\n", "            ", "return", "None", "\n", "\n", "", "if", "self", ".", "_batch_gen_val", "is", "None", ":", "\n", "            ", "self", ".", "reset_batch_generator", "(", "train", "=", "False", ",", "test", "=", "False", ",", "val", "=", "True", ")", "\n", "\n", "", "batch_inds", "=", "np", ".", "fromiter", "(", "self", ".", "_batch_gen_val", ",", "np", ".", "int", ",", "\n", "count", "=", "batch_size", ")", "\n", "return", "[", "self", ".", "_data", "[", "'in_data'", "]", "[", "batch_inds", ",", ":", "]", ",", "\n", "self", ".", "_get_outputs", "(", "self", ".", "_data", "[", "'out_data'", "]", "[", "batch_inds", ",", ":", "]", ",", "\n", "use_one_hot", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.reset_batch_generator": [[405, 432], ["dataset.Dataset._get_random_batch", "dataset.Dataset._get_random_batch", "dataset.Dataset._get_random_batch"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_random_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_random_batch", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_random_batch"], ["", "def", "reset_batch_generator", "(", "self", ",", "train", "=", "True", ",", "test", "=", "True", ",", "val", "=", "True", ")", ":", "\n", "        ", "\"\"\"The batch generation possesses a memory. Hence, the samples returned\n        depend on how many samples already have been retrieved via the next-\n        batch functions (e.g., :meth:`next_train_batch`). This method can be\n        used to reset these generators.\n\n        Args:\n            train (bool): If ``True``, the generator for\n                :meth:`next_train_batch` is reset.\n            test (bool): If ``True``, the generator for :meth:`next_test_batch`\n                is reset.\n            val (bool): If ``True``, the generator for :meth:`next_val_batch`\n                is reset, if a validation set exists.\n        \"\"\"", "\n", "if", "train", ":", "\n", "            ", "self", ".", "_batch_gen_train", "=", "Dataset", ".", "_get_random_batch", "(", "self", ".", "_data", "[", "'train_inds'", "]", ")", "\n", "\n", "", "if", "test", ":", "\n", "            ", "self", ".", "_batch_gen_test", "=", "Dataset", ".", "_get_random_batch", "(", "self", ".", "_data", "[", "'test_inds'", "]", ",", "\n", "self", ".", "_shuffle_test_samples", ")", "\n", "\n", "", "if", "val", "and", "self", ".", "_data", "[", "'val_inds'", "]", "is", "not", "None", ":", "\n", "            ", "self", ".", "_batch_gen_val", "=", "Dataset", ".", "_get_random_batch", "(", "self", ".", "_data", "[", "'val_inds'", "]", ",", "\n", "self", ".", "_shuffle_val_samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_identifier": [[433, 441], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\n\n        Returns:\n            (str): The dataset its (unique) identifier.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.is_image_dataset": [[442, 460], ["numpy.size", "numpy.size", "numpy.size", "numpy.size"], "methods", ["None"], ["", "def", "is_image_dataset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Are input (resp. output) samples images?\n        \n        Note, for sequence datasets, this method just returns whether a single\n        frame encodes an image.\n\n        Returns:\n            (tuple): Tuple containing two booleans:\n\n            - **input_is_img**\n            - **output_is_img**\n        \"\"\"", "\n", "# Note, if these comparisons do not hold, the method has to be", "\n", "# overwritten.", "\n", "in_img", "=", "np", ".", "size", "(", "self", ".", "in_shape", ")", "==", "3", "and", "self", ".", "in_shape", "[", "-", "1", "]", "in", "[", "1", ",", "3", ",", "4", "]", "\n", "out_img", "=", "np", ".", "size", "(", "self", ".", "out_shape", ")", "==", "3", "and", "self", ".", "out_shape", "[", "-", "1", "]", "in", "[", "1", ",", "3", ",", "4", "]", "\n", "return", "(", "in_img", ",", "out_img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.tf_input_map": [[461, 483], ["None"], "methods", ["None"], ["", "def", "tf_input_map", "(", "self", ",", "mode", "=", "'inference'", ")", ":", "\n", "        ", "\"\"\"This method should be used by the map function of the Tensorflow\n        Dataset interface (``tf.data.Dataset.map``). In the default case, this\n        is just an identity map, as the data is already in memory.\n\n        There might be cases, in which the full dataset is too large for the\n        working memory, and therefore the data currently needed by Tensorflow\n        has to be loaded from disk. This function should be used as an\n        interface for this process.\n\n        Args:\n            mode (str): Is the data needed for training or inference? This\n                distinction is important, as it might change the way the data is\n                processed (e.g., special random data augmentation might apply\n                during training but not during inference. The parameter is a\n                string with the valid values being ``train`` and ``inference``.\n\n        Returns:\n            (function): A function handle, that maps the given input tensor to\n            the preprocessed input tensor.\n        \"\"\"", "\n", "return", "lambda", "x", ":", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.tf_output_map": [[484, 496], ["None"], "methods", ["None"], ["", "def", "tf_output_map", "(", "self", ",", "mode", "=", "'inference'", ")", ":", "\n", "        ", "\"\"\"Similar to method :meth:`tf_input_map`, just for dataset outputs.\n\n        Note, in this default implementation, it is also just an identity map.\n        \n        Args:\n            (....): See docstring of method :meth:`tf_input_map`.\n\n        Returns:\n            (function): A function handle.\n        \"\"\"", "\n", "return", "lambda", "x", ":", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.input_to_torch_tensor": [[497, 525], ["from_numpy().float().to", "from_numpy().float", "from_numpy"], "methods", ["None"], ["", "def", "input_to_torch_tensor", "(", "self", ",", "x", ",", "device", ",", "mode", "=", "'inference'", ",", "\n", "force_no_preprocessing", "=", "False", ")", ":", "\n", "        ", "\"\"\"This method can be used to map the internal numpy arrays to PyTorch\n        tensors.\n\n        Note, subclasses might overwrite this method and add data preprocessing/\n        augmentation.\n\n        Args:\n            x (numpy.ndarray): A 2D numpy array, containing inputs as provided\n                by this dataset.\n            device (torch.device or int): The PyTorch device onto which the\n                input should be mapped.\n            mode (str): See docstring of method :meth:`tf_input_map`.\n                  Valid values are: ``train`` and ``inference``.\n            force_no_preprocessing (bool): In case preprocessing is applied to\n                the inputs (e.g., normalization or random flips/crops), this\n                option can be used to prohibit any kind of manipulation. Hence,\n                the inputs are transformed into PyTorch tensors on an \"as is\"\n                basis.\n\n        Returns:\n            (torch.Tensor): The given input ``x`` as PyTorch tensor.\n        \"\"\"", "\n", "# Note, this import is only needed for the functions:", "\n", "# input_to_torch_tensor() and output_to_torch_tensor()", "\n", "from", "torch", "import", "from_numpy", "\n", "return", "from_numpy", "(", "x", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.output_to_torch_tensor": [[526, 542], ["from_numpy().float().to", "from_numpy().float", "from_numpy"], "methods", ["None"], ["", "def", "output_to_torch_tensor", "(", "self", ",", "y", ",", "device", ",", "mode", "=", "'inference'", ",", "\n", "force_no_preprocessing", "=", "False", ")", ":", "\n", "        ", "\"\"\"Similar to method :meth:`input_to_torch_tensor`, just for dataset\n        outputs.\n\n        Note, in this default implementation, it is also does not perform any\n        data preprocessing.\n\n        Args:\n            (....): See docstring of method :meth:`input_to_torch_tensor`.\n\n        Returns:\n            (torch.Tensor): The given output ``y`` as PyTorch tensor.\n        \"\"\"", "\n", "from", "torch", "import", "from_numpy", "\n", "return", "from_numpy", "(", "y", ")", ".", "float", "(", ")", ".", "to", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.plot_samples": [[543, 626], ["dataset.Dataset._plot_config", "int", "int", "matplotlib.figure", "matplotlib.figure", "matplotlib.GridSpec", "matplotlib.GridSpec", "matplotlib.suptitle", "matplotlib.suptitle", "range", "min", "numpy.ceil", "numpy.ceil", "matplotlib.ion", "matplotlib.ion", "matplotlib.GridSpecFromSubplotSpec", "matplotlib.GridSpecFromSubplotSpec", "dataset.Dataset._plot_sample", "matplotlib.show", "matplotlib.show", "matplotlib.savefig", "matplotlib.savefig", "dataset.Dataset._to_one_hot", "numpy.prod", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._plot_config", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._plot_sample", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot"], ["", "def", "plot_samples", "(", "self", ",", "title", ",", "inputs", ",", "outputs", "=", "None", ",", "predictions", "=", "None", ",", "\n", "num_samples_per_row", "=", "4", ",", "show", "=", "True", ",", "filename", "=", "None", ",", "\n", "interactive", "=", "False", ",", "figsize", "=", "(", "10", ",", "6", ")", ")", ":", "\n", "        ", "\"\"\"Plot samples belonging to this dataset. Each sample will be plotted\n        in its own subplot.\n\n        Args:\n            title (str): The title of the whole figure.\n            inputs (numpy.ndarray): A 2D numpy array, where each row is an input\n                sample.\n            outputs (numpy.ndarray, optional): A 2D numpy array of actual\n                dataset targets.\n            predictions (numpy.ndarray, optional): A 2D numpy array of predicted\n                output samples (i.e., output predicted by a neural network).\n            num_samples_per_row (int): Maximum number of samples plotted\n                per row in the generated figure.\n            show (bool): Whether the plot should be shown.\n            filename (str, optional): If provided, the figure will be stored\n                under this filename.\n            interactive (bool): Turn on interactive mode. We mainly\n                use this option to ensure that the program will run in\n                background while figure is displayed. The figure will be\n                displayed until another one is displayed, the user closes it or\n                the program has terminated. If this option is deactivated, the\n                program will freeze until the user closes the figure.\n                Note, if using the iPython inline backend, this option has no\n                effect.\n            figsize (tuple): A tuple, determining the size of the\n                figure in inches.\n        \"\"\"", "\n", "# Determine the configs for the grid of this figure.", "\n", "pc", "=", "self", ".", "_plot_config", "(", "inputs", ",", "outputs", "=", "outputs", ",", "\n", "predictions", "=", "predictions", ")", "\n", "\n", "# Reverse one-hot encoding.", "\n", "if", "self", ".", "classification", ":", "\n", "            ", "num_time_steps", "=", "1", "\n", "if", "self", ".", "sequence", ":", "\n", "                ", "num_time_steps", "=", "self", ".", "_data", "[", "'out_data'", "]", ".", "shape", "[", "1", "]", "//", "np", ".", "prod", "(", "self", ".", "out_shape", ")", "\n", "\n", "", "one_hot_size", "=", "num_time_steps", "*", "self", ".", "num_classes", "\n", "if", "outputs", "is", "not", "None", "and", "outputs", ".", "shape", "[", "1", "]", "==", "one_hot_size", ":", "\n", "                ", "outputs", "=", "self", ".", "_to_one_hot", "(", "outputs", ",", "True", ")", "\n", "\n", "# Note, we don't reverse the encoding for predictions, as this", "\n", "# might be important for the subsequent plotting method.", "\n", "\n", "", "", "num_plots", "=", "inputs", ".", "shape", "[", "0", "]", "\n", "num_cols", "=", "int", "(", "min", "(", "num_plots", ",", "num_samples_per_row", ")", ")", "\n", "num_rows", "=", "int", "(", "np", ".", "ceil", "(", "num_plots", "/", "num_samples_per_row", ")", ")", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "figsize", ")", "\n", "outer_grid", "=", "gridspec", ".", "GridSpec", "(", "num_rows", ",", "num_cols", ",", "\n", "wspace", "=", "pc", "[", "'outer_wspace'", "]", ",", "\n", "hspace", "=", "pc", "[", "'outer_hspace'", "]", ")", "\n", "\n", "plt", ".", "suptitle", "(", "title", ",", "size", "=", "20", ")", "\n", "if", "interactive", ":", "\n", "            ", "plt", ".", "ion", "(", ")", "\n", "\n", "", "outs", "=", "None", "\n", "preds", "=", "None", "\n", "\n", "for", "i", "in", "range", "(", "num_plots", ")", ":", "\n", "            ", "inner_grid", "=", "gridspec", ".", "GridSpecFromSubplotSpec", "(", "pc", "[", "'num_inner_rows'", "]", ",", "\n", "pc", "[", "'num_inner_cols'", "]", ",", "subplot_spec", "=", "outer_grid", "[", "i", "]", ",", "\n", "wspace", "=", "pc", "[", "'inner_wspace'", "]", ",", "hspace", "=", "pc", "[", "'inner_hspace'", "]", ")", "\n", "\n", "if", "outputs", "is", "not", "None", ":", "\n", "                ", "outs", "=", "outputs", "[", "i", ",", ":", "]", "\n", "", "if", "predictions", "is", "not", "None", ":", "\n", "                ", "preds", "=", "predictions", "[", "i", ",", ":", "]", "\n", "\n", "", "self", ".", "_plot_sample", "(", "fig", ",", "inner_grid", ",", "pc", "[", "'num_inner_plots'", "]", ",", "i", ",", "\n", "inputs", "[", "i", ",", "np", ".", "newaxis", "]", ",", "outputs", "=", "outs", ",", "\n", "predictions", "=", "preds", ")", "\n", "\n", "", "if", "show", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n", "", "if", "filename", "is", "not", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "filename", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._plot_sample": [[627, 658], ["None"], "methods", ["None"], ["", "", "@", "abstractmethod", "\n", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Add a custom sample plot to the given Axes object.\n\n        Note, this method is called by the :meth:`plot_samples` method.\n\n        Note, that the number of inner subplots is configured via the method:\n        :meth:`_plot_config`.\n\n        Args:\n            fig: An instance of class matplotlib.figure.Figure, that will\n                contains the given Axes object.\n            inner_grid: An object of the class\n                matplotlib.gridspec.GridSpecFromSubplotSpec. It can be used to\n                access the subplots of a single sample via\n                    ax = plt.Subplot(fig, inner_grid[i])\n                where i is a number between 0 and num_inner_plots-1.\n                The retrieved axes has to be added to the figure via:\n                    fig.add_subplot(ax)\n            num_inner_plots: The number inner subplots.\n            ind: The index of the \"outer\" subplot.\n            inputs: A 2D numpy array, containing a single sample (1 row).\n            outputs (optional): A 2D numpy array, containing a single sample \n                (1 row). If this is a classification dataset, then samples are\n                given as single labels (not one-hot encoded, irrespective of\n                the attribute is_one_hot).\n            predictions (optional): A 2D numpy array, containing a single \n                sample (1 row).\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._plot_config": [[659, 683], ["dict"], "methods", ["None"], ["", "def", "_plot_config", "(", "self", ",", "inputs", ",", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Defines properties, used by the method :meth:`plot_samples`.\n\n        This method can be overwritten, if these configs need to be different\n        for a certain dataset.\n\n        Args:\n            The given arguments are the same as the same-named arguments of\n            the method :meth:`plot_samples`. They might be used by subclass\n            implementations to determine the configs.\n\n        Returns:\n            (dict): A dictionary with the plot configs.\n        \"\"\"", "\n", "plot_configs", "=", "dict", "(", ")", "\n", "plot_configs", "[", "'outer_wspace'", "]", "=", "0.4", "\n", "plot_configs", "[", "'outer_hspace'", "]", "=", "0.4", "\n", "plot_configs", "[", "'inner_hspace'", "]", "=", "0.2", "\n", "plot_configs", "[", "'inner_wspace'", "]", "=", "0.2", "\n", "plot_configs", "[", "'num_inner_rows'", "]", "=", "1", "\n", "plot_configs", "[", "'num_inner_cols'", "]", "=", "1", "\n", "plot_configs", "[", "'num_inner_plots'", "]", "=", "1", "\n", "\n", "return", "plot_configs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_outputs": [[684, 707], ["dataset.Dataset._to_one_hot", "dataset.Dataset._to_one_hot"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot"], ["", "def", "_get_outputs", "(", "self", ",", "data", ",", "use_one_hot", "=", "None", ")", ":", "\n", "        ", "\"\"\"A helper method for the output data getter methods. It will ensure,\n        that the output encoding is correct.\n\n        Args:\n            data: The data to be returned (maybe after a change of encoding).\n            use_one_hot: How data should be encoded.\n\n        Returns:\n            See documentation of method :meth:`get_train_outputs`.\n        \"\"\"", "\n", "if", "self", ".", "classification", ":", "\n", "            ", "if", "use_one_hot", "is", "None", ":", "\n", "                ", "use_one_hot", "=", "self", ".", "is_one_hot", "\n", "\n", "", "if", "use_one_hot", "!=", "self", ".", "is_one_hot", ":", "\n", "# Toggle current encoding.", "\n", "                ", "if", "self", ".", "is_one_hot", ":", "\n", "                    ", "return", "self", ".", "_to_one_hot", "(", "data", ",", "reverse", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "return", "self", ".", "_to_one_hot", "(", "data", ")", "\n", "\n", "", "", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot": [[708, 744], ["RuntimeError", "sklearn.preprocessing.OneHotEncoder", "dataset.Dataset._one_hot_encoder.fit", "numpy.reshape", "numpy.reshape", "dataset.Dataset._one_hot_encoder.transform().toarray", "numpy.prod", "numpy.prod", "numpy.repmat", "numpy.repmat", "dataset.Dataset._one_hot_encoder.transform", "range", "numpy.arange", "numpy.arange", "numpy.argwhere", "numpy.argwhere"], "methods", ["None"], ["", "def", "_to_one_hot", "(", "self", ",", "labels", ",", "reverse", "=", "False", ")", ":", "\n", "        ", "\"\"\" Transform a list of labels into a 1-hot encoding.\n\n        Args:\n            labels: A list of class labels.\n            reverse: If true, then one-hot encoded samples are transformed back\n                to categorical labels.\n\n        Returns:\n            The 1-hot encoded labels.\n        \"\"\"", "\n", "if", "not", "self", ".", "classification", ":", "\n", "            ", "raise", "RuntimeError", "(", "'This method can only be called for '", "+", "\n", "'classification datasets.'", ")", "\n", "\n", "# Initialize encoder.", "\n", "", "if", "self", ".", "_one_hot_encoder", "is", "None", ":", "\n", "            ", "self", ".", "_one_hot_encoder", "=", "OneHotEncoder", "(", "categories", "=", "[", "range", "(", "self", ".", "num_classes", ")", "]", ")", "\n", "num_time_steps", "=", "1", "\n", "if", "self", ".", "sequence", ":", "\n", "                ", "num_time_steps", "=", "labels", ".", "shape", "[", "1", "]", "//", "np", ".", "prod", "(", "self", ".", "out_shape", ")", "\n", "", "self", ".", "_one_hot_encoder", ".", "fit", "(", "npm", ".", "repmat", "(", "\n", "np", ".", "arange", "(", "self", ".", "num_classes", ")", ",", "num_time_steps", ",", "1", ")", ".", "T", ")", "\n", "\n", "", "if", "reverse", ":", "\n", "# Unfortunately, there is no inverse function in the OneHotEncoder", "\n", "# class. Therefore, we take the one-hot-encoded \"labels\" samples", "\n", "# and take the indices of all 1 entries. Note, that these indices", "\n", "# are returned as tuples, where the second column contains the", "\n", "# original column indices. These column indices from \"labels\"", "\n", "# mudolo the number of classes results in the original labels.", "\n", "            ", "return", "np", ".", "reshape", "(", "np", ".", "argwhere", "(", "labels", ")", "[", ":", ",", "1", "]", "%", "self", ".", "num_classes", ",", "\n", "(", "labels", ".", "shape", "[", "0", "]", ",", "-", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_one_hot_encoder", ".", "transform", "(", "labels", ")", ".", "toarray", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._get_random_batch": [[745, 817], ["numpy.arange", "numpy.arange", "numpy.shape", "numpy.shape", "numpy.random.shuffle", "numpy.random.shuffle"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_get_random_batch", "(", "indices", ",", "shuffle", "=", "True", ")", ":", "\n", "        ", "\"\"\"Returns a generator that yields sample indices from the randomly\n        shuffled given dataset (prescribed by those indices). After a whole\n        sweep through the array is completed, the indices are shuffled again.\n\n        The implementation is inspired by another method I've implemented:\n            The method \"random_shuffle_loop\": from https://git.io/fNDZJ\n\n        Note, to get reproducable behavior, set a numpy random seed.\n\n        Args:\n            indices: A 1D numpy array of indices.\n            shuffle (default: True): Whether the indices are shuffled every\n                                     epoch. If this option is deactivated, the\n                                     indices are simply processed in the\n                                     given order every round.\n\n        Returns:\n            A generator object, that yields indices from the given array.\n\n        Example Usage:\n        >>> import iterlist\n        >>> batch_size = 32\n        >>> batch_generator = Dataset._get_random_batch(indices)\n        >>> batch_inds = np.array( \\\n        ...     list(itertools.islice(batch_generator, batch_size)))\n        \n        Runtime Benchmark: What is a fast way to call the method?\n        Note, that np.fromiter only works since indices is a 1D array.\n        >>> import timeit\n        >>> setup = '''\n        ... import numpy as np\n        ... import itertools\n        ... from dataset import Dataset\n        ... \n        ... indices = np.random.randint(0, high=100, size=100)\n        ... \n        ... batch_size = 32\n        ... batch_generator = Dataset._get_random_batch(indices)\n        ... '''\n        >>> print(timeit.timeit(\n        ...     stmt=\"batch_inds = np.array(list(itertools.islice(\" +\n        ...          \"batch_generator, batch_size)))\",\n        ...     setup=setup, number=100000))\n        1.1637690339994151\n        >>> print(timeit.timeit(\n        ...     stmt=\"batch_inds = np.stack([next(batch_generator) \" +\n        ...          \"for i in range(batch_size)])\",\n        ...     setup=setup, number=100000))\n        6.16505571999005\n        >>> print(timeit.timeit(\n        ...     stmt=\"batch_inds = np.fromiter(itertools.islice(\" +\n        ...         batch_generator, batch_size), int)\",\n        ...     setup=setup, number=100000))\n        0.9456974960048683\n        >>> print(timeit.timeit(\n        ...     stmt=\"batch_inds = np.fromiter(batch_generator, int, \" +\n        ...                                    count=batch_size)\",\n        ...     setup=setup, number=100000))\n        0.871306327986531\n        \"\"\"", "\n", "num_samples", "=", "np", ".", "shape", "(", "indices", ")", "[", "0", "]", "\n", "arr_inds", "=", "np", ".", "arange", "(", "num_samples", ")", "\n", "i", "=", "num_samples", "\n", "while", "True", ":", "\n", "            ", "if", "i", "==", "num_samples", ":", "\n", "                ", "i", "=", "0", "\n", "if", "shuffle", ":", "\n", "                    ", "np", ".", "random", ".", "shuffle", "(", "arr_inds", ")", "\n", "", "", "yield", "indices", "[", "arr_inds", "[", "i", "]", "]", "\n", "i", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.__init__": [[88, 106], ["data.dataset.Dataset.__init__"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ",", "imgs_path", ",", "png_format", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_imgs_path", "=", "imgs_path", "\n", "self", ".", "_png_format_used", "=", "png_format", "\n", "\n", "# The wrapper is currently not meant for sequence inputs. You can still", "\n", "# set this variable to true, if you have sequence outputs.", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "\n", "# Implementing classes should provide instances of class", "\n", "#   torch.utils.data.Dataset", "\n", "# For instance, using torchvision.datasets.ImageFolder.", "\n", "# In this way, users can reuse typical PyTorch code and don't have to", "\n", "# write custom data loaders.", "\n", "self", ".", "_torch_ds_train", "=", "None", "\n", "self", ".", "_torch_ds_test", "=", "None", "\n", "self", ".", "_torch_ds_val", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.imgs_path": [[107, 111], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "imgs_path", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`imgs_path`.\"\"\"", "\n", "return", "self", ".", "_imgs_path", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.png_format_used": [[112, 116], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "png_format_used", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`png_format_used`.\"\"\"", "\n", "return", "self", ".", "_png_format_used", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.torch_train": [[117, 123], ["NotImplementedError"], "methods", ["None"], ["", "@", "property", "\n", "def", "torch_train", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`torch_train`.\"\"\"", "\n", "if", "self", ".", "_torch_ds_train", "is", "None", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Dataset not prepared for PyTorch use!'", ")", "\n", "", "return", "self", ".", "_torch_ds_train", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.torch_test": [[124, 130], ["NotImplementedError"], "methods", ["None"], ["", "@", "property", "\n", "def", "torch_test", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`torch_test`.\"\"\"", "\n", "if", "self", ".", "_torch_ds_test", "is", "None", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'Dataset not prepared for PyTorch use!'", ")", "\n", "", "return", "self", ".", "_torch_ds_test", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.torch_val": [[131, 135], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "torch_val", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute :attr:`torch_val`.\"\"\"", "\n", "return", "self", ".", "_torch_ds_val", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs": [[136, 144], ["data.dataset.Dataset.get_train_inputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs"], ["", "def", "get_train_inputs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the inputs of all training samples.\n        \n        Returns:\n            (numpy.chararray): An np.chararray, where each row corresponds to an\n            image file name.\n        \"\"\"", "\n", "return", "Dataset", ".", "get_train_inputs", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs": [[145, 153], ["data.dataset.Dataset.get_test_inputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs"], ["", "def", "get_test_inputs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the inputs of all test samples.\n        \n        Returns:\n            (numpy.chararray): An np.chararray, where each row corresponds to an\n            image file name.\n        \"\"\"", "\n", "return", "Dataset", ".", "get_test_inputs", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_val_inputs": [[154, 163], ["data.dataset.Dataset.get_val_inputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_val_inputs"], ["", "def", "get_val_inputs", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the inputs of all validation samples.\n        \n        Returns:\n            (numpy.chararray): An np.chararray, where each row corresponds to an\n            image file name. If no validation set exists, ``None`` will be\n            returned.\n        \"\"\"", "\n", "return", "Dataset", ".", "get_val_inputs", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.read_images": [[164, 192], ["numpy.empty", "range", "os.path.join", "PIL.Image.open", "img.convert.convert.resize", "numpy.array().flatten", "numpy.prod", "str", "img.convert.convert.convert", "inputs[].squeeze", "numpy.array"], "methods", ["None"], ["", "def", "read_images", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "\"\"\"For the given filenames, read and return the images.\n\n        Args:\n            inputs (numpy.chararray): An np.chararray of filenames.\n\n        Returns:\n            (numpy.ndarray): A 2D numpy array, where each row contains a\n            picture.\n        \"\"\"", "\n", "ret", "=", "np", ".", "empty", "(", "[", "inputs", ".", "shape", "[", "0", "]", ",", "np", ".", "prod", "(", "self", ".", "in_shape", ")", "]", ",", "np", ".", "float32", ")", "\n", "\n", "for", "i", "in", "range", "(", "inputs", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "fn", "=", "os", ".", "path", ".", "join", "(", "self", ".", "imgs_path", ",", "\n", "str", "(", "inputs", "[", "i", ",", "np", ".", "newaxis", "]", ".", "squeeze", "(", ")", ")", ")", "\n", "img", "=", "Image", ".", "open", "(", "fn", ")", "\n", "#img = mpimg.imread(fn)", "\n", "if", "img", ".", "mode", "!=", "'RGB'", ":", "\n", "                ", "img", "=", "img", ".", "convert", "(", "'RGB'", ")", "\n", "", "img", "=", "img", ".", "resize", "(", "self", ".", "in_shape", "[", ":", "2", "]", ",", "Image", ".", "BILINEAR", ")", "\n", "ret", "[", "i", ",", ":", "]", "=", "np", ".", "array", "(", "img", ")", ".", "flatten", "(", ")", "\n", "\n", "# Note, the images already have pixel values between 0 and 1 for", "\n", "# PNG images.", "\n", "", "if", "not", "self", ".", "png_format_used", ":", "\n", "            ", "ret", "/=", "255.", "\n", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.tf_input_map": [[193, 229], ["os.path.join", "tf.add", "tf.read_file", "tf.image.convert_image_dtype", "tf.image.resize_images", "tf.reshape", "tf.squeeze", "tf.image.decode_png", "tf.image.decode_jpeg"], "methods", ["None"], ["", "def", "tf_input_map", "(", "self", ",", "mode", "=", "'inference'", ")", ":", "\n", "        ", "\"\"\"Note, this method has been overwritten from the base class.\n\n        It provides a function handle that loads images from file, resizes them\n        to match the internal input image size and then flattens the image to\n        a vector.\n\n        Args:\n            (....): See docstring of method\n                :meth:`data.dataset.Dataset.tf_input_map`.\n\n        Returns:\n            (function): A function handle, that maps the given input tensor to\n            the preprocessed input tensor.\n        \"\"\"", "\n", "# FIXME removed this statement from the top of this file because it", "\n", "# caused warnings when using the logging module.", "\n", "import", "tensorflow", "as", "tf", "\n", "\n", "base_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "imgs_path", ",", "''", ")", "\n", "\n", "def", "load_inputs", "(", "inputs", ")", ":", "\n", "            ", "filename", "=", "tf", ".", "add", "(", "base_path", ",", "tf", ".", "squeeze", "(", "inputs", ")", ")", "\n", "image_string", "=", "tf", ".", "read_file", "(", "filename", ")", "\n", "if", "self", ".", "png_format_used", ":", "\n", "                ", "image", "=", "tf", ".", "image", ".", "decode_png", "(", "image_string", ")", "\n", "", "else", ":", "\n", "                ", "image", "=", "tf", ".", "image", ".", "decode_jpeg", "(", "image_string", ")", "\n", "", "image", "=", "tf", ".", "image", ".", "convert_image_dtype", "(", "image", ",", "tf", ".", "float32", ")", "\n", "image", "=", "tf", ".", "image", ".", "resize_images", "(", "image", ",", "self", ".", "in_shape", "[", ":", "2", "]", ")", "\n", "# We always feed flattened images into the network.", "\n", "image", "=", "tf", ".", "reshape", "(", "image", ",", "[", "-", "1", "]", ")", "\n", "\n", "return", "image", "\n", "\n", "", "return", "load_inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.input_to_torch_tensor": [[230, 244], ["NotImplementedError"], "methods", ["None"], ["", "def", "input_to_torch_tensor", "(", "self", ",", "x", ",", "device", ",", "mode", "=", "'inference'", ",", "\n", "force_no_preprocessing", "=", "False", ")", ":", "\n", "        ", "\"\"\"Note, this method has been overwritten from the base class. It should\n        not be used for large image datasets. Instead, the class should provide\n        instances of class :class:`torch.utils.data.Dataset` for training,\n        validation and test set:\n\n            - :attr:`torch_train`\n            - :attr:`torch_test`\n            - :attr:`torch_val`\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "'Use attributes \"torch_train\", \"torch_val\" '", "+", "\n", "'and \"torch_test\" instead. Please refer to the class '", "+", "\n", "'documentation.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data.__init__": [[80, 174], ["data.dataset.Dataset.__init__", "time.time", "print", "os.path.join", "os.path.join", "os.path.isfile", "time.time", "print", "os.path.exists", "print", "os.makedirs", "os.path.join", "list", "os.path.join", "os.path.join", "cifar10_data.CIFAR10Data._read_meta", "cifar10_data.CIFAR10Data._read_batches", "cifar10_data.CIFAR10Data.torch_input_transforms", "open", "_pickle.load", "os.path.exists", "print", "urllib.request.urlretrieve", "tarfile.open", "tarfile.open.extractall", "tarfile.open.close", "os.remove", "map", "all", "os.path.exists", "os.path.exists", "cifar10_data.CIFAR10Data._to_one_hot", "map", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._read_meta", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._read_batches", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.torch_input_transforms", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot"], ["def", "__init__", "(", "self", ",", "data_path", ",", "use_one_hot", "=", "False", ",", "\n", "use_data_augmentation", "=", "False", ",", "validation_size", "=", "5000", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "'Reading CIFAR-10 dataset ...'", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "data_path", ")", ":", "\n", "            ", "print", "(", "'Creating directory \"%s\" ...'", "%", "(", "data_path", ")", ")", "\n", "os", ".", "makedirs", "(", "data_path", ")", "\n", "\n", "", "extracted_data_dir", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "\n", "CIFAR10Data", ".", "_EXTRACTED_FOLDER", ")", "\n", "\n", "# If data has been processed before.", "\n", "build_from_scratch", "=", "True", "\n", "dump_fn", "=", "os", ".", "path", ".", "join", "(", "extracted_data_dir", ",", "\n", "CIFAR10Data", ".", "_CIFAR10_DATA_DUMP", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "dump_fn", ")", ":", "\n", "            ", "build_from_scratch", "=", "False", "\n", "\n", "with", "open", "(", "dump_fn", ",", "'rb'", ")", "as", "f", ":", "\n", "                ", "self", ".", "_data", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "if", "self", ".", "_data", "[", "'is_one_hot'", "]", "!=", "use_one_hot", ":", "\n", "                    ", "reverse", "=", "True", "\n", "if", "use_one_hot", ":", "\n", "                        ", "reverse", "=", "False", "\n", "\n", "", "self", ".", "_data", "[", "'is_one_hot'", "]", "=", "use_one_hot", "\n", "self", ".", "_data", "[", "'out_data'", "]", "=", "self", ".", "_to_one_hot", "(", "\n", "self", ".", "_data", "[", "'out_data'", "]", ",", "reverse", "=", "reverse", ")", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "self", ".", "_data", "[", "'out_data'", "]", ".", "shape", "[", "1", "]", "]", "\n", "\n", "", "if", "self", ".", "num_val_samples", "!=", "validation_size", ":", "\n", "                    ", "build_from_scratch", "=", "True", "\n", "\n", "\n", "", "", "", "if", "build_from_scratch", ":", "\n", "            ", "archive_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CIFAR10Data", ".", "_DOWNLOAD_FILE", ")", "\n", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "extracted_data_dir", ")", ":", "\n", "                ", "print", "(", "'Downloading dataset ...'", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "CIFAR10Data", ".", "_DOWNLOAD_PATH", "+", "CIFAR10Data", ".", "_DOWNLOAD_FILE", ",", "archive_fn", ")", "\n", "\n", "# Extract downloaded dataset.", "\n", "tar", "=", "tarfile", ".", "open", "(", "archive_fn", ",", "\"r:gz\"", ")", "\n", "tar", ".", "extractall", "(", "path", "=", "data_path", ")", "\n", "tar", ".", "close", "(", ")", "\n", "\n", "os", ".", "remove", "(", "archive_fn", ")", "\n", "\n", "", "train_batch_fns", "=", "list", "(", "map", "(", "lambda", "p", ":", "os", ".", "path", ".", "join", "(", "\n", "extracted_data_dir", ",", "p", ")", ",", "CIFAR10Data", ".", "_TRAIN_BATCH_FNS", ")", ")", "\n", "test_batch_fn", "=", "os", ".", "path", ".", "join", "(", "extracted_data_dir", ",", "\n", "CIFAR10Data", ".", "_TEST_BATCH_FN", ")", "\n", "meta_fn", "=", "os", ".", "path", ".", "join", "(", "extracted_data_dir", ",", "\n", "CIFAR10Data", ".", "_META_DATA_FN", ")", "\n", "\n", "assert", "(", "all", "(", "map", "(", "os", ".", "path", ".", "exists", ",", "train_batch_fns", ")", ")", "and", "\n", "os", ".", "path", ".", "exists", "(", "test_batch_fn", ")", "and", "os", ".", "path", ".", "exists", "(", "meta_fn", ")", ")", "\n", "\n", "self", ".", "_data", "[", "'classification'", "]", "=", "True", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'num_classes'", "]", "=", "10", "\n", "self", ".", "_data", "[", "'is_one_hot'", "]", "=", "use_one_hot", "\n", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "32", ",", "32", ",", "3", "]", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "10", "if", "use_one_hot", "else", "1", "]", "\n", "\n", "# Fill the remaining _data fields with the information read from", "\n", "# the downloaded files.", "\n", "self", ".", "_read_meta", "(", "meta_fn", ")", "\n", "self", ".", "_read_batches", "(", "train_batch_fns", ",", "test_batch_fn", ",", "validation_size", ")", "\n", "\n", "# As the time advantage are minimal compared to the huge storage", "\n", "# requirements, we don't safe the data as pickle file.", "\n", "## Save read dataset to allow faster reading in future.", "\n", "#with open(dump_fn, 'wb') as f:", "\n", "#    pickle.dump(self._data, f)", "\n", "\n", "# Initialize PyTorch data augmentation.", "\n", "", "self", ".", "_augment_inputs", "=", "False", "\n", "if", "use_data_augmentation", ":", "\n", "            ", "self", ".", "_augment_inputs", "=", "True", "\n", "self", ".", "_train_transform", ",", "self", ".", "_test_transform", "=", "CIFAR10Data", ".", "torch_input_transforms", "(", ")", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'Elapsed time to read dataset: %f sec'", "%", "(", "end", "-", "start", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._read_meta": [[175, 198], ["dict", "open", "_pickle.load"], "methods", ["None"], ["", "def", "_read_meta", "(", "self", ",", "filename", ")", ":", "\n", "        ", "\"\"\"Read the meta data file.\n\n        This method will add an additional field to the ``_data`` attribute\n        named \"cifar10\". This dictionary will be filled with two members:\n\n            - \"label_names\": The names of the associated categorical class\n                labels.\n            - \"num_cases_per_batch\": The number of samples in each batch.\n\n        Args:\n            filename: The path to the meta data file.\n        \"\"\"", "\n", "with", "open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "meta_data", "=", "pickle", ".", "load", "(", "f", ",", "encoding", "=", "'UTF-8'", ")", "\n", "\n", "", "assert", "(", "meta_data", "[", "'num_vis'", "]", "==", "32", "*", "32", "*", "3", ")", "\n", "\n", "self", ".", "_data", "[", "'cifar10'", "]", "=", "dict", "(", ")", "\n", "\n", "self", ".", "_data", "[", "'cifar10'", "]", "[", "'label_names'", "]", "=", "meta_data", "[", "'label_names'", "]", "\n", "self", ".", "_data", "[", "'cifar10'", "]", "[", "'num_cases_per_batch'", "]", "=", "meta_data", "[", "'num_cases_per_batch'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._read_batches": [[199, 273], ["numpy.array", "enumerate", "numpy.arange", "numpy.concatenate", "numpy.reshape", "numpy.concatenate", "numpy.reshape", "numpy.rollaxis", "numpy.reshape", "open", "_pickle.load", "numpy.array", "numpy.arange", "numpy.arange", "numpy.arange", "cifar10_data.CIFAR10Data._to_one_hot", "open", "_pickle.load", "numpy.concatenate", "numpy.concatenate"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot"], ["", "def", "_read_batches", "(", "self", ",", "train_fns", ",", "test_fn", ",", "validation_size", ")", ":", "\n", "        ", "\"\"\"Read all batches from files.\n\n        The method fills the remaining mandatory fields of the _data attribute,\n        that have not been set yet in the constructor.\n\n        The images are converted to match the output shape (32, 32, 3) and\n        scaled to have values between 0 and 1. For labels, the correct encoding\n        is enforced.\n\n        Args:\n            train_fns: The filepaths of the different training batches (files\n                are assumed to be in order).\n            test_fn: Filepath of the test batch.\n            validation_size: Number of validation samples.\n        \"\"\"", "\n", "with", "open", "(", "test_fn", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "test_batch", "=", "pickle", ".", "load", "(", "f", ",", "encoding", "=", "'bytes'", ")", "\n", "\n", "# Note, that we ignore the two keys: \"batch_label\" and \"filenames\".", "\n", "", "test_labels", "=", "np", ".", "array", "(", "test_batch", "[", "'labels'", ".", "encode", "(", ")", "]", ")", "\n", "test_samples", "=", "test_batch", "[", "'data'", ".", "encode", "(", ")", "]", "\n", "\n", "# Read training batches.", "\n", "for", "i", ",", "fn", "in", "enumerate", "(", "train_fns", ")", ":", "\n", "            ", "with", "open", "(", "fn", ",", "'rb'", ")", "as", "f", ":", "\n", "                ", "curr_batch", "=", "pickle", ".", "load", "(", "f", ",", "encoding", "=", "'bytes'", ")", "\n", "\n", "", "curr_labels", "=", "np", ".", "array", "(", "curr_batch", "[", "'labels'", ".", "encode", "(", ")", "]", ")", "\n", "curr_samples", "=", "curr_batch", "[", "'data'", ".", "encode", "(", ")", "]", "\n", "\n", "if", "i", "==", "0", ":", "\n", "                ", "train_labels", "=", "curr_labels", "\n", "train_samples", "=", "curr_samples", "\n", "", "else", ":", "\n", "                ", "train_labels", "=", "np", ".", "concatenate", "(", "(", "train_labels", ",", "curr_labels", ")", ")", "\n", "train_samples", "=", "np", ".", "concatenate", "(", "(", "train_samples", ",", "curr_samples", ")", ",", "\n", "axis", "=", "0", ")", "\n", "\n", "", "", "if", "validation_size", ">", "0", ":", "\n", "            ", "assert", "(", "validation_size", "<", "train_labels", ".", "shape", "[", "0", "]", ")", "\n", "val_inds", "=", "np", ".", "arange", "(", "validation_size", ")", "\n", "train_inds", "=", "np", ".", "arange", "(", "validation_size", ",", "train_labels", ".", "size", ")", "\n", "\n", "", "else", ":", "\n", "            ", "train_inds", "=", "np", ".", "arange", "(", "train_labels", ".", "size", ")", "\n", "\n", "", "test_inds", "=", "np", ".", "arange", "(", "train_labels", ".", "size", ",", "\n", "train_labels", ".", "size", "+", "test_labels", ".", "size", ")", "\n", "\n", "labels", "=", "np", ".", "concatenate", "(", "[", "train_labels", ",", "test_labels", "]", ")", "\n", "labels", "=", "np", ".", "reshape", "(", "labels", ",", "(", "-", "1", ",", "1", ")", ")", "\n", "\n", "images", "=", "np", ".", "concatenate", "(", "[", "train_samples", ",", "test_samples", "]", ",", "axis", "=", "0", ")", "\n", "\n", "# Note, images are currently encoded in a way, that there shape", "\n", "# corresponds to (3, 32, 32). For consistency reasons, we would like to", "\n", "# change that to (32, 32, 3).", "\n", "images", "=", "np", ".", "reshape", "(", "images", ",", "(", "-", "1", ",", "3", ",", "32", ",", "32", ")", ")", "\n", "images", "=", "np", ".", "rollaxis", "(", "images", ",", "1", ",", "4", ")", "\n", "images", "=", "np", ".", "reshape", "(", "images", ",", "(", "-", "1", ",", "32", "*", "32", "*", "3", ")", ")", "\n", "# Scale images into a range between 0 and 1.", "\n", "images", "=", "images", "/", "255", "\n", "\n", "self", ".", "_data", "[", "'in_data'", "]", "=", "images", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "train_inds", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "test_inds", "\n", "if", "validation_size", ">", "0", ":", "\n", "            ", "self", ".", "_data", "[", "'val_inds'", "]", "=", "val_inds", "\n", "\n", "", "if", "self", ".", "_data", "[", "'is_one_hot'", "]", ":", "\n", "            ", "labels", "=", "self", ".", "_to_one_hot", "(", "labels", ")", "\n", "\n", "", "self", ".", "_data", "[", "'out_data'", "]", "=", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._get_batch_identifier": [[274, 283], ["None"], "methods", ["None"], ["", "def", "_get_batch_identifier", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return the identifier of the batch a given sample is drawn from.\n\n        Batches 1 to 5 are the training batches. Batch 6 is the test batch.\n\n        Args:\n            index: The sample index (row index) in _data['in_data'].\n        \"\"\"", "\n", "return", "index", "%", "10000", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data.plot_sample": [[284, 323], ["warnings.warn", "matplotlib.figure", "matplotlib.axis", "matplotlib.imshow", "matplotlib.title", "matplotlib.title", "matplotlib.ion", "numpy.reshape", "matplotlib.savefig", "matplotlib.show"], "methods", ["None"], ["", "def", "plot_sample", "(", "self", ",", "image", ",", "label", "=", "None", ",", "figsize", "=", "1.5", ",", "interactive", "=", "False", ",", "\n", "file_name", "=", "None", ")", ":", "\n", "        ", "\"\"\"Plot a single CIFAR-10 sample.\n\n        This method is thought to be helpful for evaluation and debugging\n        purposes.\n\n        .. deprecated:: 1.0\n            Please use method :meth:`data.dataset.Dataset.plot_samples` instead.\n\n        Args:\n            image: A single CIFAR-10 image (given as 1D vector).\n            label: The label of the given image.\n            figsize: The height and width of the displayed image.\n            interactive: Turn on interactive mode. Thus program will run in\n                background while figure is displayed. The figure will be\n                displayed until another one is displayed, the user closes it or\n                the program has terminated. If this option is deactivated, the\n                program will freeze until the user closes the figure.\n            file_name: (optional) If a file name is provided, then the image\n                will be written into a file instead of plotted to the screen.\n        \"\"\"", "\n", "warn", "(", "'Please use method \"plot_samples\" instead.'", ",", "DeprecationWarning", ")", "\n", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "figsize", ",", "figsize", ")", ")", "\n", "\n", "if", "label", "is", "None", ":", "\n", "            ", "plt", ".", "title", "(", "\"CIFAR-10 Sample\"", ")", "\n", "", "else", ":", "\n", "            ", "label_name", "=", "self", ".", "_data", "[", "'cifar10'", "]", "[", "'label_names'", "]", "[", "label", "]", "\n", "plt", ".", "title", "(", "'Label of shown sample: %s (%d)'", "%", "(", "label_name", ",", "label", ")", ")", "\n", "", "plt", ".", "axis", "(", "'off'", ")", "\n", "if", "interactive", ":", "\n", "            ", "plt", ".", "ion", "(", ")", "\n", "", "plt", ".", "imshow", "(", "np", ".", "reshape", "(", "image", ",", "self", ".", "in_shape", ")", ")", "\n", "if", "file_name", "is", "not", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "file_name", ",", "bbox_inches", "=", "'tight'", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data.get_identifier": [[324, 327], ["None"], "methods", ["None"], ["", "", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'CIFAR-10'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data.input_to_torch_tensor": [[328, 360], ["cifar10_data.CIFAR10Data.torch_augment_images", "data.dataset.Dataset.input_to_torch_tensor", "ValueError"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data.torch_augment_images", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor"], ["", "def", "input_to_torch_tensor", "(", "self", ",", "x", ",", "device", ",", "mode", "=", "'inference'", ",", "\n", "force_no_preprocessing", "=", "False", ")", ":", "\n", "        ", "\"\"\"This method can be used to map the internal numpy arrays to PyTorch\n        tensors.\n\n        Note, this method has been overwritten from the base class.\n\n        The input images are preprocessed if data augmentation is enabled.\n        Preprocessing involves normalization and (for training mode) random\n        perturbations.\n\n        Args:\n            (....): See docstring of method\n                :meth:`data.dataset.Dataset.input_to_torch_tensor`.\n\n        Returns:\n            (torch.Tensor): The given input ``x`` as PyTorch tensor.\n        \"\"\"", "\n", "if", "self", ".", "_augment_inputs", "and", "not", "force_no_preprocessing", ":", "\n", "            ", "if", "mode", "==", "'inference'", ":", "\n", "                ", "transform", "=", "self", ".", "_test_transform", "\n", "", "elif", "mode", "==", "'train'", ":", "\n", "                ", "transform", "=", "self", ".", "_train_transform", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'\"%s\" not a valid value for argument \"mode\".'", "\n", "%", "mode", ")", "\n", "\n", "", "return", "CIFAR10Data", ".", "torch_augment_images", "(", "x", ",", "device", ",", "transform", ")", "\n", "\n", "", "else", ":", "\n", "            ", "return", "Dataset", ".", "input_to_torch_tensor", "(", "self", ",", "x", ",", "device", ",", "\n", "mode", "=", "mode", ",", "force_no_preprocessing", "=", "force_no_preprocessing", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._plot_sample": [[361, 402], ["matplotlib.Subplot", "matplotlib.Subplot.set_axis_off", "matplotlib.Subplot.imshow", "fig.add_subplot", "matplotlib.Subplot.set_title", "numpy.asscalar", "numpy.squeeze", "matplotlib.Subplot", "matplotlib.Subplot.set_title", "matplotlib.Subplot.bar", "matplotlib.Subplot.set_xticks", "fig.add_subplot", "numpy.size", "matplotlib.Subplot.set_title", "matplotlib.Subplot.set_title", "numpy.reshape", "range", "numpy.squeeze", "range", "bars[].set_color", "numpy.size", "numpy.argmax", "numpy.asscalar", "int"], "methods", ["None"], ["", "", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract method\n        :meth:`data.dataset.Dataset._plot_sample`.\n        \"\"\"", "\n", "ax", "=", "plt", ".", "Subplot", "(", "fig", ",", "inner_grid", "[", "0", "]", ")", "\n", "\n", "if", "outputs", "is", "None", ":", "\n", "            ", "ax", ".", "set_title", "(", "\"CIFAR-10 Sample\"", ")", "\n", "", "else", ":", "\n", "            ", "assert", "(", "np", ".", "size", "(", "outputs", ")", "==", "1", ")", "\n", "label", "=", "np", ".", "asscalar", "(", "outputs", ")", "\n", "label_name", "=", "self", ".", "_data", "[", "'cifar10'", "]", "[", "'label_names'", "]", "[", "label", "]", "\n", "\n", "if", "predictions", "is", "None", ":", "\n", "                ", "ax", ".", "set_title", "(", "'Label of shown sample:\\n%s (%d)'", "%", "(", "label_name", ",", "label", ")", ")", "\n", "", "else", ":", "\n", "                ", "if", "np", ".", "size", "(", "predictions", ")", "==", "self", ".", "num_classes", ":", "\n", "                    ", "pred_label", "=", "np", ".", "argmax", "(", "predictions", ")", "\n", "", "else", ":", "\n", "                    ", "pred_label", "=", "np", ".", "asscalar", "(", "predictions", ")", "\n", "", "pred_label_name", "=", "self", ".", "_data", "[", "'cifar10'", "]", "[", "'label_names'", "]", "[", "pred_label", "]", "\n", "\n", "ax", ".", "set_title", "(", "'Label of shown sample:\\n%s (%d)'", "%", "(", "label_name", ",", "label", ")", "+", "'\\nPrediction: %s (%d)'", "%", "(", "pred_label_name", ",", "pred_label", ")", ")", "\n", "\n", "", "", "ax", ".", "set_axis_off", "(", ")", "\n", "ax", ".", "imshow", "(", "np", ".", "squeeze", "(", "np", ".", "reshape", "(", "inputs", ",", "self", ".", "in_shape", ")", ")", ")", "\n", "fig", ".", "add_subplot", "(", "ax", ")", "\n", "\n", "if", "num_inner_plots", "==", "2", ":", "\n", "            ", "ax", "=", "plt", ".", "Subplot", "(", "fig", ",", "inner_grid", "[", "1", "]", ")", "\n", "ax", ".", "set_title", "(", "'Predictions'", ")", "\n", "bars", "=", "ax", ".", "bar", "(", "range", "(", "self", ".", "num_classes", ")", ",", "np", ".", "squeeze", "(", "predictions", ")", ")", "\n", "ax", ".", "set_xticks", "(", "range", "(", "self", ".", "num_classes", ")", ")", "\n", "if", "outputs", "is", "not", "None", ":", "\n", "                ", "bars", "[", "int", "(", "label", ")", "]", ".", "set_color", "(", "'r'", ")", "\n", "", "fig", ".", "add_subplot", "(", "ax", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._plot_config": [[403, 422], ["super()._plot_config", "numpy.shape"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data._plot_config"], ["", "", "def", "_plot_config", "(", "self", ",", "inputs", ",", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Re-Implementation of method\n        :meth:`data.dataset.Dataset._plot_config`.\n\n        This method has been overriden to ensure, that there are 2 subplots,\n        in case the predictions are given.\n        \"\"\"", "\n", "plot_configs", "=", "super", "(", ")", ".", "_plot_config", "(", "inputs", ",", "outputs", "=", "outputs", ",", "\n", "predictions", "=", "predictions", ")", "\n", "\n", "if", "predictions", "is", "not", "None", "and", "np", ".", "shape", "(", "predictions", ")", "[", "1", "]", "==", "self", ".", "num_classes", ":", "\n", "            ", "plot_configs", "[", "'outer_hspace'", "]", "=", "0.6", "\n", "plot_configs", "[", "'inner_hspace'", "]", "=", "0.4", "\n", "plot_configs", "[", "'num_inner_rows'", "]", "=", "2", "\n", "#plot_configs['num_inner_cols'] = 1", "\n", "plot_configs", "[", "'num_inner_plots'", "]", "=", "2", "\n", "\n", "", "return", "plot_configs", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data.torch_input_transforms": [[423, 473], ["transforms.Normalize", "transforms.Compose", "transforms.Compose", "transforms.ToPILImage", "transforms.RandomHorizontalFlip", "transforms.RandomCrop", "transforms.ToTensor", "transforms.ToPILImage", "transforms.ToTensor"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "torch_input_transforms", "(", ")", ":", "\n", "        ", "\"\"\"Get data augmentation pipelines for CIFAR-10 inputs.\n\n        Note, the augmentation is inspired by the augmentation proposed in:\n            https://www.aiworkbox.com/lessons/augment-the-cifar10-dataset-using\\\n-the-randomhorizontalflip-and-randomcrop-transforms\n\n        Note:\n            We use the same data augmentation pipeline for CIFAR-100, as the\n            images are very similar. Here is an example where they use slightly\n            different normalization values, but we ignore this for now:\n            https://zhenye-na.github.io/2018/10/07/pytorch-resnet-cifar100.html\n\n        Returns:\n            (tuple): Tuple containing:\n\n                - **train_transform**: A transforms pipeline that applies random\n                  transformations and normalizes the image.\n                - **test_transform**: Similar to train_transform, but no random\n                  transformations are applied.\n        \"\"\"", "\n", "# Copyright 2017-2018 aiworkbox.com", "\n", "# Unfortunately, no license was visibly provided with this code.", "\n", "# Though, we note that the original license applies regarding the parts", "\n", "# of the code that have been copied from the above mentioned website (we", "\n", "# slightly modified this code).", "\n", "#", "\n", "# Note, that we use this code WITHOUT ANY WARRANTIES.", "\n", "\n", "import", "torchvision", ".", "transforms", "as", "transforms", "\n", "\n", "normalize", "=", "transforms", ".", "Normalize", "(", "mean", "=", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "std", "=", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "\n", "train_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToPILImage", "(", "'RGB'", ")", ",", "\n", "transforms", ".", "RandomHorizontalFlip", "(", ")", ",", "\n", "transforms", ".", "RandomCrop", "(", "size", "=", "[", "32", ",", "32", "]", ",", "padding", "=", "4", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "normalize", ",", "\n", "]", ")", "\n", "\n", "test_transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToPILImage", "(", "'RGB'", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "normalize", ",", "\n", "]", ")", "\n", "\n", "return", "train_transform", ",", "test_transform", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.cifar10_data.CIFAR10Data.torch_augment_images": [[474, 513], ["x.contiguous().view.contiguous().view.reshape", "stack().to", "x.contiguous().view.contiguous().view.permute", "x.contiguous().view.contiguous().view.contiguous().view", "len", "stack", "x.contiguous().view.contiguous().view.contiguous", "transform", "range"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "torch_augment_images", "(", "x", ",", "device", ",", "transform", ")", ":", "\n", "        ", "\"\"\"Augment CIFAR-10 images using a given PyTorch transformation.\n\n        Args:\n            x (numpy.ndarray): A 2D-Numpy array containing CIFAR-10 images.\n            device (torch.device or int): The PyTorch device on which the\n                resulting tensor should be.\n            transform: A :mod:`torchvision.transforms` method to modify the\n                data.\n\n        Returns:\n            (torch.Tensor): The augmented images as PyTorch tensor.\n        \"\"\"", "\n", "from", "torch", "import", "stack", "\n", "\n", "#x = torch.from_numpy(x).float()", "\n", "#x = x.view([-1, 32, 32, 3])", "\n", "#x = x.permute(0, 3, 1, 2)", "\n", "#x = torch.stack([transform(_x) for _x in x])", "\n", "#return x.to(device)", "\n", "\n", "assert", "(", "len", "(", "x", ".", "shape", ")", "==", "2", ")", "# batch size plus flattened image.", "\n", "\n", "# Transform the numpy data into a representation as expected by the", "\n", "# ToPILImage transformation.", "\n", "x", "=", "(", "x", "*", "255.0", ")", ".", "astype", "(", "'uint8'", ")", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ",", "32", ",", "32", ",", "3", ")", "\n", "\n", "x", "=", "stack", "(", "[", "transform", "(", "x", "[", "i", ",", "...", "]", ")", "for", "i", "in", "range", "(", "x", ".", "shape", "[", "0", "]", ")", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Transform tensor back to numpy shape.", "\n", "# FIXME This is a horrible solution, but at least we ensure that the", "\n", "# user gets a tensor in the same shape as always and does not have to", "\n", "# deal with cases.", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "x", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "3072", ")", "# 3072 = 32 * 32 * 3", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.celeba_data.CelebAData.__init__": [[87, 129], ["data.large_img_dataset.LargeImgDataset.__init__", "time.time", "print", "os.path.join", "os.path.join", "os.path.join", "time.time", "print", "os.path.join", "os.path.join", "os.path.exists", "FileNotFoundError", "celeba_data.CelebAData._read_dataset", "os.path.exists", "FileNotFoundError", "print", "Exception", "os.path.exists", "FileNotFoundError", "os.path.exists", "FileNotFoundError", "sys.exc_info"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.celeba_data.CelebAData._read_dataset"], ["def", "__init__", "(", "self", ",", "data_path", ",", "use_png", "=", "False", ",", "shape", "=", "None", ")", ":", "\n", "        ", "if", "use_png", ":", "\n", "            ", "imgs_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CelebAData", ".", "_PNG_IMGS", ")", "\n", "", "else", ":", "\n", "            ", "imgs_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CelebAData", ".", "_JPG_IMGS", ")", "\n", "", "super", "(", ")", ".", "__init__", "(", "imgs_path", ",", "use_png", ")", "\n", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "\n", "print", "(", "'Reading CelebA dataset ...'", ")", "\n", "\n", "# Actual data path", "\n", "root_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CelebAData", ".", "_ROOT", ")", "\n", "anno_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CelebAData", ".", "_ANNO", ")", "\n", "eval_path", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CelebAData", ".", "_EVAL", ")", "\n", "\n", "# TODO Download and extract the data.", "\n", "\n", "err_msg", "=", "'Please follow the steps described in the file '", "+", "'data/README.md to download and extract the data.'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "root_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Dataset not found in directory '", "+", "\n", "root_path", "+", "'.\\n'", "+", "err_msg", ")", "\n", "", "elif", "not", "os", ".", "path", ".", "exists", "(", "anno_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Annotations not found in directory '", "+", "\n", "anno_path", "+", "'.\\n'", "+", "err_msg", ")", "\n", "", "elif", "not", "os", ".", "path", ".", "exists", "(", "eval_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Partitioning not found in directory '", "+", "\n", "eval_path", "+", "'.\\n'", "+", "err_msg", ")", "\n", "", "elif", "not", "os", ".", "path", ".", "exists", "(", "imgs_path", ")", ":", "\n", "            ", "raise", "FileNotFoundError", "(", "'Images not found in directory '", "+", "\n", "imgs_path", "+", "'.\\n'", "+", "err_msg", ")", "\n", "\n", "", "try", ":", "\n", "            ", "self", ".", "_read_dataset", "(", "data_path", ",", "shape", "=", "shape", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "sys", ".", "exc_info", "(", ")", "[", "0", "]", ")", "\n", "raise", "Exception", "(", "'Could not read dataset. Maybe the dataset is not '", "\n", "+", "'correctly prepared.\\n'", "+", "err_msg", ")", "\n", "\n", "", "end", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "'Elapsed time to read dataset: %f sec'", "%", "(", "end", "-", "start", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.celeba_data.CelebAData._read_dataset": [[130, 285], ["dict", "collections.defaultdict", "os.path.join", "len", "len", "numpy.chararray", "numpy.empty", "os.path.join", "enumerate", "os.path.join", "range", "os.path.join", "enumerate", "numpy.asarray", "numpy.asarray", "numpy.asarray", "open", "f.readlines", "line.split", "celeba_data.CelebAData._read_dataset.to_img_path"], "methods", ["None"], ["", "def", "_read_dataset", "(", "self", ",", "data_path", ",", "shape", "=", "None", ")", ":", "\n", "        ", "\"\"\"Read the dataset into memory. Note, the images are not fetched into\n        memory, only their filenames.\n\n        Args:\n            data_path: Where is the relative location of the dataset.\n            shape (optional): The shape of the input images.\n        \"\"\"", "\n", "def", "to_img_path", "(", "filename", ")", ":", "\n", "            ", "\"\"\"The image filenames from file have to be converted, if the png\n            format is used.\n            \"\"\"", "\n", "if", "self", ".", "png_format_used", ":", "\n", "                ", "filename", "=", "os", ".", "path", ".", "splitext", "(", "filename", ")", "[", "0", "]", "+", "'.png'", "\n", "", "return", "filename", "\n", "\n", "# FIXME If we use the attributes as outputs, then this is a multi-label", "\n", "# classification task. Though, we don't capture this case in the", "\n", "# Dataset base class yet (it would destroy the current implementation", "\n", "# of one hot encodings).", "\n", "", "self", ".", "_data", "[", "'classification'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'num_classes'", "]", "=", "40", "# 40 different attributes.", "\n", "self", ".", "_data", "[", "'is_one_hot'", "]", "=", "False", "\n", "if", "shape", "is", "not", "None", ":", "\n", "            ", "assert", "(", "len", "(", "shape", ")", "==", "2", ")", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "shape", "+", "[", "3", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "218", ",", "178", ",", "3", "]", "\n", "", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "self", ".", "_data", "[", "'num_classes'", "]", "]", "\n", "\n", "self", ".", "_data", "[", "'celeba'", "]", "=", "dict", "(", ")", "\n", "\n", "# The annotations dict will contain the annotations of each image", "\n", "# except its attributes (i.e., the stuff we currently don't use).", "\n", "annotations", "=", "defaultdict", "(", "dict", ")", "\n", "\n", "## Identity", "\n", "# Read the identities. Images with the same identity show the same", "\n", "# person.", "\n", "ident_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CelebAData", ".", "_IDENTITY", ")", "\n", "with", "open", "(", "ident_fn", ")", "as", "f", ":", "\n", "            ", "ident_file", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "for", "line", "in", "ident_file", ":", "\n", "            ", "img_ident", ",", "ident", "=", "line", ".", "split", "(", ")", "\n", "img_ident", "=", "to_img_path", "(", "img_ident", ")", "\n", "annotations", "[", "img_ident", "]", "[", "'ident'", "]", "=", "int", "(", "ident", ")", "\n", "\n", "# Initialize the actual data arrays.", "\n", "", "num_imgs", "=", "len", "(", "annotations", ".", "keys", "(", ")", ")", "\n", "max_str_len", "=", "len", "(", "max", "(", "annotations", ".", "keys", "(", ")", ",", "key", "=", "len", ")", ")", "\n", "in_data", "=", "np", ".", "chararray", "(", "[", "num_imgs", ",", "1", "]", ",", "itemsize", "=", "max_str_len", ",", "\n", "unicode", "=", "True", ")", "\n", "out_data", "=", "np", ".", "empty", "(", "[", "num_imgs", ",", "self", ".", "_data", "[", "'num_classes'", "]", "]", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "## Attributes", "\n", "# Read the list of attributes. This will become the output of this", "\n", "# dataset.", "\n", "attr_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CelebAData", ".", "_ATTRIBUTES", ")", "\n", "\n", "with", "open", "(", "attr_fn", ")", "as", "f", ":", "\n", "            ", "nis", "=", "int", "(", "f", ".", "readline", "(", ")", ")", "\n", "attr_names", "=", "f", ".", "readline", "(", ")", ".", "split", "(", ")", "\n", "attribute_lines", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "assert", "(", "nis", "==", "num_imgs", ")", "\n", "assert", "(", "len", "(", "attr_names", ")", "==", "self", ".", "_data", "[", "'num_classes'", "]", ")", "\n", "self", ".", "_data", "[", "'celeba'", "]", "[", "'attr_names'", "]", "=", "attr_names", "\n", "\n", "assert", "(", "len", "(", "attribute_lines", ")", "==", "num_imgs", ")", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "attribute_lines", ")", ":", "\n", "            ", "words", "=", "line", ".", "split", "(", ")", "\n", "img_ident", "=", "to_img_path", "(", "words", "[", "0", "]", ")", "\n", "attrs", "=", "[", "int", "(", "i", ")", ">", "0", "for", "i", "in", "words", "[", "1", ":", "]", "]", "\n", "assert", "(", "len", "(", "attrs", ")", "==", "self", ".", "_data", "[", "'num_classes'", "]", ")", "\n", "\n", "# The actual index of the sample in the dataset.", "\n", "annotations", "[", "img_ident", "]", "[", "'index'", "]", "=", "i", "\n", "\n", "### Fill input and output data.", "\n", "in_data", "[", "i", ",", ":", "]", "=", "img_ident", "\n", "out_data", "[", "i", ",", ":", "]", "=", "attrs", "\n", "\n", "", "self", ".", "_data", "[", "'in_data'", "]", "=", "in_data", "\n", "self", ".", "_data", "[", "'out_data'", "]", "=", "out_data", "\n", "\n", "## Landmarks", "\n", "# Landmarks of aligned and cropped images.", "\n", "# The following landmarks are specified for each image:", "\n", "# ['lefteye', 'righteye', 'nose', 'leftmouth', 'rightmouth']", "\n", "lm_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CelebAData", ".", "_LANDMARKS", ")", "\n", "\n", "with", "open", "(", "lm_fn", ")", "as", "f", ":", "\n", "            ", "nis", "=", "int", "(", "f", ".", "readline", "(", ")", ")", "\n", "lm_names_raw", "=", "f", ".", "readline", "(", ")", ".", "split", "(", ")", "\n", "lm_lines", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "assert", "(", "nis", "==", "num_imgs", ")", "\n", "# A landmark always consists of an x and y coordinate.", "\n", "assert", "(", "len", "(", "lm_names_raw", ")", "%", "2", "==", "0", ")", "\n", "assert", "(", "len", "(", "lm_lines", ")", "==", "num_imgs", ")", "\n", "\n", "lm_names", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "lm_names_raw", ")", ",", "2", ")", ":", "\n", "            ", "assert", "(", "lm_names_raw", "[", "i", "]", ".", "endswith", "(", "'_x'", ")", "and", "lm_names_raw", "[", "i", "+", "1", "]", ".", "endswith", "(", "'_y'", ")", ")", "\n", "lm_names", ".", "append", "(", "lm_names_raw", "[", "i", "]", "[", ":", "-", "2", "]", ")", "\n", "", "self", ".", "_data", "[", "'celeba'", "]", "[", "'landmark_names'", "]", "=", "lm_names", "\n", "\n", "for", "line", "in", "lm_lines", ":", "\n", "            ", "words", "=", "line", ".", "split", "(", ")", "\n", "img_ident", "=", "to_img_path", "(", "words", "[", "0", "]", ")", "\n", "locs", "=", "[", "int", "(", "i", ")", "for", "i", "in", "words", "[", "1", ":", "]", "]", "\n", "assert", "(", "len", "(", "locs", ")", "==", "len", "(", "lm_names_raw", ")", ")", "\n", "\n", "lms", "=", "dict", "(", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "len", "(", "locs", ")", ",", "2", ")", ":", "\n", "                ", "lms", "[", "lm_names", "[", "i", "//", "2", "]", "]", "=", "(", "locs", "[", "i", "]", ",", "locs", "[", "i", "+", "1", "]", ")", "\n", "\n", "", "annotations", "[", "img_ident", "]", "[", "'landmarks'", "]", "=", "lms", "\n", "\n", "## Partitioning", "\n", "# Load partitioning (what samples belong to train (0), test (2) and", "\n", "# val (1) set?).", "\n", "", "part_fn", "=", "os", ".", "path", ".", "join", "(", "data_path", ",", "CelebAData", ".", "_PARTITIONS", ")", "\n", "with", "open", "(", "part_fn", ")", "as", "f", ":", "\n", "            ", "partitions", "=", "f", ".", "readlines", "(", ")", "\n", "\n", "", "assert", "(", "len", "(", "partitions", ")", "==", "num_imgs", ")", "\n", "\n", "train_inds", "=", "[", "]", "\n", "test_inds", "=", "[", "]", "\n", "val_inds", "=", "[", "]", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "partitions", ")", ":", "\n", "            ", "img_ident", ",", "partition", "=", "line", ".", "split", "(", ")", "\n", "img_ident", "=", "to_img_path", "(", "img_ident", ")", "\n", "partition", "=", "int", "(", "partition", ")", "\n", "\n", "assert", "(", "i", "==", "annotations", "[", "img_ident", "]", "[", "'index'", "]", ")", "\n", "\n", "if", "partition", "==", "0", ":", "\n", "                ", "train_inds", ".", "append", "(", "i", ")", "\n", "", "elif", "partition", "==", "1", ":", "\n", "                ", "val_inds", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                ", "test_inds", ".", "append", "(", "i", ")", "\n", "\n", "", "", "self", ".", "_data", "[", "'train_inds'", "]", "=", "np", ".", "asarray", "(", "train_inds", ")", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "np", ".", "asarray", "(", "test_inds", ")", "\n", "self", ".", "_data", "[", "'val_inds'", "]", "=", "np", ".", "asarray", "(", "val_inds", ")", "\n", "assert", "(", "len", "(", "train_inds", ")", "+", "len", "(", "test_inds", ")", "+", "len", "(", "val_inds", ")", "==", "num_imgs", ")", "\n", "\n", "self", ".", "_data", "[", "'celeba'", "]", "[", "'anno'", "]", "=", "annotations", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.celeba_data.CelebAData.get_attribute_names": [[286, 295], ["None"], "methods", ["None"], ["", "def", "get_attribute_names", "(", "self", ")", ":", "\n", "        ", "\"\"\"Get the names of the different attributes classified by this\n        dataset.\n\n        Returns:\n            (list): A list of attributes. The order of the list will have the\n            same order as the output labels.\n        \"\"\"", "\n", "return", "self", ".", "_data", "[", "'celeba'", "]", "[", "'attr_names'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.celeba_data.CelebAData.get_identifier": [[296, 299], ["None"], "methods", ["None"], ["", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'CelebA'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.data.celeba_data.CelebAData._plot_sample": [[300, 318], ["matplotlib.Subplot", "matplotlib.Subplot.set_title", "matplotlib.Subplot.set_axis_off", "matplotlib.Subplot.imshow", "fig.add_subplot", "celeba_data.CelebAData.read_images", "numpy.squeeze", "numpy.reshape"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.read_images"], ["", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Implementation of abstract method\n        :meth:`data.dataset.Dataset._plot_sample`.\n        \"\"\"", "\n", "ax", "=", "plt", ".", "Subplot", "(", "fig", ",", "inner_grid", "[", "0", "]", ")", "\n", "\n", "ax", ".", "set_title", "(", "\"CelebA Sample\"", ")", "\n", "# FIXME We ignore outputs and predictions for now.", "\n", "\n", "if", "inputs", ".", "size", "==", "1", ":", "\n", "            ", "img", "=", "self", ".", "read_images", "(", "inputs", ")", "\n", "", "else", ":", "\n", "            ", "img", "=", "inputs", "\n", "\n", "", "ax", ".", "set_axis_off", "(", ")", "\n", "ax", ".", "imshow", "(", "np", ".", "squeeze", "(", "np", ".", "reshape", "(", "img", ",", "self", ".", "in_shape", ")", ")", ")", "\n", "fig", ".", "add_subplot", "(", "ax", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_cifar.SplitCIFAR100Data.__init__": [[115, 198], ["range", "data.cifar100_data.CIFAR100Data.__init__", "len", "split_cifar.SplitCIFAR100Data.get_train_inputs", "split_cifar.SplitCIFAR100Data.get_test_inputs", "split_cifar.SplitCIFAR100Data.get_train_outputs", "split_cifar.SplitCIFAR100Data.get_test_outputs", "split_cifar.SplitCIFAR100Data.squeeze", "split_cifar.SplitCIFAR100Data.squeeze", "range", "numpy.arange", "numpy.concatenate", "numpy.concatenate", "print", "split_cifar.SplitCIFAR100Data._to_one_hot", "split_cifar.SplitCIFAR100Data._to_one_hot", "numpy.logical_or", "numpy.logical_or", "numpy.arange", "numpy.arange", "numpy.arange", "split_cifar.SplitCIFAR100Data.transform_outputs", "len", "str"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_mnist.SplitMNIST.transform_outputs"], ["def", "__init__", "(", "self", ",", "data_path", ",", "use_one_hot", "=", "False", ",", "validation_size", "=", "1000", ",", "\n", "use_data_augmentation", "=", "False", ",", "labels", "=", "range", "(", "0", ",", "10", ")", ",", "\n", "full_out_dim", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "data_path", ",", "use_one_hot", "=", "use_one_hot", ",", "validation_size", "=", "0", ",", "\n", "use_data_augmentation", "=", "use_data_augmentation", ")", "\n", "\n", "K", "=", "len", "(", "labels", ")", "\n", "\n", "self", ".", "_labels", "=", "labels", "\n", "\n", "train_ins", "=", "self", ".", "get_train_inputs", "(", ")", "\n", "test_ins", "=", "self", ".", "get_test_inputs", "(", ")", "\n", "\n", "train_outs", "=", "self", ".", "get_train_outputs", "(", ")", "\n", "test_outs", "=", "self", ".", "get_test_outputs", "(", ")", "\n", "\n", "# Get labels.", "\n", "if", "self", ".", "is_one_hot", ":", "\n", "            ", "train_labels", "=", "self", ".", "_to_one_hot", "(", "train_outs", ",", "reverse", "=", "True", ")", "\n", "test_labels", "=", "self", ".", "_to_one_hot", "(", "test_outs", ",", "reverse", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "train_labels", "=", "train_outs", "\n", "test_labels", "=", "test_outs", "\n", "\n", "", "train_labels", "=", "train_labels", ".", "squeeze", "(", ")", "\n", "test_labels", "=", "test_labels", ".", "squeeze", "(", ")", "\n", "\n", "train_mask", "=", "train_labels", "==", "labels", "[", "0", "]", "\n", "test_mask", "=", "test_labels", "==", "labels", "[", "0", "]", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "            ", "train_mask", "=", "np", ".", "logical_or", "(", "train_mask", ",", "train_labels", "==", "labels", "[", "k", "]", ")", "\n", "test_mask", "=", "np", ".", "logical_or", "(", "test_mask", ",", "test_labels", "==", "labels", "[", "k", "]", ")", "\n", "\n", "", "train_ins", "=", "train_ins", "[", "train_mask", ",", ":", "]", "\n", "test_ins", "=", "test_ins", "[", "test_mask", ",", ":", "]", "\n", "\n", "train_outs", "=", "train_outs", "[", "train_mask", ",", ":", "]", "\n", "test_outs", "=", "test_outs", "[", "test_mask", ",", ":", "]", "\n", "\n", "if", "validation_size", ">", "0", ":", "\n", "            ", "assert", "(", "validation_size", "<", "train_outs", ".", "shape", "[", "0", "]", ")", "\n", "val_inds", "=", "np", ".", "arange", "(", "validation_size", ")", "\n", "train_inds", "=", "np", ".", "arange", "(", "validation_size", ",", "train_outs", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "else", ":", "\n", "            ", "train_inds", "=", "np", ".", "arange", "(", "train_outs", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "test_inds", "=", "np", ".", "arange", "(", "train_outs", ".", "shape", "[", "0", "]", ",", "\n", "train_outs", ".", "shape", "[", "0", "]", "+", "test_outs", ".", "shape", "[", "0", "]", ")", "\n", "\n", "outputs", "=", "np", ".", "concatenate", "(", "[", "train_outs", ",", "test_outs", "]", ",", "axis", "=", "0", ")", "\n", "\n", "if", "not", "full_out_dim", ":", "\n", "            ", "outputs", "=", "self", ".", "transform_outputs", "(", "outputs", ")", "\n", "# Note, we may also have to adapt the output shape appropriately.", "\n", "if", "self", ".", "is_one_hot", ":", "\n", "                ", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "len", "(", "labels", ")", "]", "\n", "\n", "", "", "images", "=", "np", ".", "concatenate", "(", "[", "train_ins", ",", "test_ins", "]", ",", "axis", "=", "0", ")", "\n", "\n", "### Overwrite internal data structure. Only keep desired labels.", "\n", "\n", "# Note, we continue to pretend to be a 100 class problem, such that", "\n", "# the user has easy access to the correct labels and has the original", "\n", "# 1-hot encodings.", "\n", "if", "not", "full_out_dim", ":", "\n", "            ", "self", ".", "_data", "[", "'num_classes'", "]", "=", "10", "\n", "", "else", ":", "\n", "            ", "self", ".", "_data", "[", "'num_classes'", "]", "=", "100", "\n", "", "self", ".", "_data", "[", "'in_data'", "]", "=", "images", "\n", "self", ".", "_data", "[", "'out_data'", "]", "=", "outputs", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "train_inds", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "test_inds", "\n", "if", "validation_size", ">", "0", ":", "\n", "            ", "self", ".", "_data", "[", "'val_inds'", "]", "=", "val_inds", "\n", "\n", "", "n_val", "=", "0", "\n", "if", "validation_size", ">", "0", ":", "\n", "            ", "n_val", "=", "val_inds", ".", "size", "\n", "\n", "", "print", "(", "'Created SplitCIFAR task with labels %s and %d train, %d test '", "\n", "%", "(", "str", "(", "labels", ")", ",", "train_inds", ".", "size", ",", "test_inds", ".", "size", ")", "+", "\n", "'and %d val samples.'", "%", "(", "n_val", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_cifar.SplitCIFAR100Data.transform_outputs": [[199, 222], ["numpy.zeros", "outputs.copy", "enumerate"], "methods", ["None"], ["", "def", "transform_outputs", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"Transform the outputs from the 100D CIFAR100 dataset\n        into proper 10D labels.\n\n        Args:\n            outputs: 2D numpy array of outputs.\n\n        Returns:\n            2D numpy array of transformed outputs.\n        \"\"\"", "\n", "labels", "=", "self", ".", "_labels", "\n", "if", "self", ".", "is_one_hot", ":", "\n", "            ", "assert", "(", "outputs", ".", "shape", "[", "1", "]", "==", "self", ".", "_data", "[", "'num_classes'", "]", ")", "\n", "mask", "=", "np", ".", "zeros", "(", "self", ".", "_data", "[", "'num_classes'", "]", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "mask", "[", "labels", "]", "=", "True", "\n", "\n", "return", "outputs", "[", ":", ",", "mask", "]", "\n", "", "else", ":", "\n", "            ", "assert", "(", "outputs", ".", "shape", "[", "1", "]", "==", "1", ")", "\n", "ret", "=", "outputs", ".", "copy", "(", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "labels", ")", ":", "\n", "                ", "ret", "[", "ret", "==", "l", "]", "=", "i", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_cifar.SplitCIFAR100Data.get_identifier": [[223, 226], ["None"], "methods", ["None"], ["", "", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'SplitCIFAR100'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_cifar.get_split_CIFAR_handlers": [[37, 43], ["NotImplementedError"], "function", ["None"], ["def", "get_split_CIFAR_handlers", "(", "data_path", ",", "use_one_hot", "=", "True", ",", "validation_size", "=", "0", ",", "\n", "use_data_augmentation", "=", "False", ")", ":", "\n", "    ", "\"\"\"Function has been removed. Use :func:`get_split_cifar_handlers` instead.\n    \"\"\"", "\n", "raise", "NotImplementedError", "(", "'Function has been removed. Use function '", "+", "\n", "'\"get_split_cifar_handlers\" instead.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_cifar.get_split_cifar_handlers": [[44, 91], ["print", "handlers.append", "range", "print", "data.cifar10_data.CIFAR10Data", "handlers.append", "split_cifar.SplitCIFAR100Data", "range"], "function", ["None"], ["", "def", "get_split_cifar_handlers", "(", "data_path", ",", "use_one_hot", "=", "True", ",", "validation_size", "=", "0", ",", "\n", "use_data_augmentation", "=", "False", ",", "num_tasks", "=", "6", ")", ":", "\n", "    ", "\"\"\"This method will combine 1 object of the class\n    :class:`data.cifar10_data.CIFAR10Data` and 5 objects of the class\n    :class:`SplitCIFAR100Data`.\n\n    The SplitCIFAR benchmark consists of 6 tasks, corresponding to the images\n    in CIFAR-10 and 5 tasks from CIFAR-100 corresponding to the images with\n    labels [0-10], [10-20], [20-30], [30-40], [40-50].\n\n    Args:\n        data_path: Where should the CIFAR-10 and CIFAR-100 datasets\n            be read from? If not existing, the datasets will be downloaded\n            into this folder.\n        use_one_hot (bool): Whether the class labels should be represented in a\n            one-hot encoding.\n        validation_size: The size of the validation set of each individual\n            data handler.\n        use_data_augmentation (optional): Note, this option currently only\n            applies to input batches that are transformed using the class\n            member :meth:`data.dataset.Dataset.input_to_torch_tensor`\n            (hence, **only available for PyTorch**).\n        num_tasks (int): A number between 1 and 11, specifying the number of\n            data handlers to be returned. If ``num_tasks=6``, then there will be\n            the CIFAR-10 data handler and the first 5 splits of the CIFAR-100\n            dataset (as in the usual CIFAR benchmark for CL).\n\n    Returns:\n        (list) A list of data handlers. The first being an instance of class\n        :class:`data.cifar10_data.CIFAR10Data` and the remaining ones being an\n        instance of class :class:`SplitCIFAR100Data`.\n    \"\"\"", "\n", "assert", "(", "num_tasks", ">=", "1", "and", "num_tasks", "<=", "11", ")", "\n", "print", "(", "'Creating data handlers for SplitCIFAR tasks ...'", ")", "\n", "\n", "handlers", "=", "[", "]", "\n", "handlers", ".", "append", "(", "CIFAR10Data", "(", "data_path", ",", "use_one_hot", "=", "use_one_hot", ",", "\n", "validation_size", "=", "validation_size", ",", "\n", "use_data_augmentation", "=", "use_data_augmentation", ")", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "(", "num_tasks", "-", "1", ")", "*", "10", ",", "10", ")", ":", "\n", "        ", "handlers", ".", "append", "(", "SplitCIFAR100Data", "(", "data_path", ",", "\n", "use_one_hot", "=", "use_one_hot", ",", "validation_size", "=", "validation_size", ",", "\n", "use_data_augmentation", "=", "use_data_augmentation", ",", "labels", "=", "range", "(", "i", ",", "i", "+", "10", ")", ")", ")", "\n", "\n", "", "print", "(", "'Creating data handlers for SplitCIFAR tasks ... Done'", ")", "\n", "\n", "return", "handlers", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.__init__": [[112, 169], ["numpy.array", "data.dataset.Dataset.__init__", "numpy.random.RandomState.multivariate_normal", "numpy.random.RandomState.multivariate_normal", "numpy.vstack", "numpy.vstack", "numpy.arange", "numpy.arange", "numpy.eye", "numpy.random.RandomState", "map_function", "map_function", "map_function", "map_function", "scipy.stats.multivariate_normal.pdf().reshape", "scipy.stats.multivariate_normal.pdf"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["\n", "def", "__init__", "(", "self", ",", "mean", "=", "np", ".", "array", "(", "[", "0", ",", "0", "]", ")", ",", "cov", "=", "0.05", "**", "2", "*", "np", ".", "eye", "(", "2", ")", ",", "\n", "num_train", "=", "100", ",", "num_test", "=", "100", ",", "map_function", "=", "None", ",", "rseed", "=", "None", ")", ":", "\n", "        ", "\"\"\"Generate a new dataset.\n\n        The input data x for train and test samples will be drawn iid from the\n        given Gaussian. Per default, the map function is the probability\n        density of the given Gaussian: y = f(x) = p(x).\n\n        Args:\n            mean: The mean of the Gaussian.\n            cov: The covariance of the Gaussian.\n            num_train: Number of training samples.\n            num_test: Number of test samples.\n            map_function (optional): A function handle that receives input\n                samples and maps them to output samples. If not specified, the\n                density function will be used as map function.\n            rseed: If None, the current random state of numpy is used to\n                generate the data. Otherwise, a new random state with the given\n                seed is generated.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "warn", "(", "'Please use class '", "+", "\n", "'\"data.special.gaussian_mixture_data.GaussianData\" instead.'", ",", "\n", "DeprecationWarning", ")", "\n", "\n", "if", "rseed", "is", "None", ":", "\n", "            ", "rand", "=", "np", ".", "random", "\n", "", "else", ":", "\n", "            ", "rand", "=", "np", ".", "random", ".", "RandomState", "(", "rseed", ")", "\n", "\n", "", "n_x", "=", "mean", ".", "size", "\n", "assert", "(", "n_x", "==", "2", ")", "# Only required when using plotting functions.", "\n", "\n", "train_x", "=", "rand", ".", "multivariate_normal", "(", "mean", ",", "cov", ",", "size", "=", "num_train", ")", "\n", "test_x", "=", "rand", ".", "multivariate_normal", "(", "mean", ",", "cov", ",", "size", "=", "num_test", ")", "\n", "\n", "if", "map_function", "is", "None", ":", "\n", "            ", "map_function", "=", "lambda", "x", ":", "multivariate_normal", ".", "pdf", "(", "x", ",", "mean", ",", "cov", ")", ".", "reshape", "(", "-", "1", ",", "1", ")", "\n", "\n", "# f(x) = p(x)", "\n", "train_y", "=", "map_function", "(", "train_x", ")", "\n", "test_y", "=", "map_function", "(", "test_x", ")", "\n", "", "else", ":", "\n", "            ", "train_y", "=", "map_function", "(", "train_x", ")", "\n", "test_y", "=", "map_function", "(", "test_x", ")", "\n", "\n", "# Specify internal data structure.", "\n", "", "self", ".", "_data", "[", "'classification'", "]", "=", "False", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.mean": [[170, 174], ["None"], "methods", ["None"], ["self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'in_data'", "]", "=", "np", ".", "vstack", "(", "[", "train_x", ",", "test_x", "]", ")", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "n_x", "]", "\n", "self", ".", "_data", "[", "'out_data'", "]", "=", "np", ".", "vstack", "(", "[", "train_y", ",", "test_y", "]", ")", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "1", "]", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.cov": [[175, 179], ["None"], "methods", ["None"], ["self", ".", "_data", "[", "'train_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ")", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ",", "num_train", "+", "num_test", ")", "\n", "\n", "self", ".", "_mean", "=", "mean", "\n", "self", ".", "_cov", "=", "cov", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.get_identifier": [[180, 183], ["None"], "methods", ["None"], ["self", ".", "_map", "=", "map_function", "\n", "\n", "", "@", "property", "\n", "def", "mean", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.plot_samples": [[184, 242], ["matplotlib.figure", "matplotlib.title", "gaussian_mixture_data.GaussianData._get_function_vals", "matplotlib.contourf", "matplotlib.colorbar", "matplotlib.legend", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.ion", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.savefig", "matplotlib.show", "matplotlib.contourf.cmap", "matplotlib.contourf.cmap", "matplotlib.contourf.norm", "matplotlib.contourf.norm", "outputs.squeeze", "predictions.squeeze"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals"], ["        ", "\"\"\"Getter for read-only attribute mean.\"\"\"", "\n", "return", "self", ".", "_mean", "\n", "\n", "", "@", "property", "\n", "def", "cov", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for read-only attribute cov.\"\"\"", "\n", "return", "self", ".", "_cov", "\n", "\n", "", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'GaussianInputData'", "\n", "\n", "", "def", "plot_samples", "(", "self", ",", "title", ",", "inputs", ",", "outputs", "=", "None", ",", "predictions", "=", "None", ",", "\n", "num_samples_per_row", "=", "4", ",", "show", "=", "True", ",", "filename", "=", "None", ",", "\n", "interactive", "=", "False", ",", "figsize", "=", "(", "10", ",", "6", ")", ")", ":", "\n", "        ", "\"\"\"Plot samples belonging to this dataset.\n\n        Note, either \"outputs\" or \"predictions\" must be not None!\n\n        Args:\n            title: The title of the whole figure.\n            inputs: A 2D numpy array, where each row is an input sample.\n            outputs (optional): A 2D numpy array of actual dataset targets.\n            predictions (optional): A 2D numpy array of predicted output\n                samples (i.e., output predicted by a neural network).\n            num_samples_per_row: Maximum number of samples plotted\n                per row in the generated figure.\n            show: Whether the plot should be shown.\n            filename (optional): If provided, the figure will be stored under\n                this filename.\n            interactive: Turn on interactive mode. We mainly\n                use this option to ensure that the program will run in\n                background while figure is displayed. The figure will be\n                displayed until another one is displayed, the user closes it or\n                the program has terminated. If this option is deactivated, the\n                program will freeze until the user closes the figure.\n                Note, if using the iPython inline backend, this option has no\n                effect.\n            figsize: A tuple, determining the size of the\n                figure in inches.\n        \"\"\"", "\n", "assert", "(", "outputs", "is", "not", "None", "or", "predictions", "is", "not", "None", ")", "\n", "\n", "plt", ".", "figure", "(", "figsize", "=", "figsize", ")", "\n", "plt", ".", "title", "(", "title", ",", "size", "=", "20", ")", "\n", "if", "interactive", ":", "\n", "            ", "plt", ".", "ion", "(", ")", "\n", "\n", "", "X1", ",", "X2", ",", "Y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "f", "=", "plt", ".", "contourf", "(", "X1", ",", "X2", ",", "Y", ")", "\n", "plt", ".", "colorbar", "(", "f", ")", "\n", "\n", "if", "outputs", "is", "not", "None", ":", "\n", "            ", "plt", ".", "scatter", "(", "inputs", "[", ":", ",", "0", "]", ",", "inputs", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'b'", ",", "\n", "label", "=", "'Targets'", ",", "\n", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "outputs", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "", "if", "predictions", "is", "not", "None", ":", "\n", "            ", "plt", ".", "scatter", "(", "inputs", "[", ":", ",", "0", "]", ",", "inputs", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'r'", ",", "\n", "label", "=", "'Predictions'", ",", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData._plot_sample": [[243, 250], ["NotImplementedError"], "methods", ["None"], ["facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "predictions", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "xlabel", "(", "'x1'", ")", "\n", "plt", ".", "ylabel", "(", "'x2'", ")", "\n", "\n", "if", "filename", "is", "not", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "filename", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.plot_dataset": [[251, 284], ["matplotlib.subplots", "gaussian_mixture_data.GaussianData.get_train_inputs", "gaussian_mixture_data.GaussianData.get_train_outputs().squeeze", "gaussian_mixture_data.GaussianData.get_test_inputs", "gaussian_mixture_data.GaussianData.get_test_outputs().squeeze", "gaussian_mixture_data.GaussianData._get_function_vals", "matplotlib.contourf", "matplotlib.colorbar", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.legend", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "gaussian_mixture_data.GaussianData.get_train_outputs", "gaussian_mixture_data.GaussianData.get_test_outputs", "matplotlib.contourf.cmap", "matplotlib.contourf.cmap", "matplotlib.contourf.norm", "matplotlib.contourf.norm"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs"], ["", "if", "show", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n", "", "", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Not implemented\"\"\"", "\n", "# We overwrote the plot_samples method, so there is no need to ever call", "\n", "# this method (it's just here because the baseclass requires its", "\n", "# existence).", "\n", "raise", "NotImplementedError", "(", "'TODO implement'", ")", "\n", "\n", "", "def", "plot_dataset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Plot the whole dataset.\"\"\"", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", "\n", "train_y", "=", "self", ".", "get_train_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", "\n", "test_y", "=", "self", ".", "get_test_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "X1", ",", "X2", ",", "Y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "heatmap", "=", "plt", ".", "contourf", "(", "X1", ",", "X2", ",", "Y", ")", "\n", "plt", ".", "colorbar", "(", "heatmap", ")", "\n", "\n", "#plt.scatter(train_x[:, 0], train_x[:, 1], edgecolors='r', label='Train',", "\n", "#            facecolors='none')", "\n", "#plt.scatter(test_x[:, 0], test_x[:, 1], edgecolors='b', label='Test',", "\n", "#            facecolors='none')", "\n", "\n", "# In case outputs might be noisy, we draw facecolors to match the", "\n", "# output value rather than drawing circles with no fill.", "\n", "plt", ".", "scatter", "(", "train_x", "[", ":", ",", "0", "]", ",", "train_x", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'r'", ",", "label", "=", "'train'", ",", "\n", "facecolor", "=", "heatmap", ".", "cmap", "(", "heatmap", ".", "norm", "(", "train_y", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData._get_function_vals": [[285, 316], ["gaussian_mixture_data.GaussianData.get_train_inputs", "gaussian_mixture_data.GaussianData.get_test_inputs", "max", "numpy.linspace", "numpy.linspace", "numpy.meshgrid", "gaussian_mixture_data.GaussianData._map().reshape", "numpy.abs().max", "numpy.abs().max", "numpy.vstack", "gaussian_mixture_data.GaussianData._map", "numpy.abs", "numpy.abs", "X1.ravel", "X2.ravel"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs"], ["plt", ".", "scatter", "(", "test_x", "[", ":", ",", "0", "]", ",", "test_x", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'b'", ",", "label", "=", "'test'", ",", "\n", "facecolor", "=", "heatmap", ".", "cmap", "(", "heatmap", ".", "norm", "(", "test_y", ")", ")", ")", "\n", "\n", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "title", "(", "'Gaussian Input Dataset'", ")", "\n", "plt", ".", "xlabel", "(", "'x1'", ")", "\n", "plt", ".", "ylabel", "(", "'x2'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "return", "fig", "\n", "\n", "", "def", "_get_function_vals", "(", "self", ",", "grid_size", "=", "100", ")", ":", "\n", "        ", "\"\"\"Get real function values for a grid of equidistant x values in a\n        range that covers the test and training data. These values can be used\n        to plot the ground truth function.\n\n        Args:\n            grid_size: Width (or height) of the quadratic grid.\n\n        Returns:\n            X1, X2, Y: Three numpy arrays (2d) containing the corresponding x\n                and y values. X1 and X2 follow the np.meshgrid definition.\n        \"\"\"", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", "\n", "\n", "mu", "=", "self", ".", "_mean", "\n", "\n", "dx", "=", "max", "(", "np", ".", "abs", "(", "train_x", "-", "mu", "[", "None", ",", ":", "]", ")", ".", "max", "(", ")", ",", "\n", "np", ".", "abs", "(", "test_x", "-", "mu", "[", "None", ",", ":", "]", ")", ".", "max", "(", ")", ")", "\n", "dx", "=", "1.05", "*", "dx", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.plot_predictions": [[317, 352], ["gaussian_mixture_data.GaussianData.get_train_inputs", "gaussian_mixture_data.GaussianData.get_train_outputs().squeeze", "gaussian_mixture_data.GaussianData.get_test_inputs", "gaussian_mixture_data.GaussianData.get_test_outputs().squeeze", "gaussian_mixture_data.GaussianData._get_function_vals", "matplotlib.contourf", "matplotlib.colorbar", "matplotlib.scatter", "matplotlib.legend", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "matplotlib.scatter", "matplotlib.scatter", "gaussian_mixture_data.GaussianData.get_train_outputs", "gaussian_mixture_data.GaussianData.get_test_outputs", "matplotlib.contourf.cmap", "matplotlib.contourf.cmap", "matplotlib.contourf.cmap", "matplotlib.contourf.norm", "matplotlib.contourf.norm", "matplotlib.contourf.norm", "predictions[].squeeze", "gaussian_mixture_data.GaussianData.squeeze", "gaussian_mixture_data.GaussianData.squeeze"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs"], ["x1", "=", "np", ".", "linspace", "(", "start", "=", "mu", "[", "0", "]", "-", "dx", ",", "stop", "=", "mu", "[", "0", "]", "+", "dx", ",", "num", "=", "grid_size", ")", "\n", "x2", "=", "np", ".", "linspace", "(", "start", "=", "mu", "[", "1", "]", "-", "dx", ",", "stop", "=", "mu", "[", "1", "]", "+", "dx", ",", "num", "=", "grid_size", ")", "\n", "\n", "X1", ",", "X2", "=", "np", ".", "meshgrid", "(", "x1", ",", "x2", ")", "\n", "\n", "X", "=", "np", ".", "vstack", "(", "[", "X1", ".", "ravel", "(", ")", ",", "X2", ".", "ravel", "(", ")", "]", ")", ".", "T", "\n", "\n", "Y", "=", "self", ".", "_map", "(", "X", ")", ".", "reshape", "(", "X1", ".", "shape", ")", "\n", "\n", "return", "X1", ",", "X2", ",", "Y", "\n", "\n", "", "def", "plot_predictions", "(", "self", ",", "predictions", ",", "label", "=", "'Pred'", ",", "show_train", "=", "True", ",", "\n", "show_test", "=", "True", ")", ":", "\n", "        ", "\"\"\"Plot the dataset as well as predictions.\n\n        Args:\n            predictions: A tuple of x and y values, where the y values are\n                computed by a trained regression network. Note, that x is\n                supposed to be 2D numpy array, whereas y is a 1D numpy array.\n            label: Label of the predicted values as shown in the legend.\n            show_train: Show train samples.\n            show_test: Show test samples.\n        \"\"\"", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", "\n", "train_y", "=", "self", ".", "get_train_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", "\n", "test_y", "=", "self", ".", "get_test_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "X1", ",", "X2", ",", "Y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "f", "=", "plt", ".", "contourf", "(", "X1", ",", "X2", ",", "Y", ")", "\n", "plt", ".", "colorbar", "(", "f", ")", "\n", "\n", "if", "show_train", ":", "\n", "            ", "plt", ".", "scatter", "(", "train_x", "[", ":", ",", "0", "]", ",", "train_x", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'r'", ",", "\n", "label", "=", "'Train'", ",", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "train_y", ".", "squeeze", "(", ")", ")", ")", ")", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.GaussianData.plot_datasets": [[353, 475], ["len", "matplotlib.subplots", "matplotlib.title", "numpy.zeros", "numpy.zeros", "range", "min_x.min.min.min", "max_x.max.max.max", "numpy.linspace", "numpy.linspace", "numpy.meshgrid", "scipy.spatial.cKDTree", "scipy.spatial.cKDTree.query", "numpy.empty", "range", "Y.reshape.reshape.reshape", "matplotlib.contourf", "matplotlib.colorbar", "matplotlib.pyplot.cm.rainbow", "enumerate", "matplotlib.legend", "matplotlib.xlabel", "matplotlib.ylabel", "data.get_train_inputs", "data.get_test_inputs", "max", "numpy.vstack", "data_handlers[]._map().squeeze", "numpy.linspace", "matplotlib.savefig", "matplotlib.show", "numpy.abs().max", "numpy.abs().max", "matplotlib.scatter", "phandlers.append", "plabels.append", "len", "len", "len", "X1.ravel", "X2.ravel", "data_handlers[]._map", "numpy.abs", "numpy.abs", "matplotlib.contourf.cmap", "matplotlib.contourf.norm", "predictions[].squeeze"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs"], ["", "if", "show_test", ":", "\n", "            ", "plt", ".", "scatter", "(", "test_x", "[", ":", ",", "0", "]", ",", "test_x", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'b'", ",", "\n", "label", "=", "'Test'", ",", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "test_y", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "", "plt", ".", "scatter", "(", "predictions", "[", "0", "]", "[", ":", ",", "0", "]", ",", "predictions", "[", "0", "]", "[", ":", ",", "1", "]", ",", "edgecolors", "=", "'g'", ",", "\n", "label", "=", "label", ",", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "predictions", "[", "1", "]", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "title", "(", "'Gaussian Input Dataset'", ")", "\n", "plt", ".", "xlabel", "(", "'x1'", ")", "\n", "plt", ".", "ylabel", "(", "'x2'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "plot_datasets", "(", "data_handlers", ",", "inputs", "=", "None", ",", "predictions", "=", "None", ",", "labels", "=", "None", ",", "\n", "show", "=", "True", ",", "filename", "=", "None", ",", "figsize", "=", "(", "10", ",", "6", ")", ")", ":", "\n", "        ", "\"\"\"Plot several datasets of this class in one plot.\n\n        Args:\n            data_handlers: A list of GaussianData objects.\n            inputs (optional): A list of numpy arrays representing inputs for\n                each dataset.\n            predictions (optional): A list of numpy arrays containing the\n                predicted output values for the given input values.\n            labels (optional): A label for each dataset.\n            show: Whether the plot should be shown.\n            filename (optional): If provided, the figure will be stored under\n                this filename.\n            figsize: A tuple, determining the size of the\n                figure in inches.\n        \"\"\"", "\n", "n", "=", "len", "(", "data_handlers", ")", "\n", "assert", "(", "(", "inputs", "is", "None", "and", "predictions", "is", "None", ")", "or", "(", "inputs", "is", "not", "None", "and", "predictions", "is", "not", "None", ")", ")", "\n", "assert", "(", "(", "inputs", "is", "None", "or", "len", "(", "inputs", ")", "==", "n", ")", "and", "(", "predictions", "is", "None", "or", "len", "(", "predictions", ")", "==", "n", ")", "and", "(", "labels", "is", "None", "or", "len", "(", "labels", ")", "==", "n", ")", ")", "\n", "\n", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "figsize", "=", "figsize", ")", "\n", "#plt.figure(figsize=figsize)", "\n", "plt", ".", "title", "(", "'GaussianMixture tasks'", ",", "size", "=", "20", ")", "\n", "\n", "# We need to produce a heatmap that spans all tasks.", "\n", "min_x", "=", "np", ".", "zeros", "(", "(", "2", ",", "n", ")", ")", "\n", "max_x", "=", "np", ".", "zeros", "(", "(", "2", ",", "n", ")", ")", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "data", "=", "data_handlers", "[", "i", "]", "\n", "\n", "train_x", "=", "data", ".", "get_train_inputs", "(", ")", "\n", "test_x", "=", "data", ".", "get_test_inputs", "(", ")", "\n", "mu", "=", "data", ".", "_mean", "\n", "\n", "#dx = np.abs(np.vstack([train_x, test_x]) - mu[None, :]).max(axis=0)", "\n", "dx", "=", "max", "(", "np", ".", "abs", "(", "train_x", "-", "mu", "[", "None", ",", ":", "]", ")", ".", "max", "(", ")", ",", "\n", "np", ".", "abs", "(", "test_x", "-", "mu", "[", "None", ",", ":", "]", ")", ".", "max", "(", ")", ")", "\n", "\n", "min_x", "[", ":", ",", "i", "]", "=", "mu", "-", "dx", "\n", "max_x", "[", ":", ",", "i", "]", "=", "mu", "+", "dx", "\n", "\n", "", "min_x", "=", "min_x", ".", "min", "(", "axis", "=", "1", ")", "\n", "max_x", "=", "max_x", ".", "max", "(", "axis", "=", "1", ")", "\n", "\n", "slack", "=", "(", "max_x", "-", "min_x", ")", "*", "0.02", "\n", "min_x", "-=", "slack", "\n", "max_x", "+=", "slack", "\n", "\n", "grid_size", "=", "1000", "\n", "x1", "=", "np", ".", "linspace", "(", "start", "=", "min_x", "[", "0", "]", ",", "stop", "=", "max_x", "[", "0", "]", ",", "num", "=", "grid_size", ")", "\n", "x2", "=", "np", ".", "linspace", "(", "start", "=", "min_x", "[", "1", "]", ",", "stop", "=", "max_x", "[", "1", "]", ",", "num", "=", "grid_size", ")", "\n", "\n", "X1", ",", "X2", "=", "np", ".", "meshgrid", "(", "x1", ",", "x2", ")", "\n", "X", "=", "np", ".", "vstack", "(", "[", "X1", ".", "ravel", "(", ")", ",", "X2", ".", "ravel", "(", ")", "]", ")", ".", "T", "\n", "\n", "# Problem: Now that we have the underlying X mesh, how do we compute the", "\n", "# heatmap. Since every Gaussian has full support, we can't draw a", "\n", "# heatmap that displays all tasks with their correct Y value.", "\n", "# One options would be to just add all heat maps. For small variances", "\n", "# this would look \"almost\" correct.", "\n", "# Another option is to compute Voronoi cells for all tasks and compute", "\n", "# at each mesh position the y value corresponding to the task with the", "\n", "# nearest mean.", "\n", "\n", "# We decide here to compute y based on the nearest neighor, as this", "\n", "# seems to be the \"most correct\" plotting option.", "\n", "\n", "means", "=", "[", "d", ".", "_mean", "for", "d", "in", "data_handlers", "]", "\n", "\n", "# Plot Voronoi diagram for debugging.", "\n", "#from scipy.spatial import Voronoi, voronoi_plot_2d", "\n", "#vor = Voronoi(means)", "\n", "#voronoi_plot_2d(vor)", "\n", "\n", "vor_tree", "=", "cKDTree", "(", "means", ")", "\n", "_", ",", "minds", "=", "vor_tree", ".", "query", "(", "X", ")", "\n", "\n", "Y", "=", "np", ".", "empty", "(", "X", ".", "shape", "[", "0", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "mask", "=", "minds", "==", "i", "\n", "Y", "[", "mask", "]", "=", "data_handlers", "[", "i", "]", ".", "_map", "(", "X", "[", "mask", ",", ":", "]", ")", ".", "squeeze", "(", ")", "\n", "\n", "", "Y", "=", "Y", ".", "reshape", "(", "X1", ".", "shape", ")", "\n", "\n", "f", "=", "plt", ".", "contourf", "(", "X1", ",", "X2", ",", "Y", ")", "\n", "plt", ".", "colorbar", "(", "f", ")", "\n", "\n", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "n", ")", ")", "\n", "\n", "phandlers", "=", "[", "]", "\n", "plabels", "=", "[", "]", "\n", "\n", "for", "i", ",", "data", "in", "enumerate", "(", "data_handlers", ")", ":", "\n", "            ", "if", "labels", "is", "not", "None", ":", "\n", "                ", "lbl", "=", "labels", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "lbl", "=", "'Predictions %d'", "%", "i", "\n", "\n", "", "if", "inputs", "is", "not", "None", ":", "\n", "                ", "p", "=", "plt", ".", "scatter", "(", "inputs", "[", "i", "]", "[", ":", ",", "0", "]", ",", "inputs", "[", "i", "]", "[", ":", ",", "1", "]", ",", "\n", "edgecolors", "=", "colors", "[", "i", "]", ",", "\n", "facecolor", "=", "f", ".", "cmap", "(", "f", ".", "norm", "(", "predictions", "[", "i", "]", ".", "squeeze", "(", ")", ")", ")", ")", "\n", "phandlers", ".", "append", "(", "p", ")", "\n", "plabels", ".", "append", "(", "lbl", ")", "\n", "\n", "", "", "plt", ".", "legend", "(", "phandlers", ",", "plabels", ")", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.gaussian_mixture_data.get_gmm_tasks": [[68, 99], ["range", "len", "len", "len", "ret.append", "len", "len", "len", "gaussian_mixture_data.GaussianData"], "function", ["None"], ["\n", "warn", "(", "'Please use function '", "+", "\n", "'\"data.special.gaussian_mixture_data.get_gmm_tasks\" instead.'", ",", "\n", "DeprecationWarning", ")", "\n", "\n", "assert", "(", "len", "(", "means", ")", "==", "len", "(", "covs", ")", ")", "\n", "\n", "if", "map_functions", "is", "None", ":", "\n", "        ", "map_functions", "=", "[", "None", "]", "*", "len", "(", "means", ")", "\n", "", "else", ":", "\n", "        ", "assert", "(", "len", "(", "map_functions", ")", "==", "len", "(", "means", ")", ")", "\n", "\n", "", "ret", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "means", ")", ")", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNISTList.__init__": [[103, 114], ["print", "permuted_mnist.PermutedMNIST", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "permutations", ",", "data_path", ",", "use_one_hot", "=", "True", ",", "\n", "validation_size", "=", "0", ",", "padding", "=", "0", ",", "show_perm_change_msg", "=", "True", ")", ":", "\n", "        ", "print", "(", "'Loading MNIST into memory, that is shared among %d permutation '", "\n", "%", "(", "len", "(", "permutations", ")", ")", "+", "'tasks.'", ")", "\n", "\n", "self", ".", "_data", "=", "PermutedMNIST", "(", "data_path", ",", "use_one_hot", "=", "use_one_hot", ",", "\n", "validation_size", "=", "validation_size", ",", "permutation", "=", "None", ",", "padding", "=", "padding", ")", "\n", "\n", "self", ".", "_permutations", "=", "permutations", "\n", "\n", "self", ".", "_show_perm_change_msg", "=", "show_perm_change_msg", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNISTList.__len__": [[115, 118], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Number of tasks.\"\"\"", "\n", "return", "len", "(", "self", ".", "_permutations", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNISTList.__getitem__": [[119, 167], ["isinstance", "isinstance", "permuted_mnist.PermutedMNISTList._data.reset_batch_generator", "copy.copy", "print", "list", "print", "range", "index.indices", "len"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.reset_batch_generator"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return the underlying data object with the index'th permutation.\n\n        Args:\n            index: Index of task for which data should be returned.\n\n        Return:\n            The data loader for task ``index``.\n        \"\"\"", "\n", "### User Warning ###", "\n", "color_start", "=", "'\\033[93m'", "\n", "color_end", "=", "'\\033[0m'", "\n", "help_msg", "=", "'To disable this message, disable the flag '", "+", "'\"show_perm_change_msg\" when calling the constructor of class '", "+", "'classifier.permuted_mnist.PermutedMNISTList.'", "\n", "####################", "\n", "\n", "if", "isinstance", "(", "index", ",", "slice", ")", ":", "\n", "            ", "new_list", "=", "copy", ".", "copy", "(", "self", ")", "\n", "new_list", ".", "_permutations", "=", "self", ".", "_permutations", "[", "index", "]", "\n", "\n", "### User Warning ###", "\n", "if", "self", ".", "_show_perm_change_msg", ":", "\n", "                ", "indices", "=", "list", "(", "range", "(", "*", "index", ".", "indices", "(", "len", "(", "self", ")", ")", ")", ")", "\n", "print", "(", "color_start", "+", "'classifier.permuted_mnist.'", "+", "\n", "'PermutedMNISTList: A slice of permutations with '", "+", "\n", "'indices %s has been created. '", "%", "indices", "+", "\n", "'The applied permutation has not changed! '", "+", "color_end", "+", "\n", "help_msg", ")", "\n", "####################", "\n", "\n", "", "return", "new_list", "\n", "\n", "", "assert", "(", "isinstance", "(", "index", ",", "int", ")", ")", "\n", "self", ".", "_data", ".", "permutation", "=", "self", ".", "_permutations", "[", "index", "]", "\n", "self", ".", "_data", ".", "reset_batch_generator", "(", ")", "\n", "\n", "### User Warning ###", "\n", "if", "self", ".", "_show_perm_change_msg", ":", "\n", "            ", "color_start", "=", "'\\033[93m'", "\n", "color_end", "=", "'\\033[0m'", "\n", "\n", "print", "(", "color_start", "+", "'classifier.permuted_mnist.PermutedMNISTList:'", "+", "\n", "' Data permutation has been changed to %d. '", "%", "index", "+", "\n", "color_end", "+", "help_msg", ")", "\n", "####################", "\n", "\n", "", "return", "self", ".", "_data", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNISTList.__setitem__": [[168, 171], ["NotImplementedError"], "methods", ["None"], ["", "def", "__setitem__", "(", "self", ",", "key", ",", "value", ")", ":", "\n", "        ", "\"\"\"Not implemented.\"\"\"", "\n", "raise", "NotImplementedError", "(", "'Not yet implemented!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNISTList.__delitem__": [[172, 175], ["NotImplementedError"], "methods", ["None"], ["", "def", "__delitem__", "(", "self", ",", "key", ")", ":", "\n", "        ", "\"\"\"Not implemented.\"\"\"", "\n", "raise", "NotImplementedError", "(", "'Not yet implemented!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.__init__": [[215, 224], ["data.mnist_data.MNISTData.__init__"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ",", "data_path", ",", "use_one_hot", "=", "True", ",", "validation_size", "=", "0", ",", "\n", "permutation", "=", "None", ",", "padding", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "data_path", ",", "use_one_hot", "=", "use_one_hot", ",", "\n", "validation_size", "=", "validation_size", ")", "\n", "\n", "self", ".", "_padding", "=", "padding", "\n", "self", ".", "_input_dim", "=", "(", "28", "+", "padding", "*", "2", ")", "**", "2", "\n", "\n", "self", ".", "permutation", "=", "permutation", "# See setter below.", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.permutation": [[230, 236], ["permuted_mnist.PermutedMNIST.torch_input_transforms"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.torch_input_transforms"], ["", "@", "permutation", ".", "setter", "\n", "def", "permutation", "(", "self", ",", "value", ")", ":", "\n", "        ", "\"\"\"Setter for the attribute :attr:`permutation`.\"\"\"", "\n", "self", ".", "_permutation", "=", "value", "\n", "self", ".", "_transform", "=", "PermutedMNIST", ".", "torch_input_transforms", "(", "\n", "padding", "=", "self", ".", "_padding", ",", "permutation", "=", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.torch_in_shape": [[237, 242], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "torch_in_shape", "(", "self", ")", ":", "\n", "        ", "\"\"\"Getter for attribute :attr:`torch_in_shape`\"\"\"", "\n", "return", "[", "self", ".", "in_shape", "[", "0", "]", "+", "2", "*", "self", ".", "_padding", ",", "\n", "self", ".", "in_shape", "[", "1", "]", "+", "2", "*", "self", ".", "_padding", ",", "self", ".", "in_shape", "[", "2", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.get_identifier": [[243, 246], ["None"], "methods", ["None"], ["", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'PermutedMNIST'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor": [[247, 290], ["x.contiguous().view.contiguous().view.reshape", "stack().to", "x.contiguous().view.contiguous().view.permute", "x.contiguous().view.contiguous().view.contiguous().view", "data.mnist_data.MNISTData.input_to_torch_tensor", "len", "stack", "x.contiguous().view.contiguous().view.contiguous", "permuted_mnist.PermutedMNIST._transform", "range"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.input_to_torch_tensor"], ["", "def", "input_to_torch_tensor", "(", "self", ",", "x", ",", "device", ",", "mode", "=", "'inference'", ",", "\n", "force_no_preprocessing", "=", "False", ")", ":", "\n", "        ", "\"\"\"This method can be used to map the internal numpy arrays to PyTorch\n        tensors.\n\n        Note, this method has been overwritten from the base class.\n\n        It applies zero padding and pixel permutations.\n\n        Args:\n            (....): See docstring of method\n                :meth:`data.dataset.Dataset.input_to_torch_tensor`.\n\n        Returns:\n            (torch.Tensor): The given input ``x`` as PyTorch tensor.\n        \"\"\"", "\n", "if", "not", "force_no_preprocessing", ":", "\n", "            ", "assert", "(", "len", "(", "x", ".", "shape", ")", "==", "2", ")", "# batch size plus flattened image.", "\n", "\n", "from", "torch", "import", "stack", "\n", "\n", "img_size", "=", "28", "+", "2", "*", "self", ".", "_padding", "\n", "\n", "# Transform the numpy data into a representation as expected by the", "\n", "# ToPILImage transformation.", "\n", "x", "=", "(", "x", "*", "255.0", ")", ".", "astype", "(", "'uint8'", ")", "\n", "x", "=", "x", ".", "reshape", "(", "-", "1", ",", "28", ",", "28", ",", "1", ")", "\n", "\n", "x", "=", "stack", "(", "[", "self", ".", "_transform", "(", "x", "[", "i", ",", "...", "]", ")", "for", "i", "in", "\n", "range", "(", "x", ".", "shape", "[", "0", "]", ")", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "# Transform tensor back to numpy shape.", "\n", "# FIXME This is a horrible solution, but at least we ensure that the", "\n", "# user gets a tensor in the same shape as always and does not have to", "\n", "# deal with cases.", "\n", "x", "=", "x", ".", "permute", "(", "0", ",", "2", ",", "3", ",", "1", ")", "\n", "x", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "img_size", "**", "2", ")", "\n", "\n", "return", "x", "\n", "\n", "", "else", ":", "\n", "            ", "return", "MNISTData", ".", "input_to_torch_tensor", "(", "self", ",", "x", ",", "device", ",", "mode", "=", "mode", ",", "\n", "force_no_preprocessing", "=", "force_no_preprocessing", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.torch_input_transforms": [[291, 363], ["transforms.Compose", "image.view.view.size", "image.view.view.view", "image.view.view.view", "transforms.ToPILImage", "transforms.Pad", "transforms.ToTensor", "transforms.Lambda", "permuted_mnist.PermutedMNIST.torch_input_transforms._permutate_image_pixels"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "torch_input_transforms", "(", "permutation", "=", "None", ",", "padding", "=", "0", ")", ":", "\n", "        ", "\"\"\"Transform MNIST images to PyTorch tensors.\n\n        Args:\n            permutation: A given permutation that should be applied to all\n                images.\n            padding: Apply a given amount of zero padding.\n\n        Returns:\n            A transforms pipeline.\n        \"\"\"", "\n", "import", "torchvision", ".", "transforms", "as", "transforms", "\n", "\n", "# The following functions has been copied and modified from:", "\n", "#   https://git.io/fjqzP", "\n", "# Note, that a different license and copyright applies and that we use", "\n", "# this code WITHOUT ANY WARRANTIES.", "\n", "\"\"\"\n        MIT License\n\n        Copyright (c) 2018 Gido van de Ven\n\n        Permission is hereby granted, free of charge, to any person obtaining a copy\n        of this software and associated documentation files (the \"Software\"), to deal\n        in the Software without restriction, including without limitation the rights\n        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n        copies of the Software, and to permit persons to whom the Software is\n        furnished to do so, subject to the following conditions:\n\n        The above copyright notice and this permission notice shall be included in all\n        copies or substantial portions of the Software.\n\n        THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n        SOFTWARE.\n        \"\"\"", "\n", "def", "_permutate_image_pixels", "(", "image", ",", "permutation", ")", ":", "\n", "            ", "'''Permutate the pixels of an image according to 'permutation'.\n\n            Args:\n                image: 3D-tensor containing the image\n                permutation: <ndarray> of pixel-indices in their new order\n\n            Returns:\n                Permuted image.\n            '''", "\n", "\n", "if", "permutation", "is", "None", ":", "\n", "                ", "return", "image", "\n", "", "else", ":", "\n", "                ", "c", ",", "h", ",", "w", "=", "image", ".", "size", "(", ")", "\n", "image", "=", "image", ".", "view", "(", "c", ",", "-", "1", ")", "\n", "image", "=", "image", "[", ":", ",", "\n", "permutation", "]", "# --> same permutation for each channel", "\n", "image", "=", "image", ".", "view", "(", "c", ",", "h", ",", "w", ")", "\n", "\n", "", "return", "image", "\n", "\n", "", "transform", "=", "transforms", ".", "Compose", "(", "[", "\n", "transforms", ".", "ToPILImage", "(", "'L'", ")", ",", "\n", "transforms", ".", "Pad", "(", "padding", ")", ",", "\n", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "transforms", ".", "Lambda", "(", "\n", "lambda", "x", ":", "_permutate_image_pixels", "(", "x", ",", "permutation", ")", ")", ",", "\n", "]", ")", "\n", "\n", "return", "transform", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.permuted_mnist.PermutedMNIST.tf_input_map": [[364, 370], ["NotImplementedError"], "methods", ["None"], ["", "def", "tf_input_map", "(", "self", ",", "mode", "=", "'inference'", ")", ":", "\n", "        ", "\"\"\"Not implemented! The class currently does not support Tensorflow.\"\"\"", "\n", "# FIXME Permutations are applied on the fly when images are translated", "\n", "# PyTorch tensors. Internally, we store normal MNIST images.", "\n", "raise", "NotImplementedError", "(", "'No Tensorflow support for this class '", "+", "\n", "'implemented.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_mnist.SplitMNIST.__init__": [[83, 166], ["data.mnist_data.MNISTData.__init__", "len", "split_mnist.SplitMNIST.get_train_inputs", "split_mnist.SplitMNIST.get_test_inputs", "split_mnist.SplitMNIST.get_train_outputs", "split_mnist.SplitMNIST.get_test_outputs", "split_mnist.SplitMNIST.squeeze", "split_mnist.SplitMNIST.squeeze", "range", "numpy.arange", "numpy.concatenate", "numpy.concatenate", "print", "split_mnist.SplitMNIST._to_one_hot", "split_mnist.SplitMNIST._to_one_hot", "numpy.logical_or", "numpy.logical_or", "numpy.arange", "numpy.arange", "numpy.arange", "split_mnist.SplitMNIST.transform_outputs", "len", "len", "str"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset._to_one_hot", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_mnist.SplitMNIST.transform_outputs"], ["def", "__init__", "(", "self", ",", "data_path", ",", "use_one_hot", "=", "False", ",", "validation_size", "=", "1000", ",", "\n", "labels", "=", "[", "0", ",", "1", "]", ",", "full_out_dim", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "data_path", ",", "use_one_hot", "=", "use_one_hot", ",", "validation_size", "=", "0", ")", "\n", "\n", "K", "=", "len", "(", "labels", ")", "\n", "#assert(K == 2)", "\n", "\n", "self", ".", "_labels", "=", "labels", "\n", "\n", "train_ins", "=", "self", ".", "get_train_inputs", "(", ")", "\n", "test_ins", "=", "self", ".", "get_test_inputs", "(", ")", "\n", "\n", "train_outs", "=", "self", ".", "get_train_outputs", "(", ")", "\n", "test_outs", "=", "self", ".", "get_test_outputs", "(", ")", "\n", "\n", "# Get labels.", "\n", "if", "self", ".", "is_one_hot", ":", "\n", "            ", "train_labels", "=", "self", ".", "_to_one_hot", "(", "train_outs", ",", "reverse", "=", "True", ")", "\n", "test_labels", "=", "self", ".", "_to_one_hot", "(", "test_outs", ",", "reverse", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "train_labels", "=", "train_outs", "\n", "test_labels", "=", "test_outs", "\n", "\n", "", "train_labels", "=", "train_labels", ".", "squeeze", "(", ")", "\n", "test_labels", "=", "test_labels", ".", "squeeze", "(", ")", "\n", "\n", "train_mask", "=", "train_labels", "==", "labels", "[", "0", "]", "\n", "test_mask", "=", "test_labels", "==", "labels", "[", "0", "]", "\n", "for", "k", "in", "range", "(", "1", ",", "K", ")", ":", "\n", "            ", "train_mask", "=", "np", ".", "logical_or", "(", "train_mask", ",", "train_labels", "==", "labels", "[", "k", "]", ")", "\n", "test_mask", "=", "np", ".", "logical_or", "(", "test_mask", ",", "test_labels", "==", "labels", "[", "k", "]", ")", "\n", "\n", "", "train_ins", "=", "train_ins", "[", "train_mask", ",", ":", "]", "\n", "test_ins", "=", "test_ins", "[", "test_mask", ",", ":", "]", "\n", "\n", "train_outs", "=", "train_outs", "[", "train_mask", ",", ":", "]", "\n", "test_outs", "=", "test_outs", "[", "test_mask", ",", ":", "]", "\n", "\n", "if", "validation_size", ">", "0", ":", "\n", "            ", "assert", "(", "validation_size", "<", "train_outs", ".", "shape", "[", "0", "]", ")", "\n", "val_inds", "=", "np", ".", "arange", "(", "validation_size", ")", "\n", "train_inds", "=", "np", ".", "arange", "(", "validation_size", ",", "train_outs", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "else", ":", "\n", "            ", "train_inds", "=", "np", ".", "arange", "(", "train_outs", ".", "shape", "[", "0", "]", ")", "\n", "\n", "", "test_inds", "=", "np", ".", "arange", "(", "train_outs", ".", "shape", "[", "0", "]", ",", "\n", "train_outs", ".", "shape", "[", "0", "]", "+", "test_outs", ".", "shape", "[", "0", "]", ")", "\n", "\n", "outputs", "=", "np", ".", "concatenate", "(", "[", "train_outs", ",", "test_outs", "]", ",", "axis", "=", "0", ")", "\n", "\n", "if", "not", "full_out_dim", ":", "\n", "# Transform outputs, e.g., if 1-hot [0,0,0,1,0,0,0,0,0,0] -> [0,1]", "\n", "            ", "outputs", "=", "self", ".", "transform_outputs", "(", "outputs", ")", "\n", "# Note, we may also have to adapt the output shape appropriately.", "\n", "if", "self", ".", "is_one_hot", ":", "\n", "                ", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "len", "(", "labels", ")", "]", "\n", "\n", "", "", "images", "=", "np", ".", "concatenate", "(", "[", "train_ins", ",", "test_ins", "]", ",", "axis", "=", "0", ")", "\n", "\n", "### Overwrite internal data structure. Only keep desired labels.", "\n", "\n", "# Note, we continue to pretend to be a 10 class problem, such that", "\n", "# the user has easy access to the correct labels and has the original", "\n", "# 1-ho encodings.", "\n", "if", "not", "full_out_dim", ":", "\n", "            ", "self", ".", "_data", "[", "'num_classes'", "]", "=", "len", "(", "labels", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_data", "[", "'num_classes'", "]", "=", "10", "\n", "", "self", ".", "_data", "[", "'in_data'", "]", "=", "images", "\n", "self", ".", "_data", "[", "'out_data'", "]", "=", "outputs", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "train_inds", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "test_inds", "\n", "if", "validation_size", ">", "0", ":", "\n", "            ", "self", ".", "_data", "[", "'val_inds'", "]", "=", "val_inds", "\n", "\n", "", "n_val", "=", "0", "\n", "if", "validation_size", ">", "0", ":", "\n", "            ", "n_val", "=", "val_inds", ".", "size", "\n", "\n", "", "print", "(", "'Created SplitMNIST task with labels %s and %d train, %d test '", "\n", "%", "(", "str", "(", "labels", ")", ",", "train_inds", ".", "size", ",", "test_inds", ".", "size", ")", "+", "\n", "'and %d val samples.'", "%", "(", "n_val", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_mnist.SplitMNIST.transform_outputs": [[167, 197], ["numpy.zeros", "outputs.copy", "enumerate"], "methods", ["None"], ["", "def", "transform_outputs", "(", "self", ",", "outputs", ")", ":", "\n", "        ", "\"\"\"Transform the outputs from the 10D MNIST dataset into proper 2D\n        labels.\n\n        Example:\n            Split with labels [2,3]\n\n            1-hot encodings: [0,0,0,1,0,0,0,0,0,0] -> [0,1]\n\n            labels: 3 -> 1\n\n        Args:\n            outputs: 2D numpy array of outputs.\n\n        Returns:\n            2D numpy array of transformed outputs.\n        \"\"\"", "\n", "labels", "=", "self", ".", "_labels", "\n", "if", "self", ".", "is_one_hot", ":", "\n", "            ", "assert", "(", "outputs", ".", "shape", "[", "1", "]", "==", "self", ".", "_data", "[", "'num_classes'", "]", ")", "\n", "mask", "=", "np", ".", "zeros", "(", "self", ".", "_data", "[", "'num_classes'", "]", ",", "dtype", "=", "np", ".", "bool", ")", "\n", "mask", "[", "labels", "]", "=", "True", "\n", "\n", "return", "outputs", "[", ":", ",", "mask", "]", "\n", "", "else", ":", "\n", "            ", "assert", "(", "outputs", ".", "shape", "[", "1", "]", "==", "1", ")", "\n", "ret", "=", "outputs", ".", "copy", "(", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "labels", ")", ":", "\n", "                ", "ret", "[", "ret", "==", "l", "]", "=", "i", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_mnist.SplitMNIST.get_identifier": [[198, 201], ["None"], "methods", ["None"], ["", "", "def", "get_identifier", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'SplitMNIST'", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.split_mnist.get_split_MNIST_handlers": [[33, 65], ["print", "range", "print", "handlers.append", "split_mnist.SplitMNIST"], "function", ["None"], ["def", "get_split_MNIST_handlers", "(", "data_path", ",", "use_one_hot", "=", "True", ",", "validation_size", "=", "0", ",", "\n", "steps", "=", "2", ")", ":", "\n", "    ", "\"\"\"This method instantiates 5 objects of the class :class:`SplitMNIST` which\n    will contain a disjoint set of labels.\n\n    The SplitMNIST task consists of 5 tasks corresponding to the images with\n    labels [0,1], [2,3], [4,5], [6,7], [8,9].\n\n    Args:\n        data_path: Where should the MNIST dataset be read from? If not existing,\n            the dataset will be downloaded into this folder.\n        use_one_hot: Whether the class labels should be represented in a one-hot\n            encoding.\n        validation_size: The size of the validation set of each individual\n            data handler.\n        steps: Number of classes to put into one data handler. If default\n            every data handler will include 2 digits, otherwise 1.\n    Returns:\n        A list of data handlers, each corresponding to a :class:`SplitMNIST`\n        object,\n    \"\"\"", "\n", "print", "(", "'Creating data handlers for SplitMNIST tasks ...'", ")", "\n", "\n", "handlers", "=", "[", "]", "\n", "assert", "(", "steps", "==", "1", "or", "steps", "==", "2", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "10", ",", "steps", ")", ":", "\n", "        ", "handlers", ".", "append", "(", "SplitMNIST", "(", "data_path", ",", "use_one_hot", "=", "use_one_hot", ",", "\n", "validation_size", "=", "validation_size", ",", "labels", "=", "[", "i", ",", "i", "+", "steps", "-", "1", "]", ")", ")", "\n", "\n", "", "print", "(", "'Creating data handlers for SplitMNIST tasks ... Done'", ")", "\n", "\n", "return", "handlers", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__": [[47, 130], ["data.dataset.Dataset.__init__", "numpy.random.RandomState.uniform", "numpy.linspace().reshape", "map_function", "map_function", "numpy.arange", "numpy.arange", "numpy.random.RandomState", "numpy.random.RandomState.normal", "numpy.linspace().reshape", "map_function", "numpy.vstack", "numpy.vstack", "numpy.vstack", "numpy.vstack", "numpy.arange", "numpy.linspace", "numpy.linspace"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.__init__"], ["def", "__init__", "(", "self", ",", "train_inter", "=", "[", "-", "10", ",", "10", "]", ",", "num_train", "=", "20", ",", "\n", "test_inter", "=", "[", "-", "10", ",", "10", "]", ",", "num_test", "=", "80", ",", "val_inter", "=", "None", ",", "\n", "num_val", "=", "None", ",", "map_function", "=", "lambda", "x", ":", "x", ",", "std", "=", "0", ",", "rseed", "=", "None", ")", ":", "\n", "        ", "\"\"\"Generate a new dataset.\n\n        The input data x will be uniformly drawn for train samples and\n        equidistant for test samples. The user has to specify a function that\n        will map this random input data onto output samples y.\n\n        Args:\n            train_inter: A tuple, representing the interval from which x\n                samples are drawn in the training set. Note, this range will\n                apply to all input dimensions.\n            num_train: Number of training samples.\n            test_inter: A tuple, representing the interval from which x\n                samples are drawn in the test set. Note, this range will\n                apply to all input dimensions.\n            num_test: Number of test samples.\n            val_inter (optional): See parameter `test_inter`. If set, this\n                argument leads to the construction of a validation set. Note,\n                option `num_val` need to be specified as well.\n            num_val (optional): Number of validation samples.\n            map_function: A function handle that receives input\n                samples and maps them to output samples.\n            std: If not zero, Gaussian white noise with this std will be added\n                to the training outputs.\n            rseed: If None, the current random state of numpy is used to\n                   generate the data. Otherwise, a new random state with the\n                   given seed is generated.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "warn", "(", "'Please use class \"data.special.regression1d_data.ToyRegression\"'", "+", "\n", "' instead.'", ",", "DeprecationWarning", ")", "\n", "\n", "assert", "(", "val_inter", "is", "None", "and", "num_val", "is", "None", "or", "val_inter", "is", "not", "None", "and", "num_val", "is", "not", "None", ")", "\n", "\n", "if", "rseed", "is", "None", ":", "\n", "            ", "rand", "=", "np", ".", "random", "\n", "", "else", ":", "\n", "            ", "rand", "=", "np", ".", "random", ".", "RandomState", "(", "rseed", ")", "\n", "\n", "", "train_x", "=", "rand", ".", "uniform", "(", "low", "=", "train_inter", "[", "0", "]", ",", "high", "=", "train_inter", "[", "1", "]", ",", "\n", "size", "=", "(", "num_train", ",", "1", ")", ")", "\n", "test_x", "=", "np", ".", "linspace", "(", "start", "=", "test_inter", "[", "0", "]", ",", "stop", "=", "test_inter", "[", "1", "]", ",", "\n", "num", "=", "num_test", ")", ".", "reshape", "(", "(", "num_test", ",", "1", ")", ")", "\n", "\n", "train_y", "=", "map_function", "(", "train_x", ")", "\n", "test_y", "=", "map_function", "(", "test_x", ")", "\n", "\n", "# Perturb training outputs.", "\n", "if", "std", ">", "0", ":", "\n", "            ", "train_eps", "=", "rand", ".", "normal", "(", "loc", "=", "0.0", ",", "scale", "=", "std", ",", "size", "=", "(", "num_train", ",", "1", ")", ")", "\n", "train_y", "+=", "train_eps", "\n", "\n", "# Create validation data if requested.", "\n", "", "if", "num_val", "is", "not", "None", ":", "\n", "            ", "val_x", "=", "np", ".", "linspace", "(", "start", "=", "val_inter", "[", "0", "]", ",", "stop", "=", "val_inter", "[", "1", "]", ",", "\n", "num", "=", "num_val", ")", ".", "reshape", "(", "(", "num_val", ",", "1", ")", ")", "\n", "val_y", "=", "map_function", "(", "val_x", ")", "\n", "\n", "in_data", "=", "np", ".", "vstack", "(", "[", "train_x", ",", "test_x", ",", "val_x", "]", ")", "\n", "out_data", "=", "np", ".", "vstack", "(", "[", "train_y", ",", "test_y", ",", "val_y", "]", ")", "\n", "", "else", ":", "\n", "            ", "in_data", "=", "np", ".", "vstack", "(", "[", "train_x", ",", "test_x", "]", ")", "\n", "out_data", "=", "np", ".", "vstack", "(", "[", "train_y", ",", "test_y", "]", ")", "\n", "\n", "# Specify internal data structure.", "\n", "", "self", ".", "_data", "[", "'classification'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'sequence'", "]", "=", "False", "\n", "self", ".", "_data", "[", "'in_data'", "]", "=", "in_data", "\n", "self", ".", "_data", "[", "'in_shape'", "]", "=", "[", "1", "]", "\n", "self", ".", "_data", "[", "'out_data'", "]", "=", "out_data", "\n", "self", ".", "_data", "[", "'out_shape'", "]", "=", "[", "1", "]", "\n", "self", ".", "_data", "[", "'train_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ")", "\n", "self", ".", "_data", "[", "'test_inds'", "]", "=", "np", ".", "arange", "(", "num_train", ",", "num_train", "+", "num_test", ")", "\n", "\n", "if", "num_val", "is", "not", "None", ":", "\n", "            ", "n_start", "=", "num_train", "+", "num_test", "\n", "self", ".", "_data", "[", "'val_inds'", "]", "=", "np", ".", "arange", "(", "n_start", ",", "n_start", "+", "num_val", ")", "\n", "\n", "", "self", ".", "_map", "=", "map_function", "\n", "self", ".", "_train_inter", "=", "train_inter", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.train_x_range": [[131, 135], ["None"], "methods", ["None"], ["self", ".", "_test_inter", "=", "test_inter", "\n", "self", ".", "_val_inter", "=", "val_inter", "\n", "\n", "", "@", "property", "\n", "def", "train_x_range", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.test_x_range": [[136, 140], ["None"], "methods", ["None"], ["        ", "\"\"\"Getter for read-only attribute train_x_range.\"\"\"", "\n", "return", "self", ".", "_train_inter", "\n", "\n", "", "@", "property", "\n", "def", "test_x_range", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.val_x_range": [[141, 145], ["None"], "methods", ["None"], ["        ", "\"\"\"Getter for read-only attribute test_x_range.\"\"\"", "\n", "return", "self", ".", "_test_inter", "\n", "\n", "", "@", "property", "\n", "def", "val_x_range", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals": [[146, 176], ["numpy.linspace().reshape", "regression1d_data.ToyRegression._map", "min", "max", "min", "max", "numpy.linspace"], "methods", ["None"], ["        ", "\"\"\"Getter for read-only attribute val_x_range.\"\"\"", "\n", "return", "self", ".", "_val_inter", "\n", "\n", "", "def", "_get_function_vals", "(", "self", ",", "num_samples", "=", "100", ",", "x_range", "=", "None", ")", ":", "\n", "        ", "\"\"\"Get real function values for equidistant x values in a range that\n        covers the test and training data. These values can be used to plot the\n        ground truth function.\n\n        Args:\n            num_samples: Number of samples to be produced.\n            x_range: If a specific range should be used to gather function\n                values.\n\n        Returns:\n            x, y: Two numpy arrays containing the corresponding x and y values.\n        \"\"\"", "\n", "if", "x_range", "is", "None", ":", "\n", "            ", "min_x", "=", "min", "(", "self", ".", "_train_inter", "[", "0", "]", ",", "self", ".", "_test_inter", "[", "0", "]", ")", "\n", "max_x", "=", "max", "(", "self", ".", "_train_inter", "[", "1", "]", ",", "self", ".", "_test_inter", "[", "1", "]", ")", "\n", "if", "self", ".", "num_val_samples", ">", "0", ":", "\n", "                ", "min_x", "=", "min", "(", "min_x", ",", "self", ".", "_val_inter", "[", "0", "]", ")", "\n", "max_x", "=", "max", "(", "max_x", ",", "self", ".", "_val_inter", "[", "1", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "min_x", "=", "x_range", "[", "0", "]", "\n", "max_x", "=", "x_range", "[", "1", "]", "\n", "\n", "", "slack_x", "=", "0.05", "*", "(", "max_x", "-", "min_x", ")", "\n", "\n", "sample_x", "=", "np", ".", "linspace", "(", "start", "=", "min_x", "-", "slack_x", ",", "stop", "=", "max_x", "+", "slack_x", ",", "\n", "num", "=", "num_samples", ")", ".", "reshape", "(", "(", "num_samples", ",", "1", ")", ")", "\n", "sample_y", "=", "self", ".", "_map", "(", "sample_x", ")", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.plot_dataset": [[177, 213], ["regression1d_data.ToyRegression.get_train_inputs().squeeze", "regression1d_data.ToyRegression.get_train_outputs().squeeze", "regression1d_data.ToyRegression.get_test_inputs().squeeze", "regression1d_data.ToyRegression.get_test_outputs().squeeze", "regression1d_data.ToyRegression._get_function_vals", "matplotlib.locator_params", "matplotlib.locator_params", "matplotlib.plot", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.legend", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "regression1d_data.ToyRegression.get_val_inputs().squeeze", "regression1d_data.ToyRegression.get_val_outputs().squeeze", "matplotlib.scatter", "matplotlib.show", "regression1d_data.ToyRegression.get_train_inputs", "regression1d_data.ToyRegression.get_train_outputs", "regression1d_data.ToyRegression.get_test_inputs", "regression1d_data.ToyRegression.get_test_outputs", "regression1d_data.ToyRegression.get_val_inputs", "regression1d_data.ToyRegression.get_val_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_val_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_val_outputs"], ["\n", "return", "sample_x", ",", "sample_y", "\n", "\n", "", "def", "plot_dataset", "(", "self", ",", "show", "=", "True", ")", ":", "\n", "        ", "\"\"\"Plot the whole dataset.\n\n        Args:\n            show: Whether the plot should be shown.\n        \"\"\"", "\n", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "train_y", "=", "self", ".", "get_train_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "test_y", "=", "self", ".", "get_test_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "if", "self", ".", "num_val_samples", ">", "0", ":", "\n", "            ", "val_x", "=", "self", ".", "get_val_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "val_y", "=", "self", ".", "get_val_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "", "sample_x", ",", "sample_y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "\n", "# The default matplotlib setting is usually too high for most plots.", "\n", "plt", ".", "locator_params", "(", "axis", "=", "'y'", ",", "nbins", "=", "2", ")", "\n", "plt", ".", "locator_params", "(", "axis", "=", "'x'", ",", "nbins", "=", "6", ")", "\n", "\n", "plt", ".", "plot", "(", "sample_x", ",", "sample_y", ",", "color", "=", "'k'", ",", "label", "=", "'f(x)'", ",", "\n", "linestyle", "=", "'dashed'", ",", "linewidth", "=", ".5", ")", "\n", "plt", ".", "scatter", "(", "train_x", ",", "train_y", ",", "color", "=", "'r'", ",", "label", "=", "'Train'", ")", "\n", "plt", ".", "scatter", "(", "test_x", ",", "test_y", ",", "color", "=", "'b'", ",", "label", "=", "'Test'", ",", "alpha", "=", "0.8", ")", "\n", "if", "self", ".", "num_val_samples", ">", "0", ":", "\n", "            ", "plt", ".", "scatter", "(", "val_x", ",", "val_y", ",", "color", "=", "'g'", ",", "label", "=", "'Val'", ",", "alpha", "=", "0.5", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "title", "(", "'1D-Regression Dataset'", ")", "\n", "plt", ".", "xlabel", "(", "'$x$'", ")", "\n", "plt", ".", "ylabel", "(", "'$y$'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.plot_predictions": [[214, 245], ["regression1d_data.ToyRegression.get_train_inputs().squeeze", "regression1d_data.ToyRegression.get_train_outputs().squeeze", "regression1d_data.ToyRegression.get_test_inputs().squeeze", "regression1d_data.ToyRegression.get_test_outputs().squeeze", "regression1d_data.ToyRegression._get_function_vals", "matplotlib.plot", "matplotlib.scatter", "matplotlib.legend", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.show", "matplotlib.scatter", "matplotlib.scatter", "regression1d_data.ToyRegression.get_train_inputs", "regression1d_data.ToyRegression.get_train_outputs", "regression1d_data.ToyRegression.get_test_inputs", "regression1d_data.ToyRegression.get_test_outputs"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_train_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_train_outputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.large_img_dataset.LargeImgDataset.get_test_inputs", "home.repos.pwc.inspect_result.chrhenning_hypercl.data.dataset.Dataset.get_test_outputs"], ["if", "show", ":", "\n", "            ", "plt", ".", "show", "(", ")", "\n", "\n", "", "", "def", "plot_predictions", "(", "self", ",", "predictions", ",", "label", "=", "'Pred'", ",", "show_train", "=", "True", ",", "\n", "show_test", "=", "True", ")", ":", "\n", "        ", "\"\"\"Plot the dataset as well as predictions.\n\n        Args:\n            predictions: A tuple of x and y values, where the y values are\n                         computed by a trained regression network.\n                         Note, that we assume the x values to be sorted.\n            label: Label of the predicted values as shown in the legend.\n            show_train: Show train samples.\n            show_test: Show test samples.\n        \"\"\"", "\n", "train_x", "=", "self", ".", "get_train_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "train_y", "=", "self", ".", "get_train_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "test_x", "=", "self", ".", "get_test_inputs", "(", ")", ".", "squeeze", "(", ")", "\n", "test_y", "=", "self", ".", "get_test_outputs", "(", ")", ".", "squeeze", "(", ")", "\n", "\n", "sample_x", ",", "sample_y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "plt", ".", "plot", "(", "sample_x", ",", "sample_y", ",", "color", "=", "'k'", ",", "label", "=", "'f(x)'", ",", "\n", "linestyle", "=", "'dashed'", ",", "linewidth", "=", ".5", ")", "\n", "if", "show_train", ":", "\n", "            ", "plt", ".", "scatter", "(", "train_x", ",", "train_y", ",", "color", "=", "'r'", ",", "label", "=", "'Train'", ")", "\n", "", "if", "show_test", ":", "\n", "            ", "plt", ".", "scatter", "(", "test_x", ",", "test_y", ",", "color", "=", "'b'", ",", "label", "=", "'Test'", ")", "\n", "", "plt", ".", "scatter", "(", "predictions", "[", "0", "]", ",", "predictions", "[", "1", "]", ",", "color", "=", "'g'", ",", "label", "=", "label", ")", "\n", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "title", "(", "'1D-Regression Dataset'", ")", "\n", "plt", ".", "xlabel", "(", "'$x$'", ")", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.get_identifier": [[246, 249], ["None"], "methods", ["None"], ["plt", ".", "ylabel", "(", "'$y$'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "", "def", "get_identifier", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.plot_samples": [[250, 303], ["matplotlib.figure", "matplotlib.title", "regression1d_data.ToyRegression._get_function_vals", "matplotlib.plot", "matplotlib.legend", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.ion", "matplotlib.scatter", "matplotlib.scatter", "matplotlib.savefig", "matplotlib.show"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals"], ["        ", "\"\"\"Returns the name of the dataset.\"\"\"", "\n", "return", "'1DRegression'", "\n", "\n", "", "def", "plot_samples", "(", "self", ",", "title", ",", "inputs", ",", "outputs", "=", "None", ",", "predictions", "=", "None", ",", "\n", "num_samples_per_row", "=", "4", ",", "show", "=", "True", ",", "filename", "=", "None", ",", "\n", "interactive", "=", "False", ",", "figsize", "=", "(", "10", ",", "6", ")", ")", ":", "\n", "        ", "\"\"\"Plot samples belonging to this dataset.\n\n        Note, either \"outputs\" or \"predictions\" must be not None!\n\n        Args:\n            title: The title of the whole figure.\n            inputs: A 2D numpy array, where each row is an input sample.\n            outputs (optional): A 2D numpy array of actual dataset targets.\n            predictions (optional): A 2D numpy array of predicted output\n                samples (i.e., output predicted by a neural network).\n            num_samples_per_row: Maximum number of samples plotted\n                per row in the generated figure.\n            show: Whether the plot should be shown.\n            filename (optional): If provided, the figure will be stored under\n                this filename.\n            interactive: Turn on interactive mode. We mainly\n                use this option to ensure that the program will run in\n                background while figure is displayed. The figure will be\n                displayed until another one is displayed, the user closes it or\n                the program has terminated. If this option is deactivated, the\n                program will freeze until the user closes the figure.\n                Note, if using the iPython inline backend, this option has no\n                effect.\n            figsize: A tuple, determining the size of the\n                figure in inches.\n        \"\"\"", "\n", "assert", "(", "outputs", "is", "not", "None", "or", "predictions", "is", "not", "None", ")", "\n", "\n", "plt", ".", "figure", "(", "figsize", "=", "figsize", ")", "\n", "plt", ".", "title", "(", "title", ",", "size", "=", "20", ")", "\n", "if", "interactive", ":", "\n", "            ", "plt", ".", "ion", "(", ")", "\n", "\n", "", "sample_x", ",", "sample_y", "=", "self", ".", "_get_function_vals", "(", ")", "\n", "plt", ".", "plot", "(", "sample_x", ",", "sample_y", ",", "color", "=", "'k'", ",", "label", "=", "'f(x)'", ",", "\n", "linestyle", "=", "'dashed'", ",", "linewidth", "=", ".5", ")", "\n", "if", "outputs", "is", "not", "None", ":", "\n", "            ", "plt", ".", "scatter", "(", "inputs", ",", "outputs", ",", "color", "=", "'b'", ",", "label", "=", "'Targets'", ")", "\n", "", "if", "predictions", "is", "not", "None", ":", "\n", "            ", "plt", ".", "scatter", "(", "inputs", ",", "predictions", ",", "color", "=", "'r'", ",", "label", "=", "'Predictions'", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "xlabel", "(", "'$x$'", ")", "\n", "plt", ".", "ylabel", "(", "'$y$'", ")", "\n", "\n", "if", "filename", "is", "not", "None", ":", "\n", "            ", "plt", ".", "savefig", "(", "filename", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n", "", "if", "show", ":", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._plot_sample": [[304, 311], ["NotImplementedError"], "methods", ["None"], ["            ", "plt", ".", "show", "(", ")", "\n", "\n", "", "", "def", "_plot_sample", "(", "self", ",", "fig", ",", "inner_grid", ",", "num_inner_plots", ",", "ind", ",", "inputs", ",", "\n", "outputs", "=", "None", ",", "predictions", "=", "None", ")", ":", "\n", "        ", "\"\"\"Not implemented\"\"\"", "\n", "# We overwrote the plot_samples method, so there is no need to ever call", "\n", "# this method (it's just here because the baseclass requires its", "\n", "# existence).", "\n"]], "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression.plot_datasets": [[312, 412], ["len", "utils.misc.get_colorbrewer2_colors", "matplotlib.subplots", "matplotlib.title", "enumerate", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.tight_layout", "len", "warnings.warn", "matplotlib.pyplot.cm.rainbow", "data._get_function_vals", "matplotlib.plot", "phandlers.append", "plabels.append", "axes.grid", "axes.set_facecolor", "axes.axhline", "axes.axvline", "axes.tick_params", "matplotlib.legend", "matplotlib.savefig", "matplotlib.show", "len", "numpy.linspace", "matplotlib.scatter", "phandlers.append", "plabels.append", "len", "matplotlib.yticks", "matplotlib.xticks", "axes.yaxis.get_major_ticks", "axes.xaxis.get_major_ticks", "len", "len", "len", "tick.label.set_fontsize", "tick.label.set_fontsize", "axes.get_ylim", "axes.get_xlim"], "methods", ["home.repos.pwc.inspect_result.chrhenning_hypercl.utils.misc.get_colorbrewer2_colors", "home.repos.pwc.inspect_result.chrhenning_hypercl.special.regression1d_data.ToyRegression._get_function_vals"], ["raise", "NotImplementedError", "(", "'TODO implement'", ")", "\n", "\n", "", "@", "staticmethod", "\n", "def", "plot_datasets", "(", "data_handlers", ",", "inputs", "=", "None", ",", "predictions", "=", "None", ",", "labels", "=", "None", ",", "\n", "fun_xranges", "=", "None", ",", "show", "=", "True", ",", "filename", "=", "None", ",", "\n", "figsize", "=", "(", "10", ",", "6", ")", ",", "publication_style", "=", "False", ")", ":", "\n", "        ", "\"\"\"Plot several datasets of this class in one plot.\n\n        Args:\n            data_handlers: A list of ToyRegression objects.\n            inputs (optional): A list of numpy arrays representing inputs for\n                each dataset.\n            predictions (optional): A list of numpy arrays containing the\n                predicted output values for the given input values.\n            labels (optional): A label for each dataset.\n            fun_xranges (optional): List of x ranges in which the true\n                underlying function per dataset should be sketched.\n            show: Whether the plot should be shown.\n            filename (optional): If provided, the figure will be stored under\n                this filename.\n            figsize: A tuple, determining the size of the figure in inches.\n            publication_style: whether the plots should be in publication style\n        \"\"\"", "\n", "n", "=", "len", "(", "data_handlers", ")", "\n", "assert", "(", "(", "inputs", "is", "None", "and", "predictions", "is", "None", ")", "or", "(", "inputs", "is", "not", "None", "and", "predictions", "is", "not", "None", ")", ")", "\n", "assert", "(", "(", "inputs", "is", "None", "or", "len", "(", "inputs", ")", "==", "n", ")", "and", "(", "predictions", "is", "None", "or", "len", "(", "predictions", ")", "==", "n", ")", "and", "(", "labels", "is", "None", "or", "len", "(", "labels", ")", "==", "n", ")", ")", "\n", "assert", "(", "fun_xranges", "is", "None", "or", "len", "(", "fun_xranges", ")", "==", "n", ")", "\n", "\n", "# Set-up matplotlib to adhere to our graphical conventions.", "\n", "#misc.configure_matplotlib_params(fig_size=1.2*np.array([1.6, 1]),", "\n", "#                                 font_size=8)", "\n", "\n", "# Get a colorscheme from colorbrewer2.org.", "\n", "colors", "=", "misc", ".", "get_colorbrewer2_colors", "(", "family", "=", "'Dark2'", ")", "\n", "if", "n", ">", "len", "(", "colors", ")", ":", "\n", "            ", "warn", "(", "'Changing to automatic color scheme as we don\\'t have '", "+", "\n", "'as many manual colors as tasks.'", ")", "\n", "colors", "=", "cm", ".", "rainbow", "(", "np", ".", "linspace", "(", "0", ",", "1", ",", "n", ")", ")", "\n", "\n", "", "if", "publication_style", ":", "\n", "            ", "ts", ",", "lw", ",", "ms", "=", "60", ",", "15", ",", "140", "# text fontsize, line width, marker size", "\n", "figsize", "=", "(", "12", ",", "6", ")", "\n", "", "else", ":", "\n", "            ", "ts", ",", "lw", ",", "ms", "=", "12", ",", "2", ",", "15", "\n", "\n", "", "fig", ",", "axes", "=", "plt", ".", "subplots", "(", "figsize", "=", "figsize", ")", "\n", "plt", ".", "title", "(", "'1D regression'", ",", "size", "=", "ts", ",", "pad", "=", "ts", ")", "\n", "\n", "phandlers", "=", "[", "]", "\n", "plabels", "=", "[", "]", "\n", "\n", "for", "i", ",", "data", "in", "enumerate", "(", "data_handlers", ")", ":", "\n", "            ", "if", "labels", "is", "not", "None", ":", "\n", "                ", "lbl", "=", "labels", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "lbl", "=", "'Function %d'", "%", "i", "\n", "\n", "", "fun_xrange", "=", "None", "\n", "if", "fun_xranges", "is", "not", "None", ":", "\n", "                ", "fun_xrange", "=", "fun_xranges", "[", "i", "]", "\n", "", "sample_x", ",", "sample_y", "=", "data", ".", "_get_function_vals", "(", "x_range", "=", "fun_xrange", ")", "\n", "p", ",", "=", "plt", ".", "plot", "(", "sample_x", ",", "sample_y", ",", "color", "=", "colors", "[", "i", "]", ",", "\n", "linestyle", "=", "'dashed'", ",", "linewidth", "=", "lw", "/", "3", ")", "\n", "\n", "phandlers", ".", "append", "(", "p", ")", "\n", "plabels", ".", "append", "(", "lbl", ")", "\n", "if", "inputs", "is", "not", "None", ":", "\n", "                ", "p", "=", "plt", ".", "scatter", "(", "inputs", "[", "i", "]", ",", "predictions", "[", "i", "]", ",", "color", "=", "colors", "[", "i", "]", ",", "\n", "s", "=", "ms", ")", "\n", "phandlers", ".", "append", "(", "p", ")", "\n", "plabels", ".", "append", "(", "'Predictions'", ")", "\n", "\n", "", "", "if", "publication_style", ":", "\n", "            ", "axes", ".", "grid", "(", "False", ")", "\n", "axes", ".", "set_facecolor", "(", "'w'", ")", "\n", "axes", ".", "axhline", "(", "y", "=", "axes", ".", "get_ylim", "(", ")", "[", "0", "]", ",", "color", "=", "'k'", ",", "lw", "=", "lw", ")", "\n", "axes", ".", "axvline", "(", "x", "=", "axes", ".", "get_xlim", "(", ")", "[", "0", "]", ",", "color", "=", "'k'", ",", "lw", "=", "lw", ")", "\n", "if", "len", "(", "data_handlers", ")", "==", "3", ":", "\n", "                ", "plt", ".", "yticks", "(", "[", "-", "1", ",", "0", ",", "1", "]", ",", "fontsize", "=", "ts", ")", "\n", "plt", ".", "xticks", "(", "[", "-", "2.5", ",", "0", ",", "2.5", "]", ",", "fontsize", "=", "ts", ")", "\n", "", "else", ":", "\n", "                ", "for", "tick", "in", "axes", ".", "yaxis", ".", "get_major_ticks", "(", ")", ":", "\n", "                    ", "tick", ".", "label", ".", "set_fontsize", "(", "ts", ")", "\n", "", "for", "tick", "in", "axes", ".", "xaxis", ".", "get_major_ticks", "(", ")", ":", "\n", "                    ", "tick", ".", "label", ".", "set_fontsize", "(", "ts", ")", "\n", "", "", "axes", ".", "tick_params", "(", "axis", "=", "'both'", ",", "length", "=", "lw", ",", "direction", "=", "'out'", ",", "width", "=", "lw", "/", "2.", ")", "\n", "", "else", ":", "\n", "            ", "plt", ".", "legend", "(", "phandlers", ",", "plabels", ")", "\n", "\n", "", "plt", ".", "xlabel", "(", "'$x$'", ",", "fontsize", "=", "ts", ")", "\n", "plt", ".", "ylabel", "(", "'$y$'", ",", "fontsize", "=", "ts", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "\n", "if", "filename", "is", "not", "None", ":", "\n", "#plt.savefig(filename + '.pdf', bbox_inches='tight')", "\n", "            ", "plt", ".", "savefig", "(", "filename", ",", "bbox_inches", "=", "'tight'", ")", "\n", "\n", "", "if", "show", ":", "\n"]]}