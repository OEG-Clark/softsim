{"home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.log_torch_garbage": [[11, 34], ["logger.debug", "collections.Counter", "gc.get_objects", "logger.debug", "collections.Counter.most_common", "logger.debug", "torch.is_tensor", "hasattr", "logger.debug", "torch.is_tensor", "type", "obj.size", "type", "logger.debug", "type", "obj.size", "type"], "function", ["None"], ["def", "log_torch_garbage", "(", "verbose", "=", "False", ")", ":", "\n", "    ", "\"\"\"Outputs a list / summary of all tensors to the console.\"\"\"", "\n", "logger", ".", "debug", "(", "'Logging PyTorch garbage'", ")", "\n", "obj_counts", "=", "Counter", "(", ")", "\n", "for", "obj", "in", "gc", ".", "get_objects", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "if", "torch", ".", "is_tensor", "(", "obj", ")", ":", "\n", "                ", "if", "verbose", ":", "\n", "                    ", "logger", ".", "debug", "(", "'type: %s, size: %s, is_cuda: %s'", ",", "\n", "type", "(", "obj", ")", ",", "obj", ".", "size", "(", ")", ",", "obj", ".", "is_cuda", ")", "\n", "", "obj_counts", "[", "(", "type", "(", "obj", ")", ",", "obj", ".", "is_cuda", ")", "]", "+=", "1", "\n", "", "elif", "hasattr", "(", "obj", ",", "'data'", ")", ":", "\n", "                ", "if", "torch", ".", "is_tensor", "(", "obj", ".", "data", ")", ":", "\n", "                    ", "if", "verbose", ":", "\n", "                        ", "logger", ".", "debug", "(", "'type: %s, size: %s, is_cuda: %s'", ",", "\n", "type", "(", "obj", ")", ",", "obj", ".", "size", "(", ")", ",", "obj", ".", "is_cuda", ")", "\n", "", "obj_counts", "[", "(", "type", "(", "obj", ")", ",", "obj", ".", "is_cuda", ")", "]", "+=", "1", "\n", "", "", "", "except", "(", "KeyError", ",", "OSError", ",", "RuntimeError", ")", ":", "\n", "            ", "continue", "\n", "", "", "logger", ".", "debug", "(", "'Summary stats'", ")", "\n", "for", "key", ",", "count", "in", "obj_counts", ".", "most_common", "(", ")", ":", "\n", "        ", "obj_type", ",", "is_cuda", "=", "key", "\n", "logger", ".", "debug", "(", "'type: %s, is_cuda: %s, count: %i'", ",", "obj_type", ",", "is_cuda", ",", "count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.parallel_sample": [[35, 39], ["torch.multinomial", "torch.multinomial.view", "probs.view"], "function", ["None"], ["", "", "def", "parallel_sample", "(", "probs", ":", "torch", ".", "FloatTensor", ")", "->", "torch", ".", "LongTensor", ":", "\n", "    ", "*", "output_shape", ",", "n_categories", "=", "probs", ".", "shape", "\n", "samples", "=", "torch", ".", "multinomial", "(", "probs", ".", "view", "(", "-", "1", ",", "n_categories", ")", ",", "num_samples", "=", "1", ",", "replacement", "=", "True", ")", "\n", "return", "samples", ".", "view", "(", "*", "output_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.sample_from_logp": [[40, 64], ["torch.exp", "torch.cumsum", "torch.rand().unsqueeze", "torch.cumsum.lt().sum", "torch.ones", "torch.rand", "torch.cumsum.lt"], "function", ["None"], ["", "def", "sample_from_logp", "(", "logp", ":", "torch", ".", "Tensor", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "    ", "\"\"\"\n    Draws samples from a tensor of log probabilities. API matches ``torch.max()``.\n\n    Parameters\n    ----------\n    logp : ``torch.Tensor``\n        Tensor of shape ``(batch_size, ..., n_categories)`` of log probabilities.\n\n    Returns\n    -------\n    A tuple consisting of:\n    selected_logp : ``torch.Tensor``\n        Tensor of shape ``(batch_size, ...)`` containing the selected log probabilities.\n    selected_idx : ``torch.Tensor``\n        Tensor of shape ``(batch_size, ...)`` containing the selected indices.\n    \"\"\"", "\n", "pdf", "=", "torch", ".", "exp", "(", "logp", ")", "\n", "cdf", "=", "torch", ".", "cumsum", "(", "pdf", ",", "dim", "=", "-", "1", ")", "\n", "rng", "=", "torch", ".", "rand", "(", "logp", ".", "shape", "[", ":", "-", "1", "]", ",", "device", "=", "logp", ".", "device", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "selected_idx", "=", "cdf", ".", "lt", "(", "rng", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "\n", "hack", "=", "torch", ".", "ones", "(", "logp", ".", "shape", "[", ":", "-", "1", "]", ",", "device", "=", "logp", ".", "device", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "selected_logp", "=", "logp", "[", "hack", ",", "selected_idx", "[", "hack", "]", "]", "\n", "return", "selected_logp", ",", "selected_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.nested_enumerate": [[66, 74], ["enumerate", "util.nested_enumerate"], "function", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.nested_enumerate"], ["", "def", "nested_enumerate", "(", "iterable", ")", ":", "\n", "    ", "try", ":", "\n", "        ", "for", "i", ",", "element", "in", "enumerate", "(", "iterable", ")", ":", "\n", "            ", "for", "item", "in", "nested_enumerate", "(", "element", ")", ":", "\n", "                ", "combo", "=", "i", ",", "*", "item", "\n", "yield", "combo", "\n", "", "", "", "except", "TypeError", ":", "\n", "        ", "yield", "(", "iterable", ",", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.kglm.KglmPredictor.predict_instance": [[19, 26], ["torch.no_grad", "allennlp.common.util.sanitize", "kglm.KglmPredictor._model.forward_on_instance"], "methods", ["None"], ["    ", "def", "predict_instance", "(", "self", ",", "instance", ")", "->", "JsonDict", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instance", "(", "instance", ")", "\n", "", "except", "RuntimeError", ":", "\n", "                ", "outputs", "=", "{", "'ERROR'", ":", "'Too big'", "}", "\n", "", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.kglm.KglmPredictor.predict_batch_instance": [[27, 34], ["torch.no_grad", "allennlp.common.util.sanitize", "kglm.KglmPredictor._model.forward_on_instances"], "methods", ["None"], ["", "", "def", "predict_batch_instance", "(", "self", ",", "instances", ":", "List", "[", "Instance", "]", ")", "->", "List", "[", "JsonDict", "]", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "outputs", "=", "self", ".", "_model", ".", "forward_on_instances", "(", "instances", ")", "\n", "", "except", "RuntimeError", ":", "\n", "                ", "outputs", "=", "{", "'ERROR'", ":", "'Too big'", "}", "\n", "", "return", "sanitize", "(", "outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.kglm.KglmPredictor._json_to_instance": [[35, 138], ["kglm.KglmPredictor._dataset_reader.text_to_instance", "kglm.data.fields.SequentialArrayField", "kglm.KglmPredictor.add_field", "kglm.data.dataset_readers.enhanced_wikitext._flatten", "kglm.data.dataset_readers.enhanced_wikitext._flatten", "numpy.zeros", "numpy.zeros", "numpy.array", "len", "enumerate", "len", "len", "len", "len", "numpy.zeros", "kglm.data.dataset_readers.enhanced_wikitext.normalize_entity_id", "tuple", "range", "kglm.data.dataset_readers.enhanced_wikitext.normalize_entity_id", "len", "len", "len", "shortlist.append", "len", "len", "kglm.KglmPredictor._dataset_reader._alias_database.token_to_uid", "len"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._flatten", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._flatten", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.normalize_entity_id", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.normalize_entity_id", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.token_to_uid"], ["", "", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "# pylint: disable=protected-access", "\n", "\n", "# Extract tokens and EOS offset", "\n", "        ", "tokens", "=", "[", "x", "+", "[", "'@@END@@'", "]", "for", "x", "in", "json_dict", "[", "'tokens'", "]", "[", "1", ":", "-", "1", "]", "]", "\n", "eos_offset", "=", "[", "[", "i", "]", "*", "len", "(", "x", ")", "for", "i", ",", "x", "in", "enumerate", "(", "tokens", ")", "]", "\n", "tokens", "=", "[", "'@@START@@'", "]", "+", "_flatten", "(", "tokens", ")", "\n", "eos_offset", "=", "[", "0", "]", "+", "_flatten", "(", "eos_offset", ")", "\n", "source", "=", "tokens", "[", ":", "-", "1", "]", "\n", "if", "self", ".", "_dataset_reader", ".", "_mode", "==", "'generative'", ":", "\n", "            ", "target", "=", "tokens", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "target", "=", "None", "\n", "\n", "# Process annotations", "\n", "", "if", "'annotations'", "not", "in", "json_dict", ":", "\n", "            ", "shortlist", "=", "None", "\n", "reverse_shortlist", "=", "None", "\n", "raw_entity_ids", "=", "None", "\n", "entity_ids", "=", "None", "\n", "relations", "=", "None", "\n", "parent_ids", "=", "None", "\n", "shortlist_inds", "=", "None", "\n", "mention_type", "=", "None", "\n", "", "else", ":", "\n", "# We maintain a \"shortlist\" of observed entities, that is used for baseline models", "\n", "# that only select entities from the set that appear in the document (as opposed to", "\n", "# the set of all possible entities).", "\n", "            ", "shortlist", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "\n", "reverse_shortlist", "=", "{", "DEFAULT_PADDING_TOKEN", ":", "0", "}", "\n", "raw_entity_ids", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "*", "len", "(", "source", ")", "\n", "entity_ids", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "*", "len", "(", "source", ")", "\n", "relations", "=", "[", "[", "DEFAULT_PADDING_TOKEN", "]", "]", "*", "len", "(", "source", ")", "\n", "parent_ids", "=", "[", "[", "DEFAULT_PADDING_TOKEN", "]", "]", "*", "len", "(", "source", ")", "\n", "shortlist_inds", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "source", ")", ",", ")", ")", "\n", "mention_type", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "source", ")", ",", ")", ")", "\n", "\n", "if", "self", ".", "_dataset_reader", ".", "_mode", "==", "\"generative\"", ":", "\n", "                ", "alias_copy_inds", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "source", ")", ",", ")", ")", "\n", "", "else", ":", "\n", "                ", "alias_copy_inds", "=", "None", "\n", "\n", "# Process annotations", "\n", "", "for", "annotation", "in", "json_dict", "[", "'annotations'", "]", ":", "\n", "\n", "# Obtain the entity identifier for the annotated span", "\n", "                ", "raw_entity_id", "=", "annotation", "[", "'id'", "]", "\n", "raw_parent_id", "=", "annotation", "[", "'parent_id'", "]", "\n", "entity_id", "=", "normalize_entity_id", "(", "raw_entity_id", ")", "\n", "if", "entity_id", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "parent_id", "=", "[", "normalize_entity_id", "(", "x", ")", "for", "x", "in", "raw_parent_id", "]", "\n", "assert", "len", "(", "parent_id", ")", "==", "len", "(", "raw_parent_id", ")", "\n", "relation", "=", "annotation", "[", "'relation'", "]", "\n", "new_entity", "=", "relation", "==", "[", "'@@NEW@@'", "]", "\n", "\n", "# If neccessary, update the shortlist. Obtain the index of the entity identifier in", "\n", "# the shortlist.", "\n", "if", "entity_id", "not", "in", "reverse_shortlist", ":", "\n", "                    ", "reverse_shortlist", "[", "entity_id", "]", "=", "len", "(", "reverse_shortlist", ")", "\n", "shortlist", ".", "append", "(", "entity_id", ")", "\n", "", "shortlist_ind", "=", "reverse_shortlist", "[", "entity_id", "]", "\n", "\n", "# Update the outputs", "\n", "# Offset is 0 in generative case, since each timestep is for predicting", "\n", "# attributes of the next token. In the discriminative case, each timestep", "\n", "# is for predicting attributes of the current token.", "\n", "mode_offset", "=", "-", "1", "if", "self", ".", "_dataset_reader", ".", "_mode", "==", "\"generative\"", "else", "0", "\n", "span", "=", "annotation", "[", "'span'", "]", "\n", "eos_offset_adjusted_span", "=", "tuple", "(", "i", "+", "eos_offset", "[", "i", "]", "for", "i", "in", "span", ")", "\n", "for", "i", "in", "range", "(", "*", "eos_offset_adjusted_span", ")", ":", "\n", "                    ", "raw_entity_ids", "[", "i", "+", "mode_offset", "]", "=", "raw_entity_id", "\n", "entity_ids", "[", "i", "+", "mode_offset", "]", "=", "entity_id", "\n", "mention_type", "[", "i", "+", "mode_offset", "]", "=", "3", "\n", "if", "new_entity", ":", "\n", "                        ", "shortlist_inds", "[", "i", "+", "mode_offset", "]", "=", "shortlist_ind", "\n", "", "else", ":", "\n", "                        ", "relations", "[", "i", "+", "mode_offset", "]", "=", "relation", "[", ":", "MAX_PARENTS", "]", "\n", "parent_ids", "[", "i", "+", "mode_offset", "]", "=", "parent_id", "[", ":", "MAX_PARENTS", "]", "\n", "", "if", "self", ".", "_dataset_reader", ".", "_mode", "==", "\"generative\"", ":", "\n", "                        ", "alias_copy_inds", "[", "i", "+", "mode_offset", "]", "=", "self", ".", "_dataset_reader", ".", "_alias_database", ".", "token_to_uid", "(", "raw_entity_id", ",", "tokens", "[", "i", "]", ")", "\n", "# Now put in proper mention type for first token", "\n", "", "", "start", "=", "annotation", "[", "'span'", "]", "[", "0", "]", "\n", "if", "new_entity", ":", "\n", "                    ", "mention_type", "[", "start", "+", "mode_offset", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "mention_type", "[", "start", "+", "mode_offset", "]", "=", "2", "\n", "\n", "", "", "", "instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "\n", "source", ",", "\n", "target", ",", "\n", "shortlist", ",", "\n", "reverse_shortlist", ",", "\n", "raw_entity_ids", ",", "\n", "entity_ids", ",", "\n", "relations", ",", "\n", "parent_ids", ",", "\n", "shortlist_inds", ",", "\n", "mention_type", ",", "\n", "alias_copy_inds", ")", "\n", "reset", "=", "SequentialArrayField", "(", "np", ".", "array", "(", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "instance", ".", "add_field", "(", "'reset'", ",", "reset", ")", "\n", "return", "instance", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.complete_the_sentence.CompleteTheSentencePredictor.__init__": [[25, 31], ["logger.warning"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", ":", "\n", "        ", "logger", ".", "warning", "(", "'CompleteTheSentencePredictor is meant to be used with '", "\n", "'`kglm.run complete-the-sentence`, if you are using '", "\n", "'`allennlp predict` then results may be incorrect.'", ")", "\n", "self", ".", "_model", "=", "model", "\n", "self", ".", "_dataset_reader", "=", "dataset_reader", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.complete_the_sentence.CompleteTheSentencePredictor._json_to_instance": [[32, 84], ["complete_the_sentence.CompleteTheSentencePredictor._dataset_reader.text_to_instance", "kglm.data.SequentialArrayField", "complete_the_sentence.CompleteTheSentencePredictor.add_field", "complete_the_sentence.CompleteTheSentencePredictor._dataset_reader.text_to_instance", "kglm.data.SequentialArrayField", "complete_the_sentence.CompleteTheSentencePredictor.add_field", "numpy.array", "allennlp.data.fields.TextField", "numpy.array", "complete_the_sentence.CompleteTheSentencePredictor.add_field", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We need to break with the Predictor expectations slightly, instead of returning a single\n        instance we return a conditioning instance (to warm up the model), and then a generative\n        instance (e.g. the token to predict on).\n        \"\"\"", "\n", "### Conditioning Instance ###", "\n", "# TODO: This is totally broken...", "\n", "\n", "# Manually add the start token", "\n", "tokens", "=", "[", "'@@START@@'", ",", "*", "json_dict", "[", "'prefix'", "]", "]", "\n", "\n", "# Also need to offset", "\n", "# start, end = json_dict['entity_indices']", "\n", "# span = [start + 1, end + 1]", "\n", "\n", "# Repackage into the expected annotation format", "\n", "# annotations = [{", "\n", "#     'id': json_dict['entity_id'],", "\n", "#     'relation': ['@@NEW@@'],", "\n", "#     'parent_id': [json_dict['entity_id']],", "\n", "#     'span': span", "\n", "# }]", "\n", "# data = {'tokens': [tokens], 'annotations': annotations}", "\n", "# conditioning_instance = self._dataset_reader.text_to_instance(data)", "\n", "conditioning_instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "tokens", "[", ":", "-", "1", "]", ")", "\n", "\n", "# Manually add the reset field here", "\n", "reset", "=", "SequentialArrayField", "(", "np", ".", "array", "(", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "conditioning_instance", ".", "add_field", "(", "'reset'", ",", "reset", ")", "\n", "\n", "# Add the shortlist", "\n", "if", "'shortlist'", "in", "json_dict", ":", "\n", "            ", "shortlist", "=", "json_dict", "[", "'shortlist'", "]", "\n", "field", "=", "TextField", "(", "\n", "[", "Token", "(", "x", ")", "for", "x", "in", "shortlist", "]", ",", "\n", "token_indexers", "=", "self", ".", "_dataset_reader", ".", "_entity_indexers", ")", "\n", "conditioning_instance", ".", "fields", "[", "'shortlist'", "]", "=", "field", "\n", "\n", "### Generative Instance ###", "\n", "\n", "# data = {'tokens': [[tokens[-1]]]}", "\n", "# generative_instance = self._dataset_reader.text_to_instance(data)", "\n", "", "generative_instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "[", "tokens", "[", "-", "1", "]", "]", ")", "\n", "# Manually add the reset field here", "\n", "reset", "=", "SequentialArrayField", "(", "np", ".", "array", "(", "0", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "generative_instance", ".", "add_field", "(", "'reset'", ",", "reset", ")", "\n", "if", "'shortlist'", "in", "json_dict", ":", "\n", "            ", "generative_instance", ".", "add_field", "(", "'shortlist'", ",", "conditioning_instance", ".", "fields", "[", "'shortlist'", "]", ")", "\n", "\n", "", "return", "conditioning_instance", ",", "generative_instance", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.complete_the_sentence.CompleteTheSentencePredictor.predict_instance": [[85, 119], ["complete_the_sentence.CompleteTheSentencePredictor._model.eval", "torch.no_grad", "complete_the_sentence.CompleteTheSentencePredictor._model._get_prediction_device", "allennlp.data.dataset.Batch", "allennlp.nn.util.move_to_device.index_instances", "allennlp.nn.util.move_to_device", "allennlp.data.dataset.Batch", "allennlp.nn.util.move_to_device.index_instances", "allennlp.nn.util.move_to_device", "complete_the_sentence.CompleteTheSentencePredictor._model.sample", "logger.debug", "complete_the_sentence.CompleteTheSentencePredictor._model.sample", "logger.debug", "complete_the_sentence.CompleteTheSentencePredictor._aggregate_word_probs", "logger.debug", "allennlp.nn.util.move_to_device.as_tensor_dict", "allennlp.nn.util.move_to_device.as_tensor_dict"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.sample.sample", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.sample.sample", "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.complete_the_sentence.CompleteTheSentencePredictor._aggregate_word_probs"], ["", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instances", ":", "Tuple", "[", "Instance", ",", "Instance", "]", ")", "->", "JsonDict", ":", "\n", "        ", "conditioning_instance", ",", "generative_instance", "=", "instances", "\n", "\n", "self", ".", "_model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# TODO: Make this a parameter somewhere", "\n", "            ", "num_samples", "=", "100", "\n", "\n", "# Duplicate instances (to sample in parallel)", "\n", "cuda_device", "=", "self", ".", "_model", ".", "_get_prediction_device", "(", ")", "\n", "conditioning_batch", "=", "Batch", "(", "[", "conditioning_instance", "]", "*", "num_samples", ")", "\n", "conditioning_batch", ".", "index_instances", "(", "self", ".", "_model", ".", "vocab", ")", "\n", "conditioning_batch", "=", "util", ".", "move_to_device", "(", "conditioning_batch", ".", "as_tensor_dict", "(", ")", ",", "cuda_device", ")", "\n", "\n", "generative_batch", "=", "Batch", "(", "[", "generative_instance", "]", "*", "num_samples", ")", "\n", "generative_batch", ".", "index_instances", "(", "self", ".", "_model", ".", "vocab", ")", "\n", "generative_batch", "=", "util", ".", "move_to_device", "(", "generative_batch", ".", "as_tensor_dict", "(", ")", ",", "cuda_device", ")", "\n", "\n", "# Sample annotations and generate next token", "\n", "self", ".", "_model", ".", "_use_shortlist", "=", "True", "\n", "conditioning_output", "=", "self", ".", "_model", ".", "sample", "(", "**", "conditioning_batch", ",", "emit_tokens", "=", "False", ")", "\n", "logger", ".", "debug", "(", "'clears condition generation'", ")", "\n", "# self._model(**conditioning_output)  # Shouldn't need to do this, but just in case", "\n", "# logger.debug('clears reconditioning')", "\n", "generative_output", "=", "self", ".", "_model", ".", "sample", "(", "**", "generative_batch", ",", "emit_tokens", "=", "True", ")", "\n", "logger", ".", "debug", "(", "'clears generation'", ")", "\n", "del", "conditioning_batch", ",", "generative_batch", "\n", "\n", "aggregate_word_probs", "=", "self", ".", "_aggregate_word_probs", "(", "generative_output", ")", "\n", "logger", ".", "debug", "(", "'clears word probs'", ")", "\n", "\n", "return", "aggregate_word_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.complete_the_sentence.CompleteTheSentencePredictor._aggregate_word_probs": [[120, 174], ["vocab.get_vocab_size", "source_vocab_probs.mean().squeeze", "dict", "vocab.get_index_to_token_vocabulary", "enumerate", "zip", "list", "list.sort", "zip", "prob.item", "vocab.get_token_from_index", "zip", "dict.items", "source_vocab_probs.mean", "raw_entity_id.item", "copy_probs.squeeze().tolist", "logger.warning", "id_map.items", "copy_probs.squeeze", "ind.item"], "methods", ["None"], ["", "", "def", "_aggregate_word_probs", "(", "self", ",", "generative_output", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "Dict", "[", "str", ",", "List", "[", "Any", "]", "]", ":", "\n", "\n", "# Get vocab", "\n", "        ", "vocab", "=", "self", ".", "_model", ".", "vocab", "\n", "vocab_size", "=", "vocab", ".", "get_vocab_size", "(", "'tokens'", ")", "\n", "\n", "# Get alias database", "\n", "alias_database", "=", "generative_output", "[", "'metadata'", "]", "[", "0", "]", "[", "'alias_database'", "]", "\n", "\n", "# Split into source and target vocab probs", "\n", "target_probs", "=", "generative_output", "[", "'target_probs'", "]", "\n", "source_vocab_probs", "=", "target_probs", "[", ":", ",", ":", ",", ":", "vocab_size", "]", "\n", "copy_vocab_probs", "=", "target_probs", "[", ":", ",", ":", ",", "vocab_size", ":", "]", "\n", "\n", "# Average source vocab prob is easy to compute", "\n", "source_vocab_avg", "=", "source_vocab_probs", ".", "mean", "(", "0", ")", ".", "squeeze", "(", ")", "\n", "prob_dict", "=", "dict", "(", ")", "\n", "index_to_token_vocabulary", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", "'tokens'", ")", "\n", "for", "i", ",", "prob", "in", "enumerate", "(", "source_vocab_avg", ")", ":", "\n", "            ", "word", "=", "index_to_token_vocabulary", "[", "i", "]", "\n", "prob_dict", "[", "word", "]", "=", "prob", ".", "item", "(", ")", "\n", "\n", "# Get alias indices", "\n", "", "alias_indices", "=", "generative_output", "[", "'alias_indices'", "]", "\n", "\n", "# The copy vocabs will be a little bit more difficult", "\n", "num_samples", "=", "target_probs", ".", "shape", "[", "0", "]", "\n", "raw_entity_ids", "=", "generative_output", "[", "'raw_entity_ids'", "]", "[", "'raw_entity_ids'", "]", "\n", "for", "alias_index", ",", "copy_probs", ",", "raw_entity_id", "in", "zip", "(", "alias_indices", ",", "copy_vocab_probs", ",", "raw_entity_ids", ")", ":", "\n", "            ", "if", "raw_entity_id", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "entity", "=", "vocab", ".", "get_token_from_index", "(", "raw_entity_id", ".", "item", "(", ")", ",", "'raw_entity_ids'", ")", "\n", "try", ":", "\n", "                ", "id_map", "=", "alias_database", ".", "_id_map_lookup", "[", "entity", "]", "\n", "", "except", ":", "\n", "                ", "logger", ".", "warning", "(", "'Error could not find id map for entity \"%s\"'", ",", "entity", ")", "\n", "continue", "\n", "", "reverse_id_map", "=", "{", "i", ":", "x", "for", "x", ",", "i", "in", "id_map", ".", "items", "(", ")", "}", "\n", "for", "ind", ",", "prob", "in", "zip", "(", "alias_index", ",", "copy_probs", ".", "squeeze", "(", ")", ".", "tolist", "(", ")", ")", ":", "\n", "                ", "if", "ind", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "word", "=", "reverse_id_map", "[", "ind", ".", "item", "(", ")", "]", "\n", "if", "word", "in", "prob_dict", ":", "\n", "                    ", "prob_dict", "[", "word", "]", "+=", "prob", "/", "num_samples", "\n", "", "else", ":", "\n", "                    ", "prob_dict", "[", "word", "]", "=", "prob", "/", "num_samples", "\n", "\n", "# Lastly, convert the prob_dict to a ranked list of words", "\n", "", "", "", "prob_list", "=", "list", "(", "prob_dict", ".", "items", "(", ")", ")", "\n", "prob_list", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "words", ",", "probs", "=", "zip", "(", "*", "prob_list", "[", ":", "1000", "]", ")", "\n", "return", "{", "'words'", ":", "words", ",", "'probs'", ":", "probs", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.complete_the_sentence_test.CompleteTheSentencePredictorTest.test_works": [[11, 23], ["allennlp.models.archival.load_archive", "kglm.predictors.complete_the_sentence.CompleteTheSentencePredictor.from_archive", "kglm.predictors.complete_the_sentence.CompleteTheSentencePredictor.from_archive.predict_json", "kglm.predictors.complete_the_sentence.CompleteTheSentencePredictor.from_archive.predict_json"], "methods", ["None"], ["    ", "def", "test_works", "(", "self", ")", ":", "\n", "        ", "inputs", "=", "{", "\n", "\"prefix\"", ":", "[", "\"Benton\"", ",", "\"Brindge\"", ",", "\"is\"", ",", "\"in\"", "]", ",", "\n", "\"expected_tail\"", ":", "\"Washington\"", ",", "\n", "\"entity_id\"", ":", "\"Q4890550\"", ",", "\n", "\"entity_indices\"", ":", "[", "0", ",", "2", "]", ",", "\n", "\"shortlist\"", ":", "[", "\"Q4890550\"", ",", "\"Q35657\"", "]", "\n", "}", "\n", "model_archive", "=", "load_archive", "(", "'kglm/tests/fixtures/kglm.model.tar.gz'", ")", "\n", "predictor", "=", "CompleteTheSentencePredictor", ".", "from_archive", "(", "model_archive", ",", "'complete-the-sentence'", ")", "\n", "predictor", ".", "predict_json", "(", "inputs", ")", "\n", "predictor", ".", "predict_json", "(", "inputs", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer.__init__": [[40, 228], ["allennlp.training.trainer_base.TrainerBase.__init__", "allennlp.training.metric_tracker.MetricTracker", "allennlp.training.checkpointer.Checkpointer", "allennlp.training.tensorboard_writer.TensorboardWriter", "trainer.LmTrainer._tensorboard.enable_activation_logging", "logger.warning", "allennlp.common.checks.ConfigurationError", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "model", ":", "Model", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "iterator", ":", "DataIterator", ",", "\n", "train_dataset", ":", "Iterable", "[", "Instance", "]", ",", "\n", "validation_dataset", ":", "Optional", "[", "Iterable", "[", "Instance", "]", "]", "=", "None", ",", "\n", "patience", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "validation_metric", ":", "str", "=", "\"-loss\"", ",", "\n", "validation_iterator", ":", "DataIterator", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "True", ",", "\n", "num_epochs", ":", "int", "=", "20", ",", "\n", "serialization_dir", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "num_serialized_models_to_keep", ":", "int", "=", "20", ",", "\n", "keep_serialized_model_every_num_seconds", ":", "int", "=", "None", ",", "\n", "model_save_interval", ":", "float", "=", "None", ",", "\n", "cuda_device", ":", "Union", "[", "int", ",", "List", "]", "=", "-", "1", ",", "\n", "grad_norm", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "grad_clipping", ":", "Optional", "[", "float", "]", "=", "None", ",", "\n", "learning_rate_scheduler", ":", "Optional", "[", "LearningRateScheduler", "]", "=", "None", ",", "\n", "summary_interval", ":", "int", "=", "100", ",", "\n", "histogram_interval", ":", "int", "=", "None", ",", "\n", "should_log_parameter_statistics", ":", "bool", "=", "True", ",", "\n", "should_log_learning_rate", ":", "bool", "=", "False", ",", "\n", "log_batch_size_period", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "moving_average", ":", "Optional", "[", "MovingAverage", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        A trainer for doing supervised learning. It just takes a labeled dataset\n        and a ``DataIterator``, and uses the supplied ``Optimizer`` to learn the weights\n        for your model over some fixed number of epochs. You can also pass in a validation\n        dataset and enable early stopping. There are many other bells and whistles as well.\n\n        Parameters\n        ----------\n        model : ``Model``, required.\n            An AllenNLP model to be optimized. Pytorch Modules can also be optimized if\n            their ``forward`` method returns a dictionary with a \"loss\" key, containing a\n            scalar tensor representing the loss function to be optimized.\n\n            If you are training your model using GPUs, your model should already be\n            on the correct device. (If you use `Trainer.from_params` this will be\n            handled for you.)\n        optimizer : ``torch.nn.Optimizer``, required.\n            An instance of a Pytorch Optimizer, instantiated with the parameters of the\n            model to be optimized.\n        iterator : ``DataIterator``, required.\n            A method for iterating over a ``Dataset``, yielding padded indexed batches.\n        train_dataset : ``Dataset``, required.\n            A ``Dataset`` to train on. The dataset should have already been indexed.\n        validation_dataset : ``Dataset``, optional, (default = None).\n            A ``Dataset`` to evaluate on. The dataset should have already been indexed.\n        patience : Optional[int] > 0, optional (default=None)\n            Number of epochs to be patient before early stopping: the training is stopped\n            after ``patience`` epochs with no improvement. If given, it must be ``> 0``.\n            If None, early stopping is disabled.\n        validation_metric : str, optional (default=\"loss\")\n            Validation metric to measure for whether to stop training using patience\n            and whether to serialize an ``is_best`` model each epoch. The metric name\n            must be prepended with either \"+\" or \"-\", which specifies whether the metric\n            is an increasing or decreasing function.\n        validation_iterator : ``DataIterator``, optional (default=None)\n            An iterator to use for the validation set.  If ``None``, then\n            use the training `iterator`.\n        shuffle: ``bool``, optional (default=True)\n            Whether to shuffle the instances in the iterator or not.\n        num_epochs : int, optional (default = 20)\n            Number of training epochs.\n        serialization_dir : str, optional (default=None)\n            Path to directory for saving and loading model files. Models will not be saved if\n            this parameter is not passed.\n        num_serialized_models_to_keep : ``int``, optional (default=20)\n            Number of previous model checkpoints to retain.  Default is to keep 20 checkpoints.\n            A value of None or -1 means all checkpoints will be kept.\n        keep_serialized_model_every_num_seconds : ``int``, optional (default=None)\n            If num_serialized_models_to_keep is not None, then occasionally it's useful to\n            save models at a given interval in addition to the last num_serialized_models_to_keep.\n            To do so, specify keep_serialized_model_every_num_seconds as the number of seconds\n            between permanently saved checkpoints.  Note that this option is only used if\n            num_serialized_models_to_keep is not None, otherwise all checkpoints are kept.\n        model_save_interval : ``float``, optional (default=None)\n            If provided, then serialize models every ``model_save_interval``\n            seconds within single epochs.  In all cases, models are also saved\n            at the end of every epoch if ``serialization_dir`` is provided.\n        cuda_device : ``int``, optional (default = -1)\n            An integer specifying the CUDA device to use. If -1, the CPU is used.\n        grad_norm : ``float``, optional, (default = None).\n            If provided, gradient norms will be rescaled to have a maximum of this value.\n        grad_clipping : ``float``, optional (default = ``None``).\n            If provided, gradients will be clipped `during the backward pass` to have an (absolute)\n            maximum of this value.  If you are getting ``NaNs`` in your gradients during training\n            that are not solved by using ``grad_norm``, you may need this.\n        learning_rate_scheduler : ``PytorchLRScheduler``, optional, (default = None)\n            A Pytorch learning rate scheduler. The learning rate will be decayed with respect to\n            this schedule at the end of each epoch. If you use\n            :class:`torch.optim.lr_scheduler.ReduceLROnPlateau`, this will use the ``validation_metric``\n            provided to determine if learning has plateaued.  To support updating the learning\n            rate on every batch, this can optionally implement ``step_batch(batch_num_total)`` which\n            updates the learning rate given the batch number.\n        summary_interval: ``int``, optional, (default = 100)\n            Number of batches between logging scalars to tensorboard\n        histogram_interval : ``int``, optional, (default = ``None``)\n            If not None, then log histograms to tensorboard every ``histogram_interval`` batches.\n            When this parameter is specified, the following additional logging is enabled:\n                * Histograms of model parameters\n                * The ratio of parameter update norm to parameter norm\n                * Histogram of layer activations\n            We log histograms of the parameters returned by\n            ``model.get_parameters_for_histogram_tensorboard_logging``.\n            The layer activations are logged for any modules in the ``Model`` that have\n            the attribute ``should_log_activations`` set to ``True``.  Logging\n            histograms requires a number of GPU-CPU copies during training and is typically\n            slow, so we recommend logging histograms relatively infrequently.\n            Note: only Modules that return tensors, tuples of tensors or dicts\n            with tensors as values currently support activation logging.\n        should_log_parameter_statistics : ``bool``, optional, (default = True)\n            Whether to send parameter statistics (mean and standard deviation\n            of parameters and gradients) to tensorboard.\n        should_log_learning_rate : ``bool``, optional, (default = False)\n            Whether to send parameter specific learning rate to tensorboard.\n        log_batch_size_period : ``int``, optional, (default = ``None``)\n            If defined, how often to log the average batch size.\n        moving_average: ``MovingAverage``, optional, (default = None)\n            If provided, we will maintain moving averages for all parameters. During training, we\n            employ a shadow variable for each parameter, which maintains the moving average. During\n            evaluation, we backup the original parameters and assign the moving averages to corresponding\n            parameters. Be careful that when saving the checkpoint, we will save the moving averages of\n            parameters. This is necessary because we want the saved model to perform as well as the validated\n            model if we load it later. But this may cause problems if you restart the training from checkpoint.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "serialization_dir", ",", "cuda_device", ")", "\n", "\n", "# I am not calling move_to_gpu here, because if the model is", "\n", "# not already on the GPU then the optimizer is going to be wrong.", "\n", "self", ".", "model", "=", "model", "\n", "\n", "self", ".", "iterator", "=", "iterator", "\n", "self", ".", "_validation_iterator", "=", "validation_iterator", "\n", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "train_data", "=", "train_dataset", "\n", "self", ".", "_validation_data", "=", "validation_dataset", "\n", "\n", "if", "patience", "is", "None", ":", "# no early stopping", "\n", "            ", "if", "validation_dataset", ":", "\n", "                ", "logger", ".", "warning", "(", "'You provided a validation dataset but patience was set to None, '", "\n", "'meaning that early stopping is disabled'", ")", "\n", "", "", "elif", "(", "not", "isinstance", "(", "patience", ",", "int", ")", ")", "or", "patience", "<=", "0", ":", "\n", "            ", "raise", "ConfigurationError", "(", "'{} is an invalid value for \"patience\": it must be a positive integer '", "\n", "'or None (if you want to disable early stopping)'", ".", "format", "(", "patience", ")", ")", "\n", "\n", "# For tracking is_best_so_far and should_stop_early", "\n", "", "self", ".", "_metric_tracker", "=", "MetricTracker", "(", "patience", ",", "validation_metric", ")", "\n", "# Get rid of + or -", "\n", "self", ".", "_validation_metric", "=", "validation_metric", "[", "1", ":", "]", "\n", "\n", "self", ".", "_num_epochs", "=", "num_epochs", "\n", "\n", "self", ".", "_checkpointer", "=", "Checkpointer", "(", "serialization_dir", ",", "\n", "keep_serialized_model_every_num_seconds", ",", "\n", "num_serialized_models_to_keep", ")", "\n", "\n", "self", ".", "_model_save_interval", "=", "model_save_interval", "\n", "\n", "self", ".", "_grad_norm", "=", "grad_norm", "\n", "self", ".", "_grad_clipping", "=", "grad_clipping", "\n", "\n", "self", ".", "_learning_rate_scheduler", "=", "learning_rate_scheduler", "\n", "self", ".", "_moving_average", "=", "moving_average", "\n", "\n", "# We keep the total batch number as an instance variable because it", "\n", "# is used inside a closure for the hook which logs activations in", "\n", "# ``_enable_activation_logging``.", "\n", "self", ".", "_batch_num_total", "=", "0", "\n", "\n", "self", ".", "_tensorboard", "=", "TensorboardWriter", "(", "\n", "get_batch_num_total", "=", "lambda", ":", "self", ".", "_batch_num_total", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "summary_interval", "=", "summary_interval", ",", "\n", "histogram_interval", "=", "histogram_interval", ",", "\n", "should_log_parameter_statistics", "=", "should_log_parameter_statistics", ",", "\n", "should_log_learning_rate", "=", "should_log_learning_rate", ")", "\n", "\n", "self", ".", "_log_batch_size_period", "=", "log_batch_size_period", "\n", "\n", "self", ".", "_last_log", "=", "0.0", "# time of last logging", "\n", "\n", "# Enable activation logging.", "\n", "if", "histogram_interval", "is", "not", "None", ":", "\n", "            ", "self", ".", "_tensorboard", ".", "enable_activation_logging", "(", "self", ".", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer.rescale_gradients": [[229, 231], ["allennlp.training.util.rescale_gradients"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer.rescale_gradients"], ["", "", "def", "rescale_gradients", "(", "self", ")", "->", "Optional", "[", "float", "]", ":", "\n", "        ", "return", "training_util", ".", "rescale_gradients", "(", "self", ".", "model", ",", "self", ".", "_grad_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer.batch_loss": [[232, 251], ["allennlp.nn.util.move_to_device", "trainer.LmTrainer.model", "trainer.LmTrainer.model.get_regularization_penalty", "RuntimeError"], "methods", ["None"], ["", "def", "batch_loss", "(", "self", ",", "batch", ":", "TensorDict", ",", "for_training", ":", "bool", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Does a forward pass on the given batches and returns the ``loss`` value in the result.\n        If ``for_training`` is `True` also applies regularization penalty.\n        \"\"\"", "\n", "batch", "=", "nn_util", ".", "move_to_device", "(", "batch", ",", "self", ".", "_cuda_devices", "[", "0", "]", ")", "\n", "output_dict", "=", "self", ".", "model", "(", "**", "batch", ")", "\n", "\n", "try", ":", "\n", "            ", "loss", "=", "output_dict", "[", "\"loss\"", "]", "\n", "if", "for_training", ":", "\n", "                ", "loss", "+=", "self", ".", "model", ".", "get_regularization_penalty", "(", ")", "\n", "", "", "except", "KeyError", ":", "\n", "            ", "if", "for_training", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"The model you are trying to optimize does not contain a\"", "\n", "\" 'loss' key in the output of model.forward(inputs).\"", ")", "\n", "", "loss", "=", "None", "\n", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer._train_epoch": [[252, 376], ["logger.info", "allennlp.common.util.peak_memory_mb", "logger.info", "allennlp.common.util.gpu_memory_mb().items", "trainer.LmTrainer.model.train", "trainer.LmTrainer.iterator", "time.time", "time.time", "set", "logger.info", "allennlp.common.tqdm.Tqdm.tqdm", "allennlp.training.util.get_metrics", "gpu_usage.append", "logger.info", "trainer.LmTrainer.model.get_parameters_for_histogram_tensorboard_logging", "trainer.LmTrainer.optimizer.zero_grad", "trainer.LmTrainer.batch_loss", "torch.isnan", "torch.isnan", "torch.isnan", "torch.isnan", "trainer.LmTrainer.backward", "trainer.LmTrainer.item", "trainer.LmTrainer._tensorboard.should_log_histograms_this_batch", "allennlp.training.util.get_metrics", "allennlp.training.util.description_from_metrics", "allennlp.common.tqdm.Tqdm.tqdm.set_description", "trainer.LmTrainer._tensorboard.should_log_this_batch", "trainer.LmTrainer._tensorboard.should_log_histograms_this_batch", "allennlp.common.util.gpu_memory_mb", "ValueError", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "trainer.LmTrainer._learning_rate_scheduler.step_batch", "trainer.LmTrainer.optimizer.step", "trainer.LmTrainer.model.named_parameters", "trainer.LmTrainer.optimizer.step", "trainer.LmTrainer._moving_average.apply", "trainer.LmTrainer._tensorboard.log_learning_rates", "trainer.LmTrainer._tensorboard.add_train_scalar", "trainer.LmTrainer._tensorboard.log_metrics", "trainer.LmTrainer._tensorboard.log_histograms", "time.time", "trainer.LmTrainer._save_checkpoint", "trainer.LmTrainer.model.parameters", "param.detach().cpu().clone", "param_updates[].sub_", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.norm().cpu", "torch.norm().cpu", "torch.norm().cpu", "torch.norm().cpu", "trainer.LmTrainer._tensorboard.add_train_scalar", "trainer.LmTrainer.model.named_parameters", "param.detach().cpu", "param_updates[].view", "time.time", "allennlp.training.util.time_to_str", "param.detach().cpu", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "allennlp.training.util.get_metrics.items", "int", "str", "param.detach", "param.view", "param.detach"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.train", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.get_metrics", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.zero_grad", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer.batch_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.get_metrics", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer._save_checkpoint"], ["", "def", "_train_epoch", "(", "self", ",", "epoch", ":", "int", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "\"\"\"\n        Trains one epoch and returns metrics.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Epoch %d/%d\"", ",", "epoch", ",", "self", ".", "_num_epochs", "-", "1", ")", "\n", "peak_cpu_usage", "=", "peak_memory_mb", "(", ")", "\n", "logger", ".", "info", "(", "f\"Peak CPU memory usage MB: {peak_cpu_usage}\"", ")", "\n", "gpu_usage", "=", "[", "]", "\n", "for", "gpu", ",", "memory", "in", "gpu_memory_mb", "(", ")", ".", "items", "(", ")", ":", "\n", "            ", "gpu_usage", ".", "append", "(", "(", "gpu", ",", "memory", ")", ")", "\n", "logger", ".", "info", "(", "f\"GPU {gpu} memory usage MB: {memory}\"", ")", "\n", "\n", "", "train_loss", "=", "0.0", "\n", "# Set the model to \"train\" mode.", "\n", "self", ".", "model", ".", "train", "(", ")", "\n", "\n", "#num_gpus = len(self._cuda_devices)", "\n", "\n", "# Get tqdm for the training batches", "\n", "raw_train_generator", "=", "self", ".", "iterator", "(", "self", ".", "train_data", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "self", ".", "shuffle", ")", "\n", "#train_generator = lazy_groups_of(raw_train_generator, num_gpus)", "\n", "#num_training_batches = math.ceil(self.iterator.get_num_batches(self.train_data)/num_gpus)", "\n", "num_training_batches", "=", "1", "\n", "self", ".", "_last_log", "=", "time", ".", "time", "(", ")", "\n", "last_save_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "batches_this_epoch", "=", "0", "\n", "if", "self", ".", "_batch_num_total", "is", "None", ":", "\n", "            ", "self", ".", "_batch_num_total", "=", "0", "\n", "\n", "", "histogram_parameters", "=", "set", "(", "self", ".", "model", ".", "get_parameters_for_histogram_tensorboard_logging", "(", ")", ")", "\n", "\n", "\n", "logger", ".", "info", "(", "\"Training\"", ")", "\n", "train_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "raw_train_generator", ",", "\n", "total", "=", "num_training_batches", ")", "\n", "cumulative_batch_size", "=", "0", "\n", "for", "batch", ",", "lr_mult", "in", "train_generator_tqdm", ":", "\n", "            ", "batches_this_epoch", "+=", "1", "\n", "self", ".", "_batch_num_total", "+=", "1", "\n", "batch_num_total", "=", "self", ".", "_batch_num_total", "\n", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "loss", "=", "self", ".", "batch_loss", "(", "batch", ",", "for_training", "=", "True", ")", "\n", "\n", "if", "torch", ".", "isnan", "(", "loss", ")", ":", "\n", "                ", "raise", "ValueError", "(", "\"nan loss encountered\"", ")", "\n", "\n", "", "loss", ".", "backward", "(", ")", "\n", "\n", "train_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "# batch_grad_norm = self.rescale_gradients()", "\n", "if", "self", ".", "_grad_clipping", ":", "\n", "                ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "\n", "self", ".", "_grad_clipping", ")", "\n", "\n", "# This does nothing if batch_num_total is None or you are using an", "\n", "# LRScheduler which doesn't update per batch.", "\n", "", "if", "self", ".", "_learning_rate_scheduler", ":", "\n", "                ", "self", ".", "_learning_rate_scheduler", ".", "step_batch", "(", "batch_num_total", ")", "\n", "\n", "# We dynamically adjust the learning rate to account for slight variations in the input", "\n", "# sequences", "\n", "", "original_lr", "=", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "batch_lr", "=", "original_lr", "*", "lr_mult", "\n", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "batch_lr", "\n", "\n", "if", "self", ".", "_tensorboard", ".", "should_log_histograms_this_batch", "(", ")", ":", "\n", "# get the magnitude of parameter updates for logging", "\n", "# We need a copy of current parameters to compute magnitude of updates,", "\n", "# and copy them to CPU so large models won't go OOM on the GPU.", "\n", "                ", "param_updates", "=", "{", "name", ":", "param", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "clone", "(", ")", "\n", "for", "name", ",", "param", "in", "self", ".", "model", ".", "named_parameters", "(", ")", "}", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "for", "name", ",", "param", "in", "self", ".", "model", ".", "named_parameters", "(", ")", ":", "\n", "                    ", "param_updates", "[", "name", "]", ".", "sub_", "(", "param", ".", "detach", "(", ")", ".", "cpu", "(", ")", ")", "\n", "update_norm", "=", "torch", ".", "norm", "(", "param_updates", "[", "name", "]", ".", "view", "(", "-", "1", ",", ")", ")", "\n", "param_norm", "=", "torch", ".", "norm", "(", "param", ".", "view", "(", "-", "1", ",", ")", ")", ".", "cpu", "(", ")", "\n", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\"gradient_update/\"", "+", "name", ",", "\n", "update_norm", "/", "(", "param_norm", "+", "1e-7", ")", ")", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "original_lr", "\n", "\n", "# Update moving averages", "\n", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "                ", "self", ".", "_moving_average", ".", "apply", "(", "batch_num_total", ")", "\n", "\n", "\n", "# Update the description with the latest metrics", "\n", "", "metrics", "=", "training_util", ".", "get_metrics", "(", "self", ".", "model", ",", "train_loss", ",", "batches_this_epoch", ")", "\n", "description", "=", "training_util", ".", "description_from_metrics", "(", "metrics", ")", "\n", "\n", "train_generator_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "\n", "# Log parameter values to Tensorboard", "\n", "if", "self", ".", "_tensorboard", ".", "should_log_this_batch", "(", ")", ":", "\n", "# self._tensorboard.log_parameter_and_gradient_statistics(self.model, batch_grad_norm)", "\n", "                ", "self", ".", "_tensorboard", ".", "log_learning_rates", "(", "self", ".", "model", ",", "self", ".", "optimizer", ")", "\n", "\n", "self", ".", "_tensorboard", ".", "add_train_scalar", "(", "\"loss/loss_train\"", ",", "metrics", "[", "\"loss\"", "]", ")", "\n", "self", ".", "_tensorboard", ".", "log_metrics", "(", "{", "\"epoch_metrics/\"", "+", "k", ":", "v", "for", "k", ",", "v", "in", "metrics", ".", "items", "(", ")", "}", ")", "\n", "\n", "", "if", "self", ".", "_tensorboard", ".", "should_log_histograms_this_batch", "(", ")", ":", "\n", "                ", "self", ".", "_tensorboard", ".", "log_histograms", "(", "self", ".", "model", ",", "histogram_parameters", ")", "\n", "\n", "# Save model if needed.", "\n", "", "if", "self", ".", "_model_save_interval", "is", "not", "None", "and", "(", "\n", "time", ".", "time", "(", ")", "-", "last_save_time", ">", "self", ".", "_model_save_interval", "\n", ")", ":", "\n", "                ", "last_save_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "_save_checkpoint", "(", "\n", "'{0}.{1}'", ".", "format", "(", "epoch", ",", "training_util", ".", "time_to_str", "(", "int", "(", "last_save_time", ")", ")", ")", "\n", ")", "\n", "", "", "metrics", "=", "training_util", ".", "get_metrics", "(", "self", ".", "model", ",", "train_loss", ",", "batches_this_epoch", ",", "reset", "=", "True", ")", "\n", "metrics", "[", "'cpu_memory_MB'", "]", "=", "peak_cpu_usage", "\n", "for", "(", "gpu_num", ",", "memory", ")", "in", "gpu_usage", ":", "\n", "            ", "metrics", "[", "'gpu_'", "+", "str", "(", "gpu_num", ")", "+", "'_memory_MB'", "]", "=", "memory", "\n", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer._validation_loss": [[378, 426], ["logger.info", "trainer.LmTrainer.model.eval", "val_iterator", "allennlp.common.tqdm.Tqdm.tqdm", "trainer.LmTrainer._moving_average.assign_average_value", "trainer.LmTrainer.batch_loss", "allennlp.training.util.get_metrics", "allennlp.training.util.description_from_metrics", "allennlp.common.tqdm.Tqdm.tqdm.set_description", "trainer.LmTrainer._moving_average.restore", "trainer.LmTrainer.detach().cpu().numpy", "trainer.LmTrainer.detach().cpu", "trainer.LmTrainer.detach"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer.batch_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.get_metrics"], ["", "def", "_validation_loss", "(", "self", ")", "->", "Tuple", "[", "float", ",", "int", "]", ":", "\n", "        ", "\"\"\"\n        Computes the validation loss. Returns it and the number of batches.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Validating\"", ")", "\n", "\n", "self", ".", "model", ".", "eval", "(", ")", "\n", "\n", "# Replace parameter values with the shadow values from the moving averages.", "\n", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "assign_average_value", "(", ")", "\n", "\n", "", "if", "self", ".", "_validation_iterator", "is", "not", "None", ":", "\n", "            ", "val_iterator", "=", "self", ".", "_validation_iterator", "\n", "", "else", ":", "\n", "            ", "val_iterator", "=", "self", ".", "iterator", "\n", "\n", "", "raw_val_generator", "=", "val_iterator", "(", "self", ".", "_validation_data", ",", "\n", "num_epochs", "=", "1", ",", "\n", "shuffle", "=", "False", ")", "\n", "# val_generator = lazy_groups_of(raw_val_generator, num_gpus)", "\n", "num_validation_batches", "=", "1", "\n", "val_generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "raw_val_generator", ",", "\n", "total", "=", "num_validation_batches", ")", "\n", "batches_this_epoch", "=", "0", "\n", "val_loss", "=", "0", "\n", "for", "batch", ",", "_", "in", "val_generator_tqdm", ":", "\n", "\n", "            ", "loss", "=", "self", ".", "batch_loss", "(", "batch", ",", "for_training", "=", "False", ")", "\n", "if", "loss", "is", "not", "None", ":", "\n", "# You shouldn't necessarily have to compute a loss for validation, so we allow for", "\n", "# `loss` to be None.  We need to be careful, though - `batches_this_epoch` is", "\n", "# currently only used as the divisor for the loss function, so we can safely only", "\n", "# count those batches for which we actually have a loss.  If this variable ever", "\n", "# gets used for something else, we might need to change things around a bit.", "\n", "                ", "batches_this_epoch", "+=", "1", "\n", "val_loss", "+=", "loss", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "# Update the description with the latest metrics", "\n", "", "val_metrics", "=", "training_util", ".", "get_metrics", "(", "self", ".", "model", ",", "val_loss", ",", "batches_this_epoch", ")", "\n", "description", "=", "training_util", ".", "description_from_metrics", "(", "val_metrics", ")", "\n", "val_generator_tqdm", ".", "set_description", "(", "description", ",", "refresh", "=", "False", ")", "\n", "\n", "# Now restore the original parameter values.", "\n", "", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "restore", "(", ")", "\n", "\n", "", "return", "val_loss", ",", "batches_this_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer.train": [[427, 552], ["logger.info", "time.time", "trainer.LmTrainer._metric_tracker.best_epoch_metrics.items", "range", "trainer.LmTrainer._checkpointer.best_model_state", "trainer.LmTrainer._restore_checkpoint", "time.time", "trainer.LmTrainer._train_epoch", "trainer.LmTrainer.items", "isinstance", "trainer.LmTrainer._tensorboard.log_metrics", "time.strftime", "trainer.LmTrainer.items", "allennlp.training.util.get_metrics.items", "trainer.LmTrainer._metric_tracker.is_best_so_far", "trainer.LmTrainer._save_checkpoint", "logger.info", "isinstance", "trainer.LmTrainer.model.load_state_dict", "traceback.print_exc", "allennlp.common.checks.ConfigurationError", "max", "key.startswith", "time.time", "time.gmtime", "allennlp.training.util.get_metrics.items", "allennlp.common.util.dump_metrics", "trainer.LmTrainer._learning_rate_scheduler.step", "time.time", "time.strftime", "str", "logger.info", "metrics.get", "max", "trainer.LmTrainer.model.parameters", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "trainer.LmTrainer._validation_loss", "allennlp.training.util.get_metrics", "trainer.LmTrainer._metric_tracker.add_metric", "trainer.LmTrainer._metric_tracker.should_stop_early", "os.path.join", "time.gmtime", "time.time", "datetime.timedelta", "metrics.get", "prm.data.clone", "logger.info", "trainer.LmTrainer.model.parameters", "float", "int"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer._restore_checkpoint", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer._train_epoch", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer._save_checkpoint", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.load_state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer._validation_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.get_metrics"], ["", "def", "train", "(", "self", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "        ", "\"\"\"\n        Trains the supplied model with the supplied parameters.\n        \"\"\"", "\n", "try", ":", "\n", "            ", "epoch_counter", "=", "self", ".", "_restore_checkpoint", "(", ")", "\n", "", "except", "RuntimeError", ":", "\n", "            ", "traceback", ".", "print_exc", "(", ")", "\n", "raise", "ConfigurationError", "(", "\"Could not recover training from the checkpoint.  Did you mean to output to \"", "\n", "\"a different serialization directory or delete the existing serialization \"", "\n", "\"directory?\"", ")", "\n", "\n", "# training_util.enable_gradient_clipping(self.model, self._grad_clipping)", "\n", "\n", "", "logger", ".", "info", "(", "\"Beginning training.\"", ")", "\n", "\n", "train_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "val_metrics", ":", "Dict", "[", "str", ",", "float", "]", "=", "{", "}", "\n", "this_epoch_val_metric", ":", "float", "=", "None", "\n", "metrics", ":", "Dict", "[", "str", ",", "Any", "]", "=", "{", "}", "\n", "epochs_trained", "=", "0", "\n", "training_start_time", "=", "time", ".", "time", "(", ")", "\n", "\n", "metrics", "[", "'best_epoch'", "]", "=", "self", ".", "_metric_tracker", ".", "best_epoch", "\n", "for", "key", ",", "value", "in", "self", ".", "_metric_tracker", ".", "best_epoch_metrics", ".", "items", "(", ")", ":", "\n", "            ", "metrics", "[", "\"best_validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "for", "epoch", "in", "range", "(", "epoch_counter", ",", "self", ".", "_num_epochs", ")", ":", "\n", "            ", "epoch_start_time", "=", "time", ".", "time", "(", ")", "\n", "train_metrics", "=", "self", ".", "_train_epoch", "(", "epoch", ")", "\n", "\n", "# get peak of memory usage", "\n", "if", "'cpu_memory_MB'", "in", "train_metrics", ":", "\n", "                ", "metrics", "[", "'peak_cpu_memory_MB'", "]", "=", "max", "(", "metrics", ".", "get", "(", "'peak_cpu_memory_MB'", ",", "0", ")", ",", "\n", "train_metrics", "[", "'cpu_memory_MB'", "]", ")", "\n", "", "for", "key", ",", "value", "in", "train_metrics", ".", "items", "(", ")", ":", "\n", "                ", "if", "key", ".", "startswith", "(", "'gpu_'", ")", ":", "\n", "                    ", "metrics", "[", "\"peak_\"", "+", "key", "]", "=", "max", "(", "metrics", ".", "get", "(", "\"peak_\"", "+", "key", ",", "0", ")", ",", "value", ")", "\n", "\n", "# If we are using an NT-ASGD optimizer, we replace the model parameters with \"average\"", "\n", "# parameters when evaluating / checkpointing.", "\n", "", "", "if", "isinstance", "(", "self", ".", "optimizer", ",", "NTASGDOptimizer", ")", ":", "\n", "                ", "if", "self", ".", "optimizer", ".", "triggered", ":", "\n", "                    ", "tmp", "=", "{", "}", "\n", "for", "prm", "in", "self", ".", "model", ".", "parameters", "(", ")", ":", "\n", "                        ", "tmp", "[", "prm", "]", "=", "prm", ".", "data", ".", "clone", "(", ")", "\n", "try", ":", "\n", "                            ", "prm", ".", "data", "=", "self", ".", "optimizer", ".", "active_optimizer", ".", "state", "[", "prm", "]", "[", "'ax'", "]", "\n", "", "except", "KeyError", ":", "\n", "                            ", "continue", "\n", "\n", "", "", "", "", "if", "self", ".", "_validation_data", "is", "not", "None", ":", "\n", "                ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# We have a validation set, so compute all the metrics on it.", "\n", "                    ", "val_loss", ",", "num_batches", "=", "self", ".", "_validation_loss", "(", ")", "\n", "val_metrics", "=", "training_util", ".", "get_metrics", "(", "self", ".", "model", ",", "val_loss", ",", "num_batches", ",", "reset", "=", "True", ")", "\n", "\n", "# Check validation metric for early stopping", "\n", "this_epoch_val_metric", "=", "val_metrics", "[", "self", ".", "_validation_metric", "]", "\n", "self", ".", "_metric_tracker", ".", "add_metric", "(", "this_epoch_val_metric", ")", "\n", "\n", "if", "self", ".", "_metric_tracker", ".", "should_stop_early", "(", ")", ":", "\n", "                        ", "logger", ".", "info", "(", "\"Ran out of patience.  Stopping training.\"", ")", "\n", "break", "\n", "\n", "", "", "", "self", ".", "_tensorboard", ".", "log_metrics", "(", "train_metrics", ",", "val_metrics", "=", "val_metrics", ",", "log_to_console", "=", "True", ")", "\n", "\n", "# Create overall metrics dict", "\n", "training_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "training_start_time", "\n", "metrics", "[", "\"training_duration\"", "]", "=", "time", ".", "strftime", "(", "\"%H:%M:%S\"", ",", "time", ".", "gmtime", "(", "training_elapsed_time", ")", ")", "\n", "metrics", "[", "\"training_start_epoch\"", "]", "=", "epoch_counter", "\n", "metrics", "[", "\"training_epochs\"", "]", "=", "epochs_trained", "\n", "metrics", "[", "\"epoch\"", "]", "=", "epoch", "\n", "\n", "for", "key", ",", "value", "in", "train_metrics", ".", "items", "(", ")", ":", "\n", "                ", "metrics", "[", "\"training_\"", "+", "key", "]", "=", "value", "\n", "", "for", "key", ",", "value", "in", "val_metrics", ".", "items", "(", ")", ":", "\n", "                ", "metrics", "[", "\"validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "if", "self", ".", "_metric_tracker", ".", "is_best_so_far", "(", ")", ":", "\n", "# Update all the best_ metrics.", "\n", "# (Otherwise they just stay the same as they were.)", "\n", "                ", "metrics", "[", "'best_epoch'", "]", "=", "epoch", "\n", "for", "key", ",", "value", "in", "val_metrics", ".", "items", "(", ")", ":", "\n", "                    ", "metrics", "[", "\"best_validation_\"", "+", "key", "]", "=", "value", "\n", "\n", "", "self", ".", "_metric_tracker", ".", "best_epoch_metrics", "=", "val_metrics", "\n", "\n", "", "if", "self", ".", "_serialization_dir", ":", "\n", "                ", "dump_metrics", "(", "os", ".", "path", ".", "join", "(", "self", ".", "_serialization_dir", ",", "f'metrics_epoch_{epoch}.json'", ")", ",", "metrics", ")", "\n", "\n", "", "if", "self", ".", "_learning_rate_scheduler", ":", "\n", "# The LRScheduler API is agnostic to whether your schedule requires a validation metric -", "\n", "# if it doesn't, the validation metric passed here is ignored.", "\n", "                ", "self", ".", "_learning_rate_scheduler", ".", "step", "(", "this_epoch_val_metric", ",", "epoch", ")", "\n", "\n", "", "self", ".", "_save_checkpoint", "(", "epoch", ")", "\n", "\n", "epoch_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "epoch_start_time", "\n", "logger", ".", "info", "(", "\"Epoch duration: %s\"", ",", "time", ".", "strftime", "(", "\"%H:%M:%S\"", ",", "time", ".", "gmtime", "(", "epoch_elapsed_time", ")", ")", ")", "\n", "\n", "if", "epoch", "<", "self", ".", "_num_epochs", "-", "1", ":", "\n", "                ", "training_elapsed_time", "=", "time", ".", "time", "(", ")", "-", "training_start_time", "\n", "estimated_time_remaining", "=", "training_elapsed_time", "*", "(", "(", "self", ".", "_num_epochs", "-", "epoch_counter", ")", "/", "float", "(", "epoch", "-", "epoch_counter", "+", "1", ")", "-", "1", ")", "\n", "formatted_time", "=", "str", "(", "datetime", ".", "timedelta", "(", "seconds", "=", "int", "(", "estimated_time_remaining", ")", ")", ")", "\n", "logger", ".", "info", "(", "\"Estimated training time remaining: %s\"", ",", "formatted_time", ")", "\n", "\n", "# Revert parameters back to original if using ASGD", "\n", "", "if", "isinstance", "(", "self", ".", "optimizer", ",", "NTASGDOptimizer", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "if", "self", ".", "optimizer", ".", "triggered", ":", "\n", "                        ", "for", "prm", "in", "self", ".", "model", ".", "parameters", "(", ")", ":", "\n", "                            ", "prm", ".", "data", "=", "tmp", "[", "prm", "]", "\n", "", "", "", "except", "UnboundLocalError", ":", "# Happens when 1st triggered", "\n", "                    ", "continue", "\n", "\n", "", "", "epochs_trained", "+=", "1", "\n", "\n", "# Load the best model state before returning", "\n", "", "best_model_state", "=", "self", ".", "_checkpointer", ".", "best_model_state", "(", ")", "\n", "if", "best_model_state", ":", "\n", "            ", "self", ".", "model", ".", "load_state_dict", "(", "best_model_state", ")", "\n", "\n", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer._save_checkpoint": [[553, 591], ["trainer.LmTrainer._checkpointer.save_checkpoint", "trainer.LmTrainer._moving_average.assign_average_value", "trainer.LmTrainer._metric_tracker.state_dict", "trainer.LmTrainer.optimizer.state_dict", "trainer.LmTrainer._learning_rate_scheduler.state_dict", "trainer.LmTrainer._moving_average.restore", "trainer.LmTrainer.model.state_dict", "trainer.LmTrainer._metric_tracker.is_best_so_far"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict"], ["", "def", "_save_checkpoint", "(", "self", ",", "epoch", ":", "Union", "[", "int", ",", "str", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Saves a checkpoint of the model to self._serialization_dir.\n        Is a no-op if self._serialization_dir is None.\n\n        Parameters\n        ----------\n        epoch : Union[int, str], required.\n            The epoch of training.  If the checkpoint is saved in the middle\n            of an epoch, the parameter is a string with the epoch and timestamp.\n        \"\"\"", "\n", "# If moving averages are used for parameters, we save", "\n", "# the moving average values into checkpoint, instead of the current values.", "\n", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "assign_average_value", "(", ")", "\n", "\n", "# These are the training states we need to persist.", "\n", "", "training_states", "=", "{", "\n", "\"metric_tracker\"", ":", "self", ".", "_metric_tracker", ".", "state_dict", "(", ")", ",", "\n", "\"optimizer\"", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "\"batch_num_total\"", ":", "self", ".", "_batch_num_total", "\n", "}", "\n", "\n", "# If we have a learning rate scheduler, we should persist that too.", "\n", "if", "self", ".", "_learning_rate_scheduler", "is", "not", "None", ":", "\n", "            ", "training_states", "[", "\"learning_rate_scheduler\"", "]", "=", "(", "\n", "self", ".", "_learning_rate_scheduler", ".", "state_dict", "(", ")", "\n", ")", "\n", "\n", "", "self", ".", "_checkpointer", ".", "save_checkpoint", "(", "\n", "model_state", "=", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "epoch", "=", "epoch", ",", "\n", "training_states", "=", "training_states", ",", "\n", "is_best_so_far", "=", "self", ".", "_metric_tracker", ".", "is_best_so_far", "(", ")", ")", "\n", "\n", "# # Restore the original values for parameters so that training will not be affected.", "\n", "if", "self", ".", "_moving_average", "is", "not", "None", ":", "\n", "            ", "self", ".", "_moving_average", ".", "restore", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer._restore_checkpoint": [[592, 645], ["trainer.LmTrainer._checkpointer.restore_checkpoint", "trainer.LmTrainer.model.load_state_dict", "trainer.LmTrainer.optimizer.load_state_dict", "allennlp.training.util.move_optimizer_to_cuda", "isinstance", "training_state.get", "trainer.LmTrainer._learning_rate_scheduler.load_state_dict", "trainer.LmTrainer._metric_tracker.load_state_dict", "trainer.LmTrainer._metric_tracker.clear", "trainer.LmTrainer._metric_tracker.add_metrics", "trainer.LmTrainer._metric_tracker.clear", "int", "training_state[].split"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.load_state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.load_state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.load_state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.load_state_dict"], ["", "", "def", "_restore_checkpoint", "(", "self", ")", "->", "int", ":", "\n", "        ", "\"\"\"\n        Restores the model and training state from the last saved checkpoint.\n        This includes an epoch count and optimizer state, which is serialized separately\n        from model parameters. This function should only be used to continue training -\n        if you wish to load a model for inference/load parts of a model into a new\n        computation graph, you should use the native Pytorch functions:\n        `` model.load_state_dict(torch.load(\"/path/to/model/weights.th\"))``\n\n        If ``self._serialization_dir`` does not exist or does not contain any checkpointed weights,\n        this function will do nothing and return 0.\n\n        Returns\n        -------\n        epoch: int\n            The epoch at which to resume training, which should be one after the epoch\n            in the saved training state.\n        \"\"\"", "\n", "model_state", ",", "training_state", "=", "self", ".", "_checkpointer", ".", "restore_checkpoint", "(", ")", "\n", "\n", "if", "not", "training_state", ":", "\n", "# No checkpoint to restore, start at 0", "\n", "            ", "return", "0", "\n", "\n", "", "self", ".", "model", ".", "load_state_dict", "(", "model_state", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "training_state", "[", "\"optimizer\"", "]", ")", "\n", "if", "self", ".", "_learning_rate_scheduler", "is", "not", "None", "and", "\"learning_rate_scheduler\"", "in", "training_state", ":", "\n", "            ", "self", ".", "_learning_rate_scheduler", ".", "load_state_dict", "(", "training_state", "[", "\"learning_rate_scheduler\"", "]", ")", "\n", "", "training_util", ".", "move_optimizer_to_cuda", "(", "self", ".", "optimizer", ")", "\n", "\n", "# Currently the ``training_state`` contains a serialized ``MetricTracker``.", "\n", "if", "\"metric_tracker\"", "in", "training_state", ":", "\n", "            ", "self", ".", "_metric_tracker", ".", "load_state_dict", "(", "training_state", "[", "\"metric_tracker\"", "]", ")", "\n", "# It used to be the case that we tracked ``val_metric_per_epoch``.", "\n", "", "elif", "\"val_metric_per_epoch\"", "in", "training_state", ":", "\n", "            ", "self", ".", "_metric_tracker", ".", "clear", "(", ")", "\n", "self", ".", "_metric_tracker", ".", "add_metrics", "(", "training_state", "[", "\"val_metric_per_epoch\"", "]", ")", "\n", "# And before that we didn't track anything.", "\n", "", "else", ":", "\n", "            ", "self", ".", "_metric_tracker", ".", "clear", "(", ")", "\n", "\n", "", "if", "isinstance", "(", "training_state", "[", "\"epoch\"", "]", ",", "int", ")", ":", "\n", "            ", "epoch_to_return", "=", "training_state", "[", "\"epoch\"", "]", "+", "1", "\n", "", "else", ":", "\n", "            ", "epoch_to_return", "=", "int", "(", "training_state", "[", "\"epoch\"", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", ")", "+", "1", "\n", "\n", "# For older checkpoints with batch_num_total missing, default to old behavior where", "\n", "# it is unchanged.", "\n", "", "batch_num_total", "=", "training_state", ".", "get", "(", "'batch_num_total'", ")", "\n", "if", "batch_num_total", "is", "not", "None", ":", "\n", "            ", "self", ".", "_batch_num_total", "=", "batch_num_total", "\n", "\n", "", "return", "epoch_to_return", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.LmTrainer.from_params": [[646, 724], ["trainer.TrainerPieces.from_params", "params.pop_int", "params.pop", "params.pop_bool", "params.pop_int", "allennlp.common.util.parse_cuda_device", "params.pop_float", "params.pop_float", "params.pop", "isinstance", "allennlp.training.optimizers.Optimizer.from_params", "params.pop_int", "params.pop_int", "params.pop_float", "params.pop_int", "params.pop_int", "params.pop_bool", "params.pop_bool", "params.pop_int", "params.assert_empty", "cls", "params.pop", "model.cuda.cuda.cuda", "params.pop", "allennlp.training.moving_average.MovingAverage.from_params", "allennlp.training.learning_rate_schedulers.LearningRateScheduler.from_params", "model.cuda.cuda.named_parameters", "params.pop"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "cls", ",", "\n", "params", ":", "Params", ",", "\n", "serialization_dir", ":", "str", ",", "\n", "recover", ":", "bool", "=", "False", ")", "->", "'LmTrainer'", ":", "\n", "        ", "pieces", "=", "TrainerPieces", ".", "from_params", "(", "params", ",", "serialization_dir", ",", "recover", ")", "\n", "\n", "# THIS SUCKS", "\n", "model", "=", "pieces", ".", "model", "\n", "iterator", "=", "pieces", ".", "iterator", "\n", "train_data", "=", "pieces", ".", "train_dataset", "\n", "validation_data", "=", "pieces", ".", "validation_dataset", "\n", "params", "=", "pieces", ".", "params", "\n", "validation_iterator", "=", "pieces", ".", "validation_iterator", "\n", "\n", "# pylint: disable=arguments-differ", "\n", "patience", "=", "params", ".", "pop_int", "(", "\"patience\"", ",", "None", ")", "\n", "validation_metric", "=", "params", ".", "pop", "(", "\"validation_metric\"", ",", "\"-loss\"", ")", "\n", "shuffle", "=", "params", ".", "pop_bool", "(", "\"shuffle\"", ",", "True", ")", "\n", "num_epochs", "=", "params", ".", "pop_int", "(", "\"num_epochs\"", ",", "20", ")", "\n", "cuda_device", "=", "parse_cuda_device", "(", "params", ".", "pop", "(", "\"cuda_device\"", ",", "-", "1", ")", ")", "\n", "grad_norm", "=", "params", ".", "pop_float", "(", "\"grad_norm\"", ",", "None", ")", "\n", "grad_clipping", "=", "params", ".", "pop_float", "(", "\"grad_clipping\"", ",", "None", ")", "\n", "lr_scheduler_params", "=", "params", ".", "pop", "(", "\"learning_rate_scheduler\"", ",", "None", ")", "\n", "\n", "if", "isinstance", "(", "cuda_device", ",", "list", ")", ":", "\n", "            ", "model_device", "=", "cuda_device", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "model_device", "=", "cuda_device", "\n", "", "if", "model_device", ">=", "0", ":", "\n", "# Moving model to GPU here so that the optimizer state gets constructed on", "\n", "# the right device.", "\n", "            ", "model", "=", "model", ".", "cuda", "(", "model_device", ")", "\n", "\n", "", "parameters", "=", "[", "[", "n", ",", "p", "]", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "p", ".", "requires_grad", "]", "\n", "optimizer", "=", "Optimizer", ".", "from_params", "(", "parameters", ",", "params", ".", "pop", "(", "\"optimizer\"", ")", ")", "\n", "if", "\"moving_average\"", "in", "params", ":", "\n", "            ", "moving_average", "=", "MovingAverage", ".", "from_params", "(", "params", ".", "pop", "(", "\"moving_average\"", ")", ",", "parameters", "=", "parameters", ")", "\n", "", "else", ":", "\n", "            ", "moving_average", "=", "None", "\n", "\n", "", "if", "lr_scheduler_params", ":", "\n", "            ", "scheduler", "=", "LearningRateScheduler", ".", "from_params", "(", "optimizer", ",", "lr_scheduler_params", ")", "\n", "", "else", ":", "\n", "            ", "scheduler", "=", "None", "\n", "\n", "", "num_serialized_models_to_keep", "=", "params", ".", "pop_int", "(", "\"num_serialized_models_to_keep\"", ",", "20", ")", "\n", "keep_serialized_model_every_num_seconds", "=", "params", ".", "pop_int", "(", "\n", "\"keep_serialized_model_every_num_seconds\"", ",", "None", ")", "\n", "model_save_interval", "=", "params", ".", "pop_float", "(", "\"model_save_interval\"", ",", "None", ")", "\n", "summary_interval", "=", "params", ".", "pop_int", "(", "\"summary_interval\"", ",", "100", ")", "\n", "histogram_interval", "=", "params", ".", "pop_int", "(", "\"histogram_interval\"", ",", "None", ")", "\n", "should_log_parameter_statistics", "=", "params", ".", "pop_bool", "(", "\"should_log_parameter_statistics\"", ",", "True", ")", "\n", "should_log_learning_rate", "=", "params", ".", "pop_bool", "(", "\"should_log_learning_rate\"", ",", "False", ")", "\n", "log_batch_size_period", "=", "params", ".", "pop_int", "(", "\"log_batch_size_period\"", ",", "None", ")", "\n", "\n", "params", ".", "assert_empty", "(", "cls", ".", "__name__", ")", "\n", "return", "cls", "(", "model", ",", "optimizer", ",", "iterator", ",", "\n", "train_data", ",", "validation_data", ",", "\n", "patience", "=", "patience", ",", "\n", "validation_metric", "=", "validation_metric", ",", "\n", "validation_iterator", "=", "validation_iterator", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_epochs", "=", "num_epochs", ",", "\n", "serialization_dir", "=", "serialization_dir", ",", "\n", "cuda_device", "=", "cuda_device", ",", "\n", "grad_norm", "=", "grad_norm", ",", "\n", "grad_clipping", "=", "grad_clipping", ",", "\n", "learning_rate_scheduler", "=", "scheduler", ",", "\n", "num_serialized_models_to_keep", "=", "num_serialized_models_to_keep", ",", "\n", "keep_serialized_model_every_num_seconds", "=", "keep_serialized_model_every_num_seconds", ",", "\n", "model_save_interval", "=", "model_save_interval", ",", "\n", "summary_interval", "=", "summary_interval", ",", "\n", "histogram_interval", "=", "histogram_interval", ",", "\n", "should_log_parameter_statistics", "=", "should_log_parameter_statistics", ",", "\n", "should_log_learning_rate", "=", "should_log_learning_rate", ",", "\n", "log_batch_size_period", "=", "log_batch_size_period", ",", "\n", "moving_average", "=", "moving_average", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.trainer.TrainerPieces.from_params": [[743, 802], ["allennlp.training.util.datasets_from_params", "set", "logger.info", "allennlp.models.model.Model.from_params", "allennlp.data.vocabulary.Vocabulary.from_params.save_to_files", "allennlp.data.iterators.data_iterator.DataIterator.from_params", "allennlp.data.iterators.data_iterator.DataIterator.from_params.index_with", "params.pop", "allennlp.training.util.datasets_from_params.get", "allennlp.training.util.datasets_from_params.get", "params.pop", "params.pop.pop", "allennlp.models.model.Model.from_params.named_parameters", "allennlp.common.util.get_frozen_and_tunable_parameter_names", "logger.info", "logger.info", "trainer.TrainerPieces", "params.pop", "os.path.exists", "allennlp.data.vocabulary.Vocabulary.from_files", "params.pop", "allennlp.data.vocabulary.Vocabulary.from_params", "os.path.join", "params.pop", "allennlp.data.iterators.data_iterator.DataIterator.from_params", "allennlp.data.iterators.data_iterator.DataIterator.from_params.index_with", "any", "logger.info", "logger.info", "allennlp.common.checks.ConfigurationError", "os.path.join", "os.path.join", "params.pop", "params.pop", "parameter.requires_grad_", "re.search", "allennlp.training.util.datasets_from_params.items"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params"], ["@", "staticmethod", "\n", "def", "from_params", "(", "params", ":", "Params", ",", "serialization_dir", ":", "str", ",", "recover", ":", "bool", "=", "False", ")", "->", "'TrainerPieces'", ":", "\n", "        ", "all_datasets", "=", "training_util", ".", "datasets_from_params", "(", "params", ")", "\n", "datasets_for_vocab_creation", "=", "set", "(", "params", ".", "pop", "(", "\"datasets_for_vocab_creation\"", ",", "all_datasets", ")", ")", "\n", "\n", "for", "dataset", "in", "datasets_for_vocab_creation", ":", "\n", "            ", "if", "dataset", "not", "in", "all_datasets", ":", "\n", "                ", "raise", "ConfigurationError", "(", "f\"invalid 'dataset_for_vocab_creation' {dataset}\"", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "\"From dataset instances, %s will be considered for vocabulary creation.\"", ",", "\n", "\", \"", ".", "join", "(", "datasets_for_vocab_creation", ")", ")", "\n", "\n", "if", "recover", "and", "os", ".", "path", ".", "exists", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", ")", ":", "\n", "            ", "vocab", "=", "Vocabulary", ".", "from_files", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", ")", "\n", "params", ".", "pop", "(", "\"vocabulary\"", ",", "{", "}", ")", "\n", "", "else", ":", "\n", "            ", "vocab", "=", "Vocabulary", ".", "from_params", "(", "\n", "params", ".", "pop", "(", "\"vocabulary\"", ",", "{", "}", ")", ",", "\n", "(", "instance", "for", "key", ",", "dataset", "in", "all_datasets", ".", "items", "(", ")", "\n", "for", "instance", "in", "dataset", "\n", "if", "key", "in", "datasets_for_vocab_creation", ")", "\n", ")", "\n", "\n", "", "model", "=", "Model", ".", "from_params", "(", "vocab", "=", "vocab", ",", "params", "=", "params", ".", "pop", "(", "'model'", ")", ")", "\n", "\n", "# Initializing the model can have side effect of expanding the vocabulary", "\n", "vocab", ".", "save_to_files", "(", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "\"vocabulary\"", ")", ")", "\n", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "params", ".", "pop", "(", "\"iterator\"", ")", ")", "\n", "iterator", ".", "index_with", "(", "model", ".", "vocab", ")", "\n", "validation_iterator_params", "=", "params", ".", "pop", "(", "\"validation_iterator\"", ",", "None", ")", "\n", "if", "validation_iterator_params", ":", "\n", "            ", "validation_iterator", "=", "DataIterator", ".", "from_params", "(", "validation_iterator_params", ")", "\n", "validation_iterator", ".", "index_with", "(", "model", ".", "vocab", ")", "\n", "", "else", ":", "\n", "            ", "validation_iterator", "=", "None", "\n", "\n", "", "train_data", "=", "all_datasets", "[", "'train'", "]", "\n", "validation_data", "=", "all_datasets", ".", "get", "(", "'validation'", ")", "\n", "test_data", "=", "all_datasets", ".", "get", "(", "'test'", ")", "\n", "\n", "trainer_params", "=", "params", ".", "pop", "(", "\"trainer\"", ")", "\n", "no_grad_regexes", "=", "trainer_params", ".", "pop", "(", "\"no_grad\"", ",", "(", ")", ")", "\n", "for", "name", ",", "parameter", "in", "model", ".", "named_parameters", "(", ")", ":", "\n", "            ", "if", "any", "(", "re", ".", "search", "(", "regex", ",", "name", ")", "for", "regex", "in", "no_grad_regexes", ")", ":", "\n", "                ", "parameter", ".", "requires_grad_", "(", "False", ")", "\n", "\n", "", "", "frozen_parameter_names", ",", "tunable_parameter_names", "=", "get_frozen_and_tunable_parameter_names", "(", "model", ")", "\n", "logger", ".", "info", "(", "\"Following parameters are Frozen  (without gradient):\"", ")", "\n", "for", "name", "in", "frozen_parameter_names", ":", "\n", "            ", "logger", ".", "info", "(", "name", ")", "\n", "", "logger", ".", "info", "(", "\"Following parameters are Tunable (with gradient):\"", ")", "\n", "for", "name", "in", "tunable_parameter_names", ":", "\n", "            ", "logger", ".", "info", "(", "name", ")", "\n", "\n", "", "return", "TrainerPieces", "(", "model", ",", "iterator", ",", "\n", "train_data", ",", "validation_data", ",", "test_data", ",", "\n", "validation_iterator", ",", "trainer_params", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.__init__": [[20, 35], ["list", "torch.optim.SGD", "torch.optim.ASGD"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "params", ":", "Iterable", "[", "torch", ".", "nn", ".", "Parameter", "]", ",", "\n", "lr", ":", "float", ",", "\n", "weight_decay", ":", "float", "=", "0", ",", "\n", "triggered", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "params", "=", "list", "(", "params", ")", "\n", "self", ".", "_triggered", "=", "triggered", "\n", "self", ".", "_sgd", "=", "torch", ".", "optim", ".", "SGD", "(", "params", ",", "\n", "lr", "=", "lr", ",", "\n", "weight_decay", "=", "weight_decay", ")", "\n", "self", ".", "_asgd", "=", "torch", ".", "optim", ".", "ASGD", "(", "params", ",", "\n", "lr", "=", "lr", ",", "\n", "t0", "=", "0", ",", "\n", "lambd", "=", "0.0", ",", "\n", "weight_decay", "=", "weight_decay", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.trigger": [[37, 40], ["logger.info"], "methods", ["None"], ["", "def", "trigger", "(", "self", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'Triggering ASGD'", ")", "\n", "self", ".", "_triggered", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.triggered": [[41, 44], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "triggered", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_triggered", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.active_optimizer": [[45, 51], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "active_optimizer", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "triggered", ":", "\n", "            ", "return", "self", ".", "_asgd", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "_sgd", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.__getstate__": [[53, 58], ["None"], "methods", ["None"], ["", "", "def", "__getstate__", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'_triggered'", ":", "self", ".", "_triggered", ",", "\n", "'_sgd'", ":", "self", ".", "_sgd", ",", "\n", "'_asgd'", ":", "self", ".", "_asgd", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.__setstate__": [[60, 62], ["nt_asgd.NTASGDOptimizer.__dict__.update"], "methods", ["None"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "self", ".", "__dict__", ".", "update", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict": [[63, 68], ["nt_asgd.NTASGDOptimizer._sgd.state_dict", "nt_asgd.NTASGDOptimizer._asgd.state_dict"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "return", "{", "\n", "'_triggered'", ":", "self", ".", "_triggered", ",", "\n", "'_sgd'", ":", "self", ".", "_sgd", ".", "state_dict", "(", ")", ",", "\n", "'_asgd'", ":", "self", ".", "_asgd", ".", "state_dict", "(", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.__repr__": [[70, 72], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "f'NTASGDOptimizer(triggered={self._triggered})'", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.load_state_dict": [[73, 78], ["copy.deepcopy", "nt_asgd.NTASGDOptimizer._sgd.load_state_dict", "nt_asgd.NTASGDOptimizer._asgd.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.load_state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.load_state_dict"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "state_dict", "=", "deepcopy", "(", "state_dict", ")", "\n", "self", ".", "_triggered", "=", "state_dict", "[", "'_triggered'", "]", "\n", "self", ".", "_sgd", ".", "load_state_dict", "(", "state_dict", "[", "'_sgd'", "]", ")", "\n", "self", ".", "_asgd", ".", "load_state_dict", "(", "state_dict", "[", "'_asgd'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.zero_grad": [[80, 82], ["nt_asgd.NTASGDOptimizer.active_optimizer.zero_grad"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.zero_grad"], ["", "def", "zero_grad", "(", "self", ")", ":", "\n", "        ", "self", ".", "active_optimizer", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.step": [[83, 85], ["nt_asgd.NTASGDOptimizer.active_optimizer.step"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "self", ".", "active_optimizer", ".", "step", "(", "closure", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.add_param_group": [[86, 88], ["nt_asgd.NTASGDOptimizer.active_optimizer.add_param_group"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.add_param_group"], ["", "def", "add_param_group", "(", "self", ",", "param_group", ")", ":", "\n", "        ", "self", ".", "active_optimizer", ".", "add_param_group", "(", "param_group", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.param_groups": [[89, 92], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "param_groups", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "active_optimizer", ".", "param_groups", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state": [[93, 96], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "state", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "active_optimizer", ".", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.__init__": [[105, 122], ["isinstance", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "\n", "optimizer", ":", "torch", ".", "optim", ".", "Optimizer", ",", "\n", "non_monotone_interval", ":", "int", ",", "\n", "mode", ":", "str", "=", "'min'", ",", "\n", "last_epoch", ":", "int", "=", "-", "1", ")", "->", "None", ":", "\n", "        ", "if", "not", "isinstance", "(", "optimizer", ",", "NTASGDOptimizer", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "'You must use an NTASDGOptimizer in '", "\n", "'order to use an NTASGDScheduler.'", ")", "\n", "", "if", "mode", "not", "in", "[", "'min'", ",", "'max'", "]", ":", "\n", "            ", "raise", "ConfigurationError", "(", "'Mode can either be \"min\" or \"max\"'", ")", "\n", "\n", "", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "non_monotone_interval", "=", "non_monotone_interval", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "last_epoch", "=", "last_epoch", "\n", "\n", "self", ".", "history", ":", "List", "[", "float", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step": [[123, 144], ["logger.debug", "logger.debug", "nt_asgd.NTASGDScheduler.history.append", "nt_asgd.NTASGDScheduler.history.append", "min", "nt_asgd.NTASGDScheduler.optimizer.trigger", "max"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.trigger"], ["", "def", "step", "(", "self", ",", "metric", ":", "float", "=", "None", ",", "epoch", ":", "int", "=", "None", ")", "->", "None", ":", "\n", "        ", "logger", ".", "debug", "(", "'Optimizer: %s'", ",", "self", ".", "optimizer", ".", "active_optimizer", ")", "\n", "logger", ".", "debug", "(", "'Metric: %0.4f'", ",", "metric", ")", "\n", "\n", "# Don't need to do anything if we've already switched from SGD to", "\n", "# ASGD.", "\n", "if", "self", ".", "optimizer", ".", "triggered", ":", "\n", "            ", "return", "\n", "# Otherwise check if it is time to trigger", "\n", "", "if", "epoch", "<=", "self", ".", "non_monotone_interval", ":", "\n", "            ", "self", ".", "history", ".", "append", "(", "metric", ")", "\n", "return", "\n", "", "if", "self", ".", "mode", "==", "'min'", ":", "\n", "            ", "best", "=", "min", "(", "self", ".", "history", "[", ":", "-", "self", ".", "non_monotone_interval", "]", ")", "\n", "worse_off", "=", "metric", ">", "best", "\n", "", "elif", "self", ".", "mode", "==", "'max'", ":", "\n", "            ", "best", "=", "max", "(", "self", ".", "history", "[", ":", "-", "self", ".", "non_monotone_interval", "]", ")", "\n", "worse_off", "=", "metric", "<", "best", "\n", "", "if", "worse_off", ":", "\n", "            ", "self", ".", "optimizer", ".", "trigger", "(", ")", "\n", "", "self", ".", "history", ".", "append", "(", "metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd_test.NTASGDOptimizerTest.setUp": [[8, 15], ["torch.nn.Linear", "kglm.training.nt_asgd.NTASGDOptimizer", "super().setUp", "nt_asgd_test.NTASGDOptimizerTest.model.parameters"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "dim", "=", "10", "\n", "self", ".", "model", "=", "torch", ".", "nn", ".", "Linear", "(", "10", ",", "10", ")", "\n", "self", ".", "optim", "=", "NTASGDOptimizer", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "30.0", ",", "\n", "weight_decay", "=", "1.2e-6", ")", "\n", "super", "(", ")", ".", "setUp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd_test.NTASGDOptimizerTest.test_trigger": [[16, 24], ["nt_asgd_test.NTASGDOptimizerTest.optim.trigger"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.trigger"], ["", "def", "test_trigger", "(", "self", ")", ":", "\n", "# Active optimizer should be SGD before triggering", "\n", "        ", "assert", "self", ".", "optim", ".", "active_optimizer", "==", "self", ".", "optim", ".", "_sgd", "\n", "assert", "not", "self", ".", "optim", ".", "triggered", "\n", "# Active optimizer should be ASGD after triggering", "\n", "self", ".", "optim", ".", "trigger", "(", ")", "\n", "assert", "self", ".", "optim", ".", "active_optimizer", "==", "self", ".", "optim", ".", "_asgd", "\n", "assert", "self", ".", "optim", ".", "triggered", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd_test.NTASGDOptimizerTest.test_awd_lstm_magic_trick": [[25, 53], ["nt_asgd_test.NTASGDOptimizerTest.optim.trigger", "range", "nt_asgd_test.NTASGDOptimizerTest.model.parameters", "nt_asgd_test.NTASGDOptimizerTest.model.parameters", "nt_asgd_test.NTASGDOptimizerTest.optim.zero_grad", "torch.randn", "nt_asgd_test.NTASGDOptimizerTest.model", "torch.randn", "loss.backward", "nt_asgd_test.NTASGDOptimizerTest.optim.step", "prm.data.clone", "[].clone", "tmp[].clone"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.trigger", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.zero_grad", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step"], ["", "def", "test_awd_lstm_magic_trick", "(", "self", ")", ":", "\n", "# Here we verify we can replicate the confusing trick done in:", "\n", "#   github.com/salesforce/awd-lstm-lm/main.py 244-260", "\n", "\n", "# We need to be in asgd mode.", "\n", "        ", "self", ".", "optim", ".", "trigger", "(", ")", "\n", "\n", "# Perform a couple iterations of \"training\".", "\n", "for", "_", "in", "range", "(", "3", ")", ":", "\n", "            ", "self", ".", "optim", ".", "zero_grad", "(", ")", "\n", "x", "=", "torch", ".", "randn", "(", "1", ",", "10", ")", "\n", "y_hat", "=", "self", ".", "model", "(", "x", ")", "\n", "y_true", "=", "torch", ".", "randn", "(", "1", ",", "10", ")", "\n", "loss", "=", "(", "y_hat", "-", "y_true", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optim", ".", "step", "(", ")", "\n", "\n", "# Now for the trick: assign the model parameters to the asgd average during evaluation.", "\n", "", "tmp", "=", "{", "}", "\n", "for", "prm", "in", "self", ".", "model", ".", "parameters", "(", ")", ":", "\n", "            ", "tmp", "[", "prm", "]", "=", "prm", ".", "data", ".", "clone", "(", ")", "\n", "prm", ".", "data", "=", "self", ".", "optim", ".", "active_optimizer", ".", "state", "[", "prm", "]", "[", "'ax'", "]", ".", "clone", "(", ")", "\n", "\n", "# HERE IS WHERE WE WOULD EVALUATE", "\n", "\n", "# Once we're done evaluating we reset the params for training", "\n", "", "for", "prm", "in", "self", ".", "model", ".", "parameters", "(", ")", ":", "\n", "            ", "prm", ".", "data", "=", "tmp", "[", "prm", "]", ".", "clone", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd_test.NTASGDSchedulerTest.setUp": [[56, 65], ["torch.nn.Linear", "kglm.training.nt_asgd.NTASGDOptimizer", "kglm.training.nt_asgd.NTASGDScheduler", "super().setUp", "nt_asgd_test.NTASGDSchedulerTest.model.parameters"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "dim", "=", "10", "\n", "self", ".", "model", "=", "torch", ".", "nn", ".", "Linear", "(", "10", ",", "10", ")", "\n", "self", ".", "optim", "=", "NTASGDOptimizer", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "\n", "lr", "=", "30.0", ",", "\n", "weight_decay", "=", "1.2e-6", ")", "\n", "self", ".", "scheduler", "=", "NTASGDScheduler", "(", "optimizer", "=", "self", ".", "optim", ",", "\n", "non_monotone_interval", "=", "3", ")", "\n", "super", "(", ")", ".", "setUp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd_test.NTASGDSchedulerTest.test_scheduler_does_not_trigger_early": [[66, 72], ["nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step"], ["", "def", "test_scheduler_does_not_trigger_early", "(", "self", ")", ":", "\n", "        ", "assert", "not", "self", ".", "optim", ".", "triggered", "\n", "self", ".", "scheduler", ".", "step", "(", "0.0", ",", "0", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "0.1", ",", "1", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "0.2", ",", "2", ")", "\n", "assert", "not", "self", ".", "optim", ".", "triggered", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd_test.NTASGDSchedulerTest.test_scheduler_does_not_trigger_if_always_improving": [[73, 83], ["nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step"], ["", "def", "test_scheduler_does_not_trigger_if_always_improving", "(", "self", ")", ":", "\n", "        ", "assert", "not", "self", ".", "optim", ".", "triggered", "\n", "self", ".", "scheduler", ".", "step", "(", "10", ",", "0", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "9", ",", "1", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "8", ",", "2", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "7", ",", "3", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "6", ",", "4", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "5", ",", "5", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "4", ",", "6", ")", "\n", "assert", "not", "self", ".", "optim", ".", "triggered", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd_test.NTASGDSchedulerTest.test_scheduler_does_trigger_when_expected": [[84, 92], ["nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step", "nt_asgd_test.NTASGDSchedulerTest.scheduler.step"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDScheduler.step"], ["", "def", "test_scheduler_does_trigger_when_expected", "(", "self", ")", ":", "\n", "        ", "assert", "not", "self", ".", "optim", ".", "triggered", "\n", "self", ".", "scheduler", ".", "step", "(", "10", ",", "0", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "9", ",", "1", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "8", ",", "2", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "7", ",", "3", ")", "\n", "self", ".", "scheduler", ".", "step", "(", "11", ",", "4", ")", "\n", "assert", "self", ".", "optim", ".", "triggered", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Perplexity.__init__": [[22, 25], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_sum_log_p", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Perplexity.__call__": [[26, 49], ["perplexity.Perplexity.unwrap_to_tensors", "torch.log_softmax", "torch.log_softmax", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "mask.float().sum", "torch.gather().squeeze.sum", "torch.gather().squeeze.sum", "torch.numel", "torch.numel", "torch.numel", "torch.numel", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "mask.float", "labels.unsqueeze", "mask.float"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "\n", "logits", ":", "torch", ".", "Tensor", ",", "\n", "labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        logits : ``torch.Tensor``, required.\n            A tensor of class logits of shape (batch_size, sequence_length, num_classes).\n        labels : ``torch.Tensor``, required.\n            A tensor of integer class labels of shape (batch_size, sequence_length).\n        mask: ``torch.Tensor``, optional (default = None).\n            A binary mask tensor of shape (batch_size, sequence_length).\n        \"\"\"", "\n", "logits", ",", "labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "logits", ",", "labels", ",", "mask", ")", "\n", "log_p", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "2", ")", "\n", "log_p", "=", "torch", ".", "gather", "(", "log_p", ",", "dim", "=", "2", ",", "index", "=", "labels", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "self", ".", "_sum_log_p", "+=", "(", "mask", ".", "float", "(", ")", "*", "log_p", ")", ".", "sum", "(", ")", "\n", "self", ".", "_total_count", "+=", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_sum_log_p", "+=", "log_p", ".", "sum", "(", ")", "\n", "self", ".", "_total_count", "+=", "torch", ".", "numel", "(", "labels", ")", "# pylint: disable=no-member", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Perplexity.get_metric": [[50, 57], ["cross_entropy.exp", "cross_entropy.exp.Perplexity.reset"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset"], ["", "", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", ")", "->", "float", ":", "\n", "        ", "cross_entropy", "=", "-", "self", ".", "_sum_log_p", "/", "self", ".", "_total_count", "\n", "perplexity", "=", "cross_entropy", ".", "exp", "(", ")", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "perplexity", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Perplexity.reset": [[58, 62], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "_sum_log_p", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.UnknownPenalizedPerplexity.__init__": [[78, 95], ["vocabulary.get_vocab_size", "vocabulary.get_token_index", "math.log"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocabulary", ":", "ExtendedVocabulary", ",", "\n", "namespace", ":", "str", "=", "'tokens'", ",", "\n", "oov_token", ":", "str", "=", "DEFAULT_OOV_TOKEN", ")", "->", "None", ":", "\n", "# Compute the penalty weight applied to p(<unk>).", "\n", "        ", "unk_vocab_size", "=", "vocabulary", ".", "get_vocab_size", "(", "namespace", "+", "'_unk'", ")", "\n", "if", "unk_vocab_size", ">", "0", ":", "\n", "            ", "self", ".", "_unk_penalty", "=", "math", ".", "log", "(", "unk_vocab_size", ")", "# pylint: disable=no-member", "\n", "", "else", ":", "\n", "            ", "self", ".", "_unk_penalty", "=", "0.0", "\n", "\n", "# Identify the index of the <unk> token.", "\n", "", "self", ".", "_unk_idx", "=", "vocabulary", ".", "get_token_index", "(", "oov_token", ",", "namespace", "=", "namespace", ")", "\n", "\n", "# Initialize the metric variables.", "\n", "self", ".", "_sum_log_p", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.UnknownPenalizedPerplexity.__call__": [[96, 123], ["perplexity.UnknownPenalizedPerplexity.unwrap_to_tensors", "labels.eq", "torch.cross_entropy", "torch.cross_entropy", "mask.sum", "log_p.sum", "torch.numel", "torch.numel", "torch.numel", "torch.numel"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "\n", "logits", ":", "torch", ".", "Tensor", ",", "\n", "labels", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        logits : ``torch.Tensor``, required.\n            A tensor of class logits of shape (batch_size, k, sequence_length).\n        labels : ``torch.Tensor``, required.\n            A tensor of integer class labels of shape (batch_size, sequence_length).\n        mask: ``torch.Tensor``, optional (default = None).\n            A binary mask tensor of shape (batch_size, sequence_length).\n        \"\"\"", "\n", "logits", ",", "labels", ",", "mask", "=", "self", ".", "unwrap_to_tensors", "(", "logits", ",", "labels", ",", "mask", ")", "\n", "log_p", "=", "-", "F", ".", "cross_entropy", "(", "logits", ",", "labels", ",", "reduction", "=", "'none'", ")", "\n", "\n", "# Apply penalty to unks", "\n", "unk_ids", "=", "labels", ".", "eq", "(", "self", ".", "_unk_idx", ")", "\n", "log_p", "[", "unk_ids", "]", "-=", "self", ".", "_unk_penalty", "\n", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "self", ".", "_sum_log_p", "+=", "(", "mask", "*", "log_p", ")", ".", "sum", "(", ")", "\n", "self", ".", "_total_count", "+=", "mask", ".", "sum", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_sum_log_p", "+=", "log_p", ".", "sum", "(", ")", "\n", "self", ".", "_total_count", "+=", "torch", ".", "numel", "(", "labels", ")", "# pylint: disable=no-member", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.UnknownPenalizedPerplexity.get_metric": [[124, 131], ["math.exp", "math.exp.UnknownPenalizedPerplexity.reset"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset"], ["", "", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", ")", "->", "float", ":", "\n", "        ", "cross_entropy", "=", "-", "self", ".", "_sum_log_p", "/", "self", ".", "_total_count", "\n", "perplexity", "=", "math", ".", "exp", "(", "cross_entropy", ")", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "perplexity", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.UnknownPenalizedPerplexity.reset": [[132, 136], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", "->", "None", ":", "\n", "        ", "self", ".", "_sum_log_p", "=", "0.0", "\n", "self", ".", "_total_count", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.__init__": [[141, 144], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "numerator", "=", "0.0", "\n", "self", ".", "denominator", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.__call__": [[145, 148], ["None"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "numerator", ",", "denominator", ")", ":", "\n", "        ", "self", ".", "numerator", "+=", "numerator", "\n", "self", ".", "denominator", "+=", "denominator", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric": [[149, 159], ["float", "float", "math.exp", "perplexity.Ppl.reset", "float"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset"], ["", "@", "overrides", "\n", "def", "get_metric", "(", "self", ",", "reset", ":", "bool", ")", ":", "\n", "        ", "ratio", "=", "float", "(", "self", ".", "numerator", ")", "/", "(", "float", "(", "self", ".", "denominator", ")", "+", "1e-13", ")", "\n", "if", "ratio", ">", "20", ":", "\n", "            ", "ppl", "=", "float", "(", "'inf'", ")", "\n", "", "else", ":", "\n", "            ", "ppl", "=", "math", ".", "exp", "(", "ratio", ")", "\n", "", "if", "reset", ":", "\n", "            ", "self", ".", "reset", "(", ")", "\n", "", "return", "ppl", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.reset": [[160, 165], ["logger.debug"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "reset", "(", "self", ")", "->", "None", ":", "\n", "        ", "logger", ".", "debug", "(", "'Resetting Ppl'", ")", "\n", "self", ".", "numerator", "=", "0.0", "\n", "self", ".", "denominator", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.__init__": [[26, 46], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.normalize", "torch.normalize", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "max_embeddings", ":", "int", ")", "->", "None", ":", "\n", "        ", "super", "(", "DynamicEmbedding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "_embedding_dim", "=", "embedding_dim", "\n", "self", ".", "_max_embeddings", "=", "max_embeddings", "\n", "self", ".", "_initial_embedding", "=", "Parameter", "(", "F", ".", "normalize", "(", "torch", ".", "randn", "(", "embedding_dim", ")", ",", "dim", "=", "0", ")", ")", "\n", "\n", "self", ".", "_distance_scalar", "=", "Parameter", "(", "torch", ".", "tensor", "(", "1e-6", ")", ")", "# pylint: disable=E1102", "\n", "self", ".", "_embedding_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "embedding_dim", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "_delta_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "embedding_dim", ",", "\n", "bias", "=", "False", ")", "\n", "\n", "self", ".", "embeddings", ":", "torch", ".", "Tensor", "=", "None", "# Storage for embeddings", "\n", "self", ".", "num_embeddings", ":", "torch", ".", "Tensor", "=", "None", "# Tracks how many embeddings are in use", "\n", "self", ".", "last_seen", ":", "torch", ".", "Tensor", "=", "None", "# Tracks last time embedding was seen", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.reset_states": [[47, 62], ["dynamic_embeddings.DynamicEmbedding._initial_embedding.new_zeros", "dynamic_embeddings.DynamicEmbedding._initial_embedding.new_zeros", "dynamic_embeddings.DynamicEmbedding._initial_embedding.new_zeros", "dynamic_embeddings.DynamicEmbedding.add_embeddings"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.add_embeddings"], ["", "def", "reset_states", "(", "self", ",", "batch_size", ":", "int", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Resets the DynamicEmbedding module for use on a new batch of sequences.\n\n        Parameters\n        ----------\n        batch_size : ``int``\n            The batch_size of the new sequence.\n        \"\"\"", "\n", "self", ".", "embeddings", "=", "self", ".", "_initial_embedding", ".", "new_zeros", "(", "batch_size", ",", "self", ".", "_max_embeddings", ",", "\n", "self", ".", "_embedding_dim", ")", "\n", "self", ".", "num_embeddings", "=", "self", ".", "_initial_embedding", ".", "new_zeros", "(", "batch_size", ",", "dtype", "=", "torch", ".", "int64", ")", "\n", "self", ".", "last_seen", "=", "self", ".", "_initial_embedding", ".", "new_zeros", "(", "batch_size", ",", "self", ".", "_max_embeddings", ",", "\n", "dtype", "=", "torch", ".", "int64", ")", "\n", "self", ".", "add_embeddings", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.detach_states": [[63, 69], ["dynamic_embeddings.DynamicEmbedding.embeddings.detach"], "methods", ["None"], ["", "def", "detach_states", "(", "self", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Detaches embeddings from the computation graph. This can be neccesary when working with\n        long sequences.\n        \"\"\"", "\n", "self", ".", "embeddings", "=", "self", ".", "embeddings", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.add_embeddings": [[70, 101], ["dynamic_embeddings.DynamicEmbedding._initial_embedding.repeat", "torch.normalize", "torch.normalize", "torch.normalize.squeeze", "dynamic_embeddings.DynamicEmbedding.num_embeddings.new_ones", "torch.randn_like", "torch.randn_like", "torch.randn_like", "torch.randn_like", "dynamic_embeddings.DynamicEmbedding.sum", "dynamic_embeddings.DynamicEmbedding.sum"], "methods", ["None"], ["", "def", "add_embeddings", "(", "self", ",", "\n", "timestep", ":", "int", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Adds new embeddings to the current collection of embeddings.\n\n        Parameters\n        ----------\n        timestep: ``int``, (optional)\n            The current time step.\n        mask: ``Optional[torch.Tensor]``\n            A tensor of shape ``(batch_size)`` indicating which sequences to add a new dynamic\n            embedding to. If no mask is provided then a new embedding is added for each sequence\n            in the batch.\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "num_embeddings", ".", "shape", "[", "0", "]", "\n", "mask", "=", "self", ".", "num_embeddings", ".", "new_ones", "(", "batch_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "", "elif", "mask", ".", "sum", "(", ")", "==", "0", ":", "\n", "            ", "return", "\n", "\n", "# Embeddings are initialized by adding a small amount of random noise to the initial", "\n", "# embedding tensor then normalizing.", "\n", "", "initial", "=", "self", ".", "_initial_embedding", ".", "repeat", "(", "(", "mask", ".", "sum", "(", ")", ",", "1", ",", "1", ")", ")", "\n", "noise", "=", "1e-4", "*", "torch", ".", "randn_like", "(", "initial", ")", "# 1e-4 is a magic number from the original implementation", "\n", "unnormalized", "=", "initial", "+", "noise", "\n", "normalized", "=", "F", ".", "normalize", "(", "unnormalized", ",", "dim", "=", "-", "1", ")", "\n", "\n", "self", ".", "embeddings", "[", "mask", ",", "self", ".", "num_embeddings", "[", "mask", "]", "]", "=", "normalized", ".", "squeeze", "(", ")", "\n", "self", ".", "last_seen", "[", "mask", ",", "self", ".", "num_embeddings", "[", "mask", "]", "]", "=", "timestep", "\n", "self", ".", "num_embeddings", "[", "mask", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.update_embeddings": [[102, 150], ["dynamic_embeddings.DynamicEmbedding._delta_projection", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "score.view.view.view", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.normalize", "torch.normalize", "torch.normalize.squeeze", "dynamic_embeddings.DynamicEmbedding.num_embeddings.new_ones", "hidden.clone", "hidden.view", "dynamic_embeddings.DynamicEmbedding.view", "dynamic_embeddings.DynamicEmbedding.sum", "dynamic_embeddings.DynamicEmbedding.sum"], "methods", ["None"], ["", "def", "update_embeddings", "(", "self", ",", "\n", "hidden", ":", "torch", ".", "Tensor", ",", "\n", "update_indices", ":", "torch", ".", "Tensor", ",", "\n", "timestep", ":", "int", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Updates existing embeddings.\n\n        Parameters\n        ----------\n        hidden : ``torch.Tensor``\n            A tensor of shape ``(batch_size, embedding_dim)`` used to update existing embeddings.\n        update_indices : ``torch.Tensor``\n            A tensor of shape ``(batch_size)`` whose elements specify which of the existing\n            embeddings to update. Only one embedding per sequence can be updated at a time.\n        timestep : ``int``\n            The current time step.\n        mask: ``Optional[torch.Tensor]``\n            A tensor of shape ``(batch_size)`` indicating which sequences in the batch to update\n            the dynamic embeddings for. If a mask is not provided then all sequences will be\n            updated.\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "num_embeddings", ".", "shape", "[", "0", "]", "\n", "mask", "=", "self", ".", "num_embeddings", ".", "new_ones", "(", "batch_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "", "elif", "mask", ".", "sum", "(", ")", "==", "0", ":", "\n", "            ", "return", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "mask", ".", "sum", "(", ")", "\n", "\n", "", "embeddings", "=", "self", ".", "embeddings", "[", "mask", ",", "update_indices", "[", "mask", "]", "]", "\n", "hidden", "=", "hidden", ".", "clone", "(", ")", "[", "mask", "]", "\n", "\n", "# Equation 8 in the paper.", "\n", "projected", "=", "self", ".", "_delta_projection", "(", "embeddings", ")", "\n", "score", "=", "torch", ".", "bmm", "(", "hidden", ".", "view", "(", "batch_size", ",", "1", ",", "-", "1", ")", ",", "\n", "projected", ".", "view", "(", "batch_size", ",", "-", "1", ",", "1", ")", ")", "\n", "score", "=", "score", ".", "view", "(", "batch_size", ",", "1", ")", "\n", "delta", "=", "torch", ".", "sigmoid", "(", "score", ")", "\n", "\n", "unnormalized", "=", "delta", "*", "embeddings", "+", "(", "1", "-", "delta", ")", "*", "hidden", "\n", "normalized", "=", "F", ".", "normalize", "(", "unnormalized", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# If the batch size is one, our approach of indexing with masks will drop the batch", "\n", "# dimension when accessing self.embeddings. Accordingly, the batch dimension of", "\n", "# normalized needs to be dropped in this case in order for assignment to work.", "\n", "self", ".", "embeddings", "[", "mask", ",", "update_indices", "[", "mask", "]", "]", "=", "normalized", ".", "squeeze", "(", "0", ")", "\n", "self", ".", "last_seen", "[", "mask", ",", "update_indices", "[", "mask", "]", "]", "=", "timestep", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.forward": [[151, 219], ["dynamic_embeddings.DynamicEmbedding._embedding_projection", "hidden[].unsqueeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "bilinear.view.view.view", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "dynamic_embeddings.DynamicEmbedding.num_embeddings[].unsqueeze", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat", "torch.arange().repeat.lt", "torch.arange().repeat.lt", "dynamic_embeddings.DynamicEmbedding.num_embeddings.new_ones", "dynamic_embeddings.DynamicEmbedding.transpose", "dynamic_embeddings.DynamicEmbedding.sum", "float", "torch.cross_entropy", "torch.cross_entropy", "dynamic_embeddings.DynamicEmbedding.sum", "dynamic_embeddings.DynamicEmbedding.sum", "dynamic_embeddings.DynamicEmbedding.last_seen[].float", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "hidden", ":", "torch", ".", "Tensor", ",", "\n", "target", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "mask", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Computes logits over the existing embeddings given the current hidden state. If target\n        ids are provided then a loss is returned as well.\n\n        Parameters\n        ----------\n        hidden : ``torch.Tensor``\n            A tensor with shape ``(batch_size, embedding_dim)`` containing the current hidden\n            states.\n        target : ``Optional[torch.Tensor]``\n            An optional tensor with shape ``(batch_size,)`` containing the target ids.\n        mask : ``Optional[torch.Tensor]``\n            An optional tensor with shape ``(batch_size,)`` indicating which terms to include in\n            the final loss.\n\n        Returns\n        -------\n        An output dictionary consisting of:\n        logits : ``torch.Tensor``\n            The entity prediction logits.\n        logit_mask : ``torch.Tensor``\n            Mask for the logit tensor.\n        loss : ``Optional[torch.Tensor]``\n            The loss.\n        \"\"\"", "\n", "if", "mask", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "num_embeddings", ".", "shape", "[", "0", "]", "\n", "mask", "=", "self", ".", "num_embeddings", ".", "new_ones", "(", "batch_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "", "elif", "mask", ".", "sum", "(", ")", "==", "0", ":", "\n", "            ", "return", "{", "'loss'", ":", "0.0", "}", "\n", "", "else", ":", "\n", "            ", "batch_size", "=", "mask", ".", "sum", "(", ")", "\n", "\n", "# First half of equation 4.", "\n", "", "embeddings", "=", "self", ".", "embeddings", "[", "mask", "]", "\n", "projected_embeddings", "=", "self", ".", "_embedding_projection", "(", "embeddings", ")", "\n", "hidden", "=", "hidden", "[", "mask", "]", ".", "unsqueeze", "(", "1", ")", "\n", "bilinear", "=", "torch", ".", "bmm", "(", "hidden", ",", "projected_embeddings", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "bilinear", "=", "bilinear", ".", "view", "(", "batch_size", ",", "-", "1", ")", "\n", "\n", "# Second half of equation 4.", "\n", "distance_score", "=", "torch", ".", "exp", "(", "self", ".", "_distance_scalar", "*", "self", ".", "last_seen", "[", "mask", "]", ".", "float", "(", ")", ")", "\n", "logits", "=", "bilinear", "+", "distance_score", "\n", "\n", "# Since we pre-allocate the embedding array, logits includes scores for all of the", "\n", "# embeddings which have not yet been initialized. We create a mask to indicate which scores", "\n", "# should be used for prediction / loss calculation.", "\n", "num_embeddings", "=", "self", ".", "num_embeddings", "[", "mask", "]", ".", "unsqueeze", "(", "1", ")", "\n", "arange", "=", "torch", ".", "arange", "(", "self", ".", "_max_embeddings", ",", "device", "=", "num_embeddings", ".", "device", ")", ".", "repeat", "(", "mask", ".", "sum", "(", ")", ",", "1", ")", "\n", "logit_mask", "=", "arange", ".", "lt", "(", "num_embeddings", ")", "\n", "logits", "[", "logit_mask", "!=", "1", "]", "=", "-", "float", "(", "'inf'", ")", "\n", "\n", "out", "=", "{", "\n", "'logits'", ":", "logits", ",", "\n", "'logit_mask'", ":", "logit_mask", "\n", "}", "\n", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "target", "=", "target", "[", "mask", "]", "\n", "loss", "=", "F", ".", "cross_entropy", "(", "logits", ",", "target", ",", "reduction", "=", "'none'", ")", "\n", "out", "[", "'loss'", "]", "=", "loss", "\n", "\n", "", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.splitcross.SplitCrossEntropyLoss.__init__": [[11, 25], ["torch.Module.__init__", "collections.defaultdict", "len", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "hidden_size", ",", "splits", ",", "verbose", "=", "False", ")", ":", "\n", "# We assume splits is [0, split1, split2, N] where N >= |V|", "\n", "# For example, a vocab of 1000 words may have splits [0] + [100, 500] + [inf]", "\n", "        ", "super", "(", "SplitCrossEntropyLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "splits", "=", "[", "0", "]", "+", "splits", "+", "[", "100", "*", "1000000", "]", "\n", "self", ".", "nsplits", "=", "len", "(", "self", ".", "splits", ")", "-", "1", "\n", "self", ".", "stats", "=", "defaultdict", "(", "list", ")", "\n", "self", ".", "verbose", "=", "verbose", "\n", "# Each of the splits that aren't in the head require a pretend token, we'll call them tombstones", "\n", "# The probability given to this tombstone is the probability of selecting an item from the represented split", "\n", "if", "self", ".", "nsplits", ">", "1", ":", "\n", "            ", "self", ".", "tail_vectors", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "nsplits", "-", "1", ",", "hidden_size", ")", ")", "\n", "self", ".", "tail_bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "self", ".", "nsplits", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.splitcross.SplitCrossEntropyLoss.logprob": [[26, 71], ["torch.nn.functional.linear", "torch.nn.functional.linear", "torch.nn.functional.linear", "torch.nn.functional.linear", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "list", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "results.append", "torch.nn.functional.linear", "torch.nn.functional.linear", "torch.nn.functional.linear", "torch.nn.functional.linear", "softmaxed_head_res[].contiguous", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "results.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "softmaxed_head_res[].contiguous.view"], "methods", ["None"], ["", "", "def", "logprob", "(", "self", ",", "weight", ",", "bias", ",", "hiddens", ",", "splits", "=", "None", ",", "softmaxed_head_res", "=", "None", ",", "verbose", "=", "False", ")", ":", "\n", "# First we perform the first softmax on the head vocabulary and the tombstones", "\n", "        ", "if", "softmaxed_head_res", "is", "None", ":", "\n", "            ", "start", ",", "end", "=", "self", ".", "splits", "[", "0", "]", ",", "self", ".", "splits", "[", "1", "]", "\n", "head_weight", "=", "None", "if", "end", "-", "start", "==", "0", "else", "weight", "[", "start", ":", "end", "]", "\n", "head_bias", "=", "None", "if", "end", "-", "start", "==", "0", "else", "bias", "[", "start", ":", "end", "]", "\n", "# We only add the tombstones if we have more than one split", "\n", "if", "self", ".", "nsplits", ">", "1", ":", "\n", "                ", "head_weight", "=", "self", ".", "tail_vectors", "if", "head_weight", "is", "None", "else", "torch", ".", "cat", "(", "[", "head_weight", ",", "self", ".", "tail_vectors", "]", ")", "\n", "head_bias", "=", "self", ".", "tail_bias", "if", "head_bias", "is", "None", "else", "torch", ".", "cat", "(", "[", "head_bias", ",", "self", ".", "tail_bias", "]", ")", "\n", "\n", "# Perform the softmax calculation for the word vectors in the head for all splits", "\n", "# We need to guard against empty splits as torch.cat does not like random lists", "\n", "", "head_res", "=", "torch", ".", "nn", ".", "functional", ".", "linear", "(", "hiddens", ",", "head_weight", ",", "bias", "=", "head_bias", ")", "\n", "softmaxed_head_res", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "head_res", ",", "dim", "=", "-", "1", ")", "\n", "\n", "", "if", "splits", "is", "None", ":", "\n", "            ", "splits", "=", "list", "(", "range", "(", "self", ".", "nsplits", ")", ")", "\n", "\n", "", "results", "=", "[", "]", "\n", "running_offset", "=", "0", "\n", "for", "idx", "in", "splits", ":", "\n", "\n", "# For those targets in the head (idx == 0) we only need to return their loss", "\n", "            ", "if", "idx", "==", "0", ":", "\n", "                ", "results", ".", "append", "(", "softmaxed_head_res", "[", ":", ",", ":", "-", "(", "self", ".", "nsplits", "-", "1", ")", "]", ")", "\n", "\n", "# If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)", "\n", "", "else", ":", "\n", "                ", "start", ",", "end", "=", "self", ".", "splits", "[", "idx", "]", ",", "self", ".", "splits", "[", "idx", "+", "1", "]", "\n", "tail_weight", "=", "weight", "[", "start", ":", "end", "]", "\n", "tail_bias", "=", "bias", "[", "start", ":", "end", "]", "\n", "\n", "# Calculate the softmax for the words in the tombstone", "\n", "tail_res", "=", "torch", ".", "nn", ".", "functional", ".", "linear", "(", "hiddens", ",", "tail_weight", ",", "bias", "=", "tail_bias", ")", "\n", "\n", "# Then we calculate p(tombstone) * p(word in tombstone)", "\n", "# Adding is equivalent to multiplication in log space", "\n", "head_entropy", "=", "(", "softmaxed_head_res", "[", ":", ",", "-", "idx", "]", ")", ".", "contiguous", "(", ")", "\n", "tail_entropy", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "tail_res", ",", "dim", "=", "-", "1", ")", "\n", "results", ".", "append", "(", "head_entropy", ".", "view", "(", "-", "1", ",", "1", ")", "+", "tail_entropy", ")", "\n", "\n", "", "", "if", "len", "(", "results", ")", ">", "1", ":", "\n", "            ", "return", "torch", ".", "cat", "(", "results", ",", "dim", "=", "1", ")", "\n", "", "return", "results", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.splitcross.SplitCrossEntropyLoss.split_on_targets": [[72, 105], ["range", "range", "split_targets.append", "split_hiddens.append", "sum", "len", "split_targets.append", "split_hiddens.append", "torch.masked_select", "torch.masked_select", "torch.masked_select", "torch.masked_select", "hiddens.masked_select().view", "hiddens.size", "len", "hiddens.masked_select", "tmp_mask.unsqueeze().expand_as", "tmp_mask.unsqueeze"], "methods", ["None"], ["", "def", "split_on_targets", "(", "self", ",", "hiddens", ",", "targets", ")", ":", "\n", "# Split the targets into those in the head and in the tail", "\n", "        ", "split_targets", "=", "[", "]", "\n", "split_hiddens", "=", "[", "]", "\n", "\n", "# Determine to which split each element belongs (for each start split value, add 1 if equal or greater)", "\n", "# This method appears slower at least for WT-103 values for approx softmax", "\n", "#masks = [(targets >= self.splits[idx]).view(1, -1) for idx in range(1, self.nsplits)]", "\n", "#mask = torch.sum(torch.cat(masks, dim=0), dim=0)", "\n", "###", "\n", "# This is equally fast for smaller splits as method below but scales linearly", "\n", "mask", "=", "None", "\n", "for", "idx", "in", "range", "(", "1", ",", "self", ".", "nsplits", ")", ":", "\n", "            ", "partial_mask", "=", "targets", ">=", "self", ".", "splits", "[", "idx", "]", "\n", "mask", "=", "mask", "+", "partial_mask", "if", "mask", "is", "not", "None", "else", "partial_mask", "\n", "###", "\n", "#masks = torch.stack([targets] * (self.nsplits - 1))", "\n", "#mask = torch.sum(masks >= self.split_starts, dim=0)", "\n", "", "for", "idx", "in", "range", "(", "self", ".", "nsplits", ")", ":", "\n", "# If there are no splits, avoid costly masked select", "\n", "            ", "if", "self", ".", "nsplits", "==", "1", ":", "\n", "                ", "split_targets", ",", "split_hiddens", "=", "[", "targets", "]", ",", "[", "hiddens", "]", "\n", "continue", "\n", "# If all the words are covered by earlier targets, we have empties so later stages don't freak out", "\n", "", "if", "sum", "(", "len", "(", "t", ")", "for", "t", "in", "split_targets", ")", "==", "len", "(", "targets", ")", ":", "\n", "                ", "split_targets", ".", "append", "(", "[", "]", ")", "\n", "split_hiddens", ".", "append", "(", "[", "]", ")", "\n", "continue", "\n", "# Are you in our split?", "\n", "", "tmp_mask", "=", "mask", "==", "idx", "\n", "split_targets", ".", "append", "(", "torch", ".", "masked_select", "(", "targets", ",", "tmp_mask", ")", ")", "\n", "split_hiddens", ".", "append", "(", "hiddens", ".", "masked_select", "(", "tmp_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "hiddens", ")", ")", ".", "view", "(", "-", "1", ",", "hiddens", ".", "size", "(", "1", ")", ")", ")", "\n", "", "return", "split_targets", ",", "split_hiddens", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.splitcross.SplitCrossEntropyLoss.forward": [[106, 172], ["splitcross.SplitCrossEntropyLoss.split_on_targets", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.nn.functional.linear", "torch.nn.functional.linear", "torch.nn.functional.linear", "torch.nn.functional.linear", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "range", "sorted", "print", "len", "hiddens.view.view.view", "len", "targets.view.view.view", "splitcross.SplitCrossEntropyLoss.stats[].append", "len", "print", "hiddens.view.view.size", "hiddens.view.view.size", "targets.view.view.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "splitcross.SplitCrossEntropyLoss.logprob", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "entropy.float().sum", "range", "len", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "splitcross.SplitCrossEntropyLoss.stats[].append", "entropy.float().sum", "int", "torch.cat.size", "torch.cat.size", "head_weight.size", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "entropy.float", "numpy.mean", "len", "split_targets[].view", "len", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "torch.nn.functional.log_softmax", "entropy.float", "split_hiddens[].size", "tail_weight.size"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.splitcross.SplitCrossEntropyLoss.split_on_targets", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.splitcross.SplitCrossEntropyLoss.logprob"], ["", "def", "forward", "(", "self", ",", "weight", ",", "bias", ",", "hiddens", ",", "targets", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "if", "self", ".", "verbose", "or", "verbose", ":", "\n", "            ", "for", "idx", "in", "sorted", "(", "self", ".", "stats", ")", ":", "\n", "                ", "print", "(", "'{}: {}'", ".", "format", "(", "idx", ",", "int", "(", "np", ".", "mean", "(", "self", ".", "stats", "[", "idx", "]", ")", ")", ")", ",", "end", "=", "', '", ")", "\n", "", "print", "(", ")", "\n", "\n", "", "total_loss", "=", "None", "\n", "if", "len", "(", "hiddens", ".", "size", "(", ")", ")", ">", "2", ":", "hiddens", "=", "hiddens", ".", "view", "(", "-", "1", ",", "hiddens", ".", "size", "(", "2", ")", ")", "\n", "if", "len", "(", "targets", ".", "size", "(", ")", ")", ">", "1", ":", "targets", "=", "targets", ".", "view", "(", "-", "1", ")", "\n", "\n", "split_targets", ",", "split_hiddens", "=", "self", ".", "split_on_targets", "(", "hiddens", ",", "targets", ")", "\n", "\n", "# First we perform the first softmax on the head vocabulary and the tombstones", "\n", "start", ",", "end", "=", "self", ".", "splits", "[", "0", "]", ",", "self", ".", "splits", "[", "1", "]", "\n", "head_weight", "=", "None", "if", "end", "-", "start", "==", "0", "else", "weight", "[", "start", ":", "end", "]", "\n", "head_bias", "=", "None", "if", "end", "-", "start", "==", "0", "else", "bias", "[", "start", ":", "end", "]", "\n", "\n", "# We only add the tombstones if we have more than one split", "\n", "if", "self", ".", "nsplits", ">", "1", ":", "\n", "            ", "head_weight", "=", "self", ".", "tail_vectors", "if", "head_weight", "is", "None", "else", "torch", ".", "cat", "(", "[", "head_weight", ",", "self", ".", "tail_vectors", "]", ")", "\n", "head_bias", "=", "self", ".", "tail_bias", "if", "head_bias", "is", "None", "else", "torch", ".", "cat", "(", "[", "head_bias", ",", "self", ".", "tail_bias", "]", ")", "\n", "\n", "# Perform the softmax calculation for the word vectors in the head for all splits", "\n", "# We need to guard against empty splits as torch.cat does not like random lists", "\n", "", "combo", "=", "torch", ".", "cat", "(", "[", "split_hiddens", "[", "i", "]", "for", "i", "in", "range", "(", "self", ".", "nsplits", ")", "if", "len", "(", "split_hiddens", "[", "i", "]", ")", "]", ")", "\n", "###", "\n", "all_head_res", "=", "torch", ".", "nn", ".", "functional", ".", "linear", "(", "combo", ",", "head_weight", ",", "bias", "=", "head_bias", ")", "\n", "softmaxed_all_head_res", "=", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "all_head_res", ",", "dim", "=", "-", "1", ")", "\n", "if", "self", ".", "verbose", "or", "verbose", ":", "\n", "            ", "self", ".", "stats", "[", "0", "]", ".", "append", "(", "combo", ".", "size", "(", ")", "[", "0", "]", "*", "head_weight", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "\n", "", "running_offset", "=", "0", "\n", "for", "idx", "in", "range", "(", "self", ".", "nsplits", ")", ":", "\n", "# If there are no targets for this split, continue", "\n", "            ", "if", "len", "(", "split_targets", "[", "idx", "]", ")", "==", "0", ":", "continue", "\n", "\n", "# For those targets in the head (idx == 0) we only need to return their loss", "\n", "if", "idx", "==", "0", ":", "\n", "                ", "softmaxed_head_res", "=", "softmaxed_all_head_res", "[", "running_offset", ":", "running_offset", "+", "len", "(", "split_hiddens", "[", "idx", "]", ")", "]", "\n", "entropy", "=", "-", "torch", ".", "gather", "(", "softmaxed_head_res", ",", "dim", "=", "1", ",", "index", "=", "split_targets", "[", "idx", "]", ".", "view", "(", "-", "1", ",", "1", ")", ")", "\n", "# If the target is in one of the splits, the probability is the p(tombstone) * p(word within tombstone)", "\n", "", "else", ":", "\n", "                ", "softmaxed_head_res", "=", "softmaxed_all_head_res", "[", "running_offset", ":", "running_offset", "+", "len", "(", "split_hiddens", "[", "idx", "]", ")", "]", "\n", "\n", "if", "self", ".", "verbose", "or", "verbose", ":", "\n", "                    ", "start", ",", "end", "=", "self", ".", "splits", "[", "idx", "]", ",", "self", ".", "splits", "[", "idx", "+", "1", "]", "\n", "tail_weight", "=", "weight", "[", "start", ":", "end", "]", "\n", "self", ".", "stats", "[", "idx", "]", ".", "append", "(", "split_hiddens", "[", "idx", "]", ".", "size", "(", ")", "[", "0", "]", "*", "tail_weight", ".", "size", "(", ")", "[", "0", "]", ")", "\n", "\n", "# Calculate the softmax for the words in the tombstone", "\n", "", "tail_res", "=", "self", ".", "logprob", "(", "weight", ",", "bias", ",", "split_hiddens", "[", "idx", "]", ",", "splits", "=", "[", "idx", "]", ",", "softmaxed_head_res", "=", "softmaxed_head_res", ")", "\n", "\n", "# Then we calculate p(tombstone) * p(word in tombstone)", "\n", "# Adding is equivalent to multiplication in log space", "\n", "head_entropy", "=", "softmaxed_head_res", "[", ":", ",", "-", "idx", "]", "\n", "# All indices are shifted - if the first split handles [0,...,499] then the 500th in the second split will be 0 indexed", "\n", "indices", "=", "(", "split_targets", "[", "idx", "]", "-", "self", ".", "splits", "[", "idx", "]", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "# Warning: if you don't squeeze, you get an N x 1 return, which acts oddly with broadcasting", "\n", "tail_entropy", "=", "torch", ".", "gather", "(", "torch", ".", "nn", ".", "functional", ".", "log_softmax", "(", "tail_res", ",", "dim", "=", "-", "1", ")", ",", "dim", "=", "1", ",", "index", "=", "indices", ")", ".", "squeeze", "(", ")", "\n", "entropy", "=", "-", "(", "head_entropy", "+", "tail_entropy", ")", "\n", "", "entropy", "[", "split_targets", "[", "idx", "]", "==", "0", "]", "=", "0", "\n", "###", "\n", "running_offset", "+=", "len", "(", "split_hiddens", "[", "idx", "]", ")", "\n", "total_loss", "=", "entropy", ".", "float", "(", ")", ".", "sum", "(", ")", "if", "total_loss", "is", "None", "else", "total_loss", "+", "entropy", ".", "float", "(", ")", ".", "sum", "(", ")", "\n", "\n", "", "return", "total_loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout": [[5, 27], ["torch.nn.functional.embedding", "embed.weight.data.new().resize_().bernoulli_().expand_as", "scale.expand_as", "embed.weight.data.new().resize_().bernoulli_", "embed.weight.data.new().resize_", "embed.weight.data.new", "embed.weight.size"], "function", ["None"], ["def", "embedded_dropout", "(", "embed", ",", "words", ",", "dropout", "=", "0.1", ",", "scale", "=", "None", ")", ":", "\n", "  ", "if", "dropout", ":", "\n", "    ", "mask", "=", "embed", ".", "weight", ".", "data", ".", "new", "(", ")", ".", "resize_", "(", "(", "embed", ".", "weight", ".", "size", "(", "0", ")", ",", "1", ")", ")", ".", "bernoulli_", "(", "1", "-", "dropout", ")", ".", "expand_as", "(", "embed", ".", "weight", ")", "/", "(", "1", "-", "dropout", ")", "\n", "masked_embed_weight", "=", "mask", "*", "embed", ".", "weight", "\n", "", "else", ":", "\n", "    ", "masked_embed_weight", "=", "embed", ".", "weight", "\n", "", "if", "scale", ":", "\n", "    ", "masked_embed_weight", "=", "scale", ".", "expand_as", "(", "masked_embed_weight", ")", "*", "masked_embed_weight", "\n", "\n", "", "try", ":", "\n", "    ", "padding_idx", "=", "embed", ".", "padding_index", "\n", "", "except", "AttributeError", ":", "\n", "    ", "padding_idx", "=", "embed", ".", "padding_idx", "\n", "\n", "", "if", "padding_idx", "is", "None", ":", "\n", "      ", "padding_idx", "=", "0", "\n", "\n", "", "X", "=", "torch", ".", "nn", ".", "functional", ".", "embedding", "(", "words", ",", "masked_embed_weight", ",", "\n", "padding_idx", ",", "embed", ".", "max_norm", ",", "embed", ".", "norm_type", ",", "\n", "embed", ".", "scale_grad_by_freq", ",", "embed", ".", "sparse", "\n", ")", "\n", "return", "X", "\n", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.weight_drop.WeightDrop.__init__": [[9, 19], ["super().__init__", "getattr", "weight_drop.WeightDrop.register_parameter", "torch.dropout", "torch.dropout", "torch.nn.Parameter", "torch.nn.Parameter"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "module", ",", "weights", ",", "dropout", "=", "0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "module", "=", "module", "\n", "self", ".", "weights", "=", "weights", "\n", "self", ".", "dropout", "=", "dropout", "\n", "for", "weight", "in", "self", ".", "weights", ":", "\n", "#Makes a copy of the weights of the selected layers.", "\n", "            ", "w", "=", "getattr", "(", "self", ".", "module", ",", "weight", ")", "\n", "self", ".", "register_parameter", "(", "f'{weight}_raw'", ",", "Parameter", "(", "w", ".", "data", ")", ")", "\n", "self", ".", "module", ".", "_parameters", "[", "weight", "]", "=", "F", ".", "dropout", "(", "w", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.weight_drop.WeightDrop._setweights": [[20, 25], ["getattr", "torch.dropout", "torch.dropout"], "methods", ["None"], ["", "", "def", "_setweights", "(", "self", ")", ":", "\n", "        ", "\"Apply dropout to the raw weights.\"", "\n", "for", "weight", "in", "self", ".", "weights", ":", "\n", "            ", "raw_w", "=", "getattr", "(", "self", ",", "f'{weight}_raw'", ")", "\n", "self", ".", "module", ".", "_parameters", "[", "weight", "]", "=", "F", ".", "dropout", "(", "raw_w", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.weight_drop.WeightDrop.forward": [[26, 29], ["weight_drop.WeightDrop._setweights", "weight_drop.WeightDrop.module.forward"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.weight_drop.WeightDrop._setweights", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.forward"], ["", "", "def", "forward", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "self", ".", "_setweights", "(", ")", "\n", "return", "self", ".", "module", ".", "forward", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.weight_drop.WeightDrop.reset": [[30, 35], ["hasattr", "getattr", "torch.dropout", "torch.dropout", "weight_drop.WeightDrop.module.reset"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "for", "weight", "in", "self", ".", "weights", ":", "\n", "            ", "raw_w", "=", "getattr", "(", "self", ",", "f'{weight}_raw'", ")", "\n", "self", ".", "module", ".", "_parameters", "[", "weight", "]", "=", "F", ".", "dropout", "(", "raw_w", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "False", ")", "\n", "", "if", "hasattr", "(", "self", ".", "module", ",", "'reset'", ")", ":", "self", ".", "module", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.__init__": [[19, 23], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "cutoff", ":", "int", ")", "->", "None", ":", "\n", "        ", "self", ".", "_cutoff", "=", "cutoff", "\n", "self", ".", "_remaining", ":", "List", "[", "Dict", "[", "int", ",", "int", "]", "]", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.__call__": [[24, 83], ["recent_entities.RecentEntities._get_candidates", "entity_ids.new_zeros", "enumerate", "entity_ids.tolist", "kglm.nn.util.nested_enumerate", "enumerate", "lookup.items", "recent_entities.RecentEntities.tolist", "enumerate", "lookup.items"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities._get_candidates", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.nested_enumerate"], ["", "def", "__call__", "(", "self", ",", "\n", "entity_ids", ":", "torch", ".", "LongTensor", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Returns the set of valid parent entities for a given mention.\n\n        Parameters\n        ----------\n        entity_ids : ``torch.LongTensor``\n            A tensor of shape ``(batch_size, sequence_length)`` whose elements are the ids\n            of the corresponding token in the ``target`` sequence.\n\n        Returns\n        -------\n        A tuple ``(candidate_ids, candidate_mask)`` containings the following elements:\n        candidate_ids : ``torch.LongTensor``\n            A tensor of shape ``(batch_size, n_candidates)`` of all of the candidates for each\n            batch element.\n        candidate_mask : ``torch.LongTensor``\n            A tensor of shape ``(batch_size, sequence_length, n_candidates)`` defining which\n            subset of candidates can be selected at the given point in the sequence.\n        \"\"\"", "\n", "batch_size", ",", "sequence_length", "=", "entity_ids", ".", "shape", "[", ":", "2", "]", "\n", "\n", "# TODO: See if we can get away without nested loops / cast to CPU.", "\n", "candidate_ids", "=", "self", ".", "_get_candidates", "(", "entity_ids", ")", "\n", "candidate_lookup", "=", "[", "{", "parent_id", ":", "j", "for", "j", ",", "parent_id", "in", "enumerate", "(", "l", ")", "}", "for", "l", "in", "candidate_ids", ".", "tolist", "(", ")", "]", "\n", "\n", "# Create mask", "\n", "candidate_mask", "=", "entity_ids", ".", "new_zeros", "(", "size", "=", "(", "batch_size", ",", "sequence_length", ",", "candidate_ids", ".", "shape", "[", "-", "1", "]", ")", ",", "\n", "dtype", "=", "torch", ".", "uint8", ")", "\n", "\n", "# Start by accounting for unfinished masks that remain from the last batch", "\n", "for", "i", ",", "lookup", "in", "enumerate", "(", "self", ".", "_remaining", ")", ":", "\n", "            ", "for", "parent_id", ",", "remainder", "in", "lookup", ".", "items", "(", ")", ":", "\n", "# Find index w.r.t. the **current** set of candidates", "\n", "                ", "k", "=", "candidate_lookup", "[", "i", "]", "[", "parent_id", "]", "\n", "# Fill in the remaining amount of mask", "\n", "candidate_mask", "[", "i", ",", ":", "remainder", ",", "k", "]", "=", "1", "\n", "# If splits are really short, then we might still have some remaining", "\n", "lookup", "[", "parent_id", "]", "-=", "sequence_length", "\n", "\n", "# Cast to list so we can use elements as keys (not possible for tensors)", "\n", "", "", "parent_id_list", "=", "entity_ids", ".", "tolist", "(", ")", "\n", "for", "i", ",", "j", ",", "*", "_", ",", "parent_id", "in", "nested_enumerate", "(", "parent_id_list", ")", ":", "\n", "            ", "if", "parent_id", "==", "0", ":", "\n", "                ", "continue", "\n", "", "else", ":", "\n", "# Fill in mask", "\n", "                ", "k", "=", "candidate_lookup", "[", "i", "]", "[", "parent_id", "]", "\n", "candidate_mask", "[", "i", ",", "j", "+", "1", ":", "j", "+", "self", ".", "_cutoff", "+", "1", ",", "k", "]", "=", "1", "\n", "# Track how many sequence elements remain", "\n", "remainder", "=", "sequence_length", "-", "(", "j", "+", "self", ".", "_cutoff", "+", "1", ")", "\n", "self", ".", "_remaining", "[", "i", "]", "[", "parent_id", "]", "=", "(", "j", "+", "self", ".", "_cutoff", "+", "1", ")", "-", "sequence_length", "\n", "\n", "# Remove any ids for non-recent parents (e.g. those without remaining mask)", "\n", "", "", "for", "i", ",", "lookup", "in", "enumerate", "(", "self", ".", "_remaining", ")", ":", "\n", "            ", "self", ".", "_remaining", "[", "i", "]", "=", "{", "key", ":", "value", "for", "key", ",", "value", "in", "lookup", ".", "items", "(", ")", "if", "value", ">", "0", "}", "\n", "\n", "", "return", "candidate_ids", ",", "candidate_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities._get_candidates": [[85, 121], ["enumerate", "max", "entity_ids.new_zeros", "enumerate", "torch.unique", "all_unique.append", "list", "entity_ids.new_tensor", "torch.cat", "recent_entities.RecentEntities._remaining[].keys", "torch.cat.view"], "methods", ["None"], ["", "def", "_get_candidates", "(", "self", ",", "\n", "entity_ids", ":", "torch", ".", "LongTensor", ")", "->", "torch", ".", "LongTensor", ":", "\n", "        ", "\"\"\"\n        Combines the unique ids from the current batch with the previous set of ids to form the\n        collection of **all** relevant ids.\n\n        Parameters\n        ----------\n        entity_ids : ``torch.LongTensor``\n            A tensor of shape ``(batch_size, sequence_length)`` whose elements are the ids\n            of the corresponding token in the ``target`` sequence.\n\n        Returns\n        -------\n        unique_entity_ids : ``torch.LongTensor``\n            A tensor of shape ``(batch_size, max_num_parents)`` containing all of the unique\n            candidate ids.\n        \"\"\"", "\n", "# Get the tensors of unique ids for each batch element and store them in a list", "\n", "all_unique", ":", "List", "[", "torch", ".", "LongTensor", "]", "=", "[", "]", "\n", "for", "i", ",", "ids", "in", "enumerate", "(", "entity_ids", ")", ":", "\n", "            ", "if", "self", ".", "_remaining", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "previous_ids", "=", "list", "(", "self", ".", "_remaining", "[", "i", "]", ".", "keys", "(", ")", ")", "\n", "previous_ids", "=", "entity_ids", ".", "new_tensor", "(", "previous_ids", ")", "\n", "ids", "=", "torch", ".", "cat", "(", "(", "ids", ".", "view", "(", "-", "1", ")", ",", "previous_ids", ")", ",", "dim", "=", "0", ")", "\n", "", "unique", "=", "torch", ".", "unique", "(", "ids", ",", "sorted", "=", "True", ")", "\n", "all_unique", ".", "append", "(", "unique", ")", "\n", "\n", "# Convert the list to a tensor by adding adequete padding.", "\n", "", "batch_size", "=", "entity_ids", ".", "shape", "[", "0", "]", "\n", "max_num_parents", "=", "max", "(", "unique", ".", "shape", "[", "0", "]", "for", "unique", "in", "all_unique", ")", "\n", "unique_entity_ids", "=", "entity_ids", ".", "new_zeros", "(", "size", "=", "(", "batch_size", ",", "max_num_parents", ")", ")", "\n", "for", "i", ",", "unique", "in", "enumerate", "(", "all_unique", ")", ":", "\n", "            ", "unique_entity_ids", "[", "i", ",", ":", "unique", ".", "shape", "[", "0", "]", "]", "=", "unique", "\n", "\n", "", "return", "unique_entity_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset": [[122, 145], ["reset.all", "RuntimeError", "enumerate", "len", "len", "reset.all", "dict", "range"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "reset", ":", "torch", ".", "ByteTensor", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        reset : ``torch.ByteTensor``\n            A tensor of shape ``(batch_size,)`` indicating whether the state (e.g. list of\n            previously seen entities) for the corresponding batch element should be reset.\n        \"\"\"", "\n", "if", "(", "len", "(", "reset", ")", "!=", "len", "(", "self", ".", "_remaining", ")", ")", "and", "not", "reset", ".", "all", "(", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Changing the batch size without resetting all internal states is '", "\n", "'undefined.'", ")", "\n", "\n", "# If everything is being reset, then we treat as if the Module has just been initialized.", "\n", "# This simplifies the case where the batch_size has been", "\n", "", "if", "reset", ".", "all", "(", ")", ":", "\n", "            ", "batch_size", "=", "reset", ".", "shape", "[", "0", "]", "\n", "self", ".", "_remaining", "=", "[", "dict", "(", ")", "for", "_", "in", "range", "(", "batch_size", ")", "]", "\n", "\n", "# Otherwise only reset the internal state for the indicated batch elements", "\n", "", "else", ":", "\n", "            ", "for", "i", ",", "should_reset", "in", "enumerate", "(", "reset", ")", ":", "\n", "                ", "if", "should_reset", ":", "\n", "                    ", "self", ".", "_remaining", "[", "i", "]", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.insert": [[147, 158], ["range", "values[].item", "values[].item"], "methods", ["None"], ["", "", "", "", "def", "insert", "(", "self", ",", "values", ":", "torch", ".", "LongTensor", ",", "mask", ":", "torch", ".", "ByteTensor", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        To deal with case in sampling where tail ids are only known after __call__\n        \"\"\"", "\n", "batch_size", "=", "values", ".", "shape", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "if", "mask", "is", "not", "None", ":", "\n", "                ", "if", "mask", "[", "i", "]", ":", "\n", "                    ", "self", ".", "_remaining", "[", "i", "]", "[", "values", "[", "i", "]", ".", "item", "(", ")", "]", "=", "self", ".", "_cutoff", "+", "1", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "_remaining", "[", "i", "]", "[", "values", "[", "i", "]", ".", "item", "(", ")", "]", "=", "self", ".", "_cutoff", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.locked_dropout.LockedDropout.forward": [[4, 13], ["x.data.new().bernoulli_", "mask.expand_as.expand_as.expand_as", "x.data.new", "x.size", "x.size"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "x", ":", "torch", ".", "Tensor", ",", "\n", "dropout", ":", "float", "=", "0.5", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "not", "self", ".", "training", "or", "not", "dropout", ":", "\n", "            ", "return", "x", "\n", "", "mask", "=", "x", ".", "data", ".", "new", "(", "x", ".", "size", "(", "0", ")", ",", "1", ",", "x", ".", "size", "(", "2", ")", ")", ".", "bernoulli_", "(", "1", "-", "dropout", ")", "\n", "mask", "=", "mask", "/", "(", "1", "-", "dropout", ")", "\n", "mask", "=", "mask", ".", "expand_as", "(", "x", ")", "\n", "return", "mask", "*", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.knowledge_graph_lookup.KnowledgeGraphLookup.__init__": [[15, 21], ["knowledge_graph_lookup.KnowledgeGraphLookup.load_edges"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.knowledge_graph_lookup.KnowledgeGraphLookup.load_edges"], ["    ", "def", "__init__", "(", "self", ",", "\n", "knowledge_graph_path", ":", "str", ",", "\n", "vocab", ":", "Vocabulary", ")", "->", "None", ":", "\n", "        ", "self", ".", "_knowledge_graph_path", "=", "knowledge_graph_path", "\n", "self", ".", "_vocab", "=", "vocab", "\n", "self", ".", "_relations", ",", "self", ".", "_tail_ids", "=", "self", ".", "load_edges", "(", "knowledge_graph_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.knowledge_graph_lookup.KnowledgeGraphLookup.load_edges": [[22, 53], ["logger.info", "knowledge_graph_lookup.KnowledgeGraphLookup._vocab.get_index_to_token_vocabulary", "tqdm.tqdm.tqdm", "open", "pickle.load", "range", "all_relations.append", "all_tail_ids.append", "len", "zip", "torch.LongTensor", "torch.LongTensor", "knowledge_graph_lookup.KnowledgeGraphLookup._vocab.get_token_index", "knowledge_graph_lookup.KnowledgeGraphLookup._vocab.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.load"], ["", "def", "load_edges", "(", "self", ",", "knowledge_graph_path", ":", "str", ")", "->", "Tuple", "[", "List", "[", "torch", ".", "LongTensor", "]", ",", "List", "[", "torch", ".", "LongTensor", "]", "]", ":", "\n", "        ", "logger", ".", "info", "(", "'Loading knowledge graph from: %s'", ",", "knowledge_graph_path", ")", "\n", "with", "open", "(", "knowledge_graph_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "knowledge_graph", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "entity_idx_to_token", "=", "self", ".", "_vocab", ".", "get_index_to_token_vocabulary", "(", "'entity_ids'", ")", "\n", "all_relations", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "all_tail_ids", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "len", "(", "entity_idx_to_token", ")", ")", ")", ":", "\n", "            ", "entity_id", "=", "entity_idx_to_token", "[", "i", "]", "\n", "try", ":", "\n", "                ", "edges", "=", "knowledge_graph", "[", "entity_id", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "relations", "=", "None", "\n", "tail_ids", "=", "None", "\n", "", "else", ":", "\n", "                ", "if", "edges", "==", "[", "]", ":", "\n", "                    ", "relations", "=", "None", "\n", "tail_ids", "=", "None", "\n", "", "else", ":", "\n", "# Get the relation and tail id tokens", "\n", "                    ", "relation_tokens", ",", "tail_id_tokens", "=", "zip", "(", "*", "knowledge_graph", "[", "entity_id", "]", ")", "\n", "# Index tokens", "\n", "relations", "=", "[", "self", ".", "_vocab", ".", "get_token_index", "(", "t", ",", "'relations'", ")", "for", "t", "in", "relation_tokens", "]", "\n", "tail_ids", "=", "[", "self", ".", "_vocab", ".", "get_token_index", "(", "t", ",", "'raw_entity_ids'", ")", "for", "t", "in", "tail_id_tokens", "]", "\n", "# Convert to tensors", "\n", "relations", "=", "torch", ".", "LongTensor", "(", "relations", ")", "\n", "tail_ids", "=", "torch", ".", "LongTensor", "(", "tail_ids", ")", "\n", "", "", "all_relations", ".", "append", "(", "relations", ")", "\n", "all_tail_ids", ".", "append", "(", "tail_ids", ")", "\n", "", "return", "all_relations", ",", "all_tail_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.knowledge_graph_lookup.KnowledgeGraphLookup.__call__": [[54, 96], ["kglm.nn.util.nested_enumerate", "indices.append", "parent_ids_list.append", "relations_list.append", "tail_ids_list.append", "tuple", "relations.to", "tail_ids.to"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.nested_enumerate"], ["", "def", "__call__", "(", "self", ",", "\n", "parent_ids", ":", "torch", ".", "LongTensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Returns all relations of the form:\n\n            (parent_id, relation, tail_id)\n\n        from the knowledge graph.\n\n        Parameters\n        -----------\n        parent_ids : ``torch.LongTensor``\n            A tensor of arbitrary shape `(N, *)` where `*` means any number of additional\n            dimensions. Each element is an id in the knowledge graph.\n\n        Returns\n        -------\n        A tuple `(relations, tail_ids)` containing the following elements:\n        relations : ``torch.LongTensor``\n            A tensor of shape `(N, *, K)` corresponding to the input shape, with an\n            additional dimension whose size `K` is the largest number of relations to\n            a parent in the input. Each element is a relation id.\n        tail_ids : ``torch.LongTensor``\n            A tensor of shape `(N, *, K)` containing the corresponding tail ids.\n        \"\"\"", "\n", "# Collect the information to load into the output tensors.", "\n", "indices", ":", "List", "[", "Tuple", "[", "int", ",", "...", "]", "]", "=", "[", "]", "\n", "parent_ids_list", ":", "List", "[", "torch", ".", "LongTensor", "]", "=", "[", "]", "\n", "relations_list", ":", "List", "[", "torch", ".", "LongTensor", "]", "=", "[", "]", "\n", "tail_ids_list", ":", "List", "[", "torch", ".", "LongTensor", "]", "=", "[", "]", "\n", "for", "*", "inds", ",", "parent_id", "in", "nested_enumerate", "(", "parent_ids", ")", ":", "\n", "# Retrieve data", "\n", "            ", "relations", "=", "self", ".", "_relations", "[", "parent_id", "]", "\n", "tail_ids", "=", "self", ".", "_tail_ids", "[", "parent_id", "]", "\n", "if", "relations", "is", "None", ":", "\n", "                ", "continue", "\n", "# Add to lists", "\n", "", "indices", ".", "append", "(", "tuple", "(", "inds", ")", ")", "\n", "parent_ids_list", ".", "append", "(", "parent_id", ")", "\n", "relations_list", ".", "append", "(", "relations", ".", "to", "(", "device", "=", "parent_ids", ".", "device", ")", ")", "\n", "tail_ids_list", ".", "append", "(", "tail_ids", ".", "to", "(", "device", "=", "parent_ids", ".", "device", ")", ")", "\n", "\n", "", "return", "indices", ",", "parent_ids_list", ",", "relations_list", ",", "tail_ids_list", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_recent_entities.RecentEntitiesTest.setUp": [[10, 15], ["allennlp.modules.token_embedders.Embedding", "kglm.modules.recent_entities.RecentEntities", "super().setUp"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "entity_embedder", "=", "Embedding", "(", "5", ",", "10", ")", "\n", "self", ".", "cutoff", "=", "2", "\n", "self", ".", "recent_entities", "=", "RecentEntities", "(", "cutoff", "=", "self", ".", "cutoff", ")", "\n", "super", "(", ")", ".", "setUp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_recent_entities.RecentEntitiesTest.test_get_candidates": [[16, 30], ["torch.tensor", "torch.ones", "test_recent_entities.RecentEntitiesTest.recent_entities.reset", "test_recent_entities.RecentEntitiesTest.recent_entities._get_candidates", "torch.tensor", "test_recent_entities.RecentEntitiesTest.equal"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities._get_candidates"], ["", "def", "test_get_candidates", "(", "self", ")", ":", "\n", "# shape: (batch_size, seq_len, max_parents)", "\n", "        ", "entity_ids_t0", "=", "torch", ".", "tensor", "(", "[", "\n", "[", "[", "1", ",", "2", "]", ",", "[", "3", ",", "0", "]", ",", "[", "4", ",", "0", "]", "]", ",", "\n", "[", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "[", "2", ",", "0", "]", "]", "\n", "]", ")", "\n", "# ``RecentEntities.reset()`` must always be called before other operations.", "\n", "reset", "=", "torch", ".", "ones", "(", "entity_ids_t0", ".", "shape", "[", "0", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "self", ".", "recent_entities", ".", "reset", "(", "reset", ")", "\n", "# ``RecentEntities._get_all_ids()`` will get all of the unique ids in each batch element.", "\n", "# We set ``sorted=True`` to ensure output is deterministic.", "\n", "all_ids", "=", "self", ".", "recent_entities", ".", "_get_candidates", "(", "entity_ids_t0", ")", "\n", "expected", "=", "torch", ".", "tensor", "(", "[", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", "]", ",", "[", "0", ",", "1", ",", "2", ",", "0", ",", "0", "]", "]", ")", "\n", "assert", "all_ids", ".", "equal", "(", "expected", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_recent_entities.RecentEntitiesTest.test_mask": [[31, 70], ["torch.tensor", "torch.ones", "test_recent_entities.RecentEntitiesTest.recent_entities.reset", "test_recent_entities.RecentEntitiesTest.recent_entities", "candidate_mask[].equal", "torch.tensor", "candidate_mask[].equal", "torch.tensor", "candidate_mask[].equal", "test_recent_entities.RecentEntitiesTest.recent_entities", "torch.tensor", "candidate_mask[].equal", "torch.tensor", "test_recent_entities.RecentEntitiesTest.recent_entities.reset", "test_recent_entities.RecentEntitiesTest.recent_entities", "torch.tensor", "candidate_mask[].equal"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset"], ["", "def", "test_mask", "(", "self", ")", ":", "\n", "# Checks that the ``__call__``` method behaves as expected. We'll work with a single", "\n", "# sequence of two entities to keep life simple.", "\n", "        ", "entity_ids_t0", "=", "torch", ".", "tensor", "(", "[", "\n", "[", "[", "1", ",", "2", "]", ",", "[", "3", ",", "0", "]", ",", "[", "4", ",", "0", "]", "]", ",", "\n", "[", "[", "1", ",", "0", "]", ",", "[", "0", ",", "0", "]", ",", "[", "2", ",", "0", "]", "]", "\n", "]", ")", "\n", "reset", "=", "torch", ".", "ones", "(", "entity_ids_t0", ".", "shape", "[", "0", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "self", ".", "recent_entities", ".", "reset", "(", "reset", ")", "\n", "\n", "candidate_ids", ",", "candidate_mask", "=", "self", ".", "recent_entities", "(", "entity_ids_t0", ")", "\n", "# We know that the candidate ids are given in the order in the last test,", "\n", "# here we check that the mask looks correct for entities in the first batch.", "\n", "# The mask for entities 1 and 2 in the first sequence should be the same", "\n", "assert", "candidate_mask", "[", "0", ",", ":", ",", "1", "]", ".", "equal", "(", "candidate_mask", "[", "0", ",", ":", ",", "2", "]", ")", "\n", "# The mask for entity 3 should be 1 for the 3rd timestep - it is not recent when it is first observed", "\n", "expected_3", "=", "torch", ".", "tensor", "(", "[", "0", ",", "0", ",", "1", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "assert", "candidate_mask", "[", "0", ",", ":", ",", "3", "]", ".", "equal", "(", "expected_3", ")", "\n", "# The mask for entity 4 should only all zeros - it will not be recent until the next batch", "\n", "expected_4", "=", "torch", ".", "tensor", "(", "[", "0", ",", "0", ",", "0", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "assert", "candidate_mask", "[", "0", ",", ":", ",", "4", "]", ".", "equal", "(", "expected_4", ")", "\n", "\n", "# Let's check that the remainders are correct", "\n", "# Entity 3 should have a remainder of 1, Entity 4 should have a remainder of 2, and", "\n", "# everything else should be filtered out.", "\n", "assert", "self", ".", "recent_entities", ".", "_remaining", "[", "0", "]", "==", "{", "3", ":", "1", ",", "4", ":", "2", "}", "\n", "\n", "# If we run on the same data again, the remainder for entity 4 should activate the mask for", "\n", "# the first and second timestep.", "\n", "candidate_ids", ",", "candidate_mask", "=", "self", ".", "recent_entities", "(", "entity_ids_t0", ")", "\n", "expected_4", "=", "torch", ".", "tensor", "(", "[", "1", ",", "1", ",", "0", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "assert", "candidate_mask", "[", "0", ",", ":", ",", "4", "]", ".", "equal", "(", "expected_4", ")", "\n", "\n", "# But this should not happen if we reset the first sequence", "\n", "reset", "=", "torch", ".", "tensor", "(", "[", "1", ",", "0", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "self", ".", "recent_entities", ".", "reset", "(", "reset", ")", "\n", "candidate_ids", ",", "candidate_mask", "=", "self", ".", "recent_entities", "(", "entity_ids_t0", ")", "\n", "expected_4", "=", "torch", ".", "tensor", "(", "[", "0", ",", "0", ",", "0", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "assert", "candidate_mask", "[", "0", ",", ":", ",", "4", "]", ".", "equal", "(", "expected_4", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_knowledge_graph_lookup.KnowledgeGraphLookupTest.setUp": [[11, 36], ["super().setUp", "allennlp.data.vocabulary.Vocabulary", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.add_token_to_namespace", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.add_token_to_namespace", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.add_token_to_namespace", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.add_token_to_namespace", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.add_token_to_namespace", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.add_token_to_namespace", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.add_token_to_namespace", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.add_token_to_namespace", "kglm.modules.KnowledgeGraphLookup", "open", "pickle.dump"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "# We assume our knowledge graphs are given in the form of pickled dictionaries, we'll", "\n", "# create a simple one here for testing purposes.", "\n", "self", ".", "temp_knowledge_graph", "=", "{", "\n", "'E1'", ":", "[", "[", "'R1'", ",", "'E2'", "]", ",", "[", "'R2'", ",", "'E3'", "]", "]", ",", "\n", "'E2'", ":", "[", "[", "'R1'", ",", "'E3'", "]", "]", ",", "\n", "}", "\n", "self", ".", "path", "=", "self", ".", "TEST_DIR", "/", "'relation.pkl'", "\n", "with", "open", "(", "self", ".", "path", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "temp_knowledge_graph", ",", "f", ")", "\n", "\n", "# We need a vocab to track the unique entity ids and relations", "\n", "", "self", ".", "vocab", "=", "Vocabulary", "(", ")", "\n", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'E1'", ",", "'raw_entity_ids'", ")", "\n", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'E2'", ",", "'raw_entity_ids'", ")", "\n", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'E3'", ",", "'raw_entity_ids'", ")", "\n", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'E1'", ",", "'entity_ids'", ")", "\n", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'E2'", ",", "'entity_ids'", ")", "\n", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'E3'", ",", "'entity_ids'", ")", "\n", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'R1'", ",", "'relations'", ")", "\n", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'R2'", ",", "'relations'", ")", "\n", "\n", "# Lastly we create the knowledge graph lookup", "\n", "self", ".", "knowledge_graph_lookup", "=", "KnowledgeGraphLookup", "(", "self", ".", "path", ",", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_knowledge_graph_lookup.KnowledgeGraphLookupTest.test_lists_are_correct": [[37, 54], ["zip", "torch.LongTensor", "torch.LongTensor", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index", "relations[].equal", "tail_ids[].equal", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index"], "methods", ["None"], ["", "def", "test_lists_are_correct", "(", "self", ")", ":", "\n", "# The lookup converts the data in the knowledge graph into lists of tensors.", "\n", "# Here we check that these lists contain the expected information.", "\n", "        ", "relations", "=", "self", ".", "knowledge_graph_lookup", ".", "_relations", "\n", "tail_ids", "=", "self", ".", "knowledge_graph_lookup", ".", "_tail_ids", "\n", "\n", "# We'll check that the information is correct for entity 'E1'. We'll start by building the", "\n", "# expected tensors from our inputs...", "\n", "expected_relations", ",", "expected_tail_ids", "=", "zip", "(", "*", "self", ".", "temp_knowledge_graph", "[", "'E1'", "]", ")", "\n", "expected_relations", "=", "[", "self", ".", "vocab", ".", "get_token_index", "(", "t", ",", "'relations'", ")", "for", "t", "in", "expected_relations", "]", "\n", "expected_tail_ids", "=", "[", "self", ".", "vocab", ".", "get_token_index", "(", "t", ",", "'raw_entity_ids'", ")", "for", "t", "in", "expected_tail_ids", "]", "\n", "expected_relations", "=", "torch", ".", "LongTensor", "(", "expected_relations", ")", "\n", "expected_tail_ids", "=", "torch", ".", "LongTensor", "(", "expected_tail_ids", ")", "\n", "# ...then checking whether the corresponding elements in the lists are correct.", "\n", "index", "=", "self", ".", "vocab", ".", "get_token_index", "(", "'E1'", ",", "'entity_ids'", ")", "\n", "assert", "relations", "[", "index", "]", ".", "equal", "(", "expected_relations", ")", "\n", "assert", "tail_ids", "[", "index", "]", ".", "equal", "(", "expected_tail_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_knowledge_graph_lookup.KnowledgeGraphLookupTest.test_lookup": [[55, 88], ["torch.LongTensor", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.knowledge_graph_lookup", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index", "zip", "zip", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index", "test_knowledge_graph_lookup.KnowledgeGraphLookupTest.vocab.get_token_index", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "observed.equal", "observed.equal"], "methods", ["None"], ["", "def", "test_lookup", "(", "self", ")", ":", "\n", "# Check that the output of the lookup matches our expectations.", "\n", "        ", "parent_ids", "=", "[", "\n", "self", ".", "vocab", ".", "get_token_index", "(", "'E1'", ",", "'entity_ids'", ")", ",", "\n", "self", ".", "vocab", ".", "get_token_index", "(", "'E2'", ",", "'entity_ids'", ")", ",", "\n", "self", ".", "vocab", ".", "get_token_index", "(", "'E3'", ",", "'entity_ids'", ")", "# Should work, even though E3 not in the KG", "\n", "]", "\n", "parent_ids", "=", "torch", ".", "LongTensor", "(", "parent_ids", ")", "\n", "indices", ",", "_", ",", "relations", ",", "tail_ids", "=", "self", ".", "knowledge_graph_lookup", "(", "parent_ids", ")", "\n", "\n", "# Lookup indices of tokens expected to be in the output", "\n", "e2", "=", "self", ".", "vocab", ".", "get_token_index", "(", "'E2'", ",", "'raw_entity_ids'", ")", "\n", "e3", "=", "self", ".", "vocab", ".", "get_token_index", "(", "'E3'", ",", "'raw_entity_ids'", ")", "\n", "r1", "=", "self", ".", "vocab", ".", "get_token_index", "(", "'R1'", ",", "'relations'", ")", "\n", "r2", "=", "self", ".", "vocab", ".", "get_token_index", "(", "'R2'", ",", "'relations'", ")", "\n", "\n", "# Expected outputs (these are directly transcribed from the KG)", "\n", "expected_indices", "=", "[", "(", "0", ",", ")", ",", "(", "1", ",", ")", "]", "\n", "expected_relations", "=", "[", "\n", "torch", ".", "LongTensor", "(", "[", "r1", ",", "r2", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "r1", "]", ")", "\n", "]", "\n", "expected_tail_ids", "=", "[", "\n", "torch", ".", "LongTensor", "(", "[", "e2", ",", "e3", "]", ")", ",", "\n", "torch", ".", "LongTensor", "(", "[", "e3", "]", ")", "\n", "]", "\n", "\n", "# Check expectations are met", "\n", "assert", "indices", "==", "expected_indices", "\n", "for", "observed", ",", "expected", "in", "zip", "(", "relations", ",", "expected_relations", ")", ":", "\n", "            ", "assert", "observed", ".", "equal", "(", "expected", ")", "\n", "", "for", "observed", ",", "expected", "in", "zip", "(", "tail_ids", ",", "expected_tail_ids", ")", ":", "\n", "            ", "assert", "observed", ".", "equal", "(", "expected", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_locked_dropout.LockedDropoutTest.setUp": [[8, 12], ["kglm.modules.locked_dropout.LockedDropout", "super().setUp"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "dropout_rate", "=", "0.75", "\n", "self", ".", "model", "=", "LockedDropout", "(", ")", "\n", "super", "(", ")", ".", "setUp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_locked_dropout.LockedDropoutTest.test_sequence_elements_droppped_properly": [[13, 25], ["test_locked_dropout.LockedDropoutTest.model.train", "torch.randn", "test_locked_dropout.LockedDropoutTest.model", "row_0.equal", "torch.randn.equal", "test_locked_dropout.LockedDropoutTest.eq", "test_locked_dropout.LockedDropoutTest.eq"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.train"], ["", "def", "test_sequence_elements_droppped_properly", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "train", "(", ")", "\n", "x", "=", "torch", ".", "randn", "(", "10", ",", "10", ",", "10", ")", "\n", "x_prime", "=", "self", ".", "model", "(", "x", ",", "self", ".", "dropout_rate", ")", "\n", "\n", "# Tensors should differ", "\n", "assert", "not", "(", "x", ".", "equal", "(", "x_prime", ")", ")", "\n", "\n", "# And zero elements should be the same across rows", "\n", "row_0", "=", "x_prime", ".", "eq", "(", "0", ")", "[", ":", ",", "0", ",", ":", "]", "\n", "row_1", "=", "x_prime", ".", "eq", "(", "0", ")", "[", ":", ",", "1", ",", ":", "]", "\n", "assert", "row_0", ".", "equal", "(", "row_1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_locked_dropout.LockedDropoutTest.test_sequence_elements_not_dropped_during_eval": [[26, 31], ["test_locked_dropout.LockedDropoutTest.model.eval", "torch.randn", "test_locked_dropout.LockedDropoutTest.model", "torch.randn.equal"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval"], ["", "def", "test_sequence_elements_not_dropped_during_eval", "(", "self", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "x", "=", "torch", ".", "randn", "(", "10", ",", "10", ",", "10", ")", "\n", "x_prime", "=", "self", ".", "model", "(", "x", ",", "self", ".", "dropout_rate", ")", "\n", "assert", "x", ".", "equal", "(", "x_prime", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_dynamic_entity_embeddings.TestDynamicEmbedding.test_initialization_and_reset": [[14, 41], ["kglm.modules.DynamicEmbedding", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertIsNone", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertIsNone", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertIsNone", "kglm.modules.DynamicEmbedding.reset_states", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertIsNotNone", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertIsNotNone", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertIsNotNone", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states"], ["    ", "def", "test_initialization_and_reset", "(", "self", ")", ":", "\n", "        ", "embedding_dim", "=", "4", "\n", "max_embeddings", "=", "10", "\n", "\n", "dynamic_embedding", "=", "DynamicEmbedding", "(", "embedding_dim", ",", "max_embeddings", ")", "\n", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "_initial_embedding", ".", "shape", ",", "(", "embedding_dim", ",", ")", ")", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "_embedding_projection", ".", "in_features", ",", "embedding_dim", ")", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "_embedding_projection", ".", "out_features", ",", "embedding_dim", ")", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "_embedding_projection", ".", "in_features", ",", "embedding_dim", ")", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "_embedding_projection", ".", "out_features", ",", "embedding_dim", ")", "\n", "self", ".", "assertIsNone", "(", "dynamic_embedding", ".", "embeddings", ")", "\n", "self", ".", "assertIsNone", "(", "dynamic_embedding", ".", "num_embeddings", ")", "\n", "self", ".", "assertIsNone", "(", "dynamic_embedding", ".", "last_seen", ")", "\n", "\n", "batch_size", "=", "2", "\n", "dynamic_embedding", ".", "reset_states", "(", "batch_size", ")", "\n", "\n", "self", ".", "assertIsNotNone", "(", "dynamic_embedding", ".", "embeddings", ")", "\n", "self", ".", "assertIsNotNone", "(", "dynamic_embedding", ".", "num_embeddings", ")", "\n", "self", ".", "assertIsNotNone", "(", "dynamic_embedding", ".", "last_seen", ")", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "embeddings", ".", "shape", ",", "\n", "(", "batch_size", ",", "max_embeddings", ",", "embedding_dim", ")", ")", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "num_embeddings", ".", "shape", ",", "\n", "(", "batch_size", ",", ")", ")", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "last_seen", ".", "shape", ",", "\n", "(", "batch_size", ",", "max_embeddings", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_dynamic_entity_embeddings.TestDynamicEmbedding.test_add_embedding": [[42, 67], ["kglm.modules.DynamicEmbedding", "kglm.modules.DynamicEmbedding.reset_states", "torch.tensor", "kglm.modules.DynamicEmbedding.add_embeddings", "torch.zeros_like", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertTrue", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertFalse", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual", "kglm.modules.DynamicEmbedding.embeddings.sum().backward", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertIsNotNone", "torch.allclose", "torch.allclose", "kglm.modules.DynamicEmbedding.embeddings.sum"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.add_embeddings"], ["", "def", "test_add_embedding", "(", "self", ")", ":", "\n", "        ", "embedding_dim", "=", "4", "\n", "max_embeddings", "=", "10", "\n", "batch_size", "=", "2", "\n", "\n", "dynamic_embedding", "=", "DynamicEmbedding", "(", "embedding_dim", ",", "max_embeddings", ")", "\n", "dynamic_embedding", ".", "reset_states", "(", "batch_size", ")", "\n", "\n", "timestep", "=", "1", "\n", "mask", "=", "torch", ".", "tensor", "(", "[", "0", ",", "1", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "# pylint: disable=E1102", "\n", "dynamic_embedding", ".", "add_embeddings", "(", "timestep", ",", "mask", ")", "\n", "\n", "# Check new embeddings[0,1] is zero and [1,1] is non-zero", "\n", "embedding_0", "=", "dynamic_embedding", ".", "embeddings", "[", "0", ",", "1", "]", "\n", "embedding_1", "=", "dynamic_embedding", ".", "embeddings", "[", "1", ",", "1", "]", "\n", "zero", "=", "torch", ".", "zeros_like", "(", "embedding_0", ")", "\n", "self", ".", "assertTrue", "(", "torch", ".", "allclose", "(", "embedding_0", ",", "zero", ")", ")", "\n", "self", ".", "assertFalse", "(", "torch", ".", "allclose", "(", "embedding_1", ",", "zero", ")", ")", "\n", "\n", "# Check last seen is correct", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "last_seen", "[", "1", ",", "1", "]", ",", "1", ")", "\n", "\n", "# Check gradient propagates to initial embedding", "\n", "dynamic_embedding", ".", "embeddings", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "self", ".", "assertIsNotNone", "(", "dynamic_embedding", ".", "_initial_embedding", ".", "grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.test_dynamic_entity_embeddings.TestDynamicEmbedding.test_update_embedding": [[68, 93], ["kglm.modules.DynamicEmbedding", "kglm.modules.DynamicEmbedding.reset_states", "torch.randn", "torch.tensor", "kglm.modules.DynamicEmbedding.embeddings[].clone", "kglm.modules.DynamicEmbedding.update_embeddings", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertFalse", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertEqual", "updated.sum().backward", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertIsNotNone", "test_dynamic_entity_embeddings.TestDynamicEmbedding.assertIsNotNone", "torch.allclose", "updated.sum"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.update_embeddings"], ["", "def", "test_update_embedding", "(", "self", ")", ":", "\n", "        ", "embedding_dim", "=", "4", "\n", "max_embeddings", "=", "10", "\n", "batch_size", "=", "1", "\n", "\n", "dynamic_embedding", "=", "DynamicEmbedding", "(", "embedding_dim", ",", "max_embeddings", ")", "\n", "dynamic_embedding", ".", "reset_states", "(", "batch_size", ")", "\n", "\n", "hidden", "=", "torch", ".", "randn", "(", "(", "batch_size", ",", "embedding_dim", ")", ",", "requires_grad", "=", "True", ")", "\n", "update_indices", "=", "torch", ".", "tensor", "(", "[", "0", "]", ")", "# pylint: disable=E1102", "\n", "timestep", "=", "1", "\n", "\n", "# Check embedding changes on update", "\n", "original", "=", "dynamic_embedding", ".", "embeddings", "[", "0", ",", "0", "]", ".", "clone", "(", ")", "\n", "dynamic_embedding", ".", "update_embeddings", "(", "hidden", ",", "update_indices", ",", "timestep", ")", "\n", "updated", "=", "dynamic_embedding", ".", "embeddings", "[", "0", ",", "0", "]", "\n", "self", ".", "assertFalse", "(", "torch", ".", "allclose", "(", "original", ",", "updated", ")", ")", "\n", "\n", "# Check last seen is correct", "\n", "self", ".", "assertEqual", "(", "dynamic_embedding", ".", "last_seen", "[", "0", ",", "0", "]", ",", "1", ")", "\n", "\n", "# Check gradient propagates to initial embedding and hidden", "\n", "updated", ".", "sum", "(", ")", ".", "backward", "(", ")", "\n", "self", ".", "assertIsNotNone", "(", "dynamic_embedding", ".", "_initial_embedding", ".", "grad", ")", "\n", "self", ".", "assertIsNotNone", "(", "hidden", ".", "grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory.__init__": [[34, 143], ["allennlp.nn.InitializerApplicator", "allennlp.models.Model.__init__", "kglm.modules.RecentEntities", "kglm.modules.LockedDropout", "entity_embedder.get_output_dim", "token_embedder.get_output_dim", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "vocab.get_token_index", "math.log", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.F1Measure", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "initializer", "rnns.append", "kglm.modules.WeightDrop", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "vocab.get_vocab_size", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "vocab.get_vocab_size", "vocab.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "token_embedder", ":", "TextFieldEmbedder", ",", "\n", "entity_embedder", ":", "TextFieldEmbedder", ",", "\n", "alias_encoder", ":", "Seq2SeqEncoder", ",", "\n", "use_shortlist", ":", "bool", ",", "\n", "hidden_size", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "cutoff", ":", "int", "=", "30", ",", "\n", "tie_weights", ":", "bool", "=", "False", ",", "\n", "dropout", ":", "float", "=", "0.4", ",", "\n", "dropouth", ":", "float", "=", "0.3", ",", "\n", "dropouti", ":", "float", "=", "0.65", ",", "\n", "dropoute", ":", "float", "=", "0.1", ",", "\n", "wdrop", ":", "float", "=", "0.5", ",", "\n", "alpha", ":", "float", "=", "2.0", ",", "\n", "beta", ":", "float", "=", "1.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ")", "->", "None", ":", "\n", "        ", "super", "(", "NoStory", ",", "self", ")", ".", "__init__", "(", "vocab", ")", "\n", "\n", "# We extract the `Embedding` layers from the `TokenEmbedders` to apply dropout later on.", "\n", "# pylint: disable=protected-access", "\n", "self", ".", "_token_embedder", "=", "token_embedder", ".", "_token_embedders", "[", "'tokens'", "]", "\n", "self", ".", "_entity_embedder", "=", "entity_embedder", ".", "_token_embedders", "[", "'entity_ids'", "]", "\n", "self", ".", "_alias_encoder", "=", "alias_encoder", "\n", "self", ".", "_recent_entities", "=", "RecentEntities", "(", "cutoff", "=", "cutoff", ")", "\n", "self", ".", "_use_shortlist", "=", "use_shortlist", "\n", "self", ".", "_hidden_size", "=", "hidden_size", "\n", "self", ".", "_num_layers", "=", "num_layers", "\n", "self", ".", "_cutoff", "=", "cutoff", "\n", "self", ".", "_tie_weights", "=", "tie_weights", "\n", "\n", "# Dropout", "\n", "self", ".", "_locked_dropout", "=", "LockedDropout", "(", ")", "\n", "self", ".", "_dropout", "=", "dropout", "\n", "self", ".", "_dropouth", "=", "dropouth", "\n", "self", ".", "_dropouti", "=", "dropouti", "\n", "self", ".", "_dropoute", "=", "dropoute", "\n", "self", ".", "_wdrop", "=", "wdrop", "\n", "\n", "# Regularization strength", "\n", "self", ".", "_alpha", "=", "alpha", "\n", "self", ".", "_beta", "=", "beta", "\n", "\n", "# RNN Encoders.", "\n", "entity_embedding_dim", "=", "entity_embedder", ".", "get_output_dim", "(", ")", "\n", "token_embedding_dim", "=", "token_embedder", ".", "get_output_dim", "(", ")", "\n", "self", ".", "entity_embedding_dim", "=", "entity_embedding_dim", "\n", "self", ".", "token_embedding_dim", "=", "token_embedding_dim", "\n", "\n", "rnns", ":", "List", "[", "torch", ".", "nn", ".", "Module", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "input_size", "=", "token_embedding_dim", "\n", "", "else", ":", "\n", "                ", "input_size", "=", "hidden_size", "\n", "", "if", "(", "i", "==", "num_layers", "-", "1", ")", ":", "\n", "                ", "output_size", "=", "token_embedding_dim", "+", "entity_embedding_dim", "\n", "", "else", ":", "\n", "                ", "output_size", "=", "hidden_size", "\n", "", "rnns", ".", "append", "(", "torch", ".", "nn", ".", "LSTM", "(", "input_size", ",", "output_size", ",", "batch_first", "=", "True", ")", ")", "\n", "", "rnns", "=", "[", "WeightDrop", "(", "rnn", ",", "[", "'weight_hh_l0'", "]", ",", "dropout", "=", "wdrop", ")", "for", "rnn", "in", "rnns", "]", "\n", "self", ".", "rnns", "=", "torch", ".", "nn", ".", "ModuleList", "(", "rnns", ")", "\n", "\n", "# Various linear transformations.", "\n", "self", ".", "_fc_mention_type", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "token_embedding_dim", ",", "\n", "out_features", "=", "2", ")", "\n", "\n", "if", "not", "use_shortlist", ":", "\n", "            ", "self", ".", "_fc_new_entity", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "entity_embedding_dim", ",", "\n", "out_features", "=", "vocab", ".", "get_vocab_size", "(", "'entity_ids'", ")", ")", "\n", "\n", "if", "tie_weights", ":", "\n", "                ", "self", ".", "_fc_new_entity", ".", "weight", "=", "self", ".", "_entity_embedder", ".", "weight", "\n", "\n", "", "", "self", ".", "_fc_condense", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "token_embedding_dim", "+", "entity_embedding_dim", ",", "\n", "out_features", "=", "token_embedding_dim", ")", "\n", "\n", "self", ".", "_fc_generate", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "token_embedding_dim", ",", "\n", "out_features", "=", "vocab", ".", "get_vocab_size", "(", "'tokens'", ")", ")", "\n", "\n", "self", ".", "_fc_copy", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "token_embedding_dim", ",", "\n", "out_features", "=", "token_embedding_dim", ")", "\n", "\n", "if", "tie_weights", ":", "\n", "            ", "self", ".", "_fc_generate", ".", "weight", "=", "self", ".", "_token_embedder", ".", "weight", "\n", "\n", "", "self", ".", "_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", "\n", "\n", "# Metrics", "\n", "self", ".", "_unk_index", "=", "vocab", ".", "get_token_index", "(", "DEFAULT_OOV_TOKEN", ")", "\n", "self", ".", "_unk_penalty", "=", "math", ".", "log", "(", "vocab", ".", "get_vocab_size", "(", "'tokens_unk'", ")", ")", "\n", "self", ".", "_ppl", "=", "Ppl", "(", ")", "\n", "self", ".", "_upp", "=", "Ppl", "(", ")", "\n", "self", ".", "_kg_ppl", "=", "Ppl", "(", ")", "# Knowledge-graph ppl", "\n", "self", ".", "_bg_ppl", "=", "Ppl", "(", ")", "# Background ppl", "\n", "self", ".", "_avg_mention_type_loss", "=", "Average", "(", ")", "\n", "self", ".", "_avg_new_entity_loss", "=", "Average", "(", ")", "\n", "self", ".", "_avg_vocab_loss", "=", "Average", "(", ")", "\n", "self", ".", "_new_mention_f1", "=", "F1Measure", "(", "positive_label", "=", "1", ")", "\n", "self", ".", "_new_entity_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "self", ".", "_new_entity_accuracy20", "=", "CategoricalAccuracy", "(", "top_k", "=", "20", ")", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory.forward": [[144, 191], ["alias_database.tensorize", "no_story.NoStory._recent_entities.reset", "reset.any", "range", "no_story.NoStory._forward_loop", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.tensorize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._forward_loop"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "target", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "reset", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "mention_type", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "raw_entity_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "entity_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "parent_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "relations", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "shortlist", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "shortlist_inds", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "alias_copy_inds", ":", "torch", ".", "Tensor", "=", "None", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "# Tensorize the alias_database - this will only perform the operation once.", "\n", "        ", "alias_database", "=", "metadata", "[", "0", "]", "[", "'alias_database'", "]", "\n", "alias_database", ".", "tensorize", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n", "# Reset the model if needed", "\n", "if", "reset", ".", "any", "(", ")", "and", "(", "self", ".", "_state", "is", "not", "None", ")", ":", "\n", "            ", "for", "layer", "in", "range", "(", "self", ".", "_num_layers", ")", ":", "\n", "                ", "h", ",", "c", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "h", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "h", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "c", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "c", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "=", "(", "h", ",", "c", ")", "\n", "", "", "self", ".", "_recent_entities", ".", "reset", "(", "reset", ")", "\n", "\n", "if", "entity_ids", "is", "not", "None", ":", "\n", "            ", "output_dict", "=", "self", ".", "_forward_loop", "(", "\n", "source", "=", "source", ",", "\n", "target", "=", "target", ",", "\n", "alias_database", "=", "alias_database", ",", "\n", "mention_type", "=", "mention_type", ",", "\n", "raw_entity_ids", "=", "raw_entity_ids", ",", "\n", "entity_ids", "=", "entity_ids", ",", "\n", "parent_ids", "=", "parent_ids", ",", "\n", "relations", "=", "relations", ",", "\n", "shortlist", "=", "shortlist", ",", "\n", "shortlist_inds", "=", "shortlist_inds", ",", "\n", "alias_copy_inds", "=", "alias_copy_inds", ")", "\n", "", "else", ":", "\n", "# TODO: Figure out what we want here - probably to do some king of inference on", "\n", "# entities / mention types.", "\n", "            ", "output_dict", "=", "{", "}", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory._encode_source": [[192, 231], ["kglm.modules.embedded_dropout", "no_story.NoStory._locked_dropout", "enumerate", "no_story.NoStory.pow().mean", "rnn", "output.contiguous.contiguous.contiguous", "tuple", "hidden_states.append", "no_story.NoStory._locked_dropout", "no_story.NoStory._locked_dropout", "no_story.NoStory.pow", "enumerate", "h.detach"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], ["", "def", "_encode_source", "(", "self", ",", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "# Extract and embed source tokens.", "\n", "        ", "source_embeddings", "=", "embedded_dropout", "(", "\n", "embed", "=", "self", ".", "_token_embedder", ",", "\n", "words", "=", "source", ",", "\n", "dropout", "=", "self", ".", "_dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "source_embeddings", "=", "self", ".", "_locked_dropout", "(", "source_embeddings", ",", "self", ".", "_dropouti", ")", "\n", "\n", "# Encode.", "\n", "current_input", "=", "source_embeddings", "\n", "hidden_states", "=", "[", "]", "\n", "for", "layer", ",", "rnn", "in", "enumerate", "(", "self", ".", "rnns", ")", ":", "\n", "# Retrieve previous hidden state for layer.", "\n", "            ", "if", "self", ".", "_state", "is", "not", "None", ":", "\n", "                ", "prev_hidden", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "", "else", ":", "\n", "                ", "prev_hidden", "=", "None", "\n", "# Forward-pass.", "\n", "", "output", ",", "hidden", "=", "rnn", "(", "current_input", ",", "prev_hidden", ")", "\n", "output", "=", "output", ".", "contiguous", "(", ")", "\n", "# Update hidden state for layer.", "\n", "hidden", "=", "tuple", "(", "h", ".", "detach", "(", ")", "for", "h", "in", "hidden", ")", "\n", "hidden_states", ".", "append", "(", "hidden", ")", "\n", "# Apply dropout.", "\n", "if", "layer", "==", "self", ".", "_num_layers", "-", "1", ":", "\n", "                ", "dropped_output", "=", "self", ".", "_locked_dropout", "(", "output", ",", "self", ".", "_dropout", ")", "\n", "", "else", ":", "\n", "                ", "dropped_output", "=", "self", ".", "_locked_dropout", "(", "output", ",", "self", ".", "_dropouth", ")", "\n", "", "current_input", "=", "dropped_output", "\n", "", "encoded", "=", "current_input", "\n", "\n", "alpha_loss", "=", "dropped_output", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "beta_loss", "=", "(", "output", "[", ":", ",", "1", ":", "]", "-", "output", "[", ":", ",", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "# Update state.", "\n", "self", ".", "_state", "=", "{", "'layer_%i'", "%", "i", ":", "h", "for", "i", ",", "h", "in", "enumerate", "(", "hidden_states", ")", "}", "\n", "\n", "return", "encoded", ",", "alpha_loss", ",", "beta_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory._mention_type_loss": [[232, 249], ["no_story.NoStory._fc_mention_type", "allennlp.nn.util.sequence_cross_entropy_with_logits", "no_story.NoStory._new_mention_f1"], "methods", ["None"], ["", "def", "_mention_type_loss", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "mention_type", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes the loss for predicting whether or not the the next token will be part of an\n        entity mention.\n        \"\"\"", "\n", "logits", "=", "self", ".", "_fc_mention_type", "(", "encoded", ")", "\n", "mention_type_loss", "=", "sequence_cross_entropy_with_logits", "(", "logits", ",", "mention_type", ",", "mask", ",", "\n", "average", "=", "'token'", ")", "\n", "# if not self.training:", "\n", "self", ".", "_new_mention_f1", "(", "predictions", "=", "logits", ",", "\n", "gold_labels", "=", "mention_type", ",", "\n", "mask", "=", "mask", ")", "\n", "\n", "return", "mention_type_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory._new_entity_loss": [[250, 310], ["allennlp.nn.util.get_text_field_mask", "kglm.modules.embedded_dropout", "no_story.NoStory._locked_dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "allennlp.nn.util.masked_log_softmax", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "no_story.NoStory._new_entity_accuracy", "no_story.NoStory._new_entity_accuracy20", "no_story.NoStory._fc_new_entity", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax.view", "target_inds.view", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "no_story.NoStory._new_entity_accuracy", "no_story.NoStory._new_entity_accuracy20", "kglm.modules.embedded_dropout.transpose", "target_inds.eq", "target_inds.view.eq", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather().squeeze.sum", "torch.gather().squeeze.sum", "target_mask.sum", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather().squeeze.sum", "torch.gather().squeeze.sum", "target_mask.sum", "target_inds.unsqueeze", "target_inds.view.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], ["", "def", "_new_entity_loss", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "target_inds", ":", "torch", ".", "Tensor", ",", "\n", "shortlist", ":", "torch", ".", "Tensor", ",", "\n", "target_mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ==========\n        target_inds : ``torch.Tensor``\n            Either the shortlist inds if using shortlist, otherwise the target entity ids.\n        \"\"\"", "\n", "if", "self", ".", "_use_shortlist", ":", "\n", "\n", "# First we embed the shortlist entries", "\n", "            ", "shortlist_mask", "=", "get_text_field_mask", "(", "shortlist", ")", "\n", "shortlist_embeddings", "=", "embedded_dropout", "(", "\n", "embed", "=", "self", ".", "_entity_embedder", ",", "\n", "words", "=", "shortlist", "[", "'entity_ids'", "]", ",", "\n", "dropout", "=", "self", ".", "_dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "\n", "# Logits are computed using the inner product that between the predicted entity embedding", "\n", "# and the embeddings of entities in the shortlist", "\n", "encodings", "=", "self", ".", "_locked_dropout", "(", "encoded", ",", "self", ".", "_dropout", ")", "\n", "logits", "=", "torch", ".", "bmm", "(", "encodings", ",", "shortlist_embeddings", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "# Take masked softmax to get log probabilties and gather the targets.", "\n", "log_probs", "=", "masked_log_softmax", "(", "logits", ",", "shortlist_mask", ")", "\n", "target_log_probs", "=", "torch", ".", "gather", "(", "log_probs", ",", "-", "1", ",", "target_inds", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "# If not generating a new mention, the action is deterministic - so the loss is 0 for these tokens.", "\n", "mask", "=", "~", "target_inds", ".", "eq", "(", "0", ")", "\n", "target_log_probs", "[", "~", "mask", "]", "=", "0", "\n", "\n", "# if not self.training:", "\n", "self", ".", "_new_entity_accuracy", "(", "predictions", "=", "log_probs", "[", "mask", "]", ",", "\n", "gold_labels", "=", "target_inds", "[", "mask", "]", ")", "\n", "self", ".", "_new_entity_accuracy20", "(", "predictions", "=", "log_probs", "[", "mask", "]", ",", "\n", "gold_labels", "=", "target_inds", "[", "mask", "]", ")", "\n", "\n", "# Return the token-wise average loss", "\n", "return", "-", "target_log_probs", ".", "sum", "(", ")", "/", "(", "target_mask", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n", "", "else", ":", "\n", "            ", "logits", "=", "self", ".", "_fc_new_entity", "(", "encoded", ")", "\n", "log_probs", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "num_categories", "=", "log_probs", ".", "shape", "[", "-", "1", "]", "\n", "flat_log_probs", "=", "log_probs", ".", "view", "(", "-", "1", ",", "num_categories", ")", "\n", "flat_target_inds", "=", "target_inds", ".", "view", "(", "-", "1", ")", "\n", "target_log_probs", "=", "torch", ".", "gather", "(", "flat_log_probs", ",", "-", "1", ",", "flat_target_inds", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "mask", "=", "~", "flat_target_inds", ".", "eq", "(", "0", ")", "\n", "target_log_probs", "[", "~", "mask", "]", "=", "0", "\n", "\n", "self", ".", "_new_entity_accuracy", "(", "predictions", "=", "flat_log_probs", "[", "mask", "]", ",", "\n", "gold_labels", "=", "flat_target_inds", "[", "mask", "]", ")", "\n", "self", ".", "_new_entity_accuracy20", "(", "predictions", "=", "flat_log_probs", "[", "mask", "]", ",", "\n", "gold_labels", "=", "flat_target_inds", "[", "mask", "]", ")", "\n", "\n", "return", "-", "target_log_probs", ".", "sum", "(", ")", "/", "(", "target_mask", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory._parent_log_probs": [[311, 363], ["no_story.NoStory._recent_entities", "logger.debug", "kglm.modules.embedded_dropout", "no_story.NoStory._locked_dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "allennlp.nn.util.masked_log_softmax", "parent_ids.unsqueeze", "candidate_ids.view", "parent_ids.unsqueeze.eq", "logger.debug", "torch.max", "torch.max", "torch.max", "torch.max", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "kglm.modules.embedded_dropout.transpose", "candidate_ids.view.eq", "allennlp.nn.util.masked_log_softmax.unsqueeze", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "mask.float"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], ["", "", "def", "_parent_log_probs", "(", "self", ",", "\n", "encoded_head", ":", "torch", ".", "Tensor", ",", "\n", "entity_ids", ":", "torch", ".", "Tensor", ",", "\n", "parent_ids", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "# Lookup recent entities (which are candidates for parents) and get their embeddings.", "\n", "        ", "candidate_ids", ",", "candidate_mask", "=", "self", ".", "_recent_entities", "(", "entity_ids", ")", "\n", "logger", ".", "debug", "(", "'Candidate ids shape: %s'", ",", "candidate_ids", ".", "shape", ")", "\n", "candidate_embeddings", "=", "embedded_dropout", "(", "self", ".", "_entity_embedder", ",", "\n", "words", "=", "candidate_ids", ",", "\n", "dropout", "=", "self", ".", "_dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "\n", "# Logits are computed using a general bilinear form that measures the similarity between", "\n", "# the projected hidden state and the embeddings of candidate entities", "\n", "encoded", "=", "self", ".", "_locked_dropout", "(", "encoded_head", ",", "self", ".", "_dropout", ")", "\n", "selection_logits", "=", "torch", ".", "bmm", "(", "encoded", ",", "candidate_embeddings", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "# Get log probabilities using masked softmax (need to double check mask works properly).", "\n", "\n", "# shape: (batch_size, sequence_length, num_candidates)", "\n", "log_probs", "=", "masked_log_softmax", "(", "selection_logits", ",", "candidate_mask", ")", "\n", "\n", "# Now for the tricky part. We need to convert the parent ids to a mask that selects the", "\n", "# relevant probabilities from log_probs. To do this we need to align the candidates with", "\n", "# the parent ids, which can be achieved by an element-wise equality comparison. We also", "\n", "# need to ensure that null parents are not selected.", "\n", "\n", "# shape: (batch_size, sequence_length, num_parents, 1)", "\n", "_parent_ids", "=", "parent_ids", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "batch_size", ",", "num_candidates", "=", "candidate_ids", ".", "shape", "\n", "# shape: (batch_size, 1, 1, num_candidates)", "\n", "_candidate_ids", "=", "candidate_ids", ".", "view", "(", "batch_size", ",", "1", ",", "1", ",", "num_candidates", ")", "\n", "\n", "# shape: (batch_size, sequence_length, num_parents, num_candidates)", "\n", "is_parent", "=", "_parent_ids", ".", "eq", "(", "_candidate_ids", ")", "\n", "# shape: (batch_size, 1, 1, num_candidates)", "\n", "non_null", "=", "~", "_candidate_ids", ".", "eq", "(", "0", ")", "\n", "\n", "# Since multiplication is addition in log-space, we can apply mask by adding its log (+", "\n", "# some small constant for numerical stability).", "\n", "mask", "=", "is_parent", "&", "non_null", "\n", "masked_log_probs", "=", "log_probs", ".", "unsqueeze", "(", "2", ")", "+", "(", "mask", ".", "float", "(", ")", "+", "1e-45", ")", ".", "log", "(", ")", "\n", "logger", ".", "debug", "(", "'Masked log probs shape: %s'", ",", "masked_log_probs", ".", "shape", ")", "\n", "\n", "# Lastly, we need to get rid of the num_candidates dimension. The easy way to do this would", "\n", "# be to marginalize it out. However, since our data is sparse (the last two dims are", "\n", "# essentially a delta function) this would add a lot of unneccesary terms to the computation graph.", "\n", "# To get around this we are going to try to use a gather.", "\n", "_", ",", "index", "=", "torch", ".", "max", "(", "mask", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "target_log_probs", "=", "torch", ".", "gather", "(", "masked_log_probs", ",", "dim", "=", "-", "1", ",", "index", "=", "index", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "return", "target_log_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory._generate_scores": [[364, 373], ["kglm.modules.embedded_dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "no_story.NoStory._fc_condense", "no_story.NoStory._fc_generate"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], ["", "def", "_generate_scores", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "entity_ids", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "entity_embeddings", "=", "embedded_dropout", "(", "embed", "=", "self", ".", "_entity_embedder", ",", "\n", "words", "=", "entity_ids", ",", "\n", "dropout", "=", "self", ".", "_dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "concatenated", "=", "torch", ".", "cat", "(", "(", "encoded", ",", "entity_embeddings", ")", ",", "dim", "=", "-", "1", ")", "\n", "condensed", "=", "self", ".", "_fc_condense", "(", "concatenated", ")", "\n", "return", "self", ".", "_fc_generate", "(", "condensed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory._copy_scores": [[374, 404], ["alias_tokens.view", "no_story.NoStory._token_embedder", "alias_tokens.view.gt", "no_story.NoStory._alias_encoder", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "no_story.NoStory._locked_dropout", "encoded.view.view.view", "projected.view.view.view", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "copy_scores.view().contiguous.view().contiguous.view().contiguous", "logger.debug", "copy_mask.sum", "encoded.view.view.new_zeros", "no_story.NoStory._fc_copy", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "copy_scores.view().contiguous.view().contiguous.view"], "methods", ["None"], ["", "def", "_copy_scores", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "alias_tokens", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "# Begin by flattening the tokens so that they fit the expected shape of a", "\n", "# ``Seq2SeqEncoder``.", "\n", "        ", "batch_size", ",", "sequence_length", ",", "num_aliases", ",", "alias_length", "=", "alias_tokens", ".", "shape", "\n", "flattened", "=", "alias_tokens", ".", "view", "(", "-", "1", ",", "alias_length", ")", "\n", "copy_mask", "=", "flattened", "!=", "0", "\n", "if", "copy_mask", ".", "sum", "(", ")", "==", "0", ":", "\n", "            ", "return", "encoded", ".", "new_zeros", "(", "(", "batch_size", ",", "sequence_length", ",", "num_aliases", "*", "alias_length", ")", ",", "\n", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "# Embed and encode the alias tokens.", "\n", "", "embedded", "=", "self", ".", "_token_embedder", "(", "flattened", ")", "\n", "mask", "=", "flattened", ".", "gt", "(", "0", ")", "\n", "encoded_aliases", "=", "self", ".", "_alias_encoder", "(", "embedded", ",", "mask", ")", "\n", "\n", "# Equation 8 in the CopyNet paper recommends applying the additional step.", "\n", "projected", "=", "torch", ".", "tanh", "(", "self", ".", "_fc_copy", "(", "encoded_aliases", ")", ")", "\n", "projected", "=", "self", ".", "_locked_dropout", "(", "projected", ",", "self", ".", "_dropout", ")", "\n", "\n", "# This part gets a little funky - we need to make sure that the first dimension in", "\n", "# `projected` and `hidden` is batch_size x sequence_length.", "\n", "encoded", "=", "encoded", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "1", ",", "-", "1", ")", "\n", "projected", "=", "projected", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ",", "num_aliases", "*", "alias_length", ")", "\n", "copy_scores", "=", "torch", ".", "bmm", "(", "encoded", ",", "projected", ")", ".", "squeeze", "(", ")", "\n", "copy_scores", "=", "copy_scores", ".", "view", "(", "batch_size", ",", "sequence_length", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "logger", ".", "debug", "(", "'Copy scores shape: %s'", ",", "copy_scores", ".", "shape", ")", "\n", "\n", "return", "copy_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory._vocab_loss": [[405, 500], ["target_tokens.view", "mask.view().byte", "alias_indices.view.view.view().gt", "mask.new_ones", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "allennlp.nn.util.masked_log_softmax", "allennlp.nn.util.masked_log_softmax.view", "allennlp.nn.util.masked_log_softmax.view.gather", "target_tokens.eq().view", "target_alias_indices.view.view.gt().view", "alias_indices.view.view.view", "target_alias_indices.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "flattened_mask.squeeze.squeeze.squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "no_story.NoStory._ppl", "no_story.NoStory._upp", "kg_mask.any", "bg_mask.any", "target_alias_indices.view.view.gt", "flattened_mask.squeeze.squeeze.float", "no_story.NoStory._kg_ppl", "no_story.NoStory._bg_ppl", "mask.view", "alias_indices.view.view.view", "target_tokens.eq", "target_alias_indices.view.view.gt", "alias_indices.view.view.eq", "torch.logsumexp.sum", "torch.logsumexp.sum", "mask.sum", "target_tokens.eq().view.squeeze", "true_unks.float", "penalized_log_probs.sum", "mask.sum", "target_tokens.eq().view.float", "combined_log_probs_source_vocab[].sum", "mask.float().sum", "penalized_log_probs_source_vocab[].sum", "mask.float().sum", "target_alias_indices.view.gt().view.squeeze", "mask.byte", "mask.byte", "combined_log_probs_source_vocab[].sum", "kg_mask.float().sum", "combined_log_probs_source_vocab[].sum", "bg_mask.float().sum", "generate_mask.float", "copy_mask.float", "mask.float", "mask.float", "kg_mask.float", "bg_mask.float"], "methods", ["None"], ["", "def", "_vocab_loss", "(", "self", ",", "\n", "generate_scores", ":", "torch", ".", "Tensor", ",", "\n", "copy_scores", ":", "torch", ".", "Tensor", ",", "\n", "target_tokens", ":", "torch", ".", "Tensor", ",", "\n", "target_alias_indices", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", "alias_indices", ":", "torch", ".", "Tensor", ",", "\n", "mention_mask", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "batch_size", ",", "sequence_length", ",", "vocab_size", "=", "generate_scores", ".", "shape", "\n", "copy_sequence_length", "=", "copy_scores", ".", "shape", "[", "-", "1", "]", "\n", "\n", "# Flat sequences make life **much** easier.", "\n", "flattened_targets", "=", "target_tokens", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "1", ")", "\n", "flattened_mask", "=", "mask", ".", "view", "(", "-", "1", ",", "1", ")", ".", "byte", "(", ")", "\n", "\n", "# In order to obtain proper log probabilities we create a mask to omit padding alias tokens", "\n", "# from the calculation.", "\n", "alias_mask", "=", "alias_indices", ".", "view", "(", "batch_size", ",", "sequence_length", ",", "-", "1", ")", ".", "gt", "(", "0", ")", "\n", "score_mask", "=", "mask", ".", "new_ones", "(", "batch_size", ",", "sequence_length", ",", "vocab_size", "+", "copy_sequence_length", ")", "\n", "score_mask", "[", ":", ",", ":", ",", "vocab_size", ":", "]", "=", "alias_mask", "\n", "\n", "# The log-probability distribution is then given by taking the masked log softmax.", "\n", "concatenated_scores", "=", "torch", ".", "cat", "(", "(", "generate_scores", ",", "copy_scores", ")", ",", "dim", "=", "-", "1", ")", "\n", "log_probs", "=", "masked_log_softmax", "(", "concatenated_scores", ",", "score_mask", ")", "\n", "\n", "# GENERATE LOSS ###", "\n", "# The generated token loss is a simple cross-entropy calculation, we can just gather", "\n", "# the log probabilties...", "\n", "flattened_log_probs", "=", "log_probs", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ")", "\n", "generate_log_probs_source_vocab", "=", "flattened_log_probs", ".", "gather", "(", "1", ",", "flattened_targets", ")", "\n", "# ...except we need to ignore the contribution of UNK tokens that are copied (only when", "\n", "# computing the loss). To do that we create a mask which is 1 only if the token is not a", "\n", "# copied UNK (or padding).", "\n", "unks", "=", "target_tokens", ".", "eq", "(", "self", ".", "_unk_index", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "copied", "=", "target_alias_indices", ".", "gt", "(", "0", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "generate_mask", "=", "~", "(", "unks", "&", "copied", ")", "&", "flattened_mask", "\n", "# Since we are in log-space we apply the mask by addition.", "\n", "generate_log_probs_extended_vocab", "=", "generate_log_probs_source_vocab", "+", "(", "generate_mask", ".", "float", "(", ")", "+", "1e-45", ")", ".", "log", "(", ")", "\n", "\n", "# COPY LOSS ###", "\n", "copy_log_probs", "=", "flattened_log_probs", "[", ":", ",", "vocab_size", ":", "]", "\n", "# When computing the loss we need to get the log probability of **only** the copied tokens.", "\n", "alias_indices", "=", "alias_indices", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ")", "\n", "target_alias_indices", "=", "target_alias_indices", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "copy_mask", "=", "alias_indices", ".", "eq", "(", "target_alias_indices", ")", "&", "flattened_mask", "&", "target_alias_indices", ".", "gt", "(", "0", ")", "\n", "copy_log_probs", "=", "copy_log_probs", "+", "(", "copy_mask", ".", "float", "(", ")", "+", "1e-45", ")", ".", "log", "(", ")", "\n", "\n", "# COMBINED LOSS ###", "\n", "# The final loss term is computed using our log probs computed w.r.t to the entire", "\n", "# vocabulary.", "\n", "combined_log_probs_extended_vocab", "=", "torch", ".", "cat", "(", "(", "generate_log_probs_extended_vocab", ",", "\n", "copy_log_probs", ")", ",", "\n", "dim", "=", "1", ")", "\n", "combined_log_probs_extended_vocab", "=", "torch", ".", "logsumexp", "(", "combined_log_probs_extended_vocab", ",", "\n", "dim", "=", "1", ")", "\n", "flattened_mask", "=", "flattened_mask", ".", "squeeze", "(", ")", "\n", "# Zero out padding loss", "\n", "combined_log_probs_extended_vocab", "=", "combined_log_probs_extended_vocab", "*", "flattened_mask", ".", "float", "(", ")", "\n", "vocab_loss", "=", "-", "combined_log_probs_extended_vocab", ".", "sum", "(", ")", "/", "(", "mask", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n", "# Unknown penalty - only applies to non-copied unks", "\n", "true_unks", "=", "unks", ".", "squeeze", "(", ")", "&", "~", "copied", ".", "squeeze", "(", ")", "&", "flattened_mask", "\n", "penalized_log_probs", "=", "combined_log_probs_extended_vocab", "-", "self", ".", "_unk_penalty", "*", "true_unks", ".", "float", "(", ")", "\n", "penalized_log_probs", "[", "~", "flattened_mask", "]", "=", "0", "\n", "penalized_vocab_loss", "=", "-", "penalized_log_probs", ".", "sum", "(", ")", "/", "(", "mask", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n", "# PERPLEXITY ###", "\n", "# Our perplexity terms are computed using the log probs computed w.r.t the source", "\n", "# vocabulary.", "\n", "combined_log_probs_source_vocab", "=", "torch", ".", "cat", "(", "(", "generate_log_probs_source_vocab", ",", "\n", "copy_log_probs", ")", ",", "\n", "dim", "=", "1", ")", "\n", "combined_log_probs_source_vocab", "=", "torch", ".", "logsumexp", "(", "combined_log_probs_source_vocab", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "# For UPP we penalize **only** p(UNK); not the copy probabilities!", "\n", "penalized_log_probs_source_vocab", "=", "generate_log_probs_source_vocab", "-", "self", ".", "_unk_penalty", "*", "unks", ".", "float", "(", ")", "\n", "penalized_log_probs_source_vocab", "=", "torch", ".", "cat", "(", "(", "penalized_log_probs_source_vocab", ",", "\n", "copy_log_probs", ")", ",", "\n", "dim", "=", "1", ")", "\n", "penalized_log_probs_source_vocab", "=", "torch", ".", "logsumexp", "(", "penalized_log_probs_source_vocab", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "kg_mask", "=", "(", "mention_mask", "*", "mask", ".", "byte", "(", ")", ")", ".", "view", "(", "-", "1", ")", "\n", "bg_mask", "=", "(", "(", "1", "-", "mention_mask", ")", "*", "mask", ".", "byte", "(", ")", ")", ".", "view", "(", "-", "1", ")", "\n", "mask", "=", "(", "kg_mask", "|", "bg_mask", ")", "\n", "\n", "self", ".", "_ppl", "(", "-", "combined_log_probs_source_vocab", "[", "mask", "]", ".", "sum", "(", ")", ",", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "self", ".", "_upp", "(", "-", "penalized_log_probs_source_vocab", "[", "mask", "]", ".", "sum", "(", ")", ",", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "if", "kg_mask", ".", "any", "(", ")", ":", "\n", "            ", "self", ".", "_kg_ppl", "(", "-", "combined_log_probs_source_vocab", "[", "kg_mask", "]", ".", "sum", "(", ")", ",", "kg_mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "", "if", "bg_mask", ".", "any", "(", ")", ":", "\n", "            ", "self", ".", "_bg_ppl", "(", "-", "combined_log_probs_source_vocab", "[", "bg_mask", "]", ".", "sum", "(", ")", ",", "bg_mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n", "", "return", "vocab_loss", ",", "penalized_vocab_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory._forward_loop": [[501, 580], ["allennlp.nn.util.get_text_field_mask", "logger.debug", "logger.debug", "logger.debug", "no_story.NoStory._encode_source", "encoded.split", "mention_type.gt().long.gt().long.gt().long", "no_story.NoStory._mention_type_loss", "no_story.NoStory._avg_mention_type_loss", "no_story.NoStory._avg_new_entity_loss", "no_story.NoStory._generate_scores", "alias_database.lookup", "no_story.NoStory._copy_scores", "no_story.NoStory._vocab_loss", "no_story.NoStory._avg_vocab_loss", "float", "no_story.NoStory._new_entity_loss", "no_story.NoStory._new_entity_loss", "float", "entity_ids.gt", "float", "allennlp.nn.util.get_text_field_mask.sum", "allennlp.nn.util.get_text_field_mask.sum", "mention_type.gt().long.gt().long.gt"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._encode_source", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._mention_type_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._generate_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.lookup", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._copy_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._vocab_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_loss"], ["", "def", "_forward_loop", "(", "self", ",", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "target", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "alias_database", ":", "AliasDatabase", ",", "\n", "mention_type", ":", "torch", ".", "Tensor", ",", "\n", "raw_entity_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "entity_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "parent_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "relations", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "shortlist", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "shortlist_inds", ":", "torch", ".", "Tensor", ",", "\n", "alias_copy_inds", ":", "torch", ".", "Tensor", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# Get the token mask and extract indexed text fields.", "\n", "# shape: (batch_size, sequence_length)", "\n", "        ", "target_mask", "=", "get_text_field_mask", "(", "target", ")", "\n", "source", "=", "source", "[", "'tokens'", "]", "\n", "target", "=", "target", "[", "'tokens'", "]", "\n", "raw_entity_ids", "=", "raw_entity_ids", "[", "'raw_entity_ids'", "]", "\n", "entity_ids", "=", "entity_ids", "[", "'entity_ids'", "]", "\n", "\n", "logger", ".", "debug", "(", "'Source & Target shape: %s'", ",", "source", ".", "shape", ")", "\n", "logger", ".", "debug", "(", "'Entity ids shape: %s'", ",", "entity_ids", ".", "shape", ")", "\n", "logger", ".", "debug", "(", "'Shortlist shape: %s'", ",", "shortlist", "[", "'entity_ids'", "]", ".", "shape", ")", "\n", "# Embed source tokens.", "\n", "# shape: (batch_size, sequence_length, embedding_dim)", "\n", "encoded", ",", "alpha_loss", ",", "beta_loss", "=", "self", ".", "_encode_source", "(", "source", ")", "\n", "splits", "=", "[", "self", ".", "token_embedding_dim", "]", "+", "[", "self", ".", "entity_embedding_dim", "]", "\n", "encoded_token", ",", "encoded_head", "=", "encoded", ".", "split", "(", "splits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Predict whether or not the next token will be an entity mention, and if so which type.", "\n", "mention_type", "=", "mention_type", ".", "gt", "(", "0", ")", ".", "long", "(", ")", "#  Map 1, 2 -> 1", "\n", "mention_type_loss", "=", "self", ".", "_mention_type_loss", "(", "encoded_token", ",", "mention_type", ",", "target_mask", ")", "\n", "self", ".", "_avg_mention_type_loss", "(", "float", "(", "mention_type_loss", ")", ")", "\n", "\n", "# For new mentions, predict which entity (among those in the supplied shortlist) will be", "\n", "# mentioned.", "\n", "if", "self", ".", "_use_shortlist", ":", "\n", "            ", "new_entity_loss", "=", "self", ".", "_new_entity_loss", "(", "encoded_head", ",", "\n", "shortlist_inds", ",", "\n", "shortlist", ",", "\n", "target_mask", ")", "\n", "", "else", ":", "\n", "            ", "new_entity_loss", "=", "self", ".", "_new_entity_loss", "(", "encoded_head", ",", "\n", "entity_ids", ",", "\n", "None", ",", "\n", "target_mask", ")", "\n", "\n", "", "self", ".", "_avg_new_entity_loss", "(", "float", "(", "new_entity_loss", ")", ")", "\n", "\n", "# Predict generation-mode scores. Note: these are W.R.T to entity_ids since we need the embedding.", "\n", "generate_scores", "=", "self", ".", "_generate_scores", "(", "encoded_token", ",", "entity_ids", ")", "\n", "\n", "# Predict copy-mode scores. Note: these are W.R.T raw_entity_ids since we need to look up aliases.", "\n", "alias_tokens", ",", "alias_inds", "=", "alias_database", ".", "lookup", "(", "raw_entity_ids", ")", "\n", "copy_scores", "=", "self", ".", "_copy_scores", "(", "encoded_token", ",", "alias_tokens", ")", "\n", "\n", "# Combine scores to get vocab loss", "\n", "vocab_loss", ",", "penalized_vocab_loss", "=", "self", ".", "_vocab_loss", "(", "generate_scores", ",", "\n", "copy_scores", ",", "\n", "target", ",", "\n", "alias_copy_inds", ",", "\n", "target_mask", ",", "\n", "alias_inds", ",", "\n", "entity_ids", ".", "gt", "(", "0", ")", ")", "\n", "self", ".", "_avg_vocab_loss", "(", "float", "(", "vocab_loss", ")", ")", "\n", "\n", "# Compute total loss. Also compute logp (needed for importance sampling evaluation).", "\n", "loss", "=", "vocab_loss", "+", "mention_type_loss", "+", "new_entity_loss", "\n", "logp", "=", "-", "(", "vocab_loss", "+", "mention_type_loss", "+", "new_entity_loss", ")", "*", "target_mask", ".", "sum", "(", ")", "\n", "penalized_logp", "=", "-", "(", "penalized_vocab_loss", "+", "mention_type_loss", "+", "new_entity_loss", ")", "*", "target_mask", ".", "sum", "(", ")", "\n", "\n", "# Activation regularization", "\n", "if", "self", ".", "_alpha", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "_alpha", "*", "alpha_loss", "\n", "# Temporal activation regularization (slowness)", "\n", "", "if", "self", ".", "_beta", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "_beta", "*", "beta_loss", "\n", "\n", "", "return", "{", "'loss'", ":", "loss", ",", "'logp'", ":", "logp", ",", "'penalized_logp'", ":", "penalized_logp", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory.train": [[581, 589], ["super().train"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.train"], ["", "@", "overrides", "\n", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "# TODO: This is a temporary hack to ensure that the internal state resets when the model", "\n", "# switches from training to evaluation. The complication arises from potentially differing", "\n", "# batch sizes (e.g. the `reset` tensor will not be the right size). In future", "\n", "# implementations this should be handled more robustly.", "\n", "        ", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory.eval": [[590, 595], ["super().eval"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval"], ["", "@", "overrides", "\n", "def", "eval", "(", "self", ")", ":", "\n", "# TODO: See train.", "\n", "        ", "super", "(", ")", ".", "eval", "(", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story.NoStory.get_metrics": [[596, 614], ["no_story.NoStory._new_mention_f1.get_metric", "no_story.NoStory._new_entity_accuracy.get_metric", "no_story.NoStory._new_entity_accuracy20.get_metric", "no_story.NoStory._ppl.get_metric", "no_story.NoStory._upp.get_metric", "no_story.NoStory._kg_ppl.get_metric", "no_story.NoStory._bg_ppl.get_metric", "no_story.NoStory._avg_mention_type_loss.get_metric", "no_story.NoStory._avg_new_entity_loss.get_metric", "no_story.NoStory._avg_vocab_loss.get_metric"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "out", "=", "{", "\n", "'ppl'", ":", "self", ".", "_ppl", ".", "get_metric", "(", "reset", ")", ",", "\n", "'upp'", ":", "self", ".", "_upp", ".", "get_metric", "(", "reset", ")", ",", "\n", "'kg_ppl'", ":", "self", ".", "_kg_ppl", ".", "get_metric", "(", "reset", ")", ",", "\n", "'bg_ppl'", ":", "self", ".", "_bg_ppl", ".", "get_metric", "(", "reset", ")", ",", "\n", "'type'", ":", "self", ".", "_avg_mention_type_loss", ".", "get_metric", "(", "reset", ")", ",", "\n", "'new'", ":", "self", ".", "_avg_new_entity_loss", ".", "get_metric", "(", "reset", ")", ",", "\n", "'vocab'", ":", "self", ".", "_avg_vocab_loss", ".", "get_metric", "(", "reset", ")", ",", "\n", "}", "\n", "# if not self.training:", "\n", "p", ",", "r", ",", "f", "=", "self", ".", "_new_mention_f1", ".", "get_metric", "(", "reset", ")", "\n", "out", "[", "'new_p'", "]", "=", "p", "\n", "out", "[", "'new_r'", "]", "=", "r", "\n", "out", "[", "'new_f1'", "]", "=", "f", "\n", "out", "[", "'new_ent_acc'", "]", "=", "self", ".", "_new_entity_accuracy", ".", "get_metric", "(", "reset", ")", "\n", "out", "[", "'new_ent_acc_20'", "]", "=", "self", ".", "_new_entity_accuracy20", ".", "get_metric", "(", "reset", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.__init__": [[36, 153], ["allennlp.nn.InitializerApplicator", "allennlp.models.Model.__init__", "kglm.modules.RecentEntities", "kglm.modules.KnowledgeGraphLookup", "kglm.modules.LockedDropout", "entity_embedder.get_output_dim", "token_embedder.get_output_dim", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "vocab.get_token_index", "math.log", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.F1Measure", "allennlp.training.metrics.F1Measure", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "initializer", "rnns.append", "kglm.modules.WeightDrop", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "vocab.get_vocab_size", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "vocab.get_vocab_size", "vocab.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["# pylint: disable=protected-access", "\n", "\n", "# Extract tokens and EOS offset", "\n", "        ", "tokens", "=", "[", "x", "+", "[", "'@@END@@'", "]", "for", "x", "in", "json_dict", "[", "'tokens'", "]", "[", "1", ":", "-", "1", "]", "]", "\n", "eos_offset", "=", "[", "[", "i", "]", "*", "len", "(", "x", ")", "for", "i", ",", "x", "in", "enumerate", "(", "tokens", ")", "]", "\n", "tokens", "=", "[", "'@@START@@'", "]", "+", "_flatten", "(", "tokens", ")", "\n", "eos_offset", "=", "[", "0", "]", "+", "_flatten", "(", "eos_offset", ")", "\n", "source", "=", "tokens", "[", ":", "-", "1", "]", "\n", "if", "self", ".", "_dataset_reader", ".", "_mode", "==", "'generative'", ":", "\n", "            ", "target", "=", "tokens", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "target", "=", "None", "\n", "\n", "# Process annotations", "\n", "", "if", "'annotations'", "not", "in", "json_dict", ":", "\n", "            ", "shortlist", "=", "None", "\n", "reverse_shortlist", "=", "None", "\n", "raw_entity_ids", "=", "None", "\n", "entity_ids", "=", "None", "\n", "relations", "=", "None", "\n", "parent_ids", "=", "None", "\n", "shortlist_inds", "=", "None", "\n", "mention_type", "=", "None", "\n", "", "else", ":", "\n", "# We maintain a \"shortlist\" of observed entities, that is used for baseline models", "\n", "# that only select entities from the set that appear in the document (as opposed to", "\n", "# the set of all possible entities).", "\n", "            ", "shortlist", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "\n", "reverse_shortlist", "=", "{", "DEFAULT_PADDING_TOKEN", ":", "0", "}", "\n", "raw_entity_ids", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "*", "len", "(", "source", ")", "\n", "entity_ids", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "*", "len", "(", "source", ")", "\n", "relations", "=", "[", "[", "DEFAULT_PADDING_TOKEN", "]", "]", "*", "len", "(", "source", ")", "\n", "parent_ids", "=", "[", "[", "DEFAULT_PADDING_TOKEN", "]", "]", "*", "len", "(", "source", ")", "\n", "shortlist_inds", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "source", ")", ",", ")", ")", "\n", "mention_type", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "source", ")", ",", ")", ")", "\n", "\n", "if", "self", ".", "_dataset_reader", ".", "_mode", "==", "\"generative\"", ":", "\n", "                ", "alias_copy_inds", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "source", ")", ",", ")", ")", "\n", "", "else", ":", "\n", "                ", "alias_copy_inds", "=", "None", "\n", "\n", "# Process annotations", "\n", "", "for", "annotation", "in", "json_dict", "[", "'annotations'", "]", ":", "\n", "\n", "# Obtain the entity identifier for the annotated span", "\n", "                ", "raw_entity_id", "=", "annotation", "[", "'id'", "]", "\n", "raw_parent_id", "=", "annotation", "[", "'parent_id'", "]", "\n", "entity_id", "=", "normalize_entity_id", "(", "raw_entity_id", ")", "\n", "if", "entity_id", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "parent_id", "=", "[", "normalize_entity_id", "(", "x", ")", "for", "x", "in", "raw_parent_id", "]", "\n", "assert", "len", "(", "parent_id", ")", "==", "len", "(", "raw_parent_id", ")", "\n", "relation", "=", "annotation", "[", "'relation'", "]", "\n", "new_entity", "=", "relation", "==", "[", "'@@NEW@@'", "]", "\n", "\n", "# If neccessary, update the shortlist. Obtain the index of the entity identifier in", "\n", "# the shortlist.", "\n", "if", "entity_id", "not", "in", "reverse_shortlist", ":", "\n", "                    ", "reverse_shortlist", "[", "entity_id", "]", "=", "len", "(", "reverse_shortlist", ")", "\n", "shortlist", ".", "append", "(", "entity_id", ")", "\n", "", "shortlist_ind", "=", "reverse_shortlist", "[", "entity_id", "]", "\n", "\n", "# Update the outputs", "\n", "# Offset is 0 in generative case, since each timestep is for predicting", "\n", "# attributes of the next token. In the discriminative case, each timestep", "\n", "# is for predicting attributes of the current token.", "\n", "mode_offset", "=", "-", "1", "if", "self", ".", "_dataset_reader", ".", "_mode", "==", "\"generative\"", "else", "0", "\n", "span", "=", "annotation", "[", "'span'", "]", "\n", "eos_offset_adjusted_span", "=", "tuple", "(", "i", "+", "eos_offset", "[", "i", "]", "for", "i", "in", "span", ")", "\n", "for", "i", "in", "range", "(", "*", "eos_offset_adjusted_span", ")", ":", "\n", "                    ", "raw_entity_ids", "[", "i", "+", "mode_offset", "]", "=", "raw_entity_id", "\n", "entity_ids", "[", "i", "+", "mode_offset", "]", "=", "entity_id", "\n", "mention_type", "[", "i", "+", "mode_offset", "]", "=", "3", "\n", "if", "new_entity", ":", "\n", "                        ", "shortlist_inds", "[", "i", "+", "mode_offset", "]", "=", "shortlist_ind", "\n", "", "else", ":", "\n", "                        ", "relations", "[", "i", "+", "mode_offset", "]", "=", "relation", "[", ":", "MAX_PARENTS", "]", "\n", "parent_ids", "[", "i", "+", "mode_offset", "]", "=", "parent_id", "[", ":", "MAX_PARENTS", "]", "\n", "", "if", "self", ".", "_dataset_reader", ".", "_mode", "==", "\"generative\"", ":", "\n", "                        ", "alias_copy_inds", "[", "i", "+", "mode_offset", "]", "=", "self", ".", "_dataset_reader", ".", "_alias_database", ".", "token_to_uid", "(", "raw_entity_id", ",", "tokens", "[", "i", "]", ")", "\n", "# Now put in proper mention type for first token", "\n", "", "", "start", "=", "annotation", "[", "'span'", "]", "[", "0", "]", "\n", "if", "new_entity", ":", "\n", "                    ", "mention_type", "[", "start", "+", "mode_offset", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "mention_type", "[", "start", "+", "mode_offset", "]", "=", "2", "\n", "\n", "", "", "", "instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "\n", "source", ",", "\n", "target", ",", "\n", "shortlist", ",", "\n", "reverse_shortlist", ",", "\n", "raw_entity_ids", ",", "\n", "entity_ids", ",", "\n", "relations", ",", "\n", "parent_ids", ",", "\n", "shortlist_inds", ",", "\n", "mention_type", ",", "\n", "alias_copy_inds", ")", "\n", "reset", "=", "SequentialArrayField", "(", "np", ".", "array", "(", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "instance", ".", "add_field", "(", "'reset'", ",", "reset", ")", "\n", "return", "instance", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.predict_mention_type": [[154, 160], ["kglm.Kglm._fc_mention_type", "torch.softmax", "torch.softmax", "kglm.nn.util.parallel_sample"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.parallel_sample"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.predict_new_entities": [[161, 194], ["kglm.Kglm._new_entity_logits", "allennlp.nn.util.get_text_field_mask", "torch.softmax", "torch.softmax", "kglm.nn.util.parallel_sample", "allennlp.nn.util.get_text_field_mask.new_zeros", "allennlp.nn.util.get_text_field_mask.new_zeros", "allennlp.nn.util.masked_softmax", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "allennlp.nn.util.get_text_field_mask.byte().any", "kglm.nn.util.parallel_sample", "shortlist[].gather", "allennlp.nn.util.get_text_field_mask.byte"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_logits", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.parallel_sample", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.parallel_sample"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.get_raw_entity_ids": [[195, 202], ["torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "kglm.nn.util.nested_enumerate", "entity_ids.tolist", "kglm.Kglm.vocab.get_token_from_index", "kglm.Kglm.vocab.get_token_index", "tuple"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.nested_enumerate"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.predict_knowledge_graph_entities": [[203, 283], ["torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "range", "new_entity_ids[].unsqueeze", "kglm.Kglm._recent_entities", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "encoded_head[].unsqueeze", "kglm.Kglm._entity_embedder", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "allennlp.nn.util.masked_softmax", "candidate_mask.any().squeeze", "candidate_mask.any().squeeze.any", "current_parent_ids[].squeeze", "encoded_relation[].unsqueeze", "kglm.Kglm._knowledge_graph_lookup", "zip", "current_relations[].squeeze", "current_raw_tail_ids[].squeeze", "current_tail_ids[].squeeze", "kglm.Kglm._recent_entities.insert", "current_mask.any", "kglm.Kglm.transpose", "kglm.nn.util.parallel_sample", "viable_candidate_ids.gather", "kglm.Kglm._relation_embedder", "torch.mv", "torch.mv", "torch.mv", "torch.mv", "torch.softmax", "torch.softmax", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "kglm.Kglm.vocab.get_token_from_index", "kglm.Kglm.vocab.get_token_index", "candidate_mask.any", "raw_tail_id.item"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.insert", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.parallel_sample"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.sample": [[284, 386], ["alias_database.tensorize", "kglm.Kglm._recent_entities.reset", "kglm.Kglm._encode_source", "encoded.split", "kglm.Kglm.predict_mention_type", "logger.debug", "kglm.Kglm.predict_new_entities", "logger.debug", "kglm.Kglm.get_raw_entity_ids", "logger.debug", "kglm.Kglm.predict_knowledge_graph_entities", "logger.debug", "reset.any", "range", "kglm.Kglm._generate_scores", "alias_database.lookup", "kglm.Kglm._copy_scores", "kglm.Kglm.vocab.get_vocab_size", "kglm.Kglm._generate_scores", "alias_database.lookup", "kglm.Kglm._copy_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "alias_tokens.gt().view", "allennlp.nn.util.masked_softmax", "allennlp.nn.util.masked_softmax.detach", "alias_indices.view", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "kglm.Kglm.eq", "kglm.Kglm.eq", "kglm.Kglm.eq", "kglm.Kglm.eq", "kglm.Kglm.eq", "kglm.Kglm.eq", "parent_ids.unsqueeze", "alias_tokens.gt"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.tensorize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._encode_source", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.predict_mention_type", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.predict_new_entities", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.get_raw_entity_ids", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.predict_knowledge_graph_entities", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._generate_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.lookup", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._copy_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._generate_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.lookup", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._copy_scores"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.forward": [[387, 439], ["alias_database.tensorize", "kglm.Kglm._recent_entities.reset", "reset.any", "range", "kglm.Kglm._forward_loop", "kglm.Kglm._greedy_decode", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.tensorize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._forward_loop", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._greedy_decode"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._forward_loop": [[440, 531], ["allennlp.nn.util.get_text_field_mask", "kglm.Kglm._encode_source", "encoded.split", "kglm.Kglm._mention_type_loss", "kglm.Kglm._avg_mention_type_loss", "logger.debug", "kglm.Kglm._avg_new_entity_loss", "logger.debug", "kglm.Kglm._knowledge_graph_entity_loss", "kglm.Kglm._avg_knowledge_graph_entity_loss", "logger.debug", "kglm.Kglm._generate_scores", "alias_database.lookup", "kglm.Kglm._copy_scores", "kglm.Kglm._vocab_loss", "float", "kglm.Kglm._new_entity_loss", "kglm.Kglm._new_entity_loss", "float", "float", "entity_ids.gt", "allennlp.nn.util.get_text_field_mask.sum", "allennlp.nn.util.get_text_field_mask.sum"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._encode_source", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._mention_type_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._knowledge_graph_entity_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._generate_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.lookup", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._copy_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._vocab_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_loss"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._greedy_decode": [[533, 637], ["allennlp.nn.util.get_text_field_mask", "kglm.Kglm._encode_source", "encoded.split", "kglm.Kglm._fc_mention_type", "torch.max", "torch.max", "torch.max", "torch.max", "mention_type.eq", "kglm.Kglm._new_entity_logits", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "kglm.nn.util.nested_enumerate", "mention_type.eq", "mention_type.eq.any", "kglm.Kglm.vocab.get_vocab_size", "kglm.Kglm._generate_scores", "alias_database.lookup", "kglm.Kglm._copy_scores", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "alias_tokens.gt().view", "allennlp.nn.util.masked_softmax", "word_probs.squeeze.squeeze.squeeze", "torch.zeros_like.squeeze().item", "torch.zeros_like.squeeze().item", "alias_indices.view.view.view", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.masked_softmax", "torch.max", "torch.max", "torch.max", "torch.max", "shortlist[].gather", "torch.softmax", "torch.softmax", "torch.max", "torch.max", "torch.max", "torch.max", "torch.zeros_like.tolist", "torch.zeros_like.tolist", "kglm.Kglm.vocab.get_token_from_index", "kglm.Kglm.vocab.get_token_index", "kglm.Kglm._recent_entities", "kglm.Kglm._entity_embedder", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "allennlp.nn.util.masked_softmax", "torch.max", "torch.max", "torch.max", "torch.max", "candidate_ids.gather", "kglm.Kglm._knowledge_graph_lookup", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "zip", "kglm.Kglm.transpose", "kglm.Kglm._relation_embedder", "torch.mv", "torch.mv", "torch.mv", "torch.mv", "torch.max", "torch.max", "torch.max", "torch.max", "kglm.Kglm.vocab.get_token_from_index", "kglm.Kglm.vocab.get_token_index", "alias_tokens.gt", "torch.zeros_like.squeeze", "torch.zeros_like.squeeze", "tuple", "raw_tail_id.item"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._encode_source", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_logits", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.nested_enumerate", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._generate_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.lookup", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._copy_scores"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.decode": [[638, 681], ["kglm.Kglm.vocab.get_vocab_size", "collections.defaultdict", "enumerate", "kglm.Kglm.vocab.get_token_from_index", "zip", "list", "list.sort", "zip", "generate_probs.tolist", "kglm.Kglm.vocab.get_token_from_index", "alias_indices.tolist", "copy_probs.tolist", "collections.defaultdict.items", "dict", "dict", "dict.items"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._encode_source": [[682, 721], ["kglm.modules.embedded_dropout", "kglm.Kglm._locked_dropout", "enumerate", "kglm.Kglm.pow().mean", "rnn", "output.contiguous.contiguous.contiguous", "tuple", "hidden_states.append", "kglm.Kglm._locked_dropout", "kglm.Kglm._locked_dropout", "kglm.Kglm.pow", "enumerate", "h.detach"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._mention_type_loss": [[722, 744], ["kglm.Kglm._fc_mention_type", "allennlp.nn.util.sequence_cross_entropy_with_logits", "kglm.Kglm._new_mention_f1", "kglm.Kglm._kg_mention_f1"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._new_entity_logits": [[745, 761], ["kglm.modules.embedded_dropout", "kglm.Kglm._locked_dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "kglm.Kglm._fc_new_entity", "kglm.modules.embedded_dropout.transpose"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._new_entity_loss": [[762, 793], ["kglm.Kglm._new_entity_logits", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.masked_log_softmax", "torch.log_softmax", "torch.log_softmax", "target_mask.float", "entity_ids.eq", "mentions.float", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather().squeeze.sum", "torch.gather().squeeze.sum", "target_mask.sum", "target_inds.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_logits"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._parent_log_probs": [[794, 846], ["kglm.Kglm._recent_entities", "kglm.modules.embedded_dropout", "kglm.Kglm._locked_dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "allennlp.nn.util.masked_log_softmax", "parent_ids.unsqueeze", "candidate_ids.view", "parent_ids.unsqueeze.eq", "torch.max", "torch.max", "torch.max", "torch.max", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "kglm.modules.embedded_dropout.transpose", "candidate_ids.view.eq", "allennlp.nn.util.masked_log_softmax.unsqueeze", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "mask.float"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._relation_log_probs": [[847, 881], ["kglm.Kglm._knowledge_graph_lookup", "kglm.Kglm._locked_dropout", "kglm.Kglm.new_empty().fill_", "zip", "kglm.Kglm._relation_embedder", "math.log", "torch.mv", "torch.mv", "torch.mv", "torch.mv", "torch.log_softmax", "torch.log_softmax", "tail_id.eq", "torch.log_softmax.masked_select", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "kglm.Kglm.new_empty", "tail_id.eq"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._knowledge_graph_entity_loss": [[882, 906], ["kglm.Kglm._parent_log_probs", "kglm.Kglm._relation_log_probs", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "kglm.Kglm._parent_ppl", "kglm.Kglm._relation_ppl", "parent_ids.eq().all", "mask.float", "mask.float().sum", "mask.float().sum", "[].sum", "[].sum", "torch.logsumexp.sum", "torch.logsumexp.sum", "target_mask.sum", "parent_ids.eq", "mask.float", "mask.float", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._parent_log_probs", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._relation_log_probs"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._generate_scores": [[907, 916], ["kglm.modules.embedded_dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "kglm.Kglm._fc_condense", "kglm.Kglm._fc_generate"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._copy_scores": [[917, 947], ["alias_tokens.view", "kglm.Kglm._token_embedder", "alias_tokens.view.gt", "kglm.Kglm._alias_encoder", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "kglm.Kglm._locked_dropout", "encoded.view.view.view", "projected.view.view.view", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "copy_scores.view().contiguous.view().contiguous.view().contiguous", "logger.debug", "copy_mask.sum", "encoded.view.view.new_zeros", "kglm.Kglm._fc_copy", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "copy_scores.view().contiguous.view().contiguous.view"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._vocab_logp": [[948, 966], ["alias_indices.view().gt", "alias_indices.view().gt.new_ones", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "allennlp.nn.util.masked_log_softmax", "alias_indices.view"], "methods", ["None"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._vocab_loss": [[967, 1056], ["target_tokens.view", "mask.view().byte", "kglm.Kglm._vocab_logp", "kglm.Kglm.view", "kglm.Kglm.view.gather", "target_tokens.eq().view", "target_alias_indices.view.view.gt().view", "alias_indices.view.view.view", "target_alias_indices.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "flattened_mask.squeeze.squeeze.squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "kglm.Kglm._ppl", "kglm.Kglm._upp", "kg_mask.any", "bg_mask.any", "target_alias_indices.view.view.gt", "flattened_mask.squeeze.squeeze.float", "kglm.Kglm._kg_ppl", "kglm.Kglm._bg_ppl", "mask.view", "target_tokens.eq", "target_alias_indices.view.view.gt", "alias_indices.view.view.eq", "torch.logsumexp.sum", "torch.logsumexp.sum", "mask.sum", "target_tokens.eq().view.squeeze", "true_unks.float", "penalized_log_probs.sum", "mask.sum", "target_tokens.eq().view.float", "combined_log_probs_source_vocab[].sum", "mask.float().sum", "penalized_log_probs_source_vocab[].sum", "mask.float().sum", "target_alias_indices.view.gt().view.squeeze", "mask.byte", "mask.byte", "combined_log_probs_source_vocab[].sum", "kg_mask.float().sum", "combined_log_probs_source_vocab[].sum", "bg_mask.float().sum", "generate_mask.float", "copy_mask.float", "mask.float", "mask.float", "kg_mask.float", "bg_mask.float"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm._vocab_logp"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.train": [[1057, 1065], ["super().train"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.train"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.eval": [[1066, 1071], ["super().eval"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm.Kglm.get_metrics": [[1072, 1097], ["kglm.Kglm._new_mention_f1.get_metric", "kglm.Kglm._kg_mention_f1.get_metric", "kglm.Kglm._new_entity_accuracy.get_metric", "kglm.Kglm._new_entity_accuracy20.get_metric", "kglm.Kglm._parent_ppl.get_metric", "kglm.Kglm._relation_ppl.get_metric", "kglm.Kglm._ppl.get_metric", "kglm.Kglm._upp.get_metric", "kglm.Kglm._kg_ppl.get_metric", "kglm.Kglm._bg_ppl.get_metric", "kglm.Kglm._avg_mention_type_loss.get_metric", "kglm.Kglm._avg_new_entity_loss.get_metric", "kglm.Kglm._avg_knowledge_graph_entity_loss.get_metric", "kglm.Kglm._avg_vocab_loss.get_metric"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric"], []], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet.__init__": [[38, 136], ["allennlp.nn.InitializerApplicator", "allennlp.models.Model.__init__", "kglm.modules.LockedDropout", "entity_embedder.get_output_dim", "entity_embedder.get_output_dim", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "vocab.get_token_index", "math.log", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "initializer", "rnns.append", "kglm.modules.WeightDrop", "vocab.get_vocab_size", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "vocab.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "token_embedder", ":", "TextFieldEmbedder", ",", "\n", "entity_embedder", ":", "TextFieldEmbedder", ",", "\n", "alias_encoder", ":", "Seq2SeqEncoder", ",", "\n", "hidden_size", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "dropout", ":", "float", "=", "0.4", ",", "\n", "dropouth", ":", "float", "=", "0.3", ",", "\n", "dropouti", ":", "float", "=", "0.65", ",", "\n", "dropoute", ":", "float", "=", "0.1", ",", "\n", "wdrop", ":", "float", "=", "0.5", ",", "\n", "alpha", ":", "float", "=", "2.0", ",", "\n", "beta", ":", "float", "=", "1.0", ",", "\n", "tie_weights", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ")", "->", "None", ":", "\n", "        ", "super", "(", "AliasCopynet", ",", "self", ")", ".", "__init__", "(", "vocab", ")", "\n", "\n", "# Model architecture - Note: we need to extract the `Embedding` layers from the", "\n", "# `TokenEmbedders` to apply dropout later on.", "\n", "# pylint: disable=protected-access", "\n", "self", ".", "_token_embedder", "=", "token_embedder", ".", "_token_embedders", "[", "'tokens'", "]", "\n", "# self._entity_embedder = entity_embedder._token_embedders['entity_ids']", "\n", "self", ".", "_alias_encoder", "=", "alias_encoder", "\n", "self", ".", "_hidden_size", "=", "hidden_size", "\n", "self", ".", "_num_layers", "=", "num_layers", "\n", "self", ".", "_tie_weights", "=", "tie_weights", "\n", "\n", "# Dropout", "\n", "self", ".", "_locked_dropout", "=", "LockedDropout", "(", ")", "\n", "self", ".", "_dropout", "=", "dropout", "\n", "self", ".", "_dropouth", "=", "dropouth", "\n", "self", ".", "_dropouti", "=", "dropouti", "\n", "self", ".", "_dropoute", "=", "dropoute", "\n", "self", ".", "_wdrop", "=", "wdrop", "\n", "\n", "# Regularization strength", "\n", "self", ".", "_alpha", "=", "alpha", "\n", "self", ".", "_beta", "=", "beta", "\n", "\n", "# RNN Encoders.", "\n", "entity_embedding_dim", "=", "entity_embedder", ".", "get_output_dim", "(", ")", "\n", "token_embedding_dim", "=", "entity_embedder", ".", "get_output_dim", "(", ")", "\n", "assert", "entity_embedding_dim", "==", "token_embedding_dim", "\n", "embedding_dim", "=", "token_embedding_dim", "\n", "\n", "rnns", ":", "List", "[", "torch", ".", "nn", ".", "Module", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "input_size", "=", "token_embedding_dim", "\n", "", "else", ":", "\n", "                ", "input_size", "=", "hidden_size", "\n", "", "if", "(", "i", "==", "num_layers", "-", "1", ")", "and", "tie_weights", ":", "\n", "                ", "output_size", "=", "token_embedding_dim", "\n", "", "else", ":", "\n", "                ", "output_size", "=", "hidden_size", "\n", "", "rnns", ".", "append", "(", "torch", ".", "nn", ".", "LSTM", "(", "input_size", ",", "output_size", ",", "batch_first", "=", "True", ")", ")", "\n", "", "rnns", "=", "[", "WeightDrop", "(", "rnn", ",", "[", "'weight_hh_l0'", "]", ",", "dropout", "=", "wdrop", ")", "for", "rnn", "in", "rnns", "]", "\n", "self", ".", "rnns", "=", "torch", ".", "nn", ".", "ModuleList", "(", "rnns", ")", "\n", "\n", "# Various linear transformations.", "\n", "# self._fc_mention = torch.nn.Linear(", "\n", "#     in_features=embedding_dim,", "\n", "#     out_features=2)", "\n", "\n", "# self._fc_entity = torch.nn.Linear(", "\n", "#     in_features=embedding_dim,", "\n", "#     out_features=embedding_dim)", "\n", "\n", "# self._fc_condense = torch.nn.Linear(", "\n", "#     in_features=2 * embedding_dim,", "\n", "#     out_features=embedding_dim)", "\n", "\n", "self", ".", "_fc_generate", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "vocab", ".", "get_vocab_size", "(", "'tokens'", ")", ")", "\n", "\n", "self", ".", "_fc_copy", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "embedding_dim", ")", "\n", "\n", "if", "tie_weights", ":", "\n", "            ", "self", ".", "_fc_generate", ".", "weight", "=", "self", ".", "_token_embedder", ".", "weight", "\n", "\n", "", "self", ".", "_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", "\n", "\n", "# Metrics", "\n", "# self._avg_mention_loss = Average()", "\n", "# self._avg_entity_loss = Average()", "\n", "# self._avg_vocab_loss = Average()", "\n", "self", ".", "_unk_index", "=", "vocab", ".", "get_token_index", "(", "DEFAULT_OOV_TOKEN", ")", "\n", "self", ".", "_unk_penalty", "=", "math", ".", "log", "(", "vocab", ".", "get_vocab_size", "(", "'tokens_unk'", ")", ")", "\n", "self", ".", "_ppl", "=", "Ppl", "(", ")", "\n", "self", ".", "_upp", "=", "Ppl", "(", ")", "\n", "self", ".", "_kg_ppl", "=", "Ppl", "(", ")", "# Knowledge-graph ppl", "\n", "self", ".", "_bg_ppl", "=", "Ppl", "(", ")", "# Background ppl", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet.forward": [[137, 176], ["alias_database.tensorize", "reset.any", "range", "alias_copynet.AliasCopynet._forward_loop", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.tensorize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._forward_loop"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "target", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "reset", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "entity_ids", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "shortlist", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "shortlist_inds", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "alias_copy_inds", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "**", "kwargs", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "# Tensorize the alias_database - this will only perform the operation once.", "\n", "        ", "alias_database", "=", "metadata", "[", "0", "]", "[", "'alias_database'", "]", "\n", "alias_database", ".", "tensorize", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n", "# Reset the model if needed", "\n", "if", "reset", ".", "any", "(", ")", "and", "(", "self", ".", "_state", "is", "not", "None", ")", ":", "\n", "            ", "for", "layer", "in", "range", "(", "self", ".", "_num_layers", ")", ":", "\n", "                ", "h", ",", "c", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "h", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "h", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "c", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "c", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "=", "(", "h", ",", "c", ")", "\n", "\n", "", "", "if", "entity_ids", "is", "not", "None", ":", "\n", "            ", "output_dict", "=", "self", ".", "_forward_loop", "(", "\n", "source", "=", "source", ",", "\n", "target", "=", "target", ",", "\n", "alias_database", "=", "alias_database", ",", "\n", "entity_ids", "=", "entity_ids", ",", "\n", "shortlist", "=", "shortlist", ",", "\n", "shortlist_inds", "=", "shortlist_inds", ",", "\n", "alias_copy_inds", "=", "alias_copy_inds", ")", "\n", "", "else", ":", "\n", "# TODO: Figure out what we want here - probably to do some king of inference on", "\n", "# entities / mention types.", "\n", "            ", "output_dict", "=", "{", "}", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet._mention_loss": [[177, 189], ["alias_copynet.AliasCopynet._fc_mention", "allennlp.nn.util.sequence_cross_entropy_with_logits"], "methods", ["None"], ["", "def", "_mention_loss", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "targets", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes the loss for predicting whether or not the the next token will be part of an\n        entity mention.\n        \"\"\"", "\n", "logits", "=", "self", ".", "_fc_mention", "(", "encoded", ")", "\n", "mention_loss", "=", "sequence_cross_entropy_with_logits", "(", "logits", ",", "targets", ",", "mask", ",", "\n", "average", "=", "'token'", ")", "\n", "return", "mention_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet._entity_loss": [[190, 217], ["alias_copynet.AliasCopynet._fc_entity", "alias_copynet.AliasCopynet._locked_dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "range", "shortlist_embeddings.transpose", "torch.cross_entropy", "torch.cross_entropy", "mask.float().sum", "shortlist_mask[].float", "mask.float"], "methods", ["None"], ["", "def", "_entity_loss", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "targets", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", "shortlist_embeddings", ":", "torch", ".", "Tensor", ",", "\n", "shortlist_mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "# Logits are computed using a bilinear form that measures the similarity between the", "\n", "# projected hidden state and the embeddings of entities in the shortlist", "\n", "        ", "projected", "=", "self", ".", "_fc_entity", "(", "encoded", ")", "\n", "projected", "=", "self", ".", "_locked_dropout", "(", "projected", ",", "self", ".", "_dropout", ")", "\n", "logits", "=", "torch", ".", "bmm", "(", "projected", ",", "shortlist_embeddings", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "# There are technically two masks that need to be accounted for: a class-wise mask which", "\n", "# specifies which logits to ignore in the class dimension, and a token-wise mask (e.g.", "\n", "# `mask`) which avoids measuring loss for predictions on non-mention tokens. In practice,", "\n", "# we only need the class-wise mask since the non-mention tokens cannot be associated with a", "\n", "# valid target.", "\n", "batch_size", "=", "encoded", ".", "shape", "[", "0", "]", "\n", "entity_loss", "=", "0.0", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "entity_loss", "+=", "F", ".", "cross_entropy", "(", "\n", "input", "=", "logits", "[", "i", "]", ",", "\n", "target", "=", "targets", "[", "i", "]", ",", "\n", "weight", "=", "shortlist_mask", "[", "i", "]", ".", "float", "(", ")", ",", "\n", "reduction", "=", "'sum'", ")", "\n", "", "entity_loss", "=", "entity_loss", "/", "(", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "return", "entity_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet._copy_scores": [[218, 247], ["alias_tokens.view", "alias_copynet.AliasCopynet._token_embedder", "alias_tokens.view.gt", "alias_copynet.AliasCopynet._alias_encoder", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "alias_copynet.AliasCopynet._locked_dropout", "encoded.view.view.view", "projected.view.view.view", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "copy_scores.view().contiguous.view().contiguous.view().contiguous", "copy_mask.sum", "encoded.view.view.new_zeros", "alias_copynet.AliasCopynet._fc_copy", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "copy_scores.view().contiguous.view().contiguous.view"], "methods", ["None"], ["", "def", "_copy_scores", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "alias_tokens", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "# Begin by flattening the tokens so that they fit the expected shape of a", "\n", "# ``Seq2SeqEncoder``.", "\n", "        ", "batch_size", ",", "sequence_length", ",", "num_aliases", ",", "alias_length", "=", "alias_tokens", ".", "shape", "\n", "flattened", "=", "alias_tokens", ".", "view", "(", "-", "1", ",", "alias_length", ")", "\n", "copy_mask", "=", "flattened", "!=", "0", "\n", "if", "copy_mask", ".", "sum", "(", ")", "==", "0", ":", "\n", "            ", "return", "encoded", ".", "new_zeros", "(", "(", "batch_size", ",", "sequence_length", ",", "num_aliases", "*", "alias_length", ")", ",", "\n", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "# Embed and encode the alias tokens.", "\n", "", "embedded", "=", "self", ".", "_token_embedder", "(", "flattened", ")", "\n", "mask", "=", "flattened", ".", "gt", "(", "0", ")", "\n", "encoded_aliases", "=", "self", ".", "_alias_encoder", "(", "embedded", ",", "mask", ")", "\n", "\n", "# Equation 8 in the CopyNet paper recommends applying the additional step.", "\n", "projected", "=", "torch", ".", "tanh", "(", "self", ".", "_fc_copy", "(", "encoded_aliases", ")", ")", "\n", "projected", "=", "self", ".", "_locked_dropout", "(", "projected", ",", "self", ".", "_dropout", ")", "\n", "\n", "# This part gets a little funky - we need to make sure that the first dimension in", "\n", "# `projected` and `hidden` is batch_size x sequence_length.", "\n", "encoded", "=", "encoded", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "1", ",", "-", "1", ")", "\n", "projected", "=", "projected", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ",", "num_aliases", "*", "alias_length", ")", "\n", "copy_scores", "=", "torch", ".", "bmm", "(", "encoded", ",", "projected", ")", ".", "squeeze", "(", ")", "\n", "copy_scores", "=", "copy_scores", ".", "view", "(", "batch_size", ",", "sequence_length", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "return", "copy_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet._vocab_loss": [[248, 335], ["target_tokens.view", "mask.view().byte", "alias_indices.view.view.view().gt", "mask.new_ones", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "allennlp.nn.util.masked_log_softmax", "allennlp.nn.util.masked_log_softmax.view", "allennlp.nn.util.masked_log_softmax.view.gather", "target_tokens.eq().view", "target_alias_indices.view.view.gt().view", "alias_indices.view.view.view", "target_alias_indices.view.view.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "alias_copynet.AliasCopynet._ppl", "alias_copynet.AliasCopynet._upp", "kg_mask.any", "bg_mask.any", "target_alias_indices.view.view.gt", "alias_copynet.AliasCopynet._kg_ppl", "alias_copynet.AliasCopynet._bg_ppl", "mask.view", "alias_indices.view.view.view", "target_tokens.eq", "target_alias_indices.view.view.gt", "alias_indices.view.view.eq", "torch.logsumexp.sum", "torch.logsumexp.sum", "mask.sum", "target_tokens.eq().view.float", "combined_log_probs_source_vocab[].sum", "mask.float().sum", "penalized_log_probs_source_vocab[].sum", "mask.float().sum", "mask.byte", "mask.byte", "combined_log_probs_source_vocab[].sum", "kg_mask.float().sum", "combined_log_probs_source_vocab[].sum", "bg_mask.float().sum", "generate_mask.float", "copy_mask.float", "mask.float", "mask.float", "kg_mask.float", "bg_mask.float"], "methods", ["None"], ["", "def", "_vocab_loss", "(", "self", ",", "\n", "generate_scores", ":", "torch", ".", "Tensor", ",", "\n", "copy_scores", ":", "torch", ".", "Tensor", ",", "\n", "target_tokens", ":", "torch", ".", "Tensor", ",", "\n", "target_alias_indices", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", "alias_indices", ":", "torch", ".", "Tensor", ",", "\n", "alias_tokens", ":", "torch", ".", "Tensor", ",", "\n", "mention_mask", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "batch_size", ",", "sequence_length", ",", "vocab_size", "=", "generate_scores", ".", "shape", "\n", "copy_sequence_length", "=", "copy_scores", ".", "shape", "[", "-", "1", "]", "\n", "\n", "# Flat sequences make life **much** easier.", "\n", "flattened_targets", "=", "target_tokens", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "1", ")", "\n", "flattened_mask", "=", "mask", ".", "view", "(", "-", "1", ",", "1", ")", ".", "byte", "(", ")", "\n", "\n", "# In order to obtain proper log probabilities we create a mask to omit padding alias tokens", "\n", "# from the calculation.", "\n", "alias_mask", "=", "alias_indices", ".", "view", "(", "batch_size", ",", "sequence_length", ",", "-", "1", ")", ".", "gt", "(", "0", ")", "\n", "score_mask", "=", "mask", ".", "new_ones", "(", "batch_size", ",", "sequence_length", ",", "vocab_size", "+", "copy_sequence_length", ")", "\n", "score_mask", "[", ":", ",", ":", ",", "vocab_size", ":", "]", "=", "alias_mask", "\n", "\n", "# The log-probability distribution is then given by taking the masked log softmax.", "\n", "concatenated_scores", "=", "torch", ".", "cat", "(", "(", "generate_scores", ",", "copy_scores", ")", ",", "dim", "=", "-", "1", ")", "\n", "log_probs", "=", "masked_log_softmax", "(", "concatenated_scores", ",", "score_mask", ")", "\n", "\n", "# GENERATE LOSS ###", "\n", "# The generated token loss is a simple cross-entropy calculation, we can just gather", "\n", "# the log probabilties...", "\n", "flattened_log_probs", "=", "log_probs", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ")", "\n", "generate_log_probs_source_vocab", "=", "flattened_log_probs", ".", "gather", "(", "1", ",", "flattened_targets", ")", "\n", "# ...except we need to ignore the contribution of UNK tokens that are copied (only when", "\n", "# computing the loss). To do that we create a mask which is 1 only if the token is not a", "\n", "# copied UNK (or padding).", "\n", "unks", "=", "target_tokens", ".", "eq", "(", "self", ".", "_unk_index", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "copied", "=", "target_alias_indices", ".", "gt", "(", "0", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "generate_mask", "=", "~", "(", "unks", "&", "copied", ")", "&", "flattened_mask", "\n", "# Since we are in log-space we apply the mask by addition.", "\n", "generate_log_probs_extended_vocab", "=", "generate_log_probs_source_vocab", "+", "(", "generate_mask", ".", "float", "(", ")", "+", "1e-45", ")", ".", "log", "(", ")", "\n", "\n", "# COPY LOSS ###", "\n", "copy_log_probs", "=", "flattened_log_probs", "[", ":", ",", "vocab_size", ":", "]", "\n", "# When computing the loss we need to get the log probability of **only** the copied tokens.", "\n", "alias_indices", "=", "alias_indices", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ")", "\n", "target_alias_indices", "=", "target_alias_indices", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "copy_mask", "=", "alias_indices", ".", "eq", "(", "target_alias_indices", ")", "&", "flattened_mask", "&", "target_alias_indices", ".", "gt", "(", "0", ")", "\n", "copy_log_probs", "=", "copy_log_probs", "+", "(", "copy_mask", ".", "float", "(", ")", "+", "1e-45", ")", ".", "log", "(", ")", "\n", "\n", "# COMBINED LOSS ###", "\n", "# The final loss term is computed using our log probs computed w.r.t to the entire", "\n", "# vocabulary.", "\n", "combined_log_probs_extended_vocab", "=", "torch", ".", "cat", "(", "(", "generate_log_probs_extended_vocab", ",", "\n", "copy_log_probs", ")", ",", "\n", "dim", "=", "1", ")", "\n", "combined_log_probs_extended_vocab", "=", "torch", ".", "logsumexp", "(", "combined_log_probs_extended_vocab", ",", "\n", "dim", "=", "1", ")", "\n", "vocab_loss", "=", "-", "combined_log_probs_extended_vocab", ".", "sum", "(", ")", "/", "(", "mask", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n", "# PERPLEXITY ###", "\n", "# Our perplexity terms are computed using the log probs computed w.r.t the source", "\n", "# vocabulary.", "\n", "combined_log_probs_source_vocab", "=", "torch", ".", "cat", "(", "(", "generate_log_probs_source_vocab", ",", "\n", "copy_log_probs", ")", ",", "\n", "dim", "=", "1", ")", "\n", "combined_log_probs_source_vocab", "=", "torch", ".", "logsumexp", "(", "combined_log_probs_source_vocab", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "# For UPP we penalize **only** p(UNK); not the copy probabilities!", "\n", "penalized_log_probs_source_vocab", "=", "generate_log_probs_source_vocab", "-", "self", ".", "_unk_penalty", "*", "unks", ".", "float", "(", ")", "\n", "penalized_log_probs_source_vocab", "=", "torch", ".", "cat", "(", "(", "penalized_log_probs_source_vocab", ",", "\n", "copy_log_probs", ")", ",", "\n", "dim", "=", "1", ")", "\n", "penalized_log_probs_source_vocab", "=", "torch", ".", "logsumexp", "(", "penalized_log_probs_source_vocab", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "kg_mask", "=", "(", "mention_mask", "*", "mask", ".", "byte", "(", ")", ")", ".", "view", "(", "-", "1", ")", "\n", "bg_mask", "=", "(", "(", "1", "-", "mention_mask", ")", "*", "mask", ".", "byte", "(", ")", ")", ".", "view", "(", "-", "1", ")", "\n", "mask", "=", "(", "kg_mask", "|", "bg_mask", ")", "\n", "\n", "self", ".", "_ppl", "(", "-", "combined_log_probs_source_vocab", "[", "mask", "]", ".", "sum", "(", ")", ",", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "self", ".", "_upp", "(", "-", "penalized_log_probs_source_vocab", "[", "mask", "]", ".", "sum", "(", ")", ",", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "if", "kg_mask", ".", "any", "(", ")", ":", "\n", "            ", "self", ".", "_kg_ppl", "(", "-", "combined_log_probs_source_vocab", "[", "kg_mask", "]", ".", "sum", "(", ")", ",", "kg_mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "", "if", "bg_mask", ".", "any", "(", ")", ":", "\n", "            ", "self", ".", "_bg_ppl", "(", "-", "combined_log_probs_source_vocab", "[", "bg_mask", "]", ".", "sum", "(", ")", ",", "bg_mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n", "", "return", "vocab_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet._forward_loop": [[336, 440], ["allennlp.nn.util.get_text_field_mask", "kglm.modules.embedded_dropout", "alias_copynet.AliasCopynet._locked_dropout", "enumerate", "alias_copynet.AliasCopynet._fc_generate", "alias_database.lookup", "alias_copynet.AliasCopynet._copy_scores", "alias_copynet.AliasCopynet._vocab_loss", "rnn", "output.contiguous.contiguous.contiguous", "tuple", "hidden_states.append", "entity_ids.gt", "alias_copynet.AliasCopynet._locked_dropout", "alias_copynet.AliasCopynet._locked_dropout", "enumerate", "h.detach", "alias_copynet.AliasCopynet.pow().mean", "alias_copynet.AliasCopynet.pow"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.lookup", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._copy_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._vocab_loss"], ["", "def", "_forward_loop", "(", "self", ",", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "target", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "alias_database", ":", "AliasDatabase", ",", "\n", "entity_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "shortlist", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "shortlist_inds", ":", "torch", ".", "Tensor", ",", "\n", "alias_copy_inds", ":", "torch", ".", "Tensor", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "# Get the token mask and unwrap the target tokens.", "\n", "        ", "target_mask", "=", "get_text_field_mask", "(", "target", ")", "\n", "target", "=", "target", "[", "'tokens'", "]", "\n", "\n", "# Embed source tokens.", "\n", "source", "=", "source", "[", "'tokens'", "]", "\n", "source_embeddings", "=", "embedded_dropout", "(", "\n", "embed", "=", "self", ".", "_token_embedder", ",", "\n", "words", "=", "source", ",", "\n", "dropout", "=", "self", ".", "_dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "source_embeddings", "=", "self", ".", "_locked_dropout", "(", "source_embeddings", ",", "self", ".", "_dropouti", ")", "\n", "\n", "# Embed entities.", "\n", "entity_ids", "=", "entity_ids", "[", "'entity_ids'", "]", "\n", "# entity_embeddings = embedded_dropout(", "\n", "#     embed=self._entity_embedder,", "\n", "#     words=entity_ids,", "\n", "#     dropout=self._dropoute if self.training else 0)", "\n", "# entity_embeddings = self._locked_dropout(entity_embeddings, self._dropouti)", "\n", "\n", "# # Embed shortlist.", "\n", "# shortlist_mask = get_text_field_mask(shortlist)", "\n", "# shortlist = shortlist['entity_ids']", "\n", "# shortlist_embeddings = embedded_dropout(", "\n", "#     embed=self._entity_embedder,", "\n", "#     words=shortlist,", "\n", "#     dropout=self._dropoute if self.training else 0)", "\n", "\n", "# Encode source tokens.", "\n", "current_input", "=", "source_embeddings", "\n", "hidden_states", "=", "[", "]", "\n", "for", "layer", ",", "rnn", "in", "enumerate", "(", "self", ".", "rnns", ")", ":", "\n", "# Retrieve previous hidden state for layer.", "\n", "            ", "if", "self", ".", "_state", "is", "not", "None", ":", "\n", "                ", "prev_hidden", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "", "else", ":", "\n", "                ", "prev_hidden", "=", "None", "\n", "# Forward-pass.", "\n", "", "output", ",", "hidden", "=", "rnn", "(", "current_input", ",", "prev_hidden", ")", "\n", "output", "=", "output", ".", "contiguous", "(", ")", "\n", "# Update hidden state for layer.", "\n", "hidden", "=", "tuple", "(", "h", ".", "detach", "(", ")", "for", "h", "in", "hidden", ")", "\n", "hidden_states", ".", "append", "(", "hidden", ")", "\n", "# Apply dropout.", "\n", "if", "layer", "==", "self", ".", "_num_layers", "-", "1", ":", "\n", "                ", "dropped_output", "=", "self", ".", "_locked_dropout", "(", "output", ",", "self", ".", "_dropout", ")", "\n", "", "else", ":", "\n", "                ", "dropped_output", "=", "self", ".", "_locked_dropout", "(", "output", ",", "self", ".", "_dropouth", ")", "\n", "", "current_input", "=", "dropped_output", "\n", "", "encoded", "=", "current_input", "\n", "self", ".", "_state", "=", "{", "'layer_%i'", "%", "i", ":", "h", "for", "i", ",", "h", "in", "enumerate", "(", "hidden_states", ")", "}", "\n", "\n", "# Predict whether or not the next token will be an entity mention. This corresponds to the", "\n", "# case that the entity's id is not a padding token.", "\n", "# mention_loss = self._mention_loss(encoded, entity_ids.gt(0), target_mask)", "\n", "\n", "# Predict which entity (among those in the supplied shortlist) is going to be", "\n", "# mentioned.", "\n", "# entity_loss = self._entity_loss(encoded,", "\n", "#                                 shortlist_inds,", "\n", "#                                 target_mask,", "\n", "#                                 shortlist_embeddings,", "\n", "#                                 shortlist_mask)", "\n", "\n", "# Predict generation-mode scores. Start by concatenating predicted entity embeddings with", "\n", "# the encoder output - then feed through a linear layer.", "\n", "# concatenated = torch.cat((encoded, entity_embeddings), dim=-1)", "\n", "# condensed = self._fc_condense(concatenated)", "\n", "generate_scores", "=", "self", ".", "_fc_generate", "(", "encoded", ")", "\n", "\n", "# Predict copy-mode scores.", "\n", "alias_tokens", ",", "alias_inds", "=", "alias_database", ".", "lookup", "(", "entity_ids", ")", "\n", "copy_scores", "=", "self", ".", "_copy_scores", "(", "encoded", ",", "alias_tokens", ")", "\n", "\n", "# Combine scores to get vocab loss", "\n", "vocab_loss", "=", "self", ".", "_vocab_loss", "(", "generate_scores", ",", "\n", "copy_scores", ",", "\n", "target", ",", "\n", "alias_copy_inds", ",", "\n", "target_mask", ",", "\n", "alias_inds", ",", "\n", "alias_tokens", ",", "\n", "entity_ids", ".", "gt", "(", "0", ")", ")", "\n", "\n", "# Compute total loss", "\n", "loss", "=", "vocab_loss", "# + mention_loss + entity_loss", "\n", "\n", "# Activation regularization", "\n", "if", "self", ".", "_alpha", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "_alpha", "*", "dropped_output", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "# Temporal activation regularization (slowness)", "\n", "", "if", "self", ".", "_beta", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "_beta", "*", "(", "output", "[", ":", ",", "1", ":", "]", "-", "output", "[", ":", ",", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "", "return", "{", "'loss'", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet.train": [[441, 449], ["super().train"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.train"], ["", "@", "overrides", "\n", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "# TODO: This is a temporary hack to ensure that the internal state resets when the model", "\n", "# switches from training to evaluation. The complication arises from potentially differing", "\n", "# batch sizes (e.g. the `reset` tensor will not be the right size). In future", "\n", "# implementations this should be handled more robustly.", "\n", "        ", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet.eval": [[450, 455], ["super().eval"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval"], ["", "@", "overrides", "\n", "def", "eval", "(", "self", ")", ":", "\n", "# TODO: See train.", "\n", "        ", "super", "(", ")", ".", "eval", "(", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.alias_copynet.AliasCopynet.get_metrics": [[456, 462], ["alias_copynet.AliasCopynet._ppl.get_metric", "alias_copynet.AliasCopynet._upp.get_metric", "alias_copynet.AliasCopynet._kg_ppl.get_metric", "alias_copynet.AliasCopynet._bg_ppl.get_metric"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "{", "\n", "'ppl'", ":", "self", ".", "_ppl", ".", "get_metric", "(", "reset", ")", ",", "\n", "'upp'", ":", "self", ".", "_upp", ".", "get_metric", "(", "reset", ")", ",", "\n", "'kg_ppl'", ":", "self", ".", "_kg_ppl", ".", "get_metric", "(", "reset", ")", ",", "\n", "'bg_ppl'", ":", "self", ".", "_bg_ppl", ".", "get_metric", "(", "reset", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_nlm.EntityNLM.__init__": [[60, 122], ["allennlp.nn.InitializerApplicator", "allennlp.models.Model.__init__", "allennlp.modules.input_variational_dropout.InputVariationalDropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "kglm.modules.DynamicEmbedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "initializer", "torch.normalize", "torch.normalize", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "vocab.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "max_mention_length", ":", "int", ",", "\n", "max_embeddings", ":", "int", ",", "\n", "tie_weights", ":", "bool", ",", "\n", "variational_dropout_rate", ":", "float", "=", "0.0", ",", "\n", "dropout_rate", ":", "float", "=", "0.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ")", "->", "None", ":", "\n", "        ", "super", "(", "EntityNLM", ",", "self", ")", ".", "__init__", "(", "vocab", ")", "\n", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "self", ".", "_embedding_dim", "=", "embedding_dim", "\n", "self", ".", "_max_mention_length", "=", "max_mention_length", "\n", "self", ".", "_max_embeddings", "=", "max_embeddings", "\n", "self", ".", "_tie_weights", "=", "tie_weights", "\n", "self", ".", "_variational_dropout_rate", "=", "variational_dropout_rate", "\n", "self", ".", "_dropout_rate", "=", "dropout_rate", "\n", "\n", "self", ".", "_state", ":", "Optional", "[", "StateDict", "]", "=", "None", "\n", "\n", "# Input variational dropout", "\n", "self", ".", "_variational_dropout", "=", "InputVariationalDropout", "(", "variational_dropout_rate", ")", "\n", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n", "# For entity type prediction", "\n", "self", ".", "_entity_type_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "2", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "_dynamic_embeddings", "=", "DynamicEmbedding", "(", "embedding_dim", "=", "embedding_dim", ",", "\n", "max_embeddings", "=", "max_embeddings", ")", "\n", "\n", "# For mention length prediction", "\n", "self", ".", "_mention_length_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "2", "*", "embedding_dim", ",", "\n", "out_features", "=", "max_mention_length", ")", "\n", "\n", "# For next word prediction", "\n", "self", ".", "_dummy_context_embedding", "=", "Parameter", "(", "F", ".", "normalize", "(", "torch", ".", "randn", "(", "1", ",", "embedding_dim", ")", ")", ")", "# TODO: Maybe squeeze", "\n", "self", ".", "_entity_output_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "embedding_dim", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "_context_output_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "embedding_dim", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "_vocab_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "vocab", ".", "get_vocab_size", "(", "'tokens'", ")", ")", "\n", "if", "tie_weights", ":", "\n", "            ", "self", ".", "_vocab_projection", ".", "weight", "=", "self", ".", "_text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "weight", "# pylint: disable=W0212", "\n", "\n", "# self._perplexity = Perplexity()", "\n", "# self._unknown_penalized_perplexity = UnknownPenalizedPerplexity(self.vocab)", "\n", "", "self", ".", "_entity_type_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "self", ".", "_entity_id_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "self", ".", "_mention_length_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "\n", "if", "tie_weights", ":", "\n", "            ", "self", ".", "_vocab_projection", ".", "weight", "=", "self", ".", "_text_field_embedder", ".", "_token_embedders", "[", "'tokens'", "]", ".", "weight", "# pylint: disable=W0212", "\n", "\n", "", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_nlm.EntityNLM.forward": [[123, 177], ["entity_nlm.EntityNLM.reset_states", "entity_nlm.EntityNLM.detach_states", "entity_nlm.EntityNLM._forward_loop"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.detach_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._forward_loop"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "tokens", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "entity_types", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "entity_ids", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "mention_lengths", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Computes the loss during training / validation.\n\n        Parameters\n        ----------\n        tokens : ``Dict[str, torch.Tensor]``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the sequence of\n            tokens.\n        entity_types : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` indicating whether or not the\n            corresponding token belongs to a mention.\n        entity_ids : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the ids of the\n            entities the corresponding token is mentioning.\n        mention_lengths : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` tracking how many remaining\n            tokens (including the current one) there are in the mention.\n        reset : ``bool``\n            Whether or not to reset the model's state. This should be done at the start of each\n            new sequence.\n\n        Returns\n        -------\n        An output dictionary consisting of:\n        loss : ``torch.Tensor``\n            The combined loss.\n        \"\"\"", "\n", "batch_size", "=", "tokens", "[", "'tokens'", "]", ".", "shape", "[", "0", "]", "\n", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset_states", "(", "batch_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "detach_states", "(", ")", "\n", "\n", "", "if", "entity_types", "is", "not", "None", ":", "\n", "            ", "output_dict", "=", "self", ".", "_forward_loop", "(", "tokens", "=", "tokens", ",", "\n", "entity_types", "=", "entity_types", ",", "\n", "entity_ids", "=", "entity_ids", ",", "\n", "mention_lengths", "=", "mention_lengths", ")", "\n", "", "else", ":", "\n", "            ", "output_dict", "=", "{", "}", "\n", "\n", "", "if", "not", "self", ".", "training", ":", "\n", "# TODO Some evaluation stuff", "\n", "            ", "pass", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_nlm.EntityNLM._forward_loop": [[178, 395], ["allennlp.nn.util.get_text_field_mask", "entity_nlm.EntityNLM._text_field_embedder", "entity_nlm.EntityNLM._variational_dropout", "entity_nlm.EntityNLM._encoder", "entity_nlm.EntityNLM.new_zeros", "range", "allennlp.nn.util.get_text_field_mask.sum", "allennlp.nn.util.get_text_field_mask.sum", "allennlp.nn.util.get_text_field_mask.sum", "allennlp.nn.util.get_text_field_mask.sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "entity_nlm.EntityNLM._dummy_context_embedding.repeat", "entity_nlm.EntityNLM._dropout", "entity_nlm.EntityNLM._dynamic_embeddings.add_embeddings", "entity_nlm.EntityNLM._dynamic_embeddings.update_embeddings", "next_entity_ids.clone.clone.clone", "entity_nlm.EntityNLM._entity_output_projection", "entity_nlm.EntityNLM._context_output_projection", "entity_nlm.EntityNLM.clone", "entity_nlm.EntityNLM._vocab_projection", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy.sum", "entity_types[].unsqueeze().detach", "entity_ids[].unsqueeze().detach", "mention_lengths[].unsqueeze().detach", "entity_nlm.EntityNLM.detach", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "next_mask.byte", "predict_all.sum", "entity_nlm.EntityNLM._entity_type_projection", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy.sum", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "entity_nlm.EntityNLM._entity_type_accuracy", "next_entity_types.sum", "next_mask.float", "[].unsqueeze().detach", "next_entity_types[].long", "predict_em.sum", "entity_nlm.EntityNLM._dynamic_embeddings", "_entity_id_loss.sum", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "entity_nlm.EntityNLM._entity_id_accuracy", "entity_nlm.EntityNLM._dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "entity_nlm.EntityNLM._mention_length_projection", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy.sum", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "entity_nlm.EntityNLM._mention_length_accuracy", "next_entity_types.sum", "entity_types[].unsqueeze", "entity_ids[].unsqueeze", "mention_lengths[].unsqueeze", "next_entity_types[].long", "[].unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.add_embeddings", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.update_embeddings"], ["", "def", "_forward_loop", "(", "self", ",", "\n", "tokens", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "entity_types", ":", "torch", ".", "Tensor", ",", "\n", "entity_ids", ":", "torch", ".", "Tensor", ",", "\n", "mention_lengths", ":", "torch", ".", "Tensor", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Performs the forward pass to calculate the loss on a chunk of training data.\n\n        Parameters\n        ----------\n        tokens : ``Dict[str, torch.Tensor]``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the sequence of\n            tokens.\n        entity_types : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` indicating whether or not the\n            corresponding token belongs to a mention.\n        entity_ids : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the ids of the\n            entities the corresponding token is mentioning.\n        mention_lengths : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` tracking how many remaining\n            tokens (including the current one) there are in the mention.\n\n        Returns\n        -------\n        An output dictionary consisting of:\n        entity_type_loss : ``torch.Tensor``\n            The loss of entity type predictions.\n        entity_id_loss : ``torch.Tensor``\n            The loss of entity id predictions.\n        mention_length_loss : ``torch.Tensor``\n            The loss of mention length predictions.\n        vocab_loss : ``torch.Tensor``\n            The loss of vocab word predictions.\n        loss : ``torch.Tensor``\n            The combined loss.\n        logp : ``torch.Tensor``\n            Instance level log-probabilities\n        \"\"\"", "\n", "batch_size", ",", "sequence_length", "=", "tokens", "[", "'tokens'", "]", ".", "shape", "\n", "\n", "# The model state allows us to recover the last timestep from the previous chunk in the", "\n", "# split. If it does not exist, then we are processing a new batch.", "\n", "if", "self", ".", "_state", "is", "not", "None", ":", "\n", "            ", "tokens", "=", "{", "field", ":", "torch", ".", "cat", "(", "(", "self", ".", "_state", "[", "'prev_tokens'", "]", "[", "field", "]", ",", "tokens", "[", "field", "]", ")", ",", "dim", "=", "1", ")", "\n", "for", "field", "in", "tokens", "}", "\n", "entity_types", "=", "torch", ".", "cat", "(", "(", "self", ".", "_state", "[", "'prev_entity_types'", "]", ",", "entity_types", ")", ",", "dim", "=", "1", ")", "\n", "entity_ids", "=", "torch", ".", "cat", "(", "(", "self", ".", "_state", "[", "'prev_entity_ids'", "]", ",", "entity_ids", ")", ",", "dim", "=", "1", ")", "\n", "mention_lengths", "=", "torch", ".", "cat", "(", "(", "self", ".", "_state", "[", "'prev_mention_lengths'", "]", ",", "mention_lengths", ")", ",", "dim", "=", "1", ")", "\n", "contexts", "=", "self", ".", "_state", "[", "'prev_contexts'", "]", "\n", "sequence_length", "+=", "1", "\n", "", "else", ":", "\n", "            ", "contexts", "=", "self", ".", "_dummy_context_embedding", ".", "repeat", "(", "batch_size", ",", "1", ")", "\n", "\n", "# Embed tokens and get RNN hidden state.", "\n", "", "mask", "=", "get_text_field_mask", "(", "tokens", ")", "\n", "embeddings", "=", "self", ".", "_text_field_embedder", "(", "tokens", ")", "\n", "embeddings", "=", "self", ".", "_variational_dropout", "(", "embeddings", ")", "\n", "hidden", "=", "self", ".", "_encoder", "(", "embeddings", ",", "mask", ")", "\n", "\n", "# Initialize losses", "\n", "entity_type_loss", "=", "0.0", "\n", "entity_id_loss", "=", "0.0", "\n", "mention_length_loss", "=", "0.0", "\n", "vocab_loss", "=", "0.0", "\n", "logp", "=", "hidden", ".", "new_zeros", "(", "batch_size", ")", "\n", "\n", "# We dynamically add entities and update their representations in sequence. The following", "\n", "# loop is designed to imitate as closely as possible lines 219-313 in:", "\n", "#   https://github.com/jiyfeng/entitynlm/blob/master/entitynlm.h", "\n", "# while still being carried out in batch.", "\n", "for", "timestep", "in", "range", "(", "sequence_length", "-", "1", ")", ":", "\n", "\n", "            ", "current_entity_types", "=", "entity_types", "[", ":", ",", "timestep", "]", "\n", "current_entity_ids", "=", "entity_ids", "[", ":", ",", "timestep", "]", "\n", "current_mention_lengths", "=", "mention_lengths", "[", ":", ",", "timestep", "]", "\n", "current_hidden", "=", "self", ".", "_dropout", "(", "hidden", "[", ":", ",", "timestep", "]", ")", "\n", "\n", "next_entity_types", "=", "entity_types", "[", ":", ",", "timestep", "+", "1", "]", "\n", "next_entity_ids", "=", "entity_ids", "[", ":", ",", "timestep", "+", "1", "]", "\n", "next_mention_lengths", "=", "mention_lengths", "[", ":", ",", "timestep", "+", "1", "]", "\n", "next_mask", "=", "mask", "[", ":", ",", "timestep", "+", "1", "]", "\n", "next_tokens", "=", "tokens", "[", "'tokens'", "]", "[", ":", ",", "timestep", "+", "1", "]", "\n", "\n", "# We add new entities to any sequence where the current entity id matches the number of", "\n", "# embeddings that currently exist for that sequence (this means we need a new one since", "\n", "# there is an additional dummy embedding).", "\n", "new_entities", "=", "current_entity_ids", "==", "self", ".", "_dynamic_embeddings", ".", "num_embeddings", "\n", "self", ".", "_dynamic_embeddings", ".", "add_embeddings", "(", "timestep", ",", "new_entities", ")", "\n", "\n", "# We also perform updates of the currently observed entities.", "\n", "self", ".", "_dynamic_embeddings", ".", "update_embeddings", "(", "hidden", "=", "current_hidden", ",", "\n", "update_indices", "=", "current_entity_ids", ",", "\n", "timestep", "=", "timestep", ",", "\n", "mask", "=", "current_entity_types", ")", "\n", "\n", "# This part is a little counter-intuitive. Because the above code adds a new embedding", "\n", "# whenever the **current** entity id matches the number of embeddings, we are one", "\n", "# embedding short if the **next** entity id has not been seen before. To deal with", "\n", "# this, we use the null embedding (e.g. the first one we created) as a proxy for the", "\n", "# new entity's embedding (since it is on average what the new entity's embedding will", "\n", "# be initialized in the next timestep). It might seem more sensible to just create the", "\n", "# embedding now, but we cannot because of the subsequent update (since this would", "\n", "# require access to the **next** hidden state, which does not exist during generation).", "\n", "next_entity_ids", "=", "next_entity_ids", ".", "clone", "(", ")", "# This prevents mutating the source data.", "\n", "next_entity_ids", "[", "next_entity_ids", "==", "self", ".", "_dynamic_embeddings", ".", "num_embeddings", "]", "=", "0", "\n", "\n", "# We only predict the types / ids / lengths of the next mention if we are not currently", "\n", "# in the process of generating it (e.g. if the current remaining mention length is 1).", "\n", "# Indexing / masking with ``predict_all`` makes it possible to do this in batch.", "\n", "predict_all", "=", "(", "current_mention_lengths", "==", "1", ")", "*", "next_mask", ".", "byte", "(", ")", "\n", "if", "predict_all", ".", "sum", "(", ")", ">", "0", ":", "\n", "\n", "# Equation 3 in the paper.", "\n", "                ", "entity_type_logits", "=", "self", ".", "_entity_type_projection", "(", "current_hidden", "[", "predict_all", "]", ")", "\n", "_entity_type_loss", "=", "F", ".", "cross_entropy", "(", "entity_type_logits", ",", "\n", "next_entity_types", "[", "predict_all", "]", ".", "long", "(", ")", ",", "\n", "reduction", "=", "'none'", ")", "\n", "entity_type_loss", "+=", "_entity_type_loss", ".", "sum", "(", ")", "\n", "\n", "entity_type_logp", "=", "torch", ".", "zeros_like", "(", "next_entity_types", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "entity_type_logp", "[", "predict_all", "]", "=", "-", "_entity_type_loss", "\n", "logp", "+=", "entity_type_logp", "\n", "\n", "self", ".", "_entity_type_accuracy", "(", "predictions", "=", "entity_type_logits", ",", "\n", "gold_labels", "=", "next_entity_types", "[", "predict_all", "]", ".", "long", "(", ")", ")", "\n", "\n", "# Only proceed to predict entity and mention length if there is in fact an entity.", "\n", "predict_em", "=", "next_entity_types", "*", "predict_all", "\n", "if", "predict_em", ".", "sum", "(", ")", ">", "0", ":", "\n", "# Equation 4 in the paper.", "\n", "                    ", "entity_id_prediction_outputs", "=", "self", ".", "_dynamic_embeddings", "(", "hidden", "=", "current_hidden", ",", "\n", "target", "=", "next_entity_ids", ",", "\n", "mask", "=", "predict_em", ")", "\n", "_entity_id_loss", "=", "entity_id_prediction_outputs", "[", "'loss'", "]", "\n", "entity_id_loss", "+=", "_entity_id_loss", ".", "sum", "(", ")", "\n", "\n", "entity_id_logp", "=", "torch", ".", "zeros_like", "(", "next_entity_ids", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "entity_id_logp", "[", "predict_em", "]", "=", "-", "_entity_id_loss", "\n", "logp", "+=", "entity_id_logp", "\n", "\n", "self", ".", "_entity_id_accuracy", "(", "predictions", "=", "entity_id_prediction_outputs", "[", "'logits'", "]", ",", "\n", "gold_labels", "=", "next_entity_ids", "[", "predict_em", "]", ")", "\n", "\n", "# Equation 5 in the paper.", "\n", "next_entity_embeddings", "=", "self", ".", "_dynamic_embeddings", ".", "embeddings", "[", "predict_em", ",", "next_entity_ids", "[", "predict_em", "]", "]", "\n", "next_entity_embeddings", "=", "self", ".", "_dropout", "(", "next_entity_embeddings", ")", "\n", "concatenated", "=", "torch", ".", "cat", "(", "(", "current_hidden", "[", "predict_em", "]", ",", "next_entity_embeddings", ")", ",", "dim", "=", "-", "1", ")", "\n", "mention_length_logits", "=", "self", ".", "_mention_length_projection", "(", "concatenated", ")", "\n", "_mention_length_loss", "=", "F", ".", "cross_entropy", "(", "mention_length_logits", ",", "\n", "next_mention_lengths", "[", "predict_em", "]", ",", "\n", "reduction", "=", "'none'", ")", "\n", "mention_length_loss", "+=", "_mention_length_loss", ".", "sum", "(", ")", "\n", "\n", "mention_length_logp", "=", "torch", ".", "zeros_like", "(", "next_mention_lengths", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "mention_length_logp", "[", "predict_em", "]", "=", "-", "_mention_length_loss", "\n", "logp", "+=", "mention_length_logp", "\n", "\n", "self", ".", "_mention_length_accuracy", "(", "predictions", "=", "mention_length_logits", ",", "\n", "gold_labels", "=", "next_mention_lengths", "[", "predict_em", "]", ")", "\n", "\n", "# Always predict the next word. This is done using the hidden state and contextual bias.", "\n", "", "", "entity_embeddings", "=", "self", ".", "_dynamic_embeddings", ".", "embeddings", "[", "next_entity_types", ",", "next_entity_ids", "[", "next_entity_types", "]", "]", "\n", "entity_embeddings", "=", "self", ".", "_entity_output_projection", "(", "entity_embeddings", ")", "\n", "context_embeddings", "=", "contexts", "[", "1", "-", "next_entity_types", "]", "\n", "context_embeddings", "=", "self", ".", "_context_output_projection", "(", "context_embeddings", ")", "\n", "\n", "# The checks in the following block of code are required to prevent adding empty", "\n", "# tensors to vocab_features (which causes a floating point error).", "\n", "vocab_features", "=", "current_hidden", ".", "clone", "(", ")", "\n", "if", "next_entity_types", ".", "sum", "(", ")", ">", "0", ":", "\n", "                ", "vocab_features", "[", "next_entity_types", "]", "=", "vocab_features", "[", "next_entity_types", "]", "+", "entity_embeddings", "\n", "", "if", "(", "1", "-", "next_entity_types", ".", "sum", "(", ")", ")", ">", "0", ":", "\n", "                ", "vocab_features", "[", "1", "-", "next_entity_types", "]", "=", "vocab_features", "[", "1", "-", "next_entity_types", "]", "+", "context_embeddings", "\n", "", "vocab_logits", "=", "self", ".", "_vocab_projection", "(", "vocab_features", ")", "\n", "\n", "_vocab_loss", "=", "F", ".", "cross_entropy", "(", "vocab_logits", ",", "next_tokens", ",", "reduction", "=", "'none'", ")", "\n", "_vocab_loss", "=", "_vocab_loss", "*", "next_mask", ".", "float", "(", ")", "\n", "vocab_loss", "+=", "_vocab_loss", ".", "sum", "(", ")", "\n", "logp", "+=", "-", "_vocab_loss", "\n", "\n", "# self._perplexity(logits=vocab_logits,", "\n", "#                  labels=next_tokens,", "\n", "#                  mask=next_mask.float())", "\n", "# self._unknown_penalized_perplexity(logits=vocab_logits,", "\n", "#                                    labels=next_tokens,", "\n", "#                                    mask=next_mask.float())", "\n", "\n", "# Lastly update contexts", "\n", "contexts", "=", "current_hidden", "\n", "\n", "# Normalize the losses", "\n", "", "entity_type_loss", "/=", "mask", ".", "sum", "(", ")", "\n", "entity_id_loss", "/=", "mask", ".", "sum", "(", ")", "\n", "mention_length_loss", "/=", "mask", ".", "sum", "(", ")", "\n", "vocab_loss", "/=", "mask", ".", "sum", "(", ")", "\n", "total_loss", "=", "entity_type_loss", "+", "entity_id_loss", "+", "mention_length_loss", "+", "vocab_loss", "\n", "\n", "output_dict", "=", "{", "\n", "'entity_type_loss'", ":", "entity_type_loss", ",", "\n", "'entity_id_loss'", ":", "entity_id_loss", ",", "\n", "'mention_length_loss'", ":", "mention_length_loss", ",", "\n", "'vocab_loss'", ":", "vocab_loss", ",", "\n", "'loss'", ":", "total_loss", ",", "\n", "'logp'", ":", "logp", "\n", "}", "\n", "\n", "# Update the model state", "\n", "self", ".", "_state", "=", "{", "\n", "'prev_tokens'", ":", "{", "field", ":", "tokens", "[", "field", "]", "[", ":", ",", "-", "1", "]", ".", "unsqueeze", "(", "1", ")", ".", "detach", "(", ")", "for", "field", "in", "tokens", "}", ",", "\n", "'prev_entity_types'", ":", "entity_types", "[", ":", ",", "-", "1", "]", ".", "unsqueeze", "(", "1", ")", ".", "detach", "(", ")", ",", "\n", "'prev_entity_ids'", ":", "entity_ids", "[", ":", ",", "-", "1", "]", ".", "unsqueeze", "(", "1", ")", ".", "detach", "(", ")", ",", "\n", "'prev_mention_lengths'", ":", "mention_lengths", "[", ":", ",", "-", "1", "]", ".", "unsqueeze", "(", "1", ")", ".", "detach", "(", ")", ",", "\n", "'prev_contexts'", ":", "contexts", ".", "detach", "(", ")", "\n", "}", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_nlm.EntityNLM.reset_states": [[396, 401], ["entity_nlm.EntityNLM._encoder.reset_states", "entity_nlm.EntityNLM._dynamic_embeddings.reset_states"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states"], ["", "def", "reset_states", "(", "self", ",", "batch_size", ":", "int", ")", "->", "None", ":", "\n", "        ", "\"\"\"Resets the model's internals. Should be called at the start of a new batch.\"\"\"", "\n", "self", ".", "_encoder", ".", "reset_states", "(", ")", "\n", "self", ".", "_dynamic_embeddings", ".", "reset_states", "(", "batch_size", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_nlm.EntityNLM.detach_states": [[402, 405], ["entity_nlm.EntityNLM._dynamic_embeddings.detach_states"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.detach_states"], ["", "def", "detach_states", "(", "self", ")", ":", "\n", "        ", "\"\"\"Detaches the model's state to enforce truncated backpropagation.\"\"\"", "\n", "self", ".", "_dynamic_embeddings", ".", "detach_states", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_nlm.EntityNLM.get_metrics": [[406, 414], ["entity_nlm.EntityNLM._entity_type_accuracy.get_metric", "entity_nlm.EntityNLM._entity_id_accuracy.get_metric", "entity_nlm.EntityNLM._mention_length_accuracy.get_metric"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "{", "\n", "# 'ppl': self._perplexity.get_metric(reset),", "\n", "# 'upp': self._unknown_penalized_perplexity.get_metric(reset),", "\n", "'et_acc'", ":", "self", ".", "_entity_type_accuracy", ".", "get_metric", "(", "reset", ")", ",", "\n", "'eid_acc'", ":", "self", ".", "_entity_id_accuracy", ".", "get_metric", "(", "reset", ")", ",", "\n", "'ml_acc'", ":", "self", ".", "_mention_length_accuracy", ".", "get_metric", "(", "reset", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc.__init__": [[35, 130], ["allennlp.nn.InitializerApplicator", "allennlp.models.Model.__init__", "kglm.modules.RecentEntities", "kglm.modules.KnowledgeGraphLookup", "kglm.modules.LockedDropout", "entity_embedder.get_output_dim", "token_embedder.get_output_dim", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "vocab.get_token_index", "math.log", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.Average", "allennlp.training.metrics.F1Measure", "allennlp.training.metrics.F1Measure", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "initializer", "rnns.append", "kglm.modules.WeightDrop", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "vocab.get_vocab_size", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "vocab.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "token_embedder", ":", "TextFieldEmbedder", ",", "\n", "entity_embedder", ":", "TextFieldEmbedder", ",", "\n", "relation_embedder", ":", "TextFieldEmbedder", ",", "\n", "knowledge_graph_path", ":", "str", ",", "\n", "use_shortlist", ":", "bool", ",", "\n", "hidden_size", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "cutoff", ":", "int", "=", "30", ",", "\n", "tie_weights", ":", "bool", "=", "False", ",", "\n", "dropout", ":", "float", "=", "0.4", ",", "\n", "dropouth", ":", "float", "=", "0.3", ",", "\n", "dropouti", ":", "float", "=", "0.65", ",", "\n", "dropoute", ":", "float", "=", "0.1", ",", "\n", "wdrop", ":", "float", "=", "0.5", ",", "\n", "alpha", ":", "float", "=", "2.0", ",", "\n", "beta", ":", "float", "=", "1.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ")", "->", "None", ":", "\n", "        ", "super", "(", "KglmDisc", ",", "self", ")", ".", "__init__", "(", "vocab", ")", "\n", "\n", "# We extract the `Embedding` layers from the `TokenEmbedders` to apply dropout later on.", "\n", "# pylint: disable=protected-access", "\n", "self", ".", "_token_embedder", "=", "token_embedder", ".", "_token_embedders", "[", "'tokens'", "]", "\n", "self", ".", "_entity_embedder", "=", "entity_embedder", ".", "_token_embedders", "[", "'entity_ids'", "]", "\n", "self", ".", "_relation_embedder", "=", "relation_embedder", ".", "_token_embedders", "[", "'relations'", "]", "\n", "self", ".", "_recent_entities", "=", "RecentEntities", "(", "cutoff", "=", "cutoff", ")", "\n", "self", ".", "_knowledge_graph_lookup", "=", "KnowledgeGraphLookup", "(", "knowledge_graph_path", ",", "vocab", "=", "vocab", ")", "\n", "self", ".", "_use_shortlist", "=", "use_shortlist", "\n", "self", ".", "_hidden_size", "=", "hidden_size", "\n", "self", ".", "_num_layers", "=", "num_layers", "\n", "self", ".", "_cutoff", "=", "cutoff", "\n", "self", ".", "_tie_weights", "=", "tie_weights", "\n", "\n", "# Dropout", "\n", "self", ".", "_locked_dropout", "=", "LockedDropout", "(", ")", "\n", "self", ".", "_dropout", "=", "dropout", "\n", "self", ".", "_dropouth", "=", "dropouth", "\n", "self", ".", "_dropouti", "=", "dropouti", "\n", "self", ".", "_dropoute", "=", "dropoute", "\n", "self", ".", "_wdrop", "=", "wdrop", "\n", "\n", "# Regularization strength", "\n", "self", ".", "_alpha", "=", "alpha", "\n", "self", ".", "_beta", "=", "beta", "\n", "\n", "# RNN Encoders.", "\n", "entity_embedding_dim", "=", "entity_embedder", ".", "get_output_dim", "(", ")", "\n", "token_embedding_dim", "=", "token_embedder", ".", "get_output_dim", "(", ")", "\n", "self", ".", "entity_embedding_dim", "=", "entity_embedding_dim", "\n", "self", ".", "token_embedding_dim", "=", "token_embedding_dim", "\n", "\n", "rnns", ":", "List", "[", "torch", ".", "nn", ".", "Module", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "input_size", "=", "token_embedding_dim", "\n", "", "else", ":", "\n", "                ", "input_size", "=", "hidden_size", "\n", "", "if", "(", "i", "==", "num_layers", "-", "1", ")", ":", "\n", "                ", "output_size", "=", "token_embedding_dim", "+", "2", "*", "entity_embedding_dim", "\n", "", "else", ":", "\n", "                ", "output_size", "=", "hidden_size", "\n", "", "rnns", ".", "append", "(", "torch", ".", "nn", ".", "LSTM", "(", "input_size", ",", "output_size", ",", "batch_first", "=", "True", ")", ")", "\n", "", "rnns", "=", "[", "WeightDrop", "(", "rnn", ",", "[", "'weight_hh_l0'", "]", ",", "dropout", "=", "wdrop", ")", "for", "rnn", "in", "rnns", "]", "\n", "self", ".", "rnns", "=", "torch", ".", "nn", ".", "ModuleList", "(", "rnns", ")", "\n", "\n", "# Various linear transformations.", "\n", "self", ".", "_fc_mention_type", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "token_embedding_dim", ",", "\n", "out_features", "=", "4", ")", "\n", "\n", "if", "not", "use_shortlist", ":", "\n", "            ", "self", ".", "_fc_new_entity", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "entity_embedding_dim", ",", "\n", "out_features", "=", "vocab", ".", "get_vocab_size", "(", "'entity_ids'", ")", ")", "\n", "\n", "if", "tie_weights", ":", "\n", "                ", "self", ".", "_fc_new_entity", ".", "weight", "=", "self", ".", "_entity_embedder", ".", "weight", "\n", "\n", "", "", "self", ".", "_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", "\n", "\n", "# Metrics", "\n", "self", ".", "_unk_index", "=", "vocab", ".", "get_token_index", "(", "DEFAULT_OOV_TOKEN", ")", "\n", "self", ".", "_unk_penalty", "=", "math", ".", "log", "(", "vocab", ".", "get_vocab_size", "(", "'tokens_unk'", ")", ")", "\n", "self", ".", "_avg_mention_type_loss", "=", "Average", "(", ")", "\n", "self", ".", "_avg_new_entity_loss", "=", "Average", "(", ")", "\n", "self", ".", "_avg_knowledge_graph_entity_loss", "=", "Average", "(", ")", "\n", "self", ".", "_new_mention_f1", "=", "F1Measure", "(", "positive_label", "=", "1", ")", "\n", "self", ".", "_kg_mention_f1", "=", "F1Measure", "(", "positive_label", "=", "2", ")", "\n", "self", ".", "_new_entity_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "self", ".", "_new_entity_accuracy20", "=", "CategoricalAccuracy", "(", "top_k", "=", "20", ")", "\n", "self", ".", "_parent_ppl", "=", "Ppl", "(", ")", "\n", "self", ".", "_relation_ppl", "=", "Ppl", "(", ")", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc.sample": [[131, 331], ["alias_database.tensorize", "kglm_disc.KglmDisc._recent_entities.reset", "allennlp.nn.util.get_text_field_mask().byte", "kglm_disc.KglmDisc._encode_source", "encoded.split", "kglm_disc.KglmDisc._fc_mention_type", "torch.softmax", "torch.softmax", "kglm.nn.util.parallel_sample", "torch.softmax.gather().log", "mention_logp.sum.sum.sum", "kglm.nn.util.parallel_sample.eq", "kglm_disc.KglmDisc._new_entity_logits", "F.softmax.gather().log.sum", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "kglm.nn.util.nested_enumerate", "torch.zeros_like().unsqueeze", "torch.zeros_like().unsqueeze", "torch.zeros_like().unsqueeze", "torch.zeros_like().unsqueeze", "kglm.nn.util.parallel_sample.eq", "range", "reset.any", "range", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.masked_softmax", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "allennlp.nn.util.get_text_field_mask.byte().any", "kglm.nn.util.parallel_sample", "torch.softmax.gather().log", "shortlist[].gather", "torch.softmax", "torch.softmax", "kglm.nn.util.parallel_sample", "torch.softmax.gather().log", "torch.zeros_like.tolist", "torch.zeros_like.tolist", "kglm_disc.KglmDisc.vocab.get_token_from_index", "kglm_disc.KglmDisc.vocab.get_token_index", "entity_ids[].unsqueeze", "kglm_disc.KglmDisc._recent_entities", "kglm_disc.KglmDisc._entity_embedder", "encoded_head[].unsqueeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "allennlp.nn.util.masked_softmax", "candidate_mask.any().squeeze", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "candidate_mask.any().squeeze.any", "parent_logp[].sum", "kglm_disc.KglmDisc._knowledge_graph_lookup", "encoded_relation[].unsqueeze", "torch.zeros_like().squeeze", "torch.zeros_like().squeeze", "torch.zeros_like().squeeze", "torch.zeros_like().squeeze", "torch.zeros_like().squeeze", "torch.zeros_like().squeeze", "torch.zeros_like().squeeze", "torch.zeros_like().squeeze", "zip", "kglm_disc.KglmDisc._recent_entities.insert", "true_raw_entity_ids.eq", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "allennlp.nn.util.get_text_field_mask", "torch.softmax.gather", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "current_mask.any", "kglm_disc.KglmDisc.transpose", "kglm.nn.util.parallel_sample", "viable_candidate_probs.gather().log", "viable_candidate_ids.gather", "viable_candidate_probs.gather().log.squeeze", "kglm_disc.KglmDisc._relation_embedder", "torch.mv", "torch.mv", "torch.mv", "torch.mv", "torch.softmax", "torch.softmax", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.softmax.gather().log", "F.softmax.gather().log.sum", "kglm_disc.KglmDisc.vocab.get_token_from_index", "kglm_disc.KglmDisc.vocab.get_token_index", "mention_type[].eq", "kglm.nn.util.parallel_sample.unsqueeze", "allennlp.nn.util.get_text_field_mask.byte", "torch.softmax.gather", "torch.softmax.gather", "tuple", "candidate_mask.any", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "raw_tail_id.item", "current_mask.any", "torch.zeros_like.unsqueeze", "torch.zeros_like.unsqueeze", "kglm.nn.util.parallel_sample.unsqueeze", "viable_candidate_probs.gather", "torch.softmax.gather", "kglm.nn.util.parallel_sample.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.tensorize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._encode_source", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.parallel_sample", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_logits", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.nested_enumerate", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.parallel_sample", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.parallel_sample", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.insert", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.parallel_sample"], ["", "def", "sample", "(", "self", ",", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "target", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "reset", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "Dict", "[", "str", ",", "Any", "]", ",", "\n", "alias_copy_inds", ":", "torch", ".", "Tensor", ",", "\n", "shortlist", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "**", "kwargs", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "# **kwargs intended to eat the other fields if they are provided.", "\n", "        ", "\"\"\"\n        Sampling annotations for the generative model. Note that unlike forward, this function\n        expects inputs from a **generative** dataset reader, not a **discriminative** one.\n        \"\"\"", "\n", "\n", "# Tensorize the alias_database - this will only perform the operation once.", "\n", "alias_database", "=", "metadata", "[", "0", "]", "[", "'alias_database'", "]", "\n", "alias_database", ".", "tensorize", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n", "# Reset the model if needed", "\n", "if", "reset", ".", "any", "(", ")", "and", "(", "self", ".", "_state", "is", "not", "None", ")", ":", "\n", "            ", "for", "layer", "in", "range", "(", "self", ".", "_num_layers", ")", ":", "\n", "                ", "h", ",", "c", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "h", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "h", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "c", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "c", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "=", "(", "h", ",", "c", ")", "\n", "", "", "self", ".", "_recent_entities", ".", "reset", "(", "reset", ")", "\n", "\n", "logp", "=", "0.0", "\n", "\n", "mask", "=", "get_text_field_mask", "(", "target", ")", ".", "byte", "(", ")", "\n", "# We encode the target tokens (**not** source) since the discriminitative model makes", "\n", "# predictions on the current token, but the generative model expects labels for the", "\n", "# **next** (e.g. target) token!", "\n", "encoded", ",", "*", "_", "=", "self", ".", "_encode_source", "(", "target", "[", "'tokens'", "]", ")", "\n", "splits", "=", "[", "self", ".", "token_embedding_dim", "]", "+", "[", "self", ".", "entity_embedding_dim", "]", "*", "2", "\n", "encoded_token", ",", "encoded_head", ",", "encoded_relation", "=", "encoded", ".", "split", "(", "splits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Compute new mention logits", "\n", "mention_logits", "=", "self", ".", "_fc_mention_type", "(", "encoded_token", ")", "\n", "mention_probs", "=", "F", ".", "softmax", "(", "mention_logits", ",", "dim", "=", "-", "1", ")", "\n", "mention_type", "=", "parallel_sample", "(", "mention_probs", ")", "\n", "mention_logp", "=", "mention_probs", ".", "gather", "(", "-", "1", ",", "mention_type", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "log", "(", ")", "\n", "mention_logp", "[", "~", "mask", "]", "=", "0", "\n", "mention_logp", "=", "mention_logp", ".", "sum", "(", ")", "\n", "\n", "# Compute entity logits", "\n", "new_entity_mask", "=", "mention_type", ".", "eq", "(", "1", ")", "\n", "new_entity_logits", "=", "self", ".", "_new_entity_logits", "(", "encoded_head", "+", "encoded_relation", ",", "shortlist", ")", "\n", "if", "self", ".", "_use_shortlist", ":", "\n", "# If using shortlist, then samples are indexed w.r.t the shortlist and entity_ids must be looked up", "\n", "            ", "shortlist_mask", "=", "get_text_field_mask", "(", "shortlist", ")", "\n", "new_entity_probs", "=", "masked_softmax", "(", "new_entity_logits", ",", "shortlist_mask", ")", "\n", "shortlist_inds", "=", "torch", ".", "zeros_like", "(", "mention_type", ")", "\n", "# Some sequences may be full of padding in which case the shortlist", "\n", "# is empty", "\n", "not_just_padding", "=", "shortlist_mask", ".", "byte", "(", ")", ".", "any", "(", "-", "1", ")", "\n", "shortlist_inds", "[", "not_just_padding", "]", "=", "parallel_sample", "(", "new_entity_probs", "[", "not_just_padding", "]", ")", "\n", "shortlist_inds", "[", "~", "new_entity_mask", "]", "=", "0", "\n", "_new_entity_logp", "=", "new_entity_probs", ".", "gather", "(", "-", "1", ",", "shortlist_inds", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "log", "(", ")", "\n", "new_entity_samples", "=", "shortlist", "[", "'entity_ids'", "]", ".", "gather", "(", "1", ",", "shortlist_inds", ")", "\n", "", "else", ":", "\n", "            ", "new_entity_logits", "=", "new_entity_logits", "\n", "# If not using shortlist, then samples are indexed w.r.t to the global vocab", "\n", "new_entity_probs", "=", "F", ".", "softmax", "(", "new_entity_logits", ",", "dim", "=", "-", "1", ")", "\n", "new_entity_samples", "=", "parallel_sample", "(", "new_entity_probs", ")", "\n", "_new_entity_logp", "=", "new_entity_probs", ".", "gather", "(", "-", "1", ",", "new_entity_samples", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "log", "(", ")", "\n", "shortlist_inds", "=", "None", "\n", "# Zero out masked tokens and non-new entity predictions", "\n", "", "_new_entity_logp", "[", "~", "mask", "]", "=", "0", "\n", "_new_entity_logp", "[", "~", "new_entity_mask", "]", "=", "0", "\n", "new_entity_logp", "=", "_new_entity_logp", ".", "sum", "(", ")", "\n", "\n", "# Start filling in the entity ids", "\n", "entity_ids", "=", "torch", ".", "zeros_like", "(", "target", "[", "'tokens'", "]", ")", "\n", "entity_ids", "[", "new_entity_mask", "]", "=", "new_entity_samples", "[", "new_entity_mask", "]", "\n", "\n", "# ...UGH... we also need the raw ids - remapping time", "\n", "raw_entity_ids", "=", "torch", ".", "zeros_like", "(", "target", "[", "'tokens'", "]", ")", "\n", "for", "*", "index", ",", "entity_id", "in", "nested_enumerate", "(", "entity_ids", ".", "tolist", "(", ")", ")", ":", "\n", "            ", "token", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "entity_id", ",", "'entity_ids'", ")", "\n", "raw_entity_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "token", ",", "'raw_entity_ids'", ")", "\n", "raw_entity_ids", "[", "tuple", "(", "index", ")", "]", "=", "raw_entity_id", "\n", "\n", "# Derived mentions need to be computed sequentially.", "\n", "", "parent_ids", "=", "torch", ".", "zeros_like", "(", "target", "[", "'tokens'", "]", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "derived_entity_mask", "=", "mention_type", ".", "eq", "(", "2", ")", "\n", "derived_entity_logp", "=", "0.0", "\n", "\n", "sequence_length", "=", "target", "[", "'tokens'", "]", ".", "shape", "[", "1", "]", "\n", "for", "i", "in", "range", "(", "sequence_length", ")", ":", "\n", "\n", "            ", "current_mask", "=", "derived_entity_mask", "[", ":", ",", "i", "]", "&", "mask", "[", ":", ",", "i", "]", "\n", "\n", "## SAMPLE PARENTS ##", "\n", "\n", "# Update recent entities with **current** entity only", "\n", "current_entity_id", "=", "entity_ids", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "1", ")", "\n", "candidate_ids", ",", "candidate_mask", "=", "self", ".", "_recent_entities", "(", "current_entity_id", ")", "\n", "\n", "# If no mentions are derived, there is no point continuing after entities have been updated.", "\n", "if", "not", "current_mask", ".", "any", "(", ")", ":", "\n", "                ", "continue", "\n", "\n", "# Otherwise we proceed", "\n", "", "candidate_embeddings", "=", "self", ".", "_entity_embedder", "(", "candidate_ids", ")", "\n", "\n", "# Compute logits w.r.t **current** hidden state only", "\n", "current_head_encoding", "=", "encoded_head", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "1", ")", "\n", "selection_logits", "=", "torch", ".", "bmm", "(", "current_head_encoding", ",", "candidate_embeddings", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "selection_probs", "=", "masked_softmax", "(", "selection_logits", ",", "candidate_mask", ")", "\n", "\n", "# Only sample if the is at least one viable candidate (e.g. if a sampling distribution", "\n", "# has no probability mass we cannot sample from it). Return zero as the parent for", "\n", "# non-viable distributions.", "\n", "viable_candidate_mask", "=", "candidate_mask", ".", "any", "(", "-", "1", ")", ".", "squeeze", "(", ")", "\n", "_parent_ids", "=", "torch", ".", "zeros_like", "(", "current_entity_id", ")", "\n", "parent_logp", "=", "torch", ".", "zeros_like", "(", "current_entity_id", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "if", "viable_candidate_mask", ".", "any", "(", ")", ":", "\n", "                ", "viable_candidate_ids", "=", "candidate_ids", "[", "viable_candidate_mask", "]", "\n", "viable_candidate_probs", "=", "selection_probs", "[", "viable_candidate_mask", "]", "\n", "viable_parent_samples", "=", "parallel_sample", "(", "viable_candidate_probs", ")", "\n", "viable_logp", "=", "viable_candidate_probs", ".", "gather", "(", "-", "1", ",", "viable_parent_samples", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "log", "(", ")", "\n", "viable_parent_ids", "=", "viable_candidate_ids", ".", "gather", "(", "-", "1", ",", "viable_parent_samples", ")", "\n", "_parent_ids", "[", "viable_candidate_mask", "]", "=", "viable_parent_ids", "\n", "parent_logp", "[", "viable_candidate_mask", "]", "=", "viable_logp", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "", "parent_ids", "[", "current_mask", ",", "i", "]", "=", "_parent_ids", "[", "current_mask", "]", "# TODO: Double-check", "\n", "derived_entity_logp", "+=", "parent_logp", "[", "current_mask", "]", ".", "sum", "(", ")", "\n", "\n", "## SAMPLE RELATIONS ##", "\n", "\n", "# Lookup sampled parent ids in the knowledge graph", "\n", "indices", ",", "parent_ids_list", ",", "relations_list", ",", "tail_ids_list", "=", "self", ".", "_knowledge_graph_lookup", "(", "_parent_ids", ")", "\n", "relation_embeddings", "=", "[", "self", ".", "_relation_embedder", "(", "r", ")", "for", "r", "in", "relations_list", "]", "\n", "\n", "# Sample tail ids", "\n", "current_relation_encoding", "=", "encoded_relation", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "1", ")", "\n", "_raw_tail_ids", "=", "torch", ".", "zeros_like", "(", "_parent_ids", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "_tail_ids", "=", "torch", ".", "zeros_like", "(", "_parent_ids", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "for", "index", ",", "relation_embedding", ",", "tail_id_lookup", "in", "zip", "(", "indices", ",", "relation_embeddings", ",", "tail_ids_list", ")", ":", "\n", "# Compute the score for each relation w.r.t the current encoding. NOTE: In the loss", "\n", "# code index has a slice. We don't need that here since there is always a", "\n", "# **single** parent.", "\n", "                ", "logits", "=", "torch", ".", "mv", "(", "relation_embedding", ",", "current_relation_encoding", "[", "index", "]", ")", "\n", "# Convert to probability", "\n", "tail_probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "# Sample", "\n", "tail_sample", "=", "torch", ".", "multinomial", "(", "tail_probs", ",", "1", ")", "\n", "# Get logp. Ignoring the current_mask here is **super** dodgy, but since we forced", "\n", "# null parents to zero we shouldn't be accumulating probabilities for unused predictions.", "\n", "tail_logp", "=", "tail_probs", ".", "gather", "(", "-", "1", ",", "tail_sample", ")", ".", "log", "(", ")", "\n", "derived_entity_logp", "+=", "tail_logp", ".", "sum", "(", ")", "# Sum is redundant, just need it to make logp a scalar", "\n", "\n", "# Map back to raw id", "\n", "raw_tail_id", "=", "tail_id_lookup", "[", "tail_sample", "]", "\n", "# Convert raw id to id", "\n", "tail_id_string", "=", "self", ".", "vocab", ".", "get_token_from_index", "(", "raw_tail_id", ".", "item", "(", ")", ",", "'raw_entity_ids'", ")", "\n", "tail_id", "=", "self", ".", "vocab", ".", "get_token_index", "(", "tail_id_string", ",", "'entity_ids'", ")", "\n", "\n", "_raw_tail_ids", "[", "index", "[", ":", "-", "1", "]", "]", "=", "raw_tail_id", "\n", "_tail_ids", "[", "index", "[", ":", "-", "1", "]", "]", "=", "tail_id", "\n", "\n", "", "raw_entity_ids", "[", "current_mask", ",", "i", "]", "=", "_raw_tail_ids", "[", "current_mask", "]", "# TODO: Double-check", "\n", "entity_ids", "[", "current_mask", ",", "i", "]", "=", "_tail_ids", "[", "current_mask", "]", "# TODO: Double-check", "\n", "\n", "self", ".", "_recent_entities", ".", "insert", "(", "_tail_ids", ",", "current_mask", ")", "\n", "\n", "## CONTINUE MENTIONS ##", "\n", "continue_mask", "=", "mention_type", "[", ":", ",", "i", "]", ".", "eq", "(", "3", ")", "&", "mask", "[", ":", ",", "i", "]", "\n", "if", "not", "current_mask", ".", "any", "(", ")", "or", "i", "==", "0", ":", "\n", "                ", "continue", "\n", "", "raw_entity_ids", "[", "continue_mask", ",", "i", "]", "=", "raw_entity_ids", "[", "continue_mask", ",", "i", "-", "1", "]", "\n", "entity_ids", "[", "continue_mask", ",", "i", "]", "=", "entity_ids", "[", "continue_mask", ",", "i", "-", "1", "]", "\n", "entity_ids", "[", "continue_mask", ",", "i", "]", "=", "entity_ids", "[", "continue_mask", ",", "i", "-", "1", "]", "\n", "parent_ids", "[", "continue_mask", ",", "i", "]", "=", "parent_ids", "[", "continue_mask", ",", "i", "-", "1", "]", "\n", "if", "self", ".", "_use_shortlist", ":", "\n", "                ", "shortlist_inds", "[", "continue_mask", ",", "i", "]", "=", "shortlist_inds", "[", "continue_mask", ",", "i", "-", "1", "]", "\n", "", "alias_copy_inds", "[", "continue_mask", ",", "i", "]", "=", "alias_copy_inds", "[", "continue_mask", ",", "i", "-", "1", "]", "\n", "\n", "# Lastly, because entities won't always match the true entity ids, we need to zero out any alias copy ids that won't be valid.", "\n", "", "true_raw_entity_ids", "=", "kwargs", "[", "'raw_entity_ids'", "]", "[", "'raw_entity_ids'", "]", "\n", "invalid_id_mask", "=", "~", "true_raw_entity_ids", ".", "eq", "(", "raw_entity_ids", ")", "\n", "alias_copy_inds", "[", "invalid_id_mask", "]", "=", "0", "\n", "\n", "# Pass denotes fields that are passed directly from input to output.", "\n", "sample", "=", "{", "\n", "'source'", ":", "source", ",", "# Pass", "\n", "'target'", ":", "target", ",", "# Pass", "\n", "'reset'", ":", "reset", ",", "# Pass", "\n", "'metadata'", ":", "metadata", ",", "# Pass", "\n", "'mention_type'", ":", "mention_type", ",", "\n", "'raw_entity_ids'", ":", "{", "'raw_entity_ids'", ":", "raw_entity_ids", "}", ",", "\n", "'entity_ids'", ":", "{", "'entity_ids'", ":", "entity_ids", "}", ",", "\n", "'parent_ids'", ":", "{", "'entity_ids'", ":", "parent_ids", "}", ",", "\n", "'relations'", ":", "{", "'relations'", ":", "None", "}", ",", "# We aren't using them - eventually should remove entirely", "\n", "'shortlist'", ":", "shortlist", ",", "# Pass", "\n", "'shortlist_inds'", ":", "shortlist_inds", ",", "\n", "'alias_copy_inds'", ":", "alias_copy_inds", "\n", "}", "\n", "logp", "=", "mention_logp", "+", "new_entity_logp", "+", "derived_entity_logp", "\n", "return", "{", "'sample'", ":", "sample", ",", "'logp'", ":", "logp", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc.forward": [[332, 375], ["alias_database.tensorize", "kglm_disc.KglmDisc._recent_entities.reset", "reset.any", "range", "kglm_disc.KglmDisc._forward_loop", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.tensorize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.recent_entities.RecentEntities.reset", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._forward_loop"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "reset", ":", "torch", ".", "Tensor", ",", "\n", "metadata", ":", "List", "[", "Dict", "[", "str", ",", "Any", "]", "]", ",", "\n", "mention_type", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "raw_entity_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "entity_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "parent_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "relations", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "shortlist", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "shortlist_inds", ":", "torch", ".", "Tensor", "=", "None", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "# Tensorize the alias_database - this will only perform the operation once.", "\n", "        ", "alias_database", "=", "metadata", "[", "0", "]", "[", "'alias_database'", "]", "\n", "alias_database", ".", "tensorize", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n", "# Reset the model if needed", "\n", "if", "reset", ".", "any", "(", ")", "and", "(", "self", ".", "_state", "is", "not", "None", ")", ":", "\n", "            ", "for", "layer", "in", "range", "(", "self", ".", "_num_layers", ")", ":", "\n", "                ", "h", ",", "c", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "h", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "h", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "c", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "c", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "=", "(", "h", ",", "c", ")", "\n", "", "", "self", ".", "_recent_entities", ".", "reset", "(", "reset", ")", "\n", "\n", "if", "entity_ids", "is", "not", "None", ":", "\n", "            ", "output_dict", "=", "self", ".", "_forward_loop", "(", "\n", "source", "=", "source", ",", "\n", "alias_database", "=", "alias_database", ",", "\n", "mention_type", "=", "mention_type", ",", "\n", "raw_entity_ids", "=", "raw_entity_ids", ",", "\n", "entity_ids", "=", "entity_ids", ",", "\n", "parent_ids", "=", "parent_ids", ",", "\n", "relations", "=", "relations", ",", "\n", "shortlist", "=", "shortlist", ",", "\n", "shortlist_inds", "=", "shortlist_inds", ")", "\n", "", "else", ":", "\n", "# TODO: Figure out what we want here - probably to do some king of inference on", "\n", "# entities / mention types.", "\n", "            ", "output_dict", "=", "{", "}", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._encode_source": [[376, 415], ["kglm.modules.embedded_dropout", "kglm_disc.KglmDisc._locked_dropout", "enumerate", "kglm_disc.KglmDisc.pow().mean", "rnn", "output.contiguous.contiguous.contiguous", "tuple", "hidden_states.append", "kglm_disc.KglmDisc._locked_dropout", "kglm_disc.KglmDisc._locked_dropout", "kglm_disc.KglmDisc.pow", "enumerate", "h.detach"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], ["", "def", "_encode_source", "(", "self", ",", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "# Extract and embed source tokens.", "\n", "        ", "source_embeddings", "=", "embedded_dropout", "(", "\n", "embed", "=", "self", ".", "_token_embedder", ",", "\n", "words", "=", "source", ",", "\n", "dropout", "=", "self", ".", "_dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "source_embeddings", "=", "self", ".", "_locked_dropout", "(", "source_embeddings", ",", "self", ".", "_dropouti", ")", "\n", "\n", "# Encode.", "\n", "current_input", "=", "source_embeddings", "\n", "hidden_states", "=", "[", "]", "\n", "for", "layer", ",", "rnn", "in", "enumerate", "(", "self", ".", "rnns", ")", ":", "\n", "# Retrieve previous hidden state for layer.", "\n", "            ", "if", "self", ".", "_state", "is", "not", "None", ":", "\n", "                ", "prev_hidden", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "", "else", ":", "\n", "                ", "prev_hidden", "=", "None", "\n", "# Forward-pass.", "\n", "", "output", ",", "hidden", "=", "rnn", "(", "current_input", ",", "prev_hidden", ")", "\n", "output", "=", "output", ".", "contiguous", "(", ")", "\n", "# Update hidden state for layer.", "\n", "hidden", "=", "tuple", "(", "h", ".", "detach", "(", ")", "for", "h", "in", "hidden", ")", "\n", "hidden_states", ".", "append", "(", "hidden", ")", "\n", "# Apply dropout.", "\n", "if", "layer", "==", "self", ".", "_num_layers", "-", "1", ":", "\n", "                ", "dropped_output", "=", "self", ".", "_locked_dropout", "(", "output", ",", "self", ".", "_dropout", ")", "\n", "", "else", ":", "\n", "                ", "dropped_output", "=", "self", ".", "_locked_dropout", "(", "output", ",", "self", ".", "_dropouth", ")", "\n", "", "current_input", "=", "dropped_output", "\n", "", "encoded", "=", "current_input", "\n", "\n", "alpha_loss", "=", "dropped_output", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "beta_loss", "=", "(", "output", "[", ":", ",", "1", ":", "]", "-", "output", "[", ":", ",", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "# Update state.", "\n", "self", ".", "_state", "=", "{", "'layer_%i'", "%", "i", ":", "h", "for", "i", ",", "h", "in", "enumerate", "(", "hidden_states", ")", "}", "\n", "\n", "return", "encoded", ",", "alpha_loss", ",", "beta_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._mention_type_loss": [[416, 436], ["kglm_disc.KglmDisc._fc_mention_type", "allennlp.nn.util.sequence_cross_entropy_with_logits", "kglm_disc.KglmDisc._new_mention_f1", "kglm_disc.KglmDisc._kg_mention_f1"], "methods", ["None"], ["", "def", "_mention_type_loss", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "mention_type", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes the loss for predicting whether or not the the next token will be part of an\n        entity mention.\n        \"\"\"", "\n", "logits", "=", "self", ".", "_fc_mention_type", "(", "encoded", ")", "\n", "mention_type_loss", "=", "sequence_cross_entropy_with_logits", "(", "logits", ",", "mention_type", ",", "mask", ",", "\n", "average", "=", "'token'", ")", "\n", "# if not self.training:", "\n", "self", ".", "_new_mention_f1", "(", "predictions", "=", "logits", ",", "\n", "gold_labels", "=", "mention_type", ",", "\n", "mask", "=", "mask", ")", "\n", "self", ".", "_kg_mention_f1", "(", "predictions", "=", "logits", ",", "\n", "gold_labels", "=", "mention_type", ",", "\n", "mask", "=", "mask", ")", "\n", "\n", "return", "mention_type_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_logits": [[437, 453], ["kglm.modules.embedded_dropout", "kglm_disc.KglmDisc._locked_dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "kglm_disc.KglmDisc._fc_new_entity", "kglm.modules.embedded_dropout.transpose"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], ["", "def", "_new_entity_logits", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "shortlist", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "if", "self", ".", "_use_shortlist", ":", "\n", "# Embed the shortlist entries", "\n", "            ", "shortlist_embeddings", "=", "embedded_dropout", "(", "\n", "embed", "=", "self", ".", "_entity_embedder", ",", "\n", "words", "=", "shortlist", "[", "'entity_ids'", "]", ",", "\n", "dropout", "=", "self", ".", "_dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "# Compute logits using inner product between the predicted entity embedding and the", "\n", "# embeddings of entities in the shortlist", "\n", "encodings", "=", "self", ".", "_locked_dropout", "(", "encoded", ",", "self", ".", "_dropout", ")", "\n", "logits", "=", "torch", ".", "bmm", "(", "encodings", ",", "shortlist_embeddings", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "", "else", ":", "\n", "            ", "logits", "=", "self", ".", "_fc_new_entity", "(", "encoded", ")", "\n", "", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_loss": [[454, 488], ["kglm_disc.KglmDisc._new_entity_logits", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "mask.any", "allennlp.nn.util.get_text_field_mask", "allennlp.nn.util.masked_log_softmax", "torch.log_softmax", "torch.log_softmax", "log_probs.view.view.view", "target_inds.view.view.view", "target_inds.view.view.eq", "kglm_disc.KglmDisc._new_entity_accuracy", "kglm_disc.KglmDisc._new_entity_accuracy20", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "torch.gather().squeeze.sum", "torch.gather().squeeze.sum", "target_mask.sum", "target_inds.view.view.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_logits"], ["", "def", "_new_entity_loss", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "target_inds", ":", "torch", ".", "Tensor", ",", "\n", "shortlist", ":", "torch", ".", "Tensor", ",", "\n", "target_mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ==========\n        target_inds : ``torch.Tensor``\n            Either the shortlist inds if using shortlist, otherwise the target entity ids.\n        \"\"\"", "\n", "logits", "=", "self", ".", "_new_entity_logits", "(", "encoded", ",", "shortlist", ")", "\n", "if", "self", ".", "_use_shortlist", ":", "\n", "# Take masked softmax to get log probabilties and gather the targets.", "\n", "            ", "shortlist_mask", "=", "get_text_field_mask", "(", "shortlist", ")", "\n", "log_probs", "=", "masked_log_softmax", "(", "logits", ",", "shortlist_mask", ")", "\n", "", "else", ":", "\n", "            ", "logits", "=", "logits", "\n", "log_probs", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "num_categories", "=", "log_probs", ".", "shape", "[", "-", "1", "]", "\n", "log_probs", "=", "log_probs", ".", "view", "(", "-", "1", ",", "num_categories", ")", "\n", "target_inds", "=", "target_inds", ".", "view", "(", "-", "1", ")", "\n", "", "target_log_probs", "=", "torch", ".", "gather", "(", "log_probs", ",", "-", "1", ",", "target_inds", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "mask", "=", "~", "target_inds", ".", "eq", "(", "0", ")", "\n", "target_log_probs", "[", "~", "mask", "]", "=", "0", "\n", "\n", "if", "mask", ".", "any", "(", ")", ":", "\n", "            ", "self", ".", "_new_entity_accuracy", "(", "predictions", "=", "log_probs", "[", "mask", "]", ",", "\n", "gold_labels", "=", "target_inds", "[", "mask", "]", ")", "\n", "self", ".", "_new_entity_accuracy20", "(", "predictions", "=", "log_probs", "[", "mask", "]", ",", "\n", "gold_labels", "=", "target_inds", "[", "mask", "]", ")", "\n", "\n", "", "return", "-", "target_log_probs", ".", "sum", "(", ")", "/", "(", "target_mask", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._parent_log_probs": [[489, 541], ["kglm_disc.KglmDisc._recent_entities", "logger.debug", "kglm.modules.embedded_dropout", "kglm_disc.KglmDisc._locked_dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "allennlp.nn.util.masked_log_softmax", "parent_ids.unsqueeze", "candidate_ids.view", "parent_ids.unsqueeze.eq", "logger.debug", "torch.max", "torch.max", "torch.max", "torch.max", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "torch.gather().squeeze", "kglm.modules.embedded_dropout.transpose", "candidate_ids.view.eq", "allennlp.nn.util.masked_log_softmax.unsqueeze", "torch.gather", "torch.gather", "torch.gather", "torch.gather", "mask.float"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], ["", "def", "_parent_log_probs", "(", "self", ",", "\n", "encoded_head", ":", "torch", ".", "Tensor", ",", "\n", "entity_ids", ":", "torch", ".", "Tensor", ",", "\n", "parent_ids", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "# Lookup recent entities (which are candidates for parents) and get their embeddings.", "\n", "        ", "candidate_ids", ",", "candidate_mask", "=", "self", ".", "_recent_entities", "(", "entity_ids", ")", "\n", "logger", ".", "debug", "(", "'Candidate ids shape: %s'", ",", "candidate_ids", ".", "shape", ")", "\n", "candidate_embeddings", "=", "embedded_dropout", "(", "self", ".", "_entity_embedder", ",", "\n", "words", "=", "candidate_ids", ",", "\n", "dropout", "=", "self", ".", "_dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "\n", "# Logits are computed using a general bilinear form that measures the similarity between", "\n", "# the projected hidden state and the embeddings of candidate entities", "\n", "encoded", "=", "self", ".", "_locked_dropout", "(", "encoded_head", ",", "self", ".", "_dropout", ")", "\n", "selection_logits", "=", "torch", ".", "bmm", "(", "encoded", ",", "candidate_embeddings", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "# Get log probabilities using masked softmax (need to double check mask works properly).", "\n", "\n", "# shape: (batch_size, sequence_length, num_candidates)", "\n", "log_probs", "=", "masked_log_softmax", "(", "selection_logits", ",", "candidate_mask", ")", "\n", "\n", "# Now for the tricky part. We need to convert the parent ids to a mask that selects the", "\n", "# relevant probabilities from log_probs. To do this we need to align the candidates with", "\n", "# the parent ids, which can be achieved by an element-wise equality comparison. We also", "\n", "# need to ensure that null parents are not selected.", "\n", "\n", "# shape: (batch_size, sequence_length, num_parents, 1)", "\n", "_parent_ids", "=", "parent_ids", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "batch_size", ",", "num_candidates", "=", "candidate_ids", ".", "shape", "\n", "# shape: (batch_size, 1, 1, num_candidates)", "\n", "_candidate_ids", "=", "candidate_ids", ".", "view", "(", "batch_size", ",", "1", ",", "1", ",", "num_candidates", ")", "\n", "\n", "# shape: (batch_size, sequence_length, num_parents, num_candidates)", "\n", "is_parent", "=", "_parent_ids", ".", "eq", "(", "_candidate_ids", ")", "\n", "# shape: (batch_size, 1, 1, num_candidates)", "\n", "non_null", "=", "~", "_candidate_ids", ".", "eq", "(", "0", ")", "\n", "\n", "# Since multiplication is addition in log-space, we can apply mask by adding its log (+", "\n", "# some small constant for numerical stability).", "\n", "mask", "=", "is_parent", "&", "non_null", "\n", "masked_log_probs", "=", "log_probs", ".", "unsqueeze", "(", "2", ")", "+", "(", "mask", ".", "float", "(", ")", "+", "1e-45", ")", ".", "log", "(", ")", "\n", "logger", ".", "debug", "(", "'Masked log probs shape: %s'", ",", "masked_log_probs", ".", "shape", ")", "\n", "\n", "# Lastly, we need to get rid of the num_candidates dimension. The easy way to do this would", "\n", "# be to marginalize it out. However, since our data is sparse (the last two dims are", "\n", "# essentially a delta function) this would add a lot of unneccesary terms to the computation graph.", "\n", "# To get around this we are going to try to use a gather.", "\n", "_", ",", "index", "=", "torch", ".", "max", "(", "mask", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "target_log_probs", "=", "torch", ".", "gather", "(", "masked_log_probs", ",", "dim", "=", "-", "1", ",", "index", "=", "index", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "return", "target_log_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._relation_log_probs": [[542, 576], ["kglm_disc.KglmDisc._knowledge_graph_lookup", "kglm_disc.KglmDisc._locked_dropout", "kglm_disc.KglmDisc.new_empty().fill_", "zip", "kglm_disc.KglmDisc._relation_embedder", "math.log", "torch.mv", "torch.mv", "torch.mv", "torch.mv", "logger.debug", "torch.log_softmax", "torch.log_softmax", "tail_id.eq", "torch.log_softmax.masked_select", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "kglm_disc.KglmDisc.new_empty", "tail_id.eq"], "methods", ["None"], ["", "def", "_relation_log_probs", "(", "self", ",", "\n", "encoded_relation", ":", "torch", ".", "Tensor", ",", "\n", "raw_entity_ids", ":", "torch", ".", "Tensor", ",", "\n", "parent_ids", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "\n", "# Lookup edges out of parents", "\n", "        ", "indices", ",", "parent_ids_list", ",", "relations_list", ",", "tail_ids_list", "=", "self", ".", "_knowledge_graph_lookup", "(", "parent_ids", ")", "\n", "\n", "# Embed relations", "\n", "relation_embeddings", "=", "[", "self", ".", "_relation_embedder", "(", "r", ")", "for", "r", "in", "relations_list", "]", "\n", "\n", "# Logits are computed using a general bilinear form that measures the similarity between", "\n", "# the projected hidden state and the embeddings of relations", "\n", "encoded", "=", "self", ".", "_locked_dropout", "(", "encoded_relation", ",", "self", ".", "_dropout", ")", "\n", "\n", "# This is a little funky, but to avoid massive amounts of padding we are going to just", "\n", "# iterate over the relation and tail_id vectors one-by-one.", "\n", "# shape: (batch_size, sequence_length, num_parents, num_relations)", "\n", "target_log_probs", "=", "encoded", ".", "new_empty", "(", "*", "parent_ids", ".", "shape", ")", ".", "fill_", "(", "math", ".", "log", "(", "1e-45", ")", ")", "\n", "for", "index", ",", "parent_id", ",", "relation_embedding", ",", "tail_id", "in", "zip", "(", "indices", ",", "parent_ids_list", ",", "relation_embeddings", ",", "tail_ids_list", ")", ":", "\n", "# First we compute the score for each relation w.r.t the current encoding, and convert", "\n", "# the scores to log-probabilities", "\n", "            ", "logits", "=", "torch", ".", "mv", "(", "relation_embedding", ",", "encoded", "[", "index", "[", ":", "-", "1", "]", "]", ")", "\n", "logger", ".", "debug", "(", "'Relation logits shape: %s'", ",", "logits", ".", "shape", ")", "\n", "log_probs", "=", "F", ".", "log_softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Next we gather the log probs for edges with the correct tail entity and sum them up", "\n", "target_id", "=", "raw_entity_ids", "[", "index", "[", ":", "-", "1", "]", "]", "\n", "mask", "=", "tail_id", ".", "eq", "(", "target_id", ")", "\n", "relevant_log_probs", "=", "log_probs", ".", "masked_select", "(", "tail_id", ".", "eq", "(", "target_id", ")", ")", "\n", "target_log_prob", "=", "torch", ".", "logsumexp", "(", "relevant_log_probs", ",", "dim", "=", "0", ")", "\n", "target_log_probs", "[", "index", "]", "=", "target_log_prob", "\n", "\n", "", "return", "target_log_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._knowledge_graph_entity_loss": [[577, 600], ["kglm_disc.KglmDisc._parent_log_probs", "kglm_disc.KglmDisc._relation_log_probs", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "kglm_disc.KglmDisc._parent_ppl", "kglm_disc.KglmDisc._relation_ppl", "parent_ids.eq().all", "mask.float", "mask.float().sum", "mask.float().sum", "[].sum", "[].sum", "torch.logsumexp.sum", "torch.logsumexp.sum", "target_mask.sum", "parent_ids.eq", "mask.float", "mask.float", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._parent_log_probs", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._relation_log_probs"], ["", "def", "_knowledge_graph_entity_loss", "(", "self", ",", "\n", "encoded_head", ":", "torch", ".", "Tensor", ",", "\n", "encoded_relation", ":", "torch", ".", "Tensor", ",", "\n", "raw_entity_ids", ":", "torch", ".", "Tensor", ",", "\n", "entity_ids", ":", "torch", ".", "Tensor", ",", "\n", "parent_ids", ":", "torch", ".", "Tensor", ",", "\n", "target_mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "# First get the log probabilities of the parents and relations that lead to the current", "\n", "# entity.", "\n", "        ", "parent_log_probs", "=", "self", ".", "_parent_log_probs", "(", "encoded_head", ",", "entity_ids", ",", "parent_ids", ")", "\n", "relation_log_probs", "=", "self", ".", "_relation_log_probs", "(", "encoded_relation", ",", "raw_entity_ids", ",", "parent_ids", ")", "\n", "# Next take their product + marginalize", "\n", "combined_log_probs", "=", "parent_log_probs", "+", "relation_log_probs", "\n", "target_log_probs", "=", "torch", ".", "logsumexp", "(", "combined_log_probs", ",", "dim", "=", "-", "1", ")", "\n", "# Zero out any non-kg predictions", "\n", "mask", "=", "~", "parent_ids", ".", "eq", "(", "0", ")", ".", "all", "(", "dim", "=", "-", "1", ")", "\n", "target_log_probs", "=", "target_log_probs", "*", "mask", ".", "float", "(", ")", "\n", "# If validating, measure ppl of the predictions:", "\n", "# if not self.training:", "\n", "self", ".", "_parent_ppl", "(", "-", "torch", ".", "logsumexp", "(", "parent_log_probs", ",", "dim", "=", "-", "1", ")", "[", "mask", "]", ".", "sum", "(", ")", ",", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", ")", "\n", "self", ".", "_relation_ppl", "(", "-", "torch", ".", "logsumexp", "(", "relation_log_probs", ",", "dim", "=", "-", "1", ")", "[", "mask", "]", ".", "sum", "(", ")", ",", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", ")", "\n", "# Lastly return the tokenwise average loss", "\n", "return", "-", "target_log_probs", ".", "sum", "(", ")", "/", "(", "target_mask", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._forward_loop": [[601, 669], ["allennlp.nn.util.get_text_field_mask", "logger.debug", "logger.debug", "logger.debug", "logger.debug", "kglm_disc.KglmDisc._encode_source", "encoded.split", "kglm_disc.KglmDisc._mention_type_loss", "kglm_disc.KglmDisc._avg_mention_type_loss", "kglm_disc.KglmDisc._avg_new_entity_loss", "kglm_disc.KglmDisc._knowledge_graph_entity_loss", "kglm_disc.KglmDisc._avg_knowledge_graph_entity_loss", "float", "kglm_disc.KglmDisc._new_entity_loss", "kglm_disc.KglmDisc._new_entity_loss", "float", "float"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._encode_source", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._mention_type_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._knowledge_graph_entity_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_loss", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc._new_entity_loss"], ["", "def", "_forward_loop", "(", "self", ",", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "alias_database", ":", "AliasDatabase", ",", "\n", "mention_type", ":", "torch", ".", "Tensor", ",", "\n", "raw_entity_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "entity_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "parent_ids", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "relations", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "shortlist", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "shortlist_inds", ":", "torch", ".", "Tensor", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "# Get the token mask and extract indexed text fields.", "\n", "# shape: (batch_size, sequence_length)", "\n", "        ", "target_mask", "=", "get_text_field_mask", "(", "source", ")", "\n", "source", "=", "source", "[", "'tokens'", "]", "\n", "raw_entity_ids", "=", "raw_entity_ids", "[", "'raw_entity_ids'", "]", "\n", "entity_ids", "=", "entity_ids", "[", "'entity_ids'", "]", "\n", "parent_ids", "=", "parent_ids", "[", "'entity_ids'", "]", "\n", "relations", "=", "relations", "[", "'relations'", "]", "\n", "\n", "logger", ".", "debug", "(", "'Source & Target shape: %s'", ",", "source", ".", "shape", ")", "\n", "logger", ".", "debug", "(", "'Entity ids shape: %s'", ",", "entity_ids", ".", "shape", ")", "\n", "logger", ".", "debug", "(", "'Relations & Parent ids shape: %s'", ",", "relations", ".", "shape", ")", "\n", "logger", ".", "debug", "(", "'Shortlist shape: %s'", ",", "shortlist", "[", "'entity_ids'", "]", ".", "shape", ")", "\n", "# Embed source tokens.", "\n", "# shape: (batch_size, sequence_length, embedding_dim)", "\n", "encoded", ",", "alpha_loss", ",", "beta_loss", "=", "self", ".", "_encode_source", "(", "source", ")", "\n", "splits", "=", "[", "self", ".", "token_embedding_dim", "]", "+", "[", "self", ".", "entity_embedding_dim", "]", "*", "2", "\n", "encoded_token", ",", "encoded_head", ",", "encoded_relation", "=", "encoded", ".", "split", "(", "splits", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# Predict whether or not the next token will be an entity mention, and if so which type.", "\n", "mention_type_loss", "=", "self", ".", "_mention_type_loss", "(", "encoded_token", ",", "mention_type", ",", "target_mask", ")", "\n", "self", ".", "_avg_mention_type_loss", "(", "float", "(", "mention_type_loss", ")", ")", "\n", "\n", "# For new mentions, predict which entity (among those in the supplied shortlist) will be", "\n", "# mentioned.", "\n", "if", "self", ".", "_use_shortlist", ":", "\n", "            ", "new_entity_loss", "=", "self", ".", "_new_entity_loss", "(", "encoded_head", "+", "encoded_relation", ",", "\n", "shortlist_inds", ",", "\n", "shortlist", ",", "\n", "target_mask", ")", "\n", "", "else", ":", "\n", "            ", "new_entity_loss", "=", "self", ".", "_new_entity_loss", "(", "encoded_head", "+", "encoded_relation", ",", "\n", "entity_ids", ",", "\n", "None", ",", "\n", "target_mask", ")", "\n", "\n", "", "self", ".", "_avg_new_entity_loss", "(", "float", "(", "new_entity_loss", ")", ")", "\n", "\n", "# For derived mentions, first predict which parent(s) to expand...", "\n", "knowledge_graph_entity_loss", "=", "self", ".", "_knowledge_graph_entity_loss", "(", "encoded_head", ",", "\n", "encoded_relation", ",", "\n", "raw_entity_ids", ",", "\n", "entity_ids", ",", "\n", "parent_ids", ",", "\n", "target_mask", ")", "\n", "self", ".", "_avg_knowledge_graph_entity_loss", "(", "float", "(", "knowledge_graph_entity_loss", ")", ")", "\n", "\n", "# Compute total loss", "\n", "loss", "=", "mention_type_loss", "+", "new_entity_loss", "+", "knowledge_graph_entity_loss", "\n", "\n", "# Activation regularization", "\n", "if", "self", ".", "_alpha", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "_alpha", "*", "alpha_loss", "\n", "# Temporal activation regularization (slowness)", "\n", "", "if", "self", ".", "_beta", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "_beta", "*", "beta_loss", "\n", "\n", "", "return", "{", "'loss'", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc.train": [[670, 678], ["super().train"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.train"], ["", "@", "overrides", "\n", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "# TODO: This is a temporary hack to ensure that the internal state resets when the model", "\n", "# switches from training to evaluation. The complication arises from potentially differing", "\n", "# batch sizes (e.g. the `reset` tensor will not be the right size). In future", "\n", "# implementations this should be handled more robustly.", "\n", "        ", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc.eval": [[679, 684], ["super().eval"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval"], ["", "@", "overrides", "\n", "def", "eval", "(", "self", ")", ":", "\n", "# TODO: See train.", "\n", "        ", "super", "(", ")", ".", "eval", "(", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_disc.KglmDisc.get_metrics": [[685, 705], ["kglm_disc.KglmDisc._new_mention_f1.get_metric", "kglm_disc.KglmDisc._kg_mention_f1.get_metric", "kglm_disc.KglmDisc._new_entity_accuracy.get_metric", "kglm_disc.KglmDisc._new_entity_accuracy20.get_metric", "kglm_disc.KglmDisc._parent_ppl.get_metric", "kglm_disc.KglmDisc._relation_ppl.get_metric", "kglm_disc.KglmDisc._avg_mention_type_loss.get_metric", "kglm_disc.KglmDisc._avg_new_entity_loss.get_metric", "kglm_disc.KglmDisc._avg_knowledge_graph_entity_loss.get_metric"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "out", "=", "{", "\n", "'type'", ":", "self", ".", "_avg_mention_type_loss", ".", "get_metric", "(", "reset", ")", ",", "\n", "'new'", ":", "self", ".", "_avg_new_entity_loss", ".", "get_metric", "(", "reset", ")", ",", "\n", "'kg'", ":", "self", ".", "_avg_knowledge_graph_entity_loss", ".", "get_metric", "(", "reset", ")", ",", "\n", "}", "\n", "# if not self.training:", "\n", "p", ",", "r", ",", "f", "=", "self", ".", "_new_mention_f1", ".", "get_metric", "(", "reset", ")", "\n", "out", "[", "'new_p'", "]", "=", "p", "\n", "out", "[", "'new_r'", "]", "=", "r", "\n", "out", "[", "'new_f1'", "]", "=", "f", "\n", "p", ",", "r", ",", "f", "=", "self", ".", "_kg_mention_f1", ".", "get_metric", "(", "reset", ")", "\n", "out", "[", "'kg_p'", "]", "=", "p", "\n", "out", "[", "'kg_r'", "]", "=", "r", "\n", "out", "[", "'kg_f1'", "]", "=", "f", "\n", "out", "[", "'new_ent_acc'", "]", "=", "self", ".", "_new_entity_accuracy", ".", "get_metric", "(", "reset", ")", "\n", "out", "[", "'new_ent_acc_20'", "]", "=", "self", ".", "_new_entity_accuracy20", ".", "get_metric", "(", "reset", ")", "\n", "out", "[", "'parent_ppl'", "]", "=", "self", ".", "_parent_ppl", ".", "get_metric", "(", "reset", ")", "\n", "out", "[", "'relation_ppl'", "]", "=", "self", ".", "_relation_ppl", ".", "get_metric", "(", "reset", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.awd_lstm.AwdLstmLanguageModel.__init__": [[44, 113], ["allennlp.nn.InitializerApplicator", "allennlp.models.Model.__init__", "kglm.modules.LockedDropout", "torch.nn.Embedding", "range", "torch.nn.ModuleList", "torch.nn.Linear", "initializer", "vocab.get_token_index", "math.log", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "vocab.get_vocab_size", "rnns.append", "kglm.modules.WeightDrop", "vocab.get_vocab_size", "vocab.get_vocab_size", "torch.nn.LSTM"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "embedding_size", ":", "int", ",", "\n", "hidden_size", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "splits", ":", "List", "[", "int", "]", "=", "[", "]", ",", "\n", "dropout", ":", "float", "=", "0.4", ",", "\n", "dropouth", ":", "float", "=", "0.3", ",", "\n", "dropouti", ":", "float", "=", "0.65", ",", "\n", "dropoute", ":", "float", "=", "0.1", ",", "\n", "wdrop", ":", "float", "=", "0.5", ",", "\n", "alpha", ":", "float", "=", "2.0", ",", "\n", "beta", ":", "float", "=", "1.0", ",", "\n", "tie_weights", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ")", "->", "None", ":", "\n", "        ", "super", "(", "AwdLstmLanguageModel", ",", "self", ")", ".", "__init__", "(", "vocab", ")", "\n", "\n", "# Model architecture", "\n", "self", ".", "embedding_size", "=", "embedding_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "tie_weights", "=", "tie_weights", "\n", "self", ".", "splits", "=", "splits", "\n", "self", ".", "alpha", "=", "alpha", "\n", "self", ".", "beta", "=", "beta", "\n", "\n", "# Dropout stuff", "\n", "self", ".", "locked_dropout", "=", "LockedDropout", "(", ")", "\n", "self", ".", "dropouti", "=", "dropouti", "\n", "self", ".", "dropouth", "=", "dropouth", "\n", "self", ".", "dropoute", "=", "dropoute", "\n", "self", ".", "dropout", "=", "dropout", "\n", "\n", "# Initialize empty state dict", "\n", "self", ".", "_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", "\n", "\n", "# Tokens are manually embedded instead of using a TokenEmbedder to make using", "\n", "# embedding_dropout easier.", "\n", "self", ".", "embedder", "=", "torch", ".", "nn", ".", "Embedding", "(", "vocab", ".", "get_vocab_size", "(", "namespace", "=", "'tokens'", ")", ",", "\n", "embedding_size", ")", "\n", "\n", "rnns", ":", "List", "[", "torch", ".", "nn", ".", "Module", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "input_size", "=", "embedding_size", "\n", "", "else", ":", "\n", "                ", "input_size", "=", "hidden_size", "\n", "", "if", "(", "i", "==", "num_layers", "-", "1", ")", "and", "tie_weights", ":", "\n", "                ", "output_size", "=", "embedding_size", "\n", "", "else", ":", "\n", "                ", "output_size", "=", "hidden_size", "\n", "", "rnns", ".", "append", "(", "torch", ".", "nn", ".", "LSTM", "(", "input_size", ",", "output_size", ",", "batch_first", "=", "True", ")", ")", "\n", "", "rnns", "=", "[", "WeightDrop", "(", "rnn", ",", "[", "'weight_hh_l0'", "]", ",", "dropout", "=", "wdrop", ")", "for", "rnn", "in", "rnns", "]", "\n", "self", ".", "rnns", "=", "torch", ".", "nn", ".", "ModuleList", "(", "rnns", ")", "\n", "\n", "self", ".", "decoder", "=", "torch", ".", "nn", ".", "Linear", "(", "output_size", ",", "vocab", ".", "get_vocab_size", "(", "namespace", "=", "'tokens'", ")", ")", "\n", "\n", "# Optionally tie weights", "\n", "if", "tie_weights", ":", "\n", "# pylint: disable=protected-access", "\n", "            ", "self", ".", "decoder", ".", "weight", "=", "self", ".", "embedder", ".", "weight", "\n", "\n", "", "initializer", "(", "self", ")", "\n", "\n", "self", ".", "_unk_index", "=", "vocab", ".", "get_token_index", "(", "DEFAULT_OOV_TOKEN", ")", "\n", "self", ".", "_unk_penalty", "=", "math", ".", "log", "(", "vocab", ".", "get_vocab_size", "(", "'tokens_unk'", ")", ")", "\n", "\n", "self", ".", "ppl", "=", "Ppl", "(", ")", "\n", "self", ".", "upp", "=", "Ppl", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.awd_lstm.AwdLstmLanguageModel.forward": [[114, 199], ["allennlp.nn.util.get_text_field_mask", "kglm.modules.embedded_dropout", "awd_lstm.AwdLstmLanguageModel.locked_dropout", "enumerate", "awd_lstm.AwdLstmLanguageModel.decoder", "allennlp.nn.util.sequence_cross_entropy_with_logits", "target.eq", "awd_lstm.AwdLstmLanguageModel.ppl", "awd_lstm.AwdLstmLanguageModel.upp", "rnn", "output.contiguous.contiguous.contiguous", "outputs.append", "tuple", "current_hidden.append", "target.contiguous", "allennlp.nn.util.get_text_field_mask.float().sum", "target.eq.float().sum", "reset.all", "logger.debug", "awd_lstm.AwdLstmLanguageModel.locked_dropout", "dropped_outputs.append", "awd_lstm.AwdLstmLanguageModel.locked_dropout", "dropped_outputs.append", "enumerate", "reset.any", "range", "h.detach", "allennlp.nn.util.get_text_field_mask.float", "awd_lstm.AwdLstmLanguageModel.pow().mean", "target.eq.float", "torch.zeros_like", "torch.zeros_like", "awd_lstm.AwdLstmLanguageModel.pow"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "target", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "reset", ":", "torch", ".", "Tensor", "=", "None", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "# THE BELOW ONLY NEEDS TO BE SATISFIED FOR THE FANCY ITERATOR, MERITY", "\n", "# ET AL JUST PROPOGATE THE HIDDEN STATE NO MATTER WHAT", "\n", "# To make life easier when evaluating the model we use a BasicIterator", "\n", "# so that we do not need to worry about the sequence truncation", "\n", "# performed by our splitting iterators. To accomodate this, we assume", "\n", "# that if reset is not given, then everything gets reset.", "\n", "        ", "if", "reset", "is", "None", ":", "\n", "            ", "self", ".", "_state", "=", "None", "\n", "", "elif", "reset", ".", "all", "(", ")", "and", "(", "self", ".", "_state", "is", "not", "None", ")", ":", "\n", "            ", "logger", ".", "debug", "(", "'RESET'", ")", "\n", "self", ".", "_state", "=", "None", "\n", "", "elif", "reset", ".", "any", "(", ")", "and", "(", "self", ".", "_state", "is", "not", "None", ")", ":", "\n", "            ", "for", "layer", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "                ", "h", ",", "c", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "h", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "h", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "c", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "c", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "=", "(", "h", ",", "c", ")", "\n", "\n", "", "", "target_mask", "=", "get_text_field_mask", "(", "target", ")", "\n", "source", "=", "source", "[", "'tokens'", "]", "\n", "target", "=", "target", "[", "'tokens'", "]", "\n", "\n", "embeddings", "=", "embedded_dropout", "(", "self", ".", "embedder", ",", "source", ",", "\n", "dropout", "=", "self", ".", "dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "embeddings", "=", "self", ".", "locked_dropout", "(", "embeddings", ",", "self", ".", "dropouti", ")", "\n", "\n", "# Iterate through RNN layers", "\n", "current_input", "=", "embeddings", "\n", "current_hidden", "=", "[", "]", "\n", "outputs", "=", "[", "]", "\n", "dropped_outputs", "=", "[", "]", "\n", "for", "layer", ",", "rnn", "in", "enumerate", "(", "self", ".", "rnns", ")", ":", "\n", "\n", "# Bookkeeping", "\n", "            ", "if", "self", ".", "_state", "is", "not", "None", ":", "\n", "                ", "prev_hidden", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "", "else", ":", "\n", "                ", "prev_hidden", "=", "None", "\n", "\n", "# Forward-pass", "\n", "", "output", ",", "hidden", "=", "rnn", "(", "current_input", ",", "prev_hidden", ")", "\n", "\n", "# More bookkeeping", "\n", "output", "=", "output", ".", "contiguous", "(", ")", "\n", "outputs", ".", "append", "(", "output", ")", "\n", "hidden", "=", "tuple", "(", "h", ".", "detach", "(", ")", "for", "h", "in", "hidden", ")", "\n", "current_hidden", ".", "append", "(", "hidden", ")", "\n", "\n", "# Apply dropout", "\n", "if", "layer", "==", "self", ".", "num_layers", "-", "1", ":", "\n", "                ", "current_input", "=", "self", ".", "locked_dropout", "(", "output", ",", "self", ".", "dropout", ")", "\n", "dropped_outputs", ".", "append", "(", "output", ")", "\n", "", "else", ":", "\n", "                ", "current_input", "=", "self", ".", "locked_dropout", "(", "output", ",", "self", ".", "dropouth", ")", "\n", "dropped_outputs", ".", "append", "(", "current_input", ")", "\n", "\n", "# Compute logits and loss", "\n", "", "", "logits", "=", "self", ".", "decoder", "(", "current_input", ")", "\n", "loss", "=", "sequence_cross_entropy_with_logits", "(", "logits", ",", "target", ".", "contiguous", "(", ")", ",", "\n", "target_mask", ",", "\n", "average", "=", "\"token\"", ")", "\n", "num_tokens", "=", "target_mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", "\n", "\n", "# Activation regularization", "\n", "if", "self", ".", "alpha", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "alpha", "*", "current_input", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "# Temporal activation regularization (slowness)", "\n", "", "if", "self", ".", "beta", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "beta", "*", "(", "output", "[", ":", ",", "1", ":", "]", "-", "output", "[", ":", ",", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "# Update metrics and state", "\n", "", "unks", "=", "target", ".", "eq", "(", "self", ".", "_unk_index", ")", "\n", "unk_penalty", "=", "self", ".", "_unk_penalty", "*", "unks", ".", "float", "(", ")", ".", "sum", "(", ")", "\n", "\n", "self", ".", "ppl", "(", "loss", "*", "num_tokens", ",", "num_tokens", ")", "\n", "self", ".", "upp", "(", "loss", "*", "num_tokens", "+", "unk_penalty", ",", "num_tokens", ")", "\n", "self", ".", "_state", "=", "{", "'layer_%i'", "%", "l", ":", "h", "for", "l", ",", "h", "in", "enumerate", "(", "current_hidden", ")", "}", "\n", "\n", "return", "{", "'loss'", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.awd_lstm.AwdLstmLanguageModel.get_metrics": [[200, 204], ["awd_lstm.AwdLstmLanguageModel.ppl.get_metric", "awd_lstm.AwdLstmLanguageModel.upp.get_metric"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "{", "\n", "'ppl'", ":", "self", ".", "ppl", ".", "get_metric", "(", "reset", ")", ",", "\n", "'upp'", ":", "self", ".", "upp", ".", "get_metric", "(", "reset", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.__init__": [[57, 97], ["allennlp.nn.InitializerApplicator", "allennlp.models.Model.__init__", "allennlp.modules.input_variational_dropout.InputVariationalDropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "kglm.modules.DynamicEmbedding", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "allennlp.training.metrics.CategoricalAccuracy", "initializer"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "text_field_embedder", ":", "TextFieldEmbedder", ",", "\n", "encoder", ":", "Seq2SeqEncoder", ",", "\n", "embedding_dim", ":", "int", ",", "\n", "max_mention_length", ":", "int", ",", "\n", "max_embeddings", ":", "int", ",", "\n", "variational_dropout_rate", ":", "float", "=", "0.0", ",", "\n", "dropout_rate", ":", "float", "=", "0.0", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ")", "->", "None", ":", "\n", "        ", "super", "(", "EntityNLMDiscriminator", ",", "self", ")", ".", "__init__", "(", "vocab", ")", "\n", "\n", "self", ".", "_text_field_embedder", "=", "text_field_embedder", "\n", "self", ".", "_encoder", "=", "encoder", "\n", "self", ".", "_embedding_dim", "=", "embedding_dim", "\n", "self", ".", "_max_mention_length", "=", "max_mention_length", "\n", "self", ".", "_max_embeddings", "=", "max_embeddings", "\n", "\n", "self", ".", "_state", ":", "Optional", "[", "StateDict", "]", "=", "None", "\n", "\n", "# Input variational dropout", "\n", "self", ".", "_variational_dropout", "=", "InputVariationalDropout", "(", "variational_dropout_rate", ")", "\n", "self", ".", "_dropout", "=", "torch", ".", "nn", ".", "Dropout", "(", "dropout_rate", ")", "\n", "\n", "# For entity type prediction", "\n", "self", ".", "_entity_type_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "2", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "_dynamic_embeddings", "=", "DynamicEmbedding", "(", "embedding_dim", "=", "embedding_dim", ",", "\n", "max_embeddings", "=", "max_embeddings", ")", "\n", "\n", "# For mention length prediction", "\n", "self", ".", "_mention_length_projection", "=", "torch", ".", "nn", ".", "Linear", "(", "in_features", "=", "2", "*", "embedding_dim", ",", "\n", "out_features", "=", "max_mention_length", ")", "\n", "\n", "self", ".", "_entity_type_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "self", ".", "_entity_id_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "self", ".", "_mention_length_accuracy", "=", "CategoricalAccuracy", "(", ")", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.forward": [[98, 148], ["entity_disc.EntityNLMDiscriminator.reset_states", "entity_disc.EntityNLMDiscriminator.detach_states", "entity_disc.EntityNLMDiscriminator._forward_loop"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.detach_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._forward_loop"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "tokens", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "entity_types", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "entity_ids", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "mention_lengths", ":", "Optional", "[", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Computes the loss during training / validation.\n\n        Parameters\n        ----------\n        tokens : ``Dict[str, torch.Tensor]``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the sequence of\n            tokens.\n        entity_types : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` indicating whether or not the\n            corresponding token belongs to a mention.\n        entity_ids : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the ids of the\n            entities the corresponding token is mentioning.\n        mention_lengths : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` tracking how many remaining\n            tokens (including the current one) there are in the mention.\n        reset : ``bool``\n            Whether or not to reset the model's state. This should be done at the start of each\n            new sequence.\n\n        Returns\n        -------\n        An output dictionary consisting of:\n        loss : ``torch.Tensor``\n            The combined loss.\n        \"\"\"", "\n", "batch_size", "=", "tokens", "[", "'tokens'", "]", ".", "shape", "[", "0", "]", "\n", "\n", "if", "reset", ":", "\n", "            ", "self", ".", "reset_states", "(", "batch_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "detach_states", "(", ")", "\n", "\n", "", "if", "entity_types", "is", "not", "None", ":", "\n", "            ", "output_dict", "=", "self", ".", "_forward_loop", "(", "tokens", "=", "tokens", ",", "\n", "entity_types", "=", "entity_types", ",", "\n", "entity_ids", "=", "entity_ids", ",", "\n", "mention_lengths", "=", "mention_lengths", ")", "\n", "", "else", ":", "\n", "            ", "output_dict", "=", "{", "}", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.sample": [[149, 273], ["entity_disc.EntityNLMDiscriminator.reset_states", "allennlp.nn.util.get_text_field_mask", "entity_disc.EntityNLMDiscriminator._text_field_embedder", "entity_disc.EntityNLMDiscriminator._encoder", "tokens[].new_ones", "entity_disc.EntityNLMDiscriminator.new_zeros", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "range", "mask[].byte", "predict_mask.sum", "entity_disc.EntityNLMDiscriminator._entity_type_projection", "torch.log_softmax", "torch.log_softmax", "kglm.nn.util.sample_from_logp", "entity_type_predictions.byte.byte.byte", "entity_disc.EntityNLMDiscriminator._dynamic_embeddings.add_embeddings", "entity_disc.EntityNLMDiscriminator._dynamic_embeddings.update_embeddings", "mask[].byte", "deterministic_mask.sum", "predict_em.sum", "entity_disc.EntityNLMDiscriminator._dynamic_embeddings", "torch.log_softmax", "torch.log_softmax", "kglm.nn.util.sample_from_logp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "entity_disc.EntityNLMDiscriminator._mention_length_projection", "torch.log_softmax", "torch.log_softmax", "kglm.nn.util.sample_from_logp"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.sample_from_logp", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.add_embeddings", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.update_embeddings", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.sample_from_logp", "home.repos.pwc.inspect_result.rloganiv_kglm-model.nn.util.sample_from_logp"], ["", "def", "sample", "(", "self", ",", "\n", "tokens", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Generates a sample from the discriminative model.\n\n        WARNING: Unlike during training, this function expects the full (unsplit) sequence of\n        tokens.\n\n        Parameters\n        ----------\n        tokens : ``Dict[str, torch.Tensor]``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the sequence of\n            tokens.\n\n        Returns\n        -------\n        An output dictionary consisting of:\n        logp : ``torch.Tensor``\n            A tensor containing the log-probability of the sample (averaged over time)\n        entity_types : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` indicating whether or not the\n            corresponding token belongs to a mention.\n        entity_ids : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the ids of the\n            entities the corresponding token is mentioning.\n        mention_lengths : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` tracking how many remaining\n            tokens (including the current one) there are in the mention.\n        \"\"\"", "\n", "batch_size", ",", "sequence_length", "=", "tokens", "[", "'tokens'", "]", ".", "shape", "\n", "\n", "# We will use a standard iterator during evaluation instead of a split iterator. Otherwise", "\n", "# it will be a pain to handle generating multiple samples for a sequence since there's no", "\n", "# way to get back to the first split.", "\n", "self", ".", "reset_states", "(", "batch_size", ")", "\n", "\n", "# Embed tokens and get RNN hidden state.", "\n", "mask", "=", "get_text_field_mask", "(", "tokens", ")", "\n", "embeddings", "=", "self", ".", "_text_field_embedder", "(", "tokens", ")", "\n", "hidden", "=", "self", ".", "_encoder", "(", "embeddings", ",", "mask", ")", "\n", "prev_mention_lengths", "=", "tokens", "[", "'tokens'", "]", ".", "new_ones", "(", "batch_size", ")", "\n", "\n", "# Initialize outputs", "\n", "logp", "=", "hidden", ".", "new_zeros", "(", "batch_size", ")", "# Track total logp for **each** generated sample", "\n", "entity_types", "=", "torch", ".", "zeros_like", "(", "tokens", "[", "'tokens'", "]", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "entity_ids", "=", "torch", ".", "zeros_like", "(", "tokens", "[", "'tokens'", "]", ")", "\n", "mention_lengths", "=", "torch", ".", "ones_like", "(", "tokens", "[", "'tokens'", "]", ")", "\n", "\n", "# Generate outputs", "\n", "for", "timestep", "in", "range", "(", "sequence_length", ")", ":", "\n", "\n", "            ", "current_hidden", "=", "hidden", "[", ":", ",", "timestep", "]", "\n", "\n", "# We only predict types / ids / lengths if the previous mention is terminated.", "\n", "predict_mask", "=", "prev_mention_lengths", "==", "1", "\n", "predict_mask", "=", "predict_mask", "*", "mask", "[", ":", ",", "timestep", "]", ".", "byte", "(", ")", "\n", "if", "predict_mask", ".", "sum", "(", ")", ">", "0", ":", "\n", "\n", "# Predict entity types", "\n", "                ", "entity_type_logits", "=", "self", ".", "_entity_type_projection", "(", "current_hidden", "[", "predict_mask", "]", ")", "\n", "entity_type_logp", "=", "F", ".", "log_softmax", "(", "entity_type_logits", ",", "dim", "=", "-", "1", ")", "\n", "entity_type_prediction_logp", ",", "entity_type_predictions", "=", "sample_from_logp", "(", "entity_type_logp", ")", "\n", "entity_type_predictions", "=", "entity_type_predictions", ".", "byte", "(", ")", "\n", "entity_types", "[", "predict_mask", ",", "timestep", "]", "=", "entity_type_predictions", "\n", "logp", "[", "predict_mask", "]", "+=", "entity_type_prediction_logp", "\n", "\n", "# Only predict entity and mention lengths if we predicted that there was a mention", "\n", "predict_em", "=", "entity_types", "[", ":", ",", "timestep", "]", "*", "predict_mask", "\n", "if", "predict_em", ".", "sum", "(", ")", ">", "0", ":", "\n", "# Predict entity ids", "\n", "                    ", "entity_id_prediction_outputs", "=", "self", ".", "_dynamic_embeddings", "(", "hidden", "=", "current_hidden", ",", "\n", "mask", "=", "predict_em", ")", "\n", "entity_id_logits", "=", "entity_id_prediction_outputs", "[", "'logits'", "]", "\n", "entity_id_logp", "=", "F", ".", "log_softmax", "(", "entity_id_logits", ",", "dim", "=", "-", "1", ")", "\n", "entity_id_prediction_logp", ",", "entity_id_predictions", "=", "sample_from_logp", "(", "entity_id_logp", ")", "\n", "\n", "# Predict mention lengths - we do this before writing the", "\n", "# entity id predictions since we'll need to reindex the new", "\n", "# entities, but need the null embeddings here.", "\n", "predicted_entity_embeddings", "=", "self", ".", "_dynamic_embeddings", ".", "embeddings", "[", "predict_em", ",", "entity_id_predictions", "]", "\n", "concatenated", "=", "torch", ".", "cat", "(", "(", "current_hidden", "[", "predict_em", "]", ",", "predicted_entity_embeddings", ")", ",", "dim", "=", "-", "1", ")", "\n", "mention_length_logits", "=", "self", ".", "_mention_length_projection", "(", "concatenated", ")", "\n", "mention_length_logp", "=", "F", ".", "log_softmax", "(", "mention_length_logits", ",", "dim", "=", "-", "1", ")", "\n", "mention_length_prediction_logp", ",", "mention_length_predictions", "=", "sample_from_logp", "(", "mention_length_logp", ")", "\n", "\n", "# Write predictions", "\n", "new_entity_mask", "=", "entity_id_predictions", "==", "0", "\n", "new_entity_labels", "=", "self", ".", "_dynamic_embeddings", ".", "num_embeddings", "[", "predict_em", "]", "\n", "entity_id_predictions", "[", "new_entity_mask", "]", "=", "new_entity_labels", "[", "new_entity_mask", "]", "\n", "entity_ids", "[", "predict_em", ",", "timestep", "]", "=", "entity_id_predictions", "\n", "logp", "[", "predict_em", "]", "+=", "entity_id_prediction_logp", "\n", "\n", "mention_lengths", "[", "predict_em", ",", "timestep", "]", "=", "mention_length_predictions", "\n", "logp", "[", "predict_em", "]", "+=", "mention_length_prediction_logp", "\n", "\n", "# Add / update entity embeddings", "\n", "", "new_entities", "=", "entity_ids", "[", ":", ",", "timestep", "]", "==", "self", ".", "_dynamic_embeddings", ".", "num_embeddings", "\n", "self", ".", "_dynamic_embeddings", ".", "add_embeddings", "(", "timestep", ",", "new_entities", ")", "\n", "\n", "self", ".", "_dynamic_embeddings", ".", "update_embeddings", "(", "hidden", "=", "current_hidden", ",", "\n", "update_indices", "=", "entity_ids", "[", ":", ",", "timestep", "]", ",", "\n", "timestep", "=", "timestep", ",", "\n", "mask", "=", "predict_em", ")", "\n", "\n", "# If the previous mentions are ongoing, we assign the output deterministically. Mention", "\n", "# lengths decrease by 1, all other outputs are copied from the previous timestep. Do", "\n", "# not need to add anything to logp since these 'predictions' have probability 1 under", "\n", "# the model.", "\n", "", "deterministic_mask", "=", "prev_mention_lengths", ">", "1", "\n", "deterministic_mask", "=", "deterministic_mask", "*", "mask", "[", ":", ",", "timestep", "]", ".", "byte", "(", ")", "\n", "if", "deterministic_mask", ".", "sum", "(", ")", ">", "1", ":", "\n", "                ", "entity_types", "[", "deterministic_mask", ",", "timestep", "]", "=", "entity_types", "[", "deterministic_mask", ",", "timestep", "-", "1", "]", "\n", "entity_ids", "[", "deterministic_mask", ",", "timestep", "]", "=", "entity_ids", "[", "deterministic_mask", ",", "timestep", "-", "1", "]", "\n", "mention_lengths", "[", "deterministic_mask", ",", "timestep", "]", "=", "mention_lengths", "[", "deterministic_mask", ",", "timestep", "-", "1", "]", "-", "1", "\n", "\n", "# Update mention lengths for next timestep", "\n", "", "prev_mention_lengths", "=", "mention_lengths", "[", ":", ",", "timestep", "]", "\n", "\n", "", "return", "{", "\n", "'logp'", ":", "logp", ",", "\n", "'sample'", ":", "{", "\n", "'entity_types'", ":", "entity_types", ",", "\n", "'entity_ids'", ":", "entity_ids", ",", "\n", "'mention_lengths'", ":", "mention_lengths", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator._forward_loop": [[276, 414], ["allennlp.nn.util.get_text_field_mask", "entity_disc.EntityNLMDiscriminator._text_field_embedder", "entity_disc.EntityNLMDiscriminator._variational_dropout", "entity_disc.EntityNLMDiscriminator._encoder", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "range", "mention_lengths.new_ones", "entity_disc.EntityNLMDiscriminator._dropout", "entity_disc.EntityNLMDiscriminator._dynamic_embeddings.add_embeddings", "entity_disc.EntityNLMDiscriminator._dynamic_embeddings.update_embeddings", "allennlp.nn.util.get_text_field_mask.sum", "allennlp.nn.util.get_text_field_mask.sum", "allennlp.nn.util.get_text_field_mask.sum", "mention_lengths.new_ones.detach", "mask[].byte", "predict_all.sum", "entity_disc.EntityNLMDiscriminator._entity_type_projection", "entity_disc.EntityNLMDiscriminator._entity_type_accuracy", "torch.cross_entropy", "torch.cross_entropy", "predict_em.sum", "current_entity_ids.clone", "entity_disc.EntityNLMDiscriminator._dynamic_embeddings", "entity_disc.EntityNLMDiscriminator._entity_id_accuracy", "entity_disc.EntityNLMDiscriminator._dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "entity_disc.EntityNLMDiscriminator._mention_length_projection", "entity_disc.EntityNLMDiscriminator._mention_length_accuracy", "current_entity_types[].long", "current_entity_types[].long", "entity_id_prediction_outputs[].sum", "torch.cross_entropy", "torch.cross_entropy"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.add_embeddings", "home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.dynamic_embeddings.DynamicEmbedding.update_embeddings"], ["", "def", "_forward_loop", "(", "self", ",", "\n", "tokens", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "entity_types", ":", "torch", ".", "Tensor", ",", "\n", "entity_ids", ":", "torch", ".", "Tensor", ",", "\n", "mention_lengths", ":", "torch", ".", "Tensor", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"\n        Performs the forward pass to calculate the loss on a chunk of training data.\n\n        Parameters\n        ----------\n        tokens : ``Dict[str, torch.Tensor]``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the sequence of\n            tokens.\n        entity_types : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` indicating whether or not the\n            corresponding token belongs to a mention.\n        entity_ids : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` containing the ids of the\n            entities the corresponding token is mentioning.\n        mention_lengths : ``torch.Tensor``\n            A tensor of shape ``(batch_size, sequence_length)`` tracking how many remaining\n            tokens (including the current one) there are in the mention.\n\n        Returns\n        -------\n        An output dictionary consisting of:\n        entity_type_loss : ``torch.Tensor``\n            The loss of entity type predictions.\n        entity_id_loss : ``torch.Tensor``\n            The loss of entity id predictions.\n        mention_length_loss : ``torch.Tensor``\n            The loss of mention length predictions.\n        loss : ``torch.Tensor``\n            The combined loss.\n        \"\"\"", "\n", "batch_size", ",", "sequence_length", "=", "tokens", "[", "'tokens'", "]", ".", "shape", "\n", "\n", "# Need to track previous mention lengths in order to know when to measure loss.", "\n", "if", "self", ".", "_state", "is", "None", ":", "\n", "            ", "prev_mention_lengths", "=", "mention_lengths", ".", "new_ones", "(", "batch_size", ")", "\n", "", "else", ":", "\n", "            ", "prev_mention_lengths", "=", "self", ".", "_state", "[", "'prev_mention_lengths'", "]", "\n", "\n", "# Embed tokens and get RNN hidden state.", "\n", "", "mask", "=", "get_text_field_mask", "(", "tokens", ")", "\n", "embeddings", "=", "self", ".", "_text_field_embedder", "(", "tokens", ")", "\n", "embeddings", "=", "self", ".", "_variational_dropout", "(", "embeddings", ")", "\n", "hidden", "=", "self", ".", "_encoder", "(", "embeddings", ",", "mask", ")", "\n", "\n", "# Initialize losses", "\n", "entity_type_loss", "=", "torch", ".", "tensor", "(", "0.0", ",", "requires_grad", "=", "True", ",", "device", "=", "hidden", ".", "device", ")", "\n", "entity_id_loss", "=", "torch", ".", "tensor", "(", "0.0", ",", "requires_grad", "=", "True", ",", "device", "=", "hidden", ".", "device", ")", "\n", "mention_length_loss", "=", "torch", ".", "tensor", "(", "0.0", ",", "requires_grad", "=", "True", ",", "device", "=", "hidden", ".", "device", ")", "\n", "\n", "for", "timestep", "in", "range", "(", "sequence_length", ")", ":", "\n", "\n", "            ", "current_entity_types", "=", "entity_types", "[", ":", ",", "timestep", "]", "\n", "current_entity_ids", "=", "entity_ids", "[", ":", ",", "timestep", "]", "\n", "current_mention_lengths", "=", "mention_lengths", "[", ":", ",", "timestep", "]", "\n", "current_hidden", "=", "hidden", "[", ":", ",", "timestep", "]", "\n", "current_hidden", "=", "self", ".", "_dropout", "(", "hidden", "[", ":", ",", "timestep", "]", ")", "\n", "\n", "# We only predict types / ids / lengths if we are not currently in the process of", "\n", "# generating a mention (e.g. if the previous remaining mention length is 1). Indexing /", "\n", "# masking with ``predict_all`` makes it possible to do this in batch.", "\n", "predict_all", "=", "prev_mention_lengths", "==", "1", "\n", "predict_all", "=", "predict_all", "*", "mask", "[", ":", ",", "timestep", "]", ".", "byte", "(", ")", "\n", "if", "predict_all", ".", "sum", "(", ")", ">", "0", ":", "\n", "\n", "# Equation 3 in the paper.", "\n", "                ", "entity_type_logits", "=", "self", ".", "_entity_type_projection", "(", "current_hidden", "[", "predict_all", "]", ")", "\n", "entity_type_loss", "=", "entity_type_loss", "+", "F", ".", "cross_entropy", "(", "\n", "entity_type_logits", ",", "\n", "current_entity_types", "[", "predict_all", "]", ".", "long", "(", ")", ",", "\n", "reduction", "=", "'sum'", ")", "\n", "self", ".", "_entity_type_accuracy", "(", "predictions", "=", "entity_type_logits", ",", "\n", "gold_labels", "=", "current_entity_types", "[", "predict_all", "]", ".", "long", "(", ")", ")", "\n", "\n", "# Only proceed to predict entity and mention length if there is in fact an entity.", "\n", "predict_em", "=", "current_entity_types", "*", "predict_all", "\n", "\n", "if", "predict_em", ".", "sum", "(", ")", ">", "0", ":", "\n", "# Equation 4 in the paper. We want new entities to correspond to a prediction of", "\n", "# zero, their embedding should be added after they've been predicted for the first", "\n", "# time.", "\n", "                    ", "modified_entity_ids", "=", "current_entity_ids", ".", "clone", "(", ")", "\n", "modified_entity_ids", "[", "modified_entity_ids", "==", "self", ".", "_dynamic_embeddings", ".", "num_embeddings", "]", "=", "0", "\n", "entity_id_prediction_outputs", "=", "self", ".", "_dynamic_embeddings", "(", "hidden", "=", "current_hidden", ",", "\n", "target", "=", "modified_entity_ids", ",", "\n", "mask", "=", "predict_em", ")", "\n", "entity_id_loss", "=", "entity_id_loss", "+", "entity_id_prediction_outputs", "[", "'loss'", "]", ".", "sum", "(", ")", "\n", "self", ".", "_entity_id_accuracy", "(", "predictions", "=", "entity_id_prediction_outputs", "[", "'logits'", "]", ",", "\n", "gold_labels", "=", "modified_entity_ids", "[", "predict_em", "]", ")", "\n", "\n", "# Equation 5 in the paper.", "\n", "predicted_entity_embeddings", "=", "self", ".", "_dynamic_embeddings", ".", "embeddings", "[", "predict_em", ",", "modified_entity_ids", "[", "predict_em", "]", "]", "\n", "predicted_entity_embeddings", "=", "self", ".", "_dropout", "(", "predicted_entity_embeddings", ")", "\n", "concatenated", "=", "torch", ".", "cat", "(", "(", "current_hidden", "[", "predict_em", "]", ",", "predicted_entity_embeddings", ")", ",", "dim", "=", "-", "1", ")", "\n", "mention_length_logits", "=", "self", ".", "_mention_length_projection", "(", "concatenated", ")", "\n", "mention_length_loss", "=", "mention_length_loss", "+", "F", ".", "cross_entropy", "(", "\n", "mention_length_logits", ",", "\n", "current_mention_lengths", "[", "predict_em", "]", ")", "\n", "self", ".", "_mention_length_accuracy", "(", "predictions", "=", "mention_length_logits", ",", "\n", "gold_labels", "=", "current_mention_lengths", "[", "predict_em", "]", ")", "\n", "\n", "# We add new entities to any sequence where the current entity id matches the number of", "\n", "# embeddings that currently exist for that sequence (this means we need a new one since", "\n", "# there is an additional dummy embedding).", "\n", "", "", "new_entities", "=", "current_entity_ids", "==", "self", ".", "_dynamic_embeddings", ".", "num_embeddings", "\n", "self", ".", "_dynamic_embeddings", ".", "add_embeddings", "(", "timestep", ",", "new_entities", ")", "\n", "\n", "# We also perform updates of the currently observed entities.", "\n", "self", ".", "_dynamic_embeddings", ".", "update_embeddings", "(", "hidden", "=", "current_hidden", ",", "\n", "update_indices", "=", "current_entity_ids", ",", "\n", "timestep", "=", "timestep", ",", "\n", "mask", "=", "current_entity_types", ")", "\n", "\n", "prev_mention_lengths", "=", "current_mention_lengths", "\n", "\n", "# Normalize the losses", "\n", "", "entity_type_loss", "=", "entity_type_loss", "/", "mask", ".", "sum", "(", ")", "\n", "entity_id_loss", "=", "entity_id_loss", "/", "mask", ".", "sum", "(", ")", "\n", "mention_length_loss", "=", "mention_length_loss", "/", "mask", ".", "sum", "(", ")", "\n", "total_loss", "=", "entity_type_loss", "+", "entity_id_loss", "+", "mention_length_loss", "\n", "\n", "output_dict", "=", "{", "\n", "'entity_type_loss'", ":", "entity_type_loss", ",", "\n", "'entity_id_loss'", ":", "entity_id_loss", ",", "\n", "'mention_length_loss'", ":", "mention_length_loss", ",", "\n", "'loss'", ":", "total_loss", "\n", "}", "\n", "\n", "# Update state", "\n", "self", ".", "_state", "=", "{", "\n", "'prev_mention_lengths'", ":", "prev_mention_lengths", ".", "detach", "(", ")", "\n", "}", "\n", "\n", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states": [[415, 420], ["entity_disc.EntityNLMDiscriminator._encoder.reset_states", "entity_disc.EntityNLMDiscriminator._dynamic_embeddings.reset_states"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states"], ["", "def", "reset_states", "(", "self", ",", "batch_size", ":", "int", ")", "->", "None", ":", "\n", "        ", "\"\"\"Resets the model's internals. Should be called at the start of a new batch.\"\"\"", "\n", "self", ".", "_encoder", ".", "reset_states", "(", ")", "\n", "self", ".", "_dynamic_embeddings", ".", "reset_states", "(", "batch_size", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.detach_states": [[421, 424], ["entity_disc.EntityNLMDiscriminator._dynamic_embeddings.detach_states"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.detach_states"], ["", "def", "detach_states", "(", "self", ")", ":", "\n", "        ", "\"\"\"Detaches the model's state to enforce truncated backpropagation.\"\"\"", "\n", "self", ".", "_dynamic_embeddings", ".", "detach_states", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.get_metrics": [[425, 431], ["entity_disc.EntityNLMDiscriminator._entity_type_accuracy.get_metric", "entity_disc.EntityNLMDiscriminator._entity_id_accuracy.get_metric", "entity_disc.EntityNLMDiscriminator._mention_length_accuracy.get_metric"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric"], ["", "@", "overrides", "\n", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "{", "\n", "'et_acc'", ":", "self", ".", "_entity_type_accuracy", ".", "get_metric", "(", "reset", ")", ",", "\n", "'eid_acc'", ":", "self", ".", "_entity_id_accuracy", ".", "get_metric", "(", "reset", ")", ",", "\n", "'ml_acc'", ":", "self", ".", "_mention_length_accuracy", ".", "get_metric", "(", "reset", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.__init__": [[38, 136], ["allennlp.nn.InitializerApplicator", "allennlp.models.Model.__init__", "kglm.modules.LockedDropout", "entity_embedder.get_output_dim", "entity_embedder.get_output_dim", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "vocab.get_token_index", "math.log", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "kglm.training.metrics.Ppl", "initializer", "rnns.append", "kglm.modules.WeightDrop", "vocab.get_vocab_size", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "torch.nn.LSTM", "vocab.get_vocab_size"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "vocab", ":", "Vocabulary", ",", "\n", "token_embedder", ":", "TextFieldEmbedder", ",", "\n", "entity_embedder", ":", "TextFieldEmbedder", ",", "\n", "alias_encoder", ":", "Seq2SeqEncoder", ",", "\n", "hidden_size", ":", "int", ",", "\n", "num_layers", ":", "int", ",", "\n", "dropout", ":", "float", "=", "0.4", ",", "\n", "dropouth", ":", "float", "=", "0.3", ",", "\n", "dropouti", ":", "float", "=", "0.65", ",", "\n", "dropoute", ":", "float", "=", "0.1", ",", "\n", "wdrop", ":", "float", "=", "0.5", ",", "\n", "alpha", ":", "float", "=", "2.0", ",", "\n", "beta", ":", "float", "=", "1.0", ",", "\n", "tie_weights", ":", "bool", "=", "False", ",", "\n", "initializer", ":", "InitializerApplicator", "=", "InitializerApplicator", "(", ")", ")", "->", "None", ":", "\n", "        ", "super", "(", "AliasCopynet", ",", "self", ")", ".", "__init__", "(", "vocab", ")", "\n", "\n", "# Model architecture - Note: we need to extract the `Embedding` layers from the", "\n", "# `TokenEmbedders` to apply dropout later on.", "\n", "# pylint: disable=protected-access", "\n", "self", ".", "_token_embedder", "=", "token_embedder", ".", "_token_embedders", "[", "'tokens'", "]", "\n", "self", ".", "_entity_embedder", "=", "entity_embedder", ".", "_token_embedders", "[", "'entity_ids'", "]", "\n", "self", ".", "_alias_encoder", "=", "alias_encoder", "\n", "self", ".", "_hidden_size", "=", "hidden_size", "\n", "self", ".", "_num_layers", "=", "num_layers", "\n", "self", ".", "_tie_weights", "=", "tie_weights", "\n", "\n", "# Dropout", "\n", "self", ".", "_locked_dropout", "=", "LockedDropout", "(", ")", "\n", "self", ".", "_dropout", "=", "dropout", "\n", "self", ".", "_dropouth", "=", "dropouth", "\n", "self", ".", "_dropouti", "=", "dropouti", "\n", "self", ".", "_dropoute", "=", "dropoute", "\n", "self", ".", "_wdrop", "=", "wdrop", "\n", "\n", "# Regularization strength", "\n", "self", ".", "_alpha", "=", "alpha", "\n", "self", ".", "_beta", "=", "beta", "\n", "\n", "# RNN Encoders. TODO: Experiment with seperate encoder for aliases.", "\n", "entity_embedding_dim", "=", "entity_embedder", ".", "get_output_dim", "(", ")", "\n", "token_embedding_dim", "=", "entity_embedder", ".", "get_output_dim", "(", ")", "\n", "assert", "entity_embedding_dim", "==", "token_embedding_dim", "\n", "embedding_dim", "=", "token_embedding_dim", "\n", "\n", "rnns", ":", "List", "[", "torch", ".", "nn", ".", "Module", "]", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "if", "i", "==", "0", ":", "\n", "                ", "input_size", "=", "token_embedding_dim", "\n", "", "else", ":", "\n", "                ", "input_size", "=", "hidden_size", "\n", "", "if", "(", "i", "==", "num_layers", "-", "1", ")", "and", "tie_weights", ":", "\n", "                ", "output_size", "=", "token_embedding_dim", "\n", "", "else", ":", "\n", "                ", "output_size", "=", "hidden_size", "\n", "", "rnns", ".", "append", "(", "torch", ".", "nn", ".", "LSTM", "(", "input_size", ",", "output_size", ",", "batch_first", "=", "True", ")", ")", "\n", "", "rnns", "=", "[", "WeightDrop", "(", "rnn", ",", "[", "'weight_hh_l0'", "]", ",", "dropout", "=", "wdrop", ")", "for", "rnn", "in", "rnns", "]", "\n", "self", ".", "rnns", "=", "torch", ".", "nn", ".", "ModuleList", "(", "rnns", ")", "\n", "\n", "# Various linear transformations.", "\n", "self", ".", "_fc_mention", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "2", ")", "\n", "\n", "self", ".", "_fc_entity", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "embedding_dim", ")", "\n", "\n", "self", ".", "_fc_condense", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "2", "*", "embedding_dim", ",", "\n", "out_features", "=", "embedding_dim", ")", "\n", "\n", "self", ".", "_fc_generate", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "vocab", ".", "get_vocab_size", "(", "'tokens'", ")", ")", "\n", "\n", "self", ".", "_fc_copy", "=", "torch", ".", "nn", ".", "Linear", "(", "\n", "in_features", "=", "embedding_dim", ",", "\n", "out_features", "=", "embedding_dim", ")", "\n", "\n", "if", "tie_weights", ":", "\n", "            ", "self", ".", "_fc_generate", ".", "weight", "=", "self", ".", "_token_embedder", ".", "weight", "\n", "\n", "", "self", ".", "_state", ":", "Optional", "[", "Dict", "[", "str", ",", "Any", "]", "]", "=", "None", "\n", "\n", "# Metrics", "\n", "# self._avg_mention_loss = Average()", "\n", "# self._avg_entity_loss = Average()", "\n", "# self._avg_vocab_loss = Average()", "\n", "self", ".", "_unk_index", "=", "vocab", ".", "get_token_index", "(", "DEFAULT_OOV_TOKEN", ")", "\n", "self", ".", "_unk_penalty", "=", "math", ".", "log", "(", "vocab", ".", "get_vocab_size", "(", "'tokens_unk'", ")", ")", "\n", "self", ".", "_ppl", "=", "Ppl", "(", ")", "\n", "self", ".", "_upp", "=", "Ppl", "(", ")", "\n", "self", ".", "_kg_ppl", "=", "Ppl", "(", ")", "# Knowledge-graph ppl", "\n", "self", ".", "_bg_ppl", "=", "Ppl", "(", ")", "# Background ppl", "\n", "\n", "initializer", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.forward": [[137, 174], ["reset.any", "range", "simplified.AliasCopynet._forward_loop", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._forward_loop"], ["", "@", "overrides", "\n", "def", "forward", "(", "self", ",", "# pylint: disable=arguments-differ", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "target", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "reset", ":", "torch", ".", "Tensor", ",", "\n", "entity_ids", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "shortlist", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", "=", "None", ",", "\n", "shortlist_inds", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "alias_copy_inds", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "alias_tokens", ":", "torch", ".", "Tensor", "=", "None", ",", "\n", "alias_inds", ":", "torch", ".", "Tensor", "=", "None", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "alias_tokens", "=", "alias_tokens", "[", "'tokens'", "]", "\n", "# Inds have fixed size and don't get truncated on split so truncate", "\n", "# now.", "\n", "alias_inds", "=", "alias_inds", "[", ":", ",", ":", ",", ":", "alias_tokens", ".", "shape", "[", "2", "]", "]", "\n", "\n", "# Reset the model if needed", "\n", "if", "reset", ".", "any", "(", ")", "and", "(", "self", ".", "_state", "is", "not", "None", ")", ":", "\n", "            ", "for", "layer", "in", "range", "(", "self", ".", "_num_layers", ")", ":", "\n", "                ", "h", ",", "c", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "h", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "h", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "c", "[", ":", ",", "reset", ",", ":", "]", "=", "torch", ".", "zeros_like", "(", "c", "[", ":", ",", "reset", ",", ":", "]", ")", "\n", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "=", "(", "h", ",", "c", ")", "\n", "\n", "", "", "if", "entity_ids", "is", "not", "None", ":", "\n", "            ", "output_dict", "=", "self", ".", "_forward_loop", "(", "\n", "source", "=", "source", ",", "\n", "target", "=", "target", ",", "\n", "alias_copy_inds", "=", "alias_copy_inds", ",", "\n", "alias_tokens", "=", "alias_tokens", ",", "\n", "alias_inds", "=", "alias_inds", ")", "\n", "", "else", ":", "\n", "# TODO: Figure out what we want here - probably to do some king of inference on", "\n", "# entities / mention types.", "\n", "            ", "output_dict", "=", "{", "}", "\n", "\n", "", "return", "output_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._mention_loss": [[175, 187], ["simplified.AliasCopynet._fc_mention", "allennlp.nn.util.sequence_cross_entropy_with_logits"], "methods", ["None"], ["", "def", "_mention_loss", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "targets", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Computes the loss for predicting whether or not the the next token will be part of an\n        entity mention.\n        \"\"\"", "\n", "logits", "=", "self", ".", "_fc_mention", "(", "encoded", ")", "\n", "mention_loss", "=", "sequence_cross_entropy_with_logits", "(", "logits", ",", "targets", ",", "mask", ",", "\n", "average", "=", "'token'", ")", "\n", "return", "mention_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._entity_loss": [[188, 215], ["simplified.AliasCopynet._fc_entity", "simplified.AliasCopynet._locked_dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "range", "shortlist_embeddings.transpose", "torch.cross_entropy", "torch.cross_entropy", "mask.float().sum", "shortlist_mask[].float", "mask.float"], "methods", ["None"], ["", "def", "_entity_loss", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "targets", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", "shortlist_embeddings", ":", "torch", ".", "Tensor", ",", "\n", "shortlist_mask", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "# Logits are computed using a bilinear form that measures the similarity between the", "\n", "# projected hidden state and the embeddings of entities in the shortlist", "\n", "        ", "projected", "=", "self", ".", "_fc_entity", "(", "encoded", ")", "\n", "projected", "=", "self", ".", "_locked_dropout", "(", "projected", ",", "self", ".", "_dropout", ")", "\n", "logits", "=", "torch", ".", "bmm", "(", "projected", ",", "shortlist_embeddings", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "# There are technically two masks that need to be accounted for: a class-wise mask which", "\n", "# specifies which logits to ignore in the class dimension, and a token-wise mask (e.g.", "\n", "# `mask`) which avoids measuring loss for predictions on non-mention tokens. In practice,", "\n", "# we only need the class-wise mask since the non-mention tokens cannot be associated with a", "\n", "# valid target.", "\n", "batch_size", "=", "encoded", ".", "shape", "[", "0", "]", "\n", "entity_loss", "=", "0.0", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "entity_loss", "+=", "F", ".", "cross_entropy", "(", "\n", "input", "=", "logits", "[", "i", "]", ",", "\n", "target", "=", "targets", "[", "i", "]", ",", "\n", "weight", "=", "shortlist_mask", "[", "i", "]", ".", "float", "(", ")", ",", "\n", "reduction", "=", "'sum'", ")", "\n", "", "entity_loss", "=", "entity_loss", "/", "(", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "return", "entity_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._copy_scores": [[216, 245], ["alias_tokens.view", "simplified.AliasCopynet._token_embedder", "alias_tokens.view.gt", "simplified.AliasCopynet._alias_encoder", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "simplified.AliasCopynet._locked_dropout", "encoded.view.view.view", "projected.view.view.view", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "copy_scores.view().contiguous.view().contiguous.view().contiguous", "copy_mask.sum", "encoded.view.view.new_zeros", "simplified.AliasCopynet._fc_copy", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "copy_scores.view().contiguous.view().contiguous.view"], "methods", ["None"], ["", "def", "_copy_scores", "(", "self", ",", "\n", "encoded", ":", "torch", ".", "Tensor", ",", "\n", "alias_tokens", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "# Begin by flattening the tokens so that they fit the expected shape of a", "\n", "# ``Seq2SeqEncoder``.", "\n", "        ", "batch_size", ",", "sequence_length", ",", "alias_length", "=", "alias_tokens", ".", "shape", "\n", "flattened", "=", "alias_tokens", ".", "view", "(", "-", "1", ",", "alias_length", ")", "\n", "copy_mask", "=", "flattened", "!=", "0", "\n", "if", "copy_mask", ".", "sum", "(", ")", "==", "0", ":", "\n", "            ", "return", "encoded", ".", "new_zeros", "(", "(", "batch_size", ",", "sequence_length", ",", "alias_length", ")", ",", "\n", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "# Embed and encode the alias tokens.", "\n", "", "embedded", "=", "self", ".", "_token_embedder", "(", "flattened", ")", "\n", "mask", "=", "flattened", ".", "gt", "(", "0", ")", "\n", "encoded_aliases", "=", "self", ".", "_alias_encoder", "(", "embedded", ",", "mask", ")", "\n", "\n", "# Equation 8 in the CopyNet paper recommends applying the additional step.", "\n", "projected", "=", "torch", ".", "tanh", "(", "self", ".", "_fc_copy", "(", "encoded_aliases", ")", ")", "\n", "projected", "=", "self", ".", "_locked_dropout", "(", "projected", ",", "self", ".", "_dropout", ")", "\n", "\n", "# This part gets a little funky - we need to make sure that the first dimension in", "\n", "# `projected` and `hidden` is batch_size x sequence_length.", "\n", "encoded", "=", "encoded", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "1", ",", "-", "1", ")", "\n", "projected", "=", "projected", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ",", "alias_length", ")", "\n", "copy_scores", "=", "torch", ".", "bmm", "(", "encoded", ",", "projected", ")", ".", "squeeze", "(", ")", "\n", "copy_scores", "=", "copy_scores", ".", "view", "(", "batch_size", ",", "sequence_length", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "\n", "return", "copy_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._vocab_loss": [[246, 324], ["target_alias_indices.view.view.gt", "target_tokens.view", "mask.byte().view.byte().view.view().byte", "alias_indices.view.view.view().gt", "allennlp.nn.util.masked_log_softmax", "allennlp.nn.util.masked_log_softmax", "generate_log_probs.view.gather.view", "flattened_log_probs.gather.view.gather", "target_tokens.eq().view", "target_alias_indices.view.view.gt().view", "copy_log_probs.view.view.view", "alias_indices.view.view.view", "target_alias_indices.view.view.view", "mask.byte().view.byte().view.byte().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "torch.logsumexp", "simplified.AliasCopynet._ppl", "simplified.AliasCopynet._upp", "kg_mask.any", "bg_mask.any", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "target_alias_indices.view.view.gt", "simplified.AliasCopynet._kg_ppl", "simplified.AliasCopynet._bg_ppl", "mask.byte().view.byte().view.view", "alias_indices.view.view.view", "target_tokens.eq", "target_alias_indices.view.view.gt", "alias_indices.view.view.eq", "mask.byte().view.byte().view.byte", "combined_log_probs[].sum", "mask.byte().view.byte().view.float().sum", "target_tokens.eq().view.float", "combined_log_probs[].sum", "mask.byte().view.byte().view.float().sum", "penalized_log_probs[].sum", "mask.byte().view.byte().view.float().sum", "mask.byte().view.byte().view.byte", "mask.byte().view.byte().view.byte", "combined_log_probs[].sum", "kg_mask.float().sum", "combined_log_probs[].sum", "bg_mask.float().sum", "generate_mask.float", "copy_mask.float", "mask.byte().view.byte().view.float", "mask.byte().view.byte().view.float", "mask.byte().view.byte().view.float", "kg_mask.float", "bg_mask.float"], "methods", ["None"], ["", "def", "_vocab_loss", "(", "self", ",", "\n", "generate_scores", ":", "torch", ".", "Tensor", ",", "\n", "copy_scores", ":", "torch", ".", "Tensor", ",", "\n", "target_tokens", ":", "torch", ".", "Tensor", ",", "\n", "target_alias_indices", ":", "torch", ".", "Tensor", ",", "\n", "mask", ":", "torch", ".", "Tensor", ",", "\n", "alias_indices", ":", "torch", ".", "Tensor", ",", "\n", "alias_tokens", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "mention_mask", "=", "target_alias_indices", ".", "gt", "(", "0", ")", "\n", "batch_size", ",", "sequence_length", ",", "vocab_size", "=", "generate_scores", ".", "shape", "\n", "copy_sequence_length", "=", "copy_scores", ".", "shape", "[", "-", "1", "]", "\n", "\n", "# Flat sequences make life **much** easier.", "\n", "flattened_targets", "=", "target_tokens", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "1", ")", "\n", "flattened_mask", "=", "mask", ".", "view", "(", "-", "1", ",", "1", ")", ".", "byte", "(", ")", "\n", "alias_mask", "=", "alias_indices", ".", "view", "(", "batch_size", ",", "sequence_length", ",", "-", "1", ")", ".", "gt", "(", "0", ")", "\n", "\n", "# The log-probability distribution is then given by taking the masked log softmax.", "\n", "generate_log_probs", "=", "masked_log_softmax", "(", "generate_scores", ",", "\n", "torch", ".", "ones_like", "(", "generate_scores", ")", ")", "\n", "copy_log_probs", "=", "masked_log_softmax", "(", "copy_scores", ",", "alias_mask", ")", "\n", "\n", "# GENERATE LOSS ###", "\n", "# The generated token loss is a simple cross-entropy calculation, we can just gather", "\n", "# the log probabilties...", "\n", "flattened_log_probs", "=", "generate_log_probs", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ")", "\n", "generate_log_probs", "=", "flattened_log_probs", ".", "gather", "(", "1", ",", "flattened_targets", ")", "\n", "# ...except we need to ignore the contribution of UNK tokens that are", "\n", "# copied (always in the simplified model). To do that we create a mask", "\n", "# which is 1 only if the token is not a copied UNK (or padding).", "\n", "unks", "=", "target_tokens", ".", "eq", "(", "self", ".", "_unk_index", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "copied", "=", "target_alias_indices", ".", "gt", "(", "0", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "generate_mask", "=", "~", "copied", "&", "flattened_mask", "\n", "# Since we are in log-space we apply the mask by addition.", "\n", "generate_log_probs", "=", "generate_log_probs", "+", "(", "generate_mask", ".", "float", "(", ")", "+", "1e-45", ")", ".", "log", "(", ")", "\n", "\n", "# COPY LOSS ###", "\n", "copy_log_probs", "=", "copy_log_probs", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ")", "\n", "# When computing the loss we need to get the log probability of **only** the copied tokens.", "\n", "alias_indices", "=", "alias_indices", ".", "view", "(", "batch_size", "*", "sequence_length", ",", "-", "1", ")", "\n", "target_alias_indices", "=", "target_alias_indices", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "copy_mask", "=", "alias_indices", ".", "eq", "(", "target_alias_indices", ")", "&", "flattened_mask", "&", "target_alias_indices", ".", "gt", "(", "0", ")", "\n", "copy_log_probs", "=", "copy_log_probs", "+", "(", "copy_mask", ".", "float", "(", ")", "+", "1e-45", ")", ".", "log", "(", ")", "\n", "\n", "# COMBINED LOSS ###", "\n", "# The final loss term is computed using our log probs computed w.r.t to the entire", "\n", "# vocabulary.", "\n", "kg_mask", "=", "(", "mention_mask", "&", "mask", ".", "byte", "(", ")", ")", ".", "view", "(", "-", "1", ")", "\n", "bg_mask", "=", "(", "~", "mention_mask", "&", "mask", ".", "byte", "(", ")", ")", ".", "view", "(", "-", "1", ")", "\n", "mask", "=", "mask", ".", "byte", "(", ")", ".", "view", "(", "-", "1", ")", "\n", "\n", "combined_log_probs", "=", "torch", ".", "cat", "(", "(", "generate_log_probs", ",", "\n", "copy_log_probs", ")", ",", "\n", "dim", "=", "1", ")", "\n", "combined_log_probs", "=", "torch", ".", "logsumexp", "(", "combined_log_probs", ",", "\n", "dim", "=", "1", ")", "\n", "vocab_loss", "=", "-", "combined_log_probs", "[", "mask", "]", ".", "sum", "(", ")", "/", "(", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n", "# PERPLEXITY ###", "\n", "# Our perplexity terms are computed using the log probs computed w.r.t the source", "\n", "# vocabulary.", "\n", "\n", "# For UPP we penalize **only** p(UNK); not the copy probabilities!", "\n", "penalized_log_probs", "=", "generate_log_probs", "-", "self", ".", "_unk_penalty", "*", "unks", ".", "float", "(", ")", "\n", "penalized_log_probs", "=", "torch", ".", "cat", "(", "(", "penalized_log_probs", ",", "\n", "copy_log_probs", ")", ",", "\n", "dim", "=", "1", ")", "\n", "penalized_log_probs", "=", "torch", ".", "logsumexp", "(", "penalized_log_probs", ",", "\n", "dim", "=", "1", ")", "\n", "\n", "self", ".", "_ppl", "(", "-", "combined_log_probs", "[", "mask", "]", ".", "sum", "(", ")", ",", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "self", ".", "_upp", "(", "-", "penalized_log_probs", "[", "mask", "]", ".", "sum", "(", ")", ",", "mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "if", "kg_mask", ".", "any", "(", ")", ":", "\n", "            ", "self", ".", "_kg_ppl", "(", "-", "combined_log_probs", "[", "kg_mask", "]", ".", "sum", "(", ")", ",", "kg_mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "", "if", "bg_mask", ".", "any", "(", ")", ":", "\n", "            ", "self", ".", "_bg_ppl", "(", "-", "combined_log_probs", "[", "bg_mask", "]", ".", "sum", "(", ")", ",", "bg_mask", ".", "float", "(", ")", ".", "sum", "(", ")", "+", "1e-13", ")", "\n", "\n", "", "return", "vocab_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._forward_loop": [[325, 396], ["allennlp.nn.util.get_text_field_mask", "kglm.modules.embedded_dropout", "simplified.AliasCopynet._locked_dropout", "enumerate", "simplified.AliasCopynet._fc_generate", "simplified.AliasCopynet._copy_scores", "simplified.AliasCopynet._vocab_loss", "rnn", "output.contiguous.contiguous.contiguous", "tuple", "hidden_states.append", "simplified.AliasCopynet._locked_dropout", "simplified.AliasCopynet._locked_dropout", "enumerate", "h.detach", "simplified.AliasCopynet.pow().mean", "simplified.AliasCopynet.pow"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.modules.embed_regularize.embedded_dropout", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._copy_scores", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet._vocab_loss"], ["", "def", "_forward_loop", "(", "self", ",", "\n", "source", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "target", ":", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ",", "\n", "alias_copy_inds", ":", "torch", ".", "Tensor", ",", "\n", "alias_tokens", ":", "torch", ".", "Tensor", ",", "\n", "alias_inds", ":", "torch", ".", "Tensor", ")", "->", "Dict", "[", "str", ",", "torch", ".", "Tensor", "]", ":", "\n", "\n", "# Get the token mask and unwrap the target tokens.", "\n", "        ", "target_mask", "=", "get_text_field_mask", "(", "target", ")", "\n", "target", "=", "target", "[", "'tokens'", "]", "\n", "\n", "# Embed source tokens.", "\n", "source", "=", "source", "[", "'tokens'", "]", "\n", "source_embeddings", "=", "embedded_dropout", "(", "\n", "embed", "=", "self", ".", "_token_embedder", ",", "\n", "words", "=", "source", ",", "\n", "dropout", "=", "self", ".", "_dropoute", "if", "self", ".", "training", "else", "0", ")", "\n", "source_embeddings", "=", "self", ".", "_locked_dropout", "(", "source_embeddings", ",", "self", ".", "_dropouti", ")", "\n", "\n", "# Encode source tokens.", "\n", "current_input", "=", "source_embeddings", "\n", "hidden_states", "=", "[", "]", "\n", "for", "layer", ",", "rnn", "in", "enumerate", "(", "self", ".", "rnns", ")", ":", "\n", "# Retrieve previous hidden state for layer.", "\n", "            ", "if", "self", ".", "_state", "is", "not", "None", ":", "\n", "                ", "prev_hidden", "=", "self", ".", "_state", "[", "'layer_%i'", "%", "layer", "]", "\n", "", "else", ":", "\n", "                ", "prev_hidden", "=", "None", "\n", "# Forward-pass.", "\n", "", "output", ",", "hidden", "=", "rnn", "(", "current_input", ",", "prev_hidden", ")", "\n", "output", "=", "output", ".", "contiguous", "(", ")", "\n", "# Update hidden state for layer.", "\n", "hidden", "=", "tuple", "(", "h", ".", "detach", "(", ")", "for", "h", "in", "hidden", ")", "\n", "hidden_states", ".", "append", "(", "hidden", ")", "\n", "# Apply dropout.", "\n", "if", "layer", "==", "self", ".", "_num_layers", "-", "1", ":", "\n", "                ", "dropped_output", "=", "self", ".", "_locked_dropout", "(", "output", ",", "self", ".", "_dropout", ")", "\n", "", "else", ":", "\n", "                ", "dropped_output", "=", "self", ".", "_locked_dropout", "(", "output", ",", "self", ".", "_dropouth", ")", "\n", "", "current_input", "=", "dropped_output", "\n", "", "encoded", "=", "current_input", "\n", "self", ".", "_state", "=", "{", "'layer_%i'", "%", "i", ":", "h", "for", "i", ",", "h", "in", "enumerate", "(", "hidden_states", ")", "}", "\n", "\n", "# Predict generation-mode scores. Start by concatenating predicted entity embeddings with", "\n", "# the encoder output - then feed through a linear layer.", "\n", "generate_scores", "=", "self", ".", "_fc_generate", "(", "encoded", ")", "\n", "\n", "# Predict copy-mode scores.", "\n", "# alias_tokens, alias_inds = alias_database.lookup(entity_ids)", "\n", "copy_scores", "=", "self", ".", "_copy_scores", "(", "encoded", ",", "alias_tokens", ")", "\n", "\n", "# Combine scores to get vocab loss", "\n", "vocab_loss", "=", "self", ".", "_vocab_loss", "(", "generate_scores", ",", "\n", "copy_scores", ",", "\n", "target", ",", "\n", "alias_copy_inds", ",", "\n", "target_mask", ",", "\n", "alias_inds", ",", "\n", "alias_tokens", ")", "\n", "\n", "# Compute total loss", "\n", "loss", "=", "vocab_loss", "\n", "\n", "# Activation regularization", "\n", "if", "self", ".", "_alpha", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "_alpha", "*", "dropped_output", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "# Temporal activation regularization (slowness)", "\n", "", "if", "self", ".", "_beta", ":", "\n", "            ", "loss", "=", "loss", "+", "self", ".", "_beta", "*", "(", "output", "[", ":", ",", "1", ":", "]", "-", "output", "[", ":", ",", ":", "-", "1", "]", ")", ".", "pow", "(", "2", ")", ".", "mean", "(", ")", "\n", "\n", "", "return", "{", "'loss'", ":", "loss", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.train": [[397, 405], ["super().train"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.train"], ["", "@", "overrides", "\n", "def", "train", "(", "self", ",", "mode", "=", "True", ")", ":", "\n", "# TODO: This is a temporary hack to ensure that the internal state resets when the model", "\n", "# switches from training to evaluation. The complication arises from potentially differing", "\n", "# batch sizes (e.g. the `reset` tensor will not be the right size). In future", "\n", "# implementations this should be handled more robustly.", "\n", "        ", "super", "(", ")", ".", "train", "(", "mode", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval": [[406, 411], ["super().eval"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval"], ["", "@", "overrides", "\n", "def", "eval", "(", "self", ")", ":", "\n", "# TODO: See train.", "\n", "        ", "super", "(", ")", ".", "eval", "(", ")", "\n", "self", ".", "_state", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.get_metrics": [[412, 418], ["simplified.AliasCopynet._ppl.get_metric", "simplified.AliasCopynet._upp.get_metric", "simplified.AliasCopynet._kg_ppl.get_metric", "simplified.AliasCopynet._bg_ppl.get_metric"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric", "home.repos.pwc.inspect_result.rloganiv_kglm-model.metrics.perplexity.Ppl.get_metric"], ["", "def", "get_metrics", "(", "self", ",", "reset", ":", "bool", "=", "False", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "return", "{", "\n", "'ppl'", ":", "self", ".", "_ppl", ".", "get_metric", "(", "reset", ")", ",", "\n", "'upp'", ":", "self", ".", "_upp", ".", "get_metric", "(", "reset", ")", ",", "\n", "'kg_ppl'", ":", "self", ".", "_kg_ppl", ".", "get_metric", "(", "reset", ")", ",", "\n", "'bg_ppl'", ":", "self", ".", "_bg_ppl", ".", "get_metric", "(", "reset", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmTest.setUp": [[15, 19], ["super().setUp", "kglm_test.KglmTest.set_up_model"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "self", ".", "set_up_model", "(", "\"kglm/tests/fixtures/training_config/kglm.json\"", ",", "\n", "\"kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmTest.test_model_can_train_save_and_load": [[20, 22], ["kglm_test.KglmTest.ensure_model_can_train_save_and_load"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.testing.kglm_model_test_case.KglmModelTestCase.ensure_model_can_train_save_and_load"], ["", "def", "test_model_can_train_save_and_load", "(", "self", ")", ":", "\n", "        ", "self", ".", "ensure_model_can_train_save_and_load", "(", "self", ".", "param_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmNoShortlistTest.setUp": [[25, 29], ["super().setUp", "kglm_test.KglmNoShortlistTest.set_up_model"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "self", ".", "set_up_model", "(", "\"kglm/tests/fixtures/training_config/kglm.no-shortlist.json\"", ",", "\n", "\"kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmNoShortlistTest.test_model_can_train_save_and_load": [[30, 32], ["kglm_test.KglmNoShortlistTest.ensure_model_can_train_save_and_load"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.testing.kglm_model_test_case.KglmModelTestCase.ensure_model_can_train_save_and_load"], ["", "def", "test_model_can_train_save_and_load", "(", "self", ")", ":", "\n", "        ", "self", ".", "ensure_model_can_train_save_and_load", "(", "self", ".", "param_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmDiscTest.setUp": [[35, 39], ["super().setUp", "kglm_test.KglmDiscTest.set_up_model"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "self", ".", "set_up_model", "(", "\"kglm/tests/fixtures/training_config/kglm-disc.json\"", ",", "\n", "\"kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmDiscTest.test_model_can_train_save_and_load": [[40, 42], ["kglm_test.KglmDiscTest.ensure_model_can_train_save_and_load"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.testing.kglm_model_test_case.KglmModelTestCase.ensure_model_can_train_save_and_load"], ["", "def", "test_model_can_train_save_and_load", "(", "self", ")", ":", "\n", "        ", "self", ".", "ensure_model_can_train_save_and_load", "(", "self", ".", "param_file", ",", "gradients_to_ignore", "=", "[", "'_overlap_weight'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmDiscTest.test_sample": [[43, 56], ["allennlp.common.Params.from_file", "allennlp.common.Params.from_file", "allennlp.data.DatasetReader.from_params", "list", "allennlp.data.DataIterator.from_params", "allennlp.data.DataIterator.from_params.index_with", "next", "kglm_test.KglmDiscTest.model.sample", "allennlp.data.DatasetReader.from_params.read", "allennlp.data.DataIterator.from_params."], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.sample.sample"], ["", "def", "test_sample", "(", "self", ")", ":", "\n", "        ", "generator_params", "=", "Params", ".", "from_file", "(", "\"kglm/tests/fixtures/training_config/kglm.json\"", ")", "\n", "params", "=", "Params", ".", "from_file", "(", "self", ".", "param_file", ")", "\n", "dataset_file", "=", "\"kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl\"", "\n", "\n", "# Need instances from 'generative' reader!", "\n", "reader_params", "=", "generator_params", "[", "'dataset_reader'", "]", "\n", "reader", "=", "DatasetReader", ".", "from_params", "(", "reader_params", ")", "\n", "instances", "=", "list", "(", "reader", ".", "read", "(", "dataset_file", ")", ")", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "generator_params", "[", "'iterator'", "]", ")", "\n", "iterator", ".", "index_with", "(", "self", ".", "model", ".", "vocab", ")", "\n", "batch", ",", "_", "=", "next", "(", "iterator", "(", "instances", ",", "shuffle", "=", "False", ")", ")", "\n", "self", ".", "model", ".", "sample", "(", "**", "batch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmDiscNoShortlistTest.setUp": [[60, 64], ["super().setUp", "kglm_test.KglmDiscNoShortlistTest.set_up_model"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "self", ".", "set_up_model", "(", "\"kglm/tests/fixtures/training_config/kglm-disc.no-shortlist.json\"", ",", "\n", "\"kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmDiscNoShortlistTest.test_model_can_train_save_and_load": [[65, 67], ["kglm_test.KglmDiscNoShortlistTest.ensure_model_can_train_save_and_load"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.testing.kglm_model_test_case.KglmModelTestCase.ensure_model_can_train_save_and_load"], ["", "def", "test_model_can_train_save_and_load", "(", "self", ")", ":", "\n", "        ", "self", ".", "ensure_model_can_train_save_and_load", "(", "self", ".", "param_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.kglm_test.KglmDiscNoShortlistTest.test_sample": [[68, 88], ["allennlp.common.Params.from_file", "allennlp.common.Params.from_file", "allennlp.data.DatasetReader.from_params", "list", "allennlp.data.DataIterator.from_params", "allennlp.data.DataIterator.from_params.index_with", "next", "torch.manual_seed", "kglm_test.KglmDiscNoShortlistTest.model.sample().get", "torch.manual_seed", "kglm_test.KglmDiscNoShortlistTest.model.sample().get", "allennlp.data.DatasetReader.from_params.read", "allennlp.data.DataIterator.from_params.", "kglm_test.KglmDiscNoShortlistTest.model.sample", "kglm_test.KglmDiscNoShortlistTest.model.sample"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.sample.sample", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.sample.sample"], ["", "def", "test_sample", "(", "self", ")", ":", "\n", "        ", "generator_params", "=", "Params", ".", "from_file", "(", "\"kglm/tests/fixtures/training_config/kglm.no-shortlist.json\"", ")", "\n", "params", "=", "Params", ".", "from_file", "(", "self", ".", "param_file", ")", "\n", "dataset_file", "=", "\"kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl\"", "\n", "\n", "# Need instances from 'generative' reader!", "\n", "reader_params", "=", "generator_params", "[", "'dataset_reader'", "]", "\n", "reader_params", "[", "'mode'", "]", "=", "'generative'", "\n", "reader", "=", "DatasetReader", ".", "from_params", "(", "reader_params", ")", "\n", "instances", "=", "list", "(", "reader", ".", "read", "(", "dataset_file", ")", ")", "\n", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "generator_params", "[", "'iterator'", "]", ")", "\n", "iterator", ".", "index_with", "(", "self", ".", "model", ".", "vocab", ")", "\n", "batch", ",", "_", "=", "next", "(", "iterator", "(", "instances", ",", "shuffle", "=", "False", ")", ")", "\n", "\n", "# Samples should match (we'll test by comparing logp)", "\n", "torch", ".", "manual_seed", "(", "123", ")", "\n", "logp1", "=", "self", ".", "model", ".", "sample", "(", "**", "batch", ")", ".", "get", "(", "'logp'", ",", "None", ")", "\n", "torch", ".", "manual_seed", "(", "123", ")", "\n", "logp2", "=", "self", ".", "model", ".", "sample", "(", "**", "batch", ")", ".", "get", "(", "'logp'", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_nlm_test.EntityNLMTest.setUp": [[13, 17], ["super().setUp", "entity_nlm_test.EntityNLMTest.set_up_model"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "self", ".", "set_up_model", "(", "\"kglm/tests/fixtures/training_config/entity_nlm.json\"", ",", "\n", "\"kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_nlm_test.EntityNLMTest.test_model_can_train_save_and_load": [[18, 27], ["entity_nlm_test.EntityNLMTest.ensure_model_can_train_save_and_load"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.testing.kglm_model_test_case.KglmModelTestCase.ensure_model_can_train_save_and_load"], ["", "def", "test_model_can_train_save_and_load", "(", "self", ")", ":", "\n", "# TODO: Construct test cases where we can obtain gradients for these components", "\n", "        ", "gradients_to_ignore", "=", "[", "\n", "'_dummy_context_embedding'", ",", "\n", "'_dynamic_embeddings._distance_scalar'", ",", "\n", "'_dynamic_embeddings._embedding_projection.weight'", "\n", "]", "\n", "self", ".", "ensure_model_can_train_save_and_load", "(", "self", ".", "param_file", ",", "\n", "gradients_to_ignore", "=", "gradients_to_ignore", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story_test.KglmTest.setUp": [[13, 17], ["super().setUp", "no_story_test.KglmTest.set_up_model"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "self", ".", "set_up_model", "(", "\"kglm/tests/fixtures/training_config/no-story.json\"", ",", "\n", "\"kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.no_story_test.KglmTest.test_model_can_train_save_and_load": [[18, 20], ["no_story_test.KglmTest.ensure_model_can_train_save_and_load"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.testing.kglm_model_test_case.KglmModelTestCase.ensure_model_can_train_save_and_load"], ["", "def", "test_model_can_train_save_and_load", "(", "self", ")", ":", "\n", "        ", "self", ".", "ensure_model_can_train_save_and_load", "(", "self", ".", "param_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.awd_lstm_test.AwdLstmLanguageModelTest.setUp": [[12, 16], ["super().setUp", "awd_lstm_test.AwdLstmLanguageModelTest.set_up_model"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "self", ".", "set_up_model", "(", "\"kglm/tests/fixtures/training_config/awd-lstm-lm.json\"", ",", "\n", "\"kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.awd_lstm_test.AwdLstmLanguageModelTest.test_model_can_train_save_and_load": [[17, 19], ["awd_lstm_test.AwdLstmLanguageModelTest.ensure_model_can_train_save_and_load"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.testing.kglm_model_test_case.KglmModelTestCase.ensure_model_can_train_save_and_load"], ["", "def", "test_model_can_train_save_and_load", "(", "self", ")", ":", "\n", "        ", "self", ".", "ensure_model_can_train_save_and_load", "(", "self", ".", "param_file", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.conll2012_test.TestConll2012DatasetReader.test_read_from_file": [[11, 62], ["pytest.mark.parametrize", "kglm.data.dataset_readers.Conll2012DatasetReader", "allennlp.common.util.ensure_list", "numpy.testing.assert_allclose", "numpy.testing.assert_allclose", "numpy.testing.assert_allclose", "kglm.data.dataset_readers.Conll2012DatasetReader.read", "len"], "methods", ["None"], ["    ", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'lazy'", ",", "(", "True", ",", "False", ")", ")", "\n", "def", "test_read_from_file", "(", "self", ",", "lazy", ")", ":", "\n", "        ", "reader", "=", "Conll2012DatasetReader", "(", "lazy", "=", "lazy", ")", "\n", "fixture_path", "=", "'kglm/tests/fixtures/coref.gold_conll'", "\n", "instances", "=", "ensure_list", "(", "reader", ".", "read", "(", "fixture_path", ")", ")", "\n", "assert", "len", "(", "instances", ")", "==", "2", "\n", "\n", "first_instance_tokens", "=", "[", "x", ".", "text", "for", "x", "in", "instances", "[", "0", "]", "[", "\"tokens\"", "]", ".", "tokens", "]", "\n", "assert", "first_instance_tokens", "==", "[", "\n", "'@@START@@'", ",", "'In'", ",", "'the'", ",", "'summer'", ",", "'of'", ",", "'@@NUM@@'", ",", "','", ",", "'a'", ",", "'picture'", ",", "'that'", ",", "\n", "'people'", ",", "'have'", ",", "'long'", ",", "'been'", ",", "'looking'", ",", "'forward'", ",", "'to'", ",", "\n", "'started'", ",", "'emerging'", ",", "'with'", ",", "'frequency'", ",", "'in'", ",", "'various'", ",", "'major'", ",", "\n", "'Hong'", ",", "'Kong'", ",", "'media'", ",", "'.'", ",", "'With'", ",", "'their'", ",", "'unique'", ",", "'charm'", ",", "','", ",", "\n", "'these'", ",", "'well'", ",", "'-'", ",", "'known'", ",", "'cartoon'", ",", "'images'", ",", "'once'", ",", "'again'", ",", "\n", "'caused'", ",", "'Hong'", ",", "'Kong'", ",", "'to'", ",", "'be'", ",", "'a'", ",", "'focus'", ",", "'of'", ",", "'worldwide'", ",", "\n", "'attention'", ",", "'.'", ",", "'The'", ",", "'world'", ",", "\"'s\"", ",", "'fifth'", ",", "'Disney'", ",", "'park'", ",", "\n", "'will'", ",", "'soon'", ",", "'open'", ",", "'to'", ",", "'the'", ",", "'public'", ",", "'here'", ",", "'.'", ",", "'@@END@@'", "\n", "]", "\n", "# {(41, 42): 1, (23, 24): 1, (28, 28): 2, (32, 37): 2}", "\n", "first_instance_entity_types", "=", "instances", "[", "0", "]", "[", "\"entity_types\"", "]", ".", "array", "\n", "# Add 1 to both indices to account for @@START@@", "\n", "# Add 1 more to the end to work with slicing", "\n", "# entity_indices = {", "\n", "#         (41+1, 42+1+1),", "\n", "#         (23+1, 24+1+1),", "\n", "#         (28+1, 28+1+1),", "\n", "#         (32+1, 37+1+1)", "\n", "# }", "\n", "assert", "first_instance_tokens", "[", "23", "+", "1", ":", "24", "+", "1", "+", "1", "]", "==", "[", "'Hong'", ",", "'Kong'", "]", "\n", "assert", "first_instance_tokens", "[", "28", "+", "1", ":", "28", "+", "1", "+", "1", "]", "==", "[", "'their'", "]", "\n", "assert", "first_instance_tokens", "[", "32", "+", "1", ":", "37", "+", "1", "+", "1", "]", "==", "[", "'these'", ",", "'well'", ",", "'-'", ",", "\n", "'known'", ",", "'cartoon'", ",", "'images'", "]", "\n", "assert", "first_instance_tokens", "[", "41", "+", "1", ":", "42", "+", "1", "+", "1", "]", "==", "[", "'Hong'", ",", "'Kong'", "]", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "first_instance_entity_types", ",", "\n", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "\n", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "1", ",", "1", ",", "0", ",", "0", ",", "\n", "0", ",", "1", ",", "0", ",", "0", ",", "0", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "0", ",", "0", ",", "0", ",", "\n", "1", ",", "1", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "\n", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "instances", "[", "0", "]", "[", "\"entity_ids\"", "]", ".", "array", ",", "\n", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "\n", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "1", ",", "1", ",", "0", ",", "0", ",", "0", ",", "2", ",", "\n", "0", ",", "0", ",", "0", ",", "2", ",", "2", ",", "2", ",", "2", ",", "2", ",", "2", ",", "0", ",", "0", ",", "0", ",", "1", ",", "1", ",", "0", ",", "\n", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "\n", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "instances", "[", "0", "]", "[", "\"mention_lengths\"", "]", ".", "array", ",", "\n", "[", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "\n", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "2", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "\n", "1", ",", "1", ",", "1", ",", "6", ",", "5", ",", "4", ",", "3", ",", "2", ",", "1", ",", "1", ",", "1", ",", "1", ",", "2", ",", "1", ",", "1", ",", "\n", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "\n", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext_test.TestEnhancedWikitextEntityNLMReader.test_read_from_file": [[11, 29], ["pytest.mark.parametrize", "kglm.data.dataset_readers.EnhancedWikitextEntityNlmReader", "allennlp.common.util.ensure_list", "numpy.testing.assert_allclose", "numpy.testing.assert_allclose", "numpy.testing.assert_allclose", "numpy.testing.assert_allclose", "numpy.testing.assert_allclose", "numpy.testing.assert_allclose", "kglm.data.dataset_readers.EnhancedWikitextEntityNlmReader.read"], "methods", ["None"], ["    ", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'lazy'", ",", "(", "True", ",", "False", ")", ")", "\n", "def", "test_read_from_file", "(", "self", ",", "lazy", ")", ":", "\n", "        ", "reader", "=", "EnhancedWikitextEntityNlmReader", "(", "lazy", "=", "lazy", ")", "\n", "fixture_path", "=", "'kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl'", "\n", "instances", "=", "ensure_list", "(", "reader", ".", "read", "(", "fixture_path", ")", ")", "\n", "\n", "first_instance_tokens", "=", "[", "x", ".", "text", "for", "x", "in", "instances", "[", "0", "]", "[", "\"tokens\"", "]", ".", "tokens", "]", "\n", "assert", "first_instance_tokens", "[", ":", "5", "]", "==", "[", "'@@START@@'", ",", "'State'", ",", "'Route'", ",", "'127'", ",", "'('", "]", "\n", "assert", "first_instance_tokens", "[", "-", "5", ":", "]", "==", "[", "'Elmer'", ",", "'Huntley'", ",", "'Bridge'", ",", "'.'", ",", "'@@END@@'", "]", "\n", "second_instance_entity_types", "=", "instances", "[", "1", "]", "[", "\"entity_types\"", "]", ".", "array", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "second_instance_entity_types", "[", ":", "5", "]", ",", "[", "0", ",", "0", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "second_instance_entity_types", "[", "-", "5", ":", "]", ",", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "instances", "[", "1", "]", "[", "\"entity_ids\"", "]", ".", "array", "[", ":", "5", "]", ",", "[", "0", ",", "0", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "instances", "[", "1", "]", "[", "\"entity_ids\"", "]", ".", "array", "[", "-", "5", ":", "]", ",", "[", "0", ",", "0", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "instances", "[", "1", "]", "[", "\"mention_lengths\"", "]", ".", "array", "[", ":", "5", "]", ",", "\n", "[", "1", ",", "1", ",", "5", ",", "4", ",", "3", "]", ")", "\n", "np", ".", "testing", ".", "assert_allclose", "(", "instances", "[", "1", "]", "[", "\"mention_lengths\"", "]", ".", "array", "[", "-", "5", ":", "]", ",", "\n", "[", "1", ",", "1", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext_test.TestEnhancedWikitextKglmReader.test_read_from_file": [[32, 113], ["pytest.mark.parametrize", "pytest.mark.parametrize", "kglm.data.dataset_readers.EnhancedWikitextKglmReader", "allennlp.common.util.ensure_list", "kglm.data.dataset_readers.EnhancedWikitextKglmReader.read", "len", "alias_database.token_to_uid", "set", "first_instance_shortlist.index"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.token_to_uid"], ["    ", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'lazy'", ",", "(", "True", ",", "False", ")", ")", "\n", "@", "pytest", ".", "mark", ".", "parametrize", "(", "'mode'", ",", "(", "\"generative\"", ",", "\"discriminative\"", ")", ")", "\n", "def", "test_read_from_file", "(", "self", ",", "lazy", ",", "mode", ")", ":", "\n", "        ", "offset", "=", "0", "if", "mode", "==", "\"generative\"", "else", "1", "\n", "alias_database_path", "=", "'kglm/tests/fixtures/enhanced-wikitext-test/alias.pkl'", "\n", "reader", "=", "EnhancedWikitextKglmReader", "(", "lazy", "=", "lazy", ",", "\n", "mode", "=", "mode", ",", "\n", "alias_database_path", "=", "alias_database_path", ")", "\n", "fixture_path", "=", "'kglm/tests/fixtures/enhanced-wikitext-test/train.jsonl'", "\n", "instances", "=", "ensure_list", "(", "reader", ".", "read", "(", "fixture_path", ")", ")", "\n", "\n", "# Test correct number of instances is being created", "\n", "assert", "len", "(", "instances", ")", "==", "3", "\n", "\n", "# Test article tokens are being read properly", "\n", "first_instance_source_tokens", "=", "[", "x", ".", "text", "for", "x", "in", "instances", "[", "0", "]", "[", "'source'", "]", ".", "tokens", "]", "\n", "assert", "first_instance_source_tokens", "[", ":", "5", "]", "==", "[", "'@@START@@'", ",", "'State'", ",", "'Route'", ",", "'127'", ",", "'('", "]", "\n", "assert", "first_instance_source_tokens", "[", "-", "5", ":", "]", "==", "[", "'the'", ",", "'Elmer'", ",", "'Huntley'", ",", "'Bridge'", ",", "'.'", "]", "\n", "\n", "if", "mode", "==", "\"discriminative\"", ":", "\n", "            ", "assert", "\"target\"", "not", "in", "instances", "[", "0", "]", "\n", "assert", "\"alias_copy_inds\"", "not", "in", "instances", "[", "0", "]", "\n", "", "else", ":", "\n", "# Test target tokens", "\n", "            ", "first_instance_target_tokens", "=", "[", "x", ".", "text", "for", "x", "in", "instances", "[", "0", "]", "[", "'target'", "]", ".", "tokens", "]", "\n", "assert", "first_instance_target_tokens", "[", ":", "5", "]", "==", "[", "'State'", ",", "'Route'", ",", "'127'", ",", "'('", ",", "'SR'", "]", "\n", "assert", "first_instance_target_tokens", "[", "-", "5", ":", "]", "==", "[", "'Elmer'", ",", "'Huntley'", ",", "'Bridge'", ",", "'.'", ",", "'@@END@@'", "]", "\n", "\n", "\n", "# Test mention type", "\n", "# Non-mention tokens are not new entities", "\n", "", "first_instance_mention_type", "=", "instances", "[", "0", "]", "[", "'mention_type'", "]", ".", "array", "\n", "assert", "first_instance_mention_type", "[", "0", "+", "offset", "]", "==", "0", "\n", "# \"state highway\" is a new entity mention", "\n", "assert", "first_instance_mention_type", "[", "16", "+", "offset", "]", "==", "1", "\n", "assert", "first_instance_mention_type", "[", "17", "+", "offset", "]", "==", "3", "\n", "# \"Washington\" is a derived entity mention", "\n", "assert", "first_instance_mention_type", "[", "27", "+", "offset", "]", "==", "2", "\n", "# \"Check that eos offestting is working", "\n", "assert", "first_instance_mention_type", "[", "57", "+", "offset", "]", "==", "1", "\n", "\n", "# Test entity id", "\n", "first_instance_entity_ids", "=", "[", "x", ".", "text", "for", "x", "in", "instances", "[", "0", "]", "[", "'entity_ids'", "]", ".", "tokens", "]", "\n", "# Non-mentions correspond to padding tokens", "\n", "assert", "first_instance_entity_ids", "[", "7", "+", "offset", ":", "9", "+", "offset", "]", "==", "[", "'@@PADDING@@'", ",", "'@@PADDING@@'", "]", "\n", "assert", "first_instance_entity_ids", "[", "7", "+", "offset", ":", "9", "+", "offset", "]", "==", "[", "'@@PADDING@@'", ",", "'@@PADDING@@'", "]", "\n", "# Mentions are the WikiData id", "\n", "assert", "first_instance_entity_ids", "[", "16", "+", "offset", ":", "18", "+", "offset", "]", "==", "[", "'Q831285'", ",", "'Q831285'", "]", "\n", "\n", "if", "mode", "==", "\"generative\"", ":", "\n", "# Test that copy indices properly match tokens to their place in aliases", "\n", "            ", "alias_database", "=", "instances", "[", "0", "]", "[", "'metadata'", "]", "[", "'alias_database'", "]", "\n", "first_instance_alias_copy_inds", "=", "instances", "[", "0", "]", "[", "'alias_copy_inds'", "]", ".", "array", "\n", "entity_id", "=", "first_instance_entity_ids", "[", "16", "+", "offset", "]", "\n", "first_mention_token", "=", "first_instance_source_tokens", "[", "17", "]", "\n", "uid", "=", "alias_database", ".", "token_to_uid", "(", "entity_id", ",", "first_mention_token", ")", "\n", "assert", "uid", "==", "first_instance_alias_copy_inds", "[", "16", "+", "offset", "]", "\n", "\n", "# Test parent ids and relations", "\n", "", "first_instance_parent_ids", "=", "[", "[", "x", ".", "text", "for", "x", "in", "y", ".", "tokens", "]", "for", "y", "in", "instances", "[", "0", "]", "[", "'parent_ids'", "]", "]", "\n", "first_instance_relations", "=", "[", "[", "x", ".", "text", "for", "x", "in", "y", ".", "tokens", "]", "for", "y", "in", "instances", "[", "0", "]", "[", "'relations'", "]", "]", "\n", "# Non-mentions correspond to a singleton padding token", "\n", "assert", "first_instance_parent_ids", "[", "7", "+", "offset", "]", "==", "[", "'@@PADDING@@'", "]", "\n", "assert", "first_instance_relations", "[", "7", "+", "offset", "]", "==", "[", "'@@PADDING@@'", "]", "\n", "# \"Washington\" has two parents", "\n", "assert", "first_instance_parent_ids", "[", "27", "+", "offset", "]", "==", "[", "'Q831285'", ",", "'Q3046581'", "]", "\n", "assert", "first_instance_relations", "[", "27", "+", "offset", "]", "==", "[", "'P131'", ",", "'P131'", "]", "\n", "\n", "\n", "# Test that shortlist is being properly generated", "\n", "first_instance_shortlist", "=", "[", "x", ".", "text", "for", "x", "in", "instances", "[", "0", "]", "[", "'shortlist'", "]", ".", "tokens", "]", "\n", "expected_entities", "=", "{", "\n", "'@@PADDING@@'", ",", "'Q831285'", ",", "'Q3046581'", ",", "'Q35657'", ",", "'Q695782'", ",", "'Q1223'", ",", "'Q452623'", ",", "\n", "'Q272074'", ",", "'Q800459'", "\n", "}", "\n", "assert", "set", "(", "first_instance_shortlist", ")", "==", "expected_entities", "\n", "\n", "# Test that shortlist inds point to correct", "\n", "first_instance_shortlist", "=", "[", "x", ".", "text", "for", "x", "in", "instances", "[", "0", "]", "[", "'shortlist'", "]", ".", "tokens", "]", "\n", "first_instance_shortlist_inds", "=", "instances", "[", "0", "]", "[", "'shortlist_inds'", "]", ".", "array", "\n", "assert", "first_instance_shortlist_inds", "[", "16", "+", "offset", "]", "==", "first_instance_shortlist", ".", "index", "(", "'Q831285'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.conll2012.Conll2012DatasetReader.__init__": [[73, 80], ["allennlp.data.dataset_readers.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "replace_numbers", ":", "bool", "=", "True", ",", "\n", "lazy", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_replace_numbers", "=", "replace_numbers", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.conll2012.Conll2012DatasetReader._read": [[81, 103], ["allennlp.common.file_utils.cached_path", "allennlp.data.dataset_readers.dataset_utils.Ontonotes", "allennlp.data.dataset_readers.dataset_utils.Ontonotes.dataset_document_iterator", "collections.defaultdict", "conll2012.canonicalize_clusters", "len", "conll2012.Conll2012DatasetReader.text_to_instance", "clusters[].append"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.conll2012.canonicalize_clusters", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "ontonotes_reader", "=", "Ontonotes", "(", ")", "\n", "for", "sentences", "in", "ontonotes_reader", ".", "dataset_document_iterator", "(", "file_path", ")", ":", "\n", "            ", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "\n", "total_tokens", "=", "0", "\n", "for", "sentence", "in", "sentences", ":", "\n", "                ", "for", "typed_span", "in", "sentence", ".", "coref_spans", ":", "\n", "# Coref annotations are on a _per sentence_", "\n", "# basis, so we need to adjust them to be relative", "\n", "# to the length of the document.", "\n", "                    ", "span_id", ",", "(", "start", ",", "end", ")", "=", "typed_span", "\n", "clusters", "[", "span_id", "]", ".", "append", "(", "(", "start", "+", "total_tokens", ",", "\n", "end", "+", "total_tokens", ")", ")", "\n", "", "total_tokens", "+=", "len", "(", "sentence", ".", "words", ")", "\n", "\n", "", "canonical_clusters", "=", "canonicalize_clusters", "(", "clusters", ")", "\n", "yield", "self", ".", "text_to_instance", "(", "[", "s", ".", "words", "for", "s", "in", "sentences", "]", ",", "canonical_clusters", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.conll2012.Conll2012DatasetReader.text_to_instance": [[104, 195], ["sorted", "allennlp.data.fields.TextField", "numpy.zeros", "numpy.zeros", "numpy.ones", "kglm.data.fields.SequentialArrayField", "kglm.data.fields.SequentialArrayField", "kglm.data.fields.SequentialArrayField", "allennlp.data.instance.Instance", "conll2012.Conll2012DatasetReader._normalize_word", "enumerate", "cluster_dict.items", "sorted.append", "allennlp.data.tokenizers.Token", "numpy.arange", "filtered_cluster.append", "len", "len", "len", "sorted", "tuple"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.conll2012.Conll2012DatasetReader._normalize_word"], ["", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "# type: ignore", "\n", "sentences", ":", "List", "[", "List", "[", "str", "]", "]", ",", "\n", "gold_clusters", ":", "Optional", "[", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", "=", "None", ")", "->", "Instance", ":", "\n", "# pylint: disable=arguments-differ", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        sentences : ``List[List[str]]``, required.\n            A list of lists representing the tokenised words and sentences in the document.\n        gold_clusters : ``Optional[List[List[Tuple[int, int]]]]``, optional (default = None)\n            A list of all clusters in the document, represented as word spans. Each cluster\n            contains some number of spans, which can be nested and overlap, but will never\n            exactly match between clusters.\n\n        Returns\n        -------\n        An ``Instance`` containing the following ``Fields``:\n            tokens : ``TextField``\n                The text of the full document.\n            entity_types : ``SequentialArrayField``\n                An array with 1's in positions corresponding to words in entities,\n                and 0's in positions corresponding to words not in entities.\n            entity_ids : ``SequentialArrayField``\n                An array with an entity index in positions corresponding to words in\n                entities, and 0's in positions corresponding to words not in entities.\n                Words in coreferring entities share the same entity ID.\n            mention_lengths : ``SequentialArrayField``\n                An array with the remaining words in each entity. For words that aren't\n                in an entity, the corresponding index is \"1\". Else, the corresponding\n                index has the number of words remaining in the entity. If the entity\n                is of length \"1\", it is assigned \"1\".\n        \"\"\"", "\n", "# Filter gold_clusters: for embedded mentions, only the", "\n", "# enclosing (outer) entity mention is kept.", "\n", "filtered_gold_clusters", "=", "[", "]", "\n", "all_entity_spans", "=", "[", "span", "for", "gold_cluster", "in", "gold_clusters", "for", "span", "in", "gold_cluster", "]", "\n", "for", "cluster", "in", "gold_clusters", ":", "\n", "            ", "filtered_cluster", "=", "[", "]", "\n", "for", "span", "in", "cluster", ":", "\n", "                ", "is_embedded_span", "=", "False", "\n", "for", "other_span", "in", "all_entity_spans", ":", "\n", "# Skip if span is equal to other_span", "\n", "                    ", "if", "span", "==", "other_span", ":", "\n", "                        ", "continue", "\n", "", "if", "span", "[", "0", "]", ">=", "other_span", "[", "0", "]", "and", "span", "[", "1", "]", "<=", "other_span", "[", "1", "]", ":", "\n", "# span is embedded within other_span, so don't use it", "\n", "                        ", "is_embedded_span", "=", "True", "\n", "break", "\n", "", "", "if", "not", "is_embedded_span", ":", "\n", "                    ", "filtered_cluster", ".", "append", "(", "span", ")", "\n", "", "", "if", "filtered_cluster", ":", "\n", "                ", "filtered_gold_clusters", ".", "append", "(", "filtered_cluster", ")", "\n", "\n", "# Sort the gold clusters, so the earlier-occurring clusters are earlier in the list", "\n", "", "", "filtered_gold_clusters", "=", "sorted", "(", "filtered_gold_clusters", ",", "key", "=", "lambda", "x", ":", "sorted", "(", "x", ")", "[", "0", "]", "[", "0", "]", ")", "\n", "\n", "flattened_sentences", "=", "[", "self", ".", "_normalize_word", "(", "word", ",", "self", ".", "_replace_numbers", ")", "\n", "for", "sentence", "in", "sentences", "\n", "for", "word", "in", "sentence", "]", "\n", "tokens", "=", "[", "'@@START@@'", ",", "*", "flattened_sentences", ",", "'@@END@@'", "]", "\n", "text_field", "=", "TextField", "(", "[", "Token", "(", "word", ")", "for", "word", "in", "tokens", "]", ",", "\n", "self", ".", "_token_indexers", ")", "\n", "fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "\"tokens\"", ":", "text_field", "}", "\n", "\n", "cluster_dict", "=", "{", "}", "\n", "if", "filtered_gold_clusters", "is", "not", "None", ":", "\n", "            ", "for", "cluster_id", ",", "cluster", "in", "enumerate", "(", "filtered_gold_clusters", ",", "1", ")", ":", "\n", "                ", "for", "mention", "in", "cluster", ":", "\n", "                    ", "cluster_dict", "[", "tuple", "(", "mention", ")", "]", "=", "cluster_id", "\n", "\n", "# Initialize fields.", "\n", "", "", "", "entity_types", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "tokens", ")", ",", ")", ")", "\n", "entity_ids", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "tokens", ")", ",", ")", ")", "\n", "mention_lengths", "=", "np", ".", "ones", "(", "shape", "=", "(", "len", "(", "tokens", ")", ",", ")", ")", "\n", "\n", "if", "cluster_dict", ":", "\n", "            ", "for", "cluster", ",", "entity_id", "in", "cluster_dict", ".", "items", "(", ")", ":", "\n", "# Fill in \"1\" for positions corresponding to words in entities", "\n", "# Need offset by 1 to account for @@START@@ token.", "\n", "                ", "entity_types", "[", "cluster", "[", "0", "]", "+", "1", ":", "cluster", "[", "1", "]", "+", "1", "+", "1", "]", "=", "1", "\n", "# Fill in entity ID", "\n", "entity_ids", "[", "cluster", "[", "0", "]", "+", "1", ":", "cluster", "[", "1", "]", "+", "1", "+", "1", "]", "=", "entity_id", "\n", "entity_length", "=", "(", "cluster", "[", "1", "]", "+", "1", ")", "-", "cluster", "[", "0", "]", "\n", "# Fill in mention length", "\n", "mention_lengths", "[", "cluster", "[", "0", "]", "+", "1", ":", "cluster", "[", "1", "]", "+", "1", "+", "1", "]", "=", "np", ".", "arange", "(", "entity_length", ",", "0", ",", "step", "=", "-", "1", ")", "\n", "\n", "", "", "fields", "[", "'entity_ids'", "]", "=", "SequentialArrayField", "(", "entity_ids", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "fields", "[", "'mention_lengths'", "]", "=", "SequentialArrayField", "(", "mention_lengths", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "fields", "[", "'entity_types'", "]", "=", "SequentialArrayField", "(", "entity_types", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.conll2012.Conll2012DatasetReader._normalize_word": [[196, 204], ["word.replace().isdigit", "word.replace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_normalize_word", "(", "word", ",", "replace_numbers", ":", "bool", ")", "->", "str", ":", "\n", "        ", "if", "word", "in", "[", "\"/.\"", ",", "\"/?\"", "]", ":", "\n", "            ", "return", "word", "[", "1", ":", "]", "\n", "", "if", "replace_numbers", ":", "\n", "            ", "if", "word", ".", "replace", "(", "'.'", ",", "''", ",", "1", ")", ".", "isdigit", "(", ")", ":", "\n", "                ", "return", "\"@@NUM@@\"", "\n", "", "", "return", "word", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.conll2012._flatten": [[20, 22], ["None"], "function", ["None"], ["def", "_flatten", "(", "nested", ":", "Iterable", "[", "str", "]", ")", ":", "\n", "    ", "return", "[", "x", "for", "seq", "in", "nested", "for", "x", "in", "seq", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.conll2012.canonicalize_clusters": [[24, 54], ["clusters.values", "list", "cluster_with_overlapping_mention.update", "merged_clusters.append", "set"], "function", ["None"], ["", "def", "canonicalize_clusters", "(", "clusters", ":", "DefaultDict", "[", "int", ",", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ")", "->", "List", "[", "List", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", ":", "\n", "    ", "\"\"\"\n    The CONLL 2012 data includes 2 annotated spans which are identical,\n    but have different ids. This checks all clusters for spans which are\n    identical, and if it finds any, merges the clusters containing the\n    identical spans.\n    \"\"\"", "\n", "merged_clusters", ":", "List", "[", "Set", "[", "Tuple", "[", "int", ",", "int", "]", "]", "]", "=", "[", "]", "\n", "for", "cluster", "in", "clusters", ".", "values", "(", ")", ":", "\n", "        ", "cluster_with_overlapping_mention", "=", "None", "\n", "for", "mention", "in", "cluster", ":", "\n", "# Look at clusters we have already processed to", "\n", "# see if they contain a mention in the current", "\n", "# cluster for comparison.", "\n", "            ", "for", "cluster2", "in", "merged_clusters", ":", "\n", "                ", "if", "mention", "in", "cluster2", ":", "\n", "# first cluster in merged clusters", "\n", "# which contains this mention.", "\n", "                    ", "cluster_with_overlapping_mention", "=", "cluster2", "\n", "break", "\n", "# Already encountered overlap - no need to keep looking.", "\n", "", "", "if", "cluster_with_overlapping_mention", "is", "not", "None", ":", "\n", "                ", "break", "\n", "", "", "if", "cluster_with_overlapping_mention", "is", "not", "None", ":", "\n", "# Merge cluster we are currently processing into", "\n", "# the cluster in the processed list.", "\n", "            ", "cluster_with_overlapping_mention", ".", "update", "(", "cluster", ")", "\n", "", "else", ":", "\n", "            ", "merged_clusters", ".", "append", "(", "set", "(", "cluster", ")", ")", "\n", "", "", "return", "[", "list", "(", "c", ")", "for", "c", "in", "merged_clusters", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextReader.__init__": [[36, 41], ["allennlp.data.dataset_readers.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "lazy", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "'tokens'", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextReader._read": [[42, 51], ["allennlp.common.file_utils.cached_path", "open", "json.loads", "enhanced_wikitext.EnhancedWikitextReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "data", "=", "json", ".", "loads", "(", "line", ")", "\n", "yield", "self", ".", "text_to_instance", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextReader.text_to_instance": [[52, 59], ["allennlp.data.instance.Instance", "allennlp.data.tokenizers.Token", "allennlp.data.fields.TextField"], "methods", ["None"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "data", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "Instance", ":", "# pylint: disable=arguments-differ", "\n", "# Flatten and pad tokens", "\n", "        ", "tokens", "=", "data", "[", "'tokens'", "]", "\n", "tokens", "=", "[", "Token", "(", "x", ")", "for", "x", "in", "tokens", "]", "\n", "fields", "=", "{", "'tokens'", ":", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "}", "\n", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextEntityNlmReader.__init__": [[64, 69], ["allennlp.data.dataset_readers.DatasetReader.__init__", "allennlp.data.token_indexers.SingleIdTokenIndexer"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "lazy", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "'tokens'", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextEntityNlmReader._read": [[70, 79], ["allennlp.common.file_utils.cached_path", "open", "json.loads", "enhanced_wikitext.EnhancedWikitextEntityNlmReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "# if `file_path` is a URL, redirect to the cache", "\n", "        ", "file_path", "=", "cached_path", "(", "file_path", ")", "\n", "\n", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "data", "=", "json", ".", "loads", "(", "line", ")", "\n", "yield", "self", ".", "text_to_instance", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextEntityNlmReader.text_to_instance": [[80, 115], ["allennlp.data.instance.Instance", "allennlp.data.tokenizers.Token", "allennlp.data.fields.TextField", "set", "numpy.zeros", "numpy.zeros", "numpy.ones", "kglm.data.fields.SequentialArrayField", "kglm.data.fields.SequentialArrayField", "kglm.data.fields.SequentialArrayField", "seen_entities.add", "range", "len", "len", "len", "len"], "methods", ["None"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "data", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "Instance", ":", "# pylint: disable=arguments-differ", "\n", "# Flatten and pad tokens", "\n", "        ", "tokens", "=", "data", "[", "'tokens'", "]", "\n", "tokens", "=", "[", "Token", "(", "x", ")", "for", "x", "in", "tokens", "]", "\n", "fields", "=", "{", "'tokens'", ":", "TextField", "(", "tokens", ",", "self", ".", "_token_indexers", ")", "}", "\n", "\n", "# If annotations are provided, process them into arrays.", "\n", "if", "'annotations'", "in", "data", ":", "\n", "\n", "# Initialize arrays and book keeping data structures.", "\n", "            ", "seen_entities", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "entity_types", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "tokens", ")", ",", ")", ")", "\n", "entity_ids", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "tokens", ")", ",", ")", ")", "\n", "mention_lengths", "=", "np", ".", "ones", "(", "shape", "=", "(", "len", "(", "tokens", ")", ",", ")", ")", "\n", "\n", "# Process annotations", "\n", "for", "annotation", "in", "data", "[", "'annotations'", "]", ":", "\n", "\n", "                ", "seen_entities", ".", "add", "(", "annotation", "[", "'id'", "]", ")", "\n", "start", ",", "end", "=", "annotation", "[", "'span'", "]", "\n", "length", "=", "end", "-", "start", "\n", "\n", "for", "i", "in", "range", "(", "*", "annotation", "[", "'span'", "]", ")", ":", "\n", "# Note: +1 offset to account for start token.", "\n", "                    ", "entity_types", "[", "i", "]", "=", "1", "\n", "entity_ids", "[", "i", "]", "=", "len", "(", "seen_entities", ")", "\n", "mention_lengths", "[", "i", "]", "=", "length", "\n", "length", "-=", "1", "\n", "\n", "", "", "fields", "[", "'entity_types'", "]", "=", "SequentialArrayField", "(", "entity_types", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "fields", "[", "'entity_ids'", "]", "=", "SequentialArrayField", "(", "entity_ids", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "fields", "[", "'mention_lengths'", "]", "=", "SequentialArrayField", "(", "mention_lengths", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextKglmReader.__init__": [[132, 177], ["allennlp.data.dataset_readers.DatasetReader.__init__", "kglm.data.AliasDatabase.load", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "isinstance", "isinstance", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.load"], ["    ", "def", "__init__", "(", "self", ",", "\n", "alias_database_path", ":", "str", ",", "\n", "mode", ":", "str", "=", "\"generative\"", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "entity_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "raw_entity_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "relation_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "lazy", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        alias_database_path : str\n            Path to the alias database.\n        mode : str, optional (default=\"generative\")\n            One of \"discriminative\" or \"generative\", indicating whether generated\n            instances are suitable for the discriminative or generative version of\n            the model.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "if", "mode", "not", "in", "{", "\"discriminative\"", ",", "\"generative\"", "}", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"Got mode {}, expected one of 'generative'\"", "\n", "\"or 'discriminative'\"", ".", "format", "(", "mode", ")", ")", "\n", "", "self", ".", "_mode", "=", "mode", "\n", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "'tokens'", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_entity_indexers", "=", "entity_indexers", "or", "{", "'entity_ids'", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'entity_ids'", ")", "}", "\n", "self", ".", "_raw_entity_indexers", "=", "raw_entity_indexers", "or", "{", "'raw_entity_ids'", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'raw_entity_ids'", ")", "}", "\n", "self", ".", "_relation_indexers", "=", "relation_indexers", "or", "{", "'relations'", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'relations'", ")", "}", "\n", "if", "'tokens'", "not", "in", "self", ".", "_token_indexers", "or", "not", "isinstance", "(", "self", ".", "_token_indexers", "[", "'tokens'", "]", ",", "SingleIdTokenIndexer", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"EnhancedWikitextReader expects 'token_indexers' to contain \"", "\n", "\"a 'single_id' token indexer called 'tokens'.\"", ")", "\n", "", "if", "'entity_ids'", "not", "in", "self", ".", "_entity_indexers", "or", "not", "isinstance", "(", "self", ".", "_entity_indexers", "[", "'entity_ids'", "]", ",", "SingleIdTokenIndexer", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"EnhancedWikitextReader expects 'entity_indexers' to contain \"", "\n", "\"a 'single_id' token indexer called 'entity_ids'.\"", ")", "\n", "", "if", "'raw_entity_ids'", "not", "in", "self", ".", "_raw_entity_indexers", "or", "not", "isinstance", "(", "self", ".", "_raw_entity_indexers", "[", "'raw_entity_ids'", "]", ",", "SingleIdTokenIndexer", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"EnhancedWikitextReader expects 'raw_entity_indexers' to contain \"", "\n", "\"a 'single_id' token indexer called 'raw_entity_ids'.\"", ")", "\n", "", "if", "'relations'", "not", "in", "self", ".", "_relation_indexers", "or", "not", "isinstance", "(", "self", ".", "_relation_indexers", "[", "'relations'", "]", ",", "SingleIdTokenIndexer", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"EnhancedWikitextReader expects 'relation_indexers' to contain \"", "\n", "\"a 'single_id' token indexer called 'relations'.\"", ")", "\n", "", "self", ".", "_alias_database", "=", "AliasDatabase", ".", "load", "(", "path", "=", "alias_database_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextKglmReader._read": [[178, 276], ["open", "json.loads", "numpy.zeros", "numpy.zeros", "enhanced_wikitext.EnhancedWikitextKglmReader.text_to_instance", "len", "len", "len", "len", "numpy.zeros", "enhanced_wikitext.normalize_entity_id", "range", "enhanced_wikitext.normalize_entity_id", "len", "len", "len", "shortlist.append", "len", "len", "enhanced_wikitext.EnhancedWikitextKglmReader._alias_database.token_to_uid", "len"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.normalize_entity_id", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.normalize_entity_id", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.token_to_uid"], ["", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "data", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "# Extract tokens", "\n", "tokens", "=", "data", "[", "'tokens'", "]", "\n", "source", "=", "tokens", "[", ":", "-", "1", "]", "\n", "if", "self", ".", "_mode", "==", "'generative'", ":", "\n", "                    ", "target", "=", "tokens", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "                    ", "target", "=", "None", "\n", "\n", "# Process annotations", "\n", "", "if", "'annotations'", "not", "in", "data", ":", "\n", "                    ", "shortlist", "=", "None", "\n", "reverse_shortlist", "=", "None", "\n", "raw_entity_ids", "=", "None", "\n", "entity_ids", "=", "None", "\n", "relations", "=", "None", "\n", "parent_ids", "=", "None", "\n", "shortlist_inds", "=", "None", "\n", "mention_type", "=", "None", "\n", "", "else", ":", "\n", "# We maintain a \"shortlist\" of observed entities, that is used for baseline models", "\n", "# that only select entities from the set that appear in the document (as opposed to", "\n", "# the set of all possible entities).", "\n", "                    ", "shortlist", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "\n", "reverse_shortlist", "=", "{", "DEFAULT_PADDING_TOKEN", ":", "0", "}", "\n", "raw_entity_ids", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "*", "len", "(", "source", ")", "\n", "entity_ids", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "*", "len", "(", "source", ")", "\n", "relations", "=", "[", "[", "DEFAULT_PADDING_TOKEN", "]", "]", "*", "len", "(", "source", ")", "\n", "parent_ids", "=", "[", "[", "DEFAULT_PADDING_TOKEN", "]", "]", "*", "len", "(", "source", ")", "\n", "shortlist_inds", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "source", ")", ",", ")", ")", "\n", "mention_type", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "source", ")", ",", ")", ")", "\n", "\n", "if", "self", ".", "_mode", "==", "\"generative\"", ":", "\n", "                        ", "alias_copy_inds", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "source", ")", ",", ")", ")", "\n", "", "else", ":", "\n", "                        ", "alias_copy_inds", "=", "None", "\n", "\n", "# Process annotations", "\n", "", "for", "annotation", "in", "data", "[", "'annotations'", "]", ":", "\n", "\n", "# Obtain the entity identifier for the annotated span", "\n", "                        ", "raw_entity_id", "=", "annotation", "[", "'id'", "]", "\n", "raw_parent_id", "=", "annotation", "[", "'parent_id'", "]", "\n", "entity_id", "=", "normalize_entity_id", "(", "raw_entity_id", ")", "\n", "if", "entity_id", "is", "None", ":", "\n", "                            ", "continue", "\n", "", "parent_id", "=", "[", "normalize_entity_id", "(", "x", ")", "for", "x", "in", "raw_parent_id", "]", "\n", "assert", "len", "(", "parent_id", ")", "==", "len", "(", "raw_parent_id", ")", "\n", "relation", "=", "annotation", "[", "'relation'", "]", "\n", "new_entity", "=", "relation", "==", "[", "'@@NEW@@'", "]", "\n", "\n", "# If neccessary, update the shortlist. Obtain the index of the entity identifier in", "\n", "# the shortlist.", "\n", "if", "entity_id", "not", "in", "reverse_shortlist", ":", "\n", "                            ", "reverse_shortlist", "[", "entity_id", "]", "=", "len", "(", "reverse_shortlist", ")", "\n", "shortlist", ".", "append", "(", "entity_id", ")", "\n", "", "shortlist_ind", "=", "reverse_shortlist", "[", "entity_id", "]", "\n", "\n", "# Update the outputs", "\n", "# Offset is 0 in generative case, since each timestep is for predicting", "\n", "# attributes of the next token. In the discriminative case, each timestep", "\n", "# is for predicting attributes of the current token.", "\n", "mode_offset", "=", "-", "1", "if", "self", ".", "_mode", "==", "\"generative\"", "else", "0", "\n", "span", "=", "annotation", "[", "'span'", "]", "\n", "for", "i", "in", "range", "(", "*", "span", ")", ":", "\n", "                            ", "raw_entity_ids", "[", "i", "+", "mode_offset", "]", "=", "raw_entity_id", "\n", "entity_ids", "[", "i", "+", "mode_offset", "]", "=", "entity_id", "\n", "mention_type", "[", "i", "+", "mode_offset", "]", "=", "3", "\n", "if", "new_entity", ":", "\n", "                                ", "shortlist_inds", "[", "i", "+", "mode_offset", "]", "=", "shortlist_ind", "\n", "", "else", ":", "\n", "                                ", "relations", "[", "i", "+", "mode_offset", "]", "=", "relation", "[", ":", "MAX_PARENTS", "]", "\n", "parent_ids", "[", "i", "+", "mode_offset", "]", "=", "parent_id", "[", ":", "MAX_PARENTS", "]", "\n", "", "if", "self", ".", "_mode", "==", "\"generative\"", ":", "\n", "                                ", "alias_copy_inds", "[", "i", "+", "mode_offset", "]", "=", "self", ".", "_alias_database", ".", "token_to_uid", "(", "raw_entity_id", ",", "tokens", "[", "i", "]", ")", "\n", "# Now put in proper mention type for first token", "\n", "", "", "start", "=", "annotation", "[", "'span'", "]", "[", "0", "]", "\n", "if", "new_entity", ":", "\n", "                            ", "mention_type", "[", "start", "+", "mode_offset", "]", "=", "1", "\n", "", "else", ":", "\n", "                            ", "mention_type", "[", "start", "+", "mode_offset", "]", "=", "2", "\n", "\n", "", "", "", "yield", "self", ".", "text_to_instance", "(", "source", ",", "\n", "target", ",", "\n", "shortlist", ",", "\n", "reverse_shortlist", ",", "\n", "raw_entity_ids", ",", "\n", "entity_ids", ",", "\n", "relations", ",", "\n", "parent_ids", ",", "\n", "shortlist_inds", ",", "\n", "mention_type", ",", "\n", "alias_copy_inds", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextKglmReader.text_to_instance": [[277, 326], ["allennlp.data.instance.Instance", "allennlp.data.fields.MetadataField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.fields.ListField", "allennlp.data.fields.ListField", "kglm.data.fields.SequentialArrayField", "kglm.data.fields.SequentialArrayField", "kglm.data.fields.SequentialArrayField", "enhanced_wikitext._tokenize", "enhanced_wikitext._tokenize", "enhanced_wikitext._tokenize", "enhanced_wikitext._tokenize", "enhanced_wikitext._tokenize", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "enhanced_wikitext._tokenize", "enhanced_wikitext._tokenize"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._tokenize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._tokenize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._tokenize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._tokenize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._tokenize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._tokenize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._tokenize"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "\n", "source", ",", "\n", "target", "=", "None", ",", "\n", "shortlist", "=", "None", ",", "\n", "reverse_shortlist", "=", "None", ",", "\n", "raw_entity_ids", "=", "None", ",", "\n", "entity_ids", "=", "None", ",", "\n", "relations", "=", "None", ",", "\n", "parent_ids", "=", "None", ",", "\n", "shortlist_inds", "=", "None", ",", "\n", "mention_type", "=", "None", ",", "\n", "alias_copy_inds", "=", "None", ")", "->", "Instance", ":", "# pylint: disable=arguments-differ", "\n", "        ", "metadata", "=", "{", "\n", "'source_tokens'", ":", "source", ",", "\n", "'alias_database'", ":", "self", ".", "_alias_database", "\n", "}", "\n", "fields", "=", "{", "\n", "'metadata'", ":", "MetadataField", "(", "metadata", ")", ",", "\n", "'source'", ":", "TextField", "(", "_tokenize", "(", "source", ")", ",", "self", ".", "_token_indexers", ")", ",", "\n", "}", "\n", "\n", "if", "target", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'target'", "]", "=", "TextField", "(", "_tokenize", "(", "target", ")", ",", "self", ".", "_token_indexers", ")", "\n", "metadata", "[", "'target_tokens'", "]", "=", "target", "\n", "", "if", "shortlist", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'shortlist'", "]", "=", "TextField", "(", "_tokenize", "(", "shortlist", ")", ",", "self", ".", "_entity_indexers", ")", "\n", "", "if", "raw_entity_ids", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'raw_entity_ids'", "]", "=", "TextField", "(", "_tokenize", "(", "raw_entity_ids", ")", ",", "self", ".", "_raw_entity_indexers", ")", "\n", "", "if", "entity_ids", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'entity_ids'", "]", "=", "TextField", "(", "_tokenize", "(", "entity_ids", ")", ",", "self", ".", "_entity_indexers", ")", "\n", "", "if", "parent_ids", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'parent_ids'", "]", "=", "ListField", "(", "[", "\n", "TextField", "(", "_tokenize", "(", "sublist", ")", ",", "\n", "token_indexers", "=", "self", ".", "_entity_indexers", ")", "\n", "for", "sublist", "in", "parent_ids", "]", ")", "\n", "", "if", "relations", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'relations'", "]", "=", "ListField", "(", "[", "\n", "TextField", "(", "_tokenize", "(", "sublist", ")", ",", "\n", "token_indexers", "=", "self", ".", "_relation_indexers", ")", "\n", "for", "sublist", "in", "relations", "]", ")", "\n", "", "if", "mention_type", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'mention_type'", "]", "=", "SequentialArrayField", "(", "mention_type", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "", "if", "shortlist_inds", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'shortlist_inds'", "]", "=", "SequentialArrayField", "(", "shortlist_inds", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "", "if", "alias_copy_inds", "is", "not", "None", ":", "\n", "            ", "fields", "[", "'alias_copy_inds'", "]", "=", "SequentialArrayField", "(", "alias_copy_inds", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.__init__": [[331, 351], ["allennlp.data.dataset_readers.DatasetReader.__init__", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "token_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "entity_indexers", ":", "Dict", "[", "str", ",", "TokenIndexer", "]", "=", "None", ",", "\n", "lazy", ":", "bool", "=", "False", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        alias_database_path : str\n            Path to the alias database.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "lazy", ")", "\n", "self", ".", "_token_indexers", "=", "token_indexers", "or", "{", "'tokens'", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "_entity_indexers", "=", "entity_indexers", "or", "{", "'entity_ids'", ":", "SingleIdTokenIndexer", "(", "namespace", "=", "'entity_ids'", ")", "}", "\n", "if", "'tokens'", "not", "in", "self", ".", "_token_indexers", "or", "not", "isinstance", "(", "self", ".", "_token_indexers", "[", "'tokens'", "]", ",", "SingleIdTokenIndexer", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"EnhancedWikitextReader expects 'token_indexers' to contain \"", "\n", "\"a 'single_id' token indexer called 'tokens'.\"", ")", "\n", "", "if", "'entity_ids'", "not", "in", "self", ".", "_entity_indexers", "or", "not", "isinstance", "(", "self", ".", "_entity_indexers", "[", "'entity_ids'", "]", ",", "SingleIdTokenIndexer", ")", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"EnhancedWikitextReader expects 'entity_indexers' to contain \"", "\n", "\"a 'single_id' token indexer called 'entities'.\"", ")", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader._read": [[353, 359], ["open", "json.loads", "enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance"], ["", "", "@", "overrides", "\n", "def", "_read", "(", "self", ",", "file_path", ":", "str", ")", "->", "Iterable", "[", "Instance", "]", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "data", "=", "json", ".", "loads", "(", "line", ")", "\n", "yield", "self", ".", "text_to_instance", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.EnhancedWikitextSimpleKglmReader.text_to_instance": [[360, 441], ["enhanced_wikitext._flatten", "allennlp.data.instance.Instance", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "numpy.zeros", "numpy.zeros", "numpy.zeros", "enumerate", "allennlp.data.fields.TextField", "kglm.data.fields.SequentialArrayField", "allennlp.data.fields.TextField", "kglm.data.fields.SequentialArrayField", "allennlp.data.fields.ListField", "kglm.data.fields.SequentialArrayField", "len", "len", "len", "range", "enumerate", "len", "allennlp.data.fields.TextField", "len", "shortlist.append", "len", "allennlp.data.tokenizers.Token", "allennlp.data.tokenizers.Token", "len", "enumerate", "allennlp.data.fields.TextField", "max", "set", "len", "allennlp.data.tokenizers.Token"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._flatten"], ["", "", "", "@", "overrides", "\n", "def", "text_to_instance", "(", "self", ",", "data", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "Instance", ":", "# pylint: disable=arguments-differ", "\n", "# Flatten and pad tokens", "\n", "        ", "tokens", "=", "_flatten", "(", "data", "[", "'tokens'", "]", ")", "\n", "tokens", "=", "[", "'@@START@@'", ",", "*", "tokens", ",", "'@@END@@'", "]", "\n", "source", "=", "[", "Token", "(", "x", ")", "for", "x", "in", "tokens", "[", ":", "-", "1", "]", "]", "\n", "target", "=", "[", "Token", "(", "x", ")", "for", "x", "in", "tokens", "[", "1", ":", "]", "]", "\n", "fields", "=", "{", "\n", "'source'", ":", "TextField", "(", "source", ",", "self", ".", "_token_indexers", ")", ",", "\n", "'target'", ":", "TextField", "(", "target", ",", "self", ".", "_token_indexers", ")", "\n", "}", "\n", "\n", "# Process annotations", "\n", "if", "'annotations'", "in", "data", ":", "\n", "\n", "# We maintain a \"shortlist\" of observed entities, that is used for baseline models", "\n", "# that only select entities from the set that appear in the document (as opposed to", "\n", "# the set of all possible entities).", "\n", "            ", "shortlist", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "\n", "reverse_shortlist", "=", "{", "DEFAULT_PADDING_TOKEN", ":", "0", "}", "\n", "\n", "entity_ids", "=", "[", "DEFAULT_PADDING_TOKEN", "]", "*", "len", "(", "target", ")", "\n", "shortlist_inds", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "target", ",", ")", ")", ")", "\n", "alias_copy_inds", "=", "np", ".", "zeros", "(", "shape", "=", "(", "len", "(", "target", ")", ",", ")", ")", "\n", "alias_tokens", "=", "[", "TextField", "(", "[", "]", ",", "self", ".", "_token_indexers", ")", "]", "*", "len", "(", "target", ")", "\n", "alias_inds", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "[", "]", "]", "*", "len", "(", "target", ")", "\n", "max_len", "=", "0", "\n", "\n", "# Process annotations", "\n", "for", "annotation", "in", "data", "[", "'annotations'", "]", ":", "\n", "\n", "# Obtain the entity identifier for the annotated span", "\n", "                ", "entity_id", "=", "annotation", "[", "'id'", "]", "\n", "alias", "=", "annotation", "[", "'alias'", "]", "\n", "alias_map", "=", "{", "token", ":", "i", "+", "1", "for", "i", ",", "token", "in", "enumerate", "(", "set", "(", "alias", ")", ")", "}", "\n", "\n", "# If neccessary, update the shortlist. Obtain the index of the entity identifier in", "\n", "# the shortlist.", "\n", "if", "entity_id", "not", "in", "reverse_shortlist", ":", "\n", "                    ", "reverse_shortlist", "[", "entity_id", "]", "=", "len", "(", "reverse_shortlist", ")", "\n", "shortlist", ".", "append", "(", "entity_id", ")", "\n", "", "shortlist_ind", "=", "reverse_shortlist", "[", "entity_id", "]", "\n", "\n", "# Update the outputs", "\n", "for", "i", "in", "range", "(", "*", "annotation", "[", "'span'", "]", ")", ":", "\n", "# Note: +1 offset to account for start token.", "\n", "                    ", "if", "tokens", "[", "i", "+", "1", "]", "not", "in", "alias_map", ":", "\n", "                        ", "continue", "\n", "", "else", ":", "\n", "                        ", "entity_ids", "[", "i", "]", "=", "entity_id", "\n", "shortlist_inds", "[", "i", "]", "=", "shortlist_ind", "\n", "alias_copy_inds", "[", "i", "]", "=", "alias_map", "[", "tokens", "[", "i", "+", "1", "]", "]", "\n", "alias_inds", "[", "i", "]", "=", "[", "alias_map", "[", "token", "]", "for", "token", "in", "alias", "]", "\n", "alias_tokens", "[", "i", "]", "=", "TextField", "(", "[", "Token", "(", "x", ")", "for", "x", "in", "alias", "]", ",", "\n", "self", ".", "_token_indexers", ")", "\n", "max_len", "=", "max", "(", "max_len", ",", "len", "(", "alias", ")", ")", "\n", "\n", "# Make alias_inds into a numpy array", "\n", "", "", "", "alias_ind_array", "=", "np", ".", "zeros", "(", "(", "len", "(", "target", ")", ",", "max_len", ")", ")", "\n", "for", "i", ",", "arr", "in", "enumerate", "(", "alias_inds", ")", ":", "\n", "                ", "for", "j", ",", "ind", "in", "enumerate", "(", "arr", ")", ":", "\n", "                    ", "alias_ind_array", "[", "i", ",", "j", "]", "=", "ind", "\n", "\n", "", "", "fields", "[", "'entity_ids'", "]", "=", "TextField", "(", "\n", "[", "Token", "(", "x", ")", "for", "x", "in", "entity_ids", "]", ",", "\n", "token_indexers", "=", "self", ".", "_entity_indexers", ")", "\n", "fields", "[", "'alias_copy_inds'", "]", "=", "SequentialArrayField", "(", "\n", "alias_copy_inds", ",", "\n", "dtype", "=", "np", ".", "int64", ")", "\n", "fields", "[", "'shortlist'", "]", "=", "TextField", "(", "\n", "[", "Token", "(", "x", ")", "for", "x", "in", "shortlist", "]", ",", "\n", "token_indexers", "=", "self", ".", "_entity_indexers", ")", "\n", "fields", "[", "'shortlist_inds'", "]", "=", "SequentialArrayField", "(", "\n", "shortlist_inds", ",", "\n", "dtype", "=", "np", ".", "int64", ")", "\n", "fields", "[", "'alias_tokens'", "]", "=", "ListField", "(", "alias_tokens", ")", "\n", "fields", "[", "'alias_inds'", "]", "=", "SequentialArrayField", "(", "\n", "alias_ind_array", ",", "\n", "dtype", "=", "np", ".", "int64", ")", "\n", "\n", "", "return", "Instance", "(", "fields", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._flatten": [[27, 29], ["None"], "function", ["None"], ["def", "_flatten", "(", "nested", ":", "Iterable", "[", "str", "]", ")", ":", "\n", "    ", "return", "[", "x", "for", "seq", "in", "nested", "for", "x", "in", "seq", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext._tokenize": [[30, 32], ["allennlp.data.tokenizers.Token"], "function", ["None"], ["", "def", "_tokenize", "(", "iterable", ":", "Iterable", "[", "str", "]", ")", ":", "\n", "    ", "return", "[", "Token", "(", "x", ")", "for", "x", "in", "iterable", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.dataset_readers.enhanced_wikitext.normalize_entity_id": [[117, 127], ["None"], "function", ["None"], ["", "", "def", "normalize_entity_id", "(", "raw_entity_id", ":", "str", ")", "->", "str", ":", "\n", "    ", "if", "raw_entity_id", "[", "0", "]", "==", "'T'", ":", "\n", "        ", "entity_id", "=", "'@@DATE@@'", "\n", "", "elif", "raw_entity_id", "[", "0", "]", "==", "'V'", ":", "\n", "        ", "entity_id", "=", "'@@QUANTITY@@'", "\n", "", "elif", "raw_entity_id", "[", "0", "]", "in", "[", "'P'", ",", "'Q'", "]", ":", "\n", "        ", "entity_id", "=", "raw_entity_id", "\n", "", "else", ":", "\n", "        ", "entity_id", "=", "None", "\n", "", "return", "entity_id", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.vocabulary_test.TestVocabulary.setUp": [[13, 20], ["allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.fields.TextField", "allennlp.data.Instance", "allennlp.data.dataset.Batch", "super().setUp", "allennlp.data.Token"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "token_indexer", "=", "SingleIdTokenIndexer", "(", "\"tokens\"", ")", "\n", "text_field", "=", "TextField", "(", "[", "Token", "(", "t", ")", "for", "t", "in", "[", "\"a\"", ",", "\"a\"", ",", "\"a\"", ",", "\"a\"", ",", "\"b\"", ",", "\"b\"", ",", "\"c\"", ",", "\"c\"", ",", "\"c\"", "]", "]", ",", "\n", "{", "\"tokens\"", ":", "token_indexer", "}", ")", "\n", "self", ".", "instance", "=", "Instance", "(", "{", "\"text\"", ":", "text_field", "}", ")", "\n", "self", ".", "dataset", "=", "Batch", "(", "[", "self", ".", "instance", "]", ")", "\n", "super", "(", "TestVocabulary", ",", "self", ")", ".", "setUp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.vocabulary_test.TestVocabulary.test_unk_namespace_is_empty_if_vocab_unconstrained": [[21, 25], ["kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances.get_index_to_token_vocabulary"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_instances"], ["", "def", "test_unk_namespace_is_empty_if_vocab_unconstrained", "(", "self", ")", ":", "\n", "        ", "vocab", "=", "ExtendedVocabulary", ".", "from_instances", "(", "self", ".", "dataset", ")", "\n", "words", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", "'tokens_unk'", ")", "\n", "assert", "not", "words", "# This checks that there's nothing in ``words`` w/out pylint complaining", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.vocabulary_test.TestVocabulary.test_from_dataset_respects_max_vocab_size_single_int": [[26, 36], ["kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances.get_index_to_token_vocabulary().values", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances.get_index_to_token_vocabulary().values", "len", "len", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances.get_index_to_token_vocabulary", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances.get_index_to_token_vocabulary"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_instances", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_instances"], ["", "def", "test_from_dataset_respects_max_vocab_size_single_int", "(", "self", ")", ":", "\n", "        ", "max_vocab_size", "=", "1", "\n", "vocab", "=", "ExtendedVocabulary", ".", "from_instances", "(", "self", ".", "dataset", ",", "max_vocab_size", "=", "max_vocab_size", ")", "\n", "words", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", ")", ".", "values", "(", ")", "\n", "# Additional 2 tokens are '@@PADDING@@' and '@@UNKNOWN@@' by default", "\n", "assert", "len", "(", "words", ")", "==", "max_vocab_size", "+", "2", "\n", "\n", "vocab", "=", "ExtendedVocabulary", ".", "from_instances", "(", "self", ".", "dataset", ",", "min_count", "=", "None", ")", "\n", "words", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", ")", ".", "values", "(", ")", "\n", "assert", "len", "(", "words", ")", "==", "5", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.vocabulary_test.TestVocabulary.test_from_dataset_respects_min_count": [[37, 49], ["kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances.get_index_to_token_vocabulary().values", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances.get_index_to_token_vocabulary().values", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances.get_index_to_token_vocabulary", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_instances.get_index_to_token_vocabulary"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_instances", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_instances"], ["", "def", "test_from_dataset_respects_min_count", "(", "self", ")", ":", "\n", "        ", "vocab", "=", "ExtendedVocabulary", ".", "from_instances", "(", "self", ".", "dataset", ",", "min_count", "=", "{", "'tokens'", ":", "4", "}", ")", "\n", "words", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", ")", ".", "values", "(", ")", "\n", "assert", "'a'", "in", "words", "\n", "assert", "'b'", "not", "in", "words", "\n", "assert", "'c'", "not", "in", "words", "\n", "\n", "vocab", "=", "ExtendedVocabulary", ".", "from_instances", "(", "self", ".", "dataset", ",", "min_count", "=", "None", ")", "\n", "words", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", ")", ".", "values", "(", ")", "\n", "assert", "'a'", "in", "words", "\n", "assert", "'b'", "in", "words", "\n", "assert", "'c'", "in", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.vocabulary_test.TestVocabulary.test_saving_and_loading": [[50, 89], ["kglm.data.extended_vocabulary.ExtendedVocabulary", "kglm.data.extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "kglm.data.extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "kglm.data.extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "kglm.data.extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "kglm.data.extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "kglm.data.extended_vocabulary.ExtendedVocabulary.save_to_files", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_vocab_size", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_from_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_from_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_from_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_vocab_size", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_from_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_from_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_from_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_from_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_token_index", "kglm.data.extended_vocabulary.ExtendedVocabulary.get_index_to_token_vocabulary", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_index_to_token_vocabulary", "kglm.data.extended_vocabulary.ExtendedVocabulary.get_index_to_token_vocabulary", "kglm.data.extended_vocabulary.ExtendedVocabulary.from_files.get_index_to_token_vocabulary"], "methods", ["None"], ["", "def", "test_saving_and_loading", "(", "self", ")", ":", "\n", "# pylint: disable=protected-access", "\n", "        ", "vocab_dir", "=", "self", ".", "TEST_DIR", "/", "'vocab_save'", "\n", "\n", "vocab", "=", "ExtendedVocabulary", "(", "non_padded_namespaces", "=", "[", "\"a\"", ",", "\"c\"", "]", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"a0\"", ",", "namespace", "=", "\"a\"", ")", "# non-padded, should start at 0", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"a1\"", ",", "namespace", "=", "\"a\"", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"a2\"", ",", "namespace", "=", "\"a\"", ")", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"b2\"", ",", "namespace", "=", "\"b\"", ")", "# padded, should start at 2", "\n", "vocab", ".", "add_token_to_namespace", "(", "\"b3\"", ",", "namespace", "=", "\"b\"", ")", "\n", "\n", "vocab", ".", "save_to_files", "(", "vocab_dir", ")", "\n", "vocab2", "=", "ExtendedVocabulary", ".", "from_files", "(", "vocab_dir", ")", "\n", "\n", "assert", "vocab2", ".", "_non_padded_namespaces", "==", "{", "\"a\"", ",", "\"c\"", "}", "\n", "\n", "# Check namespace a.", "\n", "assert", "vocab2", ".", "get_vocab_size", "(", "namespace", "=", "'a'", ")", "==", "3", "\n", "assert", "vocab2", ".", "get_token_from_index", "(", "0", ",", "namespace", "=", "'a'", ")", "==", "'a0'", "\n", "assert", "vocab2", ".", "get_token_from_index", "(", "1", ",", "namespace", "=", "'a'", ")", "==", "'a1'", "\n", "assert", "vocab2", ".", "get_token_from_index", "(", "2", ",", "namespace", "=", "'a'", ")", "==", "'a2'", "\n", "assert", "vocab2", ".", "get_token_index", "(", "'a0'", ",", "namespace", "=", "'a'", ")", "==", "0", "\n", "assert", "vocab2", ".", "get_token_index", "(", "'a1'", ",", "namespace", "=", "'a'", ")", "==", "1", "\n", "assert", "vocab2", ".", "get_token_index", "(", "'a2'", ",", "namespace", "=", "'a'", ")", "==", "2", "\n", "\n", "# Check namespace b.", "\n", "assert", "vocab2", ".", "get_vocab_size", "(", "namespace", "=", "'b'", ")", "==", "4", "# (unk + padding + two tokens)", "\n", "assert", "vocab2", ".", "get_token_from_index", "(", "0", ",", "namespace", "=", "'b'", ")", "==", "vocab", ".", "_padding_token", "\n", "assert", "vocab2", ".", "get_token_from_index", "(", "1", ",", "namespace", "=", "'b'", ")", "==", "vocab", ".", "_oov_token", "\n", "assert", "vocab2", ".", "get_token_from_index", "(", "2", ",", "namespace", "=", "'b'", ")", "==", "'b2'", "\n", "assert", "vocab2", ".", "get_token_from_index", "(", "3", ",", "namespace", "=", "'b'", ")", "==", "'b3'", "\n", "assert", "vocab2", ".", "get_token_index", "(", "vocab", ".", "_padding_token", ",", "namespace", "=", "'b'", ")", "==", "0", "\n", "assert", "vocab2", ".", "get_token_index", "(", "vocab", ".", "_oov_token", ",", "namespace", "=", "'b'", ")", "==", "1", "\n", "assert", "vocab2", ".", "get_token_index", "(", "'b2'", ",", "namespace", "=", "'b'", ")", "==", "2", "\n", "assert", "vocab2", ".", "get_token_index", "(", "'b3'", ",", "namespace", "=", "'b'", ")", "==", "3", "\n", "\n", "# Check the dictionaries containing the reverse mapping are identical.", "\n", "assert", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"a\"", ")", "==", "vocab2", ".", "get_index_to_token_vocabulary", "(", "\"a\"", ")", "\n", "assert", "vocab", ".", "get_index_to_token_vocabulary", "(", "\"b\"", ")", "==", "vocab2", ".", "get_index_to_token_vocabulary", "(", "\"b\"", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database_test.AliasDatabaseTest.setUp": [[15, 48], ["allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.token_indexers.SingleIdTokenIndexer", "allennlp.data.fields.TextField", "allennlp.data.fields.TextField", "allennlp.data.Instance", "allennlp.data.dataset.Batch", "allennlp.data.vocabulary.Vocabulary.from_instances", "alias_database_test.AliasDatabaseTest.dataset.index_instances", "super().setUp", "numpy.array", "numpy.array", "allennlp.data.Token", "allennlp.data.Token"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_instances", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "token_lookup", "=", "{", "\n", "'Entity1'", ":", "[", "[", "'Robert'", ",", "'Logan'", "]", ",", "[", "'Robby'", "]", "]", ",", "\n", "'Entity2'", ":", "[", "[", "'Jimmy'", "]", "]", "\n", "}", "\n", "self", ".", "id_map_lookup", "=", "{", "\n", "'Entity1'", ":", "{", "'Robert'", ":", "1", ",", "'Logan'", ":", "2", ",", "'Robby'", ":", "3", "}", ",", "\n", "'Entity2'", ":", "{", "'Jimmy'", ":", "1", "}", "\n", "}", "\n", "self", ".", "id_array_lookup", "=", "{", "\n", "'Entity1'", ":", "np", ".", "array", "(", "[", "[", "1", ",", "2", "]", ",", "[", "3", ",", "0", "]", "]", ",", "dtype", "=", "int", ")", ",", "\n", "'Entity2'", ":", "np", ".", "array", "(", "[", "[", "1", "]", "]", ",", "dtype", "=", "int", ")", "\n", "}", "\n", "self", ".", "token_to_entity_lookup", "=", "{", "\n", "'Robert'", ":", "{", "'Entity1'", "}", ",", "\n", "'Logan'", ":", "{", "'Entity1'", "}", ",", "\n", "'Robby'", ":", "{", "'Entity1'", "}", ",", "\n", "'Jimmy'", ":", "{", "'Entity2'", "}", "\n", "}", "\n", "token_indexer", "=", "SingleIdTokenIndexer", "(", ")", "\n", "entity_indexer", "=", "SingleIdTokenIndexer", "(", "namespace", "=", "'entity_ids'", ")", "\n", "text_field", "=", "TextField", "(", "[", "Token", "(", "t", ")", "for", "t", "in", "[", "'Robby'", ",", "'is'", ",", "'a'", ",", "'nickname'", ",", "'for'", ",", "'Robert'", "]", "]", ",", "\n", "{", "'tokens'", ":", "token_indexer", "}", ")", "\n", "entity_field", "=", "TextField", "(", "[", "Token", "(", "t", ")", "for", "t", "in", "[", "'Entity1'", ",", "''", ",", "''", ",", "''", ",", "''", ",", "'Entity1'", "]", "]", ",", "\n", "{", "'entity_ids'", ":", "entity_indexer", "}", ")", "\n", "self", ".", "instance", "=", "Instance", "(", "{", "\n", "'tokens'", ":", "text_field", ",", "\n", "'entity_identifiers'", ":", "entity_field", "\n", "}", ")", "\n", "self", ".", "dataset", "=", "Batch", "(", "[", "self", ".", "instance", "]", ")", "\n", "self", ".", "vocab", "=", "Vocabulary", ".", "from_instances", "(", "self", ".", "dataset", ")", "\n", "self", ".", "dataset", ".", "index_instances", "(", "self", ".", "vocab", ")", "\n", "super", "(", "AliasDatabaseTest", ",", "self", ")", ".", "setUp", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database_test.AliasDatabaseTest.test_load": [[49, 67], ["kglm.data.AliasDatabase.load", "len"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.load"], ["", "def", "test_load", "(", "self", ")", ":", "\n", "# Test that the load function has the expected behavior", "\n", "        ", "alias_database", "=", "AliasDatabase", ".", "load", "(", "'kglm/tests/fixtures/enhanced-wikitext-test/alias.pkl'", ")", "\n", "test_entity", "=", "'Q156216'", "# Benton County", "\n", "\n", "# Check that aliases are tokenized properly", "\n", "expected_tokenized_aliases", "=", "[", "\n", "[", "'Benton'", ",", "'County'", "]", ",", "\n", "[", "'Benton'", ",", "'County'", ",", "','", ",", "'Washington'", "]", "\n", "]", "\n", "assert", "alias_database", ".", "_token_lookup", "[", "test_entity", "]", "==", "expected_tokenized_aliases", "\n", "\n", "# Check that the id map has 4 unique tokens", "\n", "assert", "len", "(", "alias_database", ".", "_id_map_lookup", "[", "test_entity", "]", ")", "==", "4", "\n", "\n", "# Check that the first token in each alias has the same local id", "\n", "test_id_array", "=", "alias_database", ".", "_id_array_lookup", "[", "test_entity", "]", "\n", "assert", "test_id_array", "[", "0", ",", "0", "]", "==", "test_id_array", "[", "1", ",", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database_test.AliasDatabaseTest.test_token_to_uid": [[68, 75], ["kglm.data.AliasDatabase", "kglm.data.AliasDatabase.token_to_uid", "kglm.data.AliasDatabase.token_to_uid"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.token_to_uid", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.token_to_uid"], ["", "def", "test_token_to_uid", "(", "self", ")", ":", "\n", "        ", "alias_database", "=", "AliasDatabase", "(", "token_lookup", "=", "self", ".", "token_lookup", ",", "\n", "id_map_lookup", "=", "self", ".", "id_map_lookup", ",", "\n", "id_array_lookup", "=", "self", ".", "id_array_lookup", ",", "\n", "token_to_entity_lookup", "=", "self", ".", "token_to_entity_lookup", ")", "\n", "assert", "alias_database", ".", "token_to_uid", "(", "'Entity1'", ",", "'Robert'", ")", "==", "1", "\n", "assert", "alias_database", ".", "token_to_uid", "(", "'Entity1'", ",", "'Nelson'", ")", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database_test.AliasDatabaseTest.test_tensorize_and_lookup": [[76, 117], ["kglm.data.AliasDatabase", "kglm.data.AliasDatabase.tensorize", "alias_database_test.AliasDatabaseTest.dataset.as_tensor_dict", "kglm.data.AliasDatabase.lookup", "kglm.data.AliasDatabase.reverse_lookup", "alias_database_test.AliasDatabaseTest.vocab.get_token_index", "alias_database_test.AliasDatabaseTest.vocab.get_token_index", "alias_database_test.AliasDatabaseTest.vocab.get_token_index"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.tensorize", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.lookup", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.reverse_lookup"], ["", "def", "test_tensorize_and_lookup", "(", "self", ")", ":", "\n", "# Tensor fields should be empty when ``AliasDatabase``` is created", "\n", "        ", "alias_database", "=", "AliasDatabase", "(", "token_lookup", "=", "self", ".", "token_lookup", ",", "\n", "id_map_lookup", "=", "self", ".", "id_map_lookup", ",", "\n", "id_array_lookup", "=", "self", ".", "id_array_lookup", ",", "\n", "token_to_entity_lookup", "=", "self", ".", "token_to_entity_lookup", ")", "\n", "assert", "not", "alias_database", ".", "is_tensorized", "\n", "\n", "# But should exist after ``AliasDatabase`` is tensorized", "\n", "alias_database", ".", "tensorize", "(", "self", ".", "vocab", ")", "\n", "return", "\n", "\n", "assert", "alias_database", ".", "is_tensorized", "\n", "assert", "alias_database", ".", "_global_id_lookup", "!=", "[", "]", "\n", "assert", "alias_database", ".", "_local_id_lookup", "!=", "[", "]", "\n", "\n", "tensor_dict", "=", "self", ".", "dataset", ".", "as_tensor_dict", "(", ")", "\n", "entity_ids", "=", "tensor_dict", "[", "'entity_identifiers'", "]", "[", "'entity_ids'", "]", "\n", "tokens", "=", "tensor_dict", "[", "'tokens'", "]", "[", "'tokens'", "]", "\n", "global_tensor", ",", "local_tensor", "=", "alias_database", ".", "lookup", "(", "entity_ids", ")", "\n", "entity_id_tensor", "=", "alias_database", ".", "reverse_lookup", "(", "tokens", ")", "\n", "\n", "# The first two dimensions should match the batch_size and sequence length of the index.", "\n", "# The next dimensions should be the max number of aliases of all entities (in this case 2", "\n", "# for 'Robert Logan', and 'Robby') and the max length of the aliases (again 2 because", "\n", "# 'Robert Logan' is two tokens).", "\n", "assert", "global_tensor", ".", "shape", "==", "(", "1", ",", "6", ",", "2", ",", "2", ")", "\n", "assert", "local_tensor", ".", "shape", "==", "(", "1", ",", "6", ",", "2", ",", "2", ")", "\n", "assert", "entity_id_tensor", ".", "shape", "==", "(", "1", ",", "6", ",", "1", ")", "\n", "\n", "# Check that the global ids match the vocabulary indices", "\n", "assert", "global_tensor", "[", "0", ",", "0", ",", "0", ",", "0", "]", "==", "self", ".", "vocab", ".", "get_token_index", "(", "'Robert'", ",", "namespace", "=", "'tokens'", ")", "\n", "assert", "global_tensor", "[", "0", ",", "1", ",", "0", ",", "0", "]", "==", "0", "# Padding since not an alias", "\n", "\n", "assert", "local_tensor", "[", "0", ",", "0", ",", "0", ",", "0", "]", "==", "1", "\n", "assert", "local_tensor", "[", "0", ",", "1", ",", "0", ",", "0", "]", "==", "0", "# Padding since not an alias", "\n", "\n", "match_token_idx", "=", "self", ".", "vocab", ".", "get_token_index", "(", "'Entity1'", ",", "namespace", "=", "'entity_ids'", ")", "\n", "nonmatch_token_idx", "=", "self", ".", "vocab", ".", "get_token_index", "(", "'Entity2'", ",", "namespace", "=", "'entity_ids'", ")", "\n", "assert", "entity_id_tensor", "[", "0", ",", "0", ",", "match_token_idx", "]", "==", "1", "\n", "assert", "entity_id_tensor", "[", "0", ",", "0", ",", "nonmathc_token_idx", "]", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp": [[14, 35], ["super().setUp", "allennlp.data.Vocabulary", "fancy_iterator_test.FancyIteratorTest.vocab.add_token_to_namespace", "fancy_iterator_test.FancyIteratorTest.vocab.add_token_to_namespace", "fancy_iterator_test.FancyIteratorTest.vocab.add_token_to_namespace", "fancy_iterator_test.FancyIteratorTest.vocab.add_token_to_namespace", "fancy_iterator_test.FancyIteratorTest.vocab.add_token_to_namespace", "fancy_iterator_test.FancyIteratorTest.vocab.add_token_to_namespace", "fancy_iterator_test.FancyIteratorTest.vocab.add_token_to_namespace", "fancy_iterator_test.FancyIteratorTest.vocab.add_token_to_namespace", "allennlp.data.token_indexers.SingleIdTokenIndexer", "fancy_iterator_test.FancyIteratorTest.create_instance", "fancy_iterator_test.FancyIteratorTest.create_instance", "fancy_iterator_test.FancyIteratorTest.create_instance", "fancy_iterator_test.FancyIteratorTest.create_instance", "fancy_iterator_test.FancyIteratorTest.create_instance"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.setUp", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.create_instance", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.create_instance", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.create_instance", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.create_instance", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.create_instance"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "setUp", "(", ")", "\n", "self", ".", "token_indexers", "=", "{", "\"tokens\"", ":", "SingleIdTokenIndexer", "(", ")", "}", "\n", "self", ".", "vocab", "=", "Vocabulary", "(", ")", "\n", "self", ".", "this_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'this'", ")", "\n", "self", ".", "is_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'is'", ")", "\n", "self", ".", "a_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'a'", ")", "\n", "self", ".", "sentence_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'sentence'", ")", "\n", "self", ".", "another_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'another'", ")", "\n", "self", ".", "yet_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'yet'", ")", "\n", "self", ".", "very_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'very'", ")", "\n", "self", ".", "long_index", "=", "self", ".", "vocab", ".", "add_token_to_namespace", "(", "'long'", ")", "\n", "instances", "=", "[", "\n", "self", ".", "create_instance", "(", "[", "\"this\"", ",", "\"is\"", ",", "\"a\"", ",", "\"sentence\"", "]", ")", ",", "\n", "self", ".", "create_instance", "(", "[", "\"this\"", ",", "\"is\"", ",", "\"another\"", ",", "\"sentence\"", "]", ")", ",", "\n", "self", ".", "create_instance", "(", "[", "\"yet\"", ",", "\"another\"", ",", "\"sentence\"", "]", ")", ",", "\n", "self", ".", "create_instance", "(", "[", "\"this\"", ",", "\"is\"", ",", "\"a\"", ",", "\"very\"", ",", "\"very\"", ",", "\"very\"", ",", "\"very\"", ",", "\"long\"", ",", "\"sentence\"", "]", ")", ",", "\n", "self", ".", "create_instance", "(", "[", "\"sentence\"", "]", ")", ",", "\n", "]", "\n", "\n", "self", ".", "instances", "=", "instances", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.create_instance": [[36, 40], ["allennlp.data.Instance", "allennlp.data.Token", "allennlp.data.fields.TextField"], "methods", ["None"], ["", "def", "create_instance", "(", "self", ",", "str_tokens", ":", "List", "[", "str", "]", ")", ":", "\n", "        ", "tokens", "=", "[", "Token", "(", "t", ")", "for", "t", "in", "str_tokens", "]", "\n", "instance", "=", "Instance", "(", "{", "'source'", ":", "TextField", "(", "tokens", ",", "self", ".", "token_indexers", ")", "}", ")", "\n", "return", "instance", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.fancy_iterator_test.FancyIteratorTest.test_truncate": [[41, 83], ["kglm.data.iterators.FancyIterator", "kglm.data.iterators.FancyIterator.index_with", "list", "kglm.data.iterators.FancyIterator", "kglm.data.iterators.FancyIterator.index_with", "list", "kglm.data.iterators.FancyIterator", "kglm.data.iterators.FancyIterator.index_with", "kglm.data.iterators.FancyIterator", "kglm.data.iterators.FancyIterator.index_with", "list", "kglm.data.iterators.FancyIterator.", "len", "kglm.data.iterators.FancyIterator.", "len", "fancy_iterator_test.FancyIteratorTest.assertRaises", "list", "kglm.data.iterators.FancyIterator.", "len", "kglm.data.iterators.FancyIterator."], "methods", ["None"], ["", "def", "test_truncate", "(", "self", ")", ":", "\n", "# Checks that the truncate parameter works as intended.", "\n", "\n", "# Since split size is less than the length of the \"very ... very long\" sentence, the", "\n", "# iterator should return one batch when the truncation is enabled.", "\n", "        ", "split_size", "=", "4", "\n", "truncated_iterator", "=", "FancyIterator", "(", "batch_size", "=", "5", ",", "\n", "split_size", "=", "split_size", ",", "\n", "splitting_keys", "=", "[", "'source'", "]", ",", "\n", "truncate", "=", "True", ")", "\n", "truncated_iterator", ".", "index_with", "(", "self", ".", "vocab", ")", "\n", "batches", "=", "list", "(", "truncated_iterator", "(", "self", ".", "instances", ",", "num_epochs", "=", "1", ")", ")", "\n", "assert", "len", "(", "batches", ")", "==", "1", "\n", "\n", "# When truncation is disabled the iterator should return 3 batches instead.", "\n", "non_truncated_iterator", "=", "FancyIterator", "(", "batch_size", "=", "5", ",", "\n", "split_size", "=", "split_size", ",", "\n", "splitting_keys", "=", "[", "'source'", "]", ",", "\n", "truncate", "=", "False", ")", "\n", "non_truncated_iterator", ".", "index_with", "(", "self", ".", "vocab", ")", "\n", "batches", "=", "list", "(", "non_truncated_iterator", "(", "self", ".", "instances", ",", "num_epochs", "=", "1", ")", ")", "\n", "assert", "len", "(", "batches", ")", "==", "3", "\n", "\n", "# When the batch size is larger than the number of instances, truncation will the iterator", "\n", "# to return zero batches of data (since some of the instances in the batch would consist", "\n", "# entirely of padding). Check that the iterator raises an error in this case.", "\n", "invalid_iterator", "=", "FancyIterator", "(", "batch_size", "=", "6", ",", "\n", "split_size", "=", "split_size", ",", "\n", "splitting_keys", "=", "[", "'source'", "]", ",", "\n", "truncate", "=", "True", ")", "\n", "invalid_iterator", ".", "index_with", "(", "self", ".", "vocab", ")", "\n", "with", "self", ".", "assertRaises", "(", "ConfigurationError", ")", ":", "\n", "            ", "batches", "=", "list", "(", "invalid_iterator", "(", "self", ".", "instances", ",", "num_epochs", "=", "1", ")", ")", "\n", "\n", "# If truncation is disabled then this should not cause an issue", "\n", "", "valid_iterator", "=", "FancyIterator", "(", "batch_size", "=", "6", ",", "\n", "split_size", "=", "split_size", ",", "\n", "splitting_keys", "=", "[", "'source'", "]", ",", "\n", "truncate", "=", "False", ")", "\n", "valid_iterator", ".", "index_with", "(", "self", ".", "vocab", ")", "\n", "batches", "=", "list", "(", "valid_iterator", "(", "self", ".", "instances", ",", "num_epochs", "=", "1", ")", ")", "\n", "assert", "len", "(", "batches", ")", "==", "3", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.__init__": [[27, 42], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "token_lookup", ":", "Dict", "[", "str", ",", "AliasList", "]", ",", "\n", "id_map_lookup", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", ",", "\n", "id_array_lookup", ":", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", ",", "\n", "token_to_entity_lookup", ":", "Dict", "[", "str", ",", "Set", "[", "Any", "]", "]", ")", "->", "None", ":", "\n", "        ", "self", ".", "_token_lookup", "=", "token_lookup", "or", "{", "}", "\n", "self", ".", "_id_map_lookup", "=", "id_map_lookup", "or", "{", "}", "\n", "self", ".", "_id_array_lookup", "=", "id_array_lookup", "or", "{", "}", "\n", "self", ".", "_token_to_entity_lookup", "=", "token_to_entity_lookup", "or", "{", "}", "\n", "\n", "self", ".", "is_tensorized", "=", "False", "\n", "self", ".", "_global_id_lookup", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "self", ".", "_local_id_lookup", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "self", ".", "_token_id_to_entity_id_lookup", ":", "List", "[", "torch", ".", "Tensor", "]", "=", "[", "]", "\n", "self", ".", "_num_entities", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.load": [[43, 92], ["logger.info", "allennlp.data.tokenizers.WordTokenizer", "collections.defaultdict", "allennlp.common.tqdm.Tqdm.tqdm", "cls", "open", "pickle.load", "pickle.load.items", "set", "len", "max", "numpy.zeros", "enumerate", "alias_database.tokenize_to_string", "set.update", "enumerate", "token_to_entity_lookup[].add", "alias_database.tokenize_to_string", "enumerate", "len"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.load", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.tokenize_to_string", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.tokenize_to_string"], ["", "@", "classmethod", "\n", "def", "load", "(", "cls", ",", "path", ":", "str", ")", ":", "\n", "\n", "        ", "logger", ".", "info", "(", "'Loading alias database from \"%s\". This will probably take a second.'", ",", "path", ")", "\n", "# TODO: Pretokenize the database to match the tokenization of the data itself. This", "\n", "# shouldn't be an issue ATM since I believe WordTokenizer() also uses SpaCy. But better to", "\n", "# air on the side of caution...", "\n", "tokenizer", "=", "WordTokenizer", "(", ")", "\n", "token_lookup", ":", "Dict", "[", "str", ",", "AliasList", "]", "=", "{", "}", "\n", "id_map_lookup", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "{", "}", "\n", "id_array_lookup", ":", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", "=", "{", "}", "\n", "token_to_entity_lookup", ":", "Dict", "[", "str", ",", "Set", "[", "Any", "]", "]", "=", "defaultdict", "(", "set", ")", "\n", "\n", "# Right now we only support loading the alias database from a pickle file.", "\n", "with", "open", "(", "path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "alias_lookup", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "for", "entity", ",", "aliases", "in", "Tqdm", ".", "tqdm", "(", "alias_lookup", ".", "items", "(", ")", ")", ":", "\n", "# Reverse token to potential entity lookup", "\n", "            ", "for", "alias", "in", "aliases", ":", "\n", "                ", "for", "token", "in", "tokenize_to_string", "(", "alias", ",", "tokenizer", ")", ":", "\n", "                    ", "token_to_entity_lookup", "[", "token", "]", ".", "add", "(", "entity", ")", "\n", "\n", "# Start by tokenizing the aliases", "\n", "", "", "tokenized_aliases", ":", "AliasList", "=", "[", "tokenize_to_string", "(", "alias", ",", "tokenizer", ")", "[", ":", "MAX_TOKENS", "]", "for", "alias", "in", "aliases", "]", "\n", "tokenized_aliases", "=", "tokenized_aliases", "[", ":", "MAX_ALIASES", "]", "\n", "token_lookup", "[", "entity", "]", "=", "tokenized_aliases", "\n", "\n", "# Next obtain the set of unqiue tokens appearing in aliases for this entity. Use this", "\n", "# to build a map from tokens to their unique id.", "\n", "unique_tokens", "=", "set", "(", ")", "\n", "for", "tokenized_alias", "in", "tokenized_aliases", ":", "\n", "                ", "unique_tokens", ".", "update", "(", "tokenized_alias", ")", "\n", "", "id_map", "=", "{", "token", ":", "i", "+", "1", "for", "i", ",", "token", "in", "enumerate", "(", "unique_tokens", ")", "}", "\n", "id_map_lookup", "[", "entity", "]", "=", "id_map", "\n", "\n", "# Lastly create an array associating the tokens in the alias to their corresponding ids.", "\n", "num_aliases", "=", "len", "(", "tokenized_aliases", ")", "\n", "max_alias_length", "=", "max", "(", "len", "(", "tokenized_alias", ")", "for", "tokenized_alias", "in", "tokenized_aliases", ")", "\n", "id_array", "=", "np", ".", "zeros", "(", "(", "num_aliases", ",", "max_alias_length", ")", ",", "dtype", "=", "int", ")", "\n", "for", "i", ",", "tokenized_alias", "in", "enumerate", "(", "tokenized_aliases", ")", ":", "\n", "                ", "for", "j", ",", "token", "in", "enumerate", "(", "tokenized_alias", ")", ":", "\n", "                    ", "id_array", "[", "i", ",", "j", "]", "=", "id_map", "[", "token", "]", "\n", "", "", "id_array_lookup", "[", "entity", "]", "=", "id_array", "\n", "\n", "", "return", "cls", "(", "token_lookup", "=", "token_lookup", ",", "\n", "id_map_lookup", "=", "id_map_lookup", ",", "\n", "id_array_lookup", "=", "id_array_lookup", ",", "\n", "token_to_entity_lookup", "=", "token_to_entity_lookup", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.token_to_uid": [[93, 99], ["None"], "methods", ["None"], ["", "def", "token_to_uid", "(", "self", ",", "entity", ":", "str", ",", "token", ":", "str", ")", "->", "int", ":", "\n", "        ", "if", "entity", "in", "self", ".", "_id_map_lookup", ":", "\n", "            ", "id_map", "=", "self", ".", "_id_map_lookup", "[", "entity", "]", "\n", "if", "token", "in", "id_map", ":", "\n", "                ", "return", "id_map", "[", "token", "]", "\n", "", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.tensorize": [[100, 163], ["logger.debug", "vocab.get_index_to_token_vocabulary", "range", "vocab.get_index_to_token_vocabulary", "range", "vocab.get_vocab_size", "logger.debug", "len", "len", "max", "torch.zeros", "enumerate", "alias_database.AliasDatabase._global_id_lookup.append", "torch.tensor", "alias_database.AliasDatabase._local_id_lookup.append", "len", "enumerate", "torch.tensor", "alias_database.AliasDatabase._token_id_to_entity_id_lookup.append", "alias_database.AliasDatabase._global_id_lookup.append", "alias_database.AliasDatabase._local_id_lookup.append", "len", "vocab.get_token_index", "alias_database.AliasDatabase._token_id_to_entity_id_lookup.append", "str", "vocab.get_token_index", "str"], "methods", ["None"], ["", "def", "tensorize", "(", "self", ",", "vocab", ":", "Vocabulary", ")", ":", "\n", "        ", "\"\"\"\n        Creates a list of tensors from the alias lookup.\n\n        After dataset creation, we'll mainly want to work with alias lists as lists of padded\n        tensors and their associated masks. This needs to be done **after** the vocabulary has\n        been created. Accordingly, in our current approach, this method must be called in the\n        forward pass of the model (since the operation is rather expensive we'll make sure that\n        it doesn't anything after the first time it is called).\n        \"\"\"", "\n", "# This operation is expensive, only do it once.", "\n", "if", "self", ".", "is_tensorized", ":", "\n", "            ", "return", "\n", "\n", "", "logger", ".", "debug", "(", "'Tensorizing AliasDatabase'", ")", "\n", "\n", "entity_idx_to_token", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", "'raw_entity_ids'", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "entity_idx_to_token", ")", ")", ":", "# pylint: disable=C0200", "\n", "            ", "entity", "=", "entity_idx_to_token", "[", "i", "]", "\n", "try", ":", "\n", "                ", "tokenized_aliases", "=", "self", ".", "_token_lookup", "[", "entity", "]", "\n", "", "except", "KeyError", ":", "\n", "# If we encounter non-entity tokens (e.g. padding and null) then just add", "\n", "# a blank placeholder - these should not be encountered during training.", "\n", "                ", "self", ".", "_global_id_lookup", ".", "append", "(", "None", ")", "\n", "self", ".", "_local_id_lookup", ".", "append", "(", "None", ")", "\n", "continue", "\n", "\n", "# Construct tensor of alias token indices from the global vocabulary.", "\n", "", "num_aliases", "=", "len", "(", "tokenized_aliases", ")", "\n", "max_alias_length", "=", "max", "(", "len", "(", "tokenized_alias", ")", "for", "tokenized_alias", "in", "tokenized_aliases", ")", "\n", "global_id_tensor", "=", "torch", ".", "zeros", "(", "num_aliases", ",", "max_alias_length", ",", "dtype", "=", "torch", ".", "int64", ",", "\n", "requires_grad", "=", "False", ")", "\n", "for", "j", ",", "tokenized_alias", "in", "enumerate", "(", "tokenized_aliases", ")", ":", "\n", "                ", "for", "k", ",", "token", "in", "enumerate", "(", "tokenized_alias", ")", ":", "\n", "# WARNING: Extremely janky cast to string", "\n", "                    ", "global_id_tensor", "[", "j", ",", "k", "]", "=", "vocab", ".", "get_token_index", "(", "str", "(", "token", ")", ",", "'tokens'", ")", "\n", "", "", "self", ".", "_global_id_lookup", ".", "append", "(", "global_id_tensor", ")", "\n", "\n", "# Convert array of local alias token indices into a tensor", "\n", "local_id_tensor", "=", "torch", ".", "tensor", "(", "self", ".", "_id_array_lookup", "[", "entity", "]", ",", "requires_grad", "=", "False", ")", "# pylint: disable=not-callable", "\n", "self", ".", "_local_id_lookup", ".", "append", "(", "local_id_tensor", ")", "\n", "\n", "# Build the tensorized token -> potential entities lookup.", "\n", "# NOTE: Initial approach will be to store just the necessary info to build one-hot vectors", "\n", "# on the fly since storing them will probably be way too expensive.", "\n", "", "token_idx_to_token", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", "'tokens'", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "token_idx_to_token", ")", ")", ":", "\n", "            ", "token", "=", "token_idx_to_token", "[", "i", "]", "\n", "try", ":", "\n", "                ", "potential_entities", "=", "self", ".", "_token_to_entity_lookup", "[", "token", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "self", ".", "_token_id_to_entity_id_lookup", ".", "append", "(", "None", ")", "\n", "", "else", ":", "\n", "                ", "potential_entity_ids", "=", "torch", ".", "tensor", "(", "[", "vocab", ".", "get_token_index", "(", "str", "(", "x", ")", ",", "'entity_ids'", ")", "for", "x", "in", "potential_entities", "]", ",", "\n", "dtype", "=", "torch", ".", "int64", ",", "\n", "requires_grad", "=", "False", ")", "\n", "self", ".", "_token_id_to_entity_id_lookup", ".", "append", "(", "potential_entity_ids", ")", "\n", "", "", "self", ".", "_num_entities", "=", "vocab", ".", "get_vocab_size", "(", "'entity_ids'", ")", "# Needed to get one-hot vector length", "\n", "\n", "self", ".", "is_tensorized", "=", "True", "\n", "\n", "logger", ".", "debug", "(", "'Done tensorizing AliasDatabase'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.lookup": [[164, 183], ["entity_ids.new_zeros", "entity_ids.new_zeros", "range", "range"], "methods", ["None"], ["", "def", "lookup", "(", "self", ",", "entity_ids", ":", "torch", ".", "Tensor", ")", "->", "Tuple", "[", "torch", ".", "Tensor", ",", "torch", ".", "Tensor", "]", ":", "\n", "        ", "\"\"\"Looks up alias tokens for the given entities.\"\"\"", "\n", "# Initialize empty tensors and fill them using the lookup", "\n", "batch_size", ",", "sequence_length", "=", "entity_ids", ".", "shape", "\n", "global_tensor", "=", "entity_ids", ".", "new_zeros", "(", "batch_size", ",", "sequence_length", ",", "MAX_ALIASES", ",", "MAX_TOKENS", ",", "\n", "requires_grad", "=", "False", ")", "\n", "local_tensor", "=", "entity_ids", ".", "new_zeros", "(", "batch_size", ",", "sequence_length", ",", "MAX_ALIASES", ",", "MAX_TOKENS", ",", "\n", "requires_grad", "=", "False", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "sequence_length", ")", ":", "\n", "                ", "entity_id", "=", "entity_ids", "[", "i", ",", "j", "]", "\n", "local_indices", "=", "self", ".", "_local_id_lookup", "[", "entity_id", "]", "\n", "global_indices", "=", "self", ".", "_global_id_lookup", "[", "entity_id", "]", "\n", "if", "local_indices", "is", "not", "None", ":", "\n", "                    ", "num_aliases", ",", "alias_length", "=", "local_indices", ".", "shape", "\n", "local_tensor", "[", "i", ",", "j", ",", ":", "num_aliases", ",", ":", "alias_length", "]", "=", "local_indices", "\n", "global_tensor", "[", "i", ",", "j", ",", ":", "num_aliases", ",", ":", "alias_length", "]", "=", "global_indices", "\n", "\n", "", "", "", "return", "global_tensor", ",", "local_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.reverse_lookup": [[184, 197], ["logger.debug", "tokens.new_zeros", "range", "range"], "methods", ["None"], ["", "def", "reverse_lookup", "(", "self", ",", "tokens", ":", "torch", ".", "Tensor", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"Looks up potential entity matches for the given token.\"\"\"", "\n", "batch_size", ",", "sequence_length", "=", "tokens", ".", "shape", "\n", "logger", ".", "debug", "(", "'Performing reverse lookup'", ")", "\n", "output", "=", "tokens", ".", "new_zeros", "(", "batch_size", ",", "sequence_length", ",", "self", ".", "_num_entities", ",", "\n", "dtype", "=", "torch", ".", "uint8", ",", "\n", "requires_grad", "=", "False", ")", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "sequence_length", ")", ":", "\n", "                ", "token_id", "=", "tokens", "[", "i", ",", "j", "]", "\n", "potential_entities", "=", "self", ".", "_token_id_to_entity_id_lookup", "[", "token_id", "]", "\n", "output", "[", "i", ",", "j", ",", "potential_entities", "]", "=", "1", "\n", "", "", "return", "output", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.tokenize_to_string": [[20, 23], ["tokenizer.tokenize"], "function", ["None"], ["def", "tokenize_to_string", "(", "text", ":", "str", ",", "tokenizer", ":", "Tokenizer", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"Sigh\"\"\"", "\n", "return", "[", "token", ".", "text", "for", "token", "in", "tokenizer", ".", "tokenize", "(", "text", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.__init__": [[31, 48], ["allennlp.data.vocabulary.Vocabulary.__init__"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "counter", ":", "Dict", "[", "set", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "min_count", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "max_vocab_size", ":", "Union", "[", "int", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "non_padded_namespaces", ":", "Iterable", "[", "str", "]", "=", "EXTENDED_NON_PADDED_NAMESPACES", ",", "\n", "pretrained_files", ":", "Optional", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "only_include_pretrained_words", ":", "bool", "=", "False", ",", "\n", "tokens_to_add", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "min_pretrained_embeddings", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "super", "(", "ExtendedVocabulary", ",", "self", ")", ".", "__init__", "(", "counter", "=", "counter", ",", "\n", "min_count", "=", "min_count", ",", "\n", "max_vocab_size", "=", "max_vocab_size", ",", "\n", "non_padded_namespaces", "=", "non_padded_namespaces", ",", "\n", "pretrained_files", "=", "pretrained_files", ",", "\n", "only_include_pretrained_words", "=", "only_include_pretrained_words", ",", "\n", "tokens_to_add", "=", "tokens_to_add", ",", "\n", "min_pretrained_embeddings", "=", "min_pretrained_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary._extend": [[49, 138], ["set", "extended_vocabulary.ExtendedVocabulary._token_to_index.add_non_padded_namespaces", "extended_vocabulary.ExtendedVocabulary._index_to_token.add_non_padded_namespaces", "extended_vocabulary.ExtendedVocabulary._non_padded_namespaces.update", "tokens_to_add.items", "isinstance", "collections.defaultdict", "list", "list.sort", "any", "any", "allennlp.common.checks.ConfigurationError", "allennlp.data.vocabulary._read_pretrained_tokens", "min_pretrained_embeddings.get", "set", "set", "counter[].items", "extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "tokens_to_add.get", "allennlp.data.vocabulary.namespace_match", "allennlp.data.vocabulary.namespace_match", "min_count.get", "extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "min_count.get", "extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "extended_vocabulary.ExtendedVocabulary.add_token_to_namespace", "min_count.get"], "methods", ["None"], ["", "def", "_extend", "(", "self", ",", "\n", "counter", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "min_count", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "max_vocab_size", ":", "Union", "[", "int", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "non_padded_namespaces", ":", "Iterable", "[", "str", "]", "=", "EXTENDED_NON_PADDED_NAMESPACES", ",", "\n", "pretrained_files", ":", "Optional", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "only_include_pretrained_words", ":", "bool", "=", "False", ",", "\n", "tokens_to_add", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "min_pretrained_embeddings", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Modifies the default ``Vocabulary._extend`` method so that tokens which would be\n        eliminated are instead added to \"*unk\" namespaces.\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "max_vocab_size", ",", "dict", ")", ":", "\n", "            ", "int_max_vocab_size", "=", "max_vocab_size", "\n", "max_vocab_size", "=", "defaultdict", "(", "lambda", ":", "int_max_vocab_size", ")", "# type: ignore", "\n", "", "min_count", "=", "min_count", "or", "{", "}", "\n", "pretrained_files", "=", "pretrained_files", "or", "{", "}", "\n", "min_pretrained_embeddings", "=", "min_pretrained_embeddings", "or", "{", "}", "\n", "non_padded_namespaces", "=", "set", "(", "non_padded_namespaces", ")", "\n", "counter", "=", "counter", "or", "{", "}", "\n", "tokens_to_add", "=", "tokens_to_add", "or", "{", "}", "\n", "\n", "self", ".", "_retained_counter", "=", "counter", "\n", "# Make sure vocabulary extension is safe.", "\n", "current_namespaces", "=", "{", "*", "self", ".", "_token_to_index", "}", "\n", "extension_namespaces", "=", "{", "*", "counter", ",", "*", "tokens_to_add", "}", "\n", "\n", "for", "namespace", "in", "current_namespaces", "&", "extension_namespaces", ":", "\n", "# if new namespace was already present", "\n", "# Either both should be padded or none should be.", "\n", "            ", "original_padded", "=", "not", "any", "(", "namespace_match", "(", "pattern", ",", "namespace", ")", "\n", "for", "pattern", "in", "self", ".", "_non_padded_namespaces", ")", "\n", "extension_padded", "=", "not", "any", "(", "namespace_match", "(", "pattern", ",", "namespace", ")", "\n", "for", "pattern", "in", "non_padded_namespaces", ")", "\n", "if", "original_padded", "!=", "extension_padded", ":", "\n", "                ", "raise", "ConfigurationError", "(", "\"Common namespace {} has conflicting \"", ".", "format", "(", "namespace", ")", "+", "\n", "\"setting of padded = True/False. \"", "+", "\n", "\"Hence extension cannot be done.\"", ")", "\n", "\n", "# Add new non-padded namespaces for extension", "\n", "", "", "self", ".", "_token_to_index", ".", "add_non_padded_namespaces", "(", "non_padded_namespaces", ")", "\n", "self", ".", "_index_to_token", ".", "add_non_padded_namespaces", "(", "non_padded_namespaces", ")", "\n", "self", ".", "_non_padded_namespaces", ".", "update", "(", "non_padded_namespaces", ")", "\n", "\n", "for", "namespace", "in", "counter", ":", "# pylint: disable=too-many-nested-blocks", "\n", "            ", "if", "namespace", "in", "pretrained_files", ":", "\n", "                ", "pretrained_list", "=", "_read_pretrained_tokens", "(", "pretrained_files", "[", "namespace", "]", ")", "\n", "min_embeddings", "=", "min_pretrained_embeddings", ".", "get", "(", "namespace", ",", "0", ")", "\n", "if", "min_embeddings", ">", "0", ":", "\n", "                    ", "tokens_old", "=", "tokens_to_add", ".", "get", "(", "namespace", ",", "[", "]", ")", "\n", "tokens_new", "=", "pretrained_list", "[", ":", "min_embeddings", "]", "\n", "tokens_to_add", "[", "namespace", "]", "=", "tokens_old", "+", "tokens_new", "\n", "", "pretrained_set", "=", "set", "(", "pretrained_list", ")", "\n", "", "else", ":", "\n", "                ", "pretrained_set", "=", "set", "(", ")", "\n", "", "token_counts", "=", "list", "(", "counter", "[", "namespace", "]", ".", "items", "(", ")", ")", "\n", "token_counts", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "try", ":", "\n", "                ", "max_vocab", "=", "max_vocab_size", "[", "namespace", "]", "\n", "if", "max_vocab", "is", "not", "None", ":", "\n", "                    ", "unk_counts", "=", "token_counts", "[", "max_vocab", ":", "]", "# Add these to *unk namespace", "\n", "token_counts", "=", "token_counts", "[", ":", "max_vocab", "]", "\n", "", "else", ":", "\n", "                    ", "unk_counts", "=", "[", "]", "\n", "", "", "except", "KeyError", ":", "\n", "                ", "unk_counts", "=", "[", "]", "\n", "", "for", "token", ",", "count", "in", "token_counts", ":", "\n", "                ", "if", "pretrained_set", "is", "not", "None", ":", "\n", "                    ", "if", "only_include_pretrained_words", ":", "\n", "                        ", "if", "token", "in", "pretrained_set", ":", "\n", "                            ", "if", "count", ">=", "min_count", ".", "get", "(", "namespace", ",", "1", ")", ":", "\n", "                                ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "", "else", ":", "\n", "                                ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", "+", "'_unk'", ")", "\n", "", "", "", "elif", "token", "in", "pretrained_set", "or", "count", ">=", "min_count", ".", "get", "(", "namespace", ",", "1", ")", ":", "\n", "                        ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "", "else", ":", "\n", "                        ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", "+", "'_unk'", ")", "\n", "", "", "elif", "count", ">=", "min_count", ".", "get", "(", "namespace", ",", "1", ")", ":", "\n", "                    ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "", "else", ":", "\n", "                    ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", "+", "'_unk'", ")", "\n", "", "", "for", "token", ",", "count", "in", "unk_counts", ":", "\n", "                ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", "+", "'_unk'", ")", "\n", "\n", "", "", "for", "namespace", ",", "tokens", "in", "tokens_to_add", ".", "items", "(", ")", ":", "\n", "            ", "for", "token", "in", "tokens", ":", "\n", "                ", "self", ".", "add_token_to_namespace", "(", "token", ",", "namespace", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_instances": [[139, 169], ["logger.info", "collections.defaultdict", "allennlp.common.tqdm.Tqdm.tqdm", "cls", "allennlp.data.instance.count_vocab_items", "collections.defaultdict"], "methods", ["None"], ["", "", "", "@", "classmethod", "\n", "@", "overrides", "\n", "def", "from_instances", "(", "cls", ",", "\n", "instances", ":", "Iterable", "[", "'adi.Instance'", "]", ",", "\n", "min_count", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ",", "\n", "max_vocab_size", ":", "Union", "[", "int", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "None", ",", "\n", "non_padded_namespaces", ":", "Iterable", "[", "str", "]", "=", "EXTENDED_NON_PADDED_NAMESPACES", ",", "\n", "pretrained_files", ":", "Optional", "[", "Dict", "[", "str", ",", "str", "]", "]", "=", "None", ",", "\n", "only_include_pretrained_words", ":", "bool", "=", "False", ",", "\n", "tokens_to_add", ":", "Dict", "[", "str", ",", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "min_pretrained_embeddings", ":", "Dict", "[", "str", ",", "int", "]", "=", "None", ")", "->", "'ExtendedVocabulary'", ":", "\n", "        ", "\"\"\"\n        Constructs a vocabulary given a collection of `Instances` and some parameters.\n        We count all of the vocabulary items in the instances, then pass those counts\n        and the other parameters, to :func:`__init__`.  See that method for a description\n        of what the other parameters do.\n        \"\"\"", "\n", "logger", ".", "info", "(", "\"Fitting token dictionary from dataset.\"", ")", "\n", "namespace_token_counts", ":", "Dict", "[", "Set", "[", "Any", "]", ",", "Dict", "[", "str", ",", "int", "]", "]", "=", "defaultdict", "(", "lambda", ":", "defaultdict", "(", "int", ")", ")", "\n", "for", "instance", "in", "Tqdm", ".", "tqdm", "(", "instances", ")", ":", "\n", "            ", "instance", ".", "count_vocab_items", "(", "namespace_token_counts", ")", "\n", "\n", "", "return", "cls", "(", "counter", "=", "namespace_token_counts", ",", "\n", "min_count", "=", "min_count", ",", "\n", "max_vocab_size", "=", "max_vocab_size", ",", "\n", "non_padded_namespaces", "=", "non_padded_namespaces", ",", "\n", "pretrained_files", "=", "pretrained_files", ",", "\n", "only_include_pretrained_words", "=", "only_include_pretrained_words", ",", "\n", "tokens_to_add", "=", "tokens_to_add", ",", "\n", "min_pretrained_embeddings", "=", "min_pretrained_embeddings", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params": [[171, 246], ["params.pop", "params.pop", "params.pop", "params.pop", "allennlp.data.vocabulary.pop_max_vocab_size", "params.pop", "params.pop", "params.pop", "params.pop_bool", "params.pop", "params.assert_empty", "extended_vocabulary.ExtendedVocabulary.from_instances", "cls.by_name().from_params", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "allennlp.data.vocabulary.Vocabulary.from_files", "allennlp.data.vocabulary.Vocabulary.from_files.extend_from_instances", "logger.info", "logger.info", "params.assert_empty", "cls.by_name"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_instances", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params"], ["", "@", "classmethod", "\n", "def", "from_params", "(", "cls", ",", "params", ":", "Params", ",", "instances", ":", "Iterable", "[", "'adi.Instance'", "]", "=", "None", ")", ":", "# type: ignore", "\n", "        ", "\"\"\"\n        There are two possible ways to build a vocabulary; from a\n        collection of instances, using :func:`Vocabulary.from_instances`, or\n        from a pre-saved vocabulary, using :func:`Vocabulary.from_files`.\n        You can also extend pre-saved vocabulary with collection of instances\n        using this method. This method wraps these options, allowing their\n        specification from a ``Params`` object, generated from a JSON\n        configuration file.\n        Parameters\n        ----------\n        params: Params, required.\n        instances: Iterable['adi.Instance'], optional\n            If ``params`` doesn't contain a ``directory_path`` key,\n            the ``Vocabulary`` can be built directly from a collection of\n            instances (i.e. a dataset). If ``extend`` key is set False,\n            dataset instances will be ignored and final vocabulary will be\n            one loaded from ``directory_path``. If ``extend`` key is set True,\n            dataset instances will be used to extend the vocabulary loaded\n            from ``directory_path`` and that will be final vocabulary used.\n        Returns\n        -------\n        A ``Vocabulary``.\n        \"\"\"", "\n", "# pylint: disable=arguments-differ", "\n", "# Vocabulary is ``Registrable`` so that you can configure a custom subclass,", "\n", "# but (unlike most of our registrables) almost everyone will want to use the", "\n", "# base implementation. So instead of having an abstract ``VocabularyBase`` or", "\n", "# such, we just add the logic for instantiating a registered subclass here,", "\n", "# so that most users can continue doing what they were doing.", "\n", "vocab_type", "=", "params", ".", "pop", "(", "\"type\"", ",", "None", ")", "\n", "if", "vocab_type", "is", "not", "None", ":", "\n", "            ", "return", "cls", ".", "by_name", "(", "vocab_type", ")", ".", "from_params", "(", "params", "=", "params", ",", "instances", "=", "instances", ")", "\n", "\n", "", "extend", "=", "params", ".", "pop", "(", "\"extend\"", ",", "False", ")", "\n", "vocabulary_directory", "=", "params", ".", "pop", "(", "\"directory_path\"", ",", "None", ")", "\n", "if", "not", "vocabulary_directory", "and", "not", "instances", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"You must provide either a Params object containing a \"", "\n", "\"vocab_directory key or a Dataset to build a vocabulary from.\"", ")", "\n", "", "if", "extend", "and", "not", "instances", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"'extend' is true but there are not instances passed to extend.\"", ")", "\n", "", "if", "extend", "and", "not", "vocabulary_directory", ":", "\n", "            ", "raise", "ConfigurationError", "(", "\"'extend' is true but there is not 'directory_path' to extend from.\"", ")", "\n", "\n", "", "if", "vocabulary_directory", "and", "instances", ":", "\n", "            ", "if", "extend", ":", "\n", "                ", "logger", ".", "info", "(", "\"Loading Vocab from files and extending it with dataset.\"", ")", "\n", "", "else", ":", "\n", "                ", "logger", ".", "info", "(", "\"Loading Vocab from files instead of dataset.\"", ")", "\n", "\n", "", "", "if", "vocabulary_directory", ":", "\n", "            ", "vocab", "=", "Vocabulary", ".", "from_files", "(", "vocabulary_directory", ")", "\n", "if", "not", "extend", ":", "\n", "                ", "params", ".", "assert_empty", "(", "\"Vocabulary - from files\"", ")", "\n", "return", "vocab", "\n", "", "", "if", "extend", ":", "\n", "            ", "vocab", ".", "extend_from_instances", "(", "params", ",", "instances", "=", "instances", ")", "\n", "return", "vocab", "\n", "", "min_count", "=", "params", ".", "pop", "(", "\"min_count\"", ",", "None", ")", "\n", "max_vocab_size", "=", "pop_max_vocab_size", "(", "params", ")", "\n", "non_padded_namespaces", "=", "params", ".", "pop", "(", "\"non_padded_namespaces\"", ",", "EXTENDED_NON_PADDED_NAMESPACES", ")", "\n", "pretrained_files", "=", "params", ".", "pop", "(", "\"pretrained_files\"", ",", "{", "}", ")", "\n", "min_pretrained_embeddings", "=", "params", ".", "pop", "(", "\"min_pretrained_embeddings\"", ",", "None", ")", "\n", "only_include_pretrained_words", "=", "params", ".", "pop_bool", "(", "\"only_include_pretrained_words\"", ",", "False", ")", "\n", "tokens_to_add", "=", "params", ".", "pop", "(", "\"tokens_to_add\"", ",", "None", ")", "\n", "params", ".", "assert_empty", "(", "\"Vocabulary - from dataset\"", ")", "\n", "return", "ExtendedVocabulary", ".", "from_instances", "(", "instances", "=", "instances", ",", "\n", "min_count", "=", "min_count", ",", "\n", "max_vocab_size", "=", "max_vocab_size", ",", "\n", "non_padded_namespaces", "=", "non_padded_namespaces", ",", "\n", "pretrained_files", "=", "pretrained_files", ",", "\n", "only_include_pretrained_words", "=", "only_include_pretrained_words", ",", "\n", "tokens_to_add", "=", "tokens_to_add", ",", "\n", "min_pretrained_embeddings", "=", "min_pretrained_embeddings", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.fancy_iterator.FancyIterator.__init__": [[26, 46], ["allennlp.data.iterators.data_iterator.DataIterator.__init__"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "batch_size", ":", "int", ",", "\n", "split_size", ":", "int", ",", "\n", "splitting_keys", ":", "List", "[", "str", "]", ",", "\n", "truncate", ":", "bool", "=", "True", ",", "\n", "instances_per_epoch", ":", "int", "=", "None", ",", "\n", "max_instances_in_memory", ":", "int", "=", "None", ",", "\n", "cache_instances", ":", "bool", "=", "False", ",", "\n", "track_epoch", ":", "bool", "=", "False", ",", "\n", "maximum_samples_per_batch", ":", "Tuple", "[", "str", ",", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "super", "(", "FancyIterator", ",", "self", ")", ".", "__init__", "(", "\n", "batch_size", "=", "batch_size", ",", "\n", "instances_per_epoch", "=", "instances_per_epoch", ",", "\n", "max_instances_in_memory", "=", "max_instances_in_memory", ",", "\n", "cache_instances", "=", "cache_instances", ",", "\n", "track_epoch", "=", "track_epoch", ",", "\n", "maximum_samples_per_batch", "=", "maximum_samples_per_batch", ")", "\n", "self", ".", "_splitting_keys", "=", "splitting_keys", "\n", "self", ".", "_split_size", "=", "split_size", "\n", "self", ".", "_truncate", "=", "truncate", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.fancy_iterator.FancyIterator.__call__": [[47, 110], ["id", "list", "allennlp.common.checks.ConfigurationError", "itertools.count", "range", "numpy.zeros", "copy.deepcopy", "copy.deepcopy.fields.items", "allennlp.data.instance.Instance", "fancy_iterator.FancyIterator._generate_batches", "len", "random.shuffle", "collections.deque", "fancy_iterator.FancyIterator._split", "numpy.argmin", "queues[].extend", "isinstance", "batch.get_padding_lengths", "range", "field.empty_field", "allennlp.data.iterators.data_iterator.add_epoch_number", "batch.index_instances", "batch.as_tensor_dict"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.fancy_iterator.FancyIterator._generate_batches", "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.fancy_iterator.FancyIterator._split", "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.get_padding_lengths", "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.empty_field"], ["", "def", "__call__", "(", "self", ",", "\n", "instances", ":", "Iterable", "[", "Instance", "]", ",", "\n", "num_epochs", ":", "int", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "False", ")", "->", "Iterator", "[", "TensorDict", "]", ":", "\n", "\n", "        ", "key", "=", "id", "(", "instances", ")", "\n", "starting_epoch", "=", "self", ".", "_epochs", "[", "key", "]", "\n", "\n", "# In order to ensure that we are (almost) constantly streaming data to the model we", "\n", "# need to have all of the instances in memory ($$$)", "\n", "instance_list", "=", "list", "(", "instances", ")", "\n", "\n", "if", "(", "self", ".", "_batch_size", ">", "len", "(", "instance_list", ")", ")", "and", "self", ".", "_truncate", ":", "\n", "            ", "raise", "ConfigurationError", "(", "'FancyIterator will not return any data when the batch size '", "\n", "'is larger than number of instances and truncation is enabled. '", "\n", "'To fix this either use a smaller batch size (better for '", "\n", "'training) or disable truncation (better for validation).'", ")", "\n", "\n", "", "if", "num_epochs", "is", "None", ":", "\n", "            ", "epochs", ":", "Iterable", "[", "int", "]", "=", "itertools", ".", "count", "(", "starting_epoch", ")", "\n", "", "else", ":", "\n", "            ", "epochs", "=", "range", "(", "starting_epoch", ",", "starting_epoch", "+", "num_epochs", ")", "\n", "\n", "", "for", "epoch", "in", "epochs", ":", "\n", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "random", ".", "shuffle", "(", "instance_list", ")", "\n", "\n", "# We create queues for each instance in the batch, and greedily fill them to try and", "\n", "# ensure each queue's length is roughly equal in size.", "\n", "", "queues", ":", "List", "[", "Deque", "[", "Instance", "]", "]", "=", "[", "deque", "(", ")", "for", "_", "in", "range", "(", "self", ".", "_batch_size", ")", "]", "\n", "queue_lengths", "=", "np", ".", "zeros", "(", "self", ".", "_batch_size", ",", "dtype", "=", "int", ")", "\n", "for", "instance", "in", "instances", ":", "\n", "\n", "# Now we split the instance into chunks.", "\n", "                ", "chunks", ",", "length", "=", "self", ".", "_split", "(", "instance", ")", "\n", "\n", "# Next we identify which queue is the shortest and add the chunks to that queue.", "\n", "destination", "=", "np", ".", "argmin", "(", "queue_lengths", ")", "\n", "queues", "[", "destination", "]", ".", "extend", "(", "chunks", ")", "\n", "queue_lengths", "[", "destination", "]", "+=", "length", "\n", "\n", "# We need a NULL instance to replace the output of an exhausted queue if we are evaluating", "\n", "", "prototype", "=", "deepcopy", "(", "chunks", "[", "-", "1", "]", ")", "\n", "new_fields", ":", "Dict", "[", "str", ",", "Field", "]", "=", "{", "}", "\n", "for", "name", ",", "field", "in", "prototype", ".", "fields", ".", "items", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "field", ",", "MetadataField", ")", ":", "\n", "                    ", "new_fields", "[", "name", "]", "=", "field", "\n", "", "else", ":", "\n", "                    ", "new_fields", "[", "name", "]", "=", "field", ".", "empty_field", "(", ")", "\n", "", "", "blank_instance", "=", "Instance", "(", "new_fields", ")", "\n", "\n", "for", "batch", "in", "self", ".", "_generate_batches", "(", "queues", ",", "blank_instance", ")", ":", "\n", "                ", "if", "self", ".", "_track_epoch", ":", "\n", "                    ", "add_epoch_number", "(", "batch", ",", "epoch", ")", "\n", "\n", "", "if", "self", ".", "vocab", "is", "not", "None", ":", "\n", "                    ", "batch", ".", "index_instances", "(", "self", ".", "vocab", ")", "\n", "\n", "", "padding_lengths", "=", "batch", ".", "get_padding_lengths", "(", ")", "\n", "yield", "batch", ".", "as_tensor_dict", "(", "padding_lengths", ")", ",", "1", "\n", "\n", "", "self", ".", "_epochs", "[", "key", "]", "=", "epoch", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.fancy_iterator.FancyIterator._split": [[111, 163], ["len", "list", "enumerate", "range", "list.append", "zip", "chunks.append", "kglm.data.fields.SequentialArrayField", "kglm.data.fields.SequentialArrayField", "isinstance", "allennlp.data.instance.Instance", "numpy.array", "numpy.array", "allennlp.data.fields.TextField", "isinstance", "kglm.data.fields.SequentialArrayField", "isinstance", "allennlp.data.fields.ListField", "NotImplementedError"], "methods", ["None"], ["", "", "def", "_split", "(", "self", ",", "instance", ":", "Instance", ")", "->", "Tuple", "[", "List", "[", "Instance", "]", ",", "int", "]", ":", "\n", "# Determine the size of the sequence inside the instance.", "\n", "        ", "true_length", "=", "len", "(", "instance", "[", "'source'", "]", ")", "\n", "if", "(", "true_length", "%", "self", ".", "_split_size", ")", "!=", "0", ":", "\n", "            ", "offset", "=", "1", "\n", "", "else", ":", "\n", "            ", "offset", "=", "0", "\n", "", "padded_length", "=", "self", ".", "_split_size", "*", "(", "true_length", "//", "self", ".", "_split_size", "+", "offset", ")", "\n", "\n", "# Determine the split indices.", "\n", "split_indices", "=", "list", "(", "range", "(", "0", ",", "true_length", ",", "self", ".", "_split_size", ")", ")", "\n", "if", "true_length", ">", "split_indices", "[", "-", "1", "]", ":", "\n", "            ", "split_indices", ".", "append", "(", "true_length", ")", "\n", "\n", "# Determine which fields are not going to be split", "\n", "", "constant_fields", "=", "[", "x", "for", "x", "in", "instance", ".", "fields", "if", "x", "not", "in", "self", ".", "_splitting_keys", "]", "\n", "\n", "# Create the list of chunks", "\n", "chunks", ":", "List", "[", "Instance", "]", "=", "[", "]", "\n", "\n", "for", "i", ",", "(", "start", ",", "end", ")", "in", "enumerate", "(", "zip", "(", "split_indices", "[", ":", "-", "1", "]", ",", "split_indices", "[", "1", ":", "]", ")", ")", ":", "\n", "\n", "# Copy all of the constant fields from the instance to the chunk.", "\n", "            ", "chunk_fields", "=", "{", "key", ":", "instance", "[", "key", "]", "for", "key", "in", "constant_fields", "}", "\n", "\n", "# Determine whether or not to signal model to reset.", "\n", "if", "i", "==", "0", ":", "\n", "                ", "reset", "=", "SequentialArrayField", "(", "np", ".", "array", "(", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "", "else", ":", "\n", "                ", "reset", "=", "SequentialArrayField", "(", "np", ".", "array", "(", "0", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "", "chunk_fields", "[", "'reset'", "]", "=", "reset", "\n", "\n", "# Obtain splits derived from sequence fields.", "\n", "for", "key", "in", "self", ".", "_splitting_keys", ":", "\n", "                ", "source_field", "=", "instance", "[", "key", "]", "\n", "# pylint: disable=protected-access", "\n", "if", "isinstance", "(", "source_field", ",", "TextField", ")", ":", "\n", "                    ", "split_field", "=", "TextField", "(", "source_field", ".", "tokens", "[", "start", ":", "end", "]", ",", "\n", "source_field", ".", "_token_indexers", ")", "\n", "", "elif", "isinstance", "(", "source_field", ",", "SequentialArrayField", ")", ":", "\n", "# TODO: Figure out how to use sequence dim here...", "\n", "                    ", "split_field", "=", "SequentialArrayField", "(", "source_field", ".", "array", "[", "start", ":", "end", "]", ",", "\n", "dtype", "=", "source_field", ".", "_dtype", ")", "\n", "", "elif", "isinstance", "(", "source_field", ",", "ListField", ")", ":", "\n", "                    ", "split_field", "=", "ListField", "(", "source_field", ".", "field_list", "[", "start", ":", "end", "]", ")", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "(", "'FancyIterator currently only supports splitting '", "\n", "'`TextField`s or `SequentialArrayField`s.'", ")", "\n", "", "chunk_fields", "[", "key", "]", "=", "split_field", "\n", "", "chunks", ".", "append", "(", "Instance", "(", "chunk_fields", ")", ")", "\n", "\n", "", "return", "chunks", ",", "padded_length", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.fancy_iterator.FancyIterator._generate_batches": [[164, 184], ["max", "range", "allennlp.data.dataset.Batch", "len", "instances.append", "q.popleft"], "methods", ["None"], ["", "def", "_generate_batches", "(", "self", ",", "\n", "queues", ":", "List", "[", "Deque", "[", "Instance", "]", "]", ",", "\n", "blank_instance", ":", "Instance", ")", "->", "Iterator", "[", "Batch", "]", ":", "\n", "        ", "num_iter", "=", "max", "(", "len", "(", "q", ")", "for", "q", "in", "queues", ")", "\n", "for", "_", "in", "range", "(", "num_iter", ")", ":", "\n", "            ", "instances", ":", "List", "[", "Instance", "]", "=", "[", "]", "\n", "for", "q", "in", "queues", ":", "\n", "                ", "try", ":", "\n", "                    ", "instance", "=", "q", ".", "popleft", "(", ")", "\n", "", "except", "IndexError", ":", "# A queue is depleted", "\n", "# If we're training, we break to avoid densely padded inputs (since this biases", "\n", "# the model to overfit the longer sequences).", "\n", "                    ", "if", "self", ".", "_truncate", ":", "\n", "                        ", "return", "\n", "# But if we're evaluating we do want the padding, so that we don't skip anything.", "\n", "", "else", ":", "\n", "                        ", "instance", "=", "blank_instance", "\n", "", "", "instances", ".", "append", "(", "instance", ")", "\n", "", "batch", "=", "Batch", "(", "instances", ")", "\n", "yield", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.fancy_iterator.FancyIterator.get_num_batches": [[185, 187], ["None"], "methods", ["None"], ["", "", "def", "get_num_batches", "(", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ")", "->", "float", ":", "\n", "        ", "return", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.awd_iterator.AwdIterator.__init__": [[18, 34], ["allennlp.data.iterators.DataIterator.__init__"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "split_size", ":", "int", ",", "\n", "batch_size", ":", "int", "=", "32", ",", "\n", "instances_per_epoch", ":", "int", "=", "None", ",", "\n", "max_instances_in_memory", ":", "int", "=", "None", ",", "\n", "cache_instances", ":", "bool", "=", "False", ",", "\n", "track_epoch", ":", "bool", "=", "False", ",", "\n", "maximum_samples_per_batch", ":", "Tuple", "[", "str", ",", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "super", "(", "AwdIterator", ",", "self", ")", ".", "__init__", "(", "\n", "batch_size", "=", "batch_size", ",", "\n", "instances_per_epoch", "=", "instances_per_epoch", ",", "\n", "max_instances_in_memory", "=", "max_instances_in_memory", ",", "\n", "cache_instances", "=", "cache_instances", ",", "\n", "track_epoch", "=", "track_epoch", ",", "\n", "maximum_samples_per_batch", "=", "maximum_samples_per_batch", ")", "\n", "self", ".", "_split_size", "=", "split_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.awd_iterator.AwdIterator.__call__": [[35, 88], ["id", "itertools.count", "range", "list", "torch.cat", "big_ass_sequence.view.view.narrow", "big_ass_sequence.view.view.view", "instance.index_fields", "instance.as_tensor_dict", "max", "min", "min", "torch.ones", "torch.zeros", "int", "numpy.random.random", "numpy.random.normal"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "\n", "instances", ":", "Iterable", "[", "Instance", "]", ",", "\n", "num_epochs", ":", "int", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "False", ")", "->", "Iterator", "[", "Tuple", "[", "TensorDict", ",", "float", "]", "]", ":", "\n", "        ", "key", "=", "id", "(", "instances", ")", "\n", "starting_epoch", "=", "self", ".", "_epochs", "[", "key", "]", "\n", "\n", "if", "num_epochs", "is", "None", ":", "\n", "            ", "epochs", ":", "Iterable", "[", "int", "]", "=", "itertools", ".", "count", "(", "starting_epoch", ")", "\n", "", "else", ":", "\n", "            ", "epochs", "=", "range", "(", "starting_epoch", ",", "starting_epoch", "+", "num_epochs", ")", "\n", "\n", "# Following the original implementation we will simply concatenate all", "\n", "# of the tensors together, then split it into batch_size pieces and", "\n", "# yield little chunks of the big array. Although the chunk sizes will", "\n", "# vary, the array will otherwise always be in the same order.", "\n", "", "for", "epoch", "in", "epochs", ":", "\n", "            ", "instance_list", "=", "list", "(", "instances", ")", "\n", "for", "instance", "in", "instance_list", ":", "\n", "                ", "instance", ".", "index_fields", "(", "self", ".", "vocab", ")", "\n", "\n", "", "tensor_dicts", "=", "[", "instance", ".", "as_tensor_dict", "(", ")", "for", "instance", "in", "instance_list", "]", "\n", "big_ass_sequence", "=", "torch", ".", "cat", "(", "[", "x", "[", "'tokens'", "]", "[", "'tokens'", "]", "for", "x", "in", "tensor_dicts", "]", ",", "\n", "dim", "=", "0", ")", "\n", "n_batch", "=", "big_ass_sequence", ".", "shape", "[", "0", "]", "//", "self", ".", "_batch_size", "\n", "big_ass_sequence", "=", "big_ass_sequence", ".", "narrow", "(", "0", ",", "0", ",", "n_batch", "*", "self", ".", "_batch_size", ")", "\n", "big_ass_sequence", "=", "big_ass_sequence", ".", "view", "(", "self", ".", "_batch_size", ",", "-", "1", ")", "\n", "total_length", "=", "big_ass_sequence", ".", "shape", "[", "1", "]", "\n", "i", "=", "0", "\n", "while", "i", "<", "total_length", "-", "2", ":", "\n", "                ", "if", "shuffle", ":", "\n", "                    ", "bptt", "=", "self", ".", "_split_size", "if", "np", ".", "random", ".", "random", "(", ")", "<", "0.95", "else", "self", ".", "_split_size", "/", "2", "\n", "sequence_length", "=", "max", "(", "5", ",", "int", "(", "np", ".", "random", ".", "normal", "(", "bptt", ",", "5", ")", ")", ")", "\n", "sequence_length", "=", "min", "(", "sequence_length", ",", "total_length", "-", "1", "-", "i", ")", "\n", "", "else", ":", "\n", "                    ", "bptt", "=", "self", ".", "_split_size", "\n", "sequence_length", "=", "min", "(", "self", ".", "_split_size", ",", "total_length", "-", "1", "-", "i", ")", "\n", "", "if", "i", "==", "0", ":", "\n", "                    ", "reset", "=", "torch", ".", "ones", "(", "self", ".", "_batch_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "", "else", ":", "\n", "                    ", "reset", "=", "torch", ".", "zeros", "(", "self", ".", "_batch_size", ",", "dtype", "=", "torch", ".", "uint8", ")", "\n", "\n", "", "out_dict", ":", "TensorDict", "=", "{", "\n", "'source'", ":", "{", "'tokens'", ":", "big_ass_sequence", "[", ":", ",", "i", ":", "i", "+", "sequence_length", "]", "}", ",", "\n", "'target'", ":", "{", "'tokens'", ":", "big_ass_sequence", "[", ":", ",", "i", "+", "1", ":", "i", "+", "sequence_length", "+", "1", "]", "}", ",", "\n", "'reset'", ":", "reset", "\n", "}", "\n", "lr_mult", "=", "sequence_length", "/", "bptt", "\n", "i", "+=", "sequence_length", "\n", "\n", "yield", "out_dict", ",", "lr_mult", "\n", "\n", "", "self", ".", "_epochs", "[", "key", "]", "=", "epoch", "+", "1", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.Splitter.__init__": [[41, 43], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "splitting_keys", ":", "List", "[", "str", "]", ")", "->", "None", ":", "\n", "        ", "self", ".", "_splitting_keys", "=", "splitting_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.Splitter.__call__": [[44, 65], ["split_iterator.Splitter._create_split_indices", "enumerate", "all", "RuntimeError", "zip", "split_iterator.Splitter._slice_tensor_dict"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.RandomSplitter._create_split_indices", "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.Splitter._slice_tensor_dict"], ["", "def", "__call__", "(", "self", ",", "tensor_dict", ":", "TensorDict", ",", "truncate_at", ":", "int", ")", "->", "Iterable", "[", "TensorDict", "]", ":", "\n", "\n", "        ", "if", "not", "all", "(", "key", "in", "tensor_dict", "for", "key", "in", "self", ".", "_splitting_keys", ")", ":", "\n", "            ", "missing_keys", "=", "[", "key", "for", "key", "in", "self", ".", "_splitting_keys", "if", "key", "not", "in", "tensor_dict", "]", "\n", "raise", "RuntimeError", "(", "'Tensor dict is missing splitting keys: %s'", "%", "missing_keys", ")", "\n", "\n", "", "sequence_length", "=", "truncate_at", "\n", "\n", "split_indices", "=", "self", ".", "_create_split_indices", "(", "sequence_length", ")", "\n", "# If the last split is too small, then merge with the second to last", "\n", "# split.", "\n", "if", "(", "split_indices", "[", "-", "1", "]", "-", "split_indices", "[", "-", "2", "]", ")", "<=", "1", ":", "\n", "            ", "split_indices", "[", "-", "2", "]", "=", "split_indices", "[", "-", "1", "]", "\n", "del", "split_indices", "[", "-", "1", "]", "\n", "", "for", "i", ",", "(", "start", ",", "stop", ")", "in", "enumerate", "(", "zip", "(", "split_indices", "[", ":", "-", "1", "]", ",", "split_indices", "[", "1", ":", "]", ")", ")", ":", "\n", "            ", "sliced_tensor_dict", "=", "self", ".", "_slice_tensor_dict", "(", "tensor_dict", ",", "start", ",", "stop", ")", "\n", "if", "i", "==", "0", ":", "\n", "                ", "sliced_tensor_dict", "[", "'reset'", "]", "=", "True", "\n", "", "else", ":", "\n", "                ", "sliced_tensor_dict", "[", "'reset'", "]", "=", "False", "\n", "", "yield", "sliced_tensor_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.Splitter._create_split_indices": [[66, 68], ["None"], "methods", ["None"], ["", "", "def", "_create_split_indices", "(", "self", ",", "sequence_length", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.Splitter._get_sequence_length": [[69, 79], ["sequence_lengths.pop", "split_iterator.get_sequence_length", "sequence_lengths.append", "all", "RuntimeError"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.get_sequence_length"], ["", "def", "_get_sequence_length", "(", "self", ",", "tensor_dict", ":", "TensorDict", ")", "->", "int", ":", "\n", "        ", "sequence_lengths", "=", "[", "]", "\n", "for", "key", "in", "self", ".", "_splitting_keys", ":", "\n", "# Sometimes tensor dicts can be nested. E.g. for TextFeilds.", "\n", "            ", "tensorized_field", "=", "tensor_dict", "[", "key", "]", "\n", "sequence_length", "=", "get_sequence_length", "(", "tensorized_field", ")", "\n", "sequence_lengths", ".", "append", "(", "sequence_length", ")", "\n", "", "if", "not", "all", "(", "length", "==", "sequence_lengths", "[", "0", "]", "for", "length", "in", "sequence_lengths", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "'Cannot split sequences of unequal lengths'", ")", "\n", "", "return", "sequence_lengths", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.Splitter._slice_tensor_dict": [[80, 96], ["isinstance", "split_iterator.Splitter._slice_tensor_dict._recursion"], "methods", ["None"], ["", "def", "_slice_tensor_dict", "(", "self", ",", "tensor_dict", ":", "TensorDict", ",", "start", ":", "int", ",", "end", ":", "int", ")", "->", "TensorDict", ":", "\n", "\n", "        ", "def", "_recursion", "(", "tensor_or_dict", ")", ":", "\n", "            ", "if", "isinstance", "(", "tensor_or_dict", ",", "torch", ".", "Tensor", ")", ":", "\n", "                ", "return", "tensor_or_dict", "[", ":", ",", "start", ":", "end", "]", "\n", "", "elif", "isinstance", "(", "tensor_or_dict", ",", "dict", ")", ":", "\n", "                ", "return", "{", "key", ":", "_recursion", "(", "value", ")", "for", "key", ",", "value", "in", "tensor_or_dict", ".", "items", "(", ")", "}", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'Splitter encountered unexpected value in tensor_dict'", ")", "\n", "\n", "", "", "other_keys", "=", "[", "key", "for", "key", "in", "tensor_dict", ".", "keys", "(", ")", "if", "key", "not", "in", "self", ".", "_splitting_keys", "]", "\n", "out", "=", "{", "key", ":", "tensor_dict", "[", "key", "]", "for", "key", "in", "other_keys", "}", "\n", "for", "key", "in", "self", ".", "_splitting_keys", ":", "\n", "            ", "out", "[", "key", "]", "=", "_recursion", "(", "tensor_dict", "[", "key", "]", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.FixedSplitter.__init__": [[100, 105], ["split_iterator.Splitter.__init__"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "split_size", ":", "int", ",", "\n", "splitting_keys", ":", "List", "[", "str", "]", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "splitting_keys", "=", "splitting_keys", ")", "\n", "self", ".", "_split_size", "=", "split_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.FixedSplitter._create_split_indices": [[106, 109], ["list", "range"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "_create_split_indices", "(", "self", ",", "sequence_length", ":", "int", ")", "->", "Iterable", "[", "int", "]", ":", "\n", "        ", "return", "list", "(", "range", "(", "0", ",", "sequence_length", ",", "self", ".", "_split_size", ")", ")", "+", "[", "sequence_length", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.RandomSplitter.__init__": [[113, 122], ["split_iterator.Splitter.__init__"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "mean_split_size", ":", "int", ",", "\n", "max_split_size", ":", "int", ",", "\n", "min_split_size", ":", "int", ",", "\n", "splitting_keys", ":", "List", "[", "str", "]", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "splitting_keys", "=", "splitting_keys", ")", "\n", "self", ".", "_mean_split_size", "=", "mean_split_size", "\n", "self", ".", "_max_split_size", "=", "max_split_size", "\n", "self", ".", "_min_split_size", "=", "min_split_size", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.RandomSplitter._create_split_indices": [[123, 135], ["split_indices.append", "split_indices.append", "int", "max", "min", "numpy.random.normal"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "_create_split_indices", "(", "self", ",", "sequence_length", ":", "int", ")", "->", "List", "[", "int", "]", ":", "\n", "        ", "split_indices", "=", "[", "]", "\n", "last", "=", "0", "\n", "while", "last", "<", "sequence_length", ":", "\n", "            ", "split_indices", ".", "append", "(", "last", ")", "\n", "delta", "=", "int", "(", "np", ".", "random", ".", "normal", "(", "self", ".", "_mean_split_size", ",", "5", ")", ")", "\n", "delta", "=", "max", "(", "self", ".", "_min_split_size", ",", "delta", ")", "\n", "delta", "=", "min", "(", "self", ".", "_max_split_size", ",", "delta", ")", "\n", "last", "+=", "delta", "\n", "", "split_indices", ".", "append", "(", "sequence_length", ")", "\n", "return", "split_indices", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.SplitIterator.__init__": [[176, 194], ["allennlp.data.iterators.BucketIterator.__init__"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "splitter", ":", "Splitter", ",", "\n", "sorting_keys", ":", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", ",", "\n", "padding_noise", ":", "float", "=", "0.1", ",", "\n", "biggest_batch_first", ":", "bool", "=", "False", ",", "\n", "batch_size", ":", "int", "=", "32", ",", "\n", "instances_per_epoch", ":", "int", "=", "None", ",", "\n", "max_instances_in_memory", ":", "int", "=", "None", ",", "\n", "track_epoch", ":", "bool", "=", "False", ",", "\n", "maximum_samples_per_batch", ":", "Tuple", "[", "str", ",", "int", "]", "=", "None", ")", "->", "None", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "sorting_keys", "=", "sorting_keys", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "instances_per_epoch", "=", "instances_per_epoch", ",", "\n", "max_instances_in_memory", "=", "max_instances_in_memory", ",", "\n", "cache_instances", "=", "False", ",", "\n", "track_epoch", "=", "track_epoch", ",", "\n", "maximum_samples_per_batch", "=", "maximum_samples_per_batch", ")", "\n", "self", ".", "_splitter", "=", "splitter", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.SplitIterator.__call__": [[195, 277], ["id", "itertools.count", "range", "split_iterator.SplitIterator._create_batches", "random.shuffle", "split_iterator.SplitIterator._splitter", "random.choice", "batch.get_padding_lengths", "logger.debug", "logger.debug", "batch.as_tensor_dict", "split_iterator.SplitIterator._splitter", "epoch_tensor.fill_", "allennlp.data.iterators.data_iterator.add_epoch_number", "batch.index_instances", "instance.get_padding_lengths", "split_iterator.SplitIterator._cache[].append"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.get_padding_lengths", "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.get_padding_lengths"], ["", "def", "__call__", "(", "self", ",", "\n", "instances", ":", "Iterable", "[", "Instance", "]", ",", "\n", "num_epochs", ":", "int", "=", "None", ",", "\n", "shuffle", ":", "bool", "=", "True", ")", "->", "Iterator", "[", "TensorDict", "]", ":", "\n", "        ", "\"\"\"\n        Returns a generator that yields batches over the given dataset\n        for the given number of epochs. If ``num_epochs`` is not specified,\n        it will yield batches forever.\n\n        Parameters\n        ----------\n        instances : ``Iterable[Instance]``\n            The instances in the dataset. IMPORTANT: this must be able to be\n            iterated over *multiple times*. That is, it must be either a List\n            or some other object whose ``__iter__`` method returns a fresh iterator\n            each time it's called.\n        num_epochs : ``int``, optional (default=``None``)\n            How times should we iterate over this dataset?  If ``None``, we will iterate over it\n            forever.\n        shuffle : ``bool``, optional (default=``True``)\n            If ``True``, we will shuffle the instances in ``dataset`` before constructing batches\n            and iterating over the data.\n        \"\"\"", "\n", "# Instances is likely to be a list, which cannot be used as a key,", "\n", "# so we take the object id instead.", "\n", "key", "=", "id", "(", "instances", ")", "\n", "starting_epoch", "=", "self", ".", "_epochs", "[", "key", "]", "\n", "\n", "if", "num_epochs", "is", "None", ":", "\n", "            ", "epochs", ":", "Iterable", "[", "int", "]", "=", "itertools", ".", "count", "(", "starting_epoch", ")", "\n", "", "else", ":", "\n", "            ", "epochs", "=", "range", "(", "starting_epoch", ",", "starting_epoch", "+", "num_epochs", ")", "\n", "\n", "", "for", "epoch", "in", "epochs", ":", "\n", "            ", "if", "self", ".", "_cache_instances", "and", "key", "in", "self", ".", "_cache", ":", "\n", "# Serve the results from the cache.", "\n", "                ", "tensor_dicts", "=", "self", ".", "_cache", "[", "key", "]", "\n", "\n", "if", "shuffle", ":", "\n", "                    ", "random", ".", "shuffle", "(", "tensor_dicts", ")", "\n", "", "for", "tensor_dict", "in", "tensor_dicts", ":", "\n", "                    ", "if", "self", ".", "_track_epoch", ":", "\n", "# The tensor_dict already has an \"epoch_num\" tensor,", "\n", "# so just fill it with the right value.", "\n", "                        ", "epoch_tensor", ":", "torch", ".", "Tensor", "=", "tensor_dict", "[", "'epoch_num'", "]", "\n", "epoch_tensor", ".", "fill_", "(", "epoch", ")", "\n", "", "for", "split_tensor_dict", "in", "self", ".", "_splitter", "(", "tensor_dict", ")", ":", "\n", "                        ", "yield", "split_tensor_dict", "\n", "", "", "", "else", ":", "\n", "                ", "batches", "=", "self", ".", "_create_batches", "(", "instances", ",", "shuffle", ")", "\n", "\n", "# Should we add the instances to the cache this epoch?", "\n", "add_to_cache", "=", "self", ".", "_cache_instances", "and", "key", "not", "in", "self", ".", "_cache", "\n", "\n", "for", "batch", "in", "batches", ":", "\n", "                    ", "if", "self", ".", "_track_epoch", ":", "\n", "                        ", "add_epoch_number", "(", "batch", ",", "epoch", ")", "\n", "\n", "", "if", "self", ".", "vocab", "is", "not", "None", ":", "\n", "                        ", "batch", ".", "index_instances", "(", "self", ".", "vocab", ")", "\n", "\n", "\n", "# In order to make  gradient updates fair in expectation,", "\n", "# we randomly choose a sequence to cutoff at.", "\n", "", "all_instance_lengths", "=", "[", "instance", ".", "get_padding_lengths", "(", ")", "for", "\n", "instance", "in", "batch", ".", "instances", "]", "\n", "random_instance", "=", "random", ".", "choice", "(", "all_instance_lengths", ")", "\n", "truncate_at", "=", "random_instance", "[", "'tokens'", "]", "[", "'num_tokens'", "]", "\n", "padding_lengths", "=", "batch", ".", "get_padding_lengths", "(", ")", "\n", "logger", ".", "debug", "(", "'trunacate at: %s'", ",", "truncate_at", ")", "\n", "logger", ".", "debug", "(", "'padding_lengths: %s'", ",", "padding_lengths", ")", "\n", "\n", "tensor_dict", "=", "batch", ".", "as_tensor_dict", "(", "padding_lengths", ")", "\n", "\n", "if", "add_to_cache", ":", "\n", "                        ", "self", ".", "_cache", "[", "key", "]", ".", "append", "(", "tensor_dict", ")", "\n", "\n", "", "for", "split_tensor_dict", "in", "self", ".", "_splitter", "(", "tensor_dict", ",", "truncate_at", ")", ":", "\n", "                        ", "yield", "split_tensor_dict", "\n", "\n", "# Increment epoch tracker", "\n", "", "", "", "self", ".", "_epochs", "[", "key", "]", "=", "epoch", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.SplitIterator.get_num_batches": [[278, 280], ["None"], "methods", ["None"], ["", "", "def", "get_num_batches", "(", "self", ",", "instances", ":", "Iterable", "[", "Instance", "]", ")", "->", "float", ":", "\n", "        ", "return", "1", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.get_sequence_length": [[17, 27], ["isinstance", "isinstance", "next", "split_iterator.get_sequence_length", "RuntimeError", "iter", "tensorized_field.values"], "function", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.iterators.split_iterator.get_sequence_length"], ["def", "get_sequence_length", "(", "tensorized_field", ":", "Union", "[", "torch", ".", "Tensor", ",", "TensorDict", "]", ")", "->", "int", ":", "\n", "    ", "if", "isinstance", "(", "tensorized_field", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "return", "tensorized_field", ".", "shape", "[", "1", "]", "\n", "", "elif", "isinstance", "(", "tensorized_field", ",", "dict", ")", ":", "\n", "# We are making the extreme assumption that all of the tensors in a nested TensorDict have", "\n", "# the same sequence length.", "\n", "        ", "tensorized_subfield", "=", "next", "(", "iter", "(", "tensorized_field", ".", "values", "(", ")", ")", ")", "# Get any value", "\n", "return", "get_sequence_length", "(", "tensorized_subfield", ")", "\n", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "'Failed to get sequence length of one of the fields.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.sequential_array.SequentialArrayField.__init__": [[14, 22], ["allennlp.data.fields.ArrayField.__init__"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__"], ["def", "__init__", "(", "self", ",", "\n", "array", ":", "np", ".", "ndarray", ",", "\n", "dtype", ":", "np", ".", "dtype", ",", "\n", "sequence_dim", ":", "int", "=", "0", ",", "\n", "padding_value", ":", "int", "=", "0", ")", "->", "None", ":", "\n", "        ", "ArrayField", ".", "__init__", "(", "self", ",", "array", "=", "array", ",", "padding_value", "=", "padding_value", ")", "\n", "self", ".", "_dtype", "=", "dtype", "\n", "self", ".", "_sequence_dim", "=", "sequence_dim", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.sequential_array.SequentialArrayField.sequence_length": [[23, 26], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "sequence_length", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "self", ".", "array", ".", "shape", "[", "self", ".", "_sequence_dim", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.sequential_array.SequentialArrayField.as_tensor": [[27, 44], ["numpy.asarray", "list", "tuple", "torch.from_numpy", "numpy.full", "len", "len", "range", "slice", "len", "range", "len", "len"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "max_shape", "=", "[", "padding_lengths", "[", "\"dimension_{}\"", ".", "format", "(", "i", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "padding_lengths", ")", ")", "]", "\n", "\n", "# Convert explicitly to an ndarray just in case it's an scalar (it'd end up not being an ndarray otherwise)", "\n", "return_array", "=", "np", ".", "asarray", "(", "np", ".", "full", "(", "max_shape", ",", "self", ".", "padding_value", ")", ",", "dtype", "=", "self", ".", "_dtype", ")", "\n", "\n", "# If the tensor has a different shape from the largest tensor, pad dimensions with zeros to", "\n", "# form the right shaped list of slices for insertion into the final tensor.", "\n", "slicing_shape", "=", "list", "(", "self", ".", "array", ".", "shape", ")", "\n", "if", "len", "(", "self", ".", "array", ".", "shape", ")", "<", "len", "(", "max_shape", ")", ":", "\n", "            ", "slicing_shape", "=", "slicing_shape", "+", "[", "0", "for", "_", "in", "range", "(", "len", "(", "max_shape", ")", "-", "len", "(", "self", ".", "array", ".", "shape", ")", ")", "]", "\n", "", "slices", "=", "tuple", "(", "[", "slice", "(", "0", ",", "x", ")", "for", "x", "in", "slicing_shape", "]", ")", "\n", "return_array", "[", "slices", "]", "=", "self", ".", "array", "\n", "tensor", "=", "torch", ".", "from_numpy", "(", "return_array", ")", "\n", "return", "tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.sequential_array.SequentialArrayField.empty_field": [[45, 51], ["tuple", "sequential_array.SequentialArrayField", "numpy.empty"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", ":", "# pylint: disable=no-self-use", "\n", "# Pass the padding_value, so that any outer field, e.g., `ListField[ArrayField]` uses the", "\n", "# same padding_value in the padded ArrayFields", "\n", "        ", "shape", "=", "tuple", "(", "0", "for", "_", "in", "self", ".", "array", ".", "shape", ")", "\n", "return", "SequentialArrayField", "(", "np", ".", "empty", "(", "shape", ")", ",", "dtype", "=", "self", ".", "_dtype", ",", "padding_value", "=", "self", ".", "padding_value", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.__init__": [[22, 24], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "global_object", ":", "Any", ")", "->", "None", ":", "\n", "        ", "self", ".", "_global_object", "=", "global_object", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.get_padding_lengths": [[25, 28], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "get_padding_lengths", "(", "self", ")", "->", "Dict", "[", "str", ",", "int", "]", ":", "\n", "        ", "return", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.as_tensor": [[29, 33], ["None"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "as_tensor", "(", "self", ",", "padding_lengths", ":", "Dict", "[", "str", ",", "int", "]", ")", "->", "Any", ":", "\n", "# pylint: disable=unused-argument", "\n", "        ", "return", "self", ".", "_global_object", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.empty_field": [[34, 37], ["global_object.GlobalObject"], "methods", ["None"], ["", "@", "overrides", "\n", "def", "empty_field", "(", "self", ")", "->", "'GlobalObject'", ":", "\n", "        ", "return", "GlobalObject", "(", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.batch_tensors": [[38, 42], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "@", "overrides", "\n", "def", "batch_tensors", "(", "cls", ",", "tensor_list", ":", "List", "[", "Any", "]", ")", "->", "Any", ":", "# type: ignore", "\n", "        ", "return", "tensor_list", "[", "0", "]", "# We only need one...", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.fields.global_object.GlobalObject.__str__": [[43, 45], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "return", "\"GlobalObject(%r)\"", "%", "self", ".", "_global_object", "\n", "", "", ""]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.sample.Sample.add_subparser": [[29, 54], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["None"], ["    ", "def", "add_subparser", "(", "self", ",", "name", ":", "str", ",", "parser", ":", "argparse", ".", "_SubParsersAction", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "# pylint: disable=protected-access", "\n", "        ", "description", "=", "'''Generate samples from the model'''", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "name", ",", "description", "=", "description", ",", "\n", "help", "=", "'Generate samples from the model'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'model_archive_file'", ",", "type", "=", "str", ",", "help", "=", "'path to an archived trained model'", ")", "\n", "subparser", ".", "add_argument", "(", "'alias_database'", ",", "type", "=", "str", ",", "help", "=", "'path to the alias database'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--cuda-device'", ",", "type", "=", "int", ",", "default", "=", "-", "1", ",", "help", "=", "'id of GPU to use (if any)'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--output-file'", ",", "type", "=", "str", ",", "help", "=", "'path to output file'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--batch-size'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "100", ",", "\n", "help", "=", "'Number of samples to create.'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--length'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "100", ",", "\n", "help", "=", "'Length of generated sequence.'", ")", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "sample", ")", "\n", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.sample.sample": [[55, 68], ["allennlp.models.archival.load_archive", "allennlp.common.util.prepare_environment", "model.eval", "kglm.data.AliasDatabase.load", "model.sample"], "function", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.alias_database.AliasDatabase.load", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.sample.sample"], ["", "", "def", "sample", "(", "args", ":", "argparse", ".", "Namespace", ")", ":", "\n", "    ", "model_archive", "=", "load_archive", "(", "args", ".", "model_archive_file", ",", "\n", "cuda_device", "=", "args", ".", "cuda_device", ")", "\n", "config", "=", "model_archive", ".", "config", "\n", "prepare_environment", "(", "config", ")", "\n", "model", "=", "model_archive", ".", "model", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "alias_database", "=", "AliasDatabase", ".", "load", "(", "args", ".", "alias_database", ")", "\n", "\n", "samples", "=", "model", ".", "sample", "(", "alias_database", ",", "\n", "batch_size", "=", "args", ".", "batch_size", ",", "\n", "length", "=", "args", ".", "length", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence.CompleteTheSentence.add_subparser": [[19, 54], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["None"], ["\n", "logger", "=", "logging", ".", "getLogger", "(", "__name__", ")", "\n", "\n", "\n", "@", "Predictor", ".", "register", "(", "'complete-the-sentence'", ")", "\n", "class", "CompleteTheSentencePredictor", "(", "Predictor", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "model", ":", "Model", ",", "dataset_reader", ":", "DatasetReader", ")", ":", "\n", "        ", "logger", ".", "warning", "(", "'CompleteTheSentencePredictor is meant to be used with '", "\n", "'`kglm.run complete-the-sentence`, if you are using '", "\n", "'`allennlp predict` then results may be incorrect.'", ")", "\n", "self", ".", "_model", "=", "model", "\n", "self", ".", "_dataset_reader", "=", "dataset_reader", "\n", "\n", "", "@", "overrides", "\n", "def", "_json_to_instance", "(", "self", ",", "json_dict", ":", "JsonDict", ")", "->", "Instance", ":", "\n", "        ", "\"\"\"\n        We need to break with the Predictor expectations slightly, instead of returning a single\n        instance we return a conditioning instance (to warm up the model), and then a generative\n        instance (e.g. the token to predict on).\n        \"\"\"", "\n", "### Conditioning Instance ###", "\n", "# TODO: This is totally broken...", "\n", "\n", "# Manually add the start token", "\n", "tokens", "=", "[", "'@@START@@'", ",", "*", "json_dict", "[", "'prefix'", "]", "]", "\n", "\n", "# Also need to offset", "\n", "# start, end = json_dict['entity_indices']", "\n", "# span = [start + 1, end + 1]", "\n", "\n", "# Repackage into the expected annotation format", "\n", "# annotations = [{", "\n", "#     'id': json_dict['entity_id'],", "\n", "#     'relation': ['@@NEW@@'],", "\n", "#     'parent_id': [json_dict['entity_id']],", "\n", "#     'span': span", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.__init__": [[73, 93], ["open"], "methods", ["None"], ["\n", "# data = {'tokens': [[tokens[-1]]]}", "\n", "# generative_instance = self._dataset_reader.text_to_instance(data)", "\n", "", "generative_instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "[", "tokens", "[", "-", "1", "]", "]", ")", "\n", "# Manually add the reset field here", "\n", "reset", "=", "SequentialArrayField", "(", "np", ".", "array", "(", "0", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "generative_instance", ".", "add_field", "(", "'reset'", ",", "reset", ")", "\n", "if", "'shortlist'", "in", "json_dict", ":", "\n", "            ", "generative_instance", ".", "add_field", "(", "'shortlist'", ",", "conditioning_instance", ".", "fields", "[", "'shortlist'", "]", ")", "\n", "\n", "", "return", "conditioning_instance", ",", "generative_instance", "\n", "\n", "", "@", "overrides", "\n", "def", "predict_instance", "(", "self", ",", "instances", ":", "Tuple", "[", "Instance", ",", "Instance", "]", ")", "->", "JsonDict", ":", "\n", "        ", "conditioning_instance", ",", "generative_instance", "=", "instances", "\n", "\n", "self", ".", "_model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# TODO: Make this a parameter somewhere", "\n", "            ", "num_samples", "=", "100", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._predict_json": [[94, 101], ["len", "complete_the_sentence._PredictManager._predictor.predict_batch_json", "complete_the_sentence._PredictManager._predictor.predict_json", "complete_the_sentence._PredictManager._predictor.dump_line"], "methods", ["None"], ["\n", "# Duplicate instances (to sample in parallel)", "\n", "cuda_device", "=", "self", ".", "_model", ".", "_get_prediction_device", "(", ")", "\n", "conditioning_batch", "=", "Batch", "(", "[", "conditioning_instance", "]", "*", "num_samples", ")", "\n", "conditioning_batch", ".", "index_instances", "(", "self", ".", "_model", ".", "vocab", ")", "\n", "conditioning_batch", "=", "util", ".", "move_to_device", "(", "conditioning_batch", ".", "as_tensor_dict", "(", ")", ",", "cuda_device", ")", "\n", "\n", "generative_batch", "=", "Batch", "(", "[", "generative_instance", "]", "*", "num_samples", ")", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._predict_instances": [[102, 109], ["len", "complete_the_sentence._PredictManager._predictor.predict_batch_instance", "complete_the_sentence._PredictManager._predictor.predict_instance", "complete_the_sentence._PredictManager._predictor.dump_line"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.kglm.KglmPredictor.predict_batch_instance", "home.repos.pwc.inspect_result.rloganiv_kglm-model.predictors.complete_the_sentence.CompleteTheSentencePredictor.predict_instance"], ["generative_batch", ".", "index_instances", "(", "self", ".", "_model", ".", "vocab", ")", "\n", "generative_batch", "=", "util", ".", "move_to_device", "(", "generative_batch", ".", "as_tensor_dict", "(", ")", ",", "cuda_device", ")", "\n", "\n", "# Sample annotations and generate next token", "\n", "self", ".", "_model", ".", "_use_shortlist", "=", "True", "\n", "conditioning_output", "=", "self", ".", "_model", ".", "sample", "(", "**", "conditioning_batch", ",", "emit_tokens", "=", "False", ")", "\n", "logger", ".", "debug", "(", "'clears condition generation'", ")", "\n", "# self._model(**conditioning_output)  # Shouldn't need to do this, but just in case", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._maybe_print_to_console_and_file": [[110, 119], ["print", "complete_the_sentence._PredictManager._output_file.write", "print"], "methods", ["None"], ["# logger.debug('clears reconditioning')", "\n", "generative_output", "=", "self", ".", "_model", ".", "sample", "(", "**", "generative_batch", ",", "emit_tokens", "=", "True", ")", "\n", "logger", ".", "debug", "(", "'clears generation'", ")", "\n", "del", "conditioning_batch", ",", "generative_batch", "\n", "\n", "aggregate_word_probs", "=", "self", ".", "_aggregate_word_probs", "(", "generative_output", ")", "\n", "logger", ".", "debug", "(", "'clears word probs'", ")", "\n", "\n", "return", "aggregate_word_probs", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._get_json_data": [[120, 130], ["open", "line.isspace", "complete_the_sentence._PredictManager._predictor.load_line", "line.isspace", "complete_the_sentence._PredictManager._predictor.load_line"], "methods", ["None"], ["", "", "def", "_aggregate_word_probs", "(", "self", ",", "generative_output", ":", "Dict", "[", "str", ",", "Any", "]", ")", "->", "Dict", "[", "str", ",", "List", "[", "Any", "]", "]", ":", "\n", "\n", "# Get vocab", "\n", "        ", "vocab", "=", "self", ".", "_model", ".", "vocab", "\n", "vocab_size", "=", "vocab", ".", "get_vocab_size", "(", "'tokens'", ")", "\n", "\n", "# Get alias database", "\n", "alias_database", "=", "generative_output", "[", "'metadata'", "]", "[", "0", "]", "[", "'alias_database'", "]", "\n", "\n", "# Split into source and target vocab probs", "\n", "target_probs", "=", "generative_output", "[", "'target_probs'", "]", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._get_instance_data": [[131, 138], ["allennlp.common.checks.ConfigurationError", "allennlp.common.checks.ConfigurationError", "complete_the_sentence._PredictManager._dataset_reader.read"], "methods", ["None"], ["source_vocab_probs", "=", "target_probs", "[", ":", ",", ":", ",", ":", "vocab_size", "]", "\n", "copy_vocab_probs", "=", "target_probs", "[", ":", ",", ":", ",", "vocab_size", ":", "]", "\n", "\n", "# Average source vocab prob is easy to compute", "\n", "source_vocab_avg", "=", "source_vocab_probs", ".", "mean", "(", "0", ")", ".", "squeeze", "(", ")", "\n", "prob_dict", "=", "dict", "(", ")", "\n", "index_to_token_vocabulary", "=", "vocab", ".", "get_index_to_token_vocabulary", "(", "'tokens'", ")", "\n", "for", "i", ",", "prob", "in", "enumerate", "(", "source_vocab_avg", ")", ":", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.run": [[139, 152], ["allennlp.common.util.lazy_groups_of", "allennlp.common.util.lazy_groups_of", "complete_the_sentence._PredictManager._output_file.close", "complete_the_sentence._PredictManager._get_instance_data", "zip", "complete_the_sentence._PredictManager._get_json_data", "zip", "complete_the_sentence._PredictManager._predict_instances", "complete_the_sentence._PredictManager._maybe_print_to_console_and_file", "complete_the_sentence._PredictManager._predict_json", "complete_the_sentence._PredictManager._maybe_print_to_console_and_file", "str", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._get_instance_data", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._get_json_data", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._predict_instances", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._maybe_print_to_console_and_file", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._predict_json", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager._maybe_print_to_console_and_file"], ["            ", "word", "=", "index_to_token_vocabulary", "[", "i", "]", "\n", "prob_dict", "[", "word", "]", "=", "prob", ".", "item", "(", ")", "\n", "\n", "# Get alias indices", "\n", "", "alias_indices", "=", "generative_output", "[", "'alias_indices'", "]", "\n", "\n", "# The copy vocabs will be a little bit more difficult", "\n", "num_samples", "=", "target_probs", ".", "shape", "[", "0", "]", "\n", "raw_entity_ids", "=", "generative_output", "[", "'raw_entity_ids'", "]", "[", "'raw_entity_ids'", "]", "\n", "for", "alias_index", ",", "copy_probs", ",", "raw_entity_id", "in", "zip", "(", "alias_indices", ",", "copy_vocab_probs", ",", "raw_entity_ids", ")", ":", "\n", "            ", "if", "raw_entity_id", "==", "0", ":", "\n", "                ", "continue", "\n", "\n", "", "entity", "=", "vocab", ".", "get_token_from_index", "(", "raw_entity_id", ".", "item", "(", ")", ",", "'raw_entity_ids'", ")", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._get_predictor": [[56, 69], ["allennlp.common.checks.check_for_gpu", "allennlp.models.archival.load_archive", "allennlp.models.archival.load_archive", "kglm.predictors.CompleteTheSentencePredictor.from_archive"], "function", ["None"], ["# data = {'tokens': [tokens], 'annotations': annotations}", "\n", "# conditioning_instance = self._dataset_reader.text_to_instance(data)", "\n", "conditioning_instance", "=", "self", ".", "_dataset_reader", ".", "text_to_instance", "(", "tokens", "[", ":", "-", "1", "]", ")", "\n", "\n", "# Manually add the reset field here", "\n", "reset", "=", "SequentialArrayField", "(", "np", ".", "array", "(", "1", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "conditioning_instance", ".", "add_field", "(", "'reset'", ",", "reset", ")", "\n", "\n", "# Add the shortlist", "\n", "if", "'shortlist'", "in", "json_dict", ":", "\n", "            ", "shortlist", "=", "json_dict", "[", "'shortlist'", "]", "\n", "field", "=", "TextField", "(", "\n", "[", "Token", "(", "x", ")", "for", "x", "in", "shortlist", "]", ",", "\n", "token_indexers", "=", "self", ".", "_dataset_reader", ".", "_entity_indexers", ")", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._predict": [[153, 168], ["complete_the_sentence._get_predictor", "complete_the_sentence._PredictManager", "complete_the_sentence._PredictManager.run", "print", "print", "sys.exit"], "function", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._get_predictor", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.complete_the_sentence._PredictManager.run"], ["try", ":", "\n", "                ", "id_map", "=", "alias_database", ".", "_id_map_lookup", "[", "entity", "]", "\n", "", "except", ":", "\n", "                ", "logger", ".", "warning", "(", "'Error could not find id map for entity \"%s\"'", ",", "entity", ")", "\n", "continue", "\n", "", "reverse_id_map", "=", "{", "i", ":", "x", "for", "x", ",", "i", "in", "id_map", ".", "items", "(", ")", "}", "\n", "for", "ind", ",", "prob", "in", "zip", "(", "alias_index", ",", "copy_probs", ".", "squeeze", "(", ")", ".", "tolist", "(", ")", ")", ":", "\n", "                ", "if", "ind", "==", "0", ":", "\n", "                    ", "continue", "\n", "", "word", "=", "reverse_id_map", "[", "ind", ".", "item", "(", ")", "]", "\n", "if", "word", "in", "prob_dict", ":", "\n", "                    ", "prob_dict", "[", "word", "]", "+=", "prob", "/", "num_samples", "\n", "", "else", ":", "\n", "                    ", "prob_dict", "[", "word", "]", "=", "prob", "/", "num_samples", "\n", "\n", "# Lastly, convert the prob_dict to a ranked list of words", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.evaluate_perplexity.EvaluatePerplexity.add_subparser": [[23, 66], ["parser.add_parser", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_mutually_exclusive_group", "parser.add_parser.add_mutually_exclusive_group.add_argument", "parser.add_parser.add_argument", "parser.add_parser.add_argument", "parser.add_parser.set_defaults", "parser.add_parser.add_argument", "parser.add_parser.set_defaults"], "methods", ["None"], ["    ", "def", "add_subparser", "(", "self", ",", "name", ":", "str", ",", "parser", ":", "argparse", ".", "_SubParsersAction", ")", "->", "argparse", ".", "ArgumentParser", ":", "\n", "# pylint: disable=protected-access", "\n", "        ", "description", "=", "'''Evaluate the specified model perplexity using importance sampling'''", "\n", "subparser", "=", "parser", ".", "add_parser", "(", "name", ",", "description", "=", "description", ",", "\n", "help", "=", "'Evaluate the specified module using importance sampling'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'model_archive_file'", ",", "type", "=", "str", ",", "help", "=", "'path to an archived trained model'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'sampler_archive_file'", ",", "type", "=", "str", ",", "\n", "help", "=", "'path to an archived trained model for generating samples'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'input_file'", ",", "type", "=", "str", ",", "help", "=", "'path to the file containing the evaluation data'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--output-file'", ",", "type", "=", "str", ",", "help", "=", "'path to output file'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--weights-file'", ",", "\n", "type", "=", "str", ",", "\n", "help", "=", "'a path that overrides which weights file to use'", ")", "\n", "\n", "cuda_device", "=", "subparser", ".", "add_mutually_exclusive_group", "(", "required", "=", "False", ")", "\n", "cuda_device", ".", "add_argument", "(", "'--cuda-device'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "'id of GPU to use (if any)'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'-o'", ",", "'--overrides'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "'a JSON structure used to override the experiment configuration'", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--batch-weight-key'", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"\"", ",", "\n", "help", "=", "'If non-empty, name of metric used to weight the loss on a per-batch basis.'", ")", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "evaluate_from_args", ")", "\n", "\n", "subparser", ".", "add_argument", "(", "'--num-samples'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "100", ",", "\n", "help", "=", "'Number of importance samples to draw.'", ")", "\n", "subparser", ".", "set_defaults", "(", "func", "=", "evaluate_from_args", ")", "\n", "\n", "return", "subparser", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.evaluate_perplexity.evaluate_perplexity": [[68, 129], ["allennlp.common.checks.check_for_gpu", "logger.info", "torch.no_grad", "range", "data_iterator", "allennlp.common.tqdm.Tqdm.tqdm", "model.eval", "sampler.eval", "summands.append", "penalized_summands.append", "torch.tensor", "torch.tensor", "torch.logsumexp", "torch.logsumexp", "math.exp", "math.exp", "print", "print", "allennlp.nn.util.move_to_device", "allennlp.nn.util.get_text_field_mask().float().sum().item", "sampler.sample", "model", "allennlp.nn.util.get_text_field_mask().float().sum", "math.log", "math.log", "allennlp.nn.util.get_text_field_mask().float", "allennlp.nn.util.get_text_field_mask"], "function", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.sample.sample"], ["", "", "def", "evaluate_perplexity", "(", "model", ":", "Model", ",", "\n", "sampler", ":", "Model", ",", "\n", "num_samples", ":", "int", ",", "\n", "instances", ":", "Iterator", "[", "Instance", "]", ",", "\n", "data_iterator", ":", "DataIterator", ",", "\n", "cuda_device", ":", "int", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "    ", "check_for_gpu", "(", "cuda_device", ")", "\n", "\n", "logger", ".", "info", "(", "'Iterating over dataset'", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "        ", "summands", "=", "[", "]", "\n", "penalized_summands", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_samples", ")", ":", "\n", "            ", "iterator", "=", "data_iterator", "(", "instances", ",", "num_epochs", "=", "1", ",", "shuffle", "=", "False", ")", "\n", "generator_tqdm", "=", "Tqdm", ".", "tqdm", "(", "iterator", ",", "total", "=", "0", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "sampler", ".", "eval", "(", ")", "\n", "\n", "summand", "=", "0.0", "\n", "penalized_summand", "=", "0.0", "\n", "denom", "=", "0", "\n", "for", "batch", ",", "_", "in", "generator_tqdm", ":", "\n", "\n", "                ", "batch", "=", "util", ".", "move_to_device", "(", "batch", ",", "cuda_device", ")", "\n", "\n", "# We need sequence length to help compute perplexity", "\n", "n_tokens", "=", "util", ".", "get_text_field_mask", "(", "batch", "[", "'source'", "]", ")", ".", "float", "(", ")", ".", "sum", "(", ")", ".", "item", "(", ")", "\n", "denom", "+=", "n_tokens", "\n", "\n", "# Draw a sample", "\n", "sampler_output", "=", "sampler", ".", "sample", "(", "**", "batch", ")", "\n", "sample_logp", "=", "sampler_output", "[", "'logp'", "]", "\n", "sample", "=", "sampler_output", "[", "'sample'", "]", "\n", "\n", "# Evaluate on sample", "\n", "model_output", "=", "model", "(", "**", "sample", ")", "\n", "model_logp", "=", "model_output", "[", "'logp'", "]", "\n", "model_penalized_logp", "=", "model_output", "[", "'penalized_logp'", "]", "\n", "summand", "+=", "(", "model_logp", "-", "sample_logp", ")", ".", "item", "(", ")", "\n", "penalized_summand", "+=", "(", "model_penalized_logp", "-", "sample_logp", ")", ".", "item", "(", ")", "\n", "\n", "", "summands", ".", "append", "(", "summand", ")", "\n", "penalized_summands", ".", "append", "(", "penalized_summand", ")", "\n", "t", "=", "torch", ".", "tensor", "(", "summands", ")", "\n", "p", "=", "torch", ".", "tensor", "(", "penalized_summands", ")", "\n", "t_sum", "=", "torch", ".", "logsumexp", "(", "t", ",", "dim", "=", "0", ")", "\n", "p_sum", "=", "torch", ".", "logsumexp", "(", "p", ",", "dim", "=", "0", ")", "\n", "sum_logp", "=", "(", "t_sum", "-", "math", ".", "log", "(", "i", "+", "1", ")", ")", ".", "item", "(", ")", "\n", "sum_logp_penalized", "=", "(", "p_sum", "-", "math", ".", "log", "(", "i", "+", "1", ")", ")", ".", "item", "(", ")", "\n", "ppl", "=", "math", ".", "exp", "(", "-", "sum_logp", "/", "denom", ")", "\n", "upp", "=", "math", ".", "exp", "(", "-", "sum_logp_penalized", "/", "denom", ")", "\n", "\n", "print", "(", "'PPL: %f'", "%", "ppl", ")", "\n", "print", "(", "'UPP: %f'", "%", "upp", ")", "\n", "\n", "", "", "metrics", "=", "{", "'ppl'", ":", "ppl", ",", "'upp'", ":", "upp", "}", "\n", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.evaluate_perplexity.evaluate_from_args": [[130, 176], ["logging.getLogger().setLevel", "allennlp.models.archival.load_archive", "allennlp.common.util.prepare_environment", "model.eval", "allennlp.models.archival.load_archive", "sampler.eval", "config.pop", "logger.info", "DatasetReader.from_params.read", "config.pop", "allennlp.data.iterators.DataIterator.from_params", "DataIterator.from_params.index_with", "DataIterator.from_params.eval", "evaluate_perplexity.evaluate_perplexity", "logger.info", "logger.info", "evaluate_perplexity.items", "logging.getLogger", "logging.getLogger", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "allennlp.data.dataset_readers.dataset_reader.DatasetReader.from_params", "logger.info", "logging.getLogger", "config.pop", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.commands.evaluate_perplexity.evaluate_perplexity", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params"], ["", "def", "evaluate_from_args", "(", "args", ":", "argparse", ".", "Namespace", ")", "->", "Dict", "[", "str", ",", "Any", "]", ":", "\n", "# Disable some of the more verbose logging statements", "\n", "    ", "logging", ".", "getLogger", "(", "'allennlp.common.params'", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "'allennlp.nn.initializers'", ")", ".", "disabled", "=", "True", "\n", "logging", ".", "getLogger", "(", "'allennlp.modules.token_embedders.embedding'", ")", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "\n", "# Load model from archive", "\n", "model_archive", "=", "load_archive", "(", "args", ".", "model_archive_file", ",", "args", ".", "cuda_device", ",", "args", ".", "overrides", ",", "args", ".", "weights_file", ")", "\n", "config", "=", "model_archive", ".", "config", "\n", "prepare_environment", "(", "config", ")", "\n", "model", "=", "model_archive", ".", "model", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "# Load sampler", "\n", "sampler_archive", "=", "load_archive", "(", "args", ".", "sampler_archive_file", ",", "args", ".", "cuda_device", ",", "args", ".", "overrides", ",", "args", ".", "weights_file", ")", "\n", "sampler", "=", "sampler_archive", ".", "model", "\n", "sampler", ".", "eval", "(", ")", "\n", "\n", "# Load the evaluation data. NOTE: We are using the model's reader!", "\n", "validation_dataset_reader_params", "=", "config", ".", "pop", "(", "'validation_dataset_reader'", ",", "None", ")", "\n", "if", "validation_dataset_reader_params", "is", "not", "None", ":", "\n", "        ", "dataset_reader", "=", "DatasetReader", ".", "from_params", "(", "validation_dataset_reader_params", ")", "\n", "", "else", ":", "\n", "        ", "dataset_reader", "=", "DatasetReader", ".", "from_params", "(", "config", ".", "pop", "(", "'dataset_reader'", ")", ")", "\n", "", "evaluation_data_path", "=", "args", ".", "input_file", "\n", "logger", ".", "info", "(", "'Reading evaluation data from: %s'", ",", "evaluation_data_path", ")", "\n", "instances", "=", "dataset_reader", ".", "read", "(", "evaluation_data_path", ")", "\n", "\n", "# To avoid hairy issues with splitting, we opt to use a basic iterator so that we can", "\n", "# generate samples for entire sequences.", "\n", "iterator_params", "=", "config", ".", "pop", "(", "'iterator'", ",", "'None'", ")", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "iterator_params", ")", "\n", "iterator", ".", "index_with", "(", "model", ".", "vocab", ")", "\n", "iterator", ".", "eval", "(", ")", "\n", "metrics", "=", "evaluate_perplexity", "(", "model", ",", "sampler", ",", "args", ".", "num_samples", ",", "instances", ",", "iterator", ",", "args", ".", "cuda_device", ")", "\n", "\n", "logger", ".", "info", "(", "'Finished evaluating.'", ")", "\n", "logger", ".", "info", "(", "'Metrics:'", ")", "\n", "for", "key", ",", "metric", "in", "metrics", ".", "items", "(", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'%s: %s'", ",", "key", ",", "metric", ")", "\n", "\n", "", "output_file", "=", "args", ".", "output_file", "\n", "if", "output_file", ":", "\n", "        ", "with", "open", "(", "output_file", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "json", ".", "dump", "(", "metrics", ",", "f", ",", "indent", "=", "4", ")", "\n", "", "", "return", "metrics", "\n", "\n"]], "home.repos.pwc.inspect_result.rloganiv_kglm-model.testing.kglm_model_test_case.KglmModelTestCase.ensure_model_can_train_save_and_load": [[20, 118], ["allennlp.commands.train.train_model_from_file", "allennlp.commands.train.train_model_from_file.state_dict().keys", "loaded_model.state_dict().keys", "allennlp.common.Params.from_file", "allennlp.data.DatasetReader.from_params", "allennlp.common.Params", "allennlp.data.DataIterator.from_params", "allennlp.data.DataIterator.from_params", "allennlp.data.DatasetReader.from_params.read", "allennlp.data.DataIterator.from_params.index_with", "next", "isinstance", "allennlp.data.DatasetReader.from_params.read", "allennlp.data.DataIterator.from_params.index_with", "next", "isinstance", "kglm_model_test_case.KglmModelTestCase.check_model_computes_gradients_correctly", "next.keys", "allennlp.commands.train.train_model_from_file.eval", "loaded_model.eval", "allennlp.commands.train.train_model_from_file.", "loaded_model", "loaded_model_loss.backward", "allennlp.commands.train.train_model_from_file.keys", "allennlp.models.load_archive", "numpy.testing.assert_allclose", "copy.deepcopy", "allennlp.data.DataIterator.from_params.", "allennlp.data.DataIterator.from_params.", "next.keys", "next.keys", "kglm_model_test_case.KglmModelTestCase.assert_fields_equal", "model_.modules", "kglm_model_test_case.KglmModelTestCase.assert_fields_equal", "allennlp.commands.train.train_model_from_file.state_dict", "loaded_model.state_dict", "[].cpu().numpy", "[].cpu().numpy", "iterator_params.as_dict", "hasattr", "module.reset_states", "[].cpu", "[].cpu", "allennlp.commands.train.train_model_from_file.state_dict", "loaded_model.state_dict"], "methods", ["home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.data.extended_vocabulary.ExtendedVocabulary.from_params", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.simplified.AliasCopynet.eval", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.models.entity_disc.EntityNLMDiscriminator.reset_states", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict", "home.repos.pwc.inspect_result.rloganiv_kglm-model.training.nt_asgd.NTASGDOptimizer.state_dict"], ["def", "ensure_model_can_train_save_and_load", "(", "self", ",", "\n", "param_file", ":", "str", ",", "\n", "tolerance", ":", "float", "=", "1e-4", ",", "\n", "cuda_device", ":", "int", "=", "-", "1", ",", "\n", "gradients_to_ignore", ":", "Set", "[", "str", "]", "=", "None", ",", "\n", "overrides", ":", "str", "=", "\"\"", ")", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        param_file : ``str``\n            Path to a training configuration file that we will use to train the model for this\n            test.\n        tolerance : ``float``, optional (default=1e-4)\n            When comparing model predictions between the originally-trained model and the model\n            after saving and loading, we will use this tolerance value (passed as ``rtol`` to\n            ``numpy.testing.assert_allclose``).\n        cuda_device : ``int``, optional (default=-1)\n            The device to run the test on.\n        gradients_to_ignore : ``Set[str]``, optional (default=None)\n            This test runs a gradient check to make sure that we're actually computing gradients\n            for all of the parameters in the model.  If you really want to ignore certain\n            parameters when doing that check, you can pass their names here.  This is not\n            recommended unless you're `really` sure you don't need to have non-zero gradients for\n            those parameters (e.g., some of the beam search / state machine models have\n            infrequently-used parameters that are hard to force the model to use in a small test).\n        overrides : ``str``, optional (default = \"\")\n            A JSON string that we will use to override values in the input parameter file.\n        \"\"\"", "\n", "save_dir", "=", "self", ".", "TEST_DIR", "/", "\"save_and_load_test\"", "\n", "archive_file", "=", "save_dir", "/", "\"model.tar.gz\"", "\n", "model", "=", "train_model_from_file", "(", "param_file", ",", "save_dir", ",", "overrides", "=", "overrides", ")", "\n", "loaded_model", "=", "load_archive", "(", "archive_file", ",", "cuda_device", "=", "cuda_device", ")", ".", "model", "\n", "state_keys", "=", "model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", "\n", "loaded_state_keys", "=", "loaded_model", ".", "state_dict", "(", ")", ".", "keys", "(", ")", "\n", "assert", "state_keys", "==", "loaded_state_keys", "\n", "# First we make sure that the state dict (the parameters) are the same for both models.", "\n", "for", "key", "in", "state_keys", ":", "\n", "            ", "assert_allclose", "(", "model", ".", "state_dict", "(", ")", "[", "key", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "loaded_model", ".", "state_dict", "(", ")", "[", "key", "]", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "\n", "err_msg", "=", "key", ")", "\n", "", "params", "=", "Params", ".", "from_file", "(", "param_file", ")", "\n", "reader", "=", "DatasetReader", ".", "from_params", "(", "params", "[", "'dataset_reader'", "]", ")", "\n", "\n", "# Need to duplicate params because Iterator.from_params will consume.", "\n", "iterator_params", "=", "params", "[", "'iterator'", "]", "\n", "iterator_params2", "=", "Params", "(", "copy", ".", "deepcopy", "(", "iterator_params", ".", "as_dict", "(", ")", ")", ")", "\n", "\n", "iterator", "=", "DataIterator", ".", "from_params", "(", "iterator_params", ")", "\n", "iterator2", "=", "DataIterator", ".", "from_params", "(", "iterator_params2", ")", "\n", "\n", "# We'll check that even if we index the dataset with each model separately, we still get", "\n", "# the same result out.", "\n", "model_dataset", "=", "reader", ".", "read", "(", "params", "[", "'validation_data_path'", "]", ")", "\n", "iterator", ".", "index_with", "(", "model", ".", "vocab", ")", "\n", "model_batch", "=", "next", "(", "iterator", "(", "model_dataset", ",", "shuffle", "=", "False", ")", ")", "\n", "if", "isinstance", "(", "model_batch", ",", "tuple", ")", ":", "\n", "            ", "model_batch", "=", "model_batch", "[", "0", "]", "\n", "\n", "", "loaded_dataset", "=", "reader", ".", "read", "(", "params", "[", "'validation_data_path'", "]", ")", "\n", "iterator2", ".", "index_with", "(", "loaded_model", ".", "vocab", ")", "\n", "loaded_batch", "=", "next", "(", "iterator2", "(", "loaded_dataset", ",", "shuffle", "=", "False", ")", ")", "\n", "if", "isinstance", "(", "loaded_batch", ",", "tuple", ")", ":", "\n", "            ", "loaded_batch", "=", "loaded_batch", "[", "0", "]", "\n", "\n", "# Check gradients are None for non-trainable parameters and check that", "\n", "# trainable parameters receive some gradient if they are trainable.", "\n", "", "self", ".", "check_model_computes_gradients_correctly", "(", "model", ",", "model_batch", ",", "gradients_to_ignore", ")", "\n", "\n", "# The datasets themselves should be identical.", "\n", "assert", "model_batch", ".", "keys", "(", ")", "==", "loaded_batch", ".", "keys", "(", ")", "\n", "for", "key", "in", "model_batch", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "assert_fields_equal", "(", "model_batch", "[", "key", "]", ",", "loaded_batch", "[", "key", "]", ",", "key", ",", "1e-6", ")", "\n", "\n", "# Set eval mode, to turn off things like dropout, then get predictions.", "\n", "", "model", ".", "eval", "(", ")", "\n", "loaded_model", ".", "eval", "(", ")", "\n", "# Models with stateful RNNs need their states reset to have consistent", "\n", "# behavior after loading.", "\n", "for", "model_", "in", "[", "model", ",", "loaded_model", "]", ":", "\n", "            ", "for", "module", "in", "model_", ".", "modules", "(", ")", ":", "\n", "                ", "if", "hasattr", "(", "module", ",", "'stateful'", ")", "and", "module", ".", "stateful", ":", "\n", "                    ", "module", ".", "reset_states", "(", ")", "\n", "", "", "", "model_predictions", "=", "model", "(", "**", "model_batch", ")", "\n", "loaded_model_predictions", "=", "loaded_model", "(", "**", "loaded_batch", ")", "\n", "\n", "# Check loaded model's loss exists and we can compute gradients, for continuing training.", "\n", "loaded_model_loss", "=", "loaded_model_predictions", "[", "\"loss\"", "]", "\n", "assert", "loaded_model_loss", "is", "not", "None", "\n", "loaded_model_loss", ".", "backward", "(", ")", "\n", "\n", "# Both outputs should have the same keys and the values for these keys should be close.", "\n", "for", "key", "in", "model_predictions", ".", "keys", "(", ")", ":", "\n", "            ", "self", ".", "assert_fields_equal", "(", "model_predictions", "[", "key", "]", ",", "\n", "loaded_model_predictions", "[", "key", "]", ",", "\n", "name", "=", "key", ",", "\n", "tolerance", "=", "tolerance", ")", "\n", "\n", "", "return", "model", ",", "loaded_model", "\n", "", "", ""]]}