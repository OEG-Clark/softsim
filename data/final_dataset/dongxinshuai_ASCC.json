{"home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.opts.parse_opt": [[5, 292], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "parser.parse_args.__dict__.keys", "sys.path.append", "parser.parse_args.__dict__.keys", "os.path.exists", "os.path.exists", "os.environ.keys", "str", "configparser.ConfigParser", "configparser.ConfigParser.read", "config_common.keys", "print", "type", "os.path.join", "open", "f.read", "print", "parser.parse_args.__dict__[].lower", "parser.parse_args.__dict__[].lower", "int", "parser.parse_args.__dict__[].split"], "function", ["None"], ["def", "parse_opt", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "# Data input settings", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "21", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--ascc_mode'", ",", "type", "=", "str", ",", "default", "=", "'comb_p'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--work_path'", ",", "type", "=", "str", ",", "default", "=", "'./'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--smooth_ce'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--if_ce_adp'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--h_test_start'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--vis_w_key_token'", ",", "type", "=", "int", ",", "default", "=", "None", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--snli_epochs'", ",", "type", "=", "int", ",", "default", "=", "81", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--resume_vector_only'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--l2_ball_range'", ",", "type", "=", "str", ",", "default", "=", "'sentence'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--normalize_embedding'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--lm_constraint'", ",", "type", "=", "str", ",", "default", "=", "'true'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--label_smooth'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--imdb_lm_file_path'", ",", "type", "=", "str", ",", "default", "=", "\"lm_scores/imdb_all.txt\"", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--snli_lm_file_path'", ",", "type", "=", "str", ",", "default", "=", "\"lm_scores/snli_all_save.txt\"", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--embd_freeze'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--embd_fc_freeze'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--embd_transform'", ",", "type", "=", "str", ",", "default", "=", "'true'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--certified_neighbors_file_path'", ",", "type", "=", "str", ",", "default", "=", "\"counterfitted_neighbors.json\"", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--train_attack_sparse_weight'", ",", "type", "=", "float", ",", "default", "=", "15", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--attack_sparse_weight'", ",", "type", "=", "float", ",", "default", "=", "15", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--w_optm_lr'", ",", "type", "=", "float", ",", "default", "=", "10", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--bert_w_optm_lr'", ",", "type", "=", "float", ",", "default", "=", "1", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--pert_set'", ",", "type", "=", "str", ",", "default", "=", "'ad_text_syn_p'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--ge_file_path'", ",", "type", "=", "str", ",", "default", "=", "'../../GraphEmbedding/examples/walk_embeddings_it2.pickle'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--out_syn_netx_file'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--freeze_bert_stu'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--freeze_bert_tea'", ",", "type", "=", "str", ",", "default", "=", "'true'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--atten_kl_weight'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--resume'", ",", "type", "=", "str", ",", "default", "=", "None", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--pwws_test_num'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--genetic_test_num'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--genetic_iters'", ",", "type", "=", "int", ",", "default", "=", "40", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--genetic_pop_size'", ",", "type", "=", "int", ",", "default", "=", "60", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--kl_start_epoch'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--weight_adv'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_clean'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_ball'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_kl'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--weight_mi_clean'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_mi_adv'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_mi_giveny_clean'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_mi_giveny_adv'", ",", "type", "=", "float", ",", "default", "=", "0", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--infonce_sim_metric'", ",", "type", "=", "str", ",", "default", "=", "'0429'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--bert_logit_use_RP_tea'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--new_exp'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--info_input_dim'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--test_attack_iters'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--test_attack_eps'", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--test_attack_step_size'", ",", "type", "=", "float", ",", "default", "=", "0.25", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--random_start'", ",", "type", "=", "str", ",", "default", "=", "'true'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--train_attack_iters'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--train_attack_eps'", ",", "type", "=", "float", ",", "default", "=", "5.0", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--train_attack_step_size'", ",", "type", "=", "float", ",", "default", "=", "0.25", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--use_pretrained_embeddings'", ",", "type", "=", "str", ",", "default", "=", "'true'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--imdb_synonyms_file_path'", ",", "type", "=", "str", ",", "default", "=", "\"temp/imdb.synonyms\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--imdb_bert_synonyms_file_path'", ",", "type", "=", "str", ",", "default", "=", "\"temp/imdb.bert.synonyms\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "'--snli_synonyms_file_path'", ",", "type", "=", "str", ",", "default", "=", "\"temp/snli.synonyms\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--snli_bert_synonyms_file_path'", ",", "type", "=", "str", ",", "default", "=", "\"temp/snli.bert.synonyms\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--synonyms_from_file'", ",", "type", "=", "str", ",", "default", "=", "'true'", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--bow_mean'", ",", "type", "=", "str", ",", "default", "=", "'false'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--embedding_prep'", ",", "type", "=", "str", ",", "default", "=", "'ori'", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--out_path'", ",", "type", "=", "str", ",", "default", "=", "\"./\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--train_mode'", ",", "type", "=", "str", ",", "default", "=", "\"set_radius_ad\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--config'", ",", "type", "=", "str", ",", "default", "=", "\"no_file_exists\"", ",", "\n", "help", "=", "'gpu number'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--hidden_dim'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'hidden_dim'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--max_seq_len'", ",", "type", "=", "int", ",", "default", "=", "400", ",", "\n", "help", "=", "'max_seq_len'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'batch_size'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--test_batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'test_batch_size'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--syn_batch_size'", ",", "type", "=", "int", ",", "default", "=", "2048", ",", "\n", "help", "=", "'syn_batch_size'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--embedding_dim'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "help", "=", "'embedding_dim'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--embedding_out_dim'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'embedding_dim'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "0.005", ",", "help", "=", "'learning_rate'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--ball_learning_rate'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "help", "=", "'learning_rate'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "2e-4", ",", "help", "=", "'weight_decay'", ")", "\n", "parser", ".", "add_argument", "(", "'--ball_weight_decay'", ",", "type", "=", "float", ",", "default", "=", "2e-4", ",", "help", "=", "'weight_decay'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--optimizer'", ",", "type", "=", "str", ",", "default", "=", "\"adamw\"", ",", "help", "=", "'optimizer'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--ball_optimizer'", ",", "type", "=", "str", ",", "default", "=", "\"sgd\"", ",", "help", "=", "'ball_optimizer'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--lr_scheduler'", ",", "type", "=", "str", ",", "default", "=", "\"none\"", ",", "help", "=", "'lr_scheduler'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--grad_clip'", ",", "type", "=", "float", ",", "default", "=", "1e-1", ",", "help", "=", "'grad_clip'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--RP_learnable'", ",", "type", "=", "str", ",", "default", "=", "\"RPstu_and_RPtea\"", ",", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--invertible_RP_tea'", ",", "type", "=", "str", ",", "default", "=", "\"false\"", ",", "help", "=", "''", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "\"cnn_adv\"", ",", "\n", "help", "=", "'model name'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ",", "default", "=", "\"imdb\"", ",", "\n", "help", "=", "'dataset'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--position'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "\n", "help", "=", "'gpu number'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--keep_dropout'", ",", "type", "=", "float", ",", "default", "=", "0.2", ",", "\n", "help", "=", "'keep_dropout'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--embedding_file_path'", ",", "type", "=", "str", ",", "default", "=", "\"glove/glove.840B.300d.txt\"", ",", "\n", "help", "=", "'glove or w2v'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--embedding_training'", ",", "type", "=", "str", ",", "default", "=", "\"false\"", ",", "\n", "help", "=", "'embedding_training'", ")", "\n", "#kim CNN", "\n", "parser", ".", "add_argument", "(", "'--kernel_sizes'", ",", "type", "=", "str", ",", "default", "=", "\"1,2,3,5\"", ",", "\n", "help", "=", "'kernel_sizes'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--info_layers'", ",", "type", "=", "str", ",", "default", "=", "\"12,\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--lle_k'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--dist_kl_tau'", ",", "type", "=", "float", ",", "default", "=", "1", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--dist_p_type'", ",", "type", "=", "str", ",", "default", "=", "\"pjci\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--dist_div_type'", ",", "type", "=", "str", ",", "default", "=", "\"kl\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--h_type'", ",", "type", "=", "str", ",", "default", "=", "\"discriminator\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--save_bert_fea'", ",", "type", "=", "str", ",", "default", "=", "\"false\"", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--kernel_nums'", ",", "type", "=", "str", ",", "default", "=", "\"256,256,256,256\"", ",", "\n", "help", "=", "'kernel_nums'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_type'", ",", "type", "=", "str", ",", "default", "=", "\"non-static\"", ",", "\n", "help", "=", "'embedding_type'", ")", "\n", "parser", ".", "add_argument", "(", "'--lstm_mean'", ",", "type", "=", "str", ",", "default", "=", "\"mean\"", ",", "# last", "\n", "help", "=", "'lstm_mean'", ")", "\n", "parser", ".", "add_argument", "(", "'--lstm_layers'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "# last", "\n", "help", "=", "'lstm_layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'gpu number'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu_num'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'gpu number'", ")", "\n", "parser", ".", "add_argument", "(", "'--proxy'", ",", "type", "=", "str", ",", "default", "=", "\"null\"", ",", "\n", "help", "=", "'http://proxy.xx.com:8080'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--debug'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "\n", "help", "=", "''", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--bidirectional'", ",", "type", "=", "str", ",", "default", "=", "\"true\"", ",", "\n", "help", "=", "'bidirectional'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--embedding_dir'", ",", "type", "=", "str", ",", "default", "=", "\".glove/glove.6B.300d.txt\"", ",", "\n", "help", "=", "'embedding_dir'", ")", "\n", "\n", "#", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "config", "!=", "\"no_file_exists\"", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "config", ")", ":", "\n", "            ", "config", "=", "configparser", ".", "ConfigParser", "(", ")", "\n", "config_file_path", "=", "args", ".", "config", "\n", "config", ".", "read", "(", "config_file_path", ")", "\n", "config_common", "=", "config", "[", "'COMMON'", "]", "\n", "for", "key", "in", "config_common", ".", "keys", "(", ")", ":", "\n", "                ", "args", ".", "__dict__", "[", "key", "]", "=", "config_common", "[", "key", "]", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "\"config file named %s does not exist\"", "%", "args", ".", "config", ")", "\n", "\n", "#        args.kernel_sizes = [int(i) for i in args.kernel_sizes.split(\",\")]", "\n", "#        args.kernel_nums = [int(i) for i in args.kernel_nums.split(\",\")]", "\n", "#", "\n", "#    # Check if args are valid", "\n", "#    assert args.rnn_size > 0, \"rnn_size should be greater than 0\"", "\n", "\n", "", "", "if", "\"CUDA_VISIBLE_DEVICES\"", "not", "in", "os", ".", "environ", ".", "keys", "(", ")", ":", "\n", "        ", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "str", "(", "args", ".", "gpu", ")", "\n", "\n", "# process the type for bool and list    ", "\n", "", "for", "arg", "in", "args", ".", "__dict__", ".", "keys", "(", ")", ":", "\n", "        ", "if", "type", "(", "args", ".", "__dict__", "[", "arg", "]", ")", "==", "str", ":", "\n", "            ", "if", "args", ".", "__dict__", "[", "arg", "]", ".", "lower", "(", ")", "==", "\"true\"", ":", "\n", "                ", "args", ".", "__dict__", "[", "arg", "]", "=", "True", "\n", "", "elif", "args", ".", "__dict__", "[", "arg", "]", ".", "lower", "(", ")", "==", "\"false\"", ":", "\n", "                ", "args", ".", "__dict__", "[", "arg", "]", "=", "False", "\n", "", "elif", "\",\"", "in", "args", ".", "__dict__", "[", "arg", "]", ":", "\n", "                ", "args", ".", "__dict__", "[", "arg", "]", "=", "[", "int", "(", "i", ")", "for", "i", "in", "args", ".", "__dict__", "[", "arg", "]", ".", "split", "(", "\",\"", ")", "if", "i", "!=", "''", "]", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "", "", "sys", ".", "path", ".", "append", "(", "args", ".", "work_path", ")", "\n", "\n", "for", "arg", "in", "args", ".", "__dict__", ".", "keys", "(", ")", ":", "\n", "        ", "if", "\"path\"", "in", "arg", "and", "arg", "!=", "\"work_path\"", ":", "\n", "            ", "args", ".", "__dict__", "[", "arg", "]", "=", "os", ".", "path", ".", "join", "(", "args", ".", "work_path", ",", "args", ".", "__dict__", "[", "arg", "]", ")", "\n", "\n", "", "", "if", "os", ".", "path", ".", "exists", "(", "\"proxy.config\"", ")", ":", "\n", "        ", "with", "open", "(", "\"proxy.config\"", ")", "as", "f", ":", "\n", "            ", "args", ".", "proxy", "=", "f", ".", "read", "(", ")", "\n", "print", "(", "args", ".", "proxy", ")", "\n", "\n", "", "", "return", "args", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.set_params": [[35, 49], ["print", "os.path.isfile", "torch.load", "torch.load", "torch.load", "torch.load", "OrderedDict", "state_dict.keys", "net.load_state_dict", "key.split"], "function", ["None"], ["", "def", "set_params", "(", "net", ",", "resume_model_path", ",", "data_parallel", "=", "False", ")", ":", "\n", "    ", "print", "(", "'==> Resuming from checkpoint..'", ")", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "resume_model_path", ")", ",", "'Error: '", "+", "resume_model_path", "+", "'checkpoint not found!'", "\n", "checkpoint", "=", "torch", ".", "load", "(", "resume_model_path", ")", "\n", "state_dict", "=", "checkpoint", "[", "'net'", "]", "\n", "from", "collections", "import", "OrderedDict", "\n", "sdict", "=", "OrderedDict", "(", ")", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "        ", "new_key", "=", "key", ".", "split", "(", "'module.'", ")", "[", "-", "1", "]", "\n", "if", "data_parallel", ":", "\n", "            ", "new_key", "=", "'module.'", "+", "new_key", "\n", "", "sdict", "[", "new_key", "]", "=", "state_dict", "[", "key", "]", "\n", "", "net", ".", "load_state_dict", "(", "sdict", ")", "\n", "return", "net", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train": [[51, 299], ["time.time", "models.setup", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "utils.getOptimizer", "solver.lr_scheduler.WarmupMultiStepLR", "time.time", "get_tokenizer", "range", "train_imdb_ascc.set_params", "utils.imdb_evaluation", "print", "utils.imdb_evaluation_ascc_attack", "print", "PWWS.fool_pytorch.genetic_attack", "PWWS.fool_pytorch.fool_text_classifier_pytorch", "train_imdb_ascc.set_params", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "set_params.cuda", "LabelSmoothSoftmaxCE", "range", "range", "print", "set", "print", "LMConstrainedAttackSurface.from_files", "WordSubstitutionAttackSurface.from_files", "enumerate", "solver.lr_scheduler.WarmupMultiStepLR.step", "set_params.parameters", "utils.get_lr_scheduler", "train_imdb_ascc.train.get_father"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.__init__.setup", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.getOptimizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.set_params", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.imdb_evaluation", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.imdb_evaluation_ascc_attack", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool_pytorch.genetic_attack", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool_pytorch.fool_text_classifier_pytorch", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.set_params", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.from_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.from_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.get_lr_scheduler"], ["", "def", "train", "(", "opt", ",", "train_iter", ",", "dev_iter", ",", "test_iter", ",", "syn_data", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "global_start", "=", "time", ".", "time", "(", ")", "\n", "#logger = utils.getLogger()", "\n", "model", "=", "models", ".", "setup", "(", "opt", ")", "\n", "\n", "if", "opt", ".", "resume", "!=", "None", ":", "\n", "        ", "model", "=", "set_params", "(", "model", ",", "opt", ".", "resume", ")", "\n", "\n", "", "device", "=", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "model", ".", "cuda", "(", ")", "\n", "#model=torch.nn.DataParallel(model)", "\n", "\n", "# set optimizer", "\n", "", "if", "opt", ".", "embd_freeze", "==", "True", ":", "\n", "        ", "model", ".", "embedding", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "else", ":", "\n", "        ", "model", ".", "embedding", ".", "weight", ".", "requires_grad", "=", "True", "\n", "", "params", "=", "[", "param", "for", "param", "in", "model", ".", "parameters", "(", ")", "if", "param", ".", "requires_grad", "]", "#filter(lambda p: p.requires_grad, model.parameters())", "\n", "optimizer", "=", "utils", ".", "getOptimizer", "(", "params", ",", "name", "=", "opt", ".", "optimizer", ",", "lr", "=", "opt", ".", "learning_rate", ",", "weight_decay", "=", "opt", ".", "weight_decay", ",", "scheduler", "=", "utils", ".", "get_lr_scheduler", "(", "opt", ".", "lr_scheduler", ")", ")", "\n", "scheduler", "=", "WarmupMultiStepLR", "(", "optimizer", ",", "(", "40", ",", "80", ")", ",", "0.1", ",", "1.0", "/", "10.0", ",", "2", ",", "'linear'", ")", "\n", "\n", "from", "label_smooth", "import", "LabelSmoothSoftmaxCE", "\n", "if", "opt", ".", "label_smooth", "!=", "0", ":", "\n", "        ", "assert", "(", "opt", ".", "label_smooth", "<=", "1", "and", "opt", ".", "label_smooth", ">", "0", ")", "\n", "loss_fun", "=", "LabelSmoothSoftmaxCE", "(", "lb_pos", "=", "1", "-", "opt", ".", "label_smooth", ",", "lb_neg", "=", "opt", ".", "label_smooth", ")", "\n", "", "else", ":", "\n", "        ", "loss_fun", "=", "F", ".", "cross_entropy", "\n", "\n", "", "filename", "=", "None", "\n", "acc_adv_list", "=", "[", "]", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "kl_control", "=", "0", "\n", "\n", "# initialize synonyms with the same embd", "\n", "from", "PWWS", ".", "word_level_process", "import", "word_process", ",", "get_tokenizer", "\n", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "\n", "if", "opt", ".", "embedding_prep", "==", "\"same\"", ":", "\n", "        ", "father_dict", "=", "{", "}", "\n", "for", "index", "in", "range", "(", "1", "+", "len", "(", "tokenizer", ".", "index_word", ")", ")", ":", "\n", "            ", "father_dict", "[", "index", "]", "=", "index", "\n", "\n", "", "def", "get_father", "(", "x", ")", ":", "\n", "            ", "if", "father_dict", "[", "x", "]", "==", "x", ":", "\n", "                ", "return", "x", "\n", "", "else", ":", "\n", "                ", "fa", "=", "get_father", "(", "father_dict", "[", "x", "]", ")", "\n", "father_dict", "[", "x", "]", "=", "fa", "\n", "return", "fa", "\n", "\n", "", "", "for", "index", "in", "range", "(", "len", "(", "syn_data", ")", "-", "1", ",", "0", ",", "-", "1", ")", ":", "\n", "            ", "syn_list", "=", "syn_data", "[", "index", "]", "\n", "for", "pos", "in", "syn_list", ":", "\n", "                ", "fa_pos", "=", "get_father", "(", "pos", ")", "\n", "fa_anch", "=", "get_father", "(", "index", ")", "\n", "if", "fa_pos", "==", "fa_anch", ":", "\n", "                    ", "father_dict", "[", "index", "]", "=", "index", "\n", "father_dict", "[", "fa_anch", "]", "=", "index", "\n", "", "else", ":", "\n", "                    ", "father_dict", "[", "index", "]", "=", "index", "\n", "father_dict", "[", "fa_anch", "]", "=", "index", "\n", "father_dict", "[", "fa_pos", "]", "=", "index", "\n", "\n", "", "", "", "print", "(", "\"Same embedding for synonyms as embd prep.\"", ")", "\n", "set_different_embd", "=", "set", "(", ")", "\n", "for", "key", "in", "father_dict", ":", "\n", "            ", "fa", "=", "get_father", "(", "key", ")", "\n", "set_different_embd", ".", "add", "(", "fa", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "model", ".", "embedding", ".", "weight", "[", "key", ",", ":", "]", "=", "model", ".", "embedding", ".", "weight", "[", "fa", ",", ":", "]", "\n", "", "", "print", "(", "len", "(", "set_different_embd", ")", ")", "\n", "\n", "", "elif", "opt", ".", "embedding_prep", "==", "\"ge\"", ":", "\n", "        ", "print", "(", "\"Graph embedding as embd prep.\"", ")", "\n", "ge_file_path", "=", "opt", ".", "ge_file_path", "\n", "f", "=", "open", "(", "ge_file_path", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "ge_embeddings_dict", "=", "saved", "[", "'walk_embeddings'", "]", "\n", "#model = saved['model']", "\n", "f", ".", "close", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "key", "in", "ge_embeddings_dict", ":", "\n", "                ", "model", ".", "embedding", ".", "weight", "[", "int", "(", "key", ")", ",", ":", "]", "=", "torch", ".", "FloatTensor", "(", "ge_embeddings_dict", "[", "key", "]", ")", "\n", "", "", "", "else", ":", "\n", "        ", "print", "(", "\"No embd prep.\"", ")", "\n", "\n", "", "from", "from_certified", ".", "attack_surface", "import", "WordSubstitutionAttackSurface", ",", "LMConstrainedAttackSurface", "\n", "if", "opt", ".", "lm_constraint", ":", "\n", "        ", "attack_surface", "=", "LMConstrainedAttackSurface", ".", "from_files", "(", "opt", ".", "certified_neighbors_file_path", ",", "opt", ".", "imdb_lm_file_path", ")", "\n", "", "else", ":", "\n", "        ", "attack_surface", "=", "WordSubstitutionAttackSurface", ".", "from_files", "(", "opt", ".", "certified_neighbors_file_path", ",", "opt", ".", "imdb_lm_file_path", ")", "\n", "\n", "", "best_adv_acc", "=", "0", "\n", "for", "epoch", "in", "range", "(", "21", ")", ":", "\n", "\n", "        ", "if", "opt", ".", "smooth_ce", ":", "\n", "            ", "if", "epoch", "<", "10", ":", "\n", "                ", "weight_adv", "=", "epoch", "*", "1.0", "/", "10", "\n", "weight_clean", "=", "1", "-", "weight_adv", "\n", "", "else", ":", "\n", "                ", "weight_adv", "=", "1", "\n", "weight_clean", "=", "0", "\n", "", "", "else", ":", "\n", "            ", "weight_adv", "=", "opt", ".", "weight_adv", "\n", "weight_clean", "=", "opt", ".", "weight_clean", "\n", "\n", "", "if", "epoch", ">=", "opt", ".", "kl_start_epoch", ":", "\n", "            ", "kl_control", "=", "1", "\n", "\n", "", "sum_loss", "=", "sum_loss_adv", "=", "sum_loss_kl", "=", "sum_loss_clean", "=", "0", "\n", "total", "=", "0", "\n", "\n", "for", "iters", ",", "batch", "in", "enumerate", "(", "train_iter", ")", ":", "\n", "\n", "            ", "text", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "label", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "anch", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "pos", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "neg", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "anch_valid", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", ".", "unsqueeze", "(", "2", ")", "\n", "text_like_syn", "=", "batch", "[", "6", "]", ".", "to", "(", "device", ")", "\n", "text_like_syn_valid", "=", "batch", "[", "7", "]", ".", "to", "(", "device", ")", "\n", "\n", "bs", ",", "sent_len", "=", "text", ".", "shape", "\n", "\n", "model", ".", "train", "(", ")", "\n", "\n", "# zero grad", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "if", "opt", ".", "pert_set", "==", "\"ad_text\"", ":", "\n", "                ", "attack_type_dict", "=", "{", "\n", "'num_steps'", ":", "opt", ".", "train_attack_iters", ",", "\n", "'loss_func'", ":", "'ce'", "if", "opt", ".", "if_ce_adp", "else", "'kl'", ",", "\n", "'w_optm_lr'", ":", "opt", ".", "w_optm_lr", ",", "\n", "'sparse_weight'", ":", "opt", ".", "attack_sparse_weight", ",", "\n", "'out_type'", ":", "\"text\"", "\n", "}", "\n", "embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "input", "=", "text", ")", "#in bs, len sent, vocab", "\n", "n", ",", "l", ",", "s", "=", "text_like_syn", ".", "shape", "\n", "text_like_syn_embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "input", "=", "text_like_syn", ".", "reshape", "(", "n", ",", "l", "*", "s", ")", ")", ".", "reshape", "(", "n", ",", "l", ",", "s", ",", "-", "1", ")", "\n", "text_adv", "=", "model", "(", "mode", "=", "\"get_adv_by_convex_syn\"", ",", "input", "=", "embd", ",", "label", "=", "label", ",", "text_like_syn_embd", "=", "text_like_syn_embd", ",", "text_like_syn_valid", "=", "text_like_syn_valid", ",", "text_like_syn", "=", "text_like_syn", ",", "attack_type_dict", "=", "attack_type_dict", ")", "\n", "\n", "", "elif", "opt", ".", "pert_set", "==", "\"ad_text_syn_p\"", ":", "\n", "                ", "attack_type_dict", "=", "{", "\n", "'num_steps'", ":", "opt", ".", "train_attack_iters", ",", "\n", "'loss_func'", ":", "'ce'", "if", "opt", ".", "if_ce_adp", "else", "'kl'", ",", "\n", "'w_optm_lr'", ":", "opt", ".", "w_optm_lr", ",", "\n", "'sparse_weight'", ":", "opt", ".", "train_attack_sparse_weight", ",", "\n", "'out_type'", ":", "\"comb_p\"", "\n", "}", "\n", "embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "input", "=", "text", ")", "#in bs, len sent, vocab", "\n", "n", ",", "l", ",", "s", "=", "text_like_syn", ".", "shape", "\n", "text_like_syn_embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "input", "=", "text_like_syn", ".", "reshape", "(", "n", ",", "l", "*", "s", ")", ")", ".", "reshape", "(", "n", ",", "l", ",", "s", ",", "-", "1", ")", "\n", "adv_comb_p", "=", "model", "(", "mode", "=", "\"get_adv_by_convex_syn\"", ",", "input", "=", "embd", ",", "label", "=", "label", ",", "text_like_syn_embd", "=", "text_like_syn_embd", ",", "text_like_syn_valid", "=", "text_like_syn_valid", ",", "attack_type_dict", "=", "attack_type_dict", ")", "\n", "\n", "", "elif", "opt", ".", "pert_set", "==", "\"ad_text_hotflip\"", ":", "\n", "                ", "attack_type_dict", "=", "{", "\n", "'num_steps'", ":", "opt", ".", "train_attack_iters", ",", "\n", "'loss_func'", ":", "'ce'", "if", "opt", ".", "if_ce_adp", "else", "'kl'", ",", "\n", "}", "\n", "text_adv", "=", "model", "(", "mode", "=", "\"get_adv_hotflip\"", ",", "input", "=", "text", ",", "label", "=", "label", ",", "text_like_syn_valid", "=", "text_like_syn_valid", ",", "text_like_syn", "=", "text_like_syn", ",", "attack_type_dict", "=", "attack_type_dict", ")", "\n", "\n", "", "elif", "opt", ".", "pert_set", "==", "\"l2_ball\"", ":", "\n", "                ", "set_radius", "=", "opt", ".", "train_attack_eps", "\n", "attack_type_dict", "=", "{", "\n", "'num_steps'", ":", "opt", ".", "train_attack_iters", ",", "\n", "'step_size'", ":", "opt", ".", "train_attack_step_size", "*", "set_radius", ",", "\n", "'random_start'", ":", "opt", ".", "random_start", ",", "\n", "'epsilon'", ":", "set_radius", ",", "\n", "#'loss_func': 'ce',", "\n", "'loss_func'", ":", "'ce'", "if", "opt", ".", "if_ce_adp", "else", "'kl'", ",", "\n", "'direction'", ":", "'away'", ",", "\n", "'ball_range'", ":", "opt", ".", "l2_ball_range", ",", "\n", "}", "\n", "embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "input", "=", "text", ")", "#in bs, len sent, vocab", "\n", "embd_adv", "=", "model", "(", "mode", "=", "\"get_embd_adv\"", ",", "input", "=", "embd", ",", "label", "=", "label", ",", "attack_type_dict", "=", "attack_type_dict", ")", "\n", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "# clean loss", "\n", "predicted", "=", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "input", "=", "text", ")", "\n", "loss_clean", "=", "loss_fun", "(", "predicted", ",", "label", ")", "\n", "# adv loss", "\n", "if", "opt", ".", "pert_set", "==", "\"ad_text\"", "or", "opt", ".", "pert_set", "==", "\"ad_text_hotflip\"", ":", "\n", "                ", "predicted_adv", "=", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "input", "=", "text_adv", ")", "\n", "", "elif", "opt", ".", "pert_set", "==", "\"ad_text_syn_p\"", ":", "\n", "                ", "predicted_adv", "=", "model", "(", "mode", "=", "\"text_syn_p_to_logit\"", ",", "input", "=", "text_like_syn", ",", "comb_p", "=", "adv_comb_p", ")", "\n", "", "elif", "opt", ".", "pert_set", "==", "\"l2_ball\"", ":", "\n", "                ", "predicted_adv", "=", "model", "(", "mode", "=", "\"embd_to_logit\"", ",", "input", "=", "embd_adv", ")", "\n", "\n", "", "loss_adv", "=", "loss_fun", "(", "predicted_adv", ",", "label", ")", "\n", "# kl loss", "\n", "criterion_kl", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "loss_kl", "=", "(", "1.0", "/", "bs", ")", "*", "criterion_kl", "(", "F", ".", "log_softmax", "(", "predicted_adv", ",", "dim", "=", "1", ")", ",", "\n", "F", ".", "softmax", "(", "predicted", ",", "dim", "=", "1", ")", ")", "\n", "\n", "# optimize", "\n", "loss", "=", "opt", ".", "weight_kl", "*", "kl_control", "*", "loss_kl", "+", "weight_adv", "*", "loss_adv", "+", "weight_clean", "*", "loss_clean", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "sum_loss", "+=", "loss", ".", "item", "(", ")", "\n", "sum_loss_adv", "+=", "loss_adv", ".", "item", "(", ")", "\n", "sum_loss_clean", "+=", "loss_clean", ".", "item", "(", ")", "\n", "sum_loss_kl", "+=", "loss_kl", ".", "item", "(", ")", "\n", "predicted", ",", "idx", "=", "torch", ".", "max", "(", "predicted", ",", "1", ")", "\n", "precision", "=", "(", "idx", "==", "label", ")", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "predicted_adv", ",", "idx", "=", "torch", ".", "max", "(", "predicted_adv", ",", "1", ")", "\n", "precision_adv", "=", "(", "idx", "==", "label", ")", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "total", "+=", "1", "\n", "\n", "out_log", "=", "\"%d epoch %d iters: loss: %.3f, loss_kl: %.3f, loss_adv: %.3f, loss_clean: %.3f | acc: %.3f acc_adv: %.3f | in %.3f seconds\"", "%", "(", "epoch", ",", "iters", ",", "sum_loss", "/", "total", ",", "sum_loss_kl", "/", "total", ",", "sum_loss_adv", "/", "total", ",", "sum_loss_clean", "/", "total", ",", "precision", ",", "precision_adv", ",", "time", ".", "time", "(", ")", "-", "start", ")", "\n", "start", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "out_log", ")", "\n", "\n", "\n", "", "scheduler", ".", "step", "(", ")", "\n", "\n", "if", "epoch", "%", "1", "==", "0", ":", "\n", "            ", "acc", "=", "utils", ".", "imdb_evaluation", "(", "opt", ",", "device", ",", "model", ",", "dev_iter", ")", "\n", "out_log", "=", "\"%d epoch with dev acc %.4f\"", "%", "(", "epoch", ",", "acc", ")", "\n", "print", "(", "out_log", ")", "\n", "adv_acc", "=", "utils", ".", "imdb_evaluation_ascc_attack", "(", "opt", ",", "device", ",", "model", ",", "dev_iter", ",", "tokenizer", ")", "\n", "out_log", "=", "\"%d epoch with dev adv acc against ascc attack %.4f\"", "%", "(", "epoch", ",", "adv_acc", ")", "\n", "print", "(", "out_log", ")", "\n", "\n", "#hotflip_adv_acc=utils.evaluation_hotflip_adv(opt, device, model, dev_iter, tokenizer)", "\n", "#out_log=\"%d epoch with dev hotflip adv acc %.4f\" % (epoch,hotflip_adv_acc)", "\n", "#logger.info(out_log)", "\n", "#print(out_log)", "\n", "\n", "if", "adv_acc", ">=", "best_adv_acc", ":", "\n", "                ", "best_adv_acc", "=", "adv_acc", "\n", "best_save_dir", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "out_path", ",", "\"{}_best.pth\"", ".", "format", "(", "opt", ".", "model", ")", ")", "\n", "state", "=", "{", "\n", "'net'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "'epoch'", ":", "epoch", ",", "\n", "}", "\n", "torch", ".", "save", "(", "state", ",", "best_save_dir", ")", "\n", "\n", "# restore best according to dev set", "\n", "", "", "", "model", "=", "set_params", "(", "model", ",", "best_save_dir", ")", "\n", "acc", "=", "utils", ".", "imdb_evaluation", "(", "opt", ",", "device", ",", "model", ",", "test_iter", ")", "\n", "print", "(", "\"test acc %.4f\"", "%", "(", "acc", ")", ")", "\n", "adv_acc", "=", "utils", ".", "imdb_evaluation_ascc_attack", "(", "opt", ",", "device", ",", "model", ",", "test_iter", ",", "tokenizer", ")", "\n", "print", "(", "\"test adv acc against ascc attack %.4f\"", "%", "(", "adv_acc", ")", ")", "\n", "genetic_attack", "(", "opt", ",", "device", ",", "model", ",", "attack_surface", ",", "dataset", "=", "opt", ".", "dataset", ",", "genetic_test_num", "=", "opt", ".", "genetic_test_num", ")", "\n", "fool_text_classifier_pytorch", "(", "opt", ",", "device", ",", "model", ",", "dataset", "=", "opt", ".", "dataset", ",", "clean_samples_cap", "=", "opt", ".", "pwws_test_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.main": [[300, 308], ["opts.parse_opt", "print", "dataHelper.imdb_make_synthesized_iter", "train_imdb_ascc.train"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.parse_opt", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.imdb_make_synthesized_iter", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train"], ["", "def", "main", "(", ")", ":", "\n", "    ", "opt", "=", "opts", ".", "parse_opt", "(", ")", "\n", "print", "(", "opt", ")", "\n", "\n", "assert", "(", "opt", ".", "dataset", "==", "\"imdb\"", ")", "\n", "train_iter", ",", "dev_iter", ",", "test_iter", ",", "syn_data", "=", "imdb_make_synthesized_iter", "(", "opt", ")", "\n", "\n", "train", "(", "opt", ",", "train_iter", ",", "dev_iter", ",", "test_iter", ",", "syn_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.__init__": [[32, 41], ["dataHelper.Alphabet.add", "dataHelper.Alphabet.add", "dataHelper.Alphabet.add", "dataHelper.Alphabet.get", "dataHelper.Alphabet.get", "dataHelper.Alphabet.get"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["    ", "def", "__init__", "(", "self", ",", "start_feature_id", "=", "1", ",", "alphabet_type", "=", "\"text\"", ")", ":", "\n", "        ", "self", ".", "fid", "=", "start_feature_id", "\n", "if", "alphabet_type", "==", "\"text\"", ":", "\n", "            ", "self", ".", "add", "(", "'[PADDING]'", ")", "\n", "self", ".", "add", "(", "'[UNK]'", ")", "\n", "self", ".", "add", "(", "'[END]'", ")", "\n", "self", ".", "unknow_token", "=", "self", ".", "get", "(", "'[UNK]'", ")", "\n", "self", ".", "end_token", "=", "self", ".", "get", "(", "'[END]'", ")", "\n", "self", ".", "padding_token", "=", "self", ".", "get", "(", "'[PADDING]'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add": [[42, 50], ["dataHelper.Alphabet.get"], "methods", ["None"], ["", "", "def", "add", "(", "self", ",", "item", ")", ":", "\n", "        ", "idx", "=", "self", ".", "get", "(", "item", ",", "None", ")", "\n", "if", "idx", "is", "None", ":", "\n", "            ", "idx", "=", "self", ".", "fid", "\n", "self", "[", "item", "]", "=", "idx", "\n", "# self[idx] = item", "\n", "self", ".", "fid", "+=", "1", "\n", "", "return", "idx", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.addAll": [[51, 54], ["dataHelper.Alphabet.add"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["", "def", "addAll", "(", "self", ",", "words", ")", ":", "\n", "        ", "for", "word", "in", "words", ":", "\n", "            ", "self", ".", "add", "(", "word", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump": [[55, 61], ["os.path.exists", "os.mkdir", "codecs.open", "sorted", "os.path.join", "dataHelper.Alphabet.keys", "out.write"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write"], ["", "", "def", "dump", "(", "self", ",", "fname", ",", "path", "=", "\"temp\"", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "path", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "fname", ")", ",", "\"w\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "out", ":", "\n", "            ", "for", "k", "in", "sorted", "(", "self", ".", "keys", "(", ")", ")", ":", "\n", "                ", "out", ".", "write", "(", "\"{}\\t{}\\n\"", ".", "format", "(", "k", ",", "self", "[", "k", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.DottableDict.__init__": [[63, 67], ["dict.__init__", "dataHelper.DottableDict.allowDotting"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.DottableDict.allowDotting"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "dict", ".", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "__dict__", "=", "self", "\n", "self", ".", "allowDotting", "(", ")", "\n", "", "def", "allowDotting", "(", "self", ",", "state", "=", "True", ")", ":", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.DottableDict.allowDotting": [[67, 72], ["dict"], "methods", ["None"], ["", "def", "allowDotting", "(", "self", ",", "state", "=", "True", ")", ":", "\n", "        ", "if", "state", ":", "\n", "            ", "self", ".", "__dict__", "=", "self", "\n", "", "else", ":", "\n", "            ", "self", ".", "__dict__", "=", "dict", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_PWWS.__init__": [[74, 83], ["numpy.argmax", "dataHelper.BucketIterator_PWWS.setup"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.__init__.setup"], ["    ", "def", "__init__", "(", "self", ",", "x", ",", "y", ",", "z", ",", "opt", "=", "None", ",", "batch_size", "=", "2", ",", "shuffle", "=", "True", ",", "test", "=", "False", ",", "position", "=", "False", ")", ":", "\n", "        ", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "x", "=", "x", "\n", "self", ".", "y", "=", "np", ".", "argmax", "(", "y", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "z", "=", "z", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "test", "=", "test", "\n", "if", "opt", "is", "not", "None", ":", "\n", "            ", "self", ".", "setup", "(", "opt", ")", "\n", "", "", "def", "setup", "(", "self", ",", "opt", ")", ":", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_PWWS.setup": [[83, 90], ["opt.__dict__.get", "opt.__dict__.get"], "methods", ["None"], ["", "", "def", "setup", "(", "self", ",", "opt", ")", ":", "\n", "\n", "        ", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "shuffle", "=", "opt", ".", "__dict__", ".", "get", "(", "\"shuffle\"", ",", "self", ".", "shuffle", ")", "\n", "self", ".", "position", "=", "opt", ".", "__dict__", ".", "get", "(", "\"position\"", ",", "False", ")", "\n", "if", "self", ".", "position", ":", "\n", "            ", "self", ".", "padding_token", "=", "opt", ".", "alphabet", ".", "padding_token", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_PWWS.transform": [[91, 106], ["torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "dataHelper.DottableDict", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "dataHelper.BucketIterator_PWWS.get_position", "dataHelper.DottableDict", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_PWWS.get_position"], ["", "", "def", "transform", "(", "self", ",", "batch_x", ",", "batch_y", ",", "batch_z", ")", ":", "\n", "        ", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "#data=data.reset_index()", "\n", "            ", "text", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "batch_x", ")", ".", "cuda", "(", ")", ")", "\n", "label", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "batch_y", ")", ".", "cuda", "(", ")", ")", "\n", "ori_text", "=", "batch_z", "\n", "", "else", ":", "\n", "#data=data.reset_index()", "\n", "            ", "text", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "batch_x", ")", ")", "\n", "label", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "batch_y", ")", ")", "\n", "ori_text", "=", "batch_z", "\n", "", "if", "self", ".", "position", ":", "\n", "            ", "position_tensor", "=", "self", ".", "get_position", "(", "data", ".", "text", ")", "\n", "return", "DottableDict", "(", "{", "\"text\"", ":", "(", "text", ",", "position_tensor", ")", ",", "\"label\"", ":", "label", ",", "\"ori_text\"", ":", "ori_text", "}", ")", "\n", "", "return", "DottableDict", "(", "{", "\"text\"", ":", "text", ",", "\"label\"", ":", "label", ",", "\"ori_text\"", ":", "ori_text", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_PWWS.get_position": [[107, 113], ["numpy.array", "torch.autograd.Variable", "torch.autograd.Variable", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "inst_position_tensor.cuda.cuda.cuda", "enumerate"], "methods", ["None"], ["", "def", "get_position", "(", "self", ",", "inst_data", ")", ":", "\n", "        ", "inst_position", "=", "np", ".", "array", "(", "[", "[", "pos_i", "+", "1", "if", "w_i", "!=", "self", ".", "padding_token", "else", "0", "for", "pos_i", ",", "w_i", "in", "enumerate", "(", "inst", ")", "]", "for", "inst", "in", "inst_data", "]", ")", "\n", "inst_position_tensor", "=", "Variable", "(", "torch", ".", "LongTensor", "(", "inst_position", ")", ",", "volatile", "=", "self", ".", "test", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "inst_position_tensor", "=", "inst_position_tensor", ".", "cuda", "(", ")", "\n", "", "return", "inst_position_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_PWWS.__iter__": [[114, 138], ["int", "range", "random.randint", "random.seed", "list", "random.shuffle", "numpy.array", "random.seed", "list", "random.shuffle", "numpy.array", "random.seed", "random.shuffle", "dataHelper.BucketIterator_PWWS.transform", "len", "dataHelper.BucketIterator_PWWS.transform"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "shuffle", ":", "\n", "            ", "import", "random", "\n", "seed", "=", "random", ".", "randint", "(", "0", ",", "100", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "x", "=", "list", "(", "self", ".", "x", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "x", ")", "\n", "self", ".", "x", "=", "np", ".", "array", "(", "self", ".", "x", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "y", "=", "list", "(", "self", ".", "y", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "y", ")", "\n", "self", ".", "y", "=", "np", ".", "array", "(", "self", ".", "y", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "z", ")", "\n", "\n", "", "batch_nums", "=", "int", "(", "len", "(", "self", ".", "x", ")", "/", "self", ".", "batch_size", ")", "\n", "for", "i", "in", "range", "(", "batch_nums", ")", ":", "\n", "            ", "start", "=", "i", "*", "self", ".", "batch_size", "\n", "end", "=", "(", "i", "+", "1", ")", "*", "self", ".", "batch_size", "\n", "yield", "self", ".", "transform", "(", "self", ".", "x", "[", "start", ":", "end", "]", ",", "self", ".", "y", "[", "start", ":", "end", "]", ",", "self", ".", "z", "[", "start", ":", "end", "]", ")", "\n", "", "yield", "self", ".", "transform", "(", "self", ".", "x", "[", "-", "1", "*", "self", ".", "batch_size", ":", "]", ",", "self", ".", "y", "[", "-", "1", "*", "self", ".", "batch_size", ":", "]", ",", "self", ".", "z", "[", "-", "1", "*", "self", ".", "batch_size", ":", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData.__init__": [[143, 153], ["super().__init__", "x.copy", "y.copy", "syn_data.copy", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "x", ",", "y", ",", "syn_data", ")", ":", "\n", "        ", "super", "(", "SynthesizedData", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "x", "=", "x", ".", "copy", "(", ")", "\n", "self", ".", "y", "=", "y", ".", "copy", "(", ")", "\n", "self", ".", "syn_data", "=", "syn_data", ".", "copy", "(", ")", "\n", "\n", "for", "x", "in", "range", "(", "len", "(", "self", ".", "syn_data", ")", ")", ":", "\n", "            ", "self", ".", "syn_data", "[", "x", "]", "=", "[", "syn_word", "for", "syn_word", "in", "self", ".", "syn_data", "[", "x", "]", "if", "syn_word", "!=", "x", "]", "\n", "\n", "", "self", ".", "len_voc", "=", "len", "(", "self", ".", "syn_data", ")", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData.transform": [[154, 157], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "sent", ",", "label", ",", "anch", ",", "pos", ",", "neg", ",", "anch_valid", ")", ":", "\n", "\n", "        ", "return", "torch", ".", "tensor", "(", "sent", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "label", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "anch", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "pos", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "neg", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "anch_valid", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData.__getitem__": [[158, 211], ["dataHelper.SynthesizedData.y[].argmax", "dataHelper.SynthesizedData.transform", "len", "random.sample", "set", "neg.append", "len", "random.sample.append", "anch_valid.append", "pos.append", "neg.append", "len", "pos.append", "temp.append", "len", "range", "pos.append", "random.randint", "len", "len", "temp.extend", "random.sample", "len", "pos.append", "range", "range", "range", "int", "random.sample", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ",", "max_num_anch_per_sent", "=", "100", ",", "num_pos_per_anch", "=", "20", ",", "num_neg_per_anch", "=", "100", ")", ":", "\n", "        ", "sent", "=", "self", ".", "x", "[", "index", "]", "\n", "label", "=", "self", ".", "y", "[", "index", "]", ".", "argmax", "(", ")", "\n", "\n", "#for x in sent:", "\n", "#    self.syn_data[x] = [syn_word for syn_word in self.syn_data[x] if syn_word!=x]", "\n", "#try:", "\n", "sent_for_anch", "=", "[", "x", "for", "x", "in", "sent", "if", "x", ">", "0", "and", "x", "<", "len", "(", "self", ".", "syn_data", ")", "and", "len", "(", "self", ".", "syn_data", "[", "x", "]", ")", "!=", "0", "]", "\n", "#except:", "\n", "#    print(index)", "\n", "#while(len(sent_for_anch) < max_num_anch_per_sent):", "\n", "#    sent_for_anch.extend(sent_for_anch)", "\n", "\n", "if", "len", "(", "sent_for_anch", ")", ">", "max_num_anch_per_sent", ":", "\n", "            ", "anch", "=", "random", ".", "sample", "(", "sent_for_anch", ",", "max_num_anch_per_sent", ")", "\n", "", "else", ":", "\n", "            ", "anch", "=", "sent_for_anch", "\n", "\n", "", "anch_valid", "=", "[", "1", "for", "x", "in", "anch", "]", "\n", "\n", "pos", "=", "[", "]", "\n", "neg", "=", "[", "]", "\n", "for", "word", "in", "anch", ":", "\n", "            ", "syn_set", "=", "set", "(", "self", ".", "syn_data", "[", "word", "]", ")", "\n", "if", "len", "(", "self", ".", "syn_data", "[", "word", "]", ")", "==", "0", ":", "\n", "                ", "pos", ".", "append", "(", "[", "word", "for", "i", "in", "range", "(", "num_pos_per_anch", ")", "]", ")", "\n", "", "elif", "len", "(", "self", ".", "syn_data", "[", "word", "]", ")", "<", "num_pos_per_anch", ":", "\n", "                    ", "temp", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "int", "(", "num_pos_per_anch", "/", "len", "(", "self", ".", "syn_data", "[", "word", "]", ")", ")", "+", "1", ")", ":", "\n", "                        ", "temp", ".", "extend", "(", "self", ".", "syn_data", "[", "word", "]", ")", "\n", "#pos.append(temp[:num_pos_per_anch])", "\n", "", "pos", ".", "append", "(", "random", ".", "sample", "(", "temp", ",", "num_pos_per_anch", ")", ")", "\n", "", "elif", "len", "(", "self", ".", "syn_data", "[", "word", "]", ")", ">=", "num_pos_per_anch", ":", "\n", "                ", "pos", ".", "append", "(", "random", ".", "sample", "(", "self", ".", "syn_data", "[", "word", "]", ",", "num_pos_per_anch", ")", ")", "\n", "\n", "", "count", "=", "0", "\n", "temp", "=", "[", "]", "\n", "while", "(", "count", "<", "num_neg_per_anch", ")", ":", "\n", "                ", "while", "(", "1", ")", ":", "\n", "                    ", "neg_word", "=", "random", ".", "randint", "(", "0", ",", "self", ".", "len_voc", ")", "\n", "if", "neg_word", "not", "in", "syn_set", ":", "\n", "                        ", "break", "\n", "", "", "temp", ".", "append", "(", "neg_word", ")", "\n", "count", "+=", "1", "\n", "", "neg", ".", "append", "(", "temp", ")", "\n", "\n", "", "while", "(", "len", "(", "anch", ")", "<", "max_num_anch_per_sent", ")", ":", "\n", "            ", "anch", ".", "append", "(", "0", ")", "\n", "anch_valid", ".", "append", "(", "0", ")", "\n", "pos", ".", "append", "(", "[", "0", "for", "i", "in", "range", "(", "num_pos_per_anch", ")", "]", ")", "\n", "neg", ".", "append", "(", "[", "0", "for", "i", "in", "range", "(", "num_neg_per_anch", ")", "]", ")", "\n", "\n", "", "return", "self", ".", "transform", "(", "sent", ",", "label", ",", "anch", ",", "pos", ",", "neg", ",", "anch_valid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData.__len__": [[212, 214], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData_TextLikeSyn.__init__": [[218, 220], ["dataHelper.SynthesizedData.__init__"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "x", ",", "y", ",", "syn_data", ")", ":", "\n", "        ", "super", "(", "SynthesizedData_TextLikeSyn", ",", "self", ")", ".", "__init__", "(", "opt", ",", "x", ",", "y", ",", "syn_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData_TextLikeSyn.transform": [[221, 224], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "sent", ",", "label", ",", "anch", ",", "pos", ",", "neg", ",", "anch_valid", ",", "text_like_syn", ",", "text_like_syn_valid", ")", ":", "\n", "\n", "        ", "return", "torch", ".", "tensor", "(", "sent", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "label", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "anch", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "pos", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "neg", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "anch_valid", ",", "dtype", "=", "torch", ".", "float", ")", ",", "torch", ".", "tensor", "(", "text_like_syn", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "text_like_syn_valid", ",", "dtype", "=", "torch", ".", "float", ")", ",", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData_TextLikeSyn.__getitem__": [[225, 310], ["dataHelper.SynthesizedData_TextLikeSyn.y[].argmax", "enumerate", "dataHelper.SynthesizedData_TextLikeSyn.transform", "text_like_syn_valid.append", "len", "len", "random.sample", "set", "neg.append", "len", "random.sample.append", "anch_valid.append", "pos.append", "neg.append", "len", "text_like_syn.append", "text_like_syn.append", "text_like_syn_valid[].extend", "text_like_syn[].extend", "dataHelper.SynthesizedData_TextLikeSyn.syn_data[].copy", "len", "pos.append", "random.sample.append", "dataHelper.SynthesizedData_TextLikeSyn.syn_data[].copy", "random.sample", "random.sample.append", "random.sample.append", "len", "range", "pos.append", "random.randint", "len", "len", "random.sample.append", "text_like_syn_valid[].append", "len", "len", "len", "random.sample.extend", "random.sample", "len", "pos.append", "range", "range", "range", "range", "range", "range", "range", "int", "dataHelper.SynthesizedData_TextLikeSyn.syn_data[].copy", "random.sample", "len", "dataHelper.SynthesizedData_TextLikeSyn.syn_data[].copy", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ",", "max_num_anch_per_sent", "=", "100", ",", "num_pos_per_anch", "=", "20", ",", "num_neg_per_anch", "=", "100", ",", "num_text_like_syn", "=", "20", ")", ":", "\n", "        ", "sent", "=", "self", ".", "x", "[", "index", "]", "\n", "label", "=", "self", ".", "y", "[", "index", "]", ".", "argmax", "(", ")", "\n", "\n", "text_like_syn", "=", "[", "]", "\n", "text_like_syn_valid", "=", "[", "]", "\n", "for", "x", "in", "sent", ":", "\n", "            ", "text_like_syn_valid", ".", "append", "(", "[", "]", ")", "\n", "if", "x", "<", "len", "(", "self", ".", "syn_data", ")", ":", "\n", "                ", "text_like_syn", ".", "append", "(", "self", ".", "syn_data", "[", "x", "]", ".", "copy", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "text_like_syn", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "", "for", "i", ",", "x", "in", "enumerate", "(", "sent", ")", ":", "\n", "            ", "temp", "=", "text_like_syn", "[", "i", "]", "\n", "len_temp", "=", "len", "(", "temp", ")", "\n", "if", "len_temp", "==", "0", ":", "\n", "                ", "text_like_syn_valid", "[", "i", "]", "=", "[", "1", "]", "\n", "text_like_syn_valid", "[", "i", "]", ".", "extend", "(", "[", "0", "for", "times", "in", "range", "(", "num_text_like_syn", "-", "1", ")", "]", ")", "\n", "text_like_syn", "[", "i", "]", "=", "[", "x", "]", "\n", "text_like_syn", "[", "i", "]", ".", "extend", "(", "[", "0", "for", "times", "in", "range", "(", "num_text_like_syn", "-", "1", ")", "]", ")", "\n", "", "elif", "len_temp", ">=", "num_text_like_syn", "-", "1", ":", "\n", "                ", "temp", "=", "random", ".", "sample", "(", "temp", ",", "num_text_like_syn", "-", "1", ")", "\n", "temp", ".", "append", "(", "x", ")", "\n", "text_like_syn", "[", "i", "]", "=", "temp", "\n", "text_like_syn_valid", "[", "i", "]", "=", "[", "1", "for", "times", "in", "range", "(", "num_text_like_syn", ")", "]", "\n", "assert", "(", "len", "(", "text_like_syn", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "", "else", ":", "\n", "                ", "temp", ".", "append", "(", "x", ")", "\n", "text_like_syn_valid", "[", "i", "]", "=", "[", "1", "for", "times", "in", "range", "(", "len", "(", "temp", ")", ")", "]", "\n", "while", "(", "len", "(", "temp", ")", "<", "num_text_like_syn", ")", ":", "\n", "                    ", "temp", ".", "append", "(", "0", ")", "\n", "text_like_syn_valid", "[", "i", "]", ".", "append", "(", "0", ")", "\n", "", "text_like_syn", "[", "i", "]", "=", "temp", "\n", "assert", "(", "len", "(", "text_like_syn", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "\n", "#for x in sent:", "\n", "#    self.syn_data[x] = [syn_word for syn_word in self.syn_data[x] if syn_word!=x]", "\n", "#try:", "\n", "", "", "sent_for_anch", "=", "[", "x", "for", "x", "in", "sent", "if", "x", ">", "0", "and", "x", "<", "len", "(", "self", ".", "syn_data", ")", "and", "len", "(", "self", ".", "syn_data", "[", "x", "]", ")", "!=", "0", "]", "\n", "#except:", "\n", "#    print(index)", "\n", "#while(len(sent_for_anch) < max_num_anch_per_sent):", "\n", "#    sent_for_anch.extend(sent_for_anch)", "\n", "\n", "if", "len", "(", "sent_for_anch", ")", ">", "max_num_anch_per_sent", ":", "\n", "            ", "anch", "=", "random", ".", "sample", "(", "sent_for_anch", ",", "max_num_anch_per_sent", ")", "\n", "", "else", ":", "\n", "            ", "anch", "=", "sent_for_anch", "\n", "\n", "", "anch_valid", "=", "[", "1", "for", "x", "in", "anch", "]", "\n", "\n", "pos", "=", "[", "]", "\n", "neg", "=", "[", "]", "\n", "for", "word", "in", "anch", ":", "\n", "            ", "syn_set", "=", "set", "(", "self", ".", "syn_data", "[", "word", "]", ".", "copy", "(", ")", ")", "\n", "if", "len", "(", "self", ".", "syn_data", "[", "word", "]", ")", "==", "0", ":", "\n", "                ", "pos", ".", "append", "(", "[", "word", "for", "i", "in", "range", "(", "num_pos_per_anch", ")", "]", ")", "\n", "", "elif", "len", "(", "self", ".", "syn_data", "[", "word", "]", ")", "<", "num_pos_per_anch", ":", "\n", "                    ", "temp", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "int", "(", "num_pos_per_anch", "/", "len", "(", "self", ".", "syn_data", "[", "word", "]", ")", ")", "+", "1", ")", ":", "\n", "                        ", "temp", ".", "extend", "(", "self", ".", "syn_data", "[", "word", "]", ".", "copy", "(", ")", ")", "\n", "#pos.append(temp[:num_pos_per_anch])", "\n", "", "pos", ".", "append", "(", "random", ".", "sample", "(", "temp", ",", "num_pos_per_anch", ")", ")", "\n", "", "elif", "len", "(", "self", ".", "syn_data", "[", "word", "]", ")", ">=", "num_pos_per_anch", ":", "\n", "                ", "pos", ".", "append", "(", "random", ".", "sample", "(", "self", ".", "syn_data", "[", "word", "]", ".", "copy", "(", ")", ",", "num_pos_per_anch", ")", ")", "\n", "\n", "", "count", "=", "0", "\n", "temp", "=", "[", "]", "\n", "while", "(", "count", "<", "num_neg_per_anch", ")", ":", "\n", "                ", "while", "(", "1", ")", ":", "\n", "                    ", "neg_word", "=", "random", ".", "randint", "(", "0", ",", "self", ".", "len_voc", ")", "\n", "if", "neg_word", "not", "in", "syn_set", ":", "\n", "                        ", "break", "\n", "", "", "temp", ".", "append", "(", "neg_word", ")", "\n", "count", "+=", "1", "\n", "", "neg", ".", "append", "(", "temp", ")", "\n", "\n", "", "while", "(", "len", "(", "anch", ")", "<", "max_num_anch_per_sent", ")", ":", "\n", "            ", "anch", ".", "append", "(", "0", ")", "\n", "anch_valid", ".", "append", "(", "0", ")", "\n", "pos", ".", "append", "(", "[", "0", "for", "i", "in", "range", "(", "num_pos_per_anch", ")", "]", ")", "\n", "neg", ".", "append", "(", "[", "0", "for", "i", "in", "range", "(", "num_neg_per_anch", ")", "]", ")", "\n", "\n", "", "return", "self", ".", "transform", "(", "sent", ",", "label", ",", "anch", ",", "pos", ",", "neg", ",", "anch_valid", ",", "text_like_syn", ",", "text_like_syn_valid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData_TextLikeSyn_Bert.__init__": [[314, 368], ["x.copy", "y.copy", "syn_data.copy", "range", "range", "range", "len", "dataHelper.SynthesizedData_TextLikeSyn_Bert.dataset_ids.append", "y.argmax", "len", "dataHelper.SynthesizedData_TextLikeSyn_Bert.cls_positive[].append", "range", "numpy.asarray", "numpy.asarray", "enumerate", "enumerate", "range", "range", "dataHelper.SynthesizedData_TextLikeSyn_Bert.cls_negative[].extend", "range", "range", "attack_surface.check_in", "text.split", "xx.append", "yy.append", "did.append", "dataHelper.SynthesizedData_TextLikeSyn_Bert.y[].argmax", "xx.append", "yy.append", "did.append"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.check_in"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "x", ",", "y", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "attack_surface", "=", "None", ",", "given_y", "=", "None", ")", ":", "\n", "        ", "self", ".", "x", "=", "x", ".", "copy", "(", ")", "\n", "self", ".", "y", "=", "y", ".", "copy", "(", ")", "#y is onehot", "\n", "self", ".", "syn_data", "=", "syn_data", ".", "copy", "(", ")", "\n", "self", ".", "seq_max_len", "=", "seq_max_len", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n", "self", ".", "dataset_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "x", ")", ")", ":", "\n", "            ", "self", ".", "dataset_ids", ".", "append", "(", "i", ")", "\n", "\n", "", "label", "=", "[", "y", ".", "argmax", "(", ")", "for", "y", "in", "self", ".", "y", "]", "\n", "\n", "# for sample idx", "\n", "self", ".", "cls_positive", "=", "[", "[", "]", "for", "i", "in", "range", "(", "opt", ".", "label_size", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "x", ")", ")", ":", "\n", "            ", "self", ".", "cls_positive", "[", "label", "[", "i", "]", "]", ".", "append", "(", "i", ")", "\n", "\n", "", "self", ".", "cls_negative", "=", "[", "[", "]", "for", "i", "in", "range", "(", "opt", ".", "label_size", ")", "]", "\n", "for", "i", "in", "range", "(", "opt", ".", "label_size", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "opt", ".", "label_size", ")", ":", "\n", "                ", "if", "j", "==", "i", ":", "\n", "                    ", "continue", "\n", "", "self", ".", "cls_negative", "[", "i", "]", ".", "extend", "(", "self", ".", "cls_positive", "[", "j", "]", ")", "\n", "\n", "", "", "self", ".", "cls_positive", "=", "[", "np", ".", "asarray", "(", "self", ".", "cls_positive", "[", "i", "]", ")", "for", "i", "in", "range", "(", "opt", ".", "label_size", ")", "]", "\n", "self", ".", "cls_negative", "=", "[", "np", ".", "asarray", "(", "self", ".", "cls_negative", "[", "i", "]", ")", "for", "i", "in", "range", "(", "opt", ".", "label_size", ")", "]", "\n", "#", "\n", "\n", "if", "attack_surface", "is", "not", "None", ":", "\n", "            ", "xx", "=", "[", "]", "\n", "yy", "=", "[", "]", "\n", "did", "=", "[", "]", "\n", "for", "i", ",", "text", "in", "enumerate", "(", "self", ".", "x", ")", ":", "\n", "                ", "if", "attack_surface", ".", "check_in", "(", "text", ".", "split", "(", "' '", ")", ")", ":", "\n", "                    ", "xx", ".", "append", "(", "text", ")", "\n", "yy", ".", "append", "(", "self", ".", "y", "[", "i", "]", ")", "\n", "did", ".", "append", "(", "self", ".", "dataset_ids", "[", "i", "]", ")", "\n", "", "", "self", ".", "x", "=", "xx", "\n", "self", ".", "y", "=", "yy", "\n", "self", ".", "dataset_ids", "=", "did", "\n", "\n", "", "if", "given_y", "is", "not", "None", ":", "\n", "            ", "xx", "=", "[", "]", "\n", "yy", "=", "[", "]", "\n", "did", "=", "[", "]", "\n", "for", "i", ",", "label", "in", "enumerate", "(", "self", ".", "y", ")", ":", "\n", "                ", "if", "self", ".", "y", "[", "i", "]", ".", "argmax", "(", ")", "==", "given_y", ":", "\n", "                    ", "xx", ".", "append", "(", "self", ".", "x", "[", "i", "]", ")", "\n", "yy", ".", "append", "(", "self", ".", "y", "[", "i", "]", ")", "\n", "did", ".", "append", "(", "self", ".", "dataset_ids", "[", "i", "]", ")", "\n", "", "", "self", ".", "x", "=", "xx", "\n", "self", ".", "y", "=", "yy", "\n", "self", ".", "dataset_ids", "=", "did", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData_TextLikeSyn_Bert.transform": [[375, 378], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "", "def", "transform", "(", "self", ",", "sent", ",", "label", ",", "text_like_syn", ",", "text_like_syn_valid", ",", "mask", ",", "token_type_ids", ",", "dataset_id", ",", "sample_idx", ")", ":", "\n", "\n", "        ", "return", "torch", ".", "tensor", "(", "sent", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "label", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "text_like_syn", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "text_like_syn_valid", ",", "dtype", "=", "torch", ".", "float", ")", ",", "torch", ".", "tensor", "(", "mask", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "token_type_ids", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "dataset_id", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "sample_idx", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynthesizedData_TextLikeSyn_Bert.__getitem__": [[379, 419], ["dataHelper.SynthesizedData_TextLikeSyn_Bert.tokenizer.encode_plus", "dataHelper.SynthesizedData_TextLikeSyn_Bert.x[].split", "range", "dataHelper.SynthesizedData_TextLikeSyn_Bert.y[].argmax", "enumerate", "numpy.random.choice", "numpy.hstack", "dataHelper.SynthesizedData_TextLikeSyn_Bert.transform", "text_like_syn_valid.append", "text_like_syn.append", "min", "len", "text_like_syn[].extend", "len", "text_like_syn[].append", "text_like_syn_valid[].append", "len", "len", "len", "numpy.asarray", "range", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ",", "num_text_like_syn", "=", "10", ",", "K", "=", "1000", ")", ":", "\n", "\n", "        ", "encoded", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "self", ".", "x", "[", "index", "]", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "seq_max_len", ",", "pad_to_max_length", "=", "True", ")", "\n", "\n", "sent", "=", "encoded", "[", "\"input_ids\"", "]", "\n", "mask", "=", "encoded", "[", "\"attention_mask\"", "]", "\n", "token_type_ids", "=", "encoded", "[", "\"token_type_ids\"", "]", "\n", "\n", "text_like_syn", "=", "[", "]", "\n", "text_like_syn_valid", "=", "[", "]", "\n", "for", "token", "in", "sent", ":", "\n", "            ", "text_like_syn_valid", ".", "append", "(", "[", "]", ")", "\n", "text_like_syn", ".", "append", "(", "[", "token", "]", ")", "\n", "\n", "", "splited_words", "=", "self", ".", "x", "[", "index", "]", ".", "split", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "min", "(", "self", ".", "seq_max_len", "-", "2", ",", "len", "(", "splited_words", ")", ")", ")", ":", "\n", "            ", "word", "=", "splited_words", "[", "i", "]", "\n", "if", "word", "in", "self", ".", "syn_data", ":", "\n", "                ", "text_like_syn", "[", "i", "+", "1", "]", ".", "extend", "(", "self", ".", "syn_data", "[", "word", "]", ")", "\n", "\n", "", "", "label", "=", "self", ".", "y", "[", "index", "]", ".", "argmax", "(", ")", "\n", "dataset_id", "=", "self", ".", "dataset_ids", "[", "index", "]", "\n", "\n", "for", "i", ",", "x", "in", "enumerate", "(", "sent", ")", ":", "\n", "            ", "text_like_syn_valid", "[", "i", "]", "=", "[", "1", "for", "times", "in", "range", "(", "len", "(", "text_like_syn", "[", "i", "]", ")", ")", "]", "\n", "\n", "while", "(", "len", "(", "text_like_syn", "[", "i", "]", ")", "<", "num_text_like_syn", ")", ":", "\n", "                ", "text_like_syn", "[", "i", "]", ".", "append", "(", "0", ")", "\n", "text_like_syn_valid", "[", "i", "]", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "(", "len", "(", "text_like_syn", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "assert", "(", "len", "(", "text_like_syn_valid", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "\n", "\n", "", "replace", "=", "True", "if", "K", ">", "len", "(", "self", ".", "cls_negative", "[", "label", "]", ")", "else", "False", "\n", "neg_idx", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "cls_negative", "[", "label", "]", ",", "K", ",", "replace", "=", "replace", ")", "\n", "sample_idx", "=", "np", ".", "hstack", "(", "(", "np", ".", "asarray", "(", "[", "dataset_id", "]", ")", ",", "neg_idx", ")", ")", "\n", "\n", "return", "self", ".", "transform", "(", "sent", ",", "label", ",", "text_like_syn", ",", "text_like_syn_valid", ",", "mask", ",", "token_type_ids", ",", "dataset_id", ",", "sample_idx", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert.__init__": [[422, 454], ["perm.copy", "hypo.copy", "y.copy", "syn_data.copy", "enumerate", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert.y[].argmax", "permperm.append", "hypohypo.append", "yy.append"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "perm", ",", "hypo", ",", "y", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "attack_surface", "=", "None", ",", "given_y", "=", "None", ")", ":", "\n", "        ", "self", ".", "perm", "=", "perm", ".", "copy", "(", ")", "\n", "self", ".", "hypo", "=", "hypo", ".", "copy", "(", ")", "\n", "self", ".", "y", "=", "y", ".", "copy", "(", ")", "\n", "self", ".", "syn_data", "=", "syn_data", ".", "copy", "(", ")", "\n", "self", ".", "seq_max_len", "=", "seq_max_len", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "\n", "\"\"\"\n        if attack_surface is not None:\n            xx=[]\n            yy=[]\n            for i, text in enumerate(self.x):\n                if attack_surface.check_in(text.split(' ')):\n                    xx.append(text)\n                    yy.append(y[i])\n            self.x = xx\n            self.y = yy\n        \"\"\"", "\n", "\n", "if", "given_y", "is", "not", "None", ":", "\n", "            ", "permperm", "=", "[", "]", "\n", "hypohypo", "=", "[", "]", "\n", "yy", "=", "[", "]", "\n", "for", "i", ",", "label", "in", "enumerate", "(", "self", ".", "y", ")", ":", "\n", "                ", "if", "self", ".", "y", "[", "i", "]", ".", "argmax", "(", ")", "==", "given_y", ":", "\n", "                    ", "permperm", ".", "append", "(", "self", ".", "perm", "[", "i", "]", ")", "\n", "hypohypo", ".", "append", "(", "self", ".", "hypo", "[", "i", "]", ")", "\n", "yy", ".", "append", "(", "self", ".", "y", "[", "i", "]", ")", "\n", "", "", "self", ".", "perm", "=", "permperm", "\n", "self", ".", "hypo", "=", "hypohypo", "\n", "self", ".", "y", "=", "yy", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert.transform": [[462, 465], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "", "def", "transform", "(", "self", ",", "sent", ",", "label", ",", "text_like_syn", ",", "text_like_syn_valid", ",", "mask", ",", "token_type_ids", ")", ":", "\n", "\n", "        ", "return", "torch", ".", "tensor", "(", "sent", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "label", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "text_like_syn", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "text_like_syn_valid", ",", "dtype", "=", "torch", ".", "float", ")", ",", "torch", ".", "tensor", "(", "mask", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "token_type_ids", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert.__getitem__": [[466, 500], ["dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert.tokenizer.encode_plus", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert.hypo[].split", "range", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert.y[].argmax", "enumerate", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert.transform", "text_like_syn_valid.append", "text_like_syn.append", "min", "len", "text_like_syn[].extend", "len", "text_like_syn[].append", "text_like_syn_valid[].append", "len", "len", "range", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ",", "num_text_like_syn", "=", "10", ")", ":", "\n", "\n", "        ", "encoded", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "self", ".", "hypo", "[", "index", "]", "+", "\" [SEP] \"", "+", "self", ".", "perm", "[", "index", "]", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "seq_max_len", ",", "pad_to_max_length", "=", "True", ")", "\n", "\n", "sent", "=", "encoded", "[", "\"input_ids\"", "]", "\n", "mask", "=", "encoded", "[", "\"attention_mask\"", "]", "\n", "token_type_ids", "=", "encoded", "[", "\"token_type_ids\"", "]", "\n", "\n", "text_like_syn", "=", "[", "]", "\n", "text_like_syn_valid", "=", "[", "]", "\n", "for", "token", "in", "sent", ":", "\n", "            ", "text_like_syn_valid", ".", "append", "(", "[", "]", ")", "\n", "text_like_syn", ".", "append", "(", "[", "token", "]", ")", "\n", "\n", "", "splited_words", "=", "self", ".", "hypo", "[", "index", "]", ".", "split", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "min", "(", "self", ".", "seq_max_len", "-", "3", ",", "len", "(", "splited_words", ")", ")", ")", ":", "\n", "            ", "word", "=", "splited_words", "[", "i", "]", "\n", "if", "word", "in", "self", ".", "syn_data", ":", "\n", "                ", "text_like_syn", "[", "i", "+", "1", "]", ".", "extend", "(", "self", ".", "syn_data", "[", "word", "]", ")", "\n", "\n", "", "", "label", "=", "self", ".", "y", "[", "index", "]", ".", "argmax", "(", ")", "\n", "\n", "for", "i", ",", "x", "in", "enumerate", "(", "sent", ")", ":", "\n", "            ", "text_like_syn_valid", "[", "i", "]", "=", "[", "1", "for", "times", "in", "range", "(", "len", "(", "text_like_syn", "[", "i", "]", ")", ")", "]", "\n", "\n", "while", "(", "len", "(", "text_like_syn", "[", "i", "]", ")", "<", "num_text_like_syn", ")", ":", "\n", "                ", "text_like_syn", "[", "i", "]", ".", "append", "(", "0", ")", "\n", "text_like_syn_valid", "[", "i", "]", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "(", "len", "(", "text_like_syn", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "assert", "(", "len", "(", "text_like_syn_valid", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "\n", "", "return", "self", ".", "transform", "(", "sent", ",", "label", ",", "text_like_syn", ",", "text_like_syn_valid", ",", "mask", ",", "token_type_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Snli_SynthesizedData_TextLikeSyn.__init__": [[503, 518], ["super().__init__", "x_p.copy", "x_h.copy", "y.copy", "syn_data.copy", "range", "print", "len", "max", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "x_p", ",", "x_h", ",", "y", ",", "syn_data", ")", ":", "\n", "        ", "super", "(", "Snli_SynthesizedData_TextLikeSyn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "x_p", "=", "x_p", ".", "copy", "(", ")", "\n", "self", ".", "x_h", "=", "x_h", ".", "copy", "(", ")", "\n", "self", ".", "y", "=", "y", ".", "copy", "(", ")", "\n", "self", ".", "syn_data", "=", "syn_data", ".", "copy", "(", ")", "\n", "\n", "max_syn_num", "=", "0", "\n", "\n", "for", "x", "in", "range", "(", "len", "(", "self", ".", "syn_data", ")", ")", ":", "\n", "            ", "self", ".", "syn_data", "[", "x", "]", "=", "[", "syn_word", "for", "syn_word", "in", "self", ".", "syn_data", "[", "x", "]", "if", "syn_word", "!=", "x", "]", "\n", "max_syn_num", "=", "max", "(", "max_syn_num", ",", "len", "(", "self", ".", "syn_data", "[", "x", "]", ")", "+", "1", ")", "\n", "\n", "", "print", "(", "\"max syn num:\"", ",", "max_syn_num", ")", "\n", "self", ".", "max_syn_num", "=", "None", "\n", "#self.max_syn_num = max_syn_num", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Snli_SynthesizedData_TextLikeSyn.transform": [[522, 525], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "x_p", ",", "x_h", ",", "label", ",", "x_p_text_like_syn", ",", "x_p_text_like_syn_valid", ",", "x_h_text_like_syn", ",", "x_h_text_like_syn_valid", ",", "sent_p_mask", ",", "sent_h_mask", ")", ":", "\n", "\n", "        ", "return", "torch", ".", "tensor", "(", "x_p", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "x_h", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "label", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "x_p_text_like_syn", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "x_p_text_like_syn_valid", ",", "dtype", "=", "torch", ".", "float", ")", ",", "torch", ".", "tensor", "(", "x_h_text_like_syn", ",", "dtype", "=", "torch", ".", "long", ")", ",", "torch", ".", "tensor", "(", "x_h_text_like_syn_valid", ",", "dtype", "=", "torch", ".", "float", ")", ",", "torch", ".", "tensor", "(", "sent_p_mask", ",", "dtype", "=", "torch", ".", "float", ")", ",", "torch", ".", "tensor", "(", "sent_h_mask", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Snli_SynthesizedData_TextLikeSyn.__getitem__": [[526, 618], ["dataHelper.Snli_SynthesizedData_TextLikeSyn.y[].argmax", "enumerate", "enumerate", "dataHelper.Snli_SynthesizedData_TextLikeSyn.transform", "x_p_text_like_syn_valid.append", "x_h_text_like_syn_valid.append", "len", "len", "sent_p_mask.append", "sent_p_mask.append", "len", "x_p_text_like_syn.append", "x_p_text_like_syn.append", "sent_h_mask.append", "sent_h_mask.append", "len", "x_h_text_like_syn.append", "x_h_text_like_syn.append", "x_p_text_like_syn_valid[].extend", "x_p_text_like_syn[].extend", "x_h_text_like_syn_valid[].extend", "x_h_text_like_syn[].extend", "dataHelper.Snli_SynthesizedData_TextLikeSyn.syn_data[].copy", "dataHelper.Snli_SynthesizedData_TextLikeSyn.syn_data[].copy", "random.sample", "random.sample.append", "random.sample.append", "random.sample", "random.sample.append", "random.sample.append", "len", "len", "random.sample.append", "x_p_text_like_syn_valid[].append", "len", "len", "len", "random.sample.append", "x_h_text_like_syn_valid[].append", "len", "range", "range", "range", "range", "range", "range", "range", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ",", "num_text_like_syn", "=", "10", ")", ":", "\n", "\n", "        ", "if", "self", ".", "max_syn_num", "is", "not", "None", ":", "\n", "            ", "num_text_like_syn", "=", "self", ".", "max_syn_num", "\n", "\n", "", "sent_p", "=", "self", ".", "x_p", "[", "index", "]", "\n", "sent_h", "=", "self", ".", "x_h", "[", "index", "]", "\n", "label", "=", "self", ".", "y", "[", "index", "]", ".", "argmax", "(", ")", "\n", "\n", "sent_p_mask", "=", "[", "]", "\n", "sent_h_mask", "=", "[", "]", "\n", "\n", "x_p_text_like_syn", "=", "[", "]", "\n", "x_h_text_like_syn", "=", "[", "]", "\n", "x_p_text_like_syn_valid", "=", "[", "]", "\n", "x_h_text_like_syn_valid", "=", "[", "]", "\n", "\n", "for", "x", "in", "sent_p", ":", "\n", "            ", "if", "x", "==", "0", ":", "\n", "                ", "sent_p_mask", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "sent_p_mask", ".", "append", "(", "1", ")", "\n", "\n", "", "x_p_text_like_syn_valid", ".", "append", "(", "[", "]", ")", "\n", "if", "x", "<", "len", "(", "self", ".", "syn_data", ")", ":", "\n", "                ", "x_p_text_like_syn", ".", "append", "(", "self", ".", "syn_data", "[", "x", "]", ".", "copy", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "x_p_text_like_syn", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "", "for", "x", "in", "sent_h", ":", "\n", "            ", "if", "x", "==", "0", ":", "\n", "                ", "sent_h_mask", ".", "append", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "sent_h_mask", ".", "append", "(", "1", ")", "\n", "\n", "", "x_h_text_like_syn_valid", ".", "append", "(", "[", "]", ")", "\n", "if", "x", "<", "len", "(", "self", ".", "syn_data", ")", ":", "\n", "                ", "x_h_text_like_syn", ".", "append", "(", "self", ".", "syn_data", "[", "x", "]", ".", "copy", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "x_h_text_like_syn", ".", "append", "(", "[", "]", ")", "\n", "\n", "", "", "for", "i", ",", "x", "in", "enumerate", "(", "sent_p", ")", ":", "\n", "            ", "temp", "=", "x_p_text_like_syn", "[", "i", "]", "\n", "len_temp", "=", "len", "(", "temp", ")", "\n", "if", "len_temp", "==", "0", ":", "\n", "                ", "x_p_text_like_syn_valid", "[", "i", "]", "=", "[", "1", "]", "\n", "x_p_text_like_syn_valid", "[", "i", "]", ".", "extend", "(", "[", "0", "for", "times", "in", "range", "(", "num_text_like_syn", "-", "1", ")", "]", ")", "\n", "x_p_text_like_syn", "[", "i", "]", "=", "[", "x", "]", "\n", "x_p_text_like_syn", "[", "i", "]", ".", "extend", "(", "[", "0", "for", "times", "in", "range", "(", "num_text_like_syn", "-", "1", ")", "]", ")", "\n", "", "elif", "len_temp", ">=", "num_text_like_syn", "-", "1", ":", "\n", "                ", "temp", "=", "random", ".", "sample", "(", "temp", ",", "num_text_like_syn", "-", "1", ")", "\n", "temp", ".", "append", "(", "x", ")", "\n", "x_p_text_like_syn", "[", "i", "]", "=", "temp", "\n", "x_p_text_like_syn_valid", "[", "i", "]", "=", "[", "1", "for", "times", "in", "range", "(", "num_text_like_syn", ")", "]", "\n", "assert", "(", "len", "(", "x_p_text_like_syn", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "", "else", ":", "\n", "                ", "temp", ".", "append", "(", "x", ")", "\n", "x_p_text_like_syn_valid", "[", "i", "]", "=", "[", "1", "for", "times", "in", "range", "(", "len", "(", "temp", ")", ")", "]", "\n", "while", "(", "len", "(", "temp", ")", "<", "num_text_like_syn", ")", ":", "\n", "                    ", "temp", ".", "append", "(", "0", ")", "\n", "x_p_text_like_syn_valid", "[", "i", "]", ".", "append", "(", "0", ")", "\n", "", "x_p_text_like_syn", "[", "i", "]", "=", "temp", "\n", "assert", "(", "len", "(", "x_p_text_like_syn", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "\n", "#if x_p_text_like_syn_valid[i] == [1 for times in range(num_text_like_syn)]:", "\n", "#    print(x_p_text_like_syn[i])", "\n", "#    print(x_p_text_like_syn_valid[i])", "\n", "\n", "", "", "for", "i", ",", "x", "in", "enumerate", "(", "sent_h", ")", ":", "\n", "            ", "temp", "=", "x_h_text_like_syn", "[", "i", "]", "\n", "len_temp", "=", "len", "(", "temp", ")", "\n", "if", "len_temp", "==", "0", ":", "\n", "                ", "x_h_text_like_syn_valid", "[", "i", "]", "=", "[", "1", "]", "\n", "x_h_text_like_syn_valid", "[", "i", "]", ".", "extend", "(", "[", "0", "for", "times", "in", "range", "(", "num_text_like_syn", "-", "1", ")", "]", ")", "\n", "x_h_text_like_syn", "[", "i", "]", "=", "[", "x", "]", "\n", "x_h_text_like_syn", "[", "i", "]", ".", "extend", "(", "[", "0", "for", "times", "in", "range", "(", "num_text_like_syn", "-", "1", ")", "]", ")", "\n", "", "elif", "len_temp", ">=", "num_text_like_syn", "-", "1", ":", "\n", "                ", "temp", "=", "random", ".", "sample", "(", "temp", ",", "num_text_like_syn", "-", "1", ")", "\n", "temp", ".", "append", "(", "x", ")", "\n", "x_h_text_like_syn", "[", "i", "]", "=", "temp", "\n", "x_h_text_like_syn_valid", "[", "i", "]", "=", "[", "1", "for", "times", "in", "range", "(", "num_text_like_syn", ")", "]", "\n", "assert", "(", "len", "(", "x_h_text_like_syn", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "", "else", ":", "\n", "                ", "temp", ".", "append", "(", "x", ")", "\n", "x_h_text_like_syn_valid", "[", "i", "]", "=", "[", "1", "for", "times", "in", "range", "(", "len", "(", "temp", ")", ")", "]", "\n", "while", "(", "len", "(", "temp", ")", "<", "num_text_like_syn", ")", ":", "\n", "                    ", "temp", ".", "append", "(", "0", ")", "\n", "x_h_text_like_syn_valid", "[", "i", "]", ".", "append", "(", "0", ")", "\n", "", "x_h_text_like_syn", "[", "i", "]", "=", "temp", "\n", "assert", "(", "len", "(", "x_h_text_like_syn", "[", "i", "]", ")", "==", "num_text_like_syn", ")", "\n", "\n", "", "", "return", "self", ".", "transform", "(", "sent_p", ",", "sent_h", ",", "label", ",", "x_p_text_like_syn", ",", "x_p_text_like_syn_valid", ",", "x_h_text_like_syn", ",", "x_h_text_like_syn_valid", ",", "sent_p_mask", ",", "sent_h_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Snli_SynthesizedData_TextLikeSyn.__len__": [[619, 621], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "x_p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynonymData.__init__": [[624, 636], ["super().__init__", "list", "random.shuffle", "len", "roots.copy", "random.randint", "range", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "roots", ",", "labels", ",", "opt", "=", "None", ")", ":", "\n", "        ", "super", "(", "SynonymData", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "roots", "=", "roots", "\n", "self", ".", "negative_src", "=", "list", "(", "roots", ".", "copy", "(", ")", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "negative_src", ")", "\n", "\n", "self", ".", "labels", "=", "labels", "\n", "\n", "self", ".", "len_roots", "=", "len", "(", "self", ".", "roots", ")", "\n", "\n", "self", ".", "negative_len", "=", "100", "\n", "self", ".", "negative_sample_idx_list", "=", "[", "random", ".", "randint", "(", "0", ",", "self", ".", "len_roots", "-", "1", ")", "for", "x", "in", "range", "(", "len", "(", "roots", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynonymData.transform": [[638, 664], ["numpy.array", "numpy.zeros", "set", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "list", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "root", ",", "label", ",", "negative_sample_idx_list_start", ")", ":", "\n", "\n", "        ", "root", "=", "np", ".", "array", "(", "[", "root", "]", ")", "\n", "negative", "=", "np", ".", "zeros", "(", "self", ".", "negative_len", ")", "\n", "\n", "syn_set", "=", "set", "(", "list", "(", "label", ")", ")", "\n", "start", "=", "self", ".", "negative_sample_idx_list", "[", "negative_sample_idx_list_start", "]", "\n", "\n", "count", "=", "0", "\n", "while", "(", "count", "<", "self", ".", "negative_len", ")", ":", "\n", "            ", "if", "self", ".", "negative_src", "[", "start", "]", "not", "in", "syn_set", ":", "\n", "                ", "negative", "[", "count", "]", "=", "self", ".", "negative_src", "[", "start", "]", "\n", "count", "+=", "1", "\n", "", "start", "=", "(", "start", "+", "1", ")", "%", "self", ".", "len_roots", "\n", "", "self", ".", "negative_sample_idx_list", "[", "negative_sample_idx_list_start", "]", "=", "start", "\n", "\n", "#out_root = self.totensor(root).to(torch.long)", "\n", "#out_label = self.totensor(label).to(torch.long)", "\n", "#out_negative = self.totensor(negative).to(torch.long)", "\n", "\n", "out_root", "=", "torch", ".", "from_numpy", "(", "root", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "out_label", "=", "torch", ".", "from_numpy", "(", "label", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "out_negative", "=", "torch", ".", "from_numpy", "(", "negative", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "\n", "\n", "return", "out_root", ",", "out_label", ",", "out_negative", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynonymData.__getitem__": [[665, 667], ["dataHelper.SynonymData.transform"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "return", "self", ".", "transform", "(", "self", ".", "roots", "[", "index", "]", ",", "self", ".", "labels", "[", "index", "]", ",", "index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.SynonymData.__len__": [[668, 670], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "len_roots", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.__init__": [[673, 686], ["list", "random.shuffle", "len", "roots.copy", "random.randint", "range", "len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "roots", ",", "labels", ",", "opt", "=", "None", ",", "shuffle", "=", "True", ")", ":", "\n", "        ", "self", ".", "shuffle", "=", "shuffle", "\n", "self", ".", "roots", "=", "roots", "\n", "self", ".", "negative_src", "=", "list", "(", "roots", ".", "copy", "(", ")", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "negative_src", ")", "\n", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "\n", "self", ".", "len_roots", "=", "len", "(", "self", ".", "roots", ")", "\n", "\n", "self", ".", "negative_len", "=", "100", "\n", "self", ".", "negative_sample_idx_list", "=", "[", "random", ".", "randint", "(", "0", ",", "self", ".", "len_roots", "-", "1", ")", "for", "x", "in", "range", "(", "len", "(", "roots", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform": [[688, 714], ["numpy.zeros", "range", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "dataHelper.DottableDict", "set", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor().cuda", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "list", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "methods", ["None"], ["", "def", "transform", "(", "self", ",", "roots", ",", "labels", ",", "negative_sample_idx_list_start", ")", ":", "\n", "\n", "        ", "batch_negatives", "=", "np", ".", "zeros", "(", "(", "self", ".", "batch_size", ",", "self", ".", "negative_len", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "batch_size", ")", ":", "\n", "            ", "syn_set", "=", "set", "(", "list", "(", "labels", "[", "i", "]", ")", ")", "\n", "start", "=", "self", ".", "negative_sample_idx_list", "[", "negative_sample_idx_list_start", "+", "i", "]", "\n", "\n", "count", "=", "0", "\n", "while", "(", "count", "<", "self", ".", "negative_len", ")", ":", "\n", "                ", "if", "self", ".", "negative_src", "[", "start", "]", "not", "in", "syn_set", ":", "\n", "                    ", "batch_negatives", "[", "i", ",", "count", "]", "=", "self", ".", "negative_src", "[", "start", "]", "\n", "count", "+=", "1", "\n", "", "start", "=", "(", "start", "+", "1", ")", "%", "self", ".", "len_roots", "\n", "", "self", ".", "negative_sample_idx_list", "[", "negative_sample_idx_list_start", "+", "i", "]", "=", "start", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "batch_roots", "=", "torch", ".", "LongTensor", "(", "roots", ")", ".", "cuda", "(", ")", "\n", "batch_labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", ".", "cuda", "(", ")", "\n", "batch_negatives", "=", "torch", ".", "LongTensor", "(", "batch_negatives", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "            ", "batch_roots", "=", "torch", ".", "LongTensor", "(", "roots", ")", "\n", "batch_labels", "=", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "batch_negatives", "=", "torch", ".", "LongTensor", "(", "batch_negatives", ")", "\n", "\n", "", "return", "DottableDict", "(", "{", "\"roots\"", ":", "batch_roots", ",", "\"labels\"", ":", "batch_labels", ",", "\"negatives\"", ":", "batch_negatives", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.__iter__": [[715, 738], ["int", "range", "random.randint", "list", "random.seed", "random.shuffle", "numpy.array", "list", "random.seed", "random.shuffle", "numpy.array", "random.seed", "random.shuffle", "dataHelper.BucketIterator_synonyms.transform", "len", "dataHelper.BucketIterator_synonyms.transform"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.BucketIterator_synonyms.transform"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "shuffle", ":", "\n", "            ", "seed", "=", "random", ".", "randint", "(", "0", ",", "100", ")", "\n", "self", ".", "roots", "=", "list", "(", "self", ".", "roots", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "roots", ")", "\n", "self", ".", "roots", "=", "np", ".", "array", "(", "self", ".", "roots", ")", "\n", "\n", "self", ".", "labels", "=", "list", "(", "self", ".", "labels", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "labels", ")", "\n", "self", ".", "labels", "=", "np", ".", "array", "(", "self", ".", "labels", ")", "\n", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "shuffle", "(", "self", ".", "negative_sample_idx_list", ")", "\n", "\n", "\n", "", "batch_nums", "=", "int", "(", "len", "(", "self", ".", "roots", ")", "/", "self", ".", "batch_size", ")", "\n", "for", "i", "in", "range", "(", "batch_nums", ")", ":", "\n", "            ", "start", "=", "i", "*", "self", ".", "batch_size", "\n", "end", "=", "(", "i", "+", "1", ")", "*", "self", ".", "batch_size", "\n", "yield", "self", ".", "transform", "(", "self", ".", "roots", "[", "start", ":", "end", "]", ",", "self", ".", "labels", "[", "start", ":", "end", "]", ",", "start", ")", "\n", "", "yield", "self", ".", "transform", "(", "self", ".", "roots", "[", "-", "1", "*", "self", ".", "batch_size", ":", "]", ",", "self", ".", "labels", "[", "-", "1", "*", "self", ".", "batch_size", ":", "]", ",", "self", ".", "len_roots", "-", "self", ".", "batch_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.vectors_lookup": [[741, 752], ["numpy.zeros", "print", "len", "numpy.random.uniform"], "function", ["None"], ["", "", "def", "vectors_lookup", "(", "vectors", ",", "vocab", ",", "dim", ")", ":", "\n", "    ", "embedding", "=", "np", ".", "zeros", "(", "(", "len", "(", "vocab", ")", ",", "dim", ")", ")", "\n", "count", "=", "1", "\n", "for", "word", "in", "vocab", ":", "\n", "        ", "if", "word", "in", "vectors", ":", "\n", "            ", "count", "+=", "1", "\n", "embedding", "[", "vocab", "[", "word", "]", "]", "=", "vectors", "[", "word", "]", "\n", "", "else", ":", "\n", "            ", "embedding", "[", "vocab", "[", "word", "]", "]", "=", "np", ".", "random", ".", "uniform", "(", "-", "0.5", ",", "+", "0.5", ",", "dim", ")", "#vectors['[UNKNOW]'] #.tolist()", "\n", "", "", "print", "(", "'word in embedding'", ",", "count", ")", "\n", "return", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.load_text_vec": [[754, 773], ["print", "print", "codecs.open", "tqdm.tqdm", "len", "len", "len", "line.strip().split", "vectors.keys", "len", "print", "print", "line.strip", "list", "vectors.keys"], "function", ["None"], ["", "def", "load_text_vec", "(", "alphabet", ",", "filename", "=", "\"\"", ",", "embedding_size", "=", "-", "1", ")", ":", "\n", "    ", "vectors", "=", "{", "}", "\n", "with", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "f", ")", ":", "\n", "            ", "items", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "' '", ")", "\n", "if", "len", "(", "items", ")", "==", "2", ":", "\n", "                ", "vocab_size", ",", "embedding_size", "=", "items", "[", "0", "]", ",", "items", "[", "1", "]", "\n", "print", "(", "'embedding_size'", ",", "embedding_size", ")", "\n", "print", "(", "'vocab_size in pretrained embedding'", ",", "vocab_size", ")", "\n", "", "else", ":", "\n", "                ", "word", "=", "items", "[", "0", "]", "\n", "if", "word", "in", "alphabet", ":", "\n", "                    ", "vectors", "[", "word", "]", "=", "items", "[", "1", ":", "]", "\n", "", "", "", "", "print", "(", "'words need to be found '", ",", "len", "(", "alphabet", ")", ")", "\n", "print", "(", "'words found in wor2vec embedding '", ",", "len", "(", "vectors", ".", "keys", "(", ")", ")", ")", "\n", "\n", "if", "embedding_size", "==", "-", "1", ":", "\n", "        ", "embedding_size", "=", "len", "(", "vectors", "[", "list", "(", "vectors", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ")", "\n", "", "return", "vectors", ",", "embedding_size", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.getDataSet": [[775, 781], ["dataloader.getDataset", "dataloader.getDataset.getFormatedData"], "function", ["None"], ["", "def", "getDataSet", "(", "opt", ")", ":", "\n", "    ", "import", "dataloader", "\n", "dataset", "=", "dataloader", ".", "getDataset", "(", "opt", ")", "\n", "#    files=[os.path.join(data_dir,data_name)   for data_name in ['train.txt','test.txt','dev.txt']]", "\n", "\n", "return", "dataset", ".", "getFormatedData", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.clean": [[796, 805], ["re.sub", "re.sub.lower().split", "re.sub", "re.sub.lower"], "function", ["None"], ["def", "clean", "(", "text", ")", ":", "\n", "#    text=\"'tycoon.<br'\"", "\n", "    ", "for", "token", "in", "[", "\"<br/>\"", ",", "\"<br>\"", ",", "\"<br\"", "]", ":", "\n", "         ", "text", "=", "re", ".", "sub", "(", "token", ",", "\" \"", ",", "text", ")", "\n", "", "text", "=", "re", ".", "sub", "(", "\"[\\s+\\.\\!\\/_,$%^*()\\(\\)<>+\\\"\\[\\]\\-\\?;:\\'{}`]+|[+\u2014\u2014\uff01\uff0c\u3002\uff1f\u3001~@#\uffe5%\u2026\u2026&*\uff08\uff09]+\"", ",", "\" \"", ",", "text", ")", "\n", "\n", "#    print(\"%s $$$$$ %s\" %(pre,text))     ", "\n", "\n", "return", "text", ".", "lower", "(", ")", ".", "split", "(", ")", "\n", "#@log_time_delta", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.get_clean_datas": [[806, 824], ["dataHelper.getDataSet", "print", "pickle.load", "os.path.exists", "pandas.read_csv().fillna", "df[].apply", "datas.append", "pickle.dump", "codecs.open", "os.path.exists", "os.mkdir", "codecs.open", "pandas.read_csv"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.getDataSet", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.apply", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump"], ["", "def", "get_clean_datas", "(", "opt", ")", ":", "\n", "    ", "pickle_filename", "=", "\"temp/\"", "+", "opt", ".", "dataset", "+", "\".data\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "pickle_filename", ")", "or", "opt", ".", "debug", ":", "\n", "        ", "datas", "=", "[", "]", "\n", "for", "filename", "in", "getDataSet", "(", "opt", ")", ":", "\n", "            ", "df", "=", "pd", ".", "read_csv", "(", "filename", ",", "header", "=", "None", ",", "sep", "=", "\"\\t\"", ",", "names", "=", "[", "\"text\"", ",", "\"label\"", "]", ")", ".", "fillna", "(", "'0'", ")", "\n", "\n", "#        df[\"text\"]= df[\"text\"].apply(clean).str.lower().str.split() #replace(\"[\\\",:#]\",\" \")", "\n", "df", "[", "\"text\"", "]", "=", "df", "[", "\"text\"", "]", ".", "apply", "(", "clean", ")", "\n", "datas", ".", "append", "(", "df", ")", "\n", "", "if", "opt", ".", "debug", ":", "\n", "            ", "if", "not", "os", ".", "path", ".", "exists", "(", "\"temp\"", ")", ":", "\n", "                ", "os", ".", "mkdir", "(", "\"temp\"", ")", "\n", "", "pickle", ".", "dump", "(", "datas", ",", "open", "(", "pickle_filename", ",", "\"wb\"", ")", ")", "\n", "", "return", "datas", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"load cache for data\"", ")", "\n", "return", "pickle", ".", "load", "(", "open", "(", "pickle_filename", ",", "\"rb\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.load_vocab_from_bert": [[825, 838], ["os.path.join", "dataHelper.Alphabet", "BertTokenizer.from_pretrained", "BertTokenizer.from_pretrained.ids_to_tokens.items", "dataHelper.Alphabet.add"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["", "", "def", "load_vocab_from_bert", "(", "bert_base", ")", ":", "\n", "\n", "\n", "    ", "bert_vocab_dir", "=", "os", ".", "path", ".", "join", "(", "bert_base", ",", "\"vocab.txt\"", ")", "\n", "alphabet", "=", "Alphabet", "(", "start_feature_id", "=", "0", ",", "alphabet_type", "=", "\"bert\"", ")", "\n", "\n", "from", "pytorch_pretrained_bert", "import", "BertTokenizer", "\n", "\n", "# Load pre-trained model tokenizer (vocabulary)", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "bert_vocab_dir", ")", "\n", "for", "index", ",", "word", "in", "tokenizer", ".", "ids_to_tokens", ".", "items", "(", ")", ":", "\n", "        ", "alphabet", ".", "add", "(", "word", ")", "\n", "", "return", "alphabet", ",", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.load_vocab_for_bert_selfmade": [[840, 851], ["dataHelper.Alphabet", "tokenizer_class.from_pretrained", "tokenizer_class.from_pretrained.ids_to_tokens.items", "dataHelper.Alphabet.add"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["", "def", "load_vocab_for_bert_selfmade", "(", ")", ":", "\n", "\n", "    ", "alphabet", "=", "Alphabet", "(", "start_feature_id", "=", "0", ",", "alphabet_type", "=", "\"bert\"", ")", "\n", "\n", "from", "transformers", "import", "BertTokenizer", "\n", "tokenizer_class", "=", "BertTokenizer", "\n", "tokenizer", "=", "tokenizer_class", ".", "from_pretrained", "(", "'bert-base-uncased'", ")", "\n", "\n", "for", "index", ",", "word", "in", "tokenizer", ".", "ids_to_tokens", ".", "items", "(", ")", ":", "\n", "        ", "alphabet", ".", "add", "(", "word", ")", "\n", "", "return", "alphabet", ",", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.process_with_bert": [[852, 856], ["tokenizer.convert_tokens_to_ids", "tokenizer.tokenize", "int", "len"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.tokenize"], ["", "def", "process_with_bert", "(", "text", ",", "tokenizer", ",", "max_seq_len", ")", ":", "\n", "    ", "tokens", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "tokenize", "(", "\" \"", ".", "join", "(", "text", "[", ":", "max_seq_len", "]", ")", ")", ")", "\n", "\n", "return", "tokens", "[", ":", "max_seq_len", "]", "+", "[", "0", "]", "*", "int", "(", "max_seq_len", "-", "len", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.process_with_bert_selfmade": [[857, 861], ["tokenizer.encode", "int", "min", "len", "len"], "function", ["None"], ["", "def", "process_with_bert_selfmade", "(", "text", ",", "tokenizer", ",", "max_seq_len", ")", ":", "\n", "    ", "tokens", "=", "tokenizer", ".", "encode", "(", "text", "[", ":", "max_seq_len", "]", ")", "\n", "\n", "return", "tokens", "[", ":", "min", "(", "len", "(", "tokens", ")", ",", "max_seq_len", ")", "]", "+", "[", "0", "]", "*", "int", "(", "max_seq_len", "-", "len", "(", "tokens", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.snli_make_synthesized_iter": [[862, 1003], ["PWWS.read_files.split_snli_files", "os.path.join", "torch.zeros().to", "torch.zeros().to", "dataHelper.Snli_SynthesizedData_TextLikeSyn", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.Snli_SynthesizedData_TextLikeSyn", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.Snli_SynthesizedData_TextLikeSyn", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "codecs.open", "pickle.load", "codecs.open.close", "PWWS.word_level_process.get_tokenizer", "codecs.open", "pickle.load", "codecs.open.close", "PWWS.word_level_process.update_tokenizer", "print", "PWWS.word_level_process.get_tokenizer", "print", "print", "print", "PWWS.word_level_process.get_tokenizer.fit_on_texts", "print", "PWWS.word_level_process.update_tokenizer", "codecs.open", "pickle.dump", "codecs.open.close", "print", "print", "PWWS.neural_networks.get_embedding_index", "PWWS.neural_networks.get_embedding_matrix", "torch.FloatTensor", "torch.FloatTensor", "print", "PWWS.word_level_process.text_process_for_single", "PWWS.word_level_process.text_process_for_single", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.text_process_for_single", "PWWS.word_level_process.text_process_for_single", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.text_process_for_single", "PWWS.word_level_process.text_process_for_single", "PWWS.word_level_process.label_process_for_single", "codecs.open", "pickle.dump", "codecs.open.close", "len", "codecs.open", "json.load", "syn_texts_for_tokenizer.append", "len", "len", "PWWS.word_level_process.get_tokenizer.texts_to_sequences", "torch.zeros", "torch.zeros", "range", "print", "PWWS.paraphrase.generate_synonym_list_by_dict", "len"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_snli_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.update_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.fit_on_texts", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.update_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_index", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_matrix", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.generate_synonym_list_by_dict"], ["", "def", "snli_make_synthesized_iter", "(", "opt", ")", ":", "\n", "    ", "dataset", "=", "opt", ".", "dataset", "\n", "opt", ".", "label_size", "=", "3", "\n", "train_perms", ",", "train_hypos", ",", "train_labels", ",", "dev_perms", ",", "dev_hypos", ",", "dev_labels", ",", "test_perms", ",", "test_hypos", ",", "test_labels", "=", "split_snli_files", "(", "opt", ")", "\n", "tokenizer_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "work_path", ",", "\"temp/snli_tokenizer_with_ceritified_syndata.pickle\"", ")", "\n", "\n", "if", "opt", ".", "synonyms_from_file", ":", "\n", "        ", "filename", "=", "opt", ".", "snli_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "syn_data", "=", "saved", "[", "\"syn_data\"", "]", "\n", "opt", ".", "embeddings", "=", "saved", "[", "'embeddings'", "]", "\n", "opt", ".", "vocab_size", "=", "saved", "[", "'vocab_size'", "]", "\n", "x_p_train", "=", "saved", "[", "'x_p_train'", "]", "\n", "x_h_train", "=", "saved", "[", "'x_h_train'", "]", "\n", "y_train", "=", "saved", "[", "'y_train'", "]", "\n", "x_p_test", "=", "saved", "[", "'x_p_test'", "]", "\n", "x_h_test", "=", "saved", "[", "'x_h_test'", "]", "\n", "y_test", "=", "saved", "[", "'y_test'", "]", "\n", "x_p_dev", "=", "saved", "[", "'x_p_dev'", "]", "\n", "x_h_dev", "=", "saved", "[", "'x_h_dev'", "]", "\n", "y_dev", "=", "saved", "[", "'y_dev'", "]", "\n", "\n", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "dictori", "=", "tokenizer", ".", "index_word", "\n", "\n", "\n", "f", "=", "open", "(", "tokenizer_file", ",", "'rb'", ")", "\n", "tokenizer", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "update_tokenizer", "(", "dataset", ",", "tokenizer", ")", "\n", "dictnew", "=", "tokenizer", ".", "index_word", "\n", "\n", "print", "(", "\"Check the tokenizer.\"", ")", "\n", "for", "key", "in", "dictori", ":", "\n", "            ", "assert", "(", "dictori", "[", "key", "]", "==", "dictnew", "[", "key", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "print", "(", "\"len of tokenizer before updata.\"", ",", "len", "(", "tokenizer", ".", "index_word", ")", ")", "\n", "print", "(", "\"Preparing synonyms.\"", ")", "\n", "\n", "syn_texts_for_tokenizer", "=", "[", "]", "\n", "syn_dict", "=", "{", "}", "\n", "\n", "import", "json", "\n", "with", "open", "(", "opt", ".", "certified_neighbors_file_path", ")", "as", "f", ":", "\n", "            ", "certified_neighbors", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "syn_data", "=", "[", "[", "]", "for", "i", "in", "range", "(", "1", "+", "len", "(", "tokenizer", ".", "index_word", ")", ")", "]", "\n", "for", "index", "in", "tokenizer", ".", "index_word", ":", "\n", "            ", "if", "index", "%", "100", "==", "0", ":", "\n", "                ", "print", "(", "index", ")", "\n", "", "word", "=", "tokenizer", ".", "index_word", "[", "index", "]", "\n", "#syn_text = \" \".join(generate_synonym_list_from_word(word))", "\n", "syn_text", "=", "\" \"", ".", "join", "(", "generate_synonym_list_by_dict", "(", "certified_neighbors", ",", "word", ")", ")", "\n", "syn_texts_for_tokenizer", ".", "append", "(", "syn_text", ")", "\n", "syn_dict", "[", "index", "]", "=", "syn_text", "\n", "\n", "# update tokenizer", "\n", "", "print", "(", "\"Fit on syn texts.\"", ")", "\n", "dictori", "=", "tokenizer", ".", "index_word", "\n", "tokenizer", ".", "fit_on_texts", "(", "syn_texts_for_tokenizer", ",", "freq_count", "=", "0", ")", "# to keep the original index order of the tokenizer", "\n", "dictnew", "=", "tokenizer", ".", "index_word", "\n", "print", "(", "\"Check the tokenizer.\"", ")", "\n", "for", "key", "in", "dictori", ":", "\n", "            ", "assert", "(", "dictori", "[", "key", "]", "==", "dictnew", "[", "key", "]", ")", "\n", "\n", "", "update_tokenizer", "(", "dataset", ",", "tokenizer", ")", "\n", "f", "=", "open", "(", "tokenizer_file", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "tokenizer", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "print", "(", "\"len of tokenizer after updata.\"", ",", "len", "(", "tokenizer", ".", "index_word", ")", ")", "\n", "assert", "(", "len", "(", "tokenizer", ".", "index_word", ")", "<", "config", ".", "num_words", "[", "dataset", "]", ")", "\n", "\n", "# Tokenize syn data", "\n", "print", "(", "\"Tokenize syn data.\"", ")", "\n", "for", "key", "in", "syn_dict", ":", "\n", "            ", "temp", "=", "tokenizer", ".", "texts_to_sequences", "(", "[", "syn_dict", "[", "key", "]", "]", ")", "\n", "syn_data", "[", "key", "]", "=", "temp", "[", "0", "]", "\n", "\n", "# make embd according to the updated tokenizer", "\n", "", "embedding_dim", "=", "opt", ".", "embedding_dim", "\n", "pretrained", "=", "opt", ".", "embedding_file_path", "\n", "get_embedding_index", "(", "pretrained", ",", "embedding_dim", ")", "\n", "num_words", "=", "config", ".", "num_words", "[", "dataset", "]", "\n", "opt", ".", "vocab_size", "=", "num_words", "\n", "embedding_matrix", "=", "get_embedding_matrix", "(", "opt", ",", "dataset", ",", "num_words", ",", "embedding_dim", ")", "\n", "opt", ".", "embeddings", "=", "torch", ".", "FloatTensor", "(", "embedding_matrix", ")", "\n", "\n", "# Tokenize the training data", "\n", "print", "(", "\"Tokenize training data.\"", ")", "\n", "x_p_train", "=", "text_process_for_single", "(", "tokenizer", ",", "train_perms", ",", "opt", ".", "dataset", ")", "\n", "x_h_train", "=", "text_process_for_single", "(", "tokenizer", ",", "train_hypos", ",", "opt", ".", "dataset", ")", "\n", "y_train", "=", "label_process_for_single", "(", "tokenizer", ",", "train_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "x_p_dev", "=", "text_process_for_single", "(", "tokenizer", ",", "dev_perms", ",", "opt", ".", "dataset", ")", "\n", "x_h_dev", "=", "text_process_for_single", "(", "tokenizer", ",", "dev_hypos", ",", "opt", ".", "dataset", ")", "\n", "y_dev", "=", "label_process_for_single", "(", "tokenizer", ",", "dev_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "x_p_test", "=", "text_process_for_single", "(", "tokenizer", ",", "test_perms", ",", "opt", ".", "dataset", ")", "\n", "x_h_test", "=", "text_process_for_single", "(", "tokenizer", ",", "test_hypos", ",", "opt", ".", "dataset", ")", "\n", "y_test", "=", "label_process_for_single", "(", "tokenizer", ",", "test_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "filename", "=", "opt", ".", "snli_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'wb'", ")", "\n", "saved", "=", "{", "}", "\n", "saved", "[", "'syn_data'", "]", "=", "syn_data", "\n", "saved", "[", "'embeddings'", "]", "=", "opt", ".", "embeddings", "\n", "saved", "[", "'vocab_size'", "]", "=", "opt", ".", "vocab_size", "\n", "saved", "[", "'x_p_train'", "]", "=", "x_p_train", "\n", "saved", "[", "'x_h_train'", "]", "=", "x_h_train", "\n", "saved", "[", "'y_train'", "]", "=", "y_train", "\n", "saved", "[", "'x_p_dev'", "]", "=", "x_p_dev", "\n", "saved", "[", "'x_h_dev'", "]", "=", "x_h_dev", "\n", "saved", "[", "'y_dev'", "]", "=", "y_dev", "\n", "saved", "[", "'x_p_test'", "]", "=", "x_p_test", "\n", "saved", "[", "'x_h_test'", "]", "=", "x_h_test", "\n", "saved", "[", "'y_test'", "]", "=", "y_test", "\n", "pickle", ".", "dump", "(", "saved", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "", "vocab_freq", "=", "torch", ".", "zeros", "(", "opt", ".", "embeddings", ".", "shape", "[", "0", "]", ")", ".", "to", "(", "opt", ".", "embeddings", ".", "dtype", ")", "\n", "for", "index", "in", "tokenizer", ".", "index_word", ":", "\n", "        ", "word", "=", "tokenizer", ".", "index_word", "[", "index", "]", "\n", "freq", "=", "tokenizer", ".", "word_counts", "[", "word", "]", "\n", "vocab_freq", "[", "index", "]", "=", "freq", "\n", "", "opt", ".", "vocab_freq", "=", "vocab_freq", "\n", "\n", "train_data", "=", "Snli_SynthesizedData_TextLikeSyn", "(", "opt", ",", "x_p_train", ",", "x_h_train", ",", "y_train", ",", "syn_data", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_data", ",", "opt", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "\n", "dev_data", "=", "Snli_SynthesizedData_TextLikeSyn", "(", "opt", ",", "x_p_dev", ",", "x_h_dev", ",", "y_dev", ",", "syn_data", ")", "\n", "dev_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dev_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ")", "\n", "\n", "test_data", "=", "Snli_SynthesizedData_TextLikeSyn", "(", "opt", ",", "x_p_test", ",", "x_h_test", ",", "y_test", ",", "syn_data", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "test_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ")", "\n", "\n", "return", "train_loader", ",", "dev_loader", ",", "test_loader", ",", "syn_data", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.imdb_make_synthesized_iter": [[1005, 1131], ["PWWS.read_files.split_imdb_files", "os.path.join", "torch.zeros().to", "torch.zeros().to", "dataHelper.SynthesizedData_TextLikeSyn", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.SynthesizedData_TextLikeSyn", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.SynthesizedData_TextLikeSyn", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "codecs.open", "pickle.load", "codecs.open.close", "PWWS.word_level_process.get_tokenizer", "codecs.open", "pickle.load", "codecs.open.close", "PWWS.word_level_process.update_tokenizer", "print", "PWWS.word_level_process.get_tokenizer", "print", "print", "print", "PWWS.word_level_process.get_tokenizer.fit_on_texts", "PWWS.word_level_process.update_tokenizer", "codecs.open", "pickle.dump", "codecs.open.close", "print", "print", "PWWS.neural_networks.get_embedding_index", "PWWS.neural_networks.get_embedding_matrix", "torch.FloatTensor", "torch.FloatTensor", "print", "PWWS.word_level_process.text_process_for_single", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.text_process_for_single", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.text_process_for_single", "PWWS.word_level_process.label_process_for_single", "codecs.open", "pickle.dump", "codecs.open.close", "len", "codecs.open", "json.load", "syn_texts_for_tokenizer.append", "len", "len", "PWWS.word_level_process.get_tokenizer.texts_to_sequences", "torch.zeros", "torch.zeros", "range", "print", "PWWS.paraphrase.generate_synonym_list_by_dict", "len"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.update_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.fit_on_texts", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.update_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_index", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_matrix", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.generate_synonym_list_by_dict"], ["", "def", "imdb_make_synthesized_iter", "(", "opt", ")", ":", "\n", "    ", "dataset", "=", "opt", ".", "dataset", "\n", "opt", ".", "label_size", "=", "2", "\n", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "tokenizer_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "work_path", ",", "\"temp/imdb_tokenizer_with_ceritified_syndata.pickle\"", ")", "\n", "\n", "if", "opt", ".", "synonyms_from_file", ":", "\n", "        ", "filename", "=", "opt", ".", "imdb_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "syn_data", "=", "saved", "[", "\"syn_data\"", "]", "\n", "opt", ".", "embeddings", "=", "saved", "[", "'embeddings'", "]", "\n", "opt", ".", "vocab_size", "=", "saved", "[", "'vocab_size'", "]", "\n", "x_train", "=", "saved", "[", "'x_train'", "]", "\n", "x_test", "=", "saved", "[", "'x_test'", "]", "\n", "x_dev", "=", "saved", "[", "'x_dev'", "]", "\n", "y_train", "=", "saved", "[", "'y_train'", "]", "\n", "y_test", "=", "saved", "[", "'y_test'", "]", "\n", "y_dev", "=", "saved", "[", "'y_dev'", "]", "\n", "\n", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "dictori", "=", "tokenizer", ".", "index_word", "\n", "\n", "f", "=", "open", "(", "tokenizer_file", ",", "'rb'", ")", "\n", "tokenizer", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "update_tokenizer", "(", "dataset", ",", "tokenizer", ")", "\n", "dictnew", "=", "tokenizer", ".", "index_word", "\n", "\n", "print", "(", "\"Check the tokenizer.\"", ")", "\n", "for", "key", "in", "dictori", ":", "\n", "            ", "assert", "(", "dictori", "[", "key", "]", "==", "dictnew", "[", "key", "]", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "print", "(", "\"len of tokenizer before updata.\"", ",", "len", "(", "tokenizer", ".", "index_word", ")", ")", "\n", "print", "(", "\"Preparing synonyms.\"", ")", "\n", "\n", "syn_texts_for_tokenizer", "=", "[", "]", "\n", "syn_dict", "=", "{", "}", "\n", "\n", "import", "json", "\n", "with", "open", "(", "opt", ".", "certified_neighbors_file_path", ")", "as", "f", ":", "\n", "            ", "certified_neighbors", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "syn_data", "=", "[", "[", "]", "for", "i", "in", "range", "(", "1", "+", "len", "(", "tokenizer", ".", "index_word", ")", ")", "]", "\n", "for", "index", "in", "tokenizer", ".", "index_word", ":", "\n", "            ", "if", "index", "%", "100", "==", "0", ":", "\n", "                ", "print", "(", "index", ")", "\n", "", "word", "=", "tokenizer", ".", "index_word", "[", "index", "]", "\n", "#syn_text = \" \".join(generate_synonym_list_from_word(word))", "\n", "syn_text", "=", "\" \"", ".", "join", "(", "generate_synonym_list_by_dict", "(", "certified_neighbors", ",", "word", ")", ")", "\n", "syn_texts_for_tokenizer", ".", "append", "(", "syn_text", ")", "\n", "syn_dict", "[", "index", "]", "=", "syn_text", "\n", "\n", "# update tokenizer", "\n", "", "print", "(", "\"Fit on syn texts.\"", ")", "\n", "tokenizer", ".", "fit_on_texts", "(", "syn_texts_for_tokenizer", ",", "freq_count", "=", "0", ")", "# to keep the original index order of the tokenizer", "\n", "update_tokenizer", "(", "dataset", ",", "tokenizer", ")", "\n", "f", "=", "open", "(", "tokenizer_file", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "tokenizer", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "print", "(", "\"len of tokenizer after updata.\"", ",", "len", "(", "tokenizer", ".", "index_word", ")", ")", "\n", "assert", "(", "len", "(", "tokenizer", ".", "index_word", ")", "<", "config", ".", "num_words", "[", "dataset", "]", ")", "\n", "\n", "# Tokenize syn data", "\n", "print", "(", "\"Tokenize syn data.\"", ")", "\n", "for", "key", "in", "syn_dict", ":", "\n", "            ", "temp", "=", "tokenizer", ".", "texts_to_sequences", "(", "[", "syn_dict", "[", "key", "]", "]", ")", "\n", "syn_data", "[", "key", "]", "=", "temp", "[", "0", "]", "\n", "\n", "# make embd according to the updated tokenizer", "\n", "", "embedding_dim", "=", "opt", ".", "embedding_dim", "\n", "pretrained", "=", "opt", ".", "embedding_file_path", "\n", "get_embedding_index", "(", "pretrained", ",", "embedding_dim", ")", "\n", "num_words", "=", "config", ".", "num_words", "[", "dataset", "]", "\n", "opt", ".", "vocab_size", "=", "num_words", "\n", "embedding_matrix", "=", "get_embedding_matrix", "(", "opt", ",", "dataset", ",", "num_words", ",", "embedding_dim", ")", "\n", "opt", ".", "embeddings", "=", "torch", ".", "FloatTensor", "(", "embedding_matrix", ")", "\n", "\n", "# Tokenize the training data", "\n", "print", "(", "\"Tokenize training data.\"", ")", "\n", "\n", "x_train", "=", "text_process_for_single", "(", "tokenizer", ",", "train_texts", ",", "opt", ".", "dataset", ")", "\n", "y_train", "=", "label_process_for_single", "(", "tokenizer", ",", "train_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "x_dev", "=", "text_process_for_single", "(", "tokenizer", ",", "dev_texts", ",", "opt", ".", "dataset", ")", "\n", "y_dev", "=", "label_process_for_single", "(", "tokenizer", ",", "dev_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "x_test", "=", "text_process_for_single", "(", "tokenizer", ",", "test_texts", ",", "opt", ".", "dataset", ")", "\n", "y_test", "=", "label_process_for_single", "(", "tokenizer", ",", "test_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "filename", "=", "opt", ".", "imdb_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'wb'", ")", "\n", "saved", "=", "{", "}", "\n", "saved", "[", "'syn_data'", "]", "=", "syn_data", "\n", "saved", "[", "'embeddings'", "]", "=", "opt", ".", "embeddings", "\n", "saved", "[", "'vocab_size'", "]", "=", "opt", ".", "vocab_size", "\n", "saved", "[", "'x_train'", "]", "=", "x_train", "\n", "saved", "[", "'x_test'", "]", "=", "x_test", "\n", "saved", "[", "'x_dev'", "]", "=", "x_dev", "\n", "saved", "[", "'y_train'", "]", "=", "y_train", "\n", "saved", "[", "'y_test'", "]", "=", "y_test", "\n", "saved", "[", "'y_dev'", "]", "=", "y_dev", "\n", "pickle", ".", "dump", "(", "saved", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "", "vocab_freq", "=", "torch", ".", "zeros", "(", "opt", ".", "embeddings", ".", "shape", "[", "0", "]", ")", ".", "to", "(", "opt", ".", "embeddings", ".", "dtype", ")", "\n", "for", "index", "in", "tokenizer", ".", "index_word", ":", "\n", "        ", "word", "=", "tokenizer", ".", "index_word", "[", "index", "]", "\n", "freq", "=", "tokenizer", ".", "word_counts", "[", "word", "]", "\n", "vocab_freq", "[", "index", "]", "=", "freq", "\n", "", "opt", ".", "vocab_freq", "=", "vocab_freq", "\n", "\n", "train_data", "=", "SynthesizedData_TextLikeSyn", "(", "opt", ",", "x_train", ",", "y_train", ",", "syn_data", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_data", ",", "opt", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "8", ")", "\n", "\n", "# use training data as dev", "\n", "dev_data", "=", "SynthesizedData_TextLikeSyn", "(", "opt", ",", "x_dev", ",", "y_dev", ",", "syn_data", ")", "\n", "dev_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dev_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ")", "\n", "\n", "test_data", "=", "SynthesizedData_TextLikeSyn", "(", "opt", ",", "x_test", ",", "y_test", ",", "syn_data", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "test_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ")", "\n", "return", "train_loader", ",", "dev_loader", ",", "test_loader", ",", "syn_data", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.save_word_vectors_for_show": [[1132, 1158], ["codecs.open", "pickle.load", "codecs.open.close", "codecs.open.close", "enumerate", "codecs.open", "pickle.dump", "codecs.open.close", "print", "len", "len", "for_save.append", "opt.embeddings[].numpy", "opt.embeddings[].numpy"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump"], ["", "def", "save_word_vectors_for_show", "(", "opt", ")", ":", "\n", "\n", "    ", "assert", "(", "opt", ".", "synonyms_from_file", ")", "\n", "filename", "=", "opt", ".", "synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "syn_data", "=", "saved", "[", "\"syn_data\"", "]", "\n", "opt", ".", "embeddings", "=", "saved", "[", "'embeddings'", "]", "\n", "f", ".", "close", "(", ")", "\n", "\n", "for_save", "=", "[", "]", "\n", "\n", "for", "i", ",", "syn", "in", "enumerate", "(", "syn_data", ")", ":", "\n", "        ", "if", "len", "(", "for_save", ")", "==", "100", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "syn", ")", "==", "5", ":", "\n", "            ", "for_save", ".", "append", "(", "(", "opt", ".", "embeddings", "[", "i", "]", ".", "numpy", "(", ")", ",", "[", "opt", ".", "embeddings", "[", "x", "]", ".", "numpy", "(", ")", "for", "x", "in", "syn", "]", ")", ")", "\n", "\n", "", "", "f", "=", "open", "(", "'word_vectors_for_show_5.pickle'", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "for_save", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "print", "(", "\"Done save word vectors for show.\"", ")", "\n", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.imdb_make_synthesized_iter_for_bert": [[1160, 1261], ["ModifiedBertTokenizer.from_pretrained", "dataHelper.SynthesizedData_TextLikeSyn_Bert", "dataHelper.SynthesizedData_TextLikeSyn_Bert.__getitem__", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "LMConstrainedAttackSurface.from_files", "dataHelper.SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "PWWS.read_files.split_imdb_files", "codecs.open", "pickle.load", "codecs.open.close", "print", "PWWS.paraphrase.get_syn_dict", "print", "print", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.label_process_for_single", "codecs.open", "pickle.dump", "codecs.open.close", "PWWS.read_files.split_snli_files", "PWWS.read_files.split_agnews_files", "len", "PWWS.read_files.split_yahoo_files", "ModifiedBertTokenizer.from_pretrained.encode_plus", "ModifiedBertTokenizer.from_pretrained.encode_plus"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.AttrProxy.__getitem__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.from_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.get_syn_dict", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_snli_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_agnews_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_yahoo_files"], ["", "def", "imdb_make_synthesized_iter_for_bert", "(", "opt", ")", ":", "\n", "    ", "dataset", "=", "opt", ".", "dataset", "\n", "if", "dataset", "==", "'imdb'", ":", "\n", "        ", "opt", ".", "label_size", "=", "2", "\n", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "", "elif", "dataset", "==", "'snli'", ":", "\n", "        ", "opt", ".", "label_size", "=", "2", "\n", "train_perms", ",", "train_hypos", ",", "train_labels", ",", "dev_perms", ",", "dev_hypos", ",", "dev_labels", ",", "test_perms", ",", "test_hypos", ",", "test_labels", "=", "split_snli_files", "(", "opt", ")", "\n", "", "elif", "dataset", "==", "'agnews'", ":", "\n", "# opt.label_size = ?", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_agnews_files", "(", ")", "\n", "", "elif", "dataset", "==", "'yahoo'", ":", "\n", "# opt.label_size = ?", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_yahoo_files", "(", ")", "\n", "\n", "\n", "", "from", "modified_bert_tokenizer", "import", "ModifiedBertTokenizer", "\n", "#import transformers", "\n", "#tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)", "\n", "tokenizer", "=", "ModifiedBertTokenizer", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "if", "opt", ".", "synonyms_from_file", ":", "\n", "        ", "filename", "=", "opt", ".", "imdb_bert_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "syn_data", "=", "saved", "[", "\"syn_data\"", "]", "\n", "#x_train=saved['x_train']", "\n", "#x_test=saved['x_test']", "\n", "#x_dev=saved['x_dev']", "\n", "y_train", "=", "saved", "[", "'y_train'", "]", "\n", "y_test", "=", "saved", "[", "'y_test'", "]", "\n", "y_dev", "=", "saved", "[", "'y_dev'", "]", "\n", "\n", "", "else", ":", "\n", "#tokenizer = get_tokenizer(opt)", "\n", "#print(\"len of tokenizer before updata.\", len(tokenizer.index_word))", "\n", "        ", "print", "(", "\"Preparing synonyms.\"", ")", "\n", "\n", "syn_dict", "=", "get_syn_dict", "(", "opt", ")", "\n", "syn_data", "=", "{", "}", "# key is textual word", "\n", "\n", "# Tokenize syn data", "\n", "print", "(", "\"Tokenize syn data.\"", ")", "\n", "for", "key", "in", "syn_dict", ":", "\n", "            ", "if", "len", "(", "syn_dict", "[", "key", "]", ")", "!=", "0", ":", "\n", "                ", "temp", "=", "tokenizer", ".", "encode_plus", "(", "syn_dict", "[", "key", "]", ",", "None", ",", "add_special_tokens", "=", "False", ",", "pad_to_max_length", "=", "False", ")", "[", "'input_ids'", "]", "\n", "\n", "token_of_key", "=", "tokenizer", ".", "encode_plus", "(", "key", ",", "None", ",", "add_special_tokens", "=", "False", ",", "pad_to_max_length", "=", "False", ")", "[", "\"input_ids\"", "]", "[", "0", "]", "\n", "\n", "syn_data", "[", "key", "]", "=", "temp", "\n", "\n", "#if not token_of_key in syn_data:", "\n", "#    syn_data[token_of_key] = temp", "\n", "#else:", "\n", "#    syn_data[token_of_key].append(temp)", "\n", "\n", "# Tokenize the training data", "\n", "", "", "print", "(", "\"Tokenize training data.\"", ")", "\n", "#x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)", "\n", "\n", "#x_train = text_process_for_single_bert(tokenizer, train_texts, opt.dataset)", "\n", "y_train", "=", "label_process_for_single", "(", "tokenizer", ",", "train_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "#x_dev = text_process_for_single_bert(tokenizer, dev_texts, opt.dataset)", "\n", "y_dev", "=", "label_process_for_single", "(", "tokenizer", ",", "dev_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "#x_test = text_process_for_single_bert(tokenizer, test_texts, opt.dataset)", "\n", "y_test", "=", "label_process_for_single", "(", "tokenizer", ",", "test_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "\n", "filename", "=", "opt", ".", "imdb_bert_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'wb'", ")", "\n", "saved", "=", "{", "}", "\n", "saved", "[", "'syn_data'", "]", "=", "syn_data", "\n", "#saved['x_train']=x_train", "\n", "#saved['x_test']=x_test", "\n", "#saved['x_dev']=x_dev", "\n", "saved", "[", "'y_train'", "]", "=", "y_train", "\n", "saved", "[", "'y_test'", "]", "=", "y_test", "\n", "saved", "[", "'y_dev'", "]", "=", "y_dev", "\n", "pickle", ".", "dump", "(", "saved", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "", "from", "PWWS", ".", "config", "import", "config", "\n", "seq_max_len", "=", "config", ".", "word_max_len", "[", "opt", ".", "dataset", "]", "\n", "\n", "train_data", "=", "SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "train_texts", ",", "y_train", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "given_y", "=", "None", ")", "\n", "train_data", ".", "__getitem__", "(", "0", ")", "\n", "train_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_data", ",", "opt", ".", "batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "\n", "# use training data as dev", "\n", "dev_data", "=", "SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "dev_texts", ",", "y_dev", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ")", "\n", "dev_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dev_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ")", "\n", "\n", "from", "from_certified", ".", "attack_surface", "import", "WordSubstitutionAttackSurface", ",", "LMConstrainedAttackSurface", "\n", "attack_surface", "=", "LMConstrainedAttackSurface", ".", "from_files", "(", "opt", ".", "certified_neighbors_file_path", ",", "opt", ".", "imdb_lm_file_path", ")", "\n", "\n", "test_data", "=", "SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "test_texts", ",", "y_test", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "attack_surface", "=", "attack_surface", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "test_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ")", "\n", "return", "train_loader", ",", "dev_loader", ",", "test_loader", ",", "syn_data", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.imdb_bert_make_synthesized_iter_giveny": [[1263, 1358], ["PWWS.read_files.split_imdb_files", "ModifiedBertTokenizer.from_pretrained", "len", "dataHelper.SynthesizedData_TextLikeSyn_Bert", "dataHelper.SynthesizedData_TextLikeSyn_Bert.__getitem__", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "LMConstrainedAttackSurface.from_files", "dataHelper.SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "codecs.open", "pickle.load", "codecs.open.close", "print", "PWWS.paraphrase.get_syn_dict", "print", "print", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.label_process_for_single", "codecs.open", "pickle.dump", "codecs.open.close", "len", "ModifiedBertTokenizer.from_pretrained.encode_plus", "ModifiedBertTokenizer.from_pretrained.encode_plus"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.AttrProxy.__getitem__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.from_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.get_syn_dict", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump"], ["", "def", "imdb_bert_make_synthesized_iter_giveny", "(", "opt", ")", ":", "\n", "    ", "dataset", "=", "opt", ".", "dataset", "\n", "opt", ".", "label_size", "=", "2", "\n", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "\n", "from", "modified_bert_tokenizer", "import", "ModifiedBertTokenizer", "\n", "#import transformers", "\n", "#tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)", "\n", "tokenizer", "=", "ModifiedBertTokenizer", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "if", "opt", ".", "synonyms_from_file", ":", "\n", "        ", "filename", "=", "opt", ".", "imdb_bert_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "syn_data", "=", "saved", "[", "\"syn_data\"", "]", "\n", "#x_train=saved['x_train']", "\n", "#x_test=saved['x_test']", "\n", "#x_dev=saved['x_dev']", "\n", "y_train", "=", "saved", "[", "'y_train'", "]", "\n", "y_test", "=", "saved", "[", "'y_test'", "]", "\n", "y_dev", "=", "saved", "[", "'y_dev'", "]", "\n", "\n", "", "else", ":", "\n", "#tokenizer = get_tokenizer(opt)", "\n", "#print(\"len of tokenizer before updata.\", len(tokenizer.index_word))", "\n", "        ", "print", "(", "\"Preparing synonyms.\"", ")", "\n", "\n", "syn_dict", "=", "get_syn_dict", "(", "opt", ")", "\n", "syn_data", "=", "{", "}", "# key is textual word", "\n", "\n", "# Tokenize syn data", "\n", "print", "(", "\"Tokenize syn data.\"", ")", "\n", "for", "key", "in", "syn_dict", ":", "\n", "            ", "if", "len", "(", "syn_dict", "[", "key", "]", ")", "!=", "0", ":", "\n", "                ", "temp", "=", "tokenizer", ".", "encode_plus", "(", "syn_dict", "[", "key", "]", ",", "None", ",", "add_special_tokens", "=", "False", ",", "pad_to_max_length", "=", "False", ")", "[", "'input_ids'", "]", "\n", "\n", "token_of_key", "=", "tokenizer", ".", "encode_plus", "(", "key", ",", "None", ",", "add_special_tokens", "=", "False", ",", "pad_to_max_length", "=", "False", ")", "[", "\"input_ids\"", "]", "[", "0", "]", "\n", "\n", "syn_data", "[", "key", "]", "=", "temp", "\n", "\n", "#if not token_of_key in syn_data:", "\n", "#    syn_data[token_of_key] = temp", "\n", "#else:", "\n", "#    syn_data[token_of_key].append(temp)", "\n", "\n", "# Tokenize the training data", "\n", "", "", "print", "(", "\"Tokenize training data.\"", ")", "\n", "#x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)", "\n", "\n", "#x_train = text_process_for_single_bert(tokenizer, train_texts, opt.dataset)", "\n", "y_train", "=", "label_process_for_single", "(", "tokenizer", ",", "train_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "#x_dev = text_process_for_single_bert(tokenizer, dev_texts, opt.dataset)", "\n", "y_dev", "=", "label_process_for_single", "(", "tokenizer", ",", "dev_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "#x_test = text_process_for_single_bert(tokenizer, test_texts, opt.dataset)", "\n", "y_test", "=", "label_process_for_single", "(", "tokenizer", ",", "test_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "\n", "filename", "=", "opt", ".", "imdb_bert_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'wb'", ")", "\n", "saved", "=", "{", "}", "\n", "saved", "[", "'syn_data'", "]", "=", "syn_data", "\n", "#saved['x_train']=x_train", "\n", "#saved['x_test']=x_test", "\n", "#saved['x_dev']=x_dev", "\n", "saved", "[", "'y_train'", "]", "=", "y_train", "\n", "saved", "[", "'y_test'", "]", "=", "y_test", "\n", "saved", "[", "'y_dev'", "]", "=", "y_dev", "\n", "pickle", ".", "dump", "(", "saved", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "", "from", "PWWS", ".", "config", "import", "config", "\n", "seq_max_len", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "\n", "opt", ".", "n_training_set", "=", "len", "(", "train_texts", ")", "\n", "\n", "train_data_y0", "=", "SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "train_texts", ",", "y_train", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "given_y", "=", "0", ")", "\n", "train_data_y0", ".", "__getitem__", "(", "0", ")", "\n", "train_loader_y0", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_data_y0", ",", "opt", ".", "batch_size", "//", "2", ",", "shuffle", "=", "True", ",", "num_workers", "=", "16", ")", "\n", "\n", "train_data_y1", "=", "SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "train_texts", ",", "y_train", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "given_y", "=", "1", ")", "\n", "train_loader_y1", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_data_y1", ",", "opt", ".", "batch_size", "//", "2", ",", "shuffle", "=", "True", ",", "num_workers", "=", "16", ")", "\n", "\n", "# use training data as dev", "\n", "dev_data", "=", "SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "dev_texts", ",", "y_dev", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ")", "\n", "dev_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dev_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "16", ")", "\n", "\n", "from", "from_certified", ".", "attack_surface", "import", "WordSubstitutionAttackSurface", ",", "LMConstrainedAttackSurface", "\n", "attack_surface", "=", "LMConstrainedAttackSurface", ".", "from_files", "(", "opt", ".", "certified_neighbors_file_path", ",", "opt", ".", "imdb_lm_file_path", ")", "\n", "\n", "test_data", "=", "SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "test_texts", ",", "y_test", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "attack_surface", "=", "attack_surface", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "test_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "16", ")", "\n", "return", "train_loader_y0", ",", "train_loader_y1", ",", "dev_loader", ",", "test_loader", ",", "syn_data", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.snli_make_synthesized_iter_for_bert_giveny": [[1360, 1463], ["ModifiedBertTokenizer.from_pretrained", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert.__getitem__", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "LMConstrainedAttackSurface.from_files", "dataHelper.SNLI_SynthesizedData_TextLikeSyn_Bert", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "PWWS.read_files.split_snli_files", "codecs.open", "pickle.load", "codecs.open.close", "print", "PWWS.paraphrase.get_syn_dict", "print", "print", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.label_process_for_single", "PWWS.word_level_process.label_process_for_single", "codecs.open", "pickle.dump", "codecs.open.close", "len", "ModifiedBertTokenizer.from_pretrained.encode_plus", "ModifiedBertTokenizer.from_pretrained.encode_plus"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.AttrProxy.__getitem__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.from_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_snli_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.get_syn_dict", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump"], ["", "def", "snli_make_synthesized_iter_for_bert_giveny", "(", "opt", ")", ":", "\n", "    ", "dataset", "=", "opt", ".", "dataset", "\n", "if", "dataset", "==", "'snli'", ":", "\n", "        ", "opt", ".", "label_size", "=", "3", "\n", "train_perms", ",", "train_hypos", ",", "train_labels", ",", "dev_perms", ",", "dev_hypos", ",", "dev_labels", ",", "test_perms", ",", "test_hypos", ",", "test_labels", "=", "split_snli_files", "(", "opt", ")", "\n", "\n", "", "from", "modified_bert_tokenizer", "import", "ModifiedBertTokenizer", "\n", "#import transformers", "\n", "#tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)", "\n", "tokenizer", "=", "ModifiedBertTokenizer", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "if", "opt", ".", "synonyms_from_file", ":", "\n", "        ", "filename", "=", "opt", ".", "snli_bert_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "syn_data", "=", "saved", "[", "\"syn_data\"", "]", "\n", "#x_p_train=saved['x_p_train']", "\n", "#x_h_train=saved['x_h_train']", "\n", "y_train", "=", "saved", "[", "'y_train'", "]", "\n", "#x_p_test=saved['x_p_test']", "\n", "#x_h_test=saved['x_h_test']", "\n", "y_test", "=", "saved", "[", "'y_test'", "]", "\n", "#x_p_dev=saved['x_p_dev']", "\n", "#x_h_dev=saved['x_h_dev']", "\n", "y_dev", "=", "saved", "[", "'y_dev'", "]", "\n", "", "else", ":", "\n", "#tokenizer = get_tokenizer(opt)", "\n", "#print(\"len of tokenizer before updata.\", len(tokenizer.index_word))", "\n", "        ", "print", "(", "\"Preparing synonyms.\"", ")", "\n", "\n", "syn_dict", "=", "get_syn_dict", "(", "opt", ")", "\n", "syn_data", "=", "{", "}", "# key is textual word", "\n", "\n", "# Tokenize syn data", "\n", "print", "(", "\"Tokenize syn data.\"", ")", "\n", "for", "key", "in", "syn_dict", ":", "\n", "            ", "if", "len", "(", "syn_dict", "[", "key", "]", ")", "!=", "0", ":", "\n", "                ", "temp", "=", "tokenizer", ".", "encode_plus", "(", "syn_dict", "[", "key", "]", ",", "None", ",", "add_special_tokens", "=", "False", ",", "pad_to_max_length", "=", "False", ")", "[", "'input_ids'", "]", "\n", "\n", "token_of_key", "=", "tokenizer", ".", "encode_plus", "(", "key", ",", "None", ",", "add_special_tokens", "=", "False", ",", "pad_to_max_length", "=", "False", ")", "[", "\"input_ids\"", "]", "[", "0", "]", "\n", "\n", "syn_data", "[", "key", "]", "=", "temp", "\n", "\n", "#if not token_of_key in syn_data:", "\n", "#    syn_data[token_of_key] = temp", "\n", "#else:", "\n", "#    syn_data[token_of_key].append(temp)", "\n", "\n", "# Tokenize the training data", "\n", "", "", "print", "(", "\"Tokenize training data.\"", ")", "\n", "#x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)", "\n", "\n", "#x_train = text_process_for_single_bert(tokenizer, train_texts, opt.dataset)", "\n", "y_train", "=", "label_process_for_single", "(", "tokenizer", ",", "train_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "#x_dev = text_process_for_single_bert(tokenizer, dev_texts, opt.dataset)", "\n", "y_dev", "=", "label_process_for_single", "(", "tokenizer", ",", "dev_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "#x_test = text_process_for_single_bert(tokenizer, test_texts, opt.dataset)", "\n", "y_test", "=", "label_process_for_single", "(", "tokenizer", ",", "test_labels", ",", "opt", ".", "dataset", ")", "\n", "\n", "\n", "filename", "=", "opt", ".", "snli_bert_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'wb'", ")", "\n", "saved", "=", "{", "}", "\n", "saved", "[", "'syn_data'", "]", "=", "syn_data", "\n", "#saved['x_train']=x_train", "\n", "#saved['x_test']=x_test", "\n", "#saved['x_dev']=x_dev", "\n", "saved", "[", "'y_train'", "]", "=", "y_train", "\n", "saved", "[", "'y_test'", "]", "=", "y_test", "\n", "saved", "[", "'y_dev'", "]", "=", "y_dev", "\n", "pickle", ".", "dump", "(", "saved", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "\n", "\n", "", "from", "PWWS", ".", "config", "import", "config", "\n", "seq_max_len", "=", "config", ".", "word_max_len", "[", "opt", ".", "dataset", "]", "\n", "\n", "train_data_y0", "=", "SNLI_SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "train_perms", ",", "train_hypos", ",", "y_train", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "given_y", "=", "0", ")", "\n", "train_loader_y0", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_data_y0", ",", "opt", ".", "batch_size", "//", "opt", ".", "label_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "\n", "train_data_y0", ".", "__getitem__", "(", "0", ")", "\n", "\n", "\n", "train_data_y1", "=", "SNLI_SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "train_perms", ",", "train_hypos", ",", "y_train", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "given_y", "=", "1", ")", "\n", "train_loader_y1", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_data_y1", ",", "opt", ".", "batch_size", "//", "opt", ".", "label_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "\n", "train_data_y2", "=", "SNLI_SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "train_perms", ",", "train_hypos", ",", "y_train", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "given_y", "=", "2", ")", "\n", "train_loader_y2", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "train_data_y2", ",", "opt", ".", "batch_size", "//", "opt", ".", "label_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "\n", "# use training data as dev", "\n", "dev_data", "=", "SNLI_SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "dev_perms", ",", "dev_hypos", ",", "y_dev", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ")", "\n", "dev_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dev_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ")", "\n", "\n", "from", "from_certified", ".", "attack_surface", "import", "WordSubstitutionAttackSurface", ",", "LMConstrainedAttackSurface", "\n", "attack_surface", "=", "LMConstrainedAttackSurface", ".", "from_files", "(", "opt", ".", "certified_neighbors_file_path", ",", "opt", ".", "imdb_lm_file_path", ")", "\n", "\n", "test_data", "=", "SNLI_SynthesizedData_TextLikeSyn_Bert", "(", "opt", ",", "test_perms", ",", "test_hypos", ",", "y_test", ",", "syn_data", ",", "seq_max_len", ",", "tokenizer", ",", "attack_surface", "=", "attack_surface", ")", "\n", "test_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "test_data", ",", "opt", ".", "test_batch_size", ",", "shuffle", "=", "False", ",", "num_workers", "=", "4", ")", "\n", "return", "train_loader_y0", ",", "train_loader_y1", ",", "train_loader_y2", ",", "dev_loader", ",", "test_loader", ",", "syn_data", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.load_synonyms_in_vocab": [[1465, 1530], ["dataHelper.SynonymData", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "codecs.open", "pickle.load", "codecs.open.close", "PWWS.word_level_process.get_tokenizer", "print", "codecs.open", "pickle.dump", "codecs.open.close", "PWWS.paraphrase.generate_synonym_list_from_word", "range", "print", "len", "len", "PWWS.word_level_process.get_tokenizer.texts_to_sequences", "synonym_list.append"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.generate_synonym_list_from_word", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences"], ["", "def", "load_synonyms_in_vocab", "(", "opt", ",", "max_synonym_num", "=", "1000", ")", ":", "\n", "    ", "if", "opt", ".", "synonyms_from_file", ":", "\n", "        ", "filename", "=", "opt", ".", "synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "roots", "=", "saved", "[", "\"roots\"", "]", "\n", "labels", "=", "saved", "[", "\"labels\"", "]", "\n", "\n", "#all_words_set = set()", "\n", "#tokenizer = get_tokenizer(opt)", "\n", "#for index in tokenizer.index_word:", "\n", "#    all_words_set.add(index)", "\n", "\n", "#import time", "\n", "#t0 = time.time()", "\n", "#negatives = []", "\n", "#for idx, x in enumerate(labels):", "\n", "#    negatives.append( np.array(list(all_words_set-set(list(x))))  )", "\n", "#    if idx %100 ==0:", "\n", "#        print(time.time()-t0, idx)", "\n", "#        t0=time.time()", "\n", "#print(\"Done.\")", "\n", "\n", "#f=open(filename,'wb')", "\n", "#saved['negatives']=negatives", "\n", "#pickle.dump(saved,f)", "\n", "#f.close()", "\n", "\n", "", "else", ":", "\n", "        ", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "print", "(", "\"Preparing synonyms.\"", ")", "\n", "\n", "data", "=", "[", "[", "]", "for", "i", "in", "range", "(", "len", "(", "tokenizer", ".", "index_word", ")", ")", "]", "\n", "\n", "for", "index", "in", "tokenizer", ".", "index_word", ":", "\n", "            ", "if", "index", "%", "100", "==", "0", ":", "\n", "                ", "print", "(", "index", ")", "\n", "#if index>100:", "\n", "#    break", "\n", "\n", "", "word", "=", "tokenizer", ".", "index_word", "[", "index", "]", "\n", "synonym_list_ori", "=", "generate_synonym_list_from_word", "(", "word", ")", "\n", "\n", "synonym_list", "=", "[", "]", "\n", "\n", "if", "len", "(", "synonym_list_ori", ")", "!=", "0", ":", "\n", "                ", "for", "synonym", "in", "synonym_list_ori", ":", "\n", "                    ", "temp", "=", "tokenizer", ".", "texts_to_sequences", "(", "[", "synonym", "]", ")", "\n", "if", "temp", "!=", "[", "[", "]", "]", ":", "\n", "                        ", "synonym_list", ".", "append", "(", "temp", "[", "0", "]", "[", "0", "]", ")", "\n", "\n", "", "", "", "data", "[", "index", "]", "=", "synonym_list", "\n", "\n", "", "filename", "=", "opt", ".", "synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'wb'", ")", "\n", "saved", "=", "{", "}", "\n", "saved", "[", "'data'", "]", "=", "data", "\n", "pickle", ".", "dump", "(", "saved", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n", "", "syn_data", "=", "SynonymData", "(", "roots", ",", "labels", ",", "opt", ")", "\n", "syn_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "syn_data", ",", "opt", ".", "syn_batch_size", ",", "shuffle", "=", "True", ",", "num_workers", "=", "4", ")", "\n", "#syn_data.__getitem__(0)", "\n", "return", "syn_loader", "\n", "#return BucketIterator_synonyms(roots,labels, opt)", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.label_smooth.LabelSmoothSoftmaxCE.__init__": [[4, 16], ["torch.Module.__init__", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "\n", "lb_pos", "=", "0.9", ",", "\n", "lb_neg", "=", "0.005", ",", "\n", "reduction", "=", "'mean'", ",", "\n", "lb_ignore", "=", "255", ",", "\n", ")", ":", "\n", "        ", "super", "(", "LabelSmoothSoftmaxCE", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "lb_pos", "=", "lb_pos", "\n", "self", ".", "lb_neg", "=", "lb_neg", "\n", "self", ".", "reduction", "=", "reduction", "\n", "self", ".", "lb_ignore", "=", "lb_ignore", "\n", "self", ".", "log_softmax", "=", "nn", ".", "LogSoftmax", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.label_smooth.LabelSmoothSoftmaxCE.forward": [[17, 34], ["label_smooth.LabelSmoothSoftmaxCE.log_softmax", "logits.data.clone().zero_().scatter_", "ignore.nonzero.nonzero.nonzero", "ignore.nonzero.nonzero.size", "ignore.nonzero.nonzero.chunk", "label.data.cpu", "label.unsqueeze", "logits.data.clone().zero_", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "logits.data.clone", "label.size", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "logits", ",", "label", ")", ":", "\n", "        ", "logs", "=", "self", ".", "log_softmax", "(", "logits", ")", "\n", "ignore", "=", "label", ".", "data", ".", "cpu", "(", ")", "==", "self", ".", "lb_ignore", "\n", "n_valid", "=", "(", "ignore", "==", "0", ")", ".", "sum", "(", ")", "\n", "label", "[", "ignore", "]", "=", "0", "\n", "lb_one_hot", "=", "logits", ".", "data", ".", "clone", "(", ")", ".", "zero_", "(", ")", ".", "scatter_", "(", "1", ",", "label", ".", "unsqueeze", "(", "1", ")", ",", "1", ")", "\n", "label", "=", "self", ".", "lb_pos", "*", "lb_one_hot", "+", "self", ".", "lb_neg", "*", "(", "1", "-", "lb_one_hot", ")", "\n", "ignore", "=", "ignore", ".", "nonzero", "(", ")", "\n", "_", ",", "M", "=", "ignore", ".", "size", "(", ")", "\n", "a", ",", "*", "b", "=", "ignore", ".", "chunk", "(", "M", ",", "dim", "=", "1", ")", "\n", "label", "[", "[", "a", ",", "torch", ".", "arange", "(", "label", ".", "size", "(", "1", ")", ")", ",", "*", "b", "]", "]", "=", "0", "\n", "\n", "if", "self", ".", "reduction", "==", "'mean'", ":", "\n", "            ", "loss", "=", "-", "torch", ".", "sum", "(", "torch", ".", "sum", "(", "logs", "*", "label", ",", "dim", "=", "1", ")", ")", "/", "n_valid", "\n", "", "elif", "self", ".", "reduction", "==", "'none'", ":", "\n", "            ", "loss", "=", "-", "torch", ".", "sum", "(", "logs", "*", "label", ",", "dim", "=", "1", ")", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.log_time_delta": [[18, 28], ["functools.wraps", "time.time", "func", "time.time", "print"], "function", ["None"], ["", "def", "log_time_delta", "(", "func", ")", ":", "\n", "    ", "@", "wraps", "(", "func", ")", "\n", "def", "_deco", "(", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "start", "=", "time", ".", "time", "(", ")", "\n", "ret", "=", "func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "end", "=", "time", ".", "time", "(", ")", "\n", "delta", "=", "end", "-", "start", "\n", "print", "(", "\"%s runed %.2f seconds\"", "%", "(", "func", ".", "__name__", ",", "delta", ")", ")", "\n", "return", "ret", "\n", "", "return", "_deco", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.clip_gradient": [[29, 34], ["param.grad.data.clamp_"], "function", ["None"], ["", "def", "clip_gradient", "(", "optimizer", ",", "grad_clip", ")", ":", "\n", "    ", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "for", "param", "in", "group", "[", "'params'", "]", ":", "\n", "            ", "if", "param", ".", "grad", "is", "not", "None", "and", "param", ".", "requires_grad", ":", "\n", "                ", "param", ".", "grad", ".", "data", ".", "clamp_", "(", "-", "grad_clip", ",", "grad_clip", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.sign_scale_gradient": [[35, 40], ["torch.sign", "torch.sign"], "function", ["None"], ["", "", "", "", "def", "sign_scale_gradient", "(", "optimizer", ",", "scale", ")", ":", "\n", "    ", "for", "group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "for", "param", "in", "group", "[", "'params'", "]", ":", "\n", "            ", "if", "param", ".", "grad", "is", "not", "None", "and", "param", ".", "requires_grad", ":", "\n", "                ", "param", ".", "grad", ".", "data", "=", "torch", ".", "sign", "(", "param", ".", "grad", ".", "data", ")", "*", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.snli_evaluation": [[42, 63], ["model.eval", "enumerate", "model.train", "numpy.mean", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "model", "torch.max", "torch.max", "torch.cuda.is_available", "torch.cuda.is_available", "accuracy.append", "accuracy.append", "percision.data.item", "percision.data.numpy"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train"], ["", "", "", "", "def", "snli_evaluation", "(", "opt", ",", "device", ",", "model", ",", "test_iter", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "accuracy", "=", "[", "]", "\n", "#    batch= next(iter(test_iter))", "\n", "for", "index", ",", "batch", "in", "enumerate", "(", "test_iter", ")", ":", "\n", "        ", "x_p", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "x_h", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "label", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "x_p_mask", "=", "batch", "[", "7", "]", ".", "to", "(", "device", ")", "\n", "x_h_mask", "=", "batch", "[", "8", "]", ".", "to", "(", "device", ")", "\n", "\n", "predicted", "=", "model", "(", "mode", "=", "'text_to_logit'", ",", "x_p", "=", "x_p", ",", "x_h", "=", "x_h", ",", "x_p_mask", "=", "x_p_mask", ",", "x_h_mask", "=", "x_h_mask", ")", "\n", "prob", ",", "idx", "=", "torch", ".", "max", "(", "predicted", ",", "1", ")", "\n", "percision", "=", "(", "idx", "==", "label", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "", "", "model", ".", "train", "(", ")", "\n", "return", "np", ".", "mean", "(", "accuracy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.imdb_evaluation": [[64, 82], ["model.eval", "enumerate", "model.train", "numpy.mean", "batch[].to", "batch[].to", "model", "torch.max", "torch.max", "torch.cuda.is_available", "torch.cuda.is_available", "accuracy.append", "accuracy.append", "percision.data.item", "percision.data.numpy"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train"], ["", "def", "imdb_evaluation", "(", "opt", ",", "device", ",", "model", ",", "test_iter", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "accuracy", "=", "[", "]", "\n", "#    batch= next(iter(test_iter))", "\n", "for", "index", ",", "batch", "in", "enumerate", "(", "test_iter", ")", ":", "\n", "        ", "text", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "label", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "\n", "predicted", "=", "model", "(", "mode", "=", "'text_to_logit'", ",", "input", "=", "text", ")", "\n", "prob", ",", "idx", "=", "torch", ".", "max", "(", "predicted", ",", "1", ")", "\n", "percision", "=", "(", "idx", "==", "label", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "", "", "model", ".", "train", "(", ")", "\n", "return", "np", ".", "mean", "(", "accuracy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.imdb_evaluation_bert": [[83, 103], ["model.eval", "enumerate", "model.train", "numpy.mean", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "model", "torch.max", "torch.max", "torch.cuda.is_available", "torch.cuda.is_available", "accuracy.append", "accuracy.append", "percision.data.item", "percision.data.numpy"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train"], ["", "def", "imdb_evaluation_bert", "(", "opt", ",", "device", ",", "model", ",", "test_iter", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "accuracy", "=", "[", "]", "\n", "#    batch= next(iter(test_iter))", "\n", "for", "index", ",", "batch", "in", "enumerate", "(", "test_iter", ")", ":", "\n", "        ", "text", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "label", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "bert_mask", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "bert_token_id", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "predicted", "=", "model", "(", "mode", "=", "'text_to_logit'", ",", "input", "=", "text", ",", "bert_mask", "=", "bert_mask", ",", "bert_token_id", "=", "bert_token_id", ")", "\n", "prob", ",", "idx", "=", "torch", ".", "max", "(", "predicted", ",", "1", ")", "\n", "percision", "=", "(", "idx", "==", "label", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "", "", "model", ".", "train", "(", ")", "\n", "return", "np", ".", "mean", "(", "accuracy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.snli_evaluation_ascc_attack": [[105, 161], ["model.eval", "enumerate", "model.train", "numpy.mean", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "len", "model", "model", "x_p_text_like_syn_embd.reshape.reshape", "x_h_text_like_syn_embd.reshape.reshape", "model", "model", "torch.max", "torch.max", "torch.cuda.is_available", "torch.cuda.is_available", "accuracy.append", "accuracy.append", "batch[].to.reshape", "batch[].to.reshape", "percision.data.item", "percision.data.numpy"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train"], ["", "def", "snli_evaluation_ascc_attack", "(", "opt", ",", "device", ",", "model", ",", "test_iter", ",", "tokenizer", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "accuracy", "=", "[", "]", "\n", "#    batch= next(iter(test_iter))", "\n", "for", "index", ",", "batch", "in", "enumerate", "(", "test_iter", ")", ":", "\n", "        ", "x_p", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "x_h", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "label", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "x_p_text_like_syn", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "x_p_text_like_syn_valid", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "x_h_text_like_syn", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "x_h_text_like_syn_valid", "=", "batch", "[", "6", "]", ".", "to", "(", "device", ")", "\n", "x_p_mask", "=", "batch", "[", "7", "]", ".", "to", "(", "device", ")", "\n", "x_h_mask", "=", "batch", "[", "8", "]", ".", "to", "(", "device", ")", "\n", "\n", "batch_size", "=", "len", "(", "x_p", ")", "\n", "if", "index", "*", "batch_size", ">", "9842", ":", "\n", "            ", "break", "\n", "\n", "", "attack_type_dict", "=", "{", "\n", "'num_steps'", ":", "opt", ".", "test_attack_iters", ",", "\n", "'loss_func'", ":", "'ce'", ",", "\n", "'w_optm_lr'", ":", "opt", ".", "w_optm_lr", ",", "\n", "'sparse_weight'", ":", "opt", ".", "attack_sparse_weight", ",", "\n", "'out_type'", ":", "\"text\"", ",", "\n", "'attack_hypo_only'", ":", "True", ",", "\n", "}", "\n", "embd_p", ",", "embd_h", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "x_p", "=", "x_p", ",", "x_h", "=", "x_h", ")", "#in bs, len sent, vocab", "\n", "assert", "(", "x_p_text_like_syn", ".", "shape", "==", "x_h_text_like_syn", ".", "shape", ")", "\n", "n", ",", "l", ",", "s", "=", "x_p_text_like_syn", ".", "shape", "\n", "x_p_text_like_syn_embd", ",", "x_h_text_like_syn_embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "x_p", "=", "x_p_text_like_syn", ".", "reshape", "(", "n", ",", "l", "*", "s", ")", ",", "x_h", "=", "x_h_text_like_syn", ".", "reshape", "(", "n", ",", "l", "*", "s", ")", ")", "\n", "x_p_text_like_syn_embd", "=", "x_p_text_like_syn_embd", ".", "reshape", "(", "n", ",", "l", ",", "s", ",", "-", "1", ")", "\n", "x_h_text_like_syn_embd", "=", "x_h_text_like_syn_embd", ".", "reshape", "(", "n", ",", "l", ",", "s", ",", "-", "1", ")", "\n", "\n", "adv_x_p", ",", "adv_x_h", "=", "model", "(", "mode", "=", "\"get_adv_by_convex_syn\"", ",", "x_p", "=", "embd_p", ",", "x_h", "=", "embd_h", ",", "label", "=", "label", ",", "\n", "x_p_text_like_syn", "=", "x_p_text_like_syn", ",", "\n", "x_p_text_like_syn_embd", "=", "x_p_text_like_syn_embd", ",", "x_p_text_like_syn_valid", "=", "x_p_text_like_syn_valid", ",", "\n", "x_h_text_like_syn", "=", "x_h_text_like_syn", ",", "\n", "x_h_text_like_syn_embd", "=", "x_h_text_like_syn_embd", ",", "x_h_text_like_syn_valid", "=", "x_h_text_like_syn_valid", ",", "\n", "x_p_mask", "=", "x_p_mask", ",", "x_h_mask", "=", "x_h_mask", ",", "\n", "attack_type_dict", "=", "attack_type_dict", ")", "\n", "\n", "predicted", "=", "model", "(", "mode", "=", "'text_to_logit'", ",", "x_p", "=", "x_p", ",", "x_h", "=", "adv_x_h", ",", "x_p_mask", "=", "x_p_mask", ",", "x_h_mask", "=", "x_h_mask", ")", "\n", "#print(\"_________________________________\")", "\n", "#print(inverse_tokenize(tokenizer, x_h[0]))", "\n", "#print(inverse_tokenize(tokenizer, adv_x_h[0]))", "\n", "\n", "prob", ",", "idx", "=", "torch", ".", "max", "(", "predicted", ",", "1", ")", "\n", "percision", "=", "(", "idx", "==", "label", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "", "", "model", ".", "train", "(", ")", "\n", "return", "np", ".", "mean", "(", "accuracy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.imdb_evaluation_ascc_attack": [[162, 207], ["model.eval", "enumerate", "model.train", "numpy.mean", "print", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "len", "model", "model().reshape", "model", "model", "torch.max", "torch.max", "torch.cuda.is_available", "torch.cuda.is_available", "accuracy.append", "accuracy.append", "model", "percision.data.item", "percision.data.numpy", "batch[].to.reshape"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train"], ["", "def", "imdb_evaluation_ascc_attack", "(", "opt", ",", "device", ",", "model", ",", "test_iter", ",", "tokenizer", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "accuracy", "=", "[", "]", "\n", "record_for_vis", "=", "{", "}", "\n", "record_for_vis", "[", "\"comb_p_list\"", "]", "=", "[", "]", "\n", "record_for_vis", "[", "\"embd_syn_list\"", "]", "=", "[", "]", "\n", "record_for_vis", "[", "\"syn_valid_list\"", "]", "=", "[", "]", "\n", "record_for_vis", "[", "\"text_syn_list\"", "]", "=", "[", "]", "\n", "\n", "#    batch= next(iter(test_iter))", "\n", "if", "opt", ".", "pert_set", "==", "\"convex_combination\"", ":", "\n", "        ", "print", "(", "\"ad test by convex_combination.\"", ")", "\n", "", "for", "index", ",", "batch", "in", "enumerate", "(", "test_iter", ")", ":", "\n", "        ", "text", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "label", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "text_like_syn", "=", "batch", "[", "6", "]", ".", "to", "(", "device", ")", "\n", "text_like_syn_valid", "=", "batch", "[", "7", "]", ".", "to", "(", "device", ")", "\n", "\n", "batch_size", "=", "len", "(", "text", ")", "\n", "\n", "attack_type_dict", "=", "{", "\n", "'num_steps'", ":", "opt", ".", "test_attack_iters", ",", "\n", "'loss_func'", ":", "'ce'", ",", "\n", "'w_optm_lr'", ":", "opt", ".", "w_optm_lr", ",", "\n", "'sparse_weight'", ":", "opt", ".", "attack_sparse_weight", ",", "\n", "'out_type'", ":", "\"text\"", "\n", "}", "\n", "embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "input", "=", "text", ")", "#in bs, len sent, vocab", "\n", "n", ",", "l", ",", "s", "=", "text_like_syn", ".", "shape", "\n", "text_like_syn_embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "input", "=", "text_like_syn", ".", "reshape", "(", "n", ",", "l", "*", "s", ")", ")", ".", "reshape", "(", "n", ",", "l", ",", "s", ",", "-", "1", ")", "\n", "text_adv", "=", "model", "(", "mode", "=", "\"get_adv_by_convex_syn\"", ",", "input", "=", "embd", ",", "label", "=", "label", ",", "text_like_syn_embd", "=", "text_like_syn_embd", ",", "text_like_syn_valid", "=", "text_like_syn_valid", ",", "text_like_syn", "=", "text_like_syn", ",", "attack_type_dict", "=", "attack_type_dict", ",", "text_for_vis", "=", "text", ",", "record_for_vis", "=", "record_for_vis", ")", "\n", "predicted_adv", "=", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "input", "=", "text_adv", ")", "\n", "\n", "\n", "prob", ",", "idx", "=", "torch", ".", "max", "(", "predicted_adv", ",", "1", ")", "\n", "percision", "=", "(", "idx", "==", "label", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "", "", "model", ".", "train", "(", ")", "\n", "\n", "\n", "return", "np", ".", "mean", "(", "accuracy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.imdb_evaluation_adv_bert": [[208, 352], ["model.eval", "enumerate", "model.train", "numpy.mean", "print", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "len", "model", "model().reshape().permute", "model", "model", "torch.max", "torch.max", "torch.cuda.is_available", "torch.cuda.is_available", "accuracy.append", "accuracy.append", "model().reshape", "percision.data.item", "percision.data.numpy", "model", "batch[].to.permute().reshape", "batch[].to.reshape().repeat().permute().reshape", "batch[].to.reshape().repeat().permute().reshape", "batch[].to.permute", "batch[].to.reshape().repeat().permute", "batch[].to.reshape().repeat().permute", "batch[].to.reshape().repeat", "batch[].to.reshape().repeat", "batch[].to.reshape", "batch[].to.reshape"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train"], ["", "def", "imdb_evaluation_adv_bert", "(", "opt", ",", "device", ",", "model", ",", "test_iter", ",", "tokenizer", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "accuracy", "=", "[", "]", "\n", "record_for_vis", "=", "{", "}", "\n", "record_for_vis", "[", "\"comb_p_list\"", "]", "=", "[", "]", "\n", "record_for_vis", "[", "\"embd_syn_list\"", "]", "=", "[", "]", "\n", "record_for_vis", "[", "\"syn_valid_list\"", "]", "=", "[", "]", "\n", "record_for_vis", "[", "\"text_syn_list\"", "]", "=", "[", "]", "\n", "\n", "ad_loss_res", "=", "{", "}", "\n", "ad_acc_res", "=", "{", "}", "\n", "adcombp_acc_res", "=", "{", "}", "\n", "total", "=", "0", "\n", "\n", "#    batch= next(iter(test_iter))", "\n", "if", "opt", ".", "pert_set", "==", "\"convex_combination\"", ":", "\n", "        ", "print", "(", "\"ad test by convex_combination.\"", ")", "\n", "", "for", "index", ",", "batch", "in", "enumerate", "(", "test_iter", ")", ":", "\n", "        ", "text", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "label", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "text_like_syn", "=", "batch", "[", "2", "]", ".", "to", "(", "device", ")", "\n", "text_like_syn_valid", "=", "batch", "[", "3", "]", ".", "to", "(", "device", ")", "\n", "bert_mask", "=", "batch", "[", "4", "]", ".", "to", "(", "device", ")", "\n", "bert_token_id", "=", "batch", "[", "5", "]", ".", "to", "(", "device", ")", "\n", "\n", "batch_size", "=", "len", "(", "text", ")", "\n", "total", "+=", "batch_size", "\n", "\n", "attack_type_dict", "=", "{", "\n", "'num_steps'", ":", "opt", ".", "test_attack_iters", ",", "\n", "'loss_func'", ":", "'ce'", ",", "\n", "'w_optm_lr'", ":", "opt", ".", "bert_w_optm_lr", ",", "\n", "'sparse_weight'", ":", "opt", ".", "attack_sparse_weight", ",", "\n", "'out_type'", ":", "\"text\"", "\n", "}", "\n", "embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "input", "=", "text", ",", "bert_mask", "=", "bert_mask", ",", "bert_token_id", "=", "bert_token_id", ")", "#in bs, len sent, vocab", "\n", "n", ",", "l", ",", "s", "=", "text_like_syn", ".", "shape", "\n", "text_like_syn_embd", "=", "model", "(", "mode", "=", "\"text_to_embd\"", ",", "input", "=", "text_like_syn", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "reshape", "(", "n", "*", "s", ",", "l", ")", ",", "bert_mask", "=", "bert_mask", ".", "reshape", "(", "n", ",", "l", ",", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "s", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "reshape", "(", "n", "*", "s", ",", "l", ")", ",", "bert_token_id", "=", "bert_token_id", ".", "reshape", "(", "n", ",", "l", ",", "1", ")", ".", "repeat", "(", "1", ",", "1", ",", "s", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ".", "reshape", "(", "n", "*", "s", ",", "l", ")", ")", ".", "reshape", "(", "n", ",", "s", ",", "l", ",", "-", "1", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "text_adv", "=", "model", "(", "mode", "=", "\"get_adv_by_convex_syn\"", ",", "input", "=", "embd", ",", "label", "=", "label", ",", "text_like_syn_embd", "=", "text_like_syn_embd", ",", "text_like_syn_valid", "=", "text_like_syn_valid", ",", "text_like_syn", "=", "text_like_syn", ",", "attack_type_dict", "=", "attack_type_dict", ",", "bert_mask", "=", "bert_mask", ",", "bert_token_id", "=", "bert_token_id", ")", "\n", "predicted_adv", "=", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "input", "=", "text_adv", ",", "bert_mask", "=", "bert_mask", ",", "bert_token_id", "=", "bert_token_id", ")", "\n", "\n", "\n", "'''\n        #for lr in [1000,1200,1400,1600,1800,2000,2200,2400,2800,3000,3200,3400,3600,3800,4000,4200,4400,4600,4800,5000,5200,5400]:\n        for lr in [0.01,0.1,1,10,100,1000,2000,5000]:\n            #print(\"___________________________________\")\n            #print(lr)\n            attack_type_dict = {\n                'num_steps': opt.test_attack_iters,\n                'loss_func': 'ce',\n                'w_optm_lr': lr,\n                'sparse_weight': opt.attack_sparse_weight,\n                'out_type': \"loss\"\n            }\n            loss = model(mode=\"get_adv_by_convex_syn\", input=embd, label=label, text_like_syn_embd=text_like_syn_embd, text_like_syn_valid=text_like_syn_valid, text_like_syn=text_like_syn, attack_type_dict=attack_type_dict, bert_mask=bert_mask, bert_token_id=bert_token_id)\n            loss=loss.sum().data.item()\n            #print(loss)\n            if lr in ad_loss_res:\n                ad_loss_res[lr]+=loss\n            else:\n                ad_loss_res[lr]=loss\n            #print(\"___________________________________\")\n        print(ad_loss_res)\n        '''", "\n", "'''\n        print(\"___________________________________\")\n        for sp in [0.5 ,1,2, 4, 6, 8,10,12,50, 200]:\n            #print(lr)\n            attack_type_dict = {\n                'num_steps': opt.test_attack_iters,\n                'loss_func': 'ce',\n                'w_optm_lr': opt.bert_w_optm_lr,\n                'sparse_weight': sp,\n                'out_type': \"text\"\n            }\n\n            text_adv = model(mode=\"get_adv_by_convex_syn\", input=embd, label=label, text_like_syn_embd=text_like_syn_embd, text_like_syn_valid=text_like_syn_valid, text_like_syn=text_like_syn, attack_type_dict=attack_type_dict, bert_mask=bert_mask, bert_token_id=bert_token_id)\n            temp_predicted_adv = model(mode=\"text_to_logit\", input=text_adv, bert_mask=bert_mask, bert_token_id=bert_token_id)\n            prob, idx = torch.max(temp_predicted_adv, 1) \n            correct=(idx==label).float().sum()\n            #print(loss)\n            if sp in ad_acc_res:\n                ad_acc_res[sp]+=correct\n            else:\n                ad_acc_res[sp]=correct\n\n            attack_type_dict = {\n                'num_steps': opt.test_attack_iters,\n                'loss_func': 'ce',\n                'w_optm_lr': opt.bert_w_optm_lr,\n                'sparse_weight': sp,\n                'out_type': \"comb_p\"\n            }\n            adv_comb_p = model(mode=\"get_adv_by_convex_syn\", input=embd, label=label, text_like_syn_embd=text_like_syn_embd, text_like_syn_valid=text_like_syn_valid, text_like_syn=text_like_syn, attack_type_dict=attack_type_dict, bert_mask=bert_mask, bert_token_id=bert_token_id)\n            temp_predicted_adv = model(mode=\"text_syn_p_to_logit\", input=text_like_syn, comb_p=adv_comb_p, bert_mask=bert_mask, bert_token_id=bert_token_id)\n            prob, idx = torch.max(temp_predicted_adv, 1) \n            correct=(idx==label).float().sum()\n            #print(loss)\n            if sp in adcombp_acc_res:\n                adcombp_acc_res[sp]+=correct\n            else:\n                adcombp_acc_res[sp]=correct\n\n        for k in ad_acc_res:\n            print(k, ad_acc_res[k]/total)\n            print(k, adcombp_acc_res[k]/total)\n        print(\"___________________________________\")\n        '''", "\n", "\n", "\n", "\"\"\"\n        attack_type_dict = {\n            'num_steps': opt.test_attack_iters,\n            'loss_func': 'ce',\n            'w_optm_lr': opt.w_optm_lr,\n            'sparse_weight': opt.attack_sparse_weight,\n            'out_type': \"comb_p\"\n        }\n\n        with torch.no_grad():\n            embd = model(mode=\"text_to_embd\", input=text, bert_mask=bert_mask, bert_token_id=bert_token_id) #in bs, len sent, vocab\n        n,l,s = text_like_syn.shape\n        with torch.no_grad():\n            text_like_syn_embd = model(mode=\"text_to_embd\", input=text_like_syn.reshape(n*l,s), bert_mask=bert_mask.reshape(n,l,1).repeat(1,1,s).reshape(n*l,s), bert_token_id=bert_token_id.reshape(n,l,1).repeat(1,1,s).reshape(n*l,s)).reshape(n,l,s,-1)\n        adv_comb_p = model(mode=\"get_adv_by_convex_syn\", input=embd, label=label, text_like_syn_embd=text_like_syn_embd, text_like_syn_valid=text_like_syn_valid, attack_type_dict=attack_type_dict, bert_mask=bert_mask, bert_token_id=bert_token_id)\n            \n        predicted_adv = model(mode=\"text_syn_p_to_logit\", input=text_like_syn, comb_p=adv_comb_p, bert_mask=bert_mask, bert_token_id=bert_token_id)\n        \"\"\"", "\n", "\n", "#print(\"_________________________________\")", "\n", "#print(inverse_tokenize(tokenizer, text[0]))", "\n", "#print(inverse_tokenize(tokenizer, text_adv[0]))", "\n", "\n", "prob", ",", "idx", "=", "torch", ".", "max", "(", "predicted_adv", ",", "1", ")", "\n", "percision", "=", "(", "idx", "==", "label", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "", "", "model", ".", "train", "(", ")", "\n", "\n", "\n", "return", "np", ".", "mean", "(", "accuracy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.imdb_evaluation_hotflip_adv": [[353, 392], ["model.eval", "enumerate", "model.train", "numpy.mean", "batch[].to", "batch[].to", "batch[].to", "batch[].to", "len", "len", "model", "model", "torch.max", "torch.max", "torch.cuda.is_available", "torch.cuda.is_available", "accuracy.append", "accuracy.append", "percision.data.item", "percision.data.numpy"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train"], ["", "def", "imdb_evaluation_hotflip_adv", "(", "opt", ",", "device", ",", "model", ",", "test_iter", ",", "tokenizer", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "accuracy", "=", "[", "]", "\n", "\n", "for", "index", ",", "batch", "in", "enumerate", "(", "test_iter", ")", ":", "\n", "        ", "text", "=", "batch", "[", "0", "]", ".", "to", "(", "device", ")", "\n", "label", "=", "batch", "[", "1", "]", ".", "to", "(", "device", ")", "\n", "text_like_syn", "=", "batch", "[", "6", "]", ".", "to", "(", "device", ")", "\n", "text_like_syn_valid", "=", "batch", "[", "7", "]", ".", "to", "(", "device", ")", "\n", "\n", "batch_size", "=", "len", "(", "text", ")", "\n", "\n", "batch_size", "=", "len", "(", "text", ")", "\n", "if", "index", "*", "batch_size", ">", "200", ":", "\n", "            ", "break", "\n", "\n", "", "attack_type_dict", "=", "{", "\n", "'num_steps'", ":", "opt", ".", "test_attack_iters", ",", "\n", "'loss_func'", ":", "'ce'", ",", "\n", "}", "\n", "text_adv", "=", "model", "(", "mode", "=", "\"get_adv_hotflip\"", ",", "input", "=", "text", ",", "label", "=", "label", ",", "text_like_syn_valid", "=", "text_like_syn_valid", ",", "text_like_syn", "=", "text_like_syn", ",", "attack_type_dict", "=", "attack_type_dict", ")", "\n", "\n", "predicted_adv", "=", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "input", "=", "text_adv", ")", "\n", "\n", "#print(\"_________________________________\")", "\n", "#print(inverse_tokenize(tokenizer, text[0]))", "\n", "#print(inverse_tokenize(tokenizer, text_adv[0]))", "\n", "\n", "prob", ",", "idx", "=", "torch", ".", "max", "(", "predicted_adv", ",", "1", ")", "\n", "percision", "=", "(", "idx", "==", "label", ")", ".", "float", "(", ")", ".", "mean", "(", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "item", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "accuracy", ".", "append", "(", "percision", ".", "data", ".", "numpy", "(", ")", "[", "0", "]", ")", "\n", "", "", "model", ".", "train", "(", ")", "\n", "\n", "\n", "return", "np", ".", "mean", "(", "accuracy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.inverse_tokenize": [[393, 396], ["tokenizer.sequences_to_texts", "tokenized.cpu().numpy", "tokenized.cpu"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.sequences_to_texts"], ["", "def", "inverse_tokenize", "(", "tokenizer", ",", "tokenized", ")", ":", "\n", "    ", "result", "=", "tokenizer", ".", "sequences_to_texts", "(", "[", "tokenized", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "]", ")", "\n", "return", "result", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.getOptimizer": [[397, 444], ["name.lower().strip.lower().strip", "torch.optim.Adadelta().param_groups", "torch.optim.Adadelta().param_groups", "name.lower().strip.lower", "torch.optim.Adagrad", "torch.optim.Adagrad", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.Adadelta", "torch.optim.Adadelta", "torch.optim.SparseAdam", "torch.optim.SparseAdam", "torch.optim.lr_scheduler.StepLR", "torch.optim.lr_scheduler.StepLR", "torch.optim.Adamax", "torch.optim.Adamax", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.lr_scheduler.MultiStepLR", "torch.optim.ASGD", "torch.optim.ASGD", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.lr_scheduler.ReduceLROnPlateau", "torch.optim.LBFGS", "torch.optim.LBFGS", "torch.optim.RMSprop", "torch.optim.RMSprop", "torch.optim.Rprop", "torch.optim.Rprop", "torch.optim.SGD", "torch.optim.SGD", "torch.optim.Adam", "torch.optim.Adam", "print", "torch.optim.Adam", "torch.optim.Adam"], "function", ["None"], ["", "def", "getOptimizer", "(", "params", ",", "name", "=", "\"adam\"", ",", "lr", "=", "1", ",", "weight_decay", "=", "1e-4", ",", "momentum", "=", "None", ",", "scheduler", "=", "None", ")", ":", "\n", "\n", "    ", "name", "=", "name", ".", "lower", "(", ")", ".", "strip", "(", ")", "\n", "\n", "if", "name", "==", "\"adadelta\"", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adadelta", "(", "params", ",", "lr", "=", "1.0", "*", "lr", ",", "rho", "=", "0.9", ",", "eps", "=", "1e-06", ",", "weight_decay", "=", "weight_decay", ")", ".", "param_groups", "(", ")", "\n", "", "elif", "name", "==", "\"adagrad\"", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adagrad", "(", "params", ",", "lr", "=", "0.01", "*", "lr", ",", "lr_decay", "=", "0", ",", "weight_decay", "=", "weight_decay", ")", "\n", "", "elif", "name", "==", "\"sparseadam\"", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "SparseAdam", "(", "params", ",", "lr", "=", "0.001", "*", "lr", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-08", ")", "\n", "", "elif", "name", "==", "\"adamax\"", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adamax", "(", "params", ",", "lr", "=", "0.002", "*", "lr", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-08", ",", "weight_decay", "=", "weight_decay", ")", "\n", "", "elif", "name", "==", "\"asgd\"", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "ASGD", "(", "params", ",", "lr", "=", "0.01", "*", "lr", ",", "lambd", "=", "0.0001", ",", "alpha", "=", "0.75", ",", "t0", "=", "1000000.0", ",", "weight_decay", "=", "weight_decay", ")", "\n", "", "elif", "name", "==", "\"lbfgs\"", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "LBFGS", "(", "params", ",", "lr", "=", "1", "*", "lr", ",", "max_iter", "=", "20", ",", "max_eval", "=", "None", ",", "tolerance_grad", "=", "1e-05", ",", "tolerance_change", "=", "1e-09", ",", "history_size", "=", "100", ",", "line_search_fn", "=", "None", ")", "\n", "", "elif", "name", "==", "\"rmsprop\"", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "RMSprop", "(", "params", ",", "lr", "=", "0.01", "*", "lr", ",", "alpha", "=", "0.99", ",", "eps", "=", "1e-08", ",", "weight_decay", "=", "weight_decay", ",", "momentum", "=", "0", ",", "centered", "=", "False", ")", "\n", "", "elif", "name", "==", "\"rprop\"", ":", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Rprop", "(", "params", ",", "lr", "=", "0.01", "*", "lr", ",", "etas", "=", "(", "0.5", ",", "1.2", ")", ",", "step_sizes", "=", "(", "1e-06", ",", "50", ")", ")", "\n", "", "elif", "name", "==", "\"sgd\"", ":", "\n", "#optimizer=torch.optim.SGD(params, lr=lr, momentum=0.9, dampening=0, weight_decay=1e-4, nesterov=False)", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "params", ",", "lr", "=", "lr", ",", "momentum", "=", "0.9", ",", "weight_decay", "=", "weight_decay", ")", "\n", "", "elif", "name", "==", "\"adam\"", ":", "\n", "#optimizer=torch.optim.Adam(params, lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-4)", "\n", "        ", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "params", ",", "lr", "=", "lr", ",", "weight_decay", "=", "weight_decay", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"undefined optimizer, use adam in default\"", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "params", ",", "lr", "=", "0.1", "*", "lr", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-08", ",", "weight_decay", "=", "weight_decay", ")", "\n", "\n", "", "if", "scheduler", "is", "not", "None", ":", "\n", "        ", "if", "scheduler", "==", "\"lambdalr\"", ":", "\n", "            ", "lambda1", "=", "lambda", "epoch", ":", "epoch", "//", "30", "\n", "lambda2", "=", "lambda", "epoch", ":", "0.95", "**", "epoch", "\n", "return", "torch", ".", "optim", ".", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", "=", "[", "lambda1", ",", "lambda2", "]", ")", "\n", "", "elif", "scheduler", "==", "\"steplr\"", ":", "\n", "            ", "return", "torch", ".", "optim", ".", "lr_scheduler", ".", "StepLR", "(", "optimizer", ",", "step_size", "=", "30", ",", "gamma", "=", "0.1", ")", "\n", "", "elif", "scheduler", "==", "\"multisteplr\"", ":", "\n", "            ", "return", "torch", ".", "optim", ".", "lr_scheduler", ".", "MultiStepLR", "(", "optimizer", ",", "milestones", "=", "[", "30", ",", "80", "]", ",", "gamma", "=", "0.1", ")", "\n", "", "elif", "scheduler", "==", "\"reducelronplateau\"", ":", "\n", "            ", "return", "torch", ".", "optim", ".", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "optimizer", ",", "'min'", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "\n", "", "", "else", ":", "\n", "        ", "return", "optimizer", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.get_lr_scheduler": [[445, 448], ["None"], "function", ["None"], ["", "def", "get_lr_scheduler", "(", "name", ")", ":", "\n", "# todo ", "\n", "    ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.getLogger": [[451, 471], ["str", "int", "time.localtime", "time.strftime", "os.path.basename", "logging.getLogger", "logging.basicConfig", "logging.root.setLevel", "logging.getLogger.info", "random.randint", "time.time", "time.strftime", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.getLogger"], ["", "def", "getLogger", "(", ")", ":", "\n", "    ", "import", "random", "\n", "random_str", "=", "str", "(", "random", ".", "randint", "(", "1", ",", "10000", ")", ")", "\n", "\n", "now", "=", "int", "(", "time", ".", "time", "(", ")", ")", "\n", "timeArray", "=", "time", ".", "localtime", "(", "now", ")", "\n", "timeStamp", "=", "time", ".", "strftime", "(", "\"%Y%m%d%H%M%S\"", ",", "timeArray", ")", "\n", "log_filename", "=", "\"log/\"", "+", "time", ".", "strftime", "(", "\"%Y%m%d\"", ",", "timeArray", ")", "\n", "\n", "program", "=", "os", ".", "path", ".", "basename", "(", "sys", ".", "argv", "[", "0", "]", ")", "\n", "logger", "=", "logging", ".", "getLogger", "(", "program", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "\"log\"", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "\"log\"", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "log_filename", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "log_filename", ")", "\n", "", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s: %(levelname)s: %(message)s'", ",", "datefmt", "=", "'%a, %d %b %Y %H:%M:%S'", ",", "filename", "=", "log_filename", "+", "'/qa'", "+", "timeStamp", "+", "\"_\"", "+", "random_str", "+", "'.log'", ",", "filemode", "=", "'w'", ")", "\n", "logging", ".", "root", ".", "setLevel", "(", "level", "=", "logging", ".", "INFO", ")", "\n", "logger", ".", "info", "(", "\"running %s\"", "%", "' '", ".", "join", "(", "sys", ".", "argv", ")", ")", "\n", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.parse_grid_parameters": [[472, 493], ["configparser.ConfigParser", "configparser.ConfigParser.read", "config_common.items", "float.split", "re.compile", "float.strip", "re.compile.match", "new_array.append", "type", "int", "float", "eval"], "function", ["None"], ["", "def", "parse_grid_parameters", "(", "file_path", ")", ":", "\n", "    ", "config", "=", "configparser", ".", "ConfigParser", "(", ")", "\n", "config", ".", "read", "(", "file_path", ")", "\n", "config_common", "=", "config", "[", "'COMMON'", "]", "\n", "dictionary", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "config_common", ".", "items", "(", ")", ":", "\n", "        ", "array", "=", "value", ".", "split", "(", "';'", ")", "\n", "is_numberic", "=", "re", ".", "compile", "(", "r'^[-+]?[0-9.]+$'", ")", "\n", "new_array", "=", "[", "]", "\n", "\n", "for", "value", "in", "array", ":", "\n", "            ", "value", "=", "value", ".", "strip", "(", ")", "\n", "result", "=", "is_numberic", ".", "match", "(", "value", ")", "\n", "if", "result", ":", "\n", "                ", "if", "type", "(", "eval", "(", "value", ")", ")", "==", "int", ":", "\n", "                    ", "value", "=", "int", "(", "value", ")", "\n", "", "else", ":", "\n", "                    ", "value", "=", "float", "(", "value", ")", "\n", "", "", "new_array", ".", "append", "(", "value", ")", "\n", "", "dictionary", "[", "key", "]", "=", "new_array", "\n", "", "return", "dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.is_writeable": [[494, 520], ["os.path.dirname", "os.access", "os.access", "os.access", "os.access", "os.access", "os.access"], "function", ["None"], ["", "def", "is_writeable", "(", "path", ",", "check_parent", "=", "False", ")", ":", "\n", "    ", "'''\n    Check if a given path is writeable by the current user.\n    :param path: The path to check\n    :param check_parent: If the path to check does not exist, check for the\n    ability to write to the parent directory instead\n    :returns: True or False\n    '''", "\n", "if", "os", ".", "access", "(", "path", ",", "os", ".", "F_OK", ")", "and", "os", ".", "access", "(", "path", ",", "os", ".", "W_OK", ")", ":", "\n", "# The path exists and is writeable", "\n", "        ", "return", "True", "\n", "", "if", "os", ".", "access", "(", "path", ",", "os", ".", "F_OK", ")", "and", "not", "os", ".", "access", "(", "path", ",", "os", ".", "W_OK", ")", ":", "\n", "# The path exists and is not writeable", "\n", "        ", "return", "False", "\n", "# The path does not exists or is not writeable", "\n", "", "if", "check_parent", "is", "False", ":", "\n", "# We're not allowed to check the parent directory of the provided path", "\n", "        ", "return", "False", "\n", "# Lets get the parent directory of the provided path", "\n", "", "parent_dir", "=", "os", ".", "path", ".", "dirname", "(", "path", ")", "\n", "if", "not", "os", ".", "access", "(", "parent_dir", ",", "os", ".", "F_OK", ")", ":", "\n", "# Parent directory does not exit", "\n", "        ", "return", "False", "\n", "# Finally, return if we're allowed to write in the parent directory of the", "\n", "# provided path", "\n", "", "return", "os", ".", "access", "(", "parent_dir", ",", "os", ".", "W_OK", ")", "\n", "", "def", "is_readable", "(", "path", ")", ":", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.is_readable": [[520, 531], ["os.access", "os.access"], "function", ["None"], ["", "def", "is_readable", "(", "path", ")", ":", "\n", "    ", "'''\n    Check if a given path is readable by the current user.\n    :param path: The path to check\n    :returns: True or False\n    '''", "\n", "if", "os", ".", "access", "(", "path", ",", "os", ".", "F_OK", ")", "and", "os", ".", "access", "(", "path", ",", "os", ".", "R_OK", ")", ":", "\n", "# The path exists and is readable", "\n", "        ", "return", "True", "\n", "# The path does not exist", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.AttackSurface.get_swaps": [[11, 14], ["None"], "methods", ["None"], ["  ", "def", "get_swaps", "(", "self", ",", "words", ")", ":", "\n", "    ", "\"\"\"Return valid substitutions for each position in input |words|.\"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.WordSubstitutionAttackSurface.__init__": [[16, 19], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "neighbors", ",", "lm_scores", ")", ":", "\n", "    ", "self", ".", "neighbors", "=", "neighbors", "\n", "self", ".", "lm_scores", "=", "lm_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.WordSubstitutionAttackSurface.from_files": [[20, 36], ["cls", "open", "json.load", "open", "line.strip().split", "len", "toks[].lower", "collections.defaultdict", "line.strip", "int", "float"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_files", "(", "cls", ",", "neighbors_file", ",", "lm_file", ")", ":", "\n", "    ", "with", "open", "(", "neighbors_file", ")", "as", "f", ":", "\n", "      ", "neighbors", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "lm_file", ")", "as", "f", ":", "\n", "      ", "lm_scores", "=", "{", "}", "\n", "cur_sent", "=", "None", "\n", "for", "line", "in", "f", ":", "\n", "        ", "toks", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "toks", ")", "==", "2", ":", "\n", "          ", "cur_sent", "=", "toks", "[", "1", "]", ".", "lower", "(", ")", "\n", "lm_scores", "[", "cur_sent", "]", "=", "collections", ".", "defaultdict", "(", "dict", ")", "\n", "", "else", ":", "\n", "          ", "word_idx", ",", "word", ",", "score", "=", "int", "(", "toks", "[", "1", "]", ")", ",", "toks", "[", "2", "]", ",", "float", "(", "toks", "[", "3", "]", ")", "\n", "lm_scores", "[", "cur_sent", "]", "[", "word_idx", "]", "[", "word", "]", "=", "score", "\n", "", "", "", "return", "cls", "(", "neighbors", ",", "lm_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.WordSubstitutionAttackSurface.get_swaps": [[38, 47], ["range", "len", "swaps.append", "swaps.append"], "methods", ["None"], ["", "def", "get_swaps", "(", "self", ",", "words", ")", ":", "\n", "    ", "swaps", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "words", ")", ")", ":", "\n", "      ", "word", "=", "words", "[", "i", "]", "\n", "if", "word", "in", "self", ".", "neighbors", ":", "\n", "        ", "swaps", ".", "append", "(", "self", ".", "neighbors", "[", "word", "]", ")", "\n", "", "else", ":", "\n", "        ", "swaps", ".", "append", "(", "[", "]", ")", "\n", "", "", "return", "swaps", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.WordSubstitutionAttackSurface.check_in": [[48, 52], ["word.lower"], "methods", ["None"], ["", "def", "check_in", "(", "self", ",", "words", ")", ":", "\n", "    ", "words", "=", "[", "word", ".", "lower", "(", ")", "for", "word", "in", "words", "]", "\n", "s", "=", "' '", ".", "join", "(", "words", ")", "\n", "return", "s", "in", "self", ".", "lm_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.__init__": [[55, 59], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "neighbors", ",", "lm_scores", ",", "min_log_p_diff", "=", "DEFAULT_MAX_LOG_P_DIFF", ")", ":", "\n", "    ", "self", ".", "neighbors", "=", "neighbors", "\n", "self", ".", "lm_scores", "=", "lm_scores", "\n", "self", ".", "min_log_p_diff", "=", "min_log_p_diff", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.from_files": [[60, 76], ["cls", "open", "json.load", "open", "line.strip().split", "len", "toks[].lower", "collections.defaultdict", "line.strip", "int", "float"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_files", "(", "cls", ",", "neighbors_file", ",", "lm_file", ")", ":", "\n", "    ", "with", "open", "(", "neighbors_file", ")", "as", "f", ":", "\n", "      ", "neighbors", "=", "json", ".", "load", "(", "f", ")", "\n", "", "with", "open", "(", "lm_file", ",", "encoding", "=", "\"utf-8\"", ")", "as", "f", ":", "\n", "      ", "lm_scores", "=", "{", "}", "\n", "cur_sent", "=", "None", "\n", "for", "line", "in", "f", ":", "\n", "        ", "toks", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "len", "(", "toks", ")", "==", "2", ":", "\n", "          ", "cur_sent", "=", "toks", "[", "1", "]", ".", "lower", "(", ")", "\n", "lm_scores", "[", "cur_sent", "]", "=", "collections", ".", "defaultdict", "(", "dict", ")", "\n", "", "else", ":", "\n", "          ", "word_idx", ",", "word", ",", "score", "=", "int", "(", "toks", "[", "1", "]", ")", ",", "toks", "[", "2", "]", ",", "float", "(", "toks", "[", "3", "]", ")", "\n", "lm_scores", "[", "cur_sent", "]", "[", "word_idx", "]", "[", "word", "]", "=", "score", "\n", "", "", "", "return", "cls", "(", "neighbors", ",", "lm_scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.get_swaps": [[77, 96], ["range", "word.lower", "KeyError", "len", "[].items", "swaps.append", "swaps.append", "cur_swaps.append"], "methods", ["None"], ["", "def", "get_swaps", "(", "self", ",", "words", ")", ":", "\n", "    ", "swaps", "=", "[", "]", "\n", "words", "=", "[", "word", ".", "lower", "(", ")", "for", "word", "in", "words", "]", "\n", "s", "=", "' '", ".", "join", "(", "words", ")", "\n", "if", "s", "not", "in", "self", ".", "lm_scores", ":", "\n", "      ", "raise", "KeyError", "(", "'Unrecognized sentence \"%s\"'", "%", "s", ")", "\n", "", "for", "i", "in", "range", "(", "len", "(", "words", ")", ")", ":", "\n", "      ", "if", "i", "in", "self", ".", "lm_scores", "[", "s", "]", ":", "\n", "        ", "cur_swaps", "=", "[", "]", "\n", "orig_score", "=", "self", ".", "lm_scores", "[", "s", "]", "[", "i", "]", "[", "words", "[", "i", "]", "]", "\n", "for", "swap", ",", "score", "in", "self", ".", "lm_scores", "[", "s", "]", "[", "i", "]", ".", "items", "(", ")", ":", "\n", "          ", "if", "swap", "==", "words", "[", "i", "]", ":", "continue", "\n", "if", "swap", "not", "in", "self", ".", "neighbors", "[", "words", "[", "i", "]", "]", ":", "continue", "\n", "if", "score", "-", "orig_score", ">=", "self", ".", "min_log_p_diff", ":", "\n", "            ", "cur_swaps", ".", "append", "(", "swap", ")", "\n", "", "", "swaps", ".", "append", "(", "cur_swaps", ")", "\n", "", "else", ":", "\n", "        ", "swaps", ".", "append", "(", "[", "]", ")", "\n", "", "", "return", "swaps", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.check_in": [[97, 101], ["word.lower"], "methods", ["None"], ["", "def", "check_in", "(", "self", ",", "words", ")", ":", "\n", "    ", "words", "=", "[", "word", ".", "lower", "(", ")", "for", "word", "in", "words", "]", "\n", "s", "=", "' '", ".", "join", "(", "words", ")", "\n", "return", "s", "in", "self", ".", "lm_scores", "\n", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.vsm_similarity": [[74, 78], ["max", "doc[].similarity"], "function", ["None"], ["", "def", "vsm_similarity", "(", "doc", ",", "original", ",", "synonym", ")", ":", "\n", "    ", "window_size", "=", "3", "\n", "start", "=", "max", "(", "0", ",", "original", ".", "i", "-", "window_size", ")", "\n", "return", "doc", "[", "start", ":", "original", ".", "i", "+", "window_size", "]", ".", "similarity", "(", "synonym", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._get_wordnet_pos": [[80, 87], ["spacy_token.tag_[].lower"], "function", ["None"], ["", "def", "_get_wordnet_pos", "(", "spacy_token", ")", ":", "\n", "    ", "'''Wordnet POS tag'''", "\n", "pos", "=", "spacy_token", ".", "tag_", "[", "0", "]", ".", "lower", "(", ")", "\n", "if", "pos", "in", "[", "'r'", ",", "'n'", ",", "'v'", "]", ":", "# adv, noun, verb", "\n", "        ", "return", "pos", "\n", "", "elif", "pos", "==", "'j'", ":", "\n", "        ", "return", "'a'", "# adj", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._synonym_prefilter_fn": [[89, 100], ["len", "token.text.lower", "synonym.text.split"], "function", ["None"], ["", "", "def", "_synonym_prefilter_fn", "(", "token", ",", "synonym", ")", ":", "\n", "    ", "'''\n    Similarity heuristics go here\n    '''", "\n", "if", "(", "len", "(", "synonym", ".", "text", ".", "split", "(", ")", ")", ">", "2", "or", "(", "# the synonym produced is a phrase", "\n", "synonym", ".", "lemma", "==", "token", ".", "lemma", ")", "or", "(", "# token and synonym are the same", "\n", "synonym", ".", "tag", "!=", "token", ".", "tag", ")", "or", "(", "# the pos of the token synonyms are different", "\n", "token", ".", "text", ".", "lower", "(", ")", "==", "'be'", ")", ")", ":", "# token is be", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._generate_synonym_candidates": [[103, 141], ["paraphrase._get_wordnet_pos", "nltk.corpus.wordnet.synsets", "filter", "set", "enumerate", "wordnet_synonyms.extend", "filter.append", "functools.partial", "set.add", "paraphrase.SubstitutionCandidate", "candidates.append", "synset.lemmas", "nlp", "wordnet_synonym.name().replace", "wordnet_synonym.name"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._get_wordnet_pos", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["", "", "def", "_generate_synonym_candidates", "(", "token", ",", "token_position", ",", "rank_fn", "=", "None", ")", ":", "\n", "    ", "'''\n    Generate synonym candidates.\n    For each token in the doc, the list of WordNet synonyms is expanded.\n    :return candidates, a list, whose type of element is <class '__main__.SubstitutionCandidate'>\n            like SubstitutionCandidate(token_position=0, similarity_rank=10, original_token=Soft, candidate_word='subdued')\n    '''", "\n", "if", "rank_fn", "is", "None", ":", "\n", "        ", "rank_fn", "=", "vsm_similarity", "\n", "", "candidates", "=", "[", "]", "\n", "if", "token", ".", "tag_", "in", "supported_pos_tags", ":", "\n", "        ", "wordnet_pos", "=", "_get_wordnet_pos", "(", "token", ")", "# 'r', 'a', 'n', 'v' or None", "\n", "wordnet_synonyms", "=", "[", "]", "\n", "\n", "synsets", "=", "wn", ".", "synsets", "(", "token", ".", "text", ",", "pos", "=", "wordnet_pos", ")", "\n", "for", "synset", "in", "synsets", ":", "\n", "            ", "wordnet_synonyms", ".", "extend", "(", "synset", ".", "lemmas", "(", ")", ")", "\n", "\n", "", "synonyms", "=", "[", "]", "\n", "for", "wordnet_synonym", "in", "wordnet_synonyms", ":", "\n", "            ", "spacy_synonym", "=", "nlp", "(", "wordnet_synonym", ".", "name", "(", ")", ".", "replace", "(", "'_'", ",", "' '", ")", ")", "[", "0", "]", "\n", "synonyms", ".", "append", "(", "spacy_synonym", ")", "\n", "\n", "", "synonyms", "=", "filter", "(", "partial", "(", "_synonym_prefilter_fn", ",", "token", ")", ",", "synonyms", ")", "\n", "\n", "candidate_set", "=", "set", "(", ")", "\n", "for", "_", ",", "synonym", "in", "enumerate", "(", "synonyms", ")", ":", "\n", "            ", "candidate_word", "=", "synonym", ".", "text", "\n", "if", "candidate_word", "in", "candidate_set", ":", "# avoid repetition", "\n", "                ", "continue", "\n", "", "candidate_set", ".", "add", "(", "candidate_word", ")", "\n", "candidate", "=", "SubstitutionCandidate", "(", "\n", "token_position", "=", "token_position", ",", "\n", "similarity_rank", "=", "None", ",", "\n", "original_token", "=", "token", ",", "\n", "candidate_word", "=", "candidate_word", ")", "\n", "candidates", ".", "append", "(", "candidate", ")", "\n", "", "", "return", "candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.get_syn_dict": [[144, 149], ["open", "json.load"], "function", ["None"], ["", "def", "get_syn_dict", "(", "opt", ")", ":", "\n", "    ", "import", "json", "\n", "with", "open", "(", "opt", ".", "certified_neighbors_file_path", ")", "as", "f", ":", "\n", "        ", "syn_dict", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "syn_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._generate_synonym_candidates_from_dict": [[151, 187], ["open", "json.load", "paraphrase.SubstitutionCandidate", "candidates.append"], "function", ["None"], ["", "def", "_generate_synonym_candidates_from_dict", "(", "opt", ",", "token", ",", "token_position", ",", "rank_fn", "=", "None", ")", ":", "\n", "    ", "'''\n    Generate synonym candidates.\n    For each token in the doc, the list of WordNet synonyms is expanded.\n    :return candidates, a list, whose type of element is <class '__main__.SubstitutionCandidate'>\n            like SubstitutionCandidate(token_position=0, similarity_rank=10, original_token=Soft, candidate_word='subdued')\n    '''", "\n", "if", "rank_fn", "is", "None", ":", "\n", "        ", "rank_fn", "=", "vsm_similarity", "\n", "\n", "", "candidates", "=", "[", "]", "\n", "word", "=", "token", ".", "text", "\n", "\n", "\"\"\"\n    candidate = SubstitutionCandidate(\n        token_position=token_position,\n        similarity_rank=None,\n        original_token=token,\n        candidate_word=word)\n    candidates.append(candidate)\n    \"\"\"", "\n", "import", "json", "\n", "with", "open", "(", "opt", ".", "certified_neighbors_file_path", ")", "as", "f", ":", "\n", "        ", "syn_dict", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "if", "not", "word", "in", "syn_dict", ":", "\n", "        ", "return", "candidates", "\n", "", "else", ":", "\n", "        ", "for", "candidate_word", "in", "syn_dict", "[", "word", "]", ":", "\n", "            ", "candidate", "=", "SubstitutionCandidate", "(", "\n", "token_position", "=", "token_position", ",", "\n", "similarity_rank", "=", "None", ",", "\n", "original_token", "=", "token", ",", "\n", "candidate_word", "=", "candidate_word", ")", "\n", "candidates", ".", "append", "(", "candidate", ")", "\n", "", "return", "candidates", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.generate_synonym_list_from_word": [[188, 223], ["set", "list", "nlp", "paraphrase._get_wordnet_pos", "nltk.corpus.wordnet.synsets", "enumerate", "wordnet_synonyms.extend", "synonyms.append", "set.add", "synset.lemmas", "nlp", "wordnet_synonym.name().replace", "wordnet_synonym.name"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._get_wordnet_pos", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["", "", "def", "generate_synonym_list_from_word", "(", "word", ")", ":", "\n", "    ", "'''\n    Generate synonym candidates.\n    For each token in the doc, the list of WordNet synonyms is expanded.\n    :return candidates, a list, whose type of element is <class '__main__.SubstitutionCandidate'>\n            like SubstitutionCandidate(token_position=0, similarity_rank=10, original_token=Soft, candidate_word='subdued')\n    '''", "\n", "\n", "token", "=", "nlp", "(", "word", ")", "[", "0", "]", "\n", "\n", "candidate_set", "=", "set", "(", ")", "\n", "\n", "#if token.tag_ in supported_pos_tags:", "\n", "if", "True", ":", "\n", "        ", "wordnet_pos", "=", "_get_wordnet_pos", "(", "token", ")", "# 'r', 'a', 'n', 'v' or None", "\n", "wordnet_synonyms", "=", "[", "]", "\n", "\n", "synsets", "=", "wn", ".", "synsets", "(", "token", ".", "text", ")", "\n", "for", "synset", "in", "synsets", ":", "\n", "            ", "wordnet_synonyms", ".", "extend", "(", "synset", ".", "lemmas", "(", ")", ")", "\n", "\n", "", "synonyms", "=", "[", "]", "\n", "for", "wordnet_synonym", "in", "wordnet_synonyms", ":", "\n", "            ", "spacy_synonym", "=", "nlp", "(", "wordnet_synonym", ".", "name", "(", ")", ".", "replace", "(", "'_'", ",", "' '", ")", ")", "[", "0", "]", "\n", "synonyms", ".", "append", "(", "spacy_synonym", ")", "\n", "\n", "#synonyms = filter(partial(_synonym_prefilter_fn, token), synonyms)", "\n", "\n", "", "for", "_", ",", "synonym", "in", "enumerate", "(", "synonyms", ")", ":", "\n", "            ", "candidate_word", "=", "synonym", ".", "text", "\n", "if", "candidate_word", "in", "candidate_set", ":", "# avoid repetition", "\n", "                ", "continue", "\n", "", "candidate_set", ".", "add", "(", "candidate_word", ")", "\n", "\n", "", "", "return", "list", "(", "candidate_set", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.generate_synonym_list_by_dict": [[226, 238], ["None"], "function", ["None"], ["", "def", "generate_synonym_list_by_dict", "(", "syn_dict", ",", "word", ")", ":", "\n", "    ", "'''\n    Generate synonym candidates.\n    For each token in the doc, the list of WordNet synonyms is expanded.\n    :return candidates, a list, whose type of element is <class '__main__.SubstitutionCandidate'>\n            like SubstitutionCandidate(token_position=0, similarity_rank=10, original_token=Soft, candidate_word='subdued')\n    '''", "\n", "\n", "if", "not", "word", "in", "syn_dict", ":", "\n", "        ", "return", "[", "word", "]", "\n", "", "else", ":", "\n", "        ", "return", "syn_dict", "[", "word", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._compile_perturbed_tokens": [[240, 257], ["enumerate", "final_tokens.append", "candidate.candidate_word.replace"], "function", ["None"], ["", "", "def", "_compile_perturbed_tokens", "(", "doc", ",", "accepted_candidates", ")", ":", "\n", "    ", "'''\n    Traverse the list of accepted candidates and do the token substitutions.\n    '''", "\n", "candidate_by_position", "=", "{", "}", "\n", "for", "candidate", "in", "accepted_candidates", ":", "\n", "        ", "candidate_by_position", "[", "candidate", ".", "token_position", "]", "=", "candidate", "\n", "\n", "", "final_tokens", "=", "[", "]", "\n", "for", "position", ",", "token", "in", "enumerate", "(", "doc", ")", ":", "\n", "        ", "word", "=", "token", ".", "text", "\n", "if", "position", "in", "candidate_by_position", ":", "\n", "            ", "candidate", "=", "candidate_by_position", "[", "position", "]", "\n", "word", "=", "candidate", ".", "candidate_word", ".", "replace", "(", "'_'", ",", "' '", ")", "\n", "", "final_tokens", ".", "append", "(", "word", ")", "\n", "\n", "", "return", "final_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.PWWS": [[259, 424], ["numpy.array", "paraphrase.PWWS.softmax"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "PWWS", "(", "\n", "opt", ",", "\n", "doc", ",", "\n", "true_y", ",", "\n", "dataset", ",", "\n", "word_saliency_list", "=", "None", ",", "\n", "rank_fn", "=", "None", ",", "\n", "heuristic_fn", "=", "None", ",", "# Defined in adversarial_tools.py", "\n", "halt_condition_fn", "=", "None", ",", "# Defined in adversarial_tools.py", "\n", "verbose", "=", "True", ")", ":", "\n", "\n", "# defined in Eq.(8)", "\n", "    ", "def", "softmax", "(", "x", ")", ":", "\n", "        ", "exp_x", "=", "np", ".", "exp", "(", "x", ")", "\n", "softmax_x", "=", "exp_x", "/", "np", ".", "sum", "(", "exp_x", ")", "\n", "return", "softmax_x", "\n", "\n", "", "heuristic_fn", "=", "heuristic_fn", "or", "(", "lambda", "_", ",", "candidate", ":", "candidate", ".", "similarity_rank", ")", "\n", "halt_condition_fn", "=", "halt_condition_fn", "or", "(", "lambda", "perturbed_text", ":", "False", ")", "\n", "perturbed_doc", "=", "doc", "\n", "perturbed_text", "=", "perturbed_doc", ".", "text", "\n", "\n", "substitute_count", "=", "0", "# calculate how many substitutions used in a doc", "\n", "substitute_tuple_list", "=", "[", "]", "# save the information of substitute word", "\n", "\n", "word_saliency_array", "=", "np", ".", "array", "(", "[", "word_tuple", "[", "2", "]", "for", "word_tuple", "in", "word_saliency_list", "]", ")", "\n", "word_saliency_array", "=", "softmax", "(", "word_saliency_array", ")", "\n", "\n", "NE_candidates", "=", "NE_list", ".", "L", "[", "dataset", "]", "[", "true_y", "]", "\n", "\n", "NE_tags", "=", "list", "(", "NE_candidates", ".", "keys", "(", ")", ")", "\n", "use_NE", "=", "False", "# whether use NE as a substitute #Changed by dxs", "\n", "\n", "max_len", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "\n", "# for each word w_i in x, use WordNet to build a synonym set L_i", "\n", "for", "(", "position", ",", "token", ",", "word_saliency", ",", "tag", ")", "in", "word_saliency_list", ":", "\n", "        ", "if", "position", ">=", "max_len", ":", "\n", "            ", "break", "\n", "\n", "", "candidates", "=", "[", "]", "\n", "if", "use_NE", ":", "\n", "            ", "NER_tag", "=", "token", ".", "ent_type_", "\n", "if", "NER_tag", "in", "NE_tags", ":", "\n", "                ", "candidate", "=", "SubstitutionCandidate", "(", "position", ",", "0", ",", "token", ",", "NE_candidates", "[", "NER_tag", "]", ")", "\n", "candidates", ".", "append", "(", "candidate", ")", "\n", "", "else", ":", "\n", "#candidates = _generate_synonym_candidates(token=token, token_position=position, rank_fn=rank_fn)", "\n", "                ", "candidates", "=", "_generate_synonym_candidates_from_dict", "(", "opt", ",", "token", "=", "token", ",", "token_position", "=", "position", ",", "rank_fn", "=", "rank_fn", ")", "\n", "", "", "else", ":", "\n", "#candidates = _generate_synonym_candidates(token=token, token_position=position, rank_fn=rank_fn)", "\n", "            ", "candidates", "=", "_generate_synonym_candidates_from_dict", "(", "opt", ",", "token", "=", "token", ",", "token_position", "=", "position", ",", "rank_fn", "=", "rank_fn", ")", "\n", "\n", "", "if", "len", "(", "candidates", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "perturbed_text", "=", "perturbed_doc", ".", "text", "\n", "\n", "# The substitute word selection method R(w_i;L_i) defined in Eq.(4)", "\n", "sorted_candidates", "=", "zip", "(", "map", "(", "partial", "(", "heuristic_fn", ",", "doc", ".", "text", ")", ",", "candidates", ")", ",", "candidates", ")", "\n", "# Sorted according to the return value of heuristic_fn function, that is, \\Delta P defined in Eq.(4)", "\n", "sorted_candidates", "=", "list", "(", "sorted", "(", "sorted_candidates", ",", "key", "=", "lambda", "t", ":", "t", "[", "0", "]", ")", ")", "\n", "\n", "# delta_p_star is defined in Eq.(5); substitute is w_i^*", "\n", "delta_p_star", ",", "substitute", "=", "sorted_candidates", ".", "pop", "(", ")", "\n", "\n", "# delta_p_star * word_saliency_array[position] equals H(x, x_i^*, w_i) defined in Eq.(7)", "\n", "substitute_tuple_list", ".", "append", "(", "\n", "(", "position", ",", "token", ".", "text", ",", "substitute", ",", "delta_p_star", "*", "word_saliency_array", "[", "position", "]", ",", "token", ".", "tag_", ")", ")", "\n", "\n", "# sort all the words w_i in x in descending order based on H(x, x_i^*, w_i)", "\n", "", "sorted_substitute_tuple_list", "=", "sorted", "(", "substitute_tuple_list", ",", "key", "=", "lambda", "t", ":", "t", "[", "3", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "# replace w_i in x^(i-1) with w_i^* to craft x^(i)", "\n", "NE_count", "=", "0", "# calculate how many NE used in a doc", "\n", "change_tuple_list", "=", "[", "]", "\n", "\n", "substitute_list", "=", "[", "substitute", "for", "position", ",", "token", ",", "substitute", ",", "score", ",", "tag", "in", "sorted_substitute_tuple_list", "]", "\n", "substitute_count", "=", "len", "(", "substitute_list", ")", "\n", "perturbed_text_list", "=", "_compile_perturbed_tokens", "(", "perturbed_doc", ",", "substitute_list", ")", "\n", "\n", "def", "gen", "(", "perturbed_text_list", ")", ":", "\n", "        ", "perturbed_text", "=", "\"\"", "\n", "recur", "=", "0", "\n", "reduc", "=", "0", "\n", "for", "i", ",", "word_str", "in", "enumerate", "(", "perturbed_text_list", ")", ":", "\n", "\n", "            ", "if", "reduc", "==", "1", "or", "i", "==", "0", ":", "\n", "                ", "space", "=", "\"\"", "\n", "reduc", "=", "0", "\n", "", "else", ":", "\n", "                ", "space", "=", "\" \"", "\n", "\n", "", "if", "len", "(", "word_str", ")", "==", "1", "and", "word_str", "[", "0", "]", "in", "[", "\".\"", ",", "\",\"", ",", "\"-\"", ",", "\":\"", ",", "\"!\"", ",", "\"?\"", ",", "\"(\"", ",", "\")\"", ",", "\";\"", ",", "\"<\"", ",", "\">\"", ",", "\"{\"", ",", "\"}\"", ",", "\"[\"", ",", "\"]\"", "]", ":", "\n", "                ", "space", "=", "\"\"", "\n", "if", "word_str", "[", "0", "]", "in", "[", "\"(\"", ",", "\"<\"", ",", "\"{\"", ",", "\"[\"", "]", ":", "\n", "                    ", "reduc", "=", "1", "\n", "", "", "elif", "len", "(", "word_str", ")", "==", "1", "and", "word_str", "[", "0", "]", "in", "[", "\"\\\"\"", ",", "]", ":", "\n", "                ", "if", "recur", "==", "0", ":", "\n", "                    ", "space", "=", "\" \"", "\n", "reduc", "=", "1", "\n", "", "elif", "recur", "==", "1", ":", "\n", "                    ", "space", "=", "\"\"", "\n", "", "recur", "=", "(", "recur", "+", "1", ")", "%", "2", "\n", "", "elif", "len", "(", "word_str", ")", "==", "1", "and", "word_str", "[", "0", "]", "in", "[", "\"'\"", ",", "]", ":", "\n", "                ", "space", "=", "\"\"", "\n", "reduc", "=", "1", "\n", "\n", "", "perturbed_text", "+=", "(", "space", "+", "word_str", ")", "\n", "\n", "", "return", "perturbed_text", "\n", "\n", "", "perturbed_text", "=", "gen", "(", "perturbed_text_list", ")", "\n", "perturbed_doc", "=", "nlp", "(", "perturbed_text", ")", "\n", "\n", "\"\"\"\n    for (position, token, substitute, score, tag) in sorted_substitute_tuple_list:\n        # if score <= 0:\n        #     break\n        if nlp(token)[0].ent_type_ in NE_tags and use_NE:\n            NE_count += 1\n        change_tuple_list.append((position, token, substitute, score, tag))\n        #perturbed_text = ' '.join(_compile_perturbed_tokens(perturbed_doc, [substitute]))\n        perturbed_text_list = _compile_perturbed_tokens(perturbed_doc, [substitute])\n        perturbed_text = \"\" \n        recur = 0\n        reduc=0\n        for i, word_str in enumerate(perturbed_text_list):\n\n            if reduc==1 or i==0:\n                space = \"\"\n                reduc=0\n            else:\n                spece = \" \"\n\n            if len(word_str)==1 and word_str[0] in [\".\", \",\", \"-\", \":\", \"!\", \"?\", \"(\", \")\", \";\", \"<\", \">\", \"{\",\"}\", \"[\",\"]\"]:\n                space = \"\"\n                if word_str[0] in [ \"(\", \"<\", \"{\", \"[\"]:\n                    reduc=1\n            elif len(word_str)==1 and word_str[0] in [\"\\\"\",]:\n                if recur==0:\n                    space = \" \"\n                    reduc=1\n                elif recur==1:\n                    space = \"\"\n                recur=(recur+1)%2\n            elif len(word_str)==1 and word_str[0] in [\"'\",]:\n                space = \"\"\n                reduc=1\n            \n            perturbed_text+=(space+word_str)\n        \n        perturbed_doc = nlp(perturbed_text)\n        substitute_count += 1\n        if halt_condition_fn(perturbed_text):\n            if verbose:\n                print(\"use\", substitute_count, \"substitution; use\", NE_count, 'NE')\n            sub_rate = substitute_count / len(doc)\n            NE_rate = NE_count / substitute_count\n            return perturbed_text, sub_rate, NE_rate, change_tuple_list\n    \"\"\"", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "\"use\"", ",", "substitute_count", ",", "\"substitution; use\"", ",", "NE_count", ",", "'NE'", ")", "\n", "", "sub_rate", "=", "substitute_count", "/", "len", "(", "doc", ")", "\n", "NE_rate", "=", "NE_count", "/", "substitute_count", "\n", "return", "perturbed_text", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.PWWS_snli": [[426, 550], ["numpy.array", "paraphrase.PWWS.softmax"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "PWWS_snli", "(", "\n", "opt", ",", "\n", "doc_p", ",", "\n", "doc_h", ",", "\n", "true_y", ",", "\n", "dataset", ",", "\n", "word_saliency_list", "=", "None", ",", "\n", "rank_fn", "=", "None", ",", "\n", "heuristic_fn", "=", "None", ",", "# Defined in adversarial_tools.py", "\n", "halt_condition_fn", "=", "None", ",", "# Defined in adversarial_tools.py", "\n", "verbose", "=", "True", ")", ":", "\n", "\n", "# defined in Eq.(8)", "\n", "    ", "def", "softmax", "(", "x", ")", ":", "\n", "        ", "exp_x", "=", "np", ".", "exp", "(", "x", ")", "\n", "softmax_x", "=", "exp_x", "/", "np", ".", "sum", "(", "exp_x", ")", "\n", "return", "softmax_x", "\n", "\n", "", "heuristic_fn", "=", "heuristic_fn", "or", "(", "lambda", "_", ",", "candidate", ":", "candidate", ".", "similarity_rank", ")", "\n", "halt_condition_fn", "=", "halt_condition_fn", "or", "(", "lambda", "perturbed_text", ":", "False", ")", "\n", "perturbed_doc_h", "=", "doc_h", "\n", "perturbed_text_h", "=", "perturbed_doc_h", ".", "text", "\n", "text_p", "=", "doc_p", ".", "text", "\n", "\n", "substitute_count", "=", "0", "# calculate how many substitutions used in a doc", "\n", "substitute_tuple_list", "=", "[", "]", "# save the information of substitute word", "\n", "\n", "word_saliency_array", "=", "np", ".", "array", "(", "[", "word_tuple", "[", "2", "]", "for", "word_tuple", "in", "word_saliency_list", "]", ")", "\n", "word_saliency_array", "=", "softmax", "(", "word_saliency_array", ")", "\n", "\n", "#NE_candidates = NE_list.L[dataset][true_y]", "\n", "\n", "#NE_tags = list(NE_candidates.keys())", "\n", "use_NE", "=", "False", "# whether use NE as a substitute #Changed by dxs", "\n", "\n", "max_len", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "\n", "# for each word w_i in x, use WordNet to build a synonym set L_i", "\n", "for", "(", "position", ",", "token", ",", "word_saliency", ",", "tag", ")", "in", "word_saliency_list", ":", "\n", "        ", "if", "position", ">=", "max_len", ":", "\n", "            ", "break", "\n", "\n", "", "candidates", "=", "[", "]", "\n", "\"\"\"\n        if use_NE:\n            NER_tag = token.ent_type_\n            if NER_tag in NE_tags:\n                candidate = SubstitutionCandidate(position, 0, token, NE_candidates[NER_tag])\n                candidates.append(candidate)\n            else:\n                #candidates = _generate_synonym_candidates(token=token, token_position=position, rank_fn=rank_fn)\n                candidates = _generate_synonym_candidates_from_dict(token=token, token_position=position, rank_fn=rank_fn)\n        else:\n            #candidates = _generate_synonym_candidates(token=token, token_position=position, rank_fn=rank_fn)\n            candidates = _generate_synonym_candidates_from_dict(token=token, token_position=position, rank_fn=rank_fn)\n        \"\"\"", "\n", "candidates", "=", "_generate_synonym_candidates_from_dict", "(", "opt", ",", "token", "=", "token", ",", "token_position", "=", "position", ",", "rank_fn", "=", "rank_fn", ")", "\n", "\n", "if", "len", "(", "candidates", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "", "perturbed_text_h", "=", "perturbed_doc_h", ".", "text", "\n", "\n", "# The substitute word selection method R(w_i;L_i) defined in Eq.(4)", "\n", "sorted_candidates", "=", "zip", "(", "map", "(", "partial", "(", "heuristic_fn", ",", "doc_p", ".", "text", ",", "doc_h", ".", "text", ")", ",", "candidates", ")", ",", "candidates", ")", "\n", "# Sorted according to the return value of heuristic_fn function, that is, \\Delta P defined in Eq.(4)", "\n", "sorted_candidates", "=", "list", "(", "sorted", "(", "sorted_candidates", ",", "key", "=", "lambda", "t", ":", "t", "[", "0", "]", ")", ")", "\n", "\n", "# delta_p_star is defined in Eq.(5); substitute is w_i^*", "\n", "delta_p_star", ",", "substitute", "=", "sorted_candidates", ".", "pop", "(", ")", "\n", "\n", "# delta_p_star * word_saliency_array[position] equals H(x, x_i^*, w_i) defined in Eq.(7)", "\n", "substitute_tuple_list", ".", "append", "(", "\n", "(", "position", ",", "token", ".", "text", ",", "substitute", ",", "delta_p_star", "*", "word_saliency_array", "[", "position", "]", ",", "token", ".", "tag_", ")", ")", "\n", "\n", "# sort all the words w_i in x in descending order based on H(x, x_i^*, w_i)", "\n", "", "sorted_substitute_tuple_list", "=", "sorted", "(", "substitute_tuple_list", ",", "key", "=", "lambda", "t", ":", "t", "[", "3", "]", ",", "reverse", "=", "True", ")", "\n", "\n", "# replace w_i in x^(i-1) with w_i^* to craft x^(i)", "\n", "NE_count", "=", "0", "# calculate how many NE used in a doc", "\n", "change_tuple_list", "=", "[", "]", "\n", "\n", "substitute_list", "=", "[", "substitute", "for", "position", ",", "token", ",", "substitute", ",", "score", ",", "tag", "in", "sorted_substitute_tuple_list", "]", "\n", "substitute_count", "=", "len", "(", "substitute_list", ")", "\n", "perturbed_text_list", "=", "_compile_perturbed_tokens", "(", "perturbed_doc_h", ",", "substitute_list", ")", "\n", "\n", "def", "gen", "(", "perturbed_text_list", ")", ":", "\n", "        ", "perturbed_text", "=", "\"\"", "\n", "recur", "=", "0", "\n", "reduc", "=", "0", "\n", "for", "i", ",", "word_str", "in", "enumerate", "(", "perturbed_text_list", ")", ":", "\n", "\n", "            ", "if", "reduc", "==", "1", "or", "i", "==", "0", ":", "\n", "                ", "space", "=", "\"\"", "\n", "reduc", "=", "0", "\n", "", "else", ":", "\n", "                ", "space", "=", "\" \"", "\n", "\n", "", "if", "len", "(", "word_str", ")", "==", "1", "and", "word_str", "[", "0", "]", "in", "[", "\".\"", ",", "\",\"", ",", "\"-\"", ",", "\":\"", ",", "\"!\"", ",", "\"?\"", ",", "\"(\"", ",", "\")\"", ",", "\";\"", ",", "\"<\"", ",", "\">\"", ",", "\"{\"", ",", "\"}\"", ",", "\"[\"", ",", "\"]\"", "]", ":", "\n", "                ", "space", "=", "\"\"", "\n", "if", "word_str", "[", "0", "]", "in", "[", "\"(\"", ",", "\"<\"", ",", "\"{\"", ",", "\"[\"", "]", ":", "\n", "                    ", "reduc", "=", "1", "\n", "", "", "elif", "len", "(", "word_str", ")", "==", "1", "and", "word_str", "[", "0", "]", "in", "[", "\"\\\"\"", ",", "]", ":", "\n", "                ", "if", "recur", "==", "0", ":", "\n", "                    ", "space", "=", "\" \"", "\n", "reduc", "=", "1", "\n", "", "elif", "recur", "==", "1", ":", "\n", "                    ", "space", "=", "\"\"", "\n", "", "recur", "=", "(", "recur", "+", "1", ")", "%", "2", "\n", "", "elif", "len", "(", "word_str", ")", "==", "1", "and", "word_str", "[", "0", "]", "in", "[", "\"'\"", ",", "]", ":", "\n", "                ", "space", "=", "\"\"", "\n", "reduc", "=", "1", "\n", "\n", "", "perturbed_text", "+=", "(", "space", "+", "word_str", ")", "\n", "\n", "", "return", "perturbed_text", "\n", "\n", "", "perturbed_text_h", "=", "gen", "(", "perturbed_text_list", ")", "\n", "perturbed_doc_h", "=", "nlp", "(", "perturbed_text_h", ")", "\n", "\n", "if", "verbose", ":", "\n", "        ", "print", "(", "\"use\"", ",", "substitute_count", ",", "\"substitution; use\"", ",", "NE_count", ",", "'NE'", ")", "\n", "", "sub_rate", "=", "substitute_count", "/", "len", "(", "doc_h", ")", "\n", "NE_rate", "=", "0", "\n", "return", "text_p", ",", "perturbed_text_h", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool.write_origin_input_texts": [[44, 50], ["len", "open", "range", "f.write"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write"], ["def", "write_origin_input_texts", "(", "origin_input_texts_path", ",", "test_texts", ",", "test_samples_cap", "=", "None", ")", ":", "\n", "    ", "if", "test_samples_cap", "is", "None", ":", "\n", "        ", "test_samples_cap", "=", "len", "(", "test_texts", ")", "\n", "", "with", "open", "(", "origin_input_texts_path", ",", "'a'", ")", "as", "f", ":", "\n", "        ", "for", "i", "in", "range", "(", "test_samples_cap", ")", ":", "\n", "            ", "f", ".", "write", "(", "test_texts", "[", "i", "]", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool.fool_text_classifier": [[52, 153], ["print", "word_level_process.get_tokenizer", "neural_networks.lstm.load_weights", "print", "neural_networks.lstm.evaluate", "print", "neural_networks.lstm.evaluate", "print", "adversarial_tools.ForwardGradWrapper", "adversarial_tools.ForwardGradWrapper.predict_classes", "print", "time.clock", "open", "open", "enumerate", "time.clock", "print", "print", "print", "open.close", "open.close", "read_files.split_imdb_files", "str", "os.path.isfile", "fool.write_origin_input_texts", "neural_networks.word_cnn", "str", "str", "open.write", "sum", "len", "sum", "len", "word_level_process.word_process", "read_files.split_agnews_files", "neural_networks.bd_lstm", "numpy.argmax", "adversarial_tools.adversarial_paraphrase", "sub_rate_list.append", "NE_rate_list.append", "open.write", "char_level_process.char_process", "word_level_process.word_process", "read_files.split_yahoo_files", "neural_networks.char_cnn", "numpy.argmax", "print", "print", "char_level_process.char_process", "word_level_process.word_process", "neural_networks.lstm", "numpy.argmax", "str", "char_level_process.char_process", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_classes", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool_pytorch.write_origin_input_texts", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.word_cnn", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_agnews_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.bd_lstm", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.adversarial_paraphrase", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.char_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_yahoo_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.char_cnn", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.char_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.lstm", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.char_process"], ["", "", "", "def", "fool_text_classifier", "(", ")", ":", "\n", "    ", "clean_samples_cap", "=", "args", ".", "clean_samples_cap", "# 1000", "\n", "print", "(", "'clean_samples_cap:'", ",", "clean_samples_cap", ")", "\n", "\n", "# get tokenizer", "\n", "dataset", "=", "args", ".", "dataset", "\n", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "\n", "# Read data set", "\n", "x_test", "=", "y_test", "=", "None", "\n", "test_texts", "=", "None", "\n", "if", "dataset", "==", "'imdb'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "if", "args", ".", "level", "==", "'word'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "elif", "args", ".", "level", "==", "'char'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "char_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "", "elif", "dataset", "==", "'agnews'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_agnews_files", "(", ")", "\n", "if", "args", ".", "level", "==", "'word'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "elif", "args", ".", "level", "==", "'char'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "char_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "", "elif", "dataset", "==", "'yahoo'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_yahoo_files", "(", ")", "\n", "if", "args", ".", "level", "==", "'word'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "elif", "args", ".", "level", "==", "'char'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "char_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "\n", "# Write clean examples into a txt file", "\n", "", "", "clean_texts_path", "=", "r'./fool_result/{}/clean_{}.txt'", ".", "format", "(", "dataset", ",", "str", "(", "clean_samples_cap", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "clean_texts_path", ")", ":", "\n", "        ", "write_origin_input_texts", "(", "clean_texts_path", ",", "test_texts", ")", "\n", "\n", "# Select the model and load the trained weights", "\n", "", "assert", "args", ".", "model", "[", ":", "4", "]", "==", "args", ".", "level", "\n", "model", "=", "None", "\n", "if", "args", ".", "model", "==", "\"word_cnn\"", ":", "\n", "        ", "model", "=", "word_cnn", "(", "dataset", ")", "\n", "", "elif", "args", ".", "model", "==", "\"word_bdlstm\"", ":", "\n", "        ", "model", "=", "bd_lstm", "(", "dataset", ")", "\n", "", "elif", "args", ".", "model", "==", "\"char_cnn\"", ":", "\n", "        ", "model", "=", "char_cnn", "(", "dataset", ")", "\n", "", "elif", "args", ".", "model", "==", "\"word_lstm\"", ":", "\n", "        ", "model", "=", "lstm", "(", "dataset", ")", "\n", "", "model_path", "=", "r'./runs/{}/{}.dat'", ".", "format", "(", "dataset", ",", "args", ".", "model", ")", "\n", "model", ".", "load_weights", "(", "model_path", ")", "\n", "print", "(", "'model path:'", ",", "model_path", ")", "\n", "\n", "# evaluate classification accuracy of model on clean samples", "\n", "scores_origin", "=", "model", ".", "evaluate", "(", "x_test", "[", ":", "clean_samples_cap", "]", ",", "y_test", "[", ":", "clean_samples_cap", "]", ")", "\n", "print", "(", "'clean samples origin test_loss: %f, accuracy: %f'", "%", "(", "scores_origin", "[", "0", "]", ",", "scores_origin", "[", "1", "]", ")", ")", "\n", "all_scores_origin", "=", "model", ".", "evaluate", "(", "x_test", ",", "y_test", ")", "\n", "print", "(", "'all origin test_loss: %f, accuracy: %f'", "%", "(", "all_scores_origin", "[", "0", "]", ",", "all_scores_origin", "[", "1", "]", ")", ")", "\n", "\n", "grad_guide", "=", "ForwardGradWrapper", "(", "model", ")", "\n", "classes_prediction", "=", "grad_guide", ".", "predict_classes", "(", "x_test", "[", ":", "clean_samples_cap", "]", ")", "\n", "\n", "print", "(", "'Crafting adversarial examples...'", ")", "\n", "successful_perturbations", "=", "0", "\n", "failed_perturbations", "=", "0", "\n", "sub_rate_list", "=", "[", "]", "\n", "NE_rate_list", "=", "[", "]", "\n", "\n", "start_cpu", "=", "time", ".", "clock", "(", ")", "\n", "adv_text_path", "=", "r'./fool_result/{}/{}/adv_{}.txt'", ".", "format", "(", "dataset", ",", "args", ".", "model", ",", "str", "(", "clean_samples_cap", ")", ")", "\n", "change_tuple_path", "=", "r'./fool_result/{}/{}/change_tuple_{}.txt'", ".", "format", "(", "dataset", ",", "args", ".", "model", ",", "str", "(", "clean_samples_cap", ")", ")", "\n", "file_1", "=", "open", "(", "adv_text_path", ",", "\"a\"", ")", "\n", "file_2", "=", "open", "(", "change_tuple_path", ",", "\"a\"", ")", "\n", "for", "index", ",", "text", "in", "enumerate", "(", "test_texts", "[", ":", "clean_samples_cap", "]", ")", ":", "\n", "        ", "sub_rate", "=", "0", "\n", "NE_rate", "=", "0", "\n", "if", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", "==", "classes_prediction", "[", "index", "]", ":", "\n", "# If the ground_true label is the same as the predicted label", "\n", "            ", "adv_doc", ",", "adv_y", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "=", "adversarial_paraphrase", "(", "input_text", "=", "text", ",", "\n", "true_y", "=", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", ",", "\n", "grad_guide", "=", "grad_guide", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "dataset", "=", "dataset", ",", "\n", "level", "=", "args", ".", "level", ")", "\n", "if", "adv_y", "!=", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", ":", "\n", "                ", "successful_perturbations", "+=", "1", "\n", "print", "(", "'{}. Successful example crafted.'", ".", "format", "(", "index", ")", ")", "\n", "", "else", ":", "\n", "                ", "failed_perturbations", "+=", "1", "\n", "print", "(", "'{}. Failure.'", ".", "format", "(", "index", ")", ")", "\n", "\n", "", "text", "=", "adv_doc", "\n", "sub_rate_list", ".", "append", "(", "sub_rate", ")", "\n", "NE_rate_list", ".", "append", "(", "NE_rate", ")", "\n", "file_2", ".", "write", "(", "str", "(", "index", ")", "+", "str", "(", "change_tuple_list", ")", "+", "'\\n'", ")", "\n", "", "file_1", ".", "write", "(", "text", "+", "\" sub_rate: \"", "+", "str", "(", "sub_rate", ")", "+", "\"; NE_rate: \"", "+", "str", "(", "NE_rate", ")", "+", "\"\\n\"", ")", "\n", "", "end_cpu", "=", "time", ".", "clock", "(", ")", "\n", "print", "(", "'CPU second:'", ",", "end_cpu", "-", "start_cpu", ")", "\n", "mean_sub_rate", "=", "sum", "(", "sub_rate_list", ")", "/", "len", "(", "sub_rate_list", ")", "\n", "mean_NE_rate", "=", "sum", "(", "NE_rate_list", ")", "/", "len", "(", "NE_rate_list", ")", "\n", "print", "(", "'mean substitution rate:'", ",", "mean_sub_rate", ")", "\n", "print", "(", "'mean NE rate:'", ",", "mean_NE_rate", ")", "\n", "file_1", ".", "close", "(", ")", "\n", "file_2", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool.fool_text_classifier_pytorch": [[155, 222], ["print", "word_level_process.get_tokenizer", "ForwardGradWrapper_pytorch", "ForwardGradWrapper_pytorch.predict_classes", "print", "time.clock", "open", "open", "enumerate", "time.clock", "print", "print", "print", "open.close", "open.close", "read_files.split_imdb_files", "word_level_process.word_process", "str", "str", "open.write", "sum", "len", "sum", "len", "read_files.split_agnews_files", "word_level_process.word_process", "numpy.argmax", "adversarial_tools.adversarial_paraphrase", "sub_rate_list.append", "NE_rate_list.append", "open.write", "read_files.split_yahoo_files", "word_level_process.word_process", "numpy.argmax", "print", "print", "numpy.argmax", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_classes", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_agnews_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.adversarial_paraphrase", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_yahoo_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process"], ["", "def", "fool_text_classifier_pytorch", "(", "model", ",", "dataset", "=", "'imdb'", ")", ":", "\n", "    ", "clean_samples_cap", "=", "100", "\n", "print", "(", "'clean_samples_cap:'", ",", "clean_samples_cap", ")", "\n", "\n", "# get tokenizer", "\n", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "\n", "# Read data set", "\n", "x_test", "=", "y_test", "=", "None", "\n", "test_texts", "=", "None", "\n", "if", "dataset", "==", "'imdb'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "\n", "", "elif", "dataset", "==", "'agnews'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_agnews_files", "(", ")", "\n", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "\n", "", "elif", "dataset", "==", "'yahoo'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_yahoo_files", "(", ")", "\n", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "\n", "", "grad_guide", "=", "ForwardGradWrapper_pytorch", "(", "model", ")", "\n", "classes_prediction", "=", "grad_guide", ".", "predict_classes", "(", "x_test", "[", ":", "clean_samples_cap", "]", ")", "\n", "\n", "print", "(", "'Crafting adversarial examples...'", ")", "\n", "successful_perturbations", "=", "0", "\n", "failed_perturbations", "=", "0", "\n", "sub_rate_list", "=", "[", "]", "\n", "NE_rate_list", "=", "[", "]", "\n", "\n", "start_cpu", "=", "time", ".", "clock", "(", ")", "\n", "adv_text_path", "=", "r'./fool_result/{}/adv_{}.txt'", ".", "format", "(", "dataset", ",", "str", "(", "clean_samples_cap", ")", ")", "\n", "change_tuple_path", "=", "r'./fool_result/{}/change_tuple_{}.txt'", ".", "format", "(", "dataset", ",", "str", "(", "clean_samples_cap", ")", ")", "\n", "file_1", "=", "open", "(", "adv_text_path", ",", "\"a\"", ")", "\n", "file_2", "=", "open", "(", "change_tuple_path", ",", "\"a\"", ")", "\n", "for", "index", ",", "text", "in", "enumerate", "(", "test_texts", "[", ":", "clean_samples_cap", "]", ")", ":", "\n", "        ", "sub_rate", "=", "0", "\n", "NE_rate", "=", "0", "\n", "if", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", "==", "classes_prediction", "[", "index", "]", ":", "\n", "# If the ground_true label is the same as the predicted label", "\n", "            ", "adv_doc", ",", "adv_y", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "=", "adversarial_paraphrase", "(", "input_text", "=", "text", ",", "\n", "true_y", "=", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", ",", "\n", "grad_guide", "=", "grad_guide", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "dataset", "=", "dataset", ",", "\n", "level", "=", "'word'", ")", "\n", "if", "adv_y", "!=", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", ":", "\n", "                ", "successful_perturbations", "+=", "1", "\n", "print", "(", "'{}. Successful example crafted.'", ".", "format", "(", "index", ")", ")", "\n", "", "else", ":", "\n", "                ", "failed_perturbations", "+=", "1", "\n", "print", "(", "'{}. Failure.'", ".", "format", "(", "index", ")", ")", "\n", "\n", "", "text", "=", "adv_doc", "\n", "sub_rate_list", ".", "append", "(", "sub_rate", ")", "\n", "NE_rate_list", ".", "append", "(", "NE_rate", ")", "\n", "file_2", ".", "write", "(", "str", "(", "index", ")", "+", "str", "(", "change_tuple_list", ")", "+", "'\\n'", ")", "\n", "", "file_1", ".", "write", "(", "text", "+", "\" sub_rate: \"", "+", "str", "(", "sub_rate", ")", "+", "\"; NE_rate: \"", "+", "str", "(", "NE_rate", ")", "+", "\"\\n\"", ")", "\n", "", "end_cpu", "=", "time", ".", "clock", "(", ")", "\n", "print", "(", "'CPU second:'", ",", "end_cpu", "-", "start_cpu", ")", "\n", "mean_sub_rate", "=", "sum", "(", "sub_rate_list", ")", "/", "len", "(", "sub_rate_list", ")", "\n", "mean_NE_rate", "=", "sum", "(", "NE_rate_list", ")", "/", "len", "(", "NE_rate_list", ")", "\n", "print", "(", "'mean substitution rate:'", ",", "mean_sub_rate", ")", "\n", "print", "(", "'mean NE rate:'", ",", "mean_NE_rate", ")", "\n", "file_1", ".", "close", "(", ")", "\n", "file_2", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.get_NE_list.recognize_named_entity": [[42, 53], ["copy.deepcopy", "nlp"], "function", ["None"], ["def", "recognize_named_entity", "(", "texts", ")", ":", "\n", "    ", "'''\n    Returns all NEs in the input texts and their corresponding types\n    '''", "\n", "NE_freq_dict", "=", "copy", ".", "deepcopy", "(", "NE_type_dict", ")", "\n", "\n", "for", "text", "in", "texts", ":", "\n", "        ", "doc", "=", "nlp", "(", "text", ")", "\n", "for", "word", "in", "doc", ".", "ents", ":", "\n", "            ", "NE_freq_dict", "[", "word", ".", "label_", "]", "[", "word", ".", "text", "]", "+=", "1", "\n", "", "", "return", "NE_freq_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.get_NE_list.find_adv_NE": [[55, 72], ["NE_type_dict.keys", "enumerate", "enumerate", "print", "len", "open", "f.write", "other_NE.split"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write"], ["", "def", "find_adv_NE", "(", "D_true", ",", "D_other", ")", ":", "\n", "    ", "'''\n    find NE_adv in D-D_y_true which is defined in the end of section 3.1\n    '''", "\n", "# adv_NE_list = []", "\n", "for", "type", "in", "NE_type_dict", ".", "keys", "(", ")", ":", "\n", "# find the most frequent true and other NEs of the same type", "\n", "        ", "true_NE_list", "=", "[", "NE_tuple", "[", "0", "]", "for", "(", "i", ",", "NE_tuple", ")", "in", "enumerate", "(", "D_true", "[", "type", "]", ")", "if", "i", "<", "15", "]", "\n", "other_NE_list", "=", "[", "NE_tuple", "[", "0", "]", "for", "(", "i", ",", "NE_tuple", ")", "in", "enumerate", "(", "D_other", "[", "type", "]", ")", "if", "i", "<", "30", "]", "\n", "\n", "for", "other_NE", "in", "other_NE_list", ":", "\n", "            ", "if", "other_NE", "not", "in", "true_NE_list", "and", "len", "(", "other_NE", ".", "split", "(", ")", ")", "==", "1", ":", "\n", "# adv_NE_list.append((type, other_NE))", "\n", "                ", "print", "(", "\"'\"", "+", "type", "+", "\"': '\"", "+", "other_NE", "+", "\"',\"", ")", "\n", "with", "open", "(", "'./{}.txt'", ".", "format", "(", "args", ".", "dataset", ")", ",", "'a'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "                    ", "f", ".", "write", "(", "\"'\"", "+", "type", "+", "\"': '\"", "+", "other_NE", "+", "\"',\\n\"", ")", "\n", "", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.evaluate_fool_results.read_adversarial_file": [[40, 45], ["list", "open().readlines", "re.sub", "open"], "function", ["None"], ["def", "read_adversarial_file", "(", "adversarial_text_path", ")", ":", "\n", "    ", "adversarial_text", "=", "list", "(", "open", "(", "adversarial_text_path", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", ".", "readlines", "(", ")", ")", "\n", "# remove sub_rate and NE_rate at the end of the text", "\n", "adversarial_text", "=", "[", "re", ".", "sub", "(", "' sub_rate.*'", ",", "''", ",", "s", ")", "for", "s", "in", "adversarial_text", "]", "\n", "return", "adversarial_text", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.evaluate_fool_results.get_mean_sub_rate": [[47, 60], ["list", "enumerate", "sub_rate_list.sort", "open().readlines", "re.findall", "sum", "len", "len", "all_sub_rate.append", "sub_rate_list.append", "open", "float", "float"], "function", ["None"], ["", "def", "get_mean_sub_rate", "(", "adversarial_text_path", ")", ":", "\n", "    ", "adversarial_text", "=", "list", "(", "open", "(", "adversarial_text_path", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", ".", "readlines", "(", ")", ")", "\n", "all_sub_rate", "=", "[", "]", "\n", "sub_rate_list", "=", "[", "]", "\n", "for", "index", ",", "text", "in", "enumerate", "(", "adversarial_text", ")", ":", "\n", "        ", "sub_rate", "=", "re", ".", "findall", "(", "'\\d+.\\d+(?=; NE_rate)'", ",", "text", ")", "\n", "if", "len", "(", "sub_rate", ")", "!=", "0", ":", "\n", "            ", "sub_rate", "=", "sub_rate", "[", "0", "]", "\n", "all_sub_rate", ".", "append", "(", "float", "(", "sub_rate", ")", ")", "\n", "sub_rate_list", ".", "append", "(", "(", "index", ",", "float", "(", "sub_rate", ")", ")", ")", "\n", "", "", "mean_sub_rate", "=", "sum", "(", "all_sub_rate", ")", "/", "len", "(", "all_sub_rate", ")", "\n", "sub_rate_list", ".", "sort", "(", "key", "=", "lambda", "t", ":", "t", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "return", "mean_sub_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.evaluate_fool_results.get_mean_NE_rate": [[62, 74], ["list", "enumerate", "NE_rate_list.sort", "open().readlines", "text.split", "float", "all_NE_rate.append", "NE_rate_list.append", "sum", "len", "words[].replace", "open"], "function", ["None"], ["", "def", "get_mean_NE_rate", "(", "adversarial_text_path", ")", ":", "\n", "    ", "adversarial_text", "=", "list", "(", "open", "(", "adversarial_text_path", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", ".", "readlines", "(", ")", ")", "\n", "all_NE_rate", "=", "[", "]", "\n", "NE_rate_list", "=", "[", "]", "\n", "for", "index", ",", "text", "in", "enumerate", "(", "adversarial_text", ")", ":", "\n", "        ", "words", "=", "text", ".", "split", "(", "' '", ")", "\n", "NE_rate", "=", "float", "(", "words", "[", "-", "1", "]", ".", "replace", "(", "'\\n'", ",", "''", ")", ")", "\n", "all_NE_rate", ".", "append", "(", "NE_rate", ")", "\n", "NE_rate_list", ".", "append", "(", "(", "index", ",", "NE_rate", ")", ")", "\n", "", "mean_NE_rate", "=", "sum", "(", "all_NE_rate", ")", "/", "len", "(", "all_NE_rate", ")", "\n", "NE_rate_list", ".", "sort", "(", "key", "=", "lambda", "t", ":", "t", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "return", "mean_NE_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.__init__": [[175, 205], ["collections.OrderedDict", "collections.defaultdict", "collections.defaultdict", "warnings.warn", "kwargs.pop", "TypeError", "str"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "num_words", "=", "None", ",", "\n", "filters", "=", "'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'", ",", "\n", "lower", "=", "True", ",", "\n", "split", "=", "' '", ",", "\n", "char_level", "=", "False", ",", "\n", "oov_token", "=", "None", ",", "\n", "document_count", "=", "0", ",", "\n", "use_spacy", "=", "False", ",", "\n", "**", "kwargs", ")", ":", "\n", "# Legacy support", "\n", "        ", "if", "'nb_words'", "in", "kwargs", ":", "\n", "            ", "warnings", ".", "warn", "(", "'The `nb_words` argument in `Tokenizer` '", "\n", "'has been renamed `num_words`.'", ")", "\n", "num_words", "=", "kwargs", ".", "pop", "(", "'nb_words'", ")", "\n", "", "if", "kwargs", ":", "\n", "            ", "raise", "TypeError", "(", "'Unrecognized keyword arguments: '", "+", "str", "(", "kwargs", ")", ")", "\n", "\n", "", "self", ".", "word_counts", "=", "OrderedDict", "(", ")", "\n", "self", ".", "word_docs", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "filters", "=", "filters", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "lower", "=", "lower", "\n", "self", ".", "num_words", "=", "num_words", "\n", "self", ".", "document_count", "=", "document_count", "\n", "self", ".", "char_level", "=", "char_level", "\n", "self", ".", "oov_token", "=", "oov_token", "\n", "self", ".", "index_docs", "=", "defaultdict", "(", "int", ")", "\n", "self", ".", "word_index", "=", "{", "}", "\n", "self", ".", "index_word", "=", "{", "}", "\n", "self", ".", "use_spacy", "=", "use_spacy", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.fit_on_texts": [[206, 259], ["list", "list.sort", "sorted_voc.extend", "dict", "list", "tokenizer_for_spacy.transform_texts_fit_spacy", "set", "tokenizer_for_spacy.Tokenizer.word_counts.items", "six.moves.zip", "tokenizer_for_spacy.Tokenizer.word_docs.items", "isinstance", "tokenizer_for_spacy.text_to_word_sequence", "list", "tokenizer_for_spacy.Tokenizer.word_index.items", "isinstance", "six.moves.range", "text.lower.lower.lower", "text_elem.lower", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.transform_texts_fit_spacy", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.text_to_word_sequence"], ["", "def", "fit_on_texts", "(", "self", ",", "texts", ",", "freq_count", "=", "1", ")", ":", "\n", "        ", "\"\"\"Updates internal vocabulary based on a list of texts.\n        In the case where texts contains lists,\n        we assume each entry of the lists to be a token.\n        Required before using `texts_to_sequences` or `texts_to_matrix`.\n        # Arguments\n            texts: can be a list of strings,\n                a generator of strings (for memory-efficiency),\n                or a list of list of strings.\n        \"\"\"", "\n", "if", "self", ".", "use_spacy", ":", "\n", "            ", "texts", "=", "transform_texts_fit_spacy", "(", "texts", ")", "\n", "\n", "", "for", "text", "in", "texts", ":", "\n", "            ", "self", ".", "document_count", "+=", "freq_count", "\n", "if", "self", ".", "char_level", "or", "isinstance", "(", "text", ",", "list", ")", ":", "\n", "                ", "if", "self", ".", "lower", ":", "\n", "                    ", "if", "isinstance", "(", "text", ",", "list", ")", ":", "\n", "                        ", "text", "=", "[", "text_elem", ".", "lower", "(", ")", "for", "text_elem", "in", "text", "]", "\n", "", "else", ":", "\n", "                        ", "text", "=", "text", ".", "lower", "(", ")", "\n", "", "", "seq", "=", "text", "\n", "", "else", ":", "\n", "                ", "seq", "=", "text_to_word_sequence", "(", "text", ",", "\n", "self", ".", "filters", ",", "\n", "self", ".", "lower", ",", "\n", "self", ".", "split", ")", "\n", "", "for", "w", "in", "seq", ":", "\n", "                ", "if", "w", "in", "self", ".", "word_counts", ":", "\n", "                    ", "self", ".", "word_counts", "[", "w", "]", "+=", "freq_count", "\n", "", "else", ":", "\n", "                    ", "self", ".", "word_counts", "[", "w", "]", "=", "freq_count", "\n", "", "", "for", "w", "in", "set", "(", "seq", ")", ":", "\n", "# In how many documents each word occurs", "\n", "                ", "self", ".", "word_docs", "[", "w", "]", "+=", "freq_count", "\n", "\n", "", "", "wcounts", "=", "list", "(", "self", ".", "word_counts", ".", "items", "(", ")", ")", "\n", "wcounts", ".", "sort", "(", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "# forcing the oov_token to index 1 if it exists", "\n", "if", "self", ".", "oov_token", "is", "None", ":", "\n", "            ", "sorted_voc", "=", "[", "]", "\n", "", "else", ":", "\n", "            ", "sorted_voc", "=", "[", "self", ".", "oov_token", "]", "\n", "", "sorted_voc", ".", "extend", "(", "wc", "[", "0", "]", "for", "wc", "in", "wcounts", ")", "\n", "\n", "# note that index 0 is reserved, never assigned to an existing word", "\n", "self", ".", "word_index", "=", "dict", "(", "\n", "zip", "(", "sorted_voc", ",", "list", "(", "range", "(", "1", ",", "len", "(", "sorted_voc", ")", "+", "1", ")", ")", ")", ")", "\n", "\n", "self", ".", "index_word", "=", "{", "c", ":", "w", "for", "w", ",", "c", "in", "self", ".", "word_index", ".", "items", "(", ")", "}", "\n", "\n", "for", "w", ",", "c", "in", "list", "(", "self", ".", "word_docs", ".", "items", "(", ")", ")", ":", "\n", "            ", "self", ".", "index_docs", "[", "self", ".", "word_index", "[", "w", "]", "]", "=", "c", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.fit_on_sequences": [[260, 273], ["len", "set"], "methods", ["None"], ["", "", "def", "fit_on_sequences", "(", "self", ",", "sequences", ")", ":", "\n", "        ", "\"\"\"Updates internal vocabulary based on a list of sequences.\n        Required before using `sequences_to_matrix`\n        (if `fit_on_texts` was never called).\n        # Arguments\n            sequences: A list of sequence.\n                A \"sequence\" is a list of integer word indices.\n        \"\"\"", "\n", "self", ".", "document_count", "+=", "len", "(", "sequences", ")", "\n", "for", "seq", "in", "sequences", ":", "\n", "            ", "seq", "=", "set", "(", "seq", ")", "\n", "for", "i", "in", "seq", ":", "\n", "                ", "self", ".", "index_docs", "[", "i", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences": [[274, 284], ["list", "tokenizer_for_spacy.Tokenizer.texts_to_sequences_generator"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences_generator"], ["", "", "", "def", "texts_to_sequences", "(", "self", ",", "texts", ")", ":", "\n", "        ", "\"\"\"Transforms each text in texts to a sequence of integers.\n        Only top `num_words-1` most frequent words will be taken into account.\n        Only words known by the tokenizer will be taken into account.\n        # Arguments\n            texts: A list of texts (strings).\n        # Returns\n            A list of sequences.\n        \"\"\"", "\n", "return", "list", "(", "self", ".", "texts_to_sequences_generator", "(", "texts", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences_generator": [[285, 327], ["tokenizer_for_spacy.Tokenizer.word_index.get", "tokenizer_for_spacy.transform_texts_fit_spacy", "isinstance", "tokenizer_for_spacy.text_to_word_sequence", "tokenizer_for_spacy.Tokenizer.word_index.get", "isinstance", "text.lower.lower.lower", "vect.append", "vect.append", "text_elem.lower", "vect.append"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.transform_texts_fit_spacy", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.text_to_word_sequence"], ["", "def", "texts_to_sequences_generator", "(", "self", ",", "texts", ")", ":", "\n", "        ", "\"\"\"Transforms each text in `texts` to a sequence of integers.\n        Each item in texts can also be a list,\n        in which case we assume each item of that list to be a token.\n        Only top `num_words-1` most frequent words will be taken into account.\n        Only words known by the tokenizer will be taken into account.\n        # Arguments\n            texts: A list of texts (strings).\n        # Yields\n            Yields individual sequences.\n        \"\"\"", "\n", "num_words", "=", "self", ".", "num_words", "\n", "oov_token_index", "=", "self", ".", "word_index", ".", "get", "(", "self", ".", "oov_token", ")", "\n", "\n", "if", "self", ".", "use_spacy", ":", "\n", "            ", "texts", "=", "transform_texts_fit_spacy", "(", "texts", ")", "\n", "\n", "", "for", "text", "in", "texts", ":", "\n", "            ", "if", "self", ".", "char_level", "or", "isinstance", "(", "text", ",", "list", ")", ":", "\n", "                ", "if", "self", ".", "lower", ":", "\n", "                    ", "if", "isinstance", "(", "text", ",", "list", ")", ":", "\n", "                        ", "text", "=", "[", "text_elem", ".", "lower", "(", ")", "for", "text_elem", "in", "text", "]", "\n", "", "else", ":", "\n", "                        ", "text", "=", "text", ".", "lower", "(", ")", "\n", "", "", "seq", "=", "text", "\n", "", "else", ":", "\n", "                ", "seq", "=", "text_to_word_sequence", "(", "text", ",", "\n", "self", ".", "filters", ",", "\n", "self", ".", "lower", ",", "\n", "self", ".", "split", ")", "\n", "", "vect", "=", "[", "]", "\n", "for", "w", "in", "seq", ":", "\n", "                ", "i", "=", "self", ".", "word_index", ".", "get", "(", "w", ")", "\n", "if", "i", "is", "not", "None", ":", "\n", "                    ", "if", "num_words", "and", "i", ">=", "num_words", ":", "\n", "                        ", "if", "oov_token_index", "is", "not", "None", ":", "\n", "                            ", "vect", ".", "append", "(", "oov_token_index", ")", "\n", "", "", "else", ":", "\n", "                        ", "vect", ".", "append", "(", "i", ")", "\n", "", "", "elif", "self", ".", "oov_token", "is", "not", "None", ":", "\n", "                    ", "vect", ".", "append", "(", "oov_token_index", ")", "\n", "", "", "yield", "vect", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.sequences_to_texts": [[328, 338], ["list", "tokenizer_for_spacy.Tokenizer.sequences_to_texts_generator"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.sequences_to_texts_generator"], ["", "", "def", "sequences_to_texts", "(", "self", ",", "sequences", ")", ":", "\n", "        ", "\"\"\"Transforms each sequence into a list of text.\n        Only top `num_words-1` most frequent words will be taken into account.\n        Only words known by the tokenizer will be taken into account.\n        # Arguments\n            sequences: A list of sequences (list of integers).\n        # Returns\n            A list of texts (strings)\n        \"\"\"", "\n", "return", "list", "(", "self", ".", "sequences_to_texts_generator", "(", "sequences", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.sequences_to_texts_generator": [[339, 366], ["tokenizer_for_spacy.Tokenizer.word_index.get", "tokenizer_for_spacy.Tokenizer.index_word.get", "vect.append", "vect.append", "vect.append"], "methods", ["None"], ["", "def", "sequences_to_texts_generator", "(", "self", ",", "sequences", ")", ":", "\n", "        ", "\"\"\"Transforms each sequence in `sequences` to a list of texts(strings).\n        Each sequence has to a list of integers.\n        In other words, sequences should be a list of sequences\n        Only top `num_words-1` most frequent words will be taken into account.\n        Only words known by the tokenizer will be taken into account.\n        # Arguments\n            sequences: A list of sequences.\n        # Yields\n            Yields individual texts.\n        \"\"\"", "\n", "num_words", "=", "self", ".", "num_words", "\n", "oov_token_index", "=", "self", ".", "word_index", ".", "get", "(", "self", ".", "oov_token", ")", "\n", "for", "seq", "in", "sequences", ":", "\n", "            ", "vect", "=", "[", "]", "\n", "for", "num", "in", "seq", ":", "\n", "                ", "word", "=", "self", ".", "index_word", ".", "get", "(", "num", ")", "\n", "if", "word", "is", "not", "None", ":", "\n", "                    ", "if", "num_words", "and", "num", ">=", "num_words", ":", "\n", "                        ", "if", "oov_token_index", "is", "not", "None", ":", "\n", "                            ", "vect", ".", "append", "(", "self", ".", "index_word", "[", "oov_token_index", "]", ")", "\n", "", "", "else", ":", "\n", "                        ", "vect", ".", "append", "(", "word", ")", "\n", "", "", "elif", "self", ".", "oov_token", "is", "not", "None", ":", "\n", "                    ", "vect", ".", "append", "(", "self", ".", "index_word", "[", "oov_token_index", "]", ")", "\n", "", "", "vect", "=", "' '", ".", "join", "(", "vect", ")", "\n", "yield", "vect", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_matrix": [[367, 377], ["tokenizer_for_spacy.Tokenizer.texts_to_sequences", "tokenizer_for_spacy.Tokenizer.sequences_to_matrix"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.sequences_to_matrix"], ["", "", "def", "texts_to_matrix", "(", "self", ",", "texts", ",", "mode", "=", "'binary'", ")", ":", "\n", "        ", "\"\"\"Convert a list of texts to a Numpy matrix.\n        # Arguments\n            texts: list of strings.\n            mode: one of \"binary\", \"count\", \"tfidf\", \"freq\".\n        # Returns\n            A Numpy matrix.\n        \"\"\"", "\n", "sequences", "=", "self", ".", "texts_to_sequences", "(", "texts", ")", "\n", "return", "self", ".", "sequences_to_matrix", "(", "sequences", ",", "mode", "=", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.sequences_to_matrix": [[378, 429], ["numpy.zeros", "enumerate", "ValueError", "collections.defaultdict", "list", "ValueError", "len", "collections.defaultdict.items", "len", "len", "numpy.log", "ValueError", "numpy.log", "tokenizer_for_spacy.Tokenizer.index_docs.get"], "methods", ["None"], ["", "def", "sequences_to_matrix", "(", "self", ",", "sequences", ",", "mode", "=", "'binary'", ")", ":", "\n", "        ", "\"\"\"Converts a list of sequences into a Numpy matrix.\n        # Arguments\n            sequences: list of sequences\n                (a sequence is a list of integer word indices).\n            mode: one of \"binary\", \"count\", \"tfidf\", \"freq\"\n        # Returns\n            A Numpy matrix.\n        # Raises\n            ValueError: In case of invalid `mode` argument,\n                or if the Tokenizer requires to be fit to sample data.\n        \"\"\"", "\n", "if", "not", "self", ".", "num_words", ":", "\n", "            ", "if", "self", ".", "word_index", ":", "\n", "                ", "num_words", "=", "len", "(", "self", ".", "word_index", ")", "+", "1", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "'Specify a dimension (`num_words` argument), '", "\n", "'or fit on some text data first.'", ")", "\n", "", "", "else", ":", "\n", "            ", "num_words", "=", "self", ".", "num_words", "\n", "\n", "", "if", "mode", "==", "'tfidf'", "and", "not", "self", ".", "document_count", ":", "\n", "            ", "raise", "ValueError", "(", "'Fit the Tokenizer on some data '", "\n", "'before using tfidf mode.'", ")", "\n", "\n", "", "x", "=", "np", ".", "zeros", "(", "(", "len", "(", "sequences", ")", ",", "num_words", ")", ")", "\n", "for", "i", ",", "seq", "in", "enumerate", "(", "sequences", ")", ":", "\n", "            ", "if", "not", "seq", ":", "\n", "                ", "continue", "\n", "", "counts", "=", "defaultdict", "(", "int", ")", "\n", "for", "j", "in", "seq", ":", "\n", "                ", "if", "j", ">=", "num_words", ":", "\n", "                    ", "continue", "\n", "", "counts", "[", "j", "]", "+=", "1", "\n", "", "for", "j", ",", "c", "in", "list", "(", "counts", ".", "items", "(", ")", ")", ":", "\n", "                ", "if", "mode", "==", "'count'", ":", "\n", "                    ", "x", "[", "i", "]", "[", "j", "]", "=", "c", "\n", "", "elif", "mode", "==", "'freq'", ":", "\n", "                    ", "x", "[", "i", "]", "[", "j", "]", "=", "c", "/", "len", "(", "seq", ")", "\n", "", "elif", "mode", "==", "'binary'", ":", "\n", "                    ", "x", "[", "i", "]", "[", "j", "]", "=", "1", "\n", "", "elif", "mode", "==", "'tfidf'", ":", "\n", "# Use weighting scheme 2 in", "\n", "# https://en.wikipedia.org/wiki/Tf%E2%80%93idf", "\n", "                    ", "tf", "=", "1", "+", "np", ".", "log", "(", "c", ")", "\n", "idf", "=", "np", ".", "log", "(", "1", "+", "self", ".", "document_count", "/", "\n", "(", "1", "+", "self", ".", "index_docs", ".", "get", "(", "j", ",", "0", ")", ")", ")", "\n", "x", "[", "i", "]", "[", "j", "]", "=", "tf", "*", "idf", "\n", "", "else", ":", "\n", "                    ", "raise", "ValueError", "(", "'Unknown vectorization mode:'", ",", "mode", ")", "\n", "", "", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.get_config": [[430, 457], ["json.dumps", "json.dumps", "json.dumps", "json.dumps", "json.dumps"], "methods", ["None"], ["", "def", "get_config", "(", "self", ")", ":", "\n", "        ", "'''Returns the tokenizer configuration as Python dictionary.\n        The word count dictionaries used by the tokenizer get serialized\n        into plain JSON, so that the configuration can be read by other\n        projects.\n        # Returns\n            A Python dictionary with the tokenizer configuration.\n        '''", "\n", "json_word_counts", "=", "json", ".", "dumps", "(", "self", ".", "word_counts", ")", "\n", "json_word_docs", "=", "json", ".", "dumps", "(", "self", ".", "word_docs", ")", "\n", "json_index_docs", "=", "json", ".", "dumps", "(", "self", ".", "index_docs", ")", "\n", "json_word_index", "=", "json", ".", "dumps", "(", "self", ".", "word_index", ")", "\n", "json_index_word", "=", "json", ".", "dumps", "(", "self", ".", "index_word", ")", "\n", "\n", "return", "{", "\n", "'num_words'", ":", "self", ".", "num_words", ",", "\n", "'filters'", ":", "self", ".", "filters", ",", "\n", "'lower'", ":", "self", ".", "lower", ",", "\n", "'split'", ":", "self", ".", "split", ",", "\n", "'char_level'", ":", "self", ".", "char_level", ",", "\n", "'oov_token'", ":", "self", ".", "oov_token", ",", "\n", "'document_count'", ":", "self", ".", "document_count", ",", "\n", "'word_counts'", ":", "json_word_counts", ",", "\n", "'word_docs'", ":", "json_word_docs", ",", "\n", "'index_docs'", ":", "json_index_docs", ",", "\n", "'index_word'", ":", "json_index_word", ",", "\n", "'word_index'", ":", "json_word_index", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.to_json": [[459, 475], ["tokenizer_for_spacy.Tokenizer.get_config", "json.dumps"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.get_config"], ["", "def", "to_json", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Returns a JSON string containing the tokenizer configuration.\n        To load a tokenizer from a JSON string, use\n        `keras.preprocessing.text.tokenizer_from_json(json_string)`.\n        # Arguments\n            **kwargs: Additional keyword arguments\n                to be passed to `json.dumps()`.\n        # Returns\n            A JSON string containing the tokenizer configuration.\n        \"\"\"", "\n", "config", "=", "self", ".", "get_config", "(", ")", "\n", "tokenizer_config", "=", "{", "\n", "'class_name'", ":", "self", ".", "__class__", ".", "__name__", ",", "\n", "'config'", ":", "config", "\n", "}", "\n", "return", "json", ".", "dumps", "(", "tokenizer_config", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.text_to_word_sequence": [[30, 67], ["text.replace.split", "text.replace.lower", "isinstance", "maketrans", "text.replace.translate", "text.replace.translate", "ord", "unicode", "len", "maketrans", "text.replace.translate", "text.replace.replace", "len"], "function", ["None"], ["", "def", "text_to_word_sequence", "(", "text", ",", "\n", "filters", "=", "'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'", ",", "\n", "lower", "=", "True", ",", "split", "=", "\" \"", ")", ":", "\n", "    ", "\"\"\"Converts a text to a sequence of words (or tokens).\n    # Arguments\n        text: Input text (string).\n        filters: list (or concatenation) of characters to filter out, such as\n            punctuation. Default: ``!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\\\t\\\\n``,\n            includes basic punctuation, tabs, and newlines.\n        lower: boolean. Whether to convert the input to lowercase.\n        split: str. Separator for word splitting.\n    # Returns\n        A list of words (or tokens).\n    \"\"\"", "\n", "\n", "if", "lower", ":", "\n", "        ", "text", "=", "text", ".", "lower", "(", ")", "\n", "\n", "", "if", "sys", ".", "version_info", "<", "(", "3", ",", ")", ":", "\n", "        ", "if", "isinstance", "(", "text", ",", "unicode", ")", ":", "# noqa: F821", "\n", "            ", "translate_map", "=", "{", "\n", "ord", "(", "c", ")", ":", "unicode", "(", "split", ")", "for", "c", "in", "filters", "# noqa: F821", "\n", "}", "\n", "text", "=", "text", ".", "translate", "(", "translate_map", ")", "\n", "", "elif", "len", "(", "split", ")", "==", "1", ":", "\n", "            ", "translate_map", "=", "maketrans", "(", "filters", ",", "split", "*", "len", "(", "filters", ")", ")", "\n", "text", "=", "text", ".", "translate", "(", "translate_map", ")", "\n", "", "else", ":", "\n", "            ", "for", "c", "in", "filters", ":", "\n", "                ", "text", "=", "text", ".", "replace", "(", "c", ",", "split", ")", "\n", "", "", "", "else", ":", "\n", "        ", "translate_dict", "=", "{", "c", ":", "split", "for", "c", "in", "filters", "}", "\n", "translate_map", "=", "maketrans", "(", "translate_dict", ")", "\n", "text", "=", "text", ".", "translate", "(", "translate_map", ")", "\n", "\n", "", "seq", "=", "text", ".", "split", "(", "split", ")", "\n", "return", "[", "i", "for", "i", "in", "seq", "if", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.one_hot": [[69, 93], ["tokenizer_for_spacy.hashing_trick"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.hashing_trick"], ["", "def", "one_hot", "(", "text", ",", "n", ",", "\n", "filters", "=", "'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'", ",", "\n", "lower", "=", "True", ",", "\n", "split", "=", "' '", ")", ":", "\n", "    ", "\"\"\"One-hot encodes a text into a list of word indexes of size n.\n    This is a wrapper to the `hashing_trick` function using `hash` as the\n    hashing function; unicity of word to index mapping non-guaranteed.\n    # Arguments\n        text: Input text (string).\n        n: int. Size of vocabulary.\n        filters: list (or concatenation) of characters to filter out, such as\n            punctuation. Default: ``!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\\\t\\\\n``,\n            includes basic punctuation, tabs, and newlines.\n        lower: boolean. Whether to set the text to lowercase.\n        split: str. Separator for word splitting.\n    # Returns\n        List of integers in [1, n]. Each integer encodes a word\n        (unicity non-guaranteed).\n    \"\"\"", "\n", "return", "hashing_trick", "(", "text", ",", "n", ",", "\n", "hash_function", "=", "hash", ",", "\n", "filters", "=", "filters", ",", "\n", "lower", "=", "lower", ",", "\n", "split", "=", "split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.hashing_trick": [[95, 135], ["tokenizer_for_spacy.text_to_word_sequence", "int", "tokenizer_for_spacy.hashing_trick.hash_function"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.text_to_word_sequence"], ["", "def", "hashing_trick", "(", "text", ",", "n", ",", "\n", "hash_function", "=", "None", ",", "\n", "filters", "=", "'!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'", ",", "\n", "lower", "=", "True", ",", "\n", "split", "=", "' '", ")", ":", "\n", "    ", "\"\"\"Converts a text to a sequence of indexes in a fixed-size hashing space.\n    # Arguments\n        text: Input text (string).\n        n: Dimension of the hashing space.\n        hash_function: defaults to python `hash` function, can be 'md5' or\n            any function that takes in input a string and returns a int.\n            Note that 'hash' is not a stable hashing function, so\n            it is not consistent across different runs, while 'md5'\n            is a stable hashing function.\n        filters: list (or concatenation) of characters to filter out, such as\n            punctuation. Default: ``!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\\\t\\\\n``,\n            includes basic punctuation, tabs, and newlines.\n        lower: boolean. Whether to set the text to lowercase.\n        split: str. Separator for word splitting.\n    # Returns\n        A list of integer word indices (unicity non-guaranteed).\n    `0` is a reserved index that won't be assigned to any word.\n    Two or more words may be assigned to the same index, due to possible\n    collisions by the hashing function.\n    The [probability](\n        https://en.wikipedia.org/wiki/Birthday_problem#Probability_table)\n    of a collision is in relation to the dimension of the hashing space and\n    the number of distinct objects.\n    \"\"\"", "\n", "if", "hash_function", "is", "None", ":", "\n", "        ", "hash_function", "=", "hash", "\n", "", "elif", "hash_function", "==", "'md5'", ":", "\n", "        ", "def", "hash_function", "(", "w", ")", ":", "\n", "            ", "return", "int", "(", "md5", "(", "w", ".", "encode", "(", ")", ")", ".", "hexdigest", "(", ")", ",", "16", ")", "\n", "\n", "", "", "seq", "=", "text_to_word_sequence", "(", "text", ",", "\n", "filters", "=", "filters", ",", "\n", "lower", "=", "lower", ",", "\n", "split", "=", "split", ")", "\n", "return", "[", "(", "hash_function", "(", "w", ")", "%", "(", "n", "-", "1", ")", "+", "1", ")", "for", "w", "in", "seq", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.transform_texts_fit_spacy": [[136, 148], ["enumerate", "nlp", "res_texts.append"], "function", ["None"], ["", "def", "transform_texts_fit_spacy", "(", "texts", ")", ":", "\n", "\n", "    ", "res_texts", "=", "[", "]", "\n", "\n", "for", "i", ",", "text", "in", "enumerate", "(", "texts", ")", ":", "\n", "        ", "text", "=", "nlp", "(", "text", ")", "\n", "res", "=", "\"\"", "\n", "for", "token", "in", "text", ":", "\n", "            ", "res", "+=", "(", "\" \"", "+", "token", ".", "text", ")", "\n", "", "res_texts", ".", "append", "(", "res", ")", "\n", "\n", "", "return", "res_texts", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.tokenizer_from_json": [[477, 505], ["json.loads", "json.loads.get", "json.loads", "json.loads", "json.loads", "json.loads", "json.loads", "tokenizer_for_spacy.Tokenizer", "tokenizer_config.get.pop", "tokenizer_config.get.pop", "tokenizer_config.get.pop", "int", "tokenizer_config.get.pop", "int", "tokenizer_config.get.pop", "json.loads.items", "json.loads.items"], "function", ["None"], ["", "", "def", "tokenizer_from_json", "(", "json_string", ")", ":", "\n", "    ", "\"\"\"Parses a JSON tokenizer configuration file and returns a\n    tokenizer instance.\n    # Arguments\n        json_string: JSON string encoding a tokenizer configuration.\n    # Returns\n        A Keras Tokenizer instance\n    \"\"\"", "\n", "tokenizer_config", "=", "json", ".", "loads", "(", "json_string", ")", "\n", "config", "=", "tokenizer_config", ".", "get", "(", "'config'", ")", "\n", "\n", "word_counts", "=", "json", ".", "loads", "(", "config", ".", "pop", "(", "'word_counts'", ")", ")", "\n", "word_docs", "=", "json", ".", "loads", "(", "config", ".", "pop", "(", "'word_docs'", ")", ")", "\n", "index_docs", "=", "json", ".", "loads", "(", "config", ".", "pop", "(", "'index_docs'", ")", ")", "\n", "# Integer indexing gets converted to strings with json.dumps()", "\n", "index_docs", "=", "{", "int", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "index_docs", ".", "items", "(", ")", "}", "\n", "index_word", "=", "json", ".", "loads", "(", "config", ".", "pop", "(", "'index_word'", ")", ")", "\n", "index_word", "=", "{", "int", "(", "k", ")", ":", "v", "for", "k", ",", "v", "in", "index_word", ".", "items", "(", ")", "}", "\n", "word_index", "=", "json", ".", "loads", "(", "config", ".", "pop", "(", "'word_index'", ")", ")", "\n", "\n", "tokenizer", "=", "Tokenizer", "(", "**", "config", ")", "\n", "tokenizer", ".", "word_counts", "=", "word_counts", "\n", "tokenizer", ".", "word_docs", "=", "word_docs", "\n", "tokenizer", ".", "index_docs", "=", "index_docs", "\n", "tokenizer", ".", "word_index", "=", "word_index", "\n", "tokenizer", ".", "index_word", "=", "index_word", "\n", "\n", "return", "tokenizer", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_index": [[12, 26], ["open", "open.close", "print", "line.split", "numpy.asarray", "len", "print"], "function", ["None"], ["def", "get_embedding_index", "(", "file_path", ",", "embd_dim", ")", ":", "\n", "    ", "global", "embeddings_index", "\n", "embeddings_index", "=", "{", "}", "\n", "f", "=", "open", "(", "file_path", ",", "encoding", "=", "\"utf-8\"", ")", "\n", "for", "line", "in", "f", ":", "\n", "        ", "values", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "\"\"", ".", "join", "(", "values", "[", ":", "-", "embd_dim", "]", ")", "\n", "try", ":", "\n", "            ", "coefs", "=", "np", ".", "asarray", "(", "values", "[", "-", "embd_dim", ":", "]", ",", "dtype", "=", "'float32'", ")", "\n", "", "except", ":", "\n", "            ", "print", "(", "values", ")", "\n", "", "embeddings_index", "[", "word", "]", "=", "coefs", "\n", "", "f", ".", "close", "(", ")", "\n", "print", "(", "'Found %s word vectors.'", "%", "len", "(", "embeddings_index", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_matrix": [[28, 42], ["print", "numpy.zeros", "word_index.items", "word_level_process.get_tokenizer", "embeddings_index.get"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer"], ["", "def", "get_embedding_matrix", "(", "opt", ",", "dataset", ",", "num_words", ",", "embedding_dims", ")", ":", "\n", "# global num_words, embedding_matrix, word_index", "\n", "    ", "global", "embedding_matrix", ",", "word_index", "\n", "word_index", "=", "get_tokenizer", "(", "opt", ")", ".", "word_index", "\n", "print", "(", "'Preparing embedding matrix.'", ")", "\n", "# num_words = min(num_words, len(word_index))", "\n", "embedding_matrix", "=", "np", ".", "zeros", "(", "(", "num_words", "+", "1", ",", "embedding_dims", ")", ")", "\n", "for", "word", ",", "i", "in", "word_index", ".", "items", "(", ")", ":", "\n", "        ", "if", "i", ">", "num_words", ":", "\n", "            ", "continue", "\n", "", "embedding_vector", "=", "embeddings_index", ".", "get", "(", "word", ")", "\n", "if", "embedding_vector", "is", "not", "None", ":", "\n", "            ", "embedding_matrix", "[", "i", "]", "=", "embedding_vector", "\n", "", "", "return", "embedding_matrix", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.word_cnn": [[44, 93], ["print", "keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "neural_networks.get_embedding_index", "neural_networks.get_embedding_matrix", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.layers.Dropout", "keras.layers.Conv1D", "keras.layers.GlobalMaxPooling1D", "keras.layers.Dense", "keras.layers.Dropout", "keras.layers.Activation", "keras.layers.Dense", "keras.layers.Activation", "str", "keras.layers.Embedding", "keras.layers.Embedding"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_index", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_matrix", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["", "def", "word_cnn", "(", "dataset", ",", "use_glove", "=", "False", ")", ":", "\n", "    ", "filters", "=", "250", "\n", "kernel_size", "=", "3", "\n", "hidden_dims", "=", "250", "\n", "\n", "max_len", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "num_classes", "=", "config", ".", "num_classes", "[", "dataset", "]", "\n", "loss", "=", "config", ".", "loss", "[", "dataset", "]", "\n", "activation", "=", "config", ".", "activation", "[", "dataset", "]", "\n", "embedding_dims", "=", "config", ".", "wordCNN_embedding_dims", "[", "dataset", "]", "\n", "num_words", "=", "config", ".", "num_words", "[", "dataset", "]", "\n", "\n", "print", "(", "'Build word_cnn model...'", ")", "\n", "model", "=", "Sequential", "(", ")", "\n", "\n", "if", "use_glove", ":", "\n", "        ", "file_path", "=", "r'./glove.6B.{}d.txt'", ".", "format", "(", "str", "(", "embedding_dims", ")", ")", "\n", "get_embedding_index", "(", "file_path", ")", "\n", "embedding_matrix", "=", "get_embedding_matrix", "(", "dataset", ",", "num_words", ",", "embedding_dims", ")", "\n", "model", ".", "add", "(", "Embedding", "(", "# Layer 0, Start", "\n", "input_dim", "=", "num_words", "+", "1", ",", "# Size to dictionary, has to be input + 1", "\n", "output_dim", "=", "embedding_dims", ",", "# Dimensions to generate", "\n", "weights", "=", "[", "embedding_matrix", "]", ",", "# Initialize word weights", "\n", "input_length", "=", "max_len", ",", "\n", "name", "=", "\"embedding_layer\"", ",", "\n", "trainable", "=", "False", ")", ")", "\n", "", "else", ":", "\n", "        ", "model", ".", "add", "(", "Embedding", "(", "num_words", ",", "embedding_dims", ",", "input_length", "=", "max_len", ")", ")", "\n", "", "model", ".", "add", "(", "Dropout", "(", "0.2", ")", ")", "\n", "\n", "model", ".", "add", "(", "Conv1D", "(", "filters", ",", "kernel_size", ",", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "strides", "=", "1", ")", ")", "\n", "model", ".", "add", "(", "GlobalMaxPooling1D", "(", ")", ")", "\n", "\n", "model", ".", "add", "(", "Dense", "(", "hidden_dims", ")", ")", "\n", "model", ".", "add", "(", "Dropout", "(", "0.2", ")", ")", "\n", "model", ".", "add", "(", "Activation", "(", "'relu'", ")", ")", "\n", "\n", "# for CNN_2", "\n", "# model.add(Dense(hidden_dims))", "\n", "# # model.add(Dropout(0.2))", "\n", "# model.add(Activation('relu'))", "\n", "\n", "model", ".", "add", "(", "Dense", "(", "num_classes", ")", ")", "\n", "model", ".", "add", "(", "Activation", "(", "activation", ")", ")", "\n", "\n", "model", ".", "compile", "(", "loss", "=", "loss", ",", "\n", "optimizer", "=", "'adam'", ",", "\n", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.bd_lstm": [[95, 114], ["print", "keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Embedding", "keras.layers.Bidirectional", "keras.layers.Dropout", "keras.layers.Dense", "keras.layers.LSTM"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["", "def", "bd_lstm", "(", "dataset", ")", ":", "\n", "    ", "max_len", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "num_classes", "=", "config", ".", "num_classes", "[", "dataset", "]", "\n", "loss", "=", "config", ".", "loss", "[", "dataset", "]", "\n", "activation", "=", "config", ".", "activation", "[", "dataset", "]", "\n", "embedding_dims", "=", "config", ".", "bdLSTM_embedding_dims", "[", "dataset", "]", "\n", "num_words", "=", "config", ".", "num_words", "[", "dataset", "]", "\n", "\n", "print", "(", "'Build word_bdlstm model...'", ")", "\n", "model", "=", "Sequential", "(", ")", "\n", "\n", "model", ".", "add", "(", "Embedding", "(", "num_words", ",", "embedding_dims", ",", "input_length", "=", "max_len", ")", ")", "\n", "model", ".", "add", "(", "Bidirectional", "(", "LSTM", "(", "64", ")", ")", ")", "# 64 / LSTM-2:128 / LSTM-3: 32", "\n", "model", ".", "add", "(", "Dropout", "(", "0.5", ")", ")", "\n", "model", ".", "add", "(", "Dense", "(", "num_classes", ",", "activation", "=", "activation", ")", ")", "\n", "\n", "# try using different optimizers and different optimizer configs", "\n", "model", ".", "compile", "(", "'adam'", ",", "loss", ",", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.lstm": [[116, 148], ["print", "keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "neural_networks.get_embedding_index", "neural_networks.get_embedding_matrix", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.layers.LSTM", "keras.layers.Dropout", "keras.layers.Dense", "keras.layers.Embedding", "keras.layers.Embedding", "str"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_index", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.get_embedding_matrix", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["", "def", "lstm", "(", "dataset", ",", "use_glove", "=", "True", ")", ":", "\n", "    ", "drop_out", "=", "0.3", "\n", "\n", "max_len", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "num_classes", "=", "config", ".", "num_classes", "[", "dataset", "]", "\n", "loss", "=", "config", ".", "loss", "[", "dataset", "]", "\n", "activation", "=", "config", ".", "activation", "[", "dataset", "]", "\n", "embedding_dims", "=", "config", ".", "LSTM_embedding_dims", "[", "dataset", "]", "\n", "num_words", "=", "config", ".", "num_words", "[", "dataset", "]", "\n", "\n", "print", "(", "'Build word_lstm model...'", ")", "\n", "model", "=", "Sequential", "(", ")", "\n", "if", "use_glove", ":", "\n", "        ", "file_path", "=", "r'./glove.6B.'", "+", "str", "(", "embedding_dims", ")", "+", "'d.txt'", "\n", "get_embedding_index", "(", "file_path", ")", "\n", "get_embedding_matrix", "(", "dataset", ",", "num_words", ",", "embedding_dims", ")", "\n", "model", ".", "add", "(", "Embedding", "(", "# Layer 0, Start", "\n", "input_dim", "=", "num_words", "+", "1", ",", "# Size to dictionary, has to be input + 1", "\n", "output_dim", "=", "embedding_dims", ",", "# Dimensions to generate", "\n", "weights", "=", "[", "embedding_matrix", "]", ",", "# Initialize word weights", "\n", "input_length", "=", "max_len", ",", "\n", "name", "=", "\"embedding_layer\"", ",", "\n", "trainable", "=", "False", ")", ")", "\n", "", "else", ":", "\n", "        ", "model", ".", "add", "(", "Embedding", "(", "num_words", ",", "embedding_dims", ",", "input_length", "=", "max_len", ")", ")", "\n", "\n", "", "model", ".", "add", "(", "LSTM", "(", "128", ",", "name", "=", "\"lstm_layer\"", ",", "dropout", "=", "drop_out", ",", "recurrent_dropout", "=", "drop_out", ")", ")", "\n", "model", ".", "add", "(", "Dropout", "(", "0.5", ")", ")", "\n", "model", ".", "add", "(", "Dense", "(", "num_classes", ",", "activation", "=", "activation", ",", "name", "=", "\"dense_one\"", ")", ")", "\n", "\n", "model", ".", "compile", "(", "loss", "=", "loss", ",", "optimizer", "=", "'adam'", ",", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.char_cnn": [[150, 190], ["print", "keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Embedding", "keras.layers.Conv1D", "keras.layers.MaxPool1D", "keras.layers.Conv1D", "keras.layers.MaxPool1D", "keras.layers.Conv1D", "keras.layers.Conv1D", "keras.layers.Conv1D", "keras.layers.Conv1D", "keras.layers.MaxPool1D", "keras.layers.Flatten", "keras.layers.Dense", "keras.layers.Dropout", "keras.layers.Activation", "keras.layers.Dense", "keras.layers.Dropout", "keras.layers.Activation", "keras.layers.Dense", "keras.layers.Activation"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.add"], ["", "def", "char_cnn", "(", "dataset", ")", ":", "\n", "    ", "max_len", "=", "config", ".", "char_max_len", "[", "dataset", "]", "\n", "num_classes", "=", "config", ".", "num_classes", "[", "dataset", "]", "\n", "loss", "=", "config", ".", "loss", "[", "dataset", "]", "\n", "activation", "=", "config", ".", "activation", "[", "dataset", "]", "\n", "\n", "print", "(", "'Build char_cnn model...'", ")", "\n", "model", "=", "Sequential", "(", ")", "\n", "\n", "model", ".", "add", "(", "Embedding", "(", "70", ",", "69", ",", "input_length", "=", "max_len", ")", ")", "\n", "\n", "model", ".", "add", "(", "Conv1D", "(", "256", ",", "7", ",", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "strides", "=", "1", ")", ")", "\n", "model", ".", "add", "(", "MaxPool1D", "(", "3", ")", ")", "\n", "\n", "model", ".", "add", "(", "Conv1D", "(", "256", ",", "7", ",", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "strides", "=", "1", ")", ")", "\n", "model", ".", "add", "(", "MaxPool1D", "(", "3", ")", ")", "\n", "\n", "model", ".", "add", "(", "Conv1D", "(", "256", ",", "3", ",", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "strides", "=", "1", ")", ")", "\n", "model", ".", "add", "(", "Conv1D", "(", "256", ",", "3", ",", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "strides", "=", "1", ")", ")", "\n", "model", ".", "add", "(", "Conv1D", "(", "256", ",", "3", ",", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "strides", "=", "1", ")", ")", "\n", "model", ".", "add", "(", "Conv1D", "(", "256", ",", "3", ",", "padding", "=", "'valid'", ",", "activation", "=", "'relu'", ",", "strides", "=", "1", ")", ")", "\n", "model", ".", "add", "(", "MaxPool1D", "(", "3", ")", ")", "\n", "\n", "model", ".", "add", "(", "Flatten", "(", ")", ")", "\n", "\n", "model", ".", "add", "(", "Dense", "(", "max_len", ")", ")", "\n", "model", ".", "add", "(", "Dropout", "(", "0.1", ")", ")", "\n", "model", ".", "add", "(", "Activation", "(", "'relu'", ")", ")", "\n", "\n", "model", ".", "add", "(", "Dense", "(", "max_len", ")", ")", "\n", "model", ".", "add", "(", "Dropout", "(", "0.1", ")", ")", "\n", "model", ".", "add", "(", "Activation", "(", "'relu'", ")", ")", "\n", "\n", "model", ".", "add", "(", "Dense", "(", "num_classes", ")", ")", "\n", "model", ".", "add", "(", "Activation", "(", "activation", ")", ")", "\n", "\n", "model", ".", "compile", "(", "loss", "=", "loss", ",", "\n", "optimizer", "=", "'adam'", ",", "\n", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "model", "\n", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.__init__": [[2, 4], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "stream", ")", ":", "\n", "        ", "self", ".", "stream", "=", "stream", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write": [[5, 8], ["unbuffered.Unbuffered.stream.write", "unbuffered.Unbuffered.stream.flush"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write"], ["", "def", "write", "(", "self", ",", "data", ")", ":", "\n", "        ", "self", ".", "stream", ".", "write", "(", "data", ")", "\n", "self", ".", "stream", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.writelines": [[9, 12], ["unbuffered.Unbuffered.stream.writelines", "unbuffered.Unbuffered.stream.flush"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.writelines"], ["", "def", "writelines", "(", "self", ",", "datas", ")", ":", "\n", "        ", "self", ".", "stream", ".", "writelines", "(", "datas", ")", "\n", "self", ".", "stream", ".", "flush", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.__getattr__": [[13, 15], ["getattr"], "methods", ["None"], ["", "def", "__getattr__", "(", "self", ",", "attr", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "stream", ",", "attr", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper.__init__": [[28, 41], ["keras.backend.get_session"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model", ")", ":", "\n", "        ", "'''\n        :param model: Keras model.\n            This code makes a bunch of assumptions about the model:\n            - Model has single input\n            - Embedding is the first layer\n            - Model output is a scalar (logistic regression)\n        '''", "\n", "input_tensor", "=", "model", ".", "input", "\n", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "input_tensor", "=", "input_tensor", "\n", "self", ".", "sess", "=", "K", ".", "get_session", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper.predict_prob": [[42, 45], ["adversarial_tools.ForwardGradWrapper.model.predict().squeeze", "adversarial_tools.ForwardGradWrapper.model.predict"], "methods", ["None"], ["", "def", "predict_prob", "(", "self", ",", "input_vector", ")", ":", "\n", "        ", "prob", "=", "self", ".", "model", ".", "predict", "(", "input_vector", ")", ".", "squeeze", "(", ")", "\n", "return", "prob", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper.predict_classes": [[46, 50], ["adversarial_tools.ForwardGradWrapper.model.predict", "numpy.argmax"], "methods", ["None"], ["", "def", "predict_classes", "(", "self", ",", "input_vector", ")", ":", "\n", "        ", "prediction", "=", "self", ".", "model", ".", "predict", "(", "input_vector", ")", "\n", "classes", "=", "np", ".", "argmax", "(", "prediction", ",", "axis", "=", "1", ")", "\n", "return", "classes", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch_snli.__init__": [[57, 68], ["model.eval"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model", ",", "device", ")", ":", "\n", "        ", "'''\n        :param model: Keras model.\n            This code makes a bunch of assumptions about the model:\n            - Model has single input\n            - Embedding is the first layer\n            - Model output is a scalar (logistic regression)\n        '''", "\n", "model", ".", "eval", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch_snli.get_mask": [[69, 74], ["mask.to().to.to().to.to().to", "mask.to().to.to().to.to"], "methods", ["None"], ["", "def", "get_mask", "(", "self", ",", "tensor", ")", ":", "\n", "#mask = 1- (tensor==0)", "\n", "        ", "mask", "=", "~", "(", "tensor", "==", "0", ")", "\n", "mask", "=", "mask", ".", "to", "(", "self", ".", "device", ")", ".", "to", "(", "torch", ".", "float", ")", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch_snli.predict_prob": [[75, 83], ["torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "adversarial_tools.ForwardGradWrapper_pytorch_snli.get_mask", "adversarial_tools.ForwardGradWrapper_pytorch_snli.get_mask", "adversarial_tools.ForwardGradWrapper_pytorch_snli.model().squeeze", "torch.softmax().detach().cpu().numpy", "torch.softmax().detach().cpu().numpy", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "adversarial_tools.ForwardGradWrapper_pytorch_snli.model", "torch.softmax().detach().cpu", "torch.softmax().detach().cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.softmax().detach", "torch.softmax().detach", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch_snli.get_mask", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch_snli.get_mask", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "predict_prob", "(", "self", ",", "input_vector_p", ",", "input_vector_h", ")", ":", "\n", "        ", "input_vector_p", "=", "torch", ".", "from_numpy", "(", "input_vector_p", ")", ".", "to", "(", "self", ".", "device", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "input_vector_h", "=", "torch", ".", "from_numpy", "(", "input_vector_h", ")", ".", "to", "(", "self", ".", "device", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "mask_p", "=", "self", ".", "get_mask", "(", "input_vector_p", ")", "\n", "mask_h", "=", "self", ".", "get_mask", "(", "input_vector_h", ")", "\n", "\n", "logit", "=", "self", ".", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "x_p", "=", "input_vector_p", ",", "x_h", "=", "input_vector_h", ",", "x_p_mask", "=", "mask_p", ",", "x_h_mask", "=", "mask_h", ")", ".", "squeeze", "(", "0", ")", "\n", "return", "F", ".", "softmax", "(", "logit", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch_snli.predict_classes": [[84, 94], ["torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "adversarial_tools.ForwardGradWrapper_pytorch_snli.get_mask", "adversarial_tools.ForwardGradWrapper_pytorch_snli.get_mask", "adversarial_tools.ForwardGradWrapper_pytorch_snli.model().squeeze", "logit.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.argmax", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "adversarial_tools.ForwardGradWrapper_pytorch_snli.model", "logit.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "logit.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch_snli.get_mask", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch_snli.get_mask"], ["", "def", "predict_classes", "(", "self", ",", "input_vector_p", ",", "input_vector_h", ")", ":", "\n", "        ", "input_vector_p", "=", "torch", ".", "from_numpy", "(", "input_vector_p", ")", ".", "to", "(", "self", ".", "device", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "input_vector_h", "=", "torch", ".", "from_numpy", "(", "input_vector_h", ")", ".", "to", "(", "self", ".", "device", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "mask_p", "=", "self", ".", "get_mask", "(", "input_vector_p", ")", "\n", "mask_h", "=", "self", ".", "get_mask", "(", "input_vector_h", ")", "\n", "\n", "logit", "=", "self", ".", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "x_p", "=", "input_vector_p", ",", "x_h", "=", "input_vector_h", ",", "x_p_mask", "=", "mask_p", ",", "x_h_mask", "=", "mask_h", ")", ".", "squeeze", "(", "0", ")", "\n", "logit", "=", "logit", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "classes", "=", "np", ".", "argmax", "(", "logit", ",", "axis", "=", "-", "1", ")", "\n", "return", "classes", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.__init__": [[100, 111], ["model.eval"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "model", ",", "device", ")", ":", "\n", "        ", "'''\n        :param model: Keras model.\n            This code makes a bunch of assumptions about the model:\n            - Model has single input\n            - Embedding is the first layer\n            - Model output is a scalar (logistic regression)\n        '''", "\n", "model", ".", "eval", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob": [[113, 117], ["torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "adversarial_tools.ForwardGradWrapper_pytorch.model().squeeze", "torch.softmax().detach().cpu().numpy", "torch.softmax().detach().cpu().numpy", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "adversarial_tools.ForwardGradWrapper_pytorch.model", "torch.softmax().detach().cpu", "torch.softmax().detach().cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.softmax().detach", "torch.softmax().detach", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "predict_prob", "(", "self", ",", "input_vector", ")", ":", "\n", "        ", "input_vector", "=", "torch", ".", "from_numpy", "(", "input_vector", ")", ".", "to", "(", "self", ".", "device", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "logit", "=", "self", ".", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "input", "=", "input_vector", ")", ".", "squeeze", "(", "0", ")", "\n", "return", "F", ".", "softmax", "(", "logit", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_classes": [[118, 124], ["torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "adversarial_tools.ForwardGradWrapper_pytorch.model().squeeze", "logit.detach().cpu().numpy.detach().cpu().numpy.detach().cpu().numpy", "numpy.argmax", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "adversarial_tools.ForwardGradWrapper_pytorch.model", "logit.detach().cpu().numpy.detach().cpu().numpy.detach().cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "logit.detach().cpu().numpy.detach().cpu().numpy.detach"], "methods", ["None"], ["", "def", "predict_classes", "(", "self", ",", "input_vector", ")", ":", "\n", "        ", "input_vector", "=", "torch", ".", "from_numpy", "(", "input_vector", ")", ".", "to", "(", "self", ".", "device", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "logit", "=", "self", ".", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "input", "=", "input_vector", ")", ".", "squeeze", "(", "0", ")", "\n", "logit", "=", "logit", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "classes", "=", "np", ".", "argmax", "(", "logit", ",", "axis", "=", "-", "1", ")", "\n", "return", "classes", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.adversarial_paraphrase": [[126, 219], ["nlp", "evaluate_word_saliency.evaluate_word_saliency", "paraphrase.PWWS", "grad_guide.predict_classes", "grad_guide.predict_classes", "nlp", "grad_guide.predict_prob", "grad_guide.predict_prob", "word_level_process.text_to_vector", "word_level_process.text_to_vector", "grad_guide.predict_prob", "grad_guide.predict_prob", "print", "word_level_process.text_to_vector", "word_level_process.text_to_vector", "paraphrase._compile_perturbed_tokens", "enumerate", "nlp", "word_level_process.text_to_vector", "char_level_process.doc_process().reshape", "char_level_process.doc_process().reshape", "char_level_process.doc_process().reshape", "char_level_process.doc_process().reshape", "paraphrase._compile_perturbed_tokens", "char_level_process.doc_process().reshape", "nlp", "char_level_process.doc_process", "char_level_process.doc_process", "char_level_process.doc_process", "char_level_process.doc_process", "char_level_process.doc_process", "char_level_process.get_embedding_dict", "char_level_process.get_embedding_dict", "char_level_process.get_embedding_dict", "char_level_process.get_embedding_dict", "char_level_process.get_embedding_dict"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.evaluate_word_saliency.evaluate_word_saliency", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.PWWS", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_classes", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_classes", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._compile_perturbed_tokens", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._compile_perturbed_tokens", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.get_embedding_dict", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.get_embedding_dict", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.get_embedding_dict", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.get_embedding_dict", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.get_embedding_dict"], ["", "", "def", "adversarial_paraphrase", "(", "opt", ",", "input_text", ",", "true_y", ",", "grad_guide", ",", "tokenizer", ",", "dataset", ",", "level", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "'''\n    Compute a perturbation, greedily choosing the synonym if it causes the most\n    significant change in the classification probability after replacement\n    :return perturbed_text: generated adversarial examples\n    :return perturbed_y: predicted class of perturbed_text\n    :return sub_rate: word replacement rate showed in Table 3\n    :return change_tuple_list: list of substitute words\n    '''", "\n", "\n", "def", "halt_condition_fn", "(", "perturbed_text", ")", ":", "\n", "        ", "'''\n        Halt if model output is changed.\n        '''", "\n", "perturbed_vector", "=", "None", "\n", "if", "level", "==", "'word'", ":", "\n", "            ", "perturbed_vector", "=", "text_to_vector", "(", "perturbed_text", ",", "tokenizer", ",", "dataset", ")", "\n", "", "elif", "level", "==", "'char'", ":", "\n", "            ", "max_len", "=", "config", ".", "char_max_len", "[", "dataset", "]", "\n", "perturbed_vector", "=", "doc_process", "(", "perturbed_text", ",", "get_embedding_dict", "(", ")", ",", "dataset", ")", ".", "reshape", "(", "1", ",", "max_len", ")", "\n", "", "adv_y", "=", "grad_guide", ".", "predict_classes", "(", "input_vector", "=", "perturbed_vector", ")", "\n", "if", "adv_y", "!=", "true_y", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n", "", "", "def", "heuristic_fn", "(", "text", ",", "candidate", ")", ":", "\n", "        ", "'''\n        Return the difference between the classification probability of the original\n        word and the candidate substitute synonym, which is defined in Eq.(4) and Eq.(5).\n        '''", "\n", "doc", "=", "nlp", "(", "text", ")", "\n", "origin_vector", "=", "None", "\n", "perturbed_vector", "=", "None", "\n", "if", "level", "==", "'word'", ":", "\n", "            ", "origin_vector", "=", "text_to_vector", "(", "text", ",", "tokenizer", ",", "dataset", ")", "\n", "\n", "perturbed_text_list", "=", "_compile_perturbed_tokens", "(", "doc", ",", "[", "candidate", "]", ")", "\n", "perturbed_text", "=", "\"\"", "\n", "for", "i", ",", "word_str", "in", "enumerate", "(", "perturbed_text_list", ")", ":", "\n", "                ", "if", "i", "==", "0", ":", "\n", "                    ", "perturbed_text", "+=", "word_str", "\n", "", "else", ":", "\n", "                    ", "if", "word_str", "[", "0", "]", "in", "[", "\".\"", ",", "\",\"", ",", "\"-\"", ",", "\"'\"", ",", "\":\"", ",", "\"!\"", ",", "\"?\"", ",", "\"(\"", ",", "\")\"", ",", "\";\"", ",", "\"<\"", ",", "\">\"", "]", ":", "\n", "                        ", "perturbed_text", "+=", "word_str", "\n", "", "else", ":", "\n", "                        ", "perturbed_text", "+=", "(", "\" \"", "+", "word_str", ")", "\n", "\n", "", "", "", "perturbed_doc", "=", "nlp", "(", "perturbed_text", ")", "\n", "perturbed_vector", "=", "text_to_vector", "(", "perturbed_doc", ".", "text", ",", "tokenizer", ",", "dataset", ")", "\n", "", "elif", "level", "==", "'char'", ":", "\n", "            ", "max_len", "=", "config", ".", "char_max_len", "[", "dataset", "]", "\n", "origin_vector", "=", "doc_process", "(", "text", ",", "get_embedding_dict", "(", ")", ",", "dataset", ")", ".", "reshape", "(", "1", ",", "max_len", ")", "\n", "perturbed_tokens", "=", "_compile_perturbed_tokens", "(", "nlp", "(", "input_text", ")", ",", "[", "candidate", "]", ")", "\n", "perturbed_text", "=", "' '", ".", "join", "(", "perturbed_tokens", ")", "\n", "perturbed_vector", "=", "doc_process", "(", "perturbed_text", ",", "get_embedding_dict", "(", ")", ",", "dataset", ")", ".", "reshape", "(", "1", ",", "max_len", ")", "\n", "\n", "", "origin_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector", "=", "origin_vector", ")", "\n", "perturbed_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector", "=", "perturbed_vector", ")", "\n", "delta_p", "=", "origin_prob", "[", "true_y", "]", "-", "perturbed_prob", "[", "true_y", "]", "\n", "\n", "return", "delta_p", "\n", "\n", "", "doc", "=", "nlp", "(", "input_text", ")", "\n", "\n", "# PWWS", "\n", "position_word_list", ",", "word_saliency_list", "=", "evaluate_word_saliency", "(", "doc", ",", "grad_guide", ",", "tokenizer", ",", "true_y", ",", "dataset", ",", "level", ")", "\n", "perturbed_text", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "=", "PWWS", "(", "opt", ",", "\n", "doc", ",", "\n", "true_y", ",", "\n", "dataset", ",", "\n", "word_saliency_list", "=", "word_saliency_list", ",", "\n", "heuristic_fn", "=", "heuristic_fn", ",", "\n", "halt_condition_fn", "=", "halt_condition_fn", ",", "\n", "verbose", "=", "verbose", ")", "\n", "\n", "# print(\"perturbed_text after perturb_text:\", perturbed_text)", "\n", "origin_vector", "=", "perturbed_vector", "=", "None", "\n", "if", "level", "==", "'word'", ":", "\n", "        ", "origin_vector", "=", "text_to_vector", "(", "input_text", ",", "tokenizer", ",", "dataset", ")", "\n", "perturbed_vector", "=", "text_to_vector", "(", "perturbed_text", ",", "tokenizer", ",", "dataset", ")", "\n", "", "elif", "level", "==", "'char'", ":", "\n", "        ", "max_len", "=", "config", ".", "char_max_len", "[", "dataset", "]", "\n", "origin_vector", "=", "doc_process", "(", "input_text", ",", "get_embedding_dict", "(", ")", ",", "dataset", ")", ".", "reshape", "(", "1", ",", "max_len", ")", "\n", "perturbed_vector", "=", "doc_process", "(", "perturbed_text", ",", "get_embedding_dict", "(", ")", ",", "dataset", ")", ".", "reshape", "(", "1", ",", "max_len", ")", "\n", "", "perturbed_y", "=", "grad_guide", ".", "predict_classes", "(", "input_vector", "=", "perturbed_vector", ")", "\n", "if", "verbose", ":", "\n", "        ", "origin_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector", "=", "origin_vector", ")", "\n", "perturbed_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector", "=", "perturbed_vector", ")", "\n", "raw_score", "=", "origin_prob", "[", "true_y", "]", "-", "perturbed_prob", "[", "true_y", "]", "\n", "print", "(", "'Prob before: '", ",", "origin_prob", "[", "true_y", "]", ",", "'. Prob after: '", ",", "perturbed_prob", "[", "true_y", "]", ",", "\n", "'. Prob shift: '", ",", "raw_score", ")", "\n", "", "return", "perturbed_text", ",", "perturbed_y", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.adversarial_paraphrase_snli": [[222, 343], ["nlp", "nlp", "evaluate_word_saliency.evaluate_word_saliency_snli", "paraphrase.PWWS_snli", "grad_guide.predict_classes", "grad_guide.predict_classes", "enumerate", "nlp", "grad_guide.predict_prob", "grad_guide.predict_prob", "word_level_process.text_to_vector", "word_level_process.text_to_vector", "word_level_process.text_to_vector", "word_level_process.text_to_vector", "grad_guide.predict_prob", "grad_guide.predict_prob", "print", "word_level_process.text_to_vector", "word_level_process.text_to_vector", "word_level_process.text_to_vector", "paraphrase._compile_perturbed_tokens", "adversarial_tools.adversarial_paraphrase_snli.gen"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.evaluate_word_saliency.evaluate_word_saliency_snli", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase.PWWS_snli", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_classes", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_classes", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.paraphrase._compile_perturbed_tokens"], ["", "def", "adversarial_paraphrase_snli", "(", "opt", ",", "input_text_p", ",", "input_text_h", ",", "true_y", ",", "grad_guide", ",", "tokenizer", ",", "dataset", ",", "level", ",", "verbose", "=", "True", ")", ":", "\n", "    ", "'''\n    Compute a perturbation, greedily choosing the synonym if it causes the most\n    significant change in the classification probability after replacement\n    :return perturbed_text: generated adversarial examples\n    :return perturbed_y: predicted class of perturbed_text\n    :return sub_rate: word replacement rate showed in Table 3\n    :return change_tuple_list: list of substitute words\n    '''", "\n", "\n", "def", "halt_condition_fn", "(", "perturbed_text", ")", ":", "\n", "        ", "'''\n        Halt if model output is changed.\n        '''", "\n", "perturbed_vector", "=", "None", "\n", "if", "level", "==", "'word'", ":", "\n", "            ", "perturbed_vector", "=", "text_to_vector", "(", "perturbed_text", ",", "tokenizer", ",", "dataset", ")", "\n", "", "elif", "level", "==", "'char'", ":", "\n", "            ", "max_len", "=", "config", ".", "char_max_len", "[", "dataset", "]", "\n", "perturbed_vector", "=", "doc_process", "(", "perturbed_text", ",", "get_embedding_dict", "(", ")", ",", "dataset", ")", ".", "reshape", "(", "1", ",", "max_len", ")", "\n", "", "adv_y", "=", "grad_guide", ".", "predict_classes", "(", "input_vector", "=", "perturbed_vector", ")", "\n", "if", "adv_y", "!=", "true_y", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n", "", "", "def", "gen", "(", "perturbed_text_list", ")", ":", "\n", "        ", "perturbed_text", "=", "\"\"", "\n", "recur", "=", "0", "\n", "reduc", "=", "0", "\n", "for", "i", ",", "word_str", "in", "enumerate", "(", "perturbed_text_list", ")", ":", "\n", "\n", "            ", "if", "reduc", "==", "1", "or", "i", "==", "0", ":", "\n", "                ", "space", "=", "\"\"", "\n", "reduc", "=", "0", "\n", "", "else", ":", "\n", "                ", "space", "=", "\" \"", "\n", "\n", "", "if", "len", "(", "word_str", ")", "==", "1", "and", "word_str", "[", "0", "]", "in", "[", "\".\"", ",", "\",\"", ",", "\"-\"", ",", "\":\"", ",", "\"!\"", ",", "\"?\"", ",", "\"(\"", ",", "\")\"", ",", "\";\"", ",", "\"<\"", ",", "\">\"", ",", "\"{\"", ",", "\"}\"", ",", "\"[\"", ",", "\"]\"", "]", ":", "\n", "                ", "space", "=", "\"\"", "\n", "if", "word_str", "[", "0", "]", "in", "[", "\"(\"", ",", "\"<\"", ",", "\"{\"", ",", "\"[\"", "]", ":", "\n", "                    ", "reduc", "=", "1", "\n", "", "", "elif", "len", "(", "word_str", ")", "==", "1", "and", "word_str", "[", "0", "]", "in", "[", "\"\\\"\"", ",", "]", ":", "\n", "                ", "if", "recur", "==", "0", ":", "\n", "                    ", "space", "=", "\" \"", "\n", "reduc", "=", "1", "\n", "", "elif", "recur", "==", "1", ":", "\n", "                    ", "space", "=", "\"\"", "\n", "", "recur", "=", "(", "recur", "+", "1", ")", "%", "2", "\n", "", "elif", "len", "(", "word_str", ")", "==", "1", "and", "word_str", "[", "0", "]", "in", "[", "\"'\"", ",", "]", ":", "\n", "                ", "space", "=", "\"\"", "\n", "reduc", "=", "1", "\n", "\n", "", "perturbed_text", "+=", "(", "space", "+", "word_str", ")", "\n", "\n", "", "return", "perturbed_text", "\n", "\n", "", "def", "heuristic_fn", "(", "text_p", ",", "text_h", ",", "candidate_h", ")", ":", "\n", "        ", "'''\n        Return the difference between the classification probability of the original\n        word and the candidate substitute synonym, which is defined in Eq.(4) and Eq.(5).\n        '''", "\n", "doc_h", "=", "nlp", "(", "text_h", ")", "\n", "\n", "origin_vector_h", "=", "None", "\n", "perturbed_vector_h", "=", "None", "\n", "if", "level", "==", "'word'", ":", "\n", "            ", "origin_vector_p", "=", "text_to_vector", "(", "text_p", ",", "tokenizer", ",", "dataset", ")", "\n", "origin_vector_h", "=", "text_to_vector", "(", "text_h", ",", "tokenizer", ",", "dataset", ")", "\n", "\n", "perturbed_text_list_h", "=", "_compile_perturbed_tokens", "(", "doc_h", ",", "[", "candidate_h", "]", ")", "\n", "\"\"\"\n            perturbed_text = \"\" \n            for i, word_str in enumerate(perturbed_text_list):\n                if i==0:\n                    perturbed_text+=word_str\n                else:\n                    if word_str[0] in [\".\", \",\", \"-\", \"'\", \":\", \"!\", \"?\", \"(\", \")\", \";\", \"<\", \">\"]:\n                        perturbed_text+=word_str\n                    else:\n                        perturbed_text+=(\" \"+word_str)\n            \"\"\"", "\n", "perturbed_text_h", "=", "gen", "(", "perturbed_text_list_h", ")", "\n", "\n", "perturbed_doc_h", "=", "nlp", "(", "perturbed_text_h", ")", "\n", "perturbed_vector_h", "=", "text_to_vector", "(", "perturbed_doc_h", ".", "text", ",", "tokenizer", ",", "dataset", ")", "\n", "\n", "", "origin_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector_p", "=", "origin_vector_p", ",", "input_vector_h", "=", "origin_vector_h", ")", "\n", "perturbed_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector_p", "=", "origin_vector_p", ",", "input_vector_h", "=", "perturbed_vector_h", ")", "\n", "delta_p", "=", "origin_prob", "[", "true_y", "]", "-", "perturbed_prob", "[", "true_y", "]", "\n", "\n", "return", "delta_p", "\n", "\n", "", "doc_p", "=", "nlp", "(", "input_text_p", ")", "\n", "doc_h", "=", "nlp", "(", "input_text_h", ")", "\n", "\n", "# PWWS", "\n", "position_word_list_h", ",", "word_saliency_list_h", "=", "evaluate_word_saliency_snli", "(", "doc_p", ",", "doc_h", ",", "grad_guide", ",", "tokenizer", ",", "true_y", ",", "dataset", ",", "level", ")", "\n", "perturbed_text_p", ",", "perturbed_text_h", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "=", "PWWS_snli", "(", "opt", ",", "doc_p", ",", "doc_h", ",", "\n", "true_y", ",", "\n", "dataset", ",", "\n", "word_saliency_list", "=", "word_saliency_list_h", ",", "\n", "heuristic_fn", "=", "heuristic_fn", ",", "\n", "#halt_condition_fn=halt_condition_fn,", "\n", "halt_condition_fn", "=", "None", ",", "\n", "verbose", "=", "verbose", ")", "\n", "\n", "origin_vector", "=", "perturbed_vector", "=", "None", "\n", "if", "level", "==", "'word'", ":", "\n", "        ", "origin_vector_p", "=", "text_to_vector", "(", "input_text_p", ",", "tokenizer", ",", "dataset", ")", "\n", "perturbed_vector_p", "=", "text_to_vector", "(", "perturbed_text_p", ",", "tokenizer", ",", "dataset", ")", "\n", "origin_vector_h", "=", "text_to_vector", "(", "input_text_h", ",", "tokenizer", ",", "dataset", ")", "\n", "perturbed_vector_h", "=", "text_to_vector", "(", "perturbed_text_h", ",", "tokenizer", ",", "dataset", ")", "\n", "", "perturbed_y", "=", "grad_guide", ".", "predict_classes", "(", "input_vector_p", "=", "perturbed_vector_p", ",", "input_vector_h", "=", "perturbed_vector_h", ")", "\n", "if", "verbose", ":", "\n", "        ", "origin_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector_p", "=", "origin_vector_p", ",", "input_vector_h", "=", "origin_vector_h", ")", "\n", "perturbed_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector_p", "=", "perturbed_vector_p", ",", "input_vector_h", "=", "perturbed_vector_h", ")", "\n", "raw_score", "=", "origin_prob", "[", "true_y", "]", "-", "perturbed_prob", "[", "true_y", "]", "\n", "print", "(", "'Prob before: '", ",", "origin_prob", "[", "true_y", "]", ",", "'. Prob after: '", ",", "perturbed_prob", "[", "true_y", "]", ",", "\n", "'. Prob shift: '", ",", "raw_score", ")", "\n", "", "return", "perturbed_text_p", ",", "perturbed_text_h", ",", "perturbed_y", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "\n", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.rm_tags": [[20, 23], ["re.compile", "re.compile.sub"], "function", ["None"], ["", "def", "rm_tags", "(", "text", ")", ":", "\n", "    ", "re_tag", "=", "re", ".", "compile", "(", "r'<[^>]+>'", ")", "\n", "return", "re_tag", ".", "sub", "(", "''", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_snli_files": [[25, 60], ["os.path.join", "open", "tqdm.tqdm", "json.loads", "read_files.read_snli_files.label_switch"], "function", ["None"], ["", "def", "read_snli_files", "(", "opt", ",", "filetype", ")", ":", "\n", "    ", "def", "label_switch", "(", "str", ")", ":", "\n", "        ", "if", "str", "==", "\"entailment\"", ":", "\n", "            ", "return", "[", "1", ",", "0", ",", "0", "]", "\n", "", "if", "str", "==", "\"contradiction\"", ":", "\n", "            ", "return", "[", "0", ",", "1", ",", "0", "]", "\n", "", "if", "str", "==", "\"neutral\"", ":", "\n", "            ", "return", "[", "0", ",", "0", ",", "1", "]", "\n", "", "raise", "NotImplementedError", "\n", "\n", "", "split", "=", "filetype", "\n", "totals", "=", "{", "'train'", ":", "550152", ",", "'dev'", ":", "10000", ",", "'test'", ":", "10000", "}", "\n", "all_prem", "=", "[", "]", "\n", "all_hypo", "=", "[", "]", "\n", "all_labels", "=", "[", "]", "\n", "\n", "fn", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "work_path", ",", "'data_set/snli_1.0/snli_1.0_{}.jsonl'", ".", "format", "(", "split", ")", ")", "\n", "with", "open", "(", "fn", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "f", ",", "total", "=", "totals", "[", "split", "]", ")", ":", "\n", "            ", "example", "=", "json", ".", "loads", "(", "line", ")", "\n", "prem", ",", "hypo", ",", "gold_label", "=", "example", "[", "'sentence1'", "]", ",", "example", "[", "'sentence2'", "]", ",", "example", "[", "'gold_label'", "]", "\n", "try", ":", "\n", "                ", "one_hot_label", "=", "label_switch", "(", "gold_label", ")", "\n", "\n", "from", "nltk", "import", "word_tokenize", "\n", "prem", "=", "' '", ".", "join", "(", "word_tokenize", "(", "prem", ")", ")", "\n", "hypo", "=", "' '", ".", "join", "(", "word_tokenize", "(", "hypo", ")", ")", "\n", "\n", "all_prem", ".", "append", "(", "prem", ")", "\n", "all_hypo", ".", "append", "(", "hypo", ")", "\n", "all_labels", ".", "append", "(", "one_hot_label", ")", "\n", "\n", "", "except", ":", "\n", "                ", "continue", "\n", "", "", "", "return", "all_prem", ",", "all_hypo", ",", "all_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_snli_files": [[61, 96], ["os.path.join", "os.path.exists", "print", "open", "pickle.load", "open.close", "print", "read_files.read_snli_files", "read_files.read_snli_files", "read_files.read_snli_files", "open", "pickle.dump", "open.close"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_snli_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_snli_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_snli_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump"], ["", "def", "split_snli_files", "(", "opt", ")", ":", "\n", "    ", "filename", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "work_path", ",", "\"temp/split_snli_files\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "        ", "print", "(", "'Read processed SNLI dataset'", ")", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "train_perms", "=", "saved", "[", "'train_perms'", "]", "\n", "train_hypos", "=", "saved", "[", "'train_hypos'", "]", "\n", "train_labels", "=", "saved", "[", "'train_labels'", "]", "\n", "test_perms", "=", "saved", "[", "'test_perms'", "]", "\n", "test_hypos", "=", "saved", "[", "'test_hypos'", "]", "\n", "test_labels", "=", "saved", "[", "'test_labels'", "]", "\n", "dev_perms", "=", "saved", "[", "'dev_perms'", "]", "\n", "dev_hypos", "=", "saved", "[", "'dev_hypos'", "]", "\n", "dev_labels", "=", "saved", "[", "'dev_labels'", "]", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Processing SNLI dataset'", ")", "\n", "train_perms", ",", "train_hypos", ",", "train_labels", "=", "read_snli_files", "(", "opt", ",", "'train'", ")", "\n", "dev_perms", ",", "dev_hypos", ",", "dev_labels", "=", "read_snli_files", "(", "opt", ",", "'dev'", ")", "\n", "test_perms", ",", "test_hypos", ",", "test_labels", "=", "read_snli_files", "(", "opt", ",", "'test'", ")", "\n", "f", "=", "open", "(", "filename", ",", "'wb'", ")", "\n", "saved", "=", "{", "}", "\n", "saved", "[", "'train_perms'", "]", "=", "train_perms", "\n", "saved", "[", "'train_hypos'", "]", "=", "train_hypos", "\n", "saved", "[", "'train_labels'", "]", "=", "train_labels", "\n", "saved", "[", "'test_perms'", "]", "=", "test_perms", "\n", "saved", "[", "'test_hypos'", "]", "=", "test_hypos", "\n", "saved", "[", "'test_labels'", "]", "=", "test_labels", "\n", "saved", "[", "'dev_perms'", "]", "=", "dev_perms", "\n", "saved", "[", "'dev_hypos'", "]", "=", "dev_hypos", "\n", "saved", "[", "'dev_labels'", "]", "=", "dev_labels", "\n", "pickle", ".", "dump", "(", "saved", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "return", "train_perms", ",", "train_hypos", ",", "train_labels", ",", "dev_perms", ",", "dev_hypos", ",", "dev_labels", ",", "test_perms", ",", "test_hypos", ",", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_imdb_files": [[98, 131], ["range", "range", "os.path.join", "os.listdir", "os.listdir", "all_labels.append", "all_labels.append", "file_list.append", "file_list.append", "open", "[].strip().replace", "word_tokenize", "all_texts.append", "[].strip", "f.readlines"], "function", ["None"], ["", "def", "read_imdb_files", "(", "opt", ",", "filetype", ")", ":", "\n", "\n", "    ", "all_labels", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "12500", ")", ":", "\n", "        ", "all_labels", ".", "append", "(", "[", "0", ",", "1", "]", ")", "\n", "", "for", "_", "in", "range", "(", "12500", ")", ":", "\n", "        ", "all_labels", ".", "append", "(", "[", "1", ",", "0", "]", ")", "\n", "\n", "", "all_texts", "=", "[", "]", "\n", "file_list", "=", "[", "]", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "work_path", ",", "'data_set/aclImdb/'", ")", "\n", "pos_path", "=", "path", "+", "filetype", "+", "'/pos/'", "\n", "for", "file", "in", "os", ".", "listdir", "(", "pos_path", ")", ":", "\n", "        ", "file_list", ".", "append", "(", "pos_path", "+", "file", ")", "\n", "", "neg_path", "=", "path", "+", "filetype", "+", "'/neg/'", "\n", "for", "file", "in", "os", ".", "listdir", "(", "neg_path", ")", ":", "\n", "        ", "file_list", ".", "append", "(", "neg_path", "+", "file", ")", "\n", "", "for", "file_name", "in", "file_list", ":", "\n", "        ", "with", "open", "(", "file_name", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "\n", "            ", "from", "nltk", "import", "word_tokenize", "\n", "x_raw", "=", "f", ".", "readlines", "(", ")", "[", "0", "]", ".", "strip", "(", ")", ".", "replace", "(", "'<br />'", ",", "' '", ")", "\n", "x_toks", "=", "word_tokenize", "(", "x_raw", ")", "\n", "#num_words += len(x_toks)", "\n", "all_texts", ".", "append", "(", "' '", ".", "join", "(", "x_toks", ")", ")", "\n", "\n", "\"\"\"\n            temp = f.readlines()\n            temp2 = [re.sub(r'[.]', r'. ', x) for x in temp]\n            temp3 = [re.sub(r'[ ][ ]', r' ', x) for x in temp2]\n            all_texts.append(rm_tags(\" \".join(temp3)))\n            \"\"\"", "\n", "", "", "return", "all_texts", ",", "all_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files": [[133, 170], ["os.path.join", "os.path.exists", "print", "open", "pickle.load", "open.close", "print", "read_files.read_imdb_files", "read_files.read_imdb_files", "open", "pickle.dump", "open.close"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump"], ["", "def", "split_imdb_files", "(", "opt", ")", ":", "\n", "    ", "filename", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "work_path", ",", "\"temp/split_imdb_files\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "filename", ")", ":", "\n", "        ", "print", "(", "'Read processed IMDB dataset'", ")", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "train_texts", "=", "saved", "[", "'train_texts'", "]", "\n", "train_labels", "=", "saved", "[", "'train_labels'", "]", "\n", "test_texts", "=", "saved", "[", "'test_texts'", "]", "\n", "test_labels", "=", "saved", "[", "'test_labels'", "]", "\n", "dev_texts", "=", "saved", "[", "'dev_texts'", "]", "\n", "dev_labels", "=", "saved", "[", "'dev_labels'", "]", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Processing IMDB dataset'", ")", "\n", "train_texts", ",", "train_labels", "=", "read_imdb_files", "(", "opt", ",", "'train'", ")", "\n", "test_texts", ",", "test_labels", "=", "read_imdb_files", "(", "opt", ",", "'test'", ")", "\n", "dev_texts", "=", "test_texts", "[", "12500", "-", "500", ":", "12500", "]", "+", "test_texts", "[", "25000", "-", "500", ":", "25000", "]", "\n", "dev_labels", "=", "test_labels", "[", "12500", "-", "500", ":", "12500", "]", "+", "test_labels", "[", "25000", "-", "500", ":", "25000", "]", "\n", "\n", "#test_texts = test_texts[:12500-500] + test_texts[12500:25000-500]", "\n", "#test_labels = test_labels[:12500-500] + test_labels[12500:25000-500]", "\n", "\n", "test_texts", "=", "test_texts", "[", ":", "12500", "]", "+", "test_texts", "[", "12500", ":", "25000", "]", "\n", "test_labels", "=", "test_labels", "[", ":", "12500", "]", "+", "test_labels", "[", "12500", ":", "25000", "]", "\n", "\n", "f", "=", "open", "(", "filename", ",", "'wb'", ")", "\n", "saved", "=", "{", "}", "\n", "saved", "[", "'train_texts'", "]", "=", "train_texts", "\n", "saved", "[", "'train_labels'", "]", "=", "train_labels", "\n", "saved", "[", "'test_texts'", "]", "=", "test_texts", "\n", "saved", "[", "'test_labels'", "]", "=", "test_labels", "\n", "saved", "[", "'dev_texts'", "]", "=", "dev_texts", "\n", "saved", "[", "'dev_labels'", "]", "=", "dev_labels", "\n", "pickle", ".", "dump", "(", "saved", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "return", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_yahoo_files": [[172, 196], ["sorted", "keras.utils.np_utils.to_categorical", "os.listdir", "os.path.join", "os.path.isdir", "numpy.asarray", "len", "sorted", "os.listdir", "fname.isdigit", "os.path.join", "texts.append", "open.close", "keras.utils.np_utils.to_categorical.append", "open", "open", "open.read"], "function", ["None"], ["", "def", "read_yahoo_files", "(", ")", ":", "\n", "    ", "text_data_dir", "=", "'./PWWS/data_set/yahoo_10'", "\n", "\n", "texts", "=", "[", "]", "# list of text samples", "\n", "labels_index", "=", "{", "}", "# dictionary mapping label name to numeric id", "\n", "labels", "=", "[", "]", "# list of label ids", "\n", "for", "name", "in", "sorted", "(", "os", ".", "listdir", "(", "text_data_dir", ")", ")", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "text_data_dir", ",", "name", ")", "\n", "if", "os", ".", "path", ".", "isdir", "(", "path", ")", ":", "\n", "            ", "label_id", "=", "len", "(", "labels_index", ")", "\n", "labels_index", "[", "name", "]", "=", "label_id", "\n", "for", "fname", "in", "sorted", "(", "os", ".", "listdir", "(", "path", ")", ")", ":", "\n", "                ", "if", "fname", ".", "isdigit", "(", ")", ":", "\n", "                    ", "fpath", "=", "os", ".", "path", ".", "join", "(", "path", ",", "fname", ")", "\n", "if", "sys", ".", "version_info", "<", "(", "3", ",", ")", ":", "\n", "                        ", "f", "=", "open", "(", "fpath", ")", "\n", "", "else", ":", "\n", "                        ", "f", "=", "open", "(", "fpath", ",", "encoding", "=", "'latin-1'", ")", "\n", "", "texts", ".", "append", "(", "f", ".", "read", "(", ")", ")", "\n", "f", ".", "close", "(", ")", "\n", "labels", ".", "append", "(", "label_id", ")", "\n", "\n", "", "", "", "", "labels", "=", "to_categorical", "(", "np", ".", "asarray", "(", "labels", ")", ")", "\n", "return", "texts", ",", "labels", ",", "labels_index", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_yahoo_files": [[198, 203], ["print", "read_files.read_yahoo_files", "sklearn.model_selection.train_test_split"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_yahoo_files"], ["", "def", "split_yahoo_files", "(", ")", ":", "\n", "    ", "print", "(", "'Processing Yahoo! Answers dataset'", ")", "\n", "texts", ",", "labels", ",", "_", "=", "read_yahoo_files", "(", ")", "\n", "train_texts", ",", "test_texts", ",", "train_labels", ",", "test_labels", "=", "train_test_split", "(", "texts", ",", "labels", ",", "test_size", "=", "0.2", ")", "\n", "return", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_agnews_files": [[205, 225], ["open", "csv.reader", "range", "texts.append", "labels_index.append", "numpy.zeros", "labels.append", "int"], "function", ["None"], ["", "def", "read_agnews_files", "(", "filetype", ")", ":", "\n", "    ", "texts", "=", "[", "]", "\n", "labels_index", "=", "[", "]", "# The index of label of all input sentences, which takes the values 1,2,3,4", "\n", "doc_count", "=", "0", "# number of input sentences", "\n", "path", "=", "r'./PWWS/data_set/ag_news_csv/{}.csv'", ".", "format", "(", "filetype", ")", "\n", "csvfile", "=", "open", "(", "path", ",", "'r'", ")", "\n", "for", "line", "in", "csv", ".", "reader", "(", "csvfile", ",", "delimiter", "=", "','", ",", "quotechar", "=", "'\"'", ")", ":", "\n", "        ", "content", "=", "line", "[", "1", "]", "+", "\". \"", "+", "line", "[", "2", "]", "\n", "texts", ".", "append", "(", "content", ")", "\n", "labels_index", ".", "append", "(", "line", "[", "0", "]", ")", "\n", "doc_count", "+=", "1", "\n", "\n", "# Start document processing", "\n", "", "labels", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "doc_count", ")", ":", "\n", "        ", "label_class", "=", "np", ".", "zeros", "(", "config", ".", "num_classes", "[", "'agnews'", "]", ",", "dtype", "=", "'float32'", ")", "\n", "label_class", "[", "int", "(", "labels_index", "[", "i", "]", ")", "-", "1", "]", "=", "1", "\n", "labels", ".", "append", "(", "label_class", ")", "\n", "\n", "", "return", "texts", ",", "labels", ",", "labels_index", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_agnews_files": [[227, 232], ["print", "read_files.read_agnews_files", "read_files.read_agnews_files"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_agnews_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.read_agnews_files"], ["", "def", "split_agnews_files", "(", ")", ":", "\n", "    ", "print", "(", "\"Processing AG's News dataset\"", ")", "\n", "train_texts", ",", "train_labels", ",", "_", "=", "read_agnews_files", "(", "'train'", ")", "# 120000", "\n", "test_texts", ",", "test_labels", ",", "_", "=", "read_agnews_files", "(", "'test'", ")", "# 7600", "\n", "return", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.onehot_dic_build": [[6, 23], ["np.array.append", "enumerate", "numpy.array", "numpy.zeros", "numpy.zeros", "np.array.append", "len", "len"], "function", ["None"], ["def", "onehot_dic_build", "(", ")", ":", "\n", "# use one-hot encoding", "\n", "    ", "alphabet", "=", "\"abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:'\\\"/\\\\|_@#$%^&*~`+-=<>()[]{}\"", "\n", "embedding_dic", "=", "{", "}", "\n", "embedding_w", "=", "[", "]", "\n", "# For characters that do not exist in the alphabet or empty characters, replace them with vectors 0.", "\n", "embedding_dic", "[", "\"UNK\"", "]", "=", "0", "\n", "embedding_w", ".", "append", "(", "np", ".", "zeros", "(", "len", "(", "alphabet", ")", ",", "dtype", "=", "'float32'", ")", ")", "\n", "\n", "for", "i", ",", "alpha", "in", "enumerate", "(", "alphabet", ")", ":", "\n", "        ", "onehot", "=", "np", ".", "zeros", "(", "len", "(", "alphabet", ")", ",", "dtype", "=", "'float32'", ")", "\n", "embedding_dic", "[", "alpha", "]", "=", "i", "+", "1", "\n", "onehot", "[", "i", "]", "=", "1", "\n", "embedding_w", ".", "append", "(", "onehot", ")", "\n", "\n", "", "embedding_w", "=", "np", ".", "array", "(", "embedding_w", ",", "dtype", "=", "'float32'", ")", "\n", "return", "embedding_w", ",", "embedding_dic", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.get_embedding_dict": [[25, 37], ["None"], "function", ["None"], ["", "def", "get_embedding_dict", "(", ")", ":", "\n", "    ", "return", "{", "'UNK'", ":", "0", ",", "'a'", ":", "1", ",", "'b'", ":", "2", ",", "'c'", ":", "3", ",", "'d'", ":", "4", ",", "'e'", ":", "5", ",", "'f'", ":", "6", ",", "'g'", ":", "7", ",", "'h'", ":", "8", ",", "'i'", ":", "9", ",", "'j'", ":", "10", ",", "\n", "'k'", ":", "11", ",", "'l'", ":", "12", ",", "\n", "'m'", ":", "13", ",", "'n'", ":", "14", ",", "'o'", ":", "15", ",", "'p'", ":", "16", ",", "'q'", ":", "17", ",", "'r'", ":", "18", ",", "'s'", ":", "19", ",", "'t'", ":", "20", ",", "'u'", ":", "21", ",", "'v'", ":", "22", ",", "\n", "'w'", ":", "23", ",", "'x'", ":", "24", ",", "\n", "'y'", ":", "25", ",", "'z'", ":", "26", ",", "'0'", ":", "27", ",", "'1'", ":", "28", ",", "'2'", ":", "29", ",", "'3'", ":", "30", ",", "'4'", ":", "31", ",", "'5'", ":", "32", ",", "'6'", ":", "33", ",", "'7'", ":", "34", ",", "\n", "'8'", ":", "35", ",", "'9'", ":", "36", ",", "\n", "'-'", ":", "60", ",", "','", ":", "38", ",", "';'", ":", "39", ",", "'.'", ":", "40", ",", "'!'", ":", "41", ",", "'?'", ":", "42", ",", "':'", ":", "43", ",", "\"'\"", ":", "44", ",", "'\"'", ":", "45", ",", "'/'", ":", "46", ",", "\n", "'\\\\'", ":", "47", ",", "'|'", ":", "48", ",", "\n", "'_'", ":", "49", ",", "'@'", ":", "50", ",", "'#'", ":", "51", ",", "'$'", ":", "52", ",", "'%'", ":", "53", ",", "'^'", ":", "54", ",", "'&'", ":", "55", ",", "'*'", ":", "56", ",", "'~'", ":", "57", ",", "'`'", ":", "58", ",", "\n", "'+'", ":", "59", ",", "'='", ":", "61", ",", "\n", "'<'", ":", "62", ",", "'>'", ":", "63", ",", "'('", ":", "64", ",", "')'", ":", "65", ",", "'['", ":", "66", ",", "']'", ":", "67", ",", "'{'", ":", "68", ",", "'}'", ":", "69", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process": [[39, 49], ["min", "numpy.zeros", "range", "len"], "function", ["None"], ["", "def", "doc_process", "(", "doc", ",", "embedding_dic", ",", "dataset", ")", ":", "\n", "    ", "max_len", "=", "config", ".", "char_max_len", "[", "dataset", "]", "\n", "min_len", "=", "min", "(", "max_len", ",", "len", "(", "doc", ")", ")", "\n", "doc_vec", "=", "np", ".", "zeros", "(", "max_len", ",", "dtype", "=", "'int64'", ")", "\n", "for", "j", "in", "range", "(", "min_len", ")", ":", "\n", "        ", "if", "doc", "[", "j", "]", "in", "embedding_dic", ":", "\n", "            ", "doc_vec", "[", "j", "]", "=", "embedding_dic", "[", "doc", "[", "j", "]", "]", "\n", "", "else", ":", "\n", "            ", "doc_vec", "[", "j", "]", "=", "embedding_dic", "[", "'UNK'", "]", "\n", "", "", "return", "doc_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process_for_all": [[51, 58], ["numpy.asarray", "np.asarray.append", "char_level_process.doc_process"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process"], ["", "def", "doc_process_for_all", "(", "doc", ",", "embedding_dic", ",", "dataset", ")", ":", "\n", "    ", "max_len", "=", "config", ".", "char_max_len", "[", "dataset", "]", "\n", "x", "=", "[", "]", "\n", "for", "d", "in", "doc", ":", "\n", "        ", "x", ".", "append", "(", "doc_process", "(", "d", ",", "embedding_dic", ",", "dataset", ")", ")", "\n", "", "x", "=", "np", ".", "asarray", "(", "x", ",", "dtype", "=", "'int64'", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.char_process": [[60, 79], ["char_level_process.onehot_dic_build", "range", "numpy.asarray", "numpy.array", "range", "numpy.asarray", "numpy.array", "len", "char_level_process.doc_process", "np.asarray.append", "len", "char_level_process.doc_process", "np.asarray.append", "train_texts[].lower", "test_texts[].lower"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.onehot_dic_build", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process"], ["", "def", "char_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", ":", "\n", "    ", "embedding_w", ",", "embedding_dic", "=", "onehot_dic_build", "(", ")", "\n", "\n", "x_train", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "train_texts", ")", ")", ":", "\n", "        ", "doc_vec", "=", "doc_process", "(", "train_texts", "[", "i", "]", ".", "lower", "(", ")", ",", "embedding_dic", ",", "dataset", ")", "\n", "x_train", ".", "append", "(", "doc_vec", ")", "\n", "", "x_train", "=", "np", ".", "asarray", "(", "x_train", ",", "dtype", "=", "'int64'", ")", "\n", "y_train", "=", "np", ".", "array", "(", "train_labels", ",", "dtype", "=", "'float32'", ")", "\n", "\n", "x_test", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "test_texts", ")", ")", ":", "\n", "        ", "doc_vec", "=", "doc_process", "(", "test_texts", "[", "i", "]", ".", "lower", "(", ")", ",", "embedding_dic", ",", "dataset", ")", "\n", "x_test", ".", "append", "(", "doc_vec", ")", "\n", "", "x_test", "=", "np", ".", "asarray", "(", "x_test", ",", "dtype", "=", "'int64'", ")", "\n", "y_test", "=", "np", ".", "array", "(", "test_labels", ",", "dtype", "=", "'float32'", ")", "\n", "\n", "del", "embedding_w", ",", "embedding_dic", "\n", "return", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "\n", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.update_tokenizer": [[24, 38], ["None"], "function", ["None"], ["def", "update_tokenizer", "(", "dataset", ",", "tokenizer", ")", ":", "\n", "\n", "    ", "if", "dataset", "==", "'imdb'", ":", "\n", "        ", "global", "imdb_tokenizer", "\n", "imdb_tokenizer", "=", "tokenizer", "\n", "", "elif", "dataset", "==", "'snli'", ":", "\n", "        ", "global", "snli_tokenizer", "\n", "snli_tokenizer", "=", "tokenizer", "\n", "", "elif", "dataset", "==", "'yahoo'", ":", "\n", "        ", "global", "yahoo_tokenizer", "\n", "yahoo_tokenizer", "=", "tokenizer", "\n", "", "elif", "dataset", "==", "'agnews'", ":", "\n", "        ", "global", "agnews_tokenizer", "\n", "agnews_tokenizer", "=", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer": [[39, 79], ["os.path.join", "os.path.exists", "open", "pickle.load", "open.close", "read_files.split_imdb_files", "tokenizer_for_spacy.Tokenizer", "tokenizer_for_spacy.Tokenizer.fit_on_texts", "open", "pickle.dump", "open.close", "os.path.join", "os.path.exists", "open", "pickle.load", "open.close", "read_files.split_snli_files", "tokenizer_for_spacy.Tokenizer", "tokenizer_for_spacy.Tokenizer.fit_on_texts", "open", "pickle.dump", "open.close"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.fit_on_texts", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_snli_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.fit_on_texts", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.dataHelper.Alphabet.dump"], ["", "", "def", "get_tokenizer", "(", "opt", ")", ":", "\n", "    ", "dataset", "=", "opt", ".", "dataset", "\n", "texts", "=", "None", "\n", "if", "dataset", "==", "'imdb'", ":", "\n", "        ", "global", "imdb_tokenizer", "\n", "if", "imdb_tokenizer", "is", "not", "None", ":", "\n", "            ", "return", "imdb_tokenizer", "\n", "\n", "", "imdb_tokenizer_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "work_path", ",", "\"temp/imdb_tokenizer.pickle\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "imdb_tokenizer_file", ")", ":", "\n", "            ", "f", "=", "open", "(", "imdb_tokenizer_file", ",", "'rb'", ")", "\n", "imdb_tokenizer", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "            ", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "\n", "imdb_tokenizer", "=", "Tokenizer", "(", "num_words", "=", "config", ".", "num_words", "[", "dataset", "]", ",", "use_spacy", "=", "False", ")", "\n", "imdb_tokenizer", ".", "fit_on_texts", "(", "train_texts", ")", "\n", "f", "=", "open", "(", "imdb_tokenizer_file", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "imdb_tokenizer", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "return", "imdb_tokenizer", "\n", "", "elif", "dataset", "==", "'snli'", ":", "\n", "        ", "global", "snli_tokenizer", "\n", "if", "snli_tokenizer", "is", "not", "None", ":", "\n", "            ", "return", "snli_tokenizer", "\n", "\n", "", "snli_tokenizer_file", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "work_path", ",", "\"temp/snli_tokenizer.pickle\"", ")", "\n", "if", "os", ".", "path", ".", "exists", "(", "snli_tokenizer_file", ")", ":", "\n", "            ", "f", "=", "open", "(", "snli_tokenizer_file", ",", "'rb'", ")", "\n", "snli_tokenizer", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "            ", "train_perms", ",", "train_hypos", ",", "train_labels", ",", "dev_perms", ",", "dev_hypos", ",", "dev_labels", ",", "test_perms", ",", "test_hypos", ",", "test_labels", "=", "split_snli_files", "(", "opt", ")", "\n", "snli_tokenizer", "=", "Tokenizer", "(", "num_words", "=", "config", ".", "num_words", "[", "dataset", "]", ",", "use_spacy", "=", "False", ")", "\n", "snli_tokenizer", ".", "fit_on_texts", "(", "train_perms", "+", "train_hypos", ")", "\n", "f", "=", "open", "(", "snli_tokenizer_file", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "snli_tokenizer", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "return", "snli_tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single": [[81, 86], ["tokenizer.texts_to_sequences", "keras.preprocessing.sequence.pad_sequences"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences"], ["", "", "def", "text_process_for_single", "(", "tokenizer", ",", "texts", ",", "dataset", ")", ":", "\n", "    ", "maxlen", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "seq", "=", "tokenizer", ".", "texts_to_sequences", "(", "texts", ")", "\n", "seq", "=", "sequence", ".", "pad_sequences", "(", "seq", ",", "maxlen", "=", "maxlen", ",", "padding", "=", "'post'", ",", "truncating", "=", "'post'", ")", "\n", "return", "seq", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_process_for_single_bert": [[87, 103], ["tokenizer.encode_plus", "res.append"], "function", ["None"], ["", "def", "text_process_for_single_bert", "(", "tokenizer", ",", "texts", ",", "dataset", ")", ":", "\n", "    ", "maxlen", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "\n", "res", "=", "[", "]", "\n", "\n", "for", "text", "in", "texts", ":", "\n", "        ", "encoded", "=", "tokenizer", ".", "encode_plus", "(", "\n", "text", ",", "\n", "None", ",", "\n", "add_special_tokens", "=", "True", ",", "\n", "max_length", "=", "maxlen", ",", "\n", "pad_to_max_length", "=", "True", ",", "\n", ")", "\n", "res", ".", "append", "(", "encoded", ")", "\n", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.label_process_for_single": [[104, 109], ["numpy.array"], "function", ["None"], ["", "def", "label_process_for_single", "(", "tokenizer", ",", "labels", ",", "dataset", ")", ":", "\n", "    ", "maxlen", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "\n", "out", "=", "np", ".", "array", "(", "labels", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process": [[110, 121], ["word_level_process.get_tokenizer", "get_tokenizer.texts_to_sequences", "get_tokenizer.texts_to_sequences", "keras.preprocessing.sequence.pad_sequences", "keras.preprocessing.sequence.pad_sequences", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences"], ["", "def", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", ":", "\n", "    ", "maxlen", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "\n", "x_train_seq", "=", "tokenizer", ".", "texts_to_sequences", "(", "train_texts", ")", "\n", "x_test_seq", "=", "tokenizer", ".", "texts_to_sequences", "(", "test_texts", ")", "\n", "x_train", "=", "sequence", ".", "pad_sequences", "(", "x_train_seq", ",", "maxlen", "=", "maxlen", ",", "padding", "=", "'post'", ",", "truncating", "=", "'post'", ")", "\n", "x_test", "=", "sequence", ".", "pad_sequences", "(", "x_test_seq", ",", "maxlen", "=", "maxlen", ",", "padding", "=", "'post'", ",", "truncating", "=", "'post'", ")", "\n", "y_train", "=", "np", ".", "array", "(", "train_labels", ")", "\n", "y_test", "=", "np", ".", "array", "(", "test_labels", ")", "\n", "return", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector": [[123, 128], ["tokenizer.texts_to_sequences", "keras.preprocessing.sequence.pad_sequences"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences"], ["", "def", "text_to_vector", "(", "text", ",", "tokenizer", ",", "dataset", ")", ":", "\n", "    ", "maxlen", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "vector", "=", "tokenizer", ".", "texts_to_sequences", "(", "[", "text", "]", ")", "\n", "vector", "=", "sequence", ".", "pad_sequences", "(", "vector", ",", "maxlen", "=", "maxlen", ",", "padding", "=", "'post'", ",", "truncating", "=", "'post'", ")", "\n", "return", "vector", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector_for_all": [[130, 135], ["tokenizer.texts_to_sequences", "keras.preprocessing.sequence.pad_sequences"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences"], ["", "def", "text_to_vector_for_all", "(", "text_list", ",", "tokenizer", ",", "dataset", ")", ":", "\n", "    ", "maxlen", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "vector", "=", "tokenizer", ".", "texts_to_sequences", "(", "text_list", ")", "\n", "vector", "=", "sequence", ".", "pad_sequences", "(", "vector", ",", "maxlen", "=", "maxlen", ",", "padding", "=", "'post'", ",", "truncating", "=", "'post'", ")", "\n", "return", "vector", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.training.train_text_classifier": [[34, 100], ["sklearn.utils.shuffle", "print", "print", "print", "print", "print", "keras.callbacks.TensorBoard", "print", "print", "n.eural_networks.lstm.fit", "n.eural_networks.lstm.evaluate", "print", "print", "n.eural_networks.lstm.save_weights", "read_files.split_imdb_files", "n.eural_networks.word_cnn", "word_level_process.word_process", "read_files.split_agnews_files", "n.eural_networks.bd_lstm", "char_level_process.char_process", "word_level_process.word_process", "read_files.split_yahoo_files", "n.eural_networks.char_cnn", "char_level_process.char_process", "word_level_process.word_process", "n.eural_networks.lstm", "char_level_process.char_process"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.word_cnn", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_agnews_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.bd_lstm", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.char_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_yahoo_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.char_cnn", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.char_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.lstm", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.char_process"], ["def", "train_text_classifier", "(", ")", ":", "\n", "    ", "dataset", "=", "args", ".", "dataset", "\n", "x_train", "=", "y_train", "=", "x_test", "=", "y_test", "=", "None", "\n", "if", "dataset", "==", "'imdb'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "if", "args", ".", "level", "==", "'word'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "elif", "args", ".", "level", "==", "'char'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "char_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "", "elif", "dataset", "==", "'agnews'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_agnews_files", "(", ")", "\n", "if", "args", ".", "level", "==", "'word'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "elif", "args", ".", "level", "==", "'char'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "char_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "", "elif", "dataset", "==", "'yahoo'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_yahoo_files", "(", ")", "\n", "if", "args", ".", "level", "==", "'word'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "elif", "args", ".", "level", "==", "'char'", ":", "\n", "            ", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "char_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "", "", "x_train", ",", "y_train", "=", "shuffle", "(", "x_train", ",", "y_train", ",", "random_state", "=", "0", ")", "\n", "\n", "# Take a look at the shapes", "\n", "print", "(", "'dataset:'", ",", "dataset", ",", "'; model:'", ",", "args", ".", "model", ",", "'; level:'", ",", "args", ".", "level", ")", "\n", "print", "(", "'X_train:'", ",", "x_train", ".", "shape", ")", "\n", "print", "(", "'y_train:'", ",", "y_train", ".", "shape", ")", "\n", "print", "(", "'X_test:'", ",", "x_test", ".", "shape", ")", "\n", "print", "(", "'y_test:'", ",", "y_test", ".", "shape", ")", "\n", "\n", "log_dir", "=", "r'./logs/{}/{}/'", ".", "format", "(", "dataset", ",", "args", ".", "model", ")", "\n", "tb_callback", "=", "keras", ".", "callbacks", ".", "TensorBoard", "(", "log_dir", "=", "log_dir", ",", "histogram_freq", "=", "0", ",", "write_graph", "=", "True", ")", "\n", "\n", "model_path", "=", "r'./runs/{}/{}.dat'", ".", "format", "(", "dataset", ",", "args", ".", "model", ")", "\n", "model", "=", "batch_size", "=", "epochs", "=", "None", "\n", "assert", "args", ".", "model", "[", ":", "4", "]", "==", "args", ".", "level", "\n", "\n", "if", "args", ".", "model", "==", "\"word_cnn\"", ":", "\n", "        ", "model", "=", "word_cnn", "(", "dataset", ")", "\n", "batch_size", "=", "config", ".", "wordCNN_batch_size", "[", "dataset", "]", "\n", "epochs", "=", "config", ".", "wordCNN_epochs", "[", "dataset", "]", "\n", "", "elif", "args", ".", "model", "==", "\"word_bdlstm\"", ":", "\n", "        ", "model", "=", "bd_lstm", "(", "dataset", ")", "\n", "batch_size", "=", "config", ".", "bdLSTM_batch_size", "[", "dataset", "]", "\n", "epochs", "=", "config", ".", "bdLSTM_epochs", "[", "dataset", "]", "\n", "", "elif", "args", ".", "model", "==", "\"char_cnn\"", ":", "\n", "        ", "model", "=", "char_cnn", "(", "dataset", ")", "\n", "batch_size", "=", "config", ".", "charCNN_batch_size", "[", "dataset", "]", "\n", "epochs", "=", "config", ".", "charCNN_epochs", "[", "dataset", "]", "\n", "", "elif", "args", ".", "model", "==", "\"word_lstm\"", ":", "\n", "        ", "model", "=", "lstm", "(", "dataset", ")", "\n", "batch_size", "=", "config", ".", "LSTM_batch_size", "[", "dataset", "]", "\n", "epochs", "=", "config", ".", "LSTM_epochs", "[", "dataset", "]", "\n", "\n", "", "print", "(", "'Train...'", ")", "\n", "print", "(", "'batch_size: '", ",", "batch_size", ",", "\"; epochs: \"", ",", "epochs", ")", "\n", "model", ".", "fit", "(", "x_train", ",", "y_train", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "epochs", "=", "epochs", ",", "\n", "validation_split", "=", "0.2", ",", "\n", "shuffle", "=", "True", ",", "\n", "callbacks", "=", "[", "tb_callback", "]", ")", "\n", "scores", "=", "model", ".", "evaluate", "(", "x_test", ",", "y_test", ")", "\n", "print", "(", "'test_loss: %f, accuracy: %f'", "%", "(", "scores", "[", "0", "]", ",", "scores", "[", "1", "]", ")", ")", "\n", "print", "(", "'Saving model weights...'", ")", "\n", "model", ".", "save_weights", "(", "model_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool_pytorch.write_origin_input_texts": [[23, 29], ["len", "open", "range", "f.write"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.unbuffered.Unbuffered.write"], ["def", "write_origin_input_texts", "(", "origin_input_texts_path", ",", "test_texts", ",", "test_samples_cap", "=", "None", ")", ":", "\n", "    ", "if", "test_samples_cap", "is", "None", ":", "\n", "        ", "test_samples_cap", "=", "len", "(", "test_texts", ")", "\n", "", "with", "open", "(", "origin_input_texts_path", ",", "'a'", ")", "as", "f", ":", "\n", "        ", "for", "i", "in", "range", "(", "test_samples_cap", ")", ":", "\n", "            ", "f", ".", "write", "(", "test_texts", "[", "i", "]", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool_pytorch.genetic_attack": [[30, 100], ["GeneticAdversary", "AdversarialModel", "GeneticAdversary.run", "print", "ModifiedBertTokenizer.from_pretrained", "word_level_process.get_tokenizer", "read_files.split_imdb_files", "read_files.split_agnews_files", "read_files.split_yahoo_files"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary.run", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_agnews_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_yahoo_files"], ["", "", "", "def", "genetic_attack", "(", "opt", ",", "device", ",", "model", ",", "attack_surface", ",", "dataset", "=", "'imdb'", ",", "genetic_test_num", "=", "100", ",", "test_bert", "=", "False", ")", ":", "\n", "\n", "    ", "if", "test_bert", ":", "\n", "        ", "from", "modified_bert_tokenizer", "import", "ModifiedBertTokenizer", "\n", "tokenizer", "=", "ModifiedBertTokenizer", ".", "from_pretrained", "(", "\"bert-base-uncased\"", ",", "do_lower_case", "=", "True", ")", "\n", "", "else", ":", "\n", "# get tokenizer", "\n", "        ", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "\n", "# Read data set", "\n", "", "x_test", "=", "y_test", "=", "None", "\n", "test_texts", "=", "None", "\n", "\n", "\"\"\"\n    if opt.synonyms_from_file:\n\n        if dataset == 'imdb':\n            train_texts, train_labels, dev_texts, dev_labels, test_texts, test_labels = split_imdb_files(opt)\n        elif dataset == 'agnews':\n            train_texts, train_labels, test_texts, test_labels = split_agnews_files()\n        elif dataset == 'yahoo':\n            train_texts, train_labels, test_texts, test_labels = split_yahoo_files()\n\n        filename= opt.synonyms_file_path\n        f=open(filename,'rb')\n        saved=pickle.load(f)\n        f.close()\n        #syn_data = saved[\"syn_data\"]\n        #opt.embeddings=saved['embeddings']\n        #opt.vocab_size=saved['vocab_size']\n        x_train=saved['x_train']\n        x_test=saved['x_test']\n        y_train=saved['y_train']\n        y_test=saved['y_test']\n\n    else:\n        if dataset == 'imdb':\n            train_texts, train_labels, dev_texts, dev_labels, test_texts, test_labels = split_imdb_files(opt)\n            x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)\n\n        elif dataset == 'agnews':\n            train_texts, train_labels, test_texts, test_labels = split_agnews_files()\n            x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)\n\n        elif dataset == 'yahoo':\n            train_texts, train_labels, test_texts, test_labels = split_yahoo_files()\n            x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)\n    \"\"\"", "\n", "\n", "if", "dataset", "==", "'imdb'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "#x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)", "\n", "\n", "", "elif", "dataset", "==", "'agnews'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_agnews_files", "(", ")", "\n", "#x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)", "\n", "\n", "", "elif", "dataset", "==", "'yahoo'", ":", "\n", "        ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_yahoo_files", "(", ")", "\n", "#x_train, y_train, x_test, y_test = word_process(train_texts, train_labels, test_texts, test_labels, dataset)", "\n", "\n", "", "from", ".", "attacks", "import", "GeneticAdversary", ",", "AdversarialModel", "\n", "adversary", "=", "GeneticAdversary", "(", "attack_surface", ",", "num_iters", "=", "opt", ".", "genetic_iters", ",", "pop_size", "=", "opt", ".", "genetic_pop_size", ")", "\n", "\n", "from", ".", "config", "import", "config", "\n", "wrapped_model", "=", "AdversarialModel", "(", "model", ",", "tokenizer", ",", "config", ".", "word_max_len", "[", "dataset", "]", ",", "test_bert", "=", "test_bert", ")", "\n", "\n", "adv_acc", "=", "adversary", ".", "run", "(", "wrapped_model", ",", "test_texts", ",", "test_labels", ",", "device", ",", "genetic_test_num", ",", "opt", ")", "\n", "print", "(", "\"genetic attack results:\"", ",", "adv_acc", ")", "\n", "return", "adv_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool_pytorch.genetic_attack_snli": [[101, 132], ["word_level_process.get_tokenizer", "GeneticAdversary_Snli", "AdversarialModel_Snli", "GeneticAdversary_Snli.run", "print", "read_files.split_snli_files"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary.run", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_snli_files"], ["", "def", "genetic_attack_snli", "(", "opt", ",", "device", ",", "model", ",", "attack_surface", ",", "dataset", "=", "'snli'", ",", "genetic_test_num", "=", "100", ",", "split", "=", "\"test\"", ")", ":", "\n", "\n", "# get tokenizer", "\n", "    ", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "\n", "# Read data set", "\n", "x_test", "=", "y_test", "=", "None", "\n", "test_texts", "=", "None", "\n", "\n", "if", "dataset", "==", "'snli'", ":", "\n", "        ", "train_perms", ",", "train_hypos", ",", "train_labels", ",", "dev_perms", ",", "dev_hypos", ",", "dev_labels", ",", "test_perms", ",", "test_hypos", ",", "test_labels", "=", "split_snli_files", "(", "opt", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "if", "split", "==", "\"test\"", ":", "\n", "        ", "perms", "=", "test_perms", "\n", "hypos", "=", "test_hypos", "\n", "", "elif", "split", "==", "\"dev\"", ":", "\n", "        ", "perms", "=", "dev_perms", "\n", "hypos", "=", "dev_hypos", "\n", "\n", "", "from", ".", "attacks", "import", "GeneticAdversary_Snli", ",", "AdversarialModel_Snli", "\n", "adversary", "=", "GeneticAdversary_Snli", "(", "attack_surface", ",", "num_iters", "=", "opt", ".", "genetic_iters", ",", "pop_size", "=", "opt", ".", "genetic_pop_size", ")", "\n", "\n", "from", ".", "config", "import", "config", "\n", "wrapped_model", "=", "AdversarialModel_Snli", "(", "model", ",", "tokenizer", ",", "config", ".", "word_max_len", "[", "dataset", "]", ")", "\n", "\n", "adv_acc", "=", "adversary", ".", "run", "(", "wrapped_model", ",", "test_perms", ",", "test_hypos", ",", "test_labels", ",", "device", ",", "genetic_test_num", ",", "opt", ")", "\n", "print", "(", "\"genetic attack results:\"", ",", "adv_acc", ")", "\n", "\n", "return", "adv_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool_pytorch.fool_text_classifier_pytorch": [[134, 239], ["print", "word_level_process.get_tokenizer", "adversarial_tools.ForwardGradWrapper_pytorch", "adversarial_tools.ForwardGradWrapper_pytorch.predict_classes", "print", "time.clock", "enumerate", "time.clock", "print", "print", "print", "print", "print", "print", "print", "open", "pickle.load", "open.close", "os.path.exists", "os.makedirs", "str", "str", "print", "sum", "len", "sum", "len", "read_files.split_imdb_files", "read_files.split_imdb_files", "word_level_process.word_process", "numpy.argmax", "print", "adversarial_tools.adversarial_paraphrase", "print", "sub_rate_list.append", "NE_rate_list.append", "read_files.split_agnews_files", "read_files.split_agnews_files", "word_level_process.word_process", "numpy.argmax", "print", "print", "read_files.split_yahoo_files", "read_files.split_yahoo_files", "word_level_process.word_process", "numpy.argmax"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_classes", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_imdb_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.adversarial_paraphrase", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_agnews_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_agnews_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_yahoo_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_yahoo_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.word_process"], ["", "def", "fool_text_classifier_pytorch", "(", "opt", ",", "device", ",", "model", ",", "dataset", "=", "'imdb'", ",", "clean_samples_cap", "=", "50", ")", ":", "\n", "    ", "print", "(", "'clean_samples_cap:'", ",", "clean_samples_cap", ")", "\n", "\n", "# get tokenizer", "\n", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "\n", "# Read data set", "\n", "x_test", "=", "y_test", "=", "None", "\n", "test_texts", "=", "None", "\n", "\n", "if", "opt", ".", "synonyms_from_file", ":", "\n", "\n", "        ", "if", "dataset", "==", "'imdb'", ":", "\n", "            ", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "", "elif", "dataset", "==", "'agnews'", ":", "\n", "            ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_agnews_files", "(", ")", "\n", "", "elif", "dataset", "==", "'yahoo'", ":", "\n", "            ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_yahoo_files", "(", ")", "\n", "\n", "", "filename", "=", "opt", ".", "imdb_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "#syn_data = saved[\"syn_data\"]", "\n", "#opt.embeddings=saved['embeddings']", "\n", "#opt.vocab_size=saved['vocab_size']", "\n", "x_train", "=", "saved", "[", "'x_train'", "]", "\n", "x_test", "=", "saved", "[", "'x_test'", "]", "\n", "y_train", "=", "saved", "[", "'y_train'", "]", "\n", "y_test", "=", "saved", "[", "'y_test'", "]", "\n", "\n", "", "else", ":", "\n", "        ", "if", "dataset", "==", "'imdb'", ":", "\n", "            ", "train_texts", ",", "train_labels", ",", "dev_texts", ",", "dev_labels", ",", "test_texts", ",", "test_labels", "=", "split_imdb_files", "(", "opt", ")", "\n", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "\n", "", "elif", "dataset", "==", "'agnews'", ":", "\n", "            ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_agnews_files", "(", ")", "\n", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "\n", "", "elif", "dataset", "==", "'yahoo'", ":", "\n", "            ", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", "=", "split_yahoo_files", "(", ")", "\n", "x_train", ",", "y_train", ",", "x_test", ",", "y_test", "=", "word_process", "(", "train_texts", ",", "train_labels", ",", "test_texts", ",", "test_labels", ",", "dataset", ")", "\n", "\n", "", "", "grad_guide", "=", "ForwardGradWrapper_pytorch", "(", "model", ",", "device", ")", "\n", "classes_prediction", "=", "grad_guide", ".", "predict_classes", "(", "x_test", "[", ":", "clean_samples_cap", "]", ")", "\n", "\n", "print", "(", "'Crafting adversarial examples...'", ")", "\n", "successful_perturbations", "=", "0", "\n", "failed_perturbations", "=", "0", "\n", "all_test_num", "=", "0", "\n", "\n", "sub_rate_list", "=", "[", "]", "\n", "NE_rate_list", "=", "[", "]", "\n", "\n", "start_cpu", "=", "time", ".", "clock", "(", ")", "\n", "fa_path", "=", "r'./fool_result/{}'", ".", "format", "(", "dataset", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "fa_path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "fa_path", ")", "\n", "", "adv_text_path", "=", "r'./fool_result/{}/adv_{}.txt'", ".", "format", "(", "dataset", ",", "str", "(", "clean_samples_cap", ")", ")", "\n", "change_tuple_path", "=", "r'./fool_result/{}/change_tuple_{}.txt'", ".", "format", "(", "dataset", ",", "str", "(", "clean_samples_cap", ")", ")", "\n", "#file_1 = open(adv_text_path, \"a\")", "\n", "#file_2 = open(change_tuple_path, \"a\")", "\n", "\n", "for", "index", ",", "text", "in", "enumerate", "(", "test_texts", "[", "opt", ".", "h_test_start", ":", "opt", ".", "h_test_start", "+", "clean_samples_cap", "]", ")", ":", "\n", "        ", "sub_rate", "=", "0", "\n", "NE_rate", "=", "0", "\n", "all_test_num", "+=", "1", "\n", "print", "(", "'_____{}______.'", ".", "format", "(", "index", ")", ")", "\n", "if", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", "==", "classes_prediction", "[", "index", "]", ":", "\n", "#if True:", "\n", "            ", "print", "(", "'do'", ")", "\n", "# If the ground_true label is the same as the predicted label", "\n", "adv_doc", ",", "adv_y", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "=", "adversarial_paraphrase", "(", "opt", ",", "\n", "input_text", "=", "text", ",", "\n", "true_y", "=", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", ",", "\n", "grad_guide", "=", "grad_guide", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "dataset", "=", "dataset", ",", "\n", "level", "=", "'word'", ")", "\n", "if", "adv_y", "!=", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", ":", "\n", "                ", "successful_perturbations", "+=", "1", "\n", "print", "(", "'{}. Successful example crafted.'", ".", "format", "(", "index", ")", ")", "\n", "", "else", ":", "\n", "                ", "failed_perturbations", "+=", "1", "\n", "print", "(", "'{}. Failure.'", ".", "format", "(", "index", ")", ")", "\n", "\n", "", "print", "(", "\"r acc\"", ",", "1.0", "*", "failed_perturbations", "/", "all_test_num", ")", "\n", "\n", "text", "=", "adv_doc", "\n", "sub_rate_list", ".", "append", "(", "sub_rate", ")", "\n", "NE_rate_list", ".", "append", "(", "NE_rate", ")", "\n", "#file_2.write(str(index) + str(change_tuple_list) + '\\n')", "\n", "#file_1.write(text + \" sub_rate: \" + str(sub_rate) + \"; NE_rate: \" + str(NE_rate) + \"\\n\")", "\n", "", "", "end_cpu", "=", "time", ".", "clock", "(", ")", "\n", "print", "(", "'CPU second:'", ",", "end_cpu", "-", "start_cpu", ")", "\n", "\n", "#mean_sub_rate = sum(sub_rate_list) / len(sub_rate_list)", "\n", "#mean_NE_rate = sum(NE_rate_list) / len(NE_rate_list)", "\n", "print", "(", "'substitution:'", ",", "sum", "(", "sub_rate_list", ")", ")", "\n", "print", "(", "'sum substitution:'", ",", "len", "(", "sub_rate_list", ")", ")", "\n", "print", "(", "'NE rate:'", ",", "sum", "(", "NE_rate_list", ")", ")", "\n", "print", "(", "'sum NE:'", ",", "len", "(", "NE_rate_list", ")", ")", "\n", "print", "(", "\"succ attack %d\"", "%", "(", "successful_perturbations", ")", ")", "\n", "print", "(", "\"fail attack %d\"", "%", "(", "failed_perturbations", ")", ")", "\n", "#file_1.close()", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.fool_pytorch.fool_text_classifier_pytorch_snli": [[245, 321], ["print", "word_level_process.get_tokenizer", "open", "pickle.load", "open.close", "adversarial_tools.ForwardGradWrapper_pytorch_snli", "adversarial_tools.ForwardGradWrapper_pytorch_snli.predict_classes", "print", "time.clock", "range", "time.clock", "print", "print", "read_files.split_snli_files", "print", "print", "sub_rate_list.append", "NE_rate_list.append", "numpy.argmax", "print", "adversarial_tools.adversarial_paraphrase_snli", "numpy.argmax", "print", "print", "numpy.argmax"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.get_tokenizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_classes", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.read_files.split_snli_files", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.adversarial_paraphrase_snli"], ["", "def", "fool_text_classifier_pytorch_snli", "(", "opt", ",", "device", ",", "model", ",", "dataset", "=", "'imdb'", ",", "clean_samples_cap", "=", "50", ")", ":", "\n", "    ", "print", "(", "'clean_samples_cap:'", ",", "clean_samples_cap", ")", "\n", "\n", "# get tokenizer", "\n", "tokenizer", "=", "get_tokenizer", "(", "opt", ")", "\n", "\n", "# Read data set", "\n", "x_test", "=", "y_test", "=", "None", "\n", "test_texts", "=", "None", "\n", "\n", "if", "dataset", "==", "'snli'", ":", "\n", "        ", "train_perms", ",", "train_hypos", ",", "train_labels", ",", "dev_perms", ",", "dev_hypos", ",", "dev_labels", ",", "test_perms", ",", "test_hypos", ",", "test_labels", "=", "split_snli_files", "(", "opt", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n", "", "assert", "(", "opt", ".", "synonyms_from_file", ")", "\n", "filename", "=", "opt", ".", "snli_synonyms_file_path", "\n", "f", "=", "open", "(", "filename", ",", "'rb'", ")", "\n", "saved", "=", "pickle", ".", "load", "(", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "syn_data", "=", "saved", "[", "\"syn_data\"", "]", "\n", "opt", ".", "embeddings", "=", "saved", "[", "'embeddings'", "]", "\n", "opt", ".", "vocab_size", "=", "saved", "[", "'vocab_size'", "]", "\n", "x_p_train", "=", "saved", "[", "'x_p_train'", "]", "\n", "x_h_train", "=", "saved", "[", "'x_h_train'", "]", "\n", "y_train", "=", "saved", "[", "'y_train'", "]", "\n", "x_p_test", "=", "saved", "[", "'x_p_test'", "]", "\n", "x_h_test", "=", "saved", "[", "'x_h_test'", "]", "\n", "y_test", "=", "saved", "[", "'y_test'", "]", "\n", "\n", "grad_guide", "=", "ForwardGradWrapper_pytorch_snli", "(", "model", ",", "device", ")", "\n", "classes_prediction", "=", "grad_guide", ".", "predict_classes", "(", "x_p_test", "[", ":", "clean_samples_cap", "]", ",", "x_h_test", "[", ":", "clean_samples_cap", "]", ")", "\n", "\n", "print", "(", "'Crafting adversarial examples...'", ")", "\n", "successful_perturbations", "=", "0", "\n", "failed_perturbations", "=", "0", "\n", "all_test_num", "=", "0", "\n", "\n", "sub_rate_list", "=", "[", "]", "\n", "NE_rate_list", "=", "[", "]", "\n", "\n", "start_cpu", "=", "time", ".", "clock", "(", ")", "\n", "\n", "for", "index", "in", "range", "(", "opt", ".", "h_test_start", ",", "opt", ".", "h_test_start", "+", "clean_samples_cap", ",", "1", ")", ":", "\n", "\n", "        ", "text_p", "=", "test_perms", "[", "index", "]", "\n", "text_h", "=", "test_hypos", "[", "index", "]", "\n", "\n", "sub_rate", "=", "0", "\n", "NE_rate", "=", "0", "\n", "all_test_num", "+=", "1", "\n", "print", "(", "'_____{}______.'", ".", "format", "(", "index", ")", ")", "\n", "if", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", "==", "classes_prediction", "[", "index", "]", ":", "\n", "            ", "print", "(", "'do'", ")", "\n", "# If the ground_true label is the same as the predicted label", "\n", "adv_doc_p", ",", "adv_doc_h", ",", "adv_y", ",", "sub_rate", ",", "NE_rate", ",", "change_tuple_list", "=", "adversarial_paraphrase_snli", "(", "opt", ",", "input_text_p", "=", "text_p", ",", "input_text_h", "=", "text_h", ",", "\n", "true_y", "=", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", ",", "\n", "grad_guide", "=", "grad_guide", ",", "\n", "tokenizer", "=", "tokenizer", ",", "\n", "dataset", "=", "dataset", ",", "\n", "level", "=", "'word'", ")", "\n", "if", "adv_y", "!=", "np", ".", "argmax", "(", "y_test", "[", "index", "]", ")", ":", "\n", "                ", "successful_perturbations", "+=", "1", "\n", "print", "(", "'{}. Successful example crafted.'", ".", "format", "(", "index", ")", ")", "\n", "", "else", ":", "\n", "                ", "failed_perturbations", "+=", "1", "\n", "print", "(", "'{}. Failure.'", ".", "format", "(", "index", ")", ")", "\n", "\n", "", "", "print", "(", "\"r acc\"", ",", "1.0", "*", "failed_perturbations", "/", "all_test_num", ")", "\n", "\n", "sub_rate_list", ".", "append", "(", "sub_rate", ")", "\n", "NE_rate_list", ".", "append", "(", "NE_rate", ")", "\n", "\n", "", "end_cpu", "=", "time", ".", "clock", "(", ")", "\n", "print", "(", "'CPU second:'", ",", "end_cpu", "-", "start_cpu", ")", "\n", "print", "(", "\"PWWS acc:\"", ",", "1.0", "*", "failed_perturbations", "/", "all_test_num", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.evaluate_word_saliency.evaluate_word_saliency": [[11, 62], ["word_level_process.text_to_vector", "grad_guide.predict_prob", "range", "position_word_list.append", "len", "copy.deepcopy", "grad_guide.predict_prob", "word_saliency_list.append", "char_level_process.get_embedding_dict", "char_level_process.doc_process().reshape", "grad_guide.predict_prob", "copy.deepcopy", "enumerate", "range", "len", "char_level_process.doc_process", "grad_guide.predict_prob", "word_saliency_list.append", "copy.deepcopy", "doc.text.lower"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.get_embedding_dict", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.char_level_process.doc_process", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob"], ["def", "evaluate_word_saliency", "(", "doc", ",", "grad_guide", ",", "tokenizer", ",", "input_y", ",", "dataset", ",", "level", ")", ":", "\n", "    ", "word_saliency_list", "=", "[", "]", "\n", "\n", "# zero the code of the current word and calculate the amount of change in the classification probability", "\n", "if", "level", "==", "'word'", ":", "\n", "        ", "max_len", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "text", "=", "[", "doc", "[", "position", "]", ".", "text", "for", "position", "in", "range", "(", "len", "(", "doc", ")", ")", "]", "\n", "text", "=", "' '", ".", "join", "(", "text", ")", "\n", "origin_vector", "=", "text_to_vector", "(", "text", ",", "tokenizer", ",", "dataset", ")", "\n", "origin_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector", "=", "origin_vector", ")", "\n", "for", "position", "in", "range", "(", "len", "(", "doc", ")", ")", ":", "\n", "            ", "if", "position", ">=", "max_len", ":", "\n", "                ", "break", "\n", "# get x_i^(\\hat)", "\n", "", "without_word_vector", "=", "copy", ".", "deepcopy", "(", "origin_vector", ")", "\n", "without_word_vector", "[", "0", "]", "[", "position", "]", "=", "0", "\n", "prob_without_word", "=", "grad_guide", ".", "predict_prob", "(", "input_vector", "=", "without_word_vector", ")", "\n", "\n", "# calculate S(x,w_i) defined in Eq.(6)", "\n", "word_saliency", "=", "origin_prob", "[", "input_y", "]", "-", "prob_without_word", "[", "input_y", "]", "\n", "word_saliency_list", ".", "append", "(", "(", "position", ",", "doc", "[", "position", "]", ",", "word_saliency", ",", "doc", "[", "position", "]", ".", "tag_", ")", ")", "\n", "\n", "", "", "elif", "level", "==", "'char'", ":", "\n", "        ", "max_len", "=", "config", ".", "char_max_len", "[", "dataset", "]", "\n", "embedding_dic", "=", "get_embedding_dict", "(", ")", "\n", "origin_vector", "=", "doc_process", "(", "doc", ".", "text", ".", "lower", "(", ")", ",", "embedding_dic", ",", "dataset", ")", ".", "reshape", "(", "1", ",", "max_len", ")", "\n", "origin_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector", "=", "origin_vector", ")", "\n", "\n", "find_a_word", "=", "False", "\n", "word_position", "=", "0", "\n", "without_word_vector", "=", "copy", ".", "deepcopy", "(", "origin_vector", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "doc", ".", "text", ")", ":", "\n", "            ", "if", "i", ">=", "max_len", ":", "\n", "                ", "break", "\n", "", "if", "c", "is", "not", "' '", ":", "\n", "                ", "without_word_vector", "[", "0", "]", "[", "i", "]", "=", "0", "\n", "", "else", ":", "\n", "                ", "find_a_word", "=", "True", "\n", "prob_without_word", "=", "grad_guide", ".", "predict_prob", "(", "without_word_vector", ")", "\n", "word_saliency", "=", "origin_prob", "[", "input_y", "]", "-", "prob_without_word", "[", "input_y", "]", "\n", "word_saliency_list", ".", "append", "(", "(", "word_position", ",", "doc", "[", "word_position", "]", ",", "word_saliency", ",", "doc", "[", "word_position", "]", ".", "tag_", ")", ")", "\n", "word_position", "+=", "1", "\n", "", "if", "find_a_word", ":", "\n", "                ", "without_word_vector", "=", "copy", ".", "deepcopy", "(", "origin_vector", ")", "\n", "find_a_word", "=", "False", "\n", "\n", "", "", "", "position_word_list", "=", "[", "]", "\n", "for", "word", "in", "word_saliency_list", ":", "\n", "        ", "position_word_list", ".", "append", "(", "(", "word", "[", "0", "]", ",", "word", "[", "1", "]", ")", ")", "\n", "\n", "", "return", "position_word_list", ",", "word_saliency_list", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.evaluate_word_saliency.evaluate_word_saliency_snli": [[64, 95], ["word_level_process.text_to_vector", "word_level_process.text_to_vector", "grad_guide.predict_prob", "range", "position_word_list.append", "len", "copy.deepcopy", "grad_guide.predict_prob", "word_saliency_list.append", "range", "range", "len", "len"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.word_level_process.text_to_vector", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.adversarial_tools.ForwardGradWrapper_pytorch.predict_prob"], ["", "def", "evaluate_word_saliency_snli", "(", "doc_p", ",", "doc_h", ",", "grad_guide", ",", "tokenizer", ",", "input_y", ",", "dataset", ",", "level", ")", ":", "\n", "    ", "word_saliency_list", "=", "[", "]", "\n", "\n", "# zero the code of the current word and calculate the amount of change in the classification probability", "\n", "if", "level", "==", "'word'", ":", "\n", "        ", "max_len", "=", "config", ".", "word_max_len", "[", "dataset", "]", "\n", "text_p", "=", "[", "doc_p", "[", "position", "]", ".", "text", "for", "position", "in", "range", "(", "len", "(", "doc_p", ")", ")", "]", "\n", "text_p", "=", "' '", ".", "join", "(", "text_p", ")", "\n", "origin_vector_p", "=", "text_to_vector", "(", "text_p", ",", "tokenizer", ",", "dataset", ")", "\n", "text_h", "=", "[", "doc_h", "[", "position", "]", ".", "text", "for", "position", "in", "range", "(", "len", "(", "doc_h", ")", ")", "]", "\n", "text_h", "=", "' '", ".", "join", "(", "text_h", ")", "\n", "origin_vector_h", "=", "text_to_vector", "(", "text_h", ",", "tokenizer", ",", "dataset", ")", "\n", "origin_prob", "=", "grad_guide", ".", "predict_prob", "(", "input_vector_p", "=", "origin_vector_p", ",", "input_vector_h", "=", "origin_vector_h", ")", "\n", "#attack h", "\n", "for", "position", "in", "range", "(", "len", "(", "doc_h", ")", ")", ":", "\n", "            ", "if", "position", ">=", "max_len", ":", "\n", "                ", "break", "\n", "# get x_i^(\\hat)", "\n", "", "without_word_vector_h", "=", "copy", ".", "deepcopy", "(", "origin_vector_h", ")", "\n", "without_word_vector_h", "[", "0", "]", "[", "position", "]", "=", "0", "\n", "prob_without_word", "=", "grad_guide", ".", "predict_prob", "(", "input_vector_p", "=", "origin_vector_p", ",", "input_vector_h", "=", "without_word_vector_h", ")", "\n", "\n", "# calculate S(x,w_i) defined in Eq.(6)", "\n", "word_saliency", "=", "origin_prob", "[", "input_y", "]", "-", "prob_without_word", "[", "input_y", "]", "\n", "word_saliency_list", ".", "append", "(", "(", "position", ",", "doc_h", "[", "position", "]", ",", "word_saliency", ",", "doc_h", "[", "position", "]", ".", "tag_", ")", ")", "\n", "\n", "", "", "position_word_list", "=", "[", "]", "\n", "for", "word", "in", "word_saliency_list", ":", "\n", "        ", "position_word_list", ".", "append", "(", "(", "word", "[", "0", "]", ",", "word", "[", "1", "]", ")", ")", "\n", "\n", "", "return", "position_word_list", ",", "word_saliency_list", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel_Snli.__init__": [[25, 31], ["object.__init__", "model.eval"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "tokenizer", ",", "maxlen", ")", ":", "\n", "        ", "super", "(", "AdversarialModel_Snli", ",", "self", ")", ".", "__init__", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "maxlen", "=", "maxlen", "\n", "", "def", "query", "(", "self", ",", "x_p", ",", "x_h", ",", "device", ")", ":", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel_Snli.query": [[31, 55], ["attacks.AdversarialModel_Snli.tokenizer.texts_to_sequences", "numpy.array", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "keras.preprocessing.sequence.pad_sequences", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "attacks.AdversarialModel_Snli.tokenizer.texts_to_sequences", "numpy.array", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "keras.preprocessing.sequence.pad_sequences", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "attacks.AdversarialModel_Snli.model().squeeze", "attacks.AdversarialModel_Snli.detach().cpu().numpy", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "attacks.AdversarialModel_Snli.model", "attacks.AdversarialModel_Snli.detach().cpu", "range", "range", "range", "range", "len", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "len", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "attacks.AdversarialModel_Snli.detach", "len", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences"], ["", "def", "query", "(", "self", ",", "x_p", ",", "x_h", ",", "device", ")", ":", "\n", "        ", "x_p", "=", "self", ".", "tokenizer", ".", "texts_to_sequences", "(", "[", "x_p", "]", ")", "\n", "x_p_mask", "=", "[", "1", "for", "i", "in", "range", "(", "len", "(", "x_p", "[", "0", "]", ")", ")", "]", "+", "[", "0", "for", "i", "in", "range", "(", "self", ".", "maxlen", "-", "len", "(", "x_p", "[", "0", "]", ")", ")", "]", "\n", "x_p_mask", "=", "[", "x_p_mask", "]", "\n", "x_p_mask", "=", "np", ".", "array", "(", "x_p_mask", ")", "\n", "x_p_mask", "=", "torch", ".", "from_numpy", "(", "x_p_mask", ")", ".", "to", "(", "device", ")", ".", "to", "(", "torch", ".", "float", ")", "\n", "\n", "x_p", "=", "sequence", ".", "pad_sequences", "(", "x_p", ",", "maxlen", "=", "self", ".", "maxlen", ",", "padding", "=", "'post'", ",", "truncating", "=", "'post'", ")", "\n", "x_p", "=", "torch", ".", "from_numpy", "(", "x_p", ")", ".", "to", "(", "device", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "\n", "x_h", "=", "self", ".", "tokenizer", ".", "texts_to_sequences", "(", "[", "x_h", "]", ")", "\n", "x_h_mask", "=", "[", "1", "for", "i", "in", "range", "(", "len", "(", "x_h", "[", "0", "]", ")", ")", "]", "+", "[", "0", "for", "i", "in", "range", "(", "self", ".", "maxlen", "-", "len", "(", "x_h", "[", "0", "]", ")", ")", "]", "\n", "x_h_mask", "=", "[", "x_h_mask", "]", "\n", "x_h_mask", "=", "np", ".", "array", "(", "x_h_mask", ")", "\n", "x_h_mask", "=", "torch", ".", "from_numpy", "(", "x_h_mask", ")", ".", "to", "(", "device", ")", ".", "to", "(", "torch", ".", "float", ")", "\n", "\n", "x_h", "=", "sequence", ".", "pad_sequences", "(", "x_h", ",", "maxlen", "=", "self", ".", "maxlen", ",", "padding", "=", "'post'", ",", "truncating", "=", "'post'", ")", "\n", "x_h", "=", "torch", ".", "from_numpy", "(", "x_h", ")", ".", "to", "(", "device", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "#input_vector = np.zeros((1, self.maxlen))", "\n", "#input_vector = x", "\n", "\n", "logit", "=", "self", ".", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "x_p", "=", "x_p", ",", "x_h", "=", "x_h", ",", "x_p_mask", "=", "x_p_mask", ",", "x_h_mask", "=", "x_h_mask", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "return", "logit", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "#p = F.softmax(logit).detach().cpu().numpy()", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.__init__": [[59, 66], ["object.__init__", "model.eval"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "model", ",", "tokenizer", ",", "maxlen", ",", "test_bert", "=", "False", ")", ":", "\n", "        ", "super", "(", "AdversarialModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "model", ".", "eval", "(", ")", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "maxlen", "=", "maxlen", "\n", "self", ".", "test_bert", "=", "test_bert", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.tokenize": [[67, 78], ["attacks.AdversarialModel.tokenizer.encode_plus", "attacks.AdversarialModel.tokenizer.texts_to_sequences", "keras.preprocessing.sequence.pad_sequences"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.tokenizer_for_spacy.Tokenizer.texts_to_sequences"], ["", "def", "tokenize", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "test_bert", ":", "\n", "#print(len(x.split()))", "\n", "#print(x)", "\n", "            ", "token", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "x", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "maxlen", ",", "pad_to_max_length", "=", "True", ")", "\n", "\n", "return", "token", "\n", "", "else", ":", "\n", "            ", "token", "=", "self", ".", "tokenizer", ".", "texts_to_sequences", "(", "[", "x", "]", ")", "\n", "token", "=", "sequence", ".", "pad_sequences", "(", "token", ",", "maxlen", "=", "self", ".", "maxlen", ",", "padding", "=", "'post'", ",", "truncating", "=", "'post'", ")", "\n", "return", "token", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.query": [[79, 104], ["torch.softmax().detach().cpu().numpy", "torch.softmax().detach().cpu().numpy", "attacks.AdversarialModel.tokenize", "numpy.array", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "numpy.array", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "numpy.array", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "attacks.AdversarialModel.model().squeeze", "attacks.AdversarialModel.tokenize", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "torch.from_numpy().to().to", "attacks.AdversarialModel.model().squeeze", "torch.softmax().detach().cpu", "torch.softmax().detach().cpu", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "attacks.AdversarialModel.model", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "attacks.AdversarialModel.model", "torch.softmax().detach", "torch.softmax().detach", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.softmax", "torch.softmax"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.tokenize", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.tokenize", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "", "def", "query", "(", "self", ",", "x", ",", "device", ")", ":", "\n", "\n", "#input_vector = self.tokenizer.texts_to_sequences([x])", "\n", "#input_vector = sequence.pad_sequences(input_vector, maxlen=self.maxlen, padding='post', truncating='post')", "\n", "\n", "        ", "if", "self", ".", "test_bert", ":", "\n", "            ", "token", "=", "self", ".", "tokenize", "(", "x", ")", "\n", "\n", "text", "=", "np", ".", "array", "(", "[", "token", "[", "'input_ids'", "]", "]", ")", "\n", "text", "=", "torch", ".", "tensor", "(", "text", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "device", ")", "\n", "mask", "=", "np", ".", "array", "(", "[", "token", "[", "'attention_mask'", "]", "]", ")", "\n", "#print(mask.sum())", "\n", "mask", "=", "torch", ".", "tensor", "(", "mask", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "device", ")", "\n", "token_type_ids", "=", "np", ".", "array", "(", "[", "token", "[", "\"token_type_ids\"", "]", "]", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "token_type_ids", ",", "dtype", "=", "torch", ".", "long", ")", ".", "to", "(", "device", ")", "\n", "\n", "logit", "=", "self", ".", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "input", "=", "text", ",", "bert_mask", "=", "mask", ",", "bert_token_id", "=", "token_type_ids", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "", "else", ":", "\n", "            ", "token", "=", "self", ".", "tokenize", "(", "x", ")", "\n", "input_vector", "=", "torch", ".", "from_numpy", "(", "token", ")", ".", "to", "(", "device", ")", ".", "to", "(", "torch", ".", "long", ")", "\n", "logit", "=", "self", ".", "model", "(", "mode", "=", "\"text_to_logit\"", ",", "input", "=", "input_vector", ")", ".", "squeeze", "(", "0", ")", "\n", "\n", "", "p", "=", "F", ".", "softmax", "(", "logit", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "return", "p", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.Adversary.__init__": [[107, 109], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "attack_surface", ")", ":", "\n", "        ", "self", ".", "attack_surface", "=", "attack_surface", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.Adversary.run": [[110, 122], ["None"], "methods", ["None"], ["", "def", "run", "(", "self", ",", "model", ",", "dataset", ",", "device", ",", "opts", "=", "None", ")", ":", "\n", "        ", "\"\"\"Run adversary on a dataset.\n\n        Args:\n        model: a TextClassificationModel.\n        dataset: a TextClassificationDataset.\n        device: torch device.\n        Returns: pair of\n        - list of 0-1 adversarial loss of same length as |dataset|\n        - list of list of adversarial examples (each is just a text string)\n        \"\"\"", "\n", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary_Snli.__init__": [[127, 132], ["attacks.Adversary.__init__"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "attack_surface", ",", "num_iters", "=", "20", ",", "pop_size", "=", "60", ",", "margin_goal", "=", "0", ")", ":", "\n", "        ", "super", "(", "GeneticAdversary_Snli", ",", "self", ")", ".", "__init__", "(", "attack_surface", ")", "\n", "self", ".", "num_iters", "=", "num_iters", "\n", "self", ".", "pop_size", "=", "pop_size", "\n", "self", ".", "margin_goal", "=", "margin_goal", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary_Snli.get_margins": [[134, 144], ["logits.copy", "logits.copy.max"], "methods", ["None"], ["", "def", "get_margins", "(", "self", ",", "model_output", ",", "gold_labels", ")", ":", "\n", "        ", "logits", "=", "model_output", "\n", "\n", "true_class_pred", "=", "logits", "[", "gold_labels", "]", "\n", "\n", "temp", "=", "logits", ".", "copy", "(", ")", "\n", "temp", "[", "gold_labels", "]", "=", "-", "1e20", "\n", "highest_false_pred", "=", "temp", ".", "max", "(", ")", "\n", "value_margin", "=", "true_class_pred", "-", "highest_false_pred", "\n", "return", "value_margin", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary_Snli.perturb": [[145, 165], ["all", "float", "enumerate", "list", "attacks.GeneticAdversary_Snli.get_margins", "random.sample", "model.query", "attacks.GeneticAdversary_Snli.get_margins", "model.query", "attacks.GeneticAdversary_Snli.item", "enumerate", "attacks.GeneticAdversary_Snli.item", "len", "len", "attacks.GeneticAdversary_Snli.item"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary_Snli.get_margins", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.query", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary_Snli.get_margins", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.query"], ["", "def", "perturb", "(", "self", ",", "x_p", ",", "hypo", ",", "choices", ",", "model", ",", "y", ",", "device", ")", ":", "\n", "        ", "if", "all", "(", "len", "(", "c", ")", "==", "1", "for", "c", "in", "choices", ")", ":", "\n", "            ", "value_margin", "=", "self", ".", "get_margins", "(", "model", ".", "query", "(", "x_p", ",", "' '", ".", "join", "(", "hypo", ")", ",", "device", ")", ",", "y", ")", "\n", "return", "hypo", ",", "value_margin", ".", "item", "(", ")", "\n", "", "good_idxs", "=", "[", "i", "for", "i", ",", "c", "in", "enumerate", "(", "choices", ")", "if", "len", "(", "c", ")", ">", "1", "]", "\n", "idx", "=", "random", ".", "sample", "(", "good_idxs", ",", "1", ")", "[", "0", "]", "\n", "x_h_list", "=", "[", "' '", ".", "join", "(", "hypo", "[", ":", "idx", "]", "+", "[", "w_new", "]", "+", "hypo", "[", "idx", "+", "1", ":", "]", ")", "\n", "for", "w_new", "in", "choices", "[", "idx", "]", "]", "\n", "querry_list", "=", "[", "model", ".", "query", "(", "x_p", ",", "x_h", ",", "device", ")", "for", "x_h", "in", "x_h_list", "]", "\n", "best_replacement_idx", "=", "None", "\n", "worst_margin", "=", "float", "(", "'inf'", ")", "\n", "for", "idx_in_choices", ",", "logits", "in", "enumerate", "(", "querry_list", ")", ":", "\n", "            ", "value_margin", "=", "self", ".", "get_margins", "(", "logits", ",", "y", ")", "\n", "if", "best_replacement_idx", "is", "None", "or", "value_margin", ".", "item", "(", ")", "<", "worst_margin", ":", "\n", "                ", "best_replacement_idx", "=", "idx_in_choices", "\n", "worst_margin", "=", "value_margin", ".", "item", "(", ")", "\n", "\n", "", "", "cur_words", "=", "list", "(", "hypo", ")", "\n", "cur_words", "[", "idx", "]", "=", "choices", "[", "idx", "]", "[", "best_replacement_idx", "]", "\n", "return", "cur_words", ",", "worst_margin", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary_Snli.run": [[166, 236], ["zip", "x_h.split", "print", "numpy.argmax", "model.query", "x_h.split", "attacks.GeneticAdversary_Snli.attack_surface.get_swaps", "range", "attacks.GeneticAdversary_Snli.attack_surface.check_in", "numpy.argmax", "print", "is_correct.append", "adv_exs.append", "attacks.GeneticAdversary_Snli.perturb", "numpy.array", "range", "is_correct.append", "adv_exs.append", "print", "zip", "range", "min", "is_correct.append", "adv_exs.append", "print", "numpy.sum", "attacks.GeneticAdversary_Snli.perturb", "new_population.append", "enumerate", "numpy.exp", "random.sample", "zip", "numpy.random.choice", "numpy.random.choice", "range", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.query", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.get_swaps", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.check_in", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary.perturb", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary.perturb"], ["", "def", "run", "(", "self", ",", "model", ",", "texts_p", ",", "texts_h", ",", "labels", ",", "device", ",", "genetic_test_num", ",", "opts", "=", "None", ")", ":", "\n", "        ", "is_correct", "=", "[", "]", "\n", "adv_exs", "=", "[", "]", "\n", "total", "=", "0", "\n", "acc", "=", "0", "\n", "\n", "#texts_p = texts_p[opts.h_test_start:opts.h_test_start+opts.genetic_test_num]", "\n", "#texts_h = texts_h[opts.h_test_start:opts.h_test_start+opts.genetic_test_num]", "\n", "#labels = labels[opts.h_test_start:opts.h_test_start+opts.genetic_test_num]", "\n", "\n", "for", "x_p", ",", "x_h", ",", "y", "in", "zip", "(", "texts_p", ",", "texts_h", ",", "labels", ")", ":", "\n", "\n", "            ", "words", "=", "x_h", ".", "split", "(", ")", "\n", "if", "not", "self", ".", "attack_surface", ".", "check_in", "(", "words", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "print", "(", "acc", ",", "\"/\"", ",", "total", ")", "\n", "if", "total", ">=", "genetic_test_num", ":", "\n", "                ", "break", "\n", "", "total", "+=", "1", "\n", "y", "=", "np", ".", "argmax", "(", "y", ")", "\n", "# First query the example itself", "\n", "orig_pred", "=", "model", ".", "query", "(", "x_p", ",", "x_h", ",", "device", ")", "\n", "if", "np", ".", "argmax", "(", "orig_pred", ")", "!=", "y", ":", "\n", "                ", "print", "(", "'ORIGINAL PREDICTION WAS WRONG'", ")", "\n", "is_correct", ".", "append", "(", "0", ")", "\n", "adv_exs", ".", "append", "(", "(", "x_p", ",", "x_h", ")", ")", "\n", "continue", "\n", "# Now run adversarial search", "\n", "", "x_h_words", "=", "x_h", ".", "split", "(", ")", "\n", "swaps", "=", "self", ".", "attack_surface", ".", "get_swaps", "(", "x_h_words", ")", "\n", "choices", "=", "[", "[", "w", "]", "+", "cur_swaps", "for", "w", ",", "cur_swaps", "in", "zip", "(", "x_h_words", ",", "swaps", ")", "]", "\n", "found", "=", "False", "\n", "population", "=", "[", "self", ".", "perturb", "(", "x_p", ",", "x_h_words", ",", "choices", ",", "model", ",", "y", ",", "device", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "pop_size", ")", "]", "\n", "for", "g", "in", "range", "(", "self", ".", "num_iters", ")", ":", "\n", "                ", "best_idx", "=", "min", "(", "enumerate", "(", "population", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", "[", "1", "]", ")", "[", "0", "]", "\n", "#print('Iteration %d: %.6f' % (g, population[best_idx][1]))", "\n", "if", "population", "[", "best_idx", "]", "[", "1", "]", "<", "self", ".", "margin_goal", ":", "\n", "                    ", "found", "=", "True", "\n", "is_correct", ".", "append", "(", "0", ")", "\n", "adv_exs", ".", "append", "(", "' '", ".", "join", "(", "population", "[", "best_idx", "]", "[", "0", "]", ")", ")", "\n", "#print('ADVERSARY SUCCESS on (\"%s\", %d): Found \"%s\" with margin %.2f' % (x, y, adv_exs[-1], population[best_idx][1]))", "\n", "print", "(", "'ADVERSARY SUCCESS'", ")", "\n", "break", "\n", "", "new_population", "=", "[", "population", "[", "best_idx", "]", "]", "\n", "\n", "margins", "=", "np", ".", "array", "(", "[", "m", "for", "c", ",", "m", "in", "population", "]", ")", "\n", "adv_probs", "=", "1", "/", "(", "1", "+", "np", ".", "exp", "(", "margins", ")", ")", "+", "1e-4", "\n", "# Sigmoid of negative margin, for probabilty of wrong class", "\n", "# Add 1e-6 for numerical stability", "\n", "sample_probs", "=", "adv_probs", "/", "np", ".", "sum", "(", "adv_probs", ")", "\n", "\n", "for", "i", "in", "range", "(", "1", ",", "self", ".", "pop_size", ")", ":", "\n", "                    ", "parent1", "=", "population", "[", "np", ".", "random", ".", "choice", "(", "range", "(", "len", "(", "population", ")", ")", ",", "p", "=", "sample_probs", ")", "]", "[", "0", "]", "\n", "parent2", "=", "population", "[", "np", ".", "random", ".", "choice", "(", "range", "(", "len", "(", "population", ")", ")", ",", "p", "=", "sample_probs", ")", "]", "[", "0", "]", "\n", "child", "=", "[", "random", ".", "sample", "(", "[", "w1", ",", "w2", "]", ",", "1", ")", "[", "0", "]", "for", "(", "w1", ",", "w2", ")", "in", "zip", "(", "parent1", ",", "parent2", ")", "]", "\n", "child_mut", ",", "new_margin", "=", "self", ".", "perturb", "(", "x_p", ",", "child", ",", "choices", ",", "model", ",", "y", ",", "device", ")", "\n", "new_population", ".", "append", "(", "(", "child_mut", ",", "new_margin", ")", ")", "\n", "", "population", "=", "new_population", "\n", "", "else", ":", "\n", "                ", "is_correct", ".", "append", "(", "1", ")", "\n", "adv_exs", ".", "append", "(", "[", "]", ")", "\n", "acc", "+=", "1", "\n", "#print('ADVERSARY FAILURE on (\"%s\", %d)' % (x, y))", "\n", "print", "(", "'ADVERSARY FAILURE'", ",", "'Iteration %d: %.6f'", "%", "(", "g", ",", "population", "[", "best_idx", "]", "[", "1", "]", ")", ")", "\n", "", "", "if", "total", "!=", "0", ":", "\n", "            ", "return", "acc", "*", "1.0", "/", "total", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary.__init__": [[238, 243], ["attacks.Adversary.__init__"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "attack_surface", ",", "num_iters", "=", "20", ",", "pop_size", "=", "60", ",", "margin_goal", "=", "0.5", ")", ":", "\n", "        ", "super", "(", "GeneticAdversary", ",", "self", ")", ".", "__init__", "(", "attack_surface", ")", "\n", "self", ".", "num_iters", "=", "num_iters", "\n", "self", ".", "pop_size", "=", "pop_size", "\n", "self", ".", "margin_goal", "=", "margin_goal", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary.perturb": [[245, 257], ["all", "list", "random.sample", "model.query", "min", "enumerate", "enumerate", "len", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.query"], ["", "def", "perturb", "(", "self", ",", "words", ",", "choices", ",", "model", ",", "y", ",", "device", ")", ":", "\n", "        ", "if", "all", "(", "len", "(", "c", ")", "==", "1", "for", "c", "in", "choices", ")", ":", "return", "words", "\n", "good_idxs", "=", "[", "i", "for", "i", ",", "c", "in", "enumerate", "(", "choices", ")", "if", "len", "(", "c", ")", ">", "1", "]", "\n", "idx", "=", "random", ".", "sample", "(", "good_idxs", ",", "1", ")", "[", "0", "]", "\n", "x_list", "=", "[", "' '", ".", "join", "(", "words", "[", ":", "idx", "]", "+", "[", "w_new", "]", "+", "words", "[", "idx", "+", "1", ":", "]", ")", "\n", "for", "w_new", "in", "choices", "[", "idx", "]", "]", "\n", "preds", "=", "[", "model", ".", "query", "(", "x", ",", "device", ")", "for", "x", "in", "x_list", "]", "\n", "preds_of_y", "=", "[", "p", "[", "y", "]", "for", "p", "in", "preds", "]", "\n", "best_idx", "=", "min", "(", "enumerate", "(", "preds_of_y", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "[", "0", "]", "\n", "cur_words", "=", "list", "(", "words", ")", "\n", "cur_words", "[", "idx", "]", "=", "choices", "[", "idx", "]", "[", "best_idx", "]", "\n", "return", "cur_words", ",", "preds_of_y", "[", "best_idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary.run": [[258, 323], ["zip", "x.split", "print", "numpy.argmax", "model.query", "x.split", "attacks.GeneticAdversary.attack_surface.get_swaps", "range", "attacks.GeneticAdversary.attack_surface.check_in", "numpy.argmax", "print", "is_correct.append", "adv_exs.append", "attacks.GeneticAdversary.perturb", "numpy.array", "range", "is_correct.append", "adv_exs.append", "print", "zip", "range", "min", "is_correct.append", "adv_exs.append", "print", "numpy.sum", "attacks.GeneticAdversary.perturb", "new_population.append", "enumerate", "random.sample", "zip", "numpy.random.choice", "numpy.random.choice", "range", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.AdversarialModel.query", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.get_swaps", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.from_certified.attack_surface.LMConstrainedAttackSurface.check_in", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary.perturb", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.attacks.GeneticAdversary.perturb"], ["", "def", "run", "(", "self", ",", "model", ",", "texts", ",", "labels", ",", "device", ",", "genetic_test_num", ",", "opts", "=", "None", ")", ":", "\n", "        ", "is_correct", "=", "[", "]", "\n", "adv_exs", "=", "[", "]", "\n", "total", "=", "0", "\n", "acc", "=", "0", "\n", "\n", "#texts = texts[opts.h_test_start:opts.h_test_start+opts.genetic_test_num]", "\n", "#labels = labels[opts.h_test_start:opts.h_test_start+opts.genetic_test_num]", "\n", "\n", "for", "x", ",", "y", "in", "zip", "(", "texts", ",", "labels", ")", ":", "\n", "            ", "words", "=", "x", ".", "split", "(", ")", "\n", "if", "not", "self", ".", "attack_surface", ".", "check_in", "(", "words", ")", ":", "\n", "                ", "continue", "\n", "\n", "", "print", "(", "acc", ",", "\"/\"", ",", "total", ")", "\n", "if", "total", ">=", "genetic_test_num", ":", "\n", "                ", "break", "\n", "", "total", "+=", "1", "\n", "y", "=", "np", ".", "argmax", "(", "y", ")", "\n", "# First query the example itself", "\n", "orig_pred", "=", "model", ".", "query", "(", "x", ",", "device", ")", "\n", "if", "np", ".", "argmax", "(", "orig_pred", ")", "!=", "y", ":", "\n", "                ", "print", "(", "'ORIGINAL PREDICTION WAS WRONG'", ")", "\n", "is_correct", ".", "append", "(", "0", ")", "\n", "adv_exs", ".", "append", "(", "x", ")", "\n", "continue", "\n", "# Now run adversarial search", "\n", "", "words", "=", "x", ".", "split", "(", ")", "\n", "swaps", "=", "self", ".", "attack_surface", ".", "get_swaps", "(", "words", ")", "\n", "choices", "=", "[", "[", "w", "]", "+", "cur_swaps", "for", "w", ",", "cur_swaps", "in", "zip", "(", "words", ",", "swaps", ")", "]", "\n", "found", "=", "False", "\n", "population", "=", "[", "self", ".", "perturb", "(", "words", ",", "choices", ",", "model", ",", "y", ",", "device", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "pop_size", ")", "]", "\n", "for", "g", "in", "range", "(", "self", ".", "num_iters", ")", ":", "\n", "                ", "best_idx", "=", "min", "(", "enumerate", "(", "population", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", "[", "1", "]", ")", "[", "0", "]", "\n", "#print('Iteration %d: %.6f' % (g, population[best_idx][1]))", "\n", "if", "population", "[", "best_idx", "]", "[", "1", "]", "<", "self", ".", "margin_goal", ":", "\n", "                    ", "found", "=", "True", "\n", "is_correct", ".", "append", "(", "0", ")", "\n", "adv_exs", ".", "append", "(", "' '", ".", "join", "(", "population", "[", "best_idx", "]", "[", "0", "]", ")", ")", "\n", "#print('ADVERSARY SUCCESS on (\"%s\", %d): Found \"%s\" with margin %.2f' % (x, y, adv_exs[-1], population[best_idx][1]))", "\n", "print", "(", "'ADVERSARY SUCCESS'", ")", "\n", "break", "\n", "", "new_population", "=", "[", "population", "[", "best_idx", "]", "]", "\n", "p_y", "=", "np", ".", "array", "(", "[", "m", "for", "c", ",", "m", "in", "population", "]", ")", "\n", "temp", "=", "1", "-", "p_y", "+", "1e-8", "\n", "sample_probs", "=", "(", "temp", ")", "/", "np", ".", "sum", "(", "temp", ")", "\n", "#sample_probs = sample_probs + 1e-8", "\n", "for", "i", "in", "range", "(", "1", ",", "self", ".", "pop_size", ")", ":", "\n", "                    ", "parent1", "=", "population", "[", "np", ".", "random", ".", "choice", "(", "range", "(", "len", "(", "population", ")", ")", ",", "p", "=", "sample_probs", ")", "]", "[", "0", "]", "\n", "parent2", "=", "population", "[", "np", ".", "random", ".", "choice", "(", "range", "(", "len", "(", "population", ")", ")", ",", "p", "=", "sample_probs", ")", "]", "[", "0", "]", "\n", "child", "=", "[", "random", ".", "sample", "(", "[", "w1", ",", "w2", "]", ",", "1", ")", "[", "0", "]", "for", "(", "w1", ",", "w2", ")", "in", "zip", "(", "parent1", ",", "parent2", ")", "]", "\n", "child_mut", ",", "new_p_y", "=", "self", ".", "perturb", "(", "child", ",", "choices", ",", "model", ",", "y", ",", "device", ")", "\n", "new_population", ".", "append", "(", "(", "child_mut", ",", "new_p_y", ")", ")", "\n", "", "population", "=", "new_population", "\n", "", "else", ":", "\n", "                ", "is_correct", ".", "append", "(", "1", ")", "\n", "adv_exs", ".", "append", "(", "[", "]", ")", "\n", "acc", "+=", "1", "\n", "#print('ADVERSARY FAILURE on (\"%s\", %d)' % (x, y))", "\n", "print", "(", "'ADVERSARY FAILURE'", ",", "'Iteration %d: %.6f'", "%", "(", "g", ",", "population", "[", "best_idx", "]", "[", "1", "]", ")", ")", "\n", "", "", "if", "total", "!=", "0", ":", "\n", "            ", "return", "acc", "*", "1.0", "/", "total", "\n", "", "else", ":", "\n", "            ", "return", "0", "", "", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.FastText.FastText.__init__": [[10, 32], ["models.BaseModel.BaseModel.__init__", "getattr", "torch.nn.Embedding", "torch.nn.Sequential", "FastText.FastText.properties.update", "opt.__dict__.get", "print", "torch.nn.Parameter", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "FastText", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "linear_hidden_size", "=", "getattr", "(", "opt", ",", "\"linear_hidden_size\"", ",", "2000", ")", "\n", "self", ".", "encoder", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "if", "opt", ".", "__dict__", ".", "get", "(", "\"embeddings\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "print", "(", "'load embedding'", ")", "\n", "self", ".", "encoder", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "\n", "\n", "", "self", ".", "content_fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "opt", ".", "embedding_dim", ",", "linear_hidden_size", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "linear_hidden_size", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "# nn.Linear(opt.linear_hidden_size,opt.linear_hidden_size),", "\n", "# nn.BatchNorm1d(opt.linear_hidden_size),", "\n", "# nn.ReLU(inplace=True),", "\n", "nn", ".", "Linear", "(", "linear_hidden_size", ",", "opt", ".", "label_size", ")", "\n", ")", "\n", "#        self.fc = nn.Linear(300, opt.label_size)", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"linear_hidden_size\"", ":", "linear_hidden_size", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.FastText.FastText.forward": [[34, 42], ["torch.mean", "FastText.FastText.content_fc", "FastText.FastText.encoder", "torch.mean.view", "torch.mean.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "content", ")", ":", "\n", "\n", "        ", "content_", "=", "t", ".", "mean", "(", "self", ".", "encoder", "(", "content", ")", ",", "dim", "=", "1", ")", "\n", "\n", "\n", "out", "=", "self", ".", "content_fc", "(", "content_", ".", "view", "(", "content_", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "\n", "return", "out", "\n", "", "", "if", "__name__", "==", "'__main__'", ":", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNText.CNNText.__init__": [[7, 33], ["models.BaseModel.BaseModel.__init__", "opt.__dict__.get", "opt.__dict__.get", "torch.nn.Embedding", "torch.nn.Sequential", "torch.nn.Linear", "CNNText.CNNText.properties.update", "opt.__dict__.get", "torch.nn.Parameter", "torch.nn.Conv1d", "torch.nn.ReLU", "torch.nn.MaxPool1d"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "CNNText", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "\n", "self", ".", "content_dim", "=", "opt", ".", "__dict__", ".", "get", "(", "\"content_dim\"", ",", "256", ")", "\n", "self", ".", "kernel_size", "=", "opt", ".", "__dict__", ".", "get", "(", "\"kernel_size\"", ",", "3", ")", "\n", "\n", "\n", "self", ".", "encoder", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "if", "opt", ".", "__dict__", ".", "get", "(", "\"embeddings\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "self", ".", "encoder", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "\n", "\n", "", "self", ".", "content_conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "opt", ".", "embedding_dim", ",", "\n", "out_channels", "=", "self", ".", "content_dim", ",", "\n", "kernel_size", "=", "self", ".", "kernel_size", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "(", "opt", ".", "max_seq_len", "-", "self", ".", "kernel_size", "+", "1", ")", ")", "\n", "#            nn.AdaptiveMaxPool1d()", "\n", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "content_dim", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"content_dim\"", ":", "self", ".", "content_dim", ",", "\n", "\"kernel_size\"", ":", "self", ".", "kernel_size", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNText.CNNText.forward": [[35, 42], ["CNNText.CNNText.encoder", "CNNText.CNNText.content_conv", "CNNText.CNNText.view", "CNNText.CNNText.fc", "CNNText.CNNText.permute", "CNNText.CNNText.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "content", ")", ":", "\n", "\n", "        ", "content", "=", "self", ".", "encoder", "(", "content", ")", "\n", "content_out", "=", "self", ".", "content_conv", "(", "content", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "\n", "reshaped", "=", "content_out", ".", "view", "(", "content_out", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "logits", "=", "self", ".", "fc", "(", "reshaped", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNText.parse_opt": [[45, 79], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_opt", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "# Data input settings", "\n", "parser", ".", "add_argument", "(", "'--hidden_dim'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'hidden_dim'", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'batch_size'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_dim'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "\n", "help", "=", "'embedding_dim'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "4e-4", ",", "\n", "help", "=", "'learning_rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--grad_clip'", ",", "type", "=", "float", ",", "default", "=", "1e-1", ",", "\n", "help", "=", "'grad_clip'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "\"lstm\"", ",", "\n", "help", "=", "'model name'", ")", "\n", "\n", "\n", "#", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "embedding_dim", "=", "300", "\n", "args", ".", "vocab_size", "=", "10000", "\n", "args", ".", "kernel_size", "=", "3", "\n", "args", ".", "num_classes", "=", "3", "\n", "args", ".", "content_dim", "=", "256", "\n", "args", ".", "max_seq_len", "=", "50", "\n", "\n", "#", "\n", "#    # Check if args are valid", "\n", "#    assert args.rnn_size > 0, \"rnn_size should be greater than 0\"", "\n", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNInception.Inception.__init__": [[11, 36], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "CNNInception.Inception.activa.add_module", "CNNInception.Inception.activa.add_module", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "int", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "cin", ",", "co", ",", "relu", "=", "True", ",", "norm", "=", "True", ")", ":", "\n", "        ", "super", "(", "Inception", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "co", "%", "4", "==", "0", ")", "\n", "cos", "=", "[", "int", "(", "co", "/", "4", ")", "]", "*", "4", "\n", "self", ".", "activa", "=", "nn", ".", "Sequential", "(", ")", "\n", "if", "norm", ":", "self", ".", "activa", ".", "add_module", "(", "'norm'", ",", "nn", ".", "BatchNorm1d", "(", "co", ")", ")", "\n", "if", "relu", ":", "self", ".", "activa", ".", "add_module", "(", "'relu'", ",", "nn", ".", "ReLU", "(", "True", ")", ")", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'conv1'", ",", "nn", ".", "Conv1d", "(", "cin", ",", "cos", "[", "0", "]", ",", "1", ",", "stride", "=", "1", ")", ")", ",", "\n", "]", ")", ")", "\n", "self", ".", "branch2", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'conv1'", ",", "nn", ".", "Conv1d", "(", "cin", ",", "cos", "[", "1", "]", ",", "1", ")", ")", ",", "\n", "(", "'norm1'", ",", "nn", ".", "BatchNorm1d", "(", "cos", "[", "1", "]", ")", ")", ",", "\n", "(", "'relu1'", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ",", "\n", "(", "'conv3'", ",", "nn", ".", "Conv1d", "(", "cos", "[", "1", "]", ",", "cos", "[", "1", "]", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ")", ",", "\n", "]", ")", ")", "\n", "self", ".", "branch3", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'conv1'", ",", "nn", ".", "Conv1d", "(", "cin", ",", "cos", "[", "2", "]", ",", "3", ",", "padding", "=", "1", ")", ")", ",", "\n", "(", "'norm1'", ",", "nn", ".", "BatchNorm1d", "(", "cos", "[", "2", "]", ")", ")", ",", "\n", "(", "'relu1'", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ",", "\n", "(", "'conv3'", ",", "nn", ".", "Conv1d", "(", "cos", "[", "2", "]", ",", "cos", "[", "2", "]", ",", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", ")", ",", "\n", "]", ")", ")", "\n", "self", ".", "branch4", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "#('pool',nn.MaxPool1d(2)),", "\n", "(", "'conv3'", ",", "nn", ".", "Conv1d", "(", "cin", ",", "cos", "[", "3", "]", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ")", ",", "\n", "]", ")", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNInception.Inception.forward": [[37, 44], ["CNNInception.Inception.branch1", "CNNInception.Inception.branch2", "CNNInception.Inception.branch3", "CNNInception.Inception.branch4", "CNNInception.Inception.activa", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "branch1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "branch2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "branch3", "=", "self", ".", "branch3", "(", "x", ")", "\n", "branch4", "=", "self", ".", "branch4", "(", "x", ")", "\n", "result", "=", "self", ".", "activa", "(", "torch", ".", "cat", "(", "(", "branch1", ",", "branch2", ",", "branch3", ",", "branch4", ")", ",", "1", ")", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNInception.InceptionCNN.__init__": [[47, 71], ["models.BaseModel.BaseModel.__init__", "getattr", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Sequential", "torch.nn.Sequential", "getattr", "torch.nn.Sequential", "torch.nn.Sequential", "CNNInception.InceptionCNN.properties.update", "CNNInception.Inception", "CNNInception.Inception", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "opt.__dict__.get", "torch.nn.Parameter", "torch.nn.Parameter"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "InceptionCNN", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "incept_dim", "=", "getattr", "(", "opt", ",", "\"inception_dim\"", ",", "512", ")", "\n", "self", ".", "model_name", "=", "'CNNText_inception'", "\n", "self", ".", "encoder", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "\n", "self", ".", "content_conv", "=", "nn", ".", "Sequential", "(", "\n", "Inception", "(", "opt", ".", "embedding_dim", ",", "incept_dim", ")", ",", "#(batch_size,64,opt.content_seq_len)->(batch_size,64,(opt.content_seq_len)/2)", "\n", "#Inception(incept_dim,incept_dim),#(batch_size,64,opt.content_seq_len/2)->(batch_size,32,(opt.content_seq_len)/4)", "\n", "Inception", "(", "incept_dim", ",", "incept_dim", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "opt", ".", "max_seq_len", ")", "\n", ")", "\n", "linear_hidden_size", "=", "getattr", "(", "opt", ",", "\"linear_hidden_size\"", ",", "2000", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "incept_dim", ",", "linear_hidden_size", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "linear_hidden_size", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Linear", "(", "linear_hidden_size", ",", "opt", ".", "label_size", ")", "\n", ")", "\n", "if", "opt", ".", "__dict__", ".", "get", "(", "\"embeddings\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "self", ".", "encoder", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ")", "\n", "", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"linear_hidden_size\"", ":", "linear_hidden_size", ",", "\n", "\"incept_dim\"", ":", "incept_dim", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNInception.InceptionCNN.forward": [[73, 83], ["CNNInception.InceptionCNN.encoder", "CNNInception.InceptionCNN.content_conv", "CNNInception.InceptionCNN.view", "CNNInception.InceptionCNN.fc", "content.detach.detach.detach", "content.detach.detach.permute", "CNNInception.InceptionCNN.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "content", ")", ":", "\n", "\n", "        ", "content", "=", "self", ".", "encoder", "(", "content", ")", "\n", "if", "self", ".", "opt", ".", "embedding_type", "==", "\"static\"", ":", "\n", "            ", "content", "=", "content", ".", "detach", "(", "0", ")", "\n", "\n", "", "content_out", "=", "self", ".", "content_conv", "(", "content", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "\n", "out", "=", "content_out", ".", "view", "(", "content_out", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "out", "=", "self", ".", "fc", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN.__init__": [[7, 45], ["BaseModel.BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "CNN.CNN.properties.update", "len", "len", "CNN.CNN.embedding.weight.data.copy_", "len", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "setattr", "sum", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "CNN.CNN.embedding2.weight.data.copy_", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "CNN", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "embedding_type", "=", "opt", ".", "embedding_type", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "max_sent_len", "=", "opt", ".", "max_sent_len", "\n", "self", ".", "embedding_dim", "=", "opt", ".", "embedding_dim", "\n", "self", ".", "vocab_size", "=", "opt", ".", "vocab_size", "\n", "self", ".", "CLASS_SIZE", "=", "opt", ".", "label_size", "\n", "self", ".", "FILTERS", "=", "[", "3", ",", "4", ",", "5", "]", "\n", "self", ".", "FILTER_NUM", "=", "[", "100", ",", "100", ",", "100", "]", "\n", "self", ".", "keep_dropout", "=", "opt", ".", "keep_dropout", "\n", "self", ".", "IN_CHANNEL", "=", "1", "\n", "\n", "assert", "(", "len", "(", "self", ".", "FILTERS", ")", "==", "len", "(", "self", ".", "FILTER_NUM", ")", ")", "\n", "\n", "# one for UNK and one for zero padding", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", "+", "2", ",", "self", ".", "embedding_dim", ",", "padding_idx", "=", "self", ".", "vocab_size", "+", "1", ")", "\n", "if", "self", ".", "embedding_type", "==", "\"static\"", "or", "self", ".", "embedding_type", "==", "\"non-static\"", "or", "self", ".", "embedding_type", "==", "\"multichannel\"", ":", "\n", "            ", "self", ".", "WV_MATRIX", "=", "opt", "[", "\"WV_MATRIX\"", "]", "\n", "self", ".", "embedding", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "from_numpy", "(", "self", ".", "WV_MATRIX", ")", ")", "\n", "if", "self", ".", "embedding_type", "==", "\"static\"", ":", "\n", "                ", "self", ".", "embedding", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "elif", "self", ".", "embedding_type", "==", "\"multichannel\"", ":", "\n", "                ", "self", ".", "embedding2", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", "+", "2", ",", "self", ".", "embedding_dim", ",", "padding_idx", "=", "self", ".", "VOCAB_SIZE", "+", "1", ")", "\n", "self", ".", "embedding2", ".", "weight", ".", "data", ".", "copy_", "(", "torch", ".", "from_numpy", "(", "self", ".", "WV_MATRIX", ")", ")", "\n", "self", ".", "embedding2", ".", "weight", ".", "requires_grad", "=", "False", "\n", "self", ".", "IN_CHANNEL", "=", "2", "\n", "\n", "", "", "for", "i", "in", "range", "(", "len", "(", "self", ".", "FILTERS", ")", ")", ":", "\n", "            ", "conv", "=", "nn", ".", "Conv1d", "(", "self", ".", "IN_CHANNEL", ",", "self", ".", "FILTER_NUM", "[", "i", "]", ",", "self", ".", "embedding_dim", "*", "self", ".", "FILTERS", "[", "i", "]", ",", "stride", "=", "self", ".", "WORD_DIM", ")", "\n", "setattr", "(", "self", ",", "'conv_%d'", "%", "i", ",", "conv", ")", "\n", "\n", "", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "sum", "(", "self", ".", "FILTER_NUM", ")", ",", "self", ".", "label_size", ")", "\n", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"FILTER_NUM\"", ":", "self", ".", "FILTER_NUM", ",", "\n", "\"FILTERS\"", ":", "self", ".", "FILTERS", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN.get_conv": [[47, 49], ["getattr"], "methods", ["None"], ["", "def", "get_conv", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ",", "'conv_%d'", "%", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN.forward": [[50, 65], ["CNN.CNN.embedding().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout", "CNN.CNN.fc", "CNN.CNN.embedding2().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.max_pool1d().view", "torch.max_pool1d().view", "torch.max_pool1d().view", "torch.max_pool1d().view", "CNN.CNN.embedding", "range", "CNN.CNN.embedding2", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "len", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "CNN.CNN.get_conv"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNKim.KIMCNN1D.get_conv"], ["", "def", "forward", "(", "self", ",", "inp", ")", ":", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "inp", ")", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "embedding_dim", "*", "self", ".", "max_sent_len", ")", "\n", "if", "self", ".", "embedding_type", "==", "\"multichannel\"", ":", "\n", "            ", "x2", "=", "self", ".", "embedding2", "(", "inp", ")", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "embedding_dim", "*", "self", ".", "max_sent_len", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "x2", ")", ",", "1", ")", "\n", "\n", "", "conv_results", "=", "[", "\n", "F", ".", "max_pool1d", "(", "F", ".", "relu", "(", "self", ".", "get_conv", "(", "i", ")", "(", "x", ")", ")", ",", "self", ".", "max_sent_len", "-", "self", ".", "FILTERS", "[", "i", "]", "+", "1", ")", "\n", ".", "view", "(", "-", "1", ",", "self", ".", "FILTER_NUM", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "FILTERS", ")", ")", "]", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "conv_results", ",", "1", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "keep_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN1.__init__": [[72, 95], ["BaseModel.BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "CNN.CNN1.properties.update", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "CNN1", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "V", "=", "opt", ".", "vocab_size", "\n", "D", "=", "opt", ".", "embedding_dim", "\n", "C", "=", "opt", ".", "label_size", "\n", "Ci", "=", "1", "\n", "Co", "=", "opt", ".", "kernel_num", "\n", "Ks", "=", "opt", ".", "kernel_sizes", "\n", "\n", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "V", ",", "D", ")", "\n", "#self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]", "\n", "self", ".", "convs1", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv2d", "(", "Ci", ",", "Co", ",", "(", "K", ",", "D", ")", ")", "for", "K", "in", "Ks", "]", ")", "\n", "'''\n        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n        '''", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "opt", ".", "dropout", ")", "\n", "self", ".", "fc1", "=", "nn", ".", "Linear", "(", "len", "(", "Ks", ")", "*", "Co", ",", "C", ")", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"kernel_num\"", ":", "opt", ".", "kernel_num", ",", "\n", "\"kernel_sizes\"", ":", "opt", ".", "kernel_sizes", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN1.conv_and_pool": [[97, 101], ["torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "conv", "torch.max_pool1d().squeeze.size"], "methods", ["None"], ["", "def", "conv_and_pool", "(", "self", ",", "x", ",", "conv", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "conv", "(", "x", ")", ")", ".", "squeeze", "(", "3", ")", "#(N,Co,W)", "\n", "x", "=", "F", ".", "max_pool1d", "(", "x", ",", "x", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN1.forward": [[103, 127], ["CNN.CNN1.embed", "Variable.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "CNN.CNN1.dropout", "CNN.CNN1.fc1", "Variable", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "conv", "i.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "embed", "(", "x", ")", "# (N,W,D)", "\n", "\n", "if", "self", ".", "args", ".", "static", ":", "\n", "            ", "x", "=", "Variable", "(", "x", ")", "\n", "\n", "", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "# (N,Ci,W,D)", "\n", "\n", "x", "=", "[", "F", ".", "relu", "(", "conv", "(", "x", ")", ")", ".", "squeeze", "(", "3", ")", "for", "conv", "in", "self", ".", "convs1", "]", "#[(N,Co,W), ...]*len(Ks)", "\n", "\n", "\n", "x", "=", "[", "F", ".", "max_pool1d", "(", "i", ",", "i", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "for", "i", "in", "x", "]", "#[(N,Co), ...]*len(Ks)", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "x", ",", "1", ")", "\n", "\n", "'''\n        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n        '''", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "# (N,len(Ks)*Co)", "\n", "logit", "=", "self", ".", "fc1", "(", "x", ")", "# (N,C)", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN2.__init__": [[134, 174], ["BaseModel.BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "CNN.CNN2.properties.update", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "CNN2", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", "+", "1", ",", "opt", ".", "embedding_dim", ")", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "opt", ".", "l0", ",", "256", ",", "kernel_size", "=", "7", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "3", ",", "stride", "=", "3", ")", "\n", ")", "\n", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "7", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "3", ",", "stride", "=", "3", ")", "\n", ")", "\n", "\n", "self", ".", "conv3", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "conv4", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "conv5", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "conv6", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "3", ",", "stride", "=", "3", ")", "\n", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "256", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN2.forward": [[175, 190], ["CNN.CNN2.embed", "CNN.CNN2.conv1", "CNN.CNN2.conv2", "CNN.CNN2.conv3", "CNN.CNN2.conv4", "CNN.CNN2.conv5", "CNN.CNN2.conv6", "CNN.CNN2.view", "CNN.CNN2.fc", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "CNN.CNN2.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x_input", ")", ":", "\n", "# Embedding", "\n", "        ", "x", "=", "self", ".", "embed", "(", "x_input", ")", "# dim: (batch_size, max_seq_len, embedding_size)", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "self", ".", "conv4", "(", "x", ")", "\n", "x", "=", "self", ".", "conv5", "(", "x", ")", "\n", "x", "=", "self", ".", "conv6", "(", "x", ")", "\n", "\n", "# collapse", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "\n", "return", "F", ".", "log_softmax", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN3.__init__": [[197, 215], ["BaseModel.BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "CNN.CNN3.properties.update", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "len"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "CNN3", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "embedding_dim", "=", "args", ".", "embed_dim", "\n", "embedding_num", "=", "args", ".", "num_features", "\n", "class_number", "=", "args", ".", "class_num", "\n", "in_channel", "=", "1", "\n", "out_channel", "=", "args", ".", "kernel_num", "\n", "kernel_sizes", "=", "args", ".", "kernel_sizes", "\n", "\n", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "embedding_num", "+", "1", ",", "embedding_dim", ")", "\n", "self", ".", "conv", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv2d", "(", "in_channel", ",", "out_channel", ",", "(", "K", ",", "embedding_dim", ")", ")", "for", "K", "in", "kernel_sizes", "]", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "args", ".", "dropout", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "len", "(", "kernel_sizes", ")", "*", "out_channel", ",", "class_number", ")", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"kernel_sizes\"", ":", "kernel_sizes", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN.CNN3.forward": [[217, 243], ["CNN.CNN3.embed", "torch.Variable.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "CNN.CNN3.dropout", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.Variable", "torch.Variable", "torch.Variable", "torch.Variable", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "CNN.CNN3.fc", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "conv", "i.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_x", ")", ":", "\n", "        ", "\"\"\"\n        :param input_x: a list size having the number of batch_size elements with the same length\n        :return: batch_size X num_aspects tensor\n        \"\"\"", "\n", "# Embedding", "\n", "x", "=", "self", ".", "embed", "(", "input_x", ")", "# dim: (batch_size, max_seq_len, embedding_size)", "\n", "\n", "if", "self", ".", "args", ".", "static", ":", "\n", "            ", "x", "=", "F", ".", "Variable", "(", "input_x", ")", "\n", "\n", "# Conv & max pool", "\n", "", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "# dim: (batch_size, 1, max_seq_len, embedding_size)", "\n", "\n", "# turns to be a list: [ti : i \\in kernel_sizes] where ti: tensor of dim([batch, num_kernels, max_seq_len-i+1])", "\n", "x", "=", "[", "F", ".", "relu", "(", "conv", "(", "x", ")", ")", ".", "squeeze", "(", "3", ")", "for", "conv", "in", "self", ".", "conv", "]", "\n", "\n", "# dim: [(batch_size, num_kernels), ...]*len(kernel_sizes)", "\n", "x", "=", "[", "F", ".", "max_pool1d", "(", "i", ",", "i", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "for", "i", "in", "x", "]", "\n", "x", "=", "torch", ".", "cat", "(", "x", ",", "1", ")", "\n", "\n", "# Dropout & output", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "# (batch_size,len(kernel_sizes)*num_kernels)", "\n", "logit", "=", "F", ".", "log_softmax", "(", "self", ".", "fc", "(", "x", ")", ")", "# (batch_size, num_aspects)", "\n", "\n", "return", "logit", "", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModel.BaseModel.__init__": [[10, 31], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Linear", "opt.__dict__.get", "torch.nn.Parameter"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "is_bert", "=", "False", ")", ":", "\n", "        ", "super", "(", "BaseModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model_name", "=", "'BaseModel'", "\n", "self", ".", "opt", "=", "opt", "\n", "if", "is_bert", ":", "\n", "            ", "return", "\n", "\n", "\n", "", "self", ".", "encoder", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "if", "opt", ".", "__dict__", ".", "get", "(", "\"embeddings\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "self", ".", "encoder", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "opt", ".", "embedding_dim", ",", "opt", ".", "label_size", ")", "\n", "\n", "\n", "self", ".", "properties", "=", "{", "\"model_name\"", ":", "self", ".", "__class__", ".", "__name__", ",", "\n", "#                \"embedding_dim\":self.opt.embedding_dim,", "\n", "#                \"embedding_training\":self.opt.embedding_training,", "\n", "#                \"max_seq_len\":self.opt.max_seq_len,", "\n", "\"batch_size\"", ":", "self", ".", "opt", ".", "batch_size", ",", "\n", "\"learning_rate\"", ":", "self", ".", "opt", ".", "learning_rate", ",", "\n", "\"keep_dropout\"", ":", "self", ".", "opt", ".", "keep_dropout", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModel.BaseModel.forward": [[33, 37], ["torch.mean", "BaseModel.BaseModel.fc", "BaseModel.BaseModel.encoder", "torch.mean.view", "torch.mean.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "content", ")", ":", "\n", "        ", "content_", "=", "t", ".", "mean", "(", "self", ".", "encoder", "(", "content", ")", ",", "dim", "=", "1", ")", "\n", "out", "=", "self", ".", "fc", "(", "content_", ".", "view", "(", "content_", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModel.BaseModel.save": [[39, 49], ["torch.save", "os.path.exists", "os.mkdir", "os.path.join", "os.path.join", "BaseModel.BaseModel.properties.items", "type", "str", "[].replace().replace", "str", "[].replace", "str"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModel.BaseModel.save"], ["", "def", "save", "(", "self", ",", "save_dir", "=", "\"saved_model\"", ",", "metric", "=", "None", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "save_dir", ")", "\n", "", "self", ".", "model_info", "=", "\"__\"", ".", "join", "(", "[", "k", "+", "\"_\"", "+", "str", "(", "v", ")", "if", "type", "(", "v", ")", "!=", "list", "else", "k", "+", "\"_\"", "+", "str", "(", "v", ")", "[", "1", ":", "-", "1", "]", ".", "replace", "(", "\",\"", ",", "\"_\"", ")", ".", "replace", "(", "\",\"", ",", "\"\"", ")", "for", "k", ",", "v", "in", "self", ".", "properties", ".", "items", "(", ")", "]", ")", "\n", "if", "metric", ":", "\n", "            ", "path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "str", "(", "metric", ")", "[", "2", ":", "]", "+", "\"_\"", "+", "self", ".", "model_info", ")", "\n", "", "else", ":", "\n", "            ", "path", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "self", ".", "model_info", ")", "\n", "", "t", ".", "save", "(", "self", ",", "path", ")", "\n", "return", "path", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMBI.AdvLSTMBI.__init__": [[14, 29], ["models.BaseModelAdv.AdvBaseModel.__init__", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "LSTMBI.AdvLSTMBI.init_hidden", "opt.__dict__.get", "LSTMBI.AdvLSTMBI.properties.update"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "AdvLSTMBI", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "#self.bidirectional = True", "\n", "\n", "self", ".", "bilstm", "=", "nn", ".", "LSTM", "(", "self", ".", "embedding_out_dim", ",", "opt", ".", "hidden_dim", "//", "2", ",", "num_layers", "=", "self", ".", "opt", ".", "lstm_layers", ",", "dropout", "=", "self", ".", "opt", ".", "keep_dropout", ",", "bidirectional", "=", "True", ")", "\n", "self", ".", "hidden2label", "=", "nn", ".", "Linear", "(", "opt", ".", "hidden_dim", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", ")", "\n", "self", ".", "lsmt_reduce_by_mean", "=", "opt", ".", "__dict__", ".", "get", "(", "\"lstm_mean\"", ",", "True", ")", "\n", "self", ".", "eval_adv_mode", "=", "False", "\n", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"hidden_dim\"", ":", "self", ".", "opt", ".", "hidden_dim", ",", "\n", "\"lstm_mean\"", ":", "self", ".", "lsmt_reduce_by_mean", ",", "\n", "\"lstm_layers\"", ":", "self", ".", "opt", ".", "lstm_layers", ",", "\n", "#                 \"bidirectional\":str(self.opt.bidirectional)", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMBI.AdvLSTMBI.init_hidden": [[32, 43], ["torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "        ", "if", "batch_size", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "opt", ".", "batch_size", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "opt", ".", "lstm_layers", ",", "batch_size", ",", "self", ".", "opt", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "opt", ".", "lstm_layers", ",", "batch_size", ",", "self", ".", "opt", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "opt", ".", "lstm_layers", ",", "batch_size", ",", "self", ".", "opt", ".", "hidden_dim", "//", "2", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "opt", ".", "lstm_layers", ",", "batch_size", ",", "self", ".", "opt", ".", "hidden_dim", "//", "2", ")", ")", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMBI.AdvLSTMBI.embd_to_logit": [[44, 55], ["embd.permute", "LSTMBI.AdvLSTMBI.init_hidden", "LSTMBI.AdvLSTMBI.bilstm", "LSTMBI.AdvLSTMBI.hidden2label", "lstm_out.permute", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "embd.size"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["", "def", "embd_to_logit", "(", "self", ",", "embd", ")", ":", "\n", "        ", "x", "=", "embd", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# we do this because the default parameter of lstm is False ", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", "embd", ".", "size", "(", ")", "[", "0", "]", ")", "#2x64x64", "\n", "lstm_out", ",", "self", ".", "hidden", "=", "self", ".", "bilstm", "(", "x", ",", "self", ".", "hidden", ")", "#lstm_out:200x64x128", "\n", "if", "self", ".", "lsmt_reduce_by_mean", "==", "\"mean\"", ":", "\n", "            ", "out", "=", "lstm_out", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "final", "=", "torch", ".", "mean", "(", "out", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "final", "=", "lstm_out", "[", "-", "1", "]", "\n", "", "y", "=", "self", ".", "hidden2label", "(", "final", ")", "#64x3  #lstm_out[-1]", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMBI.LSTMBI.__init__": [[59, 80], ["models.BaseModel.BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "LSTMBI.LSTMBI.init_hidden", "opt.__dict__.get", "LSTMBI.LSTMBI.properties.update"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "LSTMBI", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))", "\n", "\n", "\n", "#self.bidirectional = True", "\n", "\n", "self", ".", "bilstm", "=", "nn", ".", "LSTM", "(", "opt", ".", "embedding_dim", ",", "opt", ".", "hidden_dim", "//", "2", ",", "num_layers", "=", "self", ".", "opt", ".", "lstm_layers", ",", "dropout", "=", "self", ".", "opt", ".", "keep_dropout", ",", "bidirectional", "=", "self", ".", "opt", ".", "bidirectional", ")", "\n", "self", ".", "hidden2label", "=", "nn", ".", "Linear", "(", "opt", ".", "hidden_dim", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", ")", "\n", "self", ".", "lsmt_reduce_by_mean", "=", "opt", ".", "__dict__", ".", "get", "(", "\"lstm_mean\"", ",", "True", ")", "\n", "\n", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"hidden_dim\"", ":", "self", ".", "opt", ".", "hidden_dim", ",", "\n", "\"lstm_mean\"", ":", "self", ".", "lsmt_reduce_by_mean", ",", "\n", "\"lstm_layers\"", ":", "self", ".", "opt", ".", "lstm_layers", ",", "\n", "#                 \"bidirectional\":str(self.opt.bidirectional)", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMBI.LSTMBI.init_hidden": [[83, 94], ["torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "        ", "if", "batch_size", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "opt", ".", "batch_size", "\n", "\n", "", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "opt", ".", "lstm_layers", ",", "batch_size", ",", "self", ".", "opt", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "opt", ".", "lstm_layers", ",", "batch_size", ",", "self", ".", "opt", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "opt", ".", "lstm_layers", ",", "batch_size", ",", "self", ".", "opt", ".", "hidden_dim", "//", "2", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "opt", ".", "lstm_layers", ",", "batch_size", ",", "self", ".", "opt", ".", "hidden_dim", "//", "2", ")", ")", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "#    @profile", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMBI.LSTMBI.forward": [[95, 109], ["LSTMBI.LSTMBI.word_embeddings", "LSTMBI.LSTMBI.permute", "LSTMBI.LSTMBI.init_hidden", "LSTMBI.LSTMBI.bilstm", "LSTMBI.LSTMBI.hidden2label", "lstm_out.permute", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "sentence.size"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["", "def", "forward", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "embeds", "=", "self", ".", "word_embeddings", "(", "sentence", ")", "\n", "\n", "#        x = embeds.view(sentence.size()[1], self.batch_size, -1)", "\n", "x", "=", "embeds", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "# we do this because the default parameter of lstm is False ", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", "sentence", ".", "size", "(", ")", "[", "0", "]", ")", "#2x64x64", "\n", "lstm_out", ",", "self", ".", "hidden", "=", "self", ".", "bilstm", "(", "x", ",", "self", ".", "hidden", ")", "#lstm_out:200x64x128", "\n", "if", "self", ".", "lsmt_reduce_by_mean", "==", "\"mean\"", ":", "\n", "            ", "out", "=", "lstm_out", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "final", "=", "torch", ".", "mean", "(", "out", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "final", "=", "lstm_out", "[", "-", "1", "]", "\n", "", "y", "=", "self", ".", "hidden2label", "(", "final", ")", "#64x3  #lstm_out[-1]", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.RCNN.RCNN.__init__": [[13, 37], ["models.BaseModel.BaseModel.__init__", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.LSTM", "torch.LSTM", "torch.LSTM", "RCNN.RCNN.init_hidden", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "\n", "        ", "super", "(", "RCNN", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "hidden_dim", "=", "opt", ".", "hidden_dim", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "use_gpu", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))", "\n", "\n", "self", ".", "num_layers", "=", "1", "\n", "#self.bidirectional = True", "\n", "self", ".", "dropout", "=", "opt", ".", "keep_dropout", "\n", "self", ".", "bilstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "opt", ".", "embedding_dim", ",", "hidden_size", "=", "opt", ".", "hidden_dim", "//", "2", ",", "num_layers", "=", "self", ".", "num_layers", ",", "dropout", "=", "self", ".", "dropout", ",", "bidirectional", "=", "True", ")", "\n", "\n", "###self.hidden2label = nn.Linear(opt.hidden_dim, opt.label_size)", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", ")", "\n", "\n", "self", ".", "max_pooling", "=", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "3", ",", "stride", "=", "2", ")", "\n", "\n", "self", ".", "content_dim", "=", "256", "\n", "#self.conv =  nn.Conv1d(opt.hidden_dim, self.content_dim, opt.hidden_dim * 2, stride=opt.embedding_dim)", "\n", "self", ".", "hidden2label", "=", "nn", ".", "Linear", "(", "(", "2", "*", "opt", ".", "hidden_dim", "//", "2", "+", "opt", ".", "embedding_dim", ")", ",", "opt", ".", "label_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.RCNN.RCNN.init_hidden": [[39, 50], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "        ", "if", "batch_size", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "if", "self", ".", "use_gpu", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ")", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "#    @profile", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.RCNN.RCNN.forward": [[51, 71], ["RCNN.RCNN.word_embeddings", "RCNN.RCNN.permute", "RCNN.RCNN.init_hidden", "RCNN.RCNN.bilstm", "lstm_out.permute", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "RCNN.RCNN.max_pooling", "RCNN.RCNN.permute", "RCNN.RCNN.hidden2label", "torch.cat.permute", "torch.cat.permute", "torch.cat.permute", "sentence.size", "int", "int", "lstm_out.permute.size", "lstm_out.permute.size"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["", "def", "forward", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "embeds", "=", "self", ".", "word_embeddings", "(", "sentence", ")", "#64x200x300", "\n", "\n", "#        x = embeds.view(sentence.size()[1], self.batch_size, -1)", "\n", "x", "=", "embeds", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "#200x64x300", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", "sentence", ".", "size", "(", ")", "[", "0", "]", ")", "#2x64x128", "\n", "lstm_out", ",", "self", ".", "hidden", "=", "self", ".", "bilstm", "(", "x", ",", "self", ".", "hidden", ")", "###input (seq_len, batch, input_size) #Outupts:output, (h_n, c_n) output:(seq_len, batch, hidden_size * num_directions)", "\n", "#lstm_out 200x64x128", "\n", "\n", "c_lr", "=", "lstm_out", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "#64x200x128", "\n", "xi", "=", "torch", ".", "cat", "(", "(", "c_lr", "[", ":", ",", ":", ",", "0", ":", "int", "(", "c_lr", ".", "size", "(", ")", "[", "2", "]", "/", "2", ")", "]", ",", "embeds", ",", "c_lr", "[", ":", ",", ":", ",", "int", "(", "c_lr", ".", "size", "(", ")", "[", "2", "]", "/", "2", ")", ":", "]", ")", ",", "2", ")", "#64x200x428", "\n", "yi", "=", "torch", ".", "tanh", "(", "xi", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "#64x428x200", "\n", "y", "=", "self", ".", "max_pooling", "(", "yi", ")", "#64x428x99", "\n", "y", "=", "y", ".", "permute", "(", "2", ",", "0", ",", "1", ")", "\n", "\n", "##y = self.conv(lstm_out.permute(1,2,0)) ###64x256x1", "\n", "\n", "y", "=", "self", ".", "hidden2label", "(", "y", "[", "-", "1", "]", ")", "\n", "#y  = self.hidden2label(y[:,-1,:].view(y[:,-1,:].size()[0],-1)) ", "\n", "return", "y", "", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.BasicCNN1D.__init__": [[13, 36], ["BaseModel.BaseModel.__init__", "opt.__dict__.get", "opt.__dict__.get", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "CNNBasic.BasicCNN1D.properties.update", "opt.__dict__.get", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "BasicCNN1D", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "content_dim", "=", "opt", ".", "__dict__", ".", "get", "(", "\"content_dim\"", ",", "256", ")", "\n", "self", ".", "kernel_size", "=", "opt", ".", "__dict__", ".", "get", "(", "\"kernel_size\"", ",", "3", ")", "\n", "\n", "\n", "self", ".", "encoder", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", "+", "1", ",", "opt", ".", "embedding_dim", ")", "\n", "if", "opt", ".", "__dict__", ".", "get", "(", "\"embeddings\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "self", ".", "encoder", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "\n", "", "self", ".", "content_conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "opt", ".", "embedding_dim", ",", "\n", "out_channels", "=", "self", ".", "content_dim", ",", "#256", "\n", "kernel_size", "=", "self", ".", "kernel_size", ")", ",", "#3", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "(", "opt", ".", "max_seq_len", "-", "self", ".", "kernel_size", "+", "1", ")", ")", "\n", "#            nn.AdaptiveMaxPool1d()", "\n", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "content_dim", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"content_dim\"", ":", "self", ".", "content_dim", ",", "\n", "\"kernel_size\"", ":", "self", ".", "kernel_size", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.BasicCNN1D.embd_to_logit": [[38, 43], ["CNNBasic.BasicCNN1D.content_conv", "CNNBasic.BasicCNN1D.view", "CNNBasic.BasicCNN1D.fc", "embd.permute", "CNNBasic.BasicCNN1D.size"], "methods", ["None"], ["", "def", "embd_to_logit", "(", "self", ",", "embd", ")", ":", "\n", "        ", "content_out", "=", "self", ".", "content_conv", "(", "embd", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "#64x256x1", "\n", "reshaped", "=", "content_out", ".", "view", "(", "content_out", ".", "size", "(", "0", ")", ",", "-", "1", ")", "#64x256", "\n", "logits", "=", "self", ".", "fc", "(", "reshaped", ")", "#64x3", "\n", "return", "logits", "\n", "", "def", "text_to_embd", "(", "self", ",", "content", ")", ":", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.BasicCNN1D.text_to_embd": [[43, 46], ["CNNBasic.BasicCNN1D.encoder"], "methods", ["None"], ["", "def", "text_to_embd", "(", "self", ",", "content", ")", ":", "\n", "        ", "embd", "=", "self", ".", "encoder", "(", "content", ")", "#64x200x300", "\n", "return", "embd", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.BasicCNN1D.forward": [[47, 51], ["CNNBasic.BasicCNN1D.text_to_embd", "CNNBasic.BasicCNN1D.embd_to_logit"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit"], ["", "def", "forward", "(", "self", ",", "content", ")", ":", "\n", "        ", "embd", "=", "self", ".", "text_to_embd", "(", "content", ")", "\n", "logits", "=", "self", ".", "embd_to_logit", "(", "embd", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.AdvBasicCNN1D.__init__": [[53, 74], ["BaseModelAdv.AdvBaseModel.__init__", "opt.__dict__.get", "opt.__dict__.get", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "CNNBasic.AdvBasicCNN1D.properties.update", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "AdvBasicCNN1D", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "content_dim", "=", "opt", ".", "__dict__", ".", "get", "(", "\"content_dim\"", ",", "256", ")", "\n", "self", ".", "kernel_size", "=", "opt", ".", "__dict__", ".", "get", "(", "\"kernel_size\"", ",", "3", ")", "\n", "\n", "self", ".", "content_conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "self", ".", "embedding_out_dim", ",", "\n", "out_channels", "=", "self", ".", "content_dim", ",", "#256", "\n", "kernel_size", "=", "self", ".", "kernel_size", ")", ",", "#3", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "(", "opt", ".", "max_seq_len", "-", "self", ".", "kernel_size", "+", "1", ")", ")", ",", "\n", "#nn.MeanPool1d(kernel_size = (opt.max_seq_len - self.kernel_size + 1))", "\n", "#            nn.AdaptiveMaxPool1d()", "\n", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "content_dim", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"content_dim\"", ":", "self", ".", "content_dim", ",", "\n", "\"kernel_size\"", ":", "self", ".", "kernel_size", ",", "\n", "}", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "opt", ".", "keep_dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.AdvBasicCNN1D.embd_to_logit": [[75, 81], ["CNNBasic.AdvBasicCNN1D.content_conv", "CNNBasic.AdvBasicCNN1D.view", "CNNBasic.AdvBasicCNN1D.dropout", "CNNBasic.AdvBasicCNN1D.fc", "embd.permute", "CNNBasic.AdvBasicCNN1D.size"], "methods", ["None"], ["", "def", "embd_to_logit", "(", "self", ",", "embd", ")", ":", "\n", "        ", "content_out", "=", "self", ".", "content_conv", "(", "embd", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "#64x256x1", "\n", "reshaped", "=", "content_out", ".", "view", "(", "content_out", ".", "size", "(", "0", ")", ",", "-", "1", ")", "#64x256", "\n", "reshaped", "=", "self", ".", "dropout", "(", "reshaped", ")", "\n", "logits", "=", "self", ".", "fc", "(", "reshaped", ")", "#64x3", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.AdvBasicCNN2D.__init__": [[89, 106], ["BaseModelAdv.AdvBaseModel.__init__", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "CNNBasic.AdvBasicCNN2D.properties.update", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "AdvBasicCNN2D", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "keep_dropout", "=", "opt", ".", "keep_dropout", "\n", "in_channel", "=", "1", "\n", "self", ".", "kernel_nums", "=", "(", "64", ",", "64", ",", "64", ",", "64", ")", "\n", "self", ".", "kernel_sizes", "=", "(", "1", ",", "2", ",", "3", ",", "5", ")", "\n", "self", ".", "out_channel", "=", "self", ".", "kernel_nums", "[", "0", "]", "\n", "\n", "self", ".", "conv", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv2d", "(", "in_channel", ",", "out_channel", ",", "(", "K", ",", "self", ".", "embedding_out_dim", ")", ")", "for", "K", ",", "out_channel", "in", "zip", "(", "self", ".", "kernel_sizes", ",", "self", ".", "kernel_nums", ")", "]", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "keep_dropout", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "len", "(", "self", ".", "kernel_sizes", ")", "*", "self", ".", "out_channel", ",", "self", ".", "label_size", ")", "\n", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"kernel_nums\"", ":", "self", ".", "kernel_nums", ",", "\n", "\"kernel_sizes\"", ":", "self", ".", "kernel_sizes", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.AdvBasicCNN2D.embd_to_logit": [[108, 124], ["CNNBasic.AdvBasicCNN2D.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "CNNBasic.AdvBasicCNN2D.dropout", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "CNNBasic.AdvBasicCNN2D.fc", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "conv", "i.size"], "methods", ["None"], ["", "def", "embd_to_logit", "(", "self", ",", "x", ")", ":", "\n", "# Conv & max pool", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "# dim: (batch_size, 1, max_seq_len, embedding_size)", "\n", "\n", "# turns to be a list: [ti : i \\in kernel_sizes] where ti: tensor of dim([batch, num_kernels, max_seq_len-i+1])", "\n", "x", "=", "[", "F", ".", "relu", "(", "conv", "(", "x", ")", ")", ".", "squeeze", "(", "3", ")", "for", "conv", "in", "self", ".", "conv", "]", "\n", "\n", "# dim: [(batch_size, num_kernels), ...]*len(kernel_sizes)", "\n", "x", "=", "[", "F", ".", "max_pool1d", "(", "i", ",", "i", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "for", "i", "in", "x", "]", "\n", "x", "=", "torch", ".", "cat", "(", "x", ",", "1", ")", "\n", "\n", "# Dropout & output", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "# (batch_size,len(kernel_sizes)*num_kernels)", "\n", "logit", "=", "F", ".", "log_softmax", "(", "self", ".", "fc", "(", "x", ")", ")", "# (batch_size, num_aspects)", "\n", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.BasicCNN2D.__init__": [[134, 158], ["BaseModel.BaseModel.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "CNNBasic.BasicCNN2D.properties.update", "opt.__dict__.get", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv2d", "len", "zip"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "BasicCNN2D", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "embedding_dim", "=", "opt", ".", "embedding_dim", "\n", "self", ".", "vocab_size", "=", "opt", ".", "vocab_size", "\n", "self", ".", "label_size", "=", "opt", ".", "label_size", "\n", "self", ".", "keep_dropout", "=", "opt", ".", "keep_dropout", "\n", "in_channel", "=", "1", "\n", "self", ".", "kernel_nums", "=", "opt", ".", "kernel_nums", "\n", "self", ".", "kernel_sizes", "=", "opt", ".", "kernel_sizes", "\n", "\n", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", "+", "1", ",", "self", ".", "embedding_dim", ")", "\n", "\n", "if", "opt", ".", "__dict__", ".", "get", "(", "\"embeddings\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "self", ".", "embed", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ")", "\n", "\n", "", "self", ".", "conv", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv2d", "(", "in_channel", ",", "out_channel", ",", "(", "K", ",", "self", ".", "embedding_dim", ")", ")", "for", "K", ",", "out_channel", "in", "zip", "(", "self", ".", "kernel_sizes", ",", "self", ".", "kernel_nums", ")", "]", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "keep_dropout", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "len", "(", "self", ".", "kernel_sizes", ")", "*", "self", ".", "out_channel", ",", "self", ".", "label_size", ")", "\n", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"kernel_nums\"", ":", "self", ".", "kernel_nums", ",", "\n", "\"kernel_sizes\"", ":", "self", ".", "kernel_sizes", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.BasicCNN2D.embd_to_logit": [[188, 204], ["CNNBasic.BasicCNN2D.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "CNNBasic.BasicCNN2D.dropout", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "CNNBasic.BasicCNN2D.fc", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "conv", "i.size"], "methods", ["None"], ["def", "embd_to_logit", "(", "self", ",", "embd", ")", ":", "\n", "# Conv & max pool", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "# dim: (batch_size, 1, max_seq_len, embedding_size)", "\n", "\n", "# turns to be a list: [ti : i \\in kernel_sizes] where ti: tensor of dim([batch, num_kernels, max_seq_len-i+1])", "\n", "x", "=", "[", "F", ".", "relu", "(", "conv", "(", "x", ")", ")", ".", "squeeze", "(", "3", ")", "for", "conv", "in", "self", ".", "conv", "]", "\n", "\n", "# dim: [(batch_size, num_kernels), ...]*len(kernel_sizes)", "\n", "x", "=", "[", "F", ".", "max_pool1d", "(", "i", ",", "i", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "for", "i", "in", "x", "]", "\n", "x", "=", "torch", ".", "cat", "(", "x", ",", "1", ")", "\n", "\n", "# Dropout & output", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "# (batch_size,len(kernel_sizes)*num_kernels)", "\n", "logit", "=", "F", ".", "log_softmax", "(", "self", ".", "fc", "(", "x", ")", ")", "# (batch_size, num_aspects)", "\n", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.BasicCNN2D.text_to_embd": [[205, 208], ["CNNBasic.BasicCNN2D.embed"], "methods", ["None"], ["", "def", "text_to_embd", "(", "self", ",", "content", ")", ":", "\n", "        ", "embd", "=", "self", ".", "embed", "(", "content", ")", "# dim: (batch_size, max_seq_len, embedding_size)", "\n", "return", "embd", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.BasicCNN2D.forward": [[209, 213], ["CNNBasic.BasicCNN2D.text_to_embd", "CNNBasic.BasicCNN2D.embd_to_logit"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit"], ["", "def", "forward", "(", "self", ",", "content", ")", ":", "\n", "        ", "embd", "=", "self", ".", "text_to_embd", "(", "content", ")", "\n", "logits", "=", "self", ".", "embd_to_logit", "(", "embd", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNBasic.parse_opt": [[216, 252], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_opt", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "# Data input settings", "\n", "parser", ".", "add_argument", "(", "'--hidden_dim'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'hidden_dim'", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'batch_size'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_dim'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "\n", "help", "=", "'embedding_dim'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "4e-4", ",", "\n", "help", "=", "'learning_rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--grad_clip'", ",", "type", "=", "float", ",", "default", "=", "1e-1", ",", "\n", "help", "=", "'grad_clip'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "\"lstm\"", ",", "\n", "help", "=", "'model name'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "\"lstm\"", ",", "\n", "help", "=", "'model name'", ")", "\n", "\n", "\n", "#", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "embedding_dim", "=", "300", "\n", "args", ".", "vocab_size", "=", "10000", "\n", "args", ".", "kernel_size", "=", "3", "\n", "args", ".", "num_classes", "=", "3", "\n", "args", ".", "content_dim", "=", "256", "\n", "args", ".", "max_seq_len", "=", "50", "\n", "\n", "#", "\n", "#    # Check if args are valid", "\n", "#    assert args.rnn_size > 0, \"rnn_size should be greater than 0\"", "\n", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNKim.KIMCNN1D.__init__": [[7, 51], ["models.BaseModel.BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "torch.Linear", "CNNKim.KIMCNN1D.properties.update", "len", "len", "torch.Parameter", "torch.Parameter", "torch.Parameter", "sum", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "zip"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "KIMCNN1D", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "embedding_type", "=", "opt", ".", "embedding_type", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "max_seq_len", "=", "opt", ".", "max_seq_len", "\n", "self", ".", "embedding_dim", "=", "opt", ".", "embedding_dim", "\n", "self", ".", "vocab_size", "=", "opt", ".", "vocab_size", "\n", "self", ".", "label_size", "=", "opt", ".", "label_size", "\n", "self", ".", "kernel_sizes", "=", "opt", ".", "kernel_sizes", "\n", "self", ".", "kernel_nums", "=", "opt", ".", "kernel_nums", "\n", "self", ".", "keep_dropout", "=", "opt", ".", "keep_dropout", "\n", "self", ".", "in_channel", "=", "1", "\n", "\n", "assert", "(", "len", "(", "self", ".", "kernel_sizes", ")", "==", "len", "(", "self", ".", "kernel_nums", ")", ")", "\n", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", "+", "2", ",", "self", ".", "embedding_dim", ")", "#, padding_idx=self.vocab_size + 1", "\n", "if", "opt", ".", "use_pretrained_embeddings", ":", "\n", "            ", "self", ".", "embedding", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ")", "\n", "", "self", ".", "embedding", ".", "weight", ".", "requires_grad", "=", "True", "\n", "\n", "\n", "# one for UNK and one for zero padding", "\n", "#self.embedding = nn.Embedding(self.vocab_size + 2, self.embedding_dim) #, padding_idx=self.vocab_size + 1", "\n", "#if self.embedding_type == \"static\" or self.embedding_type == \"non-static\" or self.embedding_type == \"multichannel\":", "\n", "#self.embedding.weight=nn.Parameter(opt.embeddings)            ", "\n", "#if self.embedding_type == \"static\":", "\n", "#self.embedding.weight.requires_grad = False", "\n", "#elif self.embedding_type == \"multichannel\":", "\n", "#    self.embedding2 = nn.Embedding(self.vocab_size + 2, self.embedding_dim, padding_idx=self.vocab_size + 1)", "\n", "#    self.embedding2.weight=nn.Parameter(opt.embeddings) ", "\n", "#    self.embedding2.weight.requires_grad = False", "\n", "#    self.in_channel = 2", "\n", "#else:", "\n", "#    pass", "\n", "#", "\n", "#        for i in range(len(self.kernel_sizes)):", "\n", "#            conv = nn.Conv1d(self.in_channel, self.kernel_nums[i], self.embedding_dim * self.kernel_sizes[i], stride=self.embedding_dim)", "\n", "#            setattr(self, 'conv_%d'%i, conv)", "\n", "self", ".", "convs", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv1d", "(", "self", ".", "in_channel", ",", "num", ",", "self", ".", "embedding_dim", "*", "size", ",", "stride", "=", "self", ".", "embedding_dim", ")", "for", "size", ",", "num", "in", "zip", "(", "opt", ".", "kernel_sizes", ",", "opt", ".", "kernel_nums", ")", "]", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "sum", "(", "self", ".", "kernel_nums", ")", ",", "self", ".", "label_size", ")", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"kernel_sizes\"", ":", "self", ".", "kernel_sizes", ",", "\n", "\"kernel_nums\"", ":", "self", ".", "kernel_nums", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNKim.KIMCNN1D.get_conv": [[52, 54], ["getattr"], "methods", ["None"], ["", "def", "get_conv", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ",", "'conv_%d'", "%", "i", ")", "\n", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNKim.KIMCNN1D.embd_to_logit": [[71, 84], ["CNNKim.KIMCNN1D.view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.dropout", "torch.dropout", "torch.dropout", "CNNKim.KIMCNN1D.fc", "torch.max_pool1d().view", "torch.max_pool1d().view", "torch.max_pool1d().view", "range", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "len", "torch.relu", "torch.relu", "torch.relu"], "methods", ["None"], ["def", "embd_to_logit", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "x", "=", "x", ".", "view", "(", "-", "1", ",", "1", ",", "self", ".", "embedding_dim", "*", "self", ".", "max_seq_len", ")", "\n", "\n", "conv_results", "=", "[", "\n", "F", ".", "max_pool1d", "(", "F", ".", "relu", "(", "self", ".", "convs", "[", "i", "]", "(", "x", ")", ")", ",", "self", ".", "max_seq_len", "-", "self", ".", "kernel_sizes", "[", "i", "]", "+", "1", ")", "\n", ".", "view", "(", "-", "1", ",", "self", ".", "kernel_nums", "[", "i", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "convs", ")", ")", "]", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "conv_results", ",", "1", ")", "\n", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "keep_dropout", ",", "training", "=", "self", ".", "training", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNKim.KIMCNN1D.text_to_embd": [[85, 92], ["CNNKim.KIMCNN1D.embedding"], "methods", ["None"], ["", "def", "text_to_embd", "(", "self", ",", "inp", ")", ":", "\n", "#x = self.embedding(inp).view(-1, 1, self.embedding_dim * self.max_seq_len)", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "inp", ")", "\n", "#if self.embedding_type == \"multichannel\":", "\n", "#    x2 = self.embedding2(inp).view(-1, 1, self.embedding_dim * self.max_seq_len)", "\n", "#    x = torch.cat((x, x2), 1)", "\n", "return", "x", "\n", "", "def", "forward", "(", "self", ",", "content", ")", ":", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNKim.KIMCNN1D.forward": [[92, 96], ["CNNKim.KIMCNN1D.text_to_embd", "CNNKim.KIMCNN1D.embd_to_logit"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit"], ["", "def", "forward", "(", "self", ",", "content", ")", ":", "\n", "        ", "embd", "=", "self", ".", "text_to_embd", "(", "content", ")", "\n", "logits", "=", "self", ".", "embd_to_logit", "(", "embd", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNKim.KIMCNN2D.__init__": [[100, 134], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "sum", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "zip"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "KIMCNN2D", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "embedding_type", "=", "opt", ".", "embedding_type", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "max_seq_len", "=", "opt", ".", "max_seq_len", "\n", "self", ".", "embedding_dim", "=", "opt", ".", "embedding_dim", "\n", "self", ".", "vocab_size", "=", "opt", ".", "vocab_size", "\n", "self", ".", "label_size", "=", "opt", ".", "label_size", "\n", "self", ".", "kernel_sizes", "=", "opt", ".", "kernel_sizes", "\n", "self", ".", "kernel_nums", "=", "opt", ".", "kernel_nums", "\n", "self", ".", "keep_dropout", "=", "opt", ".", "keep_dropout", "\n", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", "+", "2", ",", "self", ".", "embedding_dim", ")", "# padding_idx=self.vocab_size + 1", "\n", "if", "self", ".", "embedding_type", "==", "\"static\"", "or", "self", ".", "embedding_type", "==", "\"non-static\"", "or", "self", ".", "embedding_type", "==", "\"multichannel\"", ":", "\n", "            ", "self", ".", "embedding", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ")", "\n", "if", "self", ".", "embedding_type", "==", "\"static\"", ":", "\n", "                ", "self", ".", "embedding", ".", "weight", ".", "requires_grad", "=", "False", "\n", "", "elif", "self", ".", "embedding_type", "==", "\"multichannel\"", ":", "\n", "                ", "self", ".", "embedding2", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", "+", "2", ",", "self", ".", "embedding_dim", ",", "padding_idx", "=", "self", ".", "vocab_size", "+", "1", ")", "\n", "self", ".", "embedding2", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ")", "\n", "self", ".", "embedding2", ".", "weight", ".", "requires_grad", "=", "False", "\n", "self", ".", "in_channel", "=", "2", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "#self.convs1 = [nn.Conv2d(Ci, Co, (K, D)) for K in Ks]", "\n", "", "", "self", ".", "convs1", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Conv2d", "(", "1", ",", "num", ",", "(", "size", ",", "opt", ".", "embedding_dim", ")", ")", "for", "size", ",", "num", "in", "zip", "(", "opt", ".", "kernel_sizes", ",", "opt", ".", "kernel_nums", ")", "]", ")", "\n", "'''\n        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n        '''", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "opt", ".", "keep_dropout", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "sum", "(", "opt", ".", "kernel_nums", ")", ",", "opt", ".", "label_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNKim.KIMCNN2D.conv_and_pool": [[135, 139], ["torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "conv", "torch.max_pool1d().squeeze.size"], "methods", ["None"], ["", "def", "conv_and_pool", "(", "self", ",", "x", ",", "conv", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "conv", "(", "x", ")", ")", ".", "squeeze", "(", "3", ")", "#(N,Co,W)", "\n", "x", "=", "F", ".", "max_pool1d", "(", "x", ",", "x", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNKim.KIMCNN2D.forward": [[141, 164], ["CNNKim.KIMCNN2D.embedding", "CNNKim.KIMCNN2D.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "CNNKim.KIMCNN2D.dropout", "CNNKim.KIMCNN2D.fc", "torch.relu().squeeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.max_pool1d().squeeze", "torch.relu", "torch.relu", "torch.relu", "torch.max_pool1d", "torch.max_pool1d", "torch.max_pool1d", "conv", "i.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "x", ")", "# (N,W,D)", "\n", "\n", "\n", "\n", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "# (N,Ci,W,D)", "\n", "\n", "x", "=", "[", "F", ".", "relu", "(", "conv", "(", "x", ")", ")", ".", "squeeze", "(", "3", ")", "for", "conv", "in", "self", ".", "convs1", "]", "#[(N,Co,W), ...]*len(Ks)", "\n", "\n", "\n", "x", "=", "[", "F", ".", "max_pool1d", "(", "i", ",", "i", ".", "size", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "for", "i", "in", "x", "]", "#[(N,Co), ...]*len(Ks)", "\n", "\n", "x", "=", "torch", ".", "cat", "(", "x", ",", "1", ")", "\n", "\n", "'''\n        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n        '''", "\n", "x", "=", "self", ".", "dropout", "(", "x", ")", "# (N,len(Ks)*Co)", "\n", "logit", "=", "self", ".", "fc", "(", "x", ")", "# (N,C)", "\n", "return", "logit", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.Dev_Att_Encoder.__init__": [[12, 24], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "ForSnli.Dev_Att_Encoder.modules", "isinstance", "m.weight.data.normal_"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "embedding_size", ",", "hidden_size", "=", "300", ",", "para_init", "=", "0.01", ")", ":", "\n", "        ", "super", "(", "Dev_Att_Encoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embedding_size", "=", "embedding_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "para_init", "=", "para_init", "\n", "\n", "self", ".", "input_linear", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "embedding_size", ",", "self", ".", "hidden_size", ",", "bias", "=", "False", ")", "# linear transformation", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "self", ".", "para_init", ")", "\n", "# m.bias.data.uniform_(-0.01, 0.01)", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.Dev_Att_Encoder.forward": [[26, 39], ["embd_1.view.view.view", "embd_2.view.view.view", "ForSnli.Dev_Att_Encoder.input_linear().view", "ForSnli.Dev_Att_Encoder.input_linear().view", "ForSnli.Dev_Att_Encoder.input_linear", "ForSnli.Dev_Att_Encoder.input_linear"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "embd_1", ",", "embd_2", ")", ":", "\n", "\n", "        ", "batch_size", ",", "sent_len", ",", "embd_size", "=", "embd_1", ".", "shape", "\n", "\n", "embd_1", "=", "embd_1", ".", "view", "(", "-", "1", ",", "embd_size", ")", "\n", "embd_2", "=", "embd_2", ".", "view", "(", "-", "1", ",", "embd_size", ")", "\n", "\n", "sent1_linear", "=", "self", ".", "input_linear", "(", "embd_1", ")", ".", "view", "(", "\n", "batch_size", ",", "sent_len", ",", "self", ".", "hidden_size", ")", "\n", "sent2_linear", "=", "self", ".", "input_linear", "(", "embd_2", ")", ".", "view", "(", "\n", "batch_size", ",", "sent_len", ",", "self", ".", "hidden_size", ")", "\n", "\n", "return", "sent1_linear", ",", "sent2_linear", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.Dev_Att_Atten.__init__": [[45, 67], ["torch.nn.Module.__init__", "ForSnli.Dev_Att_Atten._mlp_layers", "ForSnli.Dev_Att_Atten._mlp_layers", "ForSnli.Dev_Att_Atten._mlp_layers", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "ForSnli.Dev_Att_Atten.modules", "isinstance", "m.weight.data.normal_", "m.bias.data.normal_"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.Dev_Att_Atten._mlp_layers", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.Dev_Att_Atten._mlp_layers", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.Dev_Att_Atten._mlp_layers"], ["def", "__init__", "(", "self", ",", "label_size", ",", "hidden_size", "=", "300", ",", "para_init", "=", "0.01", ")", ":", "\n", "        ", "super", "(", "Dev_Att_Atten", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "label_size", "=", "label_size", "\n", "self", ".", "para_init", "=", "para_init", "\n", "\n", "self", ".", "mlp_f", "=", "self", ".", "_mlp_layers", "(", "self", ".", "hidden_size", ",", "self", ".", "hidden_size", ")", "\n", "self", ".", "mlp_g", "=", "self", ".", "_mlp_layers", "(", "2", "*", "self", ".", "hidden_size", ",", "self", ".", "hidden_size", ")", "\n", "self", ".", "mlp_h", "=", "self", ".", "_mlp_layers", "(", "2", "*", "self", ".", "hidden_size", ",", "self", ".", "hidden_size", ")", "\n", "\n", "self", ".", "final_linear", "=", "nn", ".", "Linear", "(", "\n", "self", ".", "hidden_size", ",", "self", ".", "label_size", ",", "bias", "=", "True", ")", "\n", "\n", "self", ".", "log_prob", "=", "nn", ".", "LogSoftmax", "(", ")", "\n", "\n", "'''initialize parameters'''", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "# print m", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Linear", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "self", ".", "para_init", ")", "\n", "m", ".", "bias", ".", "data", ".", "normal_", "(", "0", ",", "self", ".", "para_init", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.Dev_Att_Atten._mlp_layers": [[68, 79], ["mlp_layers.append", "mlp_layers.append", "mlp_layers.append", "mlp_layers.append", "mlp_layers.append", "mlp_layers.append", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU"], "methods", ["None"], ["", "", "", "def", "_mlp_layers", "(", "self", ",", "input_dim", ",", "output_dim", ")", ":", "\n", "        ", "mlp_layers", "=", "[", "]", "\n", "mlp_layers", ".", "append", "(", "nn", ".", "Dropout", "(", "p", "=", "0.2", ")", ")", "\n", "mlp_layers", ".", "append", "(", "nn", ".", "Linear", "(", "\n", "input_dim", ",", "output_dim", ",", "bias", "=", "True", ")", ")", "\n", "mlp_layers", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "mlp_layers", ".", "append", "(", "nn", ".", "Dropout", "(", "p", "=", "0.2", ")", ")", "\n", "mlp_layers", ".", "append", "(", "nn", ".", "Linear", "(", "\n", "output_dim", ",", "output_dim", ",", "bias", "=", "True", ")", ")", "\n", "mlp_layers", ".", "append", "(", "nn", ".", "ReLU", "(", ")", ")", "\n", "return", "nn", ".", "Sequential", "(", "*", "mlp_layers", ")", "# * used to unpack list", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.Dev_Att_Atten.forward": [[80, 140], ["sent1_linear.size", "sent2_linear.size", "ForSnli.Dev_Att_Atten.mlp_f", "ForSnli.Dev_Att_Atten.mlp_f", "f1.view.view.view", "f2.view.view.view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.softmax().view", "torch.softmax().view", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "score2.contiguous.contiguous.contiguous", "torch.softmax().view", "torch.softmax().view", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ForSnli.Dev_Att_Atten.mlp_g", "ForSnli.Dev_Att_Atten.mlp_g", "g1.view.view.view", "g2.view.view.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ForSnli.Dev_Att_Atten.mlp_h", "ForSnli.Dev_Att_Atten.final_linear", "sent1_linear.view", "sent2_linear.view", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose", "torch.bmm.contiguous", "torch.bmm.contiguous", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.cat.view", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm.view", "torch.bmm.view", "score2.contiguous.contiguous.view"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "forward", "(", "self", ",", "sent1_linear", ",", "sent2_linear", ")", ":", "\n", "        ", "'''\n            sent_linear: batch_size x length x hidden_size\n        '''", "\n", "len1", "=", "sent1_linear", ".", "size", "(", "1", ")", "\n", "len2", "=", "sent2_linear", ".", "size", "(", "1", ")", "\n", "\n", "'''attend'''", "\n", "\n", "f1", "=", "self", ".", "mlp_f", "(", "sent1_linear", ".", "view", "(", "-", "1", ",", "self", ".", "hidden_size", ")", ")", "\n", "f2", "=", "self", ".", "mlp_f", "(", "sent2_linear", ".", "view", "(", "-", "1", ",", "self", ".", "hidden_size", ")", ")", "\n", "\n", "f1", "=", "f1", ".", "view", "(", "-", "1", ",", "len1", ",", "self", ".", "hidden_size", ")", "\n", "# batch_size x len1 x hidden_size", "\n", "f2", "=", "f2", ".", "view", "(", "-", "1", ",", "len2", ",", "self", ".", "hidden_size", ")", "\n", "# batch_size x len2 x hidden_size", "\n", "\n", "score1", "=", "torch", ".", "bmm", "(", "f1", ",", "torch", ".", "transpose", "(", "f2", ",", "1", ",", "2", ")", ")", "\n", "# e_{ij} batch_size x len1 x len2", "\n", "prob1", "=", "F", ".", "softmax", "(", "score1", ".", "view", "(", "-", "1", ",", "len2", ")", ")", ".", "view", "(", "-", "1", ",", "len1", ",", "len2", ")", "\n", "# batch_size x len1 x len2", "\n", "\n", "score2", "=", "torch", ".", "transpose", "(", "score1", ".", "contiguous", "(", ")", ",", "1", ",", "2", ")", "\n", "score2", "=", "score2", ".", "contiguous", "(", ")", "\n", "# e_{ji} batch_size x len2 x len1", "\n", "prob2", "=", "F", ".", "softmax", "(", "score2", ".", "view", "(", "-", "1", ",", "len1", ")", ")", ".", "view", "(", "-", "1", ",", "len2", ",", "len1", ")", "\n", "# batch_size x len2 x len1", "\n", "\n", "sent1_combine", "=", "torch", ".", "cat", "(", "\n", "(", "sent1_linear", ",", "torch", ".", "bmm", "(", "prob1", ",", "sent2_linear", ")", ")", ",", "2", ")", "\n", "# batch_size x len1 x (hidden_size x 2)", "\n", "sent2_combine", "=", "torch", ".", "cat", "(", "\n", "(", "sent2_linear", ",", "torch", ".", "bmm", "(", "prob2", ",", "sent1_linear", ")", ")", ",", "2", ")", "\n", "# batch_size x len2 x (hidden_size x 2)", "\n", "\n", "'''sum'''", "\n", "g1", "=", "self", ".", "mlp_g", "(", "sent1_combine", ".", "view", "(", "-", "1", ",", "2", "*", "self", ".", "hidden_size", ")", ")", "\n", "g2", "=", "self", ".", "mlp_g", "(", "sent2_combine", ".", "view", "(", "-", "1", ",", "2", "*", "self", ".", "hidden_size", ")", ")", "\n", "g1", "=", "g1", ".", "view", "(", "-", "1", ",", "len1", ",", "self", ".", "hidden_size", ")", "\n", "# batch_size x len1 x hidden_size", "\n", "g2", "=", "g2", ".", "view", "(", "-", "1", ",", "len2", ",", "self", ".", "hidden_size", ")", "\n", "# batch_size x len2 x hidden_size", "\n", "\n", "sent1_output", "=", "torch", ".", "sum", "(", "g1", ",", "1", ")", "# batch_size x 1 x hidden_size", "\n", "sent1_output", "=", "torch", ".", "squeeze", "(", "sent1_output", ",", "1", ")", "\n", "sent2_output", "=", "torch", ".", "sum", "(", "g2", ",", "1", ")", "# batch_size x 1 x hidden_size", "\n", "sent2_output", "=", "torch", ".", "squeeze", "(", "sent2_output", ",", "1", ")", "\n", "\n", "input_combine", "=", "torch", ".", "cat", "(", "(", "sent1_output", ",", "sent2_output", ")", ",", "1", ")", "\n", "# batch_size x (2 * hidden_size)", "\n", "h", "=", "self", ".", "mlp_h", "(", "input_combine", ")", "\n", "# batch_size * hidden_size", "\n", "\n", "# if sample_id == 15:", "\n", "#     print '-2 layer'", "\n", "#     print h.data[:, 100:150]", "\n", "\n", "logits", "=", "self", ".", "final_linear", "(", "h", ")", "\n", "#log_prob = self.log_prob(logits)", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.__init__": [[142, 147], ["BaseModelAdv.AdvBaseModel.__init__", "ForSnli.Dev_Att_Encoder", "ForSnli.Dev_Att_Atten"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "AdvDecAtt", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "encoder", "=", "Dev_Att_Encoder", "(", "self", ".", "embedding_out_dim", ")", "\n", "self", ".", "atten", "=", "Dev_Att_Atten", "(", "self", ".", "label_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.embd_to_logit": [[148, 152], ["ForSnli.AdvDecAtt.encoder", "ForSnli.AdvDecAtt.atten"], "methods", ["None"], ["", "def", "embd_to_logit", "(", "self", ",", "embd_p", ",", "embd_h", ",", "x_p_mask", ",", "x_h_mask", ")", ":", "\n", "        ", "p_linear", ",", "h_linear", "=", "self", ".", "encoder", "(", "embd_p", ",", "embd_h", ")", "\n", "logits", "=", "self", ".", "atten", "(", "p_linear", ",", "h_linear", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.text_to_embd": [[153, 164], ["ForSnli.AdvDecAtt.embedding", "ForSnli.AdvDecAtt.embedding", "ForSnli.AdvDecAtt.linear_transform_embd_1", "ForSnli.AdvDecAtt.linear_transform_embd_1"], "methods", ["None"], ["", "def", "text_to_embd", "(", "self", ",", "x_p", ",", "x_h", ")", ":", "\n", "        ", "embd_x_p", "=", "self", ".", "embedding", "(", "x_p", ")", "\n", "embd_x_h", "=", "self", ".", "embedding", "(", "x_h", ")", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "embd_x_p", "=", "self", ".", "linear_transform_embd_1", "(", "embd_x_p", ")", "\n", "embd_x_h", "=", "self", ".", "linear_transform_embd_1", "(", "embd_x_h", ")", "\n", "#embd_x_p = F.relu(embd_x_p)", "\n", "#x = self.linear_transform_embd_2(x)", "\n", "#embd_x_h = F.relu(embd_x_h)", "\n", "#x = self.linear_transform_embd_3(x)", "\n", "", "return", "embd_x_p", ",", "embd_x_h", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.get_adv_by_convex_syn": [[165, 289], ["torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.kaiming_normal_", "torch.empty().to().to.requires_grad_", "torch.empty().to().to.requires_grad_", "torch.empty().to().to.requires_grad_", "torch.empty().to().to.requires_grad_", "utils.getOptimizer", "embd_p.detach", "embd_h.detach", "ForSnli.AdvDecAtt.embd_to_logit", "range", "ForSnli.AdvDecAtt.get_adv_by_convex_syn.get_comb_p"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.getOptimizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit"], ["", "def", "get_adv_by_convex_syn", "(", "self", ",", "embd_p", ",", "embd_h", ",", "y", ",", "x_p_text_like_syn", ",", "x_p_syn_embd", ",", "x_p_syn_valid", ",", "x_h_text_like_syn", ",", "x_h_syn_embd", ",", "x_h_syn_valid", ",", "x_p_mask", ",", "x_h_mask", ",", "attack_type_dict", ")", ":", "\n", "\n", "#noted that if attack hypo only then the output x_p_comb_p is meaningless", "\n", "\n", "# record context", "\n", "        ", "self_training_context", "=", "self", ".", "training", "\n", "# set context", "\n", "if", "self", ".", "eval_adv_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "\n", "", "device", "=", "embd_p", ".", "device", "\n", "# get param of attacks", "\n", "\n", "num_steps", "=", "attack_type_dict", "[", "'num_steps'", "]", "\n", "loss_func", "=", "attack_type_dict", "[", "'loss_func'", "]", "\n", "w_optm_lr", "=", "attack_type_dict", "[", "'w_optm_lr'", "]", "\n", "sparse_weight", "=", "attack_type_dict", "[", "'sparse_weight'", "]", "\n", "out_type", "=", "attack_type_dict", "[", "'out_type'", "]", "\n", "attack_hypo_only", "=", "attack_type_dict", "[", "'attack_hypo_only'", "]", "if", "'attack_hypo_only'", "in", "attack_type_dict", "else", "True", "\n", "\n", "batch_size", ",", "text_len", ",", "embd_dim", "=", "embd_p", ".", "shape", "\n", "batch_size", ",", "text_len", ",", "syn_num", ",", "embd_dim", "=", "x_p_syn_embd", ".", "shape", "\n", "\n", "w_p", "=", "torch", ".", "empty", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", ".", "to", "(", "device", ")", ".", "to", "(", "embd_p", ".", "dtype", ")", "\n", "w_h", "=", "torch", ".", "empty", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", ".", "to", "(", "device", ")", ".", "to", "(", "embd_p", ".", "dtype", ")", "\n", "#ww = torch.zeros(batch_size, text_len, syn_num, 1).to(device).to(embd.dtype)", "\n", "#ww = ww+500*(syn_valid.reshape(batch_size, text_len, syn_num, 1)-1)", "\n", "nn", ".", "init", ".", "kaiming_normal_", "(", "w_p", ")", "\n", "nn", ".", "init", ".", "kaiming_normal_", "(", "w_h", ")", "\n", "w_p", ".", "requires_grad_", "(", ")", "\n", "w_h", ".", "requires_grad_", "(", ")", "\n", "\n", "import", "utils", "\n", "params", "=", "[", "w_p", ",", "w_h", "]", "\n", "optimizer", "=", "utils", ".", "getOptimizer", "(", "params", ",", "name", "=", "'adam'", ",", "lr", "=", "w_optm_lr", ",", "weight_decay", "=", "2e-5", ")", "\n", "\n", "def", "get_comb_p", "(", "w", ",", "syn_valid", ")", ":", "\n", "            ", "ww", "=", "w", "*", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "+", "10000", "*", "(", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "-", "1", ")", "\n", "return", "F", ".", "softmax", "(", "ww", ",", "-", "2", ")", "\n", "\n", "", "def", "get_comb_ww", "(", "w", ",", "syn_valid", ")", ":", "\n", "            ", "ww", "=", "w", "*", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "+", "10000", "*", "(", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "-", "1", ")", "\n", "return", "ww", "\n", "\n", "", "def", "get_comb", "(", "p", ",", "syn", ")", ":", "\n", "            ", "return", "(", "p", "*", "syn", ".", "detach", "(", ")", ")", ".", "sum", "(", "-", "2", ")", "\n", "\n", "\n", "", "embd_p_ori", "=", "embd_p", ".", "detach", "(", ")", "\n", "embd_h_ori", "=", "embd_h", ".", "detach", "(", ")", "\n", "logit_ori", "=", "self", ".", "embd_to_logit", "(", "embd_p_ori", ",", "embd_h_ori", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "\n", "for", "_", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "ww_p", "=", "get_comb_ww", "(", "w_p", ",", "x_p_syn_valid", ")", "\n", "ww_h", "=", "get_comb_ww", "(", "w_h", ",", "x_h_syn_valid", ")", "\n", "#comb_p = get_comb_p(w, syn_valid)", "\n", "embd_p_adv", "=", "get_comb", "(", "F", ".", "softmax", "(", "ww_p", ",", "-", "2", ")", ",", "x_p_syn_embd", ")", "\n", "embd_h_adv", "=", "get_comb", "(", "F", ".", "softmax", "(", "ww_h", ",", "-", "2", ")", ",", "x_h_syn_embd", ")", "\n", "if", "attack_hypo_only", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_p_ori", ",", "embd_h_adv", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "", "else", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_p_adv", ",", "embd_h_adv", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "\n", "", "if", "loss_func", "==", "'ce'", ":", "\n", "                    ", "loss", "=", "-", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "elif", "loss_func", "==", "'kl'", ":", "\n", "                    ", "criterion_kl", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "loss", "=", "-", "criterion_kl", "(", "F", ".", "log_softmax", "(", "logit_adv", ",", "dim", "=", "1", ")", ",", "\n", "F", ".", "softmax", "(", "logit_ori", ".", "detach", "(", ")", ",", "dim", "=", "1", ")", ")", "\n", "\n", "#print(\"ad loss:\", loss.data.item())", "\n", "\n", "", "if", "sparse_weight", "!=", "0", ":", "\n", "#loss_sparse = (comb_p*comb_p).mean()", "\n", "                    ", "if", "attack_hypo_only", ":", "\n", "                        ", "loss_sparse", "=", "(", "-", "F", ".", "softmax", "(", "ww_h", ",", "-", "2", ")", "*", "F", ".", "log_softmax", "(", "ww_h", ",", "-", "2", ")", ")", ".", "sum", "(", "-", "2", ")", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "                        ", "loss_sparse", "=", "(", "(", "-", "F", ".", "softmax", "(", "ww_p", ",", "-", "2", ")", "*", "F", ".", "log_softmax", "(", "ww_p", ",", "-", "2", ")", ")", ".", "sum", "(", "-", "2", ")", ".", "mean", "(", ")", "+", "(", "-", "F", ".", "softmax", "(", "ww_h", ",", "-", "2", ")", "*", "F", ".", "log_softmax", "(", "ww_h", ",", "-", "2", ")", ")", ".", "sum", "(", "-", "2", ")", ".", "mean", "(", ")", ")", "/", "2", "\n", "#loss -= sparse_weight*loss_sparse", "\n", "\n", "", "loss", "=", "loss", "+", "sparse_weight", "*", "loss_sparse", "\n", "#print(loss_sparse.data.item())", "\n", "\n", "#loss*=1000", "\n", "", "", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "#print((ww-w).max())", "\n", "\n", "", "x_p_comb_p", "=", "get_comb_p", "(", "w_p", ",", "x_p_syn_valid", ")", "\n", "x_h_comb_p", "=", "get_comb_p", "(", "w_h", ",", "x_h_syn_valid", ")", "\n", "\n", "\"\"\"\n        out = get_comb(comb_p, syn)\n        delta = (out-embd_ori).reshape(batch_size*text_len,embd_dim)\n        delta = F.pairwise_distance(delta, torch.zeros_like(delta), p=2.0)\n        valid = (delta>0.01).to(device).to(delta.dtype)\n        delta = (valid*delta).sum()/valid.sum()\n        print(\"mean l2 dis between embd and embd_adv:\", delta.data.item())\n        #print(\"mean max comb_p:\", (comb_p.max(-2)[0]).mean().data.item())\n        \"\"\"", "\n", "\n", "# resume context", "\n", "if", "self_training_context", "==", "True", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "\n", "", "if", "out_type", "==", "\"comb_p\"", ":", "\n", "            ", "return", "x_p_comb_p", ".", "detach", "(", ")", ",", "x_h_comb_p", ".", "detach", "(", ")", "\n", "", "elif", "out_type", "==", "\"text\"", ":", "\n", "            ", "assert", "(", "x_p_text_like_syn", "is", "not", "None", ")", "# n l synlen", "\n", "assert", "(", "x_h_text_like_syn", "is", "not", "None", ")", "# n l synlen", "\n", "x_p_comb_p", "=", "x_p_comb_p", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "syn_num", ")", "\n", "x_h_comb_p", "=", "x_h_comb_p", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "syn_num", ")", "\n", "ind_x_p", "=", "x_p_comb_p", ".", "max", "(", "-", "1", ")", "[", "1", "]", "# shape batch_size* text_len", "\n", "ind_x_h", "=", "x_h_comb_p", ".", "max", "(", "-", "1", ")", "[", "1", "]", "# shape batch_size* text_len", "\n", "adv_text_x_p", "=", "(", "x_p_text_like_syn", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "syn_num", ")", "[", "np", ".", "arange", "(", "batch_size", "*", "text_len", ")", ",", "ind_x_p", "]", ")", ".", "reshape", "(", "batch_size", ",", "text_len", ")", "\n", "adv_text_x_h", "=", "(", "x_h_text_like_syn", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "syn_num", ")", "[", "np", ".", "arange", "(", "batch_size", "*", "text_len", ")", ",", "ind_x_h", "]", ")", ".", "reshape", "(", "batch_size", ",", "text_len", ")", "\n", "return", "adv_text_x_p", ",", "adv_text_x_h", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.get_onehot_from_input": [[291, 299], ["torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "x.reshape", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "", "def", "get_onehot_from_input", "(", "self", ",", "x", ")", ":", "\n", "        ", "embd_voc_dim", ",", "embd_dim", "=", "self", ".", "embedding", ".", "weight", ".", "shape", "\n", "\n", "bs", ",", "text_len", "=", "x", ".", "shape", "\n", "\n", "out", "=", "torch", ".", "zeros", "(", "bs", "*", "text_len", ",", "embd_voc_dim", ")", ".", "to", "(", "torch", ".", "float32", ")", ".", "to", "(", "x", ".", "device", ")", ".", "scatter_", "(", "1", ",", "x", ".", "reshape", "(", "bs", "*", "text_len", ",", "1", ")", ",", "1", ")", "\n", "\n", "return", "out", ".", "reshape", "(", "bs", ",", "text_len", ",", "embd_voc_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.get_onehot_mask_from_syn": [[300, 309], ["torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "syn.reshape", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "get_onehot_mask_from_syn", "(", "self", ",", "syn_valid", ",", "syn", ")", ":", "\n", "        ", "embd_voc_dim", ",", "embd_dim", "=", "self", ".", "embedding", ".", "weight", ".", "shape", "\n", "\n", "bs", ",", "text_len", ",", "syn_max_num", "=", "syn", ".", "shape", "\n", "\n", "out", "=", "torch", ".", "zeros", "(", "bs", "*", "text_len", ",", "embd_voc_dim", ")", ".", "to", "(", "torch", ".", "float32", ")", ".", "to", "(", "syn", ".", "device", ")", ".", "scatter_", "(", "1", ",", "syn", ".", "reshape", "(", "bs", "*", "text_len", ",", "syn_max_num", ")", ",", "1", ")", "\n", "out", "[", ":", ",", "0", "]", "=", "0", "\n", "\n", "return", "out", ".", "reshape", "(", "bs", ",", "text_len", ",", "embd_voc_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.get_embd_from_onehot": [[310, 319], ["torch.mm", "torch.mm", "torch.mm", "torch.mm", "ForSnli.AdvDecAtt.reshape", "onehot_input.reshape", "ForSnli.AdvDecAtt.linear_transform_embd_1"], "methods", ["None"], ["", "def", "get_embd_from_onehot", "(", "self", ",", "onehot_input", ")", ":", "\n", "        ", "w", "=", "self", ".", "embedding", ".", "weight", "\n", "bs", ",", "text_len", ",", "voc_d", "=", "onehot_input", ".", "shape", "\n", "embd", "=", "torch", ".", "mm", "(", "onehot_input", ".", "reshape", "(", "bs", "*", "text_len", ",", "voc_d", ")", ",", "w", ")", "\n", "embd", "=", "embd", ".", "reshape", "(", "bs", ",", "text_len", ",", "-", "1", ")", "\n", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "embd", "=", "self", ".", "linear_transform_embd_1", "(", "embd", ")", "\n", "", "return", "embd", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.get_adv_hotflip": [[320, 383], ["ForSnli.AdvDecAtt.get_onehot_mask_from_syn", "ForSnli.AdvDecAtt.text_to_embd", "ForSnli.AdvDecAtt.embd_to_logit", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "x_p.detach", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "x_h.detach", "range", "ForSnli.AdvDecAtt.eval", "ForSnli.AdvDecAtt.train", "ForSnli.AdvDecAtt.text_to_embd", "embd_h_adv.requires_grad_", "ForSnli.AdvDecAtt.train", "ForSnli.AdvDecAtt.eval", "x_p.detach.detach", "x_h_adv.detach.detach.detach", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "ForSnli.AdvDecAtt.embd_to_logit", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.max", "torch.max", "torch.max", "torch.max", "x_h_adv.detach.detach.detach", "torch.cross_entropy", "torch.cross_entropy", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm.reshape", "torch.mm.reshape", "torch.nn.KLDivLoss", "torch.nn.KLDivLoss", "torch.nn.KLDivLoss.", "grad_embd_h.reshape", "torch.mm().permute", "torch.mm().permute", "torch.mm().permute", "torch.mm().permute", "grad_embd_h.reshape", "ForSnli.AdvDecAtt.embedding.weight.permute", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "ForSnli.AdvDecAtt.linear_transform_embd_1.weight.permute"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_onehot_mask_from_syn", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "get_adv_hotflip", "(", "self", ",", "x_p", ",", "x_h", ",", "y", ",", "x_p_text_like_syn", ",", "x_p_syn_valid", ",", "x_h_text_like_syn", ",", "x_h_syn_valid", ",", "x_p_mask", ",", "x_h_mask", ",", "attack_type_dict", ")", ":", "\n", "\n", "# record context", "\n", "        ", "self_training_context", "=", "self", ".", "training", "\n", "# set context", "\n", "if", "self", ".", "eval_adv_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "\n", "", "device", "=", "x_p", ".", "device", "\n", "# get param of attacks", "\n", "\n", "num_steps", "=", "attack_type_dict", "[", "'num_steps'", "]", "\n", "loss_func", "=", "attack_type_dict", "[", "'loss_func'", "]", "\n", "\n", "batch_size", ",", "text_len", "=", "x_p", ".", "shape", "\n", "\n", "onehot_mask_h", "=", "self", ".", "get_onehot_mask_from_syn", "(", "x_h_syn_valid", ",", "x_h_text_like_syn", ")", "\n", "\n", "embd_p", ",", "embd_h", "=", "self", ".", "text_to_embd", "(", "x_p", ",", "x_h", ")", "\n", "logit", "=", "self", ".", "embd_to_logit", "(", "embd_p", ",", "embd_h", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "\n", "x_p_adv", "=", "torch", ".", "zeros_like", "(", "x_p", ")", "\n", "x_p_adv", "=", "x_p", ".", "detach", "(", ")", "\n", "\n", "x_h_adv", "=", "torch", ".", "zeros_like", "(", "x_h", ")", "\n", "x_h_adv", "=", "x_h", ".", "detach", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "embd_p_adv", ",", "embd_h_adv", "=", "self", ".", "text_to_embd", "(", "x_p_adv", ",", "x_h_adv", ")", "\n", "embd_h_adv", ".", "requires_grad_", "(", ")", "\n", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "\n", "# attack_hypo_only:", "\n", "                ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_p_adv", ",", "embd_h_adv", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "\n", "if", "loss_func", "==", "'ce'", ":", "\n", "                    ", "loss", "=", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "elif", "loss_func", "==", "'kl'", ":", "\n", "                    ", "criterion_kl", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "loss", "=", "criterion_kl", "(", "F", ".", "log_softmax", "(", "logit_adv", ",", "dim", "=", "1", ")", ",", "F", ".", "softmax", "(", "logit", ",", "dim", "=", "1", ")", ")", "\n", "\n", "", "grad_embd_h", "=", "torch", ".", "autograd", ".", "grad", "(", "loss", ",", "[", "embd_h_adv", "]", ")", "[", "0", "]", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "                    ", "grad_onehot_h", "=", "torch", ".", "mm", "(", "grad_embd_h", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "-", "1", ")", ",", "torch", ".", "mm", "(", "self", ".", "embedding", ".", "weight", ",", "self", ".", "linear_transform_embd_1", ".", "weight", ".", "permute", "(", "1", ",", "0", ")", ")", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "", "else", ":", "\n", "                    ", "grad_onehot_h", "=", "torch", ".", "mm", "(", "grad_embd_h", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "-", "1", ")", ",", "self", ".", "embedding", ".", "weight", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "", "grad_onehot_h", "=", "grad_onehot_h", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "-", "1", ")", "*", "(", "onehot_mask_h", ")", "\n", "_", ",", "argmax", "=", "torch", ".", "max", "(", "grad_onehot_h", ",", "-", "1", ")", "\n", "\n", "x_h_adv", "=", "x_h_adv", "*", "(", "argmax", "==", "0", ")", ".", "to", "(", "x_h_adv", ".", "dtype", ")", "+", "argmax", "\n", "x_h_adv", "=", "x_h_adv", ".", "detach", "(", ")", "\n", "\n", "# resume context", "\n", "", "", "if", "self_training_context", "==", "True", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "\n", "", "return", "x_p_adv", ".", "detach", "(", ")", ",", "x_h_adv", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.get_embd_adv": [[388, 475], ["len", "range", "ForSnli.AdvDecAtt.eval", "ForSnli.AdvDecAtt.train", "embd_p_ori.detach", "embd_h_ori.detach", "embd_h_ori.detach.requires_grad_", "ForSnli.AdvDecAtt.train", "ForSnli.AdvDecAtt.eval", "embd_p_ori.detach", "embd_h_ori.detach.detach", "embd_p_ori.detach", "embd_h_ori.detach", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "ForSnli.AdvDecAtt.l2_project_sent", "ForSnli.AdvDecAtt.l2_clip_sent", "embd_h_ori.detach", "ForSnli.AdvDecAtt.detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "ForSnli.AdvDecAtt.embd_to_logit", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "embd_h_ori.detach.detach", "ForSnli.AdvDecAtt.l2_project", "ForSnli.AdvDecAtt.l2_clip", "embd_p_ori.detach", "embd_h_ori.detach.detach", "ForSnli.AdvDecAtt.embd_to_logit", "ForSnli.AdvDecAtt.embd_to_logit", "torch.nn.KLDivLoss", "torch.nn.KLDivLoss", "torch.nn.KLDivLoss.", "ForSnli.AdvDecAtt.detach", "embd_h_ori.detach.detach", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "ForSnli.AdvDecAtt.detach", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_project_sent", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_clip_sent", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_project", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_clip", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "get_embd_adv", "(", "self", ",", "embd_p", ",", "embd_h", ",", "y", ",", "x_p_mask", ",", "x_h_mask", ",", "attack_type_dict", ")", ":", "\n", "\n", "# record context", "\n", "        ", "self_training_context", "=", "self", ".", "training", "\n", "# set context", "\n", "if", "self", ".", "eval_adv_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "\n", "", "device", "=", "embd_p", ".", "device", "\n", "# get param of attacks", "\n", "\n", "num_steps", "=", "attack_type_dict", "[", "'num_steps'", "]", "\n", "step_size", "=", "attack_type_dict", "[", "'step_size'", "]", "\n", "random_start", "=", "attack_type_dict", "[", "'random_start'", "]", "\n", "epsilon", "=", "attack_type_dict", "[", "'epsilon'", "]", "\n", "loss_func", "=", "attack_type_dict", "[", "'loss_func'", "]", "\n", "direction", "=", "attack_type_dict", "[", "'direction'", "]", "\n", "ball_range", "=", "attack_type_dict", "[", "'ball_range'", "]", "\n", "attack_hypo_only", "=", "attack_type_dict", "[", "'attack_hypo_only'", "]", "\n", "assert", "(", "attack_hypo_only", ")", "\n", "\n", "batch_size", "=", "len", "(", "embd_p", ")", "\n", "\n", "embd_p_ori", "=", "embd_p", "\n", "embd_h_ori", "=", "embd_h", "\n", "\n", "# random start", "\n", "if", "random_start", ":", "\n", "            ", "embd_p_adv", "=", "embd_p_ori", ".", "detach", "(", ")", "+", "0.001", "*", "torch", ".", "randn", "(", "embd_p_ori", ".", "shape", ")", ".", "to", "(", "device", ")", ".", "detach", "(", ")", "\n", "embd_h_adv", "=", "embd_h_ori", ".", "detach", "(", ")", "+", "0.001", "*", "torch", ".", "randn", "(", "embd_h_ori", ".", "shape", ")", ".", "to", "(", "device", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "embd_p_adv", "=", "embd_p_ori", ".", "detach", "(", ")", "\n", "embd_h_adv", "=", "embd_h_ori", ".", "detach", "(", ")", "\n", "\n", "", "for", "_", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "embd_h_adv", ".", "requires_grad_", "(", ")", "\n", "grad", "=", "0", "\n", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "if", "loss_func", "==", "'ce'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_p_ori", ",", "embd_h_adv", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "if", "direction", "==", "\"towards\"", ":", "\n", "                        ", "loss", "=", "-", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "elif", "direction", "==", "\"away\"", ":", "\n", "                        ", "loss", "=", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "", "elif", "loss_func", "==", "'kl'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_p_ori", ",", "embd_h_adv", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "logit_ori", "=", "self", ".", "embd_to_logit", "(", "embd_p_ori", ",", "embd_h_ori", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "assert", "(", "direction", "==", "\"away\"", ")", "\n", "\n", "criterion_kl", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "loss", "=", "criterion_kl", "(", "F", ".", "log_softmax", "(", "logit_adv", ",", "dim", "=", "1", ")", ",", "\n", "F", ".", "softmax", "(", "logit_ori", ",", "dim", "=", "1", ")", ")", "\n", "\n", "", "grad", "=", "torch", ".", "autograd", ".", "grad", "(", "loss", ",", "[", "embd_h_adv", "]", ")", "[", "0", "]", "\n", "\n", "", "if", "ball_range", "==", "'sentence'", ":", "\n", "                ", "grad", "=", "self", ".", "l2_project_sent", "(", "grad", ")", "\n", "embd_h_adv", "=", "embd_h_adv", ".", "detach", "(", ")", "+", "step_size", "*", "grad", ".", "detach", "(", ")", "\n", "perturbation", "=", "self", ".", "l2_clip_sent", "(", "embd_h_adv", "-", "embd_h_ori", ",", "epsilon", ")", "\n", "", "elif", "ball_range", "==", "'word'", ":", "\n", "                ", "grad", "=", "self", ".", "l2_project", "(", "grad", ")", "\n", "embd_h_adv", "=", "embd_h_adv", ".", "detach", "(", ")", "+", "step_size", "*", "grad", ".", "detach", "(", ")", "\n", "perturbation", "=", "self", ".", "l2_clip", "(", "embd_h_adv", "-", "embd_h_ori", ",", "epsilon", ")", "\n", "", "else", ":", "\n", "                ", "assert", "NotImplementedError", "\n", "\n", "", "embd_h_adv", "=", "embd_h_ori", ".", "detach", "(", ")", "+", "perturbation", ".", "detach", "(", ")", "\n", "\n", "# resume context", "\n", "", "if", "self_training_context", "==", "True", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "\n", "", "class", "bpda_get_embd_adv", "(", "torch", ".", "autograd", ".", "Function", ")", ":", "\n", "\n", "            ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "embd_p", ",", "embd_h", ")", ":", "\n", "                ", "return", "embd_p_ori", ".", "detach", "(", ")", ",", "embd_h_adv", ".", "detach", "(", ")", "\n", "", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "# BPDA, approximate gradients", "\n", "                ", "return", "grad_output", "\n", "\n", "", "", "return", "embd_p_ori", ".", "detach", "(", ")", ",", "embd_h_adv", ".", "detach", "(", ")", "\n", "#return bpda_get_embd_adv.apply(embd)", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt.forward": [[479, 521], ["ForSnli.AdvDecAtt.get_adv_by_convex_syn", "ForSnli.AdvDecAtt.embd_to_logit", "ForSnli.AdvDecAtt.text_to_embd", "ForSnli.AdvDecAtt.text_to_embd", "ForSnli.AdvDecAtt.embd_to_logit", "ForSnli.AdvDecAtt.text_to_embd", "x_p_text_like_syn_embd.reshape.reshape.reshape", "x_h_text_like_syn_embd.reshape.reshape.reshape", "ForSnli.AdvDecAtt.embd_to_logit", "ForSnli.AdvDecAtt.text_to_embd", "x_h_text_like_syn_embd.reshape.reshape.reshape", "ForSnli.AdvDecAtt.embd_to_logit", "ForSnli.AdvDecAtt.get_embd_adv", "ForSnli.AdvDecAtt.get_adv_hotflip", "x_p.reshape", "x_h.reshape", "x_h.reshape"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_adv_by_convex_syn", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_embd_adv", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.get_adv_hotflip"], ["", "def", "forward", "(", "self", ",", "mode", ",", "x_p", ",", "x_h", ",", "x_p_comb_p", "=", "None", ",", "x_h_comb_p", "=", "None", ",", "label", "=", "None", ",", "x_p_text_like_syn", "=", "None", ",", "x_p_text_like_syn_embd", "=", "None", ",", "x_p_text_like_syn_valid", "=", "None", ",", "x_h_text_like_syn", "=", "None", ",", "x_h_text_like_syn_embd", "=", "None", ",", "x_h_text_like_syn_valid", "=", "None", ",", "x_p_mask", "=", "None", ",", "x_h_mask", "=", "None", ",", "attack_type_dict", "=", "None", ")", ":", "\n", "        ", "if", "mode", "==", "\"get_adv_by_convex_syn\"", ":", "\n", "            ", "assert", "(", "attack_type_dict", "is", "not", "None", ")", "\n", "assert", "(", "x_p_text_like_syn_embd", "is", "not", "None", ")", "\n", "assert", "(", "x_p_text_like_syn_valid", "is", "not", "None", ")", "\n", "assert", "(", "x_h_text_like_syn_embd", "is", "not", "None", ")", "\n", "assert", "(", "x_h_text_like_syn_valid", "is", "not", "None", ")", "\n", "\n", "out", "=", "self", ".", "get_adv_by_convex_syn", "(", "x_p", ",", "x_h", ",", "label", ",", "x_p_text_like_syn", ",", "x_p_text_like_syn_embd", ",", "x_p_text_like_syn_valid", ",", "x_h_text_like_syn", ",", "x_h_text_like_syn_embd", ",", "x_h_text_like_syn_valid", ",", "x_p_mask", ",", "x_h_mask", ",", "attack_type_dict", ")", "\n", "", "if", "mode", "==", "\"embd_to_logit\"", ":", "\n", "            ", "out", "=", "self", ".", "embd_to_logit", "(", "x_p", ",", "x_h", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "", "if", "mode", "==", "\"text_to_embd\"", ":", "\n", "            ", "out", "=", "self", ".", "text_to_embd", "(", "x_p", ",", "x_h", ")", "\n", "", "if", "mode", "==", "\"text_to_logit\"", ":", "\n", "            ", "embd_p", ",", "embd_h", "=", "self", ".", "text_to_embd", "(", "x_p", ",", "x_h", ")", "\n", "out", "=", "self", ".", "embd_to_logit", "(", "embd_p", ",", "embd_h", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "", "if", "mode", "==", "\"text_syn_p_to_logit\"", ":", "\n", "            ", "assert", "(", "x_p_comb_p", "is", "not", "None", ")", "\n", "assert", "(", "x_h_comb_p", "is", "not", "None", ")", "\n", "bs", ",", "tl", ",", "sl", "=", "x_p", ".", "shape", "\n", "x_p_text_like_syn_embd", ",", "x_h_text_like_syn_embd", "=", "self", ".", "text_to_embd", "(", "x_p", ".", "reshape", "(", "bs", ",", "tl", "*", "sl", ")", ",", "x_h", ".", "reshape", "(", "bs", ",", "tl", "*", "sl", ")", ")", "\n", "x_p_text_like_syn_embd", "=", "x_p_text_like_syn_embd", ".", "reshape", "(", "bs", ",", "tl", ",", "sl", ",", "-", "1", ")", "\n", "x_h_text_like_syn_embd", "=", "x_h_text_like_syn_embd", ".", "reshape", "(", "bs", ",", "tl", ",", "sl", ",", "-", "1", ")", "\n", "embd_p", "=", "(", "x_p_comb_p", "*", "x_p_text_like_syn_embd", ")", ".", "sum", "(", "-", "2", ")", "\n", "embd_h", "=", "(", "x_h_comb_p", "*", "x_h_text_like_syn_embd", ")", ".", "sum", "(", "-", "2", ")", "\n", "out", "=", "self", ".", "embd_to_logit", "(", "embd_p", ",", "embd_h", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "", "if", "mode", "==", "\"text_syn_p_to_logit_hypo_only\"", ":", "\n", "            ", "assert", "(", "x_h_comb_p", "is", "not", "None", ")", "\n", "bs", ",", "tl", ",", "sl", "=", "x_h", ".", "shape", "\n", "embd_p", ",", "x_h_text_like_syn_embd", "=", "self", ".", "text_to_embd", "(", "x_p", ",", "x_h", ".", "reshape", "(", "bs", ",", "tl", "*", "sl", ")", ")", "\n", "x_h_text_like_syn_embd", "=", "x_h_text_like_syn_embd", ".", "reshape", "(", "bs", ",", "tl", ",", "sl", ",", "-", "1", ")", "\n", "embd_h", "=", "(", "x_h_comb_p", "*", "x_h_text_like_syn_embd", ")", ".", "sum", "(", "-", "2", ")", "\n", "out", "=", "self", ".", "embd_to_logit", "(", "embd_p", ",", "embd_h", ",", "x_p_mask", ",", "x_h_mask", ")", "\n", "\n", "", "if", "mode", "==", "\"get_embd_adv\"", ":", "\n", "            ", "assert", "(", "attack_type_dict", "is", "not", "None", ")", "\n", "out", "=", "self", ".", "get_embd_adv", "(", "x_p", ",", "x_h", ",", "label", ",", "x_p_mask", ",", "x_h_mask", ",", "attack_type_dict", ")", "\n", "\n", "", "if", "mode", "==", "\"get_adv_hotflip\"", ":", "\n", "            ", "out", "=", "self", ".", "get_adv_hotflip", "(", "x_p", ",", "x_h", ",", "label", ",", "x_p_text_like_syn", ",", "x_p_text_like_syn_valid", ",", "x_h_text_like_syn", ",", "x_h_text_like_syn_valid", ",", "x_p_mask", ",", "x_h_mask", ",", "attack_type_dict", ")", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt_FromCert.__init__": [[525, 552], ["ForSnli.AdvDecAtt.__init__", "ForSnli.AdvDecAtt_FromCert.__init__.get_feedforward_layers"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "AdvDecAtt_FromCert", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "def", "get_feedforward_layers", "(", "num_layers", ",", "input_size", ",", "hidden_size", ",", "output_size", ")", ":", "\n", "            ", "layers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "                ", "layer_in_size", "=", "input_size", "if", "i", "==", "0", "else", "hidden_size", "\n", "layer_out_size", "=", "output_size", "if", "i", "==", "num_layers", "-", "1", "else", "hidden_size", "\n", "layers", ".", "append", "(", "nn", ".", "Dropout", "(", "self", ".", "opt", ".", "keep_dropout", ")", ")", "\n", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "layer_in_size", ",", "layer_out_size", ")", ")", "\n", "if", "i", "<", "num_layers", "-", "1", ":", "\n", "                    ", "layers", ".", "append", "(", "nn", ".", "ReLU", "(", "True", ")", ")", "\n", "", "", "return", "layers", "\n", "\n", "", "num_layers", "=", "2", "\n", "hidden_size", "=", "self", ".", "embedding_out_dim", "\n", "\n", "ff_layers", "=", "get_feedforward_layers", "(", "num_layers", ",", "self", ".", "embedding_out_dim", ",", "hidden_size", ",", "1", ")", "\n", "self", ".", "feedforward", "=", "nn", ".", "Sequential", "(", "*", "ff_layers", ")", "\n", "\n", "compare_layers", "=", "get_feedforward_layers", "(", "num_layers", ",", "2", "*", "self", ".", "embedding_out_dim", ",", "hidden_size", ",", "hidden_size", ")", "\n", "self", ".", "compare_ff", "=", "nn", ".", "Sequential", "(", "*", "compare_layers", ")", "\n", "\n", "output_layers", "=", "get_feedforward_layers", "(", "num_layers", ",", "2", "*", "hidden_size", ",", "hidden_size", ",", "hidden_size", ")", "\n", "output_layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "self", ".", "label_size", ")", ")", "\n", "output_layers", ".", "append", "(", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", ")", "\n", "self", ".", "output_layer", "=", "nn", ".", "Sequential", "(", "*", "output_layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt_FromCert.embd_to_logit": [[553, 585], ["torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "ForSnli.AdvDecAtt_FromCert.attend_on", "ForSnli.AdvDecAtt_FromCert.attend_on", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "ForSnli.AdvDecAtt_FromCert.output_layer", "x_p_mask.unsqueeze", "x_h_mask.unsqueeze", "ForSnli.AdvDecAtt_FromCert.feedforward", "x_p_mask.unsqueeze", "ForSnli.AdvDecAtt_FromCert.feedforward", "x_h_mask.unsqueeze", "hypo_weights.permute", "x_p_mask.unsqueeze", "x_h_mask.unsqueeze", "attention_masked.permute", "ForSnli.AdvDecAtt_FromCert.compare_ff", "x_p_mask.unsqueeze", "ForSnli.AdvDecAtt_FromCert.compare_ff", "x_h_mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt_FromCert.attend_on", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt_FromCert.attend_on"], ["", "def", "embd_to_logit", "(", "self", ",", "embd_p", ",", "embd_h", ",", "x_p_mask", ",", "x_h_mask", ")", ":", "\n", "        ", "\"\"\"\n        Forward pass of DecompAttentionModel.\n        Args:\n        batch: A batch dict from an EntailmentDataset with the following keys:\n            - prem: tensor of word vector indices for premise (B, p, 1)\n            - hypo: tensor of word vector indices for hypothesis (B, h, 1)\n            - prem_mask: binary mask over premise words (1 for real, 0 for pad), size (B, p)\n            - hypo_mask: binary mask over hypothesis words (1 for real, 0 for pad), size (B, h)\n            - prem_lengths: lengths of premises, size (B,)\n            - hypo_lengths: lengths of hypotheses, size (B,)\n        compute_bounds: If True compute the interval bounds and reutrn an IntervalBoundedTensor as logits. Otherwise just use the values\n        cert_eps: float, scaling factor for the interval bounds.\n        \"\"\"", "\n", "\n", "embd_p", "=", "embd_p", "*", "x_p_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "embd_h", "=", "embd_h", "*", "x_h_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "prem_weights", "=", "self", ".", "feedforward", "(", "embd_p", ")", "*", "x_p_mask", ".", "unsqueeze", "(", "-", "1", ")", "# (bXpX1)", "\n", "hypo_weights", "=", "self", ".", "feedforward", "(", "embd_h", ")", "*", "x_h_mask", ".", "unsqueeze", "(", "-", "1", ")", "# (bXhX1)", "\n", "\n", "attention", "=", "torch", ".", "bmm", "(", "prem_weights", ",", "hypo_weights", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "# (bXpX1) X (bX1Xh) => (bXpXh)", "\n", "attention_mask", "=", "x_p_mask", ".", "unsqueeze", "(", "-", "1", ")", "*", "x_h_mask", ".", "unsqueeze", "(", "1", ")", "\n", "attention_masked", "=", "attention", "+", "(", "1", "-", "attention_mask", ")", "*", "-", "1e20", "\n", "attended_prem", "=", "self", ".", "attend_on", "(", "embd_h", ",", "embd_p", ",", "attention_masked", ")", "# (bXpX2e)", "\n", "attended_hypo", "=", "self", ".", "attend_on", "(", "embd_p", ",", "embd_h", ",", "attention_masked", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "# (bXhX2e)", "\n", "compared_prem", "=", "self", ".", "compare_ff", "(", "attended_prem", ")", "*", "x_p_mask", ".", "unsqueeze", "(", "-", "1", ")", "# (bXpXhid)", "\n", "compared_hypo", "=", "self", ".", "compare_ff", "(", "attended_hypo", ")", "*", "x_h_mask", ".", "unsqueeze", "(", "-", "1", ")", "# (bXhXhid)", "\n", "prem_aggregate", "=", "torch", ".", "sum", "(", "compared_prem", ",", "dim", "=", "1", ")", "# (bXhid)", "\n", "hypo_aggregate", "=", "torch", ".", "sum", "(", "compared_hypo", ",", "dim", "=", "1", ")", "# (bXhid)", "\n", "aggregate", "=", "torch", ".", "cat", "(", "[", "prem_aggregate", ",", "hypo_aggregate", "]", ",", "dim", "=", "-", "1", ")", "# (bX2hid)", "\n", "return", "self", ".", "output_layer", "(", "aggregate", ")", "# (b, class_num)", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt_FromCert.attend_on": [[586, 597], ["torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "attend_on", "(", "self", ",", "source", ",", "target", ",", "attention", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n        - source: (bXsXe)\n        - target: (bXtXe)\n        - attention: (bXtXs)\n        \"\"\"", "\n", "attention_logsoftmax", "=", "torch", ".", "log_softmax", "(", "attention", ",", "1", ")", "\n", "attention_normalized", "=", "torch", ".", "exp", "(", "attention_logsoftmax", ")", "\n", "attended_target", "=", "torch", ".", "matmul", "(", "attention_normalized", ",", "source", ")", "# (bXtXe)", "\n", "return", "torch", ".", "cat", "(", "[", "target", ",", "attended_target", "]", ",", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvDecAtt_FromCert.text_to_embd": [[598, 610], ["ForSnli.AdvDecAtt_FromCert.embedding", "ForSnli.AdvDecAtt_FromCert.embedding", "ForSnli.AdvDecAtt_FromCert.linear_transform_embd_1", "ForSnli.AdvDecAtt_FromCert.linear_transform_embd_1", "torch.relu", "torch.relu", "torch.relu", "torch.relu"], "methods", ["None"], ["", "def", "text_to_embd", "(", "self", ",", "x_p", ",", "x_h", ")", ":", "\n", "        ", "embd_x_p", "=", "self", ".", "embedding", "(", "x_p", ")", "\n", "embd_x_h", "=", "self", ".", "embedding", "(", "x_h", ")", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "embd_x_p", "=", "self", ".", "linear_transform_embd_1", "(", "embd_x_p", ")", "\n", "embd_x_h", "=", "self", ".", "linear_transform_embd_1", "(", "embd_x_h", ")", "\n", "embd_x_p", "=", "F", ".", "relu", "(", "embd_x_p", ")", "\n", "embd_x_h", "=", "F", ".", "relu", "(", "embd_x_h", ")", "\n", "#x = self.linear_transform_embd_2(x)", "\n", "#x = F.relu(x)", "\n", "#x = self.linear_transform_embd_3(x)", "\n", "", "return", "embd_x_p", ",", "embd_x_h", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvBOW.__init__": [[613, 626], ["ForSnli.AdvDecAtt.__init__", "torch.nn.Dropout", "torch.nn.Dropout", "range", "layers.append", "layers.append", "torch.nn.Sequential", "torch.nn.Sequential", "layers.append", "layers.append", "layers.append", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "AdvBOW", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "sum_drop", "=", "nn", ".", "Dropout", "(", "self", ".", "opt", ".", "keep_dropout", ")", "\n", "layers", "=", "[", "]", "\n", "hidden_size", "=", "self", ".", "embedding_out_dim", "\n", "num_layers", "=", "3", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "2", "*", "hidden_size", ",", "2", "*", "hidden_size", ")", ")", "\n", "layers", ".", "append", "(", "nn", ".", "ReLU", "(", "True", ")", ")", "\n", "layers", ".", "append", "(", "nn", ".", "Dropout", "(", "self", ".", "opt", ".", "keep_dropout", ")", ")", "\n", "", "layers", ".", "append", "(", "nn", ".", "Linear", "(", "2", "*", "hidden_size", ",", "self", ".", "label_size", ")", ")", "\n", "layers", ".", "append", "(", "nn", ".", "LogSoftmax", "(", "dim", "=", "-", "1", ")", ")", "# why?", "\n", "self", ".", "layers", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvBOW.embd_to_logit": [[627, 639], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "ForSnli.AdvBOW.layers", "x_p_mask.unsqueeze().sum", "x_h_mask.unsqueeze().sum", "x_p_mask.unsqueeze", "x_h_mask.unsqueeze", "x_p_mask.unsqueeze", "x_h_mask.unsqueeze", "x_p_mask.unsqueeze", "x_h_mask.unsqueeze"], "methods", ["None"], ["", "def", "embd_to_logit", "(", "self", ",", "embd_p", ",", "embd_h", ",", "x_p_mask", ",", "x_h_mask", ")", ":", "\n", "\n", "        ", "if", "self", ".", "opt", ".", "bow_mean", ":", "\n", "            ", "embd_p", "=", "(", "embd_p", "*", "x_p_mask", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "sum", "(", "-", "2", ")", "/", "(", "x_p_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "sum", "(", "-", "2", ")", ")", "\n", "embd_h", "=", "(", "embd_h", "*", "x_h_mask", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "sum", "(", "-", "2", ")", "/", "(", "x_h_mask", ".", "unsqueeze", "(", "-", "1", ")", ".", "sum", "(", "-", "2", ")", ")", "\n", "", "else", ":", "\n", "            ", "embd_p", "=", "(", "embd_p", "*", "x_p_mask", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "sum", "(", "-", "2", ")", "\n", "embd_h", "=", "(", "embd_h", "*", "x_h_mask", ".", "unsqueeze", "(", "-", "1", ")", ")", ".", "sum", "(", "-", "2", ")", "\n", "\n", "", "input_encoded", "=", "torch", ".", "cat", "(", "[", "embd_p", ",", "embd_h", "]", ",", "-", "1", ")", "\n", "logits", "=", "self", ".", "layers", "(", "input_encoded", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvBOW.text_to_embd": [[640, 652], ["ForSnli.AdvBOW.embedding", "ForSnli.AdvBOW.embedding", "ForSnli.AdvBOW.linear_transform_embd_1", "ForSnli.AdvBOW.linear_transform_embd_1", "torch.relu", "torch.relu", "torch.relu", "torch.relu"], "methods", ["None"], ["", "def", "text_to_embd", "(", "self", ",", "x_p", ",", "x_h", ")", ":", "\n", "        ", "embd_x_p", "=", "self", ".", "embedding", "(", "x_p", ")", "\n", "embd_x_h", "=", "self", ".", "embedding", "(", "x_h", ")", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "embd_x_p", "=", "self", ".", "linear_transform_embd_1", "(", "embd_x_p", ")", "\n", "embd_x_h", "=", "self", ".", "linear_transform_embd_1", "(", "embd_x_h", ")", "\n", "embd_x_p", "=", "F", ".", "relu", "(", "embd_x_p", ")", "\n", "embd_x_h", "=", "F", ".", "relu", "(", "embd_x_h", ")", "\n", "#x = self.linear_transform_embd_2(x)", "\n", "#x = F.relu(x)", "\n", "#x = self.linear_transform_embd_3(x)", "\n", "", "return", "embd_x_p", ",", "embd_x_h", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.__init__": [[655, 687], ["ForSnli.AdvDecAtt.__init__", "opt.__dict__.get", "opt.__dict__.get", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "ForSnli.AdvEntailmentCNN.properties.update", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "AdvEntailmentCNN", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "content_dim", "=", "opt", ".", "__dict__", ".", "get", "(", "\"content_dim\"", ",", "256", ")", "\n", "self", ".", "kernel_size", "=", "opt", ".", "__dict__", ".", "get", "(", "\"kernel_size\"", ",", "3", ")", "\n", "\n", "self", ".", "content_p_conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "self", ".", "embedding_out_dim", ",", "\n", "out_channels", "=", "self", ".", "content_dim", ",", "#256", "\n", "kernel_size", "=", "self", ".", "kernel_size", ")", ",", "#3", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "(", "opt", ".", "max_seq_len", "-", "self", ".", "kernel_size", "+", "1", ")", ")", ",", "\n", "#nn.MeanPool1d(kernel_size = (opt.max_seq_len - self.kernel_size + 1))", "\n", "#            nn.AdaptiveMaxPool1d()", "\n", ")", "\n", "\n", "self", ".", "content_h_conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "in_channels", "=", "self", ".", "embedding_out_dim", ",", "\n", "out_channels", "=", "self", ".", "content_dim", ",", "#256", "\n", "kernel_size", "=", "self", ".", "kernel_size", ")", ",", "#3", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "(", "opt", ".", "max_seq_len", "-", "self", ".", "kernel_size", "+", "1", ")", ")", ",", "\n", "#nn.MeanPool1d(kernel_size = (opt.max_seq_len - self.kernel_size + 1))", "\n", "#            nn.AdaptiveMaxPool1d()", "\n", ")", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"content_dim\"", ":", "self", ".", "content_dim", ",", "\n", "\"kernel_size\"", ":", "self", ".", "kernel_size", ",", "\n", "}", ")", "\n", "self", ".", "dropout_p", "=", "nn", ".", "Dropout", "(", "opt", ".", "keep_dropout", ")", "\n", "self", ".", "dropout_h", "=", "nn", ".", "Dropout", "(", "opt", ".", "keep_dropout", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "self", ".", "content_dim", "*", "2", ",", "opt", ".", "label_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.text_to_embd": [[688, 700], ["ForSnli.AdvEntailmentCNN.embedding", "ForSnli.AdvEntailmentCNN.embedding", "ForSnli.AdvEntailmentCNN.linear_transform_embd_1", "ForSnli.AdvEntailmentCNN.linear_transform_embd_1", "torch.relu", "torch.relu", "torch.relu", "torch.relu"], "methods", ["None"], ["", "def", "text_to_embd", "(", "self", ",", "x_p", ",", "x_h", ")", ":", "\n", "        ", "embd_x_p", "=", "self", ".", "embedding", "(", "x_p", ")", "\n", "embd_x_h", "=", "self", ".", "embedding", "(", "x_h", ")", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "embd_x_p", "=", "self", ".", "linear_transform_embd_1", "(", "embd_x_p", ")", "\n", "embd_x_h", "=", "self", ".", "linear_transform_embd_1", "(", "embd_x_h", ")", "\n", "embd_x_p", "=", "F", ".", "relu", "(", "embd_x_p", ")", "\n", "embd_x_h", "=", "F", ".", "relu", "(", "embd_x_h", ")", "\n", "#x = self.linear_transform_embd_2(x)", "\n", "#x = F.relu(x)", "\n", "#x = self.linear_transform_embd_3(x)", "\n", "", "return", "embd_x_p", ",", "embd_x_h", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit": [[702, 713], ["ForSnli.AdvEntailmentCNN.content_p_conv", "ForSnli.AdvEntailmentCNN.view", "ForSnli.AdvEntailmentCNN.dropout_p", "ForSnli.AdvEntailmentCNN.content_h_conv", "ForSnli.AdvEntailmentCNN.view", "ForSnli.AdvEntailmentCNN.dropout_h", "ForSnli.AdvEntailmentCNN.fc", "embd_p.permute", "ForSnli.AdvEntailmentCNN.size", "embd_h.permute", "ForSnli.AdvEntailmentCNN.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "embd_to_logit", "(", "self", ",", "embd_p", ",", "embd_h", ",", "x_p_mask", ",", "x_h_mask", ")", ":", "\n", "        ", "content_p", "=", "self", ".", "content_p_conv", "(", "embd_p", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "#64x256x1", "\n", "reshaped_p", "=", "content_p", ".", "view", "(", "content_p", ".", "size", "(", "0", ")", ",", "-", "1", ")", "#64x256", "\n", "reshaped_p", "=", "self", ".", "dropout_p", "(", "reshaped_p", ")", "\n", "\n", "content_h", "=", "self", ".", "content_h_conv", "(", "embd_h", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "#64x256x1", "\n", "reshaped_h", "=", "content_h", ".", "view", "(", "content_h", ".", "size", "(", "0", ")", ",", "-", "1", ")", "#64x256", "\n", "reshaped_h", "=", "self", ".", "dropout_h", "(", "reshaped_h", ")", "\n", "\n", "logits", "=", "self", ".", "fc", "(", "torch", ".", "cat", "(", "[", "reshaped_p", ",", "reshaped_h", "]", ",", "-", "1", ")", ")", "#64x3", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.parse_opt": [[716, 752], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_opt", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "# Data input settings", "\n", "parser", ".", "add_argument", "(", "'--hidden_dim'", ",", "type", "=", "int", ",", "default", "=", "128", ",", "\n", "help", "=", "'hidden_dim'", ")", "\n", "\n", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "\n", "help", "=", "'batch_size'", ")", "\n", "parser", ".", "add_argument", "(", "'--embedding_dim'", ",", "type", "=", "int", ",", "default", "=", "300", ",", "\n", "help", "=", "'embedding_dim'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "4e-4", ",", "\n", "help", "=", "'learning_rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--grad_clip'", ",", "type", "=", "float", ",", "default", "=", "1e-1", ",", "\n", "help", "=", "'grad_clip'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "\"lstm\"", ",", "\n", "help", "=", "'model name'", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "\"lstm\"", ",", "\n", "help", "=", "'model name'", ")", "\n", "\n", "\n", "#", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "args", ".", "embedding_dim", "=", "300", "\n", "args", ".", "vocab_size", "=", "10000", "\n", "args", ".", "kernel_size", "=", "3", "\n", "args", ".", "num_classes", "=", "3", "\n", "args", ".", "content_dim", "=", "256", "\n", "args", ".", "max_seq_len", "=", "50", "\n", "\n", "#", "\n", "#    # Check if args are valid", "\n", "#    assert args.rnn_size > 0, \"rnn_size should be greater than 0\"", "\n", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTM.LSTMClassifier.__init__": [[11, 25], ["models.BaseModel.BaseModel.__init__", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "LSTM.LSTMClassifier.init_hidden", "opt.__dict__.get"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "\n", "        ", "super", "(", "LSTMClassifier", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "hidden_dim", "=", "opt", ".", "hidden_dim", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "use_gpu", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "opt", ".", "embedding_dim", ",", "opt", ".", "hidden_dim", ")", "\n", "self", ".", "hidden2label", "=", "nn", ".", "Linear", "(", "opt", ".", "hidden_dim", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", ")", "\n", "self", ".", "lsmt_reduce_by_mean", "=", "opt", ".", "__dict__", ".", "get", "(", "\"lstm_mean\"", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTM.LSTMClassifier.init_hidden": [[26, 37], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "        ", "if", "batch_size", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "if", "self", ".", "use_gpu", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "cuda", "(", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "cuda", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ")", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "#    @profile", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTM.LSTMClassifier.forward": [[38, 52], ["LSTM.LSTMClassifier.word_embeddings", "LSTM.LSTMClassifier.permute", "LSTM.LSTMClassifier.init_hidden", "LSTM.LSTMClassifier.lstm", "LSTM.LSTMClassifier.hidden2label", "lstm_out.permute", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "sentence.size"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.lstm"], ["", "def", "forward", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "embeds", "=", "self", ".", "word_embeddings", "(", "sentence", ")", "#64x200x300", "\n", "\n", "#        x = embeds.view(sentence.size()[1], self.batch_size, -1)", "\n", "x", "=", "embeds", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "#200x64x300", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", "sentence", ".", "size", "(", ")", "[", "0", "]", ")", "#1x64x128", "\n", "lstm_out", ",", "self", ".", "hidden", "=", "self", ".", "lstm", "(", "x", ",", "self", ".", "hidden", ")", "#200x64x128", "\n", "if", "self", ".", "lsmt_reduce_by_mean", "==", "\"mean\"", ":", "\n", "            ", "out", "=", "lstm_out", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "final", "=", "torch", ".", "mean", "(", "out", ",", "1", ")", "\n", "", "else", ":", "\n", "            ", "final", "=", "lstm_out", "[", "-", "1", "]", "\n", "", "y", "=", "self", ".", "hidden2label", "(", "final", ")", "#64x3", "\n", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.__init__": [[93, 150], ["BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.Linear", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.Parameter", "torch.Parameter", "torch.Parameter", "BaseModelAdv.AdvBaseModel.normalize_embedding"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.normalize_embedding"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "is_bert", "=", "False", ")", ":", "\n", "        ", "super", "(", "AdvBaseModel", ",", "self", ")", ".", "__init__", "(", "opt", ",", "is_bert", "=", "is_bert", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "if", "is_bert", ":", "\n", "            ", "return", "\n", "\n", "", "\"\"\"\n        # inverse embedding\n        print(\"making inverse embedding\")\n        \n        #inverse_embedding_weight = self.embedding.weight.detach().cpu()\n        #inverse_embedding_weight = inverse_embedding_weight.numpy()\n        #inverse_embedding_weight = np.matrix(inverse_embedding_weight)\n        #inverse_embedding_weight = np.linalg.pinv(inverse_embedding_weight)\n        #inverse_embedding_weight = torch.FloatTensor(inverse_embedding_weight)\n        \n        #self.inverse_embedding = nn.Embedding(self.embedding_dim, self.vocab_size + 2, ) \n        self.inverse_embedding = nn.Linear( self.vocab_size + 2,self.embedding_dim, bias=False)\n        \n        #self.inverse_embedding.weight=nn.Parameter(inverse_embedding_weight)            \n        #self.inverse_embedding.weight.requires_grad = False\n        \n        \n        # embd to embdnew\n        self.new_embedding_dim = self.embedding_dim\n        self.linear_transform_embd = nn.Linear(self.embedding_dim, self.new_embedding_dim)\n\n        # embdnew to embd\n        self.inverse_linear_transform_embd = nn.Linear(self.new_embedding_dim, self.embedding_dim)\n        #self.update_linear_transform_embd()\n        \"\"\"", "\n", "self", ".", "embedding_dim", "=", "opt", ".", "embedding_dim", "\n", "self", ".", "vocab_size", "=", "opt", ".", "vocab_size", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", "+", "2", ",", "opt", ".", "embedding_dim", ")", "\n", "if", "opt", ".", "use_pretrained_embeddings", ":", "\n", "            ", "self", ".", "embedding", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ")", "\n", "", "self", ".", "embedding", ".", "weight", ".", "requires_grad", "=", "True", "\n", "\n", "if", "self", ".", "opt", ".", "normalize_embedding", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "self", ".", "embedding", ".", "weight", "=", "nn", ".", "Parameter", "(", "self", ".", "normalize_embedding", "(", "self", ".", "embedding", ".", "weight", ",", "self", ".", "opt", ".", "vocab_freq", ")", ")", "\n", "\n", "", "", "self", ".", "label_size", "=", "opt", ".", "label_size", "\n", "\n", "self", ".", "embedding_out_dim", "=", "self", ".", "embedding_dim", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "self", ".", "embedding_out_dim", "=", "opt", ".", "embedding_out_dim", "\n", "self", ".", "linear_transform_embd_1", "=", "nn", ".", "Linear", "(", "self", ".", "embedding_dim", ",", "self", ".", "embedding_out_dim", ")", "\n", "#self.linear_transform_embd_2 = nn.Linear(self.latent_dim, self.latent_dim)", "\n", "#self.linear_transform_embd_3 = nn.Linear(self.latent_dim, self.embedding_dim)", "\n", "\n", "# l2 radius ", "\n", "", "self", ".", "word_synonym_radius", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", "+", "2", ",", "1", ")", "#, padding_idx=self.vocab_size + 1", "\n", "self", ".", "word_synonym_radius", ".", "weight", ".", "requires_grad", "=", "False", "\n", "self", ".", "word_synonym_radius", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros_like", "(", "self", ".", "word_synonym_radius", ".", "weight", ")", ")", "\n", "\n", "self", ".", "eval_adv_mode", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.normalize_embedding": [[151, 158], ["torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sum().unsqueeze", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "vocab_freq.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "p.unsqueeze", "p.unsqueeze", "p.unsqueeze", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow", "torch.pow"], "methods", ["None"], ["", "def", "normalize_embedding", "(", "self", ",", "weight", ",", "vocab_freq", ")", ":", "\n", "# input torch tensor", "\n", "        ", "p", "=", "vocab_freq", "/", "vocab_freq", ".", "sum", "(", ")", "\n", "mean", "=", "torch", ".", "sum", "(", "p", ".", "unsqueeze", "(", "1", ")", "*", "weight", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "var", "=", "torch", ".", "sum", "(", "p", ".", "unsqueeze", "(", "1", ")", "*", "torch", ".", "pow", "(", "weight", "-", "mean", ",", "2", ")", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "0", ")", "\n", "stddev", "=", "torch", ".", "sqrt", "(", "var", "+", "1e-6", ")", "\n", "return", "p", ".", "unsqueeze", "(", "1", ")", "*", "(", "weight", "-", "mean", ")", "/", "stddev", "# if p==0, use zero vector", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.update_linear_transform_embd": [[159, 172], ["BaseModelAdv.AdvBaseModel.inverse_linear_transform_embd.weight.detach().cpu", "BaseModelAdv.AdvBaseModel.inverse_linear_transform_embd.bias.detach().cpu", "torch.FloatTensor.numpy", "torch.FloatTensor.numpy", "torch.FloatTensor.numpy", "numpy.matrix", "numpy.matrix", "numpy.linalg.pinv", "numpy.linalg.pinv", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "bias.squeeze.squeeze.squeeze", "torch.Parameter", "torch.Parameter", "torch.Parameter", "bias.squeeze.squeeze.reshape().mm", "BaseModelAdv.AdvBaseModel.inverse_linear_transform_embd.weight.detach", "BaseModelAdv.AdvBaseModel.inverse_linear_transform_embd.bias.detach", "torch.FloatTensor.transpose", "torch.FloatTensor.transpose", "torch.FloatTensor.transpose", "bias.squeeze.squeeze.reshape"], "methods", ["None"], ["", "def", "update_linear_transform_embd", "(", "self", ")", ":", "\n", "        ", "weight", "=", "self", ".", "inverse_linear_transform_embd", ".", "weight", ".", "detach", "(", ")", ".", "cpu", "(", ")", "# W x +b ", "\n", "bias", "=", "self", ".", "inverse_linear_transform_embd", ".", "bias", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "\n", "weight", "=", "weight", ".", "numpy", "(", ")", "\n", "weight", "=", "np", ".", "matrix", "(", "weight", ")", "\n", "weight", "=", "np", ".", "linalg", ".", "pinv", "(", "weight", ")", "\n", "weight", "=", "torch", ".", "FloatTensor", "(", "weight", ")", "\n", "\n", "self", ".", "linear_transform_embd", ".", "weight", "=", "nn", ".", "Parameter", "(", "weight", ")", "\n", "bias", "=", "-", "bias", ".", "reshape", "(", "1", ",", "-", "1", ")", ".", "mm", "(", "weight", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "bias", "=", "bias", ".", "squeeze", "(", ")", "\n", "self", ".", "linear_transform_embd", ".", "bias", "=", "nn", ".", "Parameter", "(", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.update_inverse_linear_transform_embd": [[173, 186], ["BaseModelAdv.AdvBaseModel.linear_transform_embd.weight.detach().cpu", "BaseModelAdv.AdvBaseModel.linear_transform_embd.bias.detach().cpu", "torch.FloatTensor.numpy", "torch.FloatTensor.numpy", "torch.FloatTensor.numpy", "numpy.matrix", "numpy.matrix", "numpy.linalg.pinv", "numpy.linalg.pinv", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.Parameter", "torch.Parameter", "torch.Parameter", "bias.squeeze.squeeze.squeeze", "torch.Parameter", "torch.Parameter", "torch.Parameter", "bias.squeeze.squeeze.reshape().mm", "BaseModelAdv.AdvBaseModel.linear_transform_embd.weight.detach", "BaseModelAdv.AdvBaseModel.linear_transform_embd.bias.detach", "torch.FloatTensor.transpose", "torch.FloatTensor.transpose", "torch.FloatTensor.transpose", "bias.squeeze.squeeze.reshape"], "methods", ["None"], ["", "def", "update_inverse_linear_transform_embd", "(", "self", ")", ":", "\n", "        ", "weight", "=", "self", ".", "linear_transform_embd", ".", "weight", ".", "detach", "(", ")", ".", "cpu", "(", ")", "# W x +b ", "\n", "bias", "=", "self", ".", "linear_transform_embd", ".", "bias", ".", "detach", "(", ")", ".", "cpu", "(", ")", "\n", "\n", "weight", "=", "weight", ".", "numpy", "(", ")", "\n", "weight", "=", "np", ".", "matrix", "(", "weight", ")", "\n", "weight", "=", "np", ".", "linalg", ".", "pinv", "(", "weight", ")", "\n", "weight", "=", "torch", ".", "FloatTensor", "(", "weight", ")", "\n", "\n", "self", ".", "inverse_linear_transform_embd", ".", "weight", "=", "nn", ".", "Parameter", "(", "weight", ")", "\n", "bias", "=", "-", "bias", ".", "reshape", "(", "1", ",", "-", "1", ")", ".", "mm", "(", "weight", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "bias", "=", "bias", ".", "squeeze", "(", ")", "\n", "self", ".", "inverse_linear_transform_embd", ".", "bias", "=", "nn", ".", "Parameter", "(", "bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.loss_radius": [[188, 201], ["BaseModelAdv.AdvBaseModel.text_to_embd", "BaseModelAdv.AdvBaseModel.text_to_embd", "torch.nn.functional.pairwise_distance", "torch.nn.functional.pairwise_distance", "torch.nn.functional.pairwise_distance", "torch.nn.functional.pairwise_distance", "torch.nn.functional.pairwise_distance", "torch.nn.functional.pairwise_distance", "torch.nn.functional.pairwise_distance", "torch.nn.functional.pairwise_distance", "torch.nn.functional.pairwise_distance", "torch.nn.functional.pairwise_distance.reshape().max", "torch.nn.functional.pairwise_distance.reshape().max", "torch.nn.functional.pairwise_distance.reshape().max", "range", "print", "roots.reshape", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "max_radius.mean", "torch.nn.functional.pairwise_distance.reshape", "torch.nn.functional.pairwise_distance.reshape", "torch.nn.functional.pairwise_distance.reshape"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd"], ["", "def", "loss_radius", "(", "self", ",", "roots", ",", "synonyms", ")", ":", "\n", "        ", "embd_roots", "=", "self", ".", "text_to_embd", "(", "roots", ".", "reshape", "(", "-", "1", ",", "1", ")", ")", "\n", "embd_synonyms", "=", "self", ".", "text_to_embd", "(", "synonyms", ")", "\n", "n", ",", "len_syn_set", ",", "embd_dim", "=", "embd_synonyms", ".", "shape", "\n", "delta", "=", "(", "embd_synonyms", "-", "embd_roots", ")", ".", "reshape", "(", "-", "1", ",", "embd_dim", ")", "\n", "\n", "radius", "=", "torch", ".", "nn", ".", "functional", ".", "pairwise_distance", "(", "delta", ",", "torch", ".", "zeros_like", "(", "delta", ")", ",", "p", "=", "2.0", ")", "# n*len_syn_set,", "\n", "max_radius", ",", "_", "=", "radius", ".", "reshape", "(", "n", ",", "len_syn_set", ")", ".", "max", "(", "-", "1", ")", "\n", "\n", "for", "i", "in", "range", "(", "n", ")", ":", "\n", "            ", "self", ".", "word_synonym_radius", ".", "weight", "[", "roots", "[", "i", "]", ",", "0", "]", "=", "max_radius", "[", "i", "]", "\n", "\n", "", "print", "(", "max_radius", ".", "mean", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_project": [[205, 218], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "l2_project", "(", "self", ",", "grad", ")", ":", "\n", "\n", "        ", "N", ",", "L", ",", "E", "=", "grad", ".", "shape", "\n", "\n", "torch_small_constant", "=", "1e-12", "*", "torch", ".", "ones", "(", "N", ",", "L", ",", "1", ")", ".", "to", "(", "grad", ".", "dtype", ")", ".", "to", "(", "grad", ".", "device", ")", "\n", "\n", "grad_norm", "=", "grad", "*", "grad", "\n", "grad_norm", "=", "torch", ".", "sum", "(", "grad_norm", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "grad_norm", "=", "torch", ".", "sqrt", "(", "grad_norm", ")", "\n", "grad_norm", "=", "torch", ".", "max", "(", "torch_small_constant", ",", "grad_norm", ")", "\n", "grad", "=", "grad", "/", "grad_norm", "\n", "\n", "return", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_project_sent": [[219, 232], ["torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "l2_project_sent", "(", "self", ",", "grad", ")", ":", "\n", "\n", "        ", "N", ",", "L", ",", "E", "=", "grad", ".", "shape", "\n", "\n", "torch_small_constant", "=", "1e-12", "*", "torch", ".", "ones", "(", "N", ",", "1", ",", "1", ")", ".", "to", "(", "grad", ".", "dtype", ")", ".", "to", "(", "grad", ".", "device", ")", "\n", "\n", "grad_norm", "=", "grad", "*", "grad", "\n", "grad_norm", "=", "torch", ".", "sum", "(", "grad_norm", ",", "dim", "=", "(", "1", ",", "2", ")", ",", "keepdim", "=", "True", ")", "\n", "grad_norm", "=", "torch", ".", "sqrt", "(", "grad_norm", ")", "\n", "grad_norm", "=", "torch", ".", "max", "(", "torch_small_constant", ",", "grad_norm", ")", "\n", "grad", "=", "grad", "/", "grad_norm", "\n", "\n", "return", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_clip": [[233, 248], ["torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "l2_clip", "(", "self", ",", "pert", ",", "eps", ")", ":", "\n", "\n", "        ", "N", ",", "L", ",", "E", "=", "pert", ".", "shape", "\n", "torch_small_constant", "=", "1e-12", "*", "torch", ".", "ones", "(", "N", ",", "L", ",", "1", ")", ".", "to", "(", "pert", ".", "dtype", ")", ".", "to", "(", "pert", ".", "device", ")", "\n", "\n", "torch_ones", "=", "torch", ".", "ones_like", "(", "pert", ")", "\n", "\n", "pert_norm", "=", "pert", "*", "pert", "\n", "pert_norm", "=", "torch", ".", "sum", "(", "pert_norm", ",", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "pert_norm", "=", "torch", ".", "sqrt", "(", "pert_norm", ")", "\n", "pert_norm", "=", "torch", ".", "max", "(", "torch_small_constant", ",", "pert_norm", ")", "\n", "ratio", "=", "eps", "/", "pert_norm", "\n", "ratio", "=", "torch", ".", "min", "(", "torch_ones", ",", "ratio", ")", "\n", "\n", "return", "pert", "*", "ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_clip_sent": [[249, 264], ["torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "l2_clip_sent", "(", "self", ",", "pert", ",", "eps", ")", ":", "\n", "\n", "        ", "N", ",", "L", ",", "E", "=", "pert", ".", "shape", "\n", "torch_small_constant", "=", "1e-12", "*", "torch", ".", "ones", "(", "N", ",", "1", ",", "1", ")", ".", "to", "(", "pert", ".", "dtype", ")", ".", "to", "(", "pert", ".", "device", ")", "\n", "\n", "torch_ones", "=", "torch", ".", "ones_like", "(", "pert", ")", "\n", "\n", "pert_norm", "=", "pert", "*", "pert", "\n", "pert_norm", "=", "torch", ".", "sum", "(", "pert_norm", ",", "dim", "=", "(", "1", ",", "2", ")", ",", "keepdim", "=", "True", ")", "\n", "pert_norm", "=", "torch", ".", "sqrt", "(", "pert_norm", ")", "\n", "pert_norm", "=", "torch", ".", "max", "(", "torch_small_constant", ",", "pert_norm", ")", "\n", "ratio", "=", "eps", "/", "pert_norm", "\n", "ratio", "=", "torch", ".", "min", "(", "torch_ones", ",", "ratio", ")", "\n", "\n", "return", "pert", "*", "ratio", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.get_adv_by_convex_syn": [[265, 394], ["torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.empty().to().to.requires_grad_", "torch.empty().to().to.requires_grad_", "torch.empty().to().to.requires_grad_", "utils.getOptimizer", "embd.detach", "range", "BaseModelAdv.AdvBaseModel.get_adv_by_convex_syn.get_comb_p"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.getOptimizer"], ["", "def", "get_adv_by_convex_syn", "(", "self", ",", "embd", ",", "y", ",", "syn", ",", "syn_valid", ",", "text_like_syn", ",", "attack_type_dict", ",", "text_for_vis", ",", "record_for_vis", ")", ":", "\n", "\n", "# record context", "\n", "        ", "self_training_context", "=", "self", ".", "training", "\n", "# set context", "\n", "if", "self", ".", "eval_adv_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "\n", "", "device", "=", "embd", ".", "device", "\n", "# get param of attacks", "\n", "\n", "num_steps", "=", "attack_type_dict", "[", "'num_steps'", "]", "\n", "loss_func", "=", "attack_type_dict", "[", "'loss_func'", "]", "\n", "w_optm_lr", "=", "attack_type_dict", "[", "'w_optm_lr'", "]", "\n", "sparse_weight", "=", "attack_type_dict", "[", "'sparse_weight'", "]", "\n", "out_type", "=", "attack_type_dict", "[", "'out_type'", "]", "\n", "\n", "batch_size", ",", "text_len", ",", "embd_dim", "=", "embd", ".", "shape", "\n", "batch_size", ",", "text_len", ",", "syn_num", ",", "embd_dim", "=", "syn", ".", "shape", "\n", "\n", "w", "=", "torch", ".", "empty", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", ".", "to", "(", "device", ")", ".", "to", "(", "embd", ".", "dtype", ")", "\n", "#ww = torch.zeros(batch_size, text_len, syn_num, 1).to(device).to(embd.dtype)", "\n", "#ww = ww+500*(syn_valid.reshape(batch_size, text_len, syn_num, 1)-1)", "\n", "nn", ".", "init", ".", "kaiming_normal_", "(", "w", ")", "\n", "w", ".", "requires_grad_", "(", ")", "\n", "\n", "import", "utils", "\n", "params", "=", "[", "w", "]", "\n", "optimizer", "=", "utils", ".", "getOptimizer", "(", "params", ",", "name", "=", "'adam'", ",", "lr", "=", "w_optm_lr", ",", "weight_decay", "=", "2e-5", ")", "\n", "\n", "def", "get_comb_p", "(", "w", ",", "syn_valid", ")", ":", "\n", "            ", "ww", "=", "w", "*", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "+", "500", "*", "(", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "-", "1", ")", "\n", "return", "F", ".", "softmax", "(", "ww", ",", "-", "2", ")", "\n", "\n", "", "def", "get_comb_ww", "(", "w", ",", "syn_valid", ")", ":", "\n", "            ", "ww", "=", "w", "*", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "+", "500", "*", "(", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "-", "1", ")", "\n", "return", "ww", "\n", "\n", "", "def", "get_comb", "(", "p", ",", "syn", ")", ":", "\n", "            ", "return", "(", "p", "*", "syn", ".", "detach", "(", ")", ")", ".", "sum", "(", "-", "2", ")", "\n", "\n", "\n", "", "embd_ori", "=", "embd", ".", "detach", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "logit_ori", "=", "self", ".", "embd_to_logit", "(", "embd_ori", ")", "\n", "\n", "", "for", "_", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "ww", "=", "get_comb_ww", "(", "w", ",", "syn_valid", ")", "\n", "#comb_p = get_comb_p(w, syn_valid)", "\n", "embd_adv", "=", "get_comb", "(", "F", ".", "softmax", "(", "ww", ",", "-", "2", ")", ",", "syn", ")", "\n", "if", "loss_func", "==", "'ce'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_adv", ")", "\n", "loss", "=", "-", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "elif", "loss_func", "==", "'kl'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_adv", ")", "\n", "criterion_kl", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "loss", "=", "-", "criterion_kl", "(", "F", ".", "log_softmax", "(", "logit_adv", ",", "dim", "=", "1", ")", ",", "\n", "F", ".", "softmax", "(", "logit_ori", ".", "detach", "(", ")", ",", "dim", "=", "1", ")", ")", "\n", "\n", "#print(\"ad loss:\", loss.data.item())", "\n", "\n", "", "if", "sparse_weight", "!=", "0", ":", "\n", "#loss_sparse = (comb_p*comb_p).mean()", "\n", "                    ", "loss_sparse", "=", "(", "-", "F", ".", "softmax", "(", "ww", ",", "-", "2", ")", "*", "F", ".", "log_softmax", "(", "ww", ",", "-", "2", ")", ")", ".", "sum", "(", "-", "2", ")", ".", "mean", "(", ")", "\n", "#loss -= sparse_weight*loss_sparse", "\n", "\n", "loss", "=", "loss", "+", "sparse_weight", "*", "loss_sparse", "\n", "#print(loss_sparse.data.item())", "\n", "\n", "#loss*=1000", "\n", "", "", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "#print((ww-w).max())", "\n", "\n", "", "comb_p", "=", "get_comb_p", "(", "w", ",", "syn_valid", ")", "\n", "\n", "if", "self", ".", "opt", ".", "vis_w_key_token", "is", "not", "None", ":", "\n", "            ", "assert", "(", "text_for_vis", "is", "not", "None", "and", "record_for_vis", "is", "not", "None", ")", "\n", "vis_n", ",", "vis_l", "=", "text_for_vis", ".", "shape", "\n", "for", "i", "in", "range", "(", "vis_n", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "vis_l", ")", ":", "\n", "                    ", "if", "text_for_vis", "[", "i", ",", "j", "]", "==", "self", ".", "opt", ".", "vis_w_key_token", ":", "\n", "                        ", "record_for_vis", "[", "\"comb_p_list\"", "]", ".", "append", "(", "comb_p", "[", "i", ",", "j", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "record_for_vis", "[", "\"embd_syn_list\"", "]", ".", "append", "(", "syn", "[", "i", ",", "j", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "record_for_vis", "[", "\"syn_valid_list\"", "]", ".", "append", "(", "syn_valid", "[", "i", ",", "j", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "record_for_vis", "[", "\"text_syn_list\"", "]", ".", "append", "(", "text_like_syn", "[", "i", ",", "j", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "print", "(", "\"record for vis\"", ",", "len", "(", "record_for_vis", "[", "\"comb_p_list\"", "]", ")", ")", "\n", "", "if", "len", "(", "record_for_vis", "[", "\"comb_p_list\"", "]", ")", ">=", "300", ":", "\n", "                        ", "dir_name", "=", "self", ".", "opt", ".", "resume", ".", "split", "(", "self", ".", "opt", ".", "model", ")", "[", "0", "]", "\n", "file_name", "=", "self", ".", "opt", ".", "dataset", "+", "\"_vis_w_\"", "+", "str", "(", "self", ".", "opt", ".", "attack_sparse_weight", ")", "+", "\"_\"", "+", "str", "(", "self", ".", "opt", ".", "vis_w_key_token", ")", "+", "\".pkl\"", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "dir_name", ",", "file_name", ")", "\n", "f", "=", "open", "(", "file_name", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "record_for_vis", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "\n", "", "", "", "", "\"\"\"\n        out = get_comb(comb_p, syn)\n        delta = (out-embd_ori).reshape(batch_size*text_len,embd_dim)\n        delta = F.pairwise_distance(delta, torch.zeros_like(delta), p=2.0)\n        valid = (delta>0.01).to(device).to(delta.dtype)\n        delta = (valid*delta).sum()/valid.sum()\n        print(\"mean l2 dis between embd and embd_adv:\", delta.data.item())\n        #print(\"mean max comb_p:\", (comb_p.max(-2)[0]).mean().data.item())\n        \"\"\"", "\n", "\n", "if", "out_type", "==", "\"text\"", ":", "\n", "# need to be fix, has potential bugs. the trigger dependes on data.", "\n", "            ", "assert", "(", "text_like_syn", "is", "not", "None", ")", "# n l synlen", "\n", "comb_p", "=", "comb_p", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "syn_num", ")", "\n", "ind", "=", "comb_p", ".", "max", "(", "-", "1", ")", "[", "1", "]", "# shape batch_size* text_len", "\n", "out", "=", "(", "text_like_syn", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "syn_num", ")", "[", "np", ".", "arange", "(", "batch_size", "*", "text_len", ")", ",", "ind", "]", ")", ".", "reshape", "(", "batch_size", ",", "text_len", ")", "\n", "", "elif", "out_type", "==", "\"comb_p\"", ":", "\n", "            ", "out", "=", "comb_p", "\n", "\n", "# resume context", "\n", "", "if", "self_training_context", "==", "True", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "\n", "", "return", "out", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.get_onehot_from_input": [[395, 403], ["torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "x.reshape", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "get_onehot_from_input", "(", "self", ",", "x", ")", ":", "\n", "        ", "embd_voc_dim", ",", "embd_dim", "=", "self", ".", "embedding", ".", "weight", ".", "shape", "\n", "\n", "bs", ",", "text_len", "=", "x", ".", "shape", "\n", "\n", "out", "=", "torch", ".", "zeros", "(", "bs", "*", "text_len", ",", "embd_voc_dim", ")", ".", "to", "(", "torch", ".", "float32", ")", ".", "to", "(", "x", ".", "device", ")", ".", "scatter_", "(", "1", ",", "x", ".", "reshape", "(", "bs", "*", "text_len", ",", "1", ")", ",", "1", ")", "\n", "\n", "return", "out", ".", "reshape", "(", "bs", ",", "text_len", ",", "embd_voc_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.get_onehot_mask_from_syn": [[404, 413], ["torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "syn.reshape", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "get_onehot_mask_from_syn", "(", "self", ",", "syn_valid", ",", "syn", ")", ":", "\n", "        ", "embd_voc_dim", ",", "embd_dim", "=", "self", ".", "embedding", ".", "weight", ".", "shape", "\n", "\n", "bs", ",", "text_len", ",", "syn_max_num", "=", "syn", ".", "shape", "\n", "\n", "out", "=", "torch", ".", "zeros", "(", "bs", "*", "text_len", ",", "embd_voc_dim", ")", ".", "to", "(", "torch", ".", "float32", ")", ".", "to", "(", "syn", ".", "device", ")", ".", "scatter_", "(", "1", ",", "syn", ".", "reshape", "(", "bs", "*", "text_len", ",", "syn_max_num", ")", ",", "1", ")", "\n", "out", "[", ":", ",", "0", "]", "=", "0", "\n", "\n", "return", "out", ".", "reshape", "(", "bs", ",", "text_len", ",", "embd_voc_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.get_embd_from_onehot": [[414, 423], ["torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "BaseModelAdv.AdvBaseModel.reshape", "onehot_input.reshape", "BaseModelAdv.AdvBaseModel.linear_transform_embd_1"], "methods", ["None"], ["", "def", "get_embd_from_onehot", "(", "self", ",", "onehot_input", ")", ":", "\n", "        ", "w", "=", "self", ".", "embedding", ".", "weight", "\n", "bs", ",", "text_len", ",", "voc_d", "=", "onehot_input", ".", "shape", "\n", "embd", "=", "torch", ".", "mm", "(", "onehot_input", ".", "reshape", "(", "bs", "*", "text_len", ",", "voc_d", ")", ",", "w", ")", "\n", "embd", "=", "embd", ".", "reshape", "(", "bs", ",", "text_len", ",", "-", "1", ")", "\n", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "embd", "=", "self", ".", "linear_transform_embd_1", "(", "embd", ")", "\n", "", "return", "embd", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.get_adv_hotflip": [[424, 498], ["BaseModelAdv.AdvBaseModel.get_onehot_mask_from_syn", "BaseModelAdv.AdvBaseModel.text_to_embd", "BaseModelAdv.AdvBaseModel.embd_to_logit", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "x.detach", "range", "x_adv.detach.detach.detach", "BaseModelAdv.AdvBaseModel.eval", "BaseModelAdv.AdvBaseModel.train", "BaseModelAdv.AdvBaseModel.text_to_embd", "BaseModelAdv.AdvBaseModel.requires_grad_", "BaseModelAdv.AdvBaseModel.train", "BaseModelAdv.AdvBaseModel.eval", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "BaseModelAdv.AdvBaseModel.embd_to_logit", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "torch.max", "x_adv.detach.detach.detach", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm.reshape", "torch.mm.reshape", "torch.mm.reshape", "torch.KLDivLoss", "torch.KLDivLoss", "torch.KLDivLoss", "torch.KLDivLoss.", "grad_embd.reshape", "torch.mm().permute", "torch.mm().permute", "torch.mm().permute", "torch.mm().permute", "torch.mm().permute", "torch.mm().permute", "torch.mm().permute", "torch.mm().permute", "torch.mm().permute", "grad_embd.reshape", "BaseModelAdv.AdvBaseModel.embedding.weight.permute", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "BaseModelAdv.AdvBaseModel.linear_transform_embd_1.weight.permute"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_onehot_mask_from_syn", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "get_adv_hotflip", "(", "self", ",", "x", ",", "y", ",", "syn_valid", ",", "text_like_syn", ",", "attack_type_dict", ")", ":", "\n", "\n", "# record context", "\n", "        ", "self_training_context", "=", "self", ".", "training", "\n", "# set context", "\n", "if", "self", ".", "eval_adv_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "\n", "", "device", "=", "x", ".", "device", "\n", "# get param of attacks", "\n", "\n", "num_steps", "=", "attack_type_dict", "[", "'num_steps'", "]", "\n", "loss_func", "=", "attack_type_dict", "[", "'loss_func'", "]", "\n", "\n", "batch_size", ",", "text_len", "=", "x", ".", "shape", "\n", "\n", "#onehot_input = self.get_onehot_from_input(x) #bs, len, len_of_vocab", "\n", "onehot_mask", "=", "self", ".", "get_onehot_mask_from_syn", "(", "syn_valid", ",", "text_like_syn", ")", "\n", "#9.25 04:09", "\n", "#embd = self.get_embd_from_onehot(onehot_input)", "\n", "embd", "=", "self", ".", "text_to_embd", "(", "x", ")", "\n", "logit", "=", "self", ".", "embd_to_logit", "(", "embd", ")", "\n", "\n", "x_adv", "=", "torch", ".", "zeros_like", "(", "x", ")", "\n", "x_adv", "=", "x", ".", "detach", "(", ")", "\n", "\n", "\"\"\"\n        onehot_input_adv = self.get_onehot_from_input(x) #bs, len, len_of_vocab\n        onehot_input_adv.requires_grad_()\n        with torch.enable_grad():\n            embd_temp = self.get_embd_from_onehot(onehot_input_adv)\n            logit_temp = self.embd_to_logit(embd_temp)\n            if loss_func=='ce':\n                loss = F.cross_entropy(logit_temp, y, reduction='sum')\n            elif loss_func=='kl':\n                criterion_kl = nn.KLDivLoss(reduction=\"sum\")\n                loss = criterion_kl(F.log_softmax(logit_temp, dim=1), F.softmax(logit, dim=1))\n\n            grad = torch.autograd.grad(loss, [onehot_input_adv])[0]\n        \"\"\"", "\n", "\n", "for", "i", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "embd_adv", "=", "self", ".", "text_to_embd", "(", "x_adv", ")", "\n", "embd_adv", ".", "requires_grad_", "(", ")", "\n", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_adv", ")", "\n", "if", "loss_func", "==", "'ce'", ":", "\n", "                    ", "loss", "=", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "elif", "loss_func", "==", "'kl'", ":", "\n", "                    ", "criterion_kl", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "loss", "=", "criterion_kl", "(", "F", ".", "log_softmax", "(", "logit_adv", ",", "dim", "=", "1", ")", ",", "F", ".", "softmax", "(", "logit", ",", "dim", "=", "1", ")", ")", "\n", "\n", "", "grad_embd", "=", "torch", ".", "autograd", ".", "grad", "(", "loss", ",", "[", "embd_adv", "]", ")", "[", "0", "]", "\n", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "                    ", "grad_onehot", "=", "torch", ".", "mm", "(", "grad_embd", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "-", "1", ")", ",", "torch", ".", "mm", "(", "self", ".", "embedding", ".", "weight", ",", "self", ".", "linear_transform_embd_1", ".", "weight", ".", "permute", "(", "1", ",", "0", ")", ")", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "", "else", ":", "\n", "                    ", "grad_onehot", "=", "torch", ".", "mm", "(", "grad_embd", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "-", "1", ")", ",", "self", ".", "embedding", ".", "weight", ".", "permute", "(", "1", ",", "0", ")", ")", "\n", "", "grad_onehot", "=", "grad_onehot", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "-", "1", ")", "*", "(", "onehot_mask", ")", "\n", "_", ",", "argmax", "=", "torch", ".", "max", "(", "grad_onehot", ",", "-", "1", ")", "\n", "\n", "x_adv", "=", "x_adv", "*", "(", "argmax", "==", "0", ")", ".", "to", "(", "x_adv", ".", "dtype", ")", "+", "argmax", "\n", "x_adv", "=", "x_adv", ".", "detach", "(", ")", "\n", "\n", "# resume context", "\n", "", "", "if", "self_training_context", "==", "True", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "\n", "", "return", "x_adv", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.get_embd_adv": [[562, 645], ["len", "range", "embd_ori.detach.detach", "BaseModelAdv.AdvBaseModel.eval", "BaseModelAdv.AdvBaseModel.train", "embd_ori.detach", "embd_ori.detach.requires_grad_", "BaseModelAdv.AdvBaseModel.train", "BaseModelAdv.AdvBaseModel.eval", "embd_ori.detach", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "BaseModelAdv.AdvBaseModel.l2_project_sent", "BaseModelAdv.AdvBaseModel.l2_clip_sent", "embd_ori.detach", "BaseModelAdv.AdvBaseModel.detach", "embd_ori.detach.detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "BaseModelAdv.AdvBaseModel.embd_to_logit", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "embd_ori.detach.detach", "BaseModelAdv.AdvBaseModel.l2_project", "BaseModelAdv.AdvBaseModel.l2_clip", "BaseModelAdv.AdvBaseModel.embd_to_logit", "BaseModelAdv.AdvBaseModel.embd_to_logit", "torch.KLDivLoss", "torch.KLDivLoss", "torch.KLDivLoss", "torch.KLDivLoss.", "BaseModelAdv.AdvBaseModel.detach", "embd_ori.detach.detach", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "BaseModelAdv.AdvBaseModel.detach", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_project_sent", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_clip_sent", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_project", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_clip", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["def", "get_embd_adv", "(", "self", ",", "embd", ",", "y", ",", "attack_type_dict", ")", ":", "\n", "\n", "# record context", "\n", "        ", "self_training_context", "=", "self", ".", "training", "\n", "# set context", "\n", "if", "self", ".", "eval_adv_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "\n", "", "device", "=", "embd", ".", "device", "\n", "# get param of attacks", "\n", "\n", "num_steps", "=", "attack_type_dict", "[", "'num_steps'", "]", "\n", "step_size", "=", "attack_type_dict", "[", "'step_size'", "]", "\n", "random_start", "=", "attack_type_dict", "[", "'random_start'", "]", "\n", "epsilon", "=", "attack_type_dict", "[", "'epsilon'", "]", "\n", "loss_func", "=", "attack_type_dict", "[", "'loss_func'", "]", "\n", "direction", "=", "attack_type_dict", "[", "'direction'", "]", "\n", "ball_range", "=", "attack_type_dict", "[", "'ball_range'", "]", "\n", "\n", "batch_size", "=", "len", "(", "embd", ")", "\n", "\n", "embd_ori", "=", "embd", "\n", "\n", "# random start", "\n", "if", "random_start", ":", "\n", "            ", "embd_adv", "=", "embd_ori", ".", "detach", "(", ")", "+", "0.001", "*", "torch", ".", "randn", "(", "embd_ori", ".", "shape", ")", ".", "to", "(", "device", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "embd_adv", "=", "embd_ori", ".", "detach", "(", ")", "\n", "\n", "\n", "", "for", "_", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "embd_adv", ".", "requires_grad_", "(", ")", "\n", "grad", "=", "0", "\n", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "if", "loss_func", "==", "'ce'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_adv", ")", "\n", "if", "direction", "==", "\"towards\"", ":", "\n", "                        ", "loss", "=", "-", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "elif", "direction", "==", "\"away\"", ":", "\n", "                        ", "loss", "=", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "", "elif", "loss_func", "==", "'kl'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_adv", ")", "\n", "logit_ori", "=", "self", ".", "embd_to_logit", "(", "embd_ori", ")", "\n", "assert", "(", "direction", "==", "\"away\"", ")", "\n", "\n", "criterion_kl", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "loss", "=", "criterion_kl", "(", "F", ".", "log_softmax", "(", "logit_adv", ",", "dim", "=", "1", ")", ",", "\n", "F", ".", "softmax", "(", "logit_ori", ",", "dim", "=", "1", ")", ")", "\n", "\n", "", "grad", "=", "torch", ".", "autograd", ".", "grad", "(", "loss", ",", "[", "embd_adv", "]", ")", "[", "0", "]", "\n", "\n", "", "if", "ball_range", "==", "'sentence'", ":", "\n", "                ", "grad", "=", "self", ".", "l2_project_sent", "(", "grad", ")", "\n", "embd_adv", "=", "embd_adv", ".", "detach", "(", ")", "+", "step_size", "*", "grad", ".", "detach", "(", ")", "\n", "perturbation", "=", "self", ".", "l2_clip_sent", "(", "embd_adv", "-", "embd_ori", ",", "epsilon", ")", "\n", "", "elif", "ball_range", "==", "'word'", ":", "\n", "                ", "grad", "=", "self", ".", "l2_project", "(", "grad", ")", "\n", "embd_adv", "=", "embd_adv", ".", "detach", "(", ")", "+", "step_size", "*", "grad", ".", "detach", "(", ")", "\n", "perturbation", "=", "self", ".", "l2_clip", "(", "embd_adv", "-", "embd_ori", ",", "epsilon", ")", "\n", "", "else", ":", "\n", "                ", "assert", "NotImplementedError", "\n", "\n", "", "embd_adv", "=", "embd_ori", ".", "detach", "(", ")", "+", "perturbation", ".", "detach", "(", ")", "\n", "\n", "# resume context", "\n", "", "if", "self_training_context", "==", "True", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "\n", "", "class", "bpda_get_embd_adv", "(", "torch", ".", "autograd", ".", "Function", ")", ":", "\n", "\n", "            ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "embd", ")", ":", "\n", "                ", "return", "embd_adv", ".", "detach", "(", ")", "\n", "", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "# BPDA, approximate gradients", "\n", "                ", "return", "grad_output", "\n", "\n", "", "", "return", "embd_adv", ".", "detach", "(", ")", "\n", "#return bpda_get_embd_adv.apply(embd)", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.embd_to_embdnew": [[648, 651], ["BaseModelAdv.AdvBaseModel.linear_transform_embd"], "methods", ["None"], ["", "def", "embd_to_embdnew", "(", "self", ",", "embd", ")", ":", "\n", "        ", "embdnew", "=", "self", ".", "linear_transform_embd", "(", "embd", ")", "\n", "return", "embdnew", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.embd_to_text": [[652, 656], ["embd.reshape().mm", "embd.reshape().mm.reshape", "embd.reshape"], "methods", ["None"], ["", "def", "embd_to_text", "(", "self", ",", "embd", ")", ":", "\n", "        ", "bs", ",", "sen_len", ",", "embd_dim", "=", "embd", ".", "shape", "\n", "text", "=", "embd", ".", "reshape", "(", "-", "1", ",", "embd_dim", ")", ".", "mm", "(", "self", ".", "inverse_embedding", ".", "weight", ")", "\n", "return", "text", ".", "reshape", "(", "bs", ",", "sen_len", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.embdnew_to_embd": [[657, 660], ["BaseModelAdv.AdvBaseModel.inverse_linear_transform_embd"], "methods", ["None"], ["", "def", "embdnew_to_embd", "(", "self", ",", "embdnew", ")", ":", "\n", "        ", "embd", "=", "self", ".", "inverse_linear_transform_embd", "(", "embdnew", ")", "\n", "return", "embd", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.loss_text_adv": [[661, 666], ["torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "input.reshape", "label.reshape"], "methods", ["None"], ["", "def", "loss_text_adv", "(", "self", ",", "input", ",", "label", ")", ":", "\n", "#p = -F.log_softmax(input, dim=-1)", "\n", "#loss = p*(label.to(p.dtype)).sum()", "\n", "        ", "input_shape", "=", "input", ".", "shape", "\n", "return", "F", ".", "cross_entropy", "(", "input", ".", "reshape", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", ",", "label", ".", "reshape", "(", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.text_to_radius": [[667, 670], ["BaseModelAdv.AdvBaseModel.word_synonym_radius", "BaseModelAdv.AdvBaseModel.detach"], "methods", ["None"], ["", "def", "text_to_radius", "(", "self", ",", "inp", ")", ":", "\n", "        ", "saved_radius", "=", "self", ".", "word_synonym_radius", "(", "inp", ")", "# n, len, 1", "\n", "return", "saved_radius", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.text_to_embd": [[671, 680], ["BaseModelAdv.AdvBaseModel.embedding", "BaseModelAdv.AdvBaseModel.linear_transform_embd_1"], "methods", ["None"], ["", "def", "text_to_embd", "(", "self", ",", "inp", ")", ":", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "inp", ")", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "x", "=", "self", ".", "linear_transform_embd_1", "(", "x", ")", "\n", "#x = F.relu(x)", "\n", "#x = self.linear_transform_embd_2(x)", "\n", "#x = F.relu(x)", "\n", "#x = self.linear_transform_embd_3(x)", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.forward": [[681, 731], ["BaseModelAdv.AdvBaseModel.get_embd_adv", "BaseModelAdv.AdvBaseModel.get_adv_by_convex_syn", "BaseModelAdv.AdvBaseModel.embd_to_logit", "BaseModelAdv.AdvBaseModel.text_to_embd", "BaseModelAdv.AdvBaseModel.text_to_radius", "BaseModelAdv.AdvBaseModel.text_to_embd", "BaseModelAdv.AdvBaseModel.embd_to_logit", "BaseModelAdv.AdvBaseModel.text_to_embd().reshape", "BaseModelAdv.AdvBaseModel.embd_to_logit", "BaseModelAdv.AdvBaseModel.embd_to_embdnew", "BaseModelAdv.AdvBaseModel.embd_to_text", "BaseModelAdv.AdvBaseModel.embdnew_to_embd", "BaseModelAdv.AdvBaseModel.update_inverse_linear_transform_embd", "BaseModelAdv.AdvBaseModel.update_linear_transform_embd", "BaseModelAdv.AdvBaseModel.loss_text_adv", "BaseModelAdv.AdvBaseModel.loss_radius", "BaseModelAdv.AdvBaseModel.get_adv_hotflip", "BaseModelAdv.AdvBaseModel.text_to_embd", "input.reshape"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_embd_adv", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_adv_by_convex_syn", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_radius", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.embd_to_embdnew", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.embd_to_text", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.embdnew_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.update_inverse_linear_transform_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.update_linear_transform_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.loss_text_adv", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.loss_radius", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.get_adv_hotflip", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd"], ["", "def", "forward", "(", "self", ",", "mode", ",", "input", ",", "comb_p", "=", "None", ",", "label", "=", "None", ",", "text_like_syn_embd", "=", "None", ",", "text_like_syn_valid", "=", "None", ",", "text_like_syn", "=", "None", ",", "attack_type_dict", "=", "None", ",", "text_for_vis", "=", "None", ",", "record_for_vis", "=", "None", ")", ":", "\n", "        ", "if", "mode", "==", "\"get_embd_adv\"", ":", "\n", "            ", "assert", "(", "attack_type_dict", "is", "not", "None", ")", "\n", "out", "=", "self", ".", "get_embd_adv", "(", "input", ",", "label", ",", "attack_type_dict", ")", "\n", "", "if", "mode", "==", "\"get_adv_by_convex_syn\"", ":", "\n", "            ", "assert", "(", "attack_type_dict", "is", "not", "None", ")", "\n", "assert", "(", "text_like_syn_embd", "is", "not", "None", ")", "\n", "assert", "(", "text_like_syn_valid", "is", "not", "None", ")", "\n", "out", "=", "self", ".", "get_adv_by_convex_syn", "(", "input", ",", "label", ",", "text_like_syn_embd", ",", "text_like_syn_valid", ",", "text_like_syn", ",", "attack_type_dict", ",", "text_for_vis", ",", "record_for_vis", ")", "\n", "", "if", "mode", "==", "\"embd_to_logit\"", ":", "\n", "            ", "out", "=", "self", ".", "embd_to_logit", "(", "input", ")", "\n", "", "if", "mode", "==", "\"text_to_embd\"", ":", "\n", "            ", "out", "=", "self", ".", "text_to_embd", "(", "input", ")", "\n", "", "if", "mode", "==", "\"text_to_radius\"", ":", "\n", "            ", "out", "=", "self", ".", "text_to_radius", "(", "input", ")", "\n", "", "if", "mode", "==", "\"text_to_logit\"", ":", "\n", "            ", "embd", "=", "self", ".", "text_to_embd", "(", "input", ")", "\n", "#embd = self.embd_to_embdnew(embd)", "\n", "out", "=", "self", ".", "embd_to_logit", "(", "embd", ")", "\n", "", "if", "mode", "==", "\"text_syn_p_to_logit\"", ":", "\n", "            ", "assert", "(", "comb_p", "is", "not", "None", ")", "\n", "bs", ",", "tl", ",", "sl", "=", "input", ".", "shape", "\n", "text_like_syn_embd", "=", "self", ".", "text_to_embd", "(", "input", ".", "reshape", "(", "bs", ",", "tl", "*", "sl", ")", ")", ".", "reshape", "(", "bs", ",", "tl", ",", "sl", ",", "-", "1", ")", "\n", "embd", "=", "(", "comb_p", "*", "text_like_syn_embd", ")", ".", "sum", "(", "-", "2", ")", "\n", "out", "=", "self", ".", "embd_to_logit", "(", "embd", ")", "\n", "", "if", "mode", "==", "\"embd_to_embdnew\"", ":", "\n", "            ", "out", "=", "self", ".", "embd_to_embdnew", "(", "input", ")", "\n", "", "if", "mode", "==", "\"embd_to_text\"", ":", "\n", "            ", "out", "=", "self", ".", "embd_to_text", "(", "input", ")", "\n", "", "if", "mode", "==", "\"embdnew_to_embd\"", ":", "\n", "            ", "out", "=", "self", ".", "embdnew_to_embd", "(", "input", ")", "\n", "", "if", "mode", "==", "\"update_inverse_linear_transform_embd\"", ":", "\n", "            ", "self", ".", "update_inverse_linear_transform_embd", "(", ")", "\n", "out", "=", "None", "\n", "", "if", "mode", "==", "\"update_linear_transform_embd\"", ":", "\n", "            ", "self", ".", "update_linear_transform_embd", "(", ")", "\n", "out", "=", "None", "\n", "", "if", "mode", "==", "\"loss_text_adv\"", ":", "\n", "            ", "out", "=", "self", ".", "loss_text_adv", "(", "input", ",", "label", ")", "\n", "", "if", "mode", "==", "\"loss_radius\"", ":", "\n", "            ", "out", "=", "self", ".", "loss_radius", "(", "input", ",", "label", ")", "\n", "\n", "", "if", "mode", "==", "\"get_adv_hotflip\"", ":", "\n", "            ", "assert", "(", "attack_type_dict", "is", "not", "None", ")", "\n", "assert", "(", "text_like_syn", "is", "not", "None", ")", "\n", "assert", "(", "text_like_syn_valid", "is", "not", "None", ")", "\n", "out", "=", "self", ".", "get_adv_hotflip", "(", "input", ",", "label", ",", "text_like_syn_valid", ",", "text_like_syn", ",", "attack_type_dict", ")", "\n", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.RNN_CNN.RNN_CNN.__init__": [[9, 28], ["models.BaseModel.BaseModel.__init__", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.LSTM", "torch.LSTM", "torch.LSTM", "RNN_CNN.RNN_CNN.init_hidden", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Linear", "torch.Linear", "torch.Linear", "RNN_CNN.RNN_CNN.properties.update"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "\n", "        ", "super", "(", "RNN_CNN", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "hidden_dim", "=", "opt", ".", "hidden_dim", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "use_gpu", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "opt", ".", "embedding_dim", ",", "opt", ".", "hidden_dim", ")", "\n", "###self.hidden2label = nn.Linear(opt.hidden_dim, opt.label_size)", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", ")", "\n", "\n", "self", ".", "content_dim", "=", "256", "\n", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "opt", ".", "hidden_dim", ",", "out_channels", "=", "self", ".", "content_dim", ",", "kernel_size", "=", "opt", ".", "hidden_dim", "*", "2", ",", "stride", "=", "opt", ".", "embedding_dim", ")", "\n", "self", ".", "hidden2label", "=", "nn", ".", "Linear", "(", "self", ".", "content_dim", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"content_dim\"", ":", "self", ".", "content_dim", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.RNN_CNN.RNN_CNN.init_hidden": [[30, 41], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "        ", "if", "batch_size", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "if", "self", ".", "use_gpu", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "cuda", "(", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "cuda", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ")", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "#    @profile", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.RNN_CNN.RNN_CNN.forward": [[42, 55], ["RNN_CNN.RNN_CNN.word_embeddings", "RNN_CNN.RNN_CNN.permute", "RNN_CNN.RNN_CNN.init_hidden", "RNN_CNN.RNN_CNN.lstm", "RNN_CNN.RNN_CNN.conv", "RNN_CNN.RNN_CNN.hidden2label", "lstm_out.permute", "RNN_CNN.RNN_CNN.view", "sentence.size", "RNN_CNN.RNN_CNN.size"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.PWWS.neural_networks.lstm"], ["", "def", "forward", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "embeds", "=", "self", ".", "word_embeddings", "(", "sentence", ")", "#64x200x300", "\n", "\n", "#        x = embeds.view(sentence.size()[1], self.batch_size, -1)", "\n", "x", "=", "embeds", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "#200x64x300", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", "sentence", ".", "size", "(", ")", "[", "0", "]", ")", "#1x64x128", "\n", "lstm_out", ",", "self", ".", "hidden", "=", "self", ".", "lstm", "(", "x", ",", "self", ".", "hidden", ")", "###input (seq_len, batch, input_size) #Outupts:output, (h_n, c_n) output:(seq_len, batch, hidden_size * num_directions)", "\n", "#lstm_out 200x64x128  lstm_out.permute(1,2,0):64x128x200", "\n", "y", "=", "self", ".", "conv", "(", "lstm_out", ".", "permute", "(", "1", ",", "2", ",", "0", ")", ")", "###64x256x1", "\n", "###y = self.conv(lstm_out.permute(1,2,0).contiguous().view(self.batch_size,128,-1))", "\n", "#y  = self.hidden2label(y.view(sentence.size()[0],-1))", "\n", "y", "=", "self", ".", "hidden2label", "(", "y", ".", "view", "(", "y", ".", "size", "(", ")", "[", "0", "]", ",", "-", "1", ")", ")", "#64x3", "\n", "return", "y", "", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN_Inception.Inception.__init__": [[11, 36], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "CNN_Inception.Inception.activa.add_module", "CNN_Inception.Inception.activa.add_module", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "cin", ",", "co", ",", "relu", "=", "True", ",", "norm", "=", "True", ")", ":", "\n", "        ", "super", "(", "Inception", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "co", "%", "4", "==", "0", ")", "\n", "cos", "=", "[", "co", "/", "4", "]", "*", "4", "\n", "self", ".", "activa", "=", "nn", ".", "Sequential", "(", ")", "\n", "if", "norm", ":", "self", ".", "activa", ".", "add_module", "(", "'norm'", ",", "nn", ".", "BatchNorm1d", "(", "co", ")", ")", "\n", "if", "relu", ":", "self", ".", "activa", ".", "add_module", "(", "'relu'", ",", "nn", ".", "ReLU", "(", "True", ")", ")", "\n", "self", ".", "branch1", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'conv1'", ",", "nn", ".", "Conv1d", "(", "cin", ",", "cos", "[", "0", "]", ",", "1", ",", "stride", "=", "1", ")", ")", ",", "\n", "]", ")", ")", "\n", "self", ".", "branch2", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'conv1'", ",", "nn", ".", "Conv1d", "(", "cin", ",", "cos", "[", "1", "]", ",", "1", ")", ")", ",", "\n", "(", "'norm1'", ",", "nn", ".", "BatchNorm1d", "(", "cos", "[", "1", "]", ")", ")", ",", "\n", "(", "'relu1'", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ",", "\n", "(", "'conv3'", ",", "nn", ".", "Conv1d", "(", "cos", "[", "1", "]", ",", "cos", "[", "1", "]", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ")", ",", "\n", "]", ")", ")", "\n", "self", ".", "branch3", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "(", "'conv1'", ",", "nn", ".", "Conv1d", "(", "cin", ",", "cos", "[", "2", "]", ",", "3", ",", "padding", "=", "1", ")", ")", ",", "\n", "(", "'norm1'", ",", "nn", ".", "BatchNorm1d", "(", "cos", "[", "2", "]", ")", ")", ",", "\n", "(", "'relu1'", ",", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", ",", "\n", "(", "'conv3'", ",", "nn", ".", "Conv1d", "(", "cos", "[", "2", "]", ",", "cos", "[", "2", "]", ",", "5", ",", "stride", "=", "1", ",", "padding", "=", "2", ")", ")", ",", "\n", "]", ")", ")", "\n", "self", ".", "branch4", "=", "nn", ".", "Sequential", "(", "OrderedDict", "(", "[", "\n", "#('pool',nn.MaxPool1d(2)),", "\n", "(", "'conv3'", ",", "nn", ".", "Conv1d", "(", "cin", ",", "cos", "[", "3", "]", ",", "3", ",", "stride", "=", "1", ",", "padding", "=", "1", ")", ")", ",", "\n", "]", ")", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN_Inception.Inception.forward": [[37, 44], ["CNN_Inception.Inception.branch1", "CNN_Inception.Inception.branch2", "CNN_Inception.Inception.branch3", "CNN_Inception.Inception.branch4", "CNN_Inception.Inception.activa", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "branch1", "=", "self", ".", "branch1", "(", "x", ")", "\n", "branch2", "=", "self", ".", "branch2", "(", "x", ")", "\n", "branch3", "=", "self", ".", "branch3", "(", "x", ")", "\n", "branch4", "=", "self", ".", "branch4", "(", "x", ")", "\n", "result", "=", "self", ".", "activa", "(", "torch", ".", "cat", "(", "(", "branch1", ",", "branch2", ",", "branch3", ",", "branch4", ")", ",", "1", ")", ")", "\n", "return", "result", "\n", "", "", "class", "CNNText_inception", "(", "BaseModel", ")", ":", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN_Inception.CNNText_inception.__init__": [[45, 71], ["models.BaseModel.BaseModel.__init__", "getattr", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Sequential", "torch.nn.Sequential", "getattr", "torch.nn.Sequential", "torch.nn.Sequential", "CNN_Inception.CNNText_inception.properties.update", "CNN_Inception.Inception", "CNN_Inception.Inception", "torch.nn.MaxPool1d", "torch.nn.MaxPool1d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "opt.__dict__.get", "print", "CNN_Inception.CNNText_inception.encoder.weight.data.copy_", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "CNNText_inception", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "incept_dim", "=", "getattr", "(", "opt", ",", "\"inception_dim\"", ",", "512", ")", "\n", "\n", "\n", "self", ".", "encoder", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "\n", "self", ".", "content_conv", "=", "nn", ".", "Sequential", "(", "\n", "Inception", "(", "opt", ".", "embedding_dim", ",", "incept_dim", ")", ",", "#(batch_size,64,opt.content_seq_len)->(batch_size,64,(opt.content_seq_len)/2)", "\n", "#Inception(incept_dim,incept_dim),#(batch_size,64,opt.content_seq_len/2)->(batch_size,32,(opt.content_seq_len)/4)", "\n", "Inception", "(", "incept_dim", ",", "incept_dim", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "opt", ".", "max_seq_len", ")", "\n", ")", "\n", "opt", ".", "hidden_size", "=", "getattr", "(", "opt", ",", "\"linear_hidden_size\"", ",", "2000", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "incept_dim", ",", "opt", ".", "hidden_size", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "opt", ".", "hidden_size", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Linear", "(", "opt", ".", "hidden_size", ",", "opt", ".", "label_size", ")", "\n", ")", "\n", "if", "opt", ".", "__dict__", ".", "get", "(", "\"embeddings\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "print", "(", "'load embedding'", ")", "\n", "self", ".", "encoder", ".", "weight", ".", "data", ".", "copy_", "(", "t", ".", "from_numpy", "(", "opt", ".", "embeddings", ")", ")", "\n", "", "self", ".", "properties", ".", "update", "(", "\n", "{", "\"inception_dim\"", ":", "incept_dim", ",", "\n", "\"hidden_size\"", ":", "opt", ".", "hidden_size", ",", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNN_Inception.CNNText_inception.forward": [[73, 83], ["CNN_Inception.CNNText_inception.encoder", "CNN_Inception.CNNText_inception.content_conv", "CNN_Inception.CNNText_inception.view", "CNN_Inception.CNNText_inception.fc", "content.detach.detach.detach", "content.detach.detach.permute", "CNN_Inception.CNNText_inception.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "content", ")", ":", "\n", "\n", "        ", "content", "=", "self", ".", "encoder", "(", "content", ")", "\n", "if", "self", ".", "opt", ".", "static", ":", "\n", "            ", "content", "=", "content", ".", "detach", "(", "0", ")", "\n", "\n", "", "content_out", "=", "self", ".", "content_conv", "(", "content", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "\n", "out", "=", "content_out", ".", "view", "(", "content_out", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "out", "=", "self", ".", "fc", "(", "out", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMwithAttention.LSTMAttention.__init__": [[10, 29], ["super().__init__", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "LSTMwithAttention.LSTMAttention.init_hidden", "opt.__dict__.get", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "\n", "        ", "super", "(", "LSTMAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "opt", ".", "hidden_dim", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "use_gpu", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))", "\n", "\n", "self", ".", "num_layers", "=", "opt", ".", "lstm_layers", "\n", "#self.bidirectional = True", "\n", "self", ".", "dropout", "=", "opt", ".", "keep_dropout", "\n", "self", ".", "bilstm", "=", "nn", ".", "LSTM", "(", "opt", ".", "embedding_dim", ",", "opt", ".", "hidden_dim", "//", "2", ",", "batch_first", "=", "True", ",", "num_layers", "=", "self", ".", "num_layers", ",", "dropout", "=", "self", ".", "dropout", ",", "bidirectional", "=", "True", ")", "\n", "self", ".", "hidden2label", "=", "nn", ".", "Linear", "(", "opt", ".", "hidden_dim", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", ")", "\n", "self", ".", "mean", "=", "opt", ".", "__dict__", ".", "get", "(", "\"lstm_mean\"", ",", "True", ")", "\n", "self", ".", "attn_fc", "=", "torch", ".", "nn", ".", "Linear", "(", "opt", ".", "embedding_dim", ",", "1", ")", "\n", "", "def", "init_hidden", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMwithAttention.LSTMAttention.init_hidden": [[29, 40], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "        ", "if", "batch_size", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "if", "self", ".", "use_gpu", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ")", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMwithAttention.LSTMAttention.attention": [[42, 50], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "merged_state.squeeze().unsqueeze.squeeze().unsqueeze.squeeze().unsqueeze", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.nn.functional.softmax().unsqueeze", "torch.nn.functional.softmax().unsqueeze", "torch.nn.functional.softmax().unsqueeze", "torch.nn.functional.softmax().unsqueeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "torch.bmm().squeeze", "merged_state.squeeze().unsqueeze.squeeze().unsqueeze.squeeze", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.nn.functional.softmax", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.nn.functional.softmax().unsqueeze.squeeze", "torch.nn.functional.softmax().unsqueeze.squeeze", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "attention", "(", "self", ",", "rnn_out", ",", "state", ")", ":", "\n", "        ", "merged_state", "=", "torch", ".", "cat", "(", "[", "s", "for", "s", "in", "state", "]", ",", "1", ")", "\n", "merged_state", "=", "merged_state", ".", "squeeze", "(", "0", ")", ".", "unsqueeze", "(", "2", ")", "\n", "# (batch, seq_len, cell_size) * (batch, cell_size, 1) = (batch, seq_len, 1)", "\n", "weights", "=", "torch", ".", "bmm", "(", "rnn_out", ",", "merged_state", ")", "# nsh nhl", "\n", "weights", "=", "torch", ".", "nn", ".", "functional", ".", "softmax", "(", "weights", ".", "squeeze", "(", "2", ")", ")", ".", "unsqueeze", "(", "2", ")", "\n", "# (batch, cell_size, seq_len) * (batch, seq_len, 1) = (batch, cell_size, 1)", "\n", "return", "torch", ".", "bmm", "(", "torch", ".", "transpose", "(", "rnn_out", ",", "1", ",", "2", ")", ",", "weights", ")", ".", "squeeze", "(", "2", ")", "\n", "# end method attention", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMwithAttention.LSTMAttention.forward": [[53, 61], ["LSTMwithAttention.LSTMAttention.word_embeddings", "LSTMwithAttention.LSTMAttention.init_hidden", "LSTMwithAttention.LSTMAttention.bilstm", "LSTMwithAttention.LSTMAttention.attention", "LSTMwithAttention.LSTMAttention.hidden2label", "X.size"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.LSTMwithAttention.LSTMAttention.attention"], ["", "def", "forward", "(", "self", ",", "X", ")", ":", "\n", "        ", "embedded", "=", "self", ".", "word_embeddings", "(", "X", ")", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "X", ".", "size", "(", ")", "[", "0", "]", ")", "#", "\n", "rnn_out", ",", "hidden", "=", "self", ".", "bilstm", "(", "embedded", ",", "hidden", ")", "\n", "h_n", ",", "c_n", "=", "hidden", "\n", "attn_out", "=", "self", ".", "attention", "(", "rnn_out", ",", "h_n", ")", "#nh out", "\n", "logits", "=", "self", ".", "hidden2label", "(", "attn_out", ")", "\n", "return", "logits", "", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MLP.AttrProxy.__init__": [[29, 32], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "module", ",", "prefix", ")", ":", "\n", "        ", "self", ".", "module", "=", "module", "\n", "self", ".", "prefix", "=", "prefix", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MLP.AttrProxy.__getitem__": [[33, 35], ["getattr", "str"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "module", ",", "self", ".", "prefix", "+", "str", "(", "i", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MLP.MemN2N.__init__": [[38, 59], ["torch.Module.__init__", "range", "MLP.AttrProxy", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding.weight.data.normal_", "MLP.MemN2N.add_module", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "MLP.MemN2N.encoding.cuda", "MLP.position_encoding"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.position_encoding"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "MemN2N", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "use_cuda", "=", "opt", "[", "\"use_cuda\"", "]", "\n", "num_vocab", "=", "opt", "[", "\"num_vocab\"", "]", "\n", "embedding_dim", "=", "opt", "[", "\"embedding_dim\"", "]", "\n", "sentence_size", "=", "opt", "[", "\"sentence_size\"", "]", "\n", "self", ".", "max_hops", "=", "opt", "[", "\"max_hops\"", "]", "\n", "\n", "for", "hop", "in", "range", "(", "self", ".", "max_hops", "+", "1", ")", ":", "\n", "            ", "C", "=", "nn", ".", "Embedding", "(", "num_vocab", ",", "embedding_dim", ",", "padding_idx", "=", "0", ")", "\n", "C", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.1", ")", "\n", "self", ".", "add_module", "(", "\"C_{}\"", ".", "format", "(", "hop", ")", ",", "C", ")", "\n", "", "self", ".", "C", "=", "AttrProxy", "(", "self", ",", "\"C_\"", ")", "\n", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", ")", "\n", "self", ".", "encoding", "=", "Variable", "(", "torch", ".", "FloatTensor", "(", "\n", "position_encoding", "(", "sentence_size", ",", "embedding_dim", ")", ")", ",", "requires_grad", "=", "False", ")", "\n", "\n", "if", "use_cuda", ":", "\n", "            ", "self", ".", "encoding", "=", "self", ".", "encoding", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MLP.MemN2N.forward": [[60, 91], ["story.size", "list", "MLP.MemN2N.encoding.unsqueeze().expand_as", "list.append", "range", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "embed_A.view.view.view", "MLP.MemN2N.encoding.unsqueeze().unsqueeze().expand_as", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "u[].unsqueeze().expand_as", "MLP.MemN2N.softmax", "embed_C.view.view.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "prob.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "list.append", "MLP.MemN2N.C[].weight.transpose", "MLP.MemN2N.softmax", "MLP.MemN2N.encoding.unsqueeze", "story.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "story.view", "story.size", "MLP.MemN2N.encoding.unsqueeze().unsqueeze", "u[].unsqueeze", "story.size", "prob.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "embed_A.view.view.size", "embed_C.view.view.size", "MLP.MemN2N.encoding.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "", "def", "forward", "(", "self", ",", "story", ",", "query", ")", ":", "\n", "        ", "story_size", "=", "story", ".", "size", "(", ")", "\n", "\n", "u", "=", "list", "(", ")", "\n", "query_embed", "=", "self", ".", "C", "[", "0", "]", "(", "query", ")", "\n", "# weired way to perform reduce_dot", "\n", "encoding", "=", "self", ".", "encoding", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "query_embed", ")", "\n", "u", ".", "append", "(", "torch", ".", "sum", "(", "query_embed", "*", "encoding", ",", "1", ")", ")", "\n", "\n", "for", "hop", "in", "range", "(", "self", ".", "max_hops", ")", ":", "\n", "            ", "embed_A", "=", "self", ".", "C", "[", "hop", "]", "(", "story", ".", "view", "(", "story", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "embed_A", "=", "embed_A", ".", "view", "(", "story_size", "+", "(", "embed_A", ".", "size", "(", "-", "1", ")", ",", ")", ")", "\n", "\n", "encoding", "=", "self", ".", "encoding", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "embed_A", ")", "\n", "m_A", "=", "torch", ".", "sum", "(", "embed_A", "*", "encoding", ",", "2", ")", "\n", "\n", "u_temp", "=", "u", "[", "-", "1", "]", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "m_A", ")", "\n", "prob", "=", "self", ".", "softmax", "(", "torch", ".", "sum", "(", "m_A", "*", "u_temp", ",", "2", ")", ")", "\n", "\n", "embed_C", "=", "self", ".", "C", "[", "hop", "+", "1", "]", "(", "story", ".", "view", "(", "story", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "embed_C", "=", "embed_C", ".", "view", "(", "story_size", "+", "(", "embed_C", ".", "size", "(", "-", "1", ")", ",", ")", ")", "\n", "m_C", "=", "torch", ".", "sum", "(", "embed_C", "*", "encoding", ",", "2", ")", "\n", "\n", "prob", "=", "prob", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "m_C", ")", "\n", "o_k", "=", "torch", ".", "sum", "(", "m_C", "*", "prob", ",", "1", ")", "\n", "\n", "u_k", "=", "u", "[", "-", "1", "]", "+", "o_k", "\n", "u", ".", "append", "(", "u_k", ")", "\n", "\n", "", "a_hat", "=", "u", "[", "-", "1", "]", "@", "self", ".", "C", "[", "self", ".", "max_hops", "]", ".", "weight", ".", "transpose", "(", "0", ",", "1", ")", "\n", "return", "a_hat", ",", "self", ".", "softmax", "(", "a_hat", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MLP.position_encoding": [[11, 22], ["numpy.ones", "range", "numpy.transpose", "range"], "function", ["None"], ["def", "position_encoding", "(", "sentence_size", ",", "embedding_dim", ")", ":", "\n", "    ", "encoding", "=", "np", ".", "ones", "(", "(", "embedding_dim", ",", "sentence_size", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "ls", "=", "sentence_size", "+", "1", "\n", "le", "=", "embedding_dim", "+", "1", "\n", "for", "i", "in", "range", "(", "1", ",", "le", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "1", ",", "ls", ")", ":", "\n", "            ", "encoding", "[", "i", "-", "1", ",", "j", "-", "1", "]", "=", "(", "i", "-", "(", "embedding_dim", "+", "1", ")", "/", "2", ")", "*", "(", "j", "-", "(", "sentence_size", "+", "1", ")", "/", "2", ")", "\n", "", "", "encoding", "=", "1", "+", "4", "*", "encoding", "/", "embedding_dim", "/", "sentence_size", "\n", "# Make position encoding of time words identity to avoid modifying them", "\n", "encoding", "[", ":", ",", "-", "1", "]", "=", "1.0", "\n", "return", "np", ".", "transpose", "(", "encoding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.CapsuleLayer.__init__": [[31, 49], ["torch.nn.Module.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.nn.Conv1d", "torch.nn.Conv1d", "int"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_capsules", ",", "num_route_nodes", ",", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "None", ",", "stride", "=", "None", ",", "\n", "num_iterations", "=", "NUM_ROUTING_ITERATIONS", ",", "padding", "=", "0", ")", ":", "\n", "        ", "super", "(", "CapsuleLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_route_nodes", "=", "num_route_nodes", "\n", "self", ".", "num_iterations", "=", "num_iterations", "\n", "\n", "self", ".", "num_capsules", "=", "num_capsules", "\n", "\n", "\n", "\n", "if", "num_route_nodes", "!=", "-", "1", ":", "\n", "            ", "self", ".", "route_weights", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "num_capsules", ",", "num_route_nodes", ",", "in_channels", ",", "out_channels", ")", ")", "\n", "", "else", ":", "\n", "            ", "prime", "=", "[", "3", ",", "5", ",", "7", ",", "9", ",", "11", ",", "13", ",", "17", ",", "19", ",", "23", "]", "\n", "sizes", "=", "prime", "[", ":", "self", ".", "num_capsules", "]", "\n", "self", ".", "capsules", "=", "nn", ".", "ModuleList", "(", "\n", "[", "nn", ".", "Conv1d", "(", "in_channels", ",", "out_channels", ",", "kernel_size", "=", "i", ",", "stride", "=", "2", ",", "padding", "=", "int", "(", "(", "i", "-", "1", ")", "/", "2", ")", ")", "for", "i", "in", "sizes", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.CapsuleLayer.squash": [[50, 54], ["torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt"], "methods", ["None"], ["", "", "def", "squash", "(", "self", ",", "tensor", ",", "dim", "=", "-", "1", ")", ":", "\n", "        ", "squared_norm", "=", "(", "tensor", "**", "2", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "scale", "=", "squared_norm", "/", "(", "1", "+", "squared_norm", ")", "\n", "return", "scale", "*", "tensor", "/", "torch", ".", "sqrt", "(", "squared_norm", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.CapsuleLayer.forward": [[55, 77], ["torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "Capsule.CapsuleLayer.squash", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "Capsule.softmax", "Capsule.CapsuleLayer.squash", "capsule().view", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum", "torch.mul().sum", "x.size", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.matmul.size", "torch.matmul.size", "capsule", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.mul", "torch.matmul.size", "torch.matmul.size"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.CapsuleLayer.squash", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.CapsuleLayer.squash"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "if", "self", ".", "num_route_nodes", "!=", "-", "1", ":", "\n", "            ", "priors", "=", "torch", ".", "matmul", "(", "x", "[", "None", ",", ":", ",", ":", ",", "None", ",", ":", "]", ",", "self", ".", "route_weights", "[", ":", ",", "None", ",", ":", ",", ":", ",", ":", "]", ")", "\n", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "logits", "=", "torch", ".", "autograd", ".", "Variable", "(", "torch", ".", "zeros", "(", "priors", ".", "size", "(", ")", ")", ")", ".", "cuda", "(", ")", "\n", "", "else", ":", "\n", "                ", "logits", "=", "torch", ".", "autograd", ".", "Variable", "(", "torch", ".", "zeros", "(", "priors", ".", "size", "(", ")", ")", ")", "\n", "", "for", "i", "in", "range", "(", "self", ".", "num_iterations", ")", ":", "\n", "                ", "probs", "=", "softmax", "(", "logits", ",", "dim", "=", "2", ")", "\n", "outputs", "=", "self", ".", "squash", "(", "(", "torch", ".", "mul", "(", "probs", ",", "priors", ")", ")", ".", "sum", "(", "dim", "=", "2", ",", "keepdim", "=", "True", ")", ")", "\n", "\n", "if", "i", "!=", "self", ".", "num_iterations", "-", "1", ":", "\n", "                    ", "delta_logits", "=", "(", "torch", ".", "mul", "(", "priors", ",", "outputs", ")", ")", ".", "sum", "(", "dim", "=", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "logits", "=", "logits", "+", "delta_logits", "\n", "", "", "", "else", ":", "\n", "            ", "outputs", "=", "[", "capsule", "(", "x", ")", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ",", "1", ")", "for", "capsule", "in", "self", ".", "capsules", "]", "\n", "outputs", "=", "torch", ".", "cat", "(", "outputs", ",", "dim", "=", "-", "1", ")", "\n", "outputs", "=", "self", ".", "squash", "(", "outputs", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.CapsuleNet.__init__": [[80, 106], ["models.BaseModel.BaseModel.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "Capsule.CapsuleLayer", "Capsule.CapsuleLayer", "torch.nn.Sequential", "torch.nn.Sequential", "opt.__dict__.get", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Conv2d", "torch.nn.Conv2d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Sigmoid", "torch.nn.Sigmoid", "int", "int", "int"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "CapsuleNet", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "\n", "self", ".", "label_size", "=", "opt", ".", "label_size", "\n", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", "+", "1", ",", "opt", ".", "embedding_dim", ")", "\n", "self", ".", "opt", ".", "cnn_dim", "=", "1", "\n", "self", ".", "kernel_size", "=", "3", "\n", "self", ".", "kernel_size_primary", "=", "3", "\n", "if", "opt", ".", "__dict__", ".", "get", "(", "\"embeddings\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "self", ".", "embed", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "\n", "", "self", ".", "primary_capsules", "=", "CapsuleLayer", "(", "num_capsules", "=", "8", ",", "num_route_nodes", "=", "-", "1", ",", "in_channels", "=", "256", ",", "out_channels", "=", "32", ")", "\n", "self", ".", "digit_capsules", "=", "CapsuleLayer", "(", "num_capsules", "=", "opt", ".", "label_size", ",", "num_route_nodes", "=", "int", "(", "32", "*", "opt", ".", "max_seq_len", "/", "2", ")", ",", "in_channels", "=", "8", ",", "\n", "out_channels", "=", "16", ")", "\n", "if", "self", ".", "opt", ".", "cnn_dim", "==", "2", ":", "\n", "            ", "self", ".", "conv_2d", "=", "nn", ".", "Conv2d", "(", "in_channels", "=", "1", ",", "out_channels", "=", "256", ",", "kernel_size", "=", "(", "self", ".", "kernel_size", ",", "opt", ".", "embedding_dim", ")", ",", "stride", "=", "(", "1", ",", "opt", ".", "embedding_dim", ")", ",", "padding", "=", "(", "int", "(", "(", "self", ".", "kernel_size", "-", "1", ")", "/", "2", ")", ",", "0", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_1d", "=", "nn", ".", "Conv1d", "(", "in_channels", "=", "1", ",", "out_channels", "=", "256", ",", "kernel_size", "=", "opt", ".", "embedding_dim", "*", "self", ".", "kernel_size", ",", "stride", "=", "opt", ".", "embedding_dim", ",", "padding", "=", "opt", ".", "embedding_dim", "*", "int", "(", "(", "self", ".", "kernel_size", "-", "1", ")", "/", "2", ")", ")", "\n", "\n", "", "self", ".", "decoder", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "16", "*", "self", ".", "label_size", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Linear", "(", "512", ",", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Linear", "(", "1024", ",", "784", ")", ",", "\n", "nn", ".", "Sigmoid", "(", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.CapsuleNet.forward": [[108, 137], ["Capsule.CapsuleNet.embed", "Capsule.CapsuleNet.primary_capsules", "Capsule.CapsuleNet.digit_capsules().squeeze().transpose", "torch.softmax", "torch.softmax", "Capsule.CapsuleNet.decoder", "x.unsqueeze.unsqueeze.view", "torch.relu", "torch.relu", "x.unsqueeze.unsqueeze.unsqueeze", "torch.relu().squeeze", "torch.relu().squeeze", "torch.softmax.max", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "x.unsqueeze.unsqueeze.size", "Capsule.CapsuleNet.conv_1d", "Capsule.CapsuleNet.digit_capsules().squeeze", "Variable().cuda().index_select", "Variable().index_select", "x.unsqueeze.unsqueeze.size", "x.unsqueeze.unsqueeze.size", "x.unsqueeze.unsqueeze.size", "torch.relu", "torch.relu", "Capsule.CapsuleNet.conv_2d", "Capsule.CapsuleNet.digit_capsules", "Variable().cuda", "Variable", "torch.sparse.torch.eye", "torch.sparse.torch.eye", "torch.sparse.torch.eye", "torch.sparse.torch.eye", "Variable", "torch.sparse.torch.eye", "torch.sparse.torch.eye", "torch.sparse.torch.eye", "torch.sparse.torch.eye"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", "=", "None", ",", "reconstruct", "=", "False", ")", ":", "\n", "#x = next(iter(train_iter)).text[0]", "\n", "\n", "        ", "x", "=", "self", ".", "embed", "(", "x", ")", "\n", "if", "self", ".", "opt", ".", "cnn_dim", "==", "1", ":", "\n", "            ", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "1", ",", "x", ".", "size", "(", "-", "1", ")", "*", "x", ".", "size", "(", "-", "2", ")", ")", "\n", "x_conv", "=", "F", ".", "relu", "(", "self", ".", "conv_1d", "(", "x", ")", ",", "inplace", "=", "True", ")", "\n", "", "else", ":", "\n", "\n", "            ", "x", "=", "x", ".", "unsqueeze", "(", "1", ")", "\n", "x_conv", "=", "F", ".", "relu", "(", "self", ".", "conv_2d", "(", "x", ")", ",", "inplace", "=", "True", ")", ".", "squeeze", "(", "3", ")", "\n", "\n", "", "x", "=", "self", ".", "primary_capsules", "(", "x_conv", ")", "\n", "x", "=", "self", ".", "digit_capsules", "(", "x", ")", ".", "squeeze", "(", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "classes", "=", "(", "x", "**", "2", ")", ".", "sum", "(", "dim", "=", "-", "1", ")", "**", "0.5", "\n", "classes", "=", "F", ".", "softmax", "(", "classes", ")", "\n", "if", "not", "reconstruct", ":", "\n", "            ", "return", "classes", "\n", "", "if", "y", "is", "None", ":", "\n", "# In all batches, get the most active capsule.", "\n", "            ", "_", ",", "max_length_indices", "=", "classes", ".", "max", "(", "dim", "=", "1", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                ", "y", "=", "Variable", "(", "torch", ".", "sparse", ".", "torch", ".", "eye", "(", "self", ".", "label_size", ")", ")", ".", "cuda", "(", ")", ".", "index_select", "(", "dim", "=", "0", ",", "index", "=", "max_length_indices", ".", "data", ")", "\n", "", "else", ":", "\n", "                ", "y", "=", "Variable", "(", "torch", ".", "sparse", ".", "torch", ".", "eye", "(", "self", ".", "label_size", ")", ")", ".", "index_select", "(", "dim", "=", "0", ",", "index", "=", "max_length_indices", ".", "data", ")", "\n", "", "", "reconstructions", "=", "self", ".", "decoder", "(", "(", "x", "*", "y", "[", ":", ",", ":", ",", "None", "]", ")", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "\n", "return", "classes", ",", "reconstructions", "\n", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax": [[21, 25], ["input.transpose", "torch.softmax", "F.softmax.view().transpose", "input.transpose.contiguous().view", "len", "input.transpose.size", "F.softmax.view", "len", "input.size", "input.transpose.contiguous", "input.size", "input.transpose.size"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["def", "softmax", "(", "input", ",", "dim", "=", "1", ")", ":", "\n", "    ", "transposed_input", "=", "input", ".", "transpose", "(", "dim", ",", "len", "(", "input", ".", "size", "(", ")", ")", "-", "1", ")", "\n", "softmaxed_output", "=", "F", ".", "softmax", "(", "transposed_input", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "transposed_input", ".", "size", "(", "-", "1", ")", ")", ")", "\n", "return", "softmaxed_output", ".", "view", "(", "*", "transposed_input", ".", "size", "(", ")", ")", ".", "transpose", "(", "dim", ",", "len", "(", "input", ".", "size", "(", ")", ")", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.__init__": [[93, 148], ["BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.Linear", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.Parameter", "torch.Parameter", "torch.Parameter", "BaseBertModelAdv.AdvBaseModel.normalize_embedding"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.normalize_embedding"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "AdvBaseModel", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "opt", "=", "opt", "\n", "\n", "\"\"\"\n        # inverse embedding\n        print(\"making inverse embedding\")\n        \n        #inverse_embedding_weight = self.embedding.weight.detach().cpu()\n        #inverse_embedding_weight = inverse_embedding_weight.numpy()\n        #inverse_embedding_weight = np.matrix(inverse_embedding_weight)\n        #inverse_embedding_weight = np.linalg.pinv(inverse_embedding_weight)\n        #inverse_embedding_weight = torch.FloatTensor(inverse_embedding_weight)\n        \n        #self.inverse_embedding = nn.Embedding(self.embedding_dim, self.vocab_size + 2, ) \n        self.inverse_embedding = nn.Linear( self.vocab_size + 2,self.embedding_dim, bias=False)\n        \n        #self.inverse_embedding.weight=nn.Parameter(inverse_embedding_weight)            \n        #self.inverse_embedding.weight.requires_grad = False\n        \n        \n        # embd to embdnew\n        self.new_embedding_dim = self.embedding_dim\n        self.linear_transform_embd = nn.Linear(self.embedding_dim, self.new_embedding_dim)\n\n        # embdnew to embd\n        self.inverse_linear_transform_embd = nn.Linear(self.new_embedding_dim, self.embedding_dim)\n        #self.update_linear_transform_embd()\n        \"\"\"", "\n", "self", ".", "embedding_dim", "=", "opt", ".", "embedding_dim", "\n", "self", ".", "vocab_size", "=", "opt", ".", "vocab_size", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", "+", "2", ",", "opt", ".", "embedding_dim", ")", "\n", "if", "opt", ".", "use_pretrained_embeddings", ":", "\n", "            ", "self", ".", "embedding", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ")", "\n", "", "self", ".", "embedding", ".", "weight", ".", "requires_grad", "=", "True", "\n", "\n", "if", "self", ".", "opt", ".", "normalize_embedding", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "self", ".", "embedding", ".", "weight", "=", "nn", ".", "Parameter", "(", "self", ".", "normalize_embedding", "(", "self", ".", "embedding", ".", "weight", ",", "self", ".", "opt", ".", "vocab_freq", ")", ")", "\n", "\n", "", "", "self", ".", "label_size", "=", "opt", ".", "label_size", "\n", "\n", "self", ".", "embedding_out_dim", "=", "self", ".", "embedding_dim", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "self", ".", "embedding_out_dim", "=", "opt", ".", "embedding_out_dim", "\n", "self", ".", "linear_transform_embd_1", "=", "nn", ".", "Linear", "(", "self", ".", "embedding_dim", ",", "self", ".", "embedding_out_dim", ")", "\n", "#self.linear_transform_embd_2 = nn.Linear(self.latent_dim, self.latent_dim)", "\n", "#self.linear_transform_embd_3 = nn.Linear(self.latent_dim, self.embedding_dim)", "\n", "\n", "# l2 radius ", "\n", "", "self", ".", "word_synonym_radius", "=", "nn", ".", "Embedding", "(", "self", ".", "vocab_size", "+", "2", ",", "1", ")", "#, padding_idx=self.vocab_size + 1", "\n", "self", ".", "word_synonym_radius", ".", "weight", ".", "requires_grad", "=", "False", "\n", "self", ".", "word_synonym_radius", ".", "weight", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros_like", "(", "self", ".", "word_synonym_radius", ".", "weight", ")", ")", "\n", "\n", "self", ".", "eval_adv_mode", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_adv_by_convex_syn": [[150, 269], ["torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.empty().to().to", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.init.kaiming_normal_", "torch.empty().to().to.requires_grad_", "torch.empty().to().to.requires_grad_", "torch.empty().to().to.requires_grad_", "utils.getOptimizer", "embd.detach", "BaseBertModelAdv.AdvBaseModel.embd_to_logit", "range", "BaseBertModelAdv.AdvBaseModel.get_adv_by_convex_syn.get_comb_p"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.utils.getOptimizer", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit"], ["", "def", "get_adv_by_convex_syn", "(", "self", ",", "embd", ",", "y", ",", "syn", ",", "syn_valid", ",", "text_like_syn", ",", "attack_type_dict", ",", "text_for_vis", ",", "record_for_vis", ")", ":", "\n", "\n", "# record context", "\n", "        ", "self_training_context", "=", "self", ".", "training", "\n", "# set context", "\n", "if", "self", ".", "eval_adv_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "\n", "", "device", "=", "embd", ".", "device", "\n", "# get param of attacks", "\n", "\n", "num_steps", "=", "attack_type_dict", "[", "'num_steps'", "]", "\n", "loss_func", "=", "attack_type_dict", "[", "'loss_func'", "]", "\n", "w_optm_lr", "=", "attack_type_dict", "[", "'w_optm_lr'", "]", "\n", "sparse_weight", "=", "attack_type_dict", "[", "'sparse_weight'", "]", "\n", "out_type", "=", "attack_type_dict", "[", "'out_type'", "]", "\n", "\n", "batch_size", ",", "text_len", ",", "embd_dim", "=", "embd", ".", "shape", "\n", "batch_size", ",", "text_len", ",", "syn_num", ",", "embd_dim", "=", "syn", ".", "shape", "\n", "\n", "w", "=", "torch", ".", "empty", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", ".", "to", "(", "device", ")", ".", "to", "(", "embd", ".", "dtype", ")", "\n", "#ww = torch.zeros(batch_size, text_len, syn_num, 1).to(device).to(embd.dtype)", "\n", "#ww = ww+500*(syn_valid.reshape(batch_size, text_len, syn_num, 1)-1)", "\n", "nn", ".", "init", ".", "kaiming_normal_", "(", "w", ")", "\n", "w", ".", "requires_grad_", "(", ")", "\n", "\n", "import", "utils", "\n", "params", "=", "[", "w", "]", "\n", "optimizer", "=", "utils", ".", "getOptimizer", "(", "params", ",", "name", "=", "'adam'", ",", "lr", "=", "w_optm_lr", ",", "weight_decay", "=", "2e-5", ")", "\n", "\n", "def", "get_comb_p", "(", "w", ",", "syn_valid", ")", ":", "\n", "            ", "ww", "=", "w", "*", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "+", "500", "*", "(", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "-", "1", ")", "\n", "return", "F", ".", "softmax", "(", "ww", ",", "-", "2", ")", "\n", "\n", "", "def", "get_comb_ww", "(", "w", ",", "syn_valid", ")", ":", "\n", "            ", "ww", "=", "w", "*", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "+", "500", "*", "(", "syn_valid", ".", "reshape", "(", "batch_size", ",", "text_len", ",", "syn_num", ",", "1", ")", "-", "1", ")", "\n", "return", "ww", "\n", "\n", "", "def", "get_comb", "(", "p", ",", "syn", ")", ":", "\n", "            ", "return", "(", "p", "*", "syn", ".", "detach", "(", ")", ")", ".", "sum", "(", "-", "2", ")", "\n", "\n", "\n", "", "embd_ori", "=", "embd", ".", "detach", "(", ")", "\n", "logit_ori", "=", "self", ".", "embd_to_logit", "(", "embd_ori", ")", "\n", "\n", "for", "_", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "ww", "=", "get_comb_ww", "(", "w", ",", "syn_valid", ")", "\n", "#comb_p = get_comb_p(w, syn_valid)", "\n", "embd_adv", "=", "get_comb", "(", "F", ".", "softmax", "(", "ww", ",", "-", "2", ")", ",", "syn", ")", "\n", "if", "loss_func", "==", "'ce'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_adv", ")", "\n", "loss", "=", "-", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "elif", "loss_func", "==", "'kl'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_adv", ")", "\n", "criterion_kl", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "loss", "=", "-", "criterion_kl", "(", "F", ".", "log_softmax", "(", "logit_adv", ",", "dim", "=", "1", ")", ",", "\n", "F", ".", "softmax", "(", "logit_ori", ".", "detach", "(", ")", ",", "dim", "=", "1", ")", ")", "\n", "\n", "#print(\"ad loss:\", loss.data.item())", "\n", "\n", "", "if", "sparse_weight", "!=", "0", ":", "\n", "#loss_sparse = (comb_p*comb_p).mean()", "\n", "                    ", "loss_sparse", "=", "(", "-", "F", ".", "softmax", "(", "ww", ",", "-", "2", ")", "*", "F", ".", "log_softmax", "(", "ww", ",", "-", "2", ")", ")", ".", "sum", "(", "-", "2", ")", ".", "mean", "(", ")", "\n", "#loss -= sparse_weight*loss_sparse", "\n", "\n", "loss", "=", "loss", "+", "sparse_weight", "*", "loss_sparse", "\n", "#print(loss_sparse.data.item())", "\n", "\n", "#loss*=1000", "\n", "", "", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "#print((ww-w).max())", "\n", "\n", "", "comb_p", "=", "get_comb_p", "(", "w", ",", "syn_valid", ")", "\n", "\n", "if", "self", ".", "opt", ".", "vis_w_key_token", "is", "not", "None", ":", "\n", "            ", "assert", "(", "text_for_vis", "is", "not", "None", "and", "record_for_vis", "is", "not", "None", ")", "\n", "vis_n", ",", "vis_l", "=", "text_for_vis", ".", "shape", "\n", "for", "i", "in", "range", "(", "vis_n", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "vis_l", ")", ":", "\n", "                    ", "if", "text_for_vis", "[", "i", ",", "j", "]", "==", "self", ".", "opt", ".", "vis_w_key_token", ":", "\n", "                        ", "record_for_vis", "[", "\"comb_p_list\"", "]", ".", "append", "(", "comb_p", "[", "i", ",", "j", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "record_for_vis", "[", "\"embd_syn_list\"", "]", ".", "append", "(", "syn", "[", "i", ",", "j", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "record_for_vis", "[", "\"syn_valid_list\"", "]", ".", "append", "(", "syn_valid", "[", "i", ",", "j", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "record_for_vis", "[", "\"text_syn_list\"", "]", ".", "append", "(", "text_like_syn", "[", "i", ",", "j", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "print", "(", "\"record for vis\"", ",", "len", "(", "record_for_vis", "[", "\"comb_p_list\"", "]", ")", ")", "\n", "", "if", "len", "(", "record_for_vis", "[", "\"comb_p_list\"", "]", ")", ">=", "300", ":", "\n", "                        ", "dir_name", "=", "self", ".", "opt", ".", "resume", ".", "split", "(", "self", ".", "opt", ".", "model", ")", "[", "0", "]", "\n", "file_name", "=", "self", ".", "opt", ".", "dataset", "+", "\"_vis_w_\"", "+", "str", "(", "self", ".", "opt", ".", "attack_sparse_weight", ")", "+", "\"_\"", "+", "str", "(", "self", ".", "opt", ".", "vis_w_key_token", ")", "+", "\".pkl\"", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "dir_name", ",", "file_name", ")", "\n", "f", "=", "open", "(", "file_name", ",", "'wb'", ")", "\n", "pickle", ".", "dump", "(", "record_for_vis", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "\n", "\n", "", "", "", "", "if", "out_type", "==", "\"text\"", ":", "\n", "# need to be fix, has potential bugs. the trigger dependes on data.", "\n", "            ", "assert", "(", "text_like_syn", "is", "not", "None", ")", "# n l synlen", "\n", "comb_p", "=", "comb_p", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "syn_num", ")", "\n", "ind", "=", "comb_p", ".", "max", "(", "-", "1", ")", "[", "1", "]", "# shape batch_size* text_len", "\n", "out", "=", "(", "text_like_syn", ".", "reshape", "(", "batch_size", "*", "text_len", ",", "syn_num", ")", "[", "np", ".", "arange", "(", "batch_size", "*", "text_len", ")", ",", "ind", "]", ")", ".", "reshape", "(", "batch_size", ",", "text_len", ")", "\n", "", "elif", "out_type", "==", "\"comb_p\"", ":", "\n", "            ", "out", "=", "comb_p", "\n", "\n", "# resume context", "\n", "", "if", "self_training_context", "==", "True", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "\n", "", "return", "out", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_onehot_from_input": [[270, 278], ["torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "x.reshape", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "get_onehot_from_input", "(", "self", ",", "x", ")", ":", "\n", "        ", "embd_voc_dim", ",", "embd_dim", "=", "self", ".", "embedding", ".", "weight", ".", "shape", "\n", "\n", "bs", ",", "text_len", "=", "x", ".", "shape", "\n", "\n", "out", "=", "torch", ".", "zeros", "(", "bs", "*", "text_len", ",", "embd_voc_dim", ")", ".", "to", "(", "torch", ".", "float32", ")", ".", "to", "(", "x", ".", "device", ")", ".", "scatter_", "(", "1", ",", "x", ".", "reshape", "(", "bs", "*", "text_len", ",", "1", ")", ",", "1", ")", "\n", "\n", "return", "out", ".", "reshape", "(", "bs", ",", "text_len", ",", "embd_voc_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_onehot_mask_from_syn": [[279, 288], ["torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "torch.zeros().to().to().scatter_.reshape", "syn.reshape", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "get_onehot_mask_from_syn", "(", "self", ",", "syn_valid", ",", "syn", ")", ":", "\n", "        ", "embd_voc_dim", ",", "embd_dim", "=", "self", ".", "embedding", ".", "weight", ".", "shape", "\n", "\n", "bs", ",", "text_len", ",", "syn_max_num", "=", "syn", ".", "shape", "\n", "\n", "out", "=", "torch", ".", "zeros", "(", "bs", "*", "text_len", ",", "embd_voc_dim", ")", ".", "to", "(", "torch", ".", "float32", ")", ".", "to", "(", "syn", ".", "device", ")", ".", "scatter_", "(", "1", ",", "syn", ".", "reshape", "(", "bs", "*", "text_len", ",", "syn_max_num", ")", ",", "1", ")", "\n", "out", "[", ":", ",", "0", "]", "=", "0", "\n", "\n", "return", "out", ".", "reshape", "(", "bs", ",", "text_len", ",", "embd_voc_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_embd_from_onehot": [[289, 298], ["torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "BaseBertModelAdv.AdvBaseModel.reshape", "onehot_input.reshape", "BaseBertModelAdv.AdvBaseModel.linear_transform_embd_1"], "methods", ["None"], ["", "def", "get_embd_from_onehot", "(", "self", ",", "onehot_input", ")", ":", "\n", "        ", "w", "=", "self", ".", "embedding", ".", "weight", "\n", "bs", ",", "text_len", ",", "voc_d", "=", "onehot_input", ".", "shape", "\n", "embd", "=", "torch", ".", "mm", "(", "onehot_input", ".", "reshape", "(", "bs", "*", "text_len", ",", "voc_d", ")", ",", "w", ")", "\n", "embd", "=", "embd", ".", "reshape", "(", "bs", ",", "text_len", ",", "-", "1", ")", "\n", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "embd", "=", "self", ".", "linear_transform_embd_1", "(", "embd", ")", "\n", "", "return", "embd", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_embd_adv": [[300, 383], ["len", "range", "embd_ori.detach.detach", "BaseBertModelAdv.AdvBaseModel.eval", "BaseBertModelAdv.AdvBaseModel.train", "embd_ori.detach", "embd_ori.detach.requires_grad_", "BaseBertModelAdv.AdvBaseModel.train", "BaseBertModelAdv.AdvBaseModel.eval", "embd_ori.detach", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "torch.enable_grad", "BaseBertModelAdv.AdvBaseModel.l2_project_sent", "BaseBertModelAdv.AdvBaseModel.l2_clip_sent", "embd_ori.detach", "BaseBertModelAdv.AdvBaseModel.detach", "embd_ori.detach.detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "torch.randn().to().detach", "BaseBertModelAdv.AdvBaseModel.embd_to_logit", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "embd_ori.detach.detach", "BaseBertModelAdv.AdvBaseModel.l2_project", "BaseBertModelAdv.AdvBaseModel.l2_clip", "BaseBertModelAdv.AdvBaseModel.embd_to_logit", "BaseBertModelAdv.AdvBaseModel.embd_to_logit", "torch.KLDivLoss", "torch.KLDivLoss", "torch.KLDivLoss", "torch.KLDivLoss.", "BaseBertModelAdv.AdvBaseModel.detach", "embd_ori.detach.detach", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.randn().to", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "BaseBertModelAdv.AdvBaseModel.detach", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.None.train_imdb_ascc.train", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_project_sent", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_clip_sent", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_project", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.l2_clip", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "get_embd_adv", "(", "self", ",", "embd", ",", "y", ",", "attack_type_dict", ")", ":", "\n", "\n", "# record context", "\n", "        ", "self_training_context", "=", "self", ".", "training", "\n", "# set context", "\n", "if", "self", ".", "eval_adv_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "\n", "", "device", "=", "embd", ".", "device", "\n", "# get param of attacks", "\n", "\n", "num_steps", "=", "attack_type_dict", "[", "'num_steps'", "]", "\n", "step_size", "=", "attack_type_dict", "[", "'step_size'", "]", "\n", "random_start", "=", "attack_type_dict", "[", "'random_start'", "]", "\n", "epsilon", "=", "attack_type_dict", "[", "'epsilon'", "]", "\n", "loss_func", "=", "attack_type_dict", "[", "'loss_func'", "]", "\n", "direction", "=", "attack_type_dict", "[", "'direction'", "]", "\n", "ball_range", "=", "attack_type_dict", "[", "'ball_range'", "]", "\n", "\n", "batch_size", "=", "len", "(", "embd", ")", "\n", "\n", "embd_ori", "=", "embd", "\n", "\n", "# random start", "\n", "if", "random_start", ":", "\n", "            ", "embd_adv", "=", "embd_ori", ".", "detach", "(", ")", "+", "0.001", "*", "torch", ".", "randn", "(", "embd_ori", ".", "shape", ")", ".", "to", "(", "device", ")", ".", "detach", "(", ")", "\n", "", "else", ":", "\n", "            ", "embd_adv", "=", "embd_ori", ".", "detach", "(", ")", "\n", "\n", "\n", "", "for", "_", "in", "range", "(", "num_steps", ")", ":", "\n", "            ", "embd_adv", ".", "requires_grad_", "(", ")", "\n", "grad", "=", "0", "\n", "with", "torch", ".", "enable_grad", "(", ")", ":", "\n", "                ", "if", "loss_func", "==", "'ce'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_adv", ")", "\n", "if", "direction", "==", "\"towards\"", ":", "\n", "                        ", "loss", "=", "-", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "elif", "direction", "==", "\"away\"", ":", "\n", "                        ", "loss", "=", "F", ".", "cross_entropy", "(", "logit_adv", ",", "y", ",", "reduction", "=", "'sum'", ")", "\n", "", "", "elif", "loss_func", "==", "'kl'", ":", "\n", "                    ", "logit_adv", "=", "self", ".", "embd_to_logit", "(", "embd_adv", ")", "\n", "logit_ori", "=", "self", ".", "embd_to_logit", "(", "embd_ori", ")", "\n", "assert", "(", "direction", "==", "\"away\"", ")", "\n", "\n", "criterion_kl", "=", "nn", ".", "KLDivLoss", "(", "reduction", "=", "\"sum\"", ")", "\n", "loss", "=", "criterion_kl", "(", "F", ".", "log_softmax", "(", "logit_adv", ",", "dim", "=", "1", ")", ",", "\n", "F", ".", "softmax", "(", "logit_ori", ",", "dim", "=", "1", ")", ")", "\n", "\n", "", "grad", "=", "torch", ".", "autograd", ".", "grad", "(", "loss", ",", "[", "embd_adv", "]", ")", "[", "0", "]", "\n", "\n", "", "if", "ball_range", "==", "'sentence'", ":", "\n", "                ", "grad", "=", "self", ".", "l2_project_sent", "(", "grad", ")", "\n", "embd_adv", "=", "embd_adv", ".", "detach", "(", ")", "+", "step_size", "*", "grad", ".", "detach", "(", ")", "\n", "perturbation", "=", "self", ".", "l2_clip_sent", "(", "embd_adv", "-", "embd_ori", ",", "epsilon", ")", "\n", "", "elif", "ball_range", "==", "'word'", ":", "\n", "                ", "grad", "=", "self", ".", "l2_project", "(", "grad", ")", "\n", "embd_adv", "=", "embd_adv", ".", "detach", "(", ")", "+", "step_size", "*", "grad", ".", "detach", "(", ")", "\n", "perturbation", "=", "self", ".", "l2_clip", "(", "embd_adv", "-", "embd_ori", ",", "epsilon", ")", "\n", "", "else", ":", "\n", "                ", "assert", "NotImplementedError", "\n", "\n", "", "embd_adv", "=", "embd_ori", ".", "detach", "(", ")", "+", "perturbation", ".", "detach", "(", ")", "\n", "\n", "# resume context", "\n", "", "if", "self_training_context", "==", "True", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "\n", "", "class", "bpda_get_embd_adv", "(", "torch", ".", "autograd", ".", "Function", ")", ":", "\n", "\n", "            ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "embd", ")", ":", "\n", "                ", "return", "embd_adv", ".", "detach", "(", ")", "\n", "", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "# BPDA, approximate gradients", "\n", "                ", "return", "grad_output", "\n", "\n", "", "", "return", "embd_adv", ".", "detach", "(", ")", "\n", "#return bpda_get_embd_adv.apply(embd)", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.embd_to_embdnew": [[386, 389], ["BaseBertModelAdv.AdvBaseModel.linear_transform_embd"], "methods", ["None"], ["", "def", "embd_to_embdnew", "(", "self", ",", "embd", ")", ":", "\n", "        ", "embdnew", "=", "self", ".", "linear_transform_embd", "(", "embd", ")", "\n", "return", "embdnew", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.embd_to_text": [[390, 394], ["embd.reshape().mm", "embd.reshape().mm.reshape", "embd.reshape"], "methods", ["None"], ["", "def", "embd_to_text", "(", "self", ",", "embd", ")", ":", "\n", "        ", "bs", ",", "sen_len", ",", "embd_dim", "=", "embd", ".", "shape", "\n", "text", "=", "embd", ".", "reshape", "(", "-", "1", ",", "embd_dim", ")", ".", "mm", "(", "self", ".", "inverse_embedding", ".", "weight", ")", "\n", "return", "text", ".", "reshape", "(", "bs", ",", "sen_len", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.embdnew_to_embd": [[395, 398], ["BaseBertModelAdv.AdvBaseModel.inverse_linear_transform_embd"], "methods", ["None"], ["", "def", "embdnew_to_embd", "(", "self", ",", "embdnew", ")", ":", "\n", "        ", "embd", "=", "self", ".", "inverse_linear_transform_embd", "(", "embdnew", ")", "\n", "return", "embd", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.loss_text_adv": [[399, 404], ["torch.cross_entropy", "torch.cross_entropy", "torch.cross_entropy", "input.reshape", "label.reshape"], "methods", ["None"], ["", "def", "loss_text_adv", "(", "self", ",", "input", ",", "label", ")", ":", "\n", "#p = -F.log_softmax(input, dim=-1)", "\n", "#loss = p*(label.to(p.dtype)).sum()", "\n", "        ", "input_shape", "=", "input", ".", "shape", "\n", "return", "F", ".", "cross_entropy", "(", "input", ".", "reshape", "(", "-", "1", ",", "input_shape", "[", "-", "1", "]", ")", ",", "label", ".", "reshape", "(", "-", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_radius": [[405, 408], ["BaseBertModelAdv.AdvBaseModel.word_synonym_radius", "BaseBertModelAdv.AdvBaseModel.detach"], "methods", ["None"], ["", "def", "text_to_radius", "(", "self", ",", "inp", ")", ":", "\n", "        ", "saved_radius", "=", "self", ".", "word_synonym_radius", "(", "inp", ")", "# n, len, 1", "\n", "return", "saved_radius", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd": [[409, 418], ["BaseBertModelAdv.AdvBaseModel.embedding", "BaseBertModelAdv.AdvBaseModel.linear_transform_embd_1"], "methods", ["None"], ["", "def", "text_to_embd", "(", "self", ",", "inp", ")", ":", "\n", "        ", "x", "=", "self", ".", "embedding", "(", "inp", ")", "\n", "if", "self", ".", "opt", ".", "embd_transform", ":", "\n", "            ", "x", "=", "self", ".", "linear_transform_embd_1", "(", "x", ")", "\n", "#x = F.relu(x)", "\n", "#x = self.linear_transform_embd_2(x)", "\n", "#x = F.relu(x)", "\n", "#x = self.linear_transform_embd_3(x)", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.forward": [[419, 464], ["BaseBertModelAdv.AdvBaseModel.get_embd_adv", "BaseBertModelAdv.AdvBaseModel.get_adv_by_convex_syn", "BaseBertModelAdv.AdvBaseModel.embd_to_logit", "BaseBertModelAdv.AdvBaseModel.text_to_embd", "BaseBertModelAdv.AdvBaseModel.text_to_radius", "BaseBertModelAdv.AdvBaseModel.text_to_embd", "BaseBertModelAdv.AdvBaseModel.embd_to_logit", "BaseBertModelAdv.AdvBaseModel.text_to_embd().reshape", "BaseBertModelAdv.AdvBaseModel.embd_to_logit", "BaseBertModelAdv.AdvBaseModel.embd_to_embdnew", "BaseBertModelAdv.AdvBaseModel.embd_to_text", "BaseBertModelAdv.AdvBaseModel.embdnew_to_embd", "BaseBertModelAdv.AdvBaseModel.update_inverse_linear_transform_embd", "BaseBertModelAdv.AdvBaseModel.update_linear_transform_embd", "BaseBertModelAdv.AdvBaseModel.loss_text_adv", "BaseBertModelAdv.AdvBaseModel.loss_radius", "BaseBertModelAdv.AdvBaseModel.text_to_embd", "input.reshape"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_embd_adv", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.get_adv_by_convex_syn", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_radius", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.ForSnli.AdvEntailmentCNN.embd_to_logit", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.embd_to_embdnew", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.embd_to_text", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.embdnew_to_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.update_inverse_linear_transform_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.update_linear_transform_embd", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.loss_text_adv", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseModelAdv.AdvBaseModel.loss_radius", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.BaseBertModelAdv.AdvBaseModel.text_to_embd"], ["", "def", "forward", "(", "self", ",", "mode", ",", "input", ",", "comb_p", "=", "None", ",", "label", "=", "None", ",", "text_like_syn_embd", "=", "None", ",", "text_like_syn_valid", "=", "None", ",", "text_like_syn", "=", "None", ",", "attack_type_dict", "=", "None", ",", "bert_mask", "=", "None", ",", "bert_token_id", "=", "None", ",", "text_for_vis", "=", "None", ",", "record_for_vis", "=", "None", ")", ":", "\n", "        ", "if", "mode", "==", "\"get_embd_adv\"", ":", "\n", "            ", "assert", "(", "attack_type_dict", "is", "not", "None", ")", "\n", "out", "=", "self", ".", "get_embd_adv", "(", "input", ",", "label", ",", "attack_type_dict", ")", "\n", "", "if", "mode", "==", "\"get_adv_by_convex_syn\"", ":", "\n", "            ", "assert", "(", "attack_type_dict", "is", "not", "None", ")", "\n", "assert", "(", "text_like_syn_embd", "is", "not", "None", ")", "\n", "assert", "(", "text_like_syn_valid", "is", "not", "None", ")", "\n", "out", "=", "self", ".", "get_adv_by_convex_syn", "(", "input", ",", "label", ",", "text_like_syn_embd", ",", "text_like_syn_valid", ",", "text_like_syn", ",", "attack_type_dict", ",", "text_for_vis", ",", "record_for_vis", ")", "\n", "", "if", "mode", "==", "\"embd_to_logit\"", ":", "\n", "            ", "out", "=", "self", ".", "embd_to_logit", "(", "input", ")", "\n", "", "if", "mode", "==", "\"text_to_embd\"", ":", "\n", "            ", "out", "=", "self", ".", "text_to_embd", "(", "input", ")", "\n", "", "if", "mode", "==", "\"text_to_radius\"", ":", "\n", "            ", "out", "=", "self", ".", "text_to_radius", "(", "input", ")", "\n", "", "if", "mode", "==", "\"text_to_logit\"", ":", "\n", "            ", "embd", "=", "self", ".", "text_to_embd", "(", "input", ")", "\n", "#embd = self.embd_to_embdnew(embd)", "\n", "out", "=", "self", ".", "embd_to_logit", "(", "embd", ")", "\n", "", "if", "mode", "==", "\"text_syn_p_to_logit\"", ":", "\n", "            ", "assert", "(", "comb_p", "is", "not", "None", ")", "\n", "bs", ",", "tl", ",", "sl", "=", "input", ".", "shape", "\n", "text_like_syn_embd", "=", "self", ".", "text_to_embd", "(", "input", ".", "reshape", "(", "bs", ",", "tl", "*", "sl", ")", ")", ".", "reshape", "(", "bs", ",", "tl", ",", "sl", ",", "-", "1", ")", "\n", "embd", "=", "(", "comb_p", "*", "text_like_syn_embd", ")", ".", "sum", "(", "-", "2", ")", "\n", "out", "=", "self", ".", "embd_to_logit", "(", "embd", ")", "\n", "", "if", "mode", "==", "\"embd_to_embdnew\"", ":", "\n", "            ", "out", "=", "self", ".", "embd_to_embdnew", "(", "input", ")", "\n", "", "if", "mode", "==", "\"embd_to_text\"", ":", "\n", "            ", "out", "=", "self", ".", "embd_to_text", "(", "input", ")", "\n", "", "if", "mode", "==", "\"embdnew_to_embd\"", ":", "\n", "            ", "out", "=", "self", ".", "embdnew_to_embd", "(", "input", ")", "\n", "", "if", "mode", "==", "\"update_inverse_linear_transform_embd\"", ":", "\n", "            ", "self", ".", "update_inverse_linear_transform_embd", "(", ")", "\n", "out", "=", "None", "\n", "", "if", "mode", "==", "\"update_linear_transform_embd\"", ":", "\n", "            ", "self", ".", "update_linear_transform_embd", "(", ")", "\n", "out", "=", "None", "\n", "", "if", "mode", "==", "\"loss_text_adv\"", ":", "\n", "            ", "out", "=", "self", ".", "loss_text_adv", "(", "input", ",", "label", ")", "\n", "", "if", "mode", "==", "\"loss_radius\"", ":", "\n", "            ", "out", "=", "self", ".", "loss_radius", "(", "input", ",", "label", ")", "\n", "\n", "\n", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.__init__.setup": [[15, 28], ["CNNBasic.AdvBasicCNN1D", "ForSnli.AdvDecAtt_FromCert", "ForSnli.AdvBOW", "LSTMBI.AdvLSTMBI", "Exception"], "function", ["None"], []], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNMultiLayer.MultiLayerCNN.__init__": [[11, 52], ["models.BaseModel.BaseModel.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "opt.__dict__.get", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.MaxPool1d", "torch.MaxPool1d", "torch.MaxPool1d"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", "MultiLayerCNN", ",", "self", ")", ".", "__init__", "(", "opt", ")", "\n", "self", ".", "embed", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", "+", "1", ",", "opt", ".", "embedding_dim", ")", "\n", "\n", "if", "opt", ".", "__dict__", ".", "get", "(", "\"embeddings\"", ",", "None", ")", "is", "not", "None", ":", "\n", "            ", "self", ".", "embed", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "\n", "", "self", ".", "conv1", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "opt", ".", "max_seq_len", ",", "256", ",", "kernel_size", "=", "7", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "3", ",", "stride", "=", "3", ")", "\n", ")", "\n", "\n", "self", ".", "conv2", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "7", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "3", ",", "stride", "=", "3", ")", "\n", ")", "\n", "\n", "self", ".", "conv3", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "conv4", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "conv5", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "self", ".", "conv6", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "256", ",", "256", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "MaxPool1d", "(", "kernel_size", "=", "3", ",", "stride", "=", "3", ")", "\n", ")", "\n", "\n", "self", ".", "fc", "=", "nn", ".", "Linear", "(", "256", "*", "7", ",", "opt", ".", "label_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.CNNMultiLayer.MultiLayerCNN.forward": [[53, 68], ["CNNMultiLayer.MultiLayerCNN.embed", "CNNMultiLayer.MultiLayerCNN.conv1", "CNNMultiLayer.MultiLayerCNN.conv2", "CNNMultiLayer.MultiLayerCNN.conv3", "CNNMultiLayer.MultiLayerCNN.conv4", "CNNMultiLayer.MultiLayerCNN.conv5", "CNNMultiLayer.MultiLayerCNN.conv6", "CNNMultiLayer.MultiLayerCNN.view", "CNNMultiLayer.MultiLayerCNN.fc", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "CNNMultiLayer.MultiLayerCNN.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Embedding", "\n", "        ", "x", "=", "self", ".", "embed", "(", "x", ")", "# dim: (batch_size, max_seq_len, embedding_size)", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "self", ".", "conv4", "(", "x", ")", "\n", "x", "=", "self", ".", "conv5", "(", "x", ")", "\n", "x", "=", "self", ".", "conv6", "(", "x", ")", "\n", "\n", "# collapse", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "\n", "return", "F", ".", "log_softmax", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.AttrProxy.__init__": [[27, 30], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "module", ",", "prefix", ")", ":", "\n", "        ", "self", ".", "module", "=", "module", "\n", "self", ".", "prefix", "=", "prefix", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.AttrProxy.__getitem__": [[31, 33], ["getattr", "str"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "getattr", "(", "self", ".", "module", ",", "self", ".", "prefix", "+", "str", "(", "i", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.MemN2N.__init__": [[36, 57], ["torch.Module.__init__", "range", "MemoryNetwork.AttrProxy", "torch.Softmax", "torch.Softmax", "torch.Softmax", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding.weight.data.normal_", "MemoryNetwork.MemN2N.add_module", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "MemoryNetwork.MemN2N.encoding.cuda", "MemoryNetwork.position_encoding"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.position_encoding"], ["    ", "def", "__init__", "(", "self", ",", "settings", ")", ":", "\n", "        ", "super", "(", "MemN2N", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "use_cuda", "=", "settings", "[", "\"use_cuda\"", "]", "\n", "num_vocab", "=", "settings", "[", "\"num_vocab\"", "]", "\n", "embedding_dim", "=", "settings", "[", "\"embedding_dim\"", "]", "\n", "sentence_size", "=", "settings", "[", "\"sentence_size\"", "]", "\n", "self", ".", "max_hops", "=", "settings", "[", "\"max_hops\"", "]", "\n", "\n", "for", "hop", "in", "range", "(", "self", ".", "max_hops", "+", "1", ")", ":", "\n", "            ", "C", "=", "nn", ".", "Embedding", "(", "num_vocab", ",", "embedding_dim", ",", "padding_idx", "=", "0", ")", "\n", "C", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "0.1", ")", "\n", "self", ".", "add_module", "(", "\"C_{}\"", ".", "format", "(", "hop", ")", ",", "C", ")", "\n", "", "self", ".", "C", "=", "AttrProxy", "(", "self", ",", "\"C_\"", ")", "\n", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", ")", "\n", "self", ".", "encoding", "=", "Variable", "(", "torch", ".", "FloatTensor", "(", "\n", "position_encoding", "(", "sentence_size", ",", "embedding_dim", ")", ")", ",", "requires_grad", "=", "False", ")", "\n", "\n", "if", "use_cuda", ":", "\n", "            ", "self", ".", "encoding", "=", "self", ".", "encoding", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.MemN2N.forward": [[58, 92], ["story.size", "list", "MemoryNetwork.MemN2N.encoding.unsqueeze().expand_as", "list.append", "range", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "embed_A.view.view.view", "MemoryNetwork.MemN2N.encoding.unsqueeze().unsqueeze().expand_as", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "u[].unsqueeze().expand_as", "MemoryNetwork.MemN2N.softmax", "embed_C.view.view.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "prob.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "list.append", "MemoryNetwork.MemN2N.C[].weight.transpose", "MemoryNetwork.MemN2N.softmax", "MemoryNetwork.MemN2N.encoding.unsqueeze", "story.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "story.view", "story.size", "MemoryNetwork.MemN2N.encoding.unsqueeze().unsqueeze", "u[].unsqueeze", "story.size", "prob.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "embed_A.view.view.size", "embed_C.view.view.size", "MemoryNetwork.MemN2N.encoding.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "", "def", "forward", "(", "self", ",", "query", ")", ":", "\n", "\n", "        ", "story", "=", "query", "# for text classfication", "\n", "\n", "story_size", "=", "story", ".", "size", "(", ")", "\n", "\n", "u", "=", "list", "(", ")", "\n", "query_embed", "=", "self", ".", "C", "[", "0", "]", "(", "query", ")", "\n", "# weired way to perform reduce_dot", "\n", "encoding", "=", "self", ".", "encoding", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "query_embed", ")", "\n", "u", ".", "append", "(", "torch", ".", "sum", "(", "query_embed", "*", "encoding", ",", "1", ")", ")", "\n", "\n", "for", "hop", "in", "range", "(", "self", ".", "max_hops", ")", ":", "\n", "            ", "embed_A", "=", "self", ".", "C", "[", "hop", "]", "(", "story", ".", "view", "(", "story", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "embed_A", "=", "embed_A", ".", "view", "(", "story_size", "+", "(", "embed_A", ".", "size", "(", "-", "1", ")", ",", ")", ")", "\n", "\n", "encoding", "=", "self", ".", "encoding", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "embed_A", ")", "\n", "m_A", "=", "torch", ".", "sum", "(", "embed_A", "*", "encoding", ",", "2", ")", "\n", "\n", "u_temp", "=", "u", "[", "-", "1", "]", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "m_A", ")", "\n", "prob", "=", "self", ".", "softmax", "(", "torch", ".", "sum", "(", "m_A", "*", "u_temp", ",", "2", ")", ")", "\n", "\n", "embed_C", "=", "self", ".", "C", "[", "hop", "+", "1", "]", "(", "story", ".", "view", "(", "story", ".", "size", "(", "0", ")", ",", "-", "1", ")", ")", "\n", "embed_C", "=", "embed_C", ".", "view", "(", "story_size", "+", "(", "embed_C", ".", "size", "(", "-", "1", ")", ",", ")", ")", "\n", "m_C", "=", "torch", ".", "sum", "(", "embed_C", "*", "encoding", ",", "2", ")", "\n", "\n", "prob", "=", "prob", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "m_C", ")", "\n", "o_k", "=", "torch", ".", "sum", "(", "m_C", "*", "prob", ",", "1", ")", "\n", "\n", "u_k", "=", "u", "[", "-", "1", "]", "+", "o_k", "\n", "u", ".", "append", "(", "u_k", ")", "\n", "\n", "", "a_hat", "=", "u", "[", "-", "1", "]", "@", "self", ".", "C", "[", "self", ".", "max_hops", "]", ".", "weight", ".", "transpose", "(", "0", ",", "1", ")", "\n", "return", "a_hat", ",", "self", ".", "softmax", "(", "a_hat", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.MemoryNetwork.position_encoding": [[9, 20], ["numpy.ones", "range", "numpy.transpose", "range"], "function", ["None"], ["def", "position_encoding", "(", "sentence_size", ",", "embedding_dim", ")", ":", "\n", "    ", "encoding", "=", "np", ".", "ones", "(", "(", "embedding_dim", ",", "sentence_size", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "ls", "=", "sentence_size", "+", "1", "\n", "le", "=", "embedding_dim", "+", "1", "\n", "for", "i", "in", "range", "(", "1", ",", "le", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "1", ",", "ls", ")", ":", "\n", "            ", "encoding", "[", "i", "-", "1", ",", "j", "-", "1", "]", "=", "(", "i", "-", "(", "embedding_dim", "+", "1", ")", "/", "2", ")", "*", "(", "j", "-", "(", "sentence_size", "+", "1", ")", "/", "2", ")", "\n", "", "", "encoding", "=", "1", "+", "4", "*", "encoding", "/", "embedding_dim", "/", "sentence_size", "\n", "# Make position encoding of time words identity to avoid modifying them", "\n", "encoding", "[", ":", ",", "-", "1", "]", "=", "1.0", "\n", "return", "np", ".", "transpose", "(", "encoding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.__init__": [[14, 35], ["torch.Module.__init__", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.Linear", "torch.Linear", "torch.Linear", "SelfAttention.SelfAttention.init_hidden", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "opt", "=", "opt", "\n", "super", "(", "SelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_dim", "=", "opt", ".", "hidden_dim", "\n", "self", ".", "batch_size", "=", "opt", ".", "batch_size", "\n", "self", ".", "use_gpu", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "opt", ".", "vocab_size", ",", "opt", ".", "embedding_dim", ")", "\n", "self", ".", "word_embeddings", ".", "weight", "=", "nn", ".", "Parameter", "(", "opt", ".", "embeddings", ",", "requires_grad", "=", "opt", ".", "embedding_training", ")", "\n", "#        self.word_embeddings.weight.data.copy_(torch.from_numpy(opt.embeddings))", "\n", "\n", "self", ".", "num_layers", "=", "1", "\n", "#self.bidirectional = True", "\n", "self", ".", "dropout", "=", "opt", ".", "keep_dropout", "\n", "self", ".", "bilstm", "=", "nn", ".", "LSTM", "(", "opt", ".", "embedding_dim", ",", "opt", ".", "hidden_dim", "//", "2", ",", "num_layers", "=", "self", ".", "num_layers", ",", "dropout", "=", "self", ".", "dropout", ",", "bidirectional", "=", "True", ")", "\n", "self", ".", "hidden2label", "=", "nn", ".", "Linear", "(", "opt", ".", "hidden_dim", ",", "opt", ".", "label_size", ")", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", ")", "\n", "self", ".", "self_attention", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "opt", ".", "hidden_dim", ",", "24", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", ",", "\n", "nn", ".", "Linear", "(", "24", ",", "1", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden": [[36, 47], ["torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "        ", "if", "batch_size", "is", "None", ":", "\n", "            ", "batch_size", "=", "self", ".", "batch_size", "\n", "\n", "", "if", "self", ".", "use_gpu", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ".", "cuda", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "h0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ")", "\n", "c0", "=", "Variable", "(", "torch", ".", "zeros", "(", "2", "*", "self", ".", "num_layers", ",", "batch_size", ",", "self", ".", "hidden_dim", "//", "2", ")", ")", "\n", "", "return", "(", "h0", ",", "c0", ")", "\n", "#    @profile", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.forward": [[48, 62], ["SelfAttention.SelfAttention.word_embeddings", "SelfAttention.SelfAttention.permute", "SelfAttention.SelfAttention.init_hidden", "SelfAttention.SelfAttention.bilstm", "lstm_out.permute", "SelfAttention.SelfAttention.self_attention", "torch.softmax", "torch.softmax", "torch.softmax", "SelfAttention.SelfAttention.hidden2label", "SelfAttention.SelfAttention.view", "sentence.size"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.SelfAttention.SelfAttention.init_hidden", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.Capsule.softmax"], ["", "def", "forward", "(", "self", ",", "sentence", ")", ":", "\n", "        ", "embeds", "=", "self", ".", "word_embeddings", "(", "sentence", ")", "\n", "\n", "#        x = embeds.view(sentence.size()[1], self.batch_size, -1)", "\n", "x", "=", "embeds", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "\n", "self", ".", "hidden", "=", "self", ".", "init_hidden", "(", "sentence", ".", "size", "(", ")", "[", "0", "]", ")", "#2x64x64", "\n", "lstm_out", ",", "self", ".", "hidden", "=", "self", ".", "bilstm", "(", "x", ",", "self", ".", "hidden", ")", "#lstm_out:200x64x128", "\n", "final", "=", "lstm_out", ".", "permute", "(", "1", ",", "0", ",", "2", ")", "#torch.mean(,1) ", "\n", "attn_ene", "=", "self", ".", "self_attention", "(", "final", ")", "\n", "attns", "=", "F", ".", "softmax", "(", "attn_ene", ".", "view", "(", "self", ".", "batch_size", ",", "-", "1", ")", ")", "\n", "feats", "=", "(", "final", "*", "attns", ")", ".", "sum", "(", "dim", "=", "1", ")", "\n", "y", "=", "self", ".", "hidden2label", "(", "feats", ")", "#64x3", "\n", "\n", "return", "y", "", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.__init__": [[28, 37], ["ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "coeff", ",", "name", "=", "'weight'", ",", "n_power_iterations", "=", "1", ",", "dim", "=", "0", ",", "eps", "=", "1e-12", ")", ":", "\n", "        ", "self", ".", "coeff", "=", "coeff", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "dim", "=", "dim", "\n", "if", "n_power_iterations", "<=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "'Expected n_power_iterations to be positive, but '", "\n", "'got n_power_iterations={}'", ".", "format", "(", "n_power_iterations", ")", ")", "\n", "", "self", ".", "n_power_iterations", "=", "n_power_iterations", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.reshape_weight_to_matrix": [[38, 46], ["weight_mat.permute.permute.size", "weight_mat.permute.permute.reshape", "weight_mat.permute.permute.permute", "range", "weight_mat.permute.permute.dim"], "methods", ["None"], ["", "def", "reshape_weight_to_matrix", "(", "self", ",", "weight", ")", ":", "\n", "        ", "weight_mat", "=", "weight", "\n", "if", "self", ".", "dim", "!=", "0", ":", "\n", "# permute dim to front", "\n", "            ", "weight_mat", "=", "weight_mat", ".", "permute", "(", "self", ".", "dim", ",", "\n", "*", "[", "d", "for", "d", "in", "range", "(", "weight_mat", ".", "dim", "(", ")", ")", "if", "d", "!=", "self", ".", "dim", "]", ")", "\n", "", "height", "=", "weight_mat", ".", "size", "(", "0", ")", "\n", "return", "weight_mat", ".", "reshape", "(", "height", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.compute_weight": [[47, 105], ["getattr", "getattr", "getattr", "getattr", "spectral_norm_fc.SpectralNorm.reshape_weight_to_matrix", "torch.dot", "torch.max", "torch.dot.detach", "torch.max", "torch.mv", "torch.ones().to", "torch.ones().to", "torch.no_grad", "range", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "u.clone.clone.clone", "v.clone.clone.clone", "torch.ones", "torch.ones", "torch.mv", "torch.mv", "spectral_norm_fc.SpectralNorm.t"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.reshape_weight_to_matrix"], ["", "def", "compute_weight", "(", "self", ",", "module", ",", "do_power_iteration", ")", ":", "\n", "# NB: If `do_power_iteration` is set, the `u` and `v` vectors are", "\n", "#     updated in power iteration **in-place**. This is very important", "\n", "#     because in `DataParallel` forward, the vectors (being buffers) are", "\n", "#     broadcast from the parallelized module to each module replica,", "\n", "#     which is a new module object created on the fly. And each replica", "\n", "#     runs its own spectral norm power iteration. So simply assigning", "\n", "#     the updated vectors to the module this function runs on will cause", "\n", "#     the update to be lost forever. And the next time the parallelized", "\n", "#     module is replicated, the same randomly initialized vectors are", "\n", "#     broadcast and used!", "\n", "#", "\n", "#     Therefore, to make the change propagate back, we rely on two", "\n", "#     important bahaviors (also enforced via tests):", "\n", "#       1. `DataParallel` doesn't clone storage if the broadcast tensor", "\n", "#          is alreay on correct device; and it makes sure that the", "\n", "#          parallelized module is already on `device[0]`.", "\n", "#       2. If the out tensor in `out=` kwarg has correct shape, it will", "\n", "#          just fill in the values.", "\n", "#     Therefore, since the same power iteration is performed on all", "\n", "#     devices, simply updating the tensors in-place will make sure that", "\n", "#     the module replica on `device[0]` will update the _u vector on the", "\n", "#     parallized module (by shared storage).", "\n", "#", "\n", "#    However, after we update `u` and `v` in-place, we need to **clone**", "\n", "#    them before using them to normalize the weight. This is to support", "\n", "#    backproping through two forward passes, e.g., the common pattern in", "\n", "#    GAN training: loss = D(real) - D(fake). Otherwise, engine will", "\n", "#    complain that variables needed to do backward for the first forward", "\n", "#    (i.e., the `u` and `v` vectors) are changed in the second forward.", "\n", "        ", "weight", "=", "getattr", "(", "module", ",", "self", ".", "name", "+", "'_orig'", ")", "\n", "u", "=", "getattr", "(", "module", ",", "self", ".", "name", "+", "'_u'", ")", "\n", "v", "=", "getattr", "(", "module", ",", "self", ".", "name", "+", "'_v'", ")", "\n", "sigma_log", "=", "getattr", "(", "module", ",", "self", ".", "name", "+", "'_sigma'", ")", "# for logging", "\n", "weight_mat", "=", "self", ".", "reshape_weight_to_matrix", "(", "weight", ")", "\n", "\n", "if", "do_power_iteration", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "for", "_", "in", "range", "(", "self", ".", "n_power_iterations", ")", ":", "\n", "# Spectral norm of weight equals to `u^T W v`, where `u` and `v`", "\n", "# are the first left and right singular vectors.", "\n", "# This power iteration produces approximations of `u` and `v`.", "\n", "                    ", "v", "=", "normalize", "(", "torch", ".", "mv", "(", "weight_mat", ".", "t", "(", ")", ",", "u", ")", ",", "dim", "=", "0", ",", "eps", "=", "self", ".", "eps", ",", "out", "=", "v", ")", "\n", "u", "=", "normalize", "(", "torch", ".", "mv", "(", "weight_mat", ",", "v", ")", ",", "dim", "=", "0", ",", "eps", "=", "self", ".", "eps", ",", "out", "=", "u", ")", "\n", "", "if", "self", ".", "n_power_iterations", ">", "0", ":", "\n", "# See above on why we need to clone", "\n", "                    ", "u", "=", "u", ".", "clone", "(", ")", "\n", "v", "=", "v", ".", "clone", "(", ")", "\n", "\n", "", "", "", "sigma", "=", "torch", ".", "dot", "(", "u", ",", "torch", ".", "mv", "(", "weight_mat", ",", "v", ")", ")", "\n", "# soft normalization: only when sigma larger than coeff", "\n", "factor", "=", "torch", ".", "max", "(", "torch", ".", "ones", "(", "1", ")", ".", "to", "(", "weight", ".", "device", ")", ",", "sigma", "/", "self", ".", "coeff", ")", "\n", "weight", "=", "weight", "/", "factor", "\n", "# for logging", "\n", "sigma_det", "=", "sigma", ".", "detach", "(", ")", "\n", "torch", ".", "max", "(", "torch", ".", "ones", "(", "1", ")", ".", "to", "(", "weight", ".", "device", ")", ",", "sigma_det", "/", "self", ".", "coeff", ",", "\n", "out", "=", "sigma_log", ")", "\n", "return", "weight", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.remove": [[106, 114], ["delattr", "delattr", "delattr", "delattr", "module.register_parameter", "torch.no_grad", "spectral_norm_fc.SpectralNorm.compute_weight", "torch.nn.Parameter", "spectral_norm_fc.SpectralNorm.detach"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.compute_weight"], ["", "def", "remove", "(", "self", ",", "module", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "weight", "=", "self", ".", "compute_weight", "(", "module", ",", "do_power_iteration", "=", "False", ")", "\n", "", "delattr", "(", "module", ",", "self", ".", "name", ")", "\n", "delattr", "(", "module", ",", "self", ".", "name", "+", "'_u'", ")", "\n", "delattr", "(", "module", ",", "self", ".", "name", "+", "'_v'", ")", "\n", "delattr", "(", "module", ",", "self", ".", "name", "+", "'_orig'", ")", "\n", "module", ".", "register_parameter", "(", "self", ".", "name", ",", "torch", ".", "nn", ".", "Parameter", "(", "weight", ".", "detach", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.__call__": [[115, 117], ["setattr", "spectral_norm_fc.SpectralNorm.compute_weight"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.compute_weight"], ["", "def", "__call__", "(", "self", ",", "module", ",", "inputs", ")", ":", "\n", "        ", "setattr", "(", "module", ",", "self", ".", "name", ",", "self", ".", "compute_weight", "(", "module", ",", "do_power_iteration", "=", "module", ".", "training", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm._solve_v_and_rescale": [[118, 124], ["torch.chain_matmul().squeeze", "torch.chain_matmul().squeeze.mul_", "torch.chain_matmul", "torch.dot", "weight_mat.t().mm().pinverse", "weight_mat.t", "u.unsqueeze", "torch.mv", "weight_mat.t().mm", "weight_mat.t"], "methods", ["None"], ["", "def", "_solve_v_and_rescale", "(", "self", ",", "weight_mat", ",", "u", ",", "target_sigma", ")", ":", "\n", "# Tries to returns a vector `v` s.t. `u = normalize(W @ v)`", "\n", "# (the invariant at top of this class) and `u @ W @ v = sigma`.", "\n", "# This uses pinverse in case W^T W is not invertible.", "\n", "        ", "v", "=", "torch", ".", "chain_matmul", "(", "weight_mat", ".", "t", "(", ")", ".", "mm", "(", "weight_mat", ")", ".", "pinverse", "(", ")", ",", "weight_mat", ".", "t", "(", ")", ",", "u", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "return", "v", ".", "mul_", "(", "target_sigma", "/", "torch", ".", "dot", "(", "u", ",", "torch", ".", "mv", "(", "weight_mat", ",", "v", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.apply": [[125, 160], ["module._forward_pre_hooks.items", "spectral_norm_fc.SpectralNorm", "delattr", "module.register_parameter", "setattr", "module.register_buffer", "module.register_buffer", "module.register_buffer", "module.register_forward_pre_hook", "module._register_state_dict_hook", "module._register_load_state_dict_pre_hook", "torch.no_grad", "spectral_norm_fc.SpectralNorm.reshape_weight_to_matrix", "spectral_norm_fc.SpectralNorm.reshape_weight_to_matrix", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.ones().to", "spectral_norm_fc.SpectralNormStateDictHook", "spectral_norm_fc.SpectralNormLoadStateDictPreHook", "isinstance", "RuntimeError", "weight.new_empty().normal_", "weight.new_empty().normal_", "torch.ones", "weight.new_empty", "weight.new_empty"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.reshape_weight_to_matrix", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.reshape_weight_to_matrix"], ["", "@", "staticmethod", "\n", "def", "apply", "(", "module", ",", "name", ",", "coeff", ",", "n_power_iterations", ",", "dim", ",", "eps", ")", ":", "\n", "        ", "for", "k", ",", "hook", "in", "module", ".", "_forward_pre_hooks", ".", "items", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "hook", ",", "SpectralNorm", ")", "and", "hook", ".", "name", "==", "name", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"Cannot register two spectral_norm hooks on \"", "\n", "\"the same parameter {}\"", ".", "format", "(", "name", ")", ")", "\n", "\n", "", "", "fn", "=", "SpectralNorm", "(", "coeff", ",", "name", ",", "n_power_iterations", ",", "dim", ",", "eps", ")", "\n", "weight", "=", "module", ".", "_parameters", "[", "name", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "weight_mat", "=", "fn", ".", "reshape_weight_to_matrix", "(", "weight", ")", "\n", "\n", "h", ",", "w", "=", "weight_mat", ".", "size", "(", ")", "\n", "# randomly initialize `u` and `v`", "\n", "u", "=", "normalize", "(", "weight", ".", "new_empty", "(", "h", ")", ".", "normal_", "(", "0", ",", "1", ")", ",", "dim", "=", "0", ",", "eps", "=", "fn", ".", "eps", ")", "\n", "v", "=", "normalize", "(", "weight", ".", "new_empty", "(", "w", ")", ".", "normal_", "(", "0", ",", "1", ")", ",", "dim", "=", "0", ",", "eps", "=", "fn", ".", "eps", ")", "\n", "\n", "", "delattr", "(", "module", ",", "fn", ".", "name", ")", "\n", "module", ".", "register_parameter", "(", "fn", ".", "name", "+", "\"_orig\"", ",", "weight", ")", "\n", "# We still need to assign weight back as fn.name because all sorts of", "\n", "# things may assume that it exists, e.g., when initializing weights.", "\n", "# However, we can't directly assign as it could be an nn.Parameter and", "\n", "# gets added as a parameter. Instead, we register weight.data as a plain", "\n", "# attribute.", "\n", "setattr", "(", "module", ",", "fn", ".", "name", ",", "weight", ".", "data", ")", "\n", "module", ".", "register_buffer", "(", "fn", ".", "name", "+", "\"_u\"", ",", "u", ")", "\n", "module", ".", "register_buffer", "(", "fn", ".", "name", "+", "\"_v\"", ",", "v", ")", "\n", "module", ".", "register_buffer", "(", "fn", ".", "name", "+", "\"_sigma\"", ",", "torch", ".", "ones", "(", "1", ")", ".", "to", "(", "weight", ".", "device", ")", ")", "\n", "\n", "module", ".", "register_forward_pre_hook", "(", "fn", ")", "\n", "\n", "module", ".", "_register_state_dict_hook", "(", "SpectralNormStateDictHook", "(", "fn", ")", ")", "\n", "module", ".", "_register_load_state_dict_pre_hook", "(", "SpectralNormLoadStateDictPreHook", "(", "fn", ")", ")", "\n", "return", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNormLoadStateDictPreHook.__init__": [[166, 168], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "fn", ")", ":", "\n", "        ", "self", ".", "fn", "=", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNormLoadStateDictPreHook.__call__": [[177, 190], ["local_metadata.get().get", "local_metadata.get", "torch.no_grad", "state_dict.pop", "fn.reshape_weight_to_matrix", "fn._solve_v_and_rescale"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.reshape_weight_to_matrix", "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm._solve_v_and_rescale"], ["", "def", "__call__", "(", "self", ",", "state_dict", ",", "prefix", ",", "local_metadata", ",", "strict", ",", "\n", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", ":", "\n", "        ", "fn", "=", "self", ".", "fn", "\n", "version", "=", "local_metadata", ".", "get", "(", "'spectral_norm'", ",", "{", "}", ")", ".", "get", "(", "fn", ".", "name", "+", "'.version'", ",", "None", ")", "\n", "if", "version", "is", "None", "or", "version", "<", "1", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "weight_orig", "=", "state_dict", "[", "prefix", "+", "fn", ".", "name", "+", "'_orig'", "]", "\n", "weight", "=", "state_dict", ".", "pop", "(", "prefix", "+", "fn", ".", "name", ")", "\n", "sigma", "=", "(", "weight_orig", "/", "weight", ")", ".", "mean", "(", ")", "\n", "weight_mat", "=", "fn", ".", "reshape_weight_to_matrix", "(", "weight_orig", ")", "\n", "u", "=", "state_dict", "[", "prefix", "+", "fn", ".", "name", "+", "'_u'", "]", "\n", "v", "=", "fn", ".", "_solve_v_and_rescale", "(", "weight_mat", ",", "u", ",", "sigma", ")", "\n", "state_dict", "[", "prefix", "+", "fn", ".", "name", "+", "'_v'", "]", "=", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNormStateDictHook.__init__": [[196, 198], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "fn", ")", ":", "\n", "        ", "self", ".", "fn", "=", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNormStateDictHook.__call__": [[199, 206], ["RuntimeError"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "module", ",", "state_dict", ",", "prefix", ",", "local_metadata", ")", ":", "\n", "        ", "if", "'spectral_norm'", "not", "in", "local_metadata", ":", "\n", "            ", "local_metadata", "[", "'spectral_norm'", "]", "=", "{", "}", "\n", "", "key", "=", "self", ".", "fn", ".", "name", "+", "'.version'", "\n", "if", "key", "in", "local_metadata", "[", "'spectral_norm'", "]", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Unexpected key in metadata['spectral_norm']: {}\"", ".", "format", "(", "key", ")", ")", "\n", "", "local_metadata", "[", "'spectral_norm'", "]", "[", "key", "]", "=", "self", ".", "fn", ".", "_version", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.wrapper_spectral_norm_fc": [[12, 14], ["spectral_norm_fc.spectral_norm_fc"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.spectral_norm_fc"], ["def", "wrapper_spectral_norm_fc", "(", "layer", ")", ":", "\n", "    ", "return", "spectral_norm_fc", "(", "layer", ",", "0.97", ",", "n_power_iterations", "=", "5", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.spectral_norm_fc": [[208, 258], ["spectral_norm_fc.SpectralNorm.apply", "isinstance"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.apply"], ["", "", "def", "spectral_norm_fc", "(", "module", ",", "coeff", ",", "name", "=", "'weight'", ",", "n_power_iterations", "=", "1", ",", "eps", "=", "1e-12", ",", "dim", "=", "None", ")", ":", "\n", "    ", "r\"\"\"Applies spectral normalization to a parameter in the given module.\n\n    .. math::\n         \\mathbf{W} = \\dfrac{\\mathbf{W}}{\\sigma(\\mathbf{W})} \\\\\n         \\sigma(\\mathbf{W}) = \\max_{\\mathbf{h}: \\mathbf{h} \\ne 0} \\dfrac{\\|\\mathbf{W} \\mathbf{h}\\|_2}{\\|\\mathbf{h}\\|_2}\n\n    Spectral normalization stabilizes the training of discriminators (critics)\n    in Generaive Adversarial Networks (GANs) by rescaling the weight tensor\n    with spectral norm :math:`\\sigma` of the weight matrix calculated using\n    power iteration method. If the dimension of the weight tensor is greater\n    than 2, it is reshaped to 2D in power iteration method to get spectral\n    norm. This is implemented via a hook that calculates spectral norm and\n    rescales weight before every :meth:`~Module.forward` call.\n\n    See `Spectral Normalization for Generative Adversarial Networks`_ .\n\n    .. _`Spectral Normalization for Generative Adversarial Networks`: https://arxiv.org/abs/1802.05957\n\n    Args:\n        module (nn.Module): containing module\n        name (str, optional): name of weight parameter\n        n_power_iterations (int, optional): number of power iterations to\n            calculate spectal norm\n        eps (float, optional): epsilon for numerical stability in\n            calculating norms\n        dim (int, optional): dimension corresponding to number of outputs,\n            the default is 0, except for modules that are instances of\n            ConvTranspose1/2/3d, when it is 1\n\n    Returns:\n        The original module with the spectal norm hook\n\n    Example::\n\n        >>> m = spectral_norm(nn.Linear(20, 40))\n        Linear (20 -> 40)\n        >>> m.weight_u.size()\n        torch.Size([20])\n\n    \"\"\"", "\n", "if", "dim", "is", "None", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "(", "torch", ".", "nn", ".", "ConvTranspose1d", ",", "\n", "torch", ".", "nn", ".", "ConvTranspose2d", ",", "\n", "torch", ".", "nn", ".", "ConvTranspose3d", ")", ")", ":", "\n", "            ", "dim", "=", "1", "\n", "", "else", ":", "\n", "            ", "dim", "=", "0", "\n", "", "", "SpectralNorm", ".", "apply", "(", "module", ",", "name", ",", "coeff", ",", "n_power_iterations", ",", "dim", ",", "eps", ")", "\n", "return", "module", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.remove_spectral_norm": [[261, 280], ["module._forward_pre_hooks.items", "ValueError", "isinstance", "hook.remove"], "function", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.spectral_norm_fc.SpectralNorm.remove"], ["", "def", "remove_spectral_norm", "(", "module", ",", "name", "=", "'weight'", ")", ":", "\n", "    ", "r\"\"\"Removes the spectral normalization reparameterization from a module.\n\n    Args:\n        module (nn.Module): containing module\n        name (str, optional): name of weight parameter\n\n    Example:\n        >>> m = spectral_norm(nn.Linear(40, 10))\n        >>> remove_spectral_norm(m)\n    \"\"\"", "\n", "for", "k", ",", "hook", "in", "module", ".", "_forward_pre_hooks", ".", "items", "(", ")", ":", "\n", "        ", "if", "isinstance", "(", "hook", ",", "SpectralNorm", ")", "and", "hook", ".", "name", "==", "name", ":", "\n", "            ", "hook", ".", "remove", "(", "module", ")", "\n", "del", "module", ".", "_forward_pre_hooks", "[", "k", "]", "\n", "return", "module", "\n", "\n", "", "", "raise", "ValueError", "(", "\"spectral_norm of '{}' not found in {}\"", ".", "format", "(", "\n", "name", ",", "module", ")", ")", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.HCritic.H_Discriminator.__init__": [[11, 15], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "fea_dim", ")", ":", "\n", "        ", "super", "(", "H_Discriminator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "function", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "fea_dim", "*", "2", ",", "256", ")", ",", "nn", ".", "Linear", "(", "256", ",", "128", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "128", ",", "64", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "64", ",", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.HCritic.H_Discriminator.forward": [[16, 45], ["torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "a.detach().reshape().repeat().reshape", "b.detach().reshape().repeat().reshape", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "range", "HCritic.H_Discriminator.function", "torch.nn.LogSoftmax.", "torch.optim.Adam.zero_grad", "torch.optim.Adam.zero_grad", "HCritic.H_Discriminator.function", "torch.nn.LogSoftmax.", "HCritic.H_Discriminator.function", "torch.nn.LogSoftmax.reshape", "loss_for_function.backward", "torch.optim.Adam.step", "torch.optim.Adam.step", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "math.log", "a.detach().reshape().repeat", "b.detach().reshape().repeat", "HCritic.H_Discriminator.function.parameters", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "logits_final[].sum", "torch.nn.LogSoftmax.", "math.log", "a.detach().reshape", "b.detach().reshape", "a.detach", "b.detach", "a.detach().reshape().repeat().reshape.detach", "b.detach().reshape().repeat().reshape.detach", "logits[].sum", "logits_cross[].sum", "torch.diag().sum", "torch.diag().sum", "torch.diag().sum", "torch.diag().sum", "a.detach", "b.detach", "torch.diag", "torch.diag", "torch.diag", "torch.diag"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "a", ",", "b", ",", "dataset_id", "=", "None", ",", "iters", "=", "1", ")", ":", "\n", "\n", "        ", "bs", ",", "fea_dim", "=", "a", ".", "shape", "\n", "lsoftmax", "=", "nn", ".", "LogSoftmax", "(", "1", ")", "\n", "\n", "a_expand", "=", "a", ".", "detach", "(", ")", ".", "reshape", "(", "bs", ",", "1", ",", "fea_dim", ")", ".", "repeat", "(", "1", ",", "bs", ",", "1", ")", ".", "reshape", "(", "bs", "*", "bs", ",", "fea_dim", ")", "\n", "b_expand", "=", "b", ".", "detach", "(", ")", ".", "reshape", "(", "1", ",", "bs", ",", "fea_dim", ")", ".", "repeat", "(", "bs", ",", "1", ",", "1", ")", ".", "reshape", "(", "bs", "*", "bs", ",", "fea_dim", ")", "\n", "\n", "params", "=", "[", "param", "for", "param", "in", "self", ".", "function", ".", "parameters", "(", ")", "if", "param", ".", "requires_grad", "]", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "params", ",", "lr", "=", "2e-5", ",", "weight_decay", "=", "2e-5", ")", "\n", "\n", "for", "i", "in", "range", "(", "iters", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "logits", "=", "self", ".", "function", "(", "torch", ".", "cat", "(", "(", "a", ".", "detach", "(", ")", ",", "b", ".", "detach", "(", ")", ")", ",", "1", ")", ")", "\n", "logits", "=", "lsoftmax", "(", "logits", ")", "\n", "logits_cross", "=", "self", ".", "function", "(", "torch", ".", "cat", "(", "(", "a_expand", ".", "detach", "(", ")", ",", "b_expand", ".", "detach", "(", ")", ")", ",", "1", ")", ")", "\n", "logits_cross", "=", "lsoftmax", "(", "logits_cross", ")", ".", "reshape", "(", "bs", ",", "bs", ",", "2", ")", "\n", "loss_for_function", "=", "logits", "[", ":", ",", "0", "]", ".", "sum", "(", ")", "/", "bs", "+", "math", ".", "log", "(", "bs", "-", "1", ")", "+", "(", "logits_cross", "[", ":", ",", ":", ",", "1", "]", ".", "sum", "(", ")", "-", "torch", ".", "diag", "(", "logits_cross", "[", ":", ",", ":", ",", "1", "]", ")", ".", "sum", "(", ")", ")", "/", "bs", "\n", "loss_for_function", "*=", "-", "1", "\n", "loss_for_function", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "\n", "", "logits_final", "=", "self", ".", "function", "(", "torch", ".", "cat", "(", "(", "a", ",", "b", ")", ",", "1", ")", ")", "\n", "logits_final", "=", "lsoftmax", "(", "logits_final", ")", "\n", "loss", "=", "logits_final", "[", ":", ",", "0", "]", ".", "sum", "(", ")", "/", "bs", "+", "math", ".", "log", "(", "bs", "-", "1", ")", "\n", "loss", "*=", "-", "1", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.HCritic.H_CosCritic.__init__": [[48, 54], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "self", ",", "fea_dim", ")", ":", "\n", "        ", "super", "(", "H_CosCritic", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "proj_a", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "fea_dim", ",", "512", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "512", ",", "256", ")", ")", "\n", "self", ".", "proj_b", "=", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "fea_dim", ",", "512", ")", ",", "nn", ".", "ReLU", "(", ")", ",", "nn", ".", "Linear", "(", "512", ",", "256", ")", ")", "\n", "self", ".", "temperature", "=", "0.2", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.models.HCritic.H_CosCritic.forward": [[55, 74], ["HCritic.H_CosCritic.proj_a", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "HCritic.H_CosCritic.proj_b", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.nn.functional.normalize", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax", "torch.nn.LogSoftmax.", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.diag", "torch.diag", "torch.diag", "torch.diag", "torch.transpose", "torch.transpose", "torch.transpose", "torch.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "a", ",", "b", ",", "dataset_id", "=", "None", ")", ":", "\n", "        ", "a", "=", "self", ".", "proj_a", "(", "a", ")", "\n", "a", "=", "torch", ".", "nn", ".", "functional", ".", "normalize", "(", "a", ")", "\n", "b", "=", "self", ".", "proj_b", "(", "b", ")", "\n", "b", "=", "torch", ".", "nn", ".", "functional", ".", "normalize", "(", "b", ")", "\n", "\n", "bs", ",", "fea_dim", "=", "a", ".", "shape", "\n", "f_a_b", "=", "torch", ".", "mm", "(", "a", ",", "torch", ".", "transpose", "(", "b", ",", "0", ",", "1", ")", ")", "#bs*768 * 768*bs = bs * bs", "\n", "f_a_b", "=", "f_a_b", "/", "self", ".", "temperature", "\n", "\n", "lsoftmax", "=", "nn", ".", "LogSoftmax", "(", "1", ")", "\n", "log_p_f_a_b", "=", "lsoftmax", "(", "f_a_b", ")", "\n", "\n", "out", "=", "torch", ".", "zeros", "(", "bs", ",", "2", ")", "\n", "out", "[", ":", ",", "0", "]", "=", "torch", ".", "diag", "(", "log_p_f_a_b", ")", "\n", "\n", "logit_pos", "=", "(", "a", "*", "b", ")", ".", "sum", "(", "-", "1", ")", "\n", "\n", "return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__": [[15, 42], ["super().__init__", "ValueError", "ValueError", "list", "sorted"], "methods", ["home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "optimizer", ",", "\n", "milestones", ",", "\n", "gamma", "=", "0.1", ",", "\n", "warmup_factor", "=", "1.0", "/", "3", ",", "\n", "warmup_iters", "=", "500", ",", "\n", "warmup_method", "=", "\"linear\"", ",", "\n", "last_epoch", "=", "-", "1", ",", "\n", ")", ":", "\n", "        ", "if", "not", "list", "(", "milestones", ")", "==", "sorted", "(", "milestones", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Milestones should be a list of\"", "\" increasing integers. Got {}\"", ",", "\n", "milestones", ",", "\n", ")", "\n", "\n", "", "if", "warmup_method", "not", "in", "(", "\"constant\"", ",", "\"linear\"", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Only 'constant' or 'linear' warmup_method accepted\"", "\n", "\"got {}\"", ".", "format", "(", "warmup_method", ")", "\n", ")", "\n", "", "self", ".", "milestones", "=", "milestones", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "warmup_factor", "=", "warmup_factor", "\n", "self", ".", "warmup_iters", "=", "warmup_iters", "\n", "self", ".", "warmup_method", "=", "warmup_method", "\n", "super", "(", "WarmupMultiStepLR", ",", "self", ")", ".", "__init__", "(", "optimizer", ",", "last_epoch", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.dongxinshuai_ASCC.solver.lr_scheduler.WarmupMultiStepLR.get_lr": [[43, 56], ["bisect.bisect_right"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "warmup_factor", "=", "1", "\n", "if", "self", ".", "last_epoch", "<", "self", ".", "warmup_iters", ":", "\n", "            ", "if", "self", ".", "warmup_method", "==", "\"constant\"", ":", "\n", "                ", "warmup_factor", "=", "self", ".", "warmup_factor", "\n", "", "elif", "self", ".", "warmup_method", "==", "\"linear\"", ":", "\n", "                ", "alpha", "=", "self", ".", "last_epoch", "/", "self", ".", "warmup_iters", "\n", "warmup_factor", "=", "self", ".", "warmup_factor", "*", "(", "1", "-", "alpha", ")", "+", "alpha", "\n", "", "", "return", "[", "\n", "base_lr", "\n", "*", "warmup_factor", "\n", "*", "self", ".", "gamma", "**", "bisect_right", "(", "self", ".", "milestones", ",", "self", ".", "last_epoch", ")", "\n", "for", "base_lr", "in", "self", ".", "base_lrs", "\n", "]", "\n"]]}