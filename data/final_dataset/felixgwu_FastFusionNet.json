{"home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.flatten_json": [[78, 86], ["sum", "open", "concurrent.futures.ProcessPoolExecutor", "executor.map", "json.load", "prepro.proc_train", "prepro.proc_dev"], "function", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.proc_train", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.proc_dev"], ["def", "flatten_json", "(", "file", ",", "proc_func", ")", ":", "\n", "    ", "'''A multi-processing wrapper for loading SQuAD data file.'''", "\n", "with", "open", "(", "file", ")", "as", "f", ":", "\n", "        ", "data", "=", "json", ".", "load", "(", "f", ")", "[", "'data'", "]", "\n", "", "with", "ProcessPoolExecutor", "(", "max_workers", "=", "args", ".", "threads", ")", "as", "executor", ":", "\n", "        ", "rows", "=", "executor", ".", "map", "(", "proc_func", ",", "data", ")", "\n", "", "rows", "=", "sum", "(", "rows", ",", "[", "]", ")", "\n", "return", "rows", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.proc_train": [[88, 100], ["rows.append", "len", "zip"], "function", ["None"], ["", "def", "proc_train", "(", "article", ")", ":", "\n", "    ", "'''Flatten each article in training data.'''", "\n", "rows", "=", "[", "]", "\n", "for", "paragraph", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "        ", "context", "=", "paragraph", "[", "'context'", "]", "\n", "for", "qa", "in", "paragraph", "[", "'qas'", "]", ":", "\n", "            ", "id_", ",", "question", ",", "answers", "=", "qa", "[", "'id'", "]", ",", "qa", "[", "'question'", "]", ",", "qa", "[", "'answers'", "]", "\n", "answer_starts", "=", "[", "a", "[", "'answer_start'", "]", "for", "a", "in", "answers", "]", "\n", "answers", "=", "[", "a", "[", "'text'", "]", "for", "a", "in", "answers", "]", "\n", "answer_ends", "=", "[", "answer_start", "+", "len", "(", "answer", ")", "for", "answer_start", ",", "answer", "in", "zip", "(", "answer_starts", ",", "answers", ")", "]", "\n", "rows", ".", "append", "(", "(", "id_", ",", "context", ",", "question", ",", "answers", ",", "answer_starts", ",", "answer_ends", ")", ")", "\n", "", "", "return", "rows", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.proc_dev": [[102, 112], ["rows.append"], "function", ["None"], ["", "def", "proc_dev", "(", "article", ")", ":", "\n", "    ", "'''Flatten each article in dev data'''", "\n", "rows", "=", "[", "]", "\n", "for", "paragraph", "in", "article", "[", "'paragraphs'", "]", ":", "\n", "        ", "context", "=", "paragraph", "[", "'context'", "]", "\n", "for", "qa", "in", "paragraph", "[", "'qas'", "]", ":", "\n", "            ", "id_", ",", "question", ",", "answers", "=", "qa", "[", "'id'", "]", ",", "qa", "[", "'question'", "]", ",", "qa", "[", "'answers'", "]", "\n", "answers", "=", "[", "a", "[", "'text'", "]", "for", "a", "in", "answers", "]", "\n", "rows", ".", "append", "(", "(", "id_", ",", "context", ",", "question", ",", "answers", ")", ")", "\n", "", "", "return", "rows", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.get_answer_index": [[139, 182], ["zip", "len", "len", "re.match", "len", "start_tokens.append", "end_tokens.append"], "function", ["None"], ["def", "get_answer_index", "(", "context", ",", "context_token", ",", "answer_starts", ",", "answer_ends", ")", ":", "\n", "    ", "'''\n    Get exact indices of the answer in the tokens of the passage,\n    according to the start and end position of the answer.\n\n    Args:\n        context (str): the context passage\n        context_token (list): list of tokens (str) in the context passage\n        answer_starts (list): the start position of the answer in the passage\n        answer_ends (list): the end position of the answer in the passage\n\n    Returns:\n        (int, int): start index and end index of answer\n    '''", "\n", "p_str", "=", "0", "\n", "p_token", "=", "0", "\n", "start_tokens", ",", "end_tokens", "=", "[", "]", ",", "[", "]", "\n", "for", "answer_start", ",", "answer_end", "in", "zip", "(", "answer_starts", ",", "answer_ends", ")", ":", "\n", "        ", "while", "p_str", "<", "len", "(", "context", ")", ":", "\n", "            ", "if", "re", ".", "match", "(", "'\\s'", ",", "context", "[", "p_str", "]", ")", ":", "\n", "                ", "p_str", "+=", "1", "\n", "continue", "\n", "", "token", "=", "context_token", "[", "p_token", "]", "\n", "token_len", "=", "len", "(", "token", ")", "\n", "if", "context", "[", "p_str", ":", "p_str", "+", "token_len", "]", "!=", "token", ":", "\n", "                ", "return", "(", "None", ",", "None", ")", "\n", "", "if", "p_str", "==", "answer_start", ":", "\n", "                ", "t_start", "=", "p_token", "\n", "", "p_str", "+=", "token_len", "\n", "if", "p_str", "==", "answer_end", ":", "\n", "                ", "try", ":", "\n", "                    ", "start_tokens", ".", "append", "(", "t_start", ")", "\n", "end_tokens", ".", "append", "(", "p_token", ")", "\n", "", "except", "UnboundLocalError", "as", "e", ":", "\n", "                    ", "pass", "\n", "", "finally", ":", "\n", "                    ", "break", "\n", "\n", "", "", "p_token", "+=", "1", "\n", "", "", "if", "len", "(", "start_tokens", ")", "==", "0", ":", "\n", "        ", "return", "(", "None", ",", "None", ")", "\n", "", "else", ":", "\n", "        ", "return", "(", "start_tokens", ",", "end_tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.get_spans": [[215, 225], ["zip", "spans.append", "text[].find", "len", "len"], "function", ["None"], ["", "def", "get_spans", "(", "tokens", ",", "text", ")", ":", "\n", "    ", "pos", "=", "0", "\n", "spans", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "        ", "start", "=", "pos", "+", "text", "[", "pos", ":", "]", ".", "find", "(", "token", ")", "\n", "spans", ".", "append", "(", "[", "start", ",", "start", "+", "len", "(", "token", ")", "]", ")", "\n", "pos", "=", "start", "+", "len", "(", "token", ")", "\n", "", "for", "(", "s", ",", "e", ")", ",", "token", "in", "zip", "(", "spans", ",", "tokens", ")", ":", "\n", "        ", "assert", "text", "[", "s", ":", "e", "]", "==", "token", ",", "'{}, {}\\ntext: {}\\n token: {}'", ".", "foramt", "(", "s", ",", "e", ",", "text", ",", "token", ")", "\n", "", "return", "spans", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.build_train_vocab": [[249, 268], ["sum", "sum", "log.info", "sorted.insert", "sorted.insert", "collections.Counter", "sorted", "collections.Counter", "collections.Counter", "sorted", "sorted", "collections.Counter.values", "len", "len", "collections.Counter.keys", "collections.Counter.keys"], "function", ["None"], ["def", "build_train_vocab", "(", "questions", ",", "contexts", ",", "wv_vocab", ")", ":", "# vocabulary will also be sorted accordingly", "\n", "    ", "if", "args", ".", "sort_all", ":", "\n", "        ", "counter", "=", "collections", ".", "Counter", "(", "w", "for", "doc", "in", "questions", "+", "contexts", "for", "w", "in", "doc", ")", "\n", "vocab", "=", "sorted", "(", "[", "t", "for", "t", "in", "counter", "if", "t", "in", "wv_vocab", "]", ",", "key", "=", "counter", ".", "get", ",", "reverse", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "counter_q", "=", "collections", ".", "Counter", "(", "w", "for", "doc", "in", "questions", "for", "w", "in", "doc", ")", "\n", "counter_c", "=", "collections", ".", "Counter", "(", "w", "for", "doc", "in", "contexts", "for", "w", "in", "doc", ")", "\n", "counter", "=", "counter_c", "+", "counter_q", "\n", "vocab", "=", "sorted", "(", "[", "t", "for", "t", "in", "counter_q", "if", "t", "in", "wv_vocab", "]", ",", "key", "=", "counter_q", ".", "get", ",", "reverse", "=", "True", ")", "\n", "vocab", "+=", "sorted", "(", "[", "t", "for", "t", "in", "counter_c", ".", "keys", "(", ")", "-", "counter_q", ".", "keys", "(", ")", "if", "t", "in", "wv_vocab", "]", ",", "\n", "key", "=", "counter", ".", "get", ",", "reverse", "=", "True", ")", "\n", "\n", "", "total", "=", "sum", "(", "counter", ".", "values", "(", ")", ")", "\n", "matched", "=", "sum", "(", "counter", "[", "t", "]", "for", "t", "in", "vocab", ")", "\n", "log", ".", "info", "(", "'vocab {1}/{0} OOV {2}/{3} ({4:.4f}%)'", ".", "format", "(", "\n", "len", "(", "counter", ")", ",", "len", "(", "vocab", ")", ",", "(", "total", "-", "matched", ")", ",", "total", ",", "(", "total", "-", "matched", ")", "/", "total", "*", "100", ")", ")", "\n", "vocab", ".", "insert", "(", "0", ",", "\"<PAD>\"", ")", "\n", "vocab", ".", "insert", "(", "1", ",", "\"<UNK>\"", ")", "\n", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.build_eval_vocab": [[269, 275], ["set", "list", "log.info", "set", "len", "len"], "function", ["None"], ["", "def", "build_eval_vocab", "(", "questions", ",", "contexts", ",", "train_vocab", ",", "wv_vocab", ")", ":", "# most vocabulary comes from tr_vocab", "\n", "    ", "existing_vocab", "=", "set", "(", "train_vocab", ")", "\n", "new_vocab", "=", "list", "(", "set", "(", "[", "w", "for", "doc", "in", "questions", "+", "contexts", "for", "w", "in", "doc", "if", "w", "not", "in", "existing_vocab", "and", "w", "in", "wv_vocab", "]", ")", ")", "\n", "vocab", "=", "train_vocab", "+", "new_vocab", "\n", "log", ".", "info", "(", "'train vocab {0}, total vocab {1}'", ".", "format", "(", "len", "(", "train_vocab", ")", ",", "len", "(", "vocab", ")", ")", ")", "\n", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.prepro.build_full_vocab": [[276, 282], ["set", "list", "log.info", "set", "len", "len"], "function", ["None"], ["", "def", "build_full_vocab", "(", "questions", ",", "contexts", ",", "eval_vocab", ")", ":", "\n", "    ", "existing_vocab", "=", "set", "(", "eval_vocab", ")", "\n", "new_vocab", "=", "list", "(", "set", "(", "[", "w", "for", "doc", "in", "questions", "+", "contexts", "for", "w", "in", "doc", "if", "w", "not", "in", "existing_vocab", "]", ")", ")", "\n", "vocab", "=", "eval_vocab", "+", "new_vocab", "\n", "log", ".", "info", "(", "'eval vocab {0}, total vocab {1}'", ".", "format", "(", "len", "(", "eval_vocab", ")", ",", "len", "(", "vocab", ")", ")", ")", "\n", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.eval.main": [[58, 91], ["log.info", "log.info", "torch.load", "checkpoint[].update", "log.info", "qa.utils.load_data", "log.info", "log.info", "qa.model.DocReaderModel", "range", "log.info", "log.info", "qa.utils.BatchGen", "time.perf_counter", "torch.cuda.synchronize", "qa.utils.score", "log.info", "os.path.join", "vars", "random.shuffle", "qa.model.DocReaderModel.cuda", "predictions.extend", "time.perf_counter", "len", "len", "list", "qa.model.DocReaderModel.predict", "range", "len", "len"], "function", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.AverageMeter.update", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.load_data", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.score"], ["def", "main", "(", ")", ":", "\n", "    ", "log", ".", "info", "(", "'[program starts.]'", ")", "\n", "log", ".", "info", "(", "'[loading previous model...]'", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "args", ".", "resume", ")", ")", "\n", "checkpoint", "[", "'config'", "]", ".", "update", "(", "vars", "(", "args", ")", ")", "\n", "opt", "=", "checkpoint", "[", "'config'", "]", "\n", "log", ".", "info", "(", "'[loading data...]'", ")", "\n", "train", ",", "dev", ",", "train_y", ",", "dev_y", ",", "embedding", ",", "opt", ",", "meta", "=", "load_data", "(", "opt", ",", "log", ")", "\n", "log", ".", "info", "(", "'[Data loaded.]'", ")", "\n", "log", ".", "info", "(", "'train_size: {}, dev_size: {}'", ".", "format", "(", "len", "(", "train", ")", ",", "len", "(", "dev", ")", ")", ")", "\n", "\n", "state_dict", "=", "checkpoint", "[", "'state_dict'", "]", "\n", "model", "=", "DocReaderModel", "(", "opt", ",", "embedding", ",", "state_dict", ")", "\n", "epoch_0", "=", "checkpoint", "[", "'epoch'", "]", "+", "1", "\n", "for", "i", "in", "range", "(", "checkpoint", "[", "'epoch'", "]", ")", ":", "\n", "        ", "random", ".", "shuffle", "(", "list", "(", "range", "(", "len", "(", "train", ")", ")", ")", ")", "# synchronize random seed", "\n", "", "log", ".", "info", "(", "'opt: {}'", ".", "format", "(", "opt", ")", ")", "\n", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "model", ".", "cuda", "(", ")", "\n", "\n", "", "log", ".", "info", "(", "'model:\\n{}'", ".", "format", "(", "model", ".", "network", ")", ")", "\n", "\n", "\n", "batches", "=", "BatchGen", "(", "opt", ",", "dev", ",", "batch_size", "=", "opt", "[", "'eval_batch_size'", "]", ",", "evaluation", "=", "True", ",", "max_len", "=", "args", ".", "max_eval_len", ",", "gpu", "=", "args", ".", "cuda", ",", "with_cids", "=", "False", ")", "\n", "predictions", "=", "[", "]", "\n", "start", "=", "time", ".", "perf_counter", "(", ")", "\n", "for", "batch", "in", "batches", ":", "\n", "        ", "predictions", ".", "extend", "(", "model", ".", "predict", "(", "batch", ")", ")", "\n", "", "torch", ".", "cuda", ".", "synchronize", "(", ")", "\n", "eval_time", "=", "time", ".", "perf_counter", "(", ")", "-", "start", "\n", "em", ",", "f1", "=", "score", "(", "predictions", ",", "dev_y", ")", "\n", "log", ".", "info", "(", "\"[dev EM: {} F1: {} eval_time: {:.2f} s eval_time per example: {:.3f} ms]\"", ".", "format", "(", "em", ",", "f1", ",", "eval_time", ",", "eval_time", "*", "1000.", "/", "len", "(", "dev", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.train.main": [[170, 308], ["log.info", "qa.utils.load_data", "log.info", "log.info", "log.info", "log.info", "log.info", "os.path.join", "range", "vars", "log.info", "torch.load", "qa.model.DocReaderModel", "range", "qa.model.DocReaderModel", "qa.model.DocReaderModel.cuda", "log.info", "print", "qa.utils.score", "log.info", "log.info", "open", "json.dump", "open", "print", "open", "print", "log.warning", "qa.utils.BatchGen", "datetime.datetime.now", "len", "enumerate", "train_time.total_seconds", "len", "len", "os.path.join", "opt.update", "random.shuffle", "train.lr_decay", "PyCallGraph", "qa.utils.BatchGen", "len", "len", "qa.utils.BatchGen", "qa.utils.score", "log.info", "os.path.join", "os.path.join", "qa.model.DocReaderModel.update", "datetime.datetime.now", "qa.utils.BatchGen", "qa.utils.score", "qa.utils.BatchGen", "datetime.datetime.now", "qa.utils.score", "log.warning", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "train.lr_decay", "list", "predictions.extend", "predictions.extend", "log.info", "writer.add_scalar", "writer.add_scalar", "predictions.extend", "predictions.extend", "datetime.datetime.now", "open", "print", "writer.add_scalar", "writer.add_scalar", "writer.add_scalar", "os.path.join", "os.path.join", "qa.model.DocReaderModel.save", "range", "GraphvizOutput", "Config", "qa.model.DocReaderModel.predict", "qa.model.DocReaderModel.predict", "qa.model.DocReaderModel.predict", "qa.model.DocReaderModel.predict", "train_time.total_seconds", "eval_time.total_seconds", "train_time.total_seconds", "eval_time.total_seconds", "eval_time.total_seconds", "len", "shutil.copyfile", "log.info", "os.path.exists", "os.remove", "len", "train_time.total_seconds", "os.path.join", "str().split", "str", "datetime.datetime.now", "len"], "function", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.load_data", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.score", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.AverageMeter.update", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.train.lr_decay", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.score", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.AverageMeter.update", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.score", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.score", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.train.lr_decay"], ["def", "main", "(", ")", ":", "\n", "    ", "log", ".", "info", "(", "'[program starts.]'", ")", "\n", "train", ",", "dev", ",", "train_y", ",", "dev_y", ",", "embedding", ",", "opt", ",", "meta", "=", "load_data", "(", "vars", "(", "args", ")", ",", "log", ")", "\n", "# hold out original dev set", "\n", "log", ".", "info", "(", "'[Data loaded.]'", ")", "\n", "log", ".", "info", "(", "'train_size: {}, dev_size: {}'", ".", "format", "(", "len", "(", "train", ")", ",", "len", "(", "dev", ")", ")", ")", "\n", "\n", "if", "args", ".", "resume", ":", "\n", "        ", "log", ".", "info", "(", "'[loading previous model...]'", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "args", ".", "resume", ")", ")", "\n", "if", "args", ".", "resume_options", ":", "\n", "            ", "opt", ".", "update", "(", "checkpoint", "[", "'config'", "]", ")", "\n", "", "state_dict", "=", "checkpoint", "[", "'state_dict'", "]", "\n", "model", "=", "DocReaderModel", "(", "opt", ",", "embedding", ",", "state_dict", ")", "\n", "epoch_0", "=", "checkpoint", "[", "'epoch'", "]", "+", "1", "\n", "for", "i", "in", "range", "(", "checkpoint", "[", "'epoch'", "]", ")", ":", "\n", "            ", "random", ".", "shuffle", "(", "list", "(", "range", "(", "len", "(", "train", ")", ")", ")", ")", "# synchronize random seed", "\n", "", "if", "args", ".", "reduce_lr", ":", "\n", "            ", "lr_decay", "(", "model", ".", "optimizer", ",", "lr_decay", "=", "args", ".", "reduce_lr", ")", "\n", "", "", "else", ":", "\n", "        ", "model", "=", "DocReaderModel", "(", "opt", ",", "embedding", ")", "\n", "epoch_0", "=", "1", "\n", "", "log", ".", "info", "(", "'opt: {}'", ".", "format", "(", "opt", ")", ")", "\n", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "model", ".", "cuda", "(", ")", "\n", "\n", "", "log", ".", "info", "(", "'model:\\n{}'", ".", "format", "(", "model", ".", "network", ")", ")", "\n", "\n", "opt", "[", "'with_cids'", "]", "=", "False", "\n", "\n", "if", "args", ".", "profile", ":", "\n", "        ", "from", "pycallgraph", "import", "PyCallGraph", ",", "Config", "\n", "from", "pycallgraph", ".", "output", "import", "GraphvizOutput", "\n", "log", ".", "info", "(", "'starts profiling'", ")", "\n", "with", "PyCallGraph", "(", "output", "=", "GraphvizOutput", "(", "output_file", "=", "args", ".", "profile", ")", ",", "config", "=", "Config", "(", "include_stdlib", "=", "args", ".", "profile_std", ")", ")", ":", "\n", "            ", "batches", "=", "BatchGen", "(", "opt", ",", "dev", ",", "batch_size", "=", "args", ".", "batch_size", ",", "max_len", "=", "args", ".", "max_eval_len", ",", "evaluation", "=", "True", ",", "gpu", "=", "args", ".", "cuda", ",", "with_cids", "=", "opt", "[", "'with_cids'", "]", ")", "\n", "predictions", "=", "[", "]", "\n", "for", "batch", "in", "batches", ":", "\n", "                ", "predictions", ".", "extend", "(", "model", ".", "predict", "(", "batch", ")", ")", "\n", "", "", "print", "(", "len", "(", "dev_y", ")", ",", "len", "(", "predictions", ")", ")", "\n", "dev_em", ",", "dev_f1", "=", "score", "(", "predictions", ",", "dev_y", ")", "\n", "log", ".", "info", "(", "\"[dev EM: {} F1: {}]\"", ".", "format", "(", "dev_em", ",", "dev_f1", ")", ")", "\n", "log", ".", "info", "(", "'finished profiling'", ")", "\n", "return", "\n", "\n", "", "if", "args", ".", "resume", ":", "\n", "        ", "if", "not", "'best_val_score'", "in", "checkpoint", ":", "\n", "            ", "batches", "=", "BatchGen", "(", "opt", ",", "dev", ",", "batch_size", "=", "args", ".", "batch_size", ",", "max_len", "=", "args", ".", "max_eval_len", ",", "evaluation", "=", "True", ",", "gpu", "=", "args", ".", "cuda", ",", "with_cids", "=", "opt", "[", "'with_cids'", "]", ")", "\n", "predictions", "=", "[", "]", "\n", "for", "batch", "in", "batches", ":", "\n", "                ", "predictions", ".", "extend", "(", "model", ".", "predict", "(", "batch", ")", ")", "\n", "", "em", ",", "f1", "=", "score", "(", "predictions", ",", "dev_y", ")", "\n", "log", ".", "info", "(", "\"[dev EM: {} F1: {}]\"", ".", "format", "(", "em", ",", "f1", ")", ")", "\n", "best_val_score", "=", "f1", "\n", "", "else", ":", "\n", "            ", "best_val_score", "=", "checkpoint", "[", "'best_val_score'", "]", "\n", "\n", "", "", "else", ":", "\n", "        ", "best_val_score", "=", "0.", "\n", "\n", "", "log", ".", "info", "(", "'best score is set to {:.2f}'", ".", "format", "(", "best_val_score", ")", ")", "\n", "\n", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'opt.json'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "opt", ",", "f", ",", "indent", "=", "2", ")", "\n", "", "with", "open", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'model_str.txt'", ")", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "print", "(", "'model:\\n{}\\n\\noptimizer:{}'", ".", "format", "(", "model", ".", "network", ",", "model", ".", "optimizer", ")", ",", "file", "=", "f", ")", "\n", "\n", "\n", "", "dawn_log", "=", "os", ".", "path", ".", "join", "(", "args", ".", "save_dir", ",", "'dawn_train.tsv'", ")", "\n", "with", "open", "(", "dawn_log", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "print", "(", "'epoch\\thours\\tf1Score'", ",", "file", "=", "f", ")", "\n", "", "all_train_time", "=", "0.", "\n", "\n", "for", "epoch", "in", "range", "(", "epoch_0", ",", "args", ".", "epochs", ")", ":", "\n", "        ", "log", ".", "warning", "(", "'Epoch {}'", ".", "format", "(", "epoch", ")", ")", "\n", "# train", "\n", "batches", "=", "BatchGen", "(", "opt", ",", "train", ",", "batch_size", "=", "args", ".", "batch_size", ",", "max_len", "=", "args", ".", "max_train_len", ",", "gpu", "=", "args", ".", "cuda", ",", "with_cids", "=", "opt", "[", "'with_cids'", "]", ")", "\n", "start", "=", "datetime", ".", "now", "(", ")", "\n", "num_train_batches", "=", "len", "(", "batches", ")", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "batches", ")", ":", "\n", "            ", "model", ".", "update", "(", "batch", ")", "\n", "if", "model", ".", "updates", "%", "args", ".", "log_per_updates", "==", "0", ":", "\n", "                ", "log", ".", "info", "(", "'epoch [{0:2}] updates[{1:6}] train loss[{2:.5f}] remaining[{3}]'", ".", "format", "(", "\n", "epoch", ",", "model", ".", "updates", ",", "model", ".", "train_loss", ".", "avg", ",", "\n", "str", "(", "(", "datetime", ".", "now", "(", ")", "-", "start", ")", "/", "(", "i", "+", "1", ")", "*", "(", "len", "(", "batches", ")", "-", "i", "-", "1", ")", ")", ".", "split", "(", "'.'", ")", "[", "0", "]", ")", ")", "\n", "writer", ".", "add_scalar", "(", "'train_loss_avg_iter'", ",", "model", ".", "train_loss", ".", "avg", ",", "model", ".", "updates", ")", "\n", "writer", ".", "add_scalar", "(", "'train_loss_iter'", ",", "model", ".", "train_loss", ".", "val", ",", "model", ".", "updates", ")", "\n", "\n", "", "", "train_time", "=", "datetime", ".", "now", "(", ")", "-", "start", "\n", "all_train_time", "+=", "train_time", ".", "total_seconds", "(", ")", "\n", "# eval", "\n", "if", "epoch", "%", "args", ".", "eval_per_epoch", "==", "0", ":", "\n", "            ", "train_batches", "=", "BatchGen", "(", "opt", ",", "train", "[", ":", "args", ".", "train_eval_size", "]", ",", "batch_size", "=", "args", ".", "batch_size", ",", "evaluation", "=", "True", ",", "max_len", "=", "args", ".", "max_eval_len", ",", "gpu", "=", "args", ".", "cuda", ",", "with_cids", "=", "opt", "[", "'with_cids'", "]", ")", "\n", "predictions", "=", "[", "]", "\n", "for", "batch", "in", "train_batches", ":", "\n", "                ", "predictions", ".", "extend", "(", "model", ".", "predict", "(", "batch", ")", ")", "\n", "", "train_em", ",", "train_f1", "=", "score", "(", "predictions", ",", "train_y", "[", ":", "args", ".", "train_eval_size", "]", ")", "\n", "\n", "dev_batches", "=", "BatchGen", "(", "opt", ",", "dev", ",", "batch_size", "=", "args", ".", "batch_size", ",", "evaluation", "=", "True", ",", "max_len", "=", "args", ".", "max_eval_len", ",", "gpu", "=", "args", ".", "cuda", ",", "with_cids", "=", "opt", "[", "'with_cids'", "]", ")", "\n", "predictions", "=", "[", "]", "\n", "start", "=", "datetime", ".", "now", "(", ")", "\n", "for", "batch", "in", "dev_batches", ":", "\n", "                ", "predictions", ".", "extend", "(", "model", ".", "predict", "(", "batch", ")", ")", "\n", "", "dev_em", ",", "dev_f1", "=", "score", "(", "predictions", ",", "dev_y", ")", "\n", "eval_time", "=", "datetime", ".", "now", "(", ")", "-", "start", "\n", "\n", "is_best", "=", "best_val_score", "<", "dev_f1", "\n", "if", "is_best", ":", "\n", "                ", "best_val_score", "=", "dev_f1", "\n", "\n", "", "log", ".", "warning", "(", "\"Epoch {} train loss: {:.5f} EM: {:.2f} F1: {:.2f} dev EM: {:.2f} F1: {:.2f} (best: {:.2f}) train: {:.2f} s eval: {:.2f} s\"", ".", "format", "(", "epoch", ",", "model", ".", "train_loss", ".", "avg", ",", "train_em", ",", "train_f1", ",", "dev_em", ",", "dev_f1", ",", "best_val_score", ",", "train_time", ".", "total_seconds", "(", ")", ",", "eval_time", ".", "total_seconds", "(", ")", ")", ")", "\n", "with", "open", "(", "dawn_log", ",", "'a'", ")", "as", "f", ":", "\n", "                ", "print", "(", "'{}\\t{}\\t{}'", ".", "format", "(", "epoch", ",", "all_train_time", "/", "3600.", ",", "dev_f1", ")", ",", "file", "=", "f", ")", "\n", "", "writer", ".", "add_scalar", "(", "'train_loss_avg_epoch'", ",", "model", ".", "train_loss", ".", "avg", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'time_train'", ",", "train_time", ".", "total_seconds", "(", ")", ",", "epoch", ")", ",", "\n", "writer", ".", "add_scalar", "(", "'time_eval'", ",", "eval_time", ".", "total_seconds", "(", ")", ",", "epoch", ")", ",", "\n", "writer", ".", "add_scalar", "(", "'time_per_epoch_train'", ",", "train_time", ".", "total_seconds", "(", ")", "/", "num_train_batches", ",", "epoch", ")", ",", "\n", "writer", ".", "add_scalar", "(", "'time_per_epoch_eval'", ",", "eval_time", ".", "total_seconds", "(", ")", "/", "len", "(", "dev_batches", ")", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'EM_train'", ",", "train_em", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'F1_train'", ",", "train_f1", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'EM_dev'", ",", "dev_em", ",", "epoch", ")", "\n", "writer", ".", "add_scalar", "(", "'F1_dev'", ",", "dev_f1", ",", "epoch", ")", "\n", "# save", "\n", "if", "not", "args", ".", "save_last_only", "or", "epoch", "==", "epoch_0", "+", "args", ".", "epochs", "-", "1", ":", "\n", "                ", "prev_model_file", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'checkpoint_epoch_{}.pt'", ".", "format", "(", "epoch", "-", "1", ")", ")", "\n", "model_file", "=", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'checkpoint_epoch_{}.pt'", ".", "format", "(", "epoch", ")", ")", "\n", "model", ".", "save", "(", "model_file", ",", "epoch", ",", "best_val_score", ")", "\n", "if", "is_best", ":", "\n", "                    ", "copyfile", "(", "\n", "model_file", ",", "\n", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'best_model.pt'", ")", ")", "\n", "log", ".", "info", "(", "'[new best model saved.]'", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "prev_model_file", ")", "and", "not", "args", ".", "save_checkpoints", ":", "\n", "                    ", "os", ".", "remove", "(", "prev_model_file", ")", "\n", "", "", "", "if", "args", ".", "decay_every", ">", "0", "and", "epoch", "%", "args", ".", "decay_every", "==", "0", ":", "\n", "            ", "lr_decay", "(", "model", ".", "optimizer", ",", "args", ".", "lr_decay_rate", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.None.train.lr_decay": [[310, 315], ["log.info"], "function", ["None"], ["", "", "", "def", "lr_decay", "(", "optimizer", ",", "lr_decay", ")", ":", "\n", "    ", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "        ", "param_group", "[", "'lr'", "]", "*=", "lr_decay", "\n", "", "log", ".", "info", "(", "'[learning rate reduced by {}]'", ".", "format", "(", "lr_decay", ")", ")", "\n", "return", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.encoder.RnnEncoder.__init__": [[12, 30], ["torch.Module.__init__", "opt.get", "layers.StackedBRNN", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "encoder_input_dim", "=", "opt", ".", "get", "(", "'encoder_input_dim'", ",", "512", ")", "\n", "self", ".", "rnn_output_size", "=", "2", "*", "opt", "[", "'hidden_size'", "]", "*", "opt", "[", "'doc_layers'", "]", "if", "opt", "[", "'concat_rnn_layers'", "]", "else", "2", "*", "opt", "[", "'hidden_size'", "]", "\n", "self", ".", "proj_size", "=", "600", "if", "opt", "[", "'target_type'", "]", "==", "'cove'", "else", "2048", "\n", "self", ".", "rnn", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "self", ".", "encoder_input_dim", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "opt", "[", "'doc_layers'", "]", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "dropout_output", "=", "opt", "[", "'dropout_rnn_output'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "concat_layers", "=", "opt", "[", "'concat_rnn_layers'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", ")", "\n", "if", "self", ".", "proj_size", "!=", "self", ".", "rnn_output_size", ":", "\n", "            ", "self", ".", "proj", "=", "nn", ".", "Linear", "(", "self", ".", "rnn_output_size", ",", "self", ".", "proj_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.encoder.RnnEncoder.forward": [[31, 41], ["encoder.RnnEncoder.rnn", "encoder.RnnEncoder.contiguous().view", "mask.unsqueeze.unsqueeze.unsqueeze", "x_emb.size", "encoder.RnnEncoder.proj", "mask.unsqueeze.unsqueeze.float", "encoder.RnnEncoder.contiguous().view", "encoder.RnnEncoder.contiguous", "encoder.RnnEncoder.contiguous"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_emb", ",", "mask", ")", ":", "\n", "        ", "bs", ",", "l", "=", "x_emb", ".", "size", "(", ")", "[", ":", "2", "]", "\n", "pad_mask", "=", "(", "mask", "==", "0", ")", "\n", "outputs", "=", "self", ".", "rnn", "(", "x_emb", ",", "pad_mask", ")", "\n", "if", "self", ".", "proj_size", "!=", "self", ".", "rnn_output_size", ":", "\n", "            ", "outputs", "=", "self", ".", "proj", "(", "outputs", ".", "contiguous", "(", ")", ".", "view", "(", "bs", "*", "l", ",", "-", "1", ")", ")", "\n", "", "outputs", "=", "outputs", ".", "contiguous", "(", ")", ".", "view", "(", "bs", ",", "l", ",", "-", "1", ")", "\n", "mask", "=", "mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "outputs", "=", "outputs", "*", "mask", ".", "float", "(", ")", "\n", "return", "outputs", "\n", "", "", ""]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.__init__": [[50, 66], ["torch.Module.__init__", "layers.SizeDropout.register_buffer", "torch.ones.cuda", "torch.ones.cuda", "torch.ones.cuda", "torch.ones.cuda", "layers.SizeDropout.generate_mask", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.generate_mask"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "min_size", ",", "dim", ",", "rescale", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "min_size", "=", "min_size", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "dim", "=", "dim", "\n", "self", ".", "eval_size", "=", "input_size", "\n", "self", ".", "rescale", "=", "rescale", "\n", "if", "min_size", "<", "input_size", ":", "\n", "            ", "mask", "=", "torch", ".", "cat", "(", "[", "torch", ".", "ones", "(", "min_size", ")", ",", "torch", ".", "arange", "(", "input_size", "-", "min_size", ",", "0", ",", "-", "1", ")", "/", "(", "input_size", "-", "min_size", "+", "1", ")", "]", ",", "dim", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "mask", "=", "torch", ".", "ones", "(", "input_size", ")", "\n", "", "self", ".", "register_buffer", "(", "'mask'", ",", "torch", ".", "ones", "(", "input_size", ")", ")", "\n", "# self.register_buffer('eval_mask', mask)", "\n", "self", ".", "eval_mask", "=", "mask", ".", "cuda", "(", ")", "\n", "self", ".", "train_size", "=", "input_size", "\n", "self", ".", "generate_mask", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.sample_train_size": [[67, 72], ["min", "int", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand"], "methods", ["None"], ["", "def", "sample_train_size", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "input_size", "==", "self", ".", "min_size", ":", "\n", "            ", "return", "self", ".", "input_size", "\n", "", "self", ".", "train_size", "=", "self", ".", "min_size", "+", "min", "(", "int", "(", "torch", ".", "rand", "(", "1", ")", "[", "0", "]", "*", "(", "self", ".", "input_size", "-", "self", ".", "min_size", "+", "1", ")", ")", ",", "self", ".", "input_size", ")", "## take the min in case of getting 1 from torch.rand", "\n", "return", "self", ".", "train_size", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.generate_mask": [[73, 82], ["layers.SizeDropout.mask.clone", "range", "range", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "layers.SizeDropout.unsqueeze_", "layers.SizeDropout.unsqueeze_"], "methods", ["None"], ["", "def", "generate_mask", "(", "self", ",", "max_dim", ")", ":", "\n", "        ", "curr_mask", "=", "self", ".", "mask", ".", "clone", "(", ")", "\n", "if", "self", ".", "train_size", "<", "self", ".", "input_size", ":", "\n", "            ", "curr_mask", "[", "self", ".", "train_size", ":", "]", "=", "0", "\n", "", "for", "i", "in", "range", "(", "self", ".", "dim", ")", ":", "\n", "            ", "curr_mask", ".", "unsqueeze_", "(", "0", ")", "\n", "", "for", "i", "in", "range", "(", "self", ".", "dim", "+", "1", ",", "max_dim", ")", ":", "\n", "            ", "curr_mask", ".", "unsqueeze_", "(", "-", "1", ")", "\n", "", "self", ".", "curr_mask_var", "=", "Variable", "(", "curr_mask", ",", "requires_grad", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.generate_eval_mask": [[83, 95], ["range", "range", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "layers.SizeDropout.eval_mask.clone", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda", "torch.ones().cuda.unsqueeze_", "torch.ones().cuda.unsqueeze_", "torch.ones().cuda.unsqueeze_", "torch.ones().cuda.unsqueeze_", "torch.ones().cuda.unsqueeze_", "torch.ones().cuda.unsqueeze_", "torch.ones().cuda.unsqueeze_", "torch.ones().cuda.unsqueeze_", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["None"], ["", "def", "generate_eval_mask", "(", "self", ",", "max_dim", ")", ":", "\n", "        ", "if", "self", ".", "rescale", ":", "\n", "            ", "curr_mask", "=", "self", ".", "eval_mask", ".", "clone", "(", ")", "\n", "", "else", ":", "\n", "            ", "curr_mask", "=", "torch", ".", "ones", "(", "self", ".", "input_size", ")", ".", "cuda", "(", ")", "\n", "", "if", "self", ".", "eval_size", "<", "self", ".", "input_size", ":", "\n", "            ", "curr_mask", "[", "self", ".", "eval_size", ":", "]", "=", "0", "\n", "", "for", "i", "in", "range", "(", "self", ".", "dim", ")", ":", "\n", "            ", "curr_mask", ".", "unsqueeze_", "(", "0", ")", "\n", "", "for", "i", "in", "range", "(", "self", ".", "dim", "+", "1", ",", "max_dim", ")", ":", "\n", "            ", "curr_mask", ".", "unsqueeze_", "(", "-", "1", ")", "\n", "", "self", ".", "curr_eval_mask_var", "=", "Variable", "(", "curr_mask", ",", "requires_grad", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.forward": [[96, 114], ["x.size", "x.size", "layers.SizeDropout.generate_eval_mask", "layers.SizeDropout.sample_train_size", "layers.SizeDropout.generate_mask", "isinstance", "x.dim", "x.dim", "layers.SizeDropout.generate_mask", "x.dim", "layers.SizeDropout.curr_mask_var.dim", "type", "type", "x.dim"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.generate_eval_mask", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.sample_train_size", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.generate_mask", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.generate_mask"], ["", "def", "forward", "(", "self", ",", "x", ",", "resample", "=", "True", ",", "mask", "=", "None", ")", ":", "\n", "        ", "assert", "x", ".", "size", "(", "self", ".", "dim", ")", "==", "self", ".", "input_size", ",", "'x: {}, input_size: {}'", ".", "format", "(", "x", ".", "size", "(", ")", ",", "self", ".", "input_size", ")", "\n", "if", "self", ".", "input_size", "==", "self", ".", "min_size", ":", "\n", "            ", "return", "x", "\n", "", "if", "self", ".", "training", ":", "\n", "            ", "if", "resample", ":", "\n", "                ", "self", ".", "sample_train_size", "(", ")", "\n", "self", ".", "generate_mask", "(", "x", ".", "dim", "(", ")", ")", "\n", "", "elif", "isinstance", "(", "mask", ",", "Variable", ")", ":", "\n", "                ", "self", ".", "curr_mask_var", "=", "mask", "\n", "", "elif", "x", ".", "dim", "(", ")", "!=", "self", ".", "curr_mask_var", ".", "dim", "(", ")", "or", "type", "(", "x", ".", "data", ")", "!=", "type", "(", "self", ".", "curr_mask_var", ".", "data", ")", ":", "\n", "                ", "'''# of dim doesn't match generate the mask again'''", "\n", "self", ".", "generate_mask", "(", "x", ".", "dim", "(", ")", ")", "\n", "", "x", "=", "x", "*", "self", ".", "curr_mask_var", "\n", "", "else", ":", "\n", "            ", "self", ".", "generate_eval_mask", "(", "x", ".", "dim", "(", ")", ")", "\n", "x", "=", "x", "*", "self", ".", "curr_eval_mask_var", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SizeDropout.__repr__": [[115, 118], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "'{}(input_size={}, min_size={}, dim={}, rescale={}, eval_size={})'", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "input_size", ",", "self", ".", "min_size", ",", "self", ".", "dim", ",", "self", ".", "rescale", ",", "self", ".", "eval_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.LayerNorm.__init__": [[122, 127], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "features", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "features", ")", ")", "\n", "self", ".", "beta", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "features", ")", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.LayerNorm.forward": [[128, 132], ["x.mean", "x.std"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "mean", "=", "x", ".", "mean", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "std", "=", "x", ".", "std", "(", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "return", "self", ".", "gamma", "*", "(", "x", "-", "mean", ")", "/", "(", "std", "+", "self", ".", "eps", ")", "+", "self", ".", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.LayerNormChannelFirst.__init__": [[136, 141], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.ones().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.zeros().view", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "features", ",", "eps", "=", "1e-6", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "features", ")", ".", "view", "(", "1", ",", "-", "1", ",", "1", ")", ")", "\n", "self", ".", "beta", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "features", ")", ".", "view", "(", "1", ",", "-", "1", ",", "1", ")", ")", "\n", "self", ".", "eps", "=", "eps", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.LayerNormChannelFirst.forward": [[142, 146], ["x.mean", "x.std"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "mean", "=", "x", ".", "mean", "(", "-", "2", ",", "keepdim", "=", "True", ")", "\n", "std", "=", "x", ".", "std", "(", "-", "2", ",", "keepdim", "=", "True", ")", "\n", "return", "self", ".", "gamma", "*", "(", "x", "-", "mean", ")", "/", "(", "std", "+", "self", ".", "eps", ")", "+", "self", ".", "beta", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.StackedBRNN.__init__": [[152, 195], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "layers.StackedBRNN.rnns.append", "layers.StackedBRNN.rnns.append", "layers.StackedBRNN.sds.append", "layers.StackedBRNN.ses.append", "layers.SizeDropout", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid", "torch.Sigmoid"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "num_layers", ",", "\n", "dropout_rate", "=", "0", ",", "dropout_output", "=", "False", ",", "rnn_type", "=", "nn", ".", "LSTM", ",", "\n", "variational_dropout", "=", "True", ",", "\n", "residual", "=", "False", ",", "\n", "squeeze_excitation", "=", "0", ",", "\n", "sd_min_size", "=", "0", ",", "sd_rescale", "=", "True", ",", "\n", "concat_layers", "=", "False", ",", "padding", "=", "False", ")", ":", "\n", "        ", "super", "(", "StackedBRNN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "padding", "=", "padding", "\n", "self", ".", "dropout_output", "=", "dropout_output", "\n", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "variational_dropout", "=", "variational_dropout", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "residual", "=", "residual", "\n", "self", ".", "squeeze_excitation", "=", "squeeze_excitation", "\n", "self", ".", "concat_layers", "=", "concat_layers", "\n", "self", ".", "sd_min_size", "=", "sd_min_size", "\n", "self", ".", "sd_rescale", "=", "sd_rescale", "\n", "self", ".", "rnns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "rnn_type", "=", "rnn_type", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "input_size", "=", "input_size", "if", "i", "==", "0", "else", "2", "*", "hidden_size", "\n", "if", "rnn_type", "in", "self", ".", "SRU_TYPES", ":", "\n", "                ", "self", ".", "rnns", ".", "append", "(", "self", ".", "RNN_TYPES", "[", "rnn_type", "]", "(", "input_size", ",", "hidden_size", ",", "\n", "dropout", "=", "dropout_rate", ",", "\n", "rnn_dropout", "=", "dropout_rate", ",", "\n", "use_tanh", "=", "1", ",", "\n", "bidirectional", "=", "True", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "rnns", ".", "append", "(", "self", ".", "RNN_TYPES", "[", "rnn_type", "]", "(", "input_size", ",", "hidden_size", ",", "\n", "num_layers", "=", "1", ",", "\n", "bidirectional", "=", "True", ")", ")", "\n", "", "", "if", "sd_min_size", ">", "0", ":", "\n", "            ", "self", ".", "sds", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "                ", "self", ".", "sds", ".", "append", "(", "SizeDropout", "(", "hidden_size", ",", "sd_min_size", ",", "3", ",", "sd_rescale", ")", ")", "\n", "", "", "if", "squeeze_excitation", ">", "0", ":", "\n", "            ", "self", ".", "ses", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "                ", "self", ".", "ses", ".", "append", "(", "nn", ".", "Sequential", "(", "nn", ".", "Linear", "(", "hidden_size", "*", "2", ",", "hidden_size", "*", "2", "//", "self", ".", "squeeze_excitation", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Linear", "(", "hidden_size", "*", "2", "//", "self", ".", "squeeze_excitation", ",", "hidden_size", "*", "2", ")", ",", "\n", "nn", ".", "Sigmoid", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.StackedBRNN.forward": [[196, 209], ["layers.StackedBRNN._forward_unpadded", "x_mask.data.sum", "layers.StackedBRNN._forward_unpadded", "layers.StackedBRNN._forward_padded"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.StackedBRNN._forward_unpadded", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.StackedBRNN._forward_unpadded", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.StackedBRNN._forward_padded"], ["", "", "", "def", "forward", "(", "self", ",", "x", ",", "x_mask", ")", ":", "\n", "        ", "\"\"\"Can choose to either handle or ignore variable length sequences.\n        Always handle padding in eval.\n        \"\"\"", "\n", "# No padding necessary.", "\n", "if", "x_mask", ".", "data", ".", "sum", "(", ")", "==", "0", ":", "\n", "            ", "return", "self", ".", "_forward_unpadded", "(", "x", ",", "x_mask", ")", "\n", "# Pad if we care or if its during eval.", "\n", "# if (self.padding or not self.training) and not self.rnn_type == 'sru':", "\n", "", "if", "self", ".", "padding", "and", "not", "self", ".", "rnn_type", "in", "self", ".", "SRU_TYPES", ":", "\n", "            ", "return", "self", ".", "_forward_padded", "(", "x", ",", "x_mask", ")", "\n", "# We don't care.", "\n", "", "return", "self", ".", "_forward_unpadded", "(", "x", ",", "x_mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.StackedBRNN._forward_unpadded": [[210, 252], ["x.transpose.transpose.transpose", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "range", "dropout.transpose", "x_mask.data.eq().long().sum().squeeze().float().unsqueeze", "outputs.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "layers.dropout", "layers.dropout", "rnn_output.size", "x_mask.data.eq().long().sum().squeeze().float", "rnn_output.size", "dropout.size", "x_mask.data.eq().long().sum().squeeze", "rnn_output.view", "x_mask.data.eq().long().sum", "rnn_output.sum", "x_mask.data.eq().long", "x_mask.data.eq"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "_forward_unpadded", "(", "self", ",", "x", ",", "x_mask", ")", ":", "\n", "        ", "\"\"\"Faster encoding that ignores any padding.\"\"\"", "\n", "# Transpose batch and sequence dims", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "lengths_var", "=", "Variable", "(", "x_mask", ".", "data", ".", "eq", "(", "0", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ".", "squeeze", "(", ")", ".", "float", "(", ")", ".", "unsqueeze", "(", "1", ")", ",", "requires_grad", "=", "False", ")", "\n", "\n", "# Encode all layers", "\n", "outputs", "=", "[", "x", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "rnn_input", "=", "outputs", "[", "-", "1", "]", "\n", "\n", "# Apply dropout to hidden input", "\n", "if", "self", ".", "dropout_rate", ">", "0", "and", "self", ".", "rnn_type", "not", "in", "self", ".", "SRU_TYPES", ":", "\n", "                ", "rnn_input", "=", "dropout", "(", "rnn_input", ",", "p", "=", "self", ".", "dropout_rate", ",", "training", "=", "self", ".", "training", ",", "\n", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "# Forward", "\n", "", "rnn_output", "=", "self", ".", "rnns", "[", "i", "]", "(", "rnn_input", ")", "[", "0", "]", "\n", "if", "self", ".", "residual", "and", "rnn_output", ".", "size", "(", ")", "==", "rnn_input", ".", "size", "(", ")", ":", "\n", "                ", "rnn_output", "=", "rnn_output", "+", "outputs", "[", "-", "1", "]", "\n", "\n", "", "if", "self", ".", "sd_min_size", ">", "0", ":", "\n", "                ", "bs", ",", "l", ",", "hs", "=", "rnn_output", ".", "size", "(", ")", "\n", "rnn_output", "=", "self", ".", "sds", "[", "i", "]", "(", "rnn_output", ".", "view", "(", "bs", ",", "l", ",", "2", ",", "hs", "//", "2", ")", ")", ".", "view", "(", "bs", ",", "l", ",", "hs", ")", "\n", "\n", "", "if", "self", ".", "squeeze_excitation", ">", "0", ":", "\n", "                ", "rnn_output", "=", "rnn_output", "*", "self", ".", "ses", "[", "i", "]", "(", "rnn_output", ".", "sum", "(", "0", ")", "/", "lengths_var", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "outputs", ".", "append", "(", "rnn_output", ")", "\n", "\n", "# Concat hidden layers", "\n", "", "if", "self", ".", "concat_layers", ":", "\n", "            ", "output", "=", "torch", ".", "cat", "(", "outputs", "[", "1", ":", "]", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "outputs", "[", "-", "1", "]", "\n", "\n", "# Transpose back", "\n", "", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# Dropout on output layer", "\n", "if", "self", ".", "dropout_output", "and", "self", ".", "dropout_rate", ">", "0", ":", "\n", "            ", "output", "=", "dropout", "(", "output", ",", "p", "=", "self", ".", "dropout_rate", ",", "training", "=", "self", ".", "training", ",", "\n", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.StackedBRNN._forward_padded": [[253, 334], ["x_mask.data.eq().long().sum().squeeze", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "list", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "x.transpose.transpose.index_select", "x.transpose.transpose.transpose", "range", "torch.dropout.transpose", "torch.dropout.index_select", "lengths[].float().unsqueeze", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "outputs.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.dropout.size", "x_mask.size", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.zeros().type", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout", "x_mask.data.eq().long().sum", "layers.dropout", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "rnn_output.size", "torch.dropout.data.type", "lengths[].float", "rnn_output.size", "outputs[].size", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "x_mask.data.eq().long", "torch.dropout.size", "torch.dropout.size", "rnn_output.view", "x_mask.size", "torch.dropout.size", "x_mask.data.eq", "rnn_output.sum"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "_forward_padded", "(", "self", ",", "x", ",", "x_mask", ")", ":", "\n", "        ", "\"\"\"Slower (significantly), but more precise,\n        encoding that handles padding.\"\"\"", "\n", "# Compute sorted sequence lengths", "\n", "lengths", "=", "x_mask", ".", "data", ".", "eq", "(", "0", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ".", "squeeze", "(", ")", "\n", "_", ",", "idx_sort", "=", "torch", ".", "sort", "(", "lengths", ",", "dim", "=", "0", ",", "descending", "=", "True", ")", "\n", "_", ",", "idx_unsort", "=", "torch", ".", "sort", "(", "idx_sort", ",", "dim", "=", "0", ")", "\n", "\n", "lengths_var", "=", "Variable", "(", "lengths", "[", "idx_sort", "]", ".", "float", "(", ")", ".", "unsqueeze", "(", "1", ")", ",", "requires_grad", "=", "False", ")", "\n", "lengths", "=", "list", "(", "lengths", "[", "idx_sort", "]", ")", "\n", "idx_sort", "=", "Variable", "(", "idx_sort", ")", "\n", "idx_unsort", "=", "Variable", "(", "idx_unsort", ")", "\n", "\n", "# Sort x", "\n", "x", "=", "x", ".", "index_select", "(", "0", ",", "idx_sort", ")", "\n", "\n", "# Transpose batch and sequence dims", "\n", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# Pack it up", "\n", "# rnn_input = nn.utils.rnn.pack_padded_sequence(x, lengths)", "\n", "\n", "# Encode all layers", "\n", "outputs", "=", "[", "x", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "rnn_input", "=", "outputs", "[", "-", "1", "]", "\n", "\n", "# Apply dropout to input", "\n", "if", "self", ".", "dropout_rate", ">", "0", ":", "\n", "                ", "rnn_input", "=", "dropout", "(", "rnn_input", ",", "p", "=", "self", ".", "dropout_rate", ",", "training", "=", "self", ".", "training", ",", "\n", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "", "rnn_input", "=", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "rnn_input", ",", "lengths", ")", "\n", "\n", "# if self.dropout_rate > 0:", "\n", "#     dropout_input = F.dropout(rnn_input.data,", "\n", "#                               p=self.dropout_rate,", "\n", "#                               training=self.training)", "\n", "#     rnn_input = nn.utils.rnn.PackedSequence(dropout_input,", "\n", "#                                             rnn_input.batch_sizes)", "\n", "\n", "rnn_output", "=", "self", ".", "rnns", "[", "i", "]", "(", "rnn_input", ")", "[", "0", "]", "\n", "rnn_output", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "rnn_output", ")", "[", "0", "]", "\n", "\n", "if", "self", ".", "residual", "and", "rnn_output", ".", "size", "(", ")", "==", "outputs", "[", "-", "1", "]", ".", "size", "(", ")", ":", "\n", "                ", "rnn_output", "=", "rnn_output", "+", "outputs", "[", "-", "1", "]", "\n", "\n", "", "if", "self", ".", "sd_min_size", ">", "0", ":", "\n", "                ", "bs", ",", "l", ",", "hs", "=", "rnn_output", ".", "size", "(", ")", "\n", "rnn_output", "=", "self", ".", "sds", "[", "i", "]", "(", "rnn_output", ".", "view", "(", "bs", ",", "l", ",", "2", ",", "hs", "//", "2", ")", ")", ".", "view", "(", "bs", ",", "l", ",", "hs", ")", "\n", "\n", "", "if", "self", ".", "squeeze_excitation", ">", "0", ":", "\n", "                ", "rnn_output", "=", "rnn_output", "*", "self", ".", "ses", "[", "i", "]", "(", "rnn_output", ".", "sum", "(", "0", ")", "/", "lengths_var", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "outputs", ".", "append", "(", "rnn_output", ")", "\n", "\n", "# Unpack everything", "\n", "# for i, o in enumerate(outputs[1:], 1):", "\n", "# outputs[i] = nn.utils.rnn.pad_packed_sequence(o)[0]", "\n", "\n", "# Concat hidden layers or take final", "\n", "", "if", "self", ".", "concat_layers", ":", "\n", "            ", "output", "=", "torch", ".", "cat", "(", "outputs", "[", "1", ":", "]", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "output", "=", "outputs", "[", "-", "1", "]", "\n", "\n", "# Transpose and unsort", "\n", "", "output", "=", "output", ".", "transpose", "(", "0", ",", "1", ")", "\n", "output", "=", "output", ".", "index_select", "(", "0", ",", "idx_unsort", ")", "\n", "\n", "# Pad up to original batch sequence length", "\n", "if", "output", ".", "size", "(", "1", ")", "!=", "x_mask", ".", "size", "(", "1", ")", ":", "\n", "            ", "padding", "=", "torch", ".", "zeros", "(", "output", ".", "size", "(", "0", ")", ",", "\n", "x_mask", ".", "size", "(", "1", ")", "-", "output", ".", "size", "(", "1", ")", ",", "\n", "output", ".", "size", "(", "2", ")", ")", ".", "type", "(", "output", ".", "data", ".", "type", "(", ")", ")", "\n", "output", "=", "torch", ".", "cat", "(", "[", "output", ",", "Variable", "(", "padding", ")", "]", ",", "1", ")", "\n", "\n", "# Dropout on output layer", "\n", "", "if", "self", ".", "dropout_output", "and", "self", ".", "dropout_rate", ">", "0", ":", "\n", "            ", "output", "=", "F", ".", "dropout", "(", "output", ",", "\n", "p", "=", "self", ".", "dropout_rate", ",", "\n", "training", "=", "self", ".", "training", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.DilatedResNet.__init__": [[339, 404], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.DilatedResNet.cnns.append", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.GLU", "torch.GLU", "torch.GLU", "torch.GLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "str", "Block"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "num_layers", ",", "\n", "dilation_layers", "=", "1", ",", "dilation_base", "=", "1", ",", "dilation_offset", "=", "0", ",", "\n", "input_padding", "=", "0", ",", "masked", "=", "True", ",", "\n", "growing_mode", "=", "'block'", ",", "# ['block', 'layer']", "\n", "block_type", "=", "'dilated_conv'", ",", "# ['dilated_conv', 'dilated_sep_conv', 'sep_conv']", "\n", "activation_type", "=", "'glu'", ",", "# ['glu', 'relu']", "\n", "dropout_rate", "=", "0", ",", "dropout_output", "=", "False", ")", ":", "\n", "        ", "super", "(", "DilatedResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# self.padding = padding", "\n", "self", ".", "dropout_output", "=", "dropout_output", "\n", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "input_padding", "=", "input_padding", "\n", "# self.concat_layers = concat_layers", "\n", "if", "activation_type", "==", "'glu'", ":", "\n", "            ", "self", ".", "reduce_block", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "input_size", ",", "hidden_size", "*", "2", ",", "3", ",", "padding", "=", "1", "+", "input_padding", ")", ",", "\n", "nn", ".", "GLU", "(", "dim", "=", "1", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "reduce_block", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "input_size", ",", "hidden_size", ",", "3", ",", "padding", "=", "1", "+", "input_padding", ")", ",", "\n", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ")", "\n", "", "self", ".", "cnns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "masked", "=", "masked", "\n", "assert", "num_layers", "%", "2", "==", "1", ",", "'num_layers='", "+", "str", "(", "num_layers", ")", "+", "' is not odd'", "\n", "for", "i", "in", "range", "(", "num_layers", "//", "2", ")", ":", "\n", "            ", "if", "block_type", "==", "'sep_conv'", ":", "\n", "                ", "if", "growing_mode", "==", "'block'", ":", "\n", "                    ", "kernel_size", "=", "2", "**", "(", "i", "-", "dilation_offset", "+", "2", ")", "-", "1", "if", "dilation_offset", "<=", "i", "<", "dilation_offset", "+", "dilation_layers", "else", "3", "\n", "kernel_size", "=", "(", "kernel_size", ",", "kernel_size", ")", "\n", "", "elif", "growing_mode", "==", "'layer'", ":", "\n", "                    ", "kernel_size", "=", "[", "1", ",", "1", "]", "\n", "kernel_size", "[", "0", "]", "=", "2", "**", "(", "2", "*", "i", "+", "2", "-", "dilation_offset", ")", "-", "1", "if", "dilation_offset", "<=", "(", "2", "*", "i", "+", "1", ")", "<", "dilation_offset", "+", "dilation_layers", "else", "3", "\n", "kernel_size", "[", "1", "]", "=", "2", "**", "(", "2", "*", "i", "+", "3", "-", "dilation_offset", ")", "-", "1", "if", "dilation_offset", "<=", "(", "2", "*", "i", "+", "2", ")", "<", "dilation_offset", "+", "dilation_layers", "else", "3", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "\n", "", "dilation", "=", "1", "\n", "padding", "=", "(", "kernel_size", "[", "0", "]", "//", "2", ",", "kernel_size", "[", "1", "]", "//", "2", ")", "\n", "", "elif", "block_type", "in", "{", "'dilated_conv'", ",", "'dilated_sep_conv'", "}", ":", "\n", "                ", "if", "growing_mode", "==", "'block'", ":", "\n", "                    ", "dilation", "=", "dilation_base", "**", "(", "i", "-", "dilation_offset", "+", "1", ")", "if", "dilation_offset", "<=", "i", "<", "dilation_offset", "+", "dilation_layers", "else", "1", "\n", "", "elif", "growing_mode", "==", "'layer'", ":", "\n", "                    ", "dilation", "=", "[", "1", ",", "1", "]", "\n", "dilation", "[", "0", "]", "=", "dilation_base", "**", "(", "2", "*", "i", "+", "1", "-", "dilation_offset", ")", "if", "dilation_offset", "<=", "(", "2", "*", "i", "+", "1", ")", "<", "dilation_offset", "+", "dilation_layers", "else", "1", "\n", "dilation", "[", "1", "]", "=", "dilation_base", "**", "(", "2", "*", "i", "+", "2", "-", "dilation_offset", ")", "if", "dilation_offset", "<=", "(", "2", "*", "i", "+", "2", ")", "<", "dilation_offset", "+", "dilation_layers", "else", "1", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "\n", "", "padding", "=", "dilation", "\n", "kernel_size", "=", "3", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "if", "block_type", "==", "'dilated_conv'", ":", "\n", "                ", "Block", "=", "GLUResBlock", "\n", "", "elif", "block_type", "in", "{", "'dilated_sep_conv'", ",", "'sep_conv'", "}", ":", "\n", "                ", "Block", "=", "GLUResBlock_sep", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "\n", "\n", "", "self", ".", "cnns", ".", "append", "(", "Block", "(", "hidden_size", ",", "hidden_size", ",", "\n", "hidden_size", ",", "kernel_size", "=", "kernel_size", ",", "\n", "padding", "=", "padding", ",", "\n", "dilation", "=", "dilation", ",", "\n", "dropout_rate", "=", "dropout_rate", ",", "\n", "activation_type", "=", "activation_type", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.DilatedResNet.forward": [[405, 423], ["torch.dropout.transpose", "layers.DilatedResNet.reduce_block", "torch.dropout.transpose", "torch.dropout.contiguous", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout", "cnn", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "x_mask.unsqueeze().unsqueeze", "x_mask.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "x_mask", "=", "None", ")", ":", "\n", "# swap filter dim and sequence dim", "\n", "        ", "if", "self", ".", "input_padding", ">", "0", "and", "self", ".", "masked", "and", "x_mask", "is", "not", "None", ":", "\n", "            ", "x_mask", "=", "F", ".", "pad", "(", "x_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ",", "(", "self", ".", "input_padding", ",", "self", ".", "input_padding", ",", "0", ",", "0", ")", ",", "'constant'", ",", "True", ")", "[", ":", ",", "0", ",", "0", ",", ":", "]", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "if", "self", ".", "dropout_output", "and", "self", ".", "dropout_rate", ">", "0", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_rate", ",", "\n", "training", "=", "self", ".", "training", ")", "\n", "", "x", "=", "self", ".", "reduce_block", "(", "x", ")", "\n", "for", "cnn", "in", "self", ".", "cnns", ":", "\n", "            ", "x", "=", "cnn", "(", "x", ",", "x_mask", ")", "\n", "\n", "# Dropout on output layer", "\n", "", "if", "self", ".", "dropout_output", "and", "self", ".", "dropout_rate", ">", "0", ":", "\n", "            ", "x", "=", "F", ".", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_rate", ",", "\n", "training", "=", "self", ".", "training", ")", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.GLUResBlock.__init__": [[430, 461], ["torch.Module.__init__", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "type", "type", "type", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.GLU", "torch.GLU", "torch.GLU", "torch.GLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.GLU", "torch.GLU", "torch.GLU", "torch.GLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "output_size", ",", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ",", "groups", "=", "1", ",", "dilation", "=", "1", ",", "dropout_rate", "=", "0", ",", "activation_type", "=", "'glu'", ")", ":", "\n", "        ", "super", "(", "GLUResBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "type", "(", "dilation", ")", "is", "int", ":", "\n", "            ", "dilation", "=", "(", "dilation", ",", "dilation", ")", "\n", "", "if", "type", "(", "kernel_size", ")", "is", "int", ":", "\n", "            ", "kernel_size", "=", "(", "kernel_size", ",", "kernel_size", ")", "\n", "", "if", "type", "(", "padding", ")", "is", "int", ":", "\n", "            ", "padding", "=", "(", "padding", ",", "padding", ")", "\n", "\n", "", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "drop1", "=", "nn", ".", "Dropout2d", "(", "dropout_rate", ")", "\n", "self", ".", "activation_type", "=", "activation_type", "\n", "if", "activation_type", "==", "'glu'", ":", "\n", "            ", "self", ".", "conv1", "=", "nn", ".", "Conv1d", "(", "input_size", ",", "hidden_size", "*", "2", ",", "kernel_size", "[", "0", "]", ",", "\n", "padding", "=", "padding", "[", "0", "]", ",", "dilation", "=", "dilation", "[", "0", "]", ")", "\n", "self", ".", "act1", "=", "nn", ".", "GLU", "(", "dim", "=", "1", ")", "\n", "", "elif", "activation_type", "==", "'relu'", ":", "\n", "            ", "self", ".", "conv1", "=", "nn", ".", "Conv1d", "(", "input_size", ",", "hidden_size", ",", "kernel_size", "[", "0", "]", ",", "\n", "padding", "=", "padding", "[", "0", "]", ",", "dilation", "=", "dilation", "[", "0", "]", ")", "\n", "self", ".", "act1", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "", "self", ".", "drop2", "=", "nn", ".", "Dropout2d", "(", "dropout_rate", ")", "\n", "if", "activation_type", "==", "'glu'", ":", "\n", "            ", "self", ".", "conv2", "=", "nn", ".", "Conv1d", "(", "hidden_size", ",", "output_size", "*", "2", ",", "kernel_size", "[", "1", "]", ",", "\n", "padding", "=", "padding", "[", "1", "]", ",", "dilation", "=", "dilation", "[", "1", "]", ")", "\n", "self", ".", "act2", "=", "nn", ".", "GLU", "(", "dim", "=", "1", ")", "\n", "", "elif", "activation_type", "==", "'relu'", ":", "\n", "            ", "self", ".", "conv2", "=", "nn", ".", "Conv1d", "(", "hidden_size", ",", "output_size", ",", "kernel_size", "[", "1", "]", ",", "\n", "padding", "=", "padding", "[", "1", "]", ",", "dilation", "=", "dilation", "[", "1", "]", ")", "\n", "self", ".", "act2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.GLUResBlock.forward": [[462, 482], ["layers.GLUResBlock.act1", "layers.GLUResBlock.act2", "layers.GLUResBlock.drop1", "layers.GLUResBlock.conv1", "layers.GLUResBlock.masked_fill_", "layers.GLUResBlock.drop2", "layers.GLUResBlock.conv2", "layers.GLUResBlock.masked_fill_", "x.size", "layers.GLUResBlock.size", "layers.GLUResBlock.unsqueeze", "x_mask.unsqueeze", "layers.GLUResBlock.unsqueeze", "x_mask.unsqueeze", "x.size", "layers.GLUResBlock.size", "layers.GLUResBlock.size", "x.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "x_mask", "=", "None", ",", "masked", "=", "True", ")", ":", "\n", "        ", "res", "=", "x", "\n", "res", "=", "self", ".", "drop1", "(", "res", ".", "unsqueeze", "(", "3", ")", ")", "[", ":", ",", ":", ",", ":", ",", "0", "]", "\n", "res", "=", "self", ".", "act1", "(", "self", ".", "conv1", "(", "x", ")", ")", "\n", "if", "masked", "and", "x_mask", "is", "not", "None", ":", "\n", "            ", "res", ".", "masked_fill_", "(", "x_mask", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "", "res", "=", "self", ".", "drop2", "(", "res", ".", "unsqueeze", "(", "3", ")", ")", "[", ":", ",", ":", ",", ":", ",", "0", "]", "\n", "res", "=", "self", ".", "act2", "(", "self", ".", "conv2", "(", "x", ")", ")", "\n", "if", "masked", "and", "x_mask", "is", "not", "None", ":", "\n", "            ", "res", ".", "masked_fill_", "(", "x_mask", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "", "if", "x", ".", "size", "(", "1", ")", "==", "res", ".", "size", "(", "1", ")", ":", "\n", "            ", "x", "=", "x", "+", "res", "\n", "", "elif", "x", ".", "size", "(", "1", ")", ">", "res", ".", "size", "(", "1", ")", ":", "\n", "            ", "res", "=", "res", "+", "x", "[", ":", ",", ":", "res", ".", "size", "(", "1", ")", "]", "\n", "x", "=", "res", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "+", "res", "[", ":", ",", ":", "x", ".", "size", "(", "1", ")", "]", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.GLUResBlock_sep.__init__": [[489, 524], ["torch.Module.__init__", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "torch.Dropout2d", "type", "type", "type", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.GLU", "torch.GLU", "torch.GLU", "torch.GLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.GLU", "torch.GLU", "torch.GLU", "torch.GLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "output_size", ",", "kernel_size", "=", "3", ",", "\n", "padding", "=", "1", ",", "groups", "=", "1", ",", "dilation", "=", "1", ",", "dropout_rate", "=", "0", ",", "activation_type", "=", "'glu'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "type", "(", "dilation", ")", "is", "int", ":", "\n", "            ", "dilation", "=", "(", "dilation", ",", "dilation", ")", "\n", "", "if", "type", "(", "kernel_size", ")", "is", "int", ":", "\n", "            ", "kernel_size", "=", "(", "kernel_size", ",", "kernel_size", ")", "\n", "", "if", "type", "(", "padding", ")", "is", "int", ":", "\n", "            ", "padding", "=", "(", "padding", ",", "padding", ")", "\n", "\n", "", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "drop1", "=", "nn", ".", "Dropout2d", "(", "dropout_rate", ")", "\n", "self", ".", "activation_type", "=", "activation_type", "\n", "if", "activation_type", "==", "'glu'", ":", "\n", "            ", "self", ".", "conv1_1", "=", "nn", ".", "Conv1d", "(", "input_size", ",", "input_size", ",", "kernel_size", "[", "0", "]", ",", "\n", "groups", "=", "input_size", ",", "padding", "=", "padding", "[", "0", "]", ",", "dilation", "=", "dilation", "[", "0", "]", ")", "\n", "self", ".", "conv1_2", "=", "nn", ".", "Conv1d", "(", "input_size", ",", "hidden_size", "*", "2", ",", "1", ")", "\n", "self", ".", "act1", "=", "nn", ".", "GLU", "(", "dim", "=", "1", ")", "\n", "", "elif", "activation_type", "==", "'relu'", ":", "\n", "            ", "self", ".", "conv1_1", "=", "nn", ".", "Conv1d", "(", "input_size", ",", "input_size", ",", "kernel_size", "[", "0", "]", ",", "\n", "groups", "=", "input_size", ",", "padding", "=", "padding", "[", "0", "]", ",", "dilation", "=", "dilation", "[", "0", "]", ")", "\n", "self", ".", "conv1_2", "=", "nn", ".", "Conv1d", "(", "input_size", ",", "hidden_size", ",", "1", ")", "\n", "self", ".", "act1", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n", "", "self", ".", "drop2", "=", "nn", ".", "Dropout2d", "(", "dropout_rate", ")", "\n", "if", "activation_type", "==", "'glu'", ":", "\n", "            ", "self", ".", "conv2_1", "=", "nn", ".", "Conv1d", "(", "hidden_size", ",", "hidden_size", ",", "kernel_size", "[", "1", "]", ",", "\n", "groups", "=", "hidden_size", ",", "padding", "=", "padding", "[", "1", "]", ",", "dilation", "=", "dilation", "[", "1", "]", ")", "\n", "self", ".", "conv2_2", "=", "nn", ".", "Conv1d", "(", "hidden_size", ",", "output_size", "*", "2", ",", "1", ")", "\n", "self", ".", "act2", "=", "nn", ".", "GLU", "(", "dim", "=", "1", ")", "\n", "", "elif", "activation_type", "==", "'relu'", ":", "\n", "            ", "self", ".", "conv2_1", "=", "nn", ".", "Conv1d", "(", "hidden_size", ",", "hidden_size", ",", "kernel_size", "[", "1", "]", ",", "\n", "groups", "=", "hidden_size", ",", "padding", "=", "padding", "[", "1", "]", ",", "dilation", "=", "dilation", "[", "1", "]", ")", "\n", "self", ".", "conv2_2", "=", "nn", ".", "Conv1d", "(", "hidden_size", ",", "output_size", ",", "1", ")", "\n", "self", ".", "act2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.GLUResBlock_sep.forward": [[525, 545], ["layers.GLUResBlock_sep.drop1().squeeze", "layers.GLUResBlock_sep.act1", "layers.GLUResBlock_sep.drop2().squeeze", "layers.GLUResBlock_sep.act2", "layers.GLUResBlock_sep.conv1_2", "layers.GLUResBlock_sep.masked_fill_", "layers.GLUResBlock_sep.conv2_2", "layers.GLUResBlock_sep.masked_fill_", "x.size", "layers.GLUResBlock_sep.size", "layers.GLUResBlock_sep.drop1", "layers.GLUResBlock_sep.conv1_1", "x_mask.unsqueeze", "layers.GLUResBlock_sep.drop2", "layers.GLUResBlock_sep.conv2_1", "x_mask.unsqueeze", "x.size", "layers.GLUResBlock_sep.size", "layers.GLUResBlock_sep.unsqueeze", "layers.GLUResBlock_sep.unsqueeze", "layers.GLUResBlock_sep.size", "x.size"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "x_mask", "=", "None", ",", "masked", "=", "True", ")", ":", "\n", "        ", "res", "=", "x", "\n", "res", "=", "self", ".", "drop1", "(", "res", ".", "unsqueeze", "(", "3", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "res", "=", "self", ".", "act1", "(", "self", ".", "conv1_2", "(", "self", ".", "conv1_1", "(", "x", ")", ")", ")", "\n", "if", "masked", "and", "x_mask", "is", "not", "None", ":", "\n", "            ", "res", ".", "masked_fill_", "(", "x_mask", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "", "res", "=", "self", ".", "drop2", "(", "res", ".", "unsqueeze", "(", "3", ")", ")", ".", "squeeze", "(", "3", ")", "\n", "res", "=", "self", ".", "act2", "(", "self", ".", "conv2_2", "(", "self", ".", "conv2_1", "(", "x", ")", ")", ")", "\n", "if", "masked", "and", "x_mask", "is", "not", "None", ":", "\n", "            ", "res", ".", "masked_fill_", "(", "x_mask", ".", "unsqueeze", "(", "1", ")", ",", "0", ")", "\n", "\n", "", "if", "x", ".", "size", "(", "1", ")", "==", "res", ".", "size", "(", "1", ")", ":", "\n", "            ", "x", "=", "x", "+", "res", "\n", "", "elif", "x", ".", "size", "(", "1", ")", ">", "res", ".", "size", "(", "1", ")", ":", "\n", "            ", "res", "=", "res", "+", "x", "[", ":", ",", ":", "res", ".", "size", "(", "1", ")", "]", "\n", "x", "=", "res", "\n", "", "else", ":", "\n", "            ", "x", "=", "x", "+", "res", "[", ":", ",", ":", "x", ".", "size", "(", "1", ")", "]", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MLP.__init__": [[547, 560], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "layers.MLP.linears.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "num_layers", ",", "\n", "dropout_rate", "=", "0", ",", "variational_dropout", "=", "True", ",", "\n", "concat_layers", "=", "False", ",", "output_act", "=", "True", ")", ":", "\n", "        ", "super", "(", "MLP", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "variational_dropout", "=", "variational_dropout", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "concat_layers", "=", "concat_layers", "\n", "self", ".", "linears", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "output_act", "=", "output_act", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "input_size", "=", "input_size", "if", "i", "==", "0", "else", "hidden_size", "\n", "self", ".", "linears", ".", "append", "(", "nn", ".", "Linear", "(", "input_size", ",", "hidden_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MLP.forward": [[561, 581], ["torch.relu.size", "range", "len", "torch.relu.view().contiguous", "hiddens.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "torch.relu.view().contiguous", "layers.dropout", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.view", "torch.relu.view"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "original_size", "=", "x", ".", "size", "(", ")", "\n", "if", "len", "(", "original_size", ")", "==", "3", ":", "\n", "            ", "x", "=", "x", ".", "view", "(", "-", "1", ",", "original_size", "[", "2", "]", ")", ".", "contiguous", "(", ")", "\n", "\n", "", "hiddens", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "if", "self", ".", "dropout_rate", ">", "0.", ":", "\n", "                ", "x", "=", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_rate", ",", "training", "=", "self", ".", "training", ",", "\n", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "", "if", "i", "<", "self", ".", "num_layers", "-", "1", "or", "self", ".", "output_act", ":", "\n", "                ", "x", "=", "F", ".", "relu", "(", "self", ".", "linears", "[", "i", "]", "(", "x", ")", ",", "inplace", "=", "True", ")", "\n", "", "hiddens", ".", "append", "(", "x", ")", "\n", "\n", "", "if", "self", ".", "concat_layers", ":", "\n", "            ", "x", "=", "torch", ".", "cat", "(", "hiddens", ",", "2", ")", "\n", "\n", "", "if", "len", "(", "original_size", ")", "==", "3", ":", "\n", "            ", "x", "=", "x", ".", "view", "(", "original_size", "[", "0", "]", ",", "original_size", "[", "1", "]", ",", "-", "1", ")", ".", "contiguous", "(", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SeqAttnMatch.__init__": [[588, 598], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", "=", "None", ",", "identity", "=", "False", ",", "dropout", "=", "0.", ",", "variational_dropout", "=", "False", ")", ":", "\n", "        ", "super", "(", "SeqAttnMatch", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "variational_dropout", "=", "variational_dropout", "\n", "if", "hidden_size", "is", "None", ":", "\n", "            ", "hidden_size", "=", "input_size", "\n", "", "if", "not", "identity", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "input_size", ",", "hidden_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.SeqAttnMatch.forward": [[599, 643], ["torch.relu.bmm", "y_mask.repeat.repeat.unsqueeze().expand", "scores_hook.data.masked_fill_", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax.view", "F.softmax.view.bmm", "dropout.repeat", "y_mask.repeat.repeat.repeat", "x.repeat.repeat.size", "x.repeat.repeat.size", "dropout.size", "layers.dropout", "layers.dropout", "layers.SeqAttnMatch.linear().view", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "layers.SeqAttnMatch.linear().view", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu.transpose", "scores_hook", "scores_hook.size", "scores_hook.view", "x.repeat.repeat.size", "dropout.size", "dropout.size", "x.repeat.repeat.size", "x.repeat.repeat.size", "x.repeat.repeat.size", "x.repeat.repeat.repeat", "y_mask.repeat.repeat.unsqueeze", "float", "dropout.size", "x.repeat.repeat.size", "dropout.size", "dropout.size", "layers.SeqAttnMatch.linear", "layers.SeqAttnMatch.linear", "x.repeat.repeat.view", "dropout.view", "x.repeat.repeat.size", "dropout.size"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "y_mask", ",", "scores_hook", "=", "None", ")", ":", "\n", "        ", "\"\"\"Input shapes:\n            x = batch * len1 * h\n            y = batch * len2 * h\n            y_mask = batch * len2\n        Output shapes:\n            matched_seq = batch * len1 * h\n        \"\"\"", "\n", "if", "y", ".", "size", "(", "0", ")", "==", "1", "and", "x", ".", "size", "(", "0", ")", ">", "1", ":", "\n", "            ", "y", "=", "y", ".", "repeat", "(", "x", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "y_mask", "=", "y_mask", ".", "repeat", "(", "x", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "", "elif", "x", ".", "size", "(", "0", ")", "==", "1", "and", "y", ".", "size", "(", "0", ")", ">", "1", ":", "\n", "            ", "x", "=", "x", ".", "repeat", "(", "y", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "# Project vectors", "\n", "", "if", "self", ".", "linear", "is", "not", "None", ":", "\n", "            ", "batch_size", "=", "x", ".", "size", "(", "0", ")", "\n", "len1", "=", "x", ".", "size", "(", "1", ")", "\n", "len2", "=", "y", ".", "size", "(", "1", ")", "\n", "x", "=", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ",", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "y", "=", "dropout", "(", "y", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ",", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "x_proj", "=", "self", ".", "linear", "(", "x", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "2", ")", ")", ")", ".", "view", "(", "batch_size", ",", "len1", ",", "-", "1", ")", "\n", "x_proj", "=", "F", ".", "relu", "(", "x_proj", ")", "\n", "y_proj", "=", "self", ".", "linear", "(", "y", ".", "view", "(", "-", "1", ",", "y", ".", "size", "(", "2", ")", ")", ")", ".", "view", "(", "batch_size", ",", "len2", ",", "-", "1", ")", "\n", "y_proj", "=", "F", ".", "relu", "(", "y_proj", ")", "\n", "", "else", ":", "\n", "            ", "x_proj", "=", "x", "\n", "y_proj", "=", "y", "\n", "\n", "# Compute scores", "\n", "", "scores", "=", "x_proj", ".", "bmm", "(", "y_proj", ".", "transpose", "(", "2", ",", "1", ")", ")", "\n", "if", "scores_hook", "is", "not", "None", ":", "\n", "            ", "scores", "=", "scores_hook", "(", "scores", ")", "\n", "\n", "# Mask padding", "\n", "", "y_mask", "=", "y_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "scores", ".", "size", "(", ")", ")", "\n", "scores", ".", "data", ".", "masked_fill_", "(", "y_mask", ".", "data", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "\n", "# Normalize with softmax", "\n", "alpha_flat", "=", "F", ".", "softmax", "(", "scores", ".", "view", "(", "-", "1", ",", "y", ".", "size", "(", "1", ")", ")", ",", "dim", "=", "1", ")", "\n", "alpha", "=", "alpha_flat", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "1", ")", ",", "y", ".", "size", "(", "1", ")", ")", "\n", "\n", "# Take weighted average", "\n", "matched_seq", "=", "alpha", ".", "bmm", "(", "y", ")", "\n", "return", "matched_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.BilinearSeqAttn.__init__": [[651, 657], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "x_size", ",", "y_size", ",", "identity", "=", "False", ")", ":", "\n", "        ", "super", "(", "BilinearSeqAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "identity", ":", "\n", "            ", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "y_size", ",", "x_size", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "linear", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.BilinearSeqAttn.forward": [[658, 682], ["x.repeat.repeat.bmm().squeeze", "x.repeat.bmm().squeeze.data.masked_fill_", "y.repeat.repeat.repeat", "layers.BilinearSeqAttn.linear", "y.repeat.repeat.size", "x.repeat.repeat.size", "x.repeat.repeat.size", "x.repeat.repeat.repeat", "x_mask.repeat.repeat.repeat", "x.repeat.repeat.bmm", "float", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "x.repeat.repeat.size", "y.repeat.repeat.size", "y.repeat.repeat.size", "y.repeat.repeat.size", "Wy.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "x_mask", ",", "log", "=", "False", ",", "logit", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        x = batch * len * h1\n        y = batch * h2\n        x_mask = batch * len\n        \"\"\"", "\n", "if", "y", ".", "size", "(", "0", ")", "==", "1", "and", "x", ".", "size", "(", "0", ")", ">", "1", ":", "\n", "            ", "y", "=", "y", ".", "repeat", "(", "x", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "", "elif", "x", ".", "size", "(", "0", ")", "==", "1", "and", "y", ".", "size", "(", "0", ")", ">", "1", ":", "\n", "            ", "x", "=", "x", ".", "repeat", "(", "y", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "x_mask", "=", "x_mask", ".", "repeat", "(", "y", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "\n", "", "Wy", "=", "self", ".", "linear", "(", "y", ")", "if", "self", ".", "linear", "is", "not", "None", "else", "y", "\n", "xWy", "=", "x", ".", "bmm", "(", "Wy", ".", "unsqueeze", "(", "2", ")", ")", ".", "squeeze", "(", "2", ")", "\n", "xWy", ".", "data", ".", "masked_fill_", "(", "x_mask", ".", "data", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "if", "logit", ":", "\n", "            ", "return", "xWy", "\n", "", "elif", "log", ":", "\n", "# In training we output log-softmax for NLL", "\n", "            ", "alpha", "=", "F", ".", "log_softmax", "(", "xWy", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "# ...Otherwise 0-1 probabilities", "\n", "            ", "alpha", "=", "F", ".", "softmax", "(", "xWy", ",", "dim", "=", "1", ")", "\n", "", "return", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.LinearSeqAttn.__init__": [[688, 691], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ")", ":", "\n", "        ", "super", "(", "LinearSeqAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "input_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.LinearSeqAttn.forward": [[692, 705], ["x.contiguous().view", "layers.LinearSeqAttn.linear().view", "layers.LinearSeqAttn.data.masked_fill_", "x.size", "x.size", "x.size", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "x.contiguous", "layers.LinearSeqAttn.linear", "float"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_mask", ",", "log", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        x = batch * len * hdim\n        x_mask = batch * len\n        \"\"\"", "\n", "x_flat", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", "\n", "scores", "=", "self", ".", "linear", "(", "x_flat", ")", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "scores", ".", "data", ".", "masked_fill_", "(", "x_mask", ".", "data", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "if", "log", ":", "\n", "            ", "alpha", "=", "F", ".", "log_softmax", "(", "scores", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "alpha", "=", "F", ".", "softmax", "(", "scores", ",", "dim", "=", "1", ")", "\n", "", "return", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.RNNEncoder.__init__": [[708, 720], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "layers.RNNEncoder.rnns.append", "rnn_type"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", ",", "num_layers", ",", "\n", "dropout_rate", "=", "0", ",", "dropout_output", "=", "False", ",", "rnn_type", "=", "nn", ".", "LSTM", ",", "\n", "variational_dropout", "=", "True", ",", "aux_size", "=", "0", ")", ":", "\n", "        ", "super", "(", "RNNEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "variational_dropout", "=", "variational_dropout", "\n", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "rnns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "input_size_", "=", "(", "input_size", "+", "2", "*", "hidden_size", "*", "i", ")", "\n", "if", "i", "==", "0", ":", "input_size_", "+=", "aux_size", "\n", "self", ".", "rnns", ".", "append", "(", "rnn_type", "(", "input_size_", ",", "hidden_size", ",", "num_layers", "=", "1", ",", "bidirectional", "=", "True", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.RNNEncoder.forward": [[721, 745], ["x.transpose.transpose.transpose", "range", "aux_input.transpose.transpose.transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hiddens.append", "h.transpose", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "layers.dropout"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "x_mask", ",", "aux_input", "=", "None", ")", ":", "\n", "# Transpose batch and sequence dims", "\n", "        ", "x", "=", "x", ".", "transpose", "(", "0", ",", "1", ")", "\n", "if", "aux_input", "is", "not", "None", ":", "\n", "            ", "aux_input", "=", "aux_input", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "# Encode all layers", "\n", "", "hiddens", "=", "[", "x", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "rnn_input", "=", "torch", ".", "cat", "(", "hiddens", ",", "2", ")", "\n", "if", "i", "==", "0", "and", "aux_input", "is", "not", "None", ":", "\n", "                ", "rnn_input", "=", "torch", ".", "cat", "(", "[", "rnn_input", ",", "aux_input", "]", ",", "2", ")", "\n", "\n", "# Apply dropout to input", "\n", "", "if", "self", ".", "dropout_rate", ">", "0", ":", "\n", "                ", "rnn_input", "=", "dropout", "(", "rnn_input", ",", "p", "=", "self", ".", "dropout_rate", ",", "training", "=", "self", ".", "training", ",", "\n", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "# Forward", "\n", "", "rnn_output", "=", "self", ".", "rnns", "[", "i", "]", "(", "rnn_input", ")", "[", "0", "]", "\n", "hiddens", ".", "append", "(", "rnn_output", ")", "\n", "\n", "# Transpose back", "\n", "", "hiddens", "=", "[", "h", ".", "transpose", "(", "0", ",", "1", ")", "for", "h", "in", "hiddens", "]", "\n", "return", "hiddens", "[", "1", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MTLSTM.__init__": [[748, 779], ["torch.Module.__init__", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "torch.LSTM", "dict", "dict", "layers.MTLSTM.rnn1.load_state_dict", "layers.MTLSTM.rnn2.load_state_dict", "layers.MTLSTM.embedding.parameters", "layers.MTLSTM.rnn1.parameters", "layers.MTLSTM.rnn2.parameters", "isinstance", "torch.load.items", "torch.load.items", "torch.load.items", "torch.load.items", "isinstance", "torch.load.items", "torch.load.items", "torch.load.items", "torch.load.items", "name.replace", "name.replace"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "embedding", "=", "None", ",", "padding_idx", "=", "0", ",", "with_emb", "=", "True", ")", ":", "\n", "        ", "\"\"\"Initialize an MTLSTM\n\n        Arguments:\n            embedding (Float Tensor): If not None, initialize embedding matrix with specified embedding vectors\n        \"\"\"", "\n", "super", "(", "MTLSTM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "opt", "[", "'vocab_size'", "]", ",", "opt", "[", "'embedding_dim'", "]", ",", "padding_idx", "=", "padding_idx", ")", "\n", "if", "embedding", "is", "not", "None", ":", "\n", "            ", "self", ".", "embedding", ".", "weight", ".", "data", "=", "embedding", "\n", "\n", "", "state_dict", "=", "torch", ".", "load", "(", "opt", "[", "'MTLSTM_path'", "]", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ")", "\n", "self", ".", "rnn1", "=", "nn", ".", "LSTM", "(", "300", ",", "300", ",", "num_layers", "=", "1", ",", "bidirectional", "=", "True", ")", "\n", "self", ".", "rnn2", "=", "nn", ".", "LSTM", "(", "600", ",", "300", ",", "num_layers", "=", "1", ",", "bidirectional", "=", "True", ")", "\n", "\n", "state_dict1", "=", "dict", "(", "[", "(", "name", ",", "param", ".", "data", ")", "if", "isinstance", "(", "param", ",", "nn", ".", "Parameter", ")", "else", "(", "name", ",", "param", ")", "\n", "for", "name", ",", "param", "in", "state_dict", ".", "items", "(", ")", "if", "'0'", "in", "name", "]", ")", "\n", "state_dict2", "=", "dict", "(", "[", "(", "name", ".", "replace", "(", "'1'", ",", "'0'", ")", ",", "param", ".", "data", ")", "if", "isinstance", "(", "param", ",", "nn", ".", "Parameter", ")", "else", "(", "name", ".", "replace", "(", "'1'", ",", "'0'", ")", ",", "param", ")", "\n", "for", "name", ",", "param", "in", "state_dict", ".", "items", "(", ")", "if", "'1'", "in", "name", "]", ")", "\n", "self", ".", "rnn1", ".", "load_state_dict", "(", "state_dict1", ")", "\n", "self", ".", "rnn2", ".", "load_state_dict", "(", "state_dict2", ")", "\n", "\n", "for", "p", "in", "self", ".", "embedding", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "", "for", "p", "in", "self", ".", "rnn1", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "", "for", "p", "in", "self", ".", "rnn2", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "", "self", ".", "output_size", "=", "600", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MTLSTM.setup_eval_embed": [[780, 791], ["torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "layers.MTLSTM.eval_embed.parameters", "eval_embed.size", "eval_embed.size"], "methods", ["None"], ["", "def", "setup_eval_embed", "(", "self", ",", "eval_embed", ",", "padding_idx", "=", "0", ")", ":", "\n", "        ", "\"\"\"Allow evaluation vocabulary size to be greater than training vocabulary size\n\n        Arguments:\n            eval_embed (Float Tensor): Initialize eval_embed to be the specified embedding vectors\n        \"\"\"", "\n", "self", ".", "eval_embed", "=", "nn", ".", "Embedding", "(", "eval_embed", ".", "size", "(", "0", ")", ",", "eval_embed", ".", "size", "(", "1", ")", ",", "padding_idx", "=", "padding_idx", ")", "\n", "self", ".", "eval_embed", ".", "weight", ".", "data", "=", "eval_embed", "\n", "\n", "for", "p", "in", "self", ".", "eval_embed", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MTLSTM.forward": [[792, 818], ["emb", "x_mask.data.eq().long().sum().squeeze", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "layers.MTLSTM.rnn1", "layers.MTLSTM.rnn2", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.sort", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pack_padded_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "torch.utils.rnn.pad_packed_sequence", "x_mask.data.eq().long().sum", "lens.tolist", "x_mask.data.eq().long", "x_mask.data.eq"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x_idx", ",", "x_mask", ")", ":", "\n", "        ", "\"\"\"A pretrained MT-LSTM (McCann et. al. 2017).\n        This LSTM was trained with 300d 840B GloVe on the WMT 2017 machine translation dataset.\n\n        Arguments:\n            x_idx (Long Tensor): a Long Tensor of size (batch * len).\n            x_mask (Byte Tensor): a Byte Tensor of mask for the input tensor (batch * len).\n        \"\"\"", "\n", "# emb = self.embedding if self.training else self.eval_embed", "\n", "emb", "=", "self", ".", "embedding", "\n", "x_hiddens", "=", "emb", "(", "x_idx", ")", "\n", "\n", "lengths", "=", "x_mask", ".", "data", ".", "eq", "(", "0", ")", ".", "long", "(", ")", ".", "sum", "(", "1", ")", ".", "squeeze", "(", ")", "\n", "lens", ",", "indices", "=", "torch", ".", "sort", "(", "lengths", ",", "0", ",", "True", ")", "\n", "\n", "output1", ",", "_", "=", "self", ".", "rnn1", "(", "nn", ".", "utils", ".", "rnn", ".", "pack_padded_sequence", "(", "x_hiddens", "[", "indices", "]", ",", "lens", ".", "tolist", "(", ")", ",", "batch_first", "=", "True", ")", ")", "\n", "output2", ",", "_", "=", "self", ".", "rnn2", "(", "output1", ")", "\n", "\n", "output1", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "output1", ",", "batch_first", "=", "True", ")", "[", "0", "]", "\n", "output2", "=", "nn", ".", "utils", ".", "rnn", ".", "pad_packed_sequence", "(", "output2", ",", "batch_first", "=", "True", ")", "[", "0", "]", "\n", "\n", "_", ",", "_indices", "=", "torch", ".", "sort", "(", "indices", ",", "0", ")", "\n", "output1", "=", "output1", "[", "_indices", "]", "\n", "output2", "=", "output2", "[", "_indices", "]", "\n", "\n", "return", "output1", ",", "output2", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.FullAttention.__init__": [[822, 834], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["    ", "def", "__init__", "(", "self", ",", "full_size", ",", "hidden_size", ",", "num_level", ",", "dropout", "=", "0.", ",", "variational_dropout", "=", "True", ")", ":", "\n", "        ", "super", "(", "FullAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "assert", "(", "hidden_size", "%", "num_level", "==", "0", ")", "\n", "self", ".", "full_size", "=", "full_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "attsize_per_lvl", "=", "hidden_size", "//", "num_level", "\n", "self", ".", "num_level", "=", "num_level", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "full_size", ",", "hidden_size", ",", "bias", "=", "False", ")", "\n", "self", ".", "linear_final", "=", "nn", ".", "Parameter", "(", "torch", ".", "ones", "(", "1", ",", "hidden_size", ")", ",", "requires_grad", "=", "True", ")", "\n", "self", ".", "output_size", "=", "hidden_size", "\n", "self", ".", "dropout", "=", "dropout", "\n", "self", ".", "variational_dropout", "=", "variational_dropout", "\n", "# print(\"Full Attention: (atten. {} -> {}, take {}) x {}\".format(self.full_size, self.attsize_per_lvl, hidden_size // num_level, self.num_level))", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.FullAttention.forward": [[836, 878], ["x1_att.repeat.repeat.size", "x1_att.repeat.repeat.size", "x2_att.repeat.repeat.size", "layers.dropout", "layers.dropout", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "layers.FullAttention.linear_final.expand_as", "torch.relu.view().transpose().contiguous().view", "torch.relu.view().transpose().contiguous().view", "F.relu.view().transpose().contiguous().view.bmm().view", "x2_mask.repeat.repeat.unsqueeze().unsqueeze().expand_as", "x1_key.view().transpose().contiguous().view.bmm().view.data.masked_fill_", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax.view", "F.softmax.view.bmm", "alpha_flat.view.bmm.view().transpose().contiguous().view", "x1_att.repeat.repeat.repeat", "layers.FullAttention.linear", "layers.FullAttention.linear", "x1_key.view().transpose().contiguous().view.bmm().view.view", "x2.repeat.repeat.contiguous().view().transpose().contiguous().view", "x1_att.repeat.repeat.size", "x2_att.repeat.repeat.size", "x2_att.repeat.repeat.size", "x2_att.repeat.repeat.repeat", "x2.repeat.repeat.repeat", "x2_mask.repeat.repeat.repeat", "x1_att.repeat.repeat.view", "x2_att.repeat.repeat.view", "torch.relu.view().transpose().contiguous", "torch.relu.view().transpose().contiguous", "F.relu.view().transpose().contiguous().view.bmm", "x2_mask.repeat.repeat.unsqueeze().unsqueeze", "float", "x2.repeat.repeat.size", "alpha_flat.view.bmm.view().transpose().contiguous", "x2_att.repeat.repeat.size", "x1_att.repeat.repeat.size", "x1_att.repeat.repeat.size", "x1_att.repeat.repeat.size", "x1_att.repeat.repeat.size", "F.relu.view().transpose().contiguous().view.transpose", "x2.repeat.repeat.contiguous().view().transpose().contiguous", "torch.relu.view().transpose", "torch.relu.view().transpose", "x2_mask.repeat.repeat.unsqueeze", "alpha_flat.view.bmm.view().transpose", "x2.repeat.repeat.contiguous().view().transpose", "torch.relu.view", "torch.relu.view", "alpha_flat.view.bmm.view", "x2.repeat.repeat.contiguous().view", "x2.repeat.repeat.size", "x2.repeat.repeat.contiguous"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "forward", "(", "self", ",", "x1_att", ",", "x2_att", ",", "x2", ",", "x2_mask", ")", ":", "\n", "        ", "\"\"\"\n        x1_att: batch * len1 * full_size\n        x2_att: batch * len2 * full_size\n        x2: batch * len2 * hidden_size\n        x2_mask: batch * len2\n        \"\"\"", "\n", "if", "x1_att", ".", "size", "(", "0", ")", "==", "1", "and", "x2_att", ".", "size", "(", "0", ")", ">", "1", ":", "\n", "            ", "x1_att", "=", "x1_att", ".", "repeat", "(", "x2_att", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "", "elif", "x2_att", ".", "size", "(", "0", ")", "==", "1", "and", "x1_att", ".", "size", "(", "0", ")", ">", "1", ":", "\n", "            ", "x2_att", "=", "x2_att", ".", "repeat", "(", "x1_att", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "x2", "=", "x2", ".", "repeat", "(", "x1_att", ".", "size", "(", "0", ")", ",", "1", ",", "1", ")", "\n", "x2_mask", "=", "x2_mask", ".", "repeat", "(", "x1_att", ".", "size", "(", "0", ")", ",", "1", ")", "\n", "\n", "", "batch_size", "=", "x1_att", ".", "size", "(", "0", ")", "\n", "len1", "=", "x1_att", ".", "size", "(", "1", ")", "\n", "len2", "=", "x2_att", ".", "size", "(", "1", ")", "\n", "\n", "x1_att", "=", "dropout", "(", "x1_att", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ",", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "x2_att", "=", "dropout", "(", "x2_att", ",", "p", "=", "self", ".", "dropout", ",", "training", "=", "self", ".", "training", ",", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "\n", "x1_key", "=", "F", ".", "relu", "(", "self", ".", "linear", "(", "x1_att", ".", "view", "(", "-", "1", ",", "self", ".", "full_size", ")", ")", ")", "\n", "x2_key", "=", "F", ".", "relu", "(", "self", ".", "linear", "(", "x2_att", ".", "view", "(", "-", "1", ",", "self", ".", "full_size", ")", ")", ")", "\n", "final_v", "=", "self", ".", "linear_final", ".", "expand_as", "(", "x2_key", ")", "\n", "x2_key", "=", "final_v", "*", "x2_key", "\n", "\n", "x1_rep", "=", "x1_key", ".", "view", "(", "-", "1", ",", "len1", ",", "self", ".", "num_level", ",", "self", ".", "attsize_per_lvl", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "len1", ",", "self", ".", "attsize_per_lvl", ")", "\n", "x2_rep", "=", "x2_key", ".", "view", "(", "-", "1", ",", "len2", ",", "self", ".", "num_level", ",", "self", ".", "attsize_per_lvl", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "len2", ",", "self", ".", "attsize_per_lvl", ")", "\n", "\n", "scores", "=", "x1_rep", ".", "bmm", "(", "x2_rep", ".", "transpose", "(", "1", ",", "2", ")", ")", ".", "view", "(", "-", "1", ",", "self", ".", "num_level", ",", "len1", ",", "len2", ")", "# batch * num_level * len1 * len2", "\n", "\n", "x2_mask", "=", "x2_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", ".", "expand_as", "(", "scores", ")", "\n", "scores", ".", "data", ".", "masked_fill_", "(", "x2_mask", ".", "data", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "\n", "alpha_flat", "=", "F", ".", "softmax", "(", "scores", ".", "view", "(", "-", "1", ",", "len2", ")", ",", "dim", "=", "1", ")", "\n", "alpha", "=", "alpha_flat", ".", "view", "(", "-", "1", ",", "len1", ",", "len2", ")", "\n", "# alpha = F.softmax(scores, dim=2)", "\n", "\n", "size_per_level", "=", "self", ".", "hidden_size", "//", "self", ".", "num_level", "\n", "atten_seq", "=", "alpha", ".", "bmm", "(", "x2", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "x2", ".", "size", "(", "1", ")", ",", "self", ".", "num_level", ",", "size_per_level", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "x2", ".", "size", "(", "1", ")", ",", "size_per_level", ")", ")", "\n", "\n", "return", "atten_seq", ".", "view", "(", "-", "1", ",", "self", ".", "num_level", ",", "len1", ",", "size_per_level", ")", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "len1", ",", "self", ".", "hidden_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.FullAttention.__repr__": [[879, 881], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "\"FullAttention: (atten. {} -> {}, take {}) x {}\"", ".", "format", "(", "self", ".", "full_size", ",", "self", ".", "attsize_per_lvl", ",", "self", ".", "hidden_size", "//", "self", ".", "num_level", ",", "self", ".", "num_level", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.LinearSelfAttn.__init__": [[888, 891], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "input_size", ")", ":", "\n", "        ", "super", "(", "LinearSelfAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "input_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.LinearSelfAttn.forward": [[892, 904], ["layers.dropout", "dropout.contiguous().view", "layers.LinearSelfAttn.linear().view", "layers.LinearSelfAttn.data.masked_fill_", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "dropout.size", "dropout.size", "dropout.size", "dropout.contiguous", "layers.LinearSelfAttn.linear", "float"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_mask", ")", ":", "\n", "        ", "\"\"\"\n        x = batch * len * hdim\n        x_mask = batch * len\n        \"\"\"", "\n", "x", "=", "dropout", "(", "x", ",", "p", "=", "my_dropout_p", ",", "training", "=", "self", ".", "training", ")", "\n", "\n", "x_flat", "=", "x", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "-", "1", ")", ")", "\n", "scores", "=", "self", ".", "linear", "(", "x_flat", ")", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "scores", ".", "data", ".", "masked_fill_", "(", "x_mask", ".", "data", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "alpha", "=", "F", ".", "softmax", "(", "scores", ",", "dim", "=", "1", ")", "\n", "return", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.BiAttn.__init__": [[908, 918], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.BiAttn.reset_parameters", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.BiAttn.reset_parameters"], ["def", "__init__", "(", "self", ",", "input_size", ",", "q2c", ":", "bool", "=", "True", ",", "query_dots", ":", "bool", "=", "True", ")", ":", "\n", "        ", "super", "(", "BiAttn", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "q2c", "=", "q2c", "\n", "self", ".", "query_dots", "=", "query_dots", "\n", "self", ".", "w_x", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_size", ",", "1", ")", ")", "\n", "self", ".", "w_y", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_size", ",", "1", ")", ")", "\n", "self", ".", "w_dot", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "input_size", ",", "1", ")", ")", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "Tensor", "(", "1", ")", ")", "\n", "self", ".", "reset_parameters", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.BiAttn.reset_parameters": [[919, 924], ["torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "torch.init.kaiming_uniform", "layers.BiAttn.bias.data.zero_"], "methods", ["None"], ["", "def", "reset_parameters", "(", "self", ")", ":", "\n", "        ", "nn", ".", "init", ".", "kaiming_uniform", "(", "self", ".", "w_x", ".", "data", ")", "\n", "nn", ".", "init", ".", "kaiming_uniform", "(", "self", ".", "w_y", ".", "data", ")", "\n", "nn", ".", "init", ".", "kaiming_uniform", "(", "self", ".", "w_dot", ".", "data", ")", "\n", "self", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.BiAttn.forward": [[925, 991], ["x.size", "x.size", "y.size", "x.unsqueeze", "y.unsqueeze", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax.bmm", "xy.view().mm().view", "layers.maskneginf", "layers.maskneginf", "layers.maskzero", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax.unsqueeze().bmm", "outputs.append", "outputs.append", "print", "print", "time.sleep", "y.contiguous().view().mm().view", "x_mask.unsqueeze", "y_mask.unsqueeze", "x_mask.unsqueeze", "x.unsqueeze.size", "y.unsqueeze.size", "layers.BiAttn.bias.view", "x.contiguous().view().mm().view", "xy.view().mm", "maskneginf.max", "torch.softmax.unsqueeze", "y.contiguous().view().mm", "x.contiguous().view().mm", "xy.view", "y.contiguous().view", "x.contiguous().view", "y.contiguous", "x.contiguous"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.maskneginf", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.maskneginf", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.maskzero"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ",", "x_mask", "=", "None", ",", "y_mask", "=", "None", ",", "raw_score_only", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            x: batch * len1 * hdim (context)\n            y: batch * len2 * hdim (query)\n            x_mask: batch * len1 (1 for padding, 0 for true)\n            y_mask: batch * len2 (1 for padding, 0 for true)\n        Output:\n        if raw_score_only:\n            scores: batch * len1 * len2\n        else:\n            matched_seq: batch * len1 * hdim\n\n        \"\"\"", "\n", "batch_size", "=", "x", ".", "size", "(", "0", ")", "\n", "len1", "=", "x", ".", "size", "(", "1", ")", "\n", "len2", "=", "y", ".", "size", "(", "1", ")", "\n", "\n", "# get the scores", "\n", "x_ext", "=", "x", ".", "unsqueeze", "(", "2", ")", "\n", "y_ext", "=", "y", ".", "unsqueeze", "(", "1", ")", "\n", "try", ":", "\n", "            ", "xy", "=", "x_ext", "*", "y_ext", "\n", "", "except", ":", "\n", "            ", "print", "(", "'x_ext:'", ",", "x_ext", ".", "size", "(", ")", ")", "\n", "print", "(", "'y_ext:'", ",", "y_ext", ".", "size", "(", ")", ")", "\n", "import", "time", "\n", "time", ".", "sleep", "(", "10", ")", "\n", "\n", "", "scores", "=", "self", ".", "bias", ".", "view", "(", "1", ",", "1", ",", "1", ")", "+", "x", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "input_size", ")", ".", "mm", "(", "self", ".", "w_x", ")", ".", "view", "(", "batch_size", ",", "len1", ",", "1", ")", "+", "y", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "self", ".", "input_size", ")", ".", "mm", "(", "self", ".", "w_y", ")", ".", "view", "(", "batch_size", ",", "1", ",", "len2", ")", "+", "xy", ".", "view", "(", "-", "1", ",", "self", ".", "input_size", ")", ".", "mm", "(", "self", ".", "w_dot", ")", ".", "view", "(", "batch_size", ",", "len1", ",", "len2", ")", "\n", "\n", "\n", "# fill the padding part with -inf", "\n", "if", "x_mask", "is", "not", "None", ":", "\n", "            ", "scores", "=", "maskneginf", "(", "scores", ",", "x_mask", ".", "unsqueeze", "(", "2", ")", ")", "\n", "", "if", "y_mask", "is", "not", "None", ":", "\n", "            ", "scores", "=", "maskneginf", "(", "scores", ",", "y_mask", ".", "unsqueeze", "(", "1", ")", ")", "\n", "\n", "", "if", "raw_score_only", ":", "\n", "            ", "return", "scores", "\n", "\n", "\n", "# context-to-query", "\n", "", "alpha", "=", "F", ".", "softmax", "(", "scores", ",", "dim", "=", "2", ")", "\n", "# replacing NaN with zeros (Softmax is numerically unstable)", "\n", "# no ideas how to avoid it yet", "\n", "alpha", ".", "data", "[", "alpha", ".", "data", "!=", "alpha", ".", "data", "]", "=", "0.", "\n", "\n", "c2q_attn", "=", "alpha", ".", "bmm", "(", "y", ")", "\n", "if", "x_mask", "is", "not", "None", ":", "\n", "            ", "c2q_attn", "=", "maskzero", "(", "c2q_attn", ",", "x_mask", ".", "unsqueeze", "(", "2", ")", ")", "\n", "", "outputs", "=", "[", "c2q_attn", "]", "\n", "\n", "# query-to-context", "\n", "if", "self", ".", "q2c", ":", "\n", "            ", "beta", "=", "F", ".", "softmax", "(", "scores", ".", "max", "(", "2", ")", "[", "0", "]", ",", "dim", "=", "1", ")", "\n", "q2c_attn", "=", "beta", ".", "unsqueeze", "(", "1", ")", ".", "bmm", "(", "x", ")", "\n", "outputs", ".", "append", "(", "q2c_attn", ")", "\n", "\n", "", "if", "self", ".", "query_dots", ":", "\n", "            ", "outputs", ".", "append", "(", "x", "*", "c2q_attn", ")", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.BiAttn.__repr__": [[992, 995], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "'{}(input_size={}, q2c={}, query_dots={})'", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "input_size", ",", "self", ".", "q2c", ",", "self", ".", "query_dots", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.Linear.__init__": [[999, 1003], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "d_in", ",", "d_out", ",", "bias", "=", "True", ")", ":", "\n", "        ", "super", "(", "Linear", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "d_in", ",", "d_out", ",", "bias", "=", "bias", ")", "\n", "init", ".", "xavier_normal", "(", "self", ".", "linear", ".", "weight", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.Linear.forward": [[1004, 1006], ["layers.Linear.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "linear", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.ScaledDotProductAttention.__init__": [[1011, 1016], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Dropout", "layers.BottleSoftmax"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "attn_dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "ScaledDotProductAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "temper", "=", "d_model", "**", "0.5", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "attn_dropout", ")", "\n", "self", ".", "softmax", "=", "BottleSoftmax", "(", "dim", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.ScaledDotProductAttention.forward": [[1017, 1037], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "layers.ScaledDotProductAttention.dropout", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "attn_mask.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "layers.ScaledDotProductAttention.data.masked_fill_", "k.transpose", "attn_mask.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze", "float"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "forward", "(", "self", ",", "q", ",", "k", ",", "v", ",", "attn_mask", "=", "None", ")", ":", "\n", "        ", "attn", "=", "torch", ".", "bmm", "(", "q", ",", "k", ".", "transpose", "(", "1", ",", "2", ")", ")", "/", "self", ".", "temper", "\n", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "\n", "# assert attn_mask.size() == attn.size(), \\", "\n", "#         'Attention mask shape {} mismatch ' \\", "\n", "#         'with Attention logit tensor shape ' \\", "\n", "#         '{}.'.format(attn_mask.size(), attn.size())", "\n", "# print(attn.size(), attn_mask.size())", "\n", "\n", "            ", "attn_mask", "=", "attn_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "attn", ")", "\n", "attn", ".", "data", ".", "masked_fill_", "(", "attn_mask", ".", "data", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "\n", "", "attn", "=", "F", ".", "softmax", "(", "attn", ",", "dim", "=", "-", "1", ")", "\n", "attn", ".", "data", "[", "attn", ".", "data", "!=", "attn", ".", "data", "]", "=", "0.", "\n", "attn", "=", "self", ".", "dropout", "(", "attn", ")", "\n", "output", "=", "torch", ".", "bmm", "(", "attn", ",", "v", ")", "\n", "\n", "return", "output", ",", "attn", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.Highway.__init__": [[1040, 1054], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "layers.Highway.highway_layers.append", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "hidden_size", "=", "128", ",", "num_layers", "=", "2", ",", "dropout_rate", "=", "0.", ")", ":", "\n", "        ", "super", "(", "Highway", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_size", "=", "input_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "reduction", "=", "(", "input_size", "!=", "hidden_size", ")", "\n", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "\n", "if", "self", ".", "input_size", "!=", "self", ".", "hidden_size", ":", "\n", "            ", "self", ".", "reduction", "=", "nn", ".", "Linear", "(", "input_size", ",", "hidden_size", ")", "\n", "\n", "", "self", ".", "highway_layers", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "            ", "self", ".", "highway_layers", ".", "append", "(", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", "*", "2", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.Highway.forward": [[1055, 1075], ["x.view.view.dim", "x.view.view.size", "x.view.view.size", "x.view.view.view", "layers.Highway.reduction", "layer", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "x.view.view.view", "x.view.view.size", "torch.dropout", "torch.dropout", "torch.dropout", "torch.dropout"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "", "def", "forward", "(", "self", ",", "x", ",", "x_mask", "=", "None", ")", ":", "\n", "        ", "ndim", "=", "x", ".", "dim", "(", ")", "\n", "if", "ndim", "==", "3", ":", "\n", "            ", "batch_size", "=", "x", ".", "size", "(", "0", ")", "\n", "x_len", "=", "x", ".", "size", "(", "1", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "x", ".", "size", "(", "2", ")", ")", "\n", "\n", "", "if", "self", ".", "input_size", "!=", "self", ".", "hidden_size", ":", "\n", "            ", "x", "=", "self", ".", "reduction", "(", "x", ")", "\n", "\n", "", "for", "layer", "in", "self", ".", "highway_layers", ":", "\n", "            ", "x_trans", "=", "layer", "(", "F", ".", "dropout", "(", "x", ",", "self", ".", "dropout_rate", ",", "training", "=", "self", ".", "training", ")", ")", "\n", "gate", "=", "F", ".", "sigmoid", "(", "x_trans", "[", ":", ",", "self", ".", "hidden_size", ":", "]", ")", "\n", "x_trans", "=", "F", ".", "relu", "(", "x_trans", "[", ":", ",", ":", "self", ".", "hidden_size", "]", ")", "\n", "x", "=", "x", "*", "(", "1", "-", "gate", ")", "+", "x_trans", "*", "gate", "\n", "\n", "", "if", "ndim", "==", "3", ":", "\n", "            ", "x", "=", "x", ".", "view", "(", "batch_size", ",", "x_len", ",", "-", "1", ")", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.Bottle.forward": [[1080, 1086], ["super().forward", "super().forward.view", "len", "super().forward", "input.size", "input.view", "input.size"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.forward", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.forward"], ["def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "if", "len", "(", "input", ".", "size", "(", ")", ")", "<=", "2", ":", "\n", "            ", "return", "super", "(", "Bottle", ",", "self", ")", ".", "forward", "(", "input", ")", "\n", "", "size", "=", "input", ".", "size", "(", ")", "[", ":", "2", "]", "\n", "out", "=", "super", "(", ")", ".", "forward", "(", "input", ".", "view", "(", "size", "[", "0", "]", "*", "size", "[", "1", "]", ",", "-", "1", ")", ")", "\n", "return", "out", ".", "view", "(", "size", "[", "0", "]", ",", "size", "[", "1", "]", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MultiHeadAttention.__init__": [[1102, 1126], ["torch.Module.__init__", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "layers.ScaledDotProductAttention", "layers.BottleLinear", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "torch.xavier_normal", "layers.LayerNorm", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "n_head", ",", "d_model", ",", "d_k", ",", "d_v", ",", "attn_dropout", "=", "0.0", ",", "input_layer_norm", "=", "False", ")", ":", "\n", "        ", "super", "(", "MultiHeadAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "input_layer_norm", ":", "\n", "            ", "self", ".", "layer_norm", "=", "LayerNorm", "(", "d_model", ")", "\n", "\n", "", "self", ".", "n_head", "=", "n_head", "\n", "self", ".", "d_k", "=", "d_k", "\n", "self", ".", "d_v", "=", "d_v", "\n", "\n", "self", ".", "w_qs", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "n_head", ",", "d_model", ",", "d_k", ")", ")", "\n", "self", ".", "w_ks", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "n_head", ",", "d_model", ",", "d_k", ")", ")", "\n", "self", ".", "w_vs", "=", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "n_head", ",", "d_model", ",", "d_v", ")", ")", "\n", "\n", "self", ".", "attention", "=", "ScaledDotProductAttention", "(", "d_k", ",", "attn_dropout", "=", "attn_dropout", ")", "\n", "# if n_head * d_v != d_model:", "\n", "self", ".", "proj", "=", "BottleLinear", "(", "n_head", "*", "d_v", ",", "d_model", ")", "\n", "\n", "\n", "# self.dropout = nn.Dropout(dropout)", "\n", "\n", "init", ".", "xavier_normal", "(", "self", ".", "w_qs", ")", "\n", "init", ".", "xavier_normal", "(", "self", ".", "w_ks", ")", "\n", "init", ".", "xavier_normal", "(", "self", ".", "w_vs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MultiHeadAttention.forward": [[1127, 1168], ["hasattr", "layers.MultiHeadAttention.size", "layers.MultiHeadAttention.repeat().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "layers.MultiHeadAttention.attention", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "hasattr", "layers.MultiHeadAttention.layer_norm", "attn_mask.repeat.repeat.repeat", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "layers.MultiHeadAttention.proj", "layers.MultiHeadAttention.repeat", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "q", ",", "attn_mask", "=", "None", ")", ":", "\n", "        ", "'''only supports self-attn'''", "\n", "if", "hasattr", "(", "self", ",", "'layer_norm'", ")", ":", "\n", "            ", "q", "=", "self", ".", "layer_norm", "(", "q", ")", "\n", "\n", "", "d_k", ",", "d_v", "=", "self", ".", "d_k", ",", "self", ".", "d_v", "\n", "n_head", "=", "self", ".", "n_head", "\n", "\n", "\n", "mb_size", ",", "len_q", ",", "d_model", "=", "q", ".", "size", "(", ")", "\n", "# mb_size, len_k, d_model = k.size()", "\n", "# mb_size, len_v, d_model = v.size()", "\n", "len_k", "=", "len_q", "\n", "len_v", "=", "len_q", "\n", "\n", "# treat as a (n_head) size batch", "\n", "q_s", "=", "q", ".", "repeat", "(", "n_head", ",", "1", ",", "1", ")", ".", "view", "(", "n_head", ",", "-", "1", ",", "d_model", ")", "# n_head x (mb_size*len_q) x d_model", "\n", "# k_s = k.repeat(n_head, 1, 1).view(n_head, -1, d_model) # n_head x (mb_size*len_k) x d_model", "\n", "# v_s = v.repeat(n_head, 1, 1).view(n_head, -1, d_model) # n_head x (mb_size*len_v) x d_model", "\n", "k_s", "=", "q_s", "\n", "v_s", "=", "q_s", "\n", "\n", "# treat the result as a (n_head * mb_size) size batch", "\n", "q_s", "=", "torch", ".", "bmm", "(", "q_s", ",", "self", ".", "w_qs", ")", ".", "view", "(", "-", "1", ",", "len_q", ",", "d_k", ")", "# (n_head*mb_size) x len_q x d_k", "\n", "k_s", "=", "torch", ".", "bmm", "(", "k_s", ",", "self", ".", "w_ks", ")", ".", "view", "(", "-", "1", ",", "len_k", ",", "d_k", ")", "# (n_head*mb_size) x len_k x d_k", "\n", "v_s", "=", "torch", ".", "bmm", "(", "v_s", ",", "self", ".", "w_vs", ")", ".", "view", "(", "-", "1", ",", "len_v", ",", "d_v", ")", "# (n_head*mb_size) x len_v x d_v", "\n", "\n", "# perform attention, result size = (n_head * mb_size) x len_q x d_v", "\n", "if", "attn_mask", "is", "not", "None", ":", "\n", "            ", "attn_mask", "=", "attn_mask", ".", "repeat", "(", "n_head", ",", "1", ")", "\n", "", "outputs", ",", "attns", "=", "self", ".", "attention", "(", "q_s", ",", "k_s", ",", "v_s", ",", "attn_mask", "=", "attn_mask", ")", "\n", "\n", "# back to original mb_size batch, result size = mb_size x len_q x (n_head*d_v)", "\n", "outputs", "=", "torch", ".", "cat", "(", "torch", ".", "split", "(", "outputs", ",", "mb_size", ",", "dim", "=", "0", ")", ",", "dim", "=", "-", "1", ")", "\n", "\n", "# project back to residual size", "\n", "if", "hasattr", "(", "self", ",", "'proj'", ")", ":", "\n", "            ", "outputs", "=", "self", ".", "proj", "(", "outputs", ")", "\n", "# outputs = self.dropout(outputs)", "\n", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.GBEncoderBlock.__init__": [[1173, 1205], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "layers.MultiHeadAttention", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.GBEncoderBlock.cnns.append", "layers.LayerNorm", "layers.BottleLinear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "layers.BottleLinear", "layers.GBEncoderBlock.set_pos_emb", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "layers.LayerNormChannelFirst", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.GBEncoderBlock.set_pos_emb"], ["def", "__init__", "(", "self", ",", "hidden_size", "=", "128", ",", "kernel_size", "=", "7", ",", "num_layers", "=", "4", ",", "dropout_rate", "=", "0.", ",", "variational_dropout", "=", "True", ",", "depth_drop", "=", "0.", ",", "depth_drop_start", "=", "0", ",", "depth_drop_end", "=", "None", ",", "add_pos", "=", "True", ")", ":", "\n", "        ", "'''assuming input_size == hidden_size'''", "\n", "super", "(", "GBEncoderBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "kernel_size", "=", "kernel_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "dropout_rate", "=", "dropout_rate", "\n", "self", ".", "variational_dropout", "=", "variational_dropout", "\n", "self", ".", "depth_drop", "=", "depth_drop", "\n", "self", ".", "depth_drop_start", "=", "depth_drop_start", "\n", "self", ".", "depth_drop_end", "=", "num_layers", "if", "depth_drop_end", "is", "None", "else", "depth_drop_end", "\n", "\n", "self", ".", "cnns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "for", "i", "in", "range", "(", "num_layers", ")", ":", "\n", "# no activation?", "\n", "            ", "self", ".", "cnns", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "LayerNormChannelFirst", "(", "hidden_size", ")", ",", "\n", "nn", ".", "Conv1d", "(", "hidden_size", ",", "hidden_size", ",", "kernel_size", ",", "padding", "=", "kernel_size", "//", "2", ",", "groups", "=", "hidden_size", ")", ",", "\n", "nn", ".", "Conv1d", "(", "hidden_size", ",", "hidden_size", ",", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "\n", ")", ")", "\n", "", "self", ".", "self_attn", "=", "MultiHeadAttention", "(", "8", ",", "hidden_size", ",", "hidden_size", "//", "8", ",", "hidden_size", "//", "8", ",", "input_layer_norm", "=", "True", ")", "\n", "\n", "self", ".", "ffn", "=", "nn", ".", "Sequential", "(", "\n", "LayerNorm", "(", "hidden_size", ")", ",", "\n", "BottleLinear", "(", "hidden_size", ",", "hidden_size", "*", "4", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", ",", "\n", "BottleLinear", "(", "hidden_size", "*", "4", ",", "hidden_size", ")", ",", "\n", ")", "\n", "# add position embeding to the first block", "\n", "if", "add_pos", ":", "\n", "            ", "self", ".", "set_pos_emb", "(", "2000", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.GBEncoderBlock.set_pos_emb": [[1206, 1209], ["torch.Parameter", "torch.Parameter", "torch.Parameter", "torch.Parameter", "get_position_encoding().unsqueeze_", "layers.get_position_encoding"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.get_position_encoding"], ["", "", "def", "set_pos_emb", "(", "self", ",", "l", ")", ":", "\n", "        ", "self", ".", "pos_emb", "=", "nn", ".", "Parameter", "(", "get_position_encoding", "(", "self", ".", "hidden_size", ",", "[", "l", "]", ")", ".", "unsqueeze_", "(", "0", ")", ")", "\n", "self", ".", "pos_emb", ".", "requires_grad", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.GBEncoderBlock.forward": [[1211, 1270], ["x.transpose.transpose.size", "x.transpose.transpose.size", "hasattr", "x.transpose.transpose.transpose", "x.transpose.transpose.transpose", "layers.dropout", "layers.GBEncoderBlock.self_attn", "layers.dropout", "layers.GBEncoderBlock.ffn", "layers.GBEncoderBlock.pos_emb.size", "layers.GBEncoderBlock.set_pos_emb", "dropout().transpose", "cnn", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "layers.dropout", "x.transpose.transpose.transpose"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.GBEncoderBlock.set_pos_emb", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "forward", "(", "self", ",", "x", ",", "x_mask", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        TODO add x_mask\n        x = batch * len * hidden_size\n        \"\"\"", "\n", "batch_size", "=", "x", ".", "size", "(", "0", ")", "\n", "x_len", "=", "x", ".", "size", "(", "1", ")", "\n", "\n", "drop_i", "=", "self", ".", "depth_drop_start", "\n", "\n", "if", "hasattr", "(", "self", ",", "'pos_emb'", ")", ":", "\n", "            ", "if", "x_len", ">", "self", ".", "pos_emb", ".", "size", "(", "1", ")", ":", "\n", "                ", "self", ".", "set_pos_emb", "(", "x_len", "+", "200", ")", "\n", "", "x", "=", "x", "+", "self", ".", "pos_emb", "[", ":", ",", ":", "x_len", ",", ":", "]", "\n", "# if x_mask is not None:", "\n", "# print('u1:', x.data[0].sum(1))", "\n", "# maskzero(x, x_mask.unsqueeze(2))", "\n", "# print('u2:', x.data[0].sum(1))", "\n", "\n", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "for", "cnn", "in", "self", ".", "cnns", ":", "\n", "            ", "drop_i", "+=", "1", "\n", "depth_drop_prob", "=", "self", ".", "depth_drop", "*", "drop_i", "/", "self", ".", "depth_drop_end", "\n", "if", "self", ".", "depth_drop", "<=", "0.", "or", "torch", ".", "rand", "(", "1", ")", "[", "0", "]", ">", "depth_drop_prob", ":", "\n", "                ", "x_drop", "=", "dropout", "(", "x", ".", "transpose", "(", "1", ",", "2", ")", ",", "p", "=", "self", ".", "dropout_rate", ",", "training", "=", "self", ".", "training", ",", "\n", "variational", "=", "self", ".", "variational_dropout", ")", ".", "transpose", "(", "1", ",", "2", ")", "\n", "residual", "=", "cnn", "(", "x_drop", ")", "\n", "if", "self", ".", "training", "and", "self", ".", "depth_drop", ">", "0.", ":", "\n", "                    ", "residual", "=", "residual", "/", "(", "1", "-", "depth_drop_prob", ")", "\n", "", "x", "=", "x", "+", "residual", "\n", "# if x_mask is not None:", "\n", "#     maskzero(x, x_mask.unsqueeze(1))", "\n", "", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "\n", "# print('t1:', x.data.sum())", "\n", "drop_i", "+=", "1", "\n", "depth_drop_prob", "=", "self", ".", "depth_drop", "*", "drop_i", "/", "self", ".", "depth_drop_end", "\n", "if", "self", ".", "depth_drop", "<=", "0.", "or", "torch", ".", "rand", "(", "1", ")", "[", "0", "]", ">", "depth_drop_prob", ":", "\n", "            ", "x_drop", "=", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_rate", ",", "training", "=", "self", ".", "training", ",", "\n", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "residual", "=", "self", ".", "self_attn", "(", "x_drop", ",", "x_mask", ")", "\n", "if", "self", ".", "training", "and", "self", ".", "depth_drop", ">", "0.", ":", "\n", "                ", "residual", "=", "residual", "/", "(", "1", "-", "depth_drop_prob", ")", "\n", "", "x", "=", "x", "+", "residual", "\n", "# print('t2:', x.data.sum())", "\n", "\n", "", "drop_i", "+=", "1", "\n", "depth_drop_prob", "=", "self", ".", "depth_drop", "*", "drop_i", "/", "self", ".", "depth_drop_end", "\n", "if", "self", ".", "depth_drop", "<=", "0.", "or", "torch", ".", "rand", "(", "1", ")", "[", "0", "]", ">", "depth_drop_prob", ":", "\n", "            ", "x_drop", "=", "dropout", "(", "x", ",", "p", "=", "self", ".", "dropout_rate", ",", "training", "=", "self", ".", "training", ",", "\n", "variational", "=", "self", ".", "variational_dropout", ")", "\n", "residual", "=", "self", ".", "ffn", "(", "x_drop", ")", "\n", "if", "self", ".", "training", "and", "self", ".", "depth_drop", ">", "0.", ":", "\n", "                ", "residual", "=", "residual", "/", "(", "1", "-", "depth_drop_prob", ")", "\n", "", "x", "=", "x", "+", "residual", "\n", "# if x_mask is not None:", "\n", "#     maskzero(x, x_mask.unsqueeze(2))", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MaskNegInf.forward": [[1319, 1325], ["ctx.save_for_backward", "input.masked_fill_", "mask.expand_as", "float"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ",", "mask", "=", "None", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "mask", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "input", ".", "masked_fill_", "(", "mask", ".", "expand_as", "(", "input", ")", ",", "-", "float", "(", "'inf'", ")", ")", "\n", "", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MaskNegInf.backward": [[1326, 1332], ["grad_output.masked_fill_", "mask.expand_as"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "mask", "=", "ctx", ".", "saved_variables", "[", "0", "]", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "grad_output", ".", "masked_fill_", "(", "mask", ".", "expand_as", "(", "grad_output", ")", ",", "0", ")", "\n", "", "return", "grad_output", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MaskZero.forward": [[1335, 1341], ["ctx.save_for_backward", "input.masked_fill_", "mask.expand_as"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "forward", "(", "ctx", ",", "input", ",", "mask", "=", "None", ")", ":", "\n", "        ", "ctx", ".", "save_for_backward", "(", "mask", ")", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "input", ".", "masked_fill_", "(", "mask", ".", "expand_as", "(", "input", ")", ",", "0", ")", "\n", "", "return", "input", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.MaskZero.backward": [[1342, 1349], ["print", "grad_output.sum", "grad_output.masked_fill_", "mask.expand_as"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "backward", "(", "ctx", ",", "grad_output", ")", ":", "\n", "        ", "print", "(", "'go:'", ",", "grad_output", ".", "sum", "(", ")", ")", "\n", "mask", "=", "ctx", ".", "saved_variables", "[", "0", "]", "\n", "if", "mask", "is", "not", "None", ":", "\n", "            ", "grad_output", ".", "masked_fill_", "(", "mask", ".", "expand_as", "(", "grad_output", ")", ",", "0", ")", "\n", "", "return", "grad_output", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.variational_dropout": [[26, 34], ["torch.autograd.Variable", "torch.autograd.Variable.unsqueeze().expand_as", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.bernoulli", "torch.autograd.Variable.unsqueeze", "x.data.new().zero_", "x.data.new", "x.size", "x.size"], "function", ["None"], ["def", "variational_dropout", "(", "x", ",", "p", "=", "0", ",", "training", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    x: batch * len * input_size\n    \"\"\"", "\n", "if", "training", "==", "False", "or", "p", "==", "0", ":", "\n", "        ", "return", "x", "\n", "", "dropout_mask", "=", "Variable", "(", "1.0", "/", "(", "1", "-", "p", ")", "*", "torch", ".", "bernoulli", "(", "(", "1", "-", "p", ")", "*", "(", "x", ".", "data", ".", "new", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "2", ")", ")", ".", "zero_", "(", ")", "+", "1", ")", ")", ",", "requires_grad", "=", "False", ")", "\n", "return", "dropout_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand_as", "(", "x", ")", "*", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout": [[36, 47], ["layers.variational_dropout", "torch.dropout", "len", "x.size"], "function", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.variational_dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "dropout", "(", "x", ",", "p", "=", "0", ",", "training", "=", "False", ",", "variational", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    x: (batch * len * input_size) or (any other shape)\n    \"\"\"", "\n", "if", "p", ">", "0", ":", "\n", "        ", "if", "variational", "and", "len", "(", "x", ".", "size", "(", ")", ")", "==", "3", ":", "# if x is (batch * len * input_size)", "\n", "            ", "return", "variational_dropout", "(", "x", ",", "p", "=", "p", ",", "training", "=", "training", ")", "\n", "", "else", ":", "\n", "            ", "return", "F", ".", "dropout", "(", "x", ",", "p", "=", "p", ",", "training", "=", "training", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.get_position_encoding": [[1272, 1293], ["len", "inv_timescales.unsqueeze_", "enumerate", "math.log", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.arange().unsqueeze_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "range", "torch.cat.unsqueeze_", "torch.cat.unsqueeze_", "float", "float", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.arange", "torch.arange", "torch.arange", "torch.arange"], "function", ["None"], ["", "", "def", "get_position_encoding", "(", "emb_size", ",", "lengths", ",", "min_timescale", "=", "1.0", ",", "max_timescale", "=", "1.0e4", ")", ":", "\n", "    ", "'''\n    create position embeding of size len1 (x len2 x len3 ...) x emb_size\n    reference: https://github.com/tensorflow/tensor2tensor/blob/8bdecbe434d93cb1e79c0489df20fee2d5a37dc2/tensor2tensor/layers/common_attention.py#L503\n    '''", "\n", "num_dims", "=", "len", "(", "lengths", ")", "\n", "num_timescales", "=", "emb_size", "//", "(", "num_dims", "*", "2", ")", "\n", "log_timescale_increment", "=", "(", "math", ".", "log", "(", "float", "(", "max_timescale", ")", "/", "float", "(", "min_timescale", ")", ")", "/", "(", "num_timescales", "-", "1", ")", ")", "\n", "inv_timescales", "=", "min_timescale", "*", "(", "torch", ".", "arange", "(", "num_timescales", ")", "*", "-", "log_timescale_increment", ")", ".", "exp", "(", ")", "\n", "inv_timescales", ".", "unsqueeze_", "(", "0", ")", "\n", "x", "=", "None", "\n", "for", "dim", ",", "length", "in", "enumerate", "(", "lengths", ")", ":", "\n", "        ", "position", "=", "torch", ".", "arange", "(", "length", ")", ".", "unsqueeze_", "(", "1", ")", "\n", "scaled_time", "=", "position", "*", "inv_timescales", "\n", "signal", "=", "torch", ".", "cat", "(", "[", "torch", ".", "sin", "(", "scaled_time", ")", ",", "torch", ".", "cos", "(", "scaled_time", ")", "]", ",", "dim", "=", "1", ")", "\n", "for", "_", "in", "range", "(", "dim", ")", ":", "\n", "            ", "signal", ".", "unsqueeze_", "(", "0", ")", "\n", "", "for", "_", "in", "range", "(", "num_dims", "-", "1", "-", "dim", ")", ":", "\n", "            ", "signal", ".", "unsqueeze_", "(", "-", "2", ")", "\n", "", "x", "=", "signal", "if", "x", "is", "None", "else", "x", "+", "signal", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.uniform_weights": [[1301, 1309], ["torch.autograd.Variable", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "alpha.cuda.cuda", "x_mask.eq().float", "alpha.cuda.sum().expand", "x.size", "x.size", "alpha.cuda.size", "x_mask.eq", "alpha.cuda.sum"], "function", ["None"], ["", "def", "uniform_weights", "(", "x", ",", "x_mask", ")", ":", "\n", "    ", "\"\"\"Return uniform weights over non-masked input.\"\"\"", "\n", "alpha", "=", "Variable", "(", "torch", ".", "ones", "(", "x", ".", "size", "(", "0", ")", ",", "x", ".", "size", "(", "1", ")", ")", ")", "\n", "if", "x", ".", "data", ".", "is_cuda", ":", "\n", "        ", "alpha", "=", "alpha", ".", "cuda", "(", ")", "\n", "", "alpha", "=", "alpha", "*", "x_mask", ".", "eq", "(", "0", ")", ".", "float", "(", ")", "\n", "alpha", "=", "alpha", "/", "alpha", ".", "sum", "(", "1", ")", ".", "expand", "(", "alpha", ".", "size", "(", ")", ")", "\n", "return", "alpha", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.weighted_avg": [[1311, 1316], ["weights.unsqueeze().bmm().squeeze", "weights.unsqueeze().bmm", "weights.unsqueeze"], "function", ["None"], ["", "def", "weighted_avg", "(", "x", ",", "weights", ")", ":", "\n", "    ", "\"\"\"x = batch * len * d\n    weights = batch * len\n    \"\"\"", "\n", "return", "weights", ".", "unsqueeze", "(", "1", ")", ".", "bmm", "(", "x", ")", ".", "squeeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.maskneginf": [[1351, 1353], ["MaskNegInf.apply"], "function", ["None"], ["", "", "def", "maskneginf", "(", "input", ",", "mask", ")", ":", "\n", "    ", "return", "MaskNegInf", ".", "apply", "(", "input", ",", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.maskzero": [[1355, 1357], ["MaskZero.apply"], "function", ["None"], ["", "def", "maskzero", "(", "input", ",", "mask", ")", ":", "\n", "    ", "return", "MaskZero", ".", "apply", "(", "input", ",", "mask", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.split_sentences": [[1358, 1378], ["x.unsqueeze.dim", "x.unsqueeze.transpose", "max", "enumerate", "x.unsqueeze.size", "len", "x.unsqueeze.unsqueeze", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "sentences.append", "torch.pad().transpose", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.pad"], "function", ["None"], ["", "def", "split_sentences", "(", "x", ",", "sentence_lens", ")", ":", "\n", "    ", "assert", "x", ".", "size", "(", "0", ")", "==", "len", "(", "sentence_lens", ")", "\n", "ndim", "=", "x", ".", "dim", "(", ")", "\n", "if", "ndim", "==", "2", ":", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "\n", "\n", "", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "sentences", "=", "[", "]", "\n", "max_sentence_len", "=", "max", "(", "l", "for", "s", "in", "sentence_lens", "for", "l", "in", "s", ")", "\n", "for", "i", ",", "lens", "in", "enumerate", "(", "sentence_lens", ")", ":", "\n", "        ", "pos", "=", "0", "\n", "for", "l", "in", "lens", ":", "\n", "            ", "sentences", ".", "append", "(", "F", ".", "pad", "(", "x", "[", "i", ",", ":", ",", "pos", ":", "pos", "+", "l", "]", ",", "(", "0", ",", "max_sentence_len", "-", "l", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "pos", "+=", "l", "\n", "\n", "", "", "if", "ndim", "==", "2", ":", "\n", "        ", "return", "torch", ".", "stack", "(", "sentences", ",", "0", ")", ".", "squeeze_", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "stack", "(", "sentences", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.combine_sentences": [[1379, 1402], ["x.unsqueeze.dim", "max", "torch.autograd.Variable", "enumerate", "x.unsqueeze.unsqueeze", "x.unsqueeze.data.new().zero_", "sum", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "docs.append", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "sum", "torch.cat.append", "torch.cat.append", "x.unsqueeze.data.new", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "x.unsqueeze.size"], "function", ["None"], ["", "", "def", "combine_sentences", "(", "x", ",", "sentence_lens", ")", ":", "\n", "    ", "ndim", "=", "x", ".", "dim", "(", ")", "\n", "if", "ndim", "==", "2", ":", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "docs", "=", "[", "]", "\n", "max_doc_len", "=", "max", "(", "sum", "(", "s", ")", "for", "s", "in", "sentence_lens", ")", "\n", "sent_id", "=", "0", "\n", "zeros", "=", "Variable", "(", "x", ".", "data", ".", "new", "(", "max_doc_len", ",", "x", ".", "size", "(", "2", ")", ")", ".", "zero_", "(", ")", ",", "requires_grad", "=", "False", ")", "\n", "for", "i", ",", "lens", "in", "enumerate", "(", "sentence_lens", ")", ":", "\n", "        ", "doc", "=", "[", "]", "\n", "doc_len", "=", "sum", "(", "lens", ")", "\n", "for", "l", "in", "lens", ":", "\n", "            ", "doc", ".", "append", "(", "x", "[", "sent_id", ",", ":", "l", "]", ")", "\n", "sent_id", "+=", "1", "\n", "", "if", "doc_len", "<", "max_doc_len", ":", "\n", "            ", "doc", ".", "append", "(", "zeros", "[", ":", "max_doc_len", "-", "doc_len", "]", ")", "\n", "", "doc", "=", "torch", ".", "cat", "(", "doc", ",", "0", ")", "\n", "docs", ".", "append", "(", "doc", ")", "\n", "\n", "", "if", "ndim", "==", "2", ":", "\n", "        ", "return", "torch", ".", "stack", "(", "docs", ",", "0", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "stack", "(", "docs", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.duplicate_for_sentences": [[1404, 1419], ["x.unsqueeze.dim", "enumerate", "isinstance", "torch.autograd.Variable", "x.unsqueeze.size", "len", "x.unsqueeze.unsqueeze", "duplicated.append", "torch.cat().squeeze_", "torch.cat().squeeze_", "torch.cat().squeeze_", "torch.cat().squeeze_", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x[].repeat", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "function", ["None"], ["", "", "def", "duplicate_for_sentences", "(", "x", ",", "sentence_lens", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "x", ",", "Variable", ")", ":", "\n", "        ", "x", "=", "Variable", "(", "x", ")", "\n", "", "assert", "x", ".", "size", "(", "0", ")", "==", "len", "(", "sentence_lens", ")", "\n", "ndim", "=", "x", ".", "dim", "(", ")", "\n", "if", "ndim", "==", "2", ":", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "duplicated", "=", "[", "]", "\n", "for", "i", ",", "lens", "in", "enumerate", "(", "sentence_lens", ")", ":", "\n", "        ", "duplicated", ".", "append", "(", "x", "[", "i", ":", "i", "+", "1", "]", ".", "repeat", "(", "len", "(", "lens", ")", ",", "1", ",", "1", ")", ")", "\n", "\n", "", "if", "ndim", "==", "2", ":", "\n", "        ", "return", "torch", ".", "cat", "(", "duplicated", ",", "0", ")", ".", "squeeze_", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "cat", "(", "duplicated", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.reduce_for_sentences": [[1420, 1434], ["x.unsqueeze.dim", "enumerate", "x.unsqueeze.unsqueeze", "reduced.append", "len", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "function", ["None"], ["", "", "def", "reduce_for_sentences", "(", "x", ",", "sentence_lens", ")", ":", "\n", "    ", "ndim", "=", "x", ".", "dim", "(", ")", "\n", "if", "ndim", "==", "2", ":", "\n", "        ", "x", "=", "x", ".", "unsqueeze", "(", "-", "1", ")", "\n", "", "reduced", "=", "[", "]", "\n", "offset", "=", "0", "\n", "for", "i", ",", "lens", "in", "enumerate", "(", "sentence_lens", ")", ":", "\n", "        ", "reduced", ".", "append", "(", "x", "[", "offset", "]", ")", "\n", "offset", "+=", "len", "(", "lens", ")", "\n", "\n", "", "if", "ndim", "==", "2", ":", "\n", "        ", "return", "torch", ".", "stack", "(", "reduced", ",", "0", ")", ".", "squeeze_", "(", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "return", "torch", ".", "stack", "(", "reduced", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.replace_nan_grad_hook": [[1435, 1438], ["grad.data.masked_fill_"], "function", ["None"], ["", "", "def", "replace_nan_grad_hook", "(", "grad", ")", ":", "\n", "    ", "grad", ".", "data", ".", "masked_fill_", "(", "grad", ".", "data", "!=", "grad", ".", "data", ",", "0", ")", "\n", "return", "grad", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.print_hook": [[1439, 1443], ["print", "grad.data.numel"], "function", ["None"], ["", "def", "print_hook", "(", "name", ")", ":", "\n", "    ", "def", "hook", "(", "grad", ")", ":", "\n", "        ", "print", "(", "'{}: {}/{}'", ".", "format", "(", "name", ",", "(", "grad", ".", "data", "!=", "grad", ".", "data", ")", ".", "sum", "(", ")", ",", "grad", ".", "data", ".", "numel", "(", ")", ")", ")", "\n", "", "return", "hook", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.logsumexp": [[1446, 1457], ["torch.max", "torch.max", "torch.max", "torch.max", "torch.log", "torch.log", "torch.log", "torch.log", "output.squeeze", "x.view", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "function", ["None"], ["", "def", "logsumexp", "(", "x", ",", "dim", "=", "None", ",", "keepdim", "=", "False", ")", ":", "\n", "    ", "if", "dim", "is", "None", ":", "\n", "        ", "x", ",", "dim", "=", "x", ".", "view", "(", "-", "1", ")", ",", "0", "\n", "", "xm", ",", "_", "=", "torch", ".", "max", "(", "x", ",", "dim", ",", "keepdim", "=", "True", ")", "\n", "# x = my_where(", "\n", "#     (xm == float('inf')) | (xm == float('-inf')), ", "\n", "#     xm,", "\n", "#     xm + torch.log(torch.sum(torch.exp(x - xm), dim, keepdim=True)))", "\n", "# return x if keepdim else x.squeeze(dim)", "\n", "output", "=", "xm", "+", "torch", ".", "log", "(", "torch", ".", "sum", "(", "torch", ".", "exp", "(", "x", "-", "xm", ")", ",", "dim", ",", "keepdim", "=", "True", ")", ")", "\n", "return", "output", "if", "keepdim", "else", "output", ".", "squeeze", "(", "dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.my_where": [[1460, 1465], ["torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like"], "function", ["None"], ["", "def", "my_where", "(", "cond", ",", "xt", ",", "xf", ")", ":", "\n", "    ", "ret", "=", "torch", ".", "zeros_like", "(", "xt", ")", "\n", "ret", "[", "cond", "]", "=", "xt", "[", "cond", "]", "\n", "ret", "[", "cond", "^", "1", "]", "=", "xf", "[", "cond", "^", "1", "]", "\n", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.normalize_text": [[23, 25], ["unicodedata.normalize"], "function", ["None"], ["def", "normalize_text", "(", "text", ")", ":", "\n", "    ", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.load_glove_vocab": [[26, 34], ["set", "open", "line.split", "general_utils.normalize_text", "set.add"], "function", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.normalize_text"], ["", "def", "load_glove_vocab", "(", "file", ",", "wv_dim", ")", ":", "\n", "    ", "vocab", "=", "set", "(", ")", "\n", "with", "open", "(", "file", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "elems", "=", "line", ".", "split", "(", ")", "\n", "token", "=", "normalize_text", "(", "''", ".", "join", "(", "elems", "[", "0", ":", "-", "wv_dim", "]", ")", ")", "\n", "vocab", ".", "add", "(", "token", ")", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.pre_proc_sru": [[35, 39], ["re.sub"], "function", ["None"], ["", "def", "pre_proc_sru", "(", "text", ")", ":", "\n", "    ", "'''normalize spaces in a string. From SRU DrQA code'''", "\n", "text", "=", "re", ".", "sub", "(", "'\\s+'", ",", "' '", ",", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.space_extend": [[40, 42], ["matchobj.group"], "function", ["None"], ["", "def", "space_extend", "(", "matchobj", ")", ":", "\n", "    ", "return", "' '", "+", "matchobj", ".", "group", "(", "0", ")", "+", "' '", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.pre_proc_fusion": [[43, 50], ["re.sub", "re.sub.strip", "re.sub"], "function", ["None"], ["", "def", "pre_proc_fusion", "(", "text", ")", ":", "\n", "    ", "'''from FusionNet-NLI'''", "\n", "# make hyphens, spaces clean", "\n", "text", "=", "re", ".", "sub", "(", "u'-|\\u2010|\\u2011|\\u2012|\\u2013|\\u2014|\\u2015|%|\\[|\\]|:|\\(|\\)|/'", ",", "space_extend", ",", "text", ")", "\n", "text", "=", "text", ".", "strip", "(", "' \\n'", ")", "\n", "text", "=", "re", ".", "sub", "(", "'\\s+'", ",", "' '", ",", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.pre_proc_qanet": [[61, 69], ["re.sub", "extra_split_chars_re.sub", "re.sub.strip", "re.sub"], "function", ["None"], ["def", "pre_proc_qanet", "(", "text", ")", ":", "\n", "    ", "'''from QANet code'''", "\n", "# make hyphens, spaces clean", "\n", "text", "=", "re", ".", "sub", "(", "u'-|\\u2010|\\u2011|\\u2012|\\u2013|\\u2014|\\u2015|%|\\[|\\]|:|\\(|\\)|/'", ",", "space_extend", ",", "text", ")", "\n", "text", "=", "extra_split_chars_re", ".", "sub", "(", "space_extend", ",", "text", ")", "\n", "text", "=", "text", ".", "strip", "(", "' \\n'", ")", "\n", "text", "=", "re", ".", "sub", "(", "'\\s+'", ",", "' '", ",", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.process_jsonlines": [[70, 81], ["jsonlines.open", "SNLIData", "snli_label.append", "snli_sent1.append", "snli_sent2.append"], "function", ["None"], ["", "def", "process_jsonlines", "(", "data_file", ")", ":", "\n", "    ", "with", "jsonlines", ".", "open", "(", "data_file", ")", "as", "reader", ":", "\n", "        ", "snli_label", "=", "[", "]", "\n", "snli_sent1", "=", "[", "]", "\n", "snli_sent2", "=", "[", "]", "\n", "for", "obj", "in", "reader", ":", "\n", "            ", "if", "obj", "[", "'gold_label'", "]", "!=", "'-'", ":", "\n", "                ", "snli_label", ".", "append", "(", "obj", "[", "'gold_label'", "]", ")", "\n", "snli_sent1", ".", "append", "(", "obj", "[", "'sentence1'", "]", ")", "\n", "snli_sent2", ".", "append", "(", "obj", "[", "'sentence2'", "]", ")", "\n", "", "", "return", "SNLIData", "(", "snli_label", ",", "snli_sent1", ",", "snli_sent2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.feature_gen": [[82, 101], ["zip", "collections.Counter", "sum", "A_features.append", "collections.Counter.values", "w.text.lower", "list", "w.text.lower", "w.text.lower", "w.text.lower", "zip", "w.text.lower", "w.text.lower"], "function", ["None"], ["", "", "def", "feature_gen", "(", "A_docs", ",", "B_docs", ")", ":", "\n", "    ", "A_tags", "=", "[", "[", "w", ".", "tag_", "for", "w", "in", "doc", "]", "for", "doc", "in", "A_docs", "]", "\n", "A_ents", "=", "[", "[", "w", ".", "ent_type_", "for", "w", "in", "doc", "]", "for", "doc", "in", "A_docs", "]", "\n", "A_features", "=", "[", "]", "\n", "\n", "for", "textA", ",", "textB", "in", "zip", "(", "A_docs", ",", "B_docs", ")", ":", "\n", "        ", "counter_", "=", "Counter", "(", "w", ".", "text", ".", "lower", "(", ")", "for", "w", "in", "textA", ")", "\n", "total", "=", "sum", "(", "counter_", ".", "values", "(", ")", ")", "\n", "term_freq", "=", "[", "counter_", "[", "w", ".", "text", ".", "lower", "(", ")", "]", "/", "total", "for", "w", "in", "textA", "]", "\n", "\n", "question_word", "=", "{", "w", ".", "text", "for", "w", "in", "textB", "}", "\n", "question_lower", "=", "{", "w", ".", "text", ".", "lower", "(", ")", "for", "w", "in", "textB", "}", "\n", "question_lemma", "=", "{", "w", ".", "lemma_", "if", "w", ".", "lemma_", "!=", "'-PRON-'", "else", "w", ".", "text", ".", "lower", "(", ")", "for", "w", "in", "textB", "}", "\n", "match_origin", "=", "[", "w", ".", "text", "in", "question_word", "for", "w", "in", "textA", "]", "\n", "match_lower", "=", "[", "w", ".", "text", ".", "lower", "(", ")", "in", "question_lower", "for", "w", "in", "textA", "]", "\n", "match_lemma", "=", "[", "(", "w", ".", "lemma_", "if", "w", ".", "lemma_", "!=", "'-PRON-'", "else", "w", ".", "text", ".", "lower", "(", ")", ")", "in", "question_lemma", "for", "w", "in", "textA", "]", "\n", "A_features", ".", "append", "(", "list", "(", "zip", "(", "term_freq", ",", "match_origin", ",", "match_lower", ",", "match_lemma", ")", ")", ")", "\n", "\n", "", "return", "A_tags", ",", "A_ents", ",", "A_features", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.build_embedding": [[102, 115], ["len", "numpy.random.uniform", "open", "enumerate", "line.split", "general_utils.normalize_text", "float"], "function", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.normalize_text"], ["", "def", "build_embedding", "(", "embed_file", ",", "targ_vocab", ",", "wv_dim", ")", ":", "\n", "    ", "vocab_size", "=", "len", "(", "targ_vocab", ")", "\n", "emb", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "1", ",", "(", "vocab_size", ",", "wv_dim", ")", ")", "\n", "emb", "[", "0", "]", "=", "0", "# <PAD> should be all 0 (using broadcast)", "\n", "\n", "w2id", "=", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "targ_vocab", ")", "}", "\n", "with", "open", "(", "embed_file", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "elems", "=", "line", ".", "split", "(", ")", "\n", "token", "=", "normalize_text", "(", "''", ".", "join", "(", "elems", "[", "0", ":", "-", "wv_dim", "]", ")", ")", "\n", "if", "token", "in", "w2id", ":", "\n", "                ", "emb", "[", "w2id", "[", "token", "]", "]", "=", "[", "float", "(", "v", ")", "for", "v", "in", "elems", "[", "-", "wv_dim", ":", "]", "]", "\n", "", "", "", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.general_utils.token2id": [[116, 120], ["enumerate"], "function", ["None"], ["", "def", "token2id", "(", "docs", ",", "vocab", ",", "unk_id", "=", "None", ")", ":", "\n", "    ", "w2id", "=", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "vocab", ")", "}", "\n", "ids", "=", "[", "[", "w2id", "[", "w", "]", "if", "w", "in", "w2id", "else", "unk_id", "for", "w", "in", "doc", "]", "for", "doc", "in", "docs", "]", "\n", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.AverageMeter.__init__": [[50, 52], ["utils.AverageMeter.reset"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.AverageMeter.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.AverageMeter.reset": [[53, 58], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "val", "=", "0", "\n", "self", ".", "avg", "=", "0", "\n", "self", ".", "sum", "=", "0", "\n", "self", ".", "count", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.AverageMeter.update": [[59, 64], ["None"], "methods", ["None"], ["", "def", "update", "(", "self", ",", "val", ",", "n", "=", "1", ")", ":", "\n", "        ", "self", ".", "val", "=", "val", "\n", "self", ".", "sum", "+=", "val", "*", "n", "\n", "self", ".", "count", "+=", "n", "\n", "self", ".", "avg", "=", "self", ".", "sum", "/", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.EMA.__init__": [[78, 81], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mu", ")", ":", "\n", "        ", "self", ".", "mu", "=", "mu", "\n", "self", ".", "shadow", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.EMA.register": [[82, 84], ["val.clone().cuda", "val.clone"], "methods", ["None"], ["", "def", "register", "(", "self", ",", "name", ",", "val", ")", ":", "\n", "        ", "self", ".", "shadow", "[", "name", "]", "=", "val", ".", "clone", "(", ")", ".", "cuda", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.EMA.__call__": [[85, 90], ["new_average.clone"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "name", ",", "x", ")", ":", "\n", "        ", "assert", "name", "in", "self", ".", "shadow", "\n", "new_average", "=", "self", ".", "mu", "*", "x", "+", "(", "1.0", "-", "self", ".", "mu", ")", "*", "self", ".", "shadow", "[", "name", "]", "\n", "self", ".", "shadow", "[", "name", "]", "=", "new_average", ".", "clone", "(", ")", "\n", "return", "new_average", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.BatchGen.__init__": [[177, 202], ["utils.BatchGen.opt.update", "list", "random.shuffle", "range", "range", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.AverageMeter.update"], ["    ", "def", "__init__", "(", "self", ",", "opt", "=", "{", "}", ",", "data", "=", "[", "]", ",", "batch_size", "=", "32", ",", "gpu", "=", "False", ",", "max_len", "=", "0", ",", "evaluation", "=", "False", ",", "with_cids", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        input:\n            data - list of lists\n            batch_size - int\n        \"\"\"", "\n", "self", ".", "opt", "=", "{", "'tf'", ":", "True", ",", "'use_feat_emb'", ":", "True", ",", "'pos_size'", ":", "12", ",", "'ner_size'", ":", "8", ",", "'use_elmo'", ":", "False", "}", "\n", "self", ".", "opt", ".", "update", "(", "opt", ")", "\n", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "eval", "=", "evaluation", "\n", "self", ".", "gpu", "=", "gpu", "\n", "self", ".", "max_len", "=", "max_len", "\n", "self", ".", "with_cids", "=", "with_cids", "\n", "\n", "# shuffle", "\n", "if", "not", "evaluation", ":", "\n", "            ", "indices", "=", "list", "(", "range", "(", "len", "(", "data", ")", ")", ")", "\n", "random", ".", "shuffle", "(", "indices", ")", "\n", "data", "=", "[", "data", "[", "i", "]", "for", "i", "in", "indices", "]", "\n", "", "if", "max_len", ">", "0", ":", "\n", "            ", "data", "=", "[", "d", "for", "d", "in", "data", "if", "len", "(", "d", "[", "1", "]", ")", "<=", "max_len", "]", "\n", "# chunk into batches", "\n", "", "data", "=", "[", "data", "[", "i", ":", "i", "+", "batch_size", "]", "for", "i", "in", "range", "(", "0", ",", "len", "(", "data", ")", ",", "batch_size", ")", "]", "\n", "self", ".", "data", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.BatchGen.__len__": [[203, 205], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.BatchGen.__iter__": [[206, 297], ["len", "list", "max", "torch.LongTensor().fill_", "enumerate", "len", "torch.Tensor().fill_", "enumerate", "max", "torch.LongTensor().fill_", "enumerate", "torch.eq", "torch.eq", "list", "list", "list", "zip", "torch.LongTensor", "enumerate", "torch.LongTensor().fill_", "enumerate", "torch.LongTensor().fill_", "enumerate", "torch.Tensor().fill_", "enumerate", "torch.Tensor().fill_", "enumerate", "torch.LongTensor", "torch.LongTensor().fill_", "enumerate", "torch.LongTensor().fill_", "enumerate", "torch.LongTensor", "tmp[].contiguous", "tmp[].contiguous", "context_id.pin_memory.pin_memory.pin_memory", "context_tag.pin_memory.pin_memory.pin_memory", "context_ent.pin_memory.pin_memory.pin_memory", "context_mask.pin_memory.pin_memory.pin_memory", "question_id.pin_memory.pin_memory.pin_memory", "question_mask.pin_memory.pin_memory.pin_memory", "len", "torch.LongTensor", "torch.Tensor", "torch.Tensor", "torch.LongTensor", "torch.LongTensor", "enumerate", "enumerate", "len", "torch.LongTensor", "torch.Tensor().fill_.pin_memory", "torch.LongTensor().fill_.cuda", "torch.LongTensor().fill_.cuda", "torch.LongTensor", "torch.LongTensor", "torch.Tensor", "torch.Tensor", "torch.LongTensor", "torch.LongTensor", "len", "len", "len", "len", "d.size", "d.size"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "batch", "in", "self", ".", "data", ":", "\n", "            ", "batch_size", "=", "len", "(", "batch", ")", "\n", "batch", "=", "list", "(", "zip", "(", "*", "batch", ")", ")", "\n", "\n", "\n", "context_len", "=", "max", "(", "len", "(", "x", ")", "for", "x", "in", "batch", "[", "1", "]", ")", "\n", "# print('context_len:', context_len)", "\n", "context_id", "=", "torch", ".", "LongTensor", "(", "batch_size", ",", "context_len", ")", ".", "fill_", "(", "0", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "batch", "[", "1", "]", ")", ":", "\n", "                ", "context_id", "[", "i", ",", ":", "len", "(", "doc", ")", "]", "=", "torch", ".", "LongTensor", "(", "doc", ")", "\n", "\n", "", "feature_len", "=", "len", "(", "batch", "[", "2", "]", "[", "0", "]", "[", "0", "]", ")", "\n", "\n", "context_feature", "=", "torch", ".", "Tensor", "(", "batch_size", ",", "context_len", ",", "feature_len", ")", ".", "fill_", "(", "0", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "batch", "[", "2", "]", ")", ":", "\n", "                ", "for", "j", ",", "feature", "in", "enumerate", "(", "doc", ")", ":", "\n", "                    ", "context_feature", "[", "i", ",", "j", ",", ":", "]", "=", "torch", ".", "Tensor", "(", "feature", ")", "\n", "", "", "if", "not", "self", ".", "opt", "[", "'tf'", "]", ":", "\n", "                ", "if", "self", ".", "opt", "[", "'match'", "]", ":", "\n", "                    ", "context_feature", "=", "context_feature", "[", ":", ",", ":", ",", ":", "3", "]", "\n", "", "else", ":", "\n", "                    ", "context_feature", "=", "None", "\n", "", "", "else", ":", "\n", "                ", "if", "not", "self", ".", "opt", "[", "'match'", "]", ":", "\n", "                    ", "context_feature", "=", "context_feature", "[", ":", ",", ":", ",", "3", ":", "]", "\n", "\n", "", "", "if", "self", ".", "opt", "[", "'use_feat_emb'", "]", ":", "\n", "                ", "context_tag", "=", "torch", ".", "LongTensor", "(", "batch_size", ",", "context_len", ")", ".", "fill_", "(", "0", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "batch", "[", "3", "]", ")", ":", "\n", "                    ", "context_tag", "[", "i", ",", ":", "len", "(", "doc", ")", "]", "=", "torch", ".", "LongTensor", "(", "doc", ")", "\n", "\n", "", "context_ent", "=", "torch", ".", "LongTensor", "(", "batch_size", ",", "context_len", ")", ".", "fill_", "(", "0", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "batch", "[", "4", "]", ")", ":", "\n", "                    ", "context_ent", "[", "i", ",", ":", "len", "(", "doc", ")", "]", "=", "torch", ".", "LongTensor", "(", "doc", ")", "\n", "", "", "else", ":", "\n", "# create one-hot vectors", "\n", "                ", "context_tag", "=", "torch", ".", "Tensor", "(", "batch_size", ",", "context_len", ",", "self", ".", "opt", "[", "'pos_size'", "]", ")", ".", "fill_", "(", "0", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "batch", "[", "3", "]", ")", ":", "\n", "                    ", "for", "j", ",", "tag", "in", "enumerate", "(", "doc", ")", ":", "\n", "                        ", "context_tag", "[", "i", ",", "j", ",", "tag", "]", "=", "1", "\n", "\n", "", "", "context_ent", "=", "torch", ".", "Tensor", "(", "batch_size", ",", "context_len", ",", "self", ".", "opt", "[", "'ner_size'", "]", ")", ".", "fill_", "(", "0", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "batch", "[", "4", "]", ")", ":", "\n", "                    ", "for", "j", ",", "ent", "in", "enumerate", "(", "doc", ")", ":", "\n", "                        ", "context_ent", "[", "i", ",", "j", ",", "ent", "]", "=", "1", "\n", "\n", "", "", "", "question_len", "=", "max", "(", "len", "(", "x", ")", "for", "x", "in", "batch", "[", "5", "]", ")", "\n", "question_id", "=", "torch", ".", "LongTensor", "(", "batch_size", ",", "question_len", ")", ".", "fill_", "(", "0", ")", "\n", "for", "i", ",", "doc", "in", "enumerate", "(", "batch", "[", "5", "]", ")", ":", "\n", "                ", "question_id", "[", "i", ",", ":", "len", "(", "doc", ")", "]", "=", "torch", ".", "LongTensor", "(", "doc", ")", "\n", "\n", "", "context_mask", "=", "torch", ".", "eq", "(", "context_id", ",", "0", ")", "\n", "question_mask", "=", "torch", ".", "eq", "(", "question_id", ",", "0", ")", "\n", "text", "=", "list", "(", "batch", "[", "6", "]", ")", "\n", "span", "=", "list", "(", "batch", "[", "7", "]", ")", "\n", "context_sentence_lens", "=", "list", "(", "batch", "[", "8", "]", ")", "\n", "\n", "if", "self", ".", "with_cids", ":", "\n", "                ", "context_char_id", "=", "torch", ".", "LongTensor", "(", "batch_size", ",", "context_len", ",", "50", ")", ".", "fill_", "(", "260", ")", "# 260 is padding_char", "\n", "for", "i", ",", "d", "in", "enumerate", "(", "batch", "[", "9", "]", ")", ":", "\n", "                    ", "context_char_id", "[", "i", ",", ":", "d", ".", "size", "(", "0", ")", "]", "=", "d", "\n", "", "question_char_id", "=", "torch", ".", "LongTensor", "(", "batch_size", ",", "question_len", ",", "50", ")", ".", "fill_", "(", "260", ")", "\n", "for", "i", ",", "d", "in", "enumerate", "(", "batch", "[", "10", "]", ")", ":", "\n", "                    ", "question_char_id", "[", "i", ",", ":", "d", ".", "size", "(", "0", ")", "]", "=", "d", "\n", "", "", "else", ":", "\n", "                ", "context_char_id", ",", "question_char_id", "=", "None", ",", "None", "\n", "\n", "", "if", "not", "self", ".", "eval", ":", "\n", "                ", "tmp", "=", "torch", ".", "LongTensor", "(", "[", "ex", "[", "0", "]", "for", "ex", "in", "batch", "[", "-", "1", "]", "]", ")", "\n", "y_s", "=", "tmp", "[", ":", ",", "0", "]", ".", "contiguous", "(", ")", "\n", "y_e", "=", "tmp", "[", ":", ",", "1", "]", ".", "contiguous", "(", ")", "\n", "", "elif", "context_char_id", "is", "not", "None", ":", "\n", "                ", "context_char_id", ".", "volatile", "=", "True", "\n", "question_char_id", ".", "volatile", "=", "True", "\n", "", "if", "self", ".", "gpu", ":", "\n", "                ", "context_id", "=", "context_id", ".", "pin_memory", "(", ")", "\n", "context_feature", "=", "context_feature", ".", "pin_memory", "(", ")", "if", "context_feature", "is", "not", "None", "else", "None", "\n", "context_tag", "=", "context_tag", ".", "pin_memory", "(", ")", "\n", "context_ent", "=", "context_ent", ".", "pin_memory", "(", ")", "\n", "context_mask", "=", "context_mask", ".", "pin_memory", "(", ")", "\n", "question_id", "=", "question_id", ".", "pin_memory", "(", ")", "\n", "question_mask", "=", "question_mask", ".", "pin_memory", "(", ")", "\n", "context_char_id", "=", "context_char_id", ".", "cuda", "(", ")", "if", "context_char_id", "is", "not", "None", "else", "None", "\n", "question_char_id", "=", "question_char_id", ".", "cuda", "(", ")", "if", "question_char_id", "is", "not", "None", "else", "None", "\n", "", "if", "self", ".", "eval", ":", "\n", "                ", "yield", "(", "context_id", ",", "context_feature", ",", "context_tag", ",", "context_ent", ",", "context_mask", ",", "\n", "question_id", ",", "question_mask", ",", "context_sentence_lens", ",", "context_char_id", ",", "question_char_id", ",", "text", ",", "span", ")", "\n", "", "else", ":", "\n", "                ", "yield", "(", "context_id", ",", "context_feature", ",", "context_tag", ",", "context_ent", ",", "context_mask", ",", "\n", "question_id", ",", "question_mask", ",", "context_sentence_lens", ",", "context_char_id", ",", "question_char_id", ",", "y_s", ",", "y_e", ",", "text", ",", "span", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.str2bool": [[66, 73], ["v.lower", "v.lower", "argparse.ArgumentTypeError"], "function", ["None"], ["", "", "def", "str2bool", "(", "v", ")", ":", "\n", "    ", "if", "v", ".", "lower", "(", ")", "in", "(", "'yes'", ",", "'true'", ",", "'t'", ",", "'y'", ",", "'1'", ")", ":", "\n", "        ", "return", "True", "\n", "", "elif", "v", ".", "lower", "(", ")", "in", "(", "'no'", ",", "'false'", ",", "'f'", ",", "'n'", ",", "'0'", ")", ":", "\n", "        ", "return", "False", "\n", "", "else", ":", "\n", "        ", "raise", "argparse", ".", "ArgumentTypeError", "(", "'Boolean value expected.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.load_data": [[92, 155], ["data_path.format.format", "torch.load", "torch.Tensor", "torch.Tensor.size", "torch.Tensor.size", "dev.sort", "log.info", "log.info", "torch.normal", "len", "len", "torch.save", "numpy.random.RandomState().permutation", "log.info", "log.info", "log.info", "torch.zeros", "enumerate", "log.info", "set", "set", "len", "numpy.random.RandomState", "len"], "function", ["None"], ["", "", "def", "load_data", "(", "opt", ",", "log", "=", "None", ")", ":", "\n", "    ", "if", "opt", "[", "'debug'", "]", ":", "\n", "        ", "data_path", "=", "'data/squad/sample-100{}.pth'", "\n", "", "else", ":", "\n", "        ", "data_path", "=", "'data/squad/data{}.pth'", "\n", "", "data_path", "=", "data_path", ".", "format", "(", "''", "if", "opt", "[", "'data_suffix'", "]", "==", "''", "else", "'-'", "+", "opt", "[", "'data_suffix'", "]", ")", "\n", "if", "log", ":", "\n", "        ", "log", ".", "info", "(", "'loading data from {}'", ".", "format", "(", "data_path", ")", ")", "\n", "", "saved", "=", "torch", ".", "load", "(", "data_path", ")", "\n", "if", "log", ":", "\n", "        ", "log", ".", "info", "(", "'done'", ")", "\n", "# with open('data/squad/meta.msgpack', 'rb') as f:", "\n", "# meta = msgpack.load(f, encoding='utf9')", "\n", "", "meta", "=", "saved", "[", "'meta'", "]", "\n", "embedding", "=", "torch", ".", "Tensor", "(", "meta", "[", "'embedding'", "]", ")", "\n", "opt", "[", "'pretrained_words'", "]", "=", "True", "\n", "opt", "[", "'vocab_size'", "]", "=", "embedding", ".", "size", "(", "0", ")", "\n", "opt", "[", "'embedding_dim'", "]", "=", "embedding", ".", "size", "(", "1", ")", "\n", "if", "not", "opt", "[", "'fix_embeddings'", "]", ":", "\n", "        ", "embedding", "[", "1", "]", "=", "torch", ".", "normal", "(", "torch", ".", "zeros", "(", "opt", "[", "'embedding_dim'", "]", ")", ",", "1.", ")", "\n", "", "opt", "[", "'pos_size'", "]", "=", "len", "(", "meta", "[", "'vocab_tag'", "]", ")", "if", "opt", "[", "'use_feat_emb'", "]", "else", "0", "\n", "opt", "[", "'ner_size'", "]", "=", "len", "(", "meta", "[", "'vocab_ent'", "]", ")", "if", "opt", "[", "'use_feat_emb'", "]", "else", "0", "\n", "# global id2w, w2id, full_vocab, w2cids", "\n", "id2w", "=", "meta", "[", "'vocab'", "]", "\n", "w2id", "=", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "id2w", ")", "}", "\n", "# with open(args.data_file, 'rb') as f:", "\n", "#     data = msgpack.load(f, encoding='utf8')", "\n", "data", "=", "saved", "[", "'data'", "]", "\n", "if", "'full_vocab'", "in", "meta", ":", "\n", "        ", "full_vocab", "=", "meta", "[", "'full_vocab'", "]", "\n", "", "else", ":", "\n", "        ", "if", "log", ":", "\n", "            ", "log", ".", "info", "(", "'getting full_vocab'", ")", "\n", "", "full_vocab", "=", "set", "(", "d", "[", "6", "]", "[", "s", ":", "e", "]", "for", "d", "in", "data", "[", "'train'", "]", "+", "data", "[", "'dev'", "]", "for", "s", ",", "e", "in", "d", "[", "7", "]", ")", "|", "set", "(", "meta", "[", "'vocab'", "]", ")", "\n", "saved", "[", "'meta'", "]", "[", "'full_vocab'", "]", "=", "full_vocab", "\n", "torch", ".", "save", "(", "saved", ",", "data_path", ")", "\n", "", "if", "opt", "[", "'valid_size'", "]", ">", "0", ":", "\n", "        ", "perm_idx", "=", "np", ".", "random", ".", "RandomState", "(", "4444", ")", ".", "permutation", "(", "len", "(", "data", "[", "'train'", "]", ")", ")", "\n", "train", "=", "[", "data", "[", "'train'", "]", "[", "i", "]", "for", "i", "in", "perm_idx", "[", ":", "-", "opt", "[", "'valid_size'", "]", "]", "]", "\n", "dev", "=", "[", "data", "[", "'train'", "]", "[", "i", "]", "for", "i", "in", "perm_idx", "[", "-", "opt", "[", "'valid_size'", "]", ":", "]", "]", "\n", "", "else", ":", "\n", "        ", "train", "=", "data", "[", "'train'", "]", "\n", "dev", "=", "data", "[", "'dev'", "]", "\n", "", "if", "log", ":", "\n", "        ", "log", ".", "info", "(", "'sorting dev'", ")", "\n", "", "dev", ".", "sort", "(", "key", "=", "lambda", "x", ":", "len", "(", "x", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "# if log:", "\n", "#     log.info('getting w2cids')", "\n", "# w2cids = {w: torch.IntTensor(ELMoCharacterMapper.convert_word_to_char_ids(w)) for w in full_vocab}", "\n", "# meta['w2cids'] = w2cids", "\n", "\n", "train_y", "=", "[", "x", "[", "-", "2", "]", "for", "x", "in", "train", "]", "\n", "if", "opt", "[", "'valid_size'", "]", ">", "0", ":", "\n", "        ", "dev_y", "=", "[", "x", "[", "-", "2", "]", "for", "x", "in", "dev", "]", "\n", "", "else", ":", "\n", "        ", "dev_y", "=", "[", "x", "[", "-", "1", "]", "for", "x", "in", "dev", "]", "\n", "", "if", "log", ":", "\n", "        ", "log", ".", "info", "(", "'generating cids'", ")", "\n", "", "train", "=", "[", "d", "[", ":", "9", "]", "+", "(", "None", ",", "None", ")", "+", "d", "[", "9", ":", "]", "for", "d", "in", "train", "]", "\n", "dev", "=", "[", "d", "[", ":", "9", "]", "+", "(", "None", ",", "None", ")", "+", "d", "[", "9", ":", "]", "for", "d", "in", "dev", "]", "\n", "if", "log", ":", "\n", "        ", "log", ".", "info", "(", "'done'", ")", "\n", "", "return", "train", ",", "dev", ",", "train_y", ",", "dev_y", ",", "embedding", ",", "opt", ",", "meta", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.build_embedding": [[157, 170], ["len", "numpy.random.uniform", "open", "enumerate", "line.split", "utils.normalize_text", "float"], "function", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.normalize_text"], ["", "def", "build_embedding", "(", "embed_file", ",", "targ_vocab", ",", "wv_dim", ")", ":", "\n", "    ", "vocab_size", "=", "len", "(", "targ_vocab", ")", "\n", "emb", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", ",", "1", ",", "(", "vocab_size", ",", "wv_dim", ")", ")", "\n", "emb", "[", "0", "]", "=", "0", "# <PAD> should be all 0 (using broadcast)", "\n", "\n", "w2id", "=", "{", "w", ":", "i", "for", "i", ",", "w", "in", "enumerate", "(", "targ_vocab", ")", "}", "\n", "with", "open", "(", "embed_file", ",", "encoding", "=", "\"utf8\"", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "elems", "=", "line", ".", "split", "(", ")", "\n", "token", "=", "normalize_text", "(", "''", ".", "join", "(", "elems", "[", "0", ":", "-", "wv_dim", "]", ")", ")", "\n", "if", "token", "in", "w2id", ":", "\n", "                ", "emb", "[", "w2id", "[", "token", "]", "]", "=", "[", "float", "(", "v", ")", "for", "v", "in", "elems", "[", "-", "wv_dim", ":", "]", "]", "\n", "", "", "", "return", "emb", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.normalize_text": [[172, 174], ["unicodedata.normalize"], "function", ["None"], ["", "def", "normalize_text", "(", "text", ")", ":", "\n", "    ", "return", "unicodedata", ".", "normalize", "(", "'NFD'", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils._normalize_answer": [[299, 314], ["utils._normalize_answer.white_space_fix"], "function", ["None"], ["", "", "", "", "def", "_normalize_answer", "(", "s", ")", ":", "\n", "    ", "def", "remove_articles", "(", "text", ")", ":", "\n", "        ", "return", "re", ".", "sub", "(", "r'\\b(a|an|the)\\b'", ",", "' '", ",", "text", ")", "\n", "\n", "", "def", "white_space_fix", "(", "text", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "\n", "", "def", "remove_punc", "(", "text", ")", ":", "\n", "        ", "exclude", "=", "set", "(", "string", ".", "punctuation", ")", "\n", "return", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "ch", "not", "in", "exclude", ")", "\n", "\n", "", "def", "lower", "(", "text", ")", ":", "\n", "        ", "return", "text", ".", "lower", "(", ")", "\n", "\n", "", "return", "white_space_fix", "(", "remove_articles", "(", "remove_punc", "(", "lower", "(", "s", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils._exact_match": [[316, 324], ["utils._normalize_answer", "utils._normalize_answer"], "function", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils._normalize_answer", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils._normalize_answer"], ["", "def", "_exact_match", "(", "pred", ",", "answers", ")", ":", "\n", "    ", "if", "pred", "is", "None", "or", "answers", "is", "None", ":", "\n", "        ", "return", "False", "\n", "", "pred", "=", "_normalize_answer", "(", "pred", ")", "\n", "for", "a", "in", "answers", ":", "\n", "        ", "if", "pred", "==", "_normalize_answer", "(", "a", ")", ":", "\n", "            ", "return", "True", "\n", "", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils._f1_score": [[326, 342], ["_normalize_answer().split", "max", "sum", "utils._f1_score._score"], "function", ["None"], ["", "def", "_f1_score", "(", "pred", ",", "answers", ")", ":", "\n", "    ", "def", "_score", "(", "g_tokens", ",", "a_tokens", ")", ":", "\n", "        ", "common", "=", "Counter", "(", "g_tokens", ")", "&", "Counter", "(", "a_tokens", ")", "\n", "num_same", "=", "sum", "(", "common", ".", "values", "(", ")", ")", "\n", "if", "num_same", "==", "0", ":", "\n", "            ", "return", "0", "\n", "", "precision", "=", "1.", "*", "num_same", "/", "len", "(", "g_tokens", ")", "\n", "recall", "=", "1.", "*", "num_same", "/", "len", "(", "a_tokens", ")", "\n", "f1", "=", "(", "2", "*", "precision", "*", "recall", ")", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", "\n", "\n", "", "if", "pred", "is", "None", "or", "answers", "is", "None", ":", "\n", "        ", "return", "0", "\n", "", "g_tokens", "=", "_normalize_answer", "(", "pred", ")", ".", "split", "(", ")", "\n", "scores", "=", "[", "_score", "(", "g_tokens", ",", "_normalize_answer", "(", "a", ")", ".", "split", "(", ")", ")", "for", "a", "in", "answers", "]", "\n", "return", "max", "(", "scores", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.score": [[344, 354], ["zip", "len", "len", "utils._exact_match", "utils._f1_score"], "function", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils._exact_match", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils._f1_score"], ["", "def", "score", "(", "pred", ",", "truth", ")", ":", "\n", "    ", "assert", "len", "(", "pred", ")", "==", "len", "(", "truth", ")", "\n", "f1", "=", "em", "=", "total", "=", "0", "\n", "for", "p", ",", "t", "in", "zip", "(", "pred", ",", "truth", ")", ":", "\n", "        ", "total", "+=", "1", "\n", "em", "+=", "_exact_match", "(", "p", ",", "t", ")", "\n", "f1", "+=", "_f1_score", "(", "p", ",", "t", ")", "\n", "", "em", "=", "100.", "*", "em", "/", "total", "\n", "f1", "=", "100.", "*", "f1", "/", "total", "\n", "return", "em", ",", "f1", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.utils.get_inv_group": [[355, 363], ["enumerate", "inv_group[].append"], "function", ["None"], ["", "def", "get_inv_group", "(", "group_id", ")", ":", "\n", "    ", "inv_group", "=", "{", "}", "\n", "for", "i", ",", "g", "in", "enumerate", "(", "group_id", ")", ":", "\n", "        ", "if", "g", "in", "inv_group", ":", "\n", "            ", "inv_group", "[", "g", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "            ", "inv_group", "[", "g", "]", "=", "[", "i", "]", "\n", "", "", "return", "inv_group", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RCModelProto.__init__": [[23, 28], ["torch.Module.__init__", "rnn_reader.RCModelProto.setup_emb_modules"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RCModelProto.setup_emb_modules"], ["def", "__init__", "(", "self", ",", "opt", ",", "padding_idx", "=", "0", ",", "embedding", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# Store config", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "setup_emb_modules", "(", "padding_idx", ",", "embedding", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RCModelProto.setup_emb_modules": [[29, 112], ["torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "layers.MTLSTM", "layers.SeqAttnMatch", "layers.SeqAttnMatch", "embedding.size", "embedding.size", "rnn_reader.RCModelProto.embedding.parameters", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "torch.Embedding", "rnn_reader.RCModelProto.register_buffer", "embedding.size"], "methods", ["None"], ["", "def", "setup_emb_modules", "(", "self", ",", "padding_idx", "=", "0", ",", "embedding", "=", "None", ")", ":", "\n", "        ", "opt", "=", "self", ".", "opt", "\n", "# Word embeddings", "\n", "self", ".", "paired_input_size", "=", "opt", "[", "'embedding_dim'", "]", "\n", "if", "opt", "[", "'pretrained_words'", "]", ":", "\n", "            ", "assert", "embedding", "is", "not", "None", "\n", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "embedding", ".", "size", "(", "0", ")", ",", "\n", "embedding", ".", "size", "(", "1", ")", ",", "\n", "padding_idx", "=", "padding_idx", ")", "\n", "self", ".", "embedding", ".", "weight", ".", "data", "[", "2", ":", ",", ":", "]", "=", "embedding", "[", "2", ":", ",", ":", "]", "\n", "if", "opt", "[", "'fix_embeddings'", "]", ":", "\n", "                ", "assert", "opt", "[", "'tune_partial'", "]", "==", "0", "\n", "for", "p", "in", "self", ".", "embedding", ".", "parameters", "(", ")", ":", "\n", "                    ", "p", ".", "requires_grad", "=", "False", "\n", "", "", "elif", "opt", "[", "'tune_partial'", "]", ">", "0", ":", "\n", "                ", "assert", "opt", "[", "'tune_partial'", "]", "+", "2", "<", "embedding", ".", "size", "(", "0", ")", "\n", "fixed_embedding", "=", "embedding", "[", "opt", "[", "'tune_partial'", "]", "+", "2", ":", "]", "\n", "self", ".", "register_buffer", "(", "'fixed_embedding'", ",", "fixed_embedding", ")", "\n", "self", ".", "fixed_embedding", "=", "fixed_embedding", "\n", "", "", "else", ":", "# random initialized", "\n", "            ", "self", ".", "embedding", "=", "nn", ".", "Embedding", "(", "opt", "[", "'vocab_size'", "]", ",", "\n", "opt", "[", "'embedding_dim'", "]", ",", "\n", "padding_idx", "=", "padding_idx", ")", "\n", "\n", "# Character embeddings", "\n", "", "if", "opt", "[", "'use_max_char_emb'", "]", ":", "\n", "            ", "self", ".", "max_char_emb", "=", "nn", ".", "Embedding", "(", "301", ",", "opt", "[", "'max_char_emb_size'", "]", ",", "padding_idx", "=", "260", ")", "\n", "self", ".", "paired_input_size", "+=", "opt", "[", "'max_char_emb_size'", "]", "\n", "\n", "# Contextualized embeddings", "\n", "", "if", "opt", "[", "'use_cove'", "]", ":", "\n", "            ", "self", ".", "CoVe", "=", "layers", ".", "MTLSTM", "(", "opt", ",", "embedding", ",", "padding_idx", "=", "padding_idx", ")", "\n", "self", ".", "paired_input_size", "+=", "self", ".", "CoVe", ".", "output_size", "\n", "\n", "# Input size to RNN: word emb + question emb + manual features", "\n", "", "doc_input_size", "=", "self", ".", "paired_input_size", "+", "opt", "[", "'num_features'", "]", "\n", "question_input_size", "=", "self", ".", "paired_input_size", "\n", "if", "opt", "[", "'use_feat_emb'", "]", ":", "\n", "            ", "if", "opt", "[", "'pos'", "]", ":", "\n", "                ", "doc_input_size", "+=", "opt", "[", "'pos_dim'", "]", "\n", "self", ".", "pos_embedding", "=", "nn", ".", "Embedding", "(", "opt", "[", "'pos_size'", "]", ",", "opt", "[", "'pos_dim'", "]", ")", "\n", "", "if", "opt", "[", "'ner'", "]", ":", "\n", "                ", "doc_input_size", "+=", "opt", "[", "'ner_dim'", "]", "\n", "self", ".", "ner_embedding", "=", "nn", ".", "Embedding", "(", "opt", "[", "'ner_size'", "]", ",", "opt", "[", "'ner_dim'", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "opt", "[", "'pos'", "]", ":", "\n", "                ", "doc_input_size", "+=", "opt", "[", "'pos_size'", "]", "\n", "", "if", "opt", "[", "'ner'", "]", ":", "\n", "                ", "doc_input_size", "+=", "opt", "[", "'ner_size'", "]", "\n", "\n", "# Projection for attention weighted question", "\n", "", "", "if", "opt", "[", "'use_qemb'", "]", ":", "\n", "            ", "self", ".", "qemb_match", "=", "layers", ".", "SeqAttnMatch", "(", "opt", "[", "'embedding_dim'", "]", ",", "\n", "dropout", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", ")", "\n", "# self.qemb_match = layers.FullAttention(", "\n", "#     full_size=opt['embedding_dim'],", "\n", "#     hidden_size=opt['embedding_dim'],", "\n", "#     num_level=1,", "\n", "#     dropout=opt['dropout_rnn'],", "\n", "#     variational_dropout=opt['variational_dropout'],", "\n", "# )", "\n", "doc_input_size", "+=", "opt", "[", "'embedding_dim'", "]", "\n", "", "if", "opt", "[", "'use_demb'", "]", ":", "\n", "            ", "self", ".", "demb_match", "=", "layers", ".", "SeqAttnMatch", "(", "opt", "[", "'embedding_dim'", "]", ",", "\n", "dropout", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", ")", "\n", "# self.demb_match = layers.FullAttention(", "\n", "#     full_size=opt['embedding_dim'],", "\n", "#     hidden_size=opt['embedding_dim'],", "\n", "#     num_level=1,", "\n", "#     dropout=opt['dropout_rnn'],", "\n", "#     variational_dropout=opt['variational_dropout'],", "\n", "# )", "\n", "question_input_size", "+=", "opt", "[", "'embedding_dim'", "]", "\n", "\n", "", "if", "opt", "[", "'use_qemb'", "]", "and", "opt", "[", "'use_demb'", "]", ":", "\n", "            ", "self", ".", "paired_input_size", "+=", "opt", "[", "'embedding_dim'", "]", "\n", "\n", "", "self", ".", "doc_input_size", "=", "doc_input_size", "\n", "self", ".", "question_input_size", "=", "question_input_size", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RCModelProto.forward_emb": [[113, 231], ["layers.maskzero", "layers.maskzero", "layers.maskzero", "layers.maskzero", "x1.data.masked_fill_", "x2.data.masked_fill_", "layers.dropout", "x1_all_list.append", "rnn_reader.RCModelProto.embedding", "rnn_reader.RCModelProto.embedding", "x1_all_list.append", "x2_all_list.append", "x1_paired_list.append", "x2_paired_list.append", "x1_char.size", "x1_all_list.append", "x2_all_list.append", "x1_paired_list.append", "x2_paired_list.append", "rnn_reader.RCModelProto.CoVe", "rnn_reader.RCModelProto.CoVe", "x1_all_list.append", "x2_all_list.append", "x1_paired_list.append", "x2_paired_list.append", "rnn_reader.RCModelProto.qemb_match", "x1_all_list.append", "rnn_reader.RCModelProto.demb_match", "x2_all_list.append", "x1_paired_list.append", "x2_paired_list.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x1_mask.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x2_mask.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x1_mask.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x2_mask.unsqueeze", "rnn_reader.RCModelProto.opt.get", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "rnn_reader.RCModelProto.opt.get", "dropout_mask.cuda.cuda.cuda", "rnn_reader.RCModelProto.opt.get", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "rnn_reader.RCModelProto.opt.get", "dropout_mask.cuda.cuda.cuda", "rnn_reader.RCModelProto.forward_emb.dropout"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.maskzero", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.maskzero", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.maskzero", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.maskzero", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "forward_emb", "(", "self", ",", "x1", ",", "x1_f", ",", "x1_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", "=", "None", ",", "x2_char", "=", "None", ")", ":", "\n", "        ", "\"\"\"Inputs:\n        x1 = document word indices             [batch * len_d]\n        x1_f = document word features indices  [batch * len_d * nfeat]\n        x1_pos = document POS tags             [batch * len_d]\n        x1_ner = document entity tags          [batch * len_d]\n        x1_mask = document padding mask        [batch * len_d]\n        x2 = question word indices             [batch * len_q]\n        x2_mask = question padding mask        [batch * len_q]\n        x1_char = document char indices        [batch * len_d * nchar]\n        x2_char = question char indices        [batch * len_q * nchar]\n        \"\"\"", "\n", "\n", "if", "self", ".", "training", "and", "self", ".", "opt", ".", "get", "(", "'word_dropout_c'", ",", "0.", ")", ">", "0.", ":", "\n", "            ", "dropout_mask", "=", "torch", ".", "rand", "(", "x1", ".", "size", "(", ")", ")", "<", "self", ".", "opt", ".", "get", "(", "'word_dropout_c'", ",", "0.", ")", "\n", "if", "x1", ".", "is_cuda", ":", "\n", "                ", "dropout_mask", "=", "dropout_mask", ".", "cuda", "(", ")", "\n", "", "x1", ".", "data", ".", "masked_fill_", "(", "dropout_mask", ",", "1", ")", "\n", "", "if", "self", ".", "training", "and", "self", ".", "opt", ".", "get", "(", "'word_dropout_q'", ",", "0.", ")", ">", "0.", ":", "\n", "            ", "dropout_mask", "=", "torch", ".", "rand", "(", "x2", ".", "size", "(", ")", ")", "<", "self", ".", "opt", ".", "get", "(", "'word_dropout_q'", ",", "0.", ")", "\n", "if", "x1", ".", "is_cuda", ":", "\n", "                ", "dropout_mask", "=", "dropout_mask", ".", "cuda", "(", ")", "\n", "", "x2", ".", "data", ".", "masked_fill_", "(", "dropout_mask", ",", "1", ")", "\n", "\n", "\n", "", "def", "dropout", "(", "x", ",", "p", "=", "self", ".", "opt", "[", "'dropout_rnn'", "]", ")", ":", "\n", "            ", "return", "layers", ".", "dropout", "(", "x", ",", "p", "=", "p", ",", "\n", "training", "=", "self", ".", "training", ",", "variational", "=", "self", ".", "opt", "[", "'variational_dropout'", "]", "and", "x", ".", "dim", "(", ")", "==", "3", ")", "\n", "", "feat_dict", "=", "{", "}", "\n", "x1_all_list", ",", "x2_all_list", "=", "[", "]", ",", "[", "]", "\n", "x1_paired_list", ",", "x2_paired_list", "=", "[", "]", ",", "[", "]", "\n", "\n", "if", "x1_f", "is", "not", "None", ":", "\n", "            ", "x1_all_list", ".", "append", "(", "x1_f", ")", "\n", "\n", "# Embed both document and question", "\n", "", "if", "self", ".", "opt", "[", "'use_word_emb'", "]", "or", "self", ".", "opt", "[", "'use_qemb'", "]", "or", "self", ".", "opt", "[", "'use_demb'", "]", ":", "\n", "            ", "x1_emb", "=", "self", ".", "embedding", "(", "x1", ")", "\n", "x2_emb", "=", "self", ".", "embedding", "(", "x2", ")", "\n", "if", "self", ".", "opt", "[", "'dropout_emb'", "]", ">", "0", ":", "\n", "                ", "x1_emb", "=", "dropout", "(", "x1_emb", ",", "self", ".", "opt", "[", "'dropout_emb'", "]", ")", "\n", "x2_emb", "=", "dropout", "(", "x2_emb", ",", "self", ".", "opt", "[", "'dropout_emb'", "]", ")", "\n", "", "feat_dict", "[", "'x1_emb'", "]", "=", "x1_emb", "\n", "feat_dict", "[", "'x2_emb'", "]", "=", "x2_emb", "\n", "\n", "", "if", "self", ".", "opt", "[", "'use_word_emb'", "]", ":", "\n", "            ", "x1_all_list", ".", "append", "(", "x1_emb", ")", "\n", "x2_all_list", ".", "append", "(", "x2_emb", ")", "\n", "x1_paired_list", ".", "append", "(", "x1_emb", ")", "\n", "x2_paired_list", ".", "append", "(", "x2_emb", ")", "\n", "\n", "", "if", "self", ".", "opt", "[", "'use_max_char_emb'", "]", ":", "\n", "            ", "bs", "=", "x1_char", ".", "size", "(", "0", ")", "\n", "char_len", "=", "self", ".", "opt", "[", "'max_char_emb_max_len'", "]", "\n", "char_dim", "=", "self", ".", "opt", "[", "'max_char_emb_size'", "]", "\n", "x1_max_char_emb", "=", "self", ".", "max_char_emb", "(", "x1_char", "[", ":", ",", ":", ",", ":", "char_len", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "char_len", ")", ")", ".", "view", "(", "bs", ",", "-", "1", ",", "char_len", ",", "char_dim", ")", ".", "max", "(", "2", ")", "[", "0", "]", "\n", "x2_max_char_emb", "=", "self", ".", "max_char_emb", "(", "x2_char", "[", ":", ",", ":", ",", ":", "char_len", "]", ".", "contiguous", "(", ")", ".", "view", "(", "-", "1", ",", "char_len", ")", ")", ".", "view", "(", "bs", ",", "-", "1", ",", "char_len", ",", "char_dim", ")", ".", "max", "(", "2", ")", "[", "0", "]", "\n", "if", "self", ".", "opt", "[", "'dropout_emb'", "]", ">", "0", ":", "\n", "                ", "x1_max_char_emb", "=", "dropout", "(", "x1_max_char_emb", ",", "self", ".", "opt", "[", "'dropout_emb'", "]", ")", "\n", "x2_max_char_emb", "=", "dropout", "(", "x2_max_char_emb", ",", "self", ".", "opt", "[", "'dropout_emb'", "]", ")", "\n", "", "x1_all_list", ".", "append", "(", "x1_max_char_emb", ")", "\n", "x2_all_list", ".", "append", "(", "x2_max_char_emb", ")", "\n", "x1_paired_list", ".", "append", "(", "x1_max_char_emb", ")", "\n", "x2_paired_list", ".", "append", "(", "x2_max_char_emb", ")", "\n", "\n", "# Contextualized embeddings", "\n", "", "if", "self", ".", "opt", "[", "'use_cove'", "]", ":", "\n", "            ", "_", ",", "x1_cove", "=", "self", ".", "CoVe", "(", "x1", ",", "x1_mask", ")", "\n", "_", ",", "x2_cove", "=", "self", ".", "CoVe", "(", "x2", ",", "x2_mask", ")", "\n", "if", "self", ".", "opt", "[", "'dropout_emb'", "]", ">", "0", ":", "\n", "                ", "x1_cove", "=", "dropout", "(", "x1_cove", ",", "self", ".", "opt", "[", "'dropout_emb'", "]", ")", "\n", "x2_cove", "=", "dropout", "(", "x2_cove", ",", "self", ".", "opt", "[", "'dropout_emb'", "]", ")", "\n", "", "x1_all_list", ".", "append", "(", "x1_cove", ")", "\n", "x2_all_list", ".", "append", "(", "x2_cove", ")", "\n", "x1_paired_list", ".", "append", "(", "x1_cove", ")", "\n", "x2_paired_list", ".", "append", "(", "x2_cove", ")", "\n", "feat_dict", "[", "'x1_cove'", "]", "=", "x1_cove", "\n", "feat_dict", "[", "'x2_cove'", "]", "=", "x2_cove", "\n", "\n", "", "if", "self", ".", "opt", "[", "'use_feat_emb'", "]", ":", "\n", "            ", "if", "self", ".", "opt", "[", "'pos'", "]", ":", "\n", "                ", "x1_pos_emb", "=", "self", ".", "pos_embedding", "(", "x1_pos", ")", "\n", "x1_all_list", ".", "append", "(", "x1_pos_emb", ")", "\n", "feat_dict", "[", "'x1_pos_emb'", "]", "=", "x1_pos_emb", "\n", "", "if", "self", ".", "opt", "[", "'ner'", "]", ":", "\n", "                ", "x1_ner_emb", "=", "self", ".", "ner_embedding", "(", "x1_ner", ")", "\n", "x1_all_list", ".", "append", "(", "x1_ner_emb", ")", "\n", "feat_dict", "[", "'x1_ner_emb'", "]", "=", "x1_ner_emb", "\n", "", "", "else", ":", "\n", "            ", "if", "self", ".", "opt", "[", "'pos'", "]", ":", "\n", "                ", "x1_all_list", ".", "append", "(", "x1_pos", ")", "\n", "feat_dict", "[", "'x1_pos'", "]", "=", "x1_pos", "\n", "", "if", "self", ".", "opt", "[", "'ner'", "]", ":", "\n", "                ", "x1_all_list", ".", "append", "(", "x1_ner", ")", "\n", "feat_dict", "[", "'x1_ner'", "]", "=", "x1_ner", "\n", "\n", "# Add attention-weighted question representation (word level fusion)", "\n", "", "", "if", "self", ".", "opt", "[", "'use_qemb'", "]", ":", "\n", "            ", "x1_qemb", "=", "self", ".", "qemb_match", "(", "x1_emb", ",", "x2_emb", ",", "x2_mask", ")", "\n", "# x1_qemb = self.qemb_match(x1_emb, x2_emb, x2_emb, x2_mask)", "\n", "x1_all_list", ".", "append", "(", "x1_qemb", ")", "\n", "feat_dict", "[", "'x1_qemb'", "]", "=", "x1_qemb", "\n", "\n", "", "if", "self", ".", "opt", "[", "'use_demb'", "]", ":", "\n", "            ", "x2_demb", "=", "self", ".", "demb_match", "(", "x2_emb", ",", "x1_emb", ",", "x1_mask", ")", "\n", "# x2_demb = self.demb_match(x2_emb, x1_emb, x1_emb, x1_mask)", "\n", "x2_all_list", ".", "append", "(", "x2_demb", ")", "\n", "feat_dict", "[", "'x2_demb'", "]", "=", "x2_demb", "\n", "\n", "", "if", "self", ".", "opt", "[", "'use_qemb'", "]", "and", "self", ".", "opt", "[", "'use_demb'", "]", ":", "\n", "            ", "x1_paired_list", ".", "append", "(", "x1_qemb", ")", "\n", "x2_paired_list", ".", "append", "(", "x2_demb", ")", "\n", "\n", "", "x1_paired_emb", "=", "layers", ".", "maskzero", "(", "torch", ".", "cat", "(", "x1_paired_list", ",", "2", ")", ",", "x1_mask", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "x2_paired_emb", "=", "layers", ".", "maskzero", "(", "torch", ".", "cat", "(", "x2_paired_list", ",", "2", ")", ",", "x2_mask", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "x1_full_emb", "=", "layers", ".", "maskzero", "(", "torch", ".", "cat", "(", "x1_all_list", ",", "2", ")", ",", "x1_mask", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "x2_full_emb", "=", "layers", ".", "maskzero", "(", "torch", ".", "cat", "(", "x2_all_list", ",", "2", ")", ",", "x2_mask", ".", "unsqueeze", "(", "-", "1", ")", ")", "\n", "return", "x1_paired_emb", ",", "x2_paired_emb", ",", "x1_full_emb", ",", "x2_full_emb", ",", "feat_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RCModelProto.forward": [[232, 234], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x1", ",", "x1_f", ",", "x0_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", "=", "None", ",", "x2_char", "=", "None", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RnnDocReader.__init__": [[239, 292], ["rnn_reader.RCModelProto.__init__", "layers.StackedBRNN", "layers.StackedBRNN", "layers.BilinearSeqAttn", "layers.BilinearSeqAttn", "NotImplementedError", "layers.LinearSeqAttn", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "opt", ",", "padding_idx", "=", "0", ",", "embedding", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "opt", ",", "padding_idx", ",", "embedding", ")", "\n", "\n", "# RNN document encoder", "\n", "self", ".", "doc_rnn", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "self", ".", "doc_input_size", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "opt", "[", "'doc_layers'", "]", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "dropout_output", "=", "opt", "[", "'dropout_rnn_output'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "concat_layers", "=", "opt", "[", "'concat_rnn_layers'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", ")", "\n", "\n", "# RNN question encoder", "\n", "self", ".", "question_rnn", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "self", ".", "question_input_size", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "opt", "[", "'question_layers'", "]", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "dropout_output", "=", "opt", "[", "'dropout_rnn_output'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "concat_layers", "=", "opt", "[", "'concat_rnn_layers'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", ")", "\n", "\n", "# Output sizes of rnn encoders", "\n", "doc_hidden_size", "=", "2", "*", "opt", "[", "'hidden_size'", "]", "\n", "question_hidden_size", "=", "2", "*", "opt", "[", "'hidden_size'", "]", "\n", "if", "opt", "[", "'concat_rnn_layers'", "]", ":", "\n", "            ", "doc_hidden_size", "*=", "opt", "[", "'doc_layers'", "]", "\n", "question_hidden_size", "*=", "opt", "[", "'question_layers'", "]", "\n", "\n", "# Question merging", "\n", "", "if", "opt", "[", "'question_merge'", "]", "not", "in", "[", "'avg'", ",", "'self_attn'", "]", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'question_merge = %s'", "%", "opt", "[", "'question_merge'", "]", ")", "\n", "", "if", "opt", "[", "'question_merge'", "]", "==", "'self_attn'", ":", "\n", "            ", "self", ".", "self_attn", "=", "layers", ".", "LinearSeqAttn", "(", "question_hidden_size", ")", "\n", "\n", "# Bilinear attention for span start/end", "\n", "", "self", ".", "start_attn", "=", "layers", ".", "BilinearSeqAttn", "(", "\n", "doc_hidden_size", ",", "\n", "question_hidden_size", ",", "\n", ")", "\n", "\n", "if", "opt", "[", "'end_gru'", "]", ":", "\n", "            ", "self", ".", "end_gru", "=", "nn", ".", "GRUCell", "(", "doc_hidden_size", ",", "question_hidden_size", ")", "\n", "", "self", ".", "end_attn", "=", "layers", ".", "BilinearSeqAttn", "(", "\n", "doc_hidden_size", ",", "\n", "question_hidden_size", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RnnDocReader.forward": [[294, 350], ["rnn_reader.RnnDocReader.forward_emb", "rnn_reader.RnnDocReader.doc_rnn", "rnn_reader.RnnDocReader.question_rnn", "layers.weighted_avg", "rnn_reader.RnnDocReader.start_attn", "layers.split_sentences", "layers.split_sentences().select", "layers.combine_sentences", "layers.uniform_weights", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "layers.weighted_avg", "rnn_reader.RnnDocReader.end_gru", "rnn_reader.RnnDocReader.end_attn", "rnn_reader.RnnDocReader.end_attn", "rnn_reader.RnnDocReader.self_attn", "start_scores.exp", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "layers.split_sentences", "layers.split_sentences().select.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RCModelProto.forward_emb", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.weighted_avg", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.split_sentences", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.combine_sentences", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.uniform_weights", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.weighted_avg", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.split_sentences"], ["", "def", "forward", "(", "self", ",", "x1", ",", "x1_f", ",", "x1_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", "=", "None", ",", "x2_char", "=", "None", ",", "logit", "=", "False", ")", ":", "\n", "        ", "\"\"\"Inputs:\n        x1 = document word indices             [batch * len_d]\n        x1_f = document word features indices  [batch * len_d * nfeat]\n        x1_pos = document POS tags             [batch * len_d]\n        x1_ner = document entity tags          [batch * len_d]\n        x1_mask = document padding mask        [batch * len_d]\n        x2 = question word indices             [batch * len_q]\n        x2_mask = question padding mask        [batch * len_q]\n        \"\"\"", "\n", "\n", "# Embed both document and question", "\n", "x1_paired_emb", ",", "x2_paired_emb", ",", "x1_full_emb", ",", "x2_full_emb", ",", "feat_dict", "=", "self", ".", "forward_emb", "(", "x1", ",", "x1_f", ",", "x1_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", ",", "x2_char", ")", "\n", "\n", "if", "self", ".", "opt", "[", "'sentence_level'", "]", ":", "\n", "            ", "x1_mask_backup", "=", "x1_mask", "\n", "x1_full_emb", "=", "layers", ".", "split_sentences", "(", "x1_full_emb", ",", "sent_lens", ")", "\n", "x1_mask", "=", "layers", ".", "split_sentences", "(", "x1_mask", ".", "unsqueeze", "(", "-", "1", ")", ",", "sent_lens", ")", ".", "select", "(", "2", ",", "0", ")", "\n", "# print('after:', x1_full_emb.size())", "\n", "# print('x1_mask:', x1_mask.size())", "\n", "# print(x1_full_emb.data.type())", "\n", "# print(x1_mask.data.type())", "\n", "#     print(sent_lens)", "\n", "#     assert False", "\n", "\n", "# Encode document with RNN", "\n", "", "doc_hiddens", "=", "self", ".", "doc_rnn", "(", "x1_full_emb", ",", "x1_mask", ")", "\n", "\n", "if", "self", ".", "opt", "[", "'sentence_level'", "]", ":", "\n", "            ", "x1_mask", "=", "x1_mask_backup", "\n", "doc_hiddens", "=", "layers", ".", "combine_sentences", "(", "doc_hiddens", ",", "sent_lens", ")", "\n", "\n", "# Encode question with RNN + merge hiddens", "\n", "", "question_hiddens", "=", "self", ".", "question_rnn", "(", "x2_full_emb", ",", "x2_mask", ")", "\n", "if", "self", ".", "opt", "[", "'question_merge'", "]", "==", "'avg'", ":", "\n", "            ", "q_merge_weights", "=", "layers", ".", "uniform_weights", "(", "question_hiddens", ",", "x2_mask", ")", "\n", "", "elif", "self", ".", "opt", "[", "'question_merge'", "]", "==", "'self_attn'", ":", "\n", "            ", "q_merge_weights", "=", "self", ".", "self_attn", "(", "question_hiddens", ",", "x2_mask", ")", "\n", "", "question_hidden", "=", "layers", ".", "weighted_avg", "(", "question_hiddens", ",", "q_merge_weights", ")", "\n", "\n", "# Predict start and end positions", "\n", "start_logits", "=", "self", ".", "start_attn", "(", "doc_hiddens", ",", "question_hidden", ",", "x1_mask", ",", "logit", "=", "True", ")", "\n", "start_scores", "=", "F", ".", "log_softmax", "(", "start_logits", ",", "1", ")", "if", "self", ".", "training", "else", "F", ".", "softmax", "(", "start_logits", ",", "1", ")", "\n", "if", "self", ".", "opt", "[", "'end_gru'", "]", ":", "\n", "            ", "weights", "=", "start_scores", ".", "exp", "(", ")", "if", "self", ".", "training", "else", "start_scores", "\n", "weighted_doc_hidden", "=", "layers", ".", "weighted_avg", "(", "doc_hiddens", ",", "weights", ")", "\n", "question_v_hidden", "=", "self", ".", "end_gru", "(", "weighted_doc_hidden", ",", "question_hidden", ")", "\n", "end_logits", "=", "self", ".", "end_attn", "(", "doc_hiddens", ",", "question_v_hidden", ",", "x1_mask", ",", "logit", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "end_logits", "=", "self", ".", "end_attn", "(", "doc_hiddens", ",", "question_hidden", ",", "x1_mask", ",", "logit", "=", "True", ")", "\n", "\n", "", "if", "logit", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "", "else", ":", "\n", "            ", "end_scores", "=", "F", ".", "log_softmax", "(", "end_logits", ",", "1", ")", "if", "self", ".", "training", "else", "F", ".", "softmax", "(", "end_logits", ",", "1", ")", "\n", "return", "start_scores", ",", "end_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.CnnDocReader.__init__": [[353, 399], ["rnn_reader.RCModelProto.__init__", "layers.DilatedResNet", "layers.DilatedResNet", "layers.BilinearSeqAttn", "layers.BilinearSeqAttn", "NotImplementedError", "layers.LinearSeqAttn", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["    ", "def", "__init__", "(", "self", ",", "opt", ",", "padding_idx", "=", "0", ",", "embedding", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "opt", ",", "padding_idx", ",", "embedding", ")", "\n", "\n", "# RNN document encoder", "\n", "self", ".", "doc_rnn", "=", "layers", ".", "DilatedResNet", "(", "\n", "input_size", "=", "self", ".", "doc_input_size", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "opt", "[", "'doc_layers'", "]", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "dropout_output", "=", "opt", "[", "'dropout_rnn_output'", "]", ",", "\n", "dilation_base", "=", "2", ",", "\n", "dilation_layers", "=", "3", ",", "\n", "dilation_offset", "=", "1", ",", "\n", ")", "\n", "\n", "# RNN question encoder", "\n", "self", ".", "question_rnn", "=", "layers", ".", "DilatedResNet", "(", "\n", "input_size", "=", "self", ".", "question_input_size", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "opt", "[", "'question_layers'", "]", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "dropout_output", "=", "opt", "[", "'dropout_rnn_output'", "]", ",", "\n", "dilation_base", "=", "1", ",", "\n", ")", "\n", "\n", "# Output sizes of rnn encoders", "\n", "doc_hidden_size", "=", "opt", "[", "'hidden_size'", "]", "\n", "question_hidden_size", "=", "opt", "[", "'hidden_size'", "]", "\n", "\n", "# Question merging", "\n", "if", "opt", "[", "'question_merge'", "]", "not", "in", "[", "'avg'", ",", "'self_attn'", "]", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'question_merge = %s'", "%", "opt", "[", "'question_merge'", "]", ")", "\n", "", "if", "opt", "[", "'question_merge'", "]", "==", "'self_attn'", ":", "\n", "            ", "self", ".", "self_attn", "=", "layers", ".", "LinearSeqAttn", "(", "question_hidden_size", ")", "\n", "\n", "# Bilinear attention for span start/end", "\n", "", "self", ".", "start_attn", "=", "layers", ".", "BilinearSeqAttn", "(", "\n", "doc_hidden_size", ",", "\n", "question_hidden_size", ",", "\n", ")", "\n", "\n", "if", "opt", "[", "'end_gru'", "]", ":", "\n", "            ", "self", ".", "end_gru", "=", "nn", ".", "GRUCell", "(", "doc_hidden_size", ",", "question_hidden_size", ")", "\n", "", "self", ".", "end_attn", "=", "layers", ".", "BilinearSeqAttn", "(", "\n", "doc_hidden_size", ",", "\n", "question_hidden_size", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.CnnDocReader.forward": [[401, 448], ["rnn_reader.CnnDocReader.forward_emb", "rnn_reader.CnnDocReader.doc_rnn", "rnn_reader.CnnDocReader.question_rnn", "layers.weighted_avg", "rnn_reader.CnnDocReader.start_attn", "print", "layers.split_sentences", "print", "print", "layers.combine_sentences", "layers.uniform_weights", "layers.weighted_avg", "rnn_reader.CnnDocReader.end_gru", "rnn_reader.CnnDocReader.end_attn", "rnn_reader.CnnDocReader.end_attn", "layers.split_sentences.size", "layers.split_sentences.size", "rnn_reader.CnnDocReader.self_attn", "rnn_reader.CnnDocReader.exp"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RCModelProto.forward_emb", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.weighted_avg", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.split_sentences", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.combine_sentences", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.uniform_weights", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.weighted_avg"], ["", "def", "forward", "(", "self", ",", "x1", ",", "x1_f", ",", "x1_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", "=", "None", ",", "x2_char", "=", "None", ")", ":", "\n", "        ", "\"\"\"Inputs:\n        x1 = document word indices             [batch * len_d]\n        x1_f = document word features indices  [batch * len_d * nfeat]\n        x1_pos = document POS tags             [batch * len_d]\n        x1_ner = document entity tags          [batch * len_d]\n        x1_mask = document padding mask        [batch * len_d]\n        x2 = question word indices             [batch * len_q]\n        x2_mask = question padding mask        [batch * len_q]\n        \"\"\"", "\n", "\n", "# Embed both document and question", "\n", "x1_paired_emb", ",", "x2_paired_emb", ",", "x1_full_emb", ",", "x2_full_emb", ",", "feat_dict", "=", "self", ".", "forward_emb", "(", "x1", ",", "x1_f", ",", "x1_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", ",", "x2_char", ")", "\n", "\n", "if", "self", ".", "opt", "[", "'sentence_level'", "]", ":", "\n", "            ", "x1_mask_backup", "=", "x1_mask", "\n", "print", "(", "'before:'", ",", "x1_full_emb", ".", "size", "(", ")", ")", "\n", "x1_full_emb", "=", "layers", ".", "split_sentences", "(", "x1_full_emb", ",", "sent_lens", ")", "\n", "print", "(", "'after:'", ",", "x1_full_emb", ".", "size", "(", ")", ")", "\n", "print", "(", "sent_lens", ")", "\n", "#     assert False", "\n", "\n", "# Encode document with RNN", "\n", "", "doc_hiddens", "=", "self", ".", "doc_rnn", "(", "x1_full_emb", ",", "x1_mask", ")", "\n", "\n", "if", "self", ".", "opt", "[", "'sentence_level'", "]", ":", "\n", "            ", "x1_mask", "=", "x1_mask_backup", "\n", "doc_hiddens", "=", "layers", ".", "combine_sentences", "(", "doc_hiddens", ",", "sent_lens", ")", "\n", "\n", "# Encode question with RNN + merge hiddens", "\n", "", "question_hiddens", "=", "self", ".", "question_rnn", "(", "x2_full_emb", ",", "x2_mask", ")", "\n", "if", "self", ".", "opt", "[", "'question_merge'", "]", "==", "'avg'", ":", "\n", "            ", "q_merge_weights", "=", "layers", ".", "uniform_weights", "(", "question_hiddens", ",", "x2_mask", ")", "\n", "", "elif", "self", ".", "opt", "[", "'question_merge'", "]", "==", "'self_attn'", ":", "\n", "            ", "q_merge_weights", "=", "self", ".", "self_attn", "(", "question_hiddens", ",", "x2_mask", ")", "\n", "", "question_hidden", "=", "layers", ".", "weighted_avg", "(", "question_hiddens", ",", "q_merge_weights", ")", "\n", "\n", "# Predict start and end positions", "\n", "start_scores", "=", "self", ".", "start_attn", "(", "doc_hiddens", ",", "question_hidden", ",", "x1_mask", ",", "log", "=", "self", ".", "training", ")", "\n", "if", "self", ".", "opt", "[", "'end_gru'", "]", ":", "\n", "            ", "weights", "=", "start_scores", ".", "exp", "(", ")", "if", "self", ".", "training", "else", "start_scores", "\n", "weighted_doc_hidden", "=", "layers", ".", "weighted_avg", "(", "doc_hiddens", ",", "weights", ")", "\n", "question_v_hidden", "=", "self", ".", "end_gru", "(", "weighted_doc_hidden", ",", "question_hidden", ")", "\n", "end_scores", "=", "self", ".", "end_attn", "(", "doc_hiddens", ",", "question_v_hidden", ",", "x1_mask", ",", "log", "=", "self", ".", "training", ")", "\n", "", "else", ":", "\n", "            ", "end_scores", "=", "self", ".", "end_attn", "(", "doc_hiddens", ",", "question_hidden", ",", "x1_mask", ",", "log", "=", "self", ".", "training", ")", "\n", "", "return", "start_scores", ",", "end_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.FusionNet.__init__": [[453, 575], ["rnn_reader.RCModelProto.__init__", "layers.StackedBRNN", "layers.StackedBRNN", "layers.StackedBRNN", "layers.FullAttention", "layers.StackedBRNN", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "range", "layers.BilinearSeqAttn", "layers.BilinearSeqAttn", "rnn_reader.FusionNet.self_boost_fusions.append", "rnn_reader.FusionNet.doc_final_rnns.append", "NotImplementedError", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "NotImplementedError", "layers.LinearSeqAttn", "torch.GRUCell", "torch.GRUCell", "torch.GRUCell", "layers.FullAttention", "layers.StackedBRNN"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "opt", ",", "padding_idx", "=", "0", ",", "embedding", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "opt", ",", "padding_idx", ",", "embedding", ")", "\n", "\n", "# RNN document encoder", "\n", "self", ".", "doc_rnn", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "self", ".", "doc_input_size", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "2", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "# dropout_output=opt['dropout_rnn_output'],", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "concat_layers", "=", "True", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", "residual", "=", "opt", "[", "'residual'", "]", ",", "\n", "squeeze_excitation", "=", "opt", "[", "'squeeze_excitation'", "]", ",", "\n", ")", "\n", "\n", "# RNN question encoder", "\n", "self", ".", "question_rnn", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "self", ".", "question_input_size", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "2", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "# dropout_output=opt['dropout_rnn_output'],", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "concat_layers", "=", "True", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", "residual", "=", "opt", "[", "'residual'", "]", ",", "\n", "squeeze_excitation", "=", "opt", "[", "'squeeze_excitation'", "]", ",", "\n", ")", "\n", "\n", "# Output sizes of rnn encoders", "\n", "doc_hidden_size", "=", "2", "*", "2", "*", "opt", "[", "'hidden_size'", "]", "\n", "question_hidden_size", "=", "doc_hidden_size", "\n", "\n", "self", ".", "question_urnn", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "question_hidden_size", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "opt", "[", "'fusion_understanding_layers'", "]", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", "residual", "=", "opt", "[", "'residual'", "]", ",", "\n", "squeeze_excitation", "=", "opt", "[", "'squeeze_excitation'", "]", ",", "\n", "concat_layers", "=", "False", ",", "\n", ")", "\n", "\n", "self", ".", "multi_level_fusion", "=", "layers", ".", "FullAttention", "(", "\n", "full_size", "=", "self", ".", "paired_input_size", "+", "doc_hidden_size", ",", "\n", "hidden_size", "=", "2", "*", "3", "*", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_level", "=", "3", ",", "\n", "dropout", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", ")", "\n", "\n", "self", ".", "doc_urnn", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "2", "*", "5", "*", "opt", "[", "'hidden_size'", "]", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "opt", "[", "'fusion_understanding_layers'", "]", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", "residual", "=", "opt", "[", "'residual'", "]", ",", "\n", "squeeze_excitation", "=", "opt", "[", "'squeeze_excitation'", "]", ",", "\n", "concat_layers", "=", "False", ",", "\n", ")", "\n", "\n", "\n", "self", ".", "self_boost_fusions", "=", "nn", ".", "ModuleList", "(", ")", "\n", "self", ".", "doc_final_rnns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "full_size", "=", "self", ".", "paired_input_size", "+", "4", "*", "3", "*", "opt", "[", "'hidden_size'", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "opt", "[", "'fusion_self_boost_times'", "]", ")", ":", "\n", "            ", "self", ".", "self_boost_fusions", ".", "append", "(", "layers", ".", "FullAttention", "(", "\n", "full_size", "=", "full_size", ",", "\n", "hidden_size", "=", "2", "*", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_level", "=", "1", ",", "\n", "dropout", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", ")", ")", "\n", "\n", "self", ".", "doc_final_rnns", ".", "append", "(", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "4", "*", "opt", "[", "'hidden_size'", "]", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "opt", "[", "'fusion_final_layers'", "]", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", "residual", "=", "opt", "[", "'residual'", "]", ",", "\n", "squeeze_excitation", "=", "opt", "[", "'squeeze_excitation'", "]", ",", "\n", "concat_layers", "=", "False", ",", "\n", ")", ")", "\n", "full_size", "+=", "2", "*", "opt", "[", "'hidden_size'", "]", "\n", "\n", "# Question merging", "\n", "", "if", "opt", "[", "'question_merge'", "]", "not", "in", "[", "'avg'", ",", "'self_attn'", "]", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'question_merge = %s'", "%", "opt", "[", "'question_merge'", "]", ")", "\n", "", "if", "opt", "[", "'question_merge'", "]", "==", "'self_attn'", ":", "\n", "            ", "self", ".", "quesiton_merge_attns", "=", "nn", ".", "ModuleList", "(", ")", "\n", "\n", "# Question merging", "\n", "", "if", "opt", "[", "'question_merge'", "]", "not", "in", "[", "'avg'", ",", "'self_attn'", "]", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'question_merge = %s'", "%", "opt", "[", "'question_merge'", "]", ")", "\n", "", "if", "opt", "[", "'question_merge'", "]", "==", "'self_attn'", ":", "\n", "            ", "self", ".", "self_attn", "=", "layers", ".", "LinearSeqAttn", "(", "2", "*", "opt", "[", "'hidden_size'", "]", ")", "\n", "\n", "# Bilinear attention for span start/end", "\n", "", "self", ".", "start_attn", "=", "layers", ".", "BilinearSeqAttn", "(", "\n", "2", "*", "opt", "[", "'hidden_size'", "]", ",", "\n", "2", "*", "opt", "[", "'hidden_size'", "]", ",", "\n", ")", "\n", "\n", "if", "opt", "[", "'end_gru'", "]", ":", "\n", "            ", "self", ".", "end_gru", "=", "nn", ".", "GRUCell", "(", "2", "*", "opt", "[", "'hidden_size'", "]", ",", "2", "*", "opt", "[", "'hidden_size'", "]", ")", "\n", "\n", "", "self", ".", "end_attn", "=", "layers", ".", "BilinearSeqAttn", "(", "\n", "2", "*", "opt", "[", "'hidden_size'", "]", ",", "\n", "2", "*", "opt", "[", "'hidden_size'", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.FusionNet.forward": [[577, 654], ["rnn_reader.FusionNet.forward_emb", "rnn_reader.FusionNet.doc_rnn", "rnn_reader.FusionNet.question_rnn", "rnn_reader.FusionNet.question_urnn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_reader.FusionNet.multi_level_fusion", "rnn_reader.FusionNet.doc_urnn", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "layers.weighted_avg", "rnn_reader.FusionNet.start_attn", "layers.dropout", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "layers.uniform_weights", "rnn_reader.FusionNet.forward.dropout"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RCModelProto.forward_emb", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.weighted_avg", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.uniform_weights", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "forward", "(", "self", ",", "x1", ",", "x1_f", ",", "x1_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", "=", "None", ",", "x2_char", "=", "None", ",", "logit", "=", "False", ")", ":", "\n", "        ", "\"\"\"Inputs:\n        x1 = document word indices             [batch * len_d]\n        x1_f = document word features indices  [batch * len_d * nfeat]\n        x1_pos = document POS tags             [batch * len_d]\n        x1_ner = document entity tags          [batch * len_d]\n        x1_mask = document padding mask        [batch * len_d]\n        x2 = question word indices             [batch * len_q]\n        x2_mask = question padding mask        [batch * len_q]\n        \"\"\"", "\n", "\n", "def", "dropout", "(", "x", ",", "p", "=", "self", ".", "opt", "[", "'dropout_rnn'", "]", ")", ":", "\n", "            ", "return", "layers", ".", "dropout", "(", "x", ",", "p", "=", "p", ",", "\n", "training", "=", "self", ".", "training", ",", "variational", "=", "self", ".", "opt", "[", "'variational_dropout'", "]", "and", "x", ".", "dim", "(", ")", "==", "3", ")", "\n", "\n", "# Embed both document and question", "\n", "", "x1_paired_emb", ",", "x2_paired_emb", ",", "x1_full_emb", ",", "x2_full_emb", ",", "feat_dict", "=", "self", ".", "forward_emb", "(", "x1", ",", "x1_f", ",", "x1_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", ",", "x2_char", ")", "\n", "\n", "# Encode document with RNN", "\n", "doc_hiddens", "=", "self", ".", "doc_rnn", "(", "x1_full_emb", ",", "x1_mask", ")", "\n", "# Encode question with RNN", "\n", "question_hiddens", "=", "self", ".", "question_rnn", "(", "x2_full_emb", ",", "x2_mask", ")", "\n", "\n", "# Question Understanding", "\n", "question_u_hiddens", "=", "self", ".", "question_urnn", "(", "question_hiddens", ",", "x2_mask", ")", "\n", "\n", "# Fully-Aware Multi-level Fusion", "\n", "doc_HoW", "=", "torch", ".", "cat", "(", "[", "x1_paired_emb", ",", "doc_hiddens", "]", ",", "2", ")", "\n", "question_HoW", "=", "torch", ".", "cat", "(", "[", "x2_paired_emb", ",", "question_hiddens", "]", ",", "2", ")", "\n", "question_cat_hiddens", "=", "torch", ".", "cat", "(", "[", "question_hiddens", ",", "question_u_hiddens", "]", ",", "2", ")", "\n", "doc_fusions", "=", "self", ".", "multi_level_fusion", "(", "doc_HoW", ",", "question_HoW", ",", "question_cat_hiddens", ",", "x2_mask", ")", "\n", "\n", "# Document Understanding", "\n", "doc_u_hiddens", "=", "self", ".", "doc_urnn", "(", "torch", ".", "cat", "(", "[", "doc_hiddens", ",", "doc_fusions", "]", ",", "2", ")", ",", "x1_mask", ")", "\n", "\n", "# Fully-Aware Self-Boosted Fusion", "\n", "self_boost_HoW", "=", "torch", ".", "cat", "(", "[", "x1_paired_emb", ",", "doc_hiddens", ",", "doc_fusions", ",", "doc_u_hiddens", "]", ",", "2", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "self_boost_fusions", ")", ")", ":", "\n", "            ", "doc_self_fusions", "=", "self", ".", "self_boost_fusions", "[", "i", "]", "(", "self_boost_HoW", ",", "self_boost_HoW", ",", "doc_u_hiddens", ",", "x1_mask", ")", "\n", "\n", "# Final document representation", "\n", "doc_final_hiddens", "=", "self", ".", "doc_final_rnns", "[", "i", "]", "(", "torch", ".", "cat", "(", "[", "doc_u_hiddens", ",", "doc_self_fusions", "]", ",", "2", ")", ",", "x1_mask", ")", "\n", "if", "i", "<", "len", "(", "self", ".", "self_boost_fusions", ")", "-", "1", ":", "\n", "                ", "self_boost_HoW", "=", "torch", ".", "cat", "(", "[", "self_boost_HoW", ",", "doc_final_hiddens", "]", ",", "2", ")", "\n", "doc_u_hiddens", "=", "doc_final_hiddens", "\n", "\n", "# Encode question with RNN + merge hidden, 2s", "\n", "", "", "if", "self", ".", "opt", "[", "'question_merge'", "]", "==", "'avg'", ":", "\n", "            ", "q_merge_weights", "=", "layers", ".", "uniform_weights", "(", "question_u_hiddens", ",", "x2_mask", ")", "\n", "", "elif", "self", ".", "opt", "[", "'question_merge'", "]", "==", "'self_attn'", ":", "\n", "            ", "q_merge_weights", "=", "self", ".", "self_attn", "(", "dropout", "(", "question_u_hiddens", ")", ",", "x2_mask", ")", "\n", "", "question_u_hidden", "=", "layers", ".", "weighted_avg", "(", "question_u_hiddens", ",", "q_merge_weights", ")", "\n", "\n", "# Predict start and end positions", "\n", "start_logits", "=", "self", ".", "start_attn", "(", "dropout", "(", "doc_final_hiddens", ")", ",", "dropout", "(", "question_u_hidden", ")", ",", "x1_mask", ",", "logit", "=", "True", ")", "\n", "if", "self", ".", "opt", "[", "'sentence_level'", "]", ":", "\n", "            ", "start_logits", "=", "layers", ".", "combine_sentences", "(", "start_logits", ",", "sent_lens", ")", "\n", "\n", "", "start_scores", "=", "F", ".", "log_softmax", "(", "start_logits", ",", "1", ")", "if", "self", ".", "training", "else", "F", ".", "softmax", "(", "start_logits", ",", "1", ")", "\n", "if", "self", ".", "opt", "[", "'end_gru'", "]", ":", "\n", "            ", "weights", "=", "start_scores", ".", "exp", "(", ")", "if", "self", ".", "training", "else", "start_scores", "\n", "weighted_doc_hidden", "=", "layers", ".", "weighted_avg", "(", "doc_final_hiddens", ",", "weights", ")", "\n", "question_v_hidden", "=", "self", ".", "end_gru", "(", "dropout", "(", "weighted_doc_hidden", ")", ",", "dropout", "(", "question_u_hidden", ")", ")", "\n", "# question_v_hidden = layers.dropout(question_v_hidden)", "\n", "end_logits", "=", "self", ".", "end_attn", "(", "dropout", "(", "doc_final_hiddens", ")", ",", "dropout", "(", "question_v_hidden", ")", ",", "x1_mask", ",", "logit", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "end_logits", "=", "self", ".", "end_attn", "(", "doc_final_hiddens", ",", "question_u_hidden", ",", "x1_mask", ",", "logit", "=", "True", ")", "\n", "\n", "", "if", "self", ".", "opt", "[", "'sentence_level'", "]", ":", "\n", "            ", "end_logits", "=", "layers", ".", "combine_sentences", "(", "end_logits", ",", "sent_lens", ")", "\n", "\n", "", "if", "logit", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "", "else", ":", "\n", "            ", "end_scores", "=", "F", ".", "log_softmax", "(", "end_logits", ",", "1", ")", "if", "self", ".", "training", "else", "F", ".", "softmax", "(", "end_logits", ",", "1", ")", "\n", "return", "start_scores", ",", "end_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__": [[659, 715], ["rnn_reader.RCModelProto.__init__", "layers.StackedBRNN", "layers.BiAttn", "layers.StackedBRNN", "layers.StackedBRNN", "layers.LinearSeqAttn", "layers.LinearSeqAttn", "layers.StackedBRNN"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.__init__"], ["def", "__init__", "(", "self", ",", "opt", ",", "padding_idx", "=", "0", ",", "embedding", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "opt", ",", "padding_idx", ",", "embedding", ")", "\n", "# Store config", "\n", "\n", "# RNN document encoder", "\n", "self", ".", "doc_enc", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "self", ".", "doc_input_size", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "1", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", ")", "\n", "\n", "# RNN question encoder", "\n", "if", "self", ".", "doc_input_size", "==", "self", ".", "question_input_size", ":", "\n", "            ", "self", ".", "question_enc", "=", "self", ".", "doc_enc", "\n", "", "else", ":", "\n", "            ", "self", ".", "question_enc", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "self", ".", "question_input_size", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "1", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", ")", "\n", "\n", "# Context-Query Attention Layer", "\n", "", "self", ".", "biattn", "=", "layers", ".", "BiAttn", "(", "2", "*", "opt", "[", "'hidden_size'", "]", ")", "\n", "\n", "# Model Encoder Layer", "\n", "self", ".", "model_enc", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "opt", "[", "'hidden_size'", "]", "*", "8", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "2", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", ")", "\n", "\n", "self", ".", "end_enc", "=", "layers", ".", "StackedBRNN", "(", "\n", "input_size", "=", "opt", "[", "'hidden_size'", "]", "*", "14", ",", "\n", "hidden_size", "=", "opt", "[", "'hidden_size'", "]", ",", "\n", "num_layers", "=", "1", ",", "\n", "dropout_rate", "=", "opt", "[", "'dropout_rnn'", "]", ",", "\n", "variational_dropout", "=", "opt", "[", "'variational_dropout'", "]", ",", "\n", "rnn_type", "=", "opt", "[", "'rnn_type'", "]", ",", "\n", "padding", "=", "opt", "[", "'rnn_padding'", "]", ",", "\n", ")", "\n", "\n", "# Bilinear attention for span start/end", "\n", "self", ".", "start_attn", "=", "layers", ".", "LinearSeqAttn", "(", "10", "*", "opt", "[", "'hidden_size'", "]", ")", "\n", "self", ".", "end_attn", "=", "layers", ".", "LinearSeqAttn", "(", "10", "*", "opt", "[", "'hidden_size'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.BiDAF.forward": [[716, 757], ["x1.size", "x1.size", "x2.size", "rnn_reader.BiDAF.forward_emb", "rnn_reader.BiDAF.doc_enc", "rnn_reader.BiDAF.question_enc", "rnn_reader.BiDAF.biattn", "outputs[].expand_as", "rnn_reader.BiDAF.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "rnn_reader.BiDAF.model_enc", "rnn_reader.BiDAF.start_attn", "layers.weighted_avg().unsqueeze_", "rnn_reader.BiDAF.end_enc", "rnn_reader.BiDAF.end_attn", "layers.dropout", "rnn_reader.BiDAF.forward.dropout"], "methods", ["home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.rnn_reader.RCModelProto.forward_emb", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout", "home.repos.pwc.inspect_result.felixgwu_FastFusionNet.qa.layers.dropout"], ["", "def", "forward", "(", "self", ",", "x1", ",", "x1_f", ",", "x1_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", "=", "None", ",", "x2_char", "=", "None", ")", ":", "\n", "        ", "\"\"\"Inputs:\n        x1 = document word indices             [batch * len_d]\n        x1_f = document word features indices  [batch * len_d * nfeat]\n        x1_pos = document POS tags             [batch * len_d]\n        x1_ner = document entity tags          [batch * len_d]\n        x1_mask = document padding mask        [batch * len_d]\n        x2 = question word indices             [batch * len_q]\n        x2_mask = question padding mask        [batch * len_q]\n        \"\"\"", "\n", "\n", "batch_size", "=", "x1", ".", "size", "(", "0", ")", "\n", "x1_len", "=", "x1", ".", "size", "(", "1", ")", "\n", "x2_len", "=", "x2", ".", "size", "(", "1", ")", "\n", "\n", "def", "dropout", "(", "x", ",", "p", "=", "self", ".", "opt", "[", "'dropout_rnn'", "]", ")", ":", "\n", "            ", "return", "layers", ".", "dropout", "(", "x", ",", "p", "=", "p", ",", "\n", "training", "=", "self", ".", "training", ",", "variational", "=", "self", ".", "opt", "[", "'variational_dropout'", "]", "and", "x", ".", "dim", "(", ")", "==", "3", ")", "\n", "\n", "# Embed both document and question", "\n", "", "x1_paired_emb", ",", "x2_paired_emb", ",", "x1_full_emb", ",", "x2_full_emb", ",", "feat_dict", "=", "self", ".", "forward_emb", "(", "x1", ",", "x1_f", ",", "x1_pos", ",", "x1_ner", ",", "x1_mask", ",", "x2", ",", "x2_mask", ",", "sent_lens", ",", "x1_char", ",", "x2_char", ")", "\n", "\n", "# Encode document with RNN", "\n", "doc_hiddens", "=", "self", ".", "doc_enc", "(", "x1_full_emb", ",", "x1_mask", ")", "\n", "# Encode question with RNN", "\n", "question_hiddens", "=", "self", ".", "question_enc", "(", "x2_full_emb", ",", "x2_mask", ")", "\n", "\n", "# Context-Query Attention", "\n", "outputs", "=", "self", ".", "biattn", "(", "doc_hiddens", ",", "question_hiddens", ",", "x1_mask", ",", "x2_mask", ")", "\n", "outputs", "[", "1", "]", "=", "outputs", "[", "1", "]", ".", "expand_as", "(", "outputs", "[", "2", "]", ")", "# Q2C is a vector need to expand ", "\n", "outputs", ".", "append", "(", "doc_hiddens", ")", "\n", "p0", "=", "torch", ".", "cat", "(", "outputs", ",", "2", ")", "\n", "\n", "# Predict start and end positions", "\n", "g1", "=", "self", ".", "model_enc", "(", "p0", ",", "x1_mask", ")", "\n", "start_scores", "=", "self", ".", "start_attn", "(", "dropout", "(", "torch", ".", "cat", "(", "[", "g1", ",", "p0", "]", ",", "2", ")", ")", ",", "x1_mask", ",", "log", "=", "self", ".", "training", ")", "\n", "alpha", "=", "start_scores", ".", "exp", "(", ")", "if", "self", ".", "training", "else", "start_scores", "\n", "a1i", "=", "layers", ".", "weighted_avg", "(", "g1", ",", "alpha", ")", ".", "unsqueeze_", "(", "1", ")", "\n", "g2", "=", "self", ".", "end_enc", "(", "torch", ".", "cat", "(", "[", "p0", ",", "g1", ",", "a1i", ".", "expand_as", "(", "g1", ")", ",", "g1", "*", "a1i", "]", ",", "2", ")", ",", "x1_mask", ")", "\n", "end_scores", "=", "self", ".", "end_attn", "(", "dropout", "(", "torch", ".", "cat", "(", "[", "g2", ",", "p0", "]", ",", "2", ")", ")", ",", "x1_mask", ",", "log", "=", "self", ".", "training", ")", "\n", "return", "start_scores", ",", "end_scores", "\n", "", "", ""]]}