{"home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.validator_site.app.create_app": [[19, 102], ["flask.Flask", "flask.Flask.route", "flask.Flask.route", "flask.Flask.route", "flask.Flask.config.from_pyfile", "flask.Flask.config.from_mapping", "os.makedirs", "flask.render_template", "app.create_app.make_neural_prediction"], "function", ["None"], ["def", "create_app", "(", "test_config", "=", "None", ")", ":", "\n", "# create and configure the app", "\n", "    ", "app", "=", "flask", ".", "Flask", "(", "__name__", ",", "instance_relative_config", "=", "True", ")", "\n", "\n", "if", "test_config", "is", "None", ":", "\n", "# load the instance config, if it exists, when not testing", "\n", "        ", "app", ".", "config", ".", "from_pyfile", "(", "'config.py'", ",", "silent", "=", "True", ")", "\n", "", "else", ":", "\n", "# load the test config if passed in", "\n", "        ", "app", ".", "config", ".", "from_mapping", "(", "test_config", ")", "\n", "\n", "# ensure the instance folder exists", "\n", "", "try", ":", "\n", "        ", "os", ".", "makedirs", "(", "app", ".", "instance_path", ")", "\n", "", "except", "OSError", ":", "\n", "        ", "pass", "\n", "\n", "", "@", "app", ".", "route", "(", "'/'", ")", "\n", "def", "hello", "(", ")", ":", "\n", "        ", "return", "flask", ".", "render_template", "(", "'index.html'", ",", "url_str", "=", "''", ",", "predictions", "=", "{", "}", ")", "\n", "\n", "", "@", "app", ".", "route", "(", "'/validate'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "validate", "(", ")", ":", "\n", "        ", "url", "=", "flask", ".", "request", ".", "form", "[", "\"url\"", "]", "\n", "predictions", "=", "make_neural_prediction", "(", "url", ")", "\n", "return", "flask", ".", "render_template", "(", "'index.html'", ",", "url_str", "=", "url", ",", "predictions", "=", "predictions", ")", "\n", "\n", "", "@", "app", ".", "route", "(", "'/test'", ",", "methods", "=", "[", "'POST'", "]", ")", "\n", "def", "test", "(", ")", ":", "\n", "        ", "url", "=", "\"www.dontgetsick.com\"", "\n", "predictions", "=", "{", "\n", "4", ":", "{", "\n", "'question'", ":", "\"Is it clear what sources of information were used to compile the publication (other than \"", "\n", "\"the author or producer)?\"", ",", "\n", "'answer'", ":", "'No'", ",", "\n", "'sentences'", ":", "[", "]", ",", "\n", "}", ",", "\n", "5", ":", "{", "\n", "'question'", ":", "\"Is it clear when the information used or reported in the publication was produced?\"", ",", "\n", "'answer'", ":", "\"No\"", ",", "\n", "'sentences'", ":", "[", "]", ",", "\n", "}", ",", "\n", "9", ":", "{", "\n", "'question'", ":", "\"Does it describe how each treatment works?\"", ",", "\n", "'answer'", ":", "\"Yes\"", ",", "\n", "'sentences'", ":", "[", "'The treatment works via the ephemeral properties of magic tacos.'", "]", ",", "\n", "}", ",", "\n", "10", ":", "{", "\n", "'question'", ":", "\"Does it describe the benefits of each treatment?\"", ",", "\n", "'answer'", ":", "\"Yes\"", ",", "\n", "'sentences'", ":", "[", "\n", "'The treatment ensures you never get sick.'", ",", "\n", "'The treatment ensures you never have to take a sick day.'", ",", "\n", "]", ",", "\n", "}", ",", "\n", "11", ":", "{", "\n", "'question'", ":", "\"Does it describe the risks of each treatment?\"", ",", "\n", "'answer'", ":", "\"No\"", ",", "\n", "'sentences'", ":", "[", "]", ",", "\n", "}", ",", "\n", "}", "\n", "return", "flask", ".", "render_template", "(", "'index.html'", ",", "url_str", "=", "url", ",", "predictions", "=", "predictions", ")", "\n", "\n", "", "def", "make_neural_prediction", "(", "url", ":", "str", ",", "exp_dir", "=", "DEFAULT_NEURAL_EXP_DIR", ",", "question_fold_map", "=", "None", ",", "to_gpu", "=", "DEFAULT_USE_GPU", ",", "\n", "gpu_index", "=", "0", ")", ":", "\n", "        ", "predictions", "=", "make_prediction", "(", "url", ",", "exp_dir", "=", "exp_dir", ",", "question_fold_map", "=", "question_fold_map", ",", "to_gpu", "=", "to_gpu", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n", "predictions_to_display", "=", "{", "}", "\n", "for", "q", "in", "predictions", ":", "\n", "            ", "predictions_to_display", "[", "q", "]", "=", "{", "}", "\n", "predictions_to_display", "[", "q", "]", "[", "'question'", "]", "=", "\"Q{}: {}\"", ".", "format", "(", "q", ",", "adm", ".", "questions", "[", "q", "]", ")", "\n", "if", "predictions", "[", "q", "]", "[", "'pred_class'", "]", "==", "1", ":", "\n", "                ", "predictions_to_display", "[", "q", "]", "[", "'answer'", "]", "=", "'Yes'", "\n", "predictions_to_display", "[", "q", "]", "[", "'sentences'", "]", "=", "predictions", "[", "q", "]", "[", "'sentences'", "]", "\n", "", "elif", "predictions", "[", "q", "]", "[", "'pred_class'", "]", "==", "0", ":", "\n", "                ", "predictions_to_display", "[", "q", "]", "[", "'answer'", "]", "=", "'No'", "\n", "predictions_to_display", "[", "q", "]", "[", "'sentences'", "]", "=", "[", "]", "\n", "", "else", ":", "\n", "                ", "predictions_to_display", "[", "q", "]", "[", "'answer'", "]", "=", "'Unknown'", "\n", "predictions_to_display", "[", "q", "]", "[", "'sentences'", "]", "=", "[", "]", "\n", "\n", "", "", "return", "predictions_to_display", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.HyperparamConfig.__init__": [[15, 25], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "encoder_dim", ",", "num_layers", ",", "encoder_approach", ",", "attn_method", ",", "p_dropout", ",", "l2_reg", ",", "batch_size", ",", "\n", "num_epochs", ")", ":", "\n", "        ", "self", ".", "sentencoder_dim", "=", "encoder_dim", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "encoder_approach", "=", "encoder_approach", "\n", "self", ".", "attn_method", "=", "attn_method", "\n", "self", ".", "p_dropout", "=", "p_dropout", "\n", "self", ".", "l2_reg", "=", "l2_reg", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "num_epochs", "=", "num_epochs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.HyperparamConfig.__repr__": [[26, 33], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "desc", "=", "\" sentencoder_dim:{}\\n num_layers:{} \\n encoder_approach:{} \\n attn_method:{} \\n p_dropout:{} \\n \"", "\"l2_reg:{} \\n batch_size:{} \\n num_epochs: {}\"", ".", "format", "(", "self", ".", "sentencoder_dim", ",", "self", ".", "num_layers", ",", "\n", "self", ".", "encoder_approach", ",", "self", ".", "attn_method", ",", "\n", "self", ".", "p_dropout", ",", "self", ".", "l2_reg", ",", "self", ".", "batch_size", ",", "\n", "self", ".", "num_epochs", ")", "\n", "return", "desc", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.generate_models_config": [[35, 115], ["None"], "function", ["None"], ["", "", "def", "generate_models_config", "(", "hyperparam_config", ",", "question", ",", "fold_num", ",", "fdtype", ",", "attn_enabled", "=", "True", ")", ":", "\n", "\n", "    ", "if", "(", "hyperparam_config", ".", "encoder_approach", "in", "{", "'[h_f;h_b]'", ",", "'[h_f+h_b]'", "}", ")", ":", "\n", "        ", "bidirection", "=", "True", "\n", "", "else", ":", "\n", "        ", "bidirection", "=", "False", "\n", "\n", "# determine docencoder input dim", "\n", "", "if", "(", "hyperparam_config", ".", "encoder_approach", "==", "'[h_f;h_b]'", ")", ":", "\n", "        ", "docencoder_inputdim", "=", "2", "*", "hyperparam_config", ".", "sentencoder_dim", "\n", "docencoder_hiddendim", "=", "docencoder_inputdim", "//", "4", "\n", "", "else", ":", "\n", "        ", "docencoder_inputdim", "=", "hyperparam_config", ".", "sentencoder_dim", "\n", "docencoder_hiddendim", "=", "docencoder_inputdim", "//", "2", "# for now we divide the input vector dimension by 2", "\n", "\n", "", "bidirectional_concat_flag", "=", "False", "\n", "if", "(", "hyperparam_config", ".", "encoder_approach", "==", "'[h_f;h_b]'", ")", ":", "\n", "        ", "attn_input_dim", "=", "2", "*", "docencoder_hiddendim", "\n", "if", "(", "hyperparam_config", ".", "attn_method", "==", "'additive'", ")", ":", "\n", "            ", "bidirectional_concat_flag", "=", "True", "\n", "", "", "else", ":", "# case of dot, dot_scaled or none", "\n", "        ", "attn_input_dim", "=", "docencoder_hiddendim", "\n", "\n", "", "docscorer_input_dim", "=", "attn_input_dim", "\n", "\n", "# currently generic_config is shared across all models", "\n", "# leaving it as placeholder such that custom generic configs could be passed :)", "\n", "generic_config", "=", "{", "'fdtype'", ":", "fdtype", ",", "\n", "'bidirectional_concat_flag'", ":", "bidirectional_concat_flag", ",", "\n", "'encoder_approach'", ":", "hyperparam_config", ".", "encoder_approach", "}", "\n", "\n", "sentencoder_config", "=", "{", "'input_dim'", ":", "768", ",", "# bert model embedding dimension", "\n", "'hidden_dim'", ":", "hyperparam_config", ".", "sentencoder_dim", ",", "\n", "'num_hiddenlayers'", ":", "hyperparam_config", ".", "num_layers", ",", "\n", "'bidirection'", ":", "bidirection", ",", "\n", "'pdropout'", ":", "hyperparam_config", ".", "p_dropout", ",", "\n", "'rnn_class'", ":", "nn", ".", "GRU", ",", "\n", "'nonlinear_fun'", ":", "torch", ".", "relu", ",", "\n", "'to_gpu'", ":", "True", ",", "\n", "'generic_config'", ":", "generic_config", "}", "\n", "\n", "docencoder_config", "=", "{", "'input_dim'", ":", "docencoder_inputdim", ",", "\n", "'hidden_dim'", ":", "docencoder_hiddendim", ",", "\n", "'num_hiddenlayers'", ":", "hyperparam_config", ".", "num_layers", ",", "\n", "'bidirection'", ":", "bidirection", ",", "\n", "'pdropout'", ":", "hyperparam_config", ".", "p_dropout", ",", "\n", "'rnn_class'", ":", "nn", ".", "GRU", ",", "\n", "'nonlinear_fun'", ":", "torch", ".", "relu", ",", "\n", "'to_gpu'", ":", "True", ",", "\n", "'generic_config'", ":", "generic_config", "}", "\n", "\n", "docscorer_config", "=", "{", "'input_dim'", ":", "docscorer_input_dim", "}", "\n", "if", "attn_enabled", ":", "\n", "        ", "attnmodel_config", "=", "{", "'attn_method'", ":", "hyperparam_config", ".", "attn_method", ",", "\n", "'attn_input_dim'", ":", "attn_input_dim", ",", "\n", "'generic_config'", ":", "generic_config", "}", "\n", "", "else", ":", "\n", "        ", "hyperparam_config", ".", "attn_method", "==", "'none'", "# override attention method to be none", "\n", "attnmodel_config", "=", "{", "}", "\n", "\n", "", "dataloader_config", "=", "{", "'batch_size'", ":", "hyperparam_config", ".", "batch_size", ",", "\n", "'num_workers'", ":", "0", "}", "\n", "\n", "bert_encoder_config", "=", "{", "'bert_train_flag'", ":", "False", ",", "\n", "'bert_all_output'", ":", "False", "}", "\n", "\n", "config", "=", "{", "'dataloader_config'", ":", "dataloader_config", ",", "\n", "'bert_encoder_config'", ":", "bert_encoder_config", ",", "\n", "'sent_encoder_config'", ":", "sentencoder_config", ",", "\n", "'doc_encoder_config'", ":", "docencoder_config", ",", "\n", "'attnmodel_config'", ":", "attnmodel_config", ",", "\n", "'doc_scorer_config'", ":", "docscorer_config", ",", "\n", "'generic_config'", ":", "generic_config", "}", "\n", "\n", "options", "=", "{", "'question'", ":", "question", ",", "\n", "'fold_num'", ":", "fold_num", ",", "\n", "'num_epochs'", ":", "hyperparam_config", ".", "num_epochs", ",", "\n", "'weight_decay'", ":", "hyperparam_config", ".", "l2_reg", "}", "\n", "\n", "return", "config", ",", "options", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.dump_dict_content": [[117, 121], ["os.path.join", "utilities.ReaderWriter.dump_data"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_data"], ["", "def", "dump_dict_content", "(", "dsettype_content_map", ",", "dsettypes", ",", "desc", ",", "wrk_dir", ")", ":", "\n", "    ", "for", "dsettype", "in", "dsettypes", ":", "\n", "        ", "path", "=", "os", ".", "path", ".", "join", "(", "wrk_dir", ",", "'{}_{}.pkl'", ".", "format", "(", "desc", ",", "dsettype", ")", ")", "\n", "ReaderWriter", ".", "dump_data", "(", "dsettype_content_map", "[", "dsettype", "]", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.run_neural_discern": [[123, 408], ["dataset.construct_load_dataloaders", "utilities.get_device", "torch.nn.NLLLoss", "torch.nn.NLLLoss", "options.get", "options.get", "options.get", "options.get", "model.BertEmbedder", "bertencoder_config.get", "model.BertEmbedder.type().to", "model.SentenceEncoder", "len", "model.DocCategScorer", "utilities.create_directory", "utilities.ReaderWriter.dump_data", "utilities.ReaderWriter.dump_data", "os.path.isfile", "range", "run_workflow.dump_dict_content", "os.getpid", "class_weights[].type().to", "torch.tensor().type().to", "torch.tensor().type().to", "options.get", "model.Attention", "model.DocEncoder", "model.DocEncoder_MeanPooling", "list", "list", "m.type().to", "options.get", "torch.optim.Adam", "torch.optim.Adam", "len", "int", "torch.optim.lr_scheduler.CyclicLR", "torch.optim.lr_scheduler.CyclicLR", "utilities.create_directory", "utilities.create_directory", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "utilities.ReaderWriter.read_data", "print", "utilities.plot_loss", "list", "print", "utilities.ReaderWriter.dump_data", "model.BertEmbedder.type", "list", "list", "model.DocCategScorer.parameters", "model.BertEmbedder.parameters", "m.load_state_dict", "numpy.ceil", "os.path.join", "os.path.join", "os.path.join", "print", "enumerate", "epoch_loss_avgbatch[].append", "epoch_loss_avgsamples[].append", "utilities.perfmetric_report", "score_dict.keys", "os.path.join", "class_weights[].type", "torch.tensor().type", "torch.tensor().type", "model.SentenceEncoder.parameters", "model.DocEncoder_MeanPooling.parameters", "torch.load", "torch.load", "m.type", "docs_batch.to.to", "docs_attn_mask.to.to", "docs_sents_len.type().numpy.type().numpy", "docs_labels.type().to.type().to", "os.path.join", "options.get", "m.train", "m.eval", "torch.optim.Adam.zero_grad", "torch.set_grad_enabled", "torch.set_grad_enabled", "docs_batch.to.size", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "all_logprob_scores.extend", "torch.nn.NLLLoss.", "loss_func.item", "len", "len", "run_workflow.build_predictions_df", "os.path.join", "build_predictions_df.to_csv", "torch.tensor", "torch.tensor", "docs_sents_len.type().numpy.type", "docs_labels.type().to.type", "docs_id[].item", "doc_ids.append", "model.SentenceEncoder.", "model.DocEncoder_MeanPooling.", "model.DocCategScorer.", "torch.max", "torch.max", "pred_class.append", "ref_class.append", "logprob_scores.append", "target_class.append", "loss_func.backward", "torch.optim.Adam.step", "torch.optim.lr_scheduler.CyclicLR.step", "loss_func.item", "torch.save", "torch.save", "run_workflow.dump_dict_content", "utilities.ReaderWriter.read_tensor", "print", "model.BertEmbedder.", "os.path.join", "utilities.ReaderWriter.dump_tensor", "docs_len[].item", "pred_classindx.item", "docs_labels[].item", "docs_labels[].unsqueeze", "model.restrict_grad_", "m.state_dict", "os.path.join", "run_workflow.dump_dict_content", "docs_len[].item"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.construct_load_dataloaders", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.dump_dict_content", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.plot_loss", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.perfmetric_report", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.build_predictions_df", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.save", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.save", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.dump_dict_content", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_tensor", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_tensor", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.restrict_grad_", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.dump_dict_content"], ["", "", "def", "run_neural_discern", "(", "data_partition", ",", "dsettypes", ",", "bertmodel", ",", "config", ",", "options", ",", "wrk_dir", ",", "sents_embed_dir", ",", "\n", "state_dict_dir", "=", "None", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "    ", "pid", "=", "\"{}\"", ".", "format", "(", "os", ".", "getpid", "(", ")", ")", "# process id description", "\n", "# get data loader config", "\n", "dataloader_config", "=", "config", "[", "'dataloader_config'", "]", "\n", "cld", "=", "construct_load_dataloaders", "(", "data_partition", ",", "dsettypes", ",", "dataloader_config", ",", "wrk_dir", ")", "\n", "# dictionaries by dsettypes", "\n", "data_loaders", ",", "epoch_loss_avgbatch", ",", "epoch_loss_avgsamples", ",", "score_dict", ",", "class_weights", ",", "flog_out", "=", "cld", "\n", "# print(class_weights)", "\n", "device", "=", "get_device", "(", "to_gpu", ",", "gpu_index", ")", "# gpu device", "\n", "generic_config", "=", "config", "[", "'generic_config'", "]", "\n", "fdtype", "=", "generic_config", "[", "'fdtype'", "]", "\n", "if", "(", "'train'", "in", "class_weights", ")", ":", "\n", "        ", "class_weights", "=", "class_weights", "[", "'train'", "]", ".", "type", "(", "fdtype", ")", ".", "to", "(", "device", ")", "# update class weights to fdtype tensor", "\n", "", "else", ":", "\n", "        ", "class_weights", "=", "torch", ".", "tensor", "(", "[", "1", ",", "1", "]", ")", ".", "type", "(", "fdtype", ")", ".", "to", "(", "device", ")", "# weighting all casess equally", "\n", "\n", "# print(\"class weights\", class_weights)", "\n", "", "loss_func", "=", "torch", ".", "nn", ".", "NLLLoss", "(", "weight", "=", "class_weights", ",", "reduction", "=", "'mean'", ")", "# negative log likelihood loss", "\n", "rgrad_mode", "=", "options", ".", "get", "(", "'restrict_grad_mode'", ")", "# we can add these to options", "\n", "rgrad_limit", "=", "options", ".", "get", "(", "'restrict_grad_limit'", ")", "\n", "\n", "num_epochs", "=", "options", ".", "get", "(", "'num_epochs'", ",", "50", ")", "\n", "question", "=", "options", ".", "get", "(", "'question'", ")", "-", "1", "\n", "fold_num", "=", "options", ".", "get", "(", "'fold_num'", ")", "\n", "\n", "# parse config dict", "\n", "bertencoder_config", "=", "config", "[", "'bert_encoder_config'", "]", "\n", "sentencoder_config", "=", "config", "[", "'sent_encoder_config'", "]", "\n", "docencoder_config", "=", "config", "[", "'doc_encoder_config'", "]", "\n", "attnmodel_config", "=", "config", "[", "'attnmodel_config'", "]", "\n", "docscorer_config", "=", "config", "[", "'doc_scorer_config'", "]", "\n", "\n", "# setup the models", "\n", "# bert model", "\n", "bert_encoder", "=", "BertEmbedder", "(", "bertmodel", ",", "bertencoder_config", ")", "\n", "bert_train_flag", "=", "bertencoder_config", ".", "get", "(", "'bert_train_flag'", ",", "False", ")", "\n", "bert_encoder", ".", "type", "(", "fdtype", ")", ".", "to", "(", "device", ")", "\n", "\n", "# sentence encoder model", "\n", "sent_encoder", "=", "SentenceEncoder", "(", "sentencoder_config", "[", "'input_dim'", "]", ",", "\n", "sentencoder_config", "[", "'hidden_dim'", "]", ",", "\n", "num_hiddenlayers", "=", "sentencoder_config", "[", "'num_hiddenlayers'", "]", ",", "\n", "bidirection", "=", "sentencoder_config", "[", "'bidirection'", "]", ",", "\n", "pdropout", "=", "sentencoder_config", "[", "'pdropout'", "]", ",", "\n", "config", "=", "sentencoder_config", "[", "'generic_config'", "]", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n", "# doc encoder model", "\n", "if", "attnmodel_config", ":", "\n", "\n", "        ", "attn_model", "=", "Attention", "(", "attnmodel_config", "[", "'attn_method'", "]", ",", "\n", "attnmodel_config", "[", "'attn_input_dim'", "]", ",", "\n", "config", "=", "attnmodel_config", "[", "'generic_config'", "]", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n", "doc_encoder", "=", "DocEncoder", "(", "docencoder_config", "[", "'input_dim'", "]", ",", "\n", "docencoder_config", "[", "'hidden_dim'", "]", ",", "\n", "attn_model", ",", "\n", "num_hiddenlayers", "=", "docencoder_config", "[", "'num_hiddenlayers'", "]", ",", "\n", "bidirection", "=", "docencoder_config", "[", "'bidirection'", "]", ",", "\n", "pdropout", "=", "docencoder_config", "[", "'pdropout'", "]", ",", "\n", "config", "=", "docencoder_config", "[", "'generic_config'", "]", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n", "", "else", ":", "# case of no attention model (i.e. mean pooling)", "\n", "        ", "doc_encoder", "=", "DocEncoder_MeanPooling", "(", "docencoder_config", "[", "'input_dim'", "]", ",", "\n", "docencoder_config", "[", "'hidden_dim'", "]", ",", "\n", "num_hiddenlayers", "=", "docencoder_config", "[", "'num_hiddenlayers'", "]", ",", "\n", "bidirection", "=", "docencoder_config", "[", "'bidirection'", "]", ",", "\n", "pdropout", "=", "docencoder_config", "[", "'pdropout'", "]", ",", "\n", "config", "=", "docencoder_config", "[", "'generic_config'", "]", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "# doc category scorer", "\n", "", "num_labels", "=", "len", "(", "class_weights", ")", "\n", "doc_categ_scorer", "=", "DocCategScorer", "(", "docscorer_config", "[", "'input_dim'", "]", ",", "num_labels", ")", "\n", "\n", "# define optimizer and group parameters", "\n", "models_param", "=", "list", "(", "sent_encoder", ".", "parameters", "(", ")", ")", "+", "list", "(", "doc_encoder", ".", "parameters", "(", ")", ")", "+", "list", "(", "doc_categ_scorer", ".", "parameters", "(", ")", ")", "\n", "models", "=", "[", "(", "sent_encoder", ",", "'sent_encoder'", ")", ",", "(", "doc_encoder", ",", "'doc_encoder'", ")", ",", "(", "doc_categ_scorer", ",", "'doc_categ_scorer'", ")", "]", "\n", "if", "(", "bert_train_flag", ")", ":", "\n", "        ", "models_param", "+=", "list", "(", "bert_encoder", ".", "parameters", "(", ")", ")", "\n", "models", "+=", "[", "(", "bert_encoder", ",", "'bert_encoder'", ")", "]", "\n", "\n", "", "if", "(", "state_dict_dir", ")", ":", "# load state dictionary of saved models", "\n", "        ", "for", "m", ",", "m_name", "in", "models", ":", "\n", "            ", "m", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "state_dict_dir", ",", "'{}.pkl'", ".", "format", "(", "m_name", ")", ")", ",", "map_location", "=", "device", ")", ")", "\n", "\n", "# update models fdtype and move to device", "\n", "", "", "for", "m", ",", "m_name", "in", "models", ":", "\n", "        ", "m", ".", "type", "(", "fdtype", ")", ".", "to", "(", "device", ")", "\n", "\n", "", "if", "(", "'train'", "in", "data_loaders", ")", ":", "\n", "        ", "weight_decay", "=", "options", ".", "get", "(", "'weight_decay'", ",", "1e-3", ")", "\n", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "models_param", ",", "weight_decay", "=", "weight_decay", ",", "lr", "=", "1e-3", ")", "\n", "# see paper Cyclical Learning rates for Training Neural Networks for parameters' choice", "\n", "# `https://arxive.org/pdf/1506.01186.pdf`", "\n", "# pytorch version >1.1, scheduler should be called after optimizer", "\n", "# for cyclical lr scheduler, it should be called after each batch update", "\n", "num_iter", "=", "len", "(", "data_loaders", "[", "'train'", "]", ")", "# num_train_samples/batch_size", "\n", "c_step_size", "=", "int", "(", "np", ".", "ceil", "(", "5", "*", "num_iter", ")", ")", "# this should be 2-10 times num_iter", "\n", "base_lr", "=", "3e-4", "\n", "max_lr", "=", "5", "*", "base_lr", "# 3-5 times base_lr", "\n", "cyc_scheduler", "=", "torch", ".", "optim", ".", "lr_scheduler", ".", "CyclicLR", "(", "optimizer", ",", "base_lr", ",", "max_lr", ",", "step_size_up", "=", "c_step_size", ",", "\n", "mode", "=", "'triangular'", ",", "cycle_momentum", "=", "False", ")", "\n", "\n", "# store attention weights for validation and test set", "\n", "", "if", "attnmodel_config", ":", "\n", "        ", "docid_attnweights_map", "=", "{", "dsettype", ":", "{", "}", "for", "dsettype", "in", "data_loaders", "if", "dsettype", "in", "{", "'validation'", ",", "'test'", "}", "}", "\n", "", "else", ":", "\n", "        ", "docid_attnweights_map", "=", "{", "}", "\n", "\n", "", "if", "(", "'validation'", "in", "data_loaders", ")", ":", "\n", "        ", "m_state_dict_dir", "=", "create_directory", "(", "os", ".", "path", ".", "join", "(", "wrk_dir", ",", "'model_statedict'", ")", ")", "\n", "\n", "", "if", "(", "num_epochs", ">", "1", ")", ":", "\n", "        ", "fig_dir", "=", "create_directory", "(", "os", ".", "path", ".", "join", "(", "wrk_dir", ",", "'figures'", ")", ")", "\n", "\n", "# dump config dictionaries on disk", "\n", "", "config_dir", "=", "create_directory", "(", "os", ".", "path", ".", "join", "(", "wrk_dir", ",", "'config'", ")", ")", "\n", "ReaderWriter", ".", "dump_data", "(", "config", ",", "os", ".", "path", ".", "join", "(", "config_dir", ",", "'mconfig.pkl'", ")", ")", "\n", "ReaderWriter", ".", "dump_data", "(", "options", ",", "os", ".", "path", ".", "join", "(", "config_dir", ",", "'exp_options.pkl'", ")", ")", "\n", "\n", "if", "(", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "sents_embed_dir", ",", "'bert_proc_docs.pkl'", ")", ")", ")", ":", "\n", "        ", "bert_proc_docs", "=", "ReaderWriter", ".", "read_data", "(", "os", ".", "path", ".", "join", "(", "sents_embed_dir", ",", "'bert_proc_docs.pkl'", ")", ")", "\n", "dump_embed_dict_flag", "=", "False", "\n", "", "else", ":", "\n", "        ", "print", "(", "'===NOT=== READING BERT EMBEDS'", ")", "\n", "bert_proc_docs", "=", "{", "}", "\n", "dump_embed_dict_flag", "=", "True", "\n", "\n", "# print(dump_embed_dict_flag)", "\n", "\n", "", "for", "epoch", "in", "range", "(", "num_epochs", ")", ":", "\n", "# print(\"-\"*35)", "\n", "        ", "for", "dsettype", "in", "dsettypes", ":", "\n", "            ", "print", "(", "\"device: {} | question: {} | fold_num: {} | epoch: {} | dsettype: {} | pid: {}\"", "\n", "\"\"", ".", "format", "(", "device", ",", "options", ".", "get", "(", "'question'", ")", ",", "fold_num", ",", "epoch", ",", "dsettype", ",", "pid", ")", ")", "\n", "pred_class", "=", "[", "]", "\n", "ref_class", "=", "[", "]", "\n", "doc_ids", "=", "[", "]", "\n", "all_logprob_scores", "=", "[", "]", "\n", "\n", "data_loader", "=", "data_loaders", "[", "dsettype", "]", "\n", "# total_num_samples = len(data_loader.dataset)", "\n", "epoch_loss", "=", "0.", "\n", "epoch_loss_deavrg", "=", "0.", "\n", "\n", "if", "(", "dsettype", "==", "'train'", ")", ":", "# should be only for train", "\n", "                ", "for", "m", ",", "m_name", "in", "models", ":", "\n", "                    ", "m", ".", "train", "(", ")", "\n", "", "", "else", ":", "\n", "                ", "for", "m", ",", "m_name", "in", "models", ":", "\n", "                    ", "m", ".", "eval", "(", ")", "\n", "\n", "", "", "sample_counter", "=", "0", "\n", "for", "i_batch", ",", "samples_batch", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "# print('batch num:', i_batch)", "\n", "\n", "                ", "logprob_scores", "=", "[", "]", "\n", "target_class", "=", "[", "]", "\n", "# zero model grad", "\n", "if", "(", "dsettype", "==", "'train'", ")", ":", "\n", "                    ", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "", "docs_batch", ",", "docs_len", ",", "docs_sents_len", ",", "docs_attn_mask", ",", "docs_labels", ",", "docs_id", "=", "samples_batch", "\n", "\n", "docs_batch", "=", "docs_batch", ".", "to", "(", "device", ")", "\n", "docs_attn_mask", "=", "docs_attn_mask", ".", "to", "(", "device", ")", "\n", "docs_sents_len", "=", "docs_sents_len", ".", "type", "(", "torch", ".", "int64", ")", ".", "numpy", "(", ")", "# to feed this in RNN", "\n", "docs_labels", "=", "docs_labels", ".", "type", "(", "torch", ".", "int64", ")", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "set_grad_enabled", "(", "dsettype", "==", "'train'", ")", ":", "\n", "# print(\"number of examples in batch:\", docs_batch.size(0))", "\n", "                    ", "num_docs_perbatch", "=", "docs_batch", ".", "size", "(", "0", ")", "\n", "for", "doc_indx", "in", "range", "(", "num_docs_perbatch", ")", ":", "\n", "# print('doc_indx', doc_indx)", "\n", "                        ", "doc_id", "=", "docs_id", "[", "doc_indx", "]", ".", "item", "(", ")", "\n", "doc_ids", ".", "append", "(", "doc_id", ")", "\n", "if", "(", "doc_id", "in", "bert_proc_docs", ")", ":", "\n", "# due to GPU limit", "\n", "                            ", "embed_sents", "=", "ReaderWriter", ".", "read_tensor", "(", "bert_proc_docs", "[", "doc_id", "]", ",", "device", "=", "device", ")", "\n", "# embed_sents = embed_sents.to(device)  # send to gpu device", "\n", "", "else", ":", "\n", "                            ", "print", "(", "'===NOT=== READING BERT EMBEDS 2'", ")", "\n", "embed_sents", "=", "bert_encoder", "(", "docs_batch", "[", "doc_indx", "]", ",", "docs_attn_mask", "[", "doc_indx", "]", ",", "\n", "docs_len", "[", "doc_indx", "]", ".", "item", "(", ")", ")", "\n", "# add embedding to dict", "\n", "embed_fpath", "=", "os", ".", "path", ".", "join", "(", "sents_embed_dir", ",", "'{}.pkl'", ".", "format", "(", "doc_id", ")", ")", "\n", "ReaderWriter", ".", "dump_tensor", "(", "embed_sents", ",", "embed_fpath", ")", "\n", "bert_proc_docs", "[", "doc_id", "]", "=", "embed_fpath", "\n", "\n", "", "sents_rnn_hidden", "=", "sent_encoder", "(", "embed_sents", ",", "docs_sents_len", "[", "doc_indx", "]", ",", "\n", "docs_len", "[", "doc_indx", "]", ".", "item", "(", ")", ")", "\n", "\n", "# # remove the embedding from GPU", "\n", "# bert_proc_docs[doc_id].to(cpu_device)", "\n", "# print('sents_rnn_hidden', sents_rnn_hidden.shape)", "\n", "enc_sents", "=", "sents_rnn_hidden", "\n", "# print('enc_sents', enc_sents.shape)", "\n", "doc_out", ",", "doc_attn_weights", "=", "doc_encoder", "(", "enc_sents", ")", "\n", "# print('doc_out', doc_out.shape)", "\n", "# print('doc_attn_weights', doc_attn_weights.shape)", "\n", "# tracking attention weight for validation and test examples", "\n", "if", "(", "dsettype", "in", "docid_attnweights_map", ")", ":", "\n", "                            ", "docid_attnweights_map", "[", "dsettype", "]", "[", "doc_id", "]", "=", "doc_attn_weights", "\n", "\n", "", "logsoftmax_scores", "=", "doc_categ_scorer", "(", "doc_out", ")", "\n", "__", ",", "pred_classindx", "=", "torch", ".", "max", "(", "logsoftmax_scores", ",", "1", ")", "# apply max on row level", "\n", "\n", "# print('logsoftmax_scores', logsoftmax_scores.shape)", "\n", "# print('pred_calssindx', pred_classindx.shape)", "\n", "# print('ref labels', docs_labels[doc_indx, question].unsqueeze(0).shape)", "\n", "# print('predicted class index:', pred_classindx.item())", "\n", "# print('ref class index:', docs_labels[doc_indx, question].item())", "\n", "pred_class", ".", "append", "(", "pred_classindx", ".", "item", "(", ")", ")", "\n", "ref_class", ".", "append", "(", "docs_labels", "[", "doc_indx", ",", "question", "]", ".", "item", "(", ")", ")", "\n", "\n", "logprob_scores", ".", "append", "(", "logsoftmax_scores", ")", "\n", "target_class", ".", "append", "(", "docs_labels", "[", "doc_indx", ",", "question", "]", ".", "unsqueeze", "(", "0", ")", ")", "\n", "sample_counter", "+=", "1", "\n", "\n", "# finished processing docs in batch", "\n", "", "b_logprob_scores", "=", "torch", ".", "cat", "(", "logprob_scores", ",", "dim", "=", "0", ")", "\n", "b_target_class", "=", "torch", ".", "cat", "(", "target_class", ",", "dim", "=", "0", ")", "\n", "all_logprob_scores", ".", "extend", "(", "logprob_scores", ")", "\n", "\n", "# print(\"b_logprob_scores\", b_logprob_scores.shape)", "\n", "# print(\"b_target_class\", b_target_class.shape)", "\n", "loss", "=", "loss_func", "(", "b_logprob_scores", ",", "b_target_class", ")", "\n", "if", "(", "dsettype", "==", "'train'", ")", ":", "\n", "# print(\"computing loss\")", "\n", "# backward step (i.e. compute gradients)", "\n", "                        ", "loss", ".", "backward", "(", ")", "\n", "# apply grad clipping", "\n", "if", "(", "rgrad_mode", ")", ":", "\n", "                            ", "restrict_grad_", "(", "models_param", ",", "rgrad_mode", ",", "rgrad_limit", ")", "\n", "# optimzer step -- update weights", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "# after each batch step the scheduler", "\n", "cyc_scheduler", ".", "step", "(", ")", "\n", "", "epoch_loss", "+=", "loss", ".", "item", "(", ")", "\n", "# deaverage the loss to deal with last batch with unequal size", "\n", "epoch_loss_deavrg", "+=", "loss", ".", "item", "(", ")", "*", "num_docs_perbatch", "\n", "\n", "# do some cleaning -- get more GPU ;)", "\n", "del", "docs_batch", ",", "docs_len", ",", "docs_sents_len", ",", "docs_attn_mask", ",", "docs_labels", ",", "docs_id", "\n", "# torch.cuda.ipc_collect()", "\n", "# torch.cuda.empty_cache()", "\n", "# end of epoch", "\n", "# print(\"+\"*35)", "\n", "", "", "epoch_loss_avgbatch", "[", "dsettype", "]", ".", "append", "(", "epoch_loss", "/", "len", "(", "data_loader", ")", ")", "\n", "epoch_loss_avgsamples", "[", "dsettype", "]", ".", "append", "(", "epoch_loss_deavrg", "/", "len", "(", "data_loader", ".", "dataset", ")", ")", "\n", "\n", "modelscore", "=", "perfmetric_report", "(", "pred_class", ",", "ref_class", ",", "epoch", "+", "1", ",", "flog_out", "[", "dsettype", "]", ")", "\n", "perf", "=", "modelscore", ".", "macro_f1", "\n", "if", "(", "perf", ">", "score_dict", "[", "dsettype", "]", ".", "macro_f1", ")", ":", "\n", "                ", "score_dict", "[", "dsettype", "]", "=", "modelscore", "\n", "if", "(", "dsettype", "==", "'validation'", ")", ":", "\n", "                    ", "for", "m", ",", "m_name", "in", "models", ":", "\n", "                        ", "torch", ".", "save", "(", "m", ".", "state_dict", "(", ")", ",", "os", ".", "path", ".", "join", "(", "m_state_dict_dir", ",", "'{}.pkl'", ".", "format", "(", "m_name", ")", ")", ")", "\n", "# dump attention weights for the validation data for the best peforming model", "\n", "", "if", "docid_attnweights_map", ":", "\n", "                        ", "dump_dict_content", "(", "docid_attnweights_map", ",", "[", "'validation'", "]", ",", "'docid_attnw_map'", ",", "wrk_dir", ")", "\n", "", "", "elif", "(", "dsettype", "==", "'test'", ")", ":", "\n", "# dump attention weights for the validation data", "\n", "                    ", "if", "docid_attnweights_map", ":", "\n", "                        ", "dump_dict_content", "(", "docid_attnweights_map", ",", "[", "'test'", "]", ",", "'docid_attnw_map'", ",", "wrk_dir", ")", "\n", "# save predictions", "\n", "", "", "if", "dsettype", "in", "{", "'test'", ",", "'validation'", "}", ":", "\n", "                    ", "predictions_df", "=", "build_predictions_df", "(", "doc_ids", ",", "ref_class", ",", "pred_class", ",", "all_logprob_scores", ")", "\n", "predictions_path", "=", "os", ".", "path", ".", "join", "(", "wrk_dir", ",", "'predictions.csv'", ")", "\n", "predictions_df", ".", "to_csv", "(", "predictions_path", ")", "\n", "\n", "", "", "", "", "if", "(", "num_epochs", ">", "1", ")", ":", "\n", "        ", "plot_loss", "(", "epoch_loss_avgbatch", ",", "epoch_loss_avgsamples", ",", "fig_dir", ")", "\n", "\n", "# dump_scores", "\n", "", "dump_dict_content", "(", "score_dict", ",", "list", "(", "score_dict", ".", "keys", "(", ")", ")", ",", "'score'", ",", "wrk_dir", ")", "\n", "if", "(", "dump_embed_dict_flag", ")", ":", "\n", "        ", "print", "(", "bert_proc_docs", ")", "\n", "ReaderWriter", ".", "dump_data", "(", "bert_proc_docs", ",", "os", ".", "path", ".", "join", "(", "sents_embed_dir", ",", "'bert_proc_docs.pkl'", ")", ")", "\n", "\n", "", "return", "pred_class", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.build_predictions_df": [[410, 423], ["pandas.DataFrame", "pd.DataFrame.set_index", "t.tolist", "t.tolist"], "function", ["None"], ["", "def", "build_predictions_df", "(", "ids", ",", "true_class", ",", "pred_class", ",", "logprob_scores", ")", ":", "\n", "    ", "class_0_score", "=", "[", "t", ".", "tolist", "(", ")", "[", "0", "]", "[", "0", "]", "for", "t", "in", "logprob_scores", "]", "\n", "class_1_score", "=", "[", "t", ".", "tolist", "(", ")", "[", "0", "]", "[", "1", "]", "for", "t", "in", "logprob_scores", "]", "\n", "df_dict", "=", "{", "\n", "'id'", ":", "ids", ",", "\n", "'true_class'", ":", "true_class", ",", "\n", "'pred_class'", ":", "pred_class", ",", "\n", "'logprob_score_class0'", ":", "class_0_score", ",", "\n", "'logprob_score_class1'", ":", "class_1_score", ",", "\n", "}", "\n", "predictions_df", "=", "pd", ".", "DataFrame", "(", "df_dict", ")", "\n", "predictions_df", ".", "set_index", "(", "'id'", ")", "\n", "return", "predictions_df", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.highlight_attnw_over_sents": [[425, 437], ["torch.topk", "torch.topk", "print", "print", "range", "print", "attnw.size", "max_indx.size", "[].item", "print", "print", "attnw.size", "[].item"], "function", ["None"], ["", "def", "highlight_attnw_over_sents", "(", "docid_attnweights_map", ",", "proc_articles_repr", ",", "topk", "=", "5", ")", ":", "\n", "    ", "for", "docid", "in", "docid_attnweights_map", ":", "\n", "        ", "attnw", "=", "docid_attnweights_map", "[", "docid", "]", "\n", "topk", "=", "topk", "if", "attnw", ".", "size", "(", "-", "1", ")", ">", "topk", "else", "attnw", ".", "size", "(", "-", "1", ")", "# get top", "\n", "max_val", ",", "max_indx", "=", "torch", ".", "topk", "(", "attnw", ",", "topk", ",", "dim", "=", "1", ")", "\n", "print", "(", "docid", ")", "\n", "print", "(", "\"attended sent:\"", ")", "\n", "for", "i", "in", "range", "(", "max_indx", ".", "size", "(", "-", "1", ")", ")", ":", "\n", "            ", "target_indx", "=", "max_indx", "[", "0", "]", "[", "i", "]", ".", "item", "(", ")", "\n", "print", "(", "\"sentence num:\"", ",", "target_indx", ",", "\"attnw:\"", ",", "max_val", "[", "0", "]", "[", "i", "]", ".", "item", "(", ")", ")", "\n", "print", "(", "proc_articles_repr", "[", "docid", "]", "[", "'sents_tok'", "]", "[", "target_indx", "]", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.return_attnw_over_sents": [[439, 453], ["print", "print", "torch.topk", "torch.topk", "range", "attnw.size", "max_indx.size", "[].item", "attended_sents[].append", "attnw.size", "[].item"], "function", ["None"], ["", "", "def", "return_attnw_over_sents", "(", "docid_attnweights_map", ",", "proc_articles_repr", ",", "topk", "=", "5", ")", ":", "\n", "    ", "attended_sents", "=", "{", "}", "\n", "for", "docid", "in", "docid_attnweights_map", ":", "\n", "        ", "attended_sents", "[", "docid", "]", "=", "[", "]", "\n", "attnw", "=", "docid_attnweights_map", "[", "docid", "]", "\n", "print", "(", "'docid_attnweights_map: {}'", ".", "format", "(", "docid_attnweights_map", ")", ")", "\n", "print", "(", "'attnw: {}'", ".", "format", "(", "attnw", ")", ")", "\n", "topk", "=", "topk", "if", "attnw", ".", "size", "(", "-", "1", ")", ">", "topk", "else", "attnw", ".", "size", "(", "-", "1", ")", "# get top", "\n", "max_val", ",", "max_indx", "=", "torch", ".", "topk", "(", "attnw", ",", "topk", ",", "dim", "=", "1", ")", "\n", "for", "i", "in", "range", "(", "max_indx", ".", "size", "(", "-", "1", ")", ")", ":", "\n", "            ", "target_indx", "=", "max_indx", "[", "0", "]", "[", "i", "]", ".", "item", "(", ")", "\n", "sentence", "=", "proc_articles_repr", "[", "docid", "]", "[", "'sents'", "]", "[", "target_indx", "]", "\n", "attended_sents", "[", "docid", "]", ".", "append", "(", "{", "'sentence'", ":", "sentence", ",", "'weight'", ":", "max_val", "[", "0", "]", "[", "i", "]", ".", "item", "(", ")", "}", ")", "\n", "", "", "return", "attended_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.validate_doc_attnw": [[455, 463], ["print", "print", "print", "torch.allclose", "torch.allclose", "attnw.sum().item", "[].item", "attnw.sum", "torch.tensor", "torch.tensor", "attnw.sum", "utilities.get_device", "torch.max", "torch.max"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device"], ["", "def", "validate_doc_attnw", "(", "docid_attnweights_map", ")", ":", "\n", "    ", "for", "docid", "in", "docid_attnweights_map", ":", "\n", "        ", "attnw", "=", "docid_attnweights_map", "[", "docid", "]", "\n", "print", "(", "docid", ",", "'sum:'", ",", "attnw", ".", "sum", "(", "dim", "=", "1", ")", ".", "item", "(", ")", ")", "\n", "print", "(", "docid", ",", "'max:'", ",", "torch", ".", "max", "(", "attnw", ",", "dim", "=", "1", ")", "[", "0", "]", ".", "item", "(", ")", ")", "\n", "# the sum of attention weights should be close to 1)", "\n", "assert", "torch", ".", "allclose", "(", "attnw", ".", "sum", "(", "dim", "=", "1", ")", ",", "torch", ".", "tensor", "(", "[", "1.0", "]", ",", "device", "=", "get_device", "(", "True", ")", ")", ")", "\n", "", "print", "(", "'passed validation test!!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.generate_hyperparam_space": [[465, 478], ["list", "itertools.product"], "function", ["None"], ["", "def", "generate_hyperparam_space", "(", ")", ":", "\n", "    ", "encoder_dim_vals", "=", "[", "128", ",", "256", ",", "512", "]", "# can drop 512 if gpu crashes", "\n", "l2_reg_vals", "=", "[", ".1", ",", ".01", ",", ".001", "]", "\n", "encoder_approach_vals", "=", "[", "'[h_f;h_b]'", ",", "'[h_f+h_b]'", "]", "# '[h_f]',", "\n", "num_layers_vals", "=", "[", "1", ",", "2", ",", "3", "]", "\n", "batch_size_vals", "=", "[", "4", ",", "8", ",", "16", "]", "\n", "dropout_vals", "=", "[", "0.1", ",", "0.3", ",", "0.4", "]", "\n", "attn_method_vals", "=", "[", "'additive'", ",", "'dot_scaled'", "]", "\n", "num_epochs_vals", "=", "[", "25", "]", "\n", "hyperparam_space", "=", "list", "(", "itertools", ".", "product", "(", "*", "[", "encoder_dim_vals", ",", "num_layers_vals", ",", "encoder_approach_vals", ",", "\n", "attn_method_vals", ",", "dropout_vals", ",", "l2_reg_vals", ",", "batch_size_vals", ",", "\n", "num_epochs_vals", "]", ")", ")", "\n", "return", "hyperparam_space", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.compute_numtrials": [[480, 492], ["numpy.log", "numpy.log", "int", "numpy.ceil"], "function", ["None"], ["", "def", "compute_numtrials", "(", "prob_interval_truemax", ",", "prob_estim", ")", ":", "\n", "    ", "\"\"\" computes number of trials needed for random hyperparameter search\n        see `algorithms for hyperparameter optimization paper\n        <https://papers.nips.cc/paper/4443-algorithms-for-hyper-parameter-optimization.pdf>`__\n\n        Args:\n            prob_interval_truemax: float, probability interval of the true optimal hyperparam,\n                i.e. within 5% expressed as .05\n            prob_estim: float, probability/confidence level, i.e. 95% expressed as .95\n    \"\"\"", "\n", "n", "=", "np", ".", "log", "(", "1", "-", "prob_estim", ")", "/", "np", ".", "log", "(", "1", "-", "prob_interval_truemax", ")", "\n", "return", "(", "int", "(", "np", ".", "ceil", "(", "n", ")", ")", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_hyperparam_options": [[494, 503], ["numpy.random.seed", "run_workflow.compute_numtrials", "run_workflow.generate_hyperparam_space", "numpy.random.choice", "len", "len", "len", "run_workflow.HyperparamConfig"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.compute_numtrials", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.generate_hyperparam_space"], ["", "def", "get_hyperparam_options", "(", "prob_interval_truemax", ",", "prob_estim", ",", "random_seed", "=", "42", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "random_seed", ")", "\n", "num_trials", "=", "compute_numtrials", "(", "prob_interval_truemax", ",", "prob_estim", ")", "\n", "hyperparam_space", "=", "generate_hyperparam_space", "(", ")", "\n", "if", "(", "num_trials", ">", "len", "(", "hyperparam_space", ")", ")", ":", "\n", "        ", "num_trials", "=", "len", "(", "hyperparam_space", ")", "\n", "", "indxs", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "hyperparam_space", ")", ",", "size", "=", "num_trials", ",", "replace", "=", "False", ")", "\n", "# encoder_dim, num_layers, encoder_approach, attn_method, p_dropout, l2_reg, batch_size, num_epochs", "\n", "return", "[", "HyperparamConfig", "(", "*", "hyperparam_space", "[", "indx", "]", ")", "for", "indx", "in", "indxs", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_random_question_fold_per_hyperparam_exp": [[505, 513], ["numpy.random.seed", "numpy.random.randint"], "function", ["None"], ["", "def", "get_random_question_fold_per_hyperparam_exp", "(", "questions", ",", "random_seed", "=", "42", ")", ":", "\n", "    ", "\"\"\"Get for each question the fold number to use for identifying optimal hyperparams\n    \"\"\"", "\n", "np", ".", "random", ".", "seed", "(", "random_seed", ")", "\n", "q_fold", "=", "{", "}", "\n", "for", "q", "in", "questions", ":", "\n", "        ", "q_fold", "[", "q", "]", "=", "np", ".", "random", ".", "randint", "(", "5", ")", "\n", "", "return", "q_fold", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.hyperparam_model_search": [[515, 533], ["list", "run_workflow.get_random_question_fold_per_hyperparam_exp", "get_random_question_fold_per_hyperparam_exp.items", "q_docpartitions.keys", "run_workflow.get_hyperparam_options", "enumerate", "run_workflow.generate_models_config", "print", "os.path.join", "utilities.create_directory", "run_workflow.run_neural_discern"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_random_question_fold_per_hyperparam_exp", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_hyperparam_options", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.generate_models_config", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.run_neural_discern"], ["", "def", "hyperparam_model_search", "(", "q_docpartitions", ",", "bertmodel", ",", "sents_embed_dir", ",", "root_dir", ",", "fdtype", "=", "torch", ".", "float32", ",", "num_epochs", "=", "15", ",", "\n", "prob_interval_truemax", "=", "0.05", ",", "prob_estim", "=", "0.95", ",", "random_seed", "=", "42", ",", "attention", "=", "True", ")", ":", "\n", "    ", "questions", "=", "list", "(", "q_docpartitions", ".", "keys", "(", ")", ")", "\n", "# questions = [4]  # TODO: update this", "\n", "q_fold_map", "=", "get_random_question_fold_per_hyperparam_exp", "(", "questions", ",", "random_seed", "=", "random_seed", ")", "\n", "dsettypes", "=", "[", "'train'", ",", "'validation'", "]", "\n", "for", "q", ",", "fold_num", "in", "q_fold_map", ".", "items", "(", ")", ":", "\n", "# get list of hyperparam configs", "\n", "        ", "hyperparam_options", "=", "get_hyperparam_options", "(", "prob_interval_truemax", ",", "prob_estim", ")", "\n", "data_partition", "=", "q_docpartitions", "[", "q", "]", "[", "fold_num", "]", "\n", "# encoder_dim, num_layers, encoder_approach, attn_method, p_dropout, l2_reg, batch_size, num_epochs", "\n", "for", "counter", ",", "hyperparam_config", "in", "enumerate", "(", "hyperparam_options", ")", ":", "\n", "            ", "mconfig", ",", "options", "=", "generate_models_config", "(", "hyperparam_config", ",", "q", ",", "fold_num", ",", "fdtype", ",", "attention", ")", "\n", "print", "(", "\"Running q{} hyperparam search #{}\"", ".", "format", "(", "q", ",", "counter", ")", ")", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "root_dir", ",", "'question_{}'", ".", "format", "(", "q", ")", ",", "'fold_{}'", ".", "format", "(", "fold_num", ")", ",", "\n", "'config_{}'", ".", "format", "(", "counter", ")", ")", "\n", "wrk_dir", "=", "create_directory", "(", "path", ")", "\n", "run_neural_discern", "(", "data_partition", ",", "dsettypes", ",", "bertmodel", ",", "mconfig", ",", "options", ",", "wrk_dir", ",", "sents_embed_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.run_one_questions_hyperparam_search": [[535, 549], ["run_workflow.get_hyperparam_options", "enumerate", "run_workflow.generate_models_config", "print", "utilities.create_directory", "run_workflow.run_neural_discern", "os.path.join"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_hyperparam_options", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.generate_models_config", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.run_neural_discern"], ["", "", "", "def", "run_one_questions_hyperparam_search", "(", "queue", ",", "q", ",", "fold_num", ",", "data_partition", ",", "bertmodel", ",", "sents_embed_dir", ",", "root_dir", ",", "\n", "gpu_index", ",", "fdtype", "=", "torch", ".", "float32", ",", "num_epochs", "=", "15", ",", "prob_interval_truemax", "=", "0.05", ",", "\n", "prob_estim", "=", "0.95", ",", "random_seed", "=", "42", ",", "attention", "=", "True", ")", ":", "\n", "# get list of hyperparam configs", "\n", "    ", "hyperparam_options", "=", "get_hyperparam_options", "(", "prob_interval_truemax", ",", "prob_estim", ")", "\n", "dsettypes", "=", "[", "'train'", ",", "'validation'", "]", "\n", "# encoder_dim, num_layers, encoder_approach, attn_method, p_dropout, l2_reg, batch_size, num_epochs", "\n", "for", "counter", ",", "hyperparam_config", "in", "enumerate", "(", "hyperparam_options", ")", ":", "\n", "        ", "mconfig", ",", "options", "=", "generate_models_config", "(", "hyperparam_config", ",", "q", ",", "fold_num", ",", "fdtype", ",", "attention", ")", "\n", "print", "(", "\"Running q{} hyperparam search #{}\"", ".", "format", "(", "q", ",", "counter", ")", ")", "\n", "wrk_dir", "=", "create_directory", "(", "os", ".", "path", ".", "join", "(", "root_dir", ",", "'question_{}'", ".", "format", "(", "q", ")", ",", "'fold_{}'", ".", "format", "(", "fold_num", ")", ",", "\n", "'config_{}'", ".", "format", "(", "counter", ")", ")", ")", "\n", "run_neural_discern", "(", "data_partition", ",", "dsettypes", ",", "bertmodel", ",", "mconfig", ",", "options", ",", "wrk_dir", ",", "sents_embed_dir", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.hyperparam_model_search_parallel": [[551, 579], ["run_workflow.get_random_question_fold_per_hyperparam_exp", "torch.Queue", "get_random_question_fold_per_hyperparam_exp.items", "q_processes.append", "print", "q_process.start", "q_process.join", "print", "torch.Process"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_random_question_fold_per_hyperparam_exp"], ["", "", "def", "hyperparam_model_search_parallel", "(", "questions_to_run", ",", "q_docpartitions", ",", "bertmodel", ",", "sents_embed_dir", ",", "root_dir", ",", "\n", "question_gpu_map", ",", "fdtype", "=", "torch", ".", "float32", ",", "num_epochs", "=", "15", ",", "prob_interval_truemax", "=", "0.05", ",", "\n", "prob_estim", "=", "0.95", ",", "random_seed", "=", "42", ",", "attention", "=", "True", ")", ":", "\n", "    ", "q_fold_map", "=", "get_random_question_fold_per_hyperparam_exp", "(", "questions_to_run", ",", "random_seed", "=", "random_seed", ")", "\n", "queue", "=", "mp", ".", "Queue", "(", ")", "\n", "q_processes", "=", "[", "]", "\n", "\n", "# create a process for each question's hyperparam search", "\n", "for", "q", ",", "fold_num", "in", "q_fold_map", ".", "items", "(", ")", ":", "\n", "        ", "data_partition", "=", "q_docpartitions", "[", "q", "]", "[", "fold_num", "]", "\n", "q_processes", ".", "append", "(", "mp", ".", "Process", "(", "target", "=", "run_one_questions_hyperparam_search", ",", "args", "=", "(", "queue", ",", "q", ",", "fold_num", ",", "\n", "data_partition", ",", "bertmodel", ",", "\n", "sents_embed_dir", ",", "root_dir", ",", "\n", "question_gpu_map", "[", "q", "]", ",", "\n", "fdtype", ",", "num_epochs", ",", "\n", "prob_interval_truemax", ",", "\n", "prob_estim", ",", "random_seed", ",", "\n", "attention", ")", ")", ")", "\n", "\n", "", "for", "q_process", "in", "q_processes", ":", "\n", "        ", "print", "(", "\">>> spawning hyperparam search process\"", ")", "\n", "q_process", ".", "start", "(", ")", "\n", "\n", "", "for", "q_process", "in", "q_processes", ":", "\n", "        ", "q_process", ".", "join", "(", ")", "\n", "print", "(", "\"<<< joined hyperparam search process\"", ")", "\n", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_saved_config": [[581, 585], ["utilities.ReaderWriter.read_data", "utilities.ReaderWriter.read_data", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data"], ["", "def", "get_saved_config", "(", "config_dir", ")", ":", "\n", "    ", "options", "=", "ReaderWriter", ".", "read_data", "(", "os", ".", "path", ".", "join", "(", "config_dir", ",", "'exp_options.pkl'", ")", ")", "\n", "mconfig", "=", "ReaderWriter", ".", "read_data", "(", "os", ".", "path", ".", "join", "(", "config_dir", ",", "'mconfig.pkl'", ")", ")", "\n", "return", "mconfig", ",", "options", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_index_argmax": [[587, 590], ["numpy.argmax"], "function", ["None"], ["", "def", "get_index_argmax", "(", "score_matrix", ",", "target_indx", ")", ":", "\n", "    ", "argmax_indx", "=", "np", ".", "argmax", "(", "score_matrix", ",", "axis", "=", "0", ")", "[", "target_indx", "]", "\n", "return", "argmax_indx", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_best_config_from_hyperparamsearch": [[592, 627], ["run_workflow.get_random_question_fold_per_hyperparam_exp", "get_random_question_fold_per_hyperparam_exp.items", "range", "numpy.ones", "os.path.join", "os.path.join", "os.path.isfile", "run_workflow.get_index_argmax", "run_workflow.get_saved_config", "utilities.ReaderWriter.read_data", "print", "os.path.join"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_random_question_fold_per_hyperparam_exp", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_index_argmax", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_saved_config", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data"], ["", "def", "get_best_config_from_hyperparamsearch", "(", "questions", ",", "hyperparam_search_dir", ",", "num_trials", "=", "60", ",", "metric_indx", "=", "2", ")", ":", "\n", "    ", "\"\"\"Read best models config from all models tested in hyperparamsearch phase\n\n    Args:\n        questions: list, of questions [4,5,9,10,11]\n        hyperparam_search_dir: string, path root directory where hyperparam models are stored\n        num_trials: int, number of tested models (default 60 based on 0.05 interval and 0.95 confidence interval)\n                    see :func: `compute_numtrials`\n        metric_indx:int, (default 2) using macro_f1 as performance metric to evaluate among the tested models\n    \"\"\"", "\n", "# determine best config from hyperparam search", "\n", "q_fold_map", "=", "get_random_question_fold_per_hyperparam_exp", "(", "questions", ",", "random_seed", "=", "42", ")", "\n", "q_fold_config_map", "=", "{", "}", "\n", "for", "question", ",", "fold_num", "in", "q_fold_map", ".", "items", "(", ")", ":", "\n", "        ", "scores", "=", "np", ".", "ones", "(", "(", "num_trials", ",", "5", ")", ")", "*", "-", "1", "\n", "exist_flag", "=", "False", "\n", "for", "config_num", "in", "range", "(", "num_trials", ")", ":", "\n", "            ", "fold_dir", "=", "os", ".", "path", ".", "join", "(", "hyperparam_search_dir", ",", "'question_{}'", ".", "format", "(", "question", ")", ",", "'fold_{}'", ".", "format", "(", "fold_num", ")", ")", "\n", "\n", "score_file", "=", "os", ".", "path", ".", "join", "(", "fold_dir", ",", "'config_{}'", ".", "format", "(", "config_num", ")", ",", "'score_validation.pkl'", ")", "\n", "if", "(", "os", ".", "path", ".", "isfile", "(", "score_file", ")", ")", ":", "\n", "                ", "mscore", "=", "ReaderWriter", ".", "read_data", "(", "score_file", ")", "\n", "scores", "[", "config_num", ",", "0", "]", "=", "mscore", ".", "best_epoch_indx", "\n", "scores", "[", "config_num", ",", "1", "]", "=", "mscore", ".", "micro_f1", "\n", "scores", "[", "config_num", ",", "2", "]", "=", "mscore", ".", "macro_f1", "\n", "scores", "[", "config_num", ",", "3", "]", "=", "mscore", ".", "accuracy", "\n", "scores", "[", "config_num", ",", "4", "]", "=", "mscore", ".", "auc", "\n", "exist_flag", "=", "True", "\n", "", "else", ":", "\n", "                ", "print", "(", "\"WARNING: hyperparam search dir does not exist: {}\"", ".", "format", "(", "score_file", ")", ")", "\n", "", "", "if", "(", "exist_flag", ")", ":", "\n", "            ", "argmax_indx", "=", "get_index_argmax", "(", "scores", ",", "metric_indx", ")", "\n", "mconfig", ",", "options", "=", "get_saved_config", "(", "os", ".", "path", ".", "join", "(", "fold_dir", ",", "'config_{}'", ".", "format", "(", "argmax_indx", ")", ",", "'config'", ")", ")", "\n", "q_fold_config_map", "[", "question", "]", "=", "(", "mconfig", ",", "options", ",", "argmax_indx", ")", "\n", "", "", "return", "q_fold_config_map", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.build_q_config_map_from_train_val": [[629, 637], ["os.path.join", "run_workflow.get_saved_config", "os.path.join"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_saved_config"], ["", "def", "build_q_config_map_from_train_val", "(", "train_val_dir", ",", "questions", "=", "[", "4", ",", "5", ",", "9", ",", "10", ",", "11", "]", ",", "folds", "=", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", "]", ")", ":", "\n", "    ", "q_fold_config_map", "=", "{", "}", "\n", "for", "question", "in", "questions", ":", "\n", "        ", "for", "fold_num", "in", "folds", ":", "\n", "            ", "fold_dir", "=", "os", ".", "path", ".", "join", "(", "train_val_dir", ",", "'question_{}'", ".", "format", "(", "question", ")", ",", "'fold_{}'", ".", "format", "(", "fold_num", ")", ")", "\n", "mconfig", ",", "options", "=", "get_saved_config", "(", "os", ".", "path", ".", "join", "(", "fold_dir", ",", "'config'", ")", ")", "\n", "q_fold_config_map", "[", "question", "]", "=", "(", "mconfig", ",", "options", ",", "-", "1", ")", "\n", "", "", "return", "q_fold_config_map", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.train_val_run": [[639, 654], ["os.path.join", "utilities.create_directory", "run_workflow.run_neural_discern"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.run_neural_discern"], ["", "def", "train_val_run", "(", "q_docpartitions", ",", "q_fold_config_map", ",", "bertmodel", ",", "train_val_dir", ",", "sents_embed_dir", ",", "question_gpu_map", ",", "\n", "num_epochs", "=", "25", ",", "max_folds", "=", "None", ")", ":", "\n", "    ", "dsettypes", "=", "[", "'train'", ",", "'validation'", "]", "\n", "for", "question", "in", "q_fold_config_map", ":", "\n", "        ", "mconfig", ",", "options", ",", "__", "=", "q_fold_config_map", "[", "question", "]", "\n", "options", "[", "'num_epochs'", "]", "=", "num_epochs", "# override number of epochs using user specified value", "\n", "for", "fold_num", "in", "q_docpartitions", "[", "question", "]", ":", "\n", "            ", "if", "max_folds", "is", "None", "or", "fold_num", "<", "max_folds", ":", "\n", "# update options fold num to the current fold", "\n", "                ", "options", "[", "'fold_num'", "]", "=", "fold_num", "\n", "data_partition", "=", "q_docpartitions", "[", "question", "]", "[", "fold_num", "]", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "train_val_dir", ",", "'question_{}'", ".", "format", "(", "question", ")", ",", "'fold_{}'", ".", "format", "(", "fold_num", ")", ")", "\n", "wrk_dir", "=", "create_directory", "(", "path", ")", "\n", "run_neural_discern", "(", "data_partition", ",", "dsettypes", ",", "bertmodel", ",", "mconfig", ",", "options", ",", "wrk_dir", ",", "sents_embed_dir", ",", "\n", "gpu_index", "=", "question_gpu_map", "[", "question", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.train_val_run_one_question": [[656, 670], ["os.path.join", "utilities.create_directory", "run_workflow.run_neural_discern"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.run_neural_discern"], ["", "", "", "", "def", "train_val_run_one_question", "(", "queue", ",", "question", ",", "q_docpartitions", ",", "q_fold_config_map", ",", "bertmodel", ",", "train_val_dir", ",", "\n", "sents_embed_dir", ",", "gpu_index", ",", "num_epochs", "=", "25", ",", "max_folds", "=", "None", ")", ":", "\n", "    ", "dsettypes", "=", "[", "'train'", ",", "'validation'", "]", "\n", "mconfig", ",", "options", ",", "__", "=", "q_fold_config_map", "[", "question", "]", "\n", "options", "[", "'num_epochs'", "]", "=", "num_epochs", "# override number of epochs using user specified value", "\n", "for", "fold_num", "in", "q_docpartitions", "[", "question", "]", ":", "\n", "        ", "if", "max_folds", "is", "None", "or", "fold_num", "<", "max_folds", ":", "\n", "# update options fold num to the current fold", "\n", "            ", "options", "[", "'fold_num'", "]", "=", "fold_num", "\n", "data_partition", "=", "q_docpartitions", "[", "question", "]", "[", "fold_num", "]", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "train_val_dir", ",", "'question_{}'", ".", "format", "(", "question", ")", ",", "'fold_{}'", ".", "format", "(", "fold_num", ")", ")", "\n", "wrk_dir", "=", "create_directory", "(", "path", ")", "\n", "run_neural_discern", "(", "data_partition", ",", "dsettypes", ",", "bertmodel", ",", "mconfig", ",", "options", ",", "wrk_dir", ",", "sents_embed_dir", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.test_run": [[672, 696], ["os.path.join", "os.path.exists", "utilities.create_directory", "os.path.join", "os.path.join", "utilities.create_directory", "run_workflow.run_neural_discern", "print"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.run_neural_discern"], ["", "", "", "def", "test_run", "(", "q_docpartitions", ",", "q_fold_config_map", ",", "bertmodel", ",", "train_val_dir", ",", "test_dir", ",", "sents_embed_dir", ",", "gpu_index", ",", "\n", "num_epochs", "=", "1", ")", ":", "\n", "    ", "dsettypes", "=", "[", "'test'", "]", "\n", "for", "question", "in", "q_fold_config_map", ":", "\n", "        ", "mconfig", ",", "options", ",", "__", "=", "q_fold_config_map", "[", "question", "]", "\n", "options", "[", "'num_epochs'", "]", "=", "num_epochs", "# override number of epochs using user specified value", "\n", "for", "fold_num", "in", "q_docpartitions", "[", "question", "]", ":", "\n", "# update options fold num to the current fold", "\n", "            ", "options", "[", "'fold_num'", "]", "=", "fold_num", "\n", "data_partition", "=", "q_docpartitions", "[", "question", "]", "[", "fold_num", "]", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "train_val_dir", ",", "'question_{}'", ".", "format", "(", "question", ")", ",", "'fold_{}'", ".", "format", "(", "fold_num", ")", ")", "\n", "# only run testing on question and folds that were run (may not all have been run in code test mode)", "\n", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "                ", "train_dir", "=", "create_directory", "(", "path", ")", "\n", "# load state_dict pth", "\n", "state_dict_pth", "=", "os", ".", "path", ".", "join", "(", "train_dir", ",", "'model_statedict'", ")", "\n", "\n", "path", "=", "os", ".", "path", ".", "join", "(", "test_dir", ",", "'question_{}'", ".", "format", "(", "question", ")", ",", "'fold_{}'", ".", "format", "(", "fold_num", ")", ")", "\n", "test_wrk_dir", "=", "create_directory", "(", "path", ")", "\n", "\n", "run_neural_discern", "(", "data_partition", ",", "dsettypes", ",", "bertmodel", ",", "mconfig", ",", "options", ",", "test_wrk_dir", ",", "\n", "sents_embed_dir", ",", "state_dict_dir", "=", "state_dict_pth", ",", "gpu_index", "=", "gpu_index", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'WARNING: test dir not found: {}'", ".", "format", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.predict_neural_discern": [[700, 878], ["dataset.construct_load_dataloaders", "utilities.get_device", "torch.tensor().type().to", "torch.tensor().type().to", "print", "options.get", "model.BertEmbedder", "model.BertEmbedder.type().to", "model.SentenceEncoder", "len", "model.DocCategScorer", "os.path.isfile", "print", "enumerate", "os.getpid", "model.Attention", "model.DocEncoder", "model.DocEncoder_MeanPooling", "m.type().to", "os.path.join", "utilities.ReaderWriter.read_data", "m.eval", "docs_batch.to.to", "docs_attn_mask.to.to", "docs_sents_len.type().numpy.type().numpy", "docs_labels.type().to.type().to", "torch.tensor().type", "torch.tensor().type", "model.BertEmbedder.type", "m.load_state_dict", "os.path.join", "options.get", "torch.set_grad_enabled", "torch.set_grad_enabled", "docs_batch.to.size", "range", "torch.cat", "torch.cat", "torch.cat.size", "t[].item", "t[].item", "torch.load", "torch.load", "m.type", "docs_sents_len.type().numpy.type", "docs_labels.type().to.type", "docs_id[].item", "model.SentenceEncoder.", "model.DocEncoder_MeanPooling.", "model.DocCategScorer.", "torch.max", "torch.max", "pred_class.append", "logprob_scores.append", "torch.cat.cpu", "torch.cat.cpu", "torch.tensor", "torch.tensor", "os.path.join", "utilities.ReaderWriter.read_tensor", "model.BertEmbedder.", "os.path.join", "utilities.ReaderWriter.dump_tensor", "docs_len[].item", "pred_classindx.item", "docs_len[].item"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.construct_load_dataloaders", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_tensor", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_tensor"], ["", "", "", "", "def", "predict_neural_discern", "(", "data_partition", ",", "bertmodel", ",", "config", ",", "options", ",", "wrk_dir", ",", "sents_embed_dir", ",", "\n", "state_dict_dir", "=", "None", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "    ", "dsettype", "=", "'test'", "\n", "pid", "=", "\"{}\"", ".", "format", "(", "os", ".", "getpid", "(", ")", ")", "# process id description", "\n", "# get data loader config", "\n", "dataloader_config", "=", "config", "[", "'dataloader_config'", "]", "\n", "cld", "=", "construct_load_dataloaders", "(", "data_partition", ",", "[", "dsettype", "]", ",", "dataloader_config", ",", "wrk_dir", ")", "\n", "# dictionaries by dsettypes", "\n", "data_loaders", ",", "epoch_loss_avgbatch", ",", "epoch_loss_avgsamples", ",", "score_dict", ",", "class_weights", ",", "flog_out", "=", "cld", "\n", "# print(class_weights)", "\n", "device", "=", "get_device", "(", "to_gpu", ",", "gpu_index", ")", "# gpu device", "\n", "generic_config", "=", "config", "[", "'generic_config'", "]", "\n", "fdtype", "=", "generic_config", "[", "'fdtype'", "]", "\n", "\n", "class_weights", "=", "torch", ".", "tensor", "(", "[", "1", ",", "1", "]", ")", ".", "type", "(", "fdtype", ")", ".", "to", "(", "device", ")", "# weighting all cases equally", "\n", "\n", "print", "(", "\"class weights\"", ",", "class_weights", ")", "\n", "\n", "fold_num", "=", "options", ".", "get", "(", "'fold_num'", ")", "\n", "\n", "# parse config dict", "\n", "bertencoder_config", "=", "config", "[", "'bert_encoder_config'", "]", "\n", "sentencoder_config", "=", "config", "[", "'sent_encoder_config'", "]", "\n", "docencoder_config", "=", "config", "[", "'doc_encoder_config'", "]", "\n", "attnmodel_config", "=", "config", "[", "'attnmodel_config'", "]", "\n", "docscorer_config", "=", "config", "[", "'doc_scorer_config'", "]", "\n", "\n", "# setup the models", "\n", "# bert model", "\n", "bert_encoder", "=", "BertEmbedder", "(", "bertmodel", ",", "bertencoder_config", ")", "\n", "bert_encoder", ".", "type", "(", "fdtype", ")", ".", "to", "(", "device", ")", "\n", "\n", "# sentence encoder model", "\n", "sent_encoder", "=", "SentenceEncoder", "(", "sentencoder_config", "[", "'input_dim'", "]", ",", "\n", "sentencoder_config", "[", "'hidden_dim'", "]", ",", "\n", "num_hiddenlayers", "=", "sentencoder_config", "[", "'num_hiddenlayers'", "]", ",", "\n", "bidirection", "=", "sentencoder_config", "[", "'bidirection'", "]", ",", "\n", "pdropout", "=", "sentencoder_config", "[", "'pdropout'", "]", ",", "\n", "config", "=", "sentencoder_config", "[", "'generic_config'", "]", ",", "\n", "to_gpu", "=", "to_gpu", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n", "# doc encoder model", "\n", "if", "attnmodel_config", ":", "\n", "\n", "        ", "attn_model", "=", "Attention", "(", "attnmodel_config", "[", "'attn_method'", "]", ",", "\n", "attnmodel_config", "[", "'attn_input_dim'", "]", ",", "\n", "config", "=", "attnmodel_config", "[", "'generic_config'", "]", ",", "\n", "to_gpu", "=", "to_gpu", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n", "doc_encoder", "=", "DocEncoder", "(", "docencoder_config", "[", "'input_dim'", "]", ",", "\n", "docencoder_config", "[", "'hidden_dim'", "]", ",", "\n", "attn_model", ",", "\n", "num_hiddenlayers", "=", "docencoder_config", "[", "'num_hiddenlayers'", "]", ",", "\n", "bidirection", "=", "docencoder_config", "[", "'bidirection'", "]", ",", "\n", "pdropout", "=", "docencoder_config", "[", "'pdropout'", "]", ",", "\n", "config", "=", "docencoder_config", "[", "'generic_config'", "]", ",", "\n", "to_gpu", "=", "to_gpu", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n", "", "else", ":", "# case of no attention model (i.e. mean pooling)", "\n", "        ", "doc_encoder", "=", "DocEncoder_MeanPooling", "(", "docencoder_config", "[", "'input_dim'", "]", ",", "\n", "docencoder_config", "[", "'hidden_dim'", "]", ",", "\n", "num_hiddenlayers", "=", "docencoder_config", "[", "'num_hiddenlayers'", "]", ",", "\n", "bidirection", "=", "docencoder_config", "[", "'bidirection'", "]", ",", "\n", "pdropout", "=", "docencoder_config", "[", "'pdropout'", "]", ",", "\n", "config", "=", "docencoder_config", "[", "'generic_config'", "]", ",", "\n", "to_gpu", "=", "to_gpu", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "\n", "# doc category scorer", "\n", "", "num_labels", "=", "len", "(", "class_weights", ")", "\n", "doc_categ_scorer", "=", "DocCategScorer", "(", "docscorer_config", "[", "'input_dim'", "]", ",", "num_labels", ")", "\n", "\n", "# define optimizer and group parameters", "\n", "models", "=", "[", "(", "sent_encoder", ",", "'sent_encoder'", ")", ",", "(", "doc_encoder", ",", "'doc_encoder'", ")", ",", "(", "doc_categ_scorer", ",", "'doc_categ_scorer'", ")", "]", "\n", "\n", "if", "(", "state_dict_dir", ")", ":", "# load state dictionary of saved models", "\n", "        ", "for", "m", ",", "m_name", "in", "models", ":", "\n", "            ", "m", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "state_dict_dir", ",", "'{}.pkl'", ".", "format", "(", "m_name", ")", ")", ",", "map_location", "=", "device", ")", ")", "\n", "\n", "# update models fdtype and move to device", "\n", "", "", "for", "m", ",", "m_name", "in", "models", ":", "\n", "        ", "m", ".", "type", "(", "fdtype", ")", ".", "to", "(", "device", ")", "\n", "\n", "# store attention weights for validation and test set", "\n", "", "if", "attnmodel_config", ":", "\n", "        ", "docid_attnweights_map", "=", "{", "dsettype", ":", "{", "}", "for", "dsettype", "in", "data_loaders", "if", "dsettype", "in", "{", "'validation'", ",", "'test'", "}", "}", "\n", "", "else", ":", "\n", "        ", "docid_attnweights_map", "=", "{", "}", "\n", "\n", "", "if", "(", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "sents_embed_dir", ",", "'bert_proc_docs.pkl'", ")", ")", ")", ":", "\n", "        ", "bert_proc_docs", "=", "ReaderWriter", ".", "read_data", "(", "os", ".", "path", ".", "join", "(", "sents_embed_dir", ",", "'bert_proc_docs.pkl'", ")", ")", "\n", "", "else", ":", "\n", "        ", "bert_proc_docs", "=", "{", "}", "\n", "\n", "", "print", "(", "\"device: {} | question: {} | fold_num: {} | dsettype: {} | pid: {}\"", "\n", "\"\"", ".", "format", "(", "device", ",", "options", ".", "get", "(", "'question'", ")", ",", "fold_num", ",", "dsettype", ",", "pid", ")", ")", "\n", "pred_class", "=", "[", "]", "\n", "\n", "data_loader", "=", "data_loaders", "[", "dsettype", "]", "\n", "\n", "for", "m", ",", "m_name", "in", "models", ":", "\n", "        ", "m", ".", "eval", "(", ")", "\n", "\n", "", "sample_counter", "=", "0", "\n", "for", "i_batch", ",", "samples_batch", "in", "enumerate", "(", "data_loader", ")", ":", "\n", "\n", "        ", "logprob_scores", "=", "[", "]", "\n", "\n", "docs_batch", ",", "docs_len", ",", "docs_sents_len", ",", "docs_attn_mask", ",", "docs_labels", ",", "docs_id", "=", "samples_batch", "\n", "\n", "docs_batch", "=", "docs_batch", ".", "to", "(", "device", ")", "\n", "docs_attn_mask", "=", "docs_attn_mask", ".", "to", "(", "device", ")", "\n", "docs_sents_len", "=", "docs_sents_len", ".", "type", "(", "torch", ".", "int64", ")", ".", "numpy", "(", ")", "# to feed this in RNN", "\n", "docs_labels", "=", "docs_labels", ".", "type", "(", "torch", ".", "int64", ")", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "set_grad_enabled", "(", "dsettype", "==", "'train'", ")", ":", "\n", "# print(\"number of examples in batch:\", docs_batch.size(0))", "\n", "            ", "num_docs_perbatch", "=", "docs_batch", ".", "size", "(", "0", ")", "\n", "for", "doc_indx", "in", "range", "(", "num_docs_perbatch", ")", ":", "\n", "# print('doc_indx', doc_indx)", "\n", "                ", "doc_id", "=", "docs_id", "[", "doc_indx", "]", ".", "item", "(", ")", "\n", "if", "(", "doc_id", "in", "bert_proc_docs", ")", ":", "\n", "# due to GPU limit", "\n", "                    ", "embed_sents", "=", "ReaderWriter", ".", "read_tensor", "(", "bert_proc_docs", "[", "doc_id", "]", ",", "device", ")", "\n", "", "else", ":", "\n", "                    ", "embed_sents", "=", "bert_encoder", "(", "docs_batch", "[", "doc_indx", "]", ",", "docs_attn_mask", "[", "doc_indx", "]", ",", "\n", "docs_len", "[", "doc_indx", "]", ".", "item", "(", ")", ")", "\n", "# add embedding to dict", "\n", "embed_fpath", "=", "os", ".", "path", ".", "join", "(", "sents_embed_dir", ",", "'{}.pkl'", ".", "format", "(", "doc_id", ")", ")", "\n", "ReaderWriter", ".", "dump_tensor", "(", "embed_sents", ",", "embed_fpath", ")", "\n", "bert_proc_docs", "[", "doc_id", "]", "=", "embed_fpath", "\n", "\n", "", "sents_rnn_hidden", "=", "sent_encoder", "(", "embed_sents", ",", "docs_sents_len", "[", "doc_indx", "]", ",", "docs_len", "[", "doc_indx", "]", ".", "item", "(", ")", ")", "\n", "\n", "# # remove the embedding from GPU", "\n", "# bert_proc_docs[doc_id].to(cpu_device)", "\n", "# print('sents_rnn_hidden', sents_rnn_hidden.shape)", "\n", "enc_sents", "=", "sents_rnn_hidden", "\n", "# print('enc_sents', enc_sents.shape)", "\n", "doc_out", ",", "doc_attn_weights", "=", "doc_encoder", "(", "enc_sents", ")", "\n", "# print('doc_out', doc_out.shape)", "\n", "# print('doc_attn_weights', doc_attn_weights.shape)", "\n", "# tracking attention weight for validation and test examples", "\n", "if", "(", "dsettype", "in", "docid_attnweights_map", ")", ":", "\n", "                    ", "docid_attnweights_map", "[", "dsettype", "]", "[", "doc_id", "]", "=", "doc_attn_weights", "\n", "\n", "", "logsoftmax_scores", "=", "doc_categ_scorer", "(", "doc_out", ")", "\n", "__", ",", "pred_classindx", "=", "torch", ".", "max", "(", "logsoftmax_scores", ",", "1", ")", "# apply max on row level", "\n", "\n", "pred_class", ".", "append", "(", "pred_classindx", ".", "item", "(", ")", ")", "\n", "\n", "logprob_scores", ".", "append", "(", "logsoftmax_scores", ")", "\n", "sample_counter", "+=", "1", "\n", "\n", "# log probabilities", "\n", "", "b_logprob_scores", "=", "torch", ".", "cat", "(", "logprob_scores", ",", "dim", "=", "0", ")", "\n", "\n", "# do some cleaning -- get more GPU ;)", "\n", "del", "docs_batch", ",", "docs_len", ",", "docs_sents_len", ",", "docs_attn_mask", ",", "docs_labels", ",", "docs_id", "\n", "\n", "# end of epoch", "\n", "", "", "if", "docid_attnweights_map", ":", "\n", "        ", "target_attn_w_map", "=", "docid_attnweights_map", "[", "'test'", "]", "\n", "", "else", ":", "\n", "        ", "target_attn_w_map", "=", "[", "'NA'", "]", "*", "b_logprob_scores", ".", "size", "(", "0", ")", "\n", "\n", "", "results", "=", "{", "\n", "'pred_class'", ":", "pred_class", ",", "\n", "'logprob_score_class0'", ":", "[", "t", "[", "0", "]", ".", "item", "(", ")", "for", "t", "in", "b_logprob_scores", ".", "cpu", "(", ")", "]", ",", "\n", "'logprob_score_class1'", ":", "[", "t", "[", "1", "]", ".", "item", "(", ")", "for", "t", "in", "b_logprob_scores", ".", "cpu", "(", ")", "]", ",", "\n", "}", "\n", "if", "attnmodel_config", ":", "\n", "        ", "results", "[", "'attention_weight_map'", "]", "=", "target_attn_w_map", "\n", "\n", "", "return", "results", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ModelScore.__init__": [[11, 17], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "best_epoch_indx", ",", "micro_f1", ",", "macro_f1", ",", "accuracy", ",", "auc", ")", ":", "\n", "        ", "self", ".", "best_epoch_indx", "=", "best_epoch_indx", "\n", "self", ".", "micro_f1", "=", "micro_f1", "\n", "self", ".", "macro_f1", "=", "macro_f1", "\n", "self", ".", "accuracy", "=", "accuracy", "\n", "self", ".", "auc", "=", "auc", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ModelScore.__repr__": [[18, 22], ["None"], "methods", ["None"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "desc", "=", "\" best_epoch_indx:{}\\n micro_f1:{} \\n macro_f1:{} \\n accuracy:{} \\n auc:{} \\n\"", "\"\"", ".", "format", "(", "self", ".", "best_epoch_indx", ",", "self", ".", "micro_f1", ",", "self", ".", "macro_f1", ",", "self", ".", "accuracy", ",", "self", ".", "auc", ")", "\n", "return", "desc", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.__init__": [[26, 28], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_data": [[29, 40], ["open", "pickle.dump"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "dump_data", "(", "data", ",", "file_name", ",", "mode", "=", "\"wb\"", ")", ":", "\n", "        ", "\"\"\"dump data by pickling\n\n           Args:\n               data: data to be pickled\n               file_name: file path where data will be dumped\n               mode: specify writing options i.e. binary or unicode\n        \"\"\"", "\n", "with", "open", "(", "file_name", ",", "mode", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data": [[41, 52], ["open", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load"], ["", "", "@", "staticmethod", "\n", "def", "read_data", "(", "file_name", ",", "mode", "=", "\"rb\"", ")", ":", "\n", "        ", "\"\"\"read dumped/pickled data\n\n           Args:\n               file_name: file path where data will be dumped\n               mode: specify writing options i.e. binary or unicode\n        \"\"\"", "\n", "with", "open", "(", "file_name", ",", "mode", ")", "as", "f", ":", "\n", "            ", "data", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_tensor": [[53, 66], ["torch.save"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.save"], ["", "@", "staticmethod", "\n", "def", "dump_tensor", "(", "data", ",", "file_name", ")", ":", "\n", "        ", "\"\"\"\n        Dump a tensor using PyTorch's custom serialization. Enables re-loading the tensor on a specific gpu later.\n\n        Args:\n            data: Tensor\n            file_name: file path where data will be dumped\n\n        Returns:\n\n        \"\"\"", "\n", "torch", ".", "save", "(", "data", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_tensor": [[67, 77], ["torch.load"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load"], ["", "@", "staticmethod", "\n", "def", "read_tensor", "(", "file_name", ",", "device", ")", ":", "\n", "        ", "\"\"\"read dumped/pickled data\n\n           Args:\n               file_name: file path where data will be dumped\n               device: the gpu to load the tensor on to\n        \"\"\"", "\n", "data", "=", "torch", ".", "load", "(", "file_name", ",", "map_location", "=", "device", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.write_log": [[78, 89], ["open", "f.write"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "write_log", "(", "line", ",", "outfile", ",", "mode", "=", "\"a\"", ")", ":", "\n", "        ", "\"\"\"write data to a file\n\n           Args:\n               line: string representing data to be written out\n               outfile: file path where data will be written/logged\n               mode: specify writing options i.e. append, write\n        \"\"\"", "\n", "with", "open", "(", "outfile", ",", "mode", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_log": [[90, 102], ["open"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "read_log", "(", "file_name", ",", "mode", "=", "\"r\"", ")", ":", "\n", "        ", "\"\"\"write data to a file\n\n           Args:\n               line: string representing data to be written out\n               outfile: file path where data will be written/logged\n               mode: specify writing options i.e. append, write\n        \"\"\"", "\n", "with", "open", "(", "file_name", ",", "mode", ")", "as", "f", ":", "\n", "            ", "for", "line", "in", "f", ":", "\n", "                ", "yield", "line", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory": [[104, 122], ["os.path.join", "os.path.dirname", "os.path.exists", "os.makedirs"], "function", ["None"], ["", "", "", "", "def", "create_directory", "(", "folder_name", ",", "directory", "=", "\"current\"", ")", ":", "\n", "    ", "\"\"\"create directory/folder (if it does not exist) and returns the path of the directory\n\n       Args:\n           folder_name: string representing the name of the folder to be created\n\n       Keyword Arguments:\n           directory: string representing the directory where to create the folder\n                      if `current` then the folder will be created in the current directory\n    \"\"\"", "\n", "if", "directory", "==", "\"current\"", ":", "\n", "        ", "path_current_dir", "=", "os", ".", "path", ".", "dirname", "(", "__file__", ")", "# __file__ refers to utilities.py", "\n", "", "else", ":", "\n", "        ", "path_current_dir", "=", "directory", "\n", "", "path_new_dir", "=", "os", ".", "path", ".", "join", "(", "path_current_dir", ",", "folder_name", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path_new_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path_new_dir", ")", "\n", "", "return", "(", "path_new_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device": [[124, 131], ["torch.cuda.is_available", "torch.device"], "function", ["None"], ["", "def", "get_device", "(", "to_gpu", ",", "index", "=", "0", ")", ":", "\n", "    ", "is_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "if", "(", "is_cuda", "and", "to_gpu", ")", ":", "\n", "        ", "target_device", "=", "'cuda:{}'", ".", "format", "(", "index", ")", "\n", "", "else", ":", "\n", "        ", "target_device", "=", "'cpu'", "\n", "", "return", "torch", ".", "device", "(", "target_device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.report_available_cuda_devices": [[133, 141], ["torch.cuda.device_count", "print", "range", "print", "torch.device", "utilities.get_cuda_device_stats", "print", "torch.cuda.get_device_name"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_cuda_device_stats"], ["", "def", "report_available_cuda_devices", "(", ")", ":", "\n", "    ", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "print", "(", "'number of GPUs available:'", ",", "n_gpu", ")", "\n", "for", "i", "in", "range", "(", "n_gpu", ")", ":", "\n", "        ", "print", "(", "\"cuda:{}, name:{}\"", ".", "format", "(", "i", ",", "torch", ".", "cuda", ".", "get_device_name", "(", "i", ")", ")", ")", "\n", "device", "=", "torch", ".", "device", "(", "'cuda'", ",", "i", ")", "\n", "get_cuda_device_stats", "(", "device", ")", "\n", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_cuda_device_stats": [[143, 149], ["print", "print", "print", "print", "print", "torch.cuda.memory_allocated", "torch.cuda.max_memory_allocated", "torch.cuda.memory_cached", "torch.cuda.max_memory_cached", "torch.cuda.get_device_properties"], "function", ["None"], ["", "", "def", "get_cuda_device_stats", "(", "device", ")", ":", "\n", "    ", "print", "(", "'total memory available:'", ",", "torch", ".", "cuda", ".", "get_device_properties", "(", "device", ")", ".", "total_memory", "/", "(", "1024", "**", "3", ")", ",", "'GB'", ")", "\n", "print", "(", "'total memory allocated on device:'", ",", "torch", ".", "cuda", ".", "memory_allocated", "(", "device", ")", "/", "(", "1024", "**", "3", ")", ",", "'GB'", ")", "\n", "print", "(", "'max memory allocated on device:'", ",", "torch", ".", "cuda", ".", "max_memory_allocated", "(", "device", ")", "/", "(", "1024", "**", "3", ")", ",", "'GB'", ")", "\n", "print", "(", "'total memory cached on device:'", ",", "torch", ".", "cuda", ".", "memory_cached", "(", "device", ")", "/", "(", "1024", "**", "3", ")", ",", "'GB'", ")", "\n", "print", "(", "'max memory cached  on device:'", ",", "torch", ".", "cuda", ".", "max_memory_cached", "(", "device", ")", "/", "(", "1024", "**", "3", ")", ",", "'GB'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.perfmetric_report": [[151, 182], ["sklearn.metrics.f1_score", "sklearn.metrics.f1_score", "sklearn.metrics.accuracy_score", "utilities.ModelScore", "utilities.ReaderWriter.write_log", "outcome_lst.append", "str", "str", "str", "str", "numpy.array", "sklearn.metrics.classification_report"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.write_log"], ["", "def", "perfmetric_report", "(", "pred_target", ",", "ref_target", ",", "epoch", ",", "outlog", ",", "plot_roc", "=", "True", ")", ":", "\n", "\n", "# print(ref_target.shape)", "\n", "# print(pred_target.shape)", "\n", "#", "\n", "# print(\"ref_target \\n\", ref_target)", "\n", "# print(\"pred_target \\n\", pred_target)", "\n", "\n", "    ", "outcome_lst", "=", "[", "]", "\n", "for", "arr", "in", "(", "ref_target", ",", "pred_target", ")", ":", "\n", "        ", "outcome_lst", ".", "append", "(", "np", ".", "array", "(", "arr", ")", ")", "\n", "\n", "", "lsep", "=", "\"\\n\"", "\n", "report", "=", "\"Epoch: {}\"", ".", "format", "(", "epoch", ")", "+", "lsep", "\n", "report", "+=", "\"Classification report on all events:\"", "+", "lsep", "\n", "report", "+=", "str", "(", "classification_report", "(", "outcome_lst", "[", "0", "]", ",", "outcome_lst", "[", "1", "]", ")", ")", "+", "lsep", "\n", "report", "+=", "\"macro f1:\"", "+", "lsep", "\n", "macro_f1", "=", "f1_score", "(", "outcome_lst", "[", "0", "]", ",", "outcome_lst", "[", "1", "]", ",", "average", "=", "'macro'", ")", "\n", "report", "+=", "str", "(", "macro_f1", ")", "+", "lsep", "\n", "report", "+=", "\"micro f1:\"", "+", "lsep", "\n", "micro_f1", "=", "f1_score", "(", "outcome_lst", "[", "0", "]", ",", "outcome_lst", "[", "1", "]", ",", "average", "=", "'micro'", ")", "\n", "report", "+=", "str", "(", "micro_f1", ")", "+", "lsep", "\n", "report", "+=", "\"accuracy:\"", "+", "lsep", "\n", "accuracy", "=", "accuracy_score", "(", "outcome_lst", "[", "0", "]", ",", "outcome_lst", "[", "1", "]", ")", "\n", "report", "+=", "str", "(", "accuracy", ")", "+", "lsep", "\n", "report", "+=", "\"-\"", "*", "30", "+", "lsep", "\n", "\n", "# for now we are not computing auc values -- set to 0", "\n", "modelscore", "=", "ModelScore", "(", "epoch", ",", "micro_f1", ",", "macro_f1", ",", "accuracy", ",", "0", ")", "\n", "ReaderWriter", ".", "write_log", "(", "report", ",", "outlog", ")", "\n", "return", "modelscore", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.plot_precision_recall_curve": [[184, 196], ["sklearn.metrics.precision_recall_curve", "matplotlib.pyplot.figure", "matplotlib.pyplot.plot", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.title", "matplotlib.pyplot.legend", "matplotlib.pyplot.savefig", "matplotlib.pyplot.close", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "plot_precision_recall_curve", "(", "ref_target", ",", "prob_poslabel", ",", "figname", ",", "outdir", ")", ":", "\n", "    ", "pr", ",", "rec", ",", "thresholds", "=", "precision_recall_curve", "(", "ref_target", ",", "prob_poslabel", ")", "\n", "thresholds", "[", "0", "]", "=", "1", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "9", ",", "6", ")", ")", "\n", "plt", ".", "plot", "(", "pr", ",", "rec", ",", "'bo'", ",", "label", "=", "'Precision vs Recall'", ")", "\n", "# plt.plot(np.arange(0,len(thresholds)), thresholds, 'r-', label='thresholds')", "\n", "plt", ".", "xlabel", "(", "'Precision'", ")", "\n", "plt", ".", "ylabel", "(", "'Recall'", ")", "\n", "plt", ".", "title", "(", "'Precision vs. recall curve'", ")", "\n", "plt", ".", "legend", "(", "loc", "=", "'best'", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "outdir", ",", "os", ".", "path", ".", "join", "(", "'precisionrecall_curve_{}'", ".", "format", "(", "figname", ")", "+", "\".pdf\"", ")", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.plot_roc_curve": [[198, 210], ["sklearn.metrics.roc_curve", "matplotlib.pyplot.figure", "matplotlib.pyplot.plot", "matplotlib.pyplot.plot", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.title", "matplotlib.pyplot.legend", "matplotlib.pyplot.savefig", "matplotlib.pyplot.close", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "plot_roc_curve", "(", "ref_target", ",", "prob_poslabel", ",", "figname", ",", "outdir", ")", ":", "\n", "    ", "fpr", ",", "tpr", ",", "thresholds", "=", "roc_curve", "(", "ref_target", ",", "prob_poslabel", ")", "\n", "thresholds", "[", "0", "]", "=", "1", "\n", "plt", ".", "figure", "(", "figsize", "=", "(", "9", ",", "6", ")", ")", "\n", "plt", ".", "plot", "(", "fpr", ",", "tpr", ",", "'bo'", ",", "label", "=", "'TPR vs FPR'", ")", "\n", "plt", ".", "plot", "(", "fpr", ",", "thresholds", ",", "'r-'", ",", "label", "=", "'thresholds'", ")", "\n", "plt", ".", "xlabel", "(", "'False positive rate'", ")", "\n", "plt", ".", "ylabel", "(", "'True positive rate'", ")", "\n", "plt", ".", "title", "(", "'ROC curve'", ")", "\n", "plt", ".", "legend", "(", "loc", "=", "'best'", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "outdir", ",", "os", ".", "path", ".", "join", "(", "'roc_curve_{}'", ".", "format", "(", "figname", ")", "+", "\".pdf\"", ")", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.plot_loss": [[212, 222], ["epoch_loss_avgbatch.keys", "matplotlib.pyplot.figure", "matplotlib.pyplot.plot", "matplotlib.pyplot.xlabel", "matplotlib.pyplot.ylabel", "matplotlib.pyplot.legend", "matplotlib.pyplot.savefig", "matplotlib.pyplot.close", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "plot_loss", "(", "epoch_loss_avgbatch", ",", "epoch_loss_avgsamples", ",", "wrk_dir", ")", ":", "\n", "    ", "dsettypes", "=", "epoch_loss_avgbatch", ".", "keys", "(", ")", "\n", "for", "dsettype", "in", "dsettypes", ":", "\n", "        ", "plt", ".", "figure", "(", "figsize", "=", "(", "9", ",", "6", ")", ")", "\n", "plt", ".", "plot", "(", "epoch_loss_avgbatch", "[", "dsettype", "]", ",", "'r'", ",", "epoch_loss_avgsamples", "[", "dsettype", "]", ",", "'b'", ")", "\n", "plt", ".", "xlabel", "(", "\"number of epochs\"", ")", "\n", "plt", ".", "ylabel", "(", "\"negative loglikelihood cost\"", ")", "\n", "plt", ".", "legend", "(", "[", "'epoch batch average loss'", ",", "'epoch training samples average loss'", "]", ")", "\n", "plt", ".", "savefig", "(", "os", ".", "path", ".", "join", "(", "wrk_dir", ",", "os", ".", "path", ".", "join", "(", "dsettype", "+", "\".pdf\"", ")", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.delete_directory": [[224, 227], ["os.path.isdir", "shutil.rmtree"], "function", ["None"], ["", "", "def", "delete_directory", "(", "directory", ")", ":", "\n", "    ", "if", "(", "os", ".", "path", ".", "isdir", "(", "directory", ")", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "directory", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.identify_attended_senteces": [[31, 34], ["neural.run_workflow.return_attnw_over_sents"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.return_attnw_over_sents"], ["def", "identify_attended_senteces", "(", "docid_attnweights_map", ",", "proc_articles_repr", ",", "topk", "=", "5", ")", ":", "\n", "    ", "attended_sents", "=", "return_attnw_over_sents", "(", "docid_attnweights_map", ",", "proc_articles_repr", ",", "topk", ")", "\n", "return", "attended_sents", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.run_predict": [[36, 55], ["os.path.join", "neural.utilities.create_directory", "neural.run_workflow.predict_neural_discern"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.predict_neural_discern"], ["", "def", "run_predict", "(", "q_docpartitions", ",", "q_fold_config_map", ",", "bertmodel", ",", "q_state_dict_path_map", ",", "results_dir", ",", "sents_embed_dir", ",", "\n", "question_fold_map", ",", "to_gpu", ",", "gpu_index", ",", "num_epochs", "=", "1", ")", "->", "Dict", ":", "\n", "    ", "q_predictions", "=", "{", "}", "\n", "for", "question", "in", "q_fold_config_map", ":", "\n", "        ", "mconfig", ",", "options", ",", "__", "=", "q_fold_config_map", "[", "question", "]", "\n", "options", "[", "'num_epochs'", "]", "=", "num_epochs", "# override number of epochs using user specified value", "\n", "\n", "# update options fold num to the current fold", "\n", "options", "[", "'fold_num'", "]", "=", "question_fold_map", "[", "question", "]", "\n", "data_partition", "=", "q_docpartitions", "[", "question", "]", "[", "options", "[", "'fold_num'", "]", "]", "\n", "\n", "results_path", "=", "os", ".", "path", ".", "join", "(", "results_dir", ",", "'question_{}'", ".", "format", "(", "question", ")", ",", "'fold_{}'", ".", "format", "(", "options", "[", "'fold_num'", "]", ")", ")", "\n", "results_wrk_dir", "=", "create_directory", "(", "results_path", ")", "\n", "\n", "q_predictions", "[", "question", "]", "=", "predict_neural_discern", "(", "data_partition", ",", "bertmodel", ",", "mconfig", ",", "options", ",", "\n", "results_wrk_dir", ",", "sents_embed_dir", ",", "\n", "state_dict_dir", "=", "q_state_dict_path_map", "[", "question", "]", ",", "to_gpu", "=", "to_gpu", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "", "return", "q_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.retrieve_page_from_internet": [[57, 61], ["requests.get", "requests.get.content.decode"], "function", ["None"], ["", "def", "retrieve_page_from_internet", "(", "url", ":", "str", ")", ":", "\n", "    ", "res", "=", "requests", ".", "get", "(", "url", ")", "\n", "html_page", "=", "res", ".", "content", ".", "decode", "(", "\"utf-8\"", ")", "\n", "return", "html_page", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.create_prediction_qdoc_partitions": [[63, 70], ["None"], "function", ["None"], ["", "def", "create_prediction_qdoc_partitions", "(", "questions", ":", "List", "[", "int", "]", ",", "question_fold_map", ":", "Dict", "[", "int", ",", "int", "]", ")", ":", "\n", "    ", "q_docpartitions", "=", "{", "}", "\n", "for", "q", "in", "questions", ":", "\n", "        ", "q_docpartitions", "[", "q", "]", "=", "{", "}", "\n", "fold_num", "=", "question_fold_map", "[", "q", "]", "\n", "q_docpartitions", "[", "q", "]", "[", "fold_num", "]", "=", "{", "'test'", ":", "[", "0", "]", "}", "# output of np.vectorize() ?!", "\n", "", "return", "q_docpartitions", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.build_DataDictProcessor": [[72, 78], ["neural.data_processor.DataDictProcessor", "neural.data_processor.DataDictProcessor.generate_articles_repr"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor.generate_articles_repr"], ["", "def", "build_DataDictProcessor", "(", "data_dict", ",", "vocab_path", ":", "str", ",", "processor_config", ":", "Dict", ")", ":", "\n", "# from the first notebook", "\n", "\n", "    ", "processor", "=", "DataDictProcessor", "(", "processor_config", ")", "\n", "processor", ".", "generate_articles_repr", "(", "data_dict", ")", "\n", "return", "processor", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.embed_sentences": [[80, 90], ["neural.model.BertEmbedder", "neural.model.generate_sents_embeds_from_docs", "neural.utilities.ReaderWriter.dump_data", "os.path.join"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.generate_sents_embeds_from_docs", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_data"], ["", "def", "embed_sentences", "(", "docs_data_tensor", ",", "sents_embed_path", ",", "bertmodel", ",", "bert_config", ",", "to_gpu", ",", "gpu_index", ")", ":", "\n", "# from the second notebook", "\n", "\n", "    ", "bertembeder", "=", "BertEmbedder", "(", "bertmodel", ",", "bert_config", ")", "\n", "fdtype", "=", "torch", ".", "float32", "\n", "\n", "# generate and dump bert embedding for the tokens inside the specificed embedding directory", "\n", "bert_proc_docs", "=", "generate_sents_embeds_from_docs", "(", "docs_data_tensor", ",", "bertembeder", ",", "sents_embed_path", ",", "fdtype", ",", "to_gpu", ",", "\n", "gpu_index", "=", "gpu_index", ")", "\n", "ReaderWriter", ".", "dump_data", "(", "bert_proc_docs", ",", "os", ".", "path", ".", "join", "(", "sents_embed_path", ",", "'bert_proc_docs.pkl'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.biobert_predict": [[92, 189], ["predict_with_neural.check_for_non_git_files", "pkg_resources.resource_filename", "pkg_resources.resource_filename", "pkg_resources.resource_filename", "neural.utilities.get_device", "predict_with_neural.create_prediction_qdoc_partitions", "autodiscern.Transformer", "adt.Transformer.apply", "pkg_resources.resource_filename", "neural.neural_discern_run_script.load_biobert_model", "predict_with_neural.build_DataDictProcessor", "pytorch_pretrained_bert.BertTokenizer.from_pretrained", "build_DataDictProcessor.generate_doctensor_from_articles", "print", "predict_with_neural.embed_sentences", "print", "print", "predict_with_neural.run_predict", "q_docpartitions.update", "os.path.join", "neural.run_workflow.get_saved_config", "os.path.join", "neural.dataset.generate_docpartition_per_question", "config_path_form.format", "state_dict_path_form.format", "len", "predict_with_neural.identify_attended_senteces"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.check_for_non_git_files", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.create_prediction_qdoc_partitions", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.load_biobert_model", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.build_DataDictProcessor", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor.generate_doctensor_from_articles", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.embed_sentences", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.run_predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.get_saved_config", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.generate_docpartition_per_question", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.identify_attended_senteces"], ["", "def", "biobert_predict", "(", "data_dict", ":", "dict", ",", "questions", ",", "experiment_dir", ",", "question_fold_map", ",", "to_gpu", ",", "gpu_index", ")", "->", "Dict", ":", "\n", "    ", "\"\"\"\n    Make an autoDiscern prediction for an article data_dict using the HEA BioBERT model. Includes all of the data\n    preprocessing steps as were applied for the training of the HEA BioBERT model.\n\n    Args:\n        data_dict: dictionary of {id: sub-dict}, with sub-dictionary with keys ['url', 'content', 'id', 'responses']\n\n    Returns: autodiscern predictions for the article.\n\n    \"\"\"", "\n", "check_for_non_git_files", "(", "check_metamap", "=", "False", ",", "check_biobert", "=", "True", ")", "\n", "\n", "working_dir", "=", "'predict'", "\n", "model_path_within_pkg_resources", "=", "'package_data/predictors/{}'", ".", "format", "(", "experiment_dir", ")", "\n", "experiment_model_dir", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "model_path_within_pkg_resources", ")", "\n", "\n", "vocab_path_within_pkg_resources", "=", "'package_data/pytorch_biobert/bert-base-cased-vocab.txt'", "\n", "vocab_path", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "vocab_path_within_pkg_resources", ")", "\n", "processor_config", "=", "{", "'tokenizer_max_sent_len'", ":", "300", ",", "\n", "'label_cutoff'", ":", "3", ",", "\n", "'label_avgmethod'", ":", "'round_mean'", "}", "\n", "\n", "# TODO: change this to a tempdir", "\n", "sents_embed_dir", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "'package_data/pytorch_biobert'", ")", "\n", "bert_config", "=", "{", "'bert_train_flag'", ":", "False", ",", "\n", "'bert_all_output'", ":", "False", "}", "\n", "\n", "state_dict_path_form", "=", "'train_validation/question_{}/fold_{}/model_statedict/'", "\n", "config_path_form", "=", "'test/question_{}/fold_0/config/'", "\n", "\n", "default_device", "=", "get_device", "(", "to_gpu", "=", "False", ")", "\n", "\n", "# ---", "\n", "\n", "q_partitions", "=", "create_prediction_qdoc_partitions", "(", "questions", ",", "question_fold_map", ")", "\n", "\n", "# run data processing", "\n", "# USED \"2019-05-02_15-49-09_a0745f9_sent_level_MM.pkl\"", "\n", "html_to_sentence_transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "\n", "html_to_plain_text", "=", "True", ",", "\n", "segment_into", "=", "'sentences'", ",", "\n", "flatten", "=", "True", ",", "\n", "remove_newlines", "=", "False", ",", "# in newer version", "\n", "annotate_html", "=", "True", ",", "\n", "parallelism", "=", "False", ")", "\n", "transformed_data", "=", "html_to_sentence_transformer", ".", "apply", "(", "data_dict", ")", "\n", "\n", "# load BERT model", "\n", "pytorch_dump_path", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "'package_data/pytorch_biobert'", ")", "\n", "bert_for_pretrain", "=", "load_biobert_model", "(", "pytorch_dump_path", ",", "default_device", ")", "\n", "bertmodel", "=", "bert_for_pretrain", ".", "bert", "\n", "\n", "processor", "=", "build_DataDictProcessor", "(", "transformed_data", ",", "vocab_path", ",", "processor_config", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "vocab_path", ",", "do_lower_case", "=", "False", ")", "\n", "\n", "# generate docs data tensor from the articles i.e. instance of class DocDataTensor", "\n", "docs_data_tensor", "=", "processor", ".", "generate_doctensor_from_articles", "(", "tokenizer", ")", "\n", "\n", "# create q_docpartitions", "\n", "q_docpartitions", "=", "{", "}", "\n", "for", "question", "in", "questions", ":", "\n", "        ", "q_docpartitions", ".", "update", "(", "generate_docpartition_per_question", "(", "docs_data_tensor", ",", "q_partitions", ",", "question", ")", ")", "\n", "\n", "# embed sentences", "\n", "", "print", "(", "\"Embedding sentences...\"", ")", "\n", "embed_sentences", "(", "docs_data_tensor", ",", "sents_embed_dir", ",", "bertmodel", ",", "bert_config", ",", "to_gpu", ",", "gpu_index", ")", "\n", "print", "(", "\" ... Finished embedding sentences\"", ")", "\n", "\n", "# load model configs", "\n", "q_fold_config_map", "=", "{", "}", "\n", "for", "q", "in", "questions", ":", "\n", "        ", "config_path", "=", "os", ".", "path", ".", "join", "(", "experiment_model_dir", ",", "config_path_form", ".", "format", "(", "q", ")", ")", "\n", "mconfig", ",", "options", "=", "get_saved_config", "(", "config_path", ")", "\n", "argmax_indx", "=", "-", "1", "\n", "q_fold_config_map", "[", "q", "]", "=", "(", "mconfig", ",", "options", ",", "argmax_indx", ")", "\n", "\n", "# load model state_dicts", "\n", "", "q_state_dict_path_map", "=", "{", "}", "\n", "for", "q", "in", "questions", ":", "\n", "        ", "state_dict_path", "=", "os", ".", "path", ".", "join", "(", "experiment_model_dir", ",", "state_dict_path_form", ".", "format", "(", "q", ",", "question_fold_map", "[", "q", "]", ")", ")", "\n", "q_state_dict_path_map", "[", "q", "]", "=", "state_dict_path", "\n", "\n", "", "print", "(", "\"Running predict\"", ")", "\n", "results", "=", "run_predict", "(", "q_docpartitions", ",", "q_fold_config_map", ",", "bertmodel", ",", "q_state_dict_path_map", ",", "working_dir", ",", "\n", "sents_embed_dir", ",", "question_fold_map", ",", "to_gpu", ",", "gpu_index", ",", "num_epochs", "=", "1", ")", "\n", "\n", "proc_articles_repr", "=", "processor", ".", "articles_repr", "\n", "# TODO: do not run this if monfig['attnmodel_config'] is empty dict", "\n", "# currently if model ran with no attention then 'attention_weight_map' will be {} (i.e. empty dict)", "\n", "for", "q", "in", "results", ":", "\n", "        ", "if", "len", "(", "mconfig", "[", "'attnmodel_config'", "]", ")", "==", "0", ":", "\n", "            ", "results", "[", "q", "]", "[", "'attended_sentences'", "]", "=", "{", "}", "\n", "", "else", ":", "\n", "            ", "results", "[", "q", "]", "[", "'attended_sentences'", "]", "=", "identify_attended_senteces", "(", "results", "[", "q", "]", "[", "'attention_weight_map'", "]", ",", "\n", "proc_articles_repr", ")", "\n", "", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.build_data_dict": [[191, 208], ["pandas.DataFrame"], "function", ["None"], ["", "def", "build_data_dict", "(", "url", ",", "content", ")", ":", "\n", "    ", "\"\"\"Format the html information in a data_dict as required for the prediction routine.\n\n    Args:\n        url: url of the article\n        content: html contents of the article\n\n    Returns: Dict\n    \"\"\"", "\n", "fake_responses", "=", "pd", ".", "DataFrame", "(", "{", "'fake responses'", ":", "[", "0", "]", "*", "5", "}", ")", "\n", "data_dict", "=", "{", "0", ":", "{", "'id'", ":", "0", ",", "\n", "'url'", ":", "url", ",", "\n", "'content'", ":", "content", ",", "\n", "'responses'", ":", "fake_responses", ",", "\n", "}", "\n", "}", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.parse_prediction_results": [[210, 250], ["numpy.exp"], "function", ["None"], ["", "def", "parse_prediction_results", "(", "raw_predictions", ":", "Dict", ")", "->", "Dict", ":", "\n", "    ", "\"\"\"\n    Parses the prediction dict into an easily consumable form. During train/test, predictions are usually made in batch,\n    so the return values are lists. FOr novel predictions, we only have one document to predict, so clean those lists up\n    into values! Also get rid of anything we don't care about showing on the website.\n\n    Args:\n        raw_predictions: a dictionary describing the predictions for a single article. The dictionary consists of 5\n        sub-dictionaries (one for each discern question), each of which contains the following keys:\n        - pred_class: List, containing either 0 or 1\n        - logprob_score_class0: List, containing the log probability for prediction class 0\n        - logprob_score_class1: List, containing the log probability for prediction class 1\n        - attention_weight_map: dictionary of {doc id: tensor}\n        - attended_sentences: dictionary of {doc id: List[attended_sent_dicts]}\n           where each attended_sent_dict is of the form:\n            {str('sentence'): attended_sentence, str('weight'): float }]}\n\n    Returns: A dictionary describing the predictions for a single article. Contains the following keys:\n        - pred_cass: 0 or 1\n        - probability: float\n        - sentences: List[sentences]\n\n    \"\"\"", "\n", "clean_predictions", "=", "{", "}", "\n", "for", "q", "in", "raw_predictions", ":", "\n", "        ", "clean_predictions", "[", "q", "]", "=", "{", "}", "\n", "# remove list wrapper around prediction class", "\n", "clean_predictions", "[", "q", "]", "[", "'pred_class'", "]", "=", "raw_predictions", "[", "q", "]", "[", "'pred_class'", "]", "[", "0", "]", "\n", "\n", "# convert log prob scores to prob, and only report the one associated with the predicted class for simplicity", "\n", "logprog_key", "=", "'logprob_score_class0'", "\n", "if", "clean_predictions", "[", "q", "]", "[", "'pred_class'", "]", "==", "1", ":", "\n", "            ", "logprog_key", "=", "'logprob_score_class1'", "\n", "", "clean_predictions", "[", "q", "]", "[", "'probability'", "]", "=", "np", ".", "exp", "(", "raw_predictions", "[", "q", "]", "[", "logprog_key", "]", "[", "0", "]", ")", "\n", "\n", "# extract attended sentences, ignore weights (their ordering is sufficient information)", "\n", "attended_sentence_dicts", "=", "raw_predictions", "[", "q", "]", "[", "'attended_sentences'", "]", "[", "0", "]", "\n", "clean_predictions", "[", "q", "]", "[", "'sentences'", "]", "=", "[", "sentence_dict", "[", "'sentence'", "]", "for", "sentence_dict", "in", "attended_sentence_dicts", "]", "\n", "\n", "", "return", "clean_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.check_for_metamap_files": [[252, 256], ["pkg_resources.resource_filename", "os.path.exists", "OSError"], "function", ["None"], ["", "def", "check_for_metamap_files", "(", ")", ":", "\n", "    ", "file_path", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "'package_data/public_mm_lite'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "file_path", ")", ":", "\n", "        ", "raise", "OSError", "(", "\"MetaMapLite not found at {}\"", ".", "format", "(", "file_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.check_for_biobert_files": [[258, 269], ["pkg_resources.resource_filename", "os.path.exists", "OSError"], "function", ["None"], ["", "", "def", "check_for_biobert_files", "(", ")", ":", "\n", "    ", "pytorch_biobert_files", "=", "[", "\n", "'bert-base-cased-vocab.txt'", ",", "\n", "'bert_config.json'", ",", "\n", "'biobert_statedict.pkl'", ",", "\n", "]", "\n", "\n", "for", "filename", "in", "pytorch_biobert_files", ":", "\n", "        ", "file_path", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "'package_data/pytorch_biobert/{}'", ".", "format", "(", "filename", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "file_path", ")", ":", "\n", "            ", "raise", "OSError", "(", "\"Required file not found. Please download before continuing: {}\"", ".", "format", "(", "file_path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.check_for_non_git_files": [[271, 277], ["predict_with_neural.check_for_metamap_files", "predict_with_neural.check_for_biobert_files"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.check_for_metamap_files", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.check_for_biobert_files"], ["", "", "", "def", "check_for_non_git_files", "(", "check_metamap", "=", "False", ",", "check_biobert", "=", "True", ")", ":", "\n", "    ", "if", "check_metamap", ":", "\n", "        ", "check_for_metamap_files", "(", ")", "\n", "\n", "", "if", "check_biobert", ":", "\n", "        ", "check_for_biobert_files", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.make_prediction": [[279, 303], ["predict_with_neural.retrieve_page_from_internet", "predict_with_neural.build_data_dict", "predict_with_neural.biobert_predict", "predict_with_neural.parse_prediction_results"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.retrieve_page_from_internet", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.build_data_dict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.biobert_predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.parse_prediction_results"], ["", "", "def", "make_prediction", "(", "url", ":", "str", ",", "exp_dir", "=", "DEFAULT_BIOBERT_EXP_DIR", ",", "question_fold_map", "=", "None", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", "\n", ")", "->", "Dict", ":", "\n", "    ", "\"\"\"\n    End to end function for making an autoDiscern prediction for a given url.\n\n    Args:\n        url: url of the article to make predictions for\n        exp_dir: experiment directory from which to retrieve the trained model.\n        base_dir: the path to the base directory (up to and including including `autodiscern/aa_neural/`.\n        question_fold_map: Dictionary mapping question number to fold number to use for the model.\n        to_gpu: whether to run on GPU.\n        gpu_index: index of gpu to use.\n\n    Returns: autoDiscern predictions for the article\n\n    \"\"\"", "\n", "if", "question_fold_map", "is", "None", ":", "\n", "        ", "question_fold_map", "=", "DEFAULT_QUESTION_FOLD_MAP", "\n", "\n", "", "html_content", "=", "retrieve_page_from_internet", "(", "url", ")", "\n", "data_dict", "=", "build_data_dict", "(", "url", ",", "html_content", ")", "\n", "raw_predictions", "=", "biobert_predict", "(", "data_dict", ",", "QUESTIONS", ",", "exp_dir", ",", "question_fold_map", ",", "to_gpu", ",", "gpu_index", ")", "\n", "clean_predictions", "=", "parse_prediction_results", "(", "raw_predictions", ")", "\n", "return", "clean_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.test_make_prediction": [[305, 330], ["pkg_resources.resource_filename", "predict_with_neural.build_data_dict", "predict_with_neural.biobert_predict", "predict_with_neural.parse_prediction_results", "print", "open", "f.read"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.build_data_dict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.biobert_predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.predict_with_neural.parse_prediction_results"], ["", "def", "test_make_prediction", "(", "exp_dir", "=", "DEFAULT_BIOBERT_EXP_DIR", ",", "question_fold_map", "=", "None", ",", "to_gpu", "=", "DEFAULT_USE_GPU", ",", "gpu_index", "=", "0", "\n", ")", "->", "Dict", ":", "\n", "    ", "\"\"\"\n    End to end test function for making an autoDiscern prediction, without relying on an internet connection.\n    Relies on a the existence of a test.html file.\n\n    Returns: autoDiscern predictions for the article\n\n    \"\"\"", "\n", "\n", "if", "question_fold_map", "is", "None", ":", "\n", "        ", "question_fold_map", "=", "DEFAULT_QUESTION_FOLD_MAP", "\n", "\n", "", "if", "not", "to_gpu", ":", "\n", "        ", "print", "(", "\"WARNING: to_gpu=False is not supported yet\"", ")", "\n", "\n", "", "test_data_path", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "'package_data/test/test.html'", ")", "\n", "test_article_url", "=", "'https://www.nhs.uk/conditions/tendonitis/'", "\n", "\n", "with", "open", "(", "test_data_path", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "html_content", "=", "f", ".", "read", "(", ")", "\n", "", "data_dict", "=", "build_data_dict", "(", "test_article_url", ",", "html_content", ")", "\n", "raw_predictions", "=", "biobert_predict", "(", "data_dict", ",", "QUESTIONS", ",", "exp_dir", ",", "question_fold_map", ",", "to_gpu", ",", "gpu_index", ")", "\n", "clean_predictions", "=", "parse_prediction_results", "(", "raw_predictions", ")", "\n", "return", "clean_predictions", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.DocDataTensor.__init__": [[12, 21], ["docs_batch.size", "dataset.DocDataTensor.indx_doc_map.items"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "docs_batch", ",", "docs_len", ",", "docs_sents_len", ",", "docs_attn_mask", ",", "docs_labels", ",", "indx_doc_map", ")", ":", "\n", "        ", "self", ".", "docs_batch", "=", "docs_batch", "# tensor.int64, (docs, num_sents, sents_len)", "\n", "self", ".", "docs_len", "=", "docs_len", "# tensor.int16 (docs,), number of sentences in each doc", "\n", "self", ".", "docs_sents_len", "=", "docs_sents_len", "# tensor.int16, (docs, num_sents)", "\n", "self", ".", "docs_attn_mask", "=", "docs_attn_mask", "# tensor.uint8, (docs, num_sents, sents_len)", "\n", "self", ".", "docs_labels", "=", "docs_labels", "# tensor.uint8, (docs, num_questions)", "\n", "self", ".", "indx_doc_map", "=", "indx_doc_map", "# dict, {indx:doc_id}", "\n", "self", ".", "doc_indx_map", "=", "{", "doc_id", ":", "indx", "for", "indx", ",", "doc_id", "in", "self", ".", "indx_doc_map", ".", "items", "(", ")", "}", "# dict, indx_doc_map reversed", "\n", "self", ".", "num_samples", "=", "docs_batch", ".", "size", "(", "0", ")", "# int, number of docs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.DocDataTensor.__getitem__": [[22, 26], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "indx", ")", ":", "\n", "\n", "        ", "return", "(", "self", ".", "docs_batch", "[", "indx", "]", ",", "self", ".", "docs_len", "[", "indx", "]", ",", "self", ".", "docs_sents_len", "[", "indx", "]", ",", "self", ".", "docs_attn_mask", "[", "indx", "]", ",", "\n", "self", ".", "docs_labels", "[", "indx", "]", ",", "self", ".", "indx_doc_map", "[", "indx", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.DocDataTensor.__len__": [[27, 29], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "num_samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.PartitionDataTensor.__init__": [[33, 39], ["len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "doc_data_tensor", ",", "partition_ids", ",", "dsettype", ",", "fold_num", ")", ":", "\n", "        ", "self", ".", "docs_data_tensor", "=", "doc_data_tensor", "# instance of :class:`DocDataTensor`", "\n", "self", ".", "partition_ids", "=", "partition_ids", "# list of doc ids", "\n", "self", ".", "dsettype", "=", "dsettype", "# string, dataset type (i.e. train, validation, test)", "\n", "self", ".", "fold_num", "=", "fold_num", "# int, fold number", "\n", "self", ".", "num_samples", "=", "len", "(", "self", ".", "partition_ids", ")", "# int, number of docs in the partition", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.PartitionDataTensor.__getitem__": [[40, 44], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "indx", ")", ":", "\n", "        ", "doc_id", "=", "self", ".", "partition_ids", "[", "indx", "]", "\n", "upd_indx", "=", "self", ".", "docs_data_tensor", ".", "doc_indx_map", "[", "doc_id", "]", "\n", "return", "self", ".", "docs_data_tensor", "[", "upd_indx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.PartitionDataTensor.__len__": [[45, 47], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "num_samples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.construct_load_dataloaders": [[49, 92], ["torch.utils.data.DataLoader", "utilities.ModelScore", "os.path.join"], "function", ["None"], ["", "", "def", "construct_load_dataloaders", "(", "dataset_fold", ",", "dsettypes", ",", "config", ",", "wrk_dir", ")", ":", "\n", "    ", "\"\"\"construct dataloaders for the dataset\n\n       Args:\n            dataset_fold: dictionary,\n                          example: {'train': <neural.dataset.PartitionDataTensor at 0x1cec95c96a0>,\n                                    'validation': <neural.dataset.PartitionDataTensor at 0x1cec95c9208>,\n                                    'test': <neural.dataset.PartitionDataTensor at 0x1cec95c9240>,\n                                    'class_weights': tensor([0.6957, 1.7778])\n                                   }\n            dsettype: list, ['train', 'validation', 'test']\n            config: dict, {'batch_size': int, 'num_workers': int}\n            wrk_dir: string, folder path\n    \"\"\"", "\n", "\n", "# setup data loaders", "\n", "data_loaders", "=", "{", "}", "\n", "epoch_loss_avgbatch", "=", "{", "}", "\n", "epoch_loss_avgsamples", "=", "{", "}", "\n", "flog_out", "=", "{", "}", "\n", "score_dict", "=", "{", "}", "\n", "class_weights", "=", "{", "}", "\n", "for", "dsettype", "in", "dsettypes", ":", "\n", "        ", "if", "(", "dsettype", "==", "'train'", ")", ":", "\n", "            ", "shuffle", "=", "True", "\n", "class_weights", "[", "dsettype", "]", "=", "dataset_fold", "[", "'class_weights'", "]", "\n", "", "else", ":", "\n", "            ", "shuffle", "=", "False", "\n", "class_weights", "[", "dsettype", "]", "=", "None", "\n", "", "data_loaders", "[", "dsettype", "]", "=", "DataLoader", "(", "dataset_fold", "[", "dsettype", "]", ",", "\n", "batch_size", "=", "config", "[", "'batch_size'", "]", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_workers", "=", "config", "[", "'num_workers'", "]", ")", "\n", "\n", "epoch_loss_avgbatch", "[", "dsettype", "]", "=", "[", "]", "\n", "epoch_loss_avgsamples", "[", "dsettype", "]", "=", "[", "]", "\n", "score_dict", "[", "dsettype", "]", "=", "ModelScore", "(", "0", ",", "0.0", ",", "0.0", ",", "0.0", ",", "0.0", ")", "# (best_epoch, micro_f1, macro_f1, accuracy, auc)", "\n", "if", "(", "wrk_dir", ")", ":", "\n", "            ", "flog_out", "[", "dsettype", "]", "=", "os", ".", "path", ".", "join", "(", "wrk_dir", ",", "dsettype", "+", "\".log\"", ")", "\n", "", "else", ":", "\n", "            ", "flog_out", "[", "dsettype", "]", "=", "None", "\n", "\n", "", "", "return", "(", "data_loaders", ",", "epoch_loss_avgbatch", ",", "epoch_loss_avgsamples", ",", "score_dict", ",", "class_weights", ",", "flog_out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.get_stratified_partitions": [[94, 138], ["sklearn.model_selection.StratifiedKFold", "sklearn.model_selection.StratifiedShuffleSplit", "numpy.vectorize", "print", "docs_labels[].numpy", "numpy.zeros", "sklearn.model_selection.StratifiedKFold.split", "print", "len", "numpy.zeros", "np.vectorize.", "np.vectorize.", "sklearn.model_selection.StratifiedShuffleSplit.split", "print", "print", "dataset.report_label_distrib", "print", "dataset.report_label_distrib", "print", "dataset.report_label_distrib", "print", "len", "list", "list"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.report_label_distrib", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.report_label_distrib", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.report_label_distrib"], ["", "def", "get_stratified_partitions", "(", "docs_data_tensor", ",", "questions", "=", "(", "4", ",", "5", ",", "9", ",", "10", ",", "11", ")", ",", "num_folds", "=", "5", ",", "valid_set_portion", "=", "0.1", ",", "\n", "random_state", "=", "42", ")", ":", "\n", "    ", "\"\"\"Generate 5-fold stratified sample of document ids based on the question label\n\n    Args:\n        docs_data_tensor: instance of :class:`DocDataTensor`\n    \"\"\"", "\n", "skf_trte", "=", "StratifiedKFold", "(", "n_splits", "=", "num_folds", ",", "random_state", "=", "random_state", ",", "shuffle", "=", "True", ")", "# split train and test", "\n", "# split train and validation", "\n", "sss_trv", "=", "StratifiedShuffleSplit", "(", "n_splits", "=", "1", ",", "random_state", "=", "random_state", ",", "test_size", "=", "valid_set_portion", ")", "\n", "docs_labels", "=", "docs_data_tensor", ".", "docs_labels", "# tensor (docs, num_questions)", "\n", "get_docs_id", "=", "np", ".", "vectorize", "(", "docs_data_tensor", ".", "indx_doc_map", ".", "get", ")", "# vectorized lookup", "\n", "q_partitions", "=", "{", "}", "\n", "for", "question", "in", "questions", ":", "\n", "        ", "print", "(", "\"question\"", ",", "question", ")", "\n", "q_partitions", "[", "question", "]", "=", "{", "}", "\n", "question_indx", "=", "question", "-", "1", "# indexing starts from 0", "\n", "q_labels", "=", "docs_labels", "[", ":", ",", "question_indx", "]", ".", "numpy", "(", ")", "# all labels", "\n", "x", "=", "np", ".", "zeros", "(", "len", "(", "q_labels", ")", ")", "# placeholder for compatibility reasons", "\n", "fold_count", "=", "0", "\n", "for", "train_index", ",", "test_index", "in", "skf_trte", ".", "split", "(", "x", ",", "q_labels", ")", ":", "\n", "# further split train_set to 90% train and 10% validation", "\n", "            ", "q_train_labels", "=", "q_labels", "[", "train_index", "]", "\n", "x_train", "=", "np", ".", "zeros", "(", "len", "(", "q_train_labels", ")", ")", "\n", "train_doc_ids", "=", "get_docs_id", "(", "train_index", ")", "\n", "test_doc_ids", "=", "get_docs_id", "(", "test_index", ")", "\n", "\n", "for", "tr_index", ",", "validation_index", "in", "sss_trv", ".", "split", "(", "x_train", ",", "q_train_labels", ")", ":", "# loop runs once", "\n", "                ", "tr_doc_ids", "=", "train_doc_ids", "[", "list", "(", "tr_index", ")", "]", "\n", "val_doc_ids", "=", "train_doc_ids", "[", "list", "(", "validation_index", ")", "]", "\n", "q_partitions", "[", "question", "]", "[", "fold_count", "]", "=", "{", "'train'", ":", "tr_doc_ids", ",", "\n", "'validation'", ":", "val_doc_ids", ",", "\n", "'test'", ":", "test_doc_ids", "}", "\n", "", "print", "(", "\"fold_num:\"", ",", "fold_count", ")", "\n", "print", "(", "'train data'", ")", "\n", "report_label_distrib", "(", "q_train_labels", "[", "tr_index", "]", ")", "\n", "print", "(", "'validation data'", ")", "\n", "report_label_distrib", "(", "q_train_labels", "[", "validation_index", "]", ")", "\n", "print", "(", "'test data'", ")", "\n", "report_label_distrib", "(", "q_labels", "[", "test_index", "]", ")", "\n", "print", "(", ")", "\n", "fold_count", "+=", "1", "\n", "", "print", "(", "\"-\"", "*", "25", ")", "\n", "", "return", "(", "q_partitions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.report_label_distrib": [[140, 145], ["numpy.unique", "enumerate", "counts.sum", "print"], "function", ["None"], ["", "def", "report_label_distrib", "(", "labels", ")", ":", "\n", "    ", "classes", ",", "counts", "=", "np", ".", "unique", "(", "labels", ",", "return_counts", "=", "True", ")", "\n", "norm_counts", "=", "counts", "/", "counts", ".", "sum", "(", ")", "\n", "for", "i", ",", "label", "in", "enumerate", "(", "classes", ")", ":", "\n", "        ", "print", "(", "\"class:\"", ",", "label", ",", "\"norm count:\"", ",", "norm_counts", "[", "i", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.validate_partitions": [[147, 186], ["print", "print", "isinstance", "set", "set", "print", "print", "print", "set().intersection", "set().intersection", "set().intersection", "print", "print", "print", "set().union().union", "test_set_accum.union.union", "len", "len", "len", "len", "len", "len", "numpy.abs", "numpy.abs", "len", "set", "set", "set", "len", "set().union", "len", "len", "set"], "function", ["None"], ["", "", "def", "validate_partitions", "(", "q_partitions", ",", "docs_id", ",", "valid_set_portion", "=", "0.1", ",", "test_set_portion", "=", "0.2", ",", "num_docs", "=", "269", ")", ":", "\n", "    ", "if", "(", "not", "isinstance", "(", "docs_id", ",", "set", ")", ")", ":", "\n", "        ", "docs_id", "=", "set", "(", "docs_id", ")", "\n", "", "print", "(", "\"validation partitions\"", ")", "\n", "for", "question", "in", "q_partitions", ":", "\n", "        ", "test_set_accum", "=", "set", "(", "[", "]", ")", "\n", "print", "(", "'question'", ",", "question", ")", "\n", "for", "fold_num", "in", "q_partitions", "[", "question", "]", ":", "\n", "            ", "print", "(", "'fold_num'", ",", "fold_num", ")", "\n", "tr_ids", "=", "q_partitions", "[", "question", "]", "[", "fold_num", "]", "[", "'train'", "]", "\n", "val_ids", "=", "q_partitions", "[", "question", "]", "[", "fold_num", "]", "[", "'validation'", "]", "\n", "te_ids", "=", "q_partitions", "[", "question", "]", "[", "fold_num", "]", "[", "'test'", "]", "\n", "\n", "tr_val", "=", "set", "(", "tr_ids", ")", ".", "intersection", "(", "val_ids", ")", "\n", "tr_te", "=", "set", "(", "tr_ids", ")", ".", "intersection", "(", "te_ids", ")", "\n", "te_val", "=", "set", "(", "te_ids", ")", ".", "intersection", "(", "val_ids", ")", "\n", "\n", "tr_size", "=", "len", "(", "tr_ids", ")", "+", "len", "(", "val_ids", ")", "\n", "num_docs", "=", "tr_size", "+", "len", "(", "te_ids", ")", "\n", "print", "(", "'expected validation set size:'", ",", "valid_set_portion", "*", "tr_size", ",", "'; actual validation set size:'", ",", "\n", "len", "(", "val_ids", ")", ")", "\n", "print", "(", "'expected test set size:'", ",", "test_set_portion", "*", "num_docs", ",", "'; actual test set size:'", ",", "len", "(", "te_ids", ")", ")", "\n", "print", "(", ")", "\n", "assert", "np", ".", "abs", "(", "valid_set_portion", "*", "tr_size", "-", "len", "(", "val_ids", ")", ")", "<=", "2", "# valid difference range", "\n", "assert", "np", ".", "abs", "(", "test_set_portion", "*", "num_docs", "-", "len", "(", "te_ids", ")", ")", "<=", "2", "\n", "# assert there is no overlap among train, val and test partition within a fold", "\n", "for", "s", "in", "(", "tr_val", ",", "tr_te", ",", "te_val", ")", ":", "\n", "                ", "assert", "len", "(", "s", ")", "==", "0", "\n", "\n", "", "s_union", "=", "set", "(", "tr_ids", ")", ".", "union", "(", "val_ids", ")", ".", "union", "(", "te_ids", ")", "\n", "assert", "len", "(", "s_union", ")", "==", "num_docs", "\n", "test_set_accum", "=", "test_set_accum", ".", "union", "(", "te_ids", ")", "\n", "", "print", "(", "'-'", "*", "25", ")", "\n", "# verify that assembling test sets from each of the five folds would be equivalent to all doc_ids", "\n", "assert", "len", "(", "test_set_accum", ")", "==", "num_docs", "\n", "assert", "test_set_accum", "==", "docs_id", "\n", "", "print", "(", "\"passed intersection and overlap test (i.e. train, validation and test sets are not\"", ",", "\n", "\"intersecting in each fold and the concatenation of test sets from each fold is\"", ",", "\n", "\"equivalent to the whole dataset)\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.validate_doc_tensor_repr": [[188, 210], ["print", "print", "range", "print", "tokenizer.convert_ids_to_tokens", "print", "print", "list", "docs_batch[].numpy"], "function", ["None"], ["", "def", "validate_doc_tensor_repr", "(", "doc_id", ",", "doc_data_tensor", ",", "processor", ",", "tokenizer", ")", ":", "\n", "    ", "docs_batch", "=", "doc_data_tensor", ".", "docs_batch", "\n", "doc_indx", "=", "doc_data_tensor", ".", "doc_indx_map", "[", "doc_id", "]", "\n", "num_sents", "=", "doc_data_tensor", ".", "docs_len", "[", "doc_indx", "]", "\n", "print", "(", "'num_sents:'", ",", "num_sents", ")", "\n", "print", "(", "'doc_id:'", ",", "doc_id", ")", "\n", "assert", "num_sents", "==", "processor", ".", "articles_repr", "[", "doc_id", "]", "[", "'num_sents'", "]", "\n", "pass_flag", "=", "False", "# record if we enter the loop", "\n", "for", "sent_indx", "in", "range", "(", "num_sents", ")", ":", "\n", "        ", "sent_len", "=", "doc_data_tensor", ".", "docs_sents_len", "[", "doc_indx", ",", "sent_indx", "]", "\n", "a", "=", "tokenizer", ".", "convert_ids_to_tokens", "(", "list", "(", "docs_batch", "[", "doc_indx", ",", "sent_indx", ",", ":", "sent_len", "]", ".", "numpy", "(", ")", ")", ")", "\n", "b", "=", "processor", ".", "articles_repr", "[", "doc_id", "]", "[", "'sents_tok'", "]", "[", "sent_indx", "]", "\n", "assert", "a", "==", "b", "\n", "# print(a)", "\n", "# print(b)", "\n", "pass_flag", "=", "True", "\n", "\n", "", "if", "(", "pass_flag", ")", ":", "\n", "        ", "print", "(", "\"passed!\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"failed!\"", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.generate_docpartition_per_question": [[212, 222], ["dataset.PartitionDataTensor"], "function", ["None"], ["", "def", "generate_docpartition_per_question", "(", "doc_data_tensor", ",", "q_partitions", ",", "question", ")", ":", "\n", "    ", "q_docpartitions", "=", "{", "question", ":", "{", "}", "}", "\n", "target_q_partitions", "=", "q_partitions", "[", "question", "]", "\n", "for", "fold_num", "in", "target_q_partitions", ":", "\n", "        ", "q_docpartitions", "[", "question", "]", "[", "fold_num", "]", "=", "{", "}", "\n", "for", "dsettype", "in", "target_q_partitions", "[", "fold_num", "]", ":", "\n", "            ", "target_ids", "=", "target_q_partitions", "[", "fold_num", "]", "[", "dsettype", "]", "\n", "doc_partition", "=", "PartitionDataTensor", "(", "doc_data_tensor", ",", "target_ids", ",", "dsettype", ",", "fold_num", ")", "\n", "q_docpartitions", "[", "question", "]", "[", "fold_num", "]", "[", "dsettype", "]", "=", "doc_partition", "\n", "", "", "return", "(", "q_docpartitions", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.validate_q_docpartitions": [[224, 235], ["print", "print", "print", "print", "numpy.all", "id", "numpy.equal", "numpy.array", "numpy.array"], "function", ["None"], ["", "def", "validate_q_docpartitions", "(", "q_docpartitions", ",", "q_partitions", ")", ":", "\n", "    ", "for", "question", "in", "q_docpartitions", ":", "\n", "        ", "print", "(", "'question '", ",", "question", ")", "\n", "for", "fold_num", "in", "q_docpartitions", "[", "question", "]", ":", "\n", "            ", "print", "(", "'fold_num'", ",", "fold_num", ")", "\n", "for", "dsettype", "in", "q_docpartitions", "[", "question", "]", "[", "fold_num", "]", ":", "\n", "                ", "pdtensor", "=", "q_docpartitions", "[", "question", "]", "[", "fold_num", "]", "[", "dsettype", "]", "# partition data tensor instance", "\n", "print", "(", "id", "(", "pdtensor", ".", "docs_data_tensor", ")", ")", "\n", "assert", "np", ".", "all", "(", "np", ".", "equal", "(", "np", ".", "array", "(", "pdtensor", ".", "partition_ids", ")", ",", "\n", "np", ".", "array", "(", "q_partitions", "[", "question", "]", "[", "fold_num", "]", "[", "dsettype", "]", ")", ")", ")", "\n", "", "", "", "print", "(", "\"passed test!!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.compute_class_weights": [[237, 243], ["numpy.unique", "sklearn.utils.class_weight.compute_class_weight", "labels_tensor.numpy"], "function", ["None"], ["", "def", "compute_class_weights", "(", "labels_tensor", ")", ":", "\n", "    ", "classes", ",", "counts", "=", "np", ".", "unique", "(", "labels_tensor", ",", "return_counts", "=", "True", ")", "\n", "# print(\"classes\", classes)", "\n", "# print(\"counts\", counts)", "\n", "class_weights", "=", "compute_class_weight", "(", "'balanced'", ",", "classes", ",", "labels_tensor", ".", "numpy", "(", ")", ")", "\n", "return", "class_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.compute_class_weights_per_fold_": [[245, 277], ["torch.empty().fill_", "range", "torch.from_numpy().float", "len", "out.item", "torch.empty", "torch.from_numpy", "len", "dataset.compute_class_weights"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.compute_class_weights"], ["", "def", "compute_class_weights_per_fold_", "(", "q_docpartitions", ")", ":", "\n", "    ", "\"\"\"computes inverse class weights and updates the passed dictionary\n\n    Args:\n        q_docpartitions: dictionary, {question, int: {fold_num, int: {datasettype, string:{qdocpartition, instance of\n        :class:`PartitionDataTensor`}}}}\n\n    Example:\n        q_docpartitions\n            {4: {0: {'train': <neural.dataset.PartitionDataTensor at 0x1cec95c96a0>,\n                    'validation': <neural.dataset.PartitionDataTensor at 0x1cec95c9208>,\n                    'test': <neural.dataset.PartitionDataTensor at 0x1cec95c9240>\n                    }, ..\n            }\n        is updated after computation of class weights to\n            {4: {0: {'train': <neural.dataset.PartitionDataTensor at 0x1cec95c96a0>,\n                    'validation': <neural.dataset.PartitionDataTensor at 0x1cec95c9208>,\n                    'test': <neural.dataset.PartitionDataTensor at 0x1cec95c9240>,\n                    'class_weights': tensor([0.6957, 1.7778]),\n                    }, ..\n            }\n    \"\"\"", "\n", "labels_pos", "=", "4", "# position of labels in the returned tuple when indexing PartitionDataTensor", "\n", "for", "q", "in", "q_docpartitions", ":", "\n", "        ", "q_indx", "=", "q", "-", "1", "\n", "for", "fold_num", "in", "q_docpartitions", "[", "q", "]", ":", "# looping over the numbered folds", "\n", "            ", "pdoc", "=", "q_docpartitions", "[", "q", "]", "[", "fold_num", "]", "[", "'train'", "]", "\n", "q_scores", "=", "torch", ".", "empty", "(", "len", "(", "pdoc", ")", ",", "dtype", "=", "torch", ".", "float32", ")", ".", "fill_", "(", "-", "1.0", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "pdoc", ")", ")", ":", "\n", "                ", "out", "=", "pdoc", "[", "i", "]", "[", "labels_pos", "]", "[", "q_indx", "]", "\n", "q_scores", "[", "i", "]", "=", "out", ".", "item", "(", ")", "\n", "", "q_docpartitions", "[", "q", "]", "[", "fold_num", "]", "[", "'class_weights'", "]", "=", "torch", ".", "from_numpy", "(", "compute_class_weights", "(", "q_scores", ")", ")", ".", "float", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor.__init__": [[9, 21], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "'''\n        Args\n            config: dict, specifying options\n                - torch_device: instance of torch.device -- ommitting it for now\n                - label_cutoff: int, from 1 to 5\n                - label_avgmethod: string, {round_mean, floor_mean}\n                - tokenizer_max_sent_len: int, in range(1, 512)\n        '''", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "articles_repr", "=", "None", "\n", "self", ".", "articles_dict", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor.get_artid_label_map": [[22, 37], ["[].values.mean", "numpy.round", "numpy.floor"], "methods", ["None"], ["", "def", "get_artid_label_map", "(", "self", ",", "article_id", ",", "cutoff", "=", "3", ",", "method", "=", "'round_mean'", ")", ":", "\n", "        ", "articles_dict", "=", "self", ".", "articles_dict", "\n", "# every sentence has same labels associated with the doc comprising them", "\n", "access_key", "=", "\"{}-{}\"", ".", "format", "(", "article_id", ",", "0", ")", "\n", "labels", "=", "articles_dict", "[", "access_key", "]", "[", "'responses'", "]", ".", "values", ".", "mean", "(", "axis", "=", "1", ")", "\n", "if", "(", "method", "==", "'round_mean'", ")", ":", "\n", "            ", "labels", "=", "np", ".", "round", "(", "labels", ")", "\n", "", "elif", "(", "method", "==", "'floor_mean'", ")", ":", "\n", "            ", "labels", "=", "np", ".", "floor", "(", "labels", ")", "\n", "\n", "", "mask", "=", "labels", ">=", "cutoff", "\n", "labels", "[", "mask", "]", "=", "1", "\n", "labels", "[", "~", "mask", "]", "=", "0", "\n", "\n", "return", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor.generate_doctensor_from_articles": [[38, 90], ["list", "list.sort", "data_processor.DataDictProcessor.config.get", "data_processor.DataDictProcessor.config.get", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.nn.utils.rnn.pad_sequence", "torch.cat", "torch.tensor", "dataset.DocDataTensor", "articles_repr.keys", "data_processor.DataDictProcessor._generate_doctensor_from_article", "torch.nn.utils.rnn.pad_sequence.append", "torch.nn.utils.rnn.pad_sequence.append", "torch.nn.utils.rnn.pad_sequence.append", "data_processor.DataDictProcessor.get_artid_label_map", "torch.from_numpy().unsqueeze().type", "labels.append", "torch.tensor.append", "enumerate", "torch.from_numpy().unsqueeze", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor._generate_doctensor_from_article", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor.get_artid_label_map"], ["", "def", "generate_doctensor_from_articles", "(", "self", ",", "tokenizer", ")", ":", "\n", "        ", "'''Generate tensor representation of the docs in the processed data dictionary\n\n        Args:\n            tokenizer: instance of :class:`BertTokenizer`\n\n\n        .. Note::\n\n            this function is called after having `self.articles_repr` created using :func:`self.generate_articles_repr`\n            or setting it through :func:`self.set_instance_attr`\n        '''", "\n", "articles_repr", "=", "self", ".", "articles_repr", "\n", "docs_batch", "=", "[", "]", "\n", "docs_sents_len", "=", "[", "]", "\n", "docs_attn_mask", "=", "[", "]", "\n", "docs_len", "=", "[", "]", "\n", "articles_id", "=", "list", "(", "articles_repr", ".", "keys", "(", ")", ")", "\n", "# sort for keeping the order when we map the doc_ids to sentences tensor", "\n", "articles_id", ".", "sort", "(", ")", "\n", "indx_doc_map", "=", "{", "indx", ":", "art_id", "for", "indx", ",", "art_id", "in", "enumerate", "(", "articles_id", ")", "}", "\n", "cutoff", "=", "self", ".", "config", ".", "get", "(", "'label_cutoff'", ",", "3", ")", "\n", "avg_method", "=", "self", ".", "config", ".", "get", "(", "'label_avgmethod'", ",", "'round_mean'", ")", "\n", "labels", "=", "[", "]", "\n", "\n", "for", "doc_id", "in", "articles_id", ":", "\n", "            ", "article_repr", "=", "articles_repr", "[", "doc_id", "]", "\n", "batched_sents_ids", ",", "sents_tok", ",", "sents_len", ",", "attn_mask", "=", "self", ".", "_generate_doctensor_from_article", "(", "article_repr", ",", "\n", "tokenizer", ")", "\n", "docs_batch", ".", "append", "(", "batched_sents_ids", ")", "\n", "docs_sents_len", ".", "append", "(", "sents_len", ")", "\n", "docs_attn_mask", ".", "append", "(", "attn_mask", ")", "\n", "self", ".", "articles_repr", "[", "doc_id", "]", "[", "'sents_tok'", "]", "=", "sents_tok", "\n", "# get labels for questions", "\n", "responses", "=", "self", ".", "get_artid_label_map", "(", "doc_id", ",", "cutoff", "=", "cutoff", ",", "method", "=", "avg_method", ")", "\n", "# turn numpy array into torch.Tensor with 1xC dimension where C is len(labels)", "\n", "responses", "=", "torch", ".", "from_numpy", "(", "responses", ")", ".", "unsqueeze", "(", "0", ")", ".", "type", "(", "torch", ".", "uint8", ")", "\n", "labels", ".", "append", "(", "responses", ")", "\n", "docs_len", ".", "append", "(", "article_repr", "[", "'num_sents'", "]", ")", "\n", "\n", "# (docs, num_sents, max_sent_len)", "\n", "", "docs_batch", "=", "pad_sequence", "(", "docs_batch", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "# (docs, num_sents)", "\n", "docs_sents_len", "=", "pad_sequence", "(", "docs_sents_len", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "# (docs, num_sents, max_sent_len)", "\n", "docs_attn_mask", "=", "pad_sequence", "(", "docs_attn_mask", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "# (docs, num_questions)", "\n", "docs_labels", "=", "torch", ".", "cat", "(", "labels", ",", "dim", "=", "0", ")", "\n", "# (docs, )", "\n", "docs_len", "=", "torch", ".", "tensor", "(", "docs_len", ",", "dtype", "=", "torch", ".", "int16", ")", "\n", "\n", "return", "DocDataTensor", "(", "docs_batch", ",", "docs_len", ",", "docs_sents_len", ",", "docs_attn_mask", ",", "docs_labels", ",", "indx_doc_map", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor._generate_doctensor_from_article": [[91, 132], ["data_processor.DataDictProcessor.config.get", "torch.zeros", "enumerate", "torch.nn.utils.rnn.pad_sequence", "torch.tensor", "torch.tensor", "tokenizer.tokenize", "sents_tok.append", "len", "torch.tensor.append", "tokenizer.convert_tokens_to_ids", "sents_ids.append", "torch.tensor"], "methods", ["None"], ["", "def", "_generate_doctensor_from_article", "(", "self", ",", "article_repr", ",", "tokenizer", ")", ":", "\n", "        ", "'''generate tensor representation of a processed article'''", "\n", "\n", "sents_tok", "=", "[", "]", "\n", "sents_len", "=", "[", "]", "\n", "max_sent_len", "=", "self", ".", "config", ".", "get", "(", "'tokenizer_max_sent_len'", ",", "256", ")", "\n", "if", "(", "max_sent_len", ">", "510", ")", ":", "# BERT tokenizer has max len equal to 512 (i.e. number of tokens)", "\n", "            ", "max_sent_len", "=", "510", "# make sure to leave two tokens for the CLS and SEP", "\n", "\n", "# placeholder when padding sentences with different lengths", "\n", "", "sents_ids", "=", "[", "torch", ".", "tensor", "(", "[", "0", "]", "*", "(", "max_sent_len", "+", "2", ")", ",", "dtype", "=", "torch", ".", "long", ")", "]", "\n", "num_sents", "=", "article_repr", "[", "'num_sents'", "]", "\n", "attn_mask", "=", "torch", ".", "zeros", "(", "(", "num_sents", ",", "max_sent_len", "+", "2", ")", ",", "dtype", "=", "torch", ".", "uint8", ")", "# binary mask", "\n", "\n", "for", "sent_indx", ",", "sent", "in", "enumerate", "(", "article_repr", "[", "'sents'", "]", ")", ":", "\n", "# sandw_sent = '[CLS] ' + sent + ' [SEP]'", "\n", "# toks = tokenizer.tokenize(sandw_sent)", "\n", "\n", "            ", "toks", "=", "tokenizer", ".", "tokenize", "(", "sent", ")", "\n", "# sandwich the sentence with [CLS] and [SEP] symbols", "\n", "toks", "=", "[", "'[CLS]'", "]", "+", "toks", "[", ":", "max_sent_len", "]", "+", "[", "'[SEP]'", "]", "\n", "sents_tok", ".", "append", "(", "toks", ")", "\n", "num_toks", "=", "len", "(", "toks", ")", "\n", "attn_mask", "[", "sent_indx", ",", ":", "num_toks", "]", "=", "1", "# set 1 for tokens that are not padding", "\n", "sents_len", ".", "append", "(", "num_toks", ")", "\n", "toks_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "toks", ")", "\n", "sents_ids", ".", "append", "(", "torch", ".", "tensor", "(", "toks_ids", ",", "dtype", "=", "torch", ".", "long", ")", ")", "\n", "# TODO: intervene here with BertModel to generate embedded representation (i.e. as feature extractor)", "\n", "\n", "# padd sequences to obtain (sents, max_sent_len); sents here refers to number of sents in the article", "\n", "# padd sequences to get BxTx* shape", "\n", "", "batched_sents_ids", "=", "pad_sequence", "(", "sents_ids", ",", "batch_first", "=", "True", ",", "padding_value", "=", "0", ")", "\n", "# remove first sentence placeholder", "\n", "batched_sents_ids", "=", "batched_sents_ids", "[", "1", ":", ",", ":", "]", "\n", "# print(batched_sents_ids, '\\n', batched_sents_ids.size())", "\n", "\n", "# tensorize!!", "\n", "# (batch_size, max_sent_len)", "\n", "sents_len", "=", "torch", ".", "tensor", "(", "sents_len", ",", "dtype", "=", "torch", ".", "int16", ")", "# (num_sents,)", "\n", "\n", "return", "batched_sents_ids", ",", "sents_tok", ",", "sents_len", ",", "attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor.set_instance_attr": [[133, 137], ["None"], "methods", ["None"], ["", "def", "set_instance_attr", "(", "self", ",", "articles_repr", ",", "articles_dict", ",", "config", ")", ":", "\n", "        ", "self", ".", "articles_repr", "=", "articles_repr", "\n", "self", ".", "articles_dict", "=", "articles_dict", "\n", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor.generate_articles_repr": [[138, 150], ["int", "data_processor.DataDictProcessor._generate_article_repr", "articles_repr.update", "articles_dict.update", "artsent_id.split"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor._generate_article_repr", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "def", "generate_articles_repr", "(", "self", ",", "data_dict", ")", ":", "\n", "        ", "articles_repr", "=", "{", "}", "\n", "articles_dict", "=", "{", "}", "\n", "# get articles id", "\n", "articles_id", "=", "{", "int", "(", "artsent_id", ".", "split", "(", "'-'", ")", "[", "0", "]", ")", "for", "artsent_id", "in", "data_dict", "}", "\n", "for", "article_id", "in", "articles_id", ":", "\n", "            ", "article_dict", ",", "article_repr", "=", "self", ".", "_generate_article_repr", "(", "article_id", ",", "data_dict", ")", "\n", "articles_repr", ".", "update", "(", "article_repr", ")", "\n", "articles_dict", ".", "update", "(", "article_dict", ")", "\n", "# set the instance variables", "\n", "", "self", ".", "articles_repr", "=", "articles_repr", "\n", "self", ".", "articles_dict", "=", "articles_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor._generate_article_repr": [[151, 200], ["print", "copy.deepcopy", "accum_sent_lst.append", "print", "len"], "methods", ["None"], ["", "def", "_generate_article_repr", "(", "self", ",", "article_id", ",", "data_dict", ")", ":", "\n", "        ", "'''Remove pseudo-sentences (i.e. ones that have only full-stop or one character) and generate new representaiton\n         from a parsed article\n\n        Args:\n            article_id: int/string, representing article/doc number\n            data_dict: dict, pre-processed representation of articles/docs\n\n        Returns:\n            article_dict: dict, updated/cleaned version data_dict for the specified article id\n            article_info: dict, {article_id:{'content': article text,\n                                             'sents': list of sentences,\n                                             'num_sents': number of sentences}}\n\n        .. Note::\n            Period or single character senteneces are generally generated due to having links/images that have no\n            content between the tags\n        '''", "\n", "counter", "=", "0", "\n", "article_info", "=", "{", "article_id", ":", "{", "}", "}", "\n", "article_dict", "=", "{", "}", "\n", "accum_sent_lst", "=", "[", "]", "\n", "upd_counter", "=", "0", "\n", "# using this loop with exception is in order to sequentially access the article's sentences in the correct order", "\n", "while", "True", ":", "\n", "            ", "try", ":", "\n", "                ", "sent", "=", "data_dict", "[", "'{}-{}'", ".", "format", "(", "article_id", ",", "counter", ")", "]", "[", "'content'", "]", "\n", "if", "(", "sent", "==", "'.'", "or", "len", "(", "sent", ")", "==", "1", ")", ":", "\n", "                    ", "print", "(", "\"Removing: \"", ",", "sent", ")", "\n", "", "else", ":", "\n", "                    ", "dict_key", "=", "'{}-{}'", ".", "format", "(", "article_id", ",", "upd_counter", ")", "\n", "article_dict", "[", "dict_key", "]", "=", "deepcopy", "(", "data_dict", "[", "'{}-{}'", ".", "format", "(", "article_id", ",", "counter", ")", "]", ")", "\n", "curr_dict", "=", "article_dict", "[", "'{}-{}'", ".", "format", "(", "article_id", ",", "upd_counter", ")", "]", "\n", "# update sub_id", "\n", "curr_dict", "[", "'sub_id'", "]", "=", "upd_counter", "\n", "# update metamap index info", "\n", "if", "(", "'metamap_detail'", "in", "curr_dict", ")", ":", "\n", "                        ", "for", "metamap_dict", "in", "curr_dict", "[", "'metamap_detail'", "]", ":", "\n", "                            ", "metamap_dict", "[", "'index'", "]", "=", "\"'{}-{}'\"", ".", "format", "(", "article_id", ",", "upd_counter", ")", "\n", "", "", "accum_sent_lst", ".", "append", "(", "sent", ")", "\n", "upd_counter", "+=", "1", "\n", "", "counter", "+=", "1", "\n", "", "except", "Exception", ":", "\n", "                ", "print", "(", "'Finished processing data for article id: '", ",", "article_id", ")", "\n", "article_info", "[", "article_id", "]", "[", "'sents'", "]", "=", "accum_sent_lst", "\n", "article_info", "[", "article_id", "]", "[", "'content'", "]", "=", "\" \"", ".", "join", "(", "accum_sent_lst", ")", "\n", "article_info", "[", "article_id", "]", "[", "'num_sents'", "]", "=", "upd_counter", "\n", "break", "\n", "", "", "return", "article_dict", ",", "article_info", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.Attention.__init__": [[9, 45], ["torch.Module.__init__", "utilities.get_device", "config.get", "config.get", "torch.Parameter", "torch.Parameter", "torch.Linear", "torch.Linear", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.__init__", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device"], ["    ", "def", "__init__", "(", "self", ",", "attn_method", ",", "input_dim", ",", "nonlinear_func", "=", "torch", ".", "tanh", ",", "config", "=", "{", "}", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "        ", "'''\n        Args:\n            attn_method: string, {'additive', 'dot', 'dot_scaled'}\n            input_dim: int, size of the input vector (i.e. sentence vector representation)\n\n        '''", "\n", "\n", "super", "(", "Attention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attn_method", "=", "attn_method", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "nonlinear_func", "=", "nonlinear_func", "\n", "self", ".", "device", "=", "get_device", "(", "to_gpu", ",", "gpu_index", ")", "\n", "\n", "# print(\"Modified attention model\")", "\n", "# print(\"method: \", self.attn_method)", "\n", "# print(\"input dim:\", self.input_dim)", "\n", "# print(\"generic config:\", config)", "\n", "\n", "self", ".", "fdtype", "=", "config", ".", "get", "(", "'fdtype'", ",", "torch", ".", "float32", ")", "\n", "self", ".", "bidirectional_concat_flag", "=", "config", ".", "get", "(", "'bidirectional_concat_flag'", ",", "False", ")", "\n", "if", "(", "self", ".", "bidirectional_concat_flag", ")", ":", "\n", "            ", "input_divider", "=", "2", "\n", "", "else", ":", "\n", "            ", "input_divider", "=", "1", "\n", "\n", "# print(\"input divider:\", input_divider)", "\n", "", "if", "(", "self", ".", "attn_method", "==", "'additive'", ")", ":", "\n", "            ", "self", ".", "attnW", "=", "nn", ".", "Linear", "(", "self", ".", "input_dim", ",", "self", ".", "input_dim", "//", "input_divider", ")", "\n", "queryv_dim", "=", "self", ".", "input_dim", "//", "input_divider", "# we use the mapped vector size", "\n", "", "elif", "(", "self", ".", "attn_method", "in", "{", "'dot'", ",", "'dot_scaled'", "}", ")", ":", "# only dot prodcut operation", "\n", "            ", "queryv_dim", "=", "self", ".", "input_dim", "# we use the input vector size since we will perform dot product", "\n", "if", "(", "self", ".", "attn_method", "==", "'dot_scaled'", ")", ":", "\n", "                ", "self", ".", "scaler", "=", "torch", ".", "sqrt", "(", "torch", ".", "tensor", "(", "queryv_dim", ",", "dtype", "=", "self", ".", "fdtype", ",", "device", "=", "self", ".", "device", ")", ")", "\n", "# use this as query vector against the encoder outputs", "\n", "", "", "self", ".", "queryv", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "queryv_dim", ",", "dtype", "=", "self", ".", "fdtype", ",", "device", "=", "self", ".", "device", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.Attention.forward": [[46, 71], ["model.Attention.matmul", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "model.Attention.nonlinear_func", "model.Attention.attnW"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "encoder_outputs", ")", ":", "\n", "        ", "'''Performs forward computation\n\n        Args:\n            encoder_outputs: torch.Tensor, (1, sents, encoding_dim), dtype=torch.float32\n        '''", "\n", "\n", "if", "(", "self", ".", "attn_method", "==", "'additive'", ")", ":", "\n", "# do the mapping using one-layer mlp network, followed by nonlinear element-wise operation", "\n", "            ", "encoder_map", "=", "self", ".", "nonlinear_func", "(", "self", ".", "attnW", "(", "encoder_outputs", ")", ")", "\n", "# print('encoder_map size', encoder_map.size())", "\n", "# print('queryv size', self.queryv.size())", "\n", "", "else", ":", "\n", "            ", "encoder_map", "=", "encoder_outputs", "\n", "# print('encoder_map size', encoder_map.size())", "\n", "# print('queryv size', self.queryv.size())", "\n", "# using  matmul to compute tensor vector multiplication", "\n", "", "attn_weights", "=", "encoder_map", ".", "matmul", "(", "self", ".", "queryv", ")", "\n", "if", "(", "self", ".", "attn_method", "==", "'dot_scaled'", ")", ":", "\n", "            ", "attn_weights", "=", "attn_weights", "/", "self", ".", "scaler", "\n", "# softmax", "\n", "", "attn_weights_norm", "=", "torch", ".", "softmax", "(", "attn_weights", ",", "dim", "=", "1", ")", "\n", "\n", "# returns (1, sents)", "\n", "return", "attn_weights_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.BertEmbedder.__init__": [[75, 89], ["torch.Module.__init__", "bertmodel.float", "model.BertEmbedder.config.get", "model.BertEmbedder.config.get", "model.BertEmbedder.bertmodel.eval", "model.BertEmbedder.bertmodel.train"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bertmodel", ",", "proc_config", ")", ":", "\n", "        ", "super", "(", "BertEmbedder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bertmodel", "=", "bertmodel", ".", "float", "(", ")", "\n", "self", ".", "config", "=", "proc_config", "\n", "\n", "# use BertModel as word embedder", "\n", "self", ".", "bert_train_flag", "=", "self", ".", "config", ".", "get", "(", "'bert_train_flag'", ",", "False", ")", "\n", "# for now we are taking the last layer hidden vectors", "\n", "self", ".", "bert_all_output", "=", "self", ".", "config", ".", "get", "(", "'bert_all_output'", ",", "False", ")", "\n", "\n", "if", "not", "self", ".", "bert_train_flag", ":", "\n", "            ", "self", ".", "bertmodel", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bertmodel", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.BertEmbedder.forward": [[119, 151], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.set_grad_enabled", "torch.set_grad_enabled", "torch.set_grad_enabled", "torch.set_grad_enabled", "model.BertEmbedder.bertmodel", "embed_layers_lst.append"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "doc_tensor", ",", "attention_mask", ",", "num_sents", ")", ":", "\n", "        ", "'''\n\n        Args:\n            doc_tenosr: tensor, (sents, max_sent_len)\n            attention_mask: tensor, (sents, max_sent_len)\n            num_sents: int, actual number of sentences in the document\n\n        TODO: add flags and logic to handle multiple layers embedding (i.e. 12 layers embedding)\n        '''", "\n", "\n", "embed_layers_lst", "=", "[", "]", "\n", "\n", "for", "sent_indx", "in", "range", "(", "num_sents", ")", ":", "# going over each sentenece one by one due to GPU limit :((", "\n", "# print('sent_indx', sent_indx)", "\n", "            ", "with", "torch", ".", "set_grad_enabled", "(", "self", ".", "bert_train_flag", ")", ":", "\n", "                ", "encoded_layers", ",", "__", "=", "self", ".", "bertmodel", "(", "doc_tensor", "[", "sent_indx", ":", "sent_indx", "+", "1", "]", ",", "\n", "attention_mask", "=", "attention_mask", "[", "sent_indx", ":", "sent_indx", "+", "1", "]", ",", "\n", "output_all_encoded_layers", "=", "self", ".", "bert_all_output", ")", "\n", "embed_layers_lst", ".", "append", "(", "encoded_layers", ")", "\n", "\n", "# # embed all sentences at once", "\n", "# with torch.set_grad_enabled(self.bert_train_flag):", "\n", "#     encoded_layers, __ = self.bertmodel(doc_tensor, attention_mask=attention_mask,", "\n", "#                                         output_all_encoded_layers=self.bert_all_output)", "\n", "#", "\n", "# return encoded_layers", "\n", "\n", "# concat everything", "\n", "", "", "out", "=", "torch", ".", "cat", "(", "embed_layers_lst", ",", "dim", "=", "0", ")", "\n", "# print(\"finished embedding sents using BERT!\")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.SentenceEncoder.__init__": [[154, 181], ["torch.Module.__init__", "utilities.get_device", "torch.Dropout", "torch.Dropout", "model.SentenceEncoder.config.get", "rnn_class"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.__init__", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "num_hiddenlayers", "=", "1", ",", "\n", "bidirection", "=", "False", ",", "pdropout", "=", "0.1", ",", "rnn_class", "=", "nn", ".", "GRU", ",", "\n", "nonlinear_func", "=", "torch", ".", "relu", ",", "config", "=", "{", "}", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "\n", "        ", "super", "(", "SentenceEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "# embedding dimension as input to RNN", "\n", "self", ".", "device", "=", "get_device", "(", "to_gpu", ",", "gpu_index", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "# dimension of RNN output", "\n", "self", ".", "num_hiddenlayers", "=", "num_hiddenlayers", "\n", "self", ".", "pdropout", "=", "pdropout", "\n", "self", ".", "dropout_layer", "=", "nn", ".", "Dropout", "(", "pdropout", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "fdtype", "=", "self", ".", "config", ".", "get", "(", "'fdtype'", ",", "torch", ".", "float32", ")", "\n", "\n", "if", "(", "bidirection", ")", ":", "\n", "            ", "self", ".", "num_directions", "=", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_directions", "=", "1", "\n", "\n", "# RNN module inserts dropout between layers of RNN except for the output of the last layer!", "\n", "", "if", "(", "self", ".", "num_hiddenlayers", "==", "1", "and", "self", ".", "pdropout", ">", "0", ")", ":", "\n", "            ", "rnn_dropout", "=", "0", "\n", "", "else", ":", "\n", "            ", "rnn_dropout", "=", "self", ".", "pdropout", "\n", "", "self", ".", "rnn", "=", "rnn_class", "(", "self", ".", "input_dim", ",", "self", ".", "hidden_dim", ",", "num_layers", "=", "self", ".", "num_hiddenlayers", ",", "\n", "dropout", "=", "rnn_dropout", ",", "bidirectional", "=", "bidirection", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "nonlinear_func", "=", "nonlinear_func", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.SentenceEncoder.init_hidden": [[182, 199], ["torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "isinstance", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"initialize hidden vectors at t=0\n\n        Args:\n            batch_size: int, the size of the current evaluated batch\n        \"\"\"", "\n", "# a hidden vector has the shape (num_layers*num_directions, batch, hidden_dim)", "\n", "\n", "h0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "\n", "dtype", "=", "self", ".", "fdtype", ")", "\n", "if", "(", "isinstance", "(", "self", ".", "rnn", ",", "nn", ".", "LSTM", ")", ")", ":", "\n", "            ", "c0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", "\n", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "dtype", "=", "self", ".", "fdtype", ")", "\n", "hiddenvec", "=", "(", "h0", ",", "c0", ")", "\n", "", "else", ":", "\n", "            ", "hiddenvec", "=", "h0", "\n", "", "return", "(", "hiddenvec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.SentenceEncoder._process_rnn_hidden_output": [[200, 216], ["model.SentenceEncoder.config.get", "hidden.size", "hidden.view", "hn[].sum().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat.unsqueeze", "torch.cat.unsqueeze", "hn[].sum"], "methods", ["None"], ["", "def", "_process_rnn_hidden_output", "(", "self", ",", "hidden", ")", ":", "\n", "        ", "encoder_approach", "=", "self", ".", "config", ".", "get", "(", "'encoder_approach'", ")", "\n", "lastlayer_indx", "=", "-", "1", "\n", "batch_size", "=", "hidden", ".", "size", "(", "1", ")", "\n", "\n", "hn", "=", "hidden", ".", "view", "(", "self", ".", "num_hiddenlayers", ",", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", "\n", "if", "(", "encoder_approach", "==", "'[h_f]'", ")", ":", "# keep only the last forward hidden state vector", "\n", "            ", "return", "hn", "[", "lastlayer_indx", "]", "# (1, num_sents, hidden_dim)", "\n", "", "elif", "(", "encoder_approach", "==", "'[h_f+h_b]'", ")", ":", "\n", "            ", "return", "hn", "[", "lastlayer_indx", "]", ".", "sum", "(", "dim", "=", "0", ")", ".", "unsqueeze", "(", "0", ")", "# (1, num_sents, hidden_dim)", "\n", "", "elif", "(", "encoder_approach", "==", "'[h_f;h_b]'", ")", ":", "\n", "            ", "frwd_indx", "=", "0", "\n", "bckwd_indx", "=", "1", "\n", "# (1, num_sents, 2*hidden_dim)", "\n", "hn", "=", "torch", ".", "cat", "(", "[", "hn", "[", "lastlayer_indx", ",", "frwd_indx", ",", ":", ",", ":", "]", ",", "hn", "[", "lastlayer_indx", ",", "bckwd_indx", ",", ":", ",", ":", "]", "]", ",", "dim", "=", "-", "1", ")", "\n", "return", "hn", ".", "unsqueeze", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.SentenceEncoder._run_rnn": [[217, 233], ["model.SentenceEncoder.dropout_layer", "model.SentenceEncoder.init_hidden", "torch.nn.utils.rnn.pack_padded_sequence", "torch.nn.utils.rnn.pack_padded_sequence", "model.SentenceEncoder.rnn", "model.SentenceEncoder._process_rnn_hidden_output"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder_MeanPooling.init_hidden", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.SentenceEncoder._process_rnn_hidden_output"], ["", "", "def", "_run_rnn", "(", "self", ",", "embed_sents", ",", "doc_sents_len", ",", "num_sents", ")", ":", "\n", "# apply dropout", "\n", "        ", "embed_sents", "=", "self", ".", "dropout_layer", "(", "embed_sents", ")", "\n", "# init hidden", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "num_sents", ")", "\n", "# pack the batch", "\n", "packed_embeds", "=", "pack_padded_sequence", "(", "embed_sents", ",", "doc_sents_len", "[", ":", "num_sents", "]", ",", "batch_first", "=", "True", ",", "\n", "enforce_sorted", "=", "False", ")", "\n", "# print(\"packed_embeds\", \"\\n\", packed_embeds)", "\n", "packed_rnn_out", ",", "hidden", "=", "self", ".", "rnn", "(", "packed_embeds", ",", "hidden", ")", "\n", "# print(\"packed_rnn_out\", \"\\n\", packed_rnn_out)", "\n", "# print(\"hidden\", \"\\n\", hidden)", "\n", "# we need to unpack sequences", "\n", "# unpacked_output, out_seqlen = pad_packed_sequence(packed_rnn_out, batch_first=True)", "\n", "# return unpacked_output, hidden", "\n", "return", "self", ".", "_process_rnn_hidden_output", "(", "hidden", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.SentenceEncoder.forward": [[273, 284], ["model.SentenceEncoder._run_rnn"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.SentenceEncoder._run_rnn"], ["", "def", "forward", "(", "self", ",", "embed_sents", ",", "doc_sents_len", ",", "num_sents", ")", ":", "\n", "        ", "\"\"\" perform forward computation\n\n            Args:\n                embed_sents: torch.Tensor, (sents, max_sent_len, embed_dim), dtype=torch.float32 or torch.float64\n                    depending on fdtype.\n                doc_sents_len: torch.Tensor, (sents, ), dtype=torch.int64\n                num_sents: int, actual number of sentences in the doc\n        \"\"\"", "\n", "\n", "return", "self", ".", "_run_rnn", "(", "embed_sents", ",", "doc_sents_len", ",", "num_sents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder.__init__": [[287, 319], ["torch.Module.__init__", "utilities.get_device", "torch.Dropout", "torch.Dropout", "model.DocEncoder.config.get", "rnn_class"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.__init__", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "attn_model", ",", "\n", "num_hiddenlayers", "=", "1", ",", "bidirection", "=", "False", ",", "pdropout", "=", "0.1", ",", "\n", "rnn_class", "=", "nn", ".", "GRU", ",", "nonlinear_func", "=", "torch", ".", "relu", ",", "config", "=", "{", "}", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "\n", "        ", "super", "(", "DocEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "get_device", "(", "to_gpu", ",", "gpu_index", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "# dimension of the hidden vector from rnn", "\n", "self", ".", "attn_model", "=", "attn_model", "# instance of :class:`Attention`", "\n", "self", ".", "num_hiddenlayers", "=", "num_hiddenlayers", "\n", "self", ".", "pdropout", "=", "pdropout", "\n", "self", ".", "dropout_layer", "=", "nn", ".", "Dropout", "(", "pdropout", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "# to get options for the attention module", "\n", "self", ".", "fdtype", "=", "self", ".", "config", ".", "get", "(", "'fdtype'", ",", "torch", ".", "float32", ")", "\n", "\n", "if", "(", "bidirection", ")", ":", "\n", "            ", "self", ".", "num_directions", "=", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_directions", "=", "1", "\n", "\n", "# rnn module inserts dropout between layers of rnn except for the output of the last layer!", "\n", "", "if", "(", "self", ".", "num_hiddenlayers", "==", "1", "and", "self", ".", "pdropout", ">", "0", ")", ":", "\n", "            ", "rnn_dropout", "=", "0", "\n", "", "else", ":", "\n", "            ", "rnn_dropout", "=", "self", ".", "pdropout", "\n", "\n", "", "self", ".", "rnn", "=", "rnn_class", "(", "self", ".", "input_dim", ",", "self", ".", "hidden_dim", ",", "num_layers", "=", "self", ".", "num_hiddenlayers", ",", "dropout", "=", "rnn_dropout", ",", "\n", "bidirectional", "=", "bidirection", ",", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "nonlinear_func", "=", "nonlinear_func", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder.init_hidden": [[320, 336], ["torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "isinstance", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"initialize hidden vectors at t=0\n\n        Args:\n            batch_size: int, the size of the current evaluated batch\n        \"\"\"", "\n", "# a hidden vector has the shape (num_layers*num_directions, batch, hidden_dim)", "\n", "h0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "\n", "dtype", "=", "self", ".", "fdtype", ")", "\n", "if", "(", "isinstance", "(", "self", ".", "rnn", ",", "nn", ".", "LSTM", ")", ")", ":", "\n", "            ", "c0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", "\n", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "dtype", "=", "self", ".", "fdtype", ")", "\n", "hiddenvec", "=", "(", "h0", ",", "c0", ")", "\n", "", "else", ":", "\n", "            ", "hiddenvec", "=", "h0", "\n", "", "return", "(", "hiddenvec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder._reshape_rnn_output": [[337, 347], ["model.DocEncoder.config.get"], "methods", ["None"], ["", "def", "_reshape_rnn_output", "(", "self", ",", "rnn_out", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            rnn_out: torch tensor, (batch, seq_len, num_directions * hidden_size)\n        \"\"\"", "\n", "encoder_approach", "=", "self", ".", "config", ".", "get", "(", "'encoder_approach'", ")", "\n", "if", "(", "encoder_approach", "==", "'[h_f+h_b]'", ")", ":", "\n", "            ", "return", "rnn_out", "[", ":", ",", ":", ",", ":", "self", ".", "hidden_dim", "]", "+", "rnn_out", "[", ":", ",", ":", ",", "self", ".", "hidden_dim", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "rnn_out", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder.forward": [[348, 371], ["doc_tensor.size", "model.DocEncoder.init_hidden", "model.DocEncoder.rnn", "model.DocEncoder._reshape_rnn_output", "model.DocEncoder.attn_model", "model.DocEncoder.unsqueeze().bmm", "model.DocEncoder.dropout_layer", "model.DocEncoder.squeeze", "model.DocEncoder.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder_MeanPooling.init_hidden", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder_MeanPooling._reshape_rnn_output"], ["", "", "def", "forward", "(", "self", ",", "doc_tensor", ")", ":", "\n", "        ", "\"\"\" perform forward computation\n\n            Args:\n                doc_tensor: torch.Tensor, (1, sents, encoding_dim), dtype=torch.float32\n                            currently, it accepts one batch (i.e. one doc at a time due to GPU memory limit)\n        \"\"\"", "\n", "\n", "# init hidden", "\n", "num_sents", "=", "doc_tensor", ".", "size", "(", "0", ")", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "num_sents", ")", "\n", "rnn_out", ",", "hidden", "=", "self", ".", "rnn", "(", "doc_tensor", ",", "hidden", ")", "\n", "# print('rnn_out before', rnn_out.shape)", "\n", "# print(\"rnn_out\", \"\\n\", rnn_out)", "\n", "# print(\"hidden\", \"\\n\", hidden)", "\n", "# print('rnn_out size', rnn_out.shape)", "\n", "rnn_out", "=", "self", ".", "_reshape_rnn_output", "(", "rnn_out", ")", "\n", "# print('rnn_out after', rnn_out.shape)", "\n", "attn_weights_norm", "=", "self", ".", "attn_model", "(", "rnn_out", ")", "# (1, num_sents)", "\n", "# print('attn_weights_norm size', attn_weights_norm.size())", "\n", "doc_vec", "=", "attn_weights_norm", ".", "unsqueeze", "(", "1", ")", ".", "bmm", "(", "rnn_out", ")", "# (docs, 1, num_sents) * (docs, num_sents, embed_dim)", "\n", "doc_vec", "=", "self", ".", "dropout_layer", "(", "doc_vec", ".", "squeeze", "(", "1", ")", ")", "# turning (docs, 1, embed_dim) to (docs, embed_dim)", "\n", "return", "doc_vec", ",", "attn_weights_norm", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder_MeanPooling.__init__": [[374, 405], ["torch.Module.__init__", "utilities.get_device", "torch.Dropout", "torch.Dropout", "model.DocEncoder_MeanPooling.config.get", "rnn_class"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.__init__", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "\n", "num_hiddenlayers", "=", "1", ",", "bidirection", "=", "False", ",", "pdropout", "=", "0.1", ",", "\n", "rnn_class", "=", "nn", ".", "GRU", ",", "nonlinear_func", "=", "torch", ".", "relu", ",", "config", "=", "{", "}", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "\n", "        ", "super", "(", "DocEncoder_MeanPooling", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "get_device", "(", "to_gpu", ",", "gpu_index", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "# dimension of the hidden vector from rnn", "\n", "self", ".", "num_hiddenlayers", "=", "num_hiddenlayers", "\n", "self", ".", "pdropout", "=", "pdropout", "\n", "self", ".", "dropout_layer", "=", "nn", ".", "Dropout", "(", "pdropout", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "# to get options for the attention module", "\n", "self", ".", "fdtype", "=", "self", ".", "config", ".", "get", "(", "'fdtype'", ",", "torch", ".", "float32", ")", "\n", "\n", "if", "(", "bidirection", ")", ":", "\n", "            ", "self", ".", "num_directions", "=", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_directions", "=", "1", "\n", "\n", "# rnn module inserts dropout between layers of rnn except for the output of the last layer!", "\n", "", "if", "(", "self", ".", "num_hiddenlayers", "==", "1", "and", "self", ".", "pdropout", ">", "0", ")", ":", "\n", "            ", "rnn_dropout", "=", "0", "\n", "", "else", ":", "\n", "            ", "rnn_dropout", "=", "self", ".", "pdropout", "\n", "\n", "", "self", ".", "rnn", "=", "rnn_class", "(", "self", ".", "input_dim", ",", "self", ".", "hidden_dim", ",", "num_layers", "=", "self", ".", "num_hiddenlayers", ",", "dropout", "=", "rnn_dropout", ",", "\n", "bidirectional", "=", "bidirection", ",", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "nonlinear_func", "=", "nonlinear_func", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder_MeanPooling.init_hidden": [[406, 422], ["torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "isinstance", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"initialize hidden vectors at t=0\n\n        Args:\n            batch_size: int, the size of the current evaluated batch\n        \"\"\"", "\n", "# a hidden vector has the shape (num_layers*num_directions, batch, hidden_dim)", "\n", "h0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "\n", "dtype", "=", "self", ".", "fdtype", ")", "\n", "if", "(", "isinstance", "(", "self", ".", "rnn", ",", "nn", ".", "LSTM", ")", ")", ":", "\n", "            ", "c0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", "\n", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "dtype", "=", "self", ".", "fdtype", ")", "\n", "hiddenvec", "=", "(", "h0", ",", "c0", ")", "\n", "", "else", ":", "\n", "            ", "hiddenvec", "=", "h0", "\n", "", "return", "(", "hiddenvec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder_MeanPooling._reshape_rnn_output": [[423, 433], ["model.DocEncoder_MeanPooling.config.get"], "methods", ["None"], ["", "def", "_reshape_rnn_output", "(", "self", ",", "rnn_out", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            rnn_out: torch tensor, (batch, seq_len, num_directions * hidden_size)\n        \"\"\"", "\n", "encoder_approach", "=", "self", ".", "config", ".", "get", "(", "'encoder_approach'", ")", "\n", "if", "(", "encoder_approach", "==", "'[h_f+h_b]'", ")", ":", "\n", "            ", "return", "rnn_out", "[", ":", ",", ":", ",", ":", "self", ".", "hidden_dim", "]", "+", "rnn_out", "[", ":", ",", ":", ",", "self", ".", "hidden_dim", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "rnn_out", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder_MeanPooling.forward": [[434, 455], ["doc_tensor.size", "model.DocEncoder_MeanPooling.init_hidden", "model.DocEncoder_MeanPooling.rnn", "model.DocEncoder_MeanPooling._reshape_rnn_output", "model.DocEncoder_MeanPooling.mean", "model.DocEncoder_MeanPooling.dropout_layer"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder_MeanPooling.init_hidden", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocEncoder_MeanPooling._reshape_rnn_output"], ["", "", "def", "forward", "(", "self", ",", "doc_tensor", ")", ":", "\n", "        ", "\"\"\" perform forward computation\n\n            Args:\n                doc_tensor: torch.Tensor, (1, sents, encoding_dim), dtype=torch.float32\n                            currently, it accepts one batch (i.e. one doc at a time due to GPU memory limit)\n        \"\"\"", "\n", "\n", "# init hidden", "\n", "num_sents", "=", "doc_tensor", ".", "size", "(", "0", ")", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "num_sents", ")", "\n", "rnn_out", ",", "hidden", "=", "self", ".", "rnn", "(", "doc_tensor", ",", "hidden", ")", "\n", "# print('rnn_out before', rnn_out.shape)", "\n", "# print(\"rnn_out\", \"\\n\", rnn_out)", "\n", "# print(\"hidden\", \"\\n\", hidden)", "\n", "# print('rnn_out size', rnn_out.shape)", "\n", "rnn_out", "=", "self", ".", "_reshape_rnn_output", "(", "rnn_out", ")", "\n", "# print('rnn_out after', rnn_out.shape)", "\n", "doc_vec", "=", "rnn_out", ".", "mean", "(", "axis", "=", "1", ")", "# mean pooling across the sentences (docs, embed_dim)", "\n", "doc_vec", "=", "self", ".", "dropout_layer", "(", "doc_vec", ")", "\n", "return", "doc_vec", ",", "None", "# placeholder for attn_weights", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocCategScorer.__init__": [[458, 465], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.LogSoftmax", "torch.LogSoftmax"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "num_labels", ")", ":", "\n", "\n", "        ", "super", "(", "DocCategScorer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "# dimension of the output from :class:`DocEncoder`", "\n", "# TODO: do multiple mappings using Sequential or ModuleList", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "input_dim", ",", "num_labels", ")", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.DocCategScorer.forward": [[466, 482], ["model.DocCategScorer.classifier", "model.DocCategScorer.logsoftmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "doc_tensor", ")", ":", "\n", "        ", "\"\"\" perform forward computation\n\n            Args:\n                doc_tensor: torch.Tensor, (1, encoding_dim), dtype=torch.float32\n                            currently, it accepts one batch (i.e. one doc at a time due to GPU memory limit)\n        \"\"\"", "\n", "\n", "# init hidden", "\n", "out", "=", "self", ".", "classifier", "(", "doc_tensor", ")", "\n", "# print('classifier ', out)", "\n", "# print(out.size())", "\n", "# compute log soft max", "\n", "out", "=", "self", ".", "logsoftmax", "(", "out", ")", "\n", "# print(\"classifier out\", out.shape)", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.validate_rnn_output": [[484, 507], ["config.get", "config.get", "config.get", "rnn_out.size", "rnn_hidden.view", "config.get", "rnn_out.size", "rnn_out.view", "range", "print", "range", "print", "torch.equal", "torch.equal"], "function", ["None"], ["", "", "def", "validate_rnn_output", "(", "rnn_out", ",", "rnn_hidden", ",", "config", ")", ":", "\n", "    ", "num_directions", "=", "config", ".", "get", "(", "'num_directions'", ")", "\n", "num_layers", "=", "config", ".", "get", "(", "'num_layers'", ")", "\n", "h_dim", "=", "config", ".", "get", "(", "'hidden_dim'", ")", "\n", "batch_size", "=", "rnn_out", ".", "size", "(", "0", ")", "\n", "hn", "=", "rnn_hidden", ".", "view", "(", "num_layers", ",", "num_directions", ",", "batch_size", ",", "h_dim", ")", "\n", "lastlayer_indx", "=", "-", "1", "\n", "seqs_len", "=", "config", ".", "get", "(", "'seqs_len'", ")", "\n", "max_num_elms_inseq", "=", "rnn_out", ".", "size", "(", "1", ")", "\n", "# output has shape (batch_size, max_num_elms_inseq, num_directions, h_dim)", "\n", "output", "=", "rnn_out", ".", "view", "(", "batch_size", ",", "max_num_elms_inseq", ",", "num_directions", ",", "h_dim", ")", "\n", "for", "bindx", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "for", "dir_indx", "in", "range", "(", "num_directions", ")", ":", "\n", "            ", "print", "(", "\"batch index: {}, direction index: {}\"", ".", "format", "(", "bindx", ",", "dir_indx", ")", ")", "\n", "if", "(", "dir_indx", "==", "0", ")", ":", "# in case of forward hidden vector, t=T (last time step) should be used", "\n", "                ", "t", "=", "seqs_len", "[", "bindx", "]", "-", "1", "\n", "", "else", ":", "# in case of backward hidden vector, t=0 will be the one to use", "\n", "                ", "t", "=", "0", "\n", "#             print(hn[lastlayer_indx, dir_indx, bindx, :])", "\n", "#             print(output[bindx, t, dir_indx, :])", "\n", "#             print()", "\n", "", "assert", "torch", ".", "equal", "(", "hn", "[", "lastlayer_indx", ",", "dir_indx", ",", "bindx", ",", ":", "]", ",", "output", "[", "bindx", ",", "t", ",", "dir_indx", ",", ":", "]", ")", "\n", "", "", "print", "(", "\"passed the comparison test!!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.get_model_numparams": [[509, 511], ["sum", "param.numel", "model.parameters"], "function", ["None"], ["", "def", "get_model_numparams", "(", "model", ")", ":", "\n", "    ", "return", "sum", "(", "param", ".", "numel", "(", ")", "for", "param", "in", "model", ".", "parameters", "(", ")", "if", "param", ".", "requires_grad", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.restrict_grad_": [[513, 524], ["torch.utils.clip_grad_norm", "param.grad.data.clamp_"], "function", ["None"], ["", "def", "restrict_grad_", "(", "mparams", ",", "mode", ",", "limit", ")", ":", "\n", "    ", "\"\"\"clamp/clip a gradient in-place\n    \"\"\"", "\n", "if", "(", "mode", "==", "'clip_norm'", ")", ":", "\n", "        ", "__", ",", "maxl", "=", "limit", "\n", "nn", ".", "utils", ".", "clip_grad_norm", "(", "mparams", ",", "maxl", ",", "norm_type", "=", "2", ")", "# l2 norm clipping", "\n", "", "elif", "(", "mode", "==", "'clamp'", ")", ":", "# case of clamping", "\n", "        ", "minl", ",", "maxl", "=", "limit", "\n", "for", "param", "in", "mparams", ":", "\n", "            ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "                ", "param", ".", "grad", ".", "data", ".", "clamp_", "(", "minl", ",", "maxl", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.generate_sents_embeds_from_docs": [[526, 561], ["utilities.get_device", "bertembeder.type().to", "len", "range", "print", "bertembeder", "os.path.join", "utilities.ReaderWriter.dump_tensor", "print", "bertembeder.type", "doc_batch.to", "doc_attn_mask.to", "doc_len.item"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.get_device", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_tensor"], ["", "", "", "", "def", "generate_sents_embeds_from_docs", "(", "docs_data_tensor", ",", "bertembeder", ",", "embed_dir", ",", "fdtype", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "    ", "\"\"\"Generate token embedding for sentences in docs\n\n    Args:\n        docs_data_tensor: instance of :class:`DocsDataTensor`\n        bertembeder: instance of :class:`BertEmbedder`\n        embed_dir: string, path to directory where to dump embedding per document\n        fdtype: torch dtype, {torch.float32 or torch.float64}\n        to_gpu: bool, whether to use gpu or cpu\n        gpu_index: if to_gpu, which gpu index to use\n\n    \"\"\"", "\n", "bert_proc_docs", "=", "{", "}", "\n", "device", "=", "get_device", "(", "to_gpu", "=", "to_gpu", ",", "index", "=", "gpu_index", ")", "\n", "# move bertembedder to gpu", "\n", "bertembeder", ".", "type", "(", "fdtype", ")", ".", "to", "(", "device", ")", "\n", "samples_counter", "=", "0", "\n", "num_iter", "=", "len", "(", "docs_data_tensor", ")", "# number of samples", "\n", "for", "doc_indx", "in", "range", "(", "num_iter", ")", ":", "\n", "        ", "print", "(", "doc_indx", ")", "\n", "doc_batch", ",", "doc_len", ",", "doc_sents_len", ",", "doc_attn_mask", ",", "doc_labels", ",", "doc_id", "=", "docs_data_tensor", "[", "doc_indx", "]", "\n", "# push to gpu", "\n", "embed_sents", "=", "bertembeder", "(", "doc_batch", ".", "to", "(", "device", ")", ",", "doc_attn_mask", ".", "to", "(", "device", ")", ",", "doc_len", ".", "item", "(", ")", ")", "\n", "# write to disk for now", "\n", "embed_fpath", "=", "os", ".", "path", ".", "join", "(", "embed_dir", ",", "'{}.pkl'", ".", "format", "(", "doc_id", ")", ")", "\n", "ReaderWriter", ".", "dump_tensor", "(", "embed_sents", ",", "embed_fpath", ")", "\n", "# add embedding to dict", "\n", "bert_proc_docs", "[", "doc_id", "]", "=", "embed_fpath", "\n", "# clean stuff", "\n", "del", "embed_sents", "\n", "# torch.cuda.ipc_collect()", "\n", "# torch.cuda.empty_cache()", "\n", "samples_counter", "+=", "1", "\n", "print", "(", "\"processed doc id: {}, {}/{}\"", ".", "format", "(", "doc_id", ",", "samples_counter", ",", "num_iter", ")", ")", "\n", "", "return", "bert_proc_docs", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.read_pickles": [[29, 43], ["neural.utilities.ReaderWriter.read_data", "neural.utilities.ReaderWriter.read_data", "neural.utilities.ReaderWriter.read_data", "neural.utilities.ReaderWriter.read_data", "neural.utilities.ReaderWriter.read_data", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data"], ["def", "read_pickles", "(", "data_dir", ")", ":", "\n", "\n", "# Read stored data structures", "\n", "# {question:{fold_num:{dsettype:np.array(list of doc_ids)}}}", "\n", "    ", "q_partitions", "=", "ReaderWriter", ".", "read_data", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'q_partitions.pkl'", ")", ")", "\n", "# instance of :class:`DocDataTensor`", "\n", "docs_data_tensor", "=", "ReaderWriter", ".", "read_data", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'docs_data_tensor.pkl'", ")", ")", "\n", "# instance variable for processed articles dict", "\n", "proc_articles_dict", "=", "ReaderWriter", ".", "read_data", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'processor_articles_dict.pkl'", ")", ")", "\n", "# instance variable for computed articles reprs", "\n", "proc_articles_repr", "=", "ReaderWriter", ".", "read_data", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'processor_articles_repr.pkl'", ")", ")", "\n", "proc_config", "=", "ReaderWriter", ".", "read_data", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'processor_config.pkl'", ")", ")", "\n", "\n", "return", "q_partitions", ",", "docs_data_tensor", ",", "proc_articles_dict", ",", "proc_articles_repr", ",", "proc_config", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.build_doc_partitions": [[45, 64], ["neural.data_processor.DataDictProcessor", "neural.data_processor.DataDictProcessor.set_instance_attr", "neural.dataset.compute_class_weights_per_fold_", "q_docpartitions.update", "neural.dataset.validate_q_docpartitions", "neural.dataset.generate_docpartition_per_question"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.data_processor.DataDictProcessor.set_instance_attr", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.compute_class_weights_per_fold_", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.validate_q_docpartitions", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.dataset.generate_docpartition_per_question"], ["", "def", "build_doc_partitions", "(", "questions", ",", "proc_articles_repr", ",", "proc_articles_dict", ",", "proc_config", ",", "docs_data_tensor", ",", "q_partitions", ",", "\n", "validate", "=", "False", ")", ":", "\n", "    ", "processor", "=", "DataDictProcessor", "(", "proc_config", ")", "\n", "# reassign updated articles_dict and articles_repr to processor instance", "\n", "processor", ".", "set_instance_attr", "(", "proc_articles_repr", ",", "proc_articles_dict", ",", "proc_config", ")", "\n", "\n", "# turn the list of ids in q_partitions into dataset using PartitionDataTensor", "\n", "q_docpartitions", "=", "{", "}", "\n", "for", "question", "in", "questions", ":", "\n", "        ", "q_docpartitions", ".", "update", "(", "generate_docpartition_per_question", "(", "docs_data_tensor", ",", "q_partitions", ",", "question", ")", ")", "\n", "\n", "", "if", "validate", ":", "\n", "        ", "validate_q_docpartitions", "(", "q_docpartitions", ",", "q_partitions", ")", "\n", "\n", "# add class weights in q_docpartitions", "\n", "# we will use the weights to emphasize less occuring class during training phase", "\n", "", "compute_class_weights_per_fold_", "(", "q_docpartitions", ")", "\n", "\n", "return", "q_docpartitions", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.load_biobert_model": [[66, 78], ["os.path.join", "pytorch_pretrained_bert.BertConfig.from_json_file", "print", "pytorch_pretrained_bert.BertForPreTraining", "pytorch_pretrained_bert.BertForPreTraining.load_state_dict", "torch.load", "torch.load", "str", "os.path.join"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load"], ["", "def", "load_biobert_model", "(", "biobert_pth", ",", "device", ")", ":", "\n", "    ", "\"\"\"Read saved state dict for biobert model on disk\n\n    Args:\n        biobert_pth: str, folder path where model state dictionary and config file are saved\n    \"\"\"", "\n", "bert_config_file", "=", "os", ".", "path", ".", "join", "(", "biobert_pth", ",", "'bert_config.json'", ")", "\n", "config", "=", "BertConfig", ".", "from_json_file", "(", "bert_config_file", ")", "\n", "print", "(", "\"Building PyTorch model from configuration: {}\"", ".", "format", "(", "str", "(", "config", ")", ")", ")", "\n", "model", "=", "BertForPreTraining", "(", "config", ")", "\n", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "biobert_pth", ",", "'biobert_statedict.pkl'", ")", ",", "map_location", "=", "device", ")", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.write_sents_embeddings": [[80, 94], ["neural.model.BertEmbedder", "neural.utilities.create_directory", "neural.model.generate_sents_embeds_from_docs", "neural.utilities.ReaderWriter.dump_data", "os.path.join"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.model.generate_sents_embeds_from_docs", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.dump_data"], ["", "def", "write_sents_embeddings", "(", "directory", ",", "bertmodel", ",", "sents_embed_dir_name", ",", "docs_data_tensor", ")", ":", "\n", "# === Generate sents embedding ===", "\n", "# load BertModel", "\n", "\n", "# define BertEmbedder", "\n", "    ", "bert_config", "=", "{", "'bert_train_flag'", ":", "False", ",", "\n", "'bert_all_output'", ":", "False", "}", "\n", "bertembeder", "=", "BertEmbedder", "(", "bertmodel", ",", "bert_config", ")", "\n", "sents_embed_dir", "=", "create_directory", "(", "sents_embed_dir_name", ",", "directory", ")", "\n", "fdtype", "=", "torch", ".", "float32", "\n", "\n", "# generate and dump bert embedding for the tokens inside the specificed embedding directory", "\n", "bert_proc_docs", "=", "generate_sents_embeds_from_docs", "(", "docs_data_tensor", ",", "bertembeder", ",", "sents_embed_dir", ",", "fdtype", ")", "\n", "ReaderWriter", ".", "dump_data", "(", "bert_proc_docs", ",", "os", ".", "path", ".", "join", "(", "sents_embed_dir", ",", "'bert_proc_docs.pkl'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.run_hyperparam_search": [[96, 106], ["neural.utilities.create_directory", "neural.run_workflow.hyperparam_model_search_parallel"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.hyperparam_model_search_parallel"], ["", "def", "run_hyperparam_search", "(", "questions_to_run", ",", "directory", ",", "q_docpartitions", ",", "bertmodel", ",", "sents_embed_dir", ",", "question_gpu_map", ",", "\n", "attention", ")", ":", "\n", "    ", "hyperparam_search_dir", "=", "create_directory", "(", "'hyperparam_search'", ",", "directory", ")", "\n", "hyperparam_model_search_parallel", "(", "questions_to_run", ",", "q_docpartitions", ",", "bertmodel", ",", "sents_embed_dir", ",", "\n", "hyperparam_search_dir", ",", "\n", "question_gpu_map", ",", "\n", "fdtype", "=", "torch", ".", "float32", ",", "\n", "num_epochs", "=", "15", ",", "\n", "prob_interval_truemax", "=", "0.05", ",", "\n", "prob_estim", "=", "0.95", ",", "random_seed", "=", "42", ",", "attention", "=", "attention", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.run_training_parallel": [[108, 128], ["torch.Queue", "q_processes.append", "print", "q_process.start", "q_process.join", "print", "torch.Process"], "function", ["None"], ["", "def", "run_training_parallel", "(", "questions_to_run", ",", "train_val_dir", ",", "q_docpartitions", ",", "q_config_map", ",", "bertmodel", ",", "sents_embed_dir", ",", "\n", "question_gpu_map", ",", "num_epochs", ",", "max_folds", ")", ":", "\n", "    ", "queue", "=", "mp", ".", "Queue", "(", ")", "\n", "q_processes", "=", "[", "]", "\n", "# create a process for each question model", "\n", "for", "q", "in", "questions_to_run", ":", "\n", "        ", "q_processes", ".", "append", "(", "mp", ".", "Process", "(", "target", "=", "train_val_run_one_question", ",", "args", "=", "(", "queue", ",", "q", ",", "q_docpartitions", ",", "q_config_map", ",", "\n", "bertmodel", ",", "train_val_dir", ",", "\n", "sents_embed_dir", ",", "question_gpu_map", "[", "q", "]", ",", "\n", "num_epochs", ",", "max_folds", ")", ")", ")", "\n", "\n", "", "for", "q_process", "in", "q_processes", ":", "\n", "        ", "print", "(", "\">>> spawning process\"", ")", "\n", "q_process", ".", "start", "(", ")", "\n", "\n", "", "for", "q_process", "in", "q_processes", ":", "\n", "        ", "q_process", ".", "join", "(", ")", "\n", "print", "(", "\"<<< joined process\"", ")", "\n", "\n", "", "return", "train_val_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.run_training": [[130, 136], ["neural.utilities.create_directory", "neural.run_workflow.train_val_run"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.train_val_run"], ["", "def", "run_training", "(", "directory", ",", "q_docpartitions", ",", "q_config_map", ",", "bertmodel", ",", "sents_embed_dir", ",", "question_gpu_map", ",", "num_epochs", ",", "\n", "max_folds", ")", ":", "\n", "    ", "train_val_dir", "=", "create_directory", "(", "'train_validation'", ",", "directory", ")", "\n", "train_val_run", "(", "q_docpartitions", ",", "q_config_map", ",", "bertmodel", ",", "train_val_dir", ",", "sents_embed_dir", ",", "question_gpu_map", ",", "\n", "num_epochs", ",", "max_folds", ")", "\n", "return", "train_val_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.evaluate_on_test_set": [[138, 144], ["neural.utilities.create_directory", "neural.run_workflow.test_run"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.create_directory", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.run_workflow.test_run"], ["", "def", "evaluate_on_test_set", "(", "directory", ",", "q_docpartitions", ",", "q_config_map", ",", "bertmodel", ",", "train_val_dir", ",", "sents_embed_dir", ",", "\n", "gpu_index", ")", ":", "\n", "    ", "test_dir", "=", "create_directory", "(", "'test'", ",", "directory", ")", "\n", "test_run", "(", "q_docpartitions", ",", "q_config_map", ",", "bertmodel", ",", "train_val_dir", ",", "test_dir", ",", "sents_embed_dir", ",", "gpu_index", ",", "\n", "num_epochs", "=", "1", ")", "\n", "return", "test_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.get_performance_results": [[146, 175], ["range", "range", "os.path.join", "os.path.join", "os.path.isfile", "pandas.DataFrame", "pd.DataFrame.median", "pd.DataFrame.mean", "pd.DataFrame.std", "perf_df.append", "range", "neural.utilities.ReaderWriter.read_data", "pd.DataFrame.sort_values"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.utilities.ReaderWriter.read_data"], ["", "def", "get_performance_results", "(", "question", ",", "target_dir", ",", "num_folds", ",", "dsettype", ")", ":", "\n", "    ", "all_perf", "=", "{", "}", "\n", "num_metrics", "=", "3", "\n", "perf_dict", "=", "[", "{", "}", "for", "i", "in", "range", "(", "num_metrics", ")", "]", "# track micro_f1, macro_f1, accuracy", "\n", "question_name", "=", "[", "'q{}'", ".", "format", "(", "question", ")", "]", "\n", "for", "fold_num", "in", "range", "(", "num_folds", ")", ":", "\n", "\n", "        ", "fold_dir", "=", "os", ".", "path", ".", "join", "(", "target_dir", ",", "\n", "'question_{}'", ".", "format", "(", "question", ")", ",", "\n", "'fold_{}'", ".", "format", "(", "fold_num", ")", ")", "\n", "\n", "score_file", "=", "os", ".", "path", ".", "join", "(", "fold_dir", ",", "'score_{}.pkl'", ".", "format", "(", "dsettype", ")", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "score_file", ")", ":", "\n", "            ", "mscore", "=", "ReaderWriter", ".", "read_data", "(", "score_file", ")", "\n", "perf_dict", "[", "0", "]", "[", "'fold{}'", ".", "format", "(", "fold_num", ")", "]", "=", "mscore", ".", "micro_f1", "\n", "perf_dict", "[", "1", "]", "[", "'fold{}'", ".", "format", "(", "fold_num", ")", "]", "=", "mscore", ".", "macro_f1", "\n", "perf_dict", "[", "2", "]", "[", "'fold{}'", ".", "format", "(", "fold_num", ")", "]", "=", "mscore", ".", "accuracy", "\n", "", "", "perf_df", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_metrics", ")", ":", "\n", "        ", "all_perf", "=", "perf_dict", "[", "i", "]", "\n", "all_perf_df", "=", "pd", ".", "DataFrame", "(", "all_perf", ",", "index", "=", "question_name", ")", "\n", "median", "=", "all_perf_df", ".", "median", "(", "axis", "=", "1", ")", "\n", "mean", "=", "all_perf_df", ".", "mean", "(", "axis", "=", "1", ")", "\n", "stddev", "=", "all_perf_df", ".", "std", "(", "axis", "=", "1", ")", "\n", "all_perf_df", "[", "'mean'", "]", "=", "mean", "\n", "all_perf_df", "[", "'median'", "]", "=", "median", "\n", "all_perf_df", "[", "'stddev'", "]", "=", "stddev", "\n", "perf_df", ".", "append", "(", "all_perf_df", ".", "sort_values", "(", "'mean'", ",", "ascending", "=", "False", ")", ")", "\n", "", "return", "perf_df", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.build_accuracy_dfs": [[177, 189], ["pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "neural_discern_run_script.get_performance_results", "pandas.concat", "pandas.concat", "pandas.concat"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.get_performance_results"], ["", "def", "build_accuracy_dfs", "(", "q_docpartitions", ",", "test_dir", ")", ":", "\n", "    ", "micro_f1_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "macro_f1_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "accuracy_df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "\n", "for", "q", "in", "q_docpartitions", ":", "\n", "        ", "micro_f1", ",", "macro_f1", ",", "accuracy", "=", "get_performance_results", "(", "q", ",", "test_dir", ",", "5", ",", "'test'", ")", "\n", "micro_f1_df", "=", "pd", ".", "concat", "(", "[", "micro_f1_df", ",", "micro_f1", "]", ",", "sort", "=", "True", ")", "\n", "macro_f1_df", "=", "pd", ".", "concat", "(", "[", "macro_f1_df", ",", "macro_f1", "]", ",", "sort", "=", "True", ")", "\n", "accuracy_df", "=", "pd", ".", "concat", "(", "[", "accuracy_df", ",", "accuracy", "]", ",", "sort", "=", "True", ")", "\n", "\n", "", "return", "micro_f1_df", ",", "macro_f1_df", ",", "accuracy_df", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.highlight_attnw_over_sents": [[191, 203], ["torch.topk", "torch.topk", "print", "print", "range", "print", "attnw.size", "max_indx.size", "[].item", "print", "print", "attnw.size", "[].item"], "function", ["None"], ["", "def", "highlight_attnw_over_sents", "(", "docid_attnweights_map", ",", "proc_articles_repr", ",", "topk", "=", "5", ")", ":", "\n", "    ", "for", "docid", "in", "docid_attnweights_map", ":", "\n", "        ", "attnw", "=", "docid_attnweights_map", "[", "docid", "]", "\n", "topk", "=", "topk", "if", "attnw", ".", "size", "(", "-", "1", ")", ">", "topk", "else", "attnw", ".", "size", "(", "-", "1", ")", "# get top", "\n", "max_val", ",", "max_indx", "=", "torch", ".", "topk", "(", "attnw", ",", "topk", ",", "dim", "=", "1", ")", "\n", "print", "(", "docid", ")", "\n", "print", "(", "\"attended sent:\"", ")", "\n", "for", "i", "in", "range", "(", "max_indx", ".", "size", "(", "-", "1", ")", ")", ":", "\n", "            ", "target_indx", "=", "max_indx", "[", "0", "]", "[", "i", "]", ".", "item", "(", ")", "\n", "print", "(", "\"sentence num:\"", ",", "target_indx", ",", "\"attnw:\"", ",", "max_val", "[", "0", "]", "[", "i", "]", ".", "item", "(", ")", ")", "\n", "print", "(", "proc_articles_repr", "[", "docid", "]", "[", "'sents'", "]", "[", "target_indx", "]", ")", "\n", "", "print", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.neural.neural_discern_run_script.verbose_print": [[205, 208], ["print"], "function", ["None"], ["", "", "def", "verbose_print", "(", "text", ",", "print_yn", ")", ":", "\n", "    ", "if", "print_yn", ":", "\n", "        ", "print", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.__init__": [[29, 42], ["data_manager.DataManager.identify_data_path", "data_manager.DataProcessorCacheManager"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.identify_data_path"], ["def", "__init__", "(", "self", ",", "path_hint", ")", ":", "\n", "        ", "\"\"\"\n        Initialize a DataManager pointing at a project data_path. Can refer to ~/.data_manager.yaml, which has format:\n\n            project:\n                path: ~/path/to/project\n\n        Args:\n            path_hint: a path that exists, or a key in the config for a path\n        \"\"\"", "\n", "self", ".", "data_path", "=", "self", ".", "identify_data_path", "(", "path_hint", ")", "\n", "self", ".", "data", "=", "{", "}", "\n", "self", ".", "DataProcessorCacheManager", "=", "DataProcessorCacheManager", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.identify_data_path": [[43, 62], ["pathlib.Path().expanduser", "pathlib.Path().expanduser.exists", "data_manager.DataManager.load_config", "ValueError", "pathlib.Path().expanduser", "pathlib.Path().expanduser.exists", "pathlib.Path", "ValueError", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_config"], ["", "def", "identify_data_path", "(", "self", ",", "path_hint", ")", ":", "\n", "# first check if provided data_path is a legitimate path, and use that if it is", "\n", "# otherwise (if config exists) see if the path_hint is in the config", "\n", "# else raise error", "\n", "\n", "        ", "expanded_path", "=", "Path", "(", "path_hint", ")", ".", "expanduser", "(", ")", "\n", "if", "expanded_path", ".", "exists", "(", ")", ":", "\n", "            ", "return", "expanded_path", "\n", "\n", "", "config", "=", "self", ".", "load_config", "(", ")", "\n", "if", "config", "is", "not", "None", "and", "path_hint", "in", "config", ":", "\n", "            ", "expanded_config_path", "=", "Path", "(", "config", "[", "path_hint", "]", "[", "'path'", "]", ")", ".", "expanduser", "(", ")", "\n", "if", "expanded_config_path", ".", "exists", "(", ")", ":", "\n", "                ", "return", "expanded_config_path", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"Path provided in config for '{}' does not exist: {}\"", ".", "format", "(", "path_hint", ",", "\n", "expanded_config_path", ")", ")", "\n", "\n", "", "", "raise", "ValueError", "(", "\"Path does not exist: {}\"", ".", "format", "(", "path_hint", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_config": [[63, 71], ["pathlib.Path().expanduser", "pathlib.Path().expanduser.exists", "yaml.safe_load", "pathlib.Path", "open"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "load_config", "(", ")", ":", "\n", "        ", "config_path", "=", "Path", "(", "'~/.data_manager.yaml'", ")", ".", "expanduser", "(", ")", "\n", "if", "config_path", ".", "exists", "(", ")", ":", "\n", "            ", "config", "=", "yaml", ".", "safe_load", "(", "open", "(", "config_path", ")", ")", "\n", "return", "config", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.build_dicts": [[72, 90], ["data_manager.DataManager._load_articles", "data_manager.DataManager._load_responses", "print", "data_manager.DataManager.data[].to_dict", "autodiscern.convert_list_of_dicts_to_dict_of_dicts", "list", "print", "print", "pandas.pivot_table", "autodiscern.convert_list_of_dicts_to_dict_of_dicts.keys", "len", "list", "data_dict[].keys"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._load_articles", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._load_responses", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.convert_list_of_dicts_to_dict_of_dicts"], ["", "", "def", "build_dicts", "(", "self", ")", ":", "\n", "        ", "\"\"\"Build a dictionary of data dictionaries, keyed on their entity_ids. \"\"\"", "\n", "self", ".", "_load_articles", "(", "'html_articles'", ")", "\n", "self", ".", "_load_responses", "(", ")", "\n", "print", "(", "\"Building data dicts...\"", ")", "\n", "data_list_of_dicts", "=", "self", ".", "data", "[", "'html_articles'", "]", ".", "to_dict", "(", "'records'", ")", "\n", "data_dict", "=", "adt", ".", "convert_list_of_dicts_to_dict_of_dicts", "(", "data_list_of_dicts", ")", "\n", "\n", "for", "id", "in", "data_dict", ":", "\n", "            ", "entity_responses", "=", "self", ".", "data", "[", "'responses'", "]", "[", "self", ".", "data", "[", "'responses'", "]", "[", "'entity_id'", "]", "==", "id", "]", "\n", "responses_pivoted", "=", "pd", ".", "pivot_table", "(", "entity_responses", ",", "index", "=", "'questionID'", ",", "columns", "=", "'uid'", ",", "values", "=", "'answer'", ",", "\n", "aggfunc", "=", "'median'", ")", "\n", "data_dict", "[", "id", "]", "[", "'responses'", "]", "=", "responses_pivoted", "\n", "\n", "", "ids", "=", "list", "(", "data_dict", ".", "keys", "(", ")", ")", "\n", "print", "(", "\" ... {} data dicts built\"", ".", "format", "(", "len", "(", "ids", ")", ")", ")", "\n", "print", "(", "\"Available keys: {}\"", ".", "format", "(", "\", \"", ".", "join", "(", "list", "(", "data_dict", "[", "ids", "[", "0", "]", "]", ".", "keys", "(", ")", ")", ")", ")", ")", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._load_articles": [[91, 123], ["print", "pandas.DataFrame", "pathlib.Path", "glob.glob", "pandas.read_csv", "articles[].astype", "pandas.merge", "pandas.read_csv", "pandas.merge", "print", "pathlib.Path.__str__", "len", "print", "pathlib.Path", "pathlib.Path", "open", "f.read", "articles.append.append.append", "os.path.basename().split", "int", "os.path.basename"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "def", "_load_articles", "(", "self", ",", "version_id", ":", "str", ")", "->", "None", ":", "\n", "        ", "\"\"\"Loads all files located in self.data_path/data/<version_id> into a pd.df at self.data dict[version_id]\"\"\"", "\n", "\n", "print", "(", "\"Loading articles...\"", ")", "\n", "\n", "articles", "=", "pd", ".", "DataFrame", "(", "columns", "=", "[", "'entity_id'", ",", "'content'", "]", ")", "\n", "articles_path", "=", "Path", "(", "self", ".", "data_path", ",", "\"data/{}/*\"", ".", "format", "(", "version_id", ")", ")", "\n", "\n", "files", "=", "glob", ".", "glob", "(", "articles_path", ".", "__str__", "(", ")", ")", "\n", "\n", "if", "len", "(", "files", ")", "==", "0", ":", "\n", "            ", "print", "(", "\"WARNING: no files found at {}\"", ".", "format", "(", "articles_path", ")", ")", "\n", "\n", "", "for", "file", "in", "files", ":", "\n", "# to enforce encoding -- it generates errors without it!", "\n", "            ", "with", "open", "(", "file", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "# keep what's after the last / and before the .", "\n", "                ", "entity_id", "=", "os", ".", "path", ".", "basename", "(", "file", ")", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "content", "=", "f", ".", "read", "(", ")", "\n", "articles", "=", "articles", ".", "append", "(", "{", "'entity_id'", ":", "int", "(", "entity_id", ")", ",", "'content'", ":", "content", "}", ",", "ignore_index", "=", "True", ")", "\n", "\n", "# only keep articles listed in the target_ids file", "\n", "", "", "target_ids", "=", "pd", ".", "read_csv", "(", "Path", "(", "self", ".", "data_path", ",", "\"data/target_ids.csv\"", ")", ")", "\n", "articles", "[", "'entity_id'", "]", "=", "articles", "[", "'entity_id'", "]", ".", "astype", "(", "int", ")", "\n", "articles", "=", "pd", ".", "merge", "(", "articles", ",", "target_ids", ",", "on", "=", "'entity_id'", ")", "\n", "\n", "# add the article urls", "\n", "article_urls", "=", "pd", ".", "read_csv", "(", "Path", "(", "self", ".", "data_path", ",", "\"data/urls.csv\"", ")", ")", "\n", "articles", "=", "pd", ".", "merge", "(", "articles", ",", "article_urls", ",", "on", "=", "'entity_id'", ")", "\n", "\n", "print", "(", "\" ... {} articles loaded\"", ".", "format", "(", "articles", ".", "shape", "[", "0", "]", ")", ")", "\n", "self", ".", "data", "[", "version_id", "]", "=", "articles", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._load_responses": [[124, 128], ["print", "pandas.read_csv", "print", "pathlib.Path"], "methods", ["None"], ["", "def", "_load_responses", "(", "self", ")", "->", "None", ":", "\n", "        ", "print", "(", "\"Loading responses...\"", ")", "\n", "self", ".", "data", "[", "'responses'", "]", "=", "pd", ".", "read_csv", "(", "Path", "(", "self", ".", "data_path", ",", "\"data/responses.csv\"", ")", ")", "\n", "print", "(", "\" ... {} responses loaded\"", ".", "format", "(", "self", ".", "data", "[", "'responses'", "]", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._articles": [[129, 134], ["data_manager.DataManager._load_articles"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._load_articles"], ["", "def", "_articles", "(", "self", ",", "version", ")", "->", "pd", ".", "DataFrame", ":", "\n", "        ", "version_id", "=", "\"{}_articles\"", ".", "format", "(", "version", ")", "\n", "if", "version_id", "not", "in", "self", ".", "data", ":", "\n", "            ", "self", ".", "_load_articles", "(", "version_id", ")", "\n", "", "return", "self", ".", "data", "[", "version_id", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.get_repo_path": [[135, 138], ["os.path.dirname", "os.path.abspath", "inspect.getfile", "inspect.currentframe"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_repo_path", "(", ")", ":", "\n", "        ", "return", "os", ".", "path", ".", "dirname", "(", "os", ".", "path", ".", "abspath", "(", "inspect", ".", "getfile", "(", "inspect", ".", "currentframe", "(", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.generate_filename_for_file": [[139, 149], ["cls.get_repo_path", "cls._get_git_hash", "datetime.datetime.now().strftime", "datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.get_repo_path", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._get_git_hash"], ["", "@", "classmethod", "\n", "def", "generate_filename_for_file", "(", "cls", ",", "tag", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "        ", "repo_path", "=", "cls", ".", "get_repo_path", "(", ")", "\n", "git_hash", "=", "cls", ".", "_get_git_hash", "(", "repo_path", ")", "\n", "timestamp", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y-%m-%d_%H-%M-%S'", ")", "\n", "if", "tag", ":", "\n", "            ", "filename", "=", "\"{}_{}_{}\"", ".", "format", "(", "timestamp", ",", "git_hash", ",", "tag", ")", "\n", "", "else", ":", "\n", "            ", "filename", "=", "\"{}_{}\"", ".", "format", "(", "timestamp", ",", "git_hash", ")", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.save_transformed_data": [[150, 172], ["data_manager.DataManager.generate_filename_for_file", "pathlib.Path", "print", "data_manager.DataManager.check_for_uncommitted_git_changes", "open", "pickle.dump"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.generate_filename_for_file", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.check_for_uncommitted_git_changes"], ["", "def", "save_transformed_data", "(", "self", ",", "data", ":", "Dict", ",", "tag", ":", "str", "=", "None", ",", "enforce_clean_git", "=", "True", ")", "->", "str", ":", "\n", "        ", "\"\"\"Save a data dictionary to data/transformed directory with a filename created from the current timestamp and\n        an optional tag.\n        Getting the path based on:\n        https://stackoverflow.com/questions/50499/how-do-i-get-the-path-and-name-of-the-file-that-is-currently-executing\n\n        Args:\n            data: dict. Can contain anything that's pickle-able.\n            tag: str. A small description of the data for easy recognition in the file system.\n\n        Returns: None\n\n        \"\"\"", "\n", "if", "enforce_clean_git", ":", "\n", "            ", "self", ".", "check_for_uncommitted_git_changes", "(", ")", "\n", "\n", "", "filename", "=", "self", ".", "generate_filename_for_file", "(", "tag", ")", "\n", "filepath", "=", "Path", "(", "self", ".", "data_path", ",", "\"data/transformed_data\"", ",", "\"{}.pkl\"", ".", "format", "(", "filename", ")", ")", "\n", "with", "open", "(", "filepath", ",", "\"wb+\"", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "", "print", "(", "\"Saved data to {}\"", ".", "format", "(", "filepath", ")", ")", "\n", "return", "filepath", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.cache_data_processor": [[173, 183], ["data_manager.DataManager.generate_filename_for_file", "pathlib.Path", "data_manager.DataManager.DataProcessorCacheManager.save", "data_manager.DataManager.check_for_uncommitted_git_changes"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.generate_filename_for_file", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.save", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.check_for_uncommitted_git_changes"], ["", "def", "cache_data_processor", "(", "self", ",", "data", ":", "Any", ",", "processing_func", ":", "Callable", ",", "tag", ":", "str", "=", "None", ",", "data_file_type", "=", "'pkl'", ",", "\n", "enforce_clean_git", "=", "True", ")", "->", "str", ":", "\n", "        ", "if", "enforce_clean_git", ":", "\n", "            ", "self", ".", "check_for_uncommitted_git_changes", "(", ")", "\n", "\n", "", "file_name", "=", "self", ".", "generate_filename_for_file", "(", "tag", ")", "\n", "file_data_path", "=", "Path", "(", "self", ".", "data_path", ",", "\"data/transformed_data\"", ")", "\n", "self", ".", "DataProcessorCacheManager", ".", "save", "(", "data", ",", "processing_func", ",", "file_name", "=", "file_name", ",", "data_file_type", "=", "data_file_type", ",", "\n", "file_dir_path", "=", "file_data_path", ")", "\n", "return", "file_name", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_transformed_data": [[184, 194], ["pathlib.Path", "pathlib.Path", "open", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load"], ["", "def", "load_transformed_data", "(", "self", ",", "filename", ":", "str", ")", "->", "Dict", ":", "\n", "        ", "\"\"\"Load a pickled data dictionary from the data/transformed directory.\n        filename can be provided with or without the .pkl extension\"\"\"", "\n", "\n", "if", "'.pkl'", "in", "filename", ":", "\n", "            ", "filepath", "=", "Path", "(", "self", ".", "data_path", ",", "\"data/transformed_data/{}\"", ".", "format", "(", "filename", ")", ")", "\n", "", "else", ":", "\n", "            ", "filepath", "=", "Path", "(", "self", ".", "data_path", ",", "\"data/transformed_data/{}.pkl\"", ".", "format", "(", "filename", ")", ")", "\n", "", "with", "open", "(", "filepath", ",", "\"rb+\"", ")", "as", "f", ":", "\n", "            ", "return", "pickle", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_cached_data_processor": [[195, 198], ["pathlib.Path", "data_manager.DataManager.DataProcessorCacheManager.load"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load"], ["", "", "def", "load_cached_data_processor", "(", "self", ",", "file_name", ":", "str", ")", "->", "'DataProcessor'", ":", "\n", "        ", "file_data_path", "=", "Path", "(", "self", ".", "data_path", ",", "\"data/transformed_data\"", ")", "\n", "return", "self", ".", "DataProcessorCacheManager", ".", "load", "(", "file_name", "=", "file_name", ",", "file_dir_path", "=", "file_data_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_most_recent_transformed_data": [[199, 215], ["pathlib.Path", "glob.glob", "filenames.sort", "print", "data_manager.DataManager.load_transformed_data", "pathlib.Path.__str__", "len", "print", "os.path.basename"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_transformed_data"], ["", "def", "load_most_recent_transformed_data", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load the most recent pickled data dictionary from the data/transformed directory,\n        as determined by the timestamp in the filename. \"\"\"", "\n", "\n", "filepath", "=", "Path", "(", "self", ".", "data_path", ",", "\"data/transformed_data/*\"", ")", "\n", "files", "=", "glob", ".", "glob", "(", "filepath", ".", "__str__", "(", ")", ")", "\n", "\n", "if", "len", "(", "files", ")", "==", "0", ":", "\n", "            ", "print", "(", "\"ERROR: no files found at {}\"", ".", "format", "(", "filepath", ")", ")", "\n", "return", "\n", "\n", "", "filenames", "=", "[", "os", ".", "path", ".", "basename", "(", "file", ")", "for", "file", "in", "files", "]", "\n", "filenames", ".", "sort", "(", ")", "\n", "chosen_one", "=", "filenames", "[", "-", "1", "]", "\n", "print", "(", "\"Loading {}\"", ".", "format", "(", "chosen_one", ")", ")", "\n", "return", "self", ".", "load_transformed_data", "(", "chosen_one", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.save_experiment": [[216, 221], ["data_manager.DataInterfaceManager.select", "pathlib.Path", "data_manager.DataInterfaceManager.select", "pathlib.Path"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.select", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.select"], ["", "def", "save_experiment", "(", "self", ",", "experiment_object", ":", "Any", ",", "file_name", ":", "str", ")", "->", "str", ":", "\n", "        ", "data_interface", "=", "DataInterfaceManager", ".", "select", "(", "file_name", ",", "default_file_type", "=", "'dill'", ")", "\n", "file_dir_path", "=", "Path", "(", "self", ".", "data_path", ",", "'experiment_objects'", ")", "\n", "data_interface", ".", "save", "(", "experiment_object", ",", "file_name", "=", "file_name", ",", "file_dir_path", "=", "file_dir_path", ")", "\n", "return", "Path", "(", "file_dir_path", ",", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_experiment": [[222, 230], ["str", "data_manager.DataInterfaceManager.select", "pathlib.Path", "data_manager.DataInterfaceManager.select"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.select", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.select"], ["", "def", "load_experiment", "(", "self", ",", "file_name", ":", "str", ")", "->", "Any", ":", "\n", "# convert the file_name to a string, because file names are experiment id numbers, so an int may be passed", "\n", "        ", "file_name", "=", "str", "(", "file_name", ")", "\n", "\n", "data_interface", "=", "DataInterfaceManager", ".", "select", "(", "file_name", ",", "default_file_type", "=", "'dill'", ")", "\n", "file_dir_path", "=", "Path", "(", "self", ".", "data_path", ",", "'experiment_objects'", ")", "\n", "experiment_object", "=", "data_interface", ".", "load", "(", "file_name", "=", "file_name", ",", "file_dir_path", "=", "file_dir_path", ")", "\n", "return", "experiment_object", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._get_git_hash": [[231, 248], ["subprocess.check_output", "subprocess.check_output.strip().decode", "subprocess.check_output.strip"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_get_git_hash", "(", "cls", ",", "path", ":", "str", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Get the short hash of latest git commit, first checking that all changes have been committed.\n        If there are uncommitted changes, raise an error.\n        This ensures that the git hash returned captures the true state of the code.\n\n        Arguments:\n            path (str): Path to git repo.\n\n        Returns:\n            git_hash (str): Short hash of latest commit on the active branch of the git repo.\n        \"\"\"", "\n", "git_hash_raw", "=", "subprocess", ".", "check_output", "(", "[", "'git'", ",", "'rev-parse'", ",", "'--short'", ",", "'HEAD'", "]", ",", "\n", "cwd", "=", "path", ")", "\n", "git_hash", "=", "git_hash_raw", ".", "strip", "(", ")", ".", "decode", "(", "\"utf-8\"", ")", "\n", "return", "git_hash", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.check_for_uncommitted_git_changes": [[249, 253], ["cls.get_repo_path", "cls._check_for_uncommitted_git_changes_at_path"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.get_repo_path", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._check_for_uncommitted_git_changes_at_path"], ["", "@", "classmethod", "\n", "def", "check_for_uncommitted_git_changes", "(", "cls", ")", ":", "\n", "        ", "repo_path", "=", "cls", ".", "get_repo_path", "(", ")", "\n", "return", "cls", ".", "_check_for_uncommitted_git_changes_at_path", "(", "repo_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._check_for_uncommitted_git_changes_at_path": [[254, 289], ["git.Repo", "item.strip", "len", "RuntimeError", "open", "item.startswith", "git.Repo.index.diff", "os.path.join", "line.strip", "item.startswith", "os.path.basename", "any", "f.readlines", "item.endswith", "line.startswith"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_check_for_uncommitted_git_changes_at_path", "(", "cls", ",", "repo_path", ":", "str", ")", "->", "bool", ":", "\n", "        ", "\"\"\"\n        Check if there are uncommitted changes in the git repo, and raise an error if there are.\n\n        Args:\n            repo_path: str. Path to the repo to check.\n\n        Returns: bool. False: no uncommitted changes found, Repo is valid.\n            True: uncommitted changes found. Repo is not valid.\n        \"\"\"", "\n", "repo", "=", "Repo", "(", "repo_path", ",", "search_parent_directories", "=", "True", ")", "\n", "\n", "try", ":", "\n", "# get list of gitignore filenames and extensions as these wouldn't have been code synced over", "\n", "# and therefore would appears as if they were uncommitted changes", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "repo", ".", "working_tree_dir", ",", "'.gitignore'", ")", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "gitignore", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "f", ".", "readlines", "(", ")", "if", "not", "line", ".", "startswith", "(", "'#'", ")", "and", "line", "!=", "'\\n'", "]", "\n", "", "", "except", "FileNotFoundError", ":", "\n", "            ", "gitignore", "=", "[", "]", "\n", "\n", "", "gitignore_files", "=", "[", "item", "for", "item", "in", "gitignore", "if", "not", "item", ".", "startswith", "(", "'*'", ")", "]", "\n", "gitignore_ext", "=", "[", "item", ".", "strip", "(", "'*'", ")", "for", "item", "in", "gitignore", "if", "item", ".", "startswith", "(", "'*'", ")", "]", "\n", "\n", "# get list of changed files, but ignore ones in gitignore (either by filename match or extension match)", "\n", "changed_files", "=", "[", "item", ".", "a_path", "for", "item", "in", "repo", ".", "index", ".", "diff", "(", "None", ")", "\n", "if", "os", ".", "path", ".", "basename", "(", "item", ".", "a_path", ")", "not", "in", "gitignore_files", "]", "\n", "changed_files", "=", "[", "item", "for", "item", "in", "changed_files", "\n", "if", "not", "any", "(", "[", "item", ".", "endswith", "(", "ext", ")", "for", "ext", "in", "gitignore_ext", "]", ")", "]", "\n", "\n", "if", "len", "(", "changed_files", ")", ">", "0", ":", "\n", "            ", "raise", "RuntimeError", "(", "'There are uncommitted changes in files: {}'", "\n", "'\\nCommit them before proceeding. '", ".", "format", "(", "', '", ".", "join", "(", "changed_files", ")", ")", ")", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.html_articles": [[290, 293], ["data_manager.DataManager._articles"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._articles"], ["", "@", "property", "\n", "def", "html_articles", "(", "self", ")", "->", "pd", ".", "DataFrame", ":", "\n", "        ", "return", "self", ".", "_articles", "(", "'html'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.clean_articles": [[294, 298], ["print", "data_manager.DataManager._articles"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._articles"], ["", "@", "property", "\n", "def", "clean_articles", "(", "self", ")", "->", "pd", ".", "DataFrame", ":", "\n", "        ", "print", "(", "\"WARNING: this function is deprecated\"", ")", "\n", "return", "self", ".", "_articles", "(", "'cleaned_text'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.selected_html_articles": [[299, 303], ["print", "data_manager.DataManager._articles"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._articles"], ["", "@", "property", "\n", "def", "selected_html_articles", "(", "self", ")", "->", "pd", ".", "DataFrame", ":", "\n", "        ", "print", "(", "\"WARNING: this function is deprecated\"", ")", "\n", "return", "self", ".", "_articles", "(", "'remove_selected_html'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.no_html_articles": [[304, 308], ["print", "data_manager.DataManager._articles"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._articles"], ["", "@", "property", "\n", "def", "no_html_articles", "(", "self", ")", "->", "pd", ".", "DataFrame", ":", "\n", "        ", "print", "(", "\"WARNING: this function is deprecated\"", ")", "\n", "return", "self", ".", "_articles", "(", "'remove_all_html'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.responses": [[309, 314], ["data_manager.DataManager._load_responses"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._load_responses"], ["", "@", "property", "\n", "def", "responses", "(", "self", ")", "->", "pd", ".", "DataFrame", ":", "\n", "        ", "if", "'responses'", "not", "in", "self", ".", "data", ":", "\n", "            ", "self", ".", "_load_responses", "(", ")", "\n", "", "return", "self", ".", "data", "[", "'responses'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._load_metamap_semantics": [[315, 319], ["pkg_resources.resource_filename", "pandas.read_csv"], "methods", ["None"], ["", "def", "_load_metamap_semantics", "(", "self", ")", ":", "\n", "        ", "metamap_semantics_path", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "\n", "'data/metamap_semantics/metamap_semantics.csv'", ")", "\n", "self", ".", "data", "[", "'metamap_semantics'", "]", "=", "pd", ".", "read_csv", "(", "metamap_semantics_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.metamap_semantics": [[320, 325], ["data_manager.DataManager._load_metamap_semantics"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager._load_metamap_semantics"], ["", "@", "property", "\n", "def", "metamap_semantics", "(", "self", ")", "->", "pd", ".", "DataFrame", ":", "\n", "        ", "if", "'metamap_semantics'", "not", "in", "self", ".", "data", ":", "\n", "            ", "self", ".", "_load_metamap_semantics", "(", ")", "\n", "", "return", "self", ".", "data", "[", "'metamap_semantics'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessor.__init__": [[329, 333], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "processor_func", ",", "code", ")", ":", "\n", "        ", "self", ".", "data_set", "=", "data", "\n", "self", ".", "processor_func", "=", "processor_func", "\n", "self", ".", "code", "=", "code", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessor.data": [[334, 337], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "data", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "data_set", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessor.func": [[338, 341], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "func", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "processor_func", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessor.rerun": [[342, 344], ["data_manager.DataProcessor.processor_func"], "methods", ["None"], ["", "def", "rerun", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "processor_func", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessor.view_code": [[345, 347], ["None"], "methods", ["None"], ["", "def", "view_code", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "code", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessorCacheManager.__init__": [[351, 356], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "processor_designation", "=", "'_processor'", "\n", "self", ".", "processor_data_interface", "=", "DillDataInterface", "\n", "self", ".", "code_designation", "=", "'_code'", "\n", "self", ".", "code_data_interface", "=", "TextDataInterface", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessorCacheManager.save": [[357, 367], ["data_manager.DataProcessorCacheManager.check_name_already_exists", "data_manager.DataInterfaceManager.select", "processing_func", "data_manager.DataInterfaceManager.select", "data_manager.DataProcessorCacheManager.processor_data_interface.save", "inspect.getsource", "data_manager.DataProcessorCacheManager.code_data_interface.save", "ValueError"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessorCacheManager.check_name_already_exists", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.select", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.select", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.save", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.save"], ["", "def", "save", "(", "self", ",", "data", ":", "Any", ",", "processing_func", ":", "Callable", ",", "file_name", ":", "str", ",", "data_file_type", ":", "str", ",", "file_dir_path", ":", "str", ")", ":", "\n", "        ", "if", "self", ".", "check_name_already_exists", "(", "file_name", ",", "file_dir_path", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\"That data processor name is already in use\"", ")", "\n", "\n", "", "data_interface", "=", "DataInterfaceManager", ".", "select", "(", "data_file_type", ")", "\n", "data", "=", "processing_func", "(", "data", ")", "\n", "data_interface", ".", "save", "(", "data", ",", "file_name", ",", "file_dir_path", ")", "\n", "self", ".", "processor_data_interface", ".", "save", "(", "processing_func", ",", "file_name", "+", "self", ".", "processor_designation", ",", "file_dir_path", ")", "\n", "processing_func_code", "=", "inspect", ".", "getsource", "(", "processing_func", ")", "\n", "self", ".", "code_data_interface", ".", "save", "(", "processing_func_code", ",", "file_name", "+", "self", ".", "code_designation", ",", "file_dir_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessorCacheManager.load": [[368, 397], ["data_manager.DataProcessorCacheManager.processor_data_interface.load", "data_manager.DataProcessorCacheManager.code_data_interface.load", "data_manager.DataInterfaceManager.select", "data_manager.DataInterfaceManager.select", "data_manager.DataProcessor", "file_name.split", "data_manager.DataProcessorCacheManager.get_data_processor_data_type"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.select", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.select", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessorCacheManager.get_data_processor_data_type"], ["", "def", "load", "(", "self", ",", "file_name", ":", "str", ",", "file_dir_path", ":", "str", ")", "->", "DataProcessor", ":", "\n", "        ", "\"\"\"\n        Load a cached data processor- the data and the function that generated it.\n        Accepts a file name with or without a file extension.\n\n        Args:\n            file_name: The base name of the data file. May include the file extension, otherwise the file extension\n                will be deduced.\n            file_dir_path: the path to the directory where cached data processors are stored.\n\n        Returns: Tuple(data, processing_func)\n\n        \"\"\"", "\n", "data_file_extension", "=", "None", "\n", "if", "'.'", "in", "file_name", ":", "\n", "            ", "file_name", ",", "data_file_extension", "=", "file_name", ".", "split", "(", "'.'", ")", "\n", "\n", "# load the processor", "\n", "", "processing_func", "=", "self", ".", "processor_data_interface", ".", "load", "(", "file_name", "+", "self", ".", "processor_designation", ",", "file_dir_path", ")", "\n", "\n", "# load the data", "\n", "code", "=", "self", ".", "code_data_interface", ".", "load", "(", "file_name", "+", "self", ".", "code_designation", ",", "file_dir_path", ")", "\n", "\n", "# find and load the data", "\n", "if", "data_file_extension", "is", "None", ":", "\n", "            ", "data_file_extension", "=", "self", ".", "get_data_processor_data_type", "(", "file_name", ",", "file_dir_path", ")", "\n", "", "data_interface", "=", "DataInterfaceManager", ".", "select", "(", "data_file_extension", ")", "\n", "data", "=", "data_interface", ".", "load", "(", "file_name", ",", "file_dir_path", ")", "\n", "return", "DataProcessor", "(", "data", ",", "processing_func", ",", "code", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessorCacheManager.check_name_already_exists": [[398, 404], ["data_manager.DataProcessorCacheManager.list_cached_data_processors"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessorCacheManager.list_cached_data_processors"], ["", "def", "check_name_already_exists", "(", "self", ",", "file_name", ",", "file_dir_path", ")", ":", "\n", "        ", "existing_data_processors", "=", "self", ".", "list_cached_data_processors", "(", "file_dir_path", ")", "\n", "if", "file_name", "in", "existing_data_processors", ":", "\n", "            ", "return", "True", "\n", "", "else", ":", "\n", "            ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessorCacheManager.get_data_processor_data_type": [[405, 419], ["pathlib.Path", "glob.glob", "pathlib.Path.__str__", "len", "ValueError", "os.path.basename().split", "len", "ValueError", "os.path.basename"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "", "@", "staticmethod", "\n", "def", "get_data_processor_data_type", "(", "file_name", ",", "file_dir_path", ")", ":", "\n", "        ", "data_path", "=", "Path", "(", "\"{}/{}.*\"", ".", "format", "(", "file_dir_path", ",", "file_name", ")", ")", "\n", "processor_files", "=", "glob", ".", "glob", "(", "data_path", ".", "__str__", "(", ")", ")", "\n", "\n", "if", "len", "(", "processor_files", ")", "==", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\"No data file found for processor {}\"", ".", "format", "(", "file_name", ")", ")", "\n", "", "elif", "len", "(", "processor_files", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Something went wrong- there's more than one file that matches this processor name: \"", "\n", "\"{}\"", ".", "format", "(", "\"\\n - \"", ".", "join", "(", "processor_files", ")", ")", ")", "\n", "\n", "", "data_file", "=", "processor_files", "[", "0", "]", "\n", "data_file_extension", "=", "os", ".", "path", ".", "basename", "(", "data_file", ")", ".", "split", "(", "'.'", ")", "[", "1", "]", "\n", "return", "data_file_extension", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataProcessorCacheManager.list_cached_data_processors": [[420, 427], ["pathlib.Path", "glob.glob", "pathlib.Path.__str__", "os.path.basename().split", "os.path.basename"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "def", "list_cached_data_processors", "(", "self", ",", "file_dir_path", ":", "str", ")", ":", "\n", "# glob for all processor files", "\n", "        ", "processors_path", "=", "Path", "(", "\"{}/*{}.{}\"", ".", "format", "(", "file_dir_path", ",", "self", ".", "processor_designation", ",", "\n", "self", ".", "processor_data_interface", ".", "file_extension", ")", ")", "\n", "processor_files", "=", "glob", ".", "glob", "(", "processors_path", ".", "__str__", "(", ")", ")", "\n", "processor_names", "=", "[", "os", ".", "path", ".", "basename", "(", "file", ")", ".", "split", "(", "'.'", ")", "[", "0", "]", "for", "file", "in", "processor_files", "]", "\n", "return", "processor_names", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.construct_file_path": [[433, 449], ["pathlib.Path", "pathlib.Path"], "methods", ["None"], ["@", "classmethod", "\n", "def", "construct_file_path", "(", "cls", ",", "file_name", ":", "str", ",", "file_dir_path", ":", "str", ")", "->", "Path", ":", "\n", "        ", "\"\"\"Construct the file path.\n         Can handle both cases of the file extension being included in the file_name or not\n\n        Args:\n            file_name: File name, with or without file extension\n            file_dir_path: Path for the file\n\n        Returns: Path\n\n        \"\"\"", "\n", "if", "'.{}'", ".", "format", "(", "cls", ".", "file_extension", ")", "in", "file_name", ":", "\n", "            ", "return", "Path", "(", "file_dir_path", ",", "file_name", ")", "\n", "", "else", ":", "\n", "            ", "return", "Path", "(", "file_dir_path", ",", "\"{}.{}\"", ".", "format", "(", "file_name", ",", "cls", ".", "file_extension", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.save": [[450, 454], ["cls.construct_file_path", "cls._interface_specific_save"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.construct_file_path", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.TextDataInterface._interface_specific_save"], ["", "", "@", "classmethod", "\n", "def", "save", "(", "cls", ",", "data", ":", "Any", ",", "file_name", ":", "str", ",", "file_dir_path", ":", "str", ")", "->", "None", ":", "\n", "        ", "file_path", "=", "cls", ".", "construct_file_path", "(", "file_name", ",", "file_dir_path", ")", "\n", "return", "cls", ".", "_interface_specific_save", "(", "data", ",", "file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface._interface_specific_save": [[455, 458], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_interface_specific_save", "(", "cls", ",", "data", ":", "Any", ",", "file_path", ")", "->", "None", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load": [[459, 463], ["cls.construct_file_path", "cls._interface_specific_load"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.construct_file_path", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.TextDataInterface._interface_specific_load"], ["", "@", "classmethod", "\n", "def", "load", "(", "cls", ",", "file_name", ":", "str", ",", "file_dir_path", ":", "str", ")", "->", "Any", ":", "\n", "        ", "file_path", "=", "cls", ".", "construct_file_path", "(", "file_name", ",", "file_dir_path", ")", "\n", "return", "cls", ".", "_interface_specific_load", "(", "file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface._interface_specific_load": [[464, 467], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_interface_specific_load", "(", "cls", ",", "file_path", ")", "->", "Any", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.PickleDataInterface._interface_specific_save": [[473, 477], ["open", "pickle.dump"], "methods", ["None"], ["@", "classmethod", "\n", "def", "_interface_specific_save", "(", "cls", ",", "data", ":", "Any", ",", "file_path", ")", "->", "None", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "\"wb+\"", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.PickleDataInterface._interface_specific_load": [[478, 482], ["open", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load"], ["", "", "@", "classmethod", "\n", "def", "_interface_specific_load", "(", "cls", ",", "file_path", ")", "->", "Any", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "\"rb+\"", ")", "as", "f", ":", "\n", "            ", "return", "pickle", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DillDataInterface._interface_specific_save": [[488, 492], ["open", "dill.dump"], "methods", ["None"], ["@", "classmethod", "\n", "def", "_interface_specific_save", "(", "cls", ",", "data", ":", "Any", ",", "file_path", ")", "->", "None", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "\"wb+\"", ")", "as", "f", ":", "\n", "            ", "dill", ".", "dump", "(", "data", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DillDataInterface._interface_specific_load": [[493, 497], ["open", "dill.load"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load"], ["", "", "@", "classmethod", "\n", "def", "_interface_specific_load", "(", "cls", ",", "file_path", ")", "->", "Any", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "\"rb+\"", ")", "as", "f", ":", "\n", "            ", "return", "dill", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.CSVDataInterface._interface_specific_save": [[503, 506], ["data.to_csv"], "methods", ["None"], ["@", "classmethod", "\n", "def", "_interface_specific_save", "(", "cls", ",", "data", ",", "file_path", ")", ":", "\n", "        ", "data", ".", "to_csv", "(", "file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.CSVDataInterface._interface_specific_load": [[507, 510], ["pandas.read_csv"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_interface_specific_load", "(", "cls", ",", "file_path", ")", ":", "\n", "        ", "return", "pd", ".", "read_csv", "(", "file_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.TextDataInterface._interface_specific_save": [[516, 520], ["open", "f.write"], "methods", ["None"], ["@", "classmethod", "\n", "def", "_interface_specific_save", "(", "cls", ",", "data", ",", "file_path", ")", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "'w+'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.TextDataInterface._interface_specific_load": [[521, 526], ["open", "f.read"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "_interface_specific_load", "(", "cls", ",", "file_path", ")", ":", "\n", "        ", "with", "open", "(", "file_path", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "data", "=", "f", ".", "read", "(", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.create": [[538, 545], ["ValueError", "list", "cls.registered_interfaces.keys"], "methods", ["None"], ["@", "classmethod", "\n", "def", "create", "(", "cls", ",", "file_type", ":", "str", ")", "->", "DataInterface", ":", "\n", "        ", "if", "file_type", "in", "cls", ".", "registered_interfaces", ":", "\n", "            ", "return", "cls", ".", "registered_interfaces", "[", "file_type", "]", "(", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"File type {} not recognized. Supported file types include {}\"", ".", "format", "(", "\n", "file_type", ",", "list", "(", "cls", ".", "registered_interfaces", ".", "keys", "(", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.select": [[546, 568], ["file_hint.split", "cls.create", "cls.create", "cls.create", "ValueError", "list", "cls.registered_interfaces.keys"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.create", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.create", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterfaceManager.create"], ["", "", "@", "classmethod", "\n", "def", "select", "(", "cls", ",", "file_hint", ":", "str", ",", "default_file_type", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Select the appropriate data interface based on the file_hint.\n\n        Args:\n            file_hint: May be a file name with an extension, or just a file extension.\n            default_file_type: default file type to use, if the file_hint doesn't specify.\n\n        Returns: A DataInterface.\n\n        \"\"\"", "\n", "if", "'.'", "in", "file_hint", ":", "\n", "            ", "file_name", ",", "file_extension", "=", "file_hint", ".", "split", "(", "'.'", ")", "\n", "return", "cls", ".", "create", "(", "file_extension", ")", "\n", "", "elif", "file_hint", "in", "cls", ".", "registered_interfaces", ":", "\n", "            ", "return", "cls", ".", "create", "(", "file_hint", ")", "\n", "", "elif", "default_file_type", "is", "not", "None", ":", "\n", "            ", "return", "cls", ".", "create", "(", "default_file_type", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"File hint {} not recognized. Supported file types include {}\"", ".", "format", "(", "\n", "file_hint", ",", "list", "(", "cls", ".", "registered_interfaces", ".", "keys", "(", ")", ")", ")", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.__init__": [[6, 19], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "model", ":", "Callable", ",", "encoders", ":", "Dict", ",", "preprocess_func", ":", "Callable", ",", "transform_func", ":", "Callable", ")", ":", "\n", "        ", "\"\"\"\n        Base class for a machine learning model predictor.\n\n        Args:\n            model: A trained model with a `predict` method.\n            encoders: Dictionary of encoders to be used during feature transformation.\n            transform_func: Function to transform input data into features for the model.\n        \"\"\"", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "preprocess_fun", "=", "preprocess_func", "\n", "self", ".", "encoders", "=", "encoders", "\n", "self", ".", "transform_func", "=", "transform_func", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict": [[20, 44], ["predictor.Predictor.preprocess_fun", "predictor.Predictor.transform_func", "predictor.Predictor.model.predict"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict"], ["", "def", "predict", "(", "self", ",", "data_point", ":", "Dict", "[", "str", ",", "Any", "]", ")", ":", "\n", "        ", "\"\"\"\n        Make a prediction.\n\n        Args:\n            data_point: A dictionary containing all the keys necessary to describe the data point.\n\n        Returns: prediction value.\n\n        \"\"\"", "\n", "# wrap the data_point in a dict to mimic the structure expected by preprocess_func,", "\n", "#   which is a dict of data point dicts", "\n", "data_point", "[", "'entity_id'", "]", "=", "0", "\n", "wrapped_data_point", "=", "{", "0", ":", "data_point", "}", "\n", "wrapped_data_processed", "=", "self", ".", "preprocess_fun", "(", "wrapped_data_point", ")", "\n", "\n", "# unwrap the data point", "\n", "data_processed", "=", "wrapped_data_processed", "[", "0", "]", "\n", "\n", "# but then put it in a list, because that's what the transform_func expects :(", "\n", "rewrapped_data_processed", "=", "[", "data_processed", "]", "\n", "\n", "x", ",", "feature_cols", "=", "self", ".", "transform_func", "(", "rewrapped_data_processed", ",", "self", ".", "encoders", ")", "\n", "return", "self", ".", "model", ".", "predict", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.__init__": [[26, 99], ["print", "print", "transformations.Transformer.transforms.append", "transformations.Transformer.transforms.append", "transformations.Transformer.transforms.append", "transformations.Transformer.annotations.append", "transformations.Transformer.annotations.append", "transformations.Transformer.transforms.append", "transformations.Transformer.transforms.append", "transformations.Transformer.transforms.append", "WordTokenizer", "transformations.Transformer.transforms.append", "nltk.tokenize.punkt.PunktSentenceTokenizer", "transformations.Transformer.transforms.append", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "leave_some_html", ":", "bool", "=", "False", ",", "html_to_plain_text", ":", "bool", "=", "False", ",", "segment_into", ":", "str", "=", "None", ",", "\n", "remove_newlines", ":", "bool", "=", "True", ",", "flatten", ":", "bool", "=", "False", ",", "annotate_html", ":", "bool", "=", "False", ",", "\n", "parallelism", ":", "bool", "=", "False", ",", "num_cores", "=", "8", ")", ":", "\n", "        ", "\"\"\"\n        Sets the parameters of the Transformer object.\n\n        Args:\n            leave_some_html: bool. Whether or not to leave some html in the text.\n            html_to_plain_text: bool. Convert html tags into plain text so as not to break text segmentation.\n\n            segment_into: str. segment the text into words, sentences, or paragraphs.\n            flatten: bool. Whether to flatten the segmented texts list(dict(list)) into a single master list(dict).\n\n            annotate_html: bool. Set annotations on the dict about the presence of html tags. Also removes html tags.\n\n            parallelism: bool. Whether to run the transforms in parallel. Not compatible with sentence segmentation.\n            num_cores: int. Number of cores to use when using multiprocessing.\n\n        Returns: None\n\n        \"\"\"", "\n", "self", ".", "transforms", "=", "[", "]", "\n", "self", ".", "annotations", "=", "[", "]", "\n", "self", ".", "parallelism", "=", "parallelism", "\n", "self", ".", "num_cores", "=", "num_cores", "\n", "self", ".", "flatten", "=", "flatten", "\n", "\n", "if", "leave_some_html", "and", "segment_into", "is", "not", "None", "and", "html_to_plain_text", "is", "False", ":", "\n", "            ", "print", "(", "\"WARNING: segmentation does not work well with html remaining in the sentence. Consider setting \"", "\n", "\"html_to_plain_text to True. \"", ")", "\n", "\n", "", "if", "flatten", "and", "segment_into", "is", "None", ":", "\n", "            ", "print", "(", "\"WARNING: Flattening should only be applied when segmentation is also applied. Make sur you know \"", "\n", "\"what you're doing! \"", ")", "\n", "\n", "# text cleaning transform to use", "\n", "", "if", "leave_some_html", ":", "\n", "            ", "if", "html_to_plain_text", ":", "\n", "                ", "self", ".", "transforms", ".", "append", "(", "self", ".", "_to_limited_html_plain_text", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "transforms", ".", "append", "(", "self", ".", "_to_limited_html", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "transforms", ".", "append", "(", "self", ".", "_to_text", ")", "\n", "\n", "# segmentation transform to use", "\n", "", "if", "segment_into", "is", "None", ":", "\n", "            ", "pass", "\n", "", "elif", "segment_into", "in", "{", "'w'", ",", "'word'", ",", "'words'", "}", ":", "\n", "            ", "from", "allennlp", ".", "data", ".", "tokenizers", ".", "word_tokenizer", "import", "WordTokenizer", "\n", "self", ".", "transforms", ".", "append", "(", "self", ".", "_to_words", ")", "\n", "self", ".", "segmentation_type", "=", "'words'", "\n", "self", ".", "segmenter_helper_obj", "=", "WordTokenizer", "(", ")", "\n", "", "elif", "segment_into", "in", "{", "'s'", ",", "'sent'", ",", "'sents'", ",", "'sentence'", ",", "'sentences'", "}", ":", "\n", "            ", "self", ".", "transforms", ".", "append", "(", "self", ".", "_to_sentences", ")", "\n", "self", ".", "segmentation_type", "=", "'sentences'", "\n", "# nlp = spacy.load('en', disable=['ner'])", "\n", "# self.segmenter_helper_obj = nlp", "\n", "self", ".", "segmenter_helper_obj", "=", "PunktSentenceTokenizer", "(", ")", "\n", "", "elif", "segment_into", "in", "{", "'p'", ",", "'para'", ",", "'paragraph'", ",", "'paragraphs'", "}", ":", "\n", "            ", "self", ".", "transforms", ".", "append", "(", "self", ".", "_to_paragraphs", ")", "\n", "self", ".", "segmentation_type", "=", "'paragraphs'", "\n", "self", ".", "segmenter_helper_obj", "=", "None", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid segment_type: {}\"", ".", "format", "(", "segment_into", ")", ")", "\n", "\n", "", "if", "remove_newlines", ":", "\n", "            ", "self", ".", "transforms", ".", "append", "(", "self", ".", "remove_newlines", ")", "\n", "self", ".", "transforms", ".", "append", "(", "self", ".", "regex_out_punctuation_and_white_space", ")", "\n", "\n", "# annotations to use", "\n", "", "if", "annotate_html", ":", "\n", "            ", "self", ".", "annotations", ".", "append", "(", "self", ".", "_annotate_and_clean_html", ")", "\n", "self", ".", "annotations", ".", "append", "(", "self", ".", "_annotate_internal_external_links", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._apply_transforms_to_str": [[100, 105], ["f"], "methods", ["None"], ["", "", "def", "_apply_transforms_to_str", "(", "self", ",", "content", ":", "str", ")", "->", "str", ":", "\n", "        ", "\"\"\"Apply list all transforms to str. \"\"\"", "\n", "for", "f", "in", "self", ".", "transforms", ":", "\n", "            ", "content", "=", "f", "(", "content", ")", "\n", "", "return", "content", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._transform_worker": [[106, 111], ["transformations.Transformer._apply_transforms_to_str"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._apply_transforms_to_str"], ["", "def", "_transform_worker", "(", "self", ",", "obj", ":", "Dict", ")", "->", "Dict", ":", "\n", "        ", "\"\"\"Create a new transformed object. \"\"\"", "\n", "transformed_obj", "=", "{", "key", ":", "obj", "[", "key", "]", "for", "key", "in", "obj", "if", "key", "!=", "'content'", "}", "\n", "transformed_obj", "[", "'content'", "]", "=", "self", ".", "_apply_transforms_to_str", "(", "obj", "[", "'content'", "]", ")", "\n", "return", "transformed_obj", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._apply_annotations_to_dict": [[112, 118], ["f"], "methods", ["None"], ["", "def", "_apply_annotations_to_dict", "(", "self", ",", "content", ":", "Dict", ")", "->", "Dict", ":", "\n", "        ", "\"\"\"Apply list all annotations to dict. \"\"\"", "\n", "\n", "for", "f", "in", "self", ".", "annotations", ":", "\n", "            ", "content", "=", "f", "(", "content", ")", "\n", "", "return", "content", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotation_worker": [[119, 122], ["transformations.Transformer._apply_annotations_to_dict"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._apply_annotations_to_dict"], ["", "def", "_annotation_worker", "(", "self", ",", "obj", ":", "Dict", ")", "->", "Dict", ":", "\n", "        ", "\"\"\"Create a new transformed object. \"\"\"", "\n", "return", "self", ".", "_apply_annotations_to_dict", "(", "obj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply_in_parallel": [[123, 130], ["multiprocessing.Pool", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join"], "methods", ["None"], ["", "def", "apply_in_parallel", "(", "self", ",", "input_list", ":", "List", "[", "Dict", "]", ",", "worker", ":", "Callable", ")", "->", "List", ":", "\n", "        ", "\"\"\"Run all transforms on input_list in parallel. \"\"\"", "\n", "pool", "=", "mp", ".", "Pool", "(", "self", ".", "num_cores", ")", "\n", "results", "=", "pool", ".", "map", "(", "worker", ",", "(", "i", "for", "i", "in", "input_list", ")", ")", "\n", "pool", ".", "close", "(", ")", "\n", "pool", ".", "join", "(", ")", "\n", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply_in_series": [[131, 137], ["results.append", "worker"], "methods", ["None"], ["", "def", "apply_in_series", "(", "self", ",", "input_list", ":", "List", "[", "Dict", "]", ",", "worker", ":", "Callable", ")", "->", "List", ":", "\n", "        ", "\"\"\"Run all transforms on input_list in series. \"\"\"", "\n", "results", "=", "[", "]", "\n", "for", "i", "in", "input_list", ":", "\n", "            ", "results", ".", "append", "(", "worker", "(", "i", ")", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._apply_transforms": [[138, 144], ["transformations.Transformer.apply_in_parallel", "transformations.Transformer.apply_in_series"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply_in_parallel", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply_in_series"], ["", "def", "_apply_transforms", "(", "self", ",", "input_list", ":", "List", "[", "Dict", "]", ")", "->", "List", ":", "\n", "        ", "if", "self", ".", "parallelism", ":", "\n", "            ", "result", "=", "self", ".", "apply_in_parallel", "(", "input_list", ",", "worker", "=", "self", ".", "_transform_worker", ")", "\n", "", "else", ":", "\n", "            ", "result", "=", "self", ".", "apply_in_series", "(", "input_list", ",", "worker", "=", "self", ".", "_transform_worker", ")", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._apply_annotations": [[145, 151], ["transformations.Transformer.apply_in_parallel", "transformations.Transformer.apply_in_series"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply_in_parallel", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply_in_series"], ["", "def", "_apply_annotations", "(", "self", ",", "input_list", ":", "List", "[", "Dict", "]", ")", "->", "List", ":", "\n", "        ", "if", "self", ".", "parallelism", ":", "\n", "            ", "result", "=", "self", ".", "apply_in_parallel", "(", "input_list", ",", "worker", "=", "self", ".", "_annotation_worker", ")", "\n", "", "else", ":", "\n", "            ", "result", "=", "self", ".", "apply_in_series", "(", "input_list", ",", "worker", "=", "self", ".", "_annotation_worker", ")", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply": [[152, 172], ["transformations.Transformer._apply_transforms", "transformations.Transformer._apply_annotations", "transformations.convert_list_of_dicts_to_dict_of_dicts", "transformations.Transformer._flatten_text_dicts"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._apply_transforms", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._apply_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.convert_list_of_dicts_to_dict_of_dicts", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._flatten_text_dicts"], ["", "def", "apply", "(", "self", ",", "inputs", ":", "Dict", "[", "int", ",", "Dict", "]", ")", "->", "Dict", "[", "int", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"Run all transforms and annotations on input_list.\n        Converts a Dict of {entity_id_int: data_dict} to a list of [data_dict] for processing, and then back to dict\n        of dicts on return.\n        \"\"\"", "\n", "\n", "input_list", "=", "[", "inputs", "[", "id", "]", "for", "id", "in", "inputs", "]", "\n", "\n", "# run transformations, including segmentation, at the string level", "\n", "result", "=", "self", ".", "_apply_transforms", "(", "input_list", ")", "\n", "\n", "# if segmentation occurred, flatten list of dicts with now lists in 'content' into one single list of dicts", "\n", "if", "self", ".", "flatten", ":", "\n", "            ", "result", "=", "self", ".", "_flatten_text_dicts", "(", "result", ")", "\n", "\n", "# run annotations, at the dict level", "\n", "", "result", "=", "self", ".", "_apply_annotations", "(", "result", ")", "\n", "\n", "back_to_dict", "=", "convert_list_of_dicts_to_dict_of_dicts", "(", "result", ")", "\n", "return", "back_to_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._to_limited_html": [[175, 196], ["bs4.BeautifulSoup", "transformations.Transformer.remove_tags_and_contents", "transformations.Transformer.remove_other_xml", "transformations.Transformer.reformat_html_link_tags", "transformations.Transformer.replace_html", "transformations.Transformer.replace_chars", "transformations.Transformer.regex_out_punctuation_and_white_space", "transformations.Transformer.condense_line_breaks"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.remove_tags_and_contents", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.remove_other_xml", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.reformat_html_link_tags", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_html", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_chars", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks"], ["", "def", "_to_limited_html", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "soup", "=", "BeautifulSoup", "(", "x", ",", "features", "=", "\"html.parser\"", ")", "\n", "soup", "=", "self", ".", "remove_tags_and_contents", "(", "soup", ",", "[", "'style'", ",", "'script'", "]", ")", "\n", "soup", "=", "self", ".", "remove_other_xml", "(", "soup", ")", "\n", "soup", "=", "self", ".", "reformat_html_link_tags", "(", "soup", ")", "\n", "\n", "tags_to_keep", "=", "{", "'h1'", ",", "'h2'", ",", "'h3'", ",", "'h4'", "}", "\n", "tags_to_keep_with_attr", "=", "{", "'a'", "}", "\n", "tags_to_replace", "=", "{", "\n", "'br'", ":", "(", "'.\\n'", ",", "'.\\n'", ")", ",", "\n", "'p'", ":", "(", "'\\n'", ",", "'\\n'", ")", ",", "\n", "}", "\n", "default_tag_replacement_str", "=", "''", "\n", "text", "=", "self", ".", "replace_html", "(", "soup", ",", "tags_to_keep", ",", "tags_to_keep_with_attr", ",", "tags_to_replace", ",", "\n", "default_tag_replacement_str", ")", "\n", "\n", "text", "=", "self", ".", "replace_chars", "(", "text", ",", "[", "'\\t'", ",", "'\\xa0'", "]", ",", "' '", ")", "\n", "text", "=", "self", ".", "regex_out_punctuation_and_white_space", "(", "text", ")", "\n", "text", "=", "self", ".", "condense_line_breaks", "(", "text", ")", "\n", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._to_limited_html_plain_text": [[197, 226], ["transformations.Transformer.clear_non_rendered_html", "bs4.BeautifulSoup", "transformations.Transformer.remove_tags_and_contents", "transformations.Transformer.remove_other_xml", "set", "set", "transformations.Transformer.replace_html", "transformations.Transformer.replace_chars", "transformations.Transformer.regex_out_punctuation_and_white_space", "transformations.Transformer.condense_line_breaks"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.clear_non_rendered_html", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.remove_tags_and_contents", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.remove_other_xml", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_html", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_chars", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks"], ["", "def", "_to_limited_html_plain_text", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "clean_x", "=", "self", ".", "clear_non_rendered_html", "(", "x", ")", "\n", "soup", "=", "BeautifulSoup", "(", "clean_x", ",", "features", "=", "\"html.parser\"", ")", "\n", "soup", "=", "self", ".", "remove_tags_and_contents", "(", "soup", ",", "[", "'style'", ",", "'script'", "]", ")", "\n", "soup", "=", "self", ".", "remove_other_xml", "(", "soup", ")", "\n", "\n", "tags_to_keep", "=", "set", "(", ")", "\n", "tags_to_keep_with_attr", "=", "set", "(", ")", "\n", "tags_to_replace", "=", "{", "\n", "'br'", ":", "(", "'.\\n'", ",", "'.\\n'", ")", ",", "\n", "'h1'", ":", "(", "' thisisah1tag '", ",", "'. \\n'", ")", ",", "\n", "'h2'", ":", "(", "' thisisah2tag '", ",", "'. \\n'", ")", ",", "\n", "'h3'", ":", "(", "' thisisah3tag '", ",", "'. \\n'", ")", ",", "\n", "'h4'", ":", "(", "' thisisah4tag '", ",", "'. \\n'", ")", ",", "\n", "'a'", ":", "(", "' thisisalinktag '", ",", "' '", ")", ",", "\n", "'li'", ":", "(", "'\\n thisisalistitemtag '", ",", "'. \\n'", ")", ",", "\n", "'tr'", ":", "(", "'\\n thisisatablerowtag '", ",", "'. \\n'", ")", ",", "\n", "'p'", ":", "(", "'\\n'", ",", "'. \\n'", ")", ",", "\n", "'div'", ":", "(", "'. \\n'", ",", "'. \\n'", ")", ",", "\n", "}", "\n", "default_tag_replacement_str", "=", "''", "\n", "text", "=", "self", ".", "replace_html", "(", "soup", ",", "tags_to_keep", ",", "tags_to_keep_with_attr", ",", "tags_to_replace", ",", "\n", "default_tag_replacement_str", ",", "include_link_domains", "=", "True", ")", "\n", "\n", "text", "=", "self", ".", "replace_chars", "(", "text", ",", "[", "'\\t'", ",", "'\\xa0'", "]", ",", "' '", ")", "\n", "text", "=", "self", ".", "regex_out_punctuation_and_white_space", "(", "text", ")", "\n", "text", "=", "self", ".", "condense_line_breaks", "(", "text", ")", "\n", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._to_text": [[227, 253], ["transformations.Transformer.clear_non_rendered_html", "bs4.BeautifulSoup", "transformations.Transformer.remove_tags_and_contents", "transformations.Transformer.remove_other_xml", "set", "set", "transformations.Transformer.replace_html", "transformations.Transformer.replace_chars", "transformations.Transformer.regex_out_punctuation_and_white_space", "transformations.Transformer.condense_line_breaks"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.clear_non_rendered_html", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.remove_tags_and_contents", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.remove_other_xml", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_html", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_chars", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks"], ["", "def", "_to_text", "(", "self", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "clean_x", "=", "self", ".", "clear_non_rendered_html", "(", "x", ")", "\n", "soup", "=", "BeautifulSoup", "(", "clean_x", ",", "features", "=", "\"html.parser\"", ")", "\n", "soup", "=", "self", ".", "remove_tags_and_contents", "(", "soup", ",", "[", "'style'", ",", "'script'", "]", ")", "\n", "soup", "=", "self", ".", "remove_other_xml", "(", "soup", ")", "\n", "\n", "tags_to_keep", "=", "set", "(", ")", "\n", "tags_to_keep_with_attr", "=", "set", "(", ")", "\n", "tags_to_replace", "=", "{", "\n", "'br'", ":", "(", "'.\\n'", ",", "'.\\n'", ")", ",", "\n", "'h1'", ":", "(", "'\\n'", ",", "'. \\n'", ")", ",", "\n", "'h2'", ":", "(", "'\\n'", ",", "'. \\n'", ")", ",", "\n", "'h3'", ":", "(", "'\\n'", ",", "'. \\n'", ")", ",", "\n", "'h4'", ":", "(", "'\\n'", ",", "'. \\n'", ")", ",", "\n", "'p'", ":", "(", "'\\n'", ",", "'. \\n'", ")", ",", "\n", "'div'", ":", "(", "'\\n'", ",", "'. \\n'", ")", ",", "\n", "}", "\n", "default_tag_replacement_str", "=", "''", "\n", "text", "=", "self", ".", "replace_html", "(", "soup", ",", "tags_to_keep", ",", "tags_to_keep_with_attr", ",", "tags_to_replace", ",", "\n", "default_tag_replacement_str", ")", "\n", "\n", "text", "=", "self", ".", "replace_chars", "(", "text", ",", "[", "'\\t'", ",", "'\\xa0'", "]", ",", "' '", ")", "\n", "text", "=", "self", ".", "regex_out_punctuation_and_white_space", "(", "text", ")", "\n", "text", "=", "self", ".", "condense_line_breaks", "(", "text", ")", "\n", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._to_words": [[256, 259], ["str", "tok.tokenize"], "methods", ["None"], ["", "def", "_to_words", "(", "self", ",", "x", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "tok", "=", "self", ".", "segmenter_helper_obj", "\n", "return", "[", "str", "(", "t", ")", "for", "t", "in", "tok", ".", "tokenize", "(", "x", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._to_sentences": [[260, 268], ["sent.strip", "tokenizer.tokenize"], "methods", ["None"], ["", "def", "_to_sentences", "(", "self", ",", "x", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "# spacy sentence tokenizer version", "\n", "# nlp = self.segmenter_helper_obj", "\n", "# doc = nlp(x)", "\n", "# result = [sent.string.strip() for sent in doc.sents]", "\n", "        ", "tokenizer", "=", "self", ".", "segmenter_helper_obj", "\n", "result", "=", "[", "sent", ".", "strip", "(", ")", "for", "sent", "in", "tokenizer", ".", "tokenize", "(", "x", ")", "]", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._to_paragraphs": [[269, 272], ["transformations.Transformer.condense_line_breaks", "transformations.Transformer.split"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "def", "_to_paragraphs", "(", "self", ",", "x", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "        ", "x", "=", "self", ".", "condense_line_breaks", "(", "x", ")", "\n", "return", "x", ".", "split", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._flatten_text_dicts": [[273, 283], ["enumerate", "output.append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_flatten_text_dicts", "(", "list_of_dicts", ":", "List", "[", "Dict", "]", ")", "->", "List", "[", "Dict", "]", ":", "\n", "        ", "output", "=", "[", "]", "\n", "for", "parent_dict", "in", "list_of_dicts", ":", "\n", "            ", "for", "num", ",", "i", "in", "enumerate", "(", "parent_dict", "[", "'content'", "]", ")", ":", "\n", "                ", "child_dict", "=", "{", "key", ":", "parent_dict", "[", "key", "]", "for", "key", "in", "parent_dict", "if", "key", "!=", "'content'", "}", "\n", "child_dict", "[", "'content'", "]", "=", "i", "\n", "child_dict", "[", "'sub_id'", "]", "=", "num", "\n", "output", ".", "append", "(", "child_dict", ")", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.remove_tags_and_contents": [[285, 293], ["soup.find_all", "tag.decompose"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "remove_tags_and_contents", "(", "soup", ":", "BeautifulSoup", ",", "tags", ":", "List", "[", "str", "]", ")", "->", "BeautifulSoup", ":", "\n", "        ", "\"\"\"Remove specific tags from the html, including their entire contents.\"\"\"", "\n", "for", "tag", "in", "soup", ".", "find_all", "(", "True", ")", ":", "\n", "            ", "if", "tag", ".", "name", "in", "tags", ":", "\n", "# delete tag and its contents", "\n", "                ", "tag", ".", "decompose", "(", ")", "\n", "", "", "return", "soup", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.remove_other_xml": [[294, 304], ["soup.find_all", "tag.extract", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "remove_other_xml", "(", "soup", ":", "BeautifulSoup", ")", "->", "BeautifulSoup", ":", "\n", "        ", "for", "tag", "in", "soup", ".", "find_all", "(", "string", "=", "lambda", "text", ":", "isinstance", "(", "text", ",", "Comment", ")", "\n", "or", "isinstance", "(", "text", ",", "CData", ")", "\n", "or", "isinstance", "(", "text", ",", "ProcessingInstruction", ")", "\n", "or", "isinstance", "(", "text", ",", "Declaration", ")", "\n", "or", "isinstance", "(", "text", ",", "Doctype", ")", "\n", ")", ":", "\n", "            ", "tag", ".", "extract", "(", ")", "\n", "", "return", "soup", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.reformat_html_link_tags": [[305, 330], ["soup.find_all", "dict", "tldextract.extract"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "reformat_html_link_tags", "(", "soup", ":", "BeautifulSoup", ")", "->", "BeautifulSoup", ":", "\n", "        ", "\"\"\"\n        Reformat html link tags to have no attributes other than the href or src.\n        Set the href/src to just the domain name of the link.\n\n        Note: we want drugs.rcpsych.co.uk and depression.rcpsych.co.uk to both resolve to rcpsych.\n\n        Args:\n            soup: BeautifulSoup object parsing an html\n\n        Returns: BeautifulSoup\n\n        \"\"\"", "\n", "for", "tag", "in", "soup", ".", "find_all", "(", "True", ")", ":", "\n", "            ", "if", "tag", ".", "name", "==", "'a'", ":", "\n", "                ", "attrs", "=", "dict", "(", "tag", ".", "attrs", ")", "\n", "for", "attr", "in", "attrs", ":", "\n", "                    ", "if", "attr", "in", "[", "'src'", ",", "'href'", "]", ":", "\n", "                        ", "url", "=", "tag", ".", "attrs", "[", "attr", "]", "\n", "domain", "=", "tldextract", ".", "extract", "(", "url", ")", ".", "domain", "\n", "tag", ".", "attrs", "[", "attr", "]", "=", "domain", "\n", "", "else", ":", "\n", "                        ", "del", "tag", ".", "attrs", "[", "attr", "]", "\n", "", "", "", "", "return", "soup", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.get_domain_from_link_tag": [[331, 344], ["dict", "tldextract.extract"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_domain_from_link_tag", "(", "tag", ":", "Tag", ")", "->", "str", ":", "\n", "        ", "\"\"\"Extract the domain from a url. If no domain is found, assume link is a filepath, and return NA\"\"\"", "\n", "attrs", "=", "dict", "(", "tag", ".", "attrs", ")", "\n", "for", "attr", "in", "attrs", ":", "\n", "            ", "if", "attr", "in", "[", "'src'", ",", "'href'", "]", ":", "\n", "                ", "url", "=", "tag", ".", "attrs", "[", "attr", "]", "\n", "domain", "=", "tldextract", ".", "extract", "(", "url", ")", ".", "domain", "\n", "if", "domain", "!=", "''", ":", "\n", "                    ", "return", "domain", "\n", "", "else", ":", "\n", "                    ", "return", "'NA'", "\n", "", "", "", "return", "'NA'", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_html": [[345, 392], ["set", "soup.find_all", "transformations.Transformer.soup_to_text_with_tags", "set", "tags_to_replace_with_str.keys", "soup.find_all", "tags_to_replace_with_str.get", "tag.insert_before", "tag.insert_after", "tag.unwrap", "transformations.Transformer.get_domain_from_link_tag", "start_tag_replacement.rstrip"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.soup_to_text_with_tags", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.get_domain_from_link_tag"], ["", "def", "replace_html", "(", "self", ",", "soup", ":", "BeautifulSoup", ",", "tags_to_keep", ":", "Set", "[", "str", "]", ",", "tags_to_keep_with_attr", ":", "Set", "[", "str", "]", ",", "\n", "tags_to_replace_with_str", ":", "Dict", "[", "str", ",", "Tuple", "[", "str", ",", "str", "]", "]", ",", "default_tag_replacement_str", ":", "str", ",", "\n", "include_link_domains", "=", "False", ")", "->", "str", ":", "\n", "        ", "\"\"\"\n        Finds all tags in an html BeautifulSoup object and replaces/keeps the tags in accordance with args.\n\n        Args:\n            soup: BeautifulSoup object parsing an html\n            tags_to_keep: html tags to leave but remove tag attributes\n            tags_to_keep_with_attr: html tags to leave intact\n            tags_to_replace_with_str: html tags to replace with strings defined in replacement Tuple(start_tag, end_tag)\n            default_tag_replacement_str: string to use if no replacement is defined in tags_to_replace_with_str\n            include_link_domains: bool. Append the domain of the linked url to the replacement tag\n\n        Returns: str\n\n        \"\"\"", "\n", "\n", "all_tags", "=", "set", "(", "[", "tag", ".", "name", "for", "tag", "in", "soup", ".", "find_all", "(", ")", "]", ")", "\n", "tags_to_replace", "=", "all_tags", "-", "tags_to_keep", "-", "tags_to_keep_with_attr", "\n", "tags_to_replace", "=", "tags_to_replace", "|", "set", "(", "tags_to_replace_with_str", ".", "keys", "(", ")", ")", "\n", "\n", "default_replacement_tuple", "=", "(", "default_tag_replacement_str", ",", "default_tag_replacement_str", ")", "\n", "\n", "for", "tag", "in", "soup", ".", "find_all", "(", "True", ")", ":", "\n", "            ", "if", "tag", ".", "name", "in", "tags_to_keep_with_attr", ":", "\n", "# keep tag, including attributes", "\n", "                ", "pass", "\n", "", "elif", "tag", ".", "name", "in", "tags_to_keep", ":", "\n", "# keep tag but clear all attributes", "\n", "                ", "tag", ".", "attrs", "=", "{", "}", "\n", "", "elif", "tag", ".", "name", "in", "tags_to_replace", ":", "\n", "# if tag replacement is not specified, use the default", "\n", "                ", "r", "=", "tags_to_replace_with_str", ".", "get", "(", "tag", ".", "name", ",", "default_replacement_tuple", ")", "\n", "start_tag_replacement", "=", "r", "[", "0", "]", "\n", "end_tag_replacement", "=", "r", "[", "1", "]", "\n", "\n", "if", "tag", ".", "name", "==", "'a'", "and", "include_link_domains", ":", "\n", "                    ", "domain", "=", "self", ".", "get_domain_from_link_tag", "(", "tag", ")", "\n", "start_tag_replacement", "=", "start_tag_replacement", ".", "rstrip", "(", ")", "+", "domain", "+", "' '", "\n", "", "tag", ".", "insert_before", "(", "start_tag_replacement", ")", "\n", "tag", ".", "insert_after", "(", "end_tag_replacement", ")", "\n", "# remove the tag without removing the tag's contents (text and children tags)", "\n", "tag", ".", "unwrap", "(", ")", "\n", "\n", "", "", "text", "=", "self", ".", "soup_to_text_with_tags", "(", "soup", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.soup_to_text_with_tags": [[393, 399], ["str", "html.unescape"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "soup_to_text_with_tags", "(", "soup", ":", "BeautifulSoup", ")", "->", "str", ":", "\n", "        ", "\"\"\"Convert a BeautifulSoup object to a string while leaving the html tags in place.\"\"\"", "\n", "text", "=", "str", "(", "soup", ")", "\n", "text", "=", "html", ".", "unescape", "(", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.clear_non_rendered_html": [[402, 405], ["text.replace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "clear_non_rendered_html", "(", "text", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "text", ".", "replace", "(", "'\\n'", ",", "' '", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space": [[406, 428], ["text.replace().lstrip.replace().lstrip.replace", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "text.replace().lstrip.replace().lstrip.lstrip", "text.replace().lstrip.replace().lstrip.replace().lstrip", "len", "text.replace().lstrip.replace().lstrip.replace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "regex_out_punctuation_and_white_space", "(", "text", ":", "str", ")", "->", "str", ":", "\n", "        ", "\"\"\"Clean up excess whitespace and punctuation.\"\"\"", "\n", "text", "=", "text", ".", "replace", "(", "'?.'", ",", "'?'", ")", "\n", "\n", "# replaces multiple spaces wth a single space", "\n", "text", "=", "re", ".", "sub", "(", "r' +'", ",", "' '", ",", "text", ")", "\n", "# replace occurences of '.' followed by any combination of '.', ' ', or '\\n' with single '.'", "\n", "#  for handling html -> '.' replacement.", "\n", "text", "=", "re", ".", "sub", "(", "r\"[.][. ]{2,}\"", ",", "'. '", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"[?][. ]{2,}\"", ",", "'? '", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"[!][. ]{2,}\"", ",", "'! '", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"[.][. \\n]{2,}\"", ",", "'. \\n'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"[?][. \\n]{2,}\"", ",", "'? \\n'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r\"[!][. \\n]{2,}\"", ",", "'! \\n'", ",", "text", ")", "\n", "\n", "# if there is a period at the very start of the document, remove it (replace 1 time)", "\n", "text", "=", "text", ".", "lstrip", "(", ")", "\n", "if", "len", "(", "text", ")", ">", "0", "and", "text", "[", "0", "]", "==", "'.'", ":", "\n", "            ", "text", "=", "text", ".", "replace", "(", "'.'", ",", "''", ",", "1", ")", ".", "lstrip", "(", ")", "\n", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks": [[429, 440], ["re.sub().strip", "re.sub", "re.sub", "re.sub"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "condense_line_breaks", "(", "text", ":", "str", ")", "->", "str", ":", "\n", "# replaces multiple spaces wth a single space", "\n", "        ", "text", "=", "re", ".", "sub", "(", "r' +'", ",", "' '", ",", "text", ")", ".", "strip", "(", ")", "\n", "\n", "# replace html line breaks with new line characters", "\n", "text", "=", "re", ".", "sub", "(", "r'<br[/]*>'", ",", "'\\n'", ",", "text", ")", "\n", "\n", "# replace any combination of ' ' and '\\n' with single ' \\n'", "\n", "text", "=", "re", ".", "sub", "(", "r\"[ \\n]{2,}\"", ",", "' \\n'", ",", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_chars": [[441, 447], ["x.replace.replace.replace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "replace_chars", "(", "x", ":", "str", ",", "chars_to_replace", ":", "List", "[", "str", "]", ",", "replacement_char", ":", "str", ")", "->", "str", ":", "\n", "        ", "\"\"\"Replace all chars_to_replace with replacement_char. \"\"\"", "\n", "for", "p", "in", "chars_to_replace", ":", "\n", "            ", "x", "=", "x", ".", "replace", "(", "p", ",", "replacement_char", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.remove_newlines": [[448, 451], ["cls.replace_chars"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_chars"], ["", "@", "classmethod", "\n", "def", "remove_newlines", "(", "cls", ",", "x", ":", "str", ")", "->", "str", ":", "\n", "        ", "return", "cls", ".", "replace_chars", "(", "x", ",", "[", "'\\n'", "]", ",", "' '", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._retrieve_domain_from_plaintexttag": [[454, 462], ["token.find", "tag_and_domain.replace", "token.replace"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_retrieve_domain_from_plaintexttag", "(", "token", ":", "str", ")", "->", "(", "str", ",", "str", ")", ":", "\n", "        ", "plaintexttag", "=", "\"thisisalinktag\"", "\n", "tag_pos", "=", "token", ".", "find", "(", "plaintexttag", ")", "\n", "tag_and_domain", "=", "token", "[", "tag_pos", ":", "]", "\n", "domain", "=", "tag_and_domain", ".", "replace", "(", "plaintexttag", ",", "''", ")", "\n", "cleaned_token", "=", "token", ".", "replace", "(", "tag_and_domain", ",", "''", ")", "\n", "return", "cleaned_token", ",", "domain", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_and_clean_html": [[463, 499], ["d[].split", "enumerate", "text_without_tags.rstrip", "found_tags.append", "d[].replace().strip", "found_tags.append", "cls._retrieve_domain_from_plaintexttag", "domains.append", "d[].replace"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._retrieve_domain_from_plaintexttag"], ["", "@", "classmethod", "\n", "def", "_annotate_and_clean_html", "(", "cls", ",", "d", ":", "Dict", ",", "extract_domains", "=", "True", ")", "->", "Dict", ":", "\n", "        ", "tags", "=", "{", "\n", "'thisisah1tag'", ":", "'h1'", ",", "\n", "'thisisah2tag'", ":", "'h2'", ",", "\n", "'thisisah3tag'", ":", "'h3'", ",", "\n", "'thisisah4tag'", ":", "'h4'", ",", "\n", "'thisisalinktag'", ":", "'a'", ",", "\n", "'thisisalistitemtag'", ":", "'li'", ",", "\n", "'thisisatablerowtag'", ":", "'tr'", ",", "\n", "}", "\n", "found_tags", "=", "[", "]", "\n", "domains", "=", "[", "]", "\n", "for", "plaintexttag", "in", "tags", ":", "\n", "            ", "if", "plaintexttag", "in", "d", "[", "'content'", "]", ":", "\n", "                ", "if", "plaintexttag", "==", "'thisisalinktag'", "and", "extract_domains", ":", "\n", "# TODO: this space splitting is not robust", "\n", "                    ", "tokens", "=", "d", "[", "'content'", "]", ".", "split", "(", "' '", ")", "\n", "text_without_tags", "=", "''", "\n", "for", "i", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "                        ", "if", "plaintexttag", "in", "token", ":", "\n", "                            ", "cleaned_token", ",", "domain", "=", "cls", ".", "_retrieve_domain_from_plaintexttag", "(", "token", ")", "\n", "domains", ".", "append", "(", "domain", ")", "\n", "if", "cleaned_token", "!=", "''", ":", "\n", "                                ", "text_without_tags", "+=", "cleaned_token", "+", "' '", "\n", "", "", "else", ":", "\n", "                            ", "if", "token", "!=", "''", ":", "\n", "                                ", "text_without_tags", "+=", "token", "+", "' '", "\n", "", "", "", "d", "[", "'content'", "]", "=", "text_without_tags", ".", "rstrip", "(", ")", "\n", "found_tags", ".", "append", "(", "tags", "[", "plaintexttag", "]", ")", "\n", "", "else", ":", "\n", "                    ", "d", "[", "'content'", "]", "=", "d", "[", "'content'", "]", ".", "replace", "(", "plaintexttag", ",", "' '", ")", ".", "strip", "(", ")", "\n", "found_tags", ".", "append", "(", "tags", "[", "plaintexttag", "]", ")", "\n", "", "", "", "d", "[", "'html_tags'", "]", "=", "found_tags", "\n", "d", "[", "'domains'", "]", "=", "domains", "\n", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_internal_external_links": [[500, 516], ["d.keys", "print", "tldextract.extract", "d[].append", "d[].append", "d[].append"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_annotate_internal_external_links", "(", "d", ":", "Dict", ")", "->", "Dict", ":", "\n", "        ", "if", "'url'", "not", "in", "d", ".", "keys", "(", ")", ":", "\n", "            ", "print", "(", "\"WARNING: text url is not available for linked domain comparison\"", ")", "\n", "return", "d", "\n", "", "source_domain", "=", "tldextract", ".", "extract", "(", "d", "[", "'url'", "]", ")", ".", "domain", "\n", "d", "[", "'link_type'", "]", "=", "[", "]", "\n", "for", "link", "in", "d", "[", "'domains'", "]", ":", "\n", "            ", "if", "link", "==", "'NA'", ":", "\n", "# assume links that are not valid urls are internal filepaths", "\n", "                ", "d", "[", "'link_type'", "]", ".", "append", "(", "'internal'", ")", "\n", "", "elif", "link", "==", "source_domain", ":", "\n", "                ", "d", "[", "'link_type'", "]", ".", "append", "(", "'internal'", ")", "\n", "", "else", ":", "\n", "                ", "d", "[", "'link_type'", "]", ".", "append", "(", "'external'", ")", "\n", "", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.get_id": [[522, 530], ["d.keys", "d.keys"], "function", ["None"], ["", "", "def", "get_id", "(", "d", ":", "Dict", ")", "->", "str", ":", "\n", "    ", "id_key", "=", "'id'", "\n", "if", "id_key", "not", "in", "d", ".", "keys", "(", ")", "and", "'entity_id'", "in", "d", ".", "keys", "(", ")", ":", "\n", "        ", "id_key", "=", "'entity_id'", "\n", "", "identifier", "=", "d", "[", "id_key", "]", "\n", "if", "'sub_id'", "in", "d", ":", "\n", "        ", "identifier", "=", "\"{}-{}\"", ".", "format", "(", "d", "[", "id_key", "]", ",", "d", "[", "'sub_id'", "]", ")", "\n", "", "return", "identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.convert_list_of_dicts_to_dict_of_dicts": [[532, 538], ["transformations.get_id"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.get_id"], ["", "def", "convert_list_of_dicts_to_dict_of_dicts", "(", "input_list", ":", "List", "[", "Dict", "]", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "output_dict", "=", "{", "}", "\n", "for", "d", "in", "input_list", ":", "\n", "        ", "id", "=", "get_id", "(", "d", ")", "\n", "output_dict", "[", "id", "]", "=", "d", "\n", "", "return", "output_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.medical_text_detection.determine_medical_yn_metamap": [[17, 52], ["autodiscern.Transformer", "adt.Transformer.apply", "autodiscern.add_inline_citations_annotations", "autodiscern.add_metamap_annotations", "sum", "sum", "sum"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_inline_citations_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_metamap_annotations"], ["def", "determine_medical_yn_metamap", "(", "data_dict", ")", ":", "\n", "    ", "\"\"\"\n    Determine whether the webpage is medical or not.\n\n    data_dict  = {'content': html_page, 'url': url}\n    \"\"\"", "\n", "# prep the data dict into the right format for the transformer", "\n", "data_dict", "[", "'id'", "]", "=", "0", "\n", "data_dict_of_dicts", "=", "{", "0", ":", "data_dict", "}", "\n", "\n", "html_transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "False", ",", "parallelism", "=", "False", ")", "\n", "transformed_data", "=", "html_transformer", ".", "apply", "(", "data_dict_of_dicts", ")", "\n", "transformed_data", "=", "ada", ".", "add_inline_citations_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "ada", ".", "add_metamap_annotations", "(", "transformed_data", ")", "\n", "\n", "# get number of references to medical conditions", "\n", "metamap_references", "=", "transformed_data", "[", "0", "]", "[", "'metamap'", "]", "\n", "medical_disorder_ref_cnt", "=", "sum", "(", "[", "1", "for", "ref", "in", "metamap_references", "if", "ref", "==", "'Disorders'", "]", ")", "\n", "medical_drugs_ref_cnt", "=", "sum", "(", "[", "1", "for", "ref", "in", "metamap_references", "if", "ref", "==", "'Chemicals & Drugs'", "]", ")", "\n", "medical_procedures_ref_cnt", "=", "sum", "(", "[", "1", "for", "ref", "in", "metamap_references", "if", "ref", "==", "'Procedures'", "]", ")", "\n", "\n", "result", "=", "False", "\n", "if", "medical_disorder_ref_cnt", ">", "3", ":", "\n", "        ", "if", "medical_drugs_ref_cnt", "+", "medical_procedures_ref_cnt", ">", "3", ":", "\n", "            ", "result", "=", "True", "\n", "\n", "", "", "things_to_report", "=", "{", "\n", "'result'", ":", "result", ",", "\n", "'metamap'", ":", "transformed_data", "[", "0", "]", "[", "'metamap'", "]", ",", "\n", "'metamap_detail'", ":", "transformed_data", "[", "0", "]", "[", "'metamap_detail'", "]", ",", "\n", "'medical_disorder_ref_cnt'", ":", "medical_disorder_ref_cnt", ",", "\n", "'medical_drugs_ref_cnt'", ":", "medical_drugs_ref_cnt", ",", "\n", "'medical_procedures_ref_cnt'", ":", "medical_procedures_ref_cnt", ",", "\n", "}", "\n", "return", "things_to_report", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.medical_text_detection.test_determine_medical_yn_metamap": [[54, 59], ["requests.get", "requests.get.content.decode", "medical_text_detection.determine_medical_yn_metamap"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.medical_text_detection.determine_medical_yn_metamap"], ["", "def", "test_determine_medical_yn_metamap", "(", "url", ")", ":", "\n", "    ", "res", "=", "requests", ".", "get", "(", "url", ")", "\n", "html_page", "=", "res", ".", "content", ".", "decode", "(", "\"utf-8\"", ")", "\n", "data_dict", "=", "{", "'content'", ":", "html_page", ",", "'url'", ":", "url", "}", "\n", "return", "determine_medical_yn_metamap", "(", "data_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.medical_text_detection.integration_test": [[61, 103], ["print", "medical_text_detection.test_determine_medical_yn_metamap", "test_results.append", "print", "print"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.medical_text_detection.test_determine_medical_yn_metamap"], ["", "def", "integration_test", "(", "subset", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Integration test for test_determine_medical_yn_metamap\n\n    Args:\n        subset: List[str]. optional way to select a subset of test cases, by their name\n\n    Returns: Dict of test results\n\n    \"\"\"", "\n", "\n", "test_cases", "=", "[", "\n", "{", "\n", "'name'", ":", "'mayo clinic depression'", ",", "\n", "'url'", ":", "'https://www.mayoclinic.org/diseases-conditions/depression/symptoms-causes/syc-20356007'", ",", "\n", "'expected_result'", ":", "True", ",", "\n", "}", ",", "\n", "{", "\n", "'name'", ":", "'wikipedia kitten'", ",", "\n", "'url'", ":", "'https://en.wikipedia.org/wiki/Kitten'", ",", "\n", "'expected_result'", ":", "False", ",", "\n", "}", ",", "\n", "{", "\n", "'name'", ":", "'classification'", ",", "\n", "'url'", ":", "'https://en.wikipedia.org/wiki/One-class_classification'", ",", "\n", "'expected_result'", ":", "False", ",", "\n", "}", "\n", "]", "\n", "\n", "test_results", "=", "[", "]", "\n", "for", "test_case", "in", "test_cases", ":", "\n", "        ", "if", "subset", "is", "None", "or", "test_case", "[", "'name'", "]", "in", "subset", ":", "\n", "            ", "print", "(", "\"Running test case '{}'\"", ".", "format", "(", "test_case", "[", "'name'", "]", ")", ")", "\n", "test_result", "=", "test_determine_medical_yn_metamap", "(", "test_case", "[", "'url'", "]", ")", "\n", "test_results", ".", "append", "(", "test_result", ")", "\n", "\n", "if", "test_result", "[", "'result'", "]", "==", "test_case", "[", "'expected_result'", "]", ":", "\n", "                ", "print", "(", "\"=== Test case '{}' passed ===\\n\"", ".", "format", "(", "test_case", "[", "'name'", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "\">>> Test case '{}' FAILED <<<\\n\"", ".", "format", "(", "test_case", "[", "'name'", "]", ")", ")", "\n", "\n", "", "", "", "return", "test_results", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.medical_text_detection.view_relevant_metamap_details": [[105, 113], ["enumerate", "print", "print", "print", "print", "print"], "function", ["None"], ["", "def", "view_relevant_metamap_details", "(", "test_result", ")", ":", "\n", "    ", "for", "i", ",", "category", "in", "enumerate", "(", "test_result", "[", "'metamap'", "]", ")", ":", "\n", "        ", "if", "category", "in", "[", "'Disorders'", ",", "'Chemicals & Drugs'", ",", "'Procedures'", "]", ":", "\n", "            ", "print", "(", "category", ")", "\n", "print", "(", "'trigger: {}'", ".", "format", "(", "test_result", "[", "'metamap_detail'", "]", "[", "i", "]", "[", "'trigger'", "]", ")", ")", "\n", "print", "(", "'preferred_name: {}'", ".", "format", "(", "test_result", "[", "'metamap_detail'", "]", "[", "i", "]", "[", "'preferred_name'", "]", ")", ")", "\n", "print", "(", "'semtypes: {}'", ".", "format", "(", "test_result", "[", "'metamap_detail'", "]", "[", "i", "]", "[", "'semtypes'", "]", ")", ")", "\n", "print", "(", ")", "\n", "# print(test_result['metamap_detail'][i])", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_word_token_annotations": [[10, 17], ["WordTokenizer", "WordTokenizer.tokenize"], "function", ["None"], ["def", "add_word_token_annotations", "(", "inputs", ":", "Dict", "[", "str", ",", "Dict", "]", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "from", "allennlp", ".", "data", ".", "tokenizers", ".", "word_tokenizer", "import", "WordTokenizer", "\n", "tok", "=", "WordTokenizer", "(", ")", "\n", "for", "id", "in", "inputs", ":", "\n", "# have to convert tokens to text because spacy tokens are not pickleable", "\n", "        ", "inputs", "[", "id", "]", "[", "'tokens'", "]", "=", "[", "t", ".", "text", "for", "t", "in", "tok", ".", "tokenize", "(", "inputs", "[", "id", "]", "[", "'content'", "]", ")", "]", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.remove_punctuation": [[19, 23], ["s.replace().replace().replace.replace().replace().replace", "s.replace().replace().replace.translate", "str.maketrans", "s.replace().replace().replace.replace().replace", "s.replace().replace().replace.replace"], "function", ["None"], ["", "def", "remove_punctuation", "(", "s", ":", "str", ")", "->", "str", ":", "\n", "# remove smart quotes, which aren't included in string.punctuation", "\n", "    ", "s", "=", "s", ".", "replace", "(", "'\u201c'", ",", "''", ")", ".", "replace", "(", "'\u201d'", ",", "''", ")", ".", "replace", "(", "'\u2019'", ",", "''", ")", "\n", "return", "s", ".", "translate", "(", "str", ".", "maketrans", "(", "''", ",", "''", ",", "string", ".", "punctuation", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_bad_punctuation_encoding": [[25, 28], ["s.replace().replace().replace", "s.replace().replace", "s.replace"], "function", ["None"], ["", "def", "replace_bad_punctuation_encoding", "(", "s", ":", "str", ")", "->", "str", ":", "\n", "# remove smart quotes, which aren't included in string.punctuation", "\n", "    ", "return", "s", ".", "replace", "(", "'\u201c'", ",", "'\"'", ")", ".", "replace", "(", "'\u201d'", ",", "'\"'", ")", ".", "replace", "(", "'\u2019'", ",", "\"'\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.ammed_content_replace_bad_punctuation_encoding": [[30, 34], ["annotations.replace_bad_punctuation_encoding"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_bad_punctuation_encoding"], ["", "def", "ammed_content_replace_bad_punctuation_encoding", "(", "inputs", ":", "Dict", "[", "str", ",", "Dict", "]", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "for", "id", "in", "inputs", ":", "\n", "        ", "inputs", "[", "id", "]", "[", "'content'", "]", "=", "replace_bad_punctuation_encoding", "(", "inputs", "[", "id", "]", "[", "'content'", "]", ")", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_metamap_annotations": [[38, 116], ["pkg_resources.resource_filename", "pkg_resources.resource_filename", "pandas.read_csv", "metamap_semantics[].sort_values", "list", "inputs.keys", "time.time", "print", "MetaMapLite.get_instance", "MetaMapLite.get_instance.extract_concepts", "time.time", "print", "print", "print", "sentences.append", "len", "float", "concept.semtypes.replace().replace().split", "concept.semtypes.replace().replace", "int.replace().replace", "[].append", "[].append", "metamap_semantics[].isin", "metamap_semantics[].isin", "getattr", "inputs.keys", "concept.semtypes.replace", "int.replace", "int", "inputs.keys", "int", "ValueError"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "def", "add_metamap_annotations", "(", "inputs", ":", "Dict", "[", "str", ",", "Dict", "]", ",", "git_bash_pth", ":", "str", "=", "None", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "from", "pymetamap", "import", "MetaMapLite", "\n", "\n", "# if metamap_path is None:", "\n", "#     print(\"NOTE: no Metamap path provided. Using Laura's default\")", "\n", "#     metamap_path = '/Users/laurakinkead/Documents/metamap/public_mm_lite/'", "\n", "\n", "metamap_path", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "'package_data/public_mm_lite/'", ")", "\n", "metamap_semantics_filename", "=", "pkg_resources", ".", "resource_filename", "(", "'autodiscern'", ",", "\n", "'package_data/metamap_semantics/metamap_semantics.csv'", ")", "\n", "metamap_semantics", "=", "pd", ".", "read_csv", "(", "metamap_semantics_filename", ")", "\n", "groups_to_keep", "=", "[", "\n", "'Anatomy'", ",", "\n", "'Devices'", ",", "\n", "'Disorders'", ",", "\n", "'Physiology'", ",", "\n", "'Procedures'", ",", "\n", "]", "\n", "types_to_keep", "=", "[", "\n", "# Chemicals & Drugs", "\n", "'Antibiotic'", ",", "\n", "'Clincal Drug'", ",", "\n", "'Enzyme'", ",", "\n", "'Hormone'", ",", "\n", "'Pharmacologic Substance'", ",", "\n", "'Receptor'", ",", "\n", "]", "\n", "semantic_type_filter_df", "=", "metamap_semantics", "[", "metamap_semantics", "[", "'group_name'", "]", ".", "isin", "(", "groups_to_keep", ")", "|", "\n", "metamap_semantics", "[", "'name'", "]", ".", "isin", "(", "types_to_keep", ")", "\n", "]", ".", "sort_values", "(", "[", "'group_name'", ",", "'name'", "]", ")", "\n", "semantic_type_filter", "=", "list", "(", "semantic_type_filter_df", "[", "'abbreviation'", "]", ")", "\n", "\n", "# create list of ids, and list of sentences in matching order", "\n", "ids", "=", "inputs", ".", "keys", "(", ")", "\n", "sentences", "=", "[", "]", "\n", "for", "id", "in", "ids", ":", "\n", "        ", "sentences", ".", "append", "(", "inputs", "[", "id", "]", "[", "'content'", "]", ")", "\n", "\n", "# run metamap", "\n", "", "import", "time", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\"Extracting MetaMap concepts for {} documents, starting at {}...\"", ".", "format", "(", "len", "(", "sentences", ")", ",", "start_time", ")", ")", "\n", "mm", "=", "MetaMapLite", ".", "get_instance", "(", "metamap_path", ",", "git_bash_pth", "=", "git_bash_pth", ")", "\n", "concepts", ",", "error", "=", "mm", ".", "extract_concepts", "(", "sentences", ",", "ids", ")", "\n", "end_time", "=", "time", ".", "time", "(", ")", "\n", "print", "(", "\"Finished at {}. That took {}\"", ".", "format", "(", "end_time", ",", "end_time", "-", "start_time", ")", ")", "\n", "\n", "print", "(", "\"Attaching Metamap concepts...\"", ")", "\n", "# add concepts with score above 1 and matching the semantics filter to input sentences", "\n", "for", "concept", "in", "concepts", ":", "\n", "        ", "concept_dict", "=", "{", "}", "\n", "if", "float", "(", "concept", ".", "score", ")", ">", "1", ":", "\n", "            ", "semtypes", "=", "concept", ".", "semtypes", ".", "replace", "(", "'['", ",", "''", ")", ".", "replace", "(", "']'", ",", "''", ")", ".", "split", "(", "','", ")", "\n", "for", "semtype", "in", "semtypes", ":", "\n", "                ", "if", "semtype", "in", "semantic_type_filter", ":", "\n", "                    ", "for", "fld", "in", "concept", ".", "_fields", ":", "\n", "                        ", "concept_dict", "[", "fld", "]", "=", "getattr", "(", "concept", ",", "fld", ")", "\n", "\n", "# attach concept to input_dict", "\n", "", "id", "=", "concept_dict", "[", "'index'", "]", "\n", "id", "=", "id", ".", "replace", "(", "'\"'", ",", "''", ")", ".", "replace", "(", "\"'\"", ",", "''", ")", "\n", "if", "id", "not", "in", "inputs", ".", "keys", "(", ")", ":", "\n", "                        ", "if", "int", "(", "id", ")", "in", "inputs", ".", "keys", "(", ")", ":", "\n", "                            ", "id", "=", "int", "(", "id", ")", "\n", "", "else", ":", "\n", "                            ", "raise", "ValueError", "(", "\"ERROR: MetaMap index {} not found in input keys\"", ")", "\n", "\n", "", "", "if", "'metamap'", "not", "in", "inputs", "[", "id", "]", ":", "\n", "                        ", "inputs", "[", "id", "]", "[", "'metamap'", "]", "=", "[", "]", "\n", "inputs", "[", "id", "]", "[", "'metamap_detail'", "]", "=", "[", "]", "\n", "", "metamap_category", "=", "metamap_semantics", "[", "metamap_semantics", "[", "'abbreviation'", "]", "==", "semtype", "\n", "]", "[", "'group_name'", "]", ".", "iloc", "[", "0", "]", "\n", "inputs", "[", "id", "]", "[", "'metamap'", "]", ".", "append", "(", "metamap_category", ")", "\n", "inputs", "[", "id", "]", "[", "'metamap_detail'", "]", ".", "append", "(", "concept_dict", ")", "\n", "break", "\n", "\n", "", "", "", "", "print", "(", "\"Done annotating MetaMap concepts\"", ")", "\n", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_metamap_concepts": [[118, 125], ["inputs[].keys", "annotations.replace_metamap_content_with_concept_name"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_metamap_content_with_concept_name"], ["", "def", "amend_content_with_metamap_concepts", "(", "inputs", ":", "Dict", "[", "str", ",", "Dict", "]", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "for", "id", "in", "inputs", ":", "\n", "        ", "if", "'metamap'", "in", "inputs", "[", "id", "]", ".", "keys", "(", ")", ":", "\n", "            ", "inputs", "[", "id", "]", "[", "'content'", "]", "=", "replace_metamap_content_with_concept_name", "(", "inputs", "[", "id", "]", "[", "'content'", "]", ",", "\n", "inputs", "[", "id", "]", "[", "'metamap_detail'", "]", ",", "\n", "inputs", "[", "id", "]", "[", "'metamap'", "]", ",", ")", "\n", "", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.get_metamap_pos": [[127, 132], ["pos_info.split", "int", "int"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "def", "get_metamap_pos", "(", "pos_info", ":", "str", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "    ", "pos_start", ",", "pos_end", "=", "pos_info", ".", "split", "(", "'/'", ")", "\n", "pos_start", "=", "int", "(", "pos_start", ")", "\n", "pos_end", "=", "pos_start", "+", "int", "(", "pos_end", ")", "\n", "return", "pos_start", ",", "pos_end", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_metamap_content_with_concept_name": [[134, 164], ["enumerate", "annotations.split_repeated_metamap_concepts", "sorted", "annotations.prune_overlapping_metamap_details", "reversed", "content.replace.replace", "annotations.get_metamap_pos", "content.replace.replace", "mm_d[].replace().replace", "mm_d[].replace"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.split_repeated_metamap_concepts", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.prune_overlapping_metamap_details", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.get_metamap_pos"], ["", "def", "replace_metamap_content_with_concept_name", "(", "content", ":", "str", ",", "metamap_detail", ":", "List", "[", "Dict", "]", ",", "metamap_concepts", ":", "List", "[", "str", "]", "\n", ")", "->", "str", ":", "\n", "# add concepts directly into detail dicts", "\n", "    ", "for", "i", ",", "mm_d", "in", "enumerate", "(", "metamap_detail", ")", ":", "\n", "        ", "metamap_detail", "[", "i", "]", "[", "'concept'", "]", "=", "metamap_concepts", "[", "i", "]", "\n", "\n", "", "metamap_detail", "=", "split_repeated_metamap_concepts", "(", "metamap_detail", ")", "\n", "metamap_detail", "=", "sorted", "(", "metamap_detail", ",", "key", "=", "lambda", "k", ":", "k", "[", "'start_pos'", "]", ")", "\n", "metamap_detail", "=", "prune_overlapping_metamap_details", "(", "metamap_detail", ")", "\n", "\n", "# metamap position indexes are based on raw strings, where '\\n' counts as two characters, but they are only counted", "\n", "#    as 1 by Python, which breaks the position-based replacement. Convert newlines to 2-character placeholders.", "\n", "escape_char_replacements", "=", "{", "\n", "'\\n'", ":", "'^^'", ",", "\n", "\"\\'\"", ":", "'@@'", ",", "\n", "}", "\n", "for", "c", "in", "escape_char_replacements", ":", "\n", "        ", "content", "=", "content", ".", "replace", "(", "c", ",", "escape_char_replacements", "[", "c", "]", ")", "\n", "\n", "", "for", "mm_d", "in", "reversed", "(", "metamap_detail", ")", ":", "\n", "        ", "pos_start", ",", "pos_end", "=", "get_metamap_pos", "(", "mm_d", "[", "'pos_info'", "]", ")", "\n", "# replace token with \"MMConcept\" + <concept name with spaces removed>", "\n", "# and also '&' removed (present in the \"Chemicals & Drugs\" category", "\n", "concept", "=", "\"MMConcept\"", "+", "mm_d", "[", "'concept'", "]", ".", "replace", "(", "' '", ",", "''", ")", ".", "replace", "(", "'&'", ",", "''", ")", "\n", "content", "=", "''", ".", "join", "(", "(", "content", "[", ":", "pos_start", "-", "1", "]", ",", "concept", ",", "content", "[", "pos_end", "-", "1", ":", "]", ")", ")", "\n", "\n", "# flip the escape char conversions back", "\n", "", "for", "c", "in", "escape_char_replacements", ":", "\n", "        ", "content", "=", "content", ".", "replace", "(", "escape_char_replacements", "[", "c", "]", ",", "c", ")", "\n", "", "return", "content", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.split_repeated_metamap_concepts": [[166, 178], ["metamap_entry[].split", "metamap_details_split.append", "annotations.get_metamap_pos"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.get_metamap_pos"], ["", "def", "split_repeated_metamap_concepts", "(", "metamap_details", ":", "List", "[", "Dict", "]", ")", "->", "List", "[", "Dict", "]", ":", "\n", "    ", "metamap_details_split", "=", "[", "]", "\n", "for", "metamap_entry", "in", "metamap_details", ":", "\n", "        ", "pos_entries", "=", "metamap_entry", "[", "'pos_info'", "]", ".", "split", "(", "';'", ")", "\n", "for", "pos", "in", "pos_entries", ":", "\n", "            ", "metamap_details_split", ".", "append", "(", "{", "\n", "'pos_info'", ":", "pos", ",", "\n", "'start_pos'", ":", "get_metamap_pos", "(", "(", "pos", ")", ")", "[", "0", "]", ",", "\n", "'concept'", ":", "metamap_entry", "[", "'concept'", "]", ",", "\n", "'score'", ":", "metamap_entry", "[", "'score'", "]", ",", "\n", "}", ")", "\n", "", "", "return", "metamap_details_split", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.prune_overlapping_metamap_details": [[180, 203], ["enumerate", "annotations.get_metamap_pos", "annotations.get_metamap_pos", "len"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.get_metamap_pos", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.get_metamap_pos"], ["", "def", "prune_overlapping_metamap_details", "(", "mm_d", ":", "List", "[", "Dict", "]", ")", "->", "List", "[", "Dict", "]", ":", "\n", "    ", "\"\"\"Iterate over the metamap details, removing adjacent concepts that overlap, until no overlaps are found.\n    Assumes metamap concepts are listed in position order.\"\"\"", "\n", "# set overlap_found to True to enter the loop for the first time", "\n", "# subsequently, overlap_found is assumed False until an overlap is found,", "\n", "#   at which point the for loop breaks and starts over from the beginning", "\n", "#   because removing items from the list resets the indexes", "\n", "overlap_found", "=", "True", "\n", "while", "overlap_found", ":", "\n", "        ", "overlap_found", "=", "False", "\n", "for", "i", ",", "d", "in", "enumerate", "(", "mm_d", ")", ":", "\n", "            ", "if", "i", "<=", "len", "(", "mm_d", ")", "-", "2", ":", "\n", "                ", "pos_start", ",", "pos_end", "=", "get_metamap_pos", "(", "d", "[", "'pos_info'", "]", ")", "\n", "next_pos_start", ",", "next_pos_end", "=", "get_metamap_pos", "(", "mm_d", "[", "i", "+", "1", "]", "[", "'pos_info'", "]", ")", "\n", "if", "pos_start", "<=", "next_pos_start", "<=", "pos_end", "or", "pos_start", "<=", "next_pos_end", "<=", "pos_end", ":", "\n", "                    ", "overlap_found", "=", "True", "\n", "# figure out which of the two concepts to remove", "\n", "if", "mm_d", "[", "i", "]", "[", "'score'", "]", ">=", "mm_d", "[", "i", "+", "1", "]", "[", "'score'", "]", ":", "\n", "                        ", "del", "mm_d", "[", "i", "+", "1", "]", "\n", "", "else", ":", "\n", "                        ", "del", "mm_d", "[", "i", "]", "\n", "", "break", "\n", "", "", "", "", "return", "mm_d", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_link_plain_text": [[207, 211], ["annotations.replace_links_with_plain_text"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_links_with_plain_text"], ["", "def", "amend_content_with_link_plain_text", "(", "inputs", ":", "Dict", "[", "str", ",", "Dict", "]", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "for", "id", "in", "inputs", ":", "\n", "        ", "inputs", "[", "id", "]", "[", "'content'", "]", "=", "replace_links_with_plain_text", "(", "inputs", "[", "id", "]", "[", "'content'", "]", ")", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_links_with_plain_text": [[213, 225], ["re.sub"], "function", ["None"], ["", "def", "replace_links_with_plain_text", "(", "input_str", ":", "str", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Regex from http://www.regexguru.com/2008/11/detecting-urls-in-a-block-of-text/\n\n    Args:\n        input_str: string in which to replace links.\n\n    Returns: string with links replaced with \"thisisalink\".\n\n    \"\"\"", "\n", "r", "=", "r'\\b(?:(?:https?|ftp|file):\\/\\/|www\\.|ftp\\.)[-A-Za-z0-9+&@#/%=~_|$?!:,.]*[A-Za-z0-9+&@#/%=~_|$]'", "\n", "return", "re", ".", "sub", "(", "r", ",", "'thisisalink'", ",", "input_str", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_ner_type_labels": [[229, 235], ["annotations.replace_ner_with_type_labels"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_ner_with_type_labels"], ["", "def", "amend_content_with_ner_type_labels", "(", "inputs", ":", "Dict", "[", "str", ",", "Dict", "]", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "entity_types_to_not_replace", "=", "[", "'ORG'", ",", "'NORP'", ",", "'PERSON'", "]", "\n", "for", "id", "in", "inputs", ":", "\n", "        ", "inputs", "[", "id", "]", "[", "'content'", "]", "=", "replace_ner_with_type_labels", "(", "inputs", "[", "id", "]", "[", "'content'", "]", ",", "inputs", "[", "id", "]", "[", "'ner'", "]", ",", "\n", "entity_types_to_not_replace", ")", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_ner_with_type_labels": [[237, 247], ["reversed"], "function", ["None"], ["", "def", "replace_ner_with_type_labels", "(", "input_str", ":", "str", ",", "ner_info", ":", "List", "[", "Dict", "[", "str", ",", "str", "]", "]", ",", "entity_types_to_not_replace", ":", "List", "[", "str", "]", "\n", ")", "->", "str", ":", "\n", "# TODO: untested", "\n", "    ", "output_str", "=", "input_str", "\n", "for", "entity", "in", "reversed", "(", "ner_info", ")", ":", "\n", "        ", "if", "entity", "[", "'label'", "]", "not", "in", "entity_types_to_not_replace", ":", "\n", "            ", "pos_start", "=", "entity", "[", "'start_char'", "]", "\n", "pos_end", "=", "entity", "[", "'end_char'", "]", "\n", "output_str", "=", "''", ".", "join", "(", "(", "output_str", "[", ":", "pos_start", "]", ",", "entity", "[", "'custom_label'", "]", ",", "output_str", "[", "pos_end", ":", "]", ")", ")", "\n", "", "", "return", "output_str", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_ner_annotations": [[249, 256], ["spacy.load", "annotations.execute_spacy_ner"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.execute_spacy_ner"], ["", "def", "add_ner_annotations", "(", "inputs", ":", "Dict", "[", "str", ",", "Dict", "]", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "import", "spacy", "\n", "nlp", "=", "spacy", ".", "load", "(", "'en_core_web_sm'", ")", "\n", "\n", "for", "id", "in", "inputs", ":", "\n", "        ", "inputs", "[", "id", "]", "[", "'ner'", "]", "=", "execute_spacy_ner", "(", "inputs", "[", "id", "]", "[", "'content'", "]", ",", "nlp", ")", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.execute_spacy_ner": [[258, 271], ["nlp", "annotations.select_custom_ner_label", "results.append"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.select_custom_ner_label"], ["", "def", "execute_spacy_ner", "(", "input_str", ",", "nlp", ")", ":", "\n", "    ", "results", "=", "[", "]", "\n", "doc", "=", "nlp", "(", "input_str", ")", "\n", "for", "ent", "in", "doc", ".", "ents", ":", "\n", "        ", "entity_dict", "=", "{", "\n", "'text'", ":", "ent", ".", "text", ",", "\n", "'start_char'", ":", "ent", ".", "start_char", ",", "\n", "'end_char'", ":", "ent", ".", "end_char", ",", "\n", "'label'", ":", "ent", ".", "label_", ",", "\n", "}", "\n", "entity_dict", "[", "'custom_label'", "]", "=", "select_custom_ner_label", "(", "entity_dict", "[", "'text'", "]", ",", "entity_dict", "[", "'label'", "]", ")", "\n", "results", ".", "append", "(", "entity_dict", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.select_custom_ner_label": [[273, 291], ["any", "i.isdigit"], "function", ["None"], ["", "def", "select_custom_ner_label", "(", "text", ":", "str", ",", "spacy_label", ":", "str", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Custom logic for defining more specific Named Entity labels than what spacy gives by default\n\n    Args:\n        text: the text of the entity spacy recognized\n        spacy_label: the label spacy assigned to the entity\n\n    Returns: custom label\n\n    \"\"\"", "\n", "default_label", "=", "'SPACY_NER_{}'", ".", "format", "(", "spacy_label", ")", "\n", "if", "spacy_label", "==", "'DATE'", ":", "\n", "        ", "if", "any", "(", "i", ".", "isdigit", "(", ")", "for", "i", "in", "text", ")", ":", "\n", "            ", "return", "default_label", "\n", "", "else", ":", "\n", "            ", "return", "'{}_NO_DIGIT'", ".", "format", "(", "default_label", ")", "\n", "", "", "return", "default_label", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.allennlp_ner_tagger": [[293, 302], ["len", "predictor.predict", "enumerate"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict"], ["", "def", "allennlp_ner_tagger", "(", "sentence", ":", "str", ",", "predictor", ":", "Callable", ")", "->", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", ":", "\n", "# pass this function the predictor of", "\n", "# predictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/ner-model-2018.12.18.tar.gz\")", "\n", "    ", "if", "len", "(", "sentence", ")", ">", "1", ":", "\n", "        ", "prediction", "=", "predictor", ".", "predict", "(", "sentence", ")", "\n", "# the predictor also gives logits. For now we just want to look at the tags", "\n", "return", "[", "(", "w", ",", "prediction", "[", "'tags'", "]", "[", "i", "]", ")", "for", "i", ",", "w", "in", "enumerate", "(", "prediction", "[", "'words'", "]", ")", "]", "\n", "", "else", ":", "\n", "        ", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.ner_tuples_to_html": [[304, 327], ["None"], "function", ["None"], ["", "", "def", "ner_tuples_to_html", "(", "tuples", ":", "List", "[", "Tuple", "[", "str", ",", "str", "]", "]", ")", "->", "str", ":", "\n", "    ", "\"\"\"Display the output of allennlp_ner_tagger as text color-coded by ner type.\n    Wrap this function call in IPython.display.HTML() to see output in notebook. \"\"\"", "\n", "\n", "ner_type_to_html_tag", "=", "{", "\n", "\"U-PER\"", ":", "'font  color=\"blue\"'", ",", "\n", "\"B-ORG\"", ":", "'font  color=\"green\"'", ",", "\n", "\"L-ORG\"", ":", "'font  color=\"red\"'", ",", "\n", "\"U-MISC\"", ":", "'font  color=\"orange\"'", ",", "\n", "}", "\n", "\n", "ner_html", "=", "\"\"", "\n", "for", "sentence", "in", "tuples", ":", "\n", "        ", "for", "token", "in", "sentence", ":", "\n", "            ", "text", "=", "token", "[", "0", "]", "\n", "ner_type", "=", "token", "[", "1", "]", "\n", "if", "ner_type", "==", "'O'", ":", "\n", "                ", "ner_html", "+=", "\" {} \"", ".", "format", "(", "text", ")", "\n", "", "else", ":", "\n", "                ", "tag", "=", "ner_type_to_html_tag", "[", "ner_type", "]", "\n", "ner_html", "+=", "\" <{0}>{1}</{0}>\"", ".", "format", "(", "tag", ",", "text", ")", "\n", "\n", "", "", "", "return", "ner_html", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_inline_citations_annotations": [[331, 337], ["annotations.apply_inline_citation_regex"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "add_inline_citations_annotations", "(", "inputs", ":", "Dict", "[", "str", ",", "Dict", "]", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "for", "id", "in", "inputs", ":", "\n", "        ", "sentence", "=", "inputs", "[", "id", "]", "[", "'content'", "]", "\n", "in_line_citations", "=", "apply_inline_citation_regex", "(", "sentence", ")", "\n", "inputs", "[", "id", "]", "[", "'citations'", "]", "=", "in_line_citations", "\n", "", "return", "inputs", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex": [[339, 349], ["re.findall"], "function", ["None"], ["", "def", "apply_inline_citation_regex", "(", "text", ")", ":", "\n", "    ", "year_num", "=", "r\"(?:19|20)[0-9][0-9]\"", "\n", "author_with_year", "=", "r\"[^()\\d]*\"", "+", "year_num", "\n", "multiple_authors", "=", "author_with_year", "+", "r\"(?:;\\s*\"", "+", "author_with_year", "+", "r\")*\"", "\n", "author_either_bracket_type", "=", "r\"\\(\"", "+", "multiple_authors", "+", "r\"\\)|\\[\"", "+", "multiple_authors", "+", "r\"\\]\"", "\n", "citation_number_in_square_brackets", "=", "r\"\\[\\d+(?:[-,\\s]+\\d+)*\\]\"", "\n", "regex", "=", "author_either_bracket_type", "+", "r\"|\"", "+", "citation_number_in_square_brackets", "\n", "\n", "matches", "=", "re", ".", "findall", "(", "regex", ",", "text", ")", "\n", "return", "matches", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.extract_potential_references": [[351, 378], ["bs4.BeautifulSoup", "bs4.BeautifulSoup.find_all", "any", "type", "sibling_tag.get_text", "potential_citations.extend", "tag.string.lower().split", "sibling_tag.get_text.split", "tag.string.lower", "line.strip"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "def", "extract_potential_references", "(", "text", ":", "str", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "\"\"\"\n    Find the last-most instance of a reference keywords in an html heading, and retrieve all text following the heading.\n    Args:\n        text: str. HTML of webpage.\n\n    Returns: List[str]. List of potential citations found under the heading, split into a list based on line breaks.\n\n    \"\"\"", "\n", "soup", "=", "BeautifulSoup", "(", "text", ",", "features", "=", "\"html.parser\"", ")", "\n", "\n", "reference_keywords", "=", "[", "'references'", ",", "'citations'", ",", "'bibliography'", "]", "\n", "\n", "# iterate backwards through all headers", "\n", "header_tags", "=", "soup", ".", "find_all", "(", "[", "'h1'", ",", "'h2'", ",", "'h3'", ",", "'h4'", "]", ")", "\n", "for", "tag", "in", "header_tags", "[", "-", "1", ":", "]", ":", "\n", "# if any single word in header matches a ref keyword", "\n", "        ", "if", "tag", ".", "string", "is", "not", "None", "and", "any", "(", "h", "in", "tag", ".", "string", ".", "lower", "(", ")", ".", "split", "(", "' '", ")", "for", "h", "in", "reference_keywords", ")", ":", "\n", "# return the remainder of the document", "\n", "            ", "potential_citations", "=", "[", "]", "\n", "for", "sibling_tag", "in", "tag", ".", "next_siblings", ":", "\n", "                ", "if", "type", "(", "sibling_tag", ")", "==", "Tag", ":", "\n", "                    ", "sibling_text", "=", "sibling_tag", ".", "get_text", "(", ")", "\n", "text_split", "=", "[", "line", "for", "line", "in", "sibling_text", ".", "split", "(", "'\\n'", ")", "if", "line", ".", "strip", "(", ")", "!=", "''", "]", "\n", "potential_citations", ".", "extend", "(", "text_split", ")", "\n", "", "", "return", "potential_citations", "\n", "", "", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.annotate_potential_references": [[380, 388], ["annotated_references.append"], "function", ["None"], ["", "def", "annotate_potential_references", "(", "potential_references", ":", "List", "[", "str", "]", ")", "->", "List", "[", "Tuple", "[", "str", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "    ", "\"\"\"Placeholder function for annotating candidate reference strings.\n    TODO: use NeuralParsCit here\n    \"\"\"", "\n", "annotated_references", "=", "[", "]", "\n", "for", "item", "in", "potential_references", ":", "\n", "        ", "annotated_references", ".", "append", "(", "(", "item", ",", "[", "'token'", ",", "'fake_annotation'", "]", ")", ")", "\n", "", "return", "annotated_references", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.evaluate_potential_references": [[390, 409], ["set", "selected_references.append"], "function", ["None"], ["", "def", "evaluate_potential_references", "(", "potential_references", ":", "List", "[", "Tuple", "[", "str", ",", "List", "[", "str", "]", "]", "]", ")", "->", "List", "[", "Tuple", "[", "str", ",", "List", "[", "str", "]", "]", "]", ":", "\n", "    ", "\"\"\"Select references which meet requirements based on the reference's token-based annotations.\n    Only keep references that have at least one of all (author, title).\n\n    Args:\n            potential_references: List of references and their annotations represented by tuples, like so:\n                [\n                    ('original_reference_string', [('token1', 'annotation1'), ('token2', 'annotation2'), ...],\n                    ...\n                ]\n\n    Returns: A subset of potential_references with the same data structure.\n    \"\"\"", "\n", "selected_references", "=", "[", "]", "\n", "for", "orig_ref_string", ",", "ref_annotations", "in", "potential_references", ":", "\n", "        ", "annotation_types", "=", "set", "(", "[", "annotation", "for", "token", ",", "annotation", "in", "ref_annotations", "]", ")", "\n", "if", "'title'", "in", "annotation_types", "and", "'author'", "in", "annotation_types", ":", "\n", "            ", "selected_references", ".", "append", "(", "(", "orig_ref_string", ",", "ref_annotations", ")", ")", "\n", "", "", "return", "selected_references", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_reference_annotations": [[411, 434], ["annotations.extract_potential_references", "annotations.annotate_potential_references"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.extract_potential_references", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.annotate_potential_references"], ["", "def", "add_reference_annotations", "(", "inputs", ":", "Dict", "[", "str", ",", "Dict", "]", ")", "->", "Dict", "[", "str", ",", "Dict", "]", ":", "\n", "    ", "\"\"\"\n    NOTE: this functionality is not robust, and quite frankly doesn't really work. Use at your own risk.\n\n    Add reference annotations to the document dictionary. This function takes raw html as input. In other words, this\n    function must be called before any transformation on the raw html occurs.\n\n    Args:\n        inputs: Dict with a 'content' key containing raw html.\n\n    Returns: Dict with an additional 'references' key with list of string references.\n\n    \"\"\"", "\n", "for", "entity_id", "in", "inputs", ":", "\n", "        ", "potential_references", "=", "extract_potential_references", "(", "inputs", "[", "entity_id", "]", "[", "'content'", "]", ")", "\n", "annotated_potential_references", "=", "annotate_potential_references", "(", "potential_references", ")", "\n", "\n", "# TODO: switch to actually evaluating potential reference candidates", "\n", "# selected_references = evaluate_potential_references(annotated_potential_references)", "\n", "selected_references", "=", "annotated_potential_references", "\n", "\n", "inputs", "[", "entity_id", "]", "[", "'references'", "]", "=", "selected_references", "\n", "", "return", "inputs", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.__init__": [[24, 76], ["experiment.PartitionedExperiment.build_doc_id_and_label_lists", "ValueError", "experiment.PartitionedExperiment.partition_document_ids_stratified", "print", "experiment.PartitionedExperiment.report_partition_stats", "experiment.PartitionedExperiment.partition_document_ids_by_category", "experiment.PartitionedExperiment.partition_document_ids"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.build_doc_id_and_label_lists", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids_stratified", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.report_partition_stats", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids_by_category", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids"], ["    ", "def", "__init__", "(", "self", ",", "name", ":", "str", ",", "data_dict", ":", "Dict", ",", "label_key", ":", "str", ",", "preprocessing_func", ":", "Callable", ",", "\n", "model_run_class", ":", "\"ModelRun\"", ",", "model", ",", "hyperparams", ":", "Dict", ",", "n_partitions", ":", "int", "=", "5", ",", "stratify_by", "=", "'label'", ",", "\n", "feature_subset", "=", "None", ",", "reduce_features", "=", "False", ",", "verbose", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n\n        Args:\n            name: Name for identification of the Experiment.\n            data_dict: Data to run the model on.\n            label_key: the key to use to calculate the label.\n            preprocessing_func: the function that was used to preprocess `data_dict`\n            model_run_class: An implemented subclass of ModelRun.\n            model: A model with a fit() method.\n            hyperparams: Dictionary of hyperparamters to search for the best model.\n            n_partitions: Number of partitions to split the data on and run the experiment on.\n            stratify_by: Whether to stratify the partitions by label, disease category, or None.\n            feature_subset: Subset of features to use. If not used, None is passed.\n            reduce_features: Whether to use recursive feature elimination to reduce features.\n            verbose: Whether to display verbose messages.\n        \"\"\"", "\n", "\n", "if", "n_partitions", "<", "2", ":", "\n", "            ", "raise", "ValueError", "(", "\"n_partitions must be greater than 1. Got {}.\"", ".", "format", "(", "n_partitions", ")", ")", "\n", "\n", "", "self", ".", "name", "=", "name", "\n", "self", ".", "data_dict", "=", "data_dict", "\n", "self", ".", "label_key", "=", "label_key", "\n", "self", ".", "preprocessing_func", "=", "preprocessing_func", "\n", "self", ".", "verbose", "=", "verbose", "\n", "self", ".", "model_run_class", "=", "model_run_class", "\n", "self", ".", "model", "=", "model", "\n", "self", ".", "hyperparams", "=", "hyperparams", "\n", "self", ".", "feature_subset", "=", "feature_subset", "\n", "self", ".", "reduce_features", "=", "reduce_features", "\n", "\n", "self", ".", "n_partitions", "=", "n_partitions", "\n", "\n", "document_ids", ",", "doc_labels", "=", "self", ".", "build_doc_id_and_label_lists", "(", "data_dict", ",", "model_run_class", ",", "label_key", ")", "\n", "if", "stratify_by", "==", "'label'", ":", "\n", "            ", "self", ".", "partitions_by_ids", "=", "self", ".", "partition_document_ids_stratified", "(", "document_ids", ",", "doc_labels", ",", "self", ".", "n_partitions", ")", "\n", "", "elif", "stratify_by", "==", "'category'", ":", "\n", "            ", "doc_categories", "=", "[", "self", ".", "data_dict", "[", "doc_id", "]", "[", "'categoryName'", "]", "for", "doc_id", "in", "self", ".", "data_dict", "]", "\n", "self", ".", "partitions_by_ids", "=", "self", ".", "partition_document_ids_by_category", "(", "document_ids", ",", "doc_categories", ",", "category_key", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "partitions_by_ids", "=", "self", ".", "partition_document_ids", "(", "document_ids", ",", "self", ".", "n_partitions", ")", "\n", "\n", "", "self", ".", "model_runs", "=", "{", "}", "\n", "self", ".", "all_run_results", "=", "[", "]", "\n", "self", ".", "experiment_results", "=", "[", "]", "\n", "\n", "if", "verbose", ":", "\n", "            ", "print", "(", "\"Partition Stats for {}\"", ".", "format", "(", "self", ".", "name", ")", ")", "\n", "self", ".", "report_partition_stats", "(", "self", ".", "partitions_by_ids", ",", "document_ids", ",", "doc_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.run": [[77, 111], ["list", "print", "experiment.PartitionedExperiment.summarize_runs", "experiment.PartitionedExperiment.partitions_by_ids.keys", "print", "print", "experiment.PartitionedExperiment.run_experiment_on_one_partition"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.summarize_runs", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.run_experiment_on_one_partition"], ["", "", "def", "run", "(", "self", ",", "num_partitions_to_run", "=", "None", ",", "run_hyperparam_search", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        Run the experiment.\n\n        Args:\n            num_partitions_to_run: select a subset of partitions to run, for faster testing.\n            run_hyperparam_search: argument to turn off hyperparam search, for faster testing.\n\n        Returns:\n\n        \"\"\"", "\n", "partitions_to_run", "=", "list", "(", "self", ".", "partitions_by_ids", ".", "keys", "(", ")", ")", "\n", "if", "num_partitions_to_run", "is", "not", "None", ":", "\n", "            ", "partitions_to_run", "=", "partitions_to_run", "[", ":", "num_partitions_to_run", "]", "\n", "print", "(", "\"Running only partitions {}\"", ".", "format", "(", "\", \"", ".", "join", "(", "partitions_to_run", ")", ")", ")", "\n", "\n", "", "for", "partition_name", "in", "self", ".", "partitions_by_ids", ":", "\n", "            ", "if", "partition_name", "in", "partitions_to_run", ":", "\n", "                ", "print", "(", "\"Running partition {}...\"", ".", "format", "(", "partition_name", ")", ")", "\n", "model_run", "=", "self", ".", "run_experiment_on_one_partition", "(", "data_dict", "=", "self", ".", "data_dict", ",", "\n", "label_key", "=", "self", ".", "label_key", ",", "\n", "partition_ids", "=", "self", ".", "partitions_by_ids", "[", "partition_name", "]", ",", "\n", "preprocessing_func", "=", "self", ".", "preprocessing_func", ",", "\n", "model_run_class", "=", "self", ".", "model_run_class", ",", "\n", "model", "=", "self", ".", "model", ",", "\n", "hyperparams", "=", "self", ".", "hyperparams", ",", "\n", "run_hyperparam_search", "=", "run_hyperparam_search", ",", "\n", "feature_subset", "=", "self", ".", "feature_subset", ",", "\n", "reduce_features", "=", "self", ".", "reduce_features", ")", "\n", "self", ".", "model_runs", "[", "partition_name", "]", "=", "model_run", "\n", "\n", "", "", "print", "(", "\"Compiling results\"", ")", "\n", "self", ".", "experiment_results", "=", "self", ".", "summarize_runs", "(", "self", ".", "model_runs", ")", "\n", "return", "self", ".", "experiment_results", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.run_experiment_on_one_partition": [[112, 123], ["cls.materialize_partition", "model_run_class", "model_run_class.run"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.materialize_partition", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run"], ["", "@", "classmethod", "\n", "def", "run_experiment_on_one_partition", "(", "cls", ",", "data_dict", ":", "Dict", ",", "label_key", ":", "str", ",", "partition_ids", ":", "List", "[", "int", "]", ",", "\n", "preprocessing_func", ":", "Callable", ",", "model_run_class", ":", "\"ModelRun\"", ",", "model", ",", "\n", "hyperparams", ":", "Dict", ",", "run_hyperparam_search", ":", "bool", ",", "feature_subset", ":", "List", "[", "str", "]", ",", "\n", "reduce_features", ":", "bool", ")", ":", "\n", "        ", "train_set", ",", "test_set", "=", "cls", ".", "materialize_partition", "(", "partition_ids", ",", "data_dict", ")", "\n", "mr", "=", "model_run_class", "(", "train_set", "=", "train_set", ",", "test_set", "=", "test_set", ",", "label_key", "=", "label_key", ",", "model", "=", "model", ",", "\n", "preprocessing_func", "=", "preprocessing_func", ",", "hyperparams", "=", "hyperparams", ",", "\n", "feature_subset", "=", "feature_subset", ",", "reduce_features", "=", "reduce_features", ")", "\n", "mr", ".", "run", "(", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "return", "mr", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.build_doc_id_and_label_lists": [[124, 144], ["list", "model_run_class.build_y_vector", "document_labels.keys"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.build_y_vector"], ["", "@", "staticmethod", "\n", "def", "build_doc_id_and_label_lists", "(", "data_dict", ":", "Dict", "[", "int", ",", "Dict", "]", ",", "model_run_class", ",", "label_key", ")", ":", "\n", "        ", "\"\"\"\n        Build a list of document ids and list of corresponding labels.\n        Needs to work for both doc and sentence data sets! i.e. regardless of whether the key in data_dict is the same\n          as the entity_id entry\n        Args:\n            data_dict: Dictionary of the dataset (data dicts)\n            model_run_class: The model run class to use to compute the y vector\n            label_key: the label key to compute the target label for\n\n        Returns:\n\n        \"\"\"", "\n", "document_labels", "=", "{", "}", "\n", "for", "d", "in", "data_dict", ":", "\n", "            ", "document_labels", "[", "data_dict", "[", "d", "]", "[", "'entity_id'", "]", "]", "=", "model_run_class", ".", "build_y_vector", "(", "[", "data_dict", "[", "d", "]", "]", ",", "label_key", ")", "\n", "", "document_ids", "=", "list", "(", "document_labels", ".", "keys", "(", ")", ")", "\n", "doc_labels", "=", "[", "document_labels", "[", "id", "]", "for", "id", "in", "document_ids", "]", "\n", "return", "document_ids", ",", "doc_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids": [[145, 150], ["random.shuffle", "range"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "partition_document_ids", "(", "cls", ",", "doc_list", ":", "List", "[", "int", "]", ",", "n", ":", "int", ")", "->", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"Randomly shuffle and split the doc_list into n roughly equal lists.\"\"\"", "\n", "random", ".", "shuffle", "(", "doc_list", ")", "\n", "return", "{", "'Partition {}'", ".", "format", "(", "i", ")", ":", "doc_list", "[", "i", ":", ":", "n", "]", "for", "i", "in", "range", "(", "n", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids_stratified": [[151, 162], ["sklearn.model_selection.StratifiedKFold", "numpy.zeros", "enumerate", "len", "sklearn.model_selection.StratifiedKFold.split"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "@", "classmethod", "\n", "def", "partition_document_ids_stratified", "(", "cls", ",", "doc_list", ":", "List", "[", "int", "]", ",", "label_list", ":", "List", "[", "int", "]", ",", "n", ":", "int", ")", "->", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"Randomly shuffle and split the doc_list into n roughly equal lists, stratified by label.\"\"\"", "\n", "skf", "=", "StratifiedKFold", "(", "n_splits", "=", "n", ",", "random_state", "=", "42", ",", "shuffle", "=", "True", ")", "\n", "x", "=", "np", ".", "zeros", "(", "len", "(", "label_list", ")", ")", "# split takes a X argument for backwards compatibility and is not used", "\n", "partition_indexes", "=", "[", "test_index", "for", "train_index", ",", "test_index", "in", "skf", ".", "split", "(", "x", ",", "label_list", ")", "]", "\n", "partitions", "=", "{", "}", "\n", "for", "p_id", ",", "p", "in", "enumerate", "(", "partition_indexes", ")", ":", "\n", "            ", "partitions", "[", "'Partition {}'", ".", "format", "(", "p_id", ")", "]", "=", "[", "doc_list", "[", "i", "]", "for", "i", "in", "p", "]", "\n", "", "return", "partitions", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids_by_category": [[163, 181], ["numpy.unique", "enumerate"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "partition_document_ids_by_category", "(", "cls", ",", "doc_list", ":", "List", "[", "int", "]", ",", "category_list", ":", "List", "[", "int", "]", ",", "\n", "category_key", ":", "Dict", "[", "int", ",", "str", "]", ")", "->", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ":", "\n", "        ", "\"\"\"\n        Partition the documents by their disease category, to test generalizability.\n\n        Args:\n            doc_list: list of the document ids\n            category_list: list of each document's category id, in the same order as doc_list\n            category_key: mapping for the category ids\n\n        Returns: List of List of ints, representing each partition\n        \"\"\"", "\n", "categories", "=", "np", ".", "unique", "(", "category_list", ")", "\n", "partitions", "=", "{", "}", "\n", "for", "cat_id", "in", "categories", ":", "\n", "            ", "partitions", "[", "category_key", "[", "cat_id", "]", "]", "=", "[", "doc_list", "[", "i", "]", "for", "i", ",", "val", "in", "enumerate", "(", "category_list", ")", "if", "val", "==", "cat_id", "]", "\n", "", "return", "partitions", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.materialize_partition": [[182, 190], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "materialize_partition", "(", "cls", ",", "partition_ids", ":", "List", "[", "int", "]", ",", "data_dict", ":", "Dict", ")", "->", "Tuple", "[", "List", "[", "Dict", "]", ",", "List", "[", "Dict", "]", "]", ":", "\n", "        ", "\"\"\"Create trainng and testing dataset based on the partition, which indicated the ids for the test set.\"\"\"", "\n", "\n", "train_set", "=", "[", "data_dict", "[", "d", "]", "for", "d", "in", "data_dict", "if", "data_dict", "[", "d", "]", "[", "'entity_id'", "]", "not", "in", "partition_ids", "]", "\n", "test_set", "=", "[", "data_dict", "[", "d", "]", "for", "d", "in", "data_dict", "if", "data_dict", "[", "d", "]", "[", "'entity_id'", "]", "in", "partition_ids", "]", "\n", "\n", "return", "train_set", ",", "test_set", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.report_partition_stats": [[191, 214], ["enumerate", "pandas.DataFrame", "pandas.DataFrame", "print", "print", "print", "print", "print"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "report_partition_stats", "(", "cls", ",", "partitions_by_ids", ":", "Dict", "[", "str", ",", "List", "[", "int", "]", "]", ",", "document_ids", ":", "List", "[", "int", "]", ",", "\n", "doc_labels", ":", "List", "[", "int", "]", ")", ":", "\n", "        ", "labels_dict", "=", "{", "}", "\n", "for", "i", ",", "doc_id", "in", "enumerate", "(", "document_ids", ")", ":", "\n", "            ", "labels_dict", "[", "doc_id", "]", "=", "doc_labels", "[", "i", "]", "\n", "\n", "", "for", "partition_name", "in", "partitions_by_ids", ":", "\n", "            ", "p_ids", "=", "partitions_by_ids", "[", "partition_name", "]", "\n", "labels_train", "=", "pd", ".", "DataFrame", "(", "[", "labels_dict", "[", "d", "]", "for", "d", "in", "document_ids", "if", "d", "not", "in", "p_ids", "]", ")", "\n", "labels_test", "=", "pd", ".", "DataFrame", "(", "[", "labels_dict", "[", "d", "]", "for", "d", "in", "document_ids", "if", "d", "in", "p_ids", "]", ")", "\n", "\n", "print", "(", "'\\n-Partition {}-'", ".", "format", "(", "partition_name", ")", ")", "\n", "print", "(", "\"Train: {:,.0f} data points\"", ".", "format", "(", "labels_train", ".", "shape", "[", "0", "]", ")", ")", "\n", "print", "(", "\"Test: {:,.0f} data points\"", ".", "format", "(", "labels_test", ".", "shape", "[", "0", "]", ")", ")", "\n", "\n", "train_positive", "=", "labels_train", "[", "labels_train", "[", "0", "]", "==", "1", "]", ".", "shape", "[", "0", "]", "/", "labels_train", ".", "shape", "[", "0", "]", "\n", "train_negative", "=", "labels_train", "[", "labels_train", "[", "0", "]", "==", "0", "]", ".", "shape", "[", "0", "]", "/", "labels_train", ".", "shape", "[", "0", "]", "\n", "test_positive", "=", "labels_test", "[", "labels_test", "[", "0", "]", "==", "1", "]", ".", "shape", "[", "0", "]", "/", "labels_test", ".", "shape", "[", "0", "]", "\n", "test_negative", "=", "labels_test", "[", "labels_test", "[", "0", "]", "==", "0", "]", ".", "shape", "[", "0", "]", "/", "labels_test", ".", "shape", "[", "0", "]", "\n", "\n", "print", "(", "\"Train Set: {:.0%} pos - {:.0%} neg\"", ".", "format", "(", "train_positive", ",", "train_negative", ")", ")", "\n", "print", "(", "\"Test Set: {:.0%} pos - {:.0%} neg\"", ".", "format", "(", "test_positive", ",", "test_negative", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.summarize_runs": [[215, 224], ["issubclass", "list", "type", "run_results.keys", "type", "print", "type"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "summarize_runs", "(", "cls", ",", "run_results", ":", "Dict", ")", ":", "\n", "        ", "first_key_name", "=", "list", "(", "run_results", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "if", "issubclass", "(", "type", "(", "run_results", "[", "first_key_name", "]", ")", ",", "ModelRun", ")", ":", "\n", "            ", "return", "[", "run_results", "[", "mr", "]", ".", "evaluation", "for", "mr", "in", "run_results", "]", "\n", "", "elif", "type", "(", "run_results", "[", "first_key_name", "]", ")", "==", "dict", ":", "\n", "            ", "return", "[", "{", "key", ":", "run_results", "[", "mr", "]", "[", "key", "]", ".", "evaluation", "}", "for", "mr", "in", "run_results", "for", "key", "in", "run_results", "[", "mr", "]", "]", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"Unknown type stored in passed run_results: {}\"", ".", "format", "(", "type", "(", "run_results", "[", "first_key_name", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.show_feature_importances": [[225, 239], ["pandas.DataFrame", "pandas.merge.median", "pandas.merge.sort_values", "experiment.PartitionedExperiment.model_runs[].get_feature_importances", "pandas.merge", "pandas.DataFrame"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_feature_importances"], ["", "", "def", "show_feature_importances", "(", "self", ")", ":", "\n", "        ", "all_feature_importances", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "partition_id", "in", "self", ".", "model_runs", ":", "\n", "            ", "partition_feature_importances", "=", "self", ".", "model_runs", "[", "partition_id", "]", ".", "get_feature_importances", "(", ")", "\n", "\n", "# if the model has no feature importances, just exit now", "\n", "if", "partition_feature_importances", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "                ", "return", "pd", ".", "DataFrame", "(", ")", "\n", "\n", "", "partition_feature_importances", ".", "columns", "=", "[", "partition_id", "]", "\n", "all_feature_importances", "=", "pd", ".", "merge", "(", "all_feature_importances", ",", "partition_feature_importances", ",", "how", "=", "'outer'", ",", "\n", "left_index", "=", "True", ",", "right_index", "=", "True", ")", "\n", "", "all_feature_importances", "[", "'median'", "]", "=", "all_feature_importances", ".", "median", "(", "axis", "=", "1", ")", "\n", "return", "all_feature_importances", ".", "sort_values", "(", "'median'", ",", "ascending", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.get_selected_hyperparams": [[240, 246], ["pandas.DataFrame", "experiment.PartitionedExperiment.model_runs[].get_selected_hyperparams", "pandas.concat"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.get_selected_hyperparams"], ["", "def", "get_selected_hyperparams", "(", "self", ")", ":", "\n", "        ", "all_hyperparams", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "partition_id", "in", "self", ".", "model_runs", ":", "\n", "            ", "partition_hyperparams", "=", "self", ".", "model_runs", "[", "partition_id", "]", ".", "get_selected_hyperparams", "(", "partition_id", ")", "\n", "all_hyperparams", "=", "pd", ".", "concat", "(", "[", "all_hyperparams", ",", "partition_hyperparams", "]", ")", "\n", "", "return", "all_hyperparams", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.show_evaluation": [[247, 259], ["pandas.DataFrame", "pandas.DataFrame.median", "pandas.DataFrame.mean", "pandas.DataFrame.std", "pandas.DataFrame.sort_values"], "methods", ["None"], ["", "def", "show_evaluation", "(", "self", ",", "metric", ":", "str", "=", "'accuracy'", ")", ":", "\n", "        ", "all_accuracy", "=", "{", "}", "\n", "for", "partition_id", "in", "self", ".", "model_runs", ":", "\n", "            ", "all_accuracy", "[", "partition_id", "]", "=", "self", ".", "model_runs", "[", "partition_id", "]", ".", "evaluation", "[", "metric", "]", "\n", "", "all_accuracy_df", "=", "pd", ".", "DataFrame", "(", "all_accuracy", ",", "index", "=", "[", "self", ".", "name", "]", ")", "\n", "median", "=", "all_accuracy_df", ".", "median", "(", "axis", "=", "1", ")", "\n", "mean", "=", "all_accuracy_df", ".", "mean", "(", "axis", "=", "1", ")", "\n", "stddev", "=", "all_accuracy_df", ".", "std", "(", "axis", "=", "1", ")", "\n", "all_accuracy_df", "[", "'mean'", "]", "=", "mean", "\n", "all_accuracy_df", "[", "'median'", "]", "=", "median", "\n", "all_accuracy_df", "[", "'stddev'", "]", "=", "stddev", "\n", "return", "all_accuracy_df", ".", "sort_values", "(", "'median'", ",", "ascending", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.show_accuracy": [[260, 265], ["experiment.PartitionedExperiment.show_accuracy_perlevel"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.show_accuracy_perlevel"], ["", "def", "show_accuracy", "(", "self", ")", ":", "\n", "        ", "res", "=", "{", "}", "\n", "for", "level", "in", "(", "'doc_level'", ",", "'sentence_level'", ")", ":", "\n", "            ", "res", "[", "level", "]", "=", "self", ".", "show_accuracy_perlevel", "(", "level", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.show_accuracy_perlevel": [[266, 279], ["pandas.DataFrame", "pandas.DataFrame.median", "pandas.DataFrame.mean", "pandas.DataFrame.std", "pandas.DataFrame.sort_values"], "methods", ["None"], ["", "def", "show_accuracy_perlevel", "(", "self", ",", "level", ")", ":", "\n", "        ", "metric", "=", "'accuracy'", "\n", "all_accuracy", "=", "{", "}", "\n", "for", "partition_id", "in", "self", ".", "model_runs", ":", "\n", "            ", "all_accuracy", "[", "'partition{}'", ".", "format", "(", "partition_id", ")", "]", "=", "self", ".", "model_runs", "[", "partition_id", "]", "[", "level", "]", ".", "evaluation", "[", "metric", "]", "\n", "", "all_accuracy_df", "=", "pd", ".", "DataFrame", "(", "all_accuracy", ",", "index", "=", "[", "self", ".", "name", "]", ")", "\n", "median", "=", "all_accuracy_df", ".", "median", "(", "axis", "=", "1", ")", "\n", "mean", "=", "all_accuracy_df", ".", "mean", "(", "axis", "=", "1", ")", "\n", "stddev", "=", "all_accuracy_df", ".", "std", "(", "axis", "=", "1", ")", "\n", "all_accuracy_df", "[", "'mean'", "]", "=", "mean", "\n", "all_accuracy_df", "[", "'median'", "]", "=", "median", "\n", "all_accuracy_df", "[", "'stddev'", "]", "=", "stddev", "\n", "return", "all_accuracy_df", ".", "sort_values", "(", "'median'", ",", "ascending", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.generate_predictor": [[280, 291], ["experiment.PartitionedExperiment.model_runs[].generate_predictor"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.generate_predictor"], ["", "def", "generate_predictor", "(", "self", ",", "partition", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Return a Predictor from the trained model of a specific partition.\n\n        Args:\n            partition: The partition id of the model to return. Defaults to 0.\n\n        Returns: a Predictor.\n\n        \"\"\"", "\n", "return", "self", ".", "model_runs", "[", "partition", "]", ".", "generate_predictor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.__init__": [[295, 317], ["sklearn.base.clone"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "train_set", ":", "List", "[", "Dict", "]", ",", "test_set", ":", "List", "[", "Dict", "]", ",", "label_key", ":", "str", ",", "model", ",", "hyperparams", ":", "Dict", ",", "\n", "preprocessing_func", ":", "Callable", ",", "feature_subset", "=", "None", ",", "reduce_features", "=", "False", ")", ":", "\n", "        ", "self", ".", "train_set", "=", "train_set", "\n", "self", ".", "test_set", "=", "test_set", "\n", "self", ".", "label_key", "=", "label_key", "\n", "self", ".", "encoders", "=", "{", "}", "\n", "self", ".", "model", "=", "clone", "(", "model", ")", "\n", "self", ".", "hyperparams", "=", "hyperparams", "\n", "self", ".", "feature_subset", "=", "feature_subset", "\n", "self", ".", "reduce_features", "=", "reduce_features", "\n", "\n", "self", ".", "x_train", "=", "None", "\n", "self", ".", "x_test", "=", "None", "\n", "self", ".", "y_train", "=", "None", "\n", "self", ".", "y_test", "=", "None", "\n", "self", ".", "y_train_predicted", "=", "None", "\n", "self", ".", "y_test_predicted", "=", "None", "\n", "self", ".", "feature_cols", "=", "[", "]", "\n", "self", ".", "evaluation", "=", "None", "\n", "self", ".", "rfecv", "=", "None", "\n", "\n", "self", ".", "preprocessing_func", "=", "preprocessing_func", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run": [[318, 337], ["experiment.ModelRun.build_data", "experiment.ModelRun.search_hyperparameters", "experiment.ModelRun.rfecv.predict", "experiment.ModelRun.rfecv.predict", "experiment.ModelRun.evaluate_model", "experiment.ModelRun.model.fit", "experiment.ModelRun.model.predict", "experiment.ModelRun.model.predict", "experiment.ModelRun.evaluate_model", "experiment.ModelRun.train_feature_reducer", "experiment.ModelRun.rebuild_feature_cols_from_rfecv"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceToDocModelRun.build_data", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.search_hyperparameters", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.evaluate_model", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.evaluate_model", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.train_feature_reducer", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.rebuild_feature_cols_from_rfecv"], ["", "def", "run", "(", "self", ",", "run_hyperparam_search", ":", "bool", "=", "True", ")", ":", "\n", "        ", "self", ".", "x_train", ",", "self", ".", "x_test", ",", "self", ".", "y_train", ",", "self", ".", "y_test", ",", "self", ".", "feature_cols", ",", "self", ".", "encoders", "=", "self", ".", "build_data", "(", "\n", "self", ".", "train_set", ",", "self", ".", "test_set", ",", "self", ".", "label_key", ",", "self", ".", "feature_subset", ")", "\n", "if", "run_hyperparam_search", ":", "\n", "            ", "self", ".", "model", "=", "self", ".", "search_hyperparameters", "(", "self", ".", "model", ",", "self", ".", "hyperparams", ",", "self", ".", "x_train", ",", "self", ".", "y_train", ")", "\n", "if", "self", ".", "reduce_features", ":", "\n", "                ", "self", ".", "rfecv", "=", "self", ".", "train_feature_reducer", "(", "self", ".", "model", ",", "self", ".", "x_train", ",", "self", ".", "y_train", ")", "\n", "self", ".", "model", "=", "self", ".", "rfecv", ".", "estimator_", "\n", "self", ".", "feature_cols", "=", "self", ".", "rebuild_feature_cols_from_rfecv", "(", "self", ".", "feature_cols", ",", "self", ".", "rfecv", ".", "support_", ")", "\n", "", "", "if", "self", ".", "reduce_features", ":", "\n", "            ", "self", ".", "y_train_predicted", "=", "self", ".", "rfecv", ".", "predict", "(", "self", ".", "x_train", ")", "\n", "self", ".", "y_test_predicted", "=", "self", ".", "rfecv", ".", "predict", "(", "self", ".", "x_test", ")", "\n", "self", ".", "evaluation", "=", "self", ".", "evaluate_model", "(", "self", ".", "rfecv", ",", "self", ".", "x_test", ",", "self", ".", "y_test", ",", "self", ".", "y_test_predicted", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "model", ".", "fit", "(", "self", ".", "x_train", ",", "self", ".", "y_train", ")", "\n", "self", ".", "y_train_predicted", "=", "self", ".", "model", ".", "predict", "(", "self", ".", "x_train", ")", "\n", "self", ".", "y_test_predicted", "=", "self", ".", "model", ".", "predict", "(", "self", ".", "x_test", ")", "\n", "self", ".", "evaluation", "=", "self", ".", "evaluate_model", "(", "self", ".", "model", ",", "self", ".", "x_test", ",", "self", ".", "y_test", ",", "self", ".", "y_test_predicted", ")", "\n", "", "return", "self", ".", "evaluation", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.rebuild_feature_cols_from_rfecv": [[338, 345], ["enumerate", "new_feature_cols.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "rebuild_feature_cols_from_rfecv", "(", "cls", ",", "feature_cols", ",", "support", ")", ":", "\n", "        ", "new_feature_cols", "=", "[", "]", "\n", "for", "i", ",", "val", "in", "enumerate", "(", "support", ")", ":", "\n", "            ", "if", "val", ":", "\n", "                ", "new_feature_cols", ".", "append", "(", "feature_cols", "[", "i", "]", ")", "\n", "", "", "return", "new_feature_cols", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.build_data": [[346, 382], ["cls.train_encoders", "cls.build_x_features", "cls.build_x_features", "cls.build_y_vector", "cls.build_y_vector", "cls.restrict_features_to_subset", "cls.restrict_features_to_subset"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.train_encoders", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.build_x_features", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.build_x_features", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.build_y_vector", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.build_y_vector", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.restrict_features_to_subset", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.restrict_features_to_subset"], ["", "@", "classmethod", "\n", "def", "build_data", "(", "cls", ",", "train_set", ":", "List", "[", "Dict", "]", ",", "test_set", ":", "List", "[", "Dict", "]", ",", "label_key", ":", "str", ",", "feature_subset", ":", "List", "[", "str", "]", ")", "->", "Tuple", "[", "coo_matrix", ",", "coo_matrix", ",", "List", ",", "List", ",", "List", ",", "Dict", "]", ":", "\n", "        ", "\"\"\"Orchestrates the construction of train and test x matrices, and train and test y vectors.\n\n        `build_data` takes as input:\n            - train_set: List\n            - test_set: List\n            - label_key: str. key to use in data dicts for label\n            - feature_subset: List of str, or None if NA. Subset of features to use.\n\n        `build_data` returns a Tuple of the following:\n            - x_train: Matrix\n            - x_test: Matrix\n            - y_train: List\n            - y_test: List\n            - feature_cols: List\n            - encoders: Dict. Encoders used to generate the feature set. Encoders that may want to be saved include\n            vectorizers trained on the train_set and applied to the test_set.\n\n        \"\"\"", "\n", "\n", "encoders", "=", "cls", ".", "train_encoders", "(", "train_set", ")", "\n", "\n", "x_train", ",", "feature_cols", "=", "cls", ".", "build_x_features", "(", "train_set", ",", "encoders", ")", "\n", "x_test", ",", "feature_cols", "=", "cls", ".", "build_x_features", "(", "test_set", ",", "encoders", ")", "\n", "\n", "if", "feature_subset", ":", "\n", "            ", "x_train", "=", "cls", ".", "restrict_features_to_subset", "(", "x_train", ",", "feature_cols", ",", "feature_subset", ")", "\n", "x_test", "=", "cls", ".", "restrict_features_to_subset", "(", "x_test", ",", "feature_cols", ",", "feature_subset", ")", "\n", "feature_cols", "=", "feature_subset", "\n", "\n", "", "y_train", "=", "cls", ".", "build_y_vector", "(", "train_set", ",", "label_key", ")", "\n", "y_test", "=", "cls", ".", "build_y_vector", "(", "test_set", ",", "label_key", ")", "\n", "\n", "return", "x_train", ",", "x_test", ",", "y_train", ",", "y_test", ",", "feature_cols", ",", "encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.restrict_features_to_subset": [[383, 397], ["enumerate", "pandas.DataFrame.tocsr", "print", "pandas.DataFrame", "print", "df[].todense", "column_mask.append", "type"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "restrict_features_to_subset", "(", "cls", ",", "df", ",", "old_columns", ",", "feature_subset", ")", ":", "\n", "# column_mask = [col in feature_subset for col in old_columns]", "\n", "        ", "column_mask", "=", "[", "]", "\n", "for", "i", ",", "col_name", "in", "enumerate", "(", "old_columns", ")", ":", "\n", "            ", "if", "col_name", "in", "feature_subset", ":", "\n", "                ", "column_mask", ".", "append", "(", "i", ")", "\n", "", "", "df", "=", "df", ".", "tocsr", "(", ")", "\n", "print", "(", "\"type: {}\"", ".", "format", "(", "type", "(", "df", ")", ")", ")", "\n", "# df = df[:,column_mask].todense().copy()", "\n", "df", "=", "pd", ".", "DataFrame", "(", "df", "[", ":", ",", "column_mask", "]", ".", "todense", "(", ")", ")", "\n", "df", ".", "columns", "=", "feature_subset", "\n", "print", "(", "df", ".", "shape", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.train_encoders": [[398, 410], ["NotImplementedError"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "train_encoders", "(", "cls", ",", "train_set", ":", "List", "[", "Dict", "]", ")", ":", "\n", "        ", "\"\"\"\n        Placeholder function to hold the custom encoder training functionality of a ModelRun.\n\n        Args:\n            train_set: Data set to train encoders on.\n\n        Returns:\n            Dict of encoders.\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\"The ModelRun class must be subclassed to be used, \"", "\n", "\"with the `train_encoders` function implemented.\"", ")", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.build_x_features": [[412, 425], ["NotImplementedError"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_x_features", "(", "cls", ",", "data_set", ":", "List", "[", "Dict", "]", ",", "encoders", ":", "Dict", ")", ":", "\n", "        ", "\"\"\"\n        Placeholder function to hold the custom feature building functionality of a ModelRun.\n\n        Args:\n            data_set: Data set to transform into features.\n            encoders: Dict of pre-trained encoders for use in building features.\n\n        Returns:\n            Matrix-type\n        \"\"\"", "\n", "raise", "NotImplementedError", "(", "\"The ModelRun class must be subclassed to be used, \"", "\n", "\"with the `build_x_features` function implemented.\"", ")", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.build_y_vector": [[427, 439], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_y_vector", "(", "cls", ",", "data_set", ":", "List", "[", "Dict", "]", ",", "label_key", ":", "str", ")", "->", "List", ":", "\n", "        ", "\"\"\"\n        Extract the labels from each data dict and compile into one y vector.\n        Args:\n            data_set: List of data dicts.\n            label_key: The key int he data dicts under which the label is stored.\n\n        Returns:\n            Array-type\n        \"\"\"", "\n", "return", "[", "entity_dict", "[", "label_key", "]", "for", "entity_dict", "in", "data_set", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.search_hyperparameters": [[440, 447], ["sklearn.model_selection.RandomizedSearchCV", "sklearn.model_selection.RandomizedSearchCV.fit", "print"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "search_hyperparameters", "(", "cls", ",", "model", ",", "hyperparams", ",", "x_train", ",", "y_train", ")", ":", "\n", "        ", "random_search", "=", "RandomizedSearchCV", "(", "estimator", "=", "model", ",", "param_distributions", "=", "hyperparams", ",", "n_iter", "=", "5", ",", "cv", "=", "2", ",", "verbose", "=", "2", ",", "\n", "random_state", "=", "42", ",", "n_jobs", "=", "-", "1", ",", "scoring", "=", "'f1_macro'", ")", "\n", "random_search", ".", "fit", "(", "x_train", ",", "y_train", ")", "\n", "print", "(", "random_search", ".", "best_params_", ")", "\n", "return", "random_search", ".", "best_estimator_", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.train_feature_reducer": [[448, 455], ["sklearn.feature_selection.RFECV", "sklearn.feature_selection.RFECV.fit", "print", "print", "sklearn.feature_selection.RFECV.get_params"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "train_feature_reducer", "(", "cls", ",", "model", ",", "x_train", ",", "y_train", ")", ":", "\n", "        ", "rfecv", "=", "RFECV", "(", "estimator", "=", "model", ",", "step", "=", "0.05", ",", "scoring", "=", "'f1_macro'", ",", "n_jobs", "=", "-", "1", ")", "# , cv=StratifiedKFold)", "\n", "rfecv", ".", "fit", "(", "x_train", ",", "y_train", ")", "\n", "print", "(", "\"Optimal number of features : %d\"", "%", "rfecv", ".", "n_features_", ")", "\n", "print", "(", "\"Params: {}\"", ".", "format", "(", "rfecv", ".", "get_params", "(", ")", ")", ")", "\n", "return", "rfecv", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.evaluate_model": [[456, 477], ["model.score", "sklearn.metrics.f1_score", "sklearn.metrics.f1_score"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "evaluate_model", "(", "cls", ",", "model", ":", "Callable", ",", "x_test", ",", "y_test", ",", "y_test_predicted", ")", "->", "Dict", ":", "\n", "        ", "\"\"\"\n        Calculate and return a dictionary of various evaluation metrics.\n\n        Args:\n            model: a model with a `score` method.\n            x_test: input of the test set.\n            y_test: true labels for the test set.\n            y_test_predicted: the model's predictions for the test set.\n\n        Returns:\n\n        \"\"\"", "\n", "accuracy", "=", "model", ".", "score", "(", "x_test", ",", "y_test", ")", "\n", "f1_macro", "=", "f1_score", "(", "y_test", ",", "y_test_predicted", ",", "average", "=", "'macro'", ")", "\n", "f1_micro", "=", "f1_score", "(", "y_test", ",", "y_test_predicted", ",", "average", "=", "'micro'", ")", "\n", "return", "{", "\n", "\"accuracy\"", ":", "accuracy", ",", "\n", "\"f1_macro\"", ":", "f1_macro", ",", "\n", "\"f1_micro\"", ":", "f1_micro", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.get_feature_importances": [[479, 493], ["hasattr", "pandas.DataFrame", "fi.sort_values.sort_values.sort_values", "pandas.DataFrame"], "methods", ["None"], ["", "def", "get_feature_importances", "(", "self", ")", "->", "pd", ".", "DataFrame", ":", "\n", "        ", "\"\"\"\n        Generate the feature importances of the trained model. Requires self.model to have a `feature_importances_`\n        member variable.\n\n        Returns: pd.DataFrame of feature importances in descending order.\n\n        \"\"\"", "\n", "if", "hasattr", "(", "self", ".", "model", ",", "'feature_importances_'", ")", ":", "\n", "            ", "fi", "=", "pd", ".", "DataFrame", "(", "self", ".", "model", ".", "feature_importances_", ",", "index", "=", "self", ".", "feature_cols", ")", "\n", "fi", "=", "fi", ".", "sort_values", "(", "0", ",", "ascending", "=", "False", ")", "\n", "return", "fi", "\n", "", "else", ":", "\n", "            ", "return", "pd", ".", "DataFrame", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.get_selected_hyperparams": [[494, 504], ["type", "model_hyperparam_func", "len", "pandas.DataFrame"], "methods", ["None"], ["", "", "def", "get_selected_hyperparams", "(", "self", ",", "identifier", ")", "->", "pd", ".", "DataFrame", ":", "\n", "        ", "model_hyperparam_func_map", "=", "{", "\n", "RandomForestClassifier", ":", "self", ".", "get_selected_random_forest_hyperparams", ",", "\n", "SVC", ":", "self", ".", "get_selected_svc_hyperparams", "\n", "}", "\n", "model_type", "=", "type", "(", "self", ".", "model", ")", "\n", "model_hyperparam_func", "=", "model_hyperparam_func_map", "[", "model_type", "]", "\n", "chosen_hyperparams", "=", "model_hyperparam_func", "(", ")", "\n", "chosen_hyperparams", "[", "'num_features'", "]", "=", "len", "(", "self", ".", "feature_cols", ")", "\n", "return", "pd", ".", "DataFrame", "(", "chosen_hyperparams", ",", "index", "=", "[", "identifier", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.get_selected_random_forest_hyperparams": [[505, 515], ["None"], "methods", ["None"], ["", "def", "get_selected_random_forest_hyperparams", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "chosen_hyperparams", "=", "{", "\n", "'n_estimators'", ":", "self", ".", "model", ".", "n_estimators", ",", "\n", "'max_features'", ":", "self", ".", "model", ".", "max_features", ",", "\n", "'max_depth'", ":", "self", ".", "model", ".", "max_depth", ",", "\n", "'min_samples_split'", ":", "self", ".", "model", ".", "min_samples_split", ",", "\n", "'min_samples_leaf'", ":", "self", ".", "model", ".", "min_samples_leaf", ",", "\n", "'class_weight'", ":", "self", ".", "model", ".", "class_weight", ",", "\n", "}", "\n", "return", "chosen_hyperparams", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.get_selected_svc_hyperparams": [[516, 519], ["experiment.ModelRun.model.get_params"], "methods", ["None"], ["", "def", "get_selected_svc_hyperparams", "(", "self", ")", "->", "Dict", ":", "\n", "        ", "chosen_hyperparams", "=", "self", ".", "model", ".", "get_params", "(", ")", "\n", "return", "chosen_hyperparams", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.generate_predictor": [[520, 528], ["autodiscern.predictor.Predictor"], "methods", ["None"], ["", "def", "generate_predictor", "(", "self", ")", "->", "Predictor", ":", "\n", "        ", "\"\"\"\n        Return a Predictor object from the trained model.\n\n        Returns:\n            Predictor\n        \"\"\"", "\n", "return", "Predictor", "(", "self", ".", "model", ",", "self", ".", "encoders", ",", "self", ".", "preprocessing_func", ",", "self", ".", "build_x_features", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_score_for_question": [[33, 42], ["r.loc[].mean"], "function", ["None"], ["            ", "input_divider", "=", "1", "\n", "\n", "# print(\"input divider:\", input_divider)", "\n", "", "if", "(", "self", ".", "attn_method", "==", "'additive'", ")", ":", "\n", "            ", "self", ".", "attnW", "=", "nn", ".", "Linear", "(", "self", ".", "input_dim", ",", "self", ".", "input_dim", "//", "input_divider", ")", "\n", "queryv_dim", "=", "self", ".", "input_dim", "//", "input_divider", "# we use the mapped vector size", "\n", "", "elif", "(", "self", ".", "attn_method", "in", "{", "'dot'", ",", "'dot_scaled'", "}", ")", ":", "# only dot prodcut operation", "\n", "            ", "queryv_dim", "=", "self", ".", "input_dim", "# we use the input vector size since we will perform dot product", "\n", "if", "(", "self", ".", "attn_method", "==", "'dot_scaled'", ")", ":", "\n", "                ", "self", ".", "scaler", "=", "torch", ".", "sqrt", "(", "torch", ".", "tensor", "(", "queryv_dim", ",", "dtype", "=", "self", ".", "fdtype", ",", "device", "=", "self", ".", "device", ")", ")", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.continuous_regression": [[44, 46], ["None"], "function", ["None"], ["", "", "self", ".", "queryv", "=", "nn", ".", "Parameter", "(", "torch", ".", "randn", "(", "queryv_dim", ",", "dtype", "=", "self", ".", "fdtype", ",", "device", "=", "self", ".", "device", ")", ",", "requires_grad", "=", "True", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "encoder_outputs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.neg_neutral_pos_category": [[48, 57], ["round", "round", "round", "ValueError"], "function", ["None"], ["\n", "\n", "if", "(", "self", ".", "attn_method", "==", "'additive'", ")", ":", "\n", "# do the mapping using one-layer mlp network, followed by nonlinear element-wise operation", "\n", "            ", "encoder_map", "=", "self", ".", "nonlinear_func", "(", "self", ".", "attnW", "(", "encoder_outputs", ")", ")", "\n", "# print('encoder_map size', encoder_map.size())", "\n", "# print('queryv size', self.queryv.size())", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.neg_pos_category": [[59, 66], ["round", "round", "ValueError"], "function", ["None"], ["            ", "encoder_map", "=", "encoder_outputs", "\n", "# print('encoder_map size', encoder_map.size())", "\n", "# print('queryv size', self.queryv.size())", "\n", "# using  matmul to compute tensor vector multiplication", "\n", "", "attn_weights", "=", "encoder_map", ".", "matmul", "(", "self", ".", "queryv", ")", "\n", "if", "(", "self", ".", "attn_method", "==", "'dot_scaled'", ")", ":", "\n", "            ", "attn_weights", "=", "attn_weights", "/", "self", ".", "scaler", "\n", "# softmax", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.zero_one_category": [[68, 75], ["round", "round", "ValueError"], "function", ["None"], ["\n", "# returns (1, sents)", "\n", "return", "attn_weights_norm", "\n", "\n", "\n", "", "", "class", "BertEmbedder", "(", "nn", ".", "Module", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "bertmodel", ",", "proc_config", ")", ":", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.build_data_for_question_submodels": [[77, 151], ["datetime.datetime.now", "print", "list", "nltk.sentiment.vader.SentimentIntensityAnalyzer", "datetime.datetime.now", "print", "datetime.datetime.now", "print", "list", "data.keys", "label_func", "labels.keys", "sklearn.model_selection.train_test_split", "datetime.datetime.now", "print", "pandas.concat", "pandas.concat", "datetime.datetime.now", "print", "datetime.datetime.now", "print", "model.build_remaining_feature_vector", "label_func", "[].median().median", "model.get_score_for_question", "[].median"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.build_remaining_feature_vector", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_score_for_question"], ["self", ".", "bertmodel", "=", "bertmodel", ".", "float", "(", ")", "\n", "self", ".", "config", "=", "proc_config", "\n", "\n", "# use BertModel as word embedder", "\n", "self", ".", "bert_train_flag", "=", "self", ".", "config", ".", "get", "(", "'bert_train_flag'", ",", "False", ")", "\n", "# for now we are taking the last layer hidden vectors", "\n", "self", ".", "bert_all_output", "=", "self", ".", "config", ".", "get", "(", "'bert_all_output'", ",", "False", ")", "\n", "\n", "if", "not", "self", ".", "bert_train_flag", ":", "\n", "            ", "self", ".", "bertmodel", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "bertmodel", ".", "train", "(", ")", "\n", "\n", "# def forward_batch_sents(self, doc_tensor, attention_mask, num_sents):", "\n", "#     '''", "\n", "\n", "#     Args:", "\n", "#         doc_tenosr: tensor, (sents, max_sent_len)", "\n", "#         attention_mask: tensor, (sents, max_sent_len)", "\n", "#         num_sents: int, actual number of sentences in the document", "\n", "\n", "#     TODO: add flags and logic to handle multiple layers embedding (i.e. 12 layers embedding)", "\n", "#     '''", "\n", "#     # use BertModel as word embedder", "\n", "#     bert_train_flag = self.config.get('bert_train_flag', False)", "\n", "#     # for now we are taking the last layer hidden vectors", "\n", "#     bert_all_output = self.config.get('bert_all_output', False)", "\n", "#     bertmodel = self.bertmodel", "\n", "#     if(not bert_train_flag):", "\n", "#         bertmodel.eval()", "\n", "#     else:", "\n", "#         bertmodel.train()", "\n", "\n", "#     with torch.set_grad_enabled(bert_train_flag):", "\n", "#         encoded_layers, __ = bertmodel(doc_tensor[:num_sents],", "\n", "#                                        attention_mask=attention_mask[:num_sents],", "\n", "#                                        output_all_encoded_layers=bert_all_output)", "\n", "#         print('encoded_layers shape', encoded_layers.shape)", "\n", "\n", "#     # print(\"finished embedding sents using BERT!\")", "\n", "#     return encoded_layers", "\n", "\n", "", "", "def", "forward", "(", "self", ",", "doc_tensor", ",", "attention_mask", ",", "num_sents", ")", ":", "\n", "        ", "'''\n\n        Args:\n            doc_tenosr: tensor, (sents, max_sent_len)\n            attention_mask: tensor, (sents, max_sent_len)\n            num_sents: int, actual number of sentences in the document\n\n        TODO: add flags and logic to handle multiple layers embedding (i.e. 12 layers embedding)\n        '''", "\n", "\n", "embed_layers_lst", "=", "[", "]", "\n", "\n", "for", "sent_indx", "in", "range", "(", "num_sents", ")", ":", "# going over each sentenece one by one due to GPU limit :((", "\n", "# print('sent_indx', sent_indx)", "\n", "            ", "with", "torch", ".", "set_grad_enabled", "(", "self", ".", "bert_train_flag", ")", ":", "\n", "                ", "encoded_layers", ",", "__", "=", "self", ".", "bertmodel", "(", "doc_tensor", "[", "sent_indx", ":", "sent_indx", "+", "1", "]", ",", "\n", "attention_mask", "=", "attention_mask", "[", "sent_indx", ":", "sent_indx", "+", "1", "]", ",", "\n", "output_all_encoded_layers", "=", "self", ".", "bert_all_output", ")", "\n", "embed_layers_lst", ".", "append", "(", "encoded_layers", ")", "\n", "\n", "# # embed all sentences at once", "\n", "# with torch.set_grad_enabled(self.bert_train_flag):", "\n", "#     encoded_layers, __ = self.bertmodel(doc_tensor, attention_mask=attention_mask,", "\n", "#                                         output_all_encoded_layers=self.bert_all_output)", "\n", "#", "\n", "# return encoded_layers", "\n", "\n", "# concat everything", "\n", "", "", "out", "=", "torch", ".", "cat", "(", "embed_layers_lst", ",", "dim", "=", "0", ")", "\n", "# print(\"finished embedding sents using BERT!\")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.tfidf_data": [[153, 161], ["print", "sklearn.feature_extraction.text.TfidfVectorizer", "data_dict[].fit_transform", "data_dict[].transform", "print", "print", "len"], "function", ["None"], ["", "", "class", "SentenceEncoder", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "num_hiddenlayers", "=", "1", ",", "\n", "bidirection", "=", "False", ",", "pdropout", "=", "0.1", ",", "rnn_class", "=", "nn", ".", "GRU", ",", "\n", "nonlinear_func", "=", "torch", ".", "relu", ",", "config", "=", "{", "}", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "\n", "        ", "super", "(", "SentenceEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "# embedding dimension as input to RNN", "\n", "self", ".", "device", "=", "get_device", "(", "to_gpu", ",", "gpu_index", ")", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "# dimension of RNN output", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.build_remaining_feature_vector": [[163, 173], ["pandas.concat", "model.vectorize_html", "model.vectorize_link_type", "model.vectorize_citations", "model.compute_polarity", "model.vectorize_metamap", "model.compute_bibliography_feature", "data_dict.get"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_html", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_link_type", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_citations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.compute_polarity", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_metamap", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.compute_bibliography_feature"], ["self", ".", "pdropout", "=", "pdropout", "\n", "self", ".", "dropout_layer", "=", "nn", ".", "Dropout", "(", "pdropout", ")", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "fdtype", "=", "self", ".", "config", ".", "get", "(", "'fdtype'", ",", "torch", ".", "float32", ")", "\n", "\n", "if", "(", "bidirection", ")", ":", "\n", "            ", "self", ".", "num_directions", "=", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_directions", "=", "1", "\n", "\n", "# RNN module inserts dropout between layers of RNN except for the output of the last layer!", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_html": [[175, 185], ["pandas.DataFrame"], "function", ["None"], ["            ", "rnn_dropout", "=", "0", "\n", "", "else", ":", "\n", "            ", "rnn_dropout", "=", "self", ".", "pdropout", "\n", "", "self", ".", "rnn", "=", "rnn_class", "(", "self", ".", "input_dim", ",", "self", ".", "hidden_dim", ",", "num_layers", "=", "self", ".", "num_hiddenlayers", ",", "\n", "dropout", "=", "rnn_dropout", ",", "bidirectional", "=", "bidirection", ",", "batch_first", "=", "True", ")", "\n", "self", ".", "nonlinear_func", "=", "nonlinear_func", "\n", "\n", "", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_metamap": [[187, 197], ["pandas.DataFrame", "sum"], "function", ["None"], ["\n", "# a hidden vector has the shape (num_layers*num_directions, batch, hidden_dim)", "\n", "\n", "h0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "\n", "dtype", "=", "self", ".", "fdtype", ")", "\n", "if", "(", "isinstance", "(", "self", ".", "rnn", ",", "nn", ".", "LSTM", ")", ")", ":", "\n", "            ", "c0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", "\n", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "dtype", "=", "self", ".", "fdtype", ")", "\n", "hiddenvec", "=", "(", "h0", ",", "c0", ")", "\n", "", "else", ":", "\n", "            ", "hiddenvec", "=", "h0", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_link_type": [[199, 204], ["pandas.DataFrame", "sum", "sum"], "function", ["None"], ["\n", "", "def", "_process_rnn_hidden_output", "(", "self", ",", "hidden", ")", ":", "\n", "        ", "encoder_approach", "=", "self", ".", "config", ".", "get", "(", "'encoder_approach'", ")", "\n", "lastlayer_indx", "=", "-", "1", "\n", "batch_size", "=", "hidden", ".", "size", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_citations": [[206, 208], ["pandas.DataFrame", "len"], "function", ["None"], ["if", "(", "encoder_approach", "==", "'[h_f]'", ")", ":", "# keep only the last forward hidden state vector", "\n", "            ", "return", "hn", "[", "lastlayer_indx", "]", "# (1, num_sents, hidden_dim)", "\n", "", "elif", "(", "encoder_approach", "==", "'[h_f+h_b]'", ")", ":", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.compute_polarity": [[210, 214], ["sid.polarity_scores", "pandas.DataFrame"], "function", ["None"], ["", "elif", "(", "encoder_approach", "==", "'[h_f;h_b]'", ")", ":", "\n", "            ", "frwd_indx", "=", "0", "\n", "bckwd_indx", "=", "1", "\n", "# (1, num_sents, 2*hidden_dim)", "\n", "hn", "=", "torch", ".", "cat", "(", "[", "hn", "[", "lastlayer_indx", ",", "frwd_indx", ",", ":", ",", ":", "]", ",", "hn", "[", "lastlayer_indx", ",", "bckwd_indx", ",", ":", ",", ":", "]", "]", ",", "dim", "=", "-", "1", ")", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.compute_bibliography_feature": [[216, 229], ["any", "pandas.DataFrame", "input_str.lower"], "function", ["None"], ["\n", "", "", "def", "_run_rnn", "(", "self", ",", "embed_sents", ",", "doc_sents_len", ",", "num_sents", ")", ":", "\n", "# apply dropout", "\n", "        ", "embed_sents", "=", "self", ".", "dropout_layer", "(", "embed_sents", ")", "\n", "# init hidden", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "num_sents", ")", "\n", "# pack the batch", "\n", "packed_embeds", "=", "pack_padded_sequence", "(", "embed_sents", ",", "doc_sents_len", "[", ":", "num_sents", "]", ",", "batch_first", "=", "True", ",", "\n", "enforce_sorted", "=", "False", ")", "\n", "# print(\"packed_embeds\", \"\\n\", packed_embeds)", "\n", "packed_rnn_out", ",", "hidden", "=", "self", ".", "rnn", "(", "packed_embeds", ",", "hidden", ")", "\n", "# print(\"packed_rnn_out\", \"\\n\", packed_rnn_out)", "\n", "# print(\"hidden\", \"\\n\", hidden)", "\n", "# we need to unpack sequences", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.combine_features": [[231, 243], ["print", "print", "hstack", "hstack", "data_dict[].get_feature_names", "data_dict[].extend", "print", "print", "coo_matrix", "coo_matrix"], "function", ["None"], ["# return unpacked_output, hidden", "\n", "return", "self", ".", "_process_rnn_hidden_output", "(", "hidden", ")", "\n", "\n", "# def _process_rnn_hidden_output(self, unpacked_out, hidden):", "\n", "#     encoder_approach = self.config.get('encoder_approach')", "\n", "#     lastlayer_indx = -1", "\n", "#     batch_size = hidden.size(1)", "\n", "#     hn = hidden.view(self.num_hiddenlayers, self.num_directions, batch_size, self.hidden_dim)", "\n", "#     if(encoder_approach == '[h_f]'):  # keep only the last forward hidden state vector", "\n", "#         return hn[lastlayer_indx]  # (1, num_sents, hidden_dim)", "\n", "#     else:", "\n", "#         # num_sents, max_num_tokens, num_directions, hidden_dim", "\n", "#         rnn_out = unpacked_out.view(unpacked_out.size(0), unpacked_out.size(1), self.num_directions,", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.make_error_df": [[245, 251], ["pandas.DataFrame", "abs"], "function", ["None"], ["#         frwd_indx = 0", "\n", "#         bckwd_indx = 1", "\n", "#         t0 = 0", "\n", "#         if(encoder_approach == '[h_f+h_b]'):", "\n", "#             res = hn[lastlayer_indx, frwd_indx, :, :] + rnn_out[:, t0, bckwd_indx, :]", "\n", "#             return res.unsqueeze(0)  # (1, num_sents, hidden_dim)", "\n", "#         elif(encoder_approach == '[h_f;h_b]'):", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.density_scatter_plot": [[253, 263], ["plt.subplots", "ax.set_facecolor", "plt.hist2d", "plt.xlim", "plt.ylim", "plt.colorbar", "plt.title", "plt.show"], "function", ["None"], ["#             hn = torch.cat([hn[lastlayer_indx, frwd_indx, :, :],  rnn_out[:, t0, bckwd_indx, :]], dim=-1)", "\n", "#             return hn.unsqueeze(0)", "\n", "\n", "# def _run_rnn(self, embed_sents, doc_sents_len, num_sents):", "\n", "#     # apply dropout", "\n", "#     embed_sents = self.dropout_layer(embed_sents)", "\n", "#     # init hidden", "\n", "#     hidden = self.init_hidden(num_sents)", "\n", "#     # pack the batch", "\n", "#     packed_embeds = pack_padded_sequence(embed_sents, doc_sents_len[:num_sents], batch_first=True,", "\n", "#                                          enforce_sorted=False)", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.test_model": [[265, 285], ["model.fit", "model.predict", "model.predict", "model.score", "model.make_error_df", "results_train[].mean", "model.make_error_df", "results_test[].mean", "abs().mean", "abs().mean", "sklearn.metrics.confusion_matrix", "abs", "abs"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.make_error_df", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.make_error_df"], ["#     packed_rnn_out, hidden = self.rnn(packed_embeds, hidden)", "\n", "#     # print(\"packed_rnn_out\", \"\\n\", packed_rnn_out)", "\n", "#     # print(\"hidden\", \"\\n\", hidden)", "\n", "#     # we need to unpack sequences", "\n", "#     unpacked_output, out_seqlen = pad_packed_sequence(packed_rnn_out, batch_first=True)", "\n", "#     # return unpacked_output, hidden", "\n", "#     return self._process_rnn_hidden_output(unpacked_output, hidden)", "\n", "\n", "", "def", "forward", "(", "self", ",", "embed_sents", ",", "doc_sents_len", ",", "num_sents", ")", ":", "\n", "        ", "\"\"\" perform forward computation\n\n            Args:\n                embed_sents: torch.Tensor, (sents, max_sent_len, embed_dim), dtype=torch.float32 or torch.float64\n                    depending on fdtype.\n                doc_sents_len: torch.Tensor, (sents, ), dtype=torch.int64\n                num_sents: int, actual number of sentences in the doc\n        \"\"\"", "\n", "\n", "return", "self", ".", "_run_rnn", "(", "embed_sents", ",", "doc_sents_len", ",", "num_sents", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.run_random_cv": [[287, 321], ["print", "sklearn.ensemble.RandomForestClassifier", "sklearn.model_selection.RandomizedSearchCV", "sklearn.model_selection.RandomizedSearchCV.fit"], "function", ["None"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "attn_model", ",", "\n", "num_hiddenlayers", "=", "1", ",", "bidirection", "=", "False", ",", "pdropout", "=", "0.1", ",", "\n", "rnn_class", "=", "nn", ".", "GRU", ",", "nonlinear_func", "=", "torch", ".", "relu", ",", "config", "=", "{", "}", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "\n", "        ", "super", "(", "DocEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "get_device", "(", "to_gpu", ",", "gpu_index", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "# dimension of the hidden vector from rnn", "\n", "self", ".", "attn_model", "=", "attn_model", "# instance of :class:`Attention`", "\n", "self", ".", "num_hiddenlayers", "=", "num_hiddenlayers", "\n", "self", ".", "pdropout", "=", "pdropout", "\n", "self", ".", "dropout_layer", "=", "nn", ".", "Dropout", "(", "pdropout", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "# to get options for the attention module", "\n", "self", ".", "fdtype", "=", "self", ".", "config", ".", "get", "(", "'fdtype'", ",", "torch", ".", "float32", ")", "\n", "\n", "if", "(", "bidirection", ")", ":", "\n", "            ", "self", ".", "num_directions", "=", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_directions", "=", "1", "\n", "\n", "# rnn module inserts dropout between layers of rnn except for the output of the last layer!", "\n", "", "if", "(", "self", ".", "num_hiddenlayers", "==", "1", "and", "self", ".", "pdropout", ">", "0", ")", ":", "\n", "            ", "rnn_dropout", "=", "0", "\n", "", "else", ":", "\n", "            ", "rnn_dropout", "=", "self", ".", "pdropout", "\n", "\n", "", "self", ".", "rnn", "=", "rnn_class", "(", "self", ".", "input_dim", ",", "self", ".", "hidden_dim", ",", "num_layers", "=", "self", ".", "num_hiddenlayers", ",", "dropout", "=", "rnn_dropout", ",", "\n", "bidirectional", "=", "bidirection", ",", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "nonlinear_func", "=", "nonlinear_func", "\n", "\n", "", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.compare_base_to_random_cv": [[323, 336], ["sklearn.ensemble.RandomForestClassifier", "model.test_model", "model.run_random_cv", "print", "model.test_model", "print", "print", "print", "copy.deepcopy", "copy.deepcopy"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.test_model", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.run_random_cv", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.test_model"], ["\n", "# a hidden vector has the shape (num_layers*num_directions, batch, hidden_dim)", "\n", "h0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "\n", "dtype", "=", "self", ".", "fdtype", ")", "\n", "if", "(", "isinstance", "(", "self", ".", "rnn", ",", "nn", ".", "LSTM", ")", ")", ":", "\n", "            ", "c0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", "\n", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "dtype", "=", "self", ".", "fdtype", ")", "\n", "hiddenvec", "=", "(", "h0", ",", "c0", ")", "\n", "", "else", ":", "\n", "            ", "hiddenvec", "=", "h0", "\n", "", "return", "(", "hiddenvec", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.compile_multiple_density_scatter_plots": [[338, 362], ["list", "list", "plt.subplots", "enumerate", "plt.show", "data_dict.keys", "data_dict[].keys", "len", "len", "enumerate", "model_data.keys", "ax[].hist2d", "ax[].set_title", "ax[].set_xlim", "ax[].set_ylim", "ax[].set_facecolor", "ax[].text", "ax[].set_xlim", "ax[].set_ylim"], "function", ["None"], ["        ", "\"\"\"\n        Args:\n            rnn_out: torch tensor, (batch, seq_len, num_directions * hidden_size)\n        \"\"\"", "\n", "encoder_approach", "=", "self", ".", "config", ".", "get", "(", "'encoder_approach'", ")", "\n", "if", "(", "encoder_approach", "==", "'[h_f+h_b]'", ")", ":", "\n", "            ", "return", "rnn_out", "[", ":", ",", ":", ",", ":", "self", ".", "hidden_dim", "]", "+", "rnn_out", "[", ":", ",", ":", ",", "self", ".", "hidden_dim", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "rnn_out", "\n", "\n", "", "", "def", "forward", "(", "self", ",", "doc_tensor", ")", ":", "\n", "        ", "\"\"\" perform forward computation\n\n            Args:\n                doc_tensor: torch.Tensor, (1, sents, encoding_dim), dtype=torch.float32\n                            currently, it accepts one batch (i.e. one doc at a time due to GPU memory limit)\n        \"\"\"", "\n", "\n", "# init hidden", "\n", "num_sents", "=", "doc_tensor", ".", "size", "(", "0", ")", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "num_sents", ")", "\n", "rnn_out", ",", "hidden", "=", "self", ".", "rnn", "(", "doc_tensor", ",", "hidden", ")", "\n", "# print('rnn_out before', rnn_out.shape)", "\n", "# print(\"rnn_out\", \"\\n\", rnn_out)", "\n", "# print(\"hidden\", \"\\n\", hidden)", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.plot_confusion_matrix": [[364, 414], ["sklearn.metrics.confusion_matrix", "plt.subplots", "ax.imshow", "ax.figure.colorbar", "ax.set", "plt.setp", "range", "fig.tight_layout", "sklearn.metrics.confusion_matrix.astype", "ax.get_xticklabels", "cm_p.max", "range", "sklearn.metrics.confusion_matrix.sum", "numpy.arange", "numpy.arange", "ax.text"], "function", ["None"], ["rnn_out", "=", "self", ".", "_reshape_rnn_output", "(", "rnn_out", ")", "\n", "# print('rnn_out after', rnn_out.shape)", "\n", "attn_weights_norm", "=", "self", ".", "attn_model", "(", "rnn_out", ")", "# (1, num_sents)", "\n", "# print('attn_weights_norm size', attn_weights_norm.size())", "\n", "doc_vec", "=", "attn_weights_norm", ".", "unsqueeze", "(", "1", ")", ".", "bmm", "(", "rnn_out", ")", "# (docs, 1, num_sents) * (docs, num_sents, embed_dim)", "\n", "doc_vec", "=", "self", ".", "dropout_layer", "(", "doc_vec", ".", "squeeze", "(", "1", ")", ")", "# turning (docs, 1, embed_dim) to (docs, embed_dim)", "\n", "return", "doc_vec", ",", "attn_weights_norm", "\n", "\n", "\n", "", "", "class", "DocEncoder_MeanPooling", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "\n", "num_hiddenlayers", "=", "1", ",", "bidirection", "=", "False", ",", "pdropout", "=", "0.1", ",", "\n", "rnn_class", "=", "nn", ".", "GRU", ",", "nonlinear_func", "=", "torch", ".", "relu", ",", "config", "=", "{", "}", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "\n", "        ", "super", "(", "DocEncoder_MeanPooling", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "get_device", "(", "to_gpu", ",", "gpu_index", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "# dimension of the hidden vector from rnn", "\n", "self", ".", "num_hiddenlayers", "=", "num_hiddenlayers", "\n", "self", ".", "pdropout", "=", "pdropout", "\n", "self", ".", "dropout_layer", "=", "nn", ".", "Dropout", "(", "pdropout", ")", "\n", "\n", "self", ".", "config", "=", "config", "\n", "# to get options for the attention module", "\n", "self", ".", "fdtype", "=", "self", ".", "config", ".", "get", "(", "'fdtype'", ",", "torch", ".", "float32", ")", "\n", "\n", "if", "(", "bidirection", ")", ":", "\n", "            ", "self", ".", "num_directions", "=", "2", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_directions", "=", "1", "\n", "\n", "# rnn module inserts dropout between layers of rnn except for the output of the last layer!", "\n", "", "if", "(", "self", ".", "num_hiddenlayers", "==", "1", "and", "self", ".", "pdropout", ">", "0", ")", ":", "\n", "            ", "rnn_dropout", "=", "0", "\n", "", "else", ":", "\n", "            ", "rnn_dropout", "=", "self", ".", "pdropout", "\n", "\n", "", "self", ".", "rnn", "=", "rnn_class", "(", "self", ".", "input_dim", ",", "self", ".", "hidden_dim", ",", "num_layers", "=", "self", ".", "num_hiddenlayers", ",", "dropout", "=", "rnn_dropout", ",", "\n", "bidirectional", "=", "bidirection", ",", "batch_first", "=", "True", ")", "\n", "\n", "self", ".", "nonlinear_func", "=", "nonlinear_func", "\n", "\n", "", "def", "init_hidden", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"initialize hidden vectors at t=0\n\n        Args:\n            batch_size: int, the size of the current evaluated batch\n        \"\"\"", "\n", "# a hidden vector has the shape (num_layers*num_directions, batch, hidden_dim)", "\n", "h0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "\n", "dtype", "=", "self", ".", "fdtype", ")", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.tile_images_into_square": [[416, 441], ["map", "zip", "max", "max", "len", "int", "Image.new", "map", "enumerate", "ceil", "Image.new.paste", "sqrt", "int"], "function", ["None"], ["            ", "c0", "=", "torch", ".", "zeros", "(", "self", ".", "num_hiddenlayers", "*", "self", ".", "num_directions", ",", "batch_size", ",", "self", ".", "hidden_dim", "\n", ")", ".", "to", "(", "device", "=", "self", ".", "device", ",", "dtype", "=", "self", ".", "fdtype", ")", "\n", "hiddenvec", "=", "(", "h0", ",", "c0", ")", "\n", "", "else", ":", "\n", "            ", "hiddenvec", "=", "h0", "\n", "", "return", "(", "hiddenvec", ")", "\n", "\n", "", "def", "_reshape_rnn_output", "(", "self", ",", "rnn_out", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            rnn_out: torch tensor, (batch, seq_len, num_directions * hidden_size)\n        \"\"\"", "\n", "encoder_approach", "=", "self", ".", "config", ".", "get", "(", "'encoder_approach'", ")", "\n", "if", "(", "encoder_approach", "==", "'[h_f+h_b]'", ")", ":", "\n", "            ", "return", "rnn_out", "[", ":", ",", ":", ",", ":", "self", ".", "hidden_dim", "]", "+", "rnn_out", "[", ":", ",", ":", ",", "self", ".", "hidden_dim", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "rnn_out", "\n", "\n", "", "", "def", "forward", "(", "self", ",", "doc_tensor", ")", ":", "\n", "        ", "\"\"\" perform forward computation\n\n            Args:\n                doc_tensor: torch.Tensor, (1, sents, encoding_dim), dtype=torch.float32\n                            currently, it accepts one batch (i.e. one doc at a time due to GPU memory limit)\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.tile_images_into_rectangle_of_width": [[443, 469], ["map", "zip", "max", "max", "len", "int", "Image.new", "map", "enumerate", "ceil", "int", "Image.new.paste", "floor"], "function", ["None"], ["num_sents", "=", "doc_tensor", ".", "size", "(", "0", ")", "\n", "hidden", "=", "self", ".", "init_hidden", "(", "num_sents", ")", "\n", "rnn_out", ",", "hidden", "=", "self", ".", "rnn", "(", "doc_tensor", ",", "hidden", ")", "\n", "# print('rnn_out before', rnn_out.shape)", "\n", "# print(\"rnn_out\", \"\\n\", rnn_out)", "\n", "# print(\"hidden\", \"\\n\", hidden)", "\n", "# print('rnn_out size', rnn_out.shape)", "\n", "rnn_out", "=", "self", ".", "_reshape_rnn_output", "(", "rnn_out", ")", "\n", "# print('rnn_out after', rnn_out.shape)", "\n", "doc_vec", "=", "rnn_out", ".", "mean", "(", "axis", "=", "1", ")", "# mean pooling across the sentences (docs, embed_dim)", "\n", "doc_vec", "=", "self", ".", "dropout_layer", "(", "doc_vec", ")", "\n", "return", "doc_vec", ",", "None", "# placeholder for attn_weights", "\n", "\n", "\n", "", "", "class", "DocCategScorer", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "num_labels", ")", ":", "\n", "\n", "        ", "super", "(", "DocCategScorer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "# dimension of the output from :class:`DocEncoder`", "\n", "# TODO: do multiple mappings using Sequential or ModuleList", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "input_dim", ",", "num_labels", ")", "\n", "self", ".", "logsoftmax", "=", "nn", ".", "LogSoftmax", "(", "dim", "=", "1", ")", "\n", "\n", "", "def", "forward", "(", "self", ",", "doc_tensor", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.compile_multiple_confusion_matrices": [[471, 501], ["list", "enumerate", "model.tile_images_into_rectangle_of_width", "data_dict[].keys", "enumerate", "data_dict.keys", "model_data.keys", "model.plot_confusion_matrix", "image_filenames.append", "plt.savefig"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.tile_images_into_rectangle_of_width", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.plot_confusion_matrix"], ["\n", "\n", "# init hidden", "\n", "out", "=", "self", ".", "classifier", "(", "doc_tensor", ")", "\n", "# print('classifier ', out)", "\n", "# print(out.size())", "\n", "# compute log soft max", "\n", "out", "=", "self", ".", "logsoftmax", "(", "out", ")", "\n", "# print(\"classifier out\", out.shape)", "\n", "return", "out", "\n", "\n", "\n", "", "", "def", "validate_rnn_output", "(", "rnn_out", ",", "rnn_hidden", ",", "config", ")", ":", "\n", "    ", "num_directions", "=", "config", ".", "get", "(", "'num_directions'", ")", "\n", "num_layers", "=", "config", ".", "get", "(", "'num_layers'", ")", "\n", "h_dim", "=", "config", ".", "get", "(", "'hidden_dim'", ")", "\n", "batch_size", "=", "rnn_out", ".", "size", "(", "0", ")", "\n", "hn", "=", "rnn_hidden", ".", "view", "(", "num_layers", ",", "num_directions", ",", "batch_size", ",", "h_dim", ")", "\n", "lastlayer_indx", "=", "-", "1", "\n", "seqs_len", "=", "config", ".", "get", "(", "'seqs_len'", ")", "\n", "max_num_elms_inseq", "=", "rnn_out", ".", "size", "(", "1", ")", "\n", "# output has shape (batch_size, max_num_elms_inseq, num_directions, h_dim)", "\n", "output", "=", "rnn_out", ".", "view", "(", "batch_size", ",", "max_num_elms_inseq", ",", "num_directions", ",", "h_dim", ")", "\n", "for", "bindx", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "for", "dir_indx", "in", "range", "(", "num_directions", ")", ":", "\n", "            ", "print", "(", "\"batch index: {}, direction index: {}\"", ".", "format", "(", "bindx", ",", "dir_indx", ")", ")", "\n", "if", "(", "dir_indx", "==", "0", ")", ":", "# in case of forward hidden vector, t=T (last time step) should be used", "\n", "                ", "t", "=", "seqs_len", "[", "bindx", "]", "-", "1", "\n", "", "else", ":", "# in case of backward hidden vector, t=0 will be the one to use", "\n", "                ", "t", "=", "0", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_feature_importances": [[503, 507], ["pandas.DataFrame", "fi.sort_values.sort_values"], "function", ["None"], ["#             print(output[bindx, t, dir_indx, :])", "\n", "#             print()", "\n", "", "assert", "torch", ".", "equal", "(", "hn", "[", "lastlayer_indx", ",", "dir_indx", ",", "bindx", ",", ":", "]", ",", "output", "[", "bindx", ",", "t", ",", "dir_indx", ",", ":", "]", ")", "\n", "", "", "print", "(", "\"passed the comparison test!!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.show_top_features_importances": [[509, 519], ["list", "modeling_data.keys", "print", "print", "model.get_feature_importances", "print", "list", "get_feature_importances.head"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_feature_importances"], ["", "def", "get_model_numparams", "(", "model", ")", ":", "\n", "    ", "return", "sum", "(", "param", ".", "numel", "(", ")", "for", "param", "in", "model", ".", "parameters", "(", ")", "if", "param", ".", "requires_grad", ")", "\n", "\n", "\n", "", "def", "restrict_grad_", "(", "mparams", ",", "mode", ",", "limit", ")", ":", "\n", "    ", "\"\"\"clamp/clip a gradient in-place\n    \"\"\"", "\n", "if", "(", "mode", "==", "'clip_norm'", ")", ":", "\n", "        ", "__", ",", "maxl", "=", "limit", "\n", "nn", ".", "utils", ".", "clip_grad_norm", "(", "mparams", ",", "maxl", ",", "norm_type", "=", "2", ")", "# l2 norm clipping", "\n", "", "elif", "(", "mode", "==", "'clamp'", ")", ":", "# case of clamping", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.calc_base_rate_accuracy": [[521, 539], ["pandas.DataFrame", "pandas.pivot_table", "scores.append", "sum", "len", "sum", "len"], "function", ["None"], ["for", "param", "in", "mparams", ":", "\n", "            ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "                ", "param", ".", "grad", ".", "data", ".", "clamp_", "(", "minl", ",", "maxl", ")", "\n", "\n", "\n", "", "", "", "", "def", "generate_sents_embeds_from_docs", "(", "docs_data_tensor", ",", "bertembeder", ",", "embed_dir", ",", "fdtype", ",", "to_gpu", "=", "True", ",", "gpu_index", "=", "0", ")", ":", "\n", "    ", "\"\"\"Generate token embedding for sentences in docs\n\n    Args:\n        docs_data_tensor: instance of :class:`DocsDataTensor`\n        bertembeder: instance of :class:`BertEmbedder`\n        embed_dir: string, path to directory where to dump embedding per document\n        fdtype: torch dtype, {torch.float32 or torch.float64}\n        to_gpu: bool, whether to use gpu or cpu\n        gpu_index: if to_gpu, which gpu index to use\n\n    \"\"\"", "\n", "bert_proc_docs", "=", "{", "}", "\n", "device", "=", "get_device", "(", "to_gpu", "=", "to_gpu", ",", "index", "=", "gpu_index", ")", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.calc_accuracy": [[541, 550], ["pandas.DataFrame", "pandas.pivot_table", "scores.append"], "function", ["None"], ["bertembeder", ".", "type", "(", "fdtype", ")", ".", "to", "(", "device", ")", "\n", "samples_counter", "=", "0", "\n", "num_iter", "=", "len", "(", "docs_data_tensor", ")", "# number of samples", "\n", "for", "doc_indx", "in", "range", "(", "num_iter", ")", ":", "\n", "        ", "print", "(", "doc_indx", ")", "\n", "doc_batch", ",", "doc_len", ",", "doc_sents_len", ",", "doc_attn_mask", ",", "doc_labels", ",", "doc_id", "=", "docs_data_tensor", "[", "doc_indx", "]", "\n", "# push to gpu", "\n", "embed_sents", "=", "bertembeder", "(", "doc_batch", ".", "to", "(", "device", ")", ",", "doc_attn_mask", ".", "to", "(", "device", ")", ",", "doc_len", ".", "item", "(", ")", ")", "\n", "# write to disk for now", "\n", "embed_fpath", "=", "os", ".", "path", ".", "join", "(", "embed_dir", ",", "'{}.pkl'", ".", "format", "(", "doc_id", ")", ")", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.calc_in_sample_accuracy": [[552, 569], ["pandas.DataFrame", "pandas.pivot_table", "d[].score", "in_sample_scores.append"], "function", ["None"], ["# add embedding to dict", "\n", "bert_proc_docs", "[", "doc_id", "]", "=", "embed_fpath", "\n", "# clean stuff", "\n", "del", "embed_sents", "\n", "# torch.cuda.ipc_collect()", "\n", "# torch.cuda.empty_cache()", "\n", "samples_counter", "+=", "1", "\n", "print", "(", "\"processed doc id: {}, {}/{}\"", ".", "format", "(", "doc_id", ",", "samples_counter", ",", "num_iter", ")", ")", "\n", "", "return", "bert_proc_docs", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.__init__": [[14, 17], ["nltk.data.load", "nltk.tokenize.TreebankWordTokenizer"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataInterface.load"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "splitter", "=", "nltk", ".", "data", ".", "load", "(", "'tokenizers/punkt/english.pickle'", ")", "\n", "self", ".", "tokenizer", "=", "nltk", ".", "tokenize", ".", "TreebankWordTokenizer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split": [[18, 27], ["lemmatization.Splitter.splitter.tokenize", "lemmatization.Splitter.tokenizer.tokenize"], "methods", ["None"], ["", "def", "split", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"\n        out : ['What', 'can', 'I', 'say', 'about', 'this', 'place', '.']\n        \"\"\"", "\n", "# split into single sentence", "\n", "sentences", "=", "self", ".", "splitter", ".", "tokenize", "(", "text", ")", "\n", "# tokenization in each sentences", "\n", "tokens", "=", "[", "self", ".", "tokenizer", ".", "tokenize", "(", "sent", ")", "for", "sent", "in", "sentences", "]", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.LemmatizationWithPOSTagger.__init__": [[30, 32], ["nltk.stem.WordNetLemmatizer"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "lemmatizer", "=", "WordNetLemmatizer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.LemmatizationWithPOSTagger.get_wordnet_pos": [[33, 49], ["treebank_tag.startswith", "treebank_tag.startswith", "treebank_tag.startswith", "treebank_tag.startswith"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "get_wordnet_pos", "(", "treebank_tag", ")", ":", "\n", "        ", "\"\"\"\n        return WORDNET POS compliance to WORDENT lemmatization (a,n,r,v)\n        \"\"\"", "\n", "if", "treebank_tag", ".", "startswith", "(", "'J'", ")", ":", "\n", "            ", "return", "wordnet", ".", "ADJ", "\n", "", "elif", "treebank_tag", ".", "startswith", "(", "'V'", ")", ":", "\n", "            ", "return", "wordnet", ".", "VERB", "\n", "", "elif", "treebank_tag", ".", "startswith", "(", "'N'", ")", ":", "\n", "            ", "return", "wordnet", ".", "NOUN", "\n", "", "elif", "treebank_tag", ".", "startswith", "(", "'R'", ")", ":", "\n", "            ", "return", "wordnet", ".", "ADV", "\n", "", "else", ":", "\n", "# As default pos in lemmatization is Noun", "\n", "            ", "return", "wordnet", ".", "NOUN", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.LemmatizationWithPOSTagger.pos_tag": [[50, 61], ["nltk.pos_tag", "lemmatization.LemmatizationWithPOSTagger.lemmatizer.lemmatize", "lemmatization.LemmatizationWithPOSTagger.get_wordnet_pos"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.LemmatizationWithPOSTagger.pos_tag", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Lemmatizer.lemmatize", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.LemmatizationWithPOSTagger.get_wordnet_pos"], ["", "", "def", "pos_tag", "(", "self", ",", "tokens", ")", ":", "\n", "# find the pos tagginf for each tokens [('What', 'WP'), ('can', 'MD'), ('I', 'PRP') ....", "\n", "        ", "pos_tokens", "=", "[", "nltk", ".", "pos_tag", "(", "token", ")", "for", "token", "in", "tokens", "]", "\n", "\n", "# lemmatization using pos tagg", "\n", "# convert into feature set of [('What', 'What', ['WP']), ('can', 'can', ['MD']), ...", "\n", "# ie [original WORD, Lemmatized word, POS tag]", "\n", "pos_tokens", "=", "[", "\n", "[", "(", "word", ",", "self", ".", "lemmatizer", ".", "lemmatize", "(", "word", ",", "self", ".", "get_wordnet_pos", "(", "pos_tag", ")", ")", ",", "[", "pos_tag", "]", ")", "for", "(", "word", ",", "pos_tag", ")", "in", "\n", "pos", "]", "for", "pos", "in", "pos_tokens", "]", "\n", "return", "pos_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Lemmatizer.__init__": [[65, 68], ["lemmatization.Splitter", "lemmatization.LemmatizationWithPOSTagger"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "splitter", "=", "Splitter", "(", ")", "\n", "self", ".", "lemmatizer", "=", "LemmatizationWithPOSTagger", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Lemmatizer.reassemble": [[69, 72], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "reassemble", "(", "list_of_sent_list_of_tokens", ")", ":", "\n", "        ", "return", "' '", ".", "join", "(", "[", "token", "[", "1", "]", "for", "sentence", "in", "list_of_sent_list_of_tokens", "for", "token", "in", "sentence", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Lemmatizer.lemmatize": [[73, 77], ["lemmatization.Lemmatizer.splitter.split", "lemmatization.Lemmatizer.lemmatizer.pos_tag", "lemmatization.Lemmatizer.reassemble"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.LemmatizationWithPOSTagger.pos_tag", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Lemmatizer.reassemble"], ["", "def", "lemmatize", "(", "self", ",", "text", ")", ":", "\n", "        ", "tokens", "=", "self", ".", "splitter", ".", "split", "(", "text", ")", "\n", "lemma_pos_token", "=", "self", ".", "lemmatizer", ".", "pos_tag", "(", "tokens", ")", "\n", "return", "self", ".", "reassemble", "(", "lemma_pos_token", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Lemmatizer.lemmatize_list_of_texts": [[78, 83], ["lemmatized_list_of_texts.append", "lemmatization.Lemmatizer.lemmatize"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Lemmatizer.lemmatize"], ["", "def", "lemmatize_list_of_texts", "(", "self", ",", "list_of_texts", ")", ":", "\n", "        ", "lemmatized_list_of_texts", "=", "[", "]", "\n", "for", "text", "in", "list_of_texts", ":", "\n", "            ", "lemmatized_list_of_texts", ".", "append", "(", "self", ".", "lemmatize", "(", "text", ")", ")", "\n", "", "return", "lemmatized_list_of_texts", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.predictors.DocPredictor.make_prediction": [[26, 50], ["requests.get", "requests.get.content.decode", "autodiscern.Transformer", "adt.Transformer.apply", "autodiscern.add_inline_citations_annotations", "autodiscern.add_metamap_annotations", "nltk.sentiment.vader.SentimentIntensityAnalyzer", "autodiscern.build_remaining_feature_vector", "predictors[].predict"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_inline_citations_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_metamap_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.build_remaining_feature_vector", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict"], ["", "def", "make_prediction", "(", "predictors", ":", "Dict", "[", "Predictor", "]", ",", "url", ":", "str", ")", "->", "Dict", ":", "\n", "    ", "res", "=", "requests", ".", "get", "(", "url", ")", "\n", "html_page", "=", "res", ".", "content", ".", "decode", "(", "\"utf-8\"", ")", "\n", "data_dict", "=", "{", "0", ":", "{", "'entity_id'", ":", "0", ",", "'content'", ":", "html_page", ",", "'url'", ":", "url", "}", "}", "\n", "\n", "html_transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "\n", "html_to_plain_text", "=", "True", ",", "\n", "annotate_html", "=", "True", ",", "\n", "parallelism", "=", "False", "\n", ")", "\n", "transformed_data", "=", "html_transformer", ".", "apply", "(", "data_dict", ")", "\n", "transformed_data", "=", "ada", ".", "add_inline_citations_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "ada", ".", "add_metamap_annotations", "(", "transformed_data", ",", "dm", ")", "\n", "\n", "sid", "=", "SentimentIntensityAnalyzer", "(", ")", "\n", "\n", "for", "key", "in", "data_dict", ":", "\n", "        ", "transformed_data", "[", "key", "]", "[", "'feature_vec'", "]", "=", "adm", ".", "build_remaining_feature_vector", "(", "transformed_data", "[", "key", "]", ",", "sid", ")", "\n", "\n", "", "predictions", "=", "{", "}", "\n", "for", "q", "in", "predictors", ":", "\n", "        ", "predictions", "[", "q", "]", "=", "predictors", "[", "q", "]", ".", "predict", "(", "data_dict", "[", "0", "]", ")", "\n", "\n", "", "return", "predictions", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun.__init__": [[16, 20], ["autodiscern.ModelRun.__init__", "hierarchical_sent_doc_experiment.SentenceLevelModelRun._assign_vars"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.__init__", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment._assign_vars"], ["def", "__init__", "(", "self", ",", "train_set", ":", "List", "[", "Dict", "]", ",", "test_set", ":", "List", "[", "Dict", "]", ",", "model", ",", "hyperparams", ":", "Dict", ",", "\n", "data_dict", ":", "Dict", ",", "doc_level_info", ":", "Dict", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "train_set", ",", "test_set", ",", "model", ",", "hyperparams", ")", "\n", "SentenceLevelModelRun", ".", "_assign_vars", "(", "data_dict", ",", "doc_level_info", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun._assign_vars": [[21, 28], ["print", "print", "print", "len", "len"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_assign_vars", "(", "cls", ",", "data_dict", ",", "doc_level_info", ")", ":", "\n", "        ", "cls", ".", "data_dict", "=", "data_dict", "\n", "cls", ".", "doc_level_info", "=", "doc_level_info", "\n", "print", "(", "'assigning cls vars in SentenceLevelModelRun'", ")", "\n", "print", "(", "'len(data_dict): '", ",", "len", "(", "cls", ".", "data_dict", ")", ")", "\n", "print", "(", "'len(doc_level_info): '", ",", "len", "(", "cls", ".", "doc_level_info", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun._extract_dset": [[29, 37], ["range", "dset.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_extract_dset", "(", "cls", ",", "doc_ids", ",", "doc_level_info", ",", "data_dict", ")", ":", "\n", "        ", "dset", "=", "[", "]", "\n", "for", "doc_id", "in", "doc_ids", ":", "\n", "            ", "num_sents", "=", "doc_level_info", "[", "doc_id", "]", "[", "'num_sent'", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "num_sents", ")", ":", "\n", "                ", "dset", ".", "append", "(", "data_dict", "[", "'{}-{}'", ".", "format", "(", "doc_id", ",", "i", ")", "]", ")", "\n", "", "", "return", "dset", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun._inflate_tfidf_doc_features": [[38, 47], ["x_tfidf.toarray", "numpy.repeat", "scipy.sparse.coo_matrix"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_inflate_tfidf_doc_features", "(", "cls", ",", "x_tfidf", ",", "doc_ids", ",", "doc_level_info", ")", ":", "\n", "        ", "arr_reps", "=", "x_tfidf", ".", "toarray", "(", ")", "\n", "# print('arr_reps.shape: ', arr_reps.shape)", "\n", "repeat_info", "=", "[", "doc_level_info", "[", "doc_id", "]", "[", "'num_sent'", "]", "for", "doc_id", "in", "doc_ids", "]", "\n", "# print('repeat_info: ', repeat_info)", "\n", "inflated_reps", "=", "np", ".", "repeat", "(", "arr_reps", ",", "repeat_info", ",", "axis", "=", "0", ")", "\n", "x_upd_tfidf", "=", "coo_matrix", "(", "inflated_reps", ")", "\n", "return", "x_upd_tfidf", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun.build_features": [[48, 100], ["list", "list.sort", "list", "list", "list.sort", "list.sort", "print", "print", "print", "print", "cls._extract_dset", "cls._extract_dset", "pandas.concat", "pandas.concat", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "sklearn.feature_extraction.text.TfidfVectorizer.transform", "cls._inflate_tfidf_doc_features", "cls._inflate_tfidf_doc_features", "scipy.sparse.hstack", "scipy.sparse.hstack", "sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names", "sklearn.feature_extraction.text.TfidfVectorizer.get_feature_names.extend", "doc_level_info.keys", "set", "set", "len", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix", "int", "int"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun._extract_dset", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun._extract_dset", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun._inflate_tfidf_doc_features", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun._inflate_tfidf_doc_features"], ["", "@", "classmethod", "\n", "def", "build_features", "(", "cls", ",", "train_set", ":", "List", "[", "Dict", "]", ",", "test_set", ":", "List", "[", "Dict", "]", ")", "->", "Tuple", "[", "coo_matrix", ",", "coo_matrix", ",", "List", ",", "List", ",", "\n", "List", ",", "Dict", "]", ":", "\n", "\n", "        ", "doc_level_info", "=", "SentenceLevelModelRun", ".", "doc_level_info", "\n", "document_ids", "=", "list", "(", "doc_level_info", ".", "keys", "(", ")", ")", "\n", "document_ids", ".", "sort", "(", ")", "\n", "\n", "data_dict", "=", "SentenceLevelModelRun", ".", "data_dict", "\n", "train_ids", "=", "list", "(", "set", "(", "[", "int", "(", "entity_dict", "[", "'entity_id'", "]", ")", "for", "entity_dict", "in", "train_set", "]", ")", ")", "\n", "test_ids", "=", "list", "(", "set", "(", "[", "int", "(", "entity_dict", "[", "'entity_id'", "]", ")", "for", "entity_dict", "in", "test_set", "]", ")", ")", "\n", "\n", "train_ids", ".", "sort", "(", ")", "\n", "test_ids", ".", "sort", "(", ")", "\n", "\n", "print", "(", "'build_features method..'", ")", "\n", "print", "(", "train_ids", ")", "\n", "print", "(", "test_ids", ")", "\n", "print", "(", "'len(doc_level_info): '", ",", "len", "(", "doc_level_info", ")", ")", "\n", "\n", "corpus_train", "=", "[", "doc_level_info", "[", "doc_id", "]", "[", "'content'", "]", "for", "doc_id", "in", "train_ids", "]", "\n", "corpus_test", "=", "[", "doc_level_info", "[", "doc_id", "]", "[", "'content'", "]", "for", "doc_id", "in", "test_ids", "]", "\n", "\n", "# update train set and test set lists to reflect the above order", "\n", "train_set", "=", "cls", ".", "_extract_dset", "(", "train_ids", ",", "doc_level_info", ",", "data_dict", ")", "\n", "test_set", "=", "cls", ".", "_extract_dset", "(", "test_ids", ",", "doc_level_info", ",", "data_dict", ")", "\n", "\n", "# set the updated train/test sets", "\n", "cls", ".", "train_set_upd", "=", "train_set", "\n", "cls", ".", "test_set_upd", "=", "test_set", "\n", "\n", "feature_vec_train", "=", "pd", ".", "concat", "(", "[", "entity_dict", "[", "'feature_vec'", "]", "for", "entity_dict", "in", "train_set", "]", ",", "axis", "=", "0", ")", "\n", "feature_vec_test", "=", "pd", ".", "concat", "(", "[", "entity_dict", "[", "'feature_vec'", "]", "for", "entity_dict", "in", "test_set", "]", ",", "axis", "=", "0", ")", "\n", "\n", "y_train", "=", "[", "entity_dict", "[", "'label'", "]", "for", "entity_dict", "in", "train_set", "]", "\n", "y_test", "=", "[", "entity_dict", "[", "'label'", "]", "for", "entity_dict", "in", "test_set", "]", "\n", "\n", "# tfidf using doc level info", "\n", "vectorizer", "=", "TfidfVectorizer", "(", "max_df", "=", "0.98", ",", "min_df", "=", "0.01", ",", "stop_words", "=", "'english'", ")", "\n", "x_train_tfidf", "=", "vectorizer", ".", "fit_transform", "(", "corpus_train", ")", "\n", "x_test_tfidf", "=", "vectorizer", ".", "transform", "(", "corpus_test", ")", "\n", "\n", "x_train_tfidf", "=", "cls", ".", "_inflate_tfidf_doc_features", "(", "x_train_tfidf", ",", "train_ids", ",", "doc_level_info", ")", "\n", "x_test_tfidf", "=", "cls", ".", "_inflate_tfidf_doc_features", "(", "x_test_tfidf", ",", "test_ids", ",", "doc_level_info", ")", "\n", "\n", "x_train", "=", "hstack", "(", "[", "x_train_tfidf", ",", "coo_matrix", "(", "feature_vec_train", ")", "]", ")", "\n", "x_test", "=", "hstack", "(", "[", "x_test_tfidf", ",", "coo_matrix", "(", "feature_vec_test", ")", "]", ")", "\n", "feature_cols", "=", "vectorizer", ".", "get_feature_names", "(", ")", "\n", "feature_cols", ".", "extend", "(", "feature_vec_train", ".", "columns", ")", "\n", "encoders", "=", "{", "'vectorizer'", ":", "vectorizer", "}", "\n", "\n", "return", "x_train", ",", "x_test", ",", "y_train", ",", "y_test", ",", "feature_cols", ",", "encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.SentenceToDocModelRun.build_features": [[104, 130], ["numpy.where", "numpy.where", "numpy.where", "numpy.where", "hierarchical_sent_doc_experiment.sents_to_doc_buckets_mean", "hierarchical_sent_doc_experiment.sents_to_doc_buckets_mean", "[].median", "[].median", "numpy.where", "numpy.where", "numpy.where", "numpy.where", "train_set.groupby", "test_set.groupby"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.sents_to_doc_buckets_mean", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.sents_to_doc_buckets_mean"], ["    ", "@", "classmethod", "\n", "def", "build_features", "(", "cls", ",", "train_set", ":", "pd", ".", "DataFrame", ",", "test_set", ":", "pd", ".", "DataFrame", ")", "->", "Tuple", "[", "pd", ".", "DataFrame", ",", "pd", ".", "DataFrame", ",", "List", ",", "\n", "List", ",", "List", ",", "Dict", "]", ":", "\n", "# turn string labels into numbers", "\n", "        ", "train_set", "[", "'pred_num'", "]", "=", "np", ".", "where", "(", "train_set", "[", "'sub_prediction'", "]", "==", "'positive'", ",", "2", ",", "\n", "np", ".", "where", "(", "train_set", "[", "'sub_prediction'", "]", "==", "'neutral'", ",", "1", ",", "0", ")", ")", "\n", "train_set", "[", "'label_num'", "]", "=", "np", ".", "where", "(", "train_set", "[", "'label'", "]", "==", "'positive'", ",", "2", ",", "\n", "np", ".", "where", "(", "train_set", "[", "'label'", "]", "==", "'neutral'", ",", "1", ",", "0", ")", ")", "\n", "test_set", "[", "'pred_num'", "]", "=", "np", ".", "where", "(", "test_set", "[", "'sub_prediction'", "]", "==", "'positive'", ",", "2", ",", "\n", "np", ".", "where", "(", "test_set", "[", "'sub_prediction'", "]", "==", "'neutral'", ",", "1", ",", "0", ")", ")", "\n", "test_set", "[", "'label_num'", "]", "=", "np", ".", "where", "(", "test_set", "[", "'label'", "]", "==", "'positive'", ",", "2", ",", "\n", "np", ".", "where", "(", "test_set", "[", "'label'", "]", "==", "'neutral'", ",", "1", ",", "0", ")", ")", "\n", "\n", "# generate a df where each row represents a document (the document is partitioned into 10 equal parts)", "\n", "# such that we compute average prediction on each part to form a feature (i.e. a column) in df", "\n", "# hence for each row, we have 10 features", "\n", "x_train", "=", "sents_to_doc_buckets_mean", "(", "train_set", ")", "\n", "x_test", "=", "sents_to_doc_buckets_mean", "(", "test_set", ")", "\n", "\n", "y_train", "=", "train_set", ".", "groupby", "(", "'doc_id'", ")", "[", "'label_num'", "]", ".", "median", "(", ")", "\n", "y_test", "=", "test_set", ".", "groupby", "(", "'doc_id'", ")", "[", "'label_num'", "]", ".", "median", "(", ")", "\n", "\n", "feature_cols", "=", "x_train", ".", "columns", "\n", "encoders", "=", "{", "}", "\n", "\n", "return", "x_train", ",", "x_test", ",", "y_train", ",", "y_test", ",", "feature_cols", ",", "encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.sents_to_doc_buckets_mean": [[132, 136], ["[].apply", "hierarchical_sent_doc_experiment.series_of_list_to_df_columns", "series_of_list_to_df_columns.groupby"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.series_of_list_to_df_columns"], ["", "", "def", "sents_to_doc_buckets_mean", "(", "df", ")", ":", "\n", "    ", "series_of_list_of_bucket_avgs", "=", "df", ".", "groupby", "(", "'doc_id'", ")", "[", "'pred_num'", "]", ".", "apply", "(", "calc_avg_over_ten_parts", ")", "\n", "df", "=", "series_of_list_to_df_columns", "(", "series_of_list_of_bucket_avgs", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.mean_0_when_empty": [[138, 143], ["len", "sum", "len"], "function", ["None"], ["", "def", "mean_0_when_empty", "(", "p", ")", ":", "\n", "    ", "if", "len", "(", "p", ")", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "else", ":", "\n", "        ", "return", "sum", "(", "p", ")", "/", "len", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.calc_avg_over_equal_parts": [[145, 149], ["int", "numpy.ceil", "hierarchical_sent_doc_experiment.mean_0_when_empty", "range", "len"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.mean_0_when_empty"], ["", "", "def", "calc_avg_over_equal_parts", "(", "the_list", ",", "n_partitions", ")", ":", "\n", "    ", "n", "=", "int", "(", "np", ".", "ceil", "(", "len", "(", "the_list", ")", "/", "n_partitions", ")", ")", "\n", "partitions", "=", "[", "the_list", "[", "i", "*", "n", ":", "i", "*", "n", "+", "n", "]", "for", "i", "in", "range", "(", "n_partitions", ")", "]", "\n", "return", "[", "mean_0_when_empty", "(", "p", ")", "for", "p", "in", "partitions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.calc_avg_over_ten_parts": [[151, 153], ["hierarchical_sent_doc_experiment.calc_avg_over_equal_parts"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.calc_avg_over_equal_parts"], ["", "def", "calc_avg_over_ten_parts", "(", "the_list", ")", ":", "\n", "    ", "return", "calc_avg_over_equal_parts", "(", "the_list", ",", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.hierarchical_sent_doc_experiment.series_of_list_to_df_columns": [[155, 164], ["pandas.DataFrame", "enumerate", "pandas.concat", "pandas.DataFrame"], "function", ["None"], ["", "def", "series_of_list_to_df_columns", "(", "a_series_of_lists", ")", ":", "\n", "    ", "df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "ix", "in", "a_series_of_lists", ".", "index", ":", "\n", "        ", "d", "=", "{", "}", "\n", "#     d['doc_id'] = ix", "\n", "for", "col_i", ",", "value", "in", "enumerate", "(", "a_series_of_lists", "[", "ix", "]", ")", ":", "\n", "            ", "d", "[", "col_i", "]", "=", "value", "\n", "", "df", "=", "pd", ".", "concat", "(", "[", "df", ",", "pd", ".", "DataFrame", "(", "d", ",", "index", "=", "[", "ix", "]", ")", "]", ")", "\n", "", "return", "df", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.__init__": [[13, 18], ["autodiscern.PartitionedExperiment.__init__", "YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment._assign_vars"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.__init__", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment._assign_vars"], ["def", "__init__", "(", "self", ",", "name", ":", "str", ",", "data_dict", ":", "Dict", ",", "model", ",", "hyperparams", ":", "Dict", ",", "\n", "doc_level_info", ":", "Dict", ",", "n_partitions", ":", "int", "=", "5", ",", "stratified", "=", "True", ",", "\n", "verbose", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "name", ",", "data_dict", ",", "model", ",", "hyperparams", ",", "n_partitions", ",", "stratified", ",", "verbose", ")", "\n", "Sent2Doc_2ClassProb_Experiment", ".", "_assign_vars", "(", "doc_level_info", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment._assign_vars": [[19, 22], ["None"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_assign_vars", "(", "cls", ",", "doc_level_info", ")", ":", "\n", "        ", "cls", ".", "doc_level_info", "=", "doc_level_info", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.run_experiment_on_one_partition": [[23, 45], ["cls.materialize_partition", "autodiscern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun", "autodiscern.experiments.hierarchical_sent_doc_experiment.SentenceLevelModelRun.run", "print", "cls.create_sent_to_doc_data_set", "cls.create_sent_to_doc_data_set", "YA_Sent2Doc_2ClassProb.SentenceToDocProbaModelRun", "SentenceToDocProbaModelRun.run", "len"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.materialize_partition", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.create_sent_to_doc_data_set", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.create_sent_to_doc_data_set", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run"], ["", "@", "classmethod", "\n", "def", "run_experiment_on_one_partition", "(", "cls", ",", "data_dict", ":", "Dict", ",", "partition_ids", ":", "List", "[", "int", "]", ",", "model", ",", "hyperparams", ":", "Dict", ")", ":", "\n", "\n", "        ", "train_set", ",", "test_set", "=", "cls", ".", "materialize_partition", "(", "partition_ids", ",", "data_dict", ")", "\n", "\n", "# run SentenceLevelModel", "\n", "sl_mr", "=", "SentenceLevelModelRun", "(", "train_set", "=", "train_set", ",", "test_set", "=", "test_set", ",", "model", "=", "model", ",", "hyperparams", "=", "hyperparams", ",", "\n", "data_dict", "=", "data_dict", ",", "doc_level_info", "=", "cls", ".", "doc_level_info", ")", "\n", "\n", "sl_mr", ".", "run", "(", ")", "\n", "\n", "print", "(", "\"len(sl_mr.train_set_upd): \"", ",", "len", "(", "sl_mr", ".", "train_set_upd", ")", ")", "\n", "# use predictions from SentenceLevelModel to create training set for SentenceToDocModel", "\n", "data_set_train", "=", "cls", ".", "create_sent_to_doc_data_set", "(", "sl_mr", ".", "model", ",", "sl_mr", ".", "x_train", ",", "sl_mr", ".", "train_set_upd", ")", "\n", "data_set_test", "=", "cls", ".", "create_sent_to_doc_data_set", "(", "sl_mr", ".", "model", ",", "sl_mr", ".", "x_test", ",", "sl_mr", ".", "test_set_upd", ")", "\n", "\n", "dl_mr", "=", "SentenceToDocProbaModelRun", "(", "train_set", "=", "data_set_train", ",", "test_set", "=", "data_set_test", ",", "model", "=", "model", ",", "\n", "hyperparams", "=", "hyperparams", ")", "\n", "dl_mr", ".", "run", "(", ")", "\n", "\n", "return", "{", "'sentence_level'", ":", "sl_mr", ",", "\n", "'doc_level'", ":", "dl_mr", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.create_sent_to_doc_data_set": [[46, 59], ["model.predict", "model.predict_proba", "pandas.DataFrame"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict"], ["", "@", "classmethod", "\n", "def", "create_sent_to_doc_data_set", "(", "cls", ",", "model", ",", "x_feature_set", ",", "data_set", ":", "List", ")", ":", "\n", "        ", "cat_prediction", "=", "model", ".", "predict", "(", "x_feature_set", ")", "\n", "proba_prediction", "=", "model", ".", "predict_proba", "(", "x_feature_set", ")", "\n", "new_data_set", "=", "pd", ".", "DataFrame", "(", "{", "\n", "'doc_id'", ":", "[", "d", "[", "'entity_id'", "]", "for", "d", "in", "data_set", "]", ",", "\n", "'sub_id'", ":", "[", "d", "[", "'sub_id'", "]", "for", "d", "in", "data_set", "]", ",", "\n", "'sub_prediction'", ":", "cat_prediction", ",", "\n", "'proba_0'", ":", "[", "i", "[", "0", "]", "for", "i", "in", "proba_prediction", "]", ",", "\n", "'proba_1'", ":", "[", "i", "[", "1", "]", "for", "i", "in", "proba_prediction", "]", ",", "\n", "'label'", ":", "[", "d", "[", "'label'", "]", "for", "d", "in", "data_set", "]", ",", "\n", "}", ")", "\n", "return", "new_data_set", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_feature_importances": [[60, 69], ["pandas.DataFrame", "pandas.merge.median", "pandas.merge.sort_values", "[].get_feature_importances", "pandas.merge"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_feature_importances"], ["", "def", "show_feature_importances", "(", "self", ",", "level", ")", ":", "\n", "        ", "all_feature_importances", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "partition_id", "in", "self", ".", "model_runs", ":", "\n", "            ", "partition_feature_importances", "=", "self", ".", "model_runs", "[", "partition_id", "]", "[", "level", "]", ".", "get_feature_importances", "(", ")", "\n", "partition_feature_importances", ".", "columns", "=", "[", "'partition{}'", ".", "format", "(", "partition_id", ")", "]", "\n", "all_feature_importances", "=", "pd", ".", "merge", "(", "all_feature_importances", ",", "partition_feature_importances", ",", "how", "=", "'outer'", ",", "\n", "left_index", "=", "True", ",", "right_index", "=", "True", ")", "\n", "", "all_feature_importances", "[", "'median'", "]", "=", "all_feature_importances", ".", "median", "(", "axis", "=", "1", ")", "\n", "return", "all_feature_importances", ".", "sort_values", "(", "'median'", ",", "ascending", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.build_features": [[73, 87], ["cls.sents_to_doc_buckets_mean", "cls.sents_to_doc_buckets_mean", "[].median", "[].median", "train_set.groupby", "test_set.groupby"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.sents_to_doc_buckets_mean", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.sents_to_doc_buckets_mean"], ["    ", "@", "classmethod", "\n", "def", "build_features", "(", "cls", ",", "train_set", ":", "pd", ".", "DataFrame", ",", "test_set", ":", "pd", ".", "DataFrame", ")", "->", "Tuple", "[", "pd", ".", "DataFrame", ",", "pd", ".", "DataFrame", ",", "List", ",", "\n", "List", ",", "List", ",", "Dict", "]", ":", "\n", "\n", "        ", "x_train", "=", "cls", ".", "sents_to_doc_buckets_mean", "(", "train_set", ")", "\n", "x_test", "=", "cls", ".", "sents_to_doc_buckets_mean", "(", "test_set", ")", "\n", "\n", "y_train", "=", "train_set", ".", "groupby", "(", "'doc_id'", ")", "[", "'label'", "]", ".", "median", "(", ")", "\n", "y_test", "=", "test_set", ".", "groupby", "(", "'doc_id'", ")", "[", "'label'", "]", ".", "median", "(", ")", "\n", "\n", "feature_cols", "=", "x_train", ".", "columns", "\n", "encoders", "=", "{", "}", "\n", "\n", "return", "x_train", ",", "x_test", ",", "y_train", ",", "y_test", ",", "feature_cols", ",", "encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.sents_to_doc_buckets_mean": [[88, 93], ["[].apply", "YA_Sent2Doc_2ClassProb.series_of_list_to_df_columns", "series_of_list_to_df_columns.groupby"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.series_of_list_to_df_columns"], ["", "@", "classmethod", "\n", "def", "sents_to_doc_buckets_mean", "(", "cls", ",", "df", ")", ":", "\n", "        ", "series_of_list_of_bucket_avgs", "=", "df", ".", "groupby", "(", "'doc_id'", ")", "[", "'sub_prediction'", "]", ".", "apply", "(", "calc_avg_over_ten_parts", ")", "\n", "df", "=", "series_of_list_to_df_columns", "(", "series_of_list_of_bucket_avgs", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.mean_0_when_empty": [[95, 100], ["len", "sum", "len"], "function", ["None"], ["", "", "def", "mean_0_when_empty", "(", "p", ")", ":", "\n", "    ", "if", "len", "(", "p", ")", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "else", ":", "\n", "        ", "return", "sum", "(", "p", ")", "/", "len", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.calc_avg_over_equal_parts": [[102, 106], ["int", "numpy.ceil", "YA_Sent2Doc_2ClassProb.mean_0_when_empty", "range", "len"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.mean_0_when_empty"], ["", "", "def", "calc_avg_over_equal_parts", "(", "the_list", ",", "n_partitions", ")", ":", "\n", "    ", "n", "=", "int", "(", "np", ".", "ceil", "(", "len", "(", "the_list", ")", "/", "n_partitions", ")", ")", "\n", "partitions", "=", "[", "the_list", "[", "i", "*", "n", ":", "i", "*", "n", "+", "n", "]", "for", "i", "in", "range", "(", "n_partitions", ")", "]", "\n", "return", "[", "mean_0_when_empty", "(", "p", ")", "for", "p", "in", "partitions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.calc_avg_over_ten_parts": [[108, 110], ["YA_Sent2Doc_2ClassProb.calc_avg_over_equal_parts"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.calc_avg_over_equal_parts"], ["", "def", "calc_avg_over_ten_parts", "(", "the_list", ")", ":", "\n", "    ", "return", "calc_avg_over_equal_parts", "(", "the_list", ",", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.YA_Sent2Doc_2ClassProb.series_of_list_to_df_columns": [[112, 121], ["pandas.DataFrame", "enumerate", "pandas.concat", "pandas.DataFrame"], "function", ["None"], ["", "def", "series_of_list_to_df_columns", "(", "a_series_of_lists", ")", ":", "\n", "    ", "df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "ix", "in", "a_series_of_lists", ".", "index", ":", "\n", "        ", "d", "=", "{", "}", "\n", "#     d['doc_id'] = ix", "\n", "for", "col_i", ",", "value", "in", "enumerate", "(", "a_series_of_lists", "[", "ix", "]", ")", ":", "\n", "            ", "d", "[", "col_i", "]", "=", "value", "\n", "", "df", "=", "pd", ".", "concat", "(", "[", "df", ",", "pd", ".", "DataFrame", "(", "d", ",", "index", "=", "[", "ix", "]", ")", "]", ")", "\n", "", "return", "df", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.TwoLevelSentenceExperiment.run_experiment_on_one_partition": [[13, 35], ["cls.materialize_partition", "TwoLevelSentenceExperiment.SentenceLevelModelRun", "SentenceLevelModelRun.run", "cls.create_sent_to_doc_data_set", "cls.create_sent_to_doc_data_set", "TwoLevelSentenceExperiment.SentenceToDocModelRun", "SentenceToDocModelRun.run"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.materialize_partition", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.create_sent_to_doc_data_set", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.create_sent_to_doc_data_set", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run"], ["    ", "@", "classmethod", "\n", "def", "run_experiment_on_one_partition", "(", "cls", ",", "data_dict", ":", "Dict", ",", "label_key", ":", "str", ",", "partition_ids", ":", "List", "[", "int", "]", ",", "\n", "preprocessing_func", ":", "Callable", ",", "model_run_class", ":", "ade", ".", "ModelRun", ",", "model", ",", "\n", "hyperparams", ":", "Dict", ",", "run_hyperparam_search", ":", "bool", ")", ":", "\n", "\n", "        ", "train_set", ",", "test_set", "=", "cls", ".", "materialize_partition", "(", "partition_ids", ",", "data_dict", ")", "\n", "\n", "# run SentenceLevelModel", "\n", "sl_mr", "=", "SentenceLevelModelRun", "(", "train_set", "=", "train_set", ",", "test_set", "=", "test_set", ",", "label_key", "=", "label_key", ",", "model", "=", "model", ",", "\n", "preprocessing_func", "=", "preprocessing_func", ",", "hyperparams", "=", "hyperparams", ")", "\n", "sl_mr", ".", "run", "(", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "\n", "# use predictions from SentenceLevelModel to create training set for SentenceToDocModel", "\n", "data_set_train", "=", "cls", ".", "create_sent_to_doc_data_set", "(", "sl_mr", ".", "model", ",", "sl_mr", ".", "x_train", ",", "sl_mr", ".", "train_set", ")", "\n", "data_set_test", "=", "cls", ".", "create_sent_to_doc_data_set", "(", "sl_mr", ".", "model", ",", "sl_mr", ".", "x_test", ",", "sl_mr", ".", "test_set", ")", "\n", "\n", "dl_mr", "=", "SentenceToDocModelRun", "(", "train_set", "=", "data_set_train", ",", "test_set", "=", "data_set_test", ",", "label_key", "=", "label_key", ",", "\n", "model", "=", "model", ",", "preprocessing_func", "=", "preprocessing_func", ",", "hyperparams", "=", "hyperparams", ")", "\n", "dl_mr", ".", "run", "(", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "\n", "return", "{", "'sentence_level'", ":", "sl_mr", ",", "\n", "'doc_level'", ":", "dl_mr", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.TwoLevelSentenceExperiment.create_sent_to_doc_data_set": [[36, 50], ["autodiscern.model.predict", "autodiscern.model.predict_proba", "pandas.DataFrame"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict"], ["", "@", "classmethod", "\n", "def", "create_sent_to_doc_data_set", "(", "cls", ",", "model", ",", "x_feature_set", ",", "data_set", ":", "List", ")", ":", "\n", "        ", "cat_prediction", "=", "model", ".", "predict", "(", "x_feature_set", ")", "\n", "proba_prediction", "=", "model", ".", "predict_proba", "(", "x_feature_set", ")", "\n", "new_data_set", "=", "pd", ".", "DataFrame", "(", "{", "\n", "'doc_id'", ":", "[", "d", "[", "'entity_id'", "]", "for", "d", "in", "data_set", "]", ",", "\n", "'sub_id'", ":", "[", "d", "[", "'sub_id'", "]", "for", "d", "in", "data_set", "]", ",", "\n", "'sub_prediction'", ":", "cat_prediction", ",", "\n", "'proba_0'", ":", "[", "i", "[", "0", "]", "for", "i", "in", "proba_prediction", "]", ",", "\n", "'proba_1'", ":", "[", "i", "[", "1", "]", "for", "i", "in", "proba_prediction", "]", ",", "\n", "'proba_2'", ":", "[", "i", "[", "2", "]", "for", "i", "in", "proba_prediction", "]", ",", "\n", "'label'", ":", "[", "d", "[", "'label'", "]", "for", "d", "in", "data_set", "]", ",", "\n", "}", ")", "\n", "return", "new_data_set", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceLevelModelRun.train_encoders": [[54, 58], ["print"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "train_encoders", "(", "cls", ",", "train_set", ":", "List", "[", "Dict", "]", ")", ":", "\n", "        ", "print", "(", "\"`train_encoders` is not used in the `SentenceLevelModelRun` implementation\"", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceLevelModelRun.build_x_features": [[59, 63], ["print"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_x_features", "(", "cls", ",", "data_set", ":", "pd", ".", "DataFrame", ",", "encoders", ":", "Dict", ")", ":", "\n", "        ", "print", "(", "\"`build_x_features` is not used in the `SentenceLevelModelRun` implementation\"", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceLevelModelRun.build_y_vector": [[64, 77], ["autodiscern.model.zero_one_category", "autodiscern.model.get_score_for_question"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.zero_one_category", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_score_for_question"], ["", "@", "classmethod", "\n", "def", "build_y_vector", "(", "cls", ",", "data_set", ":", "List", "[", "Dict", "]", ",", "label_key", ":", "str", ")", "->", "List", ":", "\n", "        ", "\"\"\"\n        Extract the labels from each data dict and compile into one y vector.\n        Args:\n            data_set: List of data dicts.\n            label_key: The key int he data dicts under which the label is stored.\n\n        Returns:\n            Array-type\n        \"\"\"", "\n", "return", "[", "model", ".", "zero_one_category", "(", "model", ".", "get_score_for_question", "(", "entity_dict", ",", "label_key", ")", ")", "for", "entity_dict", "in", "\n", "data_set", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceLevelModelRun.build_data": [[78, 103], ["pandas.concat", "pandas.concat", "cls.build_y_vector", "cls.build_y_vector", "cls.build_tfidf_vectors_on_doc_level", "scipy.sparse.vstack", "scipy.sparse.vstack", "scipy.sparse.hstack", "scipy.sparse.hstack", "vectorizer.get_feature_names", "vectorizer.get_feature_names.extend", "scipy.sparse.coo_matrix", "scipy.sparse.coo_matrix"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.build_y_vector", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.build_y_vector", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceLevelModelRun.build_tfidf_vectors_on_doc_level"], ["", "@", "classmethod", "\n", "def", "build_data", "(", "cls", ",", "train_set", ":", "List", "[", "Dict", "]", ",", "test_set", ":", "List", "[", "Dict", "]", ",", "label_key", ":", "str", ")", "->", "Tuple", "[", "coo_matrix", ",", "coo_matrix", ",", "List", ",", "List", ",", "List", ",", "Dict", "]", ":", "\n", "# corpus_train = [entity_dict['content'] for entity_dict in train_set]", "\n", "# corpus_test = [entity_dict['content'] for entity_dict in test_set]", "\n", "\n", "        ", "feature_vec_train", "=", "pd", ".", "concat", "(", "[", "entity_dict", "[", "'feature_vec'", "]", "for", "entity_dict", "in", "train_set", "]", ",", "axis", "=", "0", ")", "\n", "feature_vec_test", "=", "pd", ".", "concat", "(", "[", "entity_dict", "[", "'feature_vec'", "]", "for", "entity_dict", "in", "test_set", "]", ",", "axis", "=", "0", ")", "\n", "\n", "y_train", "=", "cls", ".", "build_y_vector", "(", "train_set", ",", "label_key", ")", "\n", "y_test", "=", "cls", ".", "build_y_vector", "(", "test_set", ",", "label_key", ")", "\n", "\n", "train_tfidf_by_doc", ",", "test_tfidf_by_doc", ",", "vectorizer", "=", "cls", ".", "build_tfidf_vectors_on_doc_level", "(", "train_set", ",", "test_set", ")", "\n", "\n", "# build sentence level feature vec out of doc tfidf", "\n", "x_train_tfidf", "=", "vstack", "(", "[", "train_tfidf_by_doc", "[", "entity_dict", "[", "'entity_id'", "]", "]", "for", "entity_dict", "in", "train_set", "]", ")", "\n", "x_test_tfidf", "=", "vstack", "(", "[", "test_tfidf_by_doc", "[", "entity_dict", "[", "'entity_id'", "]", "]", "for", "entity_dict", "in", "test_set", "]", ")", "\n", "\n", "x_train", "=", "hstack", "(", "[", "x_train_tfidf", ",", "coo_matrix", "(", "feature_vec_train", ")", "]", ")", "\n", "x_test", "=", "hstack", "(", "[", "x_test_tfidf", ",", "coo_matrix", "(", "feature_vec_test", ")", "]", ")", "\n", "feature_cols", "=", "vectorizer", ".", "get_feature_names", "(", ")", "\n", "feature_cols", ".", "extend", "(", "feature_vec_train", ".", "columns", ")", "\n", "encoders", "=", "{", "'vectorizer'", ":", "vectorizer", "}", "\n", "\n", "return", "x_train", ",", "x_test", ",", "y_train", ",", "y_test", ",", "feature_cols", ",", "encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceLevelModelRun.build_tfidf_vectors_on_doc_level": [[104, 138], ["list", "list", "print", "sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "sklearn.feature_extraction.text.TfidfVectorizer.transform", "print", "range", "range", "set", "train_documents.append", "set", "test_documents.append", "len", "len", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "build_tfidf_vectors_on_doc_level", "(", "train_set", ":", "List", "[", "Dict", "]", ",", "test_set", ":", "List", "[", "Dict", "]", ")", "->", "Tuple", "[", "Dict", ",", "Dict", ",", "\n", "TfidfVectorizer", "]", ":", "\n", "        ", "\"\"\"Given a list of sentences, combine the sentences back to their document representation and train a\n        TF/IDF model. Return a dict of doc: TF'IDF vector. \"\"\"", "\n", "train_document_ids", "=", "list", "(", "set", "(", "[", "d", "[", "'entity_id'", "]", "for", "d", "in", "train_set", "]", ")", ")", "\n", "train_documents", "=", "[", "]", "\n", "for", "doc_id", "in", "train_document_ids", ":", "\n", "            ", "train_documents", ".", "append", "(", "\" \"", ".", "join", "(", "[", "d", "[", "'content'", "]", "for", "d", "in", "train_set", "if", "d", "[", "'entity_id'", "]", "==", "doc_id", "]", ")", ")", "\n", "\n", "", "test_document_ids", "=", "list", "(", "set", "(", "[", "d", "[", "'entity_id'", "]", "for", "d", "in", "test_set", "]", ")", ")", "\n", "test_documents", "=", "[", "]", "\n", "for", "doc_id", "in", "test_document_ids", ":", "\n", "            ", "test_documents", ".", "append", "(", "\" \"", ".", "join", "(", "[", "d", "[", "'content'", "]", "for", "d", "in", "test_set", "if", "d", "[", "'entity_id'", "]", "==", "doc_id", "]", ")", ")", "\n", "\n", "# print(\"Some example documents:\")", "\n", "# for i in train_documents[:2]:", "\n", "#     print(i)", "\n", "\n", "", "print", "(", "\"Training vectorizer on {} documents\"", ".", "format", "(", "len", "(", "train_documents", ")", ")", ")", "\n", "vectorizer", "=", "TfidfVectorizer", "(", "max_df", "=", "0.95", ",", "min_df", "=", "0.01", ",", "max_features", "=", "200", ",", "stop_words", "=", "'english'", ")", "\n", "train_documents_tfidf", "=", "vectorizer", ".", "fit_transform", "(", "train_documents", ")", "\n", "test_documents_tfidf", "=", "vectorizer", ".", "transform", "(", "test_documents", ")", "\n", "print", "(", "\"    Generated TF/IDF with {} columns\"", ".", "format", "(", "train_documents_tfidf", ".", "shape", "[", "1", "]", ")", ")", "\n", "\n", "train_tfidf", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "train_document_ids", ")", ")", ":", "\n", "            ", "train_tfidf", "[", "train_document_ids", "[", "i", "]", "]", "=", "train_documents_tfidf", "[", "i", "]", "\n", "\n", "", "test_tfidf", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "test_document_ids", ")", ")", ":", "\n", "            ", "test_tfidf", "[", "test_document_ids", "[", "i", "]", "]", "=", "test_documents_tfidf", "[", "i", "]", "\n", "\n", "", "return", "train_tfidf", ",", "test_tfidf", ",", "vectorizer", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceToDocModelRun.build_data": [[142, 178], ["numpy.where", "numpy.where", "numpy.where", "numpy.where", "TwoLevelSentenceExperiment.sents_to_doc_buckets_mean", "TwoLevelSentenceExperiment.sents_to_doc_buckets_mean", "[].median", "[].median", "numpy.where", "numpy.where", "numpy.where", "numpy.where", "train_set.groupby", "test_set.groupby"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.sents_to_doc_buckets_mean", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.sents_to_doc_buckets_mean"], ["    ", "@", "classmethod", "\n", "def", "build_data", "(", "cls", ",", "train_set", ":", "pd", ".", "DataFrame", ",", "test_set", ":", "pd", ".", "DataFrame", ",", "label_key", ":", "str", ")", "->", "Tuple", "[", "pd", ".", "DataFrame", ",", "pd", ".", "DataFrame", ",", "List", ",", "List", ",", "List", ",", "Dict", "]", ":", "\n", "# turn string labels into numbers", "\n", "        ", "train_set", "[", "'pred_num'", "]", "=", "np", ".", "where", "(", "train_set", "[", "'sub_prediction'", "]", "==", "'positive'", ",", "2", ",", "\n", "np", ".", "where", "(", "train_set", "[", "'sub_prediction'", "]", "==", "'neutral'", ",", "1", ",", "0", ")", ")", "\n", "train_set", "[", "'label_num'", "]", "=", "np", ".", "where", "(", "train_set", "[", "label_key", "]", "==", "'positive'", ",", "2", ",", "\n", "np", ".", "where", "(", "train_set", "[", "label_key", "]", "==", "'neutral'", ",", "1", ",", "0", ")", ")", "\n", "test_set", "[", "'pred_num'", "]", "=", "np", ".", "where", "(", "test_set", "[", "'sub_prediction'", "]", "==", "'positive'", ",", "2", ",", "\n", "np", ".", "where", "(", "test_set", "[", "'sub_prediction'", "]", "==", "'neutral'", ",", "1", ",", "0", ")", ")", "\n", "test_set", "[", "'label_num'", "]", "=", "np", ".", "where", "(", "test_set", "[", "label_key", "]", "==", "'positive'", ",", "2", ",", "\n", "np", ".", "where", "(", "test_set", "[", "'label'", "]", "==", "'neutral'", ",", "1", ",", "0", ")", ")", "\n", "\n", "# build df with col for each sentence, dim = max sentences in a doc in the training set", "\n", "# discarded because: gets really big, and lots of NAs if one long doc, and relative position in doc is lost", "\n", "# # compile results by document", "\n", "# x_train = pd.pivot_table(train_set, index='doc_id', columns='sub_id',  values='pred_num', aggfunc='median'", "\n", "#                          ).fillna(na_number)", "\n", "# x_test = pd.pivot_table(test_set, index='doc_id', columns='sub_id', values='pred_num', aggfunc='median'", "\n", "#                         ).fillna(na_number)", "\n", "# # make sure x_test has the same columns as x_train", "\n", "# cols_to_add_to_x_test = [col for col in x_train.columns if col not in x_test.columns]", "\n", "# for col in cols_to_add_to_x_test:", "\n", "#     x_test[col] = na_number", "\n", "# x_test = x_test[x_train.columns]", "\n", "\n", "x_train", "=", "sents_to_doc_buckets_mean", "(", "train_set", ")", "\n", "x_test", "=", "sents_to_doc_buckets_mean", "(", "test_set", ")", "\n", "\n", "y_train", "=", "train_set", ".", "groupby", "(", "'doc_id'", ")", "[", "'label_num'", "]", ".", "median", "(", ")", "\n", "y_test", "=", "test_set", ".", "groupby", "(", "'doc_id'", ")", "[", "'label_num'", "]", ".", "median", "(", ")", "\n", "\n", "feature_cols", "=", "x_train", ".", "columns", "\n", "encoders", "=", "{", "}", "\n", "\n", "return", "x_train", ",", "x_test", ",", "y_train", ",", "y_test", ",", "feature_cols", ",", "encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceToDocModelRun.train_encoders": [[179, 183], ["print"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "train_encoders", "(", "cls", ",", "train_set", ":", "List", "[", "Dict", "]", ")", ":", "\n", "        ", "print", "(", "\"`train_encoders` is not used in the `SentenceToDocModelRun` implementation\"", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.SentenceToDocModelRun.build_x_features": [[184, 188], ["print"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_x_features", "(", "cls", ",", "data_set", ":", "pd", ".", "DataFrame", ",", "encoders", ":", "Dict", ")", ":", "\n", "        ", "print", "(", "\"`build_x_features` is not used in the `SentenceToDocModelRun` implementation\"", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.sents_to_doc_buckets_mean": [[190, 194], ["[].apply", "TwoLevelSentenceExperiment.series_of_list_to_df_columns", "series_of_list_to_df_columns.groupby"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.series_of_list_to_df_columns"], ["", "", "def", "sents_to_doc_buckets_mean", "(", "df", ")", ":", "\n", "    ", "series_of_list_of_bucket_avgs", "=", "df", ".", "groupby", "(", "'doc_id'", ")", "[", "'pred_num'", "]", ".", "apply", "(", "calc_avg_over_ten_parts", ")", "\n", "df", "=", "series_of_list_to_df_columns", "(", "series_of_list_of_bucket_avgs", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.mean_0_when_empty": [[196, 201], ["len", "sum", "len"], "function", ["None"], ["", "def", "mean_0_when_empty", "(", "p", ")", ":", "\n", "    ", "if", "len", "(", "p", ")", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "else", ":", "\n", "        ", "return", "sum", "(", "p", ")", "/", "len", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.calc_avg_over_equal_parts": [[203, 207], ["int", "numpy.ceil", "TwoLevelSentenceExperiment.mean_0_when_empty", "range", "len"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.mean_0_when_empty"], ["", "", "def", "calc_avg_over_equal_parts", "(", "the_list", ",", "n_partitions", ")", ":", "\n", "    ", "n", "=", "int", "(", "np", ".", "ceil", "(", "len", "(", "the_list", ")", "/", "n_partitions", ")", ")", "\n", "partitions", "=", "[", "the_list", "[", "i", "*", "n", ":", "i", "*", "n", "+", "n", "]", "for", "i", "in", "range", "(", "n_partitions", ")", "]", "\n", "return", "[", "mean_0_when_empty", "(", "p", ")", "for", "p", "in", "partitions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.calc_avg_over_ten_parts": [[209, 211], ["TwoLevelSentenceExperiment.calc_avg_over_equal_parts"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.calc_avg_over_equal_parts"], ["", "def", "calc_avg_over_ten_parts", "(", "the_list", ")", ":", "\n", "    ", "return", "calc_avg_over_equal_parts", "(", "the_list", ",", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.TwoLevelSentenceExperiment.series_of_list_to_df_columns": [[213, 222], ["pandas.DataFrame", "enumerate", "pandas.concat", "pandas.DataFrame"], "function", ["None"], ["", "def", "series_of_list_to_df_columns", "(", "a_series_of_lists", ")", ":", "\n", "    ", "df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "ix", "in", "a_series_of_lists", ".", "index", ":", "\n", "        ", "d", "=", "{", "}", "\n", "#     d['doc_id'] = ix", "\n", "for", "col_i", ",", "value", "in", "enumerate", "(", "a_series_of_lists", "[", "ix", "]", ")", ":", "\n", "            ", "d", "[", "col_i", "]", "=", "value", "\n", "", "df", "=", "pd", ".", "concat", "(", "[", "df", ",", "pd", ".", "DataFrame", "(", "d", ",", "index", "=", "[", "ix", "]", ")", "]", ")", "\n", "", "return", "df", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.run_experiment_on_one_partition": [[12, 34], ["cls.materialize_partition", "autodiscern.experiments.TwoLevelSentenceExperiment.SentenceLevelModelRun", "autodiscern.experiments.TwoLevelSentenceExperiment.SentenceLevelModelRun.run", "cls.create_sent_to_doc_data_set", "cls.create_sent_to_doc_data_set", "Sent2Doc_2ClassProb.SentenceToDocProbaModelRun", "SentenceToDocProbaModelRun.run"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.materialize_partition", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.create_sent_to_doc_data_set", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.create_sent_to_doc_data_set", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run"], ["    ", "@", "classmethod", "\n", "def", "run_experiment_on_one_partition", "(", "cls", ",", "data_dict", ":", "Dict", ",", "label_key", ":", "str", ",", "partition_ids", ":", "List", "[", "int", "]", ",", "\n", "preprocessing_func", ":", "Callable", ",", "model_run_class", ":", "ade", ".", "ModelRun", ",", "model", ",", "\n", "hyperparams", ":", "Dict", ",", "run_hyperparam_search", ":", "bool", ")", ":", "\n", "\n", "        ", "train_set", ",", "test_set", "=", "cls", ".", "materialize_partition", "(", "partition_ids", ",", "data_dict", ")", "\n", "\n", "# run SentenceLevelModel", "\n", "sl_mr", "=", "SentenceLevelModelRun", "(", "train_set", "=", "train_set", ",", "test_set", "=", "test_set", ",", "label_key", "=", "label_key", ",", "model", "=", "model", ",", "\n", "preprocessing_func", "=", "preprocessing_func", ",", "hyperparams", "=", "hyperparams", ")", "\n", "sl_mr", ".", "run", "(", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "\n", "# use predictions from SentenceLevelModel to create training set for SentenceToDocModel", "\n", "doc_set_train", "=", "cls", ".", "create_sent_to_doc_data_set", "(", "sl_mr", ".", "model", ",", "sl_mr", ".", "x_train", ",", "sl_mr", ".", "train_set", ",", "label_key", ")", "\n", "doc_set_test", "=", "cls", ".", "create_sent_to_doc_data_set", "(", "sl_mr", ".", "model", ",", "sl_mr", ".", "x_test", ",", "sl_mr", ".", "test_set", ",", "label_key", ")", "\n", "\n", "dl_mr", "=", "SentenceToDocProbaModelRun", "(", "train_set", "=", "doc_set_train", ",", "test_set", "=", "doc_set_test", ",", "label_key", "=", "label_key", ",", "\n", "model", "=", "model", ",", "preprocessing_func", "=", "preprocessing_func", ",", "hyperparams", "=", "hyperparams", ")", "\n", "dl_mr", ".", "run", "(", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "\n", "return", "{", "'sentence_level'", ":", "sl_mr", ",", "\n", "'doc_level'", ":", "dl_mr", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.create_sent_to_doc_data_set": [[35, 48], ["autodiscern.model.predict", "autodiscern.model.predict_proba", "pandas.DataFrame", "autodiscern.model.zero_one_category", "autodiscern.model.get_score_for_question"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.predictor.Predictor.predict", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.zero_one_category", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_score_for_question"], ["", "@", "classmethod", "\n", "def", "create_sent_to_doc_data_set", "(", "cls", ",", "model", ",", "x_feature_set", ",", "data_set", ":", "List", ",", "label_key", ":", "str", ")", ":", "\n", "        ", "cat_prediction", "=", "model", ".", "predict", "(", "x_feature_set", ")", "\n", "proba_prediction", "=", "model", ".", "predict_proba", "(", "x_feature_set", ")", "\n", "new_data_set", "=", "pd", ".", "DataFrame", "(", "{", "\n", "'doc_id'", ":", "[", "d", "[", "'entity_id'", "]", "for", "d", "in", "data_set", "]", ",", "\n", "'sub_id'", ":", "[", "d", "[", "'sub_id'", "]", "for", "d", "in", "data_set", "]", ",", "\n", "'sub_prediction'", ":", "cat_prediction", ",", "\n", "'proba_0'", ":", "[", "i", "[", "0", "]", "for", "i", "in", "proba_prediction", "]", ",", "\n", "'proba_1'", ":", "[", "i", "[", "1", "]", "for", "i", "in", "proba_prediction", "]", ",", "\n", "'label'", ":", "[", "admodel", ".", "zero_one_category", "(", "admodel", ".", "get_score_for_question", "(", "d", ",", "label_key", ")", ")", "for", "d", "in", "data_set", "]", ",", "\n", "}", ")", "\n", "return", "new_data_set", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_feature_importances": [[49, 51], ["print"], "methods", ["None"], ["", "def", "show_feature_importances", "(", "self", ")", ":", "\n", "        ", "print", "(", "\"ERROR: this is not a valid function in `Sent2Doc_2ClassProb_Experiment`.\"", "\n", "\"Use `show_sent_feature_importances` or `show_doc_feature_importances`.\"", ")", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_sent_feature_importances": [[53, 62], ["pandas.DataFrame", "pandas.merge.median", "pandas.merge.sort_values", "[].get_feature_importances", "pandas.merge"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_feature_importances"], ["", "def", "show_sent_feature_importances", "(", "self", ")", ":", "\n", "        ", "all_feature_importances", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "partition_id", "in", "self", ".", "model_runs", ":", "\n", "            ", "partition_feature_importances", "=", "self", ".", "model_runs", "[", "partition_id", "]", "[", "'sentence_level'", "]", ".", "get_feature_importances", "(", ")", "\n", "partition_feature_importances", ".", "columns", "=", "[", "partition_id", "]", "\n", "all_feature_importances", "=", "pd", ".", "merge", "(", "all_feature_importances", ",", "partition_feature_importances", ",", "how", "=", "'outer'", ",", "\n", "left_index", "=", "True", ",", "right_index", "=", "True", ")", "\n", "", "all_feature_importances", "[", "'median'", "]", "=", "all_feature_importances", ".", "median", "(", "axis", "=", "1", ")", "\n", "return", "all_feature_importances", ".", "sort_values", "(", "'median'", ",", "ascending", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_doc_feature_importances": [[63, 72], ["pandas.DataFrame", "pandas.merge.median", "pandas.merge.sort_values", "[].get_feature_importances", "pandas.merge"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_feature_importances"], ["", "def", "show_doc_feature_importances", "(", "self", ")", ":", "\n", "        ", "all_feature_importances", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "partition_id", "in", "self", ".", "model_runs", ":", "\n", "            ", "partition_feature_importances", "=", "self", ".", "model_runs", "[", "partition_id", "]", "[", "'doc_level'", "]", ".", "get_feature_importances", "(", ")", "\n", "partition_feature_importances", ".", "columns", "=", "[", "partition_id", "]", "\n", "all_feature_importances", "=", "pd", ".", "merge", "(", "all_feature_importances", ",", "partition_feature_importances", ",", "how", "=", "'outer'", ",", "\n", "left_index", "=", "True", ",", "right_index", "=", "True", ")", "\n", "", "all_feature_importances", "[", "'median'", "]", "=", "all_feature_importances", ".", "median", "(", "axis", "=", "1", ")", "\n", "return", "all_feature_importances", ".", "sort_values", "(", "'median'", ",", "ascending", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.get_selected_hyperparams": [[73, 83], ["pandas.DataFrame", "[].get_selected_hyperparams", "pandas.concat"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.get_selected_hyperparams"], ["", "def", "get_selected_hyperparams", "(", "self", ",", "model_level", "=", "'sent'", ")", ":", "\n", "        ", "level_key", "=", "'sentence_level'", "\n", "if", "'doc'", "in", "model_level", ":", "\n", "            ", "level_key", "=", "'doc_level'", "\n", "\n", "", "all_hyperparams", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "partition_id", "in", "self", ".", "model_runs", ":", "\n", "            ", "partition_hyperparams", "=", "self", ".", "model_runs", "[", "partition_id", "]", "[", "level_key", "]", ".", "get_selected_hyperparams", "(", "partition_id", ")", "\n", "all_hyperparams", "=", "pd", ".", "concat", "(", "[", "all_hyperparams", ",", "partition_hyperparams", "]", ")", "\n", "", "return", "all_hyperparams", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_evaluation": [[84, 94], ["pandas.DataFrame", "pandas.DataFrame.median", "pandas.DataFrame.std", "pandas.DataFrame.sort_values"], "methods", ["None"], ["", "def", "show_evaluation", "(", "self", ",", "metric", ":", "str", "=", "'accuracy'", ")", ":", "\n", "        ", "all_accuracy", "=", "{", "}", "\n", "for", "partition_id", "in", "self", ".", "model_runs", ":", "\n", "            ", "all_accuracy", "[", "partition_id", "]", "=", "self", ".", "model_runs", "[", "partition_id", "]", "[", "'doc_level'", "]", ".", "evaluation", "[", "metric", "]", "\n", "", "all_accuracy_df", "=", "pd", ".", "DataFrame", "(", "all_accuracy", ",", "index", "=", "[", "self", ".", "name", "]", ")", "\n", "median", "=", "all_accuracy_df", ".", "median", "(", "axis", "=", "1", ")", "\n", "stddev", "=", "all_accuracy_df", ".", "std", "(", "axis", "=", "1", ")", "\n", "all_accuracy_df", "[", "'median'", "]", "=", "median", "\n", "all_accuracy_df", "[", "'stddev'", "]", "=", "stddev", "\n", "return", "all_accuracy_df", ".", "sort_values", "(", "'median'", ",", "ascending", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.generate_predictor": [[95, 112], ["Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.model_runs[].generate_predictor"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.generate_predictor"], ["", "def", "generate_predictor", "(", "self", ",", "partition", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Return a Predictor from the trained model of a specific partition.\n\n        Args:\n            partition: The partition id of the model to return. Defaults to 0.\n\n        Returns: a Predictor.\n\n        \"\"\"", "\n", "# TODO: THIS WILL NOT WORK AS IS!!", "\n", "# def make_prediction(input):", "\n", "#     sent_predictor = self.model_runs['sentence_level'].generate_predictor()", "\n", "#     doc_predictor = self.model_runs['doc_level'].generate_predictor()", "\n", "#     return doc_predictor(sent_predictor(input))", "\n", "\n", "return", "self", ".", "model_runs", "[", "partition", "]", ".", "generate_predictor", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.train_encoders": [[116, 119], ["None"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "train_encoders", "(", "cls", ",", "train_set", ":", "List", "[", "Dict", "]", ")", ":", "\n", "        ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.build_x_features": [[120, 125], ["cls.sents_to_doc_buckets_mean"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.sents_to_doc_buckets_mean"], ["", "@", "classmethod", "\n", "def", "build_x_features", "(", "cls", ",", "data_set", ":", "pd", ".", "DataFrame", ",", "encoders", ":", "Dict", ")", ":", "\n", "        ", "x", "=", "cls", ".", "sents_to_doc_buckets_mean", "(", "data_set", ")", "\n", "feature_cols", "=", "x", ".", "columns", "\n", "return", "x", ",", "feature_cols", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.build_y_vector": [[126, 129], ["[].median", "data_set.groupby"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "build_y_vector", "(", "cls", ",", "data_set", ":", "pd", ".", "DataFrame", ",", "label_key", ":", "str", ")", "->", "List", ":", "\n", "        ", "return", "data_set", ".", "groupby", "(", "'doc_id'", ")", "[", "'label'", "]", ".", "median", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.SentenceToDocProbaModelRun.sents_to_doc_buckets_mean": [[130, 135], ["[].apply", "Sent2Doc_2ClassProb.series_of_list_to_df_columns", "series_of_list_to_df_columns.groupby"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.series_of_list_to_df_columns"], ["", "@", "classmethod", "\n", "def", "sents_to_doc_buckets_mean", "(", "cls", ",", "df", ":", "pd", ".", "DataFrame", ")", ":", "\n", "        ", "series_of_list_of_bucket_avgs", "=", "df", ".", "groupby", "(", "'doc_id'", ")", "[", "'sub_prediction'", "]", ".", "apply", "(", "calc_avg_over_ten_parts", ")", "\n", "df", "=", "series_of_list_to_df_columns", "(", "series_of_list_of_bucket_avgs", ")", "\n", "return", "df", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.mean_0_when_empty": [[137, 142], ["len", "sum", "len"], "function", ["None"], ["", "", "def", "mean_0_when_empty", "(", "p", ")", ":", "\n", "    ", "if", "len", "(", "p", ")", "==", "0", ":", "\n", "        ", "return", "0", "\n", "", "else", ":", "\n", "        ", "return", "sum", "(", "p", ")", "/", "len", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.calc_avg_over_equal_parts": [[144, 148], ["int", "numpy.ceil", "Sent2Doc_2ClassProb.mean_0_when_empty", "range", "len"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.mean_0_when_empty"], ["", "", "def", "calc_avg_over_equal_parts", "(", "the_list", ",", "n_partitions", ")", ":", "\n", "    ", "n", "=", "int", "(", "np", ".", "ceil", "(", "len", "(", "the_list", ")", "/", "n_partitions", ")", ")", "\n", "partitions", "=", "[", "the_list", "[", "i", "*", "n", ":", "i", "*", "n", "+", "n", "]", "for", "i", "in", "range", "(", "n_partitions", ")", "]", "\n", "return", "[", "mean_0_when_empty", "(", "p", ")", "for", "p", "in", "partitions", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.calc_avg_over_ten_parts": [[150, 152], ["Sent2Doc_2ClassProb.calc_avg_over_equal_parts"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.calc_avg_over_equal_parts"], ["", "def", "calc_avg_over_ten_parts", "(", "the_list", ")", ":", "\n", "    ", "return", "calc_avg_over_equal_parts", "(", "the_list", ",", "10", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.series_of_list_to_df_columns": [[154, 163], ["pandas.DataFrame", "enumerate", "pandas.concat", "pandas.DataFrame"], "function", ["None"], ["", "def", "series_of_list_to_df_columns", "(", "a_series_of_lists", ")", ":", "\n", "    ", "df", "=", "pd", ".", "DataFrame", "(", ")", "\n", "for", "ix", "in", "a_series_of_lists", ".", "index", ":", "\n", "        ", "d", "=", "{", "}", "\n", "#     d['doc_id'] = ix", "\n", "for", "col_i", ",", "value", "in", "enumerate", "(", "a_series_of_lists", "[", "ix", "]", ")", ":", "\n", "            ", "d", "[", "col_i", "]", "=", "value", "\n", "", "df", "=", "pd", ".", "concat", "(", "[", "df", ",", "pd", ".", "DataFrame", "(", "d", ",", "index", "=", "[", "ix", "]", ")", "]", ")", "\n", "", "return", "df", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.train_encoders": [[25, 34], ["sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit"], "methods", ["None"], ["    ", "@", "classmethod", "\n", "def", "train_encoders", "(", "cls", ",", "train_set", ":", "List", "[", "Dict", "]", ")", ":", "\n", "        ", "corpus_train", "=", "[", "entity_dict", "[", "'content'", "]", "for", "entity_dict", "in", "train_set", "]", "\n", "vectorizer", "=", "TfidfVectorizer", "(", "max_df", "=", "0.9999", ",", "min_df", "=", "0.0001", ",", "max_features", "=", "200", ",", "stop_words", "=", "'english'", ")", "\n", "vectorizer", ".", "fit", "(", "corpus_train", ")", "\n", "encoders", "=", "{", "\n", "'vectorizer'", ":", "vectorizer", ",", "\n", "}", "\n", "return", "encoders", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.build_x_features": [[35, 50], ["autodiscern.lemmatization.Lemmatizer", "autodiscern.lemmatization.Lemmatizer.lemmatize_list_of_texts", "encoders[].transform", "pandas.concat", "scipy.sparse.hstack", "encoders[].get_feature_names", "encoders[].get_feature_names.extend", "scipy.sparse.coo_matrix"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Lemmatizer.lemmatize_list_of_texts"], ["", "@", "classmethod", "\n", "def", "build_x_features", "(", "cls", ",", "data_set", ":", "List", "[", "Dict", "]", ",", "encoders", ")", ":", "\n", "        ", "corpus", "=", "[", "entity_dict", "[", "'content'", "]", "for", "entity_dict", "in", "data_set", "]", "\n", "\n", "lemmatizer", "=", "lemmatization", ".", "Lemmatizer", "(", ")", "\n", "corpus_lemmas", "=", "lemmatizer", ".", "lemmatize_list_of_texts", "(", "corpus", ")", "\n", "\n", "x_tfidf", "=", "encoders", "[", "'vectorizer'", "]", ".", "transform", "(", "corpus_lemmas", ")", "\n", "feature_vec", "=", "pd", ".", "concat", "(", "[", "entity_dict", "[", "'feature_vec'", "]", "for", "entity_dict", "in", "data_set", "]", ",", "axis", "=", "0", ")", "\n", "x_all", "=", "hstack", "(", "[", "x_tfidf", ",", "coo_matrix", "(", "feature_vec", ")", "]", ")", "\n", "\n", "feature_cols", "=", "encoders", "[", "'vectorizer'", "]", ".", "get_feature_names", "(", ")", "\n", "feature_cols", ".", "extend", "(", "feature_vec", ".", "columns", ")", "\n", "\n", "return", "x_all", ",", "feature_cols", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.DocExperiment.DocLevelModelRun.build_y_vector": [[51, 64], ["autodiscern.model.zero_one_category", "autodiscern.model.get_score_for_question"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.zero_one_category", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.get_score_for_question"], ["", "@", "classmethod", "\n", "def", "build_y_vector", "(", "cls", ",", "data_set", ":", "List", "[", "Dict", "]", ",", "label_key", ":", "str", ")", "->", "List", ":", "\n", "        ", "\"\"\"\n        Extract the labels from each data dict and compile into one y vector.\n        Args:\n            data_set: List of data dicts.\n            label_key: The key int he data dicts under which the label is stored.\n\n        Returns:\n            Array-type\n        \"\"\"", "\n", "return", "[", "model", ".", "zero_one_category", "(", "model", ".", "get_score_for_question", "(", "entity_dict", ",", "label_key", ")", ")", "for", "entity_dict", "in", "\n", "data_set", "]", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.data_processors.sentence_no_ammend_data_processor.sentence_no_ammend_data_preprocessor": [[8, 38], ["transformations.Transformer", "transformations.Transformer.apply", "annotations.add_inline_citations_annotations", "annotations.ammed_content_replace_bad_punctuation_encoding", "annotations.add_metamap_annotations", "annotations.add_ner_annotations", "SentimentIntensityAnalyzer", "model.build_remaining_feature_vector"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_inline_citations_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.ammed_content_replace_bad_punctuation_encoding", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_metamap_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_ner_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.build_remaining_feature_vector"], ["def", "sentence_no_ammend_data_preprocessor", "(", "data_dict", ")", ":", "\n", "    ", "from", "nltk", ".", "sentiment", ".", "vader", "import", "SentimentIntensityAnalyzer", "\n", "from", "autodiscern", "import", "transformations", ",", "annotations", ",", "model", "\n", "\n", "html_to_sentence_transformer", "=", "transformations", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "\n", "html_to_plain_text", "=", "True", ",", "\n", "segment_into", "=", "'sentences'", ",", "\n", "flatten", "=", "True", ",", "\n", "remove_newlines", "=", "False", ",", "\n", "annotate_html", "=", "True", ",", "\n", "parallelism", "=", "False", ")", "\n", "transformed_data", "=", "html_to_sentence_transformer", ".", "apply", "(", "data_dict", ")", "\n", "\n", "# do links before punctuation removal because need punc for link id,", "\n", "# and before metamap because otherwise metamap will find references to medical terms in links", "\n", "transformed_data", "=", "annotations", ".", "add_inline_citations_annotations", "(", "transformed_data", ")", "\n", "\n", "# remove punctuation with bad encodings before metamap because it messes up metamap character indexing", "\n", "# (grr encoding!)", "\n", "transformed_data", "=", "annotations", ".", "ammed_content_replace_bad_punctuation_encoding", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "add_metamap_annotations", "(", "transformed_data", ")", "\n", "\n", "transformed_data", "=", "annotations", ".", "add_ner_annotations", "(", "transformed_data", ")", "\n", "\n", "sid", "=", "SentimentIntensityAnalyzer", "(", ")", "\n", "\n", "for", "key", "in", "transformed_data", ":", "\n", "        ", "transformed_data", "[", "key", "]", "[", "'feature_vec'", "]", "=", "model", ".", "build_remaining_feature_vector", "(", "transformed_data", "[", "key", "]", ",", "sid", ")", "\n", "\n", "", "return", "transformed_data", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.data_processors.doc_no_mm_data_processor.doc_no_mm_data_preprocessor": [[8, 37], ["transformations.Transformer", "transformations.Transformer.apply", "annotations.add_inline_citations_annotations", "annotations.amend_content_with_link_plain_text", "annotations.ammed_content_replace_bad_punctuation_encoding", "annotations.add_ner_annotations", "annotations.amend_content_with_ner_type_labels", "SentimentIntensityAnalyzer", "model.build_remaining_feature_vector"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_inline_citations_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_link_plain_text", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.ammed_content_replace_bad_punctuation_encoding", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_ner_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_ner_type_labels", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.build_remaining_feature_vector"], ["def", "doc_no_mm_data_preprocessor", "(", "data_dict", ")", ":", "\n", "    ", "from", "nltk", ".", "sentiment", ".", "vader", "import", "SentimentIntensityAnalyzer", "\n", "from", "autodiscern", "import", "transformations", ",", "annotations", ",", "model", "\n", "\n", "html_to_doc_transformer", "=", "transformations", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "\n", "html_to_plain_text", "=", "True", ",", "\n", "remove_newlines", "=", "True", ",", "\n", "annotate_html", "=", "True", ",", "\n", "parallelism", "=", "False", ")", "\n", "transformed_data", "=", "html_to_doc_transformer", ".", "apply", "(", "data_dict", ")", "\n", "\n", "# do links before punctuation removal because need punc for link id,", "\n", "# and before metamap because otherwise metamap will find references to medical terms in links", "\n", "transformed_data", "=", "annotations", ".", "add_inline_citations_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_link_plain_text", "(", "transformed_data", ")", "\n", "\n", "# remove punctuation with bad encodings before metamap because it messes up metamap character indexing", "\n", "# (grr encoding!)", "\n", "transformed_data", "=", "annotations", ".", "ammed_content_replace_bad_punctuation_encoding", "(", "transformed_data", ")", "\n", "\n", "transformed_data", "=", "annotations", ".", "add_ner_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_ner_type_labels", "(", "transformed_data", ")", "\n", "\n", "sid", "=", "SentimentIntensityAnalyzer", "(", ")", "\n", "\n", "for", "key", "in", "transformed_data", ":", "\n", "        ", "transformed_data", "[", "key", "]", "[", "'feature_vec'", "]", "=", "model", ".", "build_remaining_feature_vector", "(", "transformed_data", "[", "key", "]", ",", "sid", ")", "\n", "\n", "", "return", "transformed_data", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.data_processors.sentence_no_mm_data_processor.sentence_no_mm_data_preprocessor": [[8, 39], ["transformations.Transformer", "transformations.Transformer.apply", "annotations.add_inline_citations_annotations", "annotations.amend_content_with_link_plain_text", "annotations.ammed_content_replace_bad_punctuation_encoding", "annotations.add_ner_annotations", "annotations.amend_content_with_ner_type_labels", "SentimentIntensityAnalyzer", "model.build_remaining_feature_vector"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_inline_citations_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_link_plain_text", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.ammed_content_replace_bad_punctuation_encoding", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_ner_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_ner_type_labels", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.build_remaining_feature_vector"], ["def", "sentence_no_mm_data_preprocessor", "(", "data_dict", ")", ":", "\n", "    ", "from", "nltk", ".", "sentiment", ".", "vader", "import", "SentimentIntensityAnalyzer", "\n", "from", "autodiscern", "import", "transformations", ",", "annotations", ",", "model", "\n", "\n", "html_to_sentence_transformer", "=", "transformations", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "\n", "html_to_plain_text", "=", "True", ",", "\n", "segment_into", "=", "'sentences'", ",", "\n", "flatten", "=", "True", ",", "\n", "remove_newlines", "=", "False", ",", "\n", "annotate_html", "=", "True", ",", "\n", "parallelism", "=", "False", ")", "\n", "transformed_data", "=", "html_to_sentence_transformer", ".", "apply", "(", "data_dict", ")", "\n", "\n", "# do links before punctuation removal because need punc for link id,", "\n", "# and before metamap because otherwise metamap will find references to medical terms in links", "\n", "transformed_data", "=", "annotations", ".", "add_inline_citations_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_link_plain_text", "(", "transformed_data", ")", "\n", "\n", "# remove punctuation with bad encodings before metamap because it messes up metamap character indexing", "\n", "# (grr encoding!)", "\n", "transformed_data", "=", "annotations", ".", "ammed_content_replace_bad_punctuation_encoding", "(", "transformed_data", ")", "\n", "\n", "transformed_data", "=", "annotations", ".", "add_ner_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_ner_type_labels", "(", "transformed_data", ")", "\n", "\n", "sid", "=", "SentimentIntensityAnalyzer", "(", ")", "\n", "\n", "for", "key", "in", "transformed_data", ":", "\n", "        ", "transformed_data", "[", "key", "]", "[", "'feature_vec'", "]", "=", "model", ".", "build_remaining_feature_vector", "(", "transformed_data", "[", "key", "]", ",", "sid", ")", "\n", "\n", "", "return", "transformed_data", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.data_processors.sentence_data_processor.sentence_data_preprocessor": [[8, 43], ["transformations.Transformer", "transformations.Transformer.apply", "annotations.add_inline_citations_annotations", "annotations.amend_content_with_link_plain_text", "annotations.ammed_content_replace_bad_punctuation_encoding", "annotations.add_metamap_annotations", "annotations.amend_content_with_metamap_concepts", "annotations.add_ner_annotations", "annotations.amend_content_with_ner_type_labels", "SentimentIntensityAnalyzer", "model.build_remaining_feature_vector"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_inline_citations_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_link_plain_text", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.ammed_content_replace_bad_punctuation_encoding", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_metamap_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_metamap_concepts", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_ner_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_ner_type_labels", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.build_remaining_feature_vector"], ["def", "sentence_data_preprocessor", "(", "data_dict", ")", ":", "\n", "    ", "from", "nltk", ".", "sentiment", ".", "vader", "import", "SentimentIntensityAnalyzer", "\n", "from", "autodiscern", "import", "transformations", ",", "annotations", ",", "model", "\n", "\n", "html_to_sentence_transformer", "=", "transformations", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "\n", "html_to_plain_text", "=", "True", ",", "\n", "segment_into", "=", "'sentences'", ",", "\n", "flatten", "=", "True", ",", "\n", "remove_newlines", "=", "False", ",", "\n", "annotate_html", "=", "True", ",", "\n", "parallelism", "=", "False", ")", "\n", "transformed_data", "=", "html_to_sentence_transformer", ".", "apply", "(", "data_dict", ")", "\n", "\n", "# do links before punctuation removal because need punc for link id,", "\n", "# and before metamap because otherwise metamap will find references to medical terms in links", "\n", "transformed_data", "=", "annotations", ".", "add_inline_citations_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_link_plain_text", "(", "transformed_data", ")", "\n", "\n", "# remove punctuation with bad encodings before metamap because it messes up metamap character indexing", "\n", "# (grr encoding!)", "\n", "transformed_data", "=", "annotations", ".", "ammed_content_replace_bad_punctuation_encoding", "(", "transformed_data", ")", "\n", "\n", "# do metamap before ner, becauase otherwise ner will replace portions of medical terms", "\n", "transformed_data", "=", "annotations", ".", "add_metamap_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_metamap_concepts", "(", "transformed_data", ")", "\n", "\n", "transformed_data", "=", "annotations", ".", "add_ner_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_ner_type_labels", "(", "transformed_data", ")", "\n", "\n", "sid", "=", "SentimentIntensityAnalyzer", "(", ")", "\n", "\n", "for", "key", "in", "transformed_data", ":", "\n", "        ", "transformed_data", "[", "key", "]", "[", "'feature_vec'", "]", "=", "model", ".", "build_remaining_feature_vector", "(", "transformed_data", "[", "key", "]", ",", "sid", ")", "\n", "\n", "", "return", "transformed_data", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.data_processors.doc_data_processor.doc_data_preprocessor": [[8, 41], ["transformations.Transformer", "transformations.Transformer.apply", "annotations.add_inline_citations_annotations", "annotations.amend_content_with_link_plain_text", "annotations.ammed_content_replace_bad_punctuation_encoding", "annotations.add_metamap_annotations", "annotations.amend_content_with_metamap_concepts", "annotations.add_ner_annotations", "annotations.amend_content_with_ner_type_labels", "SentimentIntensityAnalyzer", "model.build_remaining_feature_vector"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_inline_citations_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_link_plain_text", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.ammed_content_replace_bad_punctuation_encoding", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_metamap_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_metamap_concepts", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.add_ner_annotations", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.amend_content_with_ner_type_labels", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.build_remaining_feature_vector"], ["def", "doc_data_preprocessor", "(", "data_dict", ")", ":", "\n", "    ", "from", "nltk", ".", "sentiment", ".", "vader", "import", "SentimentIntensityAnalyzer", "\n", "from", "autodiscern", "import", "transformations", ",", "annotations", ",", "model", "\n", "\n", "transformer", "=", "transformations", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "\n", "html_to_plain_text", "=", "True", ",", "\n", "remove_newlines", "=", "True", ",", "\n", "annotate_html", "=", "True", ",", "\n", "parallelism", "=", "False", ")", "\n", "transformed_data", "=", "transformer", ".", "apply", "(", "data_dict", ")", "\n", "\n", "# do links before punctuation removal because need punc for link id,", "\n", "# and before metamap because otherwise metamap will find references to medical terms in links", "\n", "transformed_data", "=", "annotations", ".", "add_inline_citations_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_link_plain_text", "(", "transformed_data", ")", "\n", "\n", "# remove punctuation with bad encodings before metamap because it messes up metamap character indexing", "\n", "# (grr encoding!)", "\n", "transformed_data", "=", "annotations", ".", "ammed_content_replace_bad_punctuation_encoding", "(", "transformed_data", ")", "\n", "\n", "# do metamap before ner, becauase otherwise ner will replace portions of medical terms", "\n", "transformed_data", "=", "annotations", ".", "add_metamap_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_metamap_concepts", "(", "transformed_data", ")", "\n", "\n", "transformed_data", "=", "annotations", ".", "add_ner_annotations", "(", "transformed_data", ")", "\n", "transformed_data", "=", "annotations", ".", "amend_content_with_ner_type_labels", "(", "transformed_data", ")", "\n", "\n", "sid", "=", "SentimentIntensityAnalyzer", "(", ")", "\n", "\n", "for", "key", "in", "transformed_data", ":", "\n", "        ", "transformed_data", "[", "key", "]", "[", "'feature_vec'", "]", "=", "model", ".", "build_remaining_feature_vector", "(", "transformed_data", "[", "key", "]", ",", "sid", ")", "\n", "\n", "", "return", "transformed_data", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.first_experiment.get_exp_id": [[17, 20], ["None"], "function", ["None"], ["@", "ex", ".", "capture", "\n", "def", "get_exp_id", "(", "_run", ")", ":", "\n", "    ", "return", "_run", ".", "_id", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.first_experiment.my_config": [[21, 68], ["None"], "function", ["None"], ["", "@", "ex", ".", "config", "\n", "def", "my_config", "(", ")", ":", "\n", "\n", "    ", "test_mode", "=", "False", "\n", "if", "test_mode", ":", "\n", "        ", "run_hyperparam_search", "=", "False", "\n", "num_partitions_to_run", "=", "1", "\n", "important_qs", "=", "[", "4", "]", "\n", "", "else", ":", "\n", "        ", "important_qs", "=", "[", "4", ",", "5", ",", "9", ",", "10", ",", "11", "]", "\n", "run_hyperparam_search", "=", "True", "\n", "num_partitions_to_run", "=", "None", "\n", "\n", "", "discern_path", "=", "\"~/switchdrive/Institution/discern\"", "\n", "cache_file", "=", "'2019-07-24_15-43-08_bd60a7f_with_mm_and_ner_ammendments.pkl'", "\n", "# cache_file = '2019-07-24_13-40-50_166c23e_test_mm_and_ner_ammendments_on_subset_30'", "\n", "\n", "n_estimators_default", "=", "500", "\n", "min_samples_leaf_default", "=", "5", "\n", "max_depth_default", "=", "50", "\n", "max_features_default", "=", "'sqrt'", "\n", "class_weight_default", "=", "'balanced_subsample'", "\n", "model_class", "=", "RandomForestClassifier", "\n", "n_estimators", "=", "[", "50", ",", "100", ",", "500", "]", "\n", "# Number of features to consider at every split", "\n", "max_features", "=", "[", "'sqrt'", "]", "\n", "# Maximum number of levels in tree", "\n", "max_depth", "=", "[", "10", ",", "50", ",", "100", "]", "\n", "# Minimum number of samples required to split a node", "\n", "min_samples_split", "=", "[", "5", ",", "50", ",", "500", "]", "\n", "# Minimum number of samples required at each leaf node", "\n", "min_samples_leaf", "=", "[", "3", ",", "5", ",", "10", ",", "100", "]", "\n", "# Method of selecting samples for training each tree", "\n", "# bootstrap = [True, False]", "\n", "# Create the random grid", "\n", "hyperparams", "=", "{", "'n_estimators'", ":", "n_estimators", ",", "\n", "'max_features'", ":", "max_features", ",", "\n", "'max_depth'", ":", "max_depth", ",", "\n", "'min_samples_split'", ":", "min_samples_split", ",", "\n", "'min_samples_leaf'", ":", "min_samples_leaf", ",", "\n", "# 'bootstrap': bootstrap,", "\n", "'class_weight'", ":", "[", "'balanced_subsample'", "]", ",", "\n", "}", "\n", "\n", "model_run_class", "=", "DocLevelModelRun", "\n", "n_partitions", "=", "5", "\n", "stratify_by", "=", "'label'", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.first_experiment.my_main": [[70, 137], ["autodiscern.DataManager", "autodiscern.DataManager.load_cached_data_processor", "model_class", "pandas.concat", "print", "pathlib.Path", "ex.add_artifact", "all_q_f1[].to_string", "print", "autodiscern.DataManager.save_experiment", "ex.add_artifact", "autodiscern.experiment.PartitionedExperiment", "question_models[].run", "open", "f.write", "print", "print", "pathlib.Path", "ex.add_artifact", "first_experiment.get_exp_id", "type", "question_models[].show_evaluation", "pd.concat.to_string", "question_models[].show_feature_importances().head", "open", "f.write", "round", "question_models[].show_feature_importances().head().to_string", "col_name.lower", "int", "ex.log_scalar", "ex.log_scalar", "question_models[].show_feature_importances", "int", "question_models[].show_feature_importances().head", "col_name.split", "q.split", "question_models[].show_feature_importances"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_cached_data_processor", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.save_experiment", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.doc_experiment.get_exp_id", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_evaluation", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_feature_importances", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_feature_importances"], ["", "@", "ex", ".", "automain", "\n", "def", "my_main", "(", "discern_path", ",", "cache_file", ",", "important_qs", ",", "model_class", ",", "hyperparams", ",", "model_run_class", ",", "n_partitions", ",", "\n", "stratify_by", ",", "run_hyperparam_search", ",", "num_partitions_to_run", ",", "test_mode", ")", ":", "\n", "\n", "# convert hyperparams from a sacred.config.ReadOnlyDict to a regular Dictionary,", "\n", "#   otherwise pickling of the experiment fails", "\n", "    ", "hyperparams_dict", "=", "{", "k", ":", "hyperparams", "[", "k", "]", "for", "k", "in", "hyperparams", "}", "\n", "\n", "dm", "=", "DataManager", "(", "discern_path", ")", "\n", "data_processor", "=", "dm", ".", "load_cached_data_processor", "(", "cache_file", ")", "\n", "\n", "# initialize an modeling experiment for each DISCERN question", "\n", "model_obj", "=", "model_class", "(", ")", "\n", "question_models", "=", "{", "}", "\n", "for", "q_no", "in", "important_qs", ":", "\n", "        ", "exp_name", "=", "\"q{}\"", ".", "format", "(", "q_no", ")", "\n", "question_models", "[", "exp_name", "]", "=", "PartitionedExperiment", "(", "exp_name", ",", "data_processor", ".", "data", ",", "label_key", "=", "q_no", ",", "\n", "model_run_class", "=", "model_run_class", ",", "\n", "model", "=", "model_obj", ",", "preprocessing_func", "=", "data_processor", ".", "func", ",", "\n", "hyperparams", "=", "hyperparams_dict", ",", "n_partitions", "=", "n_partitions", ",", "\n", "stratify_by", "=", "stratify_by", ",", "verbose", "=", "True", ")", "\n", "\n", "# run the experiments", "\n", "", "for", "q", "in", "question_models", ":", "\n", "        ", "question_models", "[", "q", "]", ".", "run", "(", "num_partitions_to_run", "=", "num_partitions_to_run", ",", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "\n", "# view F1 scores", "\n", "", "all_q_f1", "=", "pd", ".", "concat", "(", "[", "question_models", "[", "q", "]", ".", "show_evaluation", "(", "metric", "=", "'f1'", ")", "for", "q", "in", "question_models", "]", ",", "axis", "=", "0", ")", "\n", "print", "(", "all_q_f1", ")", "\n", "all_f1_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/f1.txt'", ")", "\n", "with", "open", "(", "all_f1_path", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "all_q_f1", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "all_f1_path", ")", "\n", "\n", "# try saving partition accuracies as metric entries?", "\n", "\n", "# try saving individual f1 scores as metrics", "\n", "for", "q", "in", "all_q_f1", ".", "index", ":", "\n", "        ", "for", "col_name", "in", "all_q_f1", ".", "columns", ":", "\n", "            ", "if", "col_name", "not", "in", "[", "'mean'", ",", "'median'", ",", "'stddev'", "]", ":", "\n", "                ", "value", "=", "round", "(", "all_q_f1", ".", "loc", "[", "q", ",", "col_name", "]", ",", "3", ")", "\n", "# if the experiment ran with stratified partitions, include the partition id", "\n", "# otherwise, just let whatever partitions be reported in increasing order", "\n", "#   (step is automatically assigned)", "\n", "if", "'partition '", "in", "col_name", ".", "lower", "(", ")", ":", "\n", "                    ", "partiton_number", "=", "int", "(", "col_name", ".", "split", "(", "' '", ")", "[", "1", "]", ")", "\n", "ex", ".", "log_scalar", "(", "\"{}_f1\"", ".", "format", "(", "q", ")", ",", "value", ",", "partiton_number", ")", "\n", "", "else", ":", "\n", "                    ", "ex", ".", "log_scalar", "(", "\"{}_f1\"", ".", "format", "(", "q", ")", ",", "value", ")", "\n", "\n", "# save the result", "\n", "", "", "", "", "ex", ".", "result", "=", "all_q_f1", "[", "'median'", "]", ".", "to_string", "(", ")", "\n", "\n", "# view feature importances", "\n", "for", "q", "in", "question_models", ":", "\n", "        ", "print", "(", "\"{}: {}\"", ".", "format", "(", "q", ",", "model", ".", "questions", "[", "int", "(", "q", ".", "split", "(", "'q'", ")", "[", "1", "]", ")", "]", ")", ")", "\n", "print", "(", "question_models", "[", "q", "]", ".", "show_feature_importances", "(", ")", ".", "head", "(", "10", ")", ")", "\n", "feature_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/feature_importances_{}.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "feature_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "show_feature_importances", "(", ")", ".", "head", "(", "20", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "feature_path", ")", "\n", "\n", "# save the models themselves for future inspection", "\n", "", "file_name", "=", "'{}.dill'", ".", "format", "(", "get_exp_id", "(", ")", ")", "\n", "print", "(", "type", "(", "question_models", ")", ")", "\n", "save_path", "=", "dm", ".", "save_experiment", "(", "question_models", ",", "file_name", "=", "file_name", ")", "\n", "ex", ".", "add_artifact", "(", "save_path", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.aa_compare_experiment.get_exp_id": [[18, 21], ["None"], "function", ["None"], ["@", "ex", ".", "capture", "\n", "def", "get_exp_id", "(", "_run", ")", ":", "\n", "    ", "return", "_run", ".", "_id", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.aa_compare_experiment.my_config": [[23, 70], ["None"], "function", ["None"], ["", "@", "ex", ".", "config", "\n", "def", "my_config", "(", ")", ":", "\n", "\n", "    ", "test_mode", "=", "False", "\n", "if", "test_mode", ":", "\n", "        ", "run_hyperparam_search", "=", "False", "\n", "num_partitions_to_run", "=", "1", "\n", "important_qs", "=", "[", "4", "]", "\n", "", "else", ":", "\n", "        ", "important_qs", "=", "[", "4", ",", "5", ",", "9", ",", "10", ",", "11", "]", "\n", "run_hyperparam_search", "=", "True", "\n", "num_partitions_to_run", "=", "None", "\n", "\n", "\n", "", "discern_path", "=", "\"~/switchdrive/Institution/discern\"", "\n", "cache_file", "=", "'2019-08-23_15-10-10_6b9bc88_2019-08-22_15-52-08_6b9bc88_sentence_no_ammend_no_len_1'", "\n", "\n", "n_estimators_default", "=", "500", "\n", "min_samples_leaf_default", "=", "1", "\n", "max_depth_default", "=", "100", "\n", "max_features_default", "=", "'sqrt'", "\n", "class_weight_default", "=", "'balanced_subsample'", "\n", "model_class", "=", "RandomForestClassifier", "\n", "n_estimators", "=", "[", "500", "]", "\n", "# Number of features to consider at every split", "\n", "max_features", "=", "[", "'sqrt'", "]", "\n", "# Maximum number of levels in tree", "\n", "max_depth", "=", "[", "100", "]", "\n", "# Minimum number of samples required to split a node", "\n", "min_samples_split", "=", "[", "50", "]", "\n", "# Minimum number of samples required at each leaf node", "\n", "min_samples_leaf", "=", "[", "1", "]", "\n", "# Method of selecting samples for training each tree", "\n", "# bootstrap = [True, False]", "\n", "# Create the random grid", "\n", "hyperparams", "=", "{", "'n_estimators'", ":", "n_estimators", ",", "\n", "'max_features'", ":", "max_features", ",", "\n", "'max_depth'", ":", "max_depth", ",", "\n", "'min_samples_split'", ":", "min_samples_split", ",", "\n", "'min_samples_leaf'", ":", "min_samples_leaf", ",", "\n", "# 'bootstrap': bootstrap,", "\n", "'class_weight'", ":", "[", "'balanced_subsample'", "]", ",", "\n", "}", "\n", "\n", "model_run_class", "=", "SentenceLevelModelRun", "\n", "n_partitions", "=", "5", "\n", "stratify_by", "=", "'label'", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.aa_compare_experiment.my_main": [[72, 149], ["autodiscern.DataManager", "autodiscern.DataManager.load_cached_data_processor", "model_class", "print", "autodiscern.DataManager.save_experiment", "ex.add_artifact", "autodiscern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment", "question_models[].run", "pandas.concat", "print", "pathlib.Path", "ex.add_artifact", "print", "print", "pathlib.Path", "ex.add_artifact", "print", "print", "pathlib.Path", "ex.add_artifact", "pathlib.Path", "ex.add_artifact", "aa_compare_experiment.get_exp_id", "type", "open", "f.write", "question_models[].show_sent_feature_importances().head", "open", "f.write", "question_models[].show_doc_feature_importances().head", "open", "f.write", "open", "f.write", "question_models[].show_evaluation", "pd.concat.to_string", "question_models[].show_sent_feature_importances().head().to_string", "question_models[].show_doc_feature_importances().head().to_string", "question_models[].get_selected_hyperparams().to_string", "round", "question_models[].show_sent_feature_importances", "question_models[].show_doc_feature_importances", "col_name.lower", "int", "ex.log_scalar", "ex.log_scalar", "int", "question_models[].show_sent_feature_importances().head", "int", "question_models[].show_doc_feature_importances().head", "question_models[].get_selected_hyperparams", "col_name.split", "q.split", "question_models[].show_sent_feature_importances", "q.split", "question_models[].show_doc_feature_importances"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_cached_data_processor", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.save_experiment", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.doc_experiment.get_exp_id", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_evaluation", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_sent_feature_importances", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_doc_feature_importances", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.get_selected_hyperparams", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_sent_feature_importances", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_doc_feature_importances"], ["", "@", "ex", ".", "automain", "\n", "def", "my_main", "(", "discern_path", ",", "cache_file", ",", "important_qs", ",", "model_class", ",", "hyperparams", ",", "model_run_class", ",", "n_partitions", ",", "\n", "stratify_by", ",", "run_hyperparam_search", ",", "num_partitions_to_run", ")", ":", "\n", "\n", "# convert hyperparams from a sacred.config.ReadOnlyDict to a regular Dictionary,", "\n", "#   otherwise pickling of the experiment fails", "\n", "    ", "hyperparams_dict", "=", "{", "k", ":", "hyperparams", "[", "k", "]", "for", "k", "in", "hyperparams", "}", "\n", "\n", "dm", "=", "DataManager", "(", "discern_path", ")", "\n", "data_processor", "=", "dm", ".", "load_cached_data_processor", "(", "cache_file", ")", "\n", "\n", "# initialize an modeling experiment for each DISCERN question", "\n", "model_obj", "=", "model_class", "(", ")", "\n", "question_models", "=", "{", "}", "\n", "for", "q_no", "in", "important_qs", ":", "\n", "        ", "exp_name", "=", "\"q{}\"", ".", "format", "(", "q_no", ")", "\n", "question_models", "[", "exp_name", "]", "=", "Sent2Doc_2ClassProb_Experiment", "(", "exp_name", ",", "data_processor", ".", "data", ",", "label_key", "=", "q_no", ",", "\n", "model_run_class", "=", "model_run_class", ",", "\n", "model", "=", "model_obj", ",", "\n", "preprocessing_func", "=", "data_processor", ".", "func", ",", "\n", "hyperparams", "=", "hyperparams_dict", ",", "\n", "n_partitions", "=", "n_partitions", ",", "\n", "stratify_by", "=", "stratify_by", ",", "verbose", "=", "True", ")", "\n", "\n", "# run the experiments", "\n", "", "for", "q", "in", "question_models", ":", "\n", "        ", "question_models", "[", "q", "]", ".", "run", "(", "num_partitions_to_run", "=", "num_partitions_to_run", ",", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "\n", "# save F1 and accuracy scores as artifacts", "\n", "", "for", "metric", "in", "[", "'f1'", ",", "'accuracy'", "]", ":", "\n", "        ", "all_q_metric", "=", "pd", ".", "concat", "(", "[", "question_models", "[", "q", "]", ".", "show_evaluation", "(", "metric", "=", "metric", ")", "for", "q", "in", "question_models", "]", ",", "axis", "=", "0", ")", "\n", "print", "(", "all_q_metric", ")", "\n", "all_f1_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}.txt'", ".", "format", "(", "metric", ")", ")", "\n", "with", "open", "(", "all_f1_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "all_q_metric", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "all_f1_path", ")", "\n", "\n", "# save partition accuracies as metric entries?", "\n", "for", "q", "in", "all_q_metric", ".", "index", ":", "\n", "            ", "for", "col_name", "in", "all_q_metric", ".", "columns", ":", "\n", "                ", "if", "col_name", "not", "in", "[", "'mean'", ",", "'median'", ",", "'stddev'", "]", ":", "\n", "                    ", "value", "=", "round", "(", "all_q_metric", ".", "loc", "[", "q", ",", "col_name", "]", ",", "3", ")", "\n", "# if the experiment ran with stratified partitions, include the partition id", "\n", "# otherwise, just let whatever partitions be reported in increasing order", "\n", "#   (step is automatically assigned)", "\n", "if", "'partition '", "in", "col_name", ".", "lower", "(", ")", ":", "\n", "                        ", "partiton_number", "=", "int", "(", "col_name", ".", "split", "(", "' '", ")", "[", "1", "]", ")", "\n", "ex", ".", "log_scalar", "(", "\"{}_{}\"", ".", "format", "(", "q", ",", "metric", ")", ",", "value", ",", "partiton_number", ")", "\n", "", "else", ":", "\n", "                        ", "ex", ".", "log_scalar", "(", "\"{}_{}\"", ".", "format", "(", "q", ",", "metric", ")", ",", "value", ")", "\n", "\n", "# save feature importances, hyperparams, and models as artifacts", "\n", "", "", "", "", "", "for", "q", "in", "question_models", ":", "\n", "        ", "print", "(", "\"{}: {}\"", ".", "format", "(", "q", ",", "model", ".", "questions", "[", "int", "(", "q", ".", "split", "(", "'q'", ")", "[", "1", "]", ")", "]", ")", ")", "\n", "print", "(", "question_models", "[", "q", "]", ".", "show_sent_feature_importances", "(", ")", ".", "head", "(", "10", ")", ")", "\n", "sent_feature_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/feature_importances_sent_{}.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "sent_feature_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "show_sent_feature_importances", "(", ")", ".", "head", "(", "20", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "sent_feature_path", ")", "\n", "\n", "print", "(", "\"{}: {}\"", ".", "format", "(", "q", ",", "model", ".", "questions", "[", "int", "(", "q", ".", "split", "(", "'q'", ")", "[", "1", "]", ")", "]", ")", ")", "\n", "print", "(", "question_models", "[", "q", "]", ".", "show_doc_feature_importances", "(", ")", ".", "head", "(", "10", ")", ")", "\n", "doc_feature_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/feature_importances_doc_{}.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "doc_feature_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "show_doc_feature_importances", "(", ")", ".", "head", "(", "20", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "doc_feature_path", ")", "\n", "\n", "sent_hyperparam_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/hyperparams_sent_{}.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "sent_hyperparam_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "get_selected_hyperparams", "(", "model_level", "=", "'sent'", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "sent_hyperparam_path", ")", "\n", "\n", "# save the models themselves for future inspection", "\n", "", "file_name", "=", "'{}.dill'", ".", "format", "(", "get_exp_id", "(", ")", ")", "\n", "print", "(", "type", "(", "question_models", ")", ")", "\n", "save_path", "=", "dm", ".", "save_experiment", "(", "question_models", ",", "file_name", "=", "file_name", ")", "\n", "ex", ".", "add_artifact", "(", "save_path", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.sentence_experiment.get_exp_id": [[18, 21], ["None"], "function", ["None"], ["@", "ex", ".", "capture", "\n", "def", "get_exp_id", "(", "_run", ")", ":", "\n", "    ", "return", "_run", ".", "_id", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.sentence_experiment.my_config": [[23, 70], ["None"], "function", ["None"], ["", "@", "ex", ".", "config", "\n", "def", "my_config", "(", ")", ":", "\n", "\n", "    ", "test_mode", "=", "False", "\n", "if", "test_mode", ":", "\n", "        ", "run_hyperparam_search", "=", "False", "\n", "num_partitions_to_run", "=", "1", "\n", "important_qs", "=", "[", "4", "]", "\n", "", "else", ":", "\n", "        ", "important_qs", "=", "[", "4", ",", "5", ",", "9", ",", "10", ",", "11", "]", "\n", "run_hyperparam_search", "=", "True", "\n", "num_partitions_to_run", "=", "None", "\n", "\n", "\n", "", "discern_path", "=", "\"~/switchdrive/Institution/discern\"", "\n", "cache_file", "=", "'2019-08-15_06-24-58_10d88c9_sentence_no_mm'", "\n", "\n", "n_estimators_default", "=", "500", "\n", "min_samples_leaf_default", "=", "5", "\n", "max_depth_default", "=", "50", "\n", "max_features_default", "=", "'sqrt'", "\n", "class_weight_default", "=", "'balanced_subsample'", "\n", "model_class", "=", "RandomForestClassifier", "\n", "n_estimators", "=", "[", "50", ",", "100", ",", "500", "]", "\n", "# Number of features to consider at every split", "\n", "max_features", "=", "[", "'sqrt'", "]", "\n", "# Maximum number of levels in tree", "\n", "max_depth", "=", "[", "10", ",", "50", ",", "100", "]", "\n", "# Minimum number of samples required to split a node", "\n", "min_samples_split", "=", "[", "5", ",", "50", ",", "500", "]", "\n", "# Minimum number of samples required at each leaf node", "\n", "min_samples_leaf", "=", "[", "3", ",", "5", ",", "10", ",", "100", "]", "\n", "# Method of selecting samples for training each tree", "\n", "# bootstrap = [True, False]", "\n", "# Create the random grid", "\n", "hyperparams", "=", "{", "'n_estimators'", ":", "n_estimators", ",", "\n", "'max_features'", ":", "max_features", ",", "\n", "'max_depth'", ":", "max_depth", ",", "\n", "'min_samples_split'", ":", "min_samples_split", ",", "\n", "'min_samples_leaf'", ":", "min_samples_leaf", ",", "\n", "# 'bootstrap': bootstrap,", "\n", "'class_weight'", ":", "[", "'balanced_subsample'", "]", ",", "\n", "}", "\n", "\n", "model_run_class", "=", "SentenceLevelModelRun", "\n", "n_partitions", "=", "5", "\n", "stratify_by", "=", "'label'", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.sentence_experiment.my_main": [[72, 149], ["autodiscern.DataManager", "autodiscern.DataManager.load_cached_data_processor", "model_class", "autodiscern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment", "question_models[].run", "pandas.concat", "print", "pathlib.Path", "ex.add_artifact", "print", "autodiscern.DataManager.save_experiment", "ex.add_artifact", "open", "f.write", "print", "print", "pathlib.Path", "ex.add_artifact", "print", "print", "pathlib.Path", "ex.add_artifact", "pathlib.Path", "ex.add_artifact", "sentence_experiment.get_exp_id", "type", "question_models[].show_evaluation", "pd.concat.to_string", "question_models[].show_sent_feature_importances().head", "open", "f.write", "question_models[].show_doc_feature_importances().head", "open", "f.write", "open", "f.write", "round", "question_models[].show_sent_feature_importances().head().to_string", "question_models[].show_doc_feature_importances().head().to_string", "question_models[].get_selected_hyperparams().to_string", "col_name.lower", "int", "ex.log_scalar", "ex.log_scalar", "question_models[].show_sent_feature_importances", "question_models[].show_doc_feature_importances", "int", "question_models[].show_sent_feature_importances().head", "int", "question_models[].show_doc_feature_importances().head", "question_models[].get_selected_hyperparams", "col_name.split", "q.split", "question_models[].show_sent_feature_importances", "q.split", "question_models[].show_doc_feature_importances"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_cached_data_processor", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.save_experiment", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.doc_experiment.get_exp_id", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_evaluation", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_sent_feature_importances", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_doc_feature_importances", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.get_selected_hyperparams", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_sent_feature_importances", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_doc_feature_importances"], ["", "@", "ex", ".", "automain", "\n", "def", "my_main", "(", "discern_path", ",", "cache_file", ",", "important_qs", ",", "model_class", ",", "hyperparams", ",", "model_run_class", ",", "n_partitions", ",", "\n", "stratify_by", ",", "run_hyperparam_search", ",", "num_partitions_to_run", ")", ":", "\n", "\n", "# convert hyperparams from a sacred.config.ReadOnlyDict to a regular Dictionary,", "\n", "#   otherwise pickling of the experiment fails", "\n", "    ", "hyperparams_dict", "=", "{", "k", ":", "hyperparams", "[", "k", "]", "for", "k", "in", "hyperparams", "}", "\n", "\n", "dm", "=", "DataManager", "(", "discern_path", ")", "\n", "data_processor", "=", "dm", ".", "load_cached_data_processor", "(", "cache_file", ")", "\n", "\n", "# initialize an modeling experiment for each DISCERN question", "\n", "model_obj", "=", "model_class", "(", ")", "\n", "question_models", "=", "{", "}", "\n", "for", "q_no", "in", "important_qs", ":", "\n", "        ", "exp_name", "=", "\"q{}\"", ".", "format", "(", "q_no", ")", "\n", "question_models", "[", "exp_name", "]", "=", "Sent2Doc_2ClassProb_Experiment", "(", "exp_name", ",", "data_processor", ".", "data", ",", "label_key", "=", "q_no", ",", "\n", "model_run_class", "=", "model_run_class", ",", "\n", "model", "=", "model_obj", ",", "\n", "preprocessing_func", "=", "data_processor", ".", "func", ",", "\n", "hyperparams", "=", "hyperparams_dict", ",", "\n", "n_partitions", "=", "n_partitions", ",", "\n", "stratify_by", "=", "stratify_by", ",", "verbose", "=", "True", ")", "\n", "\n", "# run the experiments", "\n", "", "for", "q", "in", "question_models", ":", "\n", "        ", "question_models", "[", "q", "]", ".", "run", "(", "num_partitions_to_run", "=", "num_partitions_to_run", ",", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "\n", "# save F1 and accuracy scores as artifacts", "\n", "", "for", "metric", "in", "[", "'f1'", ",", "'accuracy'", "]", ":", "\n", "        ", "all_q_metric", "=", "pd", ".", "concat", "(", "[", "question_models", "[", "q", "]", ".", "show_evaluation", "(", "metric", "=", "metric", ")", "for", "q", "in", "question_models", "]", ",", "axis", "=", "0", ")", "\n", "print", "(", "all_q_metric", ")", "\n", "all_f1_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}.txt'", ".", "format", "(", "metric", ")", ")", "\n", "with", "open", "(", "all_f1_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "all_q_metric", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "all_f1_path", ")", "\n", "\n", "# save partition accuracies as metric entries?", "\n", "for", "q", "in", "all_q_metric", ".", "index", ":", "\n", "            ", "for", "col_name", "in", "all_q_metric", ".", "columns", ":", "\n", "                ", "if", "col_name", "not", "in", "[", "'mean'", ",", "'median'", ",", "'stddev'", "]", ":", "\n", "                    ", "value", "=", "round", "(", "all_q_metric", ".", "loc", "[", "q", ",", "col_name", "]", ",", "3", ")", "\n", "# if the experiment ran with stratified partitions, include the partition id", "\n", "# otherwise, just let whatever partitions be reported in increasing order", "\n", "#   (step is automatically assigned)", "\n", "if", "'partition '", "in", "col_name", ".", "lower", "(", ")", ":", "\n", "                        ", "partiton_number", "=", "int", "(", "col_name", ".", "split", "(", "' '", ")", "[", "1", "]", ")", "\n", "ex", ".", "log_scalar", "(", "\"{}_{}\"", ".", "format", "(", "q", ",", "metric", ")", ",", "value", ",", "partiton_number", ")", "\n", "", "else", ":", "\n", "                        ", "ex", ".", "log_scalar", "(", "\"{}_{}\"", ".", "format", "(", "q", ",", "metric", ")", ",", "value", ")", "\n", "\n", "# save feature importances, hyperparams, and models as artifacts", "\n", "", "", "", "", "for", "q", "in", "question_models", ":", "\n", "            ", "print", "(", "\"{}: {}\"", ".", "format", "(", "q", ",", "model", ".", "questions", "[", "int", "(", "q", ".", "split", "(", "'q'", ")", "[", "1", "]", ")", "]", ")", ")", "\n", "print", "(", "question_models", "[", "q", "]", ".", "show_sent_feature_importances", "(", ")", ".", "head", "(", "10", ")", ")", "\n", "sent_feature_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/feature_importances_sent_{}.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "sent_feature_path", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "show_sent_feature_importances", "(", ")", ".", "head", "(", "20", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "sent_feature_path", ")", "\n", "\n", "print", "(", "\"{}: {}\"", ".", "format", "(", "q", ",", "model", ".", "questions", "[", "int", "(", "q", ".", "split", "(", "'q'", ")", "[", "1", "]", ")", "]", ")", ")", "\n", "print", "(", "question_models", "[", "q", "]", ".", "show_doc_feature_importances", "(", ")", ".", "head", "(", "10", ")", ")", "\n", "doc_feature_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/feature_importances_doc_{}.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "doc_feature_path", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "show_doc_feature_importances", "(", ")", ".", "head", "(", "20", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "doc_feature_path", ")", "\n", "\n", "sent_hyperparam_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/hyperparams_sent_{}.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "sent_hyperparam_path", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "get_selected_hyperparams", "(", "model_level", "=", "'sent'", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "sent_hyperparam_path", ")", "\n", "\n", "# save the models themselves for future inspection", "\n", "", "file_name", "=", "'{}.dill'", ".", "format", "(", "get_exp_id", "(", ")", ")", "\n", "print", "(", "type", "(", "question_models", ")", ")", "\n", "save_path", "=", "dm", ".", "save_experiment", "(", "question_models", ",", "file_name", "=", "file_name", ")", "\n", "ex", ".", "add_artifact", "(", "save_path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.svm_doc_experiment.get_exp_id": [[19, 22], ["None"], "function", ["None"], ["@", "ex", ".", "capture", "\n", "def", "get_exp_id", "(", "_run", ")", ":", "\n", "    ", "return", "_run", ".", "_id", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.svm_doc_experiment.my_config": [[24, 55], ["None"], "function", ["None"], ["", "@", "ex", ".", "config", "\n", "def", "my_config", "(", ")", ":", "\n", "\n", "    ", "test_mode", "=", "False", "\n", "if", "test_mode", ":", "\n", "        ", "run_hyperparam_search", "=", "False", "\n", "num_partitions_to_run", "=", "1", "\n", "important_qs", "=", "[", "4", "]", "\n", "", "else", ":", "\n", "        ", "important_qs", "=", "[", "4", ",", "5", ",", "9", ",", "10", ",", "11", "]", "\n", "run_hyperparam_search", "=", "True", "\n", "num_partitions_to_run", "=", "None", "\n", "\n", "", "discern_path", "=", "\"~/switchdrive/Institution/discern\"", "\n", "cache_file", "=", "'2019-08-21_18-56-53_fae71bc_doc.pkl'", "\n", "# cache_file = '2019-07-24_15-43-08_bd60a7f_with_mm_and_ner_ammendments.pkl'", "\n", "# cache_file = '2019-07-24_13-40-50_166c23e_test_mm_and_ner_ammendments_on_subset_30'", "\n", "\n", "model_class", "=", "SVC", "\n", "\n", "hyperparams", "=", "{", "\n", "'C'", ":", "[", "0.000001", ",", "0.00001", ",", "0.0001", ",", "0.01", ",", "1", "]", ",", "\n", "'kernel'", ":", "[", "'rbf'", ",", "'poly'", ",", "'sigmoid'", "]", ",", "\n", "'gamma'", ":", "[", "'scale'", ",", "0.1", ",", "0.01", ",", "0.001", ",", "0.0001", "]", ",", "\n", "'class_weight'", ":", "[", "'balanced'", "]", ",", "\n", "}", "\n", "\n", "model_run_class", "=", "DocLevelModelRun", "\n", "n_partitions", "=", "5", "\n", "stratify_by", "=", "'label'", "\n", "feature_subset", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.svm_doc_experiment.my_main": [[57, 157], ["autodiscern.DataManager", "autodiscern.DataManager.load_cached_data_processor", "model_class", "print", "autodiscern.DataManager.save_experiment", "ex.add_artifact", "autodiscern.experiment.PartitionedExperiment", "question_models[].run", "pathlib.Path", "ex.add_artifact", "pathlib.Path", "autodiscern.model.plot_confusion_matrix", "model.plot_confusion_matrix.get_figure", "plot.get_figure.savefig", "ex.add_artifact", "pathlib.Path", "autodiscern.model.plot_confusion_matrix", "model.plot_confusion_matrix.get_figure", "plot.get_figure.savefig", "ex.add_artifact", "pandas.concat", "print", "pathlib.Path", "ex.add_artifact", "svm_doc_experiment.get_exp_id", "type", "open", "f.write", "y_true.extend", "y_pred.extend", "y_true.extend", "y_pred.extend", "open", "f.write", "question_models[].get_selected_hyperparams().to_string", "question_models[].show_evaluation", "pd.concat.to_string", "round", "question_models[].get_selected_hyperparams", "col_name.lower", "int", "ex.log_scalar", "ex.log_scalar", "col_name.split"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_cached_data_processor", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.save_experiment", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.plot_confusion_matrix", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.plot_confusion_matrix", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.doc_experiment.get_exp_id", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_evaluation", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.get_selected_hyperparams", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "@", "ex", ".", "automain", "\n", "def", "my_main", "(", "discern_path", ",", "cache_file", ",", "important_qs", ",", "model_class", ",", "hyperparams", ",", "model_run_class", ",", "n_partitions", ",", "\n", "stratify_by", ",", "run_hyperparam_search", ",", "num_partitions_to_run", ",", "feature_subset", ",", "test_mode", ")", ":", "\n", "\n", "# convert hyperparams from a sacred.config.ReadOnlyDict to a regular Dictionary,", "\n", "#   otherwise pickling of the experiment fails", "\n", "    ", "hyperparams_dict", "=", "{", "k", ":", "hyperparams", "[", "k", "]", "for", "k", "in", "hyperparams", "}", "\n", "\n", "dm", "=", "DataManager", "(", "discern_path", ")", "\n", "data_processor", "=", "dm", ".", "load_cached_data_processor", "(", "cache_file", ")", "\n", "\n", "# initialize an modeling experiment for each DISCERN question", "\n", "model_obj", "=", "model_class", "(", ")", "\n", "question_models", "=", "{", "}", "\n", "for", "q_no", "in", "important_qs", ":", "\n", "        ", "exp_name", "=", "\"q{}\"", ".", "format", "(", "q_no", ")", "\n", "question_models", "[", "exp_name", "]", "=", "PartitionedExperiment", "(", "exp_name", ",", "data_processor", ".", "data", ",", "label_key", "=", "q_no", ",", "\n", "model_run_class", "=", "model_run_class", ",", "\n", "model", "=", "model_obj", ",", "preprocessing_func", "=", "data_processor", ".", "func", ",", "\n", "hyperparams", "=", "hyperparams_dict", ",", "n_partitions", "=", "n_partitions", ",", "\n", "stratify_by", "=", "stratify_by", ",", "feature_subset", "=", "feature_subset", ",", "\n", "verbose", "=", "True", ")", "\n", "\n", "# run the experiments", "\n", "", "for", "q", "in", "question_models", ":", "\n", "        ", "question_models", "[", "q", "]", ".", "run", "(", "num_partitions_to_run", "=", "num_partitions_to_run", ",", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "\n", "# SVM does not have feature importances!", "\n", "# save feature importances as artifacts", "\n", "# print(\"{}: {}\".format(q, model.questions[int(q.split('q')[1])]))", "\n", "# print(question_models[q].show_feature_importances().head(10))", "\n", "# feature_path = Path(dm.data_path, 'results/{}_feature_importances.txt'.format(q))", "\n", "# with open(feature_path, 'w') as f:", "\n", "#     f.write(question_models[q].show_feature_importances().head(20).to_string())", "\n", "# ex.add_artifact(feature_path)", "\n", "\n", "# save hyperparams", "\n", "sent_hyperparam_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}_hyperparams.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "sent_hyperparam_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "get_selected_hyperparams", "(", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "sent_hyperparam_path", ")", "\n", "\n", "# save confusion matrix, test set", "\n", "y_true", "=", "[", "]", "\n", "y_pred", "=", "[", "]", "\n", "for", "p", "in", "question_models", "[", "q", "]", ".", "model_runs", ":", "\n", "            ", "y_true", ".", "extend", "(", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "y_test", ")", "\n", "y_pred", ".", "extend", "(", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "y_test_predicted", ")", "\n", "", "classes", "=", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "model", ".", "classes_", "\n", "\n", "conf_matrix_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}_confusion_matrix_test.png'", ".", "format", "(", "q", ")", ")", "\n", "title", "=", "\"{} Test Set\"", ".", "format", "(", "q", ")", "\n", "plot", "=", "model", ".", "plot_confusion_matrix", "(", "y_true", ",", "y_pred", ",", "classes", "=", "classes", ",", "normalize", "=", "True", ",", "title", "=", "title", ")", "\n", "fig", "=", "plot", ".", "get_figure", "(", ")", "\n", "fig", ".", "savefig", "(", "conf_matrix_path", ")", "\n", "ex", ".", "add_artifact", "(", "conf_matrix_path", ")", "\n", "\n", "# save confusion matrix, train set", "\n", "y_true", "=", "[", "]", "\n", "y_pred", "=", "[", "]", "\n", "for", "p", "in", "question_models", "[", "q", "]", ".", "model_runs", ":", "\n", "            ", "y_true", ".", "extend", "(", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "y_train", ")", "\n", "y_pred", ".", "extend", "(", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "y_train_predicted", ")", "\n", "", "classes", "=", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "model", ".", "classes_", "\n", "\n", "conf_matrix_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}_confusion_matrix_train.png'", ".", "format", "(", "q", ")", ")", "\n", "title", "=", "\"{} Train Set\"", ".", "format", "(", "q", ")", "\n", "plot", "=", "model", ".", "plot_confusion_matrix", "(", "y_true", ",", "y_pred", ",", "classes", "=", "classes", ",", "normalize", "=", "True", ",", "title", "=", "title", ")", "\n", "fig", "=", "plot", ".", "get_figure", "(", ")", "\n", "fig", ".", "savefig", "(", "conf_matrix_path", ")", "\n", "ex", ".", "add_artifact", "(", "conf_matrix_path", ")", "\n", "\n", "# save F1 and accuracy scores as artifacts", "\n", "", "for", "metric", "in", "[", "'f1'", ",", "'accuracy'", "]", ":", "\n", "        ", "all_q_metric", "=", "pd", ".", "concat", "(", "[", "question_models", "[", "q", "]", ".", "show_evaluation", "(", "metric", "=", "metric", ")", "for", "q", "in", "question_models", "]", ",", "axis", "=", "0", ")", "\n", "print", "(", "all_q_metric", ")", "\n", "all_f1_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}.txt'", ".", "format", "(", "metric", ")", ")", "\n", "with", "open", "(", "all_f1_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "all_q_metric", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "all_f1_path", ")", "\n", "\n", "# save partition accuracies as metric entries?", "\n", "for", "q", "in", "all_q_metric", ".", "index", ":", "\n", "            ", "for", "col_name", "in", "all_q_metric", ".", "columns", ":", "\n", "                ", "if", "col_name", "not", "in", "[", "'mean'", ",", "'median'", ",", "'stddev'", "]", ":", "\n", "                    ", "value", "=", "round", "(", "all_q_metric", ".", "loc", "[", "q", ",", "col_name", "]", ",", "3", ")", "\n", "# if the experiment ran with stratified partitions, include the partition id", "\n", "# otherwise, just let whatever partitions be reported in increasing order", "\n", "#   (step is automatically assigned)", "\n", "if", "'partition '", "in", "col_name", ".", "lower", "(", ")", ":", "\n", "                        ", "partiton_number", "=", "int", "(", "col_name", ".", "split", "(", "' '", ")", "[", "1", "]", ")", "\n", "ex", ".", "log_scalar", "(", "\"{}_{}\"", ".", "format", "(", "q", ",", "metric", ")", ",", "value", ",", "partiton_number", ")", "\n", "", "else", ":", "\n", "                        ", "ex", ".", "log_scalar", "(", "\"{}_{}\"", ".", "format", "(", "q", ",", "metric", ")", ",", "value", ")", "\n", "\n", "# save the models themselves for future inspection", "\n", "", "", "", "", "", "file_name", "=", "'{}.dill'", ".", "format", "(", "get_exp_id", "(", ")", ")", "\n", "print", "(", "type", "(", "question_models", ")", ")", "\n", "save_path", "=", "dm", ".", "save_experiment", "(", "question_models", ",", "file_name", "=", "file_name", ")", "\n", "ex", ".", "add_artifact", "(", "save_path", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.doc_experiment.get_exp_id": [[20, 23], ["None"], "function", ["None"], ["@", "ex", ".", "capture", "\n", "def", "get_exp_id", "(", "_run", ")", ":", "\n", "    ", "return", "_run", ".", "_id", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.doc_experiment.my_config": [[25, 103], ["None"], "function", ["None"], ["", "@", "ex", ".", "config", "\n", "def", "my_config", "(", ")", ":", "\n", "\n", "    ", "test_mode", "=", "False", "\n", "if", "test_mode", ":", "\n", "        ", "run_hyperparam_search", "=", "False", "\n", "num_partitions_to_run", "=", "1", "\n", "important_qs", "=", "[", "4", "]", "\n", "", "else", ":", "\n", "        ", "important_qs", "=", "[", "4", ",", "5", ",", "9", ",", "10", ",", "11", "]", "\n", "run_hyperparam_search", "=", "True", "\n", "num_partitions_to_run", "=", "None", "\n", "\n", "", "discern_path", "=", "\"~/switchdrive/Institution/discern\"", "\n", "cache_file", "=", "'2019-08-21_18-56-53_fae71bc_doc.pkl'", "\n", "# cache_file = '2019-07-24_15-43-08_bd60a7f_with_mm_and_ner_ammendments.pkl'", "\n", "# cache_file = '2019-07-24_13-40-50_166c23e_test_mm_and_ner_ammendments_on_subset_30'", "\n", "\n", "n_estimators_default", "=", "500", "\n", "min_samples_leaf_default", "=", "5", "\n", "max_depth_default", "=", "50", "\n", "max_features_default", "=", "'sqrt'", "\n", "class_weight_default", "=", "'balanced_subsample'", "\n", "model_class", "=", "RandomForestClassifier", "\n", "n_estimators", "=", "[", "50", ",", "100", ",", "500", "]", "\n", "# Number of features to consider at every split", "\n", "# None means consider all", "\n", "max_features", "=", "[", "'sqrt'", ",", "None", ",", "0.5", "]", "\n", "# Maximum number of levels in tree", "\n", "max_depth", "=", "[", "10", ",", "50", ",", "100", "]", "\n", "# Minimum number of samples required to split a node", "\n", "min_samples_split", "=", "[", "5", ",", "50", ",", "500", "]", "\n", "# Minimum number of samples required at each leaf node", "\n", "min_samples_leaf", "=", "[", "3", ",", "5", ",", "10", ",", "100", "]", "\n", "# Method of selecting samples for training each tree", "\n", "# bootstrap = [True, False]", "\n", "# Create the random grid", "\n", "hyperparams", "=", "{", "'n_estimators'", ":", "n_estimators", ",", "\n", "'max_features'", ":", "max_features", ",", "\n", "'max_depth'", ":", "max_depth", ",", "\n", "'min_samples_split'", ":", "min_samples_split", ",", "\n", "'min_samples_leaf'", ":", "min_samples_leaf", ",", "\n", "# 'bootstrap': bootstrap,", "\n", "'class_weight'", ":", "[", "'balanced_subsample'", "]", ",", "\n", "}", "\n", "\n", "model_run_class", "=", "DocLevelModelRun", "\n", "n_partitions", "=", "5", "\n", "stratify_by", "=", "'label'", "\n", "\n", "feature_subset_all", "=", "None", "\n", "feature_subset_custom_7", "=", "[", "'et'", ",", "'al'", ",", "'thisisalink'", ",", "'external_link_cnt'", ",", "'internal_link_cnt'", ",", "'spacy_ner_date'", ",", "\n", "'bibliography_feature'", ",", "]", "\n", "feature_subset_5", "=", "[", "'al'", ",", "'thisisalink'", ",", "'external_link_cnt'", ",", "'spacy_ner_date'", ",", "'et'", "]", "\n", "feature_subset_19", "=", "[", "'al'", ",", "'thisisalink'", ",", "'et'", ",", "'spacy_ner_date'", ",", "'external_link_cnt'", ",", "'bibliography_feature'", ",", "\n", "'new'", ",", "'american'", ",", "'national'", ",", "'available'", ",", "'severe'", ",", "'information'", ",", "'internal_link_cnt'", ",", "\n", "'MM-Disorders'", ",", "'causes'", ",", "'type'", ",", "'cause'", ",", "'try'", ",", "'spacy_ner_ordinal'", "]", "\n", "feature_subset_50", "=", "[", "'thisisalink'", ",", "'mmconceptdisorders'", ",", "'sentiment_pos'", ",", "'life'", ",", "'experience'", ",", "'course'", ",", "\n", "'different'", ",", "'et'", ",", "'sentiment_neg'", ",", "'html_h4'", ",", "'mmconceptprocedures'", ",", "'need'", ",", "'discuss'", ",", "\n", "'american'", ",", "'child'", ",", "'inflammatory'", ",", "'especially'", ",", "'include'", ",", "'way'", ",", "'anti'", ",", "'important'", ",", "\n", "'MM-Disorders'", ",", "'new'", ",", "'spacy_ner_time'", ",", "'develop'", ",", "'mmconceptphysiology'", ",", "'national'", ",", "\n", "'patient'", ",", "'try'", ",", "'effective'", ",", "'type'", ",", "'causes'", ",", "'certain'", ",", "'stop'", ",", "'al'", ",", "'sentiment_compound'", ",", "\n", "'work'", ",", "'example'", ",", "'MM-Physiology'", ",", "'number'", ",", "'MM-Anatomy'", ",", "'spacy_ner_cardinal'", ",", "\n", "'internal_link_cnt'", ",", "'make'", ",", "'group'", ",", "'pain'", ",", "'reviewed'", ",", "'doctor'", ",", "'know'", "]", "\n", "feature_subset_86", "=", "[", "'al'", ",", "'thisisalink'", ",", "'et'", ",", "'spacy_ner_date'", ",", "'external_link_cnt'", ",", "'american'", ",", "'new'", ",", "\n", "'national'", ",", "'MM-Disorders'", ",", "'use'", ",", "'internal_link_cnt'", ",", "'bibliography_feature'", ",", "'information'", ",", "\n", "'available'", ",", "'causes'", ",", "'severe'", ",", "'sentiment_compound'", ",", "'type'", ",", "'cause'", ",", "'mmconceptdisorders'", ",", "\n", "'avoid'", ",", "'loss'", ",", "'spacy_ner_gpe'", ",", "'spacy_ner_cardinal'", ",", "'pain'", ",", "'rheumatoid'", ",", "\n", "'spacy_ner_date_no_digit'", ",", "'improve'", ",", "'help'", ",", "'inline_citation_cnt'", ",", "'mmconceptanatomy'", ",", "\n", "'child'", ",", "'spacy_ner_ordinal'", ",", "'clinical'", ",", "'effective'", ",", "'include'", ",", "'sentiment_neu'", ",", "'way'", ",", "\n", "'know'", ",", "'health'", ",", "'patient'", ",", "'mmconceptphysiology'", ",", "'develop'", ",", "'different'", ",", "'people'", ",", "'course'", ",", "\n", "'therapy'", ",", "'try'", ",", "'increase'", ",", "'ask'", ",", "'time'", ",", "'doctor'", ",", "'reviewed'", ",", "'need'", ",", "'important'", ",", "'life'", ",", "\n", "'MM-Anatomy'", ",", "'anti'", ",", "'stop'", ",", "'make'", ",", "'affect'", ",", "'mmconceptchemicalsdrugs'", ",", "'risk'", ",", "'long'", ",", "\n", "'sentiment_pos'", ",", "'MM-Procedures'", ",", "'example'", ",", "'inflammatory'", ",", "'mmconceptprocedures'", ",", "'start'", ",", "\n", "'especially'", ",", "'term'", ",", "'mmconceptdevices'", ",", "'html_h4'", ",", "'experience'", ",", "'feel'", ",", "'work'", ",", "'want'", ",", "\n", "'group'", ",", "'number'", ",", "'discuss'", ",", "'MM-Physiology'", ",", "'spacy_ner_time'", ",", "'certain'", ",", "'sentiment_neg'", "]", "\n", "feature_subset", "=", "None", "\n", "reduce_features", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.doc_experiment.my_main": [[105, 227], ["autodiscern.DataManager", "autodiscern.DataManager.load_cached_data_processor", "model_class", "print", "autodiscern.DataManager.save_experiment", "ex.add_artifact", "autodiscern.experiment.PartitionedExperiment", "question_models[].run", "print", "print", "pathlib.Path", "ex.add_artifact", "pathlib.Path", "ex.add_artifact", "pathlib.Path", "autodiscern.model.plot_confusion_matrix", "model.plot_confusion_matrix.get_figure", "ax.get_figure.savefig", "ex.add_artifact", "pathlib.Path", "autodiscern.model.plot_confusion_matrix", "model.plot_confusion_matrix.get_figure", "ax.get_figure.savefig", "ex.add_artifact", "pandas.concat", "print", "pathlib.Path", "ex.add_artifact", "doc_experiment.get_exp_id", "type", "question_models[].show_feature_importances().head", "open", "f.write", "open", "f.write", "y_true.extend", "y_pred.extend", "y_true.extend", "y_pred.extend", "matplotlib.subplots", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.legend", "ax.get_figure", "pathlib.Path", "ax.get_figure.savefig", "ex.add_artifact", "open", "f.write", "question_models[].show_feature_importances().head().to_string", "question_models[].get_selected_hyperparams().to_string", "matplotlib.plot", "question_models[].show_evaluation", "pd.concat.to_string", "question_models[].show_feature_importances", "range", "round", "int", "question_models[].show_feature_importances().head", "question_models[].get_selected_hyperparams", "col_name.lower", "int", "ex.log_scalar", "ex.log_scalar", "len", "q.split", "question_models[].show_feature_importances", "col_name.split"], "function", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.load_cached_data_processor", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.data_manager.DataManager.save_experiment", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.ModelRun.run", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.plot_confusion_matrix", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.plot_confusion_matrix", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.sacred_experiments.doc_experiment.get_exp_id", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_evaluation", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_feature_importances", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.get_selected_hyperparams", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.experiments.Sent2Doc_2ClassProb.Sent2Doc_2ClassProb_Experiment.show_feature_importances", "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.lemmatization.Splitter.split"], ["", "@", "ex", ".", "automain", "\n", "def", "my_main", "(", "discern_path", ",", "cache_file", ",", "important_qs", ",", "model_class", ",", "hyperparams", ",", "model_run_class", ",", "n_partitions", ",", "\n", "stratify_by", ",", "run_hyperparam_search", ",", "num_partitions_to_run", ",", "feature_subset", ",", "reduce_features", ",", "test_mode", ")", ":", "\n", "\n", "# convert hyperparams from a sacred.config.ReadOnlyDict to a regular Dictionary,", "\n", "#   otherwise pickling of the experiment fails", "\n", "    ", "hyperparams_dict", "=", "{", "k", ":", "hyperparams", "[", "k", "]", "for", "k", "in", "hyperparams", "}", "\n", "\n", "dm", "=", "DataManager", "(", "discern_path", ")", "\n", "data_processor", "=", "dm", ".", "load_cached_data_processor", "(", "cache_file", ")", "\n", "\n", "# initialize an modeling experiment for each DISCERN question", "\n", "model_obj", "=", "model_class", "(", ")", "\n", "question_models", "=", "{", "}", "\n", "for", "q_no", "in", "important_qs", ":", "\n", "        ", "exp_name", "=", "\"q{}\"", ".", "format", "(", "q_no", ")", "\n", "question_models", "[", "exp_name", "]", "=", "PartitionedExperiment", "(", "exp_name", ",", "data_processor", ".", "data", ",", "label_key", "=", "q_no", ",", "\n", "model_run_class", "=", "model_run_class", ",", "\n", "model", "=", "model_obj", ",", "preprocessing_func", "=", "data_processor", ".", "func", ",", "\n", "hyperparams", "=", "hyperparams_dict", ",", "n_partitions", "=", "n_partitions", ",", "\n", "stratify_by", "=", "stratify_by", ",", "feature_subset", "=", "feature_subset", ",", "\n", "reduce_features", "=", "reduce_features", ",", "verbose", "=", "True", ")", "\n", "\n", "# run the experiments", "\n", "", "for", "q", "in", "question_models", ":", "\n", "        ", "question_models", "[", "q", "]", ".", "run", "(", "num_partitions_to_run", "=", "num_partitions_to_run", ",", "run_hyperparam_search", "=", "run_hyperparam_search", ")", "\n", "\n", "# save feature importances as artifacts", "\n", "print", "(", "\"{}: {}\"", ".", "format", "(", "q", ",", "model", ".", "questions", "[", "int", "(", "q", ".", "split", "(", "'q'", ")", "[", "1", "]", ")", "]", ")", ")", "\n", "print", "(", "question_models", "[", "q", "]", ".", "show_feature_importances", "(", ")", ".", "head", "(", "10", ")", ")", "\n", "feature_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}_feature_importances.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "feature_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "show_feature_importances", "(", ")", ".", "head", "(", "100", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "feature_path", ")", "\n", "\n", "# save hyperparams", "\n", "sent_hyperparam_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}_hyperparams.txt'", ".", "format", "(", "q", ")", ")", "\n", "with", "open", "(", "sent_hyperparam_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "question_models", "[", "q", "]", ".", "get_selected_hyperparams", "(", ")", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "sent_hyperparam_path", ")", "\n", "\n", "# save confusion matrix, test set", "\n", "y_true", "=", "[", "]", "\n", "y_pred", "=", "[", "]", "\n", "for", "p", "in", "question_models", "[", "q", "]", ".", "model_runs", ":", "\n", "            ", "y_true", ".", "extend", "(", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "y_test", ")", "\n", "y_pred", ".", "extend", "(", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "y_test_predicted", ")", "\n", "", "classes", "=", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "model", ".", "classes_", "\n", "\n", "conf_matrix_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}_confusion_matrix_test.png'", ".", "format", "(", "q", ")", ")", "\n", "title", "=", "\"{} Test Set\"", ".", "format", "(", "q", ")", "\n", "plot", "=", "model", ".", "plot_confusion_matrix", "(", "y_true", ",", "y_pred", ",", "classes", "=", "classes", ",", "normalize", "=", "True", ",", "title", "=", "title", ")", "\n", "fig", "=", "plot", ".", "get_figure", "(", ")", "\n", "fig", ".", "savefig", "(", "conf_matrix_path", ")", "\n", "ex", ".", "add_artifact", "(", "conf_matrix_path", ")", "\n", "\n", "# save confusion matrix, train set", "\n", "y_true", "=", "[", "]", "\n", "y_pred", "=", "[", "]", "\n", "for", "p", "in", "question_models", "[", "q", "]", ".", "model_runs", ":", "\n", "            ", "y_true", ".", "extend", "(", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "y_train", ")", "\n", "y_pred", ".", "extend", "(", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "y_train_predicted", ")", "\n", "", "classes", "=", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "model", ".", "classes_", "\n", "\n", "conf_matrix_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}_confusion_matrix_train.png'", ".", "format", "(", "q", ")", ")", "\n", "title", "=", "\"{} Train Set\"", ".", "format", "(", "q", ")", "\n", "plot", "=", "model", ".", "plot_confusion_matrix", "(", "y_true", ",", "y_pred", ",", "classes", "=", "classes", ",", "normalize", "=", "True", ",", "title", "=", "title", ")", "\n", "fig", "=", "plot", ".", "get_figure", "(", ")", "\n", "fig", ".", "savefig", "(", "conf_matrix_path", ")", "\n", "ex", ".", "add_artifact", "(", "conf_matrix_path", ")", "\n", "\n", "# save feature reduction graph, if applicable", "\n", "if", "reduce_features", ":", "\n", "            ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "plt", ".", "xlabel", "(", "\"Number of features selected (but actually this axis is not right)\"", ")", "\n", "plt", ".", "ylabel", "(", "\"Cross validation score (nb of correct classifications)\"", ")", "\n", "num_features", "=", "[", "]", "\n", "score", "=", "[", "]", "\n", "model_run_id", "=", "[", "]", "\n", "for", "p", "in", "question_models", "[", "q", "]", ".", "model_runs", ":", "\n", "                ", "grid_scores", "=", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "rfecv", ".", "grid_scores_", "\n", "# num_features.extend(range(1, len(grid_scores)+1))", "\n", "# score.extend(grid_scores)", "\n", "# model_run_id.extend([p]*len(grid_scores))", "\n", "\n", "label", "=", "'{}: {} features'", ".", "format", "(", "p", ",", "question_models", "[", "q", "]", ".", "model_runs", "[", "p", "]", ".", "rfecv", ".", "n_features_", ")", "\n", "plt", ".", "plot", "(", "range", "(", "1", ",", "len", "(", "grid_scores", ")", "+", "1", ")", ",", "grid_scores", ",", "label", "=", "label", ")", "\n", "# plt.show()", "\n", "", "plt", ".", "legend", "(", ")", "\n", "fig", "=", "ax", ".", "get_figure", "(", ")", "\n", "conf_matrix_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}_feature_reduction.png'", ".", "format", "(", "q", ")", ")", "\n", "fig", ".", "savefig", "(", "conf_matrix_path", ")", "\n", "ex", ".", "add_artifact", "(", "conf_matrix_path", ")", "\n", "\n", "# save F1 and accuracy scores as artifacts", "\n", "", "", "for", "metric", "in", "[", "'f1_macro'", ",", "'accuracy'", ",", "'f1_micro'", "]", ":", "\n", "        ", "all_q_metric", "=", "pd", ".", "concat", "(", "[", "question_models", "[", "q", "]", ".", "show_evaluation", "(", "metric", "=", "metric", ")", "for", "q", "in", "question_models", "]", ",", "axis", "=", "0", ")", "\n", "print", "(", "all_q_metric", ")", "\n", "all_f1_path", "=", "Path", "(", "dm", ".", "data_path", ",", "'results/{}.txt'", ".", "format", "(", "metric", ")", ")", "\n", "with", "open", "(", "all_f1_path", ",", "'w'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "all_q_metric", ".", "to_string", "(", ")", ")", "\n", "", "ex", ".", "add_artifact", "(", "all_f1_path", ")", "\n", "\n", "# save partition accuracies as metric entries?", "\n", "for", "q", "in", "all_q_metric", ".", "index", ":", "\n", "            ", "for", "col_name", "in", "all_q_metric", ".", "columns", ":", "\n", "                ", "if", "col_name", "not", "in", "[", "'mean'", ",", "'median'", ",", "'stddev'", "]", ":", "\n", "                    ", "value", "=", "round", "(", "all_q_metric", ".", "loc", "[", "q", ",", "col_name", "]", ",", "3", ")", "\n", "# if the experiment ran with stratified partitions, include the partition id", "\n", "# otherwise, just let whatever partitions be reported in increasing order", "\n", "#   (step is automatically assigned)", "\n", "if", "'partition '", "in", "col_name", ".", "lower", "(", ")", ":", "\n", "                        ", "partiton_number", "=", "int", "(", "col_name", ".", "split", "(", "' '", ")", "[", "1", "]", ")", "\n", "ex", ".", "log_scalar", "(", "\"{}_{}\"", ".", "format", "(", "q", ",", "metric", ")", ",", "value", ",", "partiton_number", ")", "\n", "", "else", ":", "\n", "                        ", "ex", ".", "log_scalar", "(", "\"{}_{}\"", ".", "format", "(", "q", ",", "metric", ")", ",", "value", ")", "\n", "\n", "# save the models themselves for future inspection", "\n", "", "", "", "", "", "file_name", "=", "'{}.dill'", ".", "format", "(", "get_exp_id", "(", ")", ")", "\n", "print", "(", "type", "(", "question_models", ")", ")", "\n", "save_path", "=", "dm", ".", "save_experiment", "(", "question_models", ",", "file_name", "=", "file_name", ")", "\n", "ex", ".", "add_artifact", "(", "save_path", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_experiment.TestExperiment.test_partition_document_ids_all_elements_preserved": [[7, 14], ["list", "autodiscern.PartitionedExperiment.partition_document_ids", "set", "set", "test_experiment.TestExperiment.assertEqual", "range"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids"], ["    ", "def", "test_partition_document_ids_all_elements_preserved", "(", "self", ")", ":", "\n", "        ", "doc_ids", "=", "list", "(", "range", "(", "20", ")", ")", "\n", "output", "=", "ade", ".", "PartitionedExperiment", ".", "partition_document_ids", "(", "doc_ids", ",", "4", ")", "\n", "\n", "input_set", "=", "set", "(", "doc_ids", ")", "\n", "output_set", "=", "set", "(", "[", "item", "for", "partition_name", "in", "output", "for", "item", "in", "output", "[", "partition_name", "]", "]", ")", "\n", "self", ".", "assertEqual", "(", "input_set", ",", "output_set", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_experiment.TestExperiment.test_partition_document_ids_elements_do_not_overlap": [[15, 28], ["list", "autodiscern.PartitionedExperiment.partition_document_ids", "range", "range", "len", "range", "len", "set", "set", "set.intersection", "test_experiment.TestExperiment.assertEqual", "len"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids"], ["", "def", "test_partition_document_ids_elements_do_not_overlap", "(", "self", ")", ":", "\n", "        ", "doc_ids", "=", "list", "(", "range", "(", "20", ")", ")", "\n", "output", "=", "ade", ".", "PartitionedExperiment", ".", "partition_document_ids", "(", "doc_ids", ",", "4", ")", "\n", "output_list_of_lists", "=", "[", "output", "[", "partition_name", "]", "for", "partition_name", "in", "output", "]", "\n", "\n", "# assert that the elements in each sublist are not duplicated in other sublists", "\n", "for", "i", "in", "range", "(", "len", "(", "output_list_of_lists", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "output_list_of_lists", ")", ")", ":", "\n", "                ", "if", "i", "!=", "j", ":", "\n", "                    ", "setA", "=", "set", "(", "output_list_of_lists", "[", "i", "]", ")", "\n", "setB", "=", "set", "(", "output_list_of_lists", "[", "j", "]", ")", "\n", "intersection", "=", "setA", ".", "intersection", "(", "setB", ")", "\n", "self", ".", "assertEqual", "(", "len", "(", "intersection", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_experiment.TestExperiment.test_partition_document_ids_total_number_of_elements_preserved": [[29, 33], ["list", "autodiscern.PartitionedExperiment.partition_document_ids", "test_experiment.TestExperiment.assertEqual", "range", "sum", "len", "len"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids"], ["", "", "", "", "def", "test_partition_document_ids_total_number_of_elements_preserved", "(", "self", ")", ":", "\n", "        ", "doc_ids", "=", "list", "(", "range", "(", "20", ")", ")", "\n", "output", "=", "ade", ".", "PartitionedExperiment", ".", "partition_document_ids", "(", "doc_ids", ",", "4", ")", "\n", "self", ".", "assertEqual", "(", "sum", "(", "[", "len", "(", "output", "[", "partition_name", "]", ")", "for", "partition_name", "in", "output", "]", ")", ",", "len", "(", "doc_ids", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_experiment.TestExperiment.test_partition_document_ids_stratified_all_elements_preserved": [[34, 42], ["list", "autodiscern.PartitionedExperiment.partition_document_ids_stratified", "set", "set", "test_experiment.TestExperiment.assertEqual", "range"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids_stratified"], ["", "def", "test_partition_document_ids_stratified_all_elements_preserved", "(", "self", ")", ":", "\n", "        ", "doc_ids", "=", "list", "(", "range", "(", "20", ")", ")", "\n", "labels", "=", "[", "1", "]", "*", "10", "+", "[", "2", "]", "*", "10", "\n", "output", "=", "ade", ".", "PartitionedExperiment", ".", "partition_document_ids_stratified", "(", "doc_ids", ",", "labels", ",", "4", ")", "\n", "\n", "input_set", "=", "set", "(", "doc_ids", ")", "\n", "output_set", "=", "set", "(", "[", "item", "for", "partition_name", "in", "output", "for", "item", "in", "output", "[", "partition_name", "]", "]", ")", "\n", "self", ".", "assertEqual", "(", "input_set", ",", "output_set", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_experiment.TestExperiment.test_partition_document_ids_stratified_elements_do_not_overlap": [[43, 57], ["list", "autodiscern.PartitionedExperiment.partition_document_ids_stratified", "range", "range", "len", "range", "len", "set", "set", "set.intersection", "test_experiment.TestExperiment.assertEqual", "len"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids_stratified"], ["", "def", "test_partition_document_ids_stratified_elements_do_not_overlap", "(", "self", ")", ":", "\n", "        ", "doc_ids", "=", "list", "(", "range", "(", "20", ")", ")", "\n", "labels", "=", "[", "1", "]", "*", "10", "+", "[", "2", "]", "*", "10", "\n", "output", "=", "ade", ".", "PartitionedExperiment", ".", "partition_document_ids_stratified", "(", "doc_ids", ",", "labels", ",", "4", ")", "\n", "output_list_of_lists", "=", "[", "output", "[", "partition_name", "]", "for", "partition_name", "in", "output", "]", "\n", "\n", "# assert that the elements in each sublist are not duplicated in other sublists", "\n", "for", "i", "in", "range", "(", "len", "(", "output_list_of_lists", ")", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "len", "(", "output_list_of_lists", ")", ")", ":", "\n", "                ", "if", "i", "!=", "j", ":", "\n", "                    ", "setA", "=", "set", "(", "output_list_of_lists", "[", "i", "]", ")", "\n", "setB", "=", "set", "(", "output_list_of_lists", "[", "j", "]", ")", "\n", "intersection", "=", "setA", ".", "intersection", "(", "setB", ")", "\n", "self", ".", "assertEqual", "(", "len", "(", "intersection", ")", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_experiment.TestExperiment.test_partition_document_ids_stratified_total_number_of_elements_preserved": [[58, 63], ["list", "autodiscern.PartitionedExperiment.partition_document_ids_stratified", "test_experiment.TestExperiment.assertEqual", "range", "sum", "len", "len"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids_stratified"], ["", "", "", "", "def", "test_partition_document_ids_stratified_total_number_of_elements_preserved", "(", "self", ")", ":", "\n", "        ", "doc_ids", "=", "list", "(", "range", "(", "20", ")", ")", "\n", "labels", "=", "[", "1", "]", "*", "10", "+", "[", "2", "]", "*", "10", "\n", "output", "=", "ade", ".", "PartitionedExperiment", ".", "partition_document_ids_stratified", "(", "doc_ids", ",", "labels", ",", "4", ")", "\n", "self", ".", "assertEqual", "(", "sum", "(", "[", "len", "(", "output", "[", "partition_name", "]", ")", "for", "partition_name", "in", "output", "]", ")", ",", "len", "(", "doc_ids", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_experiment.TestExperiment.test_partition_document_ids_by_category": [[64, 79], ["list", "autodiscern.PartitionedExperiment.partition_document_ids_by_category", "test_experiment.TestExperiment.assertEqual", "range", "list", "list", "list", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.partition_document_ids_by_category"], ["", "def", "test_partition_document_ids_by_category", "(", "self", ")", ":", "\n", "        ", "doc_ids", "=", "list", "(", "range", "(", "20", ")", ")", "\n", "doc_categories", "=", "[", "1", "]", "*", "5", "+", "[", "2", "]", "*", "5", "+", "[", "3", "]", "*", "10", "\n", "category_key", "=", "{", "\n", "1", ":", "'one'", ",", "\n", "2", ":", "'two'", ",", "\n", "3", ":", "'three'", ",", "\n", "}", "\n", "expected_output", "=", "{", "\n", "'one'", ":", "list", "(", "range", "(", "0", ",", "5", ")", ")", ",", "\n", "'two'", ":", "list", "(", "range", "(", "5", ",", "10", ")", ")", ",", "\n", "'three'", ":", "list", "(", "range", "(", "10", ",", "20", ")", ")", ",", "\n", "}", "\n", "output", "=", "ade", ".", "PartitionedExperiment", ".", "partition_document_ids_by_category", "(", "doc_ids", ",", "doc_categories", ",", "category_key", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_experiment.TestExperiment.test_materialize_partition": [[80, 103], ["test_experiment.TestExperiment.assertEqual", "autodiscern.PartitionedExperiment.materialize_partition"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.experiment.PartitionedExperiment.materialize_partition"], ["", "def", "test_materialize_partition", "(", "self", ")", ":", "\n", "        ", "partition_ids", "=", "[", "3", ",", "4", "]", "\n", "data_dict", "=", "{", "\n", "0", ":", "{", "'entity_id'", ":", "0", ",", "'content'", ":", "'words'", "}", ",", "\n", "1", ":", "{", "'entity_id'", ":", "1", ",", "'content'", ":", "'words'", "}", ",", "\n", "2", ":", "{", "'entity_id'", ":", "2", ",", "'content'", ":", "'words'", "}", ",", "\n", "3", ":", "{", "'entity_id'", ":", "3", ",", "'content'", ":", "'words'", "}", ",", "\n", "4", ":", "{", "'entity_id'", ":", "4", ",", "'content'", ":", "'words'", "}", ",", "\n", "}", "\n", "expected_output", "=", "(", "\n", "[", "\n", "{", "'entity_id'", ":", "0", ",", "'content'", ":", "'words'", "}", ",", "\n", "{", "'entity_id'", ":", "1", ",", "'content'", ":", "'words'", "}", ",", "\n", "{", "'entity_id'", ":", "2", ",", "'content'", ":", "'words'", "}", ",", "\n", "\n", "]", ",", "\n", "[", "\n", "{", "'entity_id'", ":", "3", ",", "'content'", ":", "'words'", "}", ",", "\n", "{", "'entity_id'", ":", "4", ",", "'content'", ":", "'words'", "}", ",", "\n", "\n", "]", ",", "\n", ")", "\n", "self", ".", "assertEqual", "(", "ade", ".", "PartitionedExperiment", ".", "materialize_partition", "(", "partition_ids", ",", "data_dict", ")", ",", "expected_output", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_model.TestModel.test_vectorize_html": [[8, 20], ["pandas.DataFrame", "pandas.testing.assert_frame_equal", "autodiscern.vectorize_html"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_html"], ["    ", "def", "test_vectorize_html", "(", "self", ",", ")", ":", "\n", "        ", "test_input", "=", "[", "'h1'", ",", "'a'", "]", "\n", "expected_output", "=", "pd", ".", "DataFrame", "(", "{", "\n", "'html_h1'", ":", "1", ",", "\n", "'html_h2'", ":", "0", ",", "\n", "'html_h3'", ":", "0", ",", "\n", "'html_h4'", ":", "0", ",", "\n", "'html_a'", ":", "1", ",", "\n", "'html_li'", ":", "0", ",", "\n", "'html_tr'", ":", "0", ",", "\n", "}", ",", "index", "=", "[", "0", "]", ")", "\n", "pd", ".", "testing", ".", "assert_frame_equal", "(", "adm", ".", "vectorize_html", "(", "test_input", ")", ",", "expected_output", ",", "check_like", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_model.TestModel.test_vectorize_metamap": [[21, 41], ["pandas.DataFrame", "pandas.testing.assert_frame_equal", "autodiscern.vectorize_metamap"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_metamap"], ["", "def", "test_vectorize_metamap", "(", "self", ",", ")", ":", "\n", "        ", "test_input", "=", "[", "'Chemicals & Drugs'", ",", "'Chemicals & Drugs'", ",", "'Disorders'", ",", "]", "\n", "expected_output", "=", "pd", ".", "DataFrame", "(", "{", "\n", "'MM-Chemicals & Drugs'", ":", "2", ",", "\n", "'MM-Disorders'", ":", "1", ",", "\n", "'MM-Activities & Behaviors'", ":", "0", ",", "\n", "'MM-Living Beings'", ":", "0", ",", "\n", "'MM-Genes & Molecular Sequences'", ":", "0", ",", "\n", "'MM-Anatomy'", ":", "0", ",", "\n", "'MM-Phenomena'", ":", "0", ",", "\n", "'MM-Occupations'", ":", "0", ",", "\n", "'MM-Physiology'", ":", "0", ",", "\n", "'MM-Concepts & Ideas'", ":", "0", ",", "\n", "'MM-Procedures'", ":", "0", ",", "\n", "'MM-Devices'", ":", "0", ",", "\n", "'MM-Objects'", ":", "0", ",", "\n", "'MM-Geographic Areas'", ":", "0", ",", "\n", "'MM-Organizations'", ":", "0", ",", "\n", "}", ",", "index", "=", "[", "0", "]", ")", "\n", "pd", ".", "testing", ".", "assert_frame_equal", "(", "adm", ".", "vectorize_metamap", "(", "test_input", ")", ",", "expected_output", ",", "check_like", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_model.TestModel.test_vectorize_link_type": [[42, 49], ["pandas.DataFrame", "pandas.testing.assert_frame_equal", "autodiscern.vectorize_link_type"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_link_type"], ["", "def", "test_vectorize_link_type", "(", "self", ",", ")", ":", "\n", "        ", "test_input", "=", "[", "'internal'", ",", "'external'", ",", "'external'", "]", "\n", "expected_output", "=", "pd", ".", "DataFrame", "(", "{", "\n", "'internal_link_cnt'", ":", "1", ",", "\n", "'external_link_cnt'", ":", "2", ",", "\n", "}", ",", "index", "=", "[", "0", "]", ")", "\n", "pd", ".", "testing", ".", "assert_frame_equal", "(", "adm", ".", "vectorize_link_type", "(", "test_input", ")", ",", "expected_output", ",", "check_like", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_model.TestModel.test_vectorize_citations": [[50, 56], ["pandas.DataFrame", "pandas.testing.assert_frame_equal", "autodiscern.vectorize_citations"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.model.vectorize_citations"], ["", "def", "test_vectorize_citations", "(", "self", ",", ")", ":", "\n", "        ", "test_input", "=", "[", "'[1]'", ",", "'[2]'", "]", "\n", "expected_output", "=", "pd", ".", "DataFrame", "(", "{", "\n", "'inline_citation_cnt'", ":", "2", ",", "\n", "}", ",", "index", "=", "[", "0", "]", ")", "\n", "pd", ".", "testing", ".", "assert_frame_equal", "(", "adm", ".", "vectorize_citations", "(", "test_input", ")", ",", "expected_output", ",", "check_like", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_data_manager.TestDataManager.test_example": [[6, 8], ["test_data_manager.TestDataManager.assertEqual"], "methods", ["None"], ["    ", "def", "test_example", "(", "self", ")", ":", "\n", "        ", "self", ".", "assertEqual", "(", "'foo'", ".", "upper", "(", ")", ",", "'FOO'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_replace_problem_chars": [[8, 12], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.replace_chars"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_chars"], ["    ", "def", "test_replace_problem_chars", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"words \\twords\\twords\"", "\n", "expected_output", "=", "\"words  words words\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "replace_chars", "(", "test_input", ",", "[", "'\\t'", "]", ",", "' '", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_regex_out_punctuation_and_white_space_replaces_extra_consecutive_chars": [[13, 17], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.regex_out_punctuation_and_white_space"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space"], ["", "def", "test_regex_out_punctuation_and_white_space_replaces_extra_consecutive_chars", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text text..\\n. text\"", "\n", "expected_output", "=", "\"text text. \\ntext\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "regex_out_punctuation_and_white_space", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_regex_out_punctuation_and_white_space_no_effect_single_period": [[18, 21], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.regex_out_punctuation_and_white_space"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space"], ["", "def", "test_regex_out_punctuation_and_white_space_no_effect_single_period", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text.\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "regex_out_punctuation_and_white_space", "(", "test_input", ")", ",", "test_input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_regex_out_punctuation_and_white_space_removes_double_space_between_words": [[22, 26], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.regex_out_punctuation_and_white_space"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space"], ["", "def", "test_regex_out_punctuation_and_white_space_removes_double_space_between_words", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text  text.\"", "\n", "expected_output", "=", "\"text text.\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "regex_out_punctuation_and_white_space", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_regex_out_punctuation_and_white_space_no_effect_period_between_words": [[27, 31], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.regex_out_punctuation_and_white_space"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space"], ["", "def", "test_regex_out_punctuation_and_white_space_no_effect_period_between_words", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text. text\"", "\n", "expected_output", "=", "\"text. text\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "regex_out_punctuation_and_white_space", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_regex_out_punctuation_and_white_space_removes_extra_consecutive_periods": [[32, 36], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.regex_out_punctuation_and_white_space"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space"], ["", "def", "test_regex_out_punctuation_and_white_space_removes_extra_consecutive_periods", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text text...\"", "\n", "expected_output", "=", "\"text text. \"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "regex_out_punctuation_and_white_space", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_regex_out_punctuation_and_white_space_removes_leading_period": [[37, 41], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.regex_out_punctuation_and_white_space"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space"], ["", "def", "test_regex_out_punctuation_and_white_space_removes_leading_period", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"\\n. \\ntext text. \"", "\n", "expected_output", "=", "\"text text. \"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "regex_out_punctuation_and_white_space", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_regex_out_punctuation_and_white_space_combines_question_mark_period": [[42, 46], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.regex_out_punctuation_and_white_space"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space"], ["", "def", "test_regex_out_punctuation_and_white_space_combines_question_mark_period", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text text?. \"", "\n", "expected_output", "=", "\"text text? \"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "regex_out_punctuation_and_white_space", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_regex_out_punctuation_and_white_space_combines_question_mark_space_period": [[47, 51], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.regex_out_punctuation_and_white_space"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.regex_out_punctuation_and_white_space"], ["", "def", "test_regex_out_punctuation_and_white_space_combines_question_mark_space_period", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text text? . \"", "\n", "expected_output", "=", "\"text text? \"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "regex_out_punctuation_and_white_space", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_condense_line_breaks_multiple_newlines": [[52, 56], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.condense_line_breaks"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks"], ["", "def", "test_condense_line_breaks_multiple_newlines", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text\\n\\ntext\"", "\n", "expected_output", "=", "\"text \\ntext\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "condense_line_breaks", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_condense_line_breaks_strips": [[57, 61], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.condense_line_breaks"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks"], ["", "def", "test_condense_line_breaks_strips", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text\\n\"", "\n", "expected_output", "=", "\"text\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "condense_line_breaks", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_condense_line_breaks_replaces_single_break_html_tag": [[62, 66], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.condense_line_breaks"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks"], ["", "def", "test_condense_line_breaks_replaces_single_break_html_tag", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text<br>text\"", "\n", "expected_output", "=", "\"text\\ntext\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "condense_line_breaks", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_condense_line_breaks_replaces_multiple_break_html_tags": [[67, 71], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.condense_line_breaks"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks"], ["", "def", "test_condense_line_breaks_replaces_multiple_break_html_tags", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text<br><br>text\"", "\n", "expected_output", "=", "\"text \\ntext\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "condense_line_breaks", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_condense_line_breaks_replaces_break_html_tags_with_bs4_slash": [[72, 76], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.condense_line_breaks"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks"], ["", "def", "test_condense_line_breaks_replaces_break_html_tags_with_bs4_slash", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text<br/>text\"", "\n", "expected_output", "=", "\"text\\ntext\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "condense_line_breaks", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_condense_line_breaks_replaces_combo_break_html_tag_and_newline": [[77, 81], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.condense_line_breaks"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.condense_line_breaks"], ["", "def", "test_condense_line_breaks_replaces_combo_break_html_tag_and_newline", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text<br>\\ntext\"", "\n", "expected_output", "=", "\"text \\ntext\"", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "condense_line_breaks", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_soup_to_text_with_tags_replaces_amp": [[82, 86], ["bs4.BeautifulSoup", "test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.soup_to_text_with_tags"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.soup_to_text_with_tags"], ["", "def", "test_soup_to_text_with_tags_replaces_amp", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "BeautifulSoup", "(", "'<html><body><h2 class=\"selectedHighlight\">Staging, grading &amp; treatment</h2></body></html>'", ",", "features", "=", "\"html.parser\"", ")", "\n", "expected_output", "=", "'<html><body><h2 class=\"selectedHighlight\">Staging, grading & treatment</h2></body></html>'", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "soup_to_text_with_tags", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_get_domain_from_link_tag_href_link_returns_domain": [[87, 94], ["bs4.BeautifulSoup", "test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.get_domain_from_link_tag"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.get_domain_from_link_tag"], ["", "def", "test_get_domain_from_link_tag_href_link_returns_domain", "(", "self", ")", ":", "\n", "        ", "soup", "=", "BeautifulSoup", "(", "\n", "'<html><body><a href=\"http://www.cat.com/health\">text</a></body></html>'", ",", "\n", "features", "=", "\"html.parser\"", ")", "\n", "test_input", "=", "soup", ".", "a", "\n", "expected_output", "=", "'cat'", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "get_domain_from_link_tag", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_get_domain_from_link_tag_src_link_returns_domain": [[95, 102], ["bs4.BeautifulSoup", "test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.get_domain_from_link_tag"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.get_domain_from_link_tag"], ["", "def", "test_get_domain_from_link_tag_src_link_returns_domain", "(", "self", ")", ":", "\n", "        ", "soup", "=", "BeautifulSoup", "(", "\n", "'<html><body><a src=\"http://www.cat.com/health\">text</a></body></html>'", ",", "\n", "features", "=", "\"html.parser\"", ")", "\n", "test_input", "=", "soup", ".", "a", "\n", "expected_output", "=", "'cat'", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "get_domain_from_link_tag", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_get_domain_from_link_tag_file_path_returns_NA": [[103, 110], ["bs4.BeautifulSoup", "test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.get_domain_from_link_tag"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.get_domain_from_link_tag"], ["", "def", "test_get_domain_from_link_tag_file_path_returns_NA", "(", "self", ")", ":", "\n", "        ", "soup", "=", "BeautifulSoup", "(", "\n", "'<html><body><a href=\"/health/deep-brain-stimulation/MY00184\">text</a></body></html>'", ",", "\n", "features", "=", "\"html.parser\"", ")", "\n", "test_input", "=", "soup", ".", "a", "\n", "expected_output", "=", "'NA'", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "get_domain_from_link_tag", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_reformat_html_link_tags_replaces_link_with_domain": [[111, 115], ["bs4.BeautifulSoup", "bs4.BeautifulSoup", "test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer.reformat_html_link_tags"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.reformat_html_link_tags"], ["", "def", "test_reformat_html_link_tags_replaces_link_with_domain", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "BeautifulSoup", "(", "'<html><body>There is more information on medication on the <a title=\"Royal College of Psychiatrists information on medications\" href=\"http://www.rcpsych.ac.uk/mentalhealthinformation/therapies.aspx \">website of the Royal College of Psychiatrists</a></body></html>'", ",", "features", "=", "\"html.parser\"", ")", "\n", "expected_output", "=", "BeautifulSoup", "(", "'<html><body>There is more information on medication on the <a href=\"rcpsych\">website of the Royal College of Psychiatrists</a></body></html>'", ",", "features", "=", "\"html.parser\"", ")", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "reformat_html_link_tags", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_replace_html": [[116, 131], ["bs4.BeautifulSoup", "autodiscern.Transformer", "autodiscern.Transformer.replace_html", "test_transformations.TestTransformations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_html"], ["", "def", "test_replace_html", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "BeautifulSoup", "(", "'<html><body><h1 font=\"Blue\">Heading1</h1><h2><i>Heading2</i></h2><a href=\"google.com\">link here</a></p></body></html>'", ",", "features", "=", "\"html.parser\"", ")", "\n", "expected_output", "=", "'<h1>Heading1</h1>thisisanh2tag Heading2.<a href=\"google.com\">link here</a>'", "\n", "\n", "tags_to_keep", "=", "{", "'h1'", "}", "\n", "tags_to_keep_with_attr", "=", "{", "'a'", "}", "\n", "tags_to_replace_with_str", "=", "{", "\n", "'h2'", ":", "(", "'thisisanh2tag '", ",", "'.'", ")", ",", "\n", "}", "\n", "default_tag_replacement_str", "=", "''", "\n", "transformer", "=", "adt", ".", "Transformer", "(", ")", "\n", "test_output", "=", "transformer", ".", "replace_html", "(", "test_input", ",", "tags_to_keep", ",", "tags_to_keep_with_attr", ",", "\n", "tags_to_replace_with_str", ",", "default_tag_replacement_str", ",", "\n", "include_link_domains", "=", "True", ")", "\n", "self", ".", "assertEqual", "(", "test_output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_replace_html_keeps_tag": [[132, 145], ["bs4.BeautifulSoup", "set", "autodiscern.Transformer", "autodiscern.Transformer.replace_html", "test_transformations.TestTransformations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_html"], ["", "def", "test_replace_html_keeps_tag", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "BeautifulSoup", "(", "'<html><body><h1>Heading1</h1></body></html>'", ",", "features", "=", "\"html.parser\"", ")", "\n", "expected_output", "=", "'<h1>Heading1</h1>'", "\n", "\n", "tags_to_keep", "=", "{", "'h1'", "}", "\n", "tags_to_keep_with_attr", "=", "set", "(", ")", "\n", "tags_to_replace_with_str", "=", "{", "}", "\n", "default_tag_replacement_str", "=", "''", "\n", "transformer", "=", "adt", ".", "Transformer", "(", ")", "\n", "test_output", "=", "transformer", ".", "replace_html", "(", "test_input", ",", "tags_to_keep", ",", "tags_to_keep_with_attr", ",", "\n", "tags_to_replace_with_str", ",", "default_tag_replacement_str", ",", "\n", "include_link_domains", "=", "True", ")", "\n", "self", ".", "assertEqual", "(", "test_output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_replace_html_replaces_tag": [[146, 159], ["bs4.BeautifulSoup", "set", "set", "autodiscern.Transformer", "autodiscern.Transformer.replace_html", "test_transformations.TestTransformations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_html"], ["", "def", "test_replace_html_replaces_tag", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "BeautifulSoup", "(", "'<html><body><h1>Heading1</h1></body></html>'", ",", "features", "=", "\"html.parser\"", ")", "\n", "expected_output", "=", "'thisisah1tag Heading1. '", "\n", "\n", "tags_to_keep", "=", "set", "(", ")", "\n", "tags_to_keep_with_attr", "=", "set", "(", ")", "\n", "tags_to_replace_with_str", "=", "{", "'h1'", ":", "(", "'thisisah1tag '", ",", "'. '", ")", "}", "\n", "default_tag_replacement_str", "=", "''", "\n", "transformer", "=", "adt", ".", "Transformer", "(", ")", "\n", "test_output", "=", "transformer", ".", "replace_html", "(", "test_input", ",", "tags_to_keep", ",", "tags_to_keep_with_attr", ",", "\n", "tags_to_replace_with_str", ",", "default_tag_replacement_str", ",", "\n", "include_link_domains", "=", "True", ")", "\n", "self", ".", "assertEqual", "(", "test_output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_replace_html_replaces_link_no_domain": [[160, 173], ["bs4.BeautifulSoup", "set", "set", "autodiscern.Transformer", "autodiscern.Transformer.replace_html", "test_transformations.TestTransformations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_html"], ["", "def", "test_replace_html_replaces_link_no_domain", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "BeautifulSoup", "(", "'<html><body>There is a <a href=\"google.com\">link here</a>.</body></html>'", ",", "features", "=", "\"html.parser\"", ")", "\n", "expected_output", "=", "'There is a thisisalinktag link here.'", "\n", "\n", "tags_to_keep", "=", "set", "(", ")", "\n", "tags_to_keep_with_attr", "=", "set", "(", ")", "\n", "tags_to_replace_with_str", "=", "{", "'a'", ":", "(", "'thisisalinktag '", ",", "''", ")", "}", "\n", "default_tag_replacement_str", "=", "''", "\n", "transformer", "=", "adt", ".", "Transformer", "(", ")", "\n", "test_output", "=", "transformer", ".", "replace_html", "(", "test_input", ",", "tags_to_keep", ",", "tags_to_keep_with_attr", ",", "\n", "tags_to_replace_with_str", ",", "default_tag_replacement_str", ",", "\n", "include_link_domains", "=", "False", ")", "\n", "self", ".", "assertEqual", "(", "test_output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_replace_html_replaces_link_with_domain": [[174, 187], ["bs4.BeautifulSoup", "set", "set", "autodiscern.Transformer", "autodiscern.Transformer.replace_html", "test_transformations.TestTransformations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.replace_html"], ["", "def", "test_replace_html_replaces_link_with_domain", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "BeautifulSoup", "(", "'<html><body>There is a <a href=\"google.com\">link here</a>.</body></html>'", ",", "features", "=", "\"html.parser\"", ")", "\n", "expected_output", "=", "'There is a thisisalinktaggoogle link here.'", "\n", "\n", "tags_to_keep", "=", "set", "(", ")", "\n", "tags_to_keep_with_attr", "=", "set", "(", ")", "\n", "tags_to_replace_with_str", "=", "{", "'a'", ":", "(", "'thisisalinktag '", ",", "''", ")", "}", "\n", "default_tag_replacement_str", "=", "''", "\n", "transformer", "=", "adt", ".", "Transformer", "(", ")", "\n", "test_output", "=", "transformer", ".", "replace_html", "(", "test_input", ",", "tags_to_keep", ",", "tags_to_keep_with_attr", ",", "\n", "tags_to_replace_with_str", ",", "default_tag_replacement_str", ",", "\n", "include_link_domains", "=", "True", ")", "\n", "self", ".", "assertEqual", "(", "test_output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_flatten_text_dicts": [[188, 200], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._flatten_text_dicts"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._flatten_text_dicts"], ["", "def", "test_flatten_text_dicts", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "[", "\n", "{", "'id'", ":", "0", ",", "'content'", ":", "[", "'word0'", ",", "'word1'", "]", "}", ",", "\n", "{", "'id'", ":", "1", ",", "'content'", ":", "[", "'word2'", ",", "'word3'", "]", "}", ",", "\n", "]", "\n", "expected_output", "=", "[", "\n", "{", "'id'", ":", "0", ",", "'sub_id'", ":", "0", ",", "'content'", ":", "'word0'", "}", ",", "\n", "{", "'id'", ":", "0", ",", "'sub_id'", ":", "1", ",", "'content'", ":", "'word1'", "}", ",", "\n", "{", "'id'", ":", "1", ",", "'sub_id'", ":", "0", ",", "'content'", ":", "'word2'", "}", ",", "\n", "{", "'id'", ":", "1", ",", "'sub_id'", ":", "1", ",", "'content'", ":", "'word3'", "}", ",", "\n", "]", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_flatten_text_dicts", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_retrieve_domain_from_plaintexttag": [[201, 205], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._retrieve_domain_from_plaintexttag"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._retrieve_domain_from_plaintexttag"], ["", "def", "test_retrieve_domain_from_plaintexttag", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"\\nthisisalinktaggoogle\"", "\n", "expected_output", "=", "(", "'\\n'", ",", "'google'", ")", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_retrieve_domain_from_plaintexttag", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_annotate_and_clean_html_finds_and_cleans_tag": [[206, 218], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._annotate_and_clean_html"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_and_clean_html"], ["", "def", "test_annotate_and_clean_html_finds_and_cleans_tag", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'content'", ":", "'thisisah1tag I am a Header.'", "\n", "}", "\n", "expected_output", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'content'", ":", "'I am a Header.'", ",", "\n", "'html_tags'", ":", "[", "'h1'", "]", ",", "\n", "'domains'", ":", "[", "]", ",", "\n", "}", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_annotate_and_clean_html", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_annotate_and_clean_html_for_links_with_domain": [[219, 231], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._annotate_and_clean_html"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_and_clean_html"], ["", "def", "test_annotate_and_clean_html_for_links_with_domain", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'content'", ":", "'thisisalinktaggoogle I am a Header.'", "\n", "}", "\n", "expected_output", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'content'", ":", "'I am a Header.'", ",", "\n", "'html_tags'", ":", "[", "'a'", "]", ",", "\n", "'domains'", ":", "[", "'google'", "]", ",", "\n", "}", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_annotate_and_clean_html", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_annotate_and_clean_html_for_links_with_domain_with_attached_new_line": [[232, 244], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._annotate_and_clean_html"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_and_clean_html"], ["", "def", "test_annotate_and_clean_html_for_links_with_domain_with_attached_new_line", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'content'", ":", "'\\nthisisalinktaggoogle I am a Header.'", "\n", "}", "\n", "expected_output", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'content'", ":", "'\\n I am a Header.'", ",", "\n", "'html_tags'", ":", "[", "'a'", "]", ",", "\n", "'domains'", ":", "[", "'google'", "]", ",", "\n", "}", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_annotate_and_clean_html", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_annotate_and_clean_html_for_multiple_links_with_domain": [[245, 257], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._annotate_and_clean_html"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_and_clean_html"], ["", "def", "test_annotate_and_clean_html_for_multiple_links_with_domain", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'content'", ":", "'thisisalinktaggoogle I am a thisisalinktagmaps Header.'", "\n", "}", "\n", "expected_output", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'content'", ":", "'I am a Header.'", ",", "\n", "'html_tags'", ":", "[", "'a'", "]", ",", "\n", "'domains'", ":", "[", "'google'", ",", "'maps'", "]", ",", "\n", "}", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_annotate_and_clean_html", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_annotate_internal_external_links_internal_link": [[258, 273], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._annotate_internal_external_links"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_internal_external_links"], ["", "def", "test_annotate_internal_external_links_internal_link", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'cat.com'", ",", "\n", "'content'", ":", "'texty text'", ",", "\n", "'domains'", ":", "[", "'cat'", "]", ",", "\n", "}", "\n", "expected_output", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'cat.com'", ",", "\n", "'content'", ":", "'texty text'", ",", "\n", "'domains'", ":", "[", "'cat'", "]", ",", "\n", "'link_type'", ":", "[", "'internal'", "]", ",", "\n", "}", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_annotate_internal_external_links", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_annotate_internal_external_links_external_link": [[274, 289], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._annotate_internal_external_links"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_internal_external_links"], ["", "def", "test_annotate_internal_external_links_external_link", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'cat.com'", ",", "\n", "'content'", ":", "'texty text'", ",", "\n", "'domains'", ":", "[", "'dog'", "]", ",", "\n", "}", "\n", "expected_output", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'cat.com'", ",", "\n", "'content'", ":", "'texty text'", ",", "\n", "'domains'", ":", "[", "'dog'", "]", ",", "\n", "'link_type'", ":", "[", "'external'", "]", ",", "\n", "}", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_annotate_internal_external_links", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_annotate_internal_external_links_no_links": [[290, 305], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._annotate_internal_external_links"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_internal_external_links"], ["", "def", "test_annotate_internal_external_links_no_links", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'cat.com'", ",", "\n", "'content'", ":", "'texty text'", ",", "\n", "'domains'", ":", "[", "]", ",", "\n", "}", "\n", "expected_output", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'cat.com'", ",", "\n", "'content'", ":", "'texty text'", ",", "\n", "'domains'", ":", "[", "]", ",", "\n", "'link_type'", ":", "[", "]", ",", "\n", "}", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_annotate_internal_external_links", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestTransformations.test_annotate_internal_external_links_mixed_links": [[306, 321], ["test_transformations.TestTransformations.assertEqual", "autodiscern.Transformer._annotate_internal_external_links"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer._annotate_internal_external_links"], ["", "def", "test_annotate_internal_external_links_mixed_links", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'cat.com'", ",", "\n", "'content'", ":", "'texty text'", ",", "\n", "'domains'", ":", "[", "'cat'", ",", "'dog'", "]", ",", "\n", "}", "\n", "expected_output", "=", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'cat.com'", ",", "\n", "'content'", ":", "'texty text'", ",", "\n", "'domains'", ":", "[", "'cat'", ",", "'dog'", "]", ",", "\n", "'link_type'", ":", "[", "'internal'", ",", "'external'", "]", ",", "\n", "}", "\n", "self", ".", "assertEqual", "(", "adt", ".", "Transformer", ".", "_annotate_internal_external_links", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.setUp": [[325, 377], ["None"], "methods", ["None"], ["    ", "def", "setUp", "(", "self", ")", ":", "\n", "        ", "self", ".", "test_input_1", "=", "{", "\n", "0", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"\"\"\n                    <div class=\"field-item even\" property=\"content:encoded\"><div id=\"selectedWebpagePart\" contenteditable=\"false\"><div id=\"selectedWebpagePart\" contenteditable=\"false\"><div class=\"mainCol2Col selectedHighlight\">\n                           <div class=\"topleader\">\n                           <div class=\"vsp\"> </div>\n                            <div class=\"leader ad\"><br></div></div><div class=\"mainContent\">\n                                \n                                <div class=\"articleHtml\">\n                    <div class=\"toolbar_ns\" style=\"float:right;margin-top:-3px\">\n                    <table><tbody></tbody></table></div>    \n                    <script>\n                    <!--//--><![CDATA[// ><!--\n                     function createToolbar() {\t \n                        if ('Antidepressants') {\n                            var st=readCookie(\"SAVVYTOPICS\");if (!st || st.indexOf(\"|Antidepressants|\")==-1) {\n                                var desc=st?\"Click here to add <i>Antidepressants to your list of topics.\":\"<strong>Stay up-to-date on the health topics that interest you.<br /><br />Click here to sign in or sign up for HealthSavvy, and add <i>Antidepressants to your list of topics.\";\n                                addToolbarButton(\"HealthSavvy\", \"tb_hsicon tool_sp\", \"#\",  savvyClick, \"HealthSavvy\",\"hs_savvy_favorite\",desc);}\n                        }\n                        addToolbarButton( \"Send this Page\",\"tb_mail tool_sp\", \"#\", function(event) {emailPage(event);return false;}, \"Send Page\",null, \"<strong>Send Using Facebook or Email.<br /><br />Click here to send this page using Facebook or email. You may add a personal message to the email.\");\n                        addToolbarButton( \"Print\",\"tb_print tool_sp\", \"#\", function(event) {printPage(event);return false;}, \"Print Article\",null, \"Click here to print this page.\"); \t   \n                     }\n                     createToolbar();  \n                    \n                    //--><!]]>\n                    </script><h1>Antidepressants</h1>\n                                <div id=\"pageOneHeader\"><div>\n                    <h3>Antidepressants are medications primarily used for treating depression.</h3></div></div></div></div><div>\n                    <a name=\"chapter_0\" href=\"http://depression.emedtv.com/undefined\" id=\"chapter_0\"></a><h2>What Are Antidepressants?</h2></div>\n                                    <div>\n                    Antidepressants are medications used to treat <a href=\"http://depression.emedtv.com/depression/depression.html\" onmouseout=\"hideDescription(event);\" onmouseover=\"showDescription(event, '/depression/depression.html', 'Depression causes unnecessary suffering for both people who have the illness and their families.', 'Depression')\">depression</a>. Some of these medications&nbsp;are blue.</div>\n                    <div>&nbsp;</div>\n                    <div><em>(Click <a title=\"Antidepressant Uses\" href=\"http://depression.emedtv.com/antidepressants/antidepressant-uses.html\" onmouseover=\"showDescription(event, '/antidepressants/antidepressant-uses.html', 'Besides depression treatment, antidepressants are also approved for other uses.', 'Antidepressant Uses')\" onmouseout=\"hideDescription(event);\">Antidepressant Uses</a> for more information on what&nbsp;they are used for, including possible <a href=\"http://drugs.emedtv.com/medicine/off-label.html\" onmouseout=\"hideDescription(event);\" onmouseover=\"showDescription(event, 'http://drugs.emedtv.com/medicine/off-label.html', 'This eMedTV page defines an off-label use as one where a physician prescribes a medication to treat a condition, even though the FDA has not approved the medicine for that specific use.', 'Off-Label')\">off-label</a> uses.)</em></div>\n                    <div>&nbsp;</div>\n                    <div>\n                    <a name=\"chapter_1\" href=\"http://depression.emedtv.com/undefined\" id=\"chapter_1\"></a><h2>Types of Antidepressants</h2></div>\n                                    <div>\n                    There are several types of antidepressants available to treat depression.</div>\n                    <div>&nbsp;</div>\n                    </div></div></div></div>\n                \"\"\"", "\n", "}", "\n", "}", "\n", "\n", "# create starting dict for expected output. add content key in individual tests", "\n", "self", ".", "expected_output", "=", "{", "\n", "0", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.test_html_to_text": [[380, 388], ["autodiscern.Transformer", "autodiscern.Transformer.apply", "test_transformations.TestAcceptanceTransformation.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply"], ["", "def", "test_html_to_text", "(", "self", ")", ":", "\n", "        ", "transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "False", ")", "\n", "\n", "test_input", "=", "self", ".", "test_input_1", "\n", "self", ".", "expected_output", "[", "0", "]", "[", "'content'", "]", "=", "\"\"\"Antidepressants. Antidepressants are medications primarily used for treating depression. What Are Antidepressants? Antidepressants are medications used to treat depression. Some of these medications are blue. (Click Antidepressant Uses for more information on what they are used for, including possible off-label uses.). Types of Antidepressants. There are several types of antidepressants available to treat depression.\"\"\"", "\n", "\n", "output", "=", "transformer", ".", "apply", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "self", ".", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.test_html_to_limited_html": [[389, 396], ["autodiscern.Transformer", "autodiscern.Transformer.apply", "test_transformations.TestAcceptanceTransformation.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply"], ["", "def", "test_html_to_limited_html", "(", "self", ")", ":", "\n", "        ", "transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "True", ")", "\n", "\n", "test_input", "=", "self", ".", "test_input_1", "\n", "self", ".", "expected_output", "[", "0", "]", "[", "'content'", "]", "=", "\"\"\"<h1>Antidepressants</h1> <h3>Antidepressants are medications primarily used for treating depression.</h3> <a href=\"emedtv\"></a><h2>What Are Antidepressants?</h2> Antidepressants are medications used to treat <a href=\"emedtv\">depression</a>. Some of these medications are blue. (Click <a href=\"emedtv\">Antidepressant Uses</a> for more information on what they are used for, including possible <a href=\"emedtv\">off-label</a> uses.) <a href=\"emedtv\"></a><h2>Types of Antidepressants</h2> There are several types of antidepressants available to treat depression.\"\"\"", "\n", "output", "=", "transformer", ".", "apply", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "self", ".", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.test_html_to_limited_html_plain_text": [[397, 404], ["autodiscern.Transformer", "autodiscern.Transformer.apply", "test_transformations.TestAcceptanceTransformation.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply"], ["", "def", "test_html_to_limited_html_plain_text", "(", "self", ")", ":", "\n", "        ", "transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "html_to_plain_text", "=", "True", ")", "\n", "\n", "test_input", "=", "self", ".", "test_input_1", "\n", "self", ".", "expected_output", "[", "0", "]", "[", "'content'", "]", "=", "\"\"\"thisisah1tag Antidepressants. thisisah3tag Antidepressants are medications primarily used for treating depression. thisisalinktagemedtv thisisah2tag What Are Antidepressants? Antidepressants are medications used to treat thisisalinktagemedtv depression . Some of these medications are blue. (Click thisisalinktagemedtv Antidepressant Uses for more information on what they are used for, including possible thisisalinktagemedtv off-label uses.). thisisalinktagemedtv thisisah2tag Types of Antidepressants. There are several types of antidepressants available to treat depression.\"\"\"", "\n", "output", "=", "transformer", ".", "apply", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "self", ".", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.test_html_to_text_to_sentences": [[426, 444], ["autodiscern.Transformer", "autodiscern.Transformer.apply", "test_transformations.TestAcceptanceTransformation.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply"], ["", "def", "test_html_to_text_to_sentences", "(", "self", ")", ":", "\n", "        ", "transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "False", ",", "segment_into", "=", "'sentences'", ",", "\n", "remove_newlines", "=", "False", ")", "\n", "\n", "test_input", "=", "self", ".", "test_input_1", "\n", "self", ".", "expected_output", "[", "0", "]", "[", "'content'", "]", "=", "[", "\n", "\"Antidepressants.\"", ",", "\n", "\"Antidepressants are medications primarily used for treating depression.\"", ",", "\n", "\"What Are Antidepressants?\"", ",", "\n", "\"Antidepressants are medications used to treat depression.\"", ",", "\n", "\"Some of these medications are blue.\"", ",", "\n", "\"(Click Antidepressant Uses for more information on what they are used for, including possible off-label uses.).\"", ",", "\n", "\"Types of Antidepressants.\"", ",", "\n", "\"There are several types of antidepressants available to treat depression.\"", ",", "\n", "]", "\n", "\n", "output", "=", "transformer", ".", "apply", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "self", ".", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.test_html_to_text_to_paragraphs": [[445, 462], ["autodiscern.Transformer", "autodiscern.Transformer.apply", "test_transformations.TestAcceptanceTransformation.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply"], ["", "def", "test_html_to_text_to_paragraphs", "(", "self", ")", ":", "\n", "        ", "transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "False", ",", "segment_into", "=", "'paragraphs'", ",", "\n", "remove_newlines", "=", "False", ")", "\n", "\n", "test_input", "=", "self", ".", "test_input_1", "\n", "self", ".", "expected_output", "[", "0", "]", "[", "'content'", "]", "=", "[", "\n", "\"Antidepressants. \"", ",", "\n", "\"Antidepressants are medications primarily used for treating depression. \"", ",", "\n", "\"What Are Antidepressants? \"", ",", "\n", "\"Antidepressants are medications used to treat depression. Some of these medications are blue. \"", ",", "\n", "\"(Click Antidepressant Uses for more information on what they are used for, including possible off-label uses.). \"", ",", "\n", "\"Types of Antidepressants. \"", ",", "\n", "\"There are several types of antidepressants available to treat depression.\"", ",", "\n", "]", "\n", "\n", "output", "=", "transformer", ".", "apply", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "self", ".", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.test_html_to_limited_html_plain_text_to_sentences": [[465, 483], ["autodiscern.Transformer", "autodiscern.Transformer.apply", "test_transformations.TestAcceptanceTransformation.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply"], ["", "def", "test_html_to_limited_html_plain_text_to_sentences", "(", "self", ")", ":", "\n", "        ", "transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "html_to_plain_text", "=", "True", ",", "segment_into", "=", "'sentences'", ",", "\n", "remove_newlines", "=", "False", ")", "\n", "\n", "test_input", "=", "self", ".", "test_input_1", "\n", "self", ".", "expected_output", "[", "0", "]", "[", "'content'", "]", "=", "[", "\n", "\"thisisah1tag Antidepressants.\"", ",", "\n", "\"thisisah3tag Antidepressants are medications primarily used for treating depression.\"", ",", "\n", "\"thisisalinktagemedtv thisisah2tag What Are Antidepressants?\"", ",", "\n", "\"Antidepressants are medications used to treat thisisalinktagemedtv depression .\"", ",", "\n", "\"Some of these medications are blue.\"", ",", "\n", "\"(Click thisisalinktagemedtv Antidepressant Uses for more information on what they are used for, including possible thisisalinktagemedtv off-label uses.).\"", ",", "\n", "\"thisisalinktagemedtv thisisah2tag Types of Antidepressants.\"", ",", "\n", "\"There are several types of antidepressants available to treat depression.\"", ",", "\n", "]", "\n", "\n", "output", "=", "transformer", ".", "apply", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "self", ".", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.test_html_to_limited_html_plain_text_to_paragraphs": [[484, 501], ["autodiscern.Transformer", "autodiscern.Transformer.apply", "test_transformations.TestAcceptanceTransformation.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply"], ["", "def", "test_html_to_limited_html_plain_text_to_paragraphs", "(", "self", ")", ":", "\n", "        ", "transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "html_to_plain_text", "=", "True", ",", "segment_into", "=", "'paragraphs'", ",", "\n", "remove_newlines", "=", "False", ")", "\n", "\n", "test_input", "=", "self", ".", "test_input_1", "\n", "self", ".", "expected_output", "[", "0", "]", "[", "'content'", "]", "=", "[", "\n", "\"thisisah1tag Antidepressants. \"", ",", "\n", "\"thisisah3tag Antidepressants are medications primarily used for treating depression. \"", ",", "\n", "\"thisisalinktagemedtv thisisah2tag What Are Antidepressants? \"", ",", "\n", "\"Antidepressants are medications used to treat thisisalinktagemedtv depression . Some of these medications are blue. \"", ",", "\n", "\"(Click thisisalinktagemedtv Antidepressant Uses for more information on what they are used for, including possible thisisalinktagemedtv off-label uses.). \"", ",", "\n", "\"thisisalinktagemedtv thisisah2tag Types of Antidepressants. \"", ",", "\n", "\"There are several types of antidepressants available to treat depression.\"", ",", "\n", "]", "\n", "\n", "output", "=", "transformer", ".", "apply", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "self", ".", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.test_html_to_limited_html_plain_text_to_sentences_flattened": [[502, 561], ["autodiscern.Transformer", "autodiscern.Transformer.apply", "list", "expected_output.keys", "test_transformations.TestAcceptanceTransformation.assertDictEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply"], ["", "def", "test_html_to_limited_html_plain_text_to_sentences_flattened", "(", "self", ")", ":", "\n", "        ", "transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "html_to_plain_text", "=", "True", ",", "segment_into", "=", "'sentences'", ",", "\n", "remove_newlines", "=", "False", ",", "flatten", "=", "True", ",", "annotate_html", "=", "False", ")", "\n", "\n", "test_input", "=", "self", ".", "test_input_1", "\n", "expected_output", "=", "{", "\n", "'0-0'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "0", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"thisisah1tag Antidepressants.\"", ",", "\n", "}", ",", "\n", "'0-1'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "1", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"thisisah3tag Antidepressants are medications primarily used for treating depression.\"", ",", "\n", "}", ",", "\n", "'0-2'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "2", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"thisisalinktagemedtv thisisah2tag What Are Antidepressants?\"", ",", "\n", "}", ",", "\n", "'0-3'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "3", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"Antidepressants are medications used to treat thisisalinktagemedtv depression .\"", ",", "\n", "}", ",", "\n", "'0-4'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "4", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"Some of these medications are blue.\"", ",", "\n", "}", ",", "\n", "'0-5'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "5", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"(Click thisisalinktagemedtv Antidepressant Uses for more information on what they are used for, including possible thisisalinktagemedtv off-label uses.).\"", ",", "\n", "}", ",", "\n", "'0-6'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "6", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"thisisalinktagemedtv thisisah2tag Types of Antidepressants.\"", ",", "\n", "}", ",", "\n", "'0-7'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "7", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"There are several types of antidepressants available to treat depression.\"", ",", "\n", "}", ",", "\n", "}", "\n", "\n", "output", "=", "transformer", ".", "apply", "(", "test_input", ")", "\n", "for", "id", "in", "list", "(", "expected_output", ".", "keys", "(", ")", ")", ":", "\n", "            ", "self", ".", "assertDictEqual", "(", "output", "[", "id", "]", ",", "expected_output", "[", "id", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_transformations.TestAcceptanceTransformation.test_html_to_limited_html_plain_text_to_sentences_flattened_annotated": [[562, 645], ["autodiscern.Transformer", "autodiscern.Transformer.apply", "list", "expected_output.keys", "test_transformations.TestAcceptanceTransformation.assertDictEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.transformations.Transformer.apply"], ["", "", "def", "test_html_to_limited_html_plain_text_to_sentences_flattened_annotated", "(", "self", ")", ":", "\n", "        ", "transformer", "=", "adt", ".", "Transformer", "(", "leave_some_html", "=", "True", ",", "html_to_plain_text", "=", "True", ",", "segment_into", "=", "'sentences'", ",", "\n", "remove_newlines", "=", "False", ",", "flatten", "=", "True", ",", "annotate_html", "=", "True", ")", "\n", "\n", "test_input", "=", "self", ".", "test_input_1", "\n", "expected_output", "=", "{", "\n", "'0-0'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "0", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"Antidepressants.\"", ",", "\n", "'html_tags'", ":", "[", "'h1'", "]", ",", "\n", "'domains'", ":", "[", "]", ",", "\n", "'link_type'", ":", "[", "]", ",", "\n", "}", ",", "\n", "'0-1'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "1", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"Antidepressants are medications primarily used for treating depression.\"", ",", "\n", "'html_tags'", ":", "[", "'h3'", "]", ",", "\n", "'domains'", ":", "[", "]", ",", "\n", "'link_type'", ":", "[", "]", ",", "\n", "}", ",", "\n", "'0-2'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "2", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"What Are Antidepressants?\"", ",", "\n", "'html_tags'", ":", "[", "'h2'", ",", "'a'", "]", ",", "\n", "'domains'", ":", "[", "'emedtv'", "]", ",", "\n", "'link_type'", ":", "[", "'internal'", "]", ",", "\n", "}", ",", "\n", "'0-3'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "3", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"Antidepressants are medications used to treat depression .\"", ",", "\n", "'html_tags'", ":", "[", "'a'", "]", ",", "\n", "'domains'", ":", "[", "'emedtv'", "]", ",", "\n", "'link_type'", ":", "[", "'internal'", "]", ",", "\n", "}", ",", "\n", "'0-4'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "4", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"Some of these medications are blue.\"", ",", "\n", "'html_tags'", ":", "[", "]", ",", "\n", "'domains'", ":", "[", "]", ",", "\n", "'link_type'", ":", "[", "]", ",", "\n", "}", ",", "\n", "'0-5'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "5", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"(Click Antidepressant Uses for more information on what they are used for, including possible off-label uses.).\"", ",", "\n", "'html_tags'", ":", "[", "'a'", "]", ",", "\n", "'domains'", ":", "[", "'emedtv'", ",", "'emedtv'", "]", ",", "\n", "'link_type'", ":", "[", "'internal'", ",", "'internal'", "]", ",", "\n", "}", ",", "\n", "'0-6'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "6", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"Types of Antidepressants.\"", ",", "\n", "'html_tags'", ":", "[", "'h2'", ",", "'a'", "]", ",", "\n", "'domains'", ":", "[", "'emedtv'", "]", ",", "\n", "'link_type'", ":", "[", "'internal'", "]", ",", "\n", "}", ",", "\n", "'0-7'", ":", "{", "\n", "'id'", ":", "0", ",", "\n", "'sub_id'", ":", "7", ",", "\n", "'url'", ":", "'http://depression.emedtv.com/antidepressants/antidepressants.html'", ",", "\n", "'content'", ":", "\"There are several types of antidepressants available to treat depression.\"", ",", "\n", "'html_tags'", ":", "[", "]", ",", "\n", "'domains'", ":", "[", "]", ",", "\n", "'link_type'", ":", "[", "]", ",", "\n", "}", ",", "\n", "}", "\n", "\n", "output", "=", "transformer", ".", "apply", "(", "test_input", ")", "\n", "for", "id", "in", "list", "(", "expected_output", ".", "keys", "(", ")", ")", ":", "\n", "            ", "self", ".", "assertDictEqual", "(", "output", "[", "id", "]", ",", "expected_output", "[", "id", "]", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_replace_metamap_with_concept_basic": [[7, 40], ["autodiscern.replace_metamap_content_with_concept_name", "test_annotations.TestAnnotations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_metamap_content_with_concept_name"], ["    ", "def", "test_replace_metamap_with_concept_basic", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'content'", ":", "'\u2022 Changes in appetite that result in weight losses or gains unrelated to dieting.'", ",", "\n", "'metamap'", ":", "[", "'Physiology'", ",", "'Disorders'", "]", ",", "\n", "'metamap_detail'", ":", "[", "\n", "{", "\n", "'index'", ":", "\"'250-6'\"", ",", "\n", "'mm'", ":", "'MMI'", ",", "\n", "'score'", ":", "'5.59'", ",", "\n", "'preferred_name'", ":", "'Desire for food'", ",", "\n", "'cui'", ":", "'C0003618'", ",", "\n", "'semtypes'", ":", "'[orgf]'", ",", "\n", "'trigger'", ":", "'\"Appetite\"-text-0-\"appetite\"-NN-0'", ",", "\n", "'pos_info'", ":", "'14/8'", ",", "\n", "'tree_codes'", ":", "'F02.830.071;G07.203.650.390.070;G10.261.390.070'", "\n", "}", ",", "\n", "{", "\n", "'index'", ":", "\"'250-6'\"", ",", "\n", "'mm'", ":", "'MMI'", ",", "\n", "'score'", ":", "'5.59'", ",", "\n", "'preferred_name'", ":", "'Weight decreased'", ",", "\n", "'cui'", ":", "'C1262477'", ",", "\n", "'semtypes'", ":", "'[fndg]'", ",", "\n", "'trigger'", ":", "'\"Weight Losses\"-text-0-\"weight losses\"-NN-0'", ",", "\n", "'pos_info'", ":", "'38/13'", ",", "\n", "'tree_codes'", ":", "'C23.888.144.243.963;G07.345.249.314.120.200.963'", "\n", "}", "\n", "]", ",", "\n", "}", "\n", "output", "=", "ada", ".", "replace_metamap_content_with_concept_name", "(", "test_input", "[", "'content'", "]", ",", "test_input", "[", "'metamap_detail'", "]", ",", "\n", "test_input", "[", "'metamap'", "]", ")", "\n", "expected_output", "=", "'\u2022 Changes in MMConceptPhysiology that result in MMConceptDisorders or gains unrelated to dieting.'", "\n", "self", ".", "assertEqual", "(", "output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_split_repeated_metamap_concepts": [[41, 70], ["autodiscern.split_repeated_metamap_concepts", "range", "len", "test_annotations.TestAnnotations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.split_repeated_metamap_concepts"], ["", "def", "test_split_repeated_metamap_concepts", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "[", "{", "'index'", ":", "'205'", ",", "\n", "'mm'", ":", "'MMI'", ",", "\n", "'score'", ":", "'26.00'", ",", "\n", "'preferred_name'", ":", "'Brain'", ",", "\n", "'cui'", ":", "'C0006104'", ",", "\n", "'semtypes'", ":", "'[bpoc]'", ",", "\n", "'trigger'", ":", "'\"Brain\"-text-0-\"brain\"-NN-0'", ",", "\n", "'pos_info'", ":", "'109/5;237/5'", ",", "\n", "'tree_codes'", ":", "'A08.186.211'", ",", "\n", "'concept'", ":", "'Anatomy'", "}", "]", "\n", "expected_ouput", "=", "[", "\n", "{", "\n", "'pos_info'", ":", "'109/5'", ",", "\n", "'start_pos'", ":", "109", ",", "\n", "'score'", ":", "'26.00'", ",", "\n", "'concept'", ":", "'Anatomy'", "\n", "}", ",", "\n", "{", "\n", "'pos_info'", ":", "'237/5'", ",", "\n", "'start_pos'", ":", "237", ",", "\n", "'score'", ":", "'26.00'", ",", "\n", "'concept'", ":", "'Anatomy'", "\n", "}", "\n", "]", "\n", "output", "=", "ada", ".", "split_repeated_metamap_concepts", "(", "test_input", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "expected_ouput", ")", ")", ":", "\n", "            ", "for", "key", "in", "expected_ouput", "[", "i", "]", ":", "\n", "                ", "self", ".", "assertEqual", "(", "output", "[", "i", "]", "[", "key", "]", ",", "expected_ouput", "[", "i", "]", "[", "key", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_prune_overlapping_metamap_details_no_overlap": [[71, 80], ["autodiscern.prune_overlapping_metamap_details", "test_annotations.TestAnnotations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.prune_overlapping_metamap_details"], ["", "", "", "def", "test_prune_overlapping_metamap_details_no_overlap", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "[", "\n", "{", "'pos_info'", ":", "'1/2'", ",", "'score'", ":", "1", "}", ",", "\n", "]", "\n", "expected_output", "=", "[", "\n", "{", "'pos_info'", ":", "'1/2'", ",", "'score'", ":", "1", "}", ",", "\n", "]", "\n", "output", "=", "ada", ".", "prune_overlapping_metamap_details", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_prune_overlapping_metamap_details_overlap": [[81, 91], ["autodiscern.prune_overlapping_metamap_details", "test_annotations.TestAnnotations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.prune_overlapping_metamap_details"], ["", "def", "test_prune_overlapping_metamap_details_overlap", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "[", "\n", "{", "'pos_info'", ":", "'5/3'", ",", "'score'", ":", "1", "}", ",", "\n", "{", "'pos_info'", ":", "'6/2'", ",", "'score'", ":", ".5", "}", ",", "\n", "]", "\n", "expected_output", "=", "[", "\n", "{", "'pos_info'", ":", "'5/3'", ",", "'score'", ":", "1", "}", ",", "\n", "]", "\n", "output", "=", "ada", ".", "prune_overlapping_metamap_details", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_prune_overlapping_metamap_details_multiple_same": [[92, 103], ["autodiscern.prune_overlapping_metamap_details", "test_annotations.TestAnnotations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.prune_overlapping_metamap_details"], ["", "def", "test_prune_overlapping_metamap_details_multiple_same", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "[", "\n", "{", "'pos_info'", ":", "'5/3'", ",", "'score'", ":", "1", "}", ",", "\n", "{", "'pos_info'", ":", "'5/3'", ",", "'score'", ":", ".5", "}", ",", "\n", "{", "'pos_info'", ":", "'5/3'", ",", "'score'", ":", ".75", "}", ",", "\n", "]", "\n", "expected_output", "=", "[", "\n", "{", "'pos_info'", ":", "'5/3'", ",", "'score'", ":", "1", "}", ",", "\n", "]", "\n", "output", "=", "ada", ".", "prune_overlapping_metamap_details", "(", "test_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_replace_metamap_content_with_concept_name_overlapping_concepts": [[104, 148], ["autodiscern.replace_metamap_content_with_concept_name", "test_annotations.TestAnnotations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_metamap_content_with_concept_name"], ["", "def", "test_replace_metamap_content_with_concept_name_overlapping_concepts", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "{", "\n", "'content'", ":", "'\u2022 Insomnia or oversleeping.'", ",", "\n", "'metamap'", ":", "[", "'Disorders'", ",", "'Disorders'", ",", "'Disorders'", "]", ",", "\n", "'metamap_detail'", ":", "[", "\n", "{", "\n", "'index'", ":", "\"'250-7'\"", ",", "\n", "'mm'", ":", "'MMI'", ",", "\n", "'score'", ":", "'4.67'", ",", "\n", "'preferred_name'", ":", "'Insomnia, CTCAE 3.0'", ",", "\n", "'cui'", ":", "'C1963237'", ",", "\n", "'semtypes'", ":", "'[fndg]'", ",", "\n", "'trigger'", ":", "'\"Insomnia\"-text-0-\"Insomnia\"-NNP-0'", ",", "\n", "'pos_info'", ":", "'3/8'", ",", "\n", "'tree_codes'", ":", "'C10.886.425.800.800;F03.870.400.800.800'", "\n", "}", ",", "\n", "{", "\n", "'index'", ":", "\"'250-7'\"", ",", "\n", "'mm'", ":", "'MMI'", ",", "\n", "'score'", ":", "'4.67'", ",", "\n", "'preferred_name'", ":", "'Insomnia, CTCAE 5.0'", ",", "\n", "'cui'", ":", "'C4554626'", ",", "\n", "'semtypes'", ":", "'[fndg]'", ",", "\n", "'trigger'", ":", "'\"Insomnia\"-text-0-\"Insomnia\"-NNP-0'", ",", "\n", "'pos_info'", ":", "'3/8'", ",", "\n", "'tree_codes'", ":", "'C10.886.425.800.800;F03.870.400.800.800'", "\n", "}", ",", "\n", "{", "\n", "'index'", ":", "\"'250-7'\"", ",", "\n", "'mm'", ":", "'MMI'", ",", "\n", "'score'", ":", "'4.67'", ",", "\n", "'preferred_name'", ":", "'Sleeplessness'", ",", "\n", "'cui'", ":", "'C0917801'", ",", "\n", "'semtypes'", ":", "'[sosy]'", ",", "\n", "'trigger'", ":", "'\"Insomnia\"-text-0-\"Insomnia\"-NNP-0'", ",", "\n", "'pos_info'", ":", "'3/8'", ",", "\n", "'tree_codes'", ":", "'C10.886.425.800.800;F03.870.400.800.800'", "\n", "}", "\n", "]", ",", "\n", "}", "\n", "output", "=", "ada", ".", "replace_metamap_content_with_concept_name", "(", "test_input", "[", "'content'", "]", ",", "test_input", "[", "'metamap_detail'", "]", ",", "\n", "test_input", "[", "'metamap'", "]", ")", "\n", "expected_output", "=", "'\u2022 MMConceptDisorders or oversleeping.'", "\n", "self", ".", "assertEqual", "(", "output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_replace_links_with_plain_text_no_link": [[149, 153], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.replace_links_with_plain_text"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_links_with_plain_text"], ["", "def", "test_replace_links_with_plain_text_no_link", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"There is no link in this sentence.\"", "\n", "expected_output", "=", "\"There is no link in this sentence.\"", "\n", "self", ".", "assertEqual", "(", "ada", ".", "replace_links_with_plain_text", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_replace_links_with_plain_text_with_link": [[154, 158], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.replace_links_with_plain_text"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.replace_links_with_plain_text"], ["", "def", "test_replace_links_with_plain_text_with_link", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"Go to www.nih.gov to learn more.\"", "\n", "expected_output", "=", "\"Go to thisisalink to learn more.\"", "\n", "self", ".", "assertEqual", "(", "ada", ".", "replace_links_with_plain_text", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_name_and_year_parens": [[159, 163], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_name_and_year_parens", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (Frood, 1942).\"", "\n", "expected_output", "=", "[", "\"(Frood, 1942)\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_name_and_year_square_brackets": [[164, 168], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_name_and_year_square_brackets", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [Frood, 1942].\"", "\n", "expected_output", "=", "[", "\"[Frood, 1942]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_name_et_al_and_year_parens": [[169, 173], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_name_et_al_and_year_parens", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (Frood et al., 1942).\"", "\n", "expected_output", "=", "[", "\"(Frood et al., 1942)\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_multiple_parens": [[174, 178], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_multiple_parens", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (Frood, 1942; Dent, 1944).\"", "\n", "expected_output", "=", "[", "\"(Frood, 1942; Dent, 1944)\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_two_distinct_citations": [[179, 183], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_two_distinct_citations", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (Frood, 1942), (Dent, 1944).\"", "\n", "expected_output", "=", "[", "\"(Frood, 1942)\"", ",", "\"(Dent, 1944)\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_citation_no_year_no_match": [[184, 188], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_citation_no_year_no_match", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (Frood).\"", "\n", "expected_output", "=", "[", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_citation_two_digit_year_no_match": [[189, 193], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_citation_two_digit_year_no_match", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (Frood, 98).\"", "\n", "expected_output", "=", "[", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_citation_no_parens_no_match": [[194, 198], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_citation_no_parens_no_match", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text from Frood, 1942.\"", "\n", "expected_output", "=", "[", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_lone_year_parens": [[199, 203], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_lone_year_parens", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (1942).\"", "\n", "expected_output", "=", "[", "\"(1942)\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_non_year_four_digit_num_parens_no_match": [[204, 208], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_non_year_four_digit_num_parens_no_match", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (1234).\"", "\n", "expected_output", "=", "[", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_multi_numbers_no_match": [[209, 213], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_multi_numbers_no_match", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (1234, 2019).\"", "\n", "expected_output", "=", "[", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_lone_year_square_brackets": [[214, 218], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_lone_year_square_brackets", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [1942].\"", "\n", "expected_output", "=", "[", "\"[1942]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_single_digit_square_brackets": [[219, 223], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_single_digit_square_brackets", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [1].\"", "\n", "expected_output", "=", "[", "\"[1]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_single_digit_parens_no_match": [[224, 228], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_single_digit_parens_no_match", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text (1).\"", "\n", "expected_output", "=", "[", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_double_digit_square_brackets": [[229, 233], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_double_digit_square_brackets", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [42].\"", "\n", "expected_output", "=", "[", "\"[42]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_list_in_square_brackets": [[234, 238], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_list_in_square_brackets", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [1,2].\"", "\n", "expected_output", "=", "[", "\"[1,2]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_list_with_space_in_square_brackets": [[239, 243], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_list_with_space_in_square_brackets", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [1, 2].\"", "\n", "expected_output", "=", "[", "\"[1, 2]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_range_in_square_brackets": [[244, 248], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_range_in_square_brackets", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [1-3].\"", "\n", "expected_output", "=", "[", "\"[1-3]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_range_in_square_brackets_with_spaces": [[249, 253], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_range_in_square_brackets_with_spaces", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [1 - 3].\"", "\n", "expected_output", "=", "[", "\"[1 - 3]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_list_and_range_in_square_brackets": [[254, 258], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_list_and_range_in_square_brackets", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [1,3-5].\"", "\n", "expected_output", "=", "[", "\"[1,3-5]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_list_and_range_with_space_in_square_brackets": [[259, 263], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_list_and_range_with_space_in_square_brackets", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [1, 3-5].\"", "\n", "expected_output", "=", "[", "\"[1, 3-5]\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_two_distinct_citations_different_types": [[264, 268], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_two_distinct_citations_different_types", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"text [1] and text (Frood, 1942).\"", "\n", "expected_output", "=", "[", "\"[1]\"", ",", "\"(Frood, 1942)\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_apply_inline_citation_regex_separate_parens_doesnt_get_roped_in": [[269, 273], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.apply_inline_citation_regex"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.apply_inline_citation_regex"], ["", "def", "test_apply_inline_citation_regex_separate_parens_doesnt_get_roped_in", "(", "self", ")", ":", "\n", "        ", "test_input", "=", "\"(CCBT) ; NICE Technology Appraisal (2006)\"", "\n", "expected_output", "=", "[", "\"(2006)\"", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "apply_inline_citation_regex", "(", "test_input", ")", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_extract_potential_references": [[274, 298], ["autodiscern.extract_potential_references", "test_annotations.TestAnnotations.assertEqual"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.extract_potential_references"], ["", "def", "test_extract_potential_references", "(", "self", ")", ":", "\n", "        ", "example_input", "=", "\"\"\"\n        <p>Thank you, we just sent a survey email to confirm your preferences. </p>\n        <h4 class=references__title>Further reading and references</h4>\n        <i class=\"icon references__toggle\"> \n            <svg role=presentation><use xlink:href=#chevron-down></use></svg>\n        </i>\n        <div class=references__content>\n            <ul class=\"list references__list u-mb\">\n                <li><p><cite><a href=http://www.nice.org.uk/guidance/cg90/chapter/introduction target=_blank rel=noopener>Depression in adults: recognition and management</a></cite>; NICE Clinical Guideline (April 2016)</p></li>\n                <li><p><cite><a href=http://cks.nice.org.uk/depression target=_blank rel=noopener>Depression</a></cite>; NICE CKS, October 2015 (UK access only)</p></li>\n                <li><p><cite><a href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22786489\" target=_blank rel=noopener>Rimer J, Dwan K, Lawlor DA, et al</a></cite>; Exercise for depression. Cochrane Database Syst Rev. 2012 Jul 117:CD004366.</p></li>\n                <li><p><cite><a href=\"http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;dopt=Abstract&amp;list_uids=22674921\" target=_blank rel=noopener>Chalder M, Wiles NJ, Campbell J, et al</a></cite>; Facilitated physical activity as a treatment for depressed adults: randomised BMJ. 2012 Jun 6344:e2758. doi: 10.1136/bmj.e2758.</p></li>\n            </ul>\n        </div>\n        \"\"\"", "\n", "expected_output", "=", "[", "\n", "\"Depression in adults: recognition and management; NICE Clinical Guideline (April 2016)\"", ",", "\n", "\"Depression; NICE CKS, October 2015 (UK access only)\"", ",", "\n", "\"Rimer J, Dwan K, Lawlor DA, et al; Exercise for depression. Cochrane Database Syst Rev. 2012 Jul 117:CD004366.\"", ",", "\n", "\"Chalder M, Wiles NJ, Campbell J, et al; Facilitated physical activity as a treatment for depressed adults: randomised BMJ. 2012 Jun 6344:e2758. doi: 10.1136/bmj.e2758.\"", ",", "\n", "]", "\n", "output", "=", "ada", ".", "extract_potential_references", "(", "example_input", ")", "\n", "self", ".", "assertEqual", "(", "output", ",", "expected_output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.tests.test_annotations.TestAnnotations.test_evaluate_potential_references": [[299, 408], ["test_annotations.TestAnnotations.assertEqual", "autodiscern.evaluate_potential_references"], "methods", ["home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.autodiscern.annotations.evaluate_potential_references"], ["", "def", "test_evaluate_potential_references", "(", "self", ")", ":", "\n", "        ", "example_input", "=", "[", "\n", "(", "'This is not a citation.'", ",", "\n", "[", "\n", "(", "'This'", ",", "'note'", ")", ",", "\n", "(", "'is'", ",", "'note'", ")", ",", "\n", "(", "'not'", ",", "'note'", ")", ",", "\n", "(", "'a'", ",", "'note'", ")", ",", "\n", "(", "'citation.'", ",", "'note'", ")", ",", "\n", "]", ")", ",", "\n", "(", "\"You can do this online at www.mhra.gov.uk/yellowcard.\"", ",", "\n", "[", "\n", "(", "'You'", ",", "'title'", ")", ",", "\n", "(", "'can'", ",", "'title'", ")", ",", "\n", "(", "'do'", ",", "'title'", ")", ",", "\n", "(", "'this'", ",", "'note'", ")", ",", "\n", "(", "'online'", ",", "'note'", ")", ",", "\n", "(", "'at'", ",", "'note'", ")", ",", "\n", "(", "'www.mhra.gov.uk/yellowcard.'", ",", "'note'", ")", ",", "\n", "]", ")", ",", "\n", "(", "\"The national guideline published in 2009 by the National Institute for Health and Care Excellence (NICE) and updated in 2016 advises regular exercise as a possible treatment.\"", ",", "\n", "[", "\n", "(", "'The'", ",", "\n", "'title'", ")", ",", "\n", "(", "'national'", ",", "'title'", ")", ",", "\n", "(", "'guideline'", ",", "'title'", ")", ",", "\n", "(", "'published'", ",", "'title'", ")", ",", "\n", "(", "'in'", ",", "'title'", ")", ",", "\n", "(", "'2009'", ",", "'date'", ")", ",", "\n", "(", "'by'", ",", "'note'", ")", ",", "\n", "(", "'the'", ",", "'note'", ")", ",", "\n", "(", "'National'", ",", "'institution'", ")", ",", "\n", "(", "'Institute'", ",", "'institution'", ")", ",", "\n", "(", "'for'", ",", "'institution'", ")", ",", "\n", "(", "'Health'", ",", "'institution'", ")", ",", "\n", "(", "'and'", ",", "'institution'", ")", ",", "\n", "(", "'Care'", ",", "'title'", ")", ",", "\n", "(", "'Excellence'", ",", "'title'", ")", ",", "\n", "(", "'(NI )'", ",", "'title'", ")", ",", "\n", "(", "'and'", ",", "'title'", ")", ",", "\n", "(", "'updated'", ",", "'title'", ")", ",", "\n", "(", "'in'", ",", "'title'", ")", ",", "\n", "(", "'2016'", ",", "'date'", ")", ",", "\n", "(", "'advises'", ",", "'title'", ")", ",", "\n", "(", "'regular'", ",", "'title'", ")", ",", "\n", "(", "'exercise'", ",", "'title'", ")", ",", "\n", "(", "'as'", ",", "'title'", ")", ",", "\n", "(", "'a'", ",", "'title'", ")", ",", "\n", "(", "'possible'", ",", "'title'", ")", ",", "\n", "(", "'treatment.'", ",", "'title'", ")", ",", "\n", "]", ")", ",", "\n", "(", "\"Chalder M, Wiles NJ, Campbell J, et al; Facilitated physical activity as a treatment for depressed adults: randomised BMJ. 2012 Jun 6344:e2758. doi: 10.1136/bmj.e2758.\"", ",", "\n", "[", "\n", "(", "'Chalder'", ",", "'author'", ")", ",", "\n", "(", "'M,'", ",", "'author'", ")", ",", "\n", "(", "'Wiles'", ",", "'author'", ")", ",", "\n", "(", "'NJ,'", ",", "'author'", ")", ",", "\n", "(", "'Campbell'", ",", "'author'", ")", ",", "\n", "(", "'J,'", ",", "'author'", ")", ",", "\n", "(", "'et'", ",", "'author'", ")", ",", "\n", "(", "'al;'", ",", "'author'", ")", ",", "\n", "(", "'Facilitated'", ",", "'title'", ")", ",", "\n", "(", "'physical'", ",", "'title'", ")", ",", "\n", "(", "'activity'", ",", "'title'", ")", ",", "\n", "(", "'as'", ",", "'title'", ")", ",", "\n", "(", "'a'", ",", "'title'", ")", ",", "\n", "(", "'treatment'", ",", "'title'", ")", ",", "\n", "(", "'for'", ",", "'title'", ")", ",", "\n", "(", "'depressed'", ",", "'title'", ")", ",", "\n", "(", "'adults:'", ",", "'title'", ")", ",", "\n", "(", "'randomised'", ",", "'title'", ")", ",", "\n", "(", "'BMJ.'", ",", "'title'", ")", ",", "\n", "(", "'2012'", ",", "'date'", ")", ",", "\n", "(", "'Jun'", ",", "'date'", ")", ",", "\n", "(", "'6344:e2758.'", ",", "'date'", ")", ",", "\n", "(", "'doi:'", ",", "'date'", ")", ",", "\n", "(", "'10.1136/bmj.e2758.'", ",", "'pages'", ")", ",", "\n", "]", ")", ",", "\n", "]", "\n", "expected_output", "=", "[", "\n", "(", "\"Chalder M, Wiles NJ, Campbell J, et al; Facilitated physical activity as a treatment for depressed adults: randomised BMJ. 2012 Jun 6344:e2758. doi: 10.1136/bmj.e2758.\"", ",", "\n", "[", "\n", "(", "'Chalder'", ",", "'author'", ")", ",", "\n", "(", "'M,'", ",", "'author'", ")", ",", "\n", "(", "'Wiles'", ",", "'author'", ")", ",", "\n", "(", "'NJ,'", ",", "'author'", ")", ",", "\n", "(", "'Campbell'", ",", "'author'", ")", ",", "\n", "(", "'J,'", ",", "'author'", ")", ",", "\n", "(", "'et'", ",", "'author'", ")", ",", "\n", "(", "'al;'", ",", "'author'", ")", ",", "\n", "(", "'Facilitated'", ",", "'title'", ")", ",", "\n", "(", "'physical'", ",", "'title'", ")", ",", "\n", "(", "'activity'", ",", "'title'", ")", ",", "\n", "(", "'as'", ",", "'title'", ")", ",", "\n", "(", "'a'", ",", "'title'", ")", ",", "\n", "(", "'treatment'", ",", "'title'", ")", ",", "\n", "(", "'for'", ",", "'title'", ")", ",", "\n", "(", "'depressed'", ",", "'title'", ")", ",", "\n", "(", "'adults:'", ",", "'title'", ")", ",", "\n", "(", "'randomised'", ",", "'title'", ")", ",", "\n", "(", "'BMJ.'", ",", "'title'", ")", ",", "\n", "(", "'2012'", ",", "'date'", ")", ",", "\n", "(", "'Jun'", ",", "'date'", ")", ",", "\n", "(", "'6344:e2758.'", ",", "'date'", ")", ",", "\n", "(", "'doi:'", ",", "'date'", ")", ",", "\n", "(", "'10.1136/bmj.e2758.'", ",", "'pages'", ")", ",", "\n", "]", ")", ",", "\n", "]", "\n", "self", ".", "assertEqual", "(", "ada", ".", "evaluate_potential_references", "(", "example_input", ")", ",", "expected_output", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.posthoc_analysis.interrater_agreement.classify": [[17, 21], ["None"], "function", ["None"], ["def", "classify", "(", "score", ")", ":", "\n", "    ", "if", "score", ">=", "3", ":", "\n", "        ", "return", "1", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.uzh-dqbm-cmi_auto-discern.posthoc_analysis.interrater_agreement.list_mean": [[87, 89], ["sum", "len"], "function", ["None"], ["", "", "def", "list_mean", "(", "l", ")", ":", "\n", "    ", "return", "sum", "(", "l", ")", "/", "len", "(", "l", ")", "\n", "\n"]]}