{"home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.main.main": [[9, 32], ["torch.set_num_threads", "arguments.get_args", "common.wrappers.make_atari", "common.wrappers.wrap_atari_dqn", "common.utils.set_global_seeds", "common.wrappers.wrap_atari_dqn.seed", "train", "common.wrappers.wrap_atari_dqn.close", "open", "f.write", "test.test", "common.wrappers.wrap_atari_dqn.close", "datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.arguments.get_args", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.make_atari", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.wrap_atari_dqn", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.set_global_seeds", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.train", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test"], ["def", "main", "(", ")", ":", "\n", "    ", "torch", ".", "set_num_threads", "(", "2", ")", "# we need to constrain the number of threads; it can default to a large value", "\n", "args", "=", "get_args", "(", ")", "\n", "\n", "# keep a history of the commands that has been executed", "\n", "with", "open", "(", "\"commandl_history.txt\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y/%m/%d %H:%M:%S\\t\"", ")", "+", "' '", ".", "join", "(", "sys", ".", "argv", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "env", "=", "make_atari", "(", "args", ".", "env", ",", "args", ".", "max_episode_steps", ",", "clip_reward", "=", "(", "not", "(", "args", ".", "no_clip", "or", "args", ".", "transform_Q", ")", ")", "and", "(", "not", "args", ".", "evaluate", ")", ")", "\n", "env", "=", "wrap_atari_dqn", "(", "env", ",", "args", ")", "\n", "\n", "set_global_seeds", "(", "args", ".", "seed", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "if", "args", ".", "evaluate", ":", "\n", "        ", "test", "(", "env", ",", "args", ")", "\n", "env", ".", "close", "(", ")", "\n", "return", "\n", "\n", "", "from", "train", "import", "train", "as", "train", "\n", "train", "(", "env", ",", "args", ")", "\n", "\n", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.arguments.get_args": [[4, 131], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.is_available"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Deep Q-Learning'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--algorithm'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "# Basic Arguments", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Disable CUDA training'", ")", "\n", "\n", "# Training Arguments", "\n", "parser", ".", "add_argument", "(", "'--max-steps'", ",", "type", "=", "int", ",", "default", "=", "50000000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps to train (equal to actual_frames / 4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--buffer-size'", ",", "type", "=", "int", ",", "default", "=", "1000000", ",", "metavar", "=", "'CAPACITY'", ",", "\n", "help", "=", "'Maximum memory buffer size'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-discard-experience'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly discard half of collected experience data'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-replace-memory'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly replace old experiences by new experiences when the memory replay is full (by default it is first-in-first-out)'", ")", "\n", "parser", ".", "add_argument", "(", "'--update-target'", ",", "type", "=", "int", ",", "default", "=", "8000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Interval of target network update'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-freq'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps between optimization steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "metavar", "=", "'\u03b3'", ",", "\n", "help", "=", "'Discount factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning-start'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'How many steps of the model to collect transitions for before learning starts'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-start'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Start value of epsilon'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-mid'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'Mid value of epsilon (at one million-th step)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-final'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'Final value of epsilon'", ")", "\n", "\n", "# Algorithm Arguments", "\n", "parser", ".", "add_argument", "(", "'--double'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Double Q Learning'", ")", "\n", "parser", ".", "add_argument", "(", "'--dueling'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Dueling Network with Default Evaluation (Avg.) on Advantages'", ")", "\n", "parser", ".", "add_argument", "(", "'--prioritized-replay'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable prioritized experience replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.6", ",", "\n", "help", "=", "'Alpha value for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--ratio-min-prio'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Allowed maximal ratio between the smallest sampling priority and the average priority, which equals the maximal importance sampling weight'", ")", "\n", "parser", ".", "add_argument", "(", "'--prio-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-10", ",", "\n", "help", "=", "'A small number added before computing the priority'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-start'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "\n", "help", "=", "'Start value of beta for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-frames'", ",", "type", "=", "float", ",", "default", "=", "50000000", ",", "\n", "help", "=", "'End step of beta schedule for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--IS-weight-only-smaller'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Divide all importance sampling correction weights by the largest weight, so that they are smaller or equal to one'", ")", "\n", "\n", "# Environment Arguments", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "type", "=", "str", ",", "default", "=", "'PongNoFrameskip-v4'", ",", "\n", "help", "=", "'Environment Name'", ")", "\n", "parser", ".", "add_argument", "(", "'--episode-life'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Whether losing one life is considered as an end of an episode(1) or not(0) from the agent's perspective\"", ")", "\n", "parser", ".", "add_argument", "(", "'--grey'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Change the observation to greyscale (default 1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-stack'", ",", "type", "=", "str", ",", "default", "=", "\"4\"", ",", "\n", "help", "=", "'Number of adjacent observations to stack'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-downscale'", ",", "type", "=", "int", ",", "default", "=", "84", ",", "# we will always crop the frame when it has a height of 250 instead of the default 210 (crop by top 28, bottom 12) ", "\n", "help", "=", "'Downscaling ratio of the frame observation (if <= 10) or image size as the downscaling target (if >10)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-episode-steps'", ",", "type", "=", "int", ",", "default", "=", "20000", ",", "\n", "help", "=", "\"The maximum number of steps allowd before resetting the real episode.\"", ")", "\n", "\n", "# Evaluation Arguments", "\n", "parser", ".", "add_argument", "(", "'--load-model'", ",", "type", "=", "str", ",", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "'Pretrained model names to load (state dict)'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Evaluate only'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-trial'", ",", "type", "=", "int", ",", "default", "=", "400", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluation_interval'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "'Steps for printing statistics'", ")", "\n", "\n", "# Optimization Arguments", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "6.25e-5", ",", "metavar", "=", "'\u03b7'", ",", "\n", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--optim'", ",", "type", "=", "str", ",", "default", "=", "'adam'", ",", "\n", "help", "=", "'Optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--adam-eps'", ",", "type", "=", "float", ",", "default", "=", "1.5e-4", ",", "\n", "help", "=", "'Epsilon of adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu-id'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Which GPU to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta2'", ",", "type", "=", "float", ",", "default", "=", "0.999", ")", "\n", "parser", ".", "add_argument", "(", "'--beta1'", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "'--grad-clip'", ",", "type", "=", "float", ",", "default", "=", "10.", ",", "# when transform-Q is used, it should be 40.", "\n", "help", "=", "\"Gradient clipping norm; 0 corresponds to no gradient clipping\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--silent'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--comment'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "\n", "# A simple option to reproduce the original DQN", "\n", "parser", ".", "add_argument", "(", "'--originalDQN'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To reproduce the original DQN\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--save-best'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To save the model when it performs best during training, averaged over 40 episodes\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "originalDQN", ":", "\n", "        ", "del", "args", ".", "originalDQN", "\n", "# the most important arguments for reproducing the original prioritized dueling DDQN", "\n", "args", ".", "algorithm", "=", "\"DQN\"", "\n", "args", ".", "lr", "=", "6.25e-5", "\n", "args", ".", "adam_eps", "=", "1.5e-4", "\n", "args", ".", "grad_clip", "=", "10.", "\n", "args", ".", "prioritized_replay", "=", "True", "\n", "args", ".", "alpha", "=", "0.6", "\n", "args", ".", "beta_start", "=", "0.4", "\n", "args", ".", "beta_frames", "=", "50000000.", "\n", "args", ".", "auto_init", "=", "False", "\n", "args", ".", "gamma", "=", "0.99", "\n", "args", ".", "double", "=", "True", "\n", "args", ".", "dueling", "=", "True", "\n", "args", ".", "episode_life", "=", "1", "\n", "args", ".", "randomly_replace_memory", "=", "False", "\n", "args", ".", "no_clip", "=", "False", "\n", "args", ".", "transform_Q", "=", "False", "\n", "\n", "", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:{}\"", ".", "format", "(", "args", ".", "gpu_id", ")", "if", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "\n", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.train.train": [[16, 181], ["model.DQN().to", "model.DQN().to", "print", "DQN().to.parameters", "common.utils.update_target", "common.utils.epsilon_scheduler", "common.utils.beta_scheduler", "env.unwrapped.ale.lives", "common.utils.print_args", "args.optim.lower", "print", "env.reset", "range", "time.time", "common.replay_buffer.PrioritizedReplayBuffer", "common.replay_buffer.ReplayBuffer", "torch.SGD", "envs.append", "env.seed", "collections.deque", "collections.deque", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "DQN().to.act", "enumerate", "model.DQN", "model.DQN", "parameters", "optimizers.AdamW", "args.optim.startswith", "range", "range", "copy.deepcopy", "random.randrange", "range", "range", "float", "zip", "env.step", "float", "sum", "parameters", "optimizers.AdamBelief", "common.utils.epsilon_scheduler.", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "data_to_store.append", "data_to_store.clear", "length_list.append", "env.reset", "common.utils.beta_scheduler.", "train.compute_td_loss", "loss_list.append", "off_policy_rate_list.append", "parameters", "optimizers.LaProp", "range", "range", "common.replay_buffer.ReplayBuffer.add", "env.unwrapped.ale.game_over", "reward_list.append", "max", "len", "common.utils.print_log", "reward_list.clear", "length_list.clear", "loss_list.clear", "kwargs.values", "time.time", "p.numel", "parameters", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "random.random", "float", "random.randrange", "DQN().to.parameters", "len", "os.path.isdir", "os.mkdir", "open", "f.write", "collections.deque.append", "numpy.mean", "collections.deque.append", "type", "v.clear", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "os.path.join", "torch.save", "torch.save", "torch.save", "DQN().to.state_dict().copy", "numpy.array().reshape", "len", "len", "os.path.join", "DQN().to.state_dict", "numpy.array"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.epsilon_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.beta_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_args", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.compute_td_loss", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.add", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_log"], ["def", "train", "(", "env", ",", "args", ")", ":", "\n", "    ", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "\n", "current_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "target_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "print", "(", "'    Total params: %.2fM'", "%", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "current_model", ".", "parameters", "(", ")", ")", "/", "1000000.0", ")", ")", "\n", "for", "para", "in", "target_model", ".", "parameters", "(", ")", ":", "para", ".", "requires_grad", "=", "False", "\n", "update_target", "(", "current_model", ",", "target_model", ")", "\n", "\n", "epsilon_by_frame", "=", "epsilon_scheduler", "(", "args", ".", "eps_start", ",", "args", ".", "eps_mid", ",", "args", ".", "eps_final", ")", "\n", "beta_by_frame", "=", "beta_scheduler", "(", "args", ".", "beta_start", ",", "args", ".", "beta_frames", ")", "\n", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "replay_buffer", "=", "PrioritizedReplayBuffer", "(", "args", ".", "buffer_size", ",", "args", ".", "alpha", ",", "args", ".", "IS_weight_only_smaller", ",", "allowed_avg_min_ratio", "=", "args", ".", "ratio_min_prio", ")", "\n", "", "else", ":", "\n", "        ", "replay_buffer", "=", "ReplayBuffer", "(", "args", ".", "buffer_size", ")", "\n", "\n", "#args.action_space = env.unwrapped.get_action_meanings()", "\n", "", "args", ".", "init_lives", "=", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "print_args", "(", "args", ")", "\n", "\n", "args", ".", "do_update_target", "=", "False", "\n", "# specify the RL algorithm to use ", "\n", "if", "args", ".", "algorithm", "!=", "\"DQN\"", "and", "args", ".", "algorithm", "!=", "\"Residual\"", "and", "args", ".", "algorithm", "!=", "\"CDQN\"", ":", "\n", "        ", "currentTask", "=", "\"DQN\"", "\n", "args", ".", "currentTask", "=", "currentTask", "\n", "", "else", ":", "\n", "        ", "currentTask", "=", "args", ".", "algorithm", "\n", "args", ".", "currentTask", "=", "args", ".", "algorithm", "\n", "\n", "# prepare the optimizer", "\n", "", "lr", "=", "args", ".", "lr", "\n", "beta1", "=", "args", ".", "beta1", "\n", "beta2", "=", "args", ".", "beta2", "\n", "parameters", "=", "current_model", ".", "parameters", "\n", "args", ".", "optim", "=", "args", ".", "optim", ".", "lower", "(", ")", "\n", "if", "args", ".", "optim", "==", "'sgd'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "SGD", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "momentum", "=", "beta1", ")", "\n", "", "elif", "args", ".", "optim", "==", "'adam'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "AdamW", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", ".", "startswith", "(", "\"adamb\"", ")", ":", "\n", "        ", "args", ".", "optim", "=", "\"adambelief\"", "\n", "optimizer", "=", "optimizers", ".", "AdamBelief", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", "==", "'laprop'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "LaProp", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "\"The specified optimizer name {} is non-existent\"", ".", "format", "(", "args", ".", "optim", ")", "\n", "\n", "", "print", "(", "currentTask", ")", "\n", "\n", "\n", "reward_list", ",", "length_list", ",", "loss_list", ",", "off_policy_rate_list", ",", "gen_loss_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "clip_reward", "=", "True", "###", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "# the number of parallelized computation is maximally \"arg.train_freq\" to guarantee that the computation order is still consistent with the original method", "\n", "num_task", "=", "args", ".", "train_freq", "\n", "args", ".", "num_task", "=", "num_task", "\n", "episode_rewards", "=", "[", "0.", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "life_lengths", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "envs", "=", "[", "env", "]", "\n", "for", "_i", "in", "range", "(", "num_task", "-", "1", ")", ":", "envs", ".", "append", "(", "copy", ".", "deepcopy", "(", "env", ")", ")", "\n", "for", "env", "in", "envs", ":", "env", ".", "seed", "(", "random", ".", "randrange", "(", "1000000", ")", ")", "\n", "states", "=", "[", "state", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "rewards", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "evaluation_interval", "=", "args", ".", "evaluation_interval", "\n", "data_to_store", "=", "[", "]", "\n", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "prev_step", "=", "0", "\n", "step_idx", "=", "1", "# initialization of step_idx", "\n", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "if", "args", ".", "save_best", ":", "\n", "        ", "recent_performances", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "40", ")", "\n", "recent_models", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "20", ")", "\n", "best_performance", "=", "-", "float", "(", "\"inf\"", ")", "\n", "", "while", "step_idx", "<=", "args", ".", "max_steps", ":", "\n", "# decide the action", "\n", "        ", "epsilons", "=", "(", "epsilon_by_frame", "(", "idx", ")", "for", "idx", "in", "range", "(", "step_idx", ",", "step_idx", "+", "num_task", ")", ")", "if", "step_idx", ">", "args", ".", "learning_start", "else", "(", "1.", "for", "idx", "in", "range", "(", "num_task", ")", ")", "\n", "\n", "tensored_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "state", ".", "_frames", "for", "state", "in", "states", "]", ")", ".", "reshape", "(", "(", "num_task", ",", "-", "1", ")", "+", "image_shape", ")", ")", ".", "to", "(", "args", ".", "device", ")", ".", "float", "(", ")", ".", "div_", "(", "255.", ")", "\n", "actions", ",", "evaluateds", ",", "(", "Qss", ",", "bestActions", ")", "=", "current_model", ".", "act", "(", "tensored_states", ",", "epsilons", ")", "\n", "\n", "for", "_i", ",", "(", "env", ",", "state", ",", "action", ",", "Qs", ",", "bestAction", ",", "reward", ")", "in", "enumerate", "(", "zip", "(", "envs", ",", "states", ",", "actions", ",", "Qss", ",", "bestActions", ",", "rewards", ")", ")", ":", "\n", "\n", "# the environment proceeds by one step (4 frames)", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "if", "clip_reward", ":", "\n", "                ", "raw_reward", ",", "reward", "=", "reward", "\n", "", "else", ":", "\n", "                ", "raw_reward", "=", "reward", "\n", "\n", "", "rewards", "[", "_i", "]", "=", "float", "(", "reward", ")", "\n", "\n", "episode_rewards", "[", "_i", "]", "+=", "raw_reward", "\n", "life_lengths", "[", "_i", "]", "+=", "1", "\n", "\n", "# store the transition into the memory replay", "\n", "if", "not", "args", ".", "randomly_discard_experience", "or", "(", "args", ".", "randomly_discard_experience", "and", "random", ".", "random", "(", ")", ">=", "0.5", ")", ":", "# the data may be randomly discarded", "\n", "                ", "data_to_store", ".", "append", "(", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "float", "(", "done", ")", ")", ")", "\n", "", "if", "data_to_store", ":", "\n", "                ", "for", "data", "in", "data_to_store", ":", "\n", "                    ", "replay_buffer", ".", "add", "(", "*", "data", ")", "\n", "if", "args", ".", "randomly_replace_memory", "and", "len", "(", "replay_buffer", ")", ">=", "args", ".", "buffer_size", ":", "\n", "# probably randomly choose an index to replace ", "\n", "                        ", "replay_buffer", ".", "_next_idx", "=", "random", ".", "randrange", "(", "args", ".", "buffer_size", ")", "\n", "", "", "data_to_store", ".", "clear", "(", ")", "\n", "\n", "# record the performance of a trajectory", "\n", "", "if", "done", ":", "\n", "                ", "length_list", ".", "append", "(", "life_lengths", "[", "_i", "]", ")", "\n", "life_lengths", "[", "_i", "]", "=", "0", "\n", "# only the reward of a real full episode is recorded ", "\n", "if", "env", ".", "unwrapped", ".", "ale", ".", "game_over", "(", ")", "or", "\"TimeLimit.truncated\"", "in", "info", ":", "\n", "                    ", "reward_list", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "if", "not", "args", ".", "silent", ":", "\n", "                        ", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "env", ")", ":", "os", ".", "mkdir", "(", "args", ".", "env", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.txt'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ",", "'a'", ")", "as", "f", ":", "\n", "                            ", "f", ".", "write", "(", "'{:.0f}\\t{}\\n'", ".", "format", "(", "step_idx", "*", "4", ",", "episode_rewards", "[", "_i", "]", ")", ")", "\n", "", "if", "args", ".", "save_best", "and", "step_idx", ">", "args", ".", "learning_start", ":", "\n", "                            ", "recent_performances", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "mean_performance", "=", "np", ".", "mean", "(", "recent_performances", ")", "\n", "if", "best_performance", "<", "mean_performance", "and", "len", "(", "recent_performances", ")", ">=", "40", ":", "\n", "                                ", "assert", "len", "(", "recent_models", ")", "==", "20", "\n", "best_performance", "=", "mean_performance", "\n", "torch", ".", "save", "(", "(", "recent_models", "[", "0", "]", ",", "step_idx", ")", ",", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.pth'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ")", "\n", "", "recent_models", ".", "append", "(", "current_model", ".", "state_dict", "(", ")", ".", "copy", "(", ")", ")", "\n", "", "", "episode_rewards", "[", "_i", "]", "=", "0.", "\n", "", "states", "[", "_i", "]", "=", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "[", "_i", "]", "=", "next_state", "\n", "\n", "# optimize", "\n", "", "if", "step_idx", "%", "args", ".", "train_freq", "==", "0", "and", "step_idx", ">", "max", "(", "args", ".", "learning_start", ",", "2", "*", "args", ".", "batch_size", ")", ":", "\n", "                ", "beta", "=", "beta_by_frame", "(", "step_idx", ")", "\n", "loss", ",", "off_policy_rate", "=", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", ")", "\n", "loss_list", ".", "append", "(", "loss", ")", ";", "off_policy_rate_list", ".", "append", "(", "off_policy_rate", ")", "\n", "\n", "# update the target network", "\n", "", "if", "step_idx", "%", "args", ".", "update_target", "==", "0", "and", "currentTask", "!=", "\"Residual\"", ":", "\n", "# we defer the update of the target network to the optimization routine to ensure that the target network is not exactly equal to current network", "\n", "                ", "args", ".", "do_update_target", "=", "True", "\n", "#update_target(current_model, target_model)", "\n", "\n", "# print the statistics", "\n", "", "if", "step_idx", "%", "evaluation_interval", "==", "0", ":", "\n", "# it works only if there is at least one episode to report; otherwise \"evaluation_interval\" is increased", "\n", "                ", "if", "len", "(", "reward_list", ")", ">", "0", ":", "\n", "                    ", "kwargs", "=", "{", "}", "\n", "kwargs", "[", "\"Off-Policy\"", "]", "=", "off_policy_rate_list", "\n", "print_log", "(", "step_idx", ",", "prev_step", ",", "prev_time", ",", "reward_list", ",", "length_list", ",", "loss_list", ",", "args", ",", "'{}{:.0e}{}'", ".", "format", "(", "currentTask", ",", "args", ".", "lr", ",", "args", ".", "comment", ")", ",", "**", "kwargs", ")", "\n", "reward_list", ".", "clear", "(", ")", ";", "length_list", ".", "clear", "(", ")", ";", "loss_list", ".", "clear", "(", ")", "\n", "for", "v", "in", "kwargs", ".", "values", "(", ")", ":", "\n", "                        ", "if", "type", "(", "v", ")", "==", "list", ":", "v", ".", "clear", "(", ")", "\n", "", "prev_step", "=", "step_idx", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "", "else", ":", "\n", "                    ", "evaluation_interval", "+=", "args", ".", "evaluation_interval", "\n", "\n", "", "", "step_idx", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.train.compute_td_loss": [[185, 337], ["torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "optimizer.zero_grad", "optimizer.step", "numpy.mean", "math.ceil", "replay_buffer.sample", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "replay_buffer.sample", "torch.ones", "torch.ones", "torch.ones", "weights.to.numpy", "weights.to.to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.addcmul", "torch.addcmul", "torch.addcmul", "current_model", "current_model.gather().squeeze", "torch.mse_loss", "F.mse_loss.backward", "list", "torch.from_numpy().to().float().div_.view", "current_model", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "list", "torch.from_numpy().to().float().div_.view", "current_model", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "replay_buffer.update_priorities", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "common.utils.update_target", "print", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().to().float().div_.size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "current_model.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "target_model", "numpy.where", "target_mask.astype.astype", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "torch.from_numpy().to().float().div_.size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "current_model.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "current_model.parameters", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model", "[].unsqueeze", "target_model", "target_model.gather().squeeze", "next_q_action.squeeze.squeeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "target_model().max", "current_model.gather", "numpy.abs", "target_model.gather().squeeze().cpu().numpy", "numpy.max", "numpy.abs", "numpy.abs", "numpy.abs", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "numpy.abs", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "math.sqrt", "open", "f.write", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "numpy.stack", "torch.from_numpy().to.unsqueeze", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "target_model.cpu().numpy", "torch.cat", "torch.cat", "torch.cat", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "torch.cat", "torch.cat", "torch.cat", "os.path.join", "target_model.gather", "target_model", "torch.from_numpy().to.unsqueeze", "target_model.gather().squeeze().cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().to.unsqueeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.argmax", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model.max", "torch.stack", "torch.stack", "torch.stack", "target_model.cpu", "numpy.concatenate", "torch.stack", "torch.stack", "torch.stack", "numpy.concatenate", "current_model.detach().cpu().numpy", "q_values.gather().squeeze.detach", "torch.mse_loss", "target_model.gather().squeeze", "current_model.detach().cpu", "target_model.gather", "next_q_action.squeeze.unsqueeze", "current_model.detach"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.update_priorities", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["def", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Calculate loss and optimize\n    \"\"\"", "\n", "global", "i_count", ",", "accu1", ",", "accu2", ",", "accu_loss", "\n", "\n", "# sample data", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "state_next_state", ",", "action_", ",", "reward_", ",", "done", ",", "weights_", ",", "true_weights", ",", "indices", "=", "replay_buffer", ".", "sample", "(", "args", ".", "batch_size", ",", "beta", ")", "\n", "weights", "=", "torch", ".", "from_numpy", "(", "weights_", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "state_next_state", ",", "action_", ",", "reward_", ",", "done", ",", "indices", "=", "replay_buffer", ".", "sample", "(", "args", ".", "batch_size", ")", "\n", "weights", "=", "torch", ".", "ones", "(", "args", ".", "batch_size", ")", ";", "weights_", "=", "weights", ".", "numpy", "(", ")", ";", "true_weights", "=", "weights_", "\n", "weights", "=", "weights", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "# we move data to GPU in chunks", "\n", "", "state_next_state", "=", "torch", ".", "from_numpy", "(", "state_next_state", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", ".", "float", "(", ")", ".", "div_", "(", "255", ")", "\n", "state", ",", "next_state", "=", "state_next_state", "\n", "action", "=", "torch", ".", "from_numpy", "(", "action_", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "gamma_mul_one_minus_done_", "=", "(", "args", ".", "gamma", "*", "(", "1.", "-", "done", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "if", "args", ".", "currentTask", "==", "\"DQN\"", ":", "\n", "# in some cases these data do not really need to be copied to GPU ", "\n", "        ", "reward", ",", "gamma_mul_one_minus_done", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "(", "reward_", ",", "gamma_mul_one_minus_done_", ")", ")", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "##### start training ##### ", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "# we use \"values\" to refer to Q values for all state-actions, and use \"value\" to refer to Q values for states", "\n", "if", "args", ".", "currentTask", "==", "\"DQN\"", ":", "\n", "        ", "if", "args", ".", "double", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q_values", "=", "current_model", "(", "next_state", ")", "\n", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "1", ")", "# **unsqueeze", "\n", "target_next_q_values", "=", "target_model", "(", "next_state", ")", "\n", "next_q_value", "=", "target_next_q_values", ".", "gather", "(", "1", ",", "next_q_action", ")", ".", "squeeze", "(", ")", "\n", "next_q_action", "=", "next_q_action", ".", "squeeze", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q_value", ",", "next_q_action", "=", "target_model", "(", "next_state", ")", ".", "max", "(", "1", ")", "\n", "\n", "", "", "expected_q_value", "=", "torch", ".", "addcmul", "(", "reward", ",", "tensor1", "=", "next_q_value", ",", "tensor2", "=", "gamma_mul_one_minus_done", ")", "\n", "q_values", "=", "current_model", "(", "state", ")", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "loss", "=", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ",", "reduction", "=", "'none'", ")", "\n", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "diff", "=", "(", "q_value", ".", "detach", "(", ")", "-", "expected_q_value", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "prios", "=", "np", ".", "abs", "(", "diff", ")", "+", "args", ".", "prio_eps", "#", "\n", "", "loss", "=", "(", "loss", "*", "weights", ")", ".", "mean", "(", ")", "/", "2.", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# we report the mean squared error instead of the Huber loss as the loss", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "report_loss", "=", "(", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ",", "reduction", "=", "'none'", ")", "*", "weights", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "", "if", "args", ".", "currentTask", "==", "\"CDQN\"", ":", "\n", "# compute the current and next state values in a single pass ", "\n", "        ", "size", "=", "list", "(", "state_next_state", ".", "size", "(", ")", ")", "\n", "current_and_next_states", "=", "state_next_state", ".", "view", "(", "[", "-", "1", "]", "+", "size", "[", "2", ":", "]", ")", "\n", "# compute the q values and the gradient", "\n", "all_q_values", "=", "current_model", "(", "current_and_next_states", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_values", ",", "next_q_values", "=", "all_q_values", "[", ":", "args", ".", "batch_size", "]", ",", "all_q_values", "[", "args", ".", "batch_size", ":", "2", "*", "args", ".", "batch_size", "]", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", ",", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "\n", "q_value", ",", "next_q_value", "=", "torch", ".", "stack", "(", "(", "q_value", ",", "next_q_value", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "next_q_values_target", "=", "target_model", "(", "next_state", ")", "\n", "if", "args", ".", "double", ":", "\n", "                ", "next_q_value_target", "=", "next_q_values_target", ".", "gather", "(", "1", ",", "next_q_action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "next_q_value_target", "=", "np", ".", "max", "(", "next_q_values_target", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "expected_q_value_self", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value", "\n", "expected_q_value_target", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value_target", "\n", "target_mask", "=", "(", "np", ".", "abs", "(", "q_value", "-", "expected_q_value_target", ")", ">=", "np", ".", "abs", "(", "q_value", "-", "expected_q_value_self", ")", ")", "\n", "expected_q_value", "=", "np", ".", "where", "(", "target_mask", ",", "expected_q_value_target", ",", "expected_q_value_self", ")", "\n", "target_mask", "=", "target_mask", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "", "diff", "=", "q_value", "-", "expected_q_value", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "prio_diff", "=", "diff", "\n", "prios", "=", "np", ".", "abs", "(", "prio_diff", ")", "+", "args", ".", "prio_eps", "\n", "# the Huber loss is used ", "\n", "", "weighted_diff", "=", "weights_", "*", "diff", "\n", "q_value_grad", "=", "1.", "/", "args", ".", "batch_size", "*", "weighted_diff", "\n", "\n", "all_grads", "=", "torch", ".", "zeros_like", "(", "all_q_values", ")", "\n", "# manually backpropagate the gradient through the term \"expected_q_value\" ", "\n", "next_q_value_grad", "=", "-", "(", "1.", "-", "target_mask", ")", "*", "q_value_grad", "\n", "next_q_value_grad", "=", "next_q_value_grad", "*", "gamma_mul_one_minus_done_", "\n", "grads", "=", "torch", ".", "from_numpy", "(", "np", ".", "concatenate", "(", "[", "q_value_grad", ",", "next_q_value_grad", "]", ",", "axis", "=", "0", ")", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "all_grads", ".", "scatter_", "(", "1", ",", "torch", ".", "cat", "(", "[", "action", ",", "next_q_action", "]", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "1", ")", ",", "grads", ")", "\n", "all_q_values", ".", "backward", "(", "all_grads", ")", "# this method makes it run faster ", "\n", "\n", "report_loss", "=", "np", ".", "dot", "(", "diff", ",", "weights_", "*", "diff", ")", "/", "args", ".", "batch_size", "\n", "\n", "", "if", "args", ".", "currentTask", "==", "\"Residual\"", ":", "\n", "# compute the current and next state values in a single pass ", "\n", "        ", "size", "=", "list", "(", "state_next_state", ".", "size", "(", ")", ")", "\n", "current_and_next_states", "=", "state_next_state", ".", "view", "(", "[", "-", "1", "]", "+", "size", "[", "2", ":", "]", ")", "\n", "# compute the q values and the gradient", "\n", "all_q_values", "=", "current_model", "(", "current_and_next_states", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_values", ",", "next_q_values", "=", "all_q_values", "[", ":", "args", ".", "batch_size", "]", ",", "all_q_values", "[", "args", ".", "batch_size", ":", "2", "*", "args", ".", "batch_size", "]", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", ",", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "\n", "q_value", ",", "next_q_value", "=", "torch", ".", "stack", "(", "(", "q_value", ",", "next_q_value", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "expected_q_value", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value", "\n", "\n", "# then compute the q values and the loss", "\n", "", "diff", "=", "q_value", "-", "expected_q_value", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "prio_diff", "=", "diff", "\n", "prios", "=", "np", ".", "abs", "(", "prio_diff", ")", "+", "args", ".", "prio_eps", "\n", "# the Huber loss is used ", "\n", "", "weighted_diff", "=", "weights_", "*", "diff", "\n", "q_value_grad", "=", "1.", "/", "args", ".", "batch_size", "*", "weighted_diff", "\n", "\n", "all_grads", "=", "torch", ".", "zeros_like", "(", "all_q_values", ")", "\n", "# manually backpropagate the gradient through the term \"expected_q_value\" ", "\n", "next_q_value_grad", "=", "-", "q_value_grad", "\n", "next_q_value_grad", "=", "next_q_value_grad", "*", "gamma_mul_one_minus_done_", "\n", "grads", "=", "torch", ".", "from_numpy", "(", "np", ".", "concatenate", "(", "[", "q_value_grad", ",", "next_q_value_grad", "]", ",", "axis", "=", "0", ")", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "all_grads", ".", "scatter_", "(", "1", ",", "torch", ".", "cat", "(", "[", "action", ",", "next_q_action", "]", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "1", ")", ",", "grads", ")", "\n", "all_q_values", ".", "backward", "(", "all_grads", ")", "# this method makes it run faster ", "\n", "\n", "report_loss", "=", "np", ".", "dot", "(", "diff", ",", "weights_", "*", "diff", ")", "/", "args", ".", "batch_size", "\n", "\n", "", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "replay_buffer", ".", "update_priorities", "(", "indices", ",", "prios", ")", "\n", "# gradient clipping ", "\n", "", "if", "args", ".", "grad_clip", ">", "0.", ":", "\n", "        ", "grad_norm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "current_model", ".", "parameters", "(", ")", ",", "max_norm", "=", "args", ".", "grad_clip", ")", "\n", "accu1", "+=", "grad_norm", "\n", "accu2", "+=", "grad_norm", "**", "2", "\n", "", "if", "args", ".", "do_update_target", ":", "update_target", "(", "current_model", ",", "target_model", ")", ";", "args", ".", "do_update_target", "=", "False", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "off_policy_rate", "=", "np", ".", "mean", "(", "(", "np", ".", "argmax", "(", "q_values", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", "!=", "action_", ")", ".", "astype", "(", "np", ".", "float", ")", "*", "true_weights", ")", "\n", "\n", "i_count", "+=", "1", "\n", "accu_loss", "+=", "report_loss", "\n", "report_period", "=", "math", ".", "ceil", "(", "args", ".", "evaluation_interval", "/", "args", ".", "train_freq", ")", "\n", "if", "i_count", "%", "report_period", "==", "0", "and", "accu1", "!=", "0.", ":", "\n", "        ", "print", "(", "\"gradient norm {:.3f} +- {:.3f}\"", ".", "format", "(", "accu1", "/", "report_period", ",", "math", ".", "sqrt", "(", "accu2", "/", "report_period", "-", "(", "accu1", "/", "report_period", ")", "**", "2", ")", ")", ")", "\n", "accu1", ",", "accu2", "=", "0.", ",", "0.", "\n", "if", "not", "args", ".", "silent", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}mse_{}.txt'", ".", "format", "(", "args", ".", "currentTask", ",", "args", ".", "comment", ")", ")", ",", "'a'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'{:.0f}\\t{}\\n'", ".", "format", "(", "(", "i_count", "*", "args", ".", "train_freq", "+", "args", ".", "learning_start", ")", "*", "4", ",", "accu_loss", "/", "report_period", ")", ")", "\n", "", "", "accu_loss", "=", "0.", "\n", "\n", "", "return", "report_loss", ",", "off_policy_rate", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.test.test": [[10, 30], ["model.DQN().to", "common.utils.epsilon_scheduler", "torch.load", "torch.load", "print", "model_dict.pop", "DQN().to.load_state_dict", "test.test_whole", "results.append", "len", "model.DQN", "os.path.join", "common.utils.epsilon_scheduler.", "open", "data_f.write", "open", "data_f.write", "print", "numpy.mean", "numpy.mean", "numpy.std", "math.sqrt", "numpy.std", "math.sqrt", "len", "len"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.epsilon_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test_whole", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN"], ["def", "test", "(", "env", ",", "args", ")", ":", "\n", "    ", "current_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "epsilon_by_frame", "=", "epsilon_scheduler", "(", "args", ".", "eps_start", ",", "args", ".", "eps_mid", ",", "args", ".", "eps_final", ")", "\n", "#current_model.eval()", "\n", "\n", "results", "=", "[", "]", "\n", "for", "filename", "in", "args", ".", "load_model", ":", "\n", "        ", "model_dict", ",", "step_idx", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}.pth'", ".", "format", "(", "filename", ")", ")", ",", "map_location", "=", "args", ".", "device", ")", "\n", "print", "(", "\"load {} at training step {}\"", ".", "format", "(", "filename", ",", "step_idx", ")", ")", "\n", "model_dict", ".", "pop", "(", "\"scale\"", ",", "None", ")", "\n", "current_model", ".", "load_state_dict", "(", "model_dict", ")", "\n", "\n", "mean", ",", "std", "=", "test_whole", "(", "current_model", ",", "env", ",", "args", ",", "filename", ",", "epsilon_by_frame", "(", "step_idx", ")", ",", "num_trial", "=", "args", ".", "num_trial", ")", "#0.01)", "\n", "results", ".", "append", "(", "mean", ")", "\n", "with", "open", "(", "'{}.txt'", ".", "format", "(", "args", ".", "env", ")", ",", "'a'", ")", "as", "data_f", ":", "\n", "            ", "data_f", ".", "write", "(", "'{}: {} +- {}\\n'", ".", "format", "(", "filename", ",", "mean", ",", "std", ")", ")", "\n", "", "", "if", "len", "(", "args", ".", "load_model", ")", ">", "1", ":", "\n", "        ", "with", "open", "(", "'{}.txt'", ".", "format", "(", "args", ".", "env", ")", ",", "'a'", ")", "as", "data_f", ":", "\n", "            ", "data_f", ".", "write", "(", "'{} +- {}\\n'", ".", "format", "(", "np", ".", "mean", "(", "results", ")", ",", "np", ".", "std", "(", "results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "results", ")", ")", ")", ")", "\n", "print", "(", "'{} +- {}'", ".", "format", "(", "np", ".", "mean", "(", "results", ")", ",", "np", ".", "std", "(", "results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "results", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.test.test_whole": [[31, 79], ["min", "range", "len", "numpy.mean", "numpy.mean", "print", "envs.append", "env.seed", "env.reset", "len", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "current_model.act", "enumerate", "reversed", "mark_remove.clear", "numpy.std", "math.sqrt", "copy.deepcopy", "random.randrange", "range", "range", "zip", "env.step", "envs.pop", "states.pop", "episode_lengths.pop", "episode_rewards.pop", "len", "range", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "env.reset", "len", "env.unwrapped.ale.game_over", "reward_results.append", "length_results.append", "torch.from_numpy().to", "torch.from_numpy().to", "mark_remove.append", "torch.from_numpy", "torch.from_numpy", "numpy.array().reshape", "numpy.array", "len"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "", "", "def", "test_whole", "(", "current_model", ",", "env", ",", "args", ",", "idx", ",", "epsilon", ",", "num_parallel", "=", "16", ",", "num_trial", "=", "400", ")", ":", "#16", "\n", "    ", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "envs", "=", "[", "env", "]", "\n", "num_parallel", "=", "min", "(", "num_parallel", ",", "num_trial", ")", "\n", "for", "_i", "in", "range", "(", "num_parallel", "-", "1", ")", ":", "envs", ".", "append", "(", "copy", ".", "deepcopy", "(", "env", ")", ")", "\n", "for", "env", "in", "envs", ":", "env", ".", "seed", "(", "random", ".", "randrange", "(", "1000000", ")", ")", "\n", "states", "=", "[", "env", ".", "reset", "(", ")", "for", "env", "in", "envs", "]", "\n", "episode_rewards", "=", "[", "0.", "for", "_i", "in", "range", "(", "num_parallel", ")", "]", "\n", "episode_lengths", "=", "[", "0", "for", "_i", "in", "range", "(", "num_parallel", ")", "]", "\n", "reward_results", "=", "[", "]", "\n", "length_results", "=", "[", "]", "\n", "\n", "trial", "=", "len", "(", "states", ")", "\n", "mark_remove", "=", "[", "]", "\n", "while", "len", "(", "envs", ")", ">", "0", ":", "\n", "# decide the action", "\n", "        ", "epsilons", "=", "(", "epsilon", "for", "_", "in", "range", "(", "len", "(", "states", ")", ")", ")", "\n", "\n", "tensored_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "state", ".", "_frames", "for", "state", "in", "states", "]", ")", ".", "reshape", "(", "(", "len", "(", "states", ")", ",", "-", "1", ")", "+", "image_shape", ")", ")", ".", "to", "(", "args", ".", "device", ")", ".", "float", "(", ")", ".", "div_", "(", "255.", ")", "\n", "actions", ",", "evaluateds", ",", "(", "Qss", ",", "bestActions", ")", "=", "current_model", ".", "act", "(", "tensored_states", ",", "epsilons", ")", "\n", "for", "_i", ",", "(", "env", ",", "action", ")", "in", "enumerate", "(", "zip", "(", "envs", ",", "actions", ")", ")", ":", "\n", "\n", "# the environment proceeds by one step (4 frames)", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "episode_rewards", "[", "_i", "]", "+=", "reward", "\n", "episode_lengths", "[", "_i", "]", "+=", "1", "\n", "\n", "if", "done", ":", "\n", "                ", "if", "env", ".", "unwrapped", ".", "ale", ".", "game_over", "(", ")", "or", "\"TimeLimit.truncated\"", "in", "info", ":", "\n", "                    ", "reward_results", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", ";", "length_results", ".", "append", "(", "episode_lengths", "[", "_i", "]", ")", "\n", "episode_rewards", "[", "_i", "]", "=", "0.", ";", "episode_lengths", "[", "_i", "]", "=", "0", "\n", "if", "trial", "<", "num_trial", ":", "\n", "                        ", "trial", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "mark_remove", ".", "append", "(", "_i", ")", "\n", "", "", "states", "[", "_i", "]", "=", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "[", "_i", "]", "=", "next_state", "\n", "", "", "for", "_i", "in", "reversed", "(", "mark_remove", ")", ":", "\n", "            ", "envs", ".", "pop", "(", "_i", ")", ";", "states", ".", "pop", "(", "_i", ")", ";", "episode_lengths", ".", "pop", "(", "_i", ")", ";", "episode_rewards", ".", "pop", "(", "_i", ")", "\n", "", "mark_remove", ".", "clear", "(", ")", "\n", "\n", "", "mean_reward", "=", "np", ".", "mean", "(", "reward_results", ")", "\n", "std_reward", "=", "np", ".", "std", "(", "reward_results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "reward_results", ")", ")", "\n", "mean_length", "=", "np", ".", "mean", "(", "length_results", ")", "\n", "print", "(", "\"Test Result - Reward {:.2f}+-{:.2f} Length {:.1f} for {}\"", ".", "format", "(", "mean_reward", ",", "std_reward", ",", "mean_length", ",", "idx", ")", ")", "\n", "return", "mean_reward", ",", "std_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.DQNBase.__init__": [[29, 56], ["torch.Module.__init__", "model.Flatten", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.DQNBase.modules", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "type", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "module.bias.data.zero_", "model.DQNBase._feature_size"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", "DQNBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "num_actions", "=", "env", ".", "action_space", ".", "n", "\n", "self", ".", "Linear", "=", "Linear", "# We have overridden the \"reset_parameters\" method for a more well-principled initialization", "\n", "\n", "self", ".", "flatten", "=", "Flatten", "(", ")", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ",", "padding", "=", "0", "if", "self", ".", "input_shape", "[", "1", "]", "!=", "105", "else", "2", ")", ",", "\n", "#nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0 if self.input_shape[1]!=105 else 2),", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "for", "module", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "type", "(", "module", ")", "==", "nn", ".", "Conv2d", ":", "init", ".", "kaiming_uniform_", "(", "module", ".", "weight", ".", "data", ",", "nonlinearity", "=", "\"relu\"", ",", "a", "=", "0.", ")", ";", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", ",", "self", ".", "num_actions", ")", "\n", ")", "\n", "self", ".", "out_bias", "=", "self", ".", "fc", "[", "-", "1", "]", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.DQNBase.forward": [[57, 62], ["model.DQNBase.features", "model.DQNBase.flatten", "model.DQNBase.fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.DQNBase._feature_size": [[63, 65], ["model.DQNBase.features().view().size", "model.DQNBase.features().view", "model.DQNBase.features", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "_feature_size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "features", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.DQNBase.act": [[66, 92], ["state.unsqueeze.unsqueeze.dim", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "state.unsqueeze.unsqueeze.unsqueeze", "model.DQNBase.forward().cpu().numpy().squeeze", "numpy.argmax", "random.random", "random.randrange", "state.unsqueeze.unsqueeze.dim", "enumerate", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.DQNBase.forward().cpu().numpy", "numpy.argmax", "numpy.copy", "state.unsqueeze.unsqueeze.size", "model.DQNBase.forward().cpu().numpy", "random.random", "random.randrange", "model.DQNBase.forward().cpu", "model.DQNBase.forward().cpu", "model.DQNBase.forward", "model.DQNBase.forward"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward"], ["", "def", "act", "(", "self", ",", "state", ",", "epsilon", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Parameters\n        ----------\n        state       torch.Tensor with appropritate device type\n        epsilon     epsilon for epsilon-greedy\n        \"\"\"", "\n", "if", "state", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "state", "=", "state", ".", "unsqueeze", "(", "0", ")", "\n", "q_values", "=", "self", ".", "forward", "(", "state", ",", "**", "kwargs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", "\n", "bestAction", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "0", ")", "\n", "", "if", "random", ".", "random", "(", ")", ">=", "epsilon", ":", "\n", "                ", "action", "=", "bestAction", "\n", "", "else", ":", "\n", "                ", "action", "=", "random", ".", "randrange", "(", "self", ".", "num_actions", ")", "\n", "", "", "elif", "state", ".", "dim", "(", ")", "==", "4", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "q_values", "=", "self", ".", "forward", "(", "state", ",", "**", "kwargs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "bestAction", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "1", ")", "\n", "action", "=", "np", ".", "copy", "(", "bestAction", ")", "\n", "", "for", "i", ",", "e", "in", "enumerate", "(", "epsilon", ")", ":", "\n", "                ", "if", "random", ".", "random", "(", ")", "<", "e", ":", "\n", "                    ", "action", "[", "i", "]", "=", "random", ".", "randrange", "(", "self", ".", "num_actions", ")", "\n", "", "", "", "else", ":", "assert", "False", ",", "\"The input state has an invalid shape {}\"", ".", "format", "(", "state", ".", "size", "(", ")", ")", "\n", "return", "action", ",", "action", "==", "bestAction", ",", "(", "q_values", ",", "bestAction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.DuelingDQN.__init__": [[99, 127], ["model.DQNBase.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.DuelingDQN.register_buffer", "model.DuelingDQN.fc[].weight.register_hook", "model.DuelingDQN.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DuelingDQN.Linear", "model.DuelingDQN.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DuelingDQN.Linear", "model.DuelingOutput", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zip", "model.DuelingDQN.fc[].weight.zero_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "model.DuelingDQN._feature_size", "model.DuelingDQN._feature_size", "model.DuelingDQN.fc[].parameters", "model.DuelingDQN.advantage[].parameters", "model.DuelingDQN.value[].parameters"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size"], ["def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", "DuelingDQN", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "advantage", "=", "self", ".", "fc", "\n", "self", ".", "value", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", ",", "1", ")", "\n", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", "*", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", "*", "2", ",", "self", ".", "num_actions", "+", "1", ")", ",", "\n", "DuelingOutput", "(", "self", ".", "num_actions", ")", "\n", ")", "\n", "# rewrite the parameters of \"self.advantage\" and \"self.value\" into \"self.fc\" so that they are combined into a single computation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "p", ",", "p_a", ",", "p_v", "in", "zip", "(", "self", ".", "fc", "[", "0", "]", ".", "parameters", "(", ")", ",", "self", ".", "advantage", "[", "0", "]", ".", "parameters", "(", ")", ",", "self", ".", "value", "[", "0", "]", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "p", "[", ":", "512", "]", "=", "p_a", ";", "p", "[", "512", ":", "512", "*", "2", "]", "=", "p_v", "\n", "", "self", ".", "fc", "[", "2", "]", ".", "weight", ".", "zero_", "(", ")", "\n", "self", ".", "fc", "[", "2", "]", ".", "weight", "[", ":", "self", ".", "num_actions", ",", ":", "512", "]", "=", "self", ".", "advantage", "[", "2", "]", ".", "weight", ";", "self", ".", "fc", "[", "2", "]", ".", "weight", "[", "-", "1", ",", "512", ":", "512", "*", "2", "]", "=", "self", ".", "value", "[", "2", "]", ".", "weight", "\n", "self", ".", "fc", "[", "2", "]", ".", "bias", "[", ":", "self", ".", "num_actions", "]", "=", "self", ".", "advantage", "[", "2", "]", ".", "bias", ";", "self", ".", "fc", "[", "2", "]", ".", "bias", "[", "-", "1", "]", "=", "self", ".", "value", "[", "2", "]", ".", "bias", "\n", "del", "self", ".", "value", ",", "self", ".", "advantage", "\n", "# mask the backpropagated gradient on \"self.fc[2].weight\"", "\n", "", "self", ".", "register_buffer", "(", "'grad_mask'", ",", "torch", ".", "zeros", "(", "self", ".", "num_actions", "+", "1", ",", "512", "*", "2", ")", ")", "\n", "self", ".", "grad_mask", "[", ":", "self", ".", "num_actions", ",", ":", "512", "]", "=", "1.", ";", "self", ".", "grad_mask", "[", "-", "1", ",", "512", ":", "512", "*", "2", "]", "=", "1.", "\n", "self", ".", "dueling_grad_hook", "=", "self", ".", "fc", "[", "2", "]", ".", "weight", ".", "register_hook", "(", "lambda", "grad", ":", "self", ".", "grad_mask", "*", "grad", ")", "\n", "\n", "self", ".", "out_bias", "=", "self", ".", "fc", "[", "-", "2", "]", ".", "bias", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.DuelingOutput.__init__": [[129, 142], ["torch.Module.__init__", "model.DuelingOutput.register_buffer", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_actions", ")", ":", "\n", "        ", "super", "(", "DuelingOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'output_matrix'", ",", "torch", ".", "Tensor", "(", "num_actions", ",", "num_actions", "+", "1", ")", ")", "\n", "# set the \"-advantage.mean(1, keepdim=True)\" term", "\n", "self", ".", "output_matrix", "[", ":", ",", ":", "]", "=", "-", "1.", "/", "num_actions", "\n", "# set the last input dim, the average value, added to all Qs", "\n", "self", ".", "output_matrix", "[", ":", ",", "-", "1", "]", "=", "1.", "\n", "# set the diagonal term", "\n", "for", "i", "in", "range", "(", "num_actions", ")", ":", "\n", "            ", "self", ".", "output_matrix", "[", "i", ",", "i", "]", "=", "(", "num_actions", "-", "1", ")", "/", "num_actions", "\n", "# this complete the definition of \"output_matrix\", which computes \"value + (advantage - advantage.mean(1, keepdim=True)) * rescale \"", "\n", "", "assert", "not", "self", ".", "output_matrix", ".", "requires_grad", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.DuelingOutput.forward": [[143, 145], ["torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "output_matrix", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.Flatten.forward": [[147, 149], ["x.view", "x.size"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.Linear.reset_parameters": [[151, 158], ["torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "model.Linear.bias.data.zero_"], "methods", ["None"], ["    ", "def", "reset_parameters", "(", "self", ")", "->", "None", ":", "\n", "        ", "init", ".", "kaiming_uniform_", "(", "self", ".", "weight", ",", "nonlinearity", "=", "\"relu\"", ",", "a", "=", "0.", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "#fan_in, _  = init._calculate_fan_in_and_fan_out(self.weight)", "\n", "#bound = 1./math.sqrt(fan_in)", "\n", "#init.uniform_(self.bias, -bound, bound)", "\n", "            ", "self", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.model.DQN": [[12, 18], ["model.DuelingDQN", "model.DQNBase"], "function", ["None"], ["def", "DQN", "(", "env", ",", "args", ")", ":", "\n", "    ", "if", "args", ".", "dueling", ":", "\n", "        ", "model", "=", "DuelingDQN", "(", "env", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "DQNBase", "(", "env", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.optimizers.AdamW.__init__": [[21, 36], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.optimizers.AdamW.__setstate__": [[37, 41], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.optimizers.AdamW.step": [[42, 120], ["closure", "math.sqrt", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.data.addcdiv_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "lr", ",", "eps", ",", "amsgrad", "=", "group", "[", "'lr'", "]", ",", "group", "[", "'eps'", "]", ",", "group", "[", "'amsgrad'", "]", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "lr", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'AdamW does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "state", "[", "'PopArt_rescale'", "]", "=", "1.", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "'max_exp_avg_sq'", "in", "state", ":", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# We incorporate the term group['lr'] into the momentum, and define the bias_correction1 such that it respects the possibly moving group['lr']", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "lr", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1. - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "lr", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "*", "sqrt_bias_correction2", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"step_size\" and \"eps\"", "\n", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "rescaling", "=", "state", "[", "'PopArt_rescale'", "]", "\n", "exp_avg", ".", "mul_", "(", "beta1", "*", "rescaling", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "(", "1.", "-", "beta1", ")", "*", "lr", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", "**", "2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "# 'eps' is first multiplied by sqrt_bias_correction2 and then divided by sqrt_bias_correction2", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.optimizers.AdamBelief.__init__": [[136, 151], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamBelief", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.optimizers.AdamBelief.__setstate__": [[152, 156], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.optimizers.AdamBelief.step": [[157, 234], ["closure", "math.sqrt", "exp_avg.mul_().add_", "exp_avg_var.mul_().addcmul_", "p.data.addcdiv_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.max", "max_exp_avg_var.sqrt().add_", "exp_avg_var.sqrt().add_", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_var.mul_", "max_exp_avg_var.sqrt", "exp_avg_var.sqrt"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "lr", ",", "eps", ",", "amsgrad", "=", "group", "[", "'lr'", "]", ",", "group", "[", "'eps'", "]", ",", "group", "[", "'amsgrad'", "]", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "lr", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'AdamW does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_var", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_var'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "'max_exp_avg_var'", "in", "state", ":", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "max_exp_avg_var", "=", "state", "[", "'max_exp_avg_var'", "]", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# We define the bias_correction1 such that it respects the possibly moving \"group['lr']\"", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "lr", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1. - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "lr", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "*", "sqrt_bias_correction2", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"step_size\" and \"eps\"", "\n", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "diff", "=", "grad", "-", "exp_avg", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "(", "1.", "-", "beta1", ")", ")", "\n", "exp_avg_var", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "diff", ",", "diff", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_var", ",", "exp_avg_var", ",", "out", "=", "max_exp_avg_var", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "# 'eps' is first multiplied by sqrt_bias_correction2 and then divided by sqrt_bias_correction2", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", "*", "lr", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.optimizers.LaProp.__init__": [[238, 254], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "4e-4", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-15", ",", "\n", "weight_decay", "=", "0.", ",", "amsgrad", "=", "False", ",", "centered", "=", "False", ")", ":", "\n", "        ", "self", ".", "centered", "=", "centered", "\n", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "LaProp", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.optimizers.LaProp.__setstate__": [[255, 259], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "LaProp", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.2.optimizers.LaProp.step": [[260, 349], ["closure", "math.sqrt", "exp_avg_sq.mul_().addcmul_", "denom.addcmul.addcmul.sqrt().add_", "exp_avg.mul_().addcdiv_", "p.data.add_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "exp_mean_avg_sq.mul_().add_", "torch.zeros_like", "exp_avg_sq.mul_", "denom.addcmul.addcmul.addcmul", "torch.max", "denom.addcmul.addcmul.sqrt", "exp_avg.mul_", "exp_mean_avg_sq.mul_"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'LaProp does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "state", "[", "'exp_mean_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "state", "[", "'PopArt_rescale'", "]", "=", "1.", "\n", "state", "[", "'Momentum_rescale'", "]", "=", "1.", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "self", ".", "centered", ":", "\n", "                    ", "exp_mean_avg_sq", "=", "state", "[", "'exp_mean_avg_sq'", "]", "\n", "", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "group", "[", "'lr'", "]", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1 - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "group", "[", "'lr'", "]", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "rescaling", "=", "state", "[", "'PopArt_rescale'", "]", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", "**", "2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "\n", "denom", "=", "exp_avg_sq", "\n", "if", "self", ".", "centered", ":", "\n", "                    ", "exp_mean_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "beta2", ")", "\n", "if", "state", "[", "'step'", "]", ">", "5", ":", "\n", "                        ", "denom", "=", "denom", ".", "addcmul", "(", "exp_mean_avg_sq", ",", "exp_mean_avg_sq", ",", "value", "=", "-", "1.", ")", "\n", "\n", "", "", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "(", "self", ".", "centered", "and", "state", "[", "'step'", "]", "<=", "5", ")", ":", "\n", "# Maintains the maximum of all (centered) 2nd moment running avg. till now", "\n", "                        ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "denom", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", "\n", "\n", "", "", "denom", "=", "denom", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", "*", "sqrt_bias_correction2", ")", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"exp_avg\" and \"eps\"", "\n", "\n", "momentum_rescaling", "=", "state", "[", "'Momentum_rescale'", "]", "\n", "exp_avg", ".", "mul_", "(", "beta1", "*", "momentum_rescaling", ")", ".", "addcdiv_", "(", "grad", ",", "denom", ",", "value", "=", "(", "1.", "-", "beta1", ")", "*", "group", "[", "'lr'", "]", "*", "sqrt_bias_correction2", ")", "\n", "\n", "p", ".", "data", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.NoopResetEnv.__init__": [[13, 22], ["gym.Wrapper.__init__", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "noop_max", "=", "30", ")", ":", "\n", "        ", "\"\"\"Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "noop_max", "=", "noop_max", "\n", "self", ".", "override_num_noops", "=", "None", "\n", "self", ".", "noop_action", "=", "0", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "0", "]", "==", "'NOOP'", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.NoopResetEnv.reset": [[23, 37], ["wrappers.NoopResetEnv.env.reset", "range", "wrappers.NoopResetEnv.unwrapped.np_random.randint", "wrappers.NoopResetEnv.env.step", "wrappers.NoopResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\" Do no-op action for a number of steps in [1, noop_max].\"\"\"", "\n", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "if", "self", ".", "override_num_noops", "is", "not", "None", ":", "\n", "            ", "noops", "=", "self", ".", "override_num_noops", "\n", "", "else", ":", "\n", "            ", "noops", "=", "self", ".", "unwrapped", ".", "np_random", ".", "randint", "(", "1", ",", "self", ".", "noop_max", "+", "1", ")", "\n", "", "assert", "noops", ">", "0", "\n", "obs", "=", "None", "\n", "for", "_", "in", "range", "(", "noops", ")", ":", "\n", "            ", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "self", ".", "noop_action", ")", "\n", "if", "done", ":", "\n", "                ", "obs", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.NoopResetEnv.step": [[38, 40], ["wrappers.NoopResetEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "step", "(", "self", ",", "ac", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "step", "(", "ac", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.TwoLatestFramesEnv.__init__": [[42, 48], ["gym.Wrapper.__init__", "collections.deque"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"Return the last two observations\"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "# most recent raw observations (for max pooling across time steps)", "\n", "self", ".", "_obs_buffer", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "2", ")", "#np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)", "\n", "self", ".", "idx", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.TwoLatestFramesEnv.step": [[49, 56], ["wrappers.TwoLatestFramesEnv.env.step", "wrappers.TwoLatestFramesEnv._obs_buffer.append"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"Store the last observation and return the storage. \n        Note that the observation is mutable and must be processed by np.maximum() before using or storing \n        \"\"\"", "\n", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "_obs_buffer", ".", "append", "(", "obs", ")", "\n", "return", "self", ".", "_obs_buffer", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.TwoLatestFramesEnv.reset": [[57, 62], ["wrappers.TwoLatestFramesEnv.env.reset", "range", "wrappers.TwoLatestFramesEnv._obs_buffer.append"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "ob", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "for", "_", "in", "range", "(", "2", ")", ":", "\n", "            ", "self", ".", "_obs_buffer", ".", "append", "(", "ob", ")", "\n", "", "return", "self", ".", "_obs_buffer", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.MaxAndSkipAndRewardClipEnv.__init__": [[65, 71], ["gym.Wrapper.__init__", "type"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "skip", "=", "4", ",", "clip", "=", "True", ")", ":", "\n", "        ", "\"\"\"Return only every `skip`-th frame\"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "assert", "type", "(", "skip", ")", "==", "int", "and", "skip", ">", "0", "\n", "self", ".", "_skip", "=", "skip", "\n", "self", ".", "clip", "=", "clip", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.MaxAndSkipAndRewardClipEnv.step": [[72, 88], ["range", "numpy.maximum", "wrappers.MaxAndSkipAndRewardClipEnv.env.step", "numpy.sign"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"Repeat action, sum reward, and max over last observations.\"\"\"", "\n", "total_reward", "=", "0.0", "\n", "total_clipped_reward", "=", "0.0", "\n", "for", "i", "in", "range", "(", "self", ".", "_skip", ")", ":", "\n", "            ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "total_reward", "+=", "reward", "\n", "if", "self", ".", "clip", ":", "\n", "                ", "total_clipped_reward", "+=", "np", ".", "sign", "(", "reward", ")", "\n", "", "if", "done", ":", "\n", "                ", "break", "\n", "# Note that the observation on the done=True frame", "\n", "# doesn't matter", "\n", "", "", "max_frame", "=", "np", ".", "maximum", "(", "*", "obs", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "reward", "=", "(", "total_reward", ",", "total_clipped_reward", ")", "if", "self", ".", "clip", "else", "total_reward", "\n", "return", "max_frame", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.MaxAndSkipAndRewardClipEnv.reset": [[89, 91], ["numpy.maximum", "wrappers.MaxAndSkipAndRewardClipEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "np", ".", "maximum", "(", "*", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.EpisodicLifeEnv.__init__": [[94, 104], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "lives", "=", "0", "# env.reset() will be called before it starts. Therefore this initial value does not matter.", "\n", "self", ".", "was_real_done", "=", "True", "\n", "# If it is not waiting for reset, we regard the reset as an accident and we truly reset the environment.", "\n", "self", ".", "waiting_for_reset", "=", "False", "\n", "self", ".", "reset_ob", "=", "None", "\n", "", "def", "step", "(", "self", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.EpisodicLifeEnv.step": [[104, 123], ["wrappers.EpisodicLifeEnv.env.step", "wrappers.EpisodicLifeEnv.env.unwrapped.ale.lives", "wrappers.EpisodicLifeEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "assert", "not", "self", ".", "waiting_for_reset", "\n", "ob", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "was_real_done", "=", "done", "# this includes the case of \"TimeLimit.truncated\"", "\n", "# check current lives, make loss of life terminal,", "\n", "# then update lives to handle bonus lives", "\n", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "# usually \"done\" must be False with \"lives < self.lives and lives > 0\", but with \"TimeLimit\" it can be True", "\n", "if", "lives", "<", "self", ".", "lives", "and", "lives", ">", "0", "and", "not", "done", ":", "\n", "# for Qbert sometimes we stay in lives == 0 condtion for a few frames", "\n", "# so its important to keep lives > 0, so that we only reset once", "\n", "# the environment advertises done.", "\n", "            ", "done", "=", "True", "\n", "# no-op step to advance from terminal/lost life state", "\n", "# In order to avoid problems caused by the step \"self.env.step(0)\" during the reset, we execute \"self.env.step(0)\" in advance.", "\n", "self", ".", "reset_ob", ",", "_", ",", "self", ".", "was_real_done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "0", ")", "\n", "self", ".", "waiting_for_reset", "=", "True", "\n", "", "self", ".", "lives", "=", "lives", "\n", "return", "ob", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.EpisodicLifeEnv.reset": [[124, 140], ["wrappers.EpisodicLifeEnv.env.unwrapped.ale.lives", "wrappers.EpisodicLifeEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"", "\n", "if", "not", "self", ".", "was_real_done", "and", "self", ".", "waiting_for_reset", ":", "\n", "# extract the result of the previous no-op step", "\n", "            ", "ob", "=", "self", ".", "reset_ob", "\n", "self", ".", "reset_ob", "=", "None", "\n", "", "else", ":", "\n", "            ", "ob", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "self", ".", "was_real_done", "=", "False", "\n", "\n", "", "self", ".", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "self", ".", "waiting_for_reset", "=", "False", "\n", "return", "ob", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.FireResetEnv.__init__": [[143, 148], ["gym.Wrapper.__init__", "len", "env.unwrapped.get_action_meanings", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"Take action on reset for environments that are fixed until firing.\"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "1", "]", "==", "'FIRE'", "\n", "assert", "len", "(", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ")", ">=", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.FireResetEnv.reset": [[149, 158], ["wrappers.FireResetEnv.env.reset", "wrappers.FireResetEnv.env.step", "wrappers.FireResetEnv.env.step", "wrappers.FireResetEnv.env.reset", "wrappers.FireResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "1", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "2", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.FireResetEnv.step": [[159, 161], ["wrappers.FireResetEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "step", "(", "self", ",", "ac", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "step", "(", "ac", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.WrapFrame.__init__": [[163, 187], ["gym.ObservationWrapper.__init__", "gym.spaces.Box", "round", "round", "round"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "downscale", "=", "84", ",", "grayscale", "=", "True", ")", ":", "\n", "        ", "\"\"\"Warp frames to 84x84 as done in the Nature paper and later work.\"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", "env", ")", "\n", "# if the size of the image is 160 x 250, we crop it from the top by 28 and from the bottom by 12, so that it becomes the default 160 x 210", "\n", "shp", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "crop40", "=", "True", "if", "shp", "[", "0", "]", "==", "250", "else", "False", "\n", "if", "downscale", ">", "10", ":", "\n", "            ", "self", ".", "_width", "=", "downscale", "\n", "self", ".", "_height", "=", "downscale", "\n", "", "else", ":", "\n", "            ", "assert", "downscale", ">", "0", ",", "\"invalid downscaling ratio {}\"", ".", "format", "(", "downscale", ")", "\n", "self", ".", "_width", "=", "round", "(", "shp", "[", "1", "]", "/", "downscale", ")", "\n", "self", ".", "_height", "=", "round", "(", "210", "/", "downscale", ")", "if", "self", ".", "crop40", "else", "round", "(", "shp", "[", "0", "]", "/", "downscale", ")", "\n", "\n", "", "self", ".", "_grayscale", "=", "grayscale", "\n", "if", "self", ".", "_grayscale", ":", "\n", "            ", "num_colors", "=", "1", "\n", "", "else", ":", "\n", "            ", "num_colors", "=", "3", "\n", "# we move the color channel dimension from shape[-1] to shape[0] ", "\n", "", "new_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "\n", "shape", "=", "(", "num_colors", ",", "self", ".", "_height", ",", "self", ".", "_width", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n", "self", ".", "observation_space", "=", "new_space", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.WrapFrame.observation": [[188, 201], ["cv2.resize", "numpy.ascontiguousarray", "cv2.cvtColor", "numpy.expand_dims", "numpy.moveaxis"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "obs", ")", ":", "\n", "        ", "frame", "=", "obs", "\n", "if", "self", ".", "crop40", ":", "frame", "=", "frame", "[", "28", ":", "238", ",", ":", ",", ":", "]", "# crop from the top by 28 and from the bottom by 12", "\n", "if", "self", ".", "_grayscale", ":", "\n", "            ", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_RGB2GRAY", ")", "\n", "# numpy array treated as an image has a shape of (height, width, color)", "\n", "", "frame", "=", "cv2", ".", "resize", "(", "frame", ",", "(", "self", ".", "_width", ",", "self", ".", "_height", ")", ",", "interpolation", "=", "cv2", ".", "INTER_AREA", ")", "\n", "if", "self", ".", "_grayscale", ":", "\n", "            ", "frame", "=", "np", ".", "expand_dims", "(", "frame", ",", "-", "1", ")", "\n", "\n", "", "frame", "=", "np", ".", "ascontiguousarray", "(", "np", ".", "moveaxis", "(", "frame", ",", "-", "1", ",", "0", ")", ")", "\n", "\n", "return", "frame", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.FrameStack.__init__": [[203, 218], ["gym.Wrapper.__init__", "collections.deque", "gym.spaces.Box", "numpy.float32", "numpy.float32", "tuple"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "k", "=", "4", ")", ":", "\n", "        ", "\"\"\"Stack k last frames.\n        Returns lazy array, which is much more memory efficient.\n\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "shp", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "k", "=", "k", "\n", "self", ".", "frames", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "k", ")", "\n", "# the color channel dimension has been moved from shape[-1] to shape[0] in \"WrapFrame\"", "\n", "# the observation is supposed to be accessed via np.array(...)", "\n", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "np", ".", "float32", "(", "0.0", ")", ",", "high", "=", "np", ".", "float32", "(", "1.0", ")", ",", "shape", "=", "(", "(", "shp", "[", "0", "]", "*", "k", ",", ")", "+", "tuple", "(", "shp", "[", "1", ":", "]", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.FrameStack.reset": [[219, 225], ["wrappers.FrameStack.env.reset", "wrappers.FrameStack.frames.clear", "range", "wrappers.FrameStack._get_ob", "wrappers.FrameStack.frames.append"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.FrameStack._get_ob"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "ob", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "self", ".", "frames", ".", "clear", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "k", ")", ":", "\n", "            ", "self", ".", "frames", ".", "append", "(", "ob", ")", "\n", "", "return", "self", ".", "_get_ob", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.FrameStack.step": [[226, 230], ["wrappers.FrameStack.env.step", "wrappers.FrameStack.frames.append", "wrappers.FrameStack._get_ob"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.FrameStack._get_ob"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "ob", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "frames", ".", "append", "(", "ob", ")", "\n", "return", "self", ".", "_get_ob", "(", ")", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.FrameStack._get_ob": [[231, 234], ["wrappers.LazyFrames", "len", "tuple"], "methods", ["None"], ["", "def", "_get_ob", "(", "self", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "frames", ")", "==", "self", ".", "k", "\n", "return", "LazyFrames", "(", "tuple", "(", "self", ".", "frames", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.LazyFrames.__init__": [[236, 243], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "frames", ")", ":", "\n", "        ", "\"\"\"This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN's 1M frames replay\n        buffers.\n        This object should only be converted to numpy array before being passed to the model.\n        You'd not believe how complex the previous solution was.\"\"\"", "\n", "self", ".", "_frames", "=", "frames", "\n", "# We avoid using the following self._out so that no float32 data will be saved into memory ", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.LazyFrames._force": [[246, 249], ["numpy.concatenate().astype", "numpy.concatenate"], "methods", ["None"], ["", "def", "_force", "(", "self", ")", ":", "\n", "# the data type in self._frames is np.uint8 ", "\n", "        ", "return", "np", ".", "concatenate", "(", "self", ".", "_frames", ",", "axis", "=", "0", ")", ".", "astype", "(", "np", ".", "float32", ")", "/", "255.", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.LazyFrames.__array__": [[250, 252], ["wrappers.LazyFrames._force"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.LazyFrames._force"], ["", "def", "__array__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_force", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.LazyFrames.__len__": [[253, 255], ["len", "wrappers.LazyFrames._force"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.LazyFrames._force"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_force", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.TimeLimit.__init__": [[257, 261], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "max_episode_steps", ")", ":", "\n", "        ", "super", "(", "TimeLimit", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "_max_episode_steps", "=", "max_episode_steps", "\n", "self", ".", "_elapsed_steps", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.TimeLimit.step": [[262, 269], ["wrappers.TimeLimit.env.step"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "step", "(", "self", ",", "ac", ")", ":", "\n", "        ", "ob", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "ac", ")", "\n", "self", ".", "_elapsed_steps", "+=", "1", "\n", "if", "self", ".", "_elapsed_steps", ">", "self", ".", "_max_episode_steps", ":", "\n", "            ", "done", "=", "True", "\n", "info", "[", "\"TimeLimit.truncated\"", "]", "=", "True", "\n", "", "return", "ob", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.TimeLimit.reset": [[270, 273], ["wrappers.TimeLimit.env.reset"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "_elapsed_steps", "=", "0", "\n", "return", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.make_atari": [[274, 284], ["gym.make", "wrappers.TwoLatestFramesEnv", "wrappers.NoopResetEnv", "wrappers.MaxAndSkipAndRewardClipEnv", "wrappers.TimeLimit"], "function", ["None"], ["", "", "def", "make_atari", "(", "env_id", ",", "max_episode_steps", "=", "None", ",", "clip_reward", "=", "True", ")", ":", "\n", "    ", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "assert", "'NoFrameskip'", "in", "env", ".", "spec", ".", "id", "\n", "env", "=", "TwoLatestFramesEnv", "(", "env", ")", "\n", "env", "=", "NoopResetEnv", "(", "env", ",", "noop_max", "=", "30", ")", "\n", "env", "=", "MaxAndSkipAndRewardClipEnv", "(", "env", ",", "skip", "=", "4", ",", "clip", "=", "clip_reward", ")", "\n", "# the number of steps measured by \"TimeLimit\" is after the step skipping of \"MaxAndSkipEnv\", and therefore is 1/4 of real frames", "\n", "if", "max_episode_steps", "is", "not", "None", "and", "max_episode_steps", ">", "0", ":", "\n", "        ", "env", "=", "TimeLimit", "(", "env", ",", "max_episode_steps", "=", "max_episode_steps", ")", "\n", "", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.wrap_deepmind": [[285, 298], ["wrappers.WrapFrame", "int", "max", "wrappers.FrameStack", "wrappers.EpisodicLifeEnv", "FireResetEnv.unwrapped.get_action_meanings", "wrappers.FireResetEnv"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "def", "wrap_deepmind", "(", "env", ",", "episode_life", "=", "True", ",", "frame_stack", "=", "4", ",", "downscale", "=", "84", ",", "greyscale", "=", "True", ")", ":", "\n", "    ", "\"\"\"Configure environment for DeepMind-style Atari.\n    \"\"\"", "\n", "if", "episode_life", ":", "\n", "        ", "env", "=", "EpisodicLifeEnv", "(", "env", ")", "\n", "", "if", "'FIRE'", "in", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ":", "\n", "# 'FIRE' is often needed to restart the game when it loses a life", "\n", "        ", "env", "=", "FireResetEnv", "(", "env", ")", "\n", "", "env", "=", "WrapFrame", "(", "env", ",", "downscale", "=", "downscale", ",", "grayscale", "=", "greyscale", ")", "\n", "frame_stack", "=", "int", "(", "frame_stack", ")", "\n", "frame_stack", "=", "max", "(", "frame_stack", ",", "1", ")", "\n", "env", "=", "FrameStack", "(", "env", ",", "frame_stack", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.wrap_atari_dqn": [[299, 306], ["wrappers.wrap_deepmind"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.wrap_deepmind"], ["", "def", "wrap_atari_dqn", "(", "env", ",", "args", ")", ":", "\n", "    ", "env", "=", "wrap_deepmind", "(", "env", ",", "\n", "episode_life", "=", "args", ".", "episode_life", ",", "\n", "frame_stack", "=", "args", ".", "frame_stack", ",", "\n", "downscale", "=", "args", ".", "frame_downscale", ",", "\n", "greyscale", "=", "args", ".", "grey", ")", "\n", "return", "env", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree.__init__": [[10, 37], ["numpy.array", "range"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "capacity", ",", "operation", ",", "neutral_element", ")", ":", "\n", "        ", "\"\"\"Build a Segment Tree data structure.\n        https://en.wikipedia.org/wiki/Segment_tree\n        Can be used as regular array, but with two\n        important differences:\n            a) setting item's value is slightly slower.\n               It is O(lg capacity) instead of O(1).\n            b) user has access to an efficient `reduce`\n               operation which reduces `operation` over\n               a contiguous subsequence of items in the\n               array.\n        Paramters\n        ---------\n        capacity: int\n            Total size of the array - must be a power of two.\n        operation: lambda obj, obj -> obj\n            and operation for combining elements (eg. sum, max)\n            must for a mathematical group together with the set of\n            possible values for array elements.\n        neutral_element: obj\n            neutral element for the operation above. eg. float('-inf')\n            for max and 0 for sum.\n        \"\"\"", "\n", "assert", "capacity", ">", "0", "and", "capacity", "&", "(", "capacity", "-", "1", ")", "==", "0", ",", "\"capacity must be positive and a power of 2.\"", "\n", "self", ".", "_capacity", "=", "capacity", "\n", "self", ".", "_value", "=", "np", ".", "array", "(", "[", "neutral_element", "for", "_", "in", "range", "(", "2", "*", "capacity", ")", "]", ")", "\n", "self", ".", "_operation", "=", "operation", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree._reduce_helper": [[38, 51], ["replay_buffer.SegmentTree._reduce_helper", "replay_buffer.SegmentTree._reduce_helper", "replay_buffer.SegmentTree._operation", "replay_buffer.SegmentTree._reduce_helper", "replay_buffer.SegmentTree._reduce_helper"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree._reduce_helper", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree._reduce_helper", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree._reduce_helper", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree._reduce_helper"], ["", "def", "_reduce_helper", "(", "self", ",", "start", ",", "end", ",", "node", ",", "node_start", ",", "node_end", ")", ":", "\n", "        ", "if", "start", "==", "node_start", "and", "end", "==", "node_end", ":", "\n", "            ", "return", "self", ".", "_value", "[", "node", "]", "\n", "", "mid", "=", "(", "node_start", "+", "node_end", ")", "//", "2", "\n", "if", "end", "<=", "mid", ":", "\n", "            ", "return", "self", ".", "_reduce_helper", "(", "start", ",", "end", ",", "2", "*", "node", ",", "node_start", ",", "mid", ")", "\n", "", "else", ":", "\n", "            ", "if", "mid", "+", "1", "<=", "start", ":", "\n", "                ", "return", "self", ".", "_reduce_helper", "(", "start", ",", "end", ",", "2", "*", "node", "+", "1", ",", "mid", "+", "1", ",", "node_end", ")", "\n", "", "else", ":", "\n", "                ", "return", "self", ".", "_operation", "(", "\n", "self", ".", "_reduce_helper", "(", "start", ",", "mid", ",", "2", "*", "node", ",", "node_start", ",", "mid", ")", ",", "\n", "self", ".", "_reduce_helper", "(", "mid", "+", "1", ",", "end", ",", "2", "*", "node", "+", "1", ",", "mid", "+", "1", ",", "node_end", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree.reduce": [[53, 74], ["replay_buffer.SegmentTree._reduce_helper"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree._reduce_helper"], ["", "", "", "def", "reduce", "(", "self", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "\n", "        ", "\"\"\"Returns result of applying `self.operation`\n        to a contiguous subsequence of the array.\n            self.operation(arr[start], operation(arr[start+1], operation(... arr[end])))\n        Parameters\n        ----------\n        start: int\n            beginning of the subsequence\n        end: int\n            end of the subsequences\n        Returns\n        -------\n        reduced: obj\n            result of reducing self.operation over the specified range of array elements.\n        \"\"\"", "\n", "if", "end", "is", "None", ":", "\n", "            ", "end", "=", "self", ".", "_capacity", "\n", "", "if", "end", "<", "0", ":", "\n", "            ", "end", "+=", "self", ".", "_capacity", "\n", "", "end", "-=", "1", "\n", "return", "self", ".", "_reduce_helper", "(", "start", ",", "end", ",", "1", ",", "0", ",", "self", ".", "_capacity", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree.__setitem__": [[75, 86], ["replay_buffer.SegmentTree._operation"], "methods", ["None"], ["", "def", "__setitem__", "(", "self", ",", "idx", ",", "val", ")", ":", "\n", "# index of the leaf", "\n", "        ", "idx", "+=", "self", ".", "_capacity", "\n", "self", ".", "_value", "[", "idx", "]", "=", "val", "\n", "idx", "//=", "2", "\n", "while", "idx", ">=", "1", ":", "\n", "            ", "self", ".", "_value", "[", "idx", "]", "=", "self", ".", "_operation", "(", "\n", "self", ".", "_value", "[", "2", "*", "idx", "]", ",", "\n", "self", ".", "_value", "[", "2", "*", "idx", "+", "1", "]", "\n", ")", "\n", "idx", "//=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree.__getitem__": [[87, 90], ["None"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "assert", "0", "<=", "idx", "<", "self", ".", "_capacity", "\n", "return", "self", ".", "_value", "[", "self", ".", "_capacity", "+", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.__init__": [[101, 106], ["replay_buffer.SegmentTree.__init__"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "capacity", ")", ":", "\n", "        ", "super", "(", "MaxSegmentTree", ",", "self", ")", ".", "__init__", "(", "\n", "capacity", "=", "capacity", ",", "\n", "operation", "=", "max", ",", "\n", "neutral_element", "=", "0.", "# we assume that all elements are larger than zero", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max": [[108, 111], ["replay_buffer.SegmentTree.reduce"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree.reduce"], ["", "def", "max", "(", "self", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "\n", "        ", "\"\"\"Returns max(arr[start], ...,  arr[end])\"\"\"", "\n", "return", "super", "(", "MaxSegmentTree", ",", "self", ")", ".", "reduce", "(", "start", ",", "end", ")", "\n", "#return self._value[1]", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MinSegmentTree.__init__": [[123, 128], ["float", "replay_buffer.SegmentTree.__init__"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "capacity", ",", "neutral_element", "=", "float", "(", "\"inf\"", ")", ")", ":", "\n", "        ", "super", "(", "MinSegmentTree", ",", "self", ")", ".", "__init__", "(", "\n", "capacity", "=", "capacity", ",", "\n", "operation", "=", "min", ",", "\n", "neutral_element", "=", "neutral_element", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.__init__": [[150, 155], ["replay_buffer.SegmentTree.__init__"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "capacity", ")", ":", "\n", "        ", "super", "(", "SumSegmentTree", ",", "self", ")", ".", "__init__", "(", "\n", "capacity", "=", "capacity", ",", "\n", "operation", "=", "operator", ".", "add", ",", "\n", "neutral_element", "=", "0.", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum": [[157, 160], ["replay_buffer.SegmentTree.reduce"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SegmentTree.reduce"], ["", "def", "sum", "(", "self", ",", "start", "=", "0", ",", "end", "=", "None", ")", ":", "\n", "        ", "\"\"\"Returns arr[start] + ... + arr[end]\"\"\"", "\n", "return", "super", "(", "SumSegmentTree", ",", "self", ")", ".", "reduce", "(", "start", ",", "end", ")", "\n", "#return self._value[1]", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.find_prefixsum_idx": [[162, 178], ["replay_buffer.compiled_find_prefixsum_idx"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_find_prefixsum_idx"], ["", "def", "find_prefixsum_idx", "(", "self", ",", "prefixsum", ")", ":", "\n", "        ", "\"\"\"Find the highest index `i` in the array such that\n            sum(arr[0] + arr[1] + ... + arr[i - i]) <= prefixsum\n        if array values are probabilities, this function\n        allows to sample indexes according to the discrete\n        probability efficiently.\n        Parameters\n        ----------\n        perfixsum: float\n            upperbound on the sum of array prefix\n        Returns\n        -------\n        idx: int\n            highest index satisfying the prefixsum constraint\n        \"\"\"", "\n", "return", "compiled_find_prefixsum_idx", "(", "prefixsum", ",", "self", ".", "_capacity", ",", "self", ".", "_value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer.__init__": [[194, 210], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "\"\"\"Create Replay buffer.\n\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped. The index of the next transition\n            to store can be accessed by \"self._next_idx\". \n        \"\"\"", "\n", "self", ".", "_storage", "=", "[", "]", "\n", "self", ".", "_maxsize", "=", "size", "\n", "self", ".", "_next_idx", "=", "0", "\n", "self", ".", "cache", "=", "None", "\n", "self", ".", "cached_data", "=", "None", "\n", "self", ".", "indices_replaced_after_caching", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer.__len__": [[211, 213], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_storage", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer.add": [[214, 225], ["len", "replay_buffer.ReplayBuffer._storage.append", "len", "replay_buffer.ReplayBuffer.indices_replaced_after_caching.append"], "methods", ["None"], ["", "def", "add", "(", "self", ",", "obs_t", ",", "action", ",", "reward", ",", "obs_tp1", ",", "done", ")", ":", "\n", "        ", "data", "=", "[", "obs_t", ",", "action", ",", "reward", ",", "obs_tp1", ",", "done", "]", "\n", "\n", "if", "self", ".", "_next_idx", ">=", "len", "(", "self", ".", "_storage", ")", ":", "\n", "            ", "self", ".", "_storage", ".", "append", "(", "data", ")", "\n", "", "else", ":", "\n", "            ", "assert", "len", "(", "self", ".", "_storage", ")", "==", "self", ".", "_maxsize", "\n", "self", ".", "_storage", "[", "self", ".", "_next_idx", "]", "=", "data", "\n", "if", "self", ".", "cache", "is", "not", "None", ":", "\n", "                ", "self", ".", "indices_replaced_after_caching", ".", "append", "(", "self", ".", "_next_idx", ")", "\n", "", "", "self", ".", "_next_idx", "=", "(", "self", ".", "_next_idx", "+", "1", ")", "%", "self", ".", "_maxsize", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer._encode_sample": [[226, 239], ["numpy.array().reshape", "obses_t.append", "actions.append", "rewards.append", "obses_tp1.append", "dones.append", "len", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "_encode_sample", "(", "self", ",", "idxes", ")", ":", "\n", "        ", "obses_t", ",", "actions", ",", "rewards", ",", "obses_tp1", ",", "dones", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "idxes", ":", "\n", "#data = self._storage[i]", "\n", "            ", "obs_t", ",", "action", ",", "reward", ",", "obs_tp1", ",", "done", "=", "self", ".", "_storage", "[", "i", "]", "\n", "obses_t", ".", "append", "(", "obs_t", ".", "_frames", ")", "\n", "actions", ".", "append", "(", "action", ")", "\n", "rewards", ".", "append", "(", "reward", ")", "\n", "obses_tp1", ".", "append", "(", "obs_tp1", ".", "_frames", ")", "\n", "dones", ".", "append", "(", "done", ")", "\n", "", "shp", "=", "obs_t", ".", "_frames", "[", "0", "]", ".", "shape", "\n", "obses_t_obses_tp1", "=", "np", ".", "array", "(", "[", "obses_t", ",", "obses_tp1", "]", ")", ".", "reshape", "(", "2", ",", "len", "(", "idxes", ")", ",", "-", "1", ",", "shp", "[", "-", "2", "]", ",", "shp", "[", "-", "1", "]", ")", "# their data types are np.uint8", "\n", "return", "obses_t_obses_tp1", ",", "np", ".", "array", "(", "actions", ",", "dtype", "=", "np", ".", "int64", ")", ",", "np", ".", "array", "(", "rewards", ",", "dtype", "=", "np", ".", "float32", ")", ",", "np", ".", "array", "(", "dones", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer.sample": [[240, 265], ["numpy.random.randint", "replay_buffer.ReplayBuffer.retrieve_cache", "len", "replay_buffer.ReplayBuffer._encode_sample"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer.retrieve_cache", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer._encode_sample"], ["", "def", "sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "\"\"\"Sample a batch of experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obs_batch, next_obs_batch: np.array\n            batch of observations, next set of observations seen after executing act_batch\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        \"\"\"", "\n", "if", "self", ".", "cache", "is", "None", ":", "\n", "# python random.randint is different from np.random.randint; np.random.randint is the same as random.randrange", "\n", "            ", "idxes", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "_storage", ")", ",", "size", "=", "batch_size", ")", "\n", "#idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]", "\n", "return", "self", ".", "_encode_sample", "(", "idxes", ")", "+", "(", "idxes", ",", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "retrieve_cache", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer._encode_next_state_data": [[266, 273], ["numpy.array", "numpy.array.append"], "methods", ["None"], ["", "", "def", "_encode_next_state_data", "(", "self", ",", "idxes", ")", ":", "\n", "        ", "obses_tp1", "=", "[", "]", "\n", "for", "i", "in", "idxes", ":", "\n", "            ", "obs_tp1", "=", "self", ".", "_storage", "[", "i", "]", "[", "3", "]", "\n", "obses_tp1", ".", "append", "(", "obs_tp1", ".", "_frames", ")", "\n", "", "obses_tp1", "=", "np", ".", "array", "(", "obses_tp1", ")", "\n", "return", "obses_tp1", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer.sample_next_state_and_cache_indices": [[274, 278], ["numpy.random.randint", "len", "replay_buffer.ReplayBuffer._encode_next_state_data"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer._encode_next_state_data"], ["", "def", "sample_next_state_and_cache_indices", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "idxes", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "_storage", ")", ",", "size", "=", "batch_size", ")", "\n", "self", ".", "cache", "=", "(", "idxes", ",", ")", "\n", "return", "self", ".", "_encode_next_state_data", "(", "idxes", ")", ",", "idxes", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer.update_and_store_cached_data": [[279, 284], ["replay_buffer.ReplayBuffer.indices_replaced_after_caching.clear", "replay_buffer.ReplayBuffer._encode_sample"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer._encode_sample"], ["", "def", "update_and_store_cached_data", "(", "self", ")", ":", "\n", "        ", "assert", "self", ".", "cache", "is", "not", "None", "\n", "idxes", "=", "self", ".", "cache", "[", "-", "1", "]", "\n", "self", ".", "cached_data", "=", "self", ".", "_encode_sample", "(", "idxes", ")", "+", "self", ".", "cache", "\n", "self", ".", "indices_replaced_after_caching", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer.retrieve_cache": [[285, 289], ["None"], "methods", ["None"], ["", "def", "retrieve_cache", "(", "self", ")", ":", "\n", "        ", "data", "=", "self", ".", "cached_data", "\n", "self", ".", "cache", ",", "self", ".", "cached_data", "=", "None", ",", "None", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.__init__": [[291, 323], ["replay_buffer.ReplayBuffer.__init__", "replay_buffer.SumSegmentTree", "replay_buffer.MaxSegmentTree", "replay_buffer.MinSegmentTree"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "alpha", ",", "IS_weight_only_smaller", ",", "allowed_avg_min_ratio", "=", "10", ")", ":", "\n", "        ", "\"\"\"Create Prioritized Replay buffer.\n\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        alpha: float\n            how much prioritization is used\n            (0 - no prioritization, 1 - full prioritization)\n\n        See Also\n        --------\n        ReplayBuffer.__init__\n        \"\"\"", "\n", "super", "(", "PrioritizedReplayBuffer", ",", "self", ")", ".", "__init__", "(", "size", ")", "\n", "assert", "alpha", ">=", "0", "\n", "self", ".", "_alpha", "=", "alpha", "\n", "\n", "it_capacity", "=", "1", "\n", "while", "it_capacity", "<", "size", ":", "\n", "            ", "it_capacity", "*=", "2", "\n", "", "self", ".", "it_capacity", "=", "it_capacity", "\n", "self", ".", "_it_sum", "=", "SumSegmentTree", "(", "it_capacity", ")", "\n", "self", ".", "_it_max", "=", "MaxSegmentTree", "(", "it_capacity", ")", "\n", "self", ".", "_max_priority", "=", "100.", "\n", "self", ".", "_max_priority", "=", "self", ".", "_max_priority", "**", "self", ".", "_alpha", "\n", "self", ".", "IS_weight_only_smaller", "=", "IS_weight_only_smaller", "\n", "if", "IS_weight_only_smaller", ":", "\n", "            ", "self", ".", "_it_min", "=", "MinSegmentTree", "(", "it_capacity", ",", "neutral_element", "=", "self", ".", "_max_priority", ")", "\n", "self", ".", "_min_priority", "=", "self", ".", "_max_priority", "\n", "", "assert", "allowed_avg_min_ratio", ">", "1", "or", "allowed_avg_min_ratio", "<=", "0", ",", "\"'allowed_avg_min_ratio' ({}) is not within the allowed range.\"", ".", "format", "(", "allowed_avg_min_ratio", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.add": [[324, 333], ["replay_buffer.compiled_setitem_sumtree", "replay_buffer.ReplayBuffer.add"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_setitem_sumtree", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.add"], ["if", "allowed_avg_min_ratio", "<=", "0", ":", "allowed_avg_min_ratio", "=", "float", "(", "\"inf\"", ")", "\n", "self", ".", "_allowed_avg_min_ratio", "=", "float", "(", "allowed_avg_min_ratio", ")", "# the maximum allowed relative difference between the min and the avg priorities", "\n", "\n", "", "def", "add", "(", "self", ",", "*", "args", ",", "prio", "=", "None", ",", "**", "kwargs", ")", ":", "# \"prio\" stands for priority", "\n", "        ", "\"\"\"See ReplayBuffer.store_effect\"\"\"", "\n", "idx", "=", "self", ".", "_next_idx", "\n", "if", "prio", "is", "None", ":", "\n", "            ", "prio", "=", "self", ".", "_max_priority", "\n", "", "else", ":", "\n", "            ", "assert", "prio", ">", "0.", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer._sample_proportional": [[334, 340], ["replay_buffer.compiled_sample_proportional", "len", "weights.astype", "len"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_sample_proportional"], ["prio", "=", "max", "(", "prio", "**", "self", ".", "_alpha", ",", "self", ".", "_it_sum", ".", "_value", "[", "1", "]", "/", "(", "len", "(", "self", ".", "_storage", ")", "*", "self", ".", "_allowed_avg_min_ratio", ")", ")", "\n", "", "compiled_setitem_sumtree", "(", "idx", ",", "prio", ",", "self", ".", "_it_sum", ".", "_value", ",", "self", ".", "it_capacity", ")", "\n", "super", "(", "PrioritizedReplayBuffer", ",", "self", ")", ".", "add", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n", "", "def", "_sample_proportional", "(", "self", ",", "batch_size", ",", "beta", "=", "1.", ")", ":", "\n", "        ", "weights", ",", "true_weights", ",", "idxes", "=", "compiled_sample_proportional", "(", "batch_size", ",", "self", ".", "_it_sum", ".", "_value", ",", "self", ".", "_it_sum", ".", "_capacity", ",", "len", "(", "self", ".", "_storage", ")", ",", "beta", ")", "\n", "if", "self", ".", "IS_weight_only_smaller", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample": [[341, 370], ["replay_buffer.PrioritizedReplayBuffer._sample_proportional", "replay_buffer.PrioritizedReplayBuffer._encode_sample", "replay_buffer.PrioritizedReplayBuffer.retrieve_cache"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer._sample_proportional", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer._encode_sample", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer.retrieve_cache"], ["# divide the weights by the largest weight possible, which corresponds to the minimal priority ", "\n", "            ", "weights", "=", "weights", "/", "(", "(", "self", ".", "_it_sum", ".", "_value", "[", "1", "]", "/", "len", "(", "self", ".", "_storage", ")", "/", "self", ".", "_min_priority", ")", "**", "beta", ")", "\n", "", "else", ":", "\n", "            ", "weights", "=", "np", ".", "minimum", "(", "weights", ",", "2.", "*", "self", ".", "_allowed_avg_min_ratio", ")", "\n", "", "return", "weights", ".", "astype", "(", "np", ".", "float32", ")", ",", "true_weights", ",", "idxes", "\n", "\n", "", "def", "sample", "(", "self", ",", "batch_size", ",", "beta", ")", ":", "\n", "        ", "\"\"\"Sample a batch of experiences.\n        compared to ReplayBuffer.sample\n        it also returns importance weights and idxes\n        of sampled experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        beta: float\n            To what degree to use importance weights\n            (0 - no corrections, 1 - full correction)\n        Returns\n        -------\n        obs_batch, next_obs_batch: np.array\n        act_batch: np.array\n        rew_batch: np.array\n        done_mask: np.array\n        weights: np.array\n        true_weights: np.array\n        idxes: np.array\n        \"\"\"", "\n", "assert", "beta", ">=", "0.", "\n", "if", "self", ".", "cache", "is", "None", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.update_priorities": [[371, 389], ["replay_buffer.compiled_update_priorities", "len", "len", "priorities.astype", "len", "replay_buffer.compiled_update_min_priority"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_update_priorities", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_update_min_priority"], ["            ", "weights", ",", "true_weights", ",", "idxes", "=", "self", ".", "_sample_proportional", "(", "batch_size", ",", "beta", ")", "\n", "encoded_sample", "=", "self", ".", "_encode_sample", "(", "idxes", ")", "\n", "return", "encoded_sample", "+", "(", "weights", ",", "true_weights", ",", "idxes", ")", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "retrieve_cache", "(", ")", "\n", "\n", "", "", "def", "update_priorities", "(", "self", ",", "idxes", ",", "priorities", ")", ":", "\n", "        ", "\"\"\"Update priorities of sampled transitions.\n        sets priority of transition at index idxes[i] in buffer\n        to priorities[i].\n        Parameters\n        ----------\n        idxes: [int]\n            List of idxes of sampled transitions\n        priorities: [float]\n            List of updated priorities corresponding to\n            transitions at the sampled idxes denoted by\n            variable `idxes`.\n        \"\"\"", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample_next_state_and_cache_indices": [[390, 395], ["replay_buffer.PrioritizedReplayBuffer._sample_proportional", "replay_buffer.PrioritizedReplayBuffer._encode_next_state_data"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer._sample_proportional", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.ReplayBuffer._encode_next_state_data"], ["assert", "len", "(", "idxes", ")", "==", "len", "(", "priorities", ")", "\n", "self", ".", "_max_priority", ",", "clipped_priorities", "=", "compiled_update_priorities", "(", "idxes", ",", "priorities", ".", "astype", "(", "float", ")", ",", "\n", "self", ".", "_alpha", ",", "self", ".", "_it_sum", ".", "_value", ",", "self", ".", "_it_max", ".", "_value", ",", "self", ".", "it_capacity", ",", "len", "(", "self", ".", "_storage", ")", ",", "self", ".", "_allowed_avg_min_ratio", ")", "\n", "if", "self", ".", "IS_weight_only_smaller", ":", "\n", "            ", "self", ".", "_min_priority", "=", "compiled_update_min_priority", "(", "idxes", ",", "clipped_priorities", ",", "self", ".", "_it_min", ".", "_value", ",", "self", ".", "it_capacity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_setitem_maxtree": [[91, 99], ["numba.njit", "max"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_setitem_maxtree", "(", "idx", ",", "val", ",", "_value", ",", "_capacity", ")", ":", "\n", "    ", "idx", "+=", "_capacity", "\n", "_value", "[", "idx", "]", "=", "val", "\n", "idx", "//=", "2", "\n", "while", "idx", ">=", "1", ":", "\n", "        ", "_value", "[", "idx", "]", "=", "max", "(", "_value", "[", "2", "*", "idx", "]", ",", "_value", "[", "2", "*", "idx", "+", "1", "]", ")", "\n", "idx", "//=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_setitem_mintree": [[113, 121], ["numba.njit", "min"], "function", ["None"], ["", "", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_setitem_mintree", "(", "idx", ",", "val", ",", "_value", ",", "_capacity", ")", ":", "\n", "    ", "idx", "+=", "_capacity", "\n", "_value", "[", "idx", "]", "=", "val", "\n", "idx", "//=", "2", "\n", "while", "idx", ">=", "1", ":", "\n", "        ", "_value", "[", "idx", "]", "=", "min", "(", "_value", "[", "2", "*", "idx", "]", ",", "_value", "[", "2", "*", "idx", "+", "1", "]", ")", "\n", "idx", "//=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_setitem_sumtree": [[130, 138], ["numba.njit"], "function", ["None"], ["", "", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_setitem_sumtree", "(", "idx", ",", "val", ",", "_value", ",", "_capacity", ")", ":", "\n", "    ", "idx", "+=", "_capacity", "\n", "_value", "[", "idx", "]", "=", "val", "\n", "idx", "//=", "2", "\n", "while", "idx", ">=", "1", ":", "\n", "        ", "_value", "[", "idx", "]", "=", "_value", "[", "2", "*", "idx", "]", "+", "_value", "[", "2", "*", "idx", "+", "1", "]", "\n", "idx", "//=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_setitem_min_sumtree": [[139, 148], ["numba.njit"], "function", ["None"], ["", "", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_setitem_min_sumtree", "(", "idx", ",", "min_val", ",", "_value", ",", "_capacity", ")", ":", "\n", "    ", "idx", "+=", "_capacity", "\n", "if", "min_val", ">", "_value", "[", "idx", "]", ":", "\n", "        ", "_value", "[", "idx", "]", "=", "min_val", "\n", "idx", "//=", "2", "\n", "while", "idx", ">=", "1", ":", "\n", "            ", "_value", "[", "idx", "]", "=", "_value", "[", "2", "*", "idx", "]", "+", "_value", "[", "2", "*", "idx", "+", "1", "]", "\n", "idx", "//=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_find_prefixsum_idx": [[179, 189], ["numba.njit"], "function", ["None"], ["", "", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_find_prefixsum_idx", "(", "prefixsum", ",", "_capacity", ",", "_value", ")", ":", "\n", "    ", "idx", "=", "1", "\n", "while", "idx", "<", "_capacity", ":", "# while non-leaf", "\n", "        ", "if", "_value", "[", "2", "*", "idx", "]", ">", "prefixsum", ":", "\n", "            ", "idx", "=", "2", "*", "idx", "\n", "", "else", ":", "\n", "            ", "prefixsum", "-=", "_value", "[", "2", "*", "idx", "]", "\n", "idx", "=", "2", "*", "idx", "+", "1", "\n", "", "", "return", "idx", "-", "_capacity", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_update_priorities": [[397, 410], ["numba.njit", "zip", "zip", "replay_buffer.compiled_setitem_maxtree", "replay_buffer.compiled_setitem_sumtree"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_setitem_maxtree", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_setitem_sumtree"], ["        ", "assert", "beta", ">=", "0.", "\n", "self", ".", "cache", "=", "self", ".", "_sample_proportional", "(", "batch_size", ",", "beta", ")", "\n", "idxes", "=", "self", ".", "cache", "[", "-", "1", "]", "\n", "return", "self", ".", "_encode_next_state_data", "(", "idxes", ")", ",", "idxes", "\n", "\n", "\n", "", "", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_update_priorities", "(", "idxes", ",", "priorities", ",", "_alpha", ",", "_value", ",", "_max_value", ",", "_capacity", ",", "length", ",", "_allowed_avg_min_ratio", ")", ":", "\n", "# change priorities to sampling probabilities first", "\n", "    ", "priorities", "=", "priorities", "**", "_alpha", "\n", "for", "idx", ",", "priority", "in", "zip", "(", "idxes", ",", "priorities", ")", ":", "\n", "        ", "assert", "0", "<=", "idx", "<", "length", "\n", "assert", "priority", ">=", "0", "# If the priority was smaller than zero, it will become nan at \"priorities**_alpha\" and fail to pass the assertion here ", "\n", "compiled_setitem_maxtree", "(", "idx", ",", "priority", ",", "_max_value", ",", "_capacity", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_update_prop_minimum_priorities": [[411, 420], ["numba.njit", "zip", "replay_buffer.compiled_setitem_min_sumtree"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_setitem_min_sumtree"], ["# this is the maximum of sampling probabilities", "\n", "", "_max_priority", "=", "_max_value", "[", "1", "]", "\n", "clipped_priorities", "=", "np", ".", "maximum", "(", "priorities", ",", "_value", "[", "1", "]", "/", "(", "length", "*", "_allowed_avg_min_ratio", ")", ")", "\n", "for", "idx", ",", "clipped_priority", "in", "zip", "(", "idxes", ",", "clipped_priorities", ")", ":", "\n", "        ", "compiled_setitem_sumtree", "(", "idx", ",", "clipped_priority", ",", "_value", ",", "_capacity", ")", "\n", "", "return", "_max_priority", ",", "clipped_priorities", "\n", "\n", "", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_update_prop_minimum_priorities", "(", "idxes", ",", "minimum_priorities", ",", "_alpha", ",", "_value", ",", "_capacity", ",", "length", ",", "_allowed_avg_min_ratio", ")", ":", "\n", "# change priorities to sampling probabilities first", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_update_min_priority": [[421, 428], ["numba.njit", "zip", "replay_buffer.compiled_setitem_mintree"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_setitem_mintree"], ["    ", "minimum_priorities", "=", "minimum_priorities", "**", "_alpha", "\n", "clipped_priorities", "=", "np", ".", "maximum", "(", "minimum_priorities", ",", "_value", "[", "1", "]", "/", "(", "length", "*", "_allowed_avg_min_ratio", ")", ")", "\n", "for", "idx", ",", "clipped_priority", "in", "zip", "(", "idxes", ",", "clipped_priorities", ")", ":", "\n", "        ", "assert", "0", "<=", "idx", "<", "length", "\n", "assert", "clipped_priority", ">=", "0", "# If the priority was smaller than zero, it will become nan at \"priorities**_alpha\" and fail to pass the assertion here ", "\n", "compiled_setitem_min_sumtree", "(", "idx", ",", "clipped_priority", ",", "_value", ",", "_capacity", ")", "\n", "", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_sample_proportional": [[429, 447], ["numba.njit", "numpy.zeros", "numpy.empty", "enumerate", "replay_buffer.compiled_find_prefixsum_idx", "true_weights.astype", "numpy.random.random", "numpy.arange", "replay_buffer.compiled_find_prefixsum_idx", "random.random"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_find_prefixsum_idx", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_find_prefixsum_idx"], ["", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_update_min_priority", "(", "idxes", ",", "clipped_priorities", ",", "_min_value", ",", "_capacity", ")", ":", "\n", "    ", "for", "idx", ",", "clipped_priority", "in", "zip", "(", "idxes", ",", "clipped_priorities", ")", ":", "\n", "        ", "compiled_setitem_mintree", "(", "idx", ",", "clipped_priority", ",", "_min_value", ",", "_capacity", ")", "\n", "# this is the minimum of sampling probabilities", "\n", "", "_min_priority", "=", "_min_value", "[", "1", "]", "\n", "return", "_min_priority", "\n", "\n", "", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_sample_proportional", "(", "batch_size", ",", "_value", ",", "_capacity", ",", "length", ",", "beta", ")", ":", "\n", "    ", "res", "=", "np", ".", "zeros", "(", "batch_size", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "weights", "=", "np", ".", "empty", "(", "batch_size", ",", "dtype", "=", "np", ".", "float64", ")", "\n", "p_total", "=", "_value", "[", "1", "]", "\n", "masses", "=", "(", "np", ".", "random", ".", "random", "(", "batch_size", ")", "+", "np", ".", "arange", "(", "batch_size", ",", "dtype", "=", "np", ".", "float64", ")", ")", "*", "(", "p_total", "/", "batch_size", ")", "\n", "for", "i", ",", "mass", "in", "enumerate", "(", "masses", ")", ":", "\n", "        ", "idx", "=", "compiled_find_prefixsum_idx", "(", "mass", ",", "_capacity", ",", "_value", ")", "\n", "p", "=", "_value", "[", "idx", "+", "_capacity", "]", "\n", "while", "p", "==", "0.", ":", "\n", "            ", "idx", "=", "compiled_find_prefixsum_idx", "(", "(", "random", ".", "random", "(", ")", "+", "i", ")", "*", "(", "p_total", "/", "batch_size", ")", ",", "_capacity", ",", "_value", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target": [[10, 13], ["current_model.state_dict", "target_model.load_state_dict"], "function", ["None"], ["def", "update_target", "(", "current_model", ",", "target_model", ")", ":", "\n", "    ", "dic", "=", "current_model", ".", "state_dict", "(", ")", "\n", "target_model", ".", "load_state_dict", "(", "dic", ",", "strict", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.epsilon_scheduler": [[14, 24], ["math.log", "math.exp"], "function", ["None"], ["", "def", "epsilon_scheduler", "(", "eps_start", ",", "eps_mid", ",", "eps_final", ")", ":", "\n", "    ", "exponential_decay_rate", "=", "1000000", "/", "math", ".", "log", "(", "eps_start", "/", "eps_mid", ")", "\n", "def", "function", "(", "step_idx", ")", ":", "\n", "        ", "if", "step_idx", "<=", "1000000", ":", "\n", "            ", "return", "eps_start", "*", "math", ".", "exp", "(", "-", "step_idx", "/", "exponential_decay_rate", ")", "\n", "", "elif", "step_idx", "<=", "40000000", ":", "\n", "            ", "return", "eps_mid", "-", "(", "eps_mid", "-", "eps_final", ")", "*", "(", "step_idx", "-", "1000000", ")", "/", "39000000", "\n", "", "else", ":", "\n", "            ", "return", "eps_final", "\n", "", "", "return", "function", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.beta_scheduler": [[25, 29], ["min"], "function", ["None"], ["", "def", "beta_scheduler", "(", "beta_start", ",", "beta_frames", ")", ":", "\n", "    ", "def", "function", "(", "frame_idx", ")", ":", "\n", "        ", "return", "min", "(", "1.0", ",", "beta_start", "+", "(", "1.0", "-", "beta_start", ")", "*", "frame_idx", "/", "beta_frames", ")", "\n", "", "return", "function", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_log": [[30, 50], ["numpy.mean", "numpy.mean", "print", "numpy.mean", "kwargs.items", "time.time", "len", "open", "f.write", "type", "numpy.mean", "len"], "function", ["None"], ["", "def", "print_log", "(", "frame", ",", "prev_frame", ",", "prev_time", ",", "reward_list", ",", "length_list", ",", "loss_list", ",", "args", ",", "data_string", "=", "''", ",", "**", "kwargs", ")", ":", "\n", "    ", "fps", "=", "(", "frame", "-", "prev_frame", ")", "/", "(", "time", ".", "time", "(", ")", "-", "prev_time", ")", "\n", "avg_reward", "=", "np", ".", "mean", "(", "reward_list", ")", "\n", "avg_length", "=", "np", ".", "mean", "(", "length_list", ")", "\n", "avg_loss", "=", "np", ".", "mean", "(", "loss_list", ")", "if", "len", "(", "loss_list", ")", "!=", "0", "else", "0.", "\n", "additionals", ",", "additionalSave", "=", "\"\"", ",", "\"\"", "\n", "if", "kwargs", ":", "\n", "        ", "for", "k", ",", "v", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "# if v is a list, we calculate its mean and print with {:.4f}; otherwise we should preprocess the value with a desired precision into the str format ", "\n", "            ", "if", "type", "(", "v", ")", "==", "list", ":", "\n", "                ", "v", "=", "np", ".", "mean", "(", "v", ")", "if", "len", "(", "v", ")", "!=", "0", "else", "0.", "\n", "v", "=", "\"{:.4f}\"", ".", "format", "(", "v", ")", "\n", "", "additionals", "+=", "\" {}: {}\"", ".", "format", "(", "k", ",", "v", ")", "\n", "additionalSave", "+=", "\"\\t{}\"", ".", "format", "(", "v", ")", "\n", "", "", "print", "(", "\"Step: {:<8} FPS: {:.1f} Avg. Reward: {:.1f} Avg.Lf.Length: {:.1f} Avg.Loss: {:.4f}{}\"", ".", "format", "(", "\n", "frame", ",", "fps", ",", "avg_reward", ",", "avg_length", ",", "avg_loss", ",", "additionals", "\n", ")", ")", "\n", "if", "not", "args", ".", "silent", ":", "\n", "        ", "with", "open", "(", "'data_{}_{}{}.txt'", ".", "format", "(", "args", ".", "optim", ",", "args", ".", "env", ",", "data_string", ")", ",", "'a'", ")", "as", "f", ":", "\n", "            ", "f", ".", "write", "(", "'{:.0f}\\t{}\\t{}{}\\n'", ".", "format", "(", "(", "frame", "+", "prev_frame", ")", "/", "2.", ",", "avg_reward", ",", "avg_loss", ",", "additionalSave", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_args": [[51, 55], ["print", "vars().items", "print", "vars", "str"], "function", ["None"], ["", "", "", "def", "print_args", "(", "args", ")", ":", "\n", "    ", "print", "(", "' '", "*", "26", "+", "'Options'", ")", "\n", "for", "k", ",", "v", "in", "vars", "(", "args", ")", ".", "items", "(", ")", ":", "\n", "        ", "print", "(", "' '", "*", "26", "+", "k", "+", "': '", "+", "str", "(", "v", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.load_model": [[56, 72], ["model.load_state_dict", "os.path.join", "os.path.join", "os.path.exists", "ValueError", "torch.load"], "function", ["None"], ["", "", "def", "load_model", "(", "model", ",", "args", ")", ":", "\n", "    ", "if", "args", ".", "load_model", "!=", "\"\"", ":", "\n", "        ", "fname", "=", "os", ".", "path", ".", "join", "(", "\"models\"", ",", "args", ".", "env", ",", "args", ".", "load_model", ")", "\n", "", "else", ":", "\n", "        ", "fname", "=", "\"\"", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "fname", "+=", "\"per-\"", "\n", "", "if", "args", ".", "dueling", ":", "\n", "            ", "fname", "+=", "\"dueling-\"", "\n", "", "if", "args", ".", "double", ":", "\n", "            ", "fname", "+=", "\"double-\"", "\n", "", "fname", "+=", "\"dqn-{}.pth\"", ".", "format", "(", "args", ".", "save_model", ")", "# when \"args.load_model\" == '', we use \"args.save_model\"", "\n", "fname", "=", "os", ".", "path", ".", "join", "(", "\"models\"", ",", "args", ".", "env", ",", "fname", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "fname", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"No model saved at {}\"", ".", "format", "(", "fname", ")", ")", "\n", "", "model", ".", "load_state_dict", "(", "torch", ".", "load", "(", "fname", ",", "map_location", "=", "args", ".", "device", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.set_global_seeds": [[73, 82], ["torch.manual_seed", "torch.cuda.is_available", "numpy.random.seed", "random.seed", "utils.set_numba_seeds", "torch.cuda.manual_seed_all"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.set_numba_seeds"], ["", "def", "set_global_seeds", "(", "seed", ")", ":", "\n", "    ", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "set_numba_seeds", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.set_numba_seeds": [[83, 87], ["numpy.random.seed", "random.seed"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed"], ["", "@", "njit", "\n", "def", "set_numba_seeds", "(", "seed", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.prop_minimum_priorities": [[396, 407], ["numpy.array", "replay_buffer.compiled_update_prop_minimum_priorities", "len", "numpy.all", "len", "numpy.all"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.compiled_update_prop_minimum_priorities"], ["", "", "def", "sample_next_state_and_cache_indices", "(", "self", ",", "batch_size", ",", "beta", ")", ":", "\n", "        ", "assert", "beta", ">=", "0.", "\n", "self", ".", "cache", "=", "self", ".", "_sample_proportional", "(", "batch_size", ",", "beta", ")", "\n", "idxes", "=", "self", ".", "cache", "[", "-", "1", "]", "\n", "return", "self", ".", "_encode_next_state_data", "(", "idxes", ")", ",", "idxes", "\n", "\n", "\n", "", "", "@", "njit", "(", "parallel", "=", "False", ")", "\n", "def", "compiled_update_priorities", "(", "idxes", ",", "priorities", ",", "_alpha", ",", "_value", ",", "_max_value", ",", "_capacity", ",", "length", ",", "_allowed_avg_min_ratio", ")", ":", "\n", "# change priorities to sampling probabilities first", "\n", "    ", "priorities", "=", "priorities", "**", "_alpha", "\n", "for", "idx", ",", "priority", "in", "zip", "(", "idxes", ",", "priorities", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.get_initialization_stat": [[6, 33], ["heuristics.extract_reward_lists", "heuristics.remove_zero_return_episodes", "min", "print", "heuristics.calculate_mean_and_init_std", "heuristics.find_out_weighted_inverse_time_scale", "max", "heuristics.extract_reward_lists_with_life_counts", "heuristics.remove_zero_return_episodes", "print"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.extract_reward_lists", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.remove_zero_return_episodes", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.calculate_mean_and_init_std", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.find_out_weighted_inverse_time_scale", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.extract_reward_lists_with_life_counts", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.remove_zero_return_episodes"], ["def", "get_initialization_stat", "(", "replay_storage", ",", "args", ",", "time_scale_multiply", ",", "provided_gamma", "=", "None", ",", "num_lives", "=", "1.", ",", "gamma_bounds", "=", "(", "0.99", ",", "0.9999", ")", ")", ":", "\n", "    ", "reward_lists", "=", "extract_reward_lists", "(", "replay_storage", ",", "args", ")", "\n", "assert", "num_lives", ">=", "0.", ",", "\"invalid arguments: num_lives {}\"", ".", "format", "(", "num_lives", ")", "\n", "# the reward structure is analyzed and \"time_scale_multiply\" is used", "\n", "assert", "time_scale_multiply", ">=", "1.", ",", "\"invalid arguments: time_scale_multiply {}\"", ".", "format", "(", "time_scale_multiply", ")", "\n", "#time_scale_multiply *= num_lives ", "\n", "# clean the lists ", "\n", "nonzero_reward_lists", "=", "remove_zero_return_episodes", "(", "reward_lists", ")", "\n", "#nonzero_reward_lists = select_behind_zero_head(nonzero_reward_lists)", "\n", "_gamma", "=", "1.", "-", "find_out_weighted_inverse_time_scale", "(", "nonzero_reward_lists", ",", "time_scale_multiply", ",", "consider_abs_only", "=", "True", ")", "#find_out_time_scale_of_nonzero(reward_lists, multiply=time_scale_multiply) ", "\n", "assert", "gamma_bounds", "[", "0", "]", "<=", "gamma_bounds", "[", "1", "]", ",", "\"gamma min {}, max {} are not valid\"", ".", "format", "(", "gamma_bounds", "[", "0", "]", ",", "gamma_bounds", "[", "1", "]", ")", "\n", "gamma", "=", "min", "(", "max", "(", "_gamma", ",", "gamma_bounds", "[", "0", "]", ")", ",", "gamma_bounds", "[", "1", "]", ")", "\n", "print", "(", "\"gamma is estimated to be {:.6f}\"", ".", "format", "(", "gamma", ")", ")", "\n", "# if a complete episode with several lives have been stored as several episodes for learning, we recombine them", "\n", "if", "num_lives", ">", "1", ":", "\n", "        ", "reward_lists", "=", "extract_reward_lists_with_life_counts", "(", "replay_storage", ",", "args", ",", "num_lives", ")", "\n", "", "if", "provided_gamma", "is", "None", ":", "provided_gamma", "=", "gamma", "\n", "mean", ",", "std", "=", "calculate_mean_and_init_std", "(", "remove_zero_return_episodes", "(", "reward_lists", ")", ",", "gamma", ",", "time_scale_multiply", "/", "2.", ",", "provided_gamma", ")", "\n", "if", "std", "is", "not", "None", ":", "\n", "        ", "scale", "=", "std", "\n", "#scale = std/math.sqrt(time_scale_multiply/2.)", "\n", "#scale = max(scale, 1.)", "\n", "", "else", ":", "\n", "        ", "scale", "=", "None", "\n", "", "print", "(", "\"mean estimated to be {:.1f}, scale set to be {:.2f}\"", ".", "format", "(", "mean", ",", "scale", ")", ")", "if", "mean", "is", "not", "None", "else", "None", "\n", "rescaled_mean", "=", "mean", "/", "scale", "if", "scale", "is", "not", "None", "else", "None", "\n", "return", "rescaled_mean", ",", "scale", ",", "provided_gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.calculate_mean_and_init_std": [[34, 79], ["numpy.mean", "numpy.concatenate", "numpy.mean", "numpy.mean", "numpy.std", "print", "numpy.std", "print", "len", "reversed", "reversed_value_lists.append", "numpy.concatenate", "reversed", "reversed_value_lists.append", "len", "numpy.hstack", "numpy.hstack", "numpy.mean", "reversed_value_list.append", "reversed_value_list.append", "range", "numpy.std", "range", "numpy.array", "numpy.array", "len", "math.sqrt", "range", "math.sqrt", "numpy.mean", "len", "len", "len", "math.sqrt", "len", "len", "len", "len", "len", "len"], "function", ["None"], ["", "def", "calculate_mean_and_init_std", "(", "reward_lists", ",", "gamma", ",", "gamma_per_reward", ",", "provided_gamma", ")", ":", "\n", "    ", "if", "len", "(", "reward_lists", ")", "==", "0", ":", "\n", "        ", "return", "None", ",", "None", "\n", "# first we compute the mean using the provided external gamma", "\n", "", "reversed_value_lists", "=", "[", "]", "\n", "for", "reward_list", "in", "reward_lists", ":", "\n", "        ", "prev_value", "=", "0.", "\n", "reversed_value_list", "=", "[", "]", "\n", "for", "r", "in", "reversed", "(", "reward_list", ")", ":", "\n", "            ", "prev_value", "*=", "provided_gamma", "\n", "prev_value", "+=", "r", "\n", "reversed_value_list", ".", "append", "(", "prev_value", ")", "\n", "", "reversed_value_lists", ".", "append", "(", "reversed_value_list", ")", "\n", "", "mean", "=", "np", ".", "mean", "(", "np", ".", "concatenate", "(", "reversed_value_lists", ")", ")", "\n", "# then we use the estimated gamma", "\n", "reversed_value_lists", "=", "[", "]", "\n", "for", "reward_list", "in", "reward_lists", ":", "\n", "        ", "prev_value", "=", "0.", "\n", "reversed_value_list", "=", "[", "]", "\n", "for", "r", "in", "reversed", "(", "reward_list", ")", ":", "\n", "            ", "prev_value", "*=", "gamma", "\n", "prev_value", "+=", "r", "\n", "reversed_value_list", ".", "append", "(", "prev_value", ")", "\n", "", "reversed_value_lists", ".", "append", "(", "reversed_value_list", ")", "\n", "", "all_values", "=", "np", ".", "concatenate", "(", "reversed_value_lists", ")", "\n", "init_values", "=", "[", "l", "[", "-", "1", "]", "for", "l", "in", "reversed_value_lists", "]", "\n", "gamma0", "=", "1.", "-", "(", "1.", "-", "gamma", ")", "*", "gamma_per_reward", "\n", "if", "len", "(", "init_values", ")", "==", "1", ":", "\n", "        ", "return", "mean", ",", "init_values", "[", "0", "]", "/", "(", "(", "1.", "-", "gamma", "**", "len", "(", "reversed_value_lists", "[", "0", "]", ")", ")", "/", "(", "1.", "-", "gamma", ")", ")", "*", "(", "(", "1.", "-", "gamma0", "**", "len", "(", "reversed_value_lists", "[", "0", "]", ")", ")", "/", "(", "1.", "-", "gamma0", ")", ")", "\n", "#std = np.std(init_values, ddof=1) if len(init_values)>1 else init_values[0]", "\n", "#std = np.std(all_values, ddof=1)", "\n", "#print(\"reward std: {:.6f}\".format(np.std(np.hstack([np.array(l) for l in reward_lists]), ddof=1)))", "\n", "", "mu_r", "=", "np", ".", "mean", "(", "[", "init_values", "[", "i", "]", "*", "(", "1.", "-", "gamma", ")", "/", "(", "1.", "-", "gamma", "**", "len", "(", "reversed_value_lists", "[", "i", "]", ")", ")", "for", "i", "in", "range", "(", "len", "(", "reversed_value_lists", ")", ")", "]", ")", "\n", "real_mu_r", "=", "np", ".", "mean", "(", "np", ".", "hstack", "(", "[", "np", ".", "array", "(", "l", ")", "for", "l", "in", "reward_lists", "]", ")", ")", "\n", "\n", "#mu_r = real_mu_r #", "\n", "real_sigma_r", "=", "np", ".", "std", "(", "np", ".", "hstack", "(", "[", "np", ".", "array", "(", "l", ")", "for", "l", "in", "reward_lists", "]", ")", ",", "ddof", "=", "1", ")", "\n", "print", "(", "\"mu_r {:.6f}, real mu_r {:.6f}, real std_r {:.6f}\"", ".", "format", "(", "mu_r", ",", "real_mu_r", ",", "real_sigma_r", ")", ")", "\n", "#mu_r = real_mu_r", "\n", "Qs", "=", "[", "init_values", "[", "i", "]", "-", "(", "1.", "-", "gamma", "**", "len", "(", "reversed_value_lists", "[", "i", "]", ")", ")", "/", "(", "1.", "-", "gamma", ")", "*", "mu_r", "for", "i", "in", "range", "(", "len", "(", "reversed_value_lists", ")", ")", "]", "\n", "sigma_r", "=", "np", ".", "std", "(", "[", "(", "Qs", "[", "i", "]", ")", "/", "math", ".", "sqrt", "(", "(", "1.", "-", "gamma", "**", "(", "2", "*", "len", "(", "reversed_value_lists", "[", "i", "]", ")", ")", ")", "/", "(", "1.", "-", "gamma", "**", "2", ")", ")", "for", "i", "in", "range", "(", "len", "(", "reversed_value_lists", ")", ")", "]", ",", "ddof", "=", "1", ")", "\n", "sigma_Q0", "=", "np", ".", "mean", "(", "[", "math", ".", "sqrt", "(", "(", "1.", "-", "gamma0", "**", "(", "2", "*", "len", "(", "l", ")", ")", ")", "/", "(", "1.", "-", "gamma0", "**", "2", ")", ")", "for", "l", "in", "reversed_value_lists", "]", ")", "*", "sigma_r", "\n", "print", "(", "\"r_std estimated at trjectory head: {:.6f}, std around the means: {:.6f}, c-independent Q std: {:.6f}, Q std predicted by r_std: {:.6f}\"", ".", "format", "(", "sigma_r", ",", "np", ".", "std", "(", "Qs", ")", ",", "sigma_Q0", ",", "np", ".", "mean", "(", "[", "math", ".", "sqrt", "(", "(", "1.", "-", "gamma0", "**", "(", "2", "*", "len", "(", "l", ")", ")", ")", "/", "(", "1.", "-", "gamma0", "**", "2", ")", ")", "for", "l", "in", "reversed_value_lists", "]", ")", "*", "real_sigma_r", ")", ")", "#np.std([sigma_r*math.sqrt((1.-gamma0**(2*len(l)))/(1.-gamma0**2)) for l in reversed_value_lists], ddof=1)", "\n", "#std = max(1., std)", "\n", "return", "mean", ",", "sigma_Q0", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.find_out_weighted_inverse_time_scale": [[80, 119], ["enumerate", "numpy.array", "len", "numpy.array", "numpy.array", "numpy.array", "np.array.append", "math.sqrt", "numpy.sqrt", "numpy.abs", "numpy.sum", "numpy.sum", "numpy.sum", "numpy.average", "numpy.all", "numpy.all", "numpy.all", "heuristics.separate_out_smallest_reward_sublist", "sublists.append", "numpy.all", "heuristics.separate_out_smallest_reward_sublist", "sublists.append", "numpy.all", "heuristics.separate_out_smallest_reward_sublist", "sublists.append", "abs", "heuristics.return_weighted_expected_time_until_next_reward", "numpy.abs", "sum", "numpy.array"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.separate_out_smallest_reward_sublist", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.separate_out_smallest_reward_sublist", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.separate_out_smallest_reward_sublist", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.return_weighted_expected_time_until_next_reward", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum"], ["", "def", "find_out_weighted_inverse_time_scale", "(", "reward_lists", ",", "multiply", ",", "consider_abs_only", "=", "False", ")", ":", "\n", "# this is our main algorithm, which is used to properly give an estimate of the time scale of the reward ", "\n", "    ", "if", "len", "(", "reward_lists", ")", "==", "0", ":", "\n", "        ", "return", "0.", "\n", "", "inverse_expected_dists", "=", "[", "]", "\n", "# we take an average of the estimates on all episodes", "\n", "for", "i", ",", "_reward_list", "in", "enumerate", "(", "reward_lists", ")", ":", "\n", "        ", "reward_list", "=", "np", ".", "array", "(", "_reward_list", ")", "\n", "sublists", "=", "[", "]", "\n", "if", "consider_abs_only", ":", "\n", "            ", "reward_list", "=", "np", ".", "abs", "(", "reward_list", ")", "###", "\n", "", "if", "not", "(", "np", ".", "all", "(", "reward_list", ">=", "0.", ")", "or", "np", ".", "all", "(", "reward_list", "<=", "0.", ")", ")", ":", "\n", "            ", "pos_mask", "=", "(", "reward_list", ">=", "0.", ")", ".", "astype", "(", "float", ")", "\n", "pos_sublist", "=", "reward_list", "*", "pos_mask", "\n", "neg_sublist", "=", "reward_list", "-", "pos_sublist", "\n", "while", "not", "np", ".", "all", "(", "pos_sublist", "==", "0.", ")", ":", "\n", "                ", "sublist", ",", "pos_sublist", "=", "separate_out_smallest_reward_sublist", "(", "pos_sublist", ")", "\n", "sublists", ".", "append", "(", "sublist", ")", "\n", "", "while", "not", "np", ".", "all", "(", "neg_sublist", "==", "0.", ")", ":", "\n", "                ", "sublist", ",", "neg_sublist", "=", "separate_out_smallest_reward_sublist", "(", "neg_sublist", ")", "\n", "sublists", ".", "append", "(", "sublist", ")", "\n", "", "", "else", ":", "\n", "            ", "while", "not", "np", ".", "all", "(", "reward_list", "==", "0.", ")", ":", "\n", "                ", "sublist", ",", "reward_list", "=", "separate_out_smallest_reward_sublist", "(", "reward_list", ")", "\n", "sublists", ".", "append", "(", "sublist", ")", "\n", "", "", "subreturns", "=", "np", ".", "array", "(", "[", "abs", "(", "sum", "(", "sublist", ")", ")", "for", "sublist", "in", "sublists", "]", ")", "\n", "subdists", "=", "np", ".", "array", "(", "[", "return_weighted_expected_time_until_next_reward", "(", "sublist", ")", "for", "sublist", "in", "sublists", "]", ")", "\n", "dist", "=", "np", ".", "sum", "(", "subdists", "*", "subreturns", ")", "/", "np", ".", "sum", "(", "subreturns", ")", "\n", "inverse_expected_dists", ".", "append", "(", "1.", "/", "dist", ")", "\n", "#if subdists[-1]<20.: # debugging", "\n", "#    print(_reward_list)", "\n", "#    start_i = sum(len(reward_lists[j]) for j in range(i))", "\n", "#    for step_idx in range(start_i, start_i+len(_reward_list)):", "\n", "#        state = storage[step_idx][3]", "\n", "#        pyplot.imsave(\"images/{}.png\".format(step_idx), state._frames[-1].squeeze())", "\n", "", "inverse_expected_dists", "=", "np", ".", "array", "(", "inverse_expected_dists", ")", "\n", "return_weights", "=", "[", "math", ".", "sqrt", "(", "np", ".", "sum", "(", "np", ".", "abs", "(", "np", ".", "array", "(", "reward_list", ")", ")", ")", ")", "for", "reward_list", "in", "reward_lists", "]", "# we use square root as the weights for the weighted average", "\n", "#print(1-np.average(inverse_expected_dists, weights=return_weights)/ multiply, 1-np.sqrt(np.average(inverse_expected_dists**2, weights=return_weights))/ multiply, 1-1./np.average(1./inverse_expected_dists, weights=return_weights)/ multiply) # we actually have three different possible averaging strategies", "\n", "return", "np", ".", "sqrt", "(", "np", ".", "average", "(", "inverse_expected_dists", "**", "2", ",", "weights", "=", "return_weights", ")", ")", "/", "multiply", "#", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.separate_out_smallest_reward_sublist": [[120, 128], ["numpy.argmin", "numpy.abs"], "function", ["None"], ["", "def", "separate_out_smallest_reward_sublist", "(", "reward_list", ")", ":", "\n", "# take out the sublist that contains the reward with the smallest absolute value, and also substract larger rewards by it ", "\n", "    ", "nonzero_rewards", "=", "reward_list", "[", "reward_list", "!=", "0.", "]", "\n", "r", "=", "nonzero_rewards", "[", "np", ".", "argmin", "(", "np", ".", "abs", "(", "nonzero_rewards", ")", ")", "]", "\n", "sublist_mask", "=", "(", "reward_list", "==", "r", ")", ".", "astype", "(", "float", ")", "\n", "sublist", "=", "reward_list", "*", "sublist_mask", "\n", "list_remain", "=", "reward_list", "-", "sublist", "\n", "return", "sublist", ",", "list_remain", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.return_weighted_expected_time_until_next_reward": [[129, 143], ["reversed", "dist_list.append", "sum", "abs"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum"], ["", "def", "return_weighted_expected_time_until_next_reward", "(", "reward_list", ")", ":", "\n", "    ", "dist_list", "=", "[", "]", "\n", "future_return", "=", "0.", "\n", "dist", "=", "0", "\n", "total_return_weight", "=", "0.", "\n", "for", "r", "in", "reversed", "(", "reward_list", ")", ":", "\n", "        ", "if", "r", "!=", "0.", ":", "\n", "            ", "dist", "=", "1", "\n", "future_return", "+=", "abs", "(", "r", ")", "\n", "", "else", ":", "\n", "            ", "dist", "+=", "1", "\n", "", "dist_list", ".", "append", "(", "future_return", "*", "dist", ")", "\n", "total_return_weight", "+=", "future_return", "\n", "", "return", "sum", "(", "dist_list", ")", "/", "total_return_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.find_out_time_scale_of_nonzero": [[144, 152], ["len", "float", "expected_dists.append", "numpy.mean", "heuristics.return_weighted_expected_time_until_next_reward"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.return_weighted_expected_time_until_next_reward"], ["", "def", "find_out_time_scale_of_nonzero", "(", "reward_lists", ",", "multiply", ")", ":", "\n", "# use the time scale of observing any nonzero reward ", "\n", "    ", "if", "len", "(", "reward_lists", ")", "==", "0", ":", "\n", "        ", "return", "float", "(", "\"inf\"", ")", "\n", "", "expected_dists", "=", "[", "]", "\n", "for", "reward_list", "in", "reward_lists", ":", "\n", "        ", "expected_dists", ".", "append", "(", "return_weighted_expected_time_until_next_reward", "(", "reward_list", ")", ")", "\n", "", "return", "np", ".", "mean", "(", "expected_dists", ")", "*", "multiply", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.extract_reward_lists": [[153, 166], ["range", "range", "len", "l.append", "reward_lists.append"], "function", ["None"], ["", "def", "extract_reward_lists", "(", "replay_storage", ",", "args", ")", ":", "\n", "    ", "reward_lists", "=", "[", "]", "\n", "for", "start_idx", "in", "range", "(", "0", ",", "args", ".", "num_task", ")", ":", "\n", "# We append the reward data into \"l\", and whenever \"done\" is True, we store \"l\" as a single reward list and start a new \"l\". ", "\n", "# In this way we can automatically ignore the incomplete episode at the endpoint of the storage.", "\n", "        ", "l", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "start_idx", ",", "len", "(", "replay_storage", ")", ",", "args", ".", "num_task", ")", ":", "\n", "            ", "data", "=", "replay_storage", "[", "i", "]", "\n", "l", ".", "append", "(", "data", "[", "2", "]", ")", "# reward", "\n", "if", "data", "[", "4", "]", ":", "# done", "\n", "                ", "reward_lists", ".", "append", "(", "l", ")", "\n", "l", "=", "[", "]", "\n", "", "", "", "return", "reward_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.extract_reward_lists_with_life_counts": [[167, 183], ["range", "range", "len", "l.append", "reward_lists.append"], "function", ["None"], ["", "def", "extract_reward_lists_with_life_counts", "(", "replay_storage", ",", "args", ",", "num_lives", ")", ":", "\n", "    ", "reward_lists", "=", "[", "]", "\n", "for", "start_idx", "in", "range", "(", "0", ",", "args", ".", "num_task", ")", ":", "\n", "# We append the reward data into \"l\", and whenever \"done\" is True, we store \"l\" as a single reward list and start a new \"l\". ", "\n", "# In this way we can automatically ignore the incomplete episode at the endpoint of the storage.", "\n", "        ", "l", "=", "[", "]", "\n", "lives", "=", "num_lives", "\n", "for", "i", "in", "range", "(", "start_idx", ",", "len", "(", "replay_storage", ")", ",", "args", ".", "num_task", ")", ":", "\n", "            ", "data", "=", "replay_storage", "[", "i", "]", "\n", "l", ".", "append", "(", "data", "[", "2", "]", ")", "# reward", "\n", "if", "data", "[", "4", "]", ":", "# done", "\n", "                ", "lives", "-=", "1", "\n", "if", "lives", "<=", "0", ":", "\n", "                    ", "reward_lists", ".", "append", "(", "l", ")", "\n", "l", "=", "[", "]", "\n", "", "", "", "", "return", "reward_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.remove_zero_return_episodes": [[184, 190], ["sum", "nonzero_lists.append", "numpy.abs"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum"], ["", "def", "remove_zero_return_episodes", "(", "reward_lists", ")", ":", "\n", "    ", "nonzero_lists", "=", "[", "]", "\n", "for", "l", "in", "reward_lists", ":", "\n", "        ", "if", "sum", "(", "np", ".", "abs", "(", "l", ")", ")", "!=", "0.", ":", "\n", "            ", "nonzero_lists", ".", "append", "(", "l", ")", "\n", "", "", "return", "nonzero_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.select_behind_zero_head": [[191, 199], ["heuristics.remove_zero_return_episodes", "enumerate", "remove_zero_return_episodes.append"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.remove_zero_return_episodes"], ["", "def", "select_behind_zero_head", "(", "reward_lists", ")", ":", "\n", "    ", "result_lists", "=", "[", "]", "\n", "for", "l", "in", "reward_lists", ":", "\n", "        ", "for", "i", ",", "r", "in", "enumerate", "(", "l", ")", ":", "\n", "            ", "if", "r", "!=", "0.", ":", "break", "\n", "", "result_lists", ".", "append", "(", "l", "[", "i", "+", "1", ":", "]", ")", "\n", "", "result_lists", "=", "remove_zero_return_episodes", "(", "result_lists", ")", "\n", "return", "result_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.all_zero": [[200, 202], ["numpy.all", "numpy.array"], "function", ["None"], ["", "def", "all_zero", "(", "reward_list", ")", ":", "\n", "    ", "return", "np", ".", "all", "(", "np", ".", "array", "(", "reward_list", ")", "==", "0.", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.__init__": [[9, 19], ["collections.deque", "type", "type"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "n_best", "=", "5", ",", "num_avg_episode", "=", "40", ")", ":", "\n", "        ", "self", ".", "n_best_agents", "=", "[", "]", "# it saves data in the form of (model_dict, training performance, number of steps)", "\n", "assert", "n_best", ">", "0", "and", "type", "(", "n_best", ")", "==", "int", "\n", "assert", "num_avg_episode", ">", "0", "and", "type", "(", "num_avg_episode", ")", "==", "int", "\n", "self", ".", "num_of_bests", "=", "n_best", "\n", "self", ".", "recent_agents", "=", "[", "]", "# it saves agents in the form of (model_dict, number of steps)", "\n", "self", ".", "episodic_performance_previous", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "num_avg_episode", ")", "# (performance, number of steps)", "\n", "self", ".", "episodic_performance_recent", "=", "[", "]", "# (performance, number of steps) ", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "num_avg_episode", "=", "num_avg_episode", "\n", "", "def", "add_agent", "(", "self", ",", "model_dict", ",", "num_step", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.add_agent": [[19, 22], ["save_agent.Agent_History.recent_agents.append", "len"], "methods", ["None"], ["", "def", "add_agent", "(", "self", ",", "model_dict", ",", "num_step", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "episodic_performance_previous", ")", "<", "self", ".", "num_avg_episode", ":", "return", "\n", "self", ".", "recent_agents", ".", "append", "(", "(", "model_dict", ",", "num_step", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.add_training_performance": [[23, 58], ["len", "save_agent.Agent_History.episodic_performance_previous.append", "save_agent.Agent_History.episodic_performance_recent.append", "save_agent.Agent_History.find_n_included_prev", "save_agent.Agent_History.recent_agents.pop", "save_agent.Agent_History.find_n_included_prev", "len", "len", "save_agent.Agent_History.n_best_agents.append", "save_agent.Agent_History.n_best_agents.sort", "len", "save_agent.Agent_History.find_idx_until", "save_agent.Agent_History.episodic_performance_previous.extend", "save_agent.Agent_History.episodic_performance_previous.extend", "save_agent.Agent_History.episodic_performance_recent.clear", "sum", "sum", "len", "len", "save_agent.Agent_History.save_agents", "save_agent.Agent_History.n_best_agents.sort", "save_agent.Agent_History.save_agents", "itertools.islice", "len"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.find_n_included_prev", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.find_n_included_prev", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.find_idx_until", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.save_agents", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.save_agents"], ["", "def", "add_training_performance", "(", "self", ",", "performance", ",", "num_step", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "recent_agents", ")", "==", "0", ":", "\n", "            ", "self", ".", "episodic_performance_previous", ".", "append", "(", "(", "performance", ",", "num_step", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "episodic_performance_recent", ".", "append", "(", "(", "performance", ",", "num_step", ")", ")", "\n", "# if we can evaluate the performance of the oldest agent stored in self.recent_agents", "\n", "while", "self", ".", "find_n_included_prev", "(", "2", "*", "self", ".", "recent_agents", "[", "0", "]", "[", "1", "]", "-", "num_step", ")", "+", "len", "(", "self", ".", "episodic_performance_recent", ")", ">=", "self", ".", "num_avg_episode", ":", "\n", "# find average performance", "\n", "                ", "n_included_prev", "=", "self", ".", "find_n_included_prev", "(", "2", "*", "self", ".", "recent_agents", "[", "0", "]", "[", "1", "]", "-", "num_step", ")", "\n", "#print(n_included_prev, type(n_included_prev))", "\n", "#print(len(self.episodic_performance_recent))", "\n", "#print(self.episodic_performance_previous)", "\n", "#print(self.episodic_performance_previous[-17:])", "\n", "#print(self.episodic_performance_previous[-n_included_prev:])", "\n", "\n", "p", "=", "(", "sum", "(", "[", "_p", "for", "_p", ",", "_i", "in", "itertools", ".", "islice", "(", "self", ".", "episodic_performance_previous", ",", "len", "(", "self", ".", "episodic_performance_previous", ")", "-", "n_included_prev", ",", "None", ")", "]", ")", "+", "sum", "(", "[", "_p", "for", "_p", ",", "_i", "in", "self", ".", "episodic_performance_recent", "]", ")", ")", "/", "(", "n_included_prev", "+", "len", "(", "self", ".", "episodic_performance_recent", ")", ")", "\n", "m", ",", "s", "=", "self", ".", "recent_agents", ".", "pop", "(", "0", ")", "\n", "if", "len", "(", "self", ".", "n_best_agents", ")", "<", "self", ".", "num_of_bests", ":", "\n", "                    ", "self", ".", "n_best_agents", ".", "append", "(", "(", "m", ",", "p", ",", "s", ")", ")", "\n", "self", ".", "n_best_agents", ".", "sort", "(", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "if", "len", "(", "self", ".", "n_best_agents", ")", "==", "self", ".", "num_of_bests", ":", "self", ".", "save_agents", "(", ")", "\n", "", "elif", "p", ">", "self", ".", "n_best_agents", "[", "-", "1", "]", "[", "1", "]", ":", "\n", "                    ", "self", ".", "n_best_agents", "[", "-", "1", "]", "=", "(", "m", ",", "p", ",", "s", ")", "\n", "self", ".", "n_best_agents", ".", "sort", "(", "reverse", "=", "True", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ")", "\n", "self", ".", "save_agents", "(", ")", "\n", "# move data from self.episodic_performance_recent to self.episodic_performance_previous", "\n", "", "if", "len", "(", "self", ".", "recent_agents", ")", ">", "0", ":", "\n", "                    ", "i", "=", "self", ".", "find_idx_until", "(", "self", ".", "recent_agents", "[", "0", "]", "[", "1", "]", ")", "\n", "self", ".", "episodic_performance_previous", ".", "extend", "(", "self", ".", "episodic_performance_recent", "[", ":", "i", "]", ")", "\n", "self", ".", "episodic_performance_recent", "=", "self", ".", "episodic_performance_recent", "[", "i", ":", "]", "\n", "", "else", ":", "\n", "                    ", "self", ".", "episodic_performance_previous", ".", "extend", "(", "self", ".", "episodic_performance_recent", ")", "\n", "self", ".", "episodic_performance_recent", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.find_n_included_prev": [[59, 65], ["len"], "methods", ["None"], ["", "", "", "", "def", "find_n_included_prev", "(", "self", ",", "from_step", ")", ":", "\n", "        ", "i", "=", "0", "\n", "for", "p", ",", "s", "in", "self", ".", "episodic_performance_previous", ":", "\n", "            ", "if", "s", ">=", "from_step", ":", "break", "\n", "i", "+=", "1", "\n", "", "return", "len", "(", "self", ".", "episodic_performance_previous", ")", "-", "i", "\n", "", "def", "find_idx_until", "(", "self", ",", "until_step", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.find_idx_until": [[65, 71], ["None"], "methods", ["None"], ["", "def", "find_idx_until", "(", "self", ",", "until_step", ")", ":", "\n", "        ", "i", "=", "0", "\n", "for", "p", ",", "s", "in", "self", ".", "episodic_performance_recent", ":", "\n", "            ", "if", "s", ">", "until_step", ":", "break", "\n", "i", "+=", "1", "\n", "", "return", "i", "\n", "", "def", "save_agents", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.save_agents": [[71, 74], ["enumerate", "torch.save", "os.path.join"], "methods", ["None"], ["", "def", "save_agents", "(", "self", ")", ":", "\n", "        ", "for", "i", ",", "(", "m", ",", "p", ",", "s", ")", "in", "enumerate", "(", "self", ".", "n_best_agents", ")", ":", "\n", "            ", "torch", ".", "save", "(", "(", "m", ",", "s", ")", ",", "os", ".", "path", ".", "join", "(", "self", ".", "args", ".", "env", ",", "'{}_{}_{}.pth'", ".", "format", "(", "self", ".", "args", ".", "currentTask", ",", "self", ".", "args", ".", "comment", ",", "i", "+", "1", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.main.main": [[9, 32], ["torch.set_num_threads", "arguments.get_args", "common.wrappers.make_atari", "common.wrappers.wrap_atari_dqn", "common.utils.set_global_seeds", "common.wrappers.wrap_atari_dqn.seed", "train", "common.wrappers.wrap_atari_dqn.close", "open", "f.write", "test.test", "common.wrappers.wrap_atari_dqn.close", "datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.arguments.get_args", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.make_atari", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.wrap_atari_dqn", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.set_global_seeds", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.train", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test"], ["def", "main", "(", ")", ":", "\n", "    ", "torch", ".", "set_num_threads", "(", "2", ")", "# we need to constrain the number of threads; it can default to a large value", "\n", "args", "=", "get_args", "(", ")", "\n", "\n", "# keep a history of the commands that has been executed", "\n", "with", "open", "(", "\"commandl_history.txt\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y/%m/%d %H:%M:%S\\t\"", ")", "+", "' '", ".", "join", "(", "sys", ".", "argv", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "env", "=", "make_atari", "(", "args", ".", "env", ",", "args", ".", "max_episode_steps", ",", "clip_reward", "=", "(", "not", "(", "args", ".", "no_clip", "or", "args", ".", "transform_Q", ")", ")", "and", "(", "not", "args", ".", "evaluate", ")", ")", "\n", "env", "=", "wrap_atari_dqn", "(", "env", ",", "args", ")", "\n", "\n", "set_global_seeds", "(", "args", ".", "seed", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "if", "args", ".", "evaluate", ":", "\n", "        ", "test", "(", "env", ",", "args", ")", "\n", "env", ".", "close", "(", ")", "\n", "return", "\n", "\n", "", "from", "train", "import", "train", "as", "train", "\n", "train", "(", "env", ",", "args", ")", "\n", "\n", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.arguments.get_args": [[4, 142], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.is_available"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Deep Q-Learning'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--algorithm'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "# Basic Arguments", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Disable CUDA training'", ")", "\n", "\n", "# Training Arguments", "\n", "parser", ".", "add_argument", "(", "'--max-steps'", ",", "type", "=", "int", ",", "default", "=", "50000000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps to train (equal to actual_frames / 4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--buffer-size'", ",", "type", "=", "int", ",", "default", "=", "1000000", ",", "metavar", "=", "'CAPACITY'", ",", "\n", "help", "=", "'Maximum memory buffer size'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-discard-experience'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly discard half of collected experience data'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-replace-memory'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly replace old experiences by new experiences when the memory replay is full (by default it is first-in-first-out)'", ")", "\n", "parser", ".", "add_argument", "(", "'--update-target'", ",", "type", "=", "int", ",", "default", "=", "8000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Interval of target network update'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-freq'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps between optimization steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "metavar", "=", "'\u03b3'", ",", "\n", "help", "=", "'Discount factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning-start'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'How many steps of the model to collect transitions for before learning starts'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-start'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Start value of epsilon'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-mid'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'Mid value of epsilon (at one million-th step)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-final'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'Final value of epsilon'", ")", "\n", "\n", "# Algorithm Arguments", "\n", "parser", ".", "add_argument", "(", "'--double'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Double Q Learning'", ")", "\n", "parser", ".", "add_argument", "(", "'--dueling'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Dueling Network with Default Evaluation (Avg.) on Advantages'", ")", "\n", "parser", ".", "add_argument", "(", "'--prioritized-replay'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable prioritized experience replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.6", ",", "\n", "help", "=", "'Alpha value for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--ratio-min-prio'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Allowed maximal ratio between the smallest sampling priority and the average priority, which equals the maximal importance sampling weight'", ")", "\n", "parser", ".", "add_argument", "(", "'--prio-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-10", ",", "\n", "help", "=", "'A small number added before computing the priority'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-start'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "\n", "help", "=", "'Start value of beta for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-frames'", ",", "type", "=", "float", ",", "default", "=", "50000000", ",", "\n", "help", "=", "'End step of beta schedule for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--IS-weight-only-smaller'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Divide all importance sampling correction weights by the largest weight, so that they are smaller or equal to one'", ")", "\n", "\n", "# Environment Arguments", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "type", "=", "str", ",", "default", "=", "'PongNoFrameskip-v4'", ",", "\n", "help", "=", "'Environment Name'", ")", "\n", "parser", ".", "add_argument", "(", "'--episode-life'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Whether losing one life is considered as an end of an episode(1) or not(0) from the agent's perspective\"", ")", "\n", "parser", ".", "add_argument", "(", "'--grey'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Change the observation to greyscale (default 1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-stack'", ",", "type", "=", "str", ",", "default", "=", "\"4\"", ",", "\n", "help", "=", "'Number of adjacent observations to stack'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-downscale'", ",", "type", "=", "int", ",", "default", "=", "84", ",", "# we will always crop the frame when it has a height of 250 instead of the default 210 (crop by top 28, bottom 12) ", "\n", "help", "=", "'Downscaling ratio of the frame observation (if <= 10) or image size as the downscaling target (if >10)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-episode-steps'", ",", "type", "=", "int", ",", "default", "=", "20000", ",", "\n", "help", "=", "\"The maximum number of steps allowd before resetting the real episode.\"", ")", "\n", "\n", "# Evaluation Arguments", "\n", "parser", ".", "add_argument", "(", "'--load-model'", ",", "type", "=", "str", ",", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "'Pretrained model names to load (state dict)'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Evaluate only'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-trial'", ",", "type", "=", "int", ",", "default", "=", "400", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluation_interval'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "'Steps for printing statistics'", ")", "\n", "\n", "# Optimization Arguments", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "6.25e-5", ",", "metavar", "=", "'\u03b7'", ",", "\n", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--optim'", ",", "type", "=", "str", ",", "default", "=", "'adam'", ",", "\n", "help", "=", "'Optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--adam-eps'", ",", "type", "=", "float", ",", "default", "=", "1.5e-4", ",", "\n", "help", "=", "'Epsilon of adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu-id'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Which GPU to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta2'", ",", "type", "=", "float", ",", "default", "=", "0.999", ")", "\n", "parser", ".", "add_argument", "(", "'--beta1'", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "'--grad-clip'", ",", "type", "=", "float", ",", "default", "=", "10.", ",", "# when transform-Q is used, it should be 40.", "\n", "help", "=", "\"Gradient clipping norm; 0 corresponds to no gradient clipping\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--silent'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--comment'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "\n", "# A simple option to reproduce the original DQN", "\n", "parser", ".", "add_argument", "(", "'--originalDQN'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To reproduce the original DQN\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--save-best'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To save the model when it performs best during training, averaged over 40 episodes\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "originalDQN", ":", "\n", "        ", "del", "args", ".", "originalDQN", "\n", "# the most important arguments for reproducing the original prioritized dueling DDQN", "\n", "args", ".", "algorithm", "=", "\"DQN\"", "\n", "args", ".", "lr", "=", "6.25e-5", "\n", "args", ".", "adam_eps", "=", "1.5e-4", "\n", "args", ".", "grad_clip", "=", "10.", "\n", "args", ".", "prioritized_replay", "=", "True", "\n", "args", ".", "alpha", "=", "0.6", "\n", "args", ".", "beta_start", "=", "0.4", "\n", "args", ".", "beta_frames", "=", "50000000.", "\n", "args", ".", "auto_init", "=", "False", "\n", "args", ".", "gamma", "=", "0.99", "\n", "args", ".", "double", "=", "True", "\n", "args", ".", "dueling", "=", "True", "\n", "args", ".", "episode_life", "=", "1", "\n", "args", ".", "randomly_replace_memory", "=", "False", "\n", "args", ".", "no_clip", "=", "False", "\n", "args", ".", "transform_Q", "=", "False", "\n", "\n", "", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:{}\"", ".", "format", "(", "args", ".", "gpu_id", ")", "if", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "\n", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.train.train": [[16, 190], ["model.DQN().to", "model.DQN().to", "print", "DQN().to.parameters", "common.utils.update_target", "common.utils.epsilon_scheduler", "common.utils.beta_scheduler", "env.unwrapped.ale.lives", "common.utils.print_args", "args.optim.lower", "print", "env.reset", "range", "time.time", "common.replay_buffer.PrioritizedReplayBuffer", "common.replay_buffer.ReplayBuffer", "torch.SGD", "envs.append", "env.seed", "collections.deque", "collections.deque", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "DQN().to.act", "enumerate", "model.DQN", "model.DQN", "parameters", "optimizers.AdamW", "args.optim.startswith", "range", "range", "copy.deepcopy", "random.randrange", "range", "range", "float", "zip", "env.step", "float", "data_to_store.append", "sum", "parameters", "optimizers.AdamBelief", "common.utils.epsilon_scheduler.", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "data_to_store.clear", "length_list.append", "env.reset", "common.utils.beta_scheduler.", "train.compute_td_loss", "loss_list.append", "off_policy_rate_list.append", "parameters", "optimizers.LaProp", "range", "range", "float", "common.replay_buffer.ReplayBuffer.add", "env.unwrapped.ale.game_over", "reward_list.append", "len", "max", "model.auto_initialize", "len", "common.utils.print_log", "reward_list.clear", "length_list.clear", "loss_list.clear", "kwargs.values", "time.time", "p.numel", "parameters", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "max", "print", "random.randrange", "common.utils.update_target", "DQN().to.parameters", "len", "os.path.isdir", "os.mkdir", "open", "f.write", "collections.deque.append", "numpy.mean", "collections.deque.append", "type", "v.clear", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.save", "torch.save", "torch.save", "DQN().to.state_dict().copy", "numpy.array().reshape", "len", "len", "DQN().to.state_dict", "numpy.array"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.epsilon_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.beta_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_args", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.compute_td_loss", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.add", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.auto_initialize", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_log", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target"], ["def", "train", "(", "env", ",", "args", ")", ":", "\n", "    ", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "\n", "current_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "target_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "print", "(", "'    Total params: %.2fM'", "%", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "current_model", ".", "parameters", "(", ")", ")", "/", "1000000.0", ")", ")", "\n", "for", "para", "in", "target_model", ".", "parameters", "(", ")", ":", "para", ".", "requires_grad", "=", "False", "\n", "update_target", "(", "current_model", ",", "target_model", ")", "\n", "\n", "epsilon_by_frame", "=", "epsilon_scheduler", "(", "args", ".", "eps_start", ",", "args", ".", "eps_mid", ",", "args", ".", "eps_final", ")", "\n", "beta_by_frame", "=", "beta_scheduler", "(", "args", ".", "beta_start", ",", "args", ".", "beta_frames", ")", "\n", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "replay_buffer", "=", "PrioritizedReplayBuffer", "(", "args", ".", "buffer_size", ",", "args", ".", "alpha", ",", "args", ".", "IS_weight_only_smaller", ",", "allowed_avg_min_ratio", "=", "args", ".", "ratio_min_prio", ")", "\n", "", "else", ":", "\n", "        ", "replay_buffer", "=", "ReplayBuffer", "(", "args", ".", "buffer_size", ")", "\n", "\n", "#args.action_space = env.unwrapped.get_action_meanings()", "\n", "", "args", ".", "init_lives", "=", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "print_args", "(", "args", ")", "\n", "\n", "args", ".", "do_update_target", "=", "False", "\n", "# specify the RL algorithm to use ", "\n", "if", "args", ".", "algorithm", "!=", "\"DQN\"", "and", "args", ".", "algorithm", "!=", "\"Residual\"", "and", "args", ".", "algorithm", "!=", "\"CDQN\"", ":", "\n", "        ", "currentTask", "=", "\"DQN\"", "\n", "args", ".", "currentTask", "=", "currentTask", "\n", "", "else", ":", "\n", "        ", "currentTask", "=", "args", ".", "algorithm", "\n", "args", ".", "currentTask", "=", "args", ".", "algorithm", "\n", "\n", "# prepare the optimizer", "\n", "", "lr", "=", "args", ".", "lr", "\n", "beta1", "=", "args", ".", "beta1", "\n", "beta2", "=", "args", ".", "beta2", "\n", "parameters", "=", "current_model", ".", "parameters", "\n", "args", ".", "optim", "=", "args", ".", "optim", ".", "lower", "(", ")", "\n", "if", "args", ".", "optim", "==", "'sgd'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "SGD", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "momentum", "=", "beta1", ")", "\n", "", "elif", "args", ".", "optim", "==", "'adam'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "AdamW", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", ".", "startswith", "(", "\"adamb\"", ")", ":", "\n", "        ", "args", ".", "optim", "=", "\"adambelief\"", "\n", "optimizer", "=", "optimizers", ".", "AdamBelief", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", "==", "'laprop'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "LaProp", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "\"The specified optimizer name {} is non-existent\"", ".", "format", "(", "args", ".", "optim", ")", "\n", "\n", "", "print", "(", "currentTask", ")", "\n", "\n", "\n", "reward_list", ",", "length_list", ",", "loss_list", ",", "off_policy_rate_list", ",", "gen_loss_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "clip_reward", "=", "True", "###", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "# the number of parallelized computation is maximally \"arg.train_freq\" to guarantee that the computation order is still consistent with the original method", "\n", "num_task", "=", "args", ".", "train_freq", "\n", "args", ".", "num_task", "=", "num_task", "\n", "episode_rewards", "=", "[", "0.", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "life_lengths", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "envs", "=", "[", "env", "]", "\n", "for", "_i", "in", "range", "(", "num_task", "-", "1", ")", ":", "envs", ".", "append", "(", "copy", ".", "deepcopy", "(", "env", ")", ")", "\n", "for", "env", "in", "envs", ":", "env", ".", "seed", "(", "random", ".", "randrange", "(", "1000000", ")", ")", "\n", "states", "=", "[", "state", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "rewards", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "evaluation_interval", "=", "args", ".", "evaluation_interval", "\n", "data_to_store", "=", "[", "]", "\n", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "prev_step", "=", "0", "\n", "step_idx", "=", "1", "# initialization of step_idx", "\n", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "if", "args", ".", "save_best", ":", "\n", "        ", "recent_performances", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "40", ")", "\n", "recent_models", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "20", ")", "\n", "best_performance", "=", "-", "float", "(", "\"inf\"", ")", "\n", "", "while", "step_idx", "<=", "args", ".", "max_steps", ":", "\n", "# decide the action", "\n", "        ", "epsilons", "=", "(", "epsilon_by_frame", "(", "idx", ")", "for", "idx", "in", "range", "(", "step_idx", ",", "step_idx", "+", "num_task", ")", ")", "if", "step_idx", ">", "args", ".", "learning_start", "else", "(", "1.", "for", "idx", "in", "range", "(", "num_task", ")", ")", "\n", "\n", "tensored_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "state", ".", "_frames", "for", "state", "in", "states", "]", ")", ".", "reshape", "(", "(", "num_task", ",", "-", "1", ")", "+", "image_shape", ")", ")", ".", "to", "(", "args", ".", "device", ")", ".", "float", "(", ")", ".", "div_", "(", "255.", ")", "\n", "actions", ",", "evaluateds", ",", "(", "Qss", ",", "bestActions", ")", "=", "current_model", ".", "act", "(", "tensored_states", ",", "epsilons", ")", "\n", "\n", "for", "_i", ",", "(", "env", ",", "state", ",", "action", ",", "Qs", ",", "bestAction", ",", "reward", ")", "in", "enumerate", "(", "zip", "(", "envs", ",", "states", ",", "actions", ",", "Qss", ",", "bestActions", ",", "rewards", ")", ")", ":", "\n", "\n", "# the environment proceeds by one step (4 frames)", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "if", "clip_reward", ":", "\n", "                ", "raw_reward", ",", "reward", "=", "reward", "\n", "", "else", ":", "\n", "                ", "raw_reward", "=", "reward", "\n", "\n", "", "rewards", "[", "_i", "]", "=", "float", "(", "reward", ")", "\n", "\n", "episode_rewards", "[", "_i", "]", "+=", "raw_reward", "\n", "life_lengths", "[", "_i", "]", "+=", "1", "\n", "\n", "# store the transition into the memory replay", "\n", "if", "not", "args", ".", "randomly_discard_experience", "or", "(", "args", ".", "randomly_discard_experience", "and", "random", ".", "random", "(", ")", ">=", "0.5", ")", ":", "# the data may be randomly discarded", "\n", "                ", "data_to_store", ".", "append", "(", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "float", "(", "done", ")", ")", ")", "\n", "", "if", "data_to_store", ":", "\n", "                ", "for", "data", "in", "data_to_store", ":", "\n", "                    ", "replay_buffer", ".", "add", "(", "*", "data", ")", "\n", "if", "args", ".", "randomly_replace_memory", "and", "len", "(", "replay_buffer", ")", ">=", "args", ".", "buffer_size", ":", "\n", "# probably randomly choose an index to replace ", "\n", "                        ", "replay_buffer", ".", "_next_idx", "=", "random", ".", "randrange", "(", "args", ".", "buffer_size", ")", "\n", "", "", "data_to_store", ".", "clear", "(", ")", "\n", "\n", "# record the performance of a trajectory", "\n", "", "if", "done", ":", "\n", "                ", "length_list", ".", "append", "(", "life_lengths", "[", "_i", "]", ")", "\n", "life_lengths", "[", "_i", "]", "=", "0", "\n", "# only the reward of a real full episode is recorded ", "\n", "if", "env", ".", "unwrapped", ".", "ale", ".", "game_over", "(", ")", "or", "\"TimeLimit.truncated\"", "in", "info", ":", "\n", "                    ", "reward_list", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "if", "not", "args", ".", "silent", ":", "\n", "                        ", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "env", ")", ":", "os", ".", "mkdir", "(", "args", ".", "env", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.txt'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ",", "'a'", ")", "as", "f", ":", "\n", "                            ", "f", ".", "write", "(", "'{:.0f}\\t{}\\n'", ".", "format", "(", "step_idx", "*", "4", ",", "episode_rewards", "[", "_i", "]", ")", ")", "\n", "", "if", "args", ".", "save_best", "and", "step_idx", ">", "args", ".", "learning_start", ":", "\n", "                            ", "recent_performances", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "mean_performance", "=", "np", ".", "mean", "(", "recent_performances", ")", "\n", "if", "best_performance", "<", "mean_performance", "and", "len", "(", "recent_performances", ")", ">=", "40", ":", "\n", "                                ", "assert", "len", "(", "recent_models", ")", "==", "20", "\n", "best_performance", "=", "mean_performance", "\n", "torch", ".", "save", "(", "(", "recent_models", "[", "0", "]", ",", "step_idx", ")", ",", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.pth'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ")", "\n", "", "recent_models", ".", "append", "(", "current_model", ".", "state_dict", "(", ")", ".", "copy", "(", ")", ")", "\n", "", "", "episode_rewards", "[", "_i", "]", "=", "0.", "\n", "", "states", "[", "_i", "]", "=", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "[", "_i", "]", "=", "next_state", "\n", "\n", "# optimize", "\n", "", "if", "step_idx", "%", "args", ".", "train_freq", "==", "0", "and", "step_idx", ">", "max", "(", "args", ".", "learning_start", ",", "2", "*", "args", ".", "batch_size", ")", ":", "\n", "                ", "beta", "=", "beta_by_frame", "(", "step_idx", ")", "\n", "loss", ",", "off_policy_rate", "=", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", ")", "\n", "loss_list", ".", "append", "(", "loss", ")", ";", "off_policy_rate_list", ".", "append", "(", "off_policy_rate", ")", "\n", "\n", "# update the target network", "\n", "", "if", "step_idx", "%", "args", ".", "update_target", "==", "0", "and", "currentTask", "!=", "\"Residual\"", ":", "\n", "# we defer the update of the target network to the optimization routine to ensure that the target network is not exactly equal to current network", "\n", "                ", "args", ".", "do_update_target", "=", "True", "\n", "#update_target(current_model, target_model)", "\n", "\n", "# print the statistics", "\n", "", "if", "step_idx", "%", "evaluation_interval", "==", "0", ":", "\n", "# it works only if there is at least one episode to report; otherwise \"evaluation_interval\" is increased", "\n", "                ", "if", "len", "(", "reward_list", ")", ">", "0", ":", "\n", "                    ", "kwargs", "=", "{", "}", "\n", "kwargs", "[", "\"Off-Policy\"", "]", "=", "off_policy_rate_list", "\n", "print_log", "(", "step_idx", ",", "prev_step", ",", "prev_time", ",", "reward_list", ",", "length_list", ",", "loss_list", ",", "args", ",", "'{}{:.0e}{}'", ".", "format", "(", "currentTask", ",", "args", ".", "lr", ",", "args", ".", "comment", ")", ",", "**", "kwargs", ")", "\n", "reward_list", ".", "clear", "(", ")", ";", "length_list", ".", "clear", "(", ")", ";", "loss_list", ".", "clear", "(", ")", "\n", "for", "v", "in", "kwargs", ".", "values", "(", ")", ":", "\n", "                        ", "if", "type", "(", "v", ")", "==", "list", ":", "v", ".", "clear", "(", ")", "\n", "", "prev_step", "=", "step_idx", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "", "else", ":", "\n", "                    ", "evaluation_interval", "+=", "args", ".", "evaluation_interval", "\n", "\n", "", "", "step_idx", "+=", "1", "\n", "\n", "", "", "", "i_count", "=", "0", "\n", "accu1", ",", "accu2", "=", "0.", ",", "0.", "\n", "accu_loss", "=", "0.", "\n", "def", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Calculate loss and optimize\n    \"\"\"", "\n", "global", "i_count", ",", "accu1", ",", "accu2", ",", "accu_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.train.compute_td_loss": [[194, 387], ["torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "optimizer.zero_grad", "optimizer.step", "numpy.mean", "math.ceil", "replay_buffer.sample", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "replay_buffer.sample", "torch.ones", "torch.ones", "torch.ones", "weights.to.numpy", "weights.to.to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.smooth_l1_loss", "F.smooth_l1_loss.backward", "list", "torch.from_numpy().to().float().div_.view", "current_model", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "list", "torch.from_numpy().to().float().div_.view", "current_model", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "replay_buffer.update_priorities", "replay_buffer.prop_minimum_priorities", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "common.utils.update_target", "print", "done.astype", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model", "current_model.gather().squeeze", "model.inverse_transform_Q", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.addcmul", "torch.addcmul", "torch.addcmul", "current_model", "current_model.gather().squeeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().to().float().div_.size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "current_model.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "target_model", "numpy.where", "target_mask.astype.astype", "numpy.clip", "model.transform_backpropagate", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "torch.from_numpy().to().float().div_.size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "current_model.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "numpy.clip", "model.transform_backpropagate", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "current_model.parameters", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model", "[].unsqueeze", "target_model", "target_model.gather().squeeze", "next_q_action.squeeze.squeeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "target_model().max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "numpy.abs", "target_model.gather().squeeze().cpu().numpy", "numpy.max", "model.inverse_transform_Q", "model.transform_Q", "numpy.abs", "numpy.abs", "numpy.where", "numpy.abs", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "model.inverse_transform_Q", "model.transform_Q", "numpy.abs", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "math.sqrt", "open", "f.write", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "numpy.stack", "current_model.gather", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model.gather", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "target_model.cpu().numpy", "numpy.stack", "numpy.stack", "torch.cat", "torch.cat", "torch.cat", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "numpy.stack", "torch.cat", "torch.cat", "torch.cat", "target_model.gather", "target_model", "torch.from_numpy().to.unsqueeze", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "model.transform_Q", "torch.from_numpy().to.unsqueeze", "torch.from_numpy().to.unsqueeze", "target_model.gather().squeeze().cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().to.unsqueeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.argmax", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model.max", "torch.stack", "torch.stack", "torch.stack", "target_model.cpu", "numpy.concatenate", "torch.stack", "torch.stack", "torch.stack", "numpy.concatenate", "current_model.detach().cpu().numpy", "torch.stack", "torch.stack", "torch.stack", "torch.mse_loss", "target_model.gather().squeeze", "q_values.gather().squeeze.detach", "current_model.detach().cpu", "q_values.gather().squeeze.detach", "target_model.gather", "next_q_action.squeeze.unsqueeze", "current_model.detach"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.update_priorities", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.prop_minimum_priorities", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_backpropagate", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_backpropagate", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["weights", "=", "torch", ".", "from_numpy", "(", "weights_", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "state_next_state", ",", "action_", ",", "reward_", ",", "done", ",", "indices", "=", "replay_buffer", ".", "sample", "(", "args", ".", "batch_size", ")", "\n", "weights", "=", "torch", ".", "ones", "(", "args", ".", "batch_size", ")", ";", "weights_", "=", "weights", ".", "numpy", "(", ")", ";", "true_weights", "=", "weights_", "\n", "weights", "=", "weights", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "# we move data to GPU in chunks", "\n", "", "state_next_state", "=", "torch", ".", "from_numpy", "(", "state_next_state", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", ".", "float", "(", ")", ".", "div_", "(", "255", ")", "\n", "state", ",", "next_state", "=", "state_next_state", "\n", "action", "=", "torch", ".", "from_numpy", "(", "action_", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "gamma_mul_one_minus_done_", "=", "(", "args", ".", "gamma", "*", "(", "1.", "-", "done", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "if", "args", ".", "currentTask", "==", "\"DQN\"", ":", "\n", "# in some cases these data do not really need to be copied to GPU ", "\n", "        ", "reward", ",", "gamma_mul_one_minus_done", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "(", "reward_", ",", "gamma_mul_one_minus_done_", ")", ")", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "##### start training ##### ", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "# we use \"values\" to refer to Q values for all state-actions, and use \"value\" to refer to Q values for states", "\n", "if", "args", ".", "currentTask", "==", "\"DQN\"", ":", "\n", "        ", "if", "args", ".", "double", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q_values", "=", "current_model", "(", "next_state", ")", "\n", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "1", ")", "# **unsqueeze", "\n", "target_next_q_values", "=", "target_model", "(", "next_state", ")", "\n", "next_q_value", "=", "target_next_q_values", ".", "gather", "(", "1", ",", "next_q_action", ")", ".", "squeeze", "(", ")", "\n", "next_q_action", "=", "next_q_action", ".", "squeeze", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q_value", ",", "next_q_action", "=", "target_model", "(", "next_state", ")", ".", "max", "(", "1", ")", "\n", "\n", "", "", "expected_q_value", "=", "torch", ".", "addcmul", "(", "reward", ",", "tensor1", "=", "next_q_value", ",", "tensor2", "=", "gamma_mul_one_minus_done", ")", "\n", "q_values", "=", "current_model", "(", "state", ")", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "loss", "=", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ",", "reduction", "=", "'none'", ")", "\n", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "diff", "=", "(", "q_value", ".", "detach", "(", ")", "-", "expected_q_value", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "prios", "=", "np", ".", "abs", "(", "diff", ")", "+", "args", ".", "prio_eps", "#", "\n", "", "loss", "=", "(", "loss", "*", "weights", ")", ".", "mean", "(", ")", "/", "2.", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# we report the mean squared error instead of the Huber loss as the loss", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "report_loss", "=", "(", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ",", "reduction", "=", "'none'", ")", "*", "weights", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "", "if", "args", ".", "currentTask", "==", "\"CDQN\"", ":", "\n", "# compute the current and next state values in a single pass ", "\n", "        ", "size", "=", "list", "(", "state_next_state", ".", "size", "(", ")", ")", "\n", "current_and_next_states", "=", "state_next_state", ".", "view", "(", "[", "-", "1", "]", "+", "size", "[", "2", ":", "]", ")", "\n", "# compute the q values and the gradient", "\n", "all_q_values", "=", "current_model", "(", "current_and_next_states", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_values", ",", "next_q_values", "=", "all_q_values", "[", ":", "args", ".", "batch_size", "]", ",", "all_q_values", "[", "args", ".", "batch_size", ":", "2", "*", "args", ".", "batch_size", "]", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", ",", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "\n", "q_value", ",", "next_q_value", "=", "torch", ".", "stack", "(", "(", "q_value", ",", "next_q_value", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "next_q_values_target", "=", "target_model", "(", "next_state", ")", "\n", "if", "args", ".", "double", ":", "\n", "                ", "next_q_value_target", "=", "next_q_values_target", ".", "gather", "(", "1", ",", "next_q_action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "next_q_value_target", "=", "np", ".", "max", "(", "next_q_values_target", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "expected_q_value_self", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value", "\n", "expected_q_value_target", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value_target", "\n", "target_mask", "=", "(", "np", ".", "abs", "(", "q_value", "-", "expected_q_value_target", ")", ">=", "np", ".", "abs", "(", "q_value", "-", "expected_q_value_self", ")", ")", "\n", "expected_q_value", "=", "np", ".", "where", "(", "target_mask", ",", "expected_q_value_target", ",", "expected_q_value_self", ")", "\n", "target_mask", "=", "target_mask", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "", "diff", "=", "q_value", "-", "expected_q_value", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "prio_diff", "=", "diff", "\n", "prios", "=", "np", ".", "abs", "(", "prio_diff", ")", "+", "args", ".", "prio_eps", "\n", "# the Huber loss is used ", "\n", "", "weighted_diff", "=", "weights_", "*", "diff", "\n", "q_value_grad", "=", "1.", "/", "args", ".", "batch_size", "*", "weighted_diff", "\n", "\n", "all_grads", "=", "torch", ".", "zeros_like", "(", "all_q_values", ")", "\n", "# manually backpropagate the gradient through the term \"expected_q_value\" ", "\n", "next_q_value_grad", "=", "-", "(", "1.", "-", "target_mask", ")", "*", "q_value_grad", "\n", "next_q_value_grad", "=", "next_q_value_grad", "*", "gamma_mul_one_minus_done_", "\n", "grads", "=", "torch", ".", "from_numpy", "(", "np", ".", "concatenate", "(", "[", "q_value_grad", ",", "next_q_value_grad", "]", ",", "axis", "=", "0", ")", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "all_grads", ".", "scatter_", "(", "1", ",", "torch", ".", "cat", "(", "[", "action", ",", "next_q_action", "]", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "1", ")", ",", "grads", ")", "\n", "all_q_values", ".", "backward", "(", "all_grads", ")", "# this method makes it run faster ", "\n", "\n", "report_loss", "=", "np", ".", "dot", "(", "diff", ",", "weights_", "*", "diff", ")", "/", "args", ".", "batch_size", "\n", "\n", "", "if", "args", ".", "currentTask", "==", "\"Residual\"", ":", "\n", "# compute the current and next state values in a single pass ", "\n", "        ", "size", "=", "list", "(", "state_next_state", ".", "size", "(", ")", ")", "\n", "current_and_next_states", "=", "state_next_state", ".", "view", "(", "[", "-", "1", "]", "+", "size", "[", "2", ":", "]", ")", "\n", "# compute the q values and the gradient", "\n", "all_q_values", "=", "current_model", "(", "current_and_next_states", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_values", ",", "next_q_values", "=", "all_q_values", "[", ":", "args", ".", "batch_size", "]", ",", "all_q_values", "[", "args", ".", "batch_size", ":", "2", "*", "args", ".", "batch_size", "]", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", ",", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "\n", "q_value", ",", "next_q_value", "=", "torch", ".", "stack", "(", "(", "q_value", ",", "next_q_value", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "expected_q_value", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value", "\n", "\n", "# then compute the q values and the loss", "\n", "", "diff", "=", "q_value", "-", "expected_q_value", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "prio_diff", "=", "diff", "\n", "prios", "=", "np", ".", "abs", "(", "prio_diff", ")", "+", "args", ".", "prio_eps", "\n", "# the Huber loss is used ", "\n", "", "weighted_diff", "=", "weights_", "*", "diff", "\n", "q_value_grad", "=", "1.", "/", "args", ".", "batch_size", "*", "weighted_diff", "\n", "\n", "all_grads", "=", "torch", ".", "zeros_like", "(", "all_q_values", ")", "\n", "# manually backpropagate the gradient through the term \"expected_q_value\" ", "\n", "next_q_value_grad", "=", "-", "q_value_grad", "\n", "next_q_value_grad", "=", "next_q_value_grad", "*", "gamma_mul_one_minus_done_", "\n", "grads", "=", "torch", ".", "from_numpy", "(", "np", ".", "concatenate", "(", "[", "q_value_grad", ",", "next_q_value_grad", "]", ",", "axis", "=", "0", ")", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "all_grads", ".", "scatter_", "(", "1", ",", "torch", ".", "cat", "(", "[", "action", ",", "next_q_action", "]", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "1", ")", ",", "grads", ")", "\n", "all_q_values", ".", "backward", "(", "all_grads", ")", "# this method makes it run faster ", "\n", "\n", "report_loss", "=", "np", ".", "dot", "(", "diff", ",", "weights_", "*", "diff", ")", "/", "args", ".", "batch_size", "\n", "\n", "", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "replay_buffer", ".", "update_priorities", "(", "indices", ",", "prios", ")", "\n", "# gradient clipping ", "\n", "", "if", "args", ".", "grad_clip", ">", "0.", ":", "\n", "        ", "grad_norm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "current_model", ".", "parameters", "(", ")", ",", "max_norm", "=", "args", ".", "grad_clip", ")", "\n", "accu1", "+=", "grad_norm", "\n", "accu2", "+=", "grad_norm", "**", "2", "\n", "", "if", "args", ".", "do_update_target", ":", "update_target", "(", "current_model", ",", "target_model", ")", ";", "args", ".", "do_update_target", "=", "False", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "off_policy_rate", "=", "np", ".", "mean", "(", "(", "np", ".", "argmax", "(", "q_values", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", "!=", "action_", ")", ".", "astype", "(", "np", ".", "float", ")", "*", "true_weights", ")", "\n", "\n", "i_count", "+=", "1", "\n", "accu_loss", "+=", "report_loss", "\n", "report_period", "=", "math", ".", "ceil", "(", "args", ".", "evaluation_interval", "/", "args", ".", "train_freq", ")", "\n", "if", "i_count", "%", "report_period", "==", "0", "and", "accu1", "!=", "0.", ":", "\n", "        ", "print", "(", "\"gradient norm {:.3f} +- {:.3f}\"", ".", "format", "(", "accu1", "/", "report_period", ",", "math", ".", "sqrt", "(", "accu2", "/", "report_period", "-", "(", "accu1", "/", "report_period", ")", "**", "2", ")", ")", ")", "\n", "accu1", ",", "accu2", "=", "0.", ",", "0.", "\n", "if", "not", "args", ".", "silent", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}mse_{}.txt'", ".", "format", "(", "args", ".", "currentTask", ",", "args", ".", "comment", ")", ")", ",", "'a'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'{:.0f}\\t{}\\n'", ".", "format", "(", "(", "i_count", "*", "args", ".", "train_freq", "+", "args", ".", "learning_start", ")", "*", "4", ",", "accu_loss", "/", "report_period", ")", ")", "\n", "", "", "accu_loss", "=", "0.", "\n", "\n", "", "return", "report_loss", ",", "off_policy_rate", "\n", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.test.test": [[10, 30], ["model.DQN().to", "common.utils.epsilon_scheduler", "torch.load", "torch.load", "print", "model_dict.pop", "DQN().to.load_state_dict", "test.test_whole", "results.append", "len", "model.DQN", "os.path.join", "common.utils.epsilon_scheduler.", "open", "data_f.write", "open", "data_f.write", "print", "numpy.mean", "numpy.mean", "numpy.std", "math.sqrt", "numpy.std", "math.sqrt", "len", "len"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.epsilon_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test_whole", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN"], ["def", "test", "(", "env", ",", "args", ")", ":", "\n", "    ", "current_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "epsilon_by_frame", "=", "epsilon_scheduler", "(", "args", ".", "eps_start", ",", "args", ".", "eps_mid", ",", "args", ".", "eps_final", ")", "\n", "#current_model.eval()", "\n", "\n", "results", "=", "[", "]", "\n", "for", "filename", "in", "args", ".", "load_model", ":", "\n", "        ", "model_dict", ",", "step_idx", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}.pth'", ".", "format", "(", "filename", ")", ")", ",", "map_location", "=", "args", ".", "device", ")", "\n", "print", "(", "\"load {} at training step {}\"", ".", "format", "(", "filename", ",", "step_idx", ")", ")", "\n", "model_dict", ".", "pop", "(", "\"scale\"", ",", "None", ")", "\n", "current_model", ".", "load_state_dict", "(", "model_dict", ")", "\n", "\n", "mean", ",", "std", "=", "test_whole", "(", "current_model", ",", "env", ",", "args", ",", "filename", ",", "epsilon_by_frame", "(", "step_idx", ")", ",", "num_trial", "=", "args", ".", "num_trial", ")", "#0.01)", "\n", "results", ".", "append", "(", "mean", ")", "\n", "with", "open", "(", "'{}.txt'", ".", "format", "(", "args", ".", "env", ")", ",", "'a'", ")", "as", "data_f", ":", "\n", "            ", "data_f", ".", "write", "(", "'{}: {} +- {}\\n'", ".", "format", "(", "filename", ",", "mean", ",", "std", ")", ")", "\n", "", "", "if", "len", "(", "args", ".", "load_model", ")", ">", "1", ":", "\n", "        ", "with", "open", "(", "'{}.txt'", ".", "format", "(", "args", ".", "env", ")", ",", "'a'", ")", "as", "data_f", ":", "\n", "            ", "data_f", ".", "write", "(", "'{} +- {}\\n'", ".", "format", "(", "np", ".", "mean", "(", "results", ")", ",", "np", ".", "std", "(", "results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "results", ")", ")", ")", ")", "\n", "print", "(", "'{} +- {}'", ".", "format", "(", "np", ".", "mean", "(", "results", ")", ",", "np", ".", "std", "(", "results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "results", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.test.test_whole": [[31, 79], ["min", "range", "len", "numpy.mean", "numpy.mean", "print", "envs.append", "env.seed", "env.reset", "len", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "current_model.act", "enumerate", "reversed", "mark_remove.clear", "numpy.std", "math.sqrt", "copy.deepcopy", "random.randrange", "range", "range", "zip", "env.step", "envs.pop", "states.pop", "episode_lengths.pop", "episode_rewards.pop", "len", "range", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "env.reset", "len", "env.unwrapped.ale.game_over", "reward_results.append", "length_results.append", "torch.from_numpy().to", "torch.from_numpy().to", "mark_remove.append", "torch.from_numpy", "torch.from_numpy", "numpy.array().reshape", "numpy.array", "len"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "", "", "def", "test_whole", "(", "current_model", ",", "env", ",", "args", ",", "idx", ",", "epsilon", ",", "num_parallel", "=", "16", ",", "num_trial", "=", "400", ")", ":", "#16", "\n", "    ", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "envs", "=", "[", "env", "]", "\n", "num_parallel", "=", "min", "(", "num_parallel", ",", "num_trial", ")", "\n", "for", "_i", "in", "range", "(", "num_parallel", "-", "1", ")", ":", "envs", ".", "append", "(", "copy", ".", "deepcopy", "(", "env", ")", ")", "\n", "for", "env", "in", "envs", ":", "env", ".", "seed", "(", "random", ".", "randrange", "(", "1000000", ")", ")", "\n", "states", "=", "[", "env", ".", "reset", "(", ")", "for", "env", "in", "envs", "]", "\n", "episode_rewards", "=", "[", "0.", "for", "_i", "in", "range", "(", "num_parallel", ")", "]", "\n", "episode_lengths", "=", "[", "0", "for", "_i", "in", "range", "(", "num_parallel", ")", "]", "\n", "reward_results", "=", "[", "]", "\n", "length_results", "=", "[", "]", "\n", "\n", "trial", "=", "len", "(", "states", ")", "\n", "mark_remove", "=", "[", "]", "\n", "while", "len", "(", "envs", ")", ">", "0", ":", "\n", "# decide the action", "\n", "        ", "epsilons", "=", "(", "epsilon", "for", "_", "in", "range", "(", "len", "(", "states", ")", ")", ")", "\n", "\n", "tensored_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "state", ".", "_frames", "for", "state", "in", "states", "]", ")", ".", "reshape", "(", "(", "len", "(", "states", ")", ",", "-", "1", ")", "+", "image_shape", ")", ")", ".", "to", "(", "args", ".", "device", ")", ".", "float", "(", ")", ".", "div_", "(", "255.", ")", "\n", "actions", ",", "evaluateds", ",", "(", "Qss", ",", "bestActions", ")", "=", "current_model", ".", "act", "(", "tensored_states", ",", "epsilons", ")", "\n", "for", "_i", ",", "(", "env", ",", "action", ")", "in", "enumerate", "(", "zip", "(", "envs", ",", "actions", ")", ")", ":", "\n", "\n", "# the environment proceeds by one step (4 frames)", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "episode_rewards", "[", "_i", "]", "+=", "reward", "\n", "episode_lengths", "[", "_i", "]", "+=", "1", "\n", "\n", "if", "done", ":", "\n", "                ", "if", "env", ".", "unwrapped", ".", "ale", ".", "game_over", "(", ")", "or", "\"TimeLimit.truncated\"", "in", "info", ":", "\n", "                    ", "reward_results", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", ";", "length_results", ".", "append", "(", "episode_lengths", "[", "_i", "]", ")", "\n", "episode_rewards", "[", "_i", "]", "=", "0.", ";", "episode_lengths", "[", "_i", "]", "=", "0", "\n", "if", "trial", "<", "num_trial", ":", "\n", "                        ", "trial", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "mark_remove", ".", "append", "(", "_i", ")", "\n", "", "", "states", "[", "_i", "]", "=", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "[", "_i", "]", "=", "next_state", "\n", "", "", "for", "_i", "in", "reversed", "(", "mark_remove", ")", ":", "\n", "            ", "envs", ".", "pop", "(", "_i", ")", ";", "states", ".", "pop", "(", "_i", ")", ";", "episode_lengths", ".", "pop", "(", "_i", ")", ";", "episode_rewards", ".", "pop", "(", "_i", ")", "\n", "", "mark_remove", ".", "clear", "(", ")", "\n", "\n", "", "mean_reward", "=", "np", ".", "mean", "(", "reward_results", ")", "\n", "std_reward", "=", "np", ".", "std", "(", "reward_results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "reward_results", ")", ")", "\n", "mean_length", "=", "np", ".", "mean", "(", "length_results", ")", "\n", "print", "(", "\"Test Result - Reward {:.2f}+-{:.2f} Length {:.1f} for {}\"", ".", "format", "(", "mean_reward", ",", "std_reward", ",", "mean_length", ",", "idx", ")", ")", "\n", "return", "mean_reward", ",", "std_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.DQNBase.__init__": [[30, 57], ["torch.Module.__init__", "model.Flatten", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.DQNBase.modules", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "type", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "module.bias.data.zero_", "model.DQNBase._feature_size"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size"], ["        ", "super", "(", "DQNBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "num_actions", "=", "env", ".", "action_space", ".", "n", "\n", "self", ".", "Linear", "=", "Linear", "# We have overridden the \"reset_parameters\" method for a more well-principled initialization", "\n", "\n", "self", ".", "flatten", "=", "Flatten", "(", ")", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ",", "padding", "=", "0", "if", "self", ".", "input_shape", "[", "1", "]", "!=", "105", "else", "2", ")", ",", "\n", "#nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0 if self.input_shape[1]!=105 else 2),", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "for", "module", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "type", "(", "module", ")", "==", "nn", ".", "Conv2d", ":", "init", ".", "kaiming_uniform_", "(", "module", ".", "weight", ".", "data", ",", "nonlinearity", "=", "\"relu\"", ",", "a", "=", "0.", ")", ";", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", ",", "self", ".", "num_actions", ")", "\n", ")", "\n", "self", ".", "out_bias", "=", "self", ".", "fc", "[", "-", "1", "]", ".", "bias", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "**", "kwargs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.DQNBase.forward": [[58, 63], ["model.DQNBase.features", "model.DQNBase.flatten", "model.DQNBase.fc"], "methods", ["None"], ["        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "x", "\n", "\n", "", "def", "_feature_size", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.DQNBase._feature_size": [[64, 66], ["model.DQNBase.features().view().size", "model.DQNBase.features().view", "model.DQNBase.features", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["        ", "return", "self", ".", "features", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n", "", "def", "act", "(", "self", ",", "state", ",", "epsilon", ",", "**", "kwargs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.DQNBase.act": [[67, 93], ["state.unsqueeze.unsqueeze.dim", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "state.unsqueeze.unsqueeze.unsqueeze", "model.DQNBase.forward().cpu().numpy().squeeze", "numpy.argmax", "random.random", "random.randrange", "state.unsqueeze.unsqueeze.dim", "enumerate", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.DQNBase.forward().cpu().numpy", "numpy.argmax", "numpy.copy", "state.unsqueeze.unsqueeze.size", "model.DQNBase.forward().cpu().numpy", "random.random", "random.randrange", "model.DQNBase.forward().cpu", "model.DQNBase.forward().cpu", "model.DQNBase.forward", "model.DQNBase.forward"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward"], ["        ", "\"\"\"\n        Parameters\n        ----------\n        state       torch.Tensor with appropritate device type\n        epsilon     epsilon for epsilon-greedy\n        \"\"\"", "\n", "if", "state", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "state", "=", "state", ".", "unsqueeze", "(", "0", ")", "\n", "q_values", "=", "self", ".", "forward", "(", "state", ",", "**", "kwargs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", "\n", "bestAction", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "0", ")", "\n", "", "if", "random", ".", "random", "(", ")", ">=", "epsilon", ":", "\n", "                ", "action", "=", "bestAction", "\n", "", "else", ":", "\n", "                ", "action", "=", "random", ".", "randrange", "(", "self", ".", "num_actions", ")", "\n", "", "", "elif", "state", ".", "dim", "(", ")", "==", "4", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "q_values", "=", "self", ".", "forward", "(", "state", ",", "**", "kwargs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "bestAction", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "1", ")", "\n", "action", "=", "np", ".", "copy", "(", "bestAction", ")", "\n", "", "for", "i", ",", "e", "in", "enumerate", "(", "epsilon", ")", ":", "\n", "                ", "if", "random", ".", "random", "(", ")", "<", "e", ":", "\n", "                    ", "action", "[", "i", "]", "=", "random", ".", "randrange", "(", "self", ".", "num_actions", ")", "\n", "", "", "", "else", ":", "assert", "False", ",", "\"The input state has an invalid shape {}\"", ".", "format", "(", "state", ".", "size", "(", ")", ")", "\n", "return", "action", ",", "action", "==", "bestAction", ",", "(", "q_values", ",", "bestAction", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.DuelingDQN.__init__": [[100, 128], ["model.DQNBase.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.DuelingDQN.register_buffer", "model.DuelingDQN.fc[].weight.register_hook", "model.DuelingDQN.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DuelingDQN.Linear", "model.DuelingDQN.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DuelingDQN.Linear", "model.DuelingOutput", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zip", "model.DuelingDQN.fc[].weight.zero_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "model.DuelingDQN._feature_size", "model.DuelingDQN._feature_size", "model.DuelingDQN.fc[].parameters", "model.DuelingDQN.advantage[].parameters", "model.DuelingDQN.value[].parameters"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size"], ["        ", "super", "(", "DuelingDQN", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "advantage", "=", "self", ".", "fc", "\n", "self", ".", "value", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", ",", "1", ")", "\n", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", "*", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", "*", "2", ",", "self", ".", "num_actions", "+", "1", ")", ",", "\n", "DuelingOutput", "(", "self", ".", "num_actions", ")", "\n", ")", "\n", "# rewrite the parameters of \"self.advantage\" and \"self.value\" into \"self.fc\" so that they are combined into a single computation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "p", ",", "p_a", ",", "p_v", "in", "zip", "(", "self", ".", "fc", "[", "0", "]", ".", "parameters", "(", ")", ",", "self", ".", "advantage", "[", "0", "]", ".", "parameters", "(", ")", ",", "self", ".", "value", "[", "0", "]", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "p", "[", ":", "512", "]", "=", "p_a", ";", "p", "[", "512", ":", "512", "*", "2", "]", "=", "p_v", "\n", "", "self", ".", "fc", "[", "2", "]", ".", "weight", ".", "zero_", "(", ")", "\n", "self", ".", "fc", "[", "2", "]", ".", "weight", "[", ":", "self", ".", "num_actions", ",", ":", "512", "]", "=", "self", ".", "advantage", "[", "2", "]", ".", "weight", ";", "self", ".", "fc", "[", "2", "]", ".", "weight", "[", "-", "1", ",", "512", ":", "512", "*", "2", "]", "=", "self", ".", "value", "[", "2", "]", ".", "weight", "\n", "self", ".", "fc", "[", "2", "]", ".", "bias", "[", ":", "self", ".", "num_actions", "]", "=", "self", ".", "advantage", "[", "2", "]", ".", "bias", ";", "self", ".", "fc", "[", "2", "]", ".", "bias", "[", "-", "1", "]", "=", "self", ".", "value", "[", "2", "]", ".", "bias", "\n", "del", "self", ".", "value", ",", "self", ".", "advantage", "\n", "# mask the backpropagated gradient on \"self.fc[2].weight\"", "\n", "", "self", ".", "register_buffer", "(", "'grad_mask'", ",", "torch", ".", "zeros", "(", "self", ".", "num_actions", "+", "1", ",", "512", "*", "2", ")", ")", "\n", "self", ".", "grad_mask", "[", ":", "self", ".", "num_actions", ",", ":", "512", "]", "=", "1.", ";", "self", ".", "grad_mask", "[", "-", "1", ",", "512", ":", "512", "*", "2", "]", "=", "1.", "\n", "self", ".", "dueling_grad_hook", "=", "self", ".", "fc", "[", "2", "]", ".", "weight", ".", "register_hook", "(", "lambda", "grad", ":", "self", ".", "grad_mask", "*", "grad", ")", "\n", "\n", "self", ".", "out_bias", "=", "self", ".", "fc", "[", "-", "2", "]", ".", "bias", "\n", "\n", "", "", "class", "DuelingOutput", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.DuelingOutput.__init__": [[130, 143], ["torch.Module.__init__", "model.DuelingOutput.register_buffer", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["        ", "super", "(", "DuelingOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'output_matrix'", ",", "torch", ".", "Tensor", "(", "num_actions", ",", "num_actions", "+", "1", ")", ")", "\n", "# set the \"-advantage.mean(1, keepdim=True)\" term", "\n", "self", ".", "output_matrix", "[", ":", ",", ":", "]", "=", "-", "1.", "/", "num_actions", "\n", "# set the last input dim, the average value, added to all Qs", "\n", "self", ".", "output_matrix", "[", ":", ",", "-", "1", "]", "=", "1.", "\n", "# set the diagonal term", "\n", "for", "i", "in", "range", "(", "num_actions", ")", ":", "\n", "            ", "self", ".", "output_matrix", "[", "i", ",", "i", "]", "=", "(", "num_actions", "-", "1", ")", "/", "num_actions", "\n", "# this complete the definition of \"output_matrix\", which computes \"value + (advantage - advantage.mean(1, keepdim=True)) * rescale \"", "\n", "", "assert", "not", "self", ".", "output_matrix", ".", "requires_grad", "\n", "\n", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.DuelingOutput.forward": [[144, 146], ["torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["None"], ["        ", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "output_matrix", ",", "None", ")", "\n", "\n", "", "", "class", "Flatten", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.Flatten.forward": [[148, 150], ["x.view", "x.size"], "methods", ["None"], ["        ", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n", "", "", "class", "Linear", "(", "nn", ".", "Linear", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.Linear.reset_parameters": [[152, 159], ["torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "model.Linear.bias.data.zero_"], "methods", ["None"], ["        ", "init", ".", "kaiming_uniform_", "(", "self", ".", "weight", ",", "nonlinearity", "=", "\"relu\"", ",", "a", "=", "0.", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "#fan_in, _  = init._calculate_fan_in_and_fan_out(self.weight)", "\n", "#bound = 1./math.sqrt(fan_in)", "\n", "#init.uniform_(self.bias, -bound, bound)", "\n", "            ", "self", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.DQN": [[13, 19], ["model.DuelingDQN", "model.DQNBase"], "function", ["None"], ["    ", "if", "args", ".", "dueling", ":", "\n", "        ", "model", "=", "DuelingDQN", "(", "env", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "DQNBase", "(", "env", ")", "\n", "", "return", "model", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.inverse_transform_Q": [[163, 168], ["type", "Qs.sign", "type", "model.jit_inverse_transform_Q", "numpy.sign", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "math.sqrt", "Qs.abs", "abs"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.jit_inverse_transform_Q"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.jit_inverse_transform_Q": [[169, 172], ["numba.njit", "numpy.sign", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.transform_Q": [[173, 178], ["type", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "type", "model.jit_transform_Q", "numpy.sign", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "math.sqrt", "Qs.abs", "abs"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.jit_transform_Q"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.jit_transform_Q": [[179, 182], ["numba.njit", "numpy.sign", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.d_transform_Q": [[184, 187], ["numba.njit", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.d_inverse_transform_Q": [[189, 192], ["numba.njit", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.transform_backpropagate": [[193, 199], ["numba.njit", "next_q_value_grad.astype", "model.d_transform_Q", "model.d_inverse_transform_Q"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.d_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.d_inverse_transform_Q"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.model.auto_initialize": [[201, 220], ["common.heuristics.get_initialization_stat", "print", "max", "print"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.get_initialization_stat", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.optimizers.AdamW.__init__": [[21, 36], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.optimizers.AdamW.__setstate__": [[37, 41], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.optimizers.AdamW.step": [[42, 120], ["closure", "math.sqrt", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.data.addcdiv_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "lr", ",", "eps", ",", "amsgrad", "=", "group", "[", "'lr'", "]", ",", "group", "[", "'eps'", "]", ",", "group", "[", "'amsgrad'", "]", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "lr", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'AdamW does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "state", "[", "'PopArt_rescale'", "]", "=", "1.", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "'max_exp_avg_sq'", "in", "state", ":", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# We incorporate the term group['lr'] into the momentum, and define the bias_correction1 such that it respects the possibly moving group['lr']", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "lr", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1. - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "lr", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "*", "sqrt_bias_correction2", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"step_size\" and \"eps\"", "\n", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "rescaling", "=", "state", "[", "'PopArt_rescale'", "]", "\n", "exp_avg", ".", "mul_", "(", "beta1", "*", "rescaling", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "(", "1.", "-", "beta1", ")", "*", "lr", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", "**", "2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "# 'eps' is first multiplied by sqrt_bias_correction2 and then divided by sqrt_bias_correction2", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.optimizers.AdamBelief.__init__": [[136, 151], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamBelief", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.optimizers.AdamBelief.__setstate__": [[152, 156], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.optimizers.AdamBelief.step": [[157, 234], ["closure", "math.sqrt", "exp_avg.mul_().add_", "exp_avg_var.mul_().addcmul_", "p.data.addcdiv_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.max", "max_exp_avg_var.sqrt().add_", "exp_avg_var.sqrt().add_", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_var.mul_", "max_exp_avg_var.sqrt", "exp_avg_var.sqrt"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "lr", ",", "eps", ",", "amsgrad", "=", "group", "[", "'lr'", "]", ",", "group", "[", "'eps'", "]", ",", "group", "[", "'amsgrad'", "]", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "lr", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'AdamW does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_var", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_var'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "'max_exp_avg_var'", "in", "state", ":", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "max_exp_avg_var", "=", "state", "[", "'max_exp_avg_var'", "]", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# We define the bias_correction1 such that it respects the possibly moving \"group['lr']\"", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "lr", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1. - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "lr", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "*", "sqrt_bias_correction2", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"step_size\" and \"eps\"", "\n", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "diff", "=", "grad", "-", "exp_avg", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "(", "1.", "-", "beta1", ")", ")", "\n", "exp_avg_var", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "diff", ",", "diff", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_var", ",", "exp_avg_var", ",", "out", "=", "max_exp_avg_var", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "# 'eps' is first multiplied by sqrt_bias_correction2 and then divided by sqrt_bias_correction2", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", "*", "lr", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.optimizers.LaProp.__init__": [[238, 254], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "4e-4", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-15", ",", "\n", "weight_decay", "=", "0.", ",", "amsgrad", "=", "False", ",", "centered", "=", "False", ")", ":", "\n", "        ", "self", ".", "centered", "=", "centered", "\n", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "LaProp", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.optimizers.LaProp.__setstate__": [[255, 259], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "LaProp", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secB.5.optimizers.LaProp.step": [[260, 349], ["closure", "math.sqrt", "exp_avg_sq.mul_().addcmul_", "denom.addcmul.addcmul.sqrt().add_", "exp_avg.mul_().addcdiv_", "p.data.add_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "exp_mean_avg_sq.mul_().add_", "torch.zeros_like", "exp_avg_sq.mul_", "denom.addcmul.addcmul.addcmul", "torch.max", "denom.addcmul.addcmul.sqrt", "exp_avg.mul_", "exp_mean_avg_sq.mul_"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'LaProp does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "state", "[", "'exp_mean_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "state", "[", "'PopArt_rescale'", "]", "=", "1.", "\n", "state", "[", "'Momentum_rescale'", "]", "=", "1.", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "self", ".", "centered", ":", "\n", "                    ", "exp_mean_avg_sq", "=", "state", "[", "'exp_mean_avg_sq'", "]", "\n", "", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "group", "[", "'lr'", "]", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1 - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "group", "[", "'lr'", "]", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "rescaling", "=", "state", "[", "'PopArt_rescale'", "]", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", "**", "2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "\n", "denom", "=", "exp_avg_sq", "\n", "if", "self", ".", "centered", ":", "\n", "                    ", "exp_mean_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "beta2", ")", "\n", "if", "state", "[", "'step'", "]", ">", "5", ":", "\n", "                        ", "denom", "=", "denom", ".", "addcmul", "(", "exp_mean_avg_sq", ",", "exp_mean_avg_sq", ",", "value", "=", "-", "1.", ")", "\n", "\n", "", "", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "(", "self", ".", "centered", "and", "state", "[", "'step'", "]", "<=", "5", ")", ":", "\n", "# Maintains the maximum of all (centered) 2nd moment running avg. till now", "\n", "                        ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "denom", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", "\n", "\n", "", "", "denom", "=", "denom", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", "*", "sqrt_bias_correction2", ")", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"exp_avg\" and \"eps\"", "\n", "\n", "momentum_rescaling", "=", "state", "[", "'Momentum_rescale'", "]", "\n", "exp_avg", ".", "mul_", "(", "beta1", "*", "momentum_rescaling", ")", ".", "addcdiv_", "(", "grad", ",", "denom", ",", "value", "=", "(", "1.", "-", "beta1", ")", "*", "group", "[", "'lr'", "]", "*", "sqrt_bias_correction2", ")", "\n", "\n", "p", ".", "data", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.__init__": [[18, 24], ["numpy.ones"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "width", "=", "10", ")", ":", "\n", "        ", "self", ".", "width", "=", "width", "\n", "self", ".", "low_bound", ",", "self", ".", "right_bound", "=", "self", ".", "height", "-", "1", ",", "self", ".", "width", "-", "1", "\n", "self", ".", "render_grid", "=", "np", ".", "ones", "(", "(", "self", ".", "height", ",", "self", ".", "width", ",", "3", ")", ")", "\n", "self", ".", "render_grid", "[", "-", "1", ",", "1", ":", "]", "=", "self", ".", "cliff_color", "\n", "self", ".", "render_grid", "[", "-", "1", ",", "-", "1", "]", "=", "self", ".", "goal_color", "\n", "", "def", "step", "(", "self", ",", "state", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.step": [[24, 39], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "if", "action", "==", "0", "and", "state", "[", "0", "]", ">", "0", ":", "state", "=", "(", "state", "[", "0", "]", "-", "1", ",", "state", "[", "1", "]", ")", "\n", "elif", "action", "==", "1", "and", "state", "[", "0", "]", "<", "self", ".", "low_bound", ":", "state", "=", "(", "state", "[", "0", "]", "+", "1", ",", "state", "[", "1", "]", ")", "\n", "elif", "action", "==", "2", "and", "state", "[", "1", "]", "<", "self", ".", "right_bound", ":", "state", "=", "(", "state", "[", "0", "]", ",", "state", "[", "1", "]", "+", "1", ")", "\n", "elif", "action", "==", "3", "and", "state", "[", "1", "]", ">", "0", ":", "state", "=", "(", "state", "[", "0", "]", ",", "state", "[", "1", "]", "-", "1", ")", "\n", "else", ":", "assert", "False", ",", "\"the action is invalid!\"", "\n", "# prepare reward and done", "\n", "done", "=", "0.", "\n", "reward", "=", "-", "1.", "\n", "# only if the state is at the lowest row, done may be True", "\n", "if", "state", "[", "0", "]", "==", "self", ".", "low_bound", ":", "\n", "            ", "if", "state", "[", "1", "]", ">", "0", ":", "\n", "                ", "done", "=", "1.", "\n", "reward", "=", "0.", "if", "state", "[", "1", "]", "==", "self", ".", "right_bound", "else", "-", "100.", "\n", "", "", "return", "state", ",", "reward", ",", "done", "\n", "", "def", "reset", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.reset": [[39, 41], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "height", "-", "1", ",", "0", ")", "\n", "", "def", "action_invalid", "(", "self", ",", "state", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.action_invalid": [[41, 43], ["None"], "methods", ["None"], ["", "def", "action_invalid", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "return", "(", "action", "==", "0", "and", "state", "[", "0", "]", "==", "0", ")", "or", "(", "action", "==", "1", "and", "state", "[", "0", "]", "==", "self", ".", "low_bound", ")", "or", "(", "action", "==", "2", "and", "state", "[", "1", "]", "==", "self", ".", "right_bound", ")", "or", "(", "action", "==", "3", "and", "state", "[", "1", "]", "==", "0", ")", "\n", "", "def", "flatten_state", "(", "self", ",", "state", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.flatten_state": [[43, 45], ["None"], "methods", ["None"], ["", "def", "flatten_state", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "state", "[", "0", "]", "*", "self", ".", "width", "+", "state", "[", "1", "]", "\n", "", "def", "flatten_state_action", "(", "self", ",", "state", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.flatten_state_action": [[45, 47], ["None"], "methods", ["None"], ["", "def", "flatten_state_action", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "return", "state", "[", "0", "]", "*", "self", ".", "width", "*", "self", ".", "num_action", "+", "state", "[", "1", "]", "*", "self", ".", "num_action", "+", "action", "\n", "", "def", "unflatten_state", "(", "self", ",", "state", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.unflatten_state": [[47, 49], ["None"], "methods", ["None"], ["", "def", "unflatten_state", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "state", "//", "self", ".", "width", ",", "state", "%", "self", ".", "width", "\n", "", "def", "get_random", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.get_random": [[49, 55], ["random.randrange", "cliff walking.GridWorld.get_random", "cliff walking.GridWorld.action_invalid"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.get_random", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.action_invalid"], ["", "def", "get_random", "(", "self", ")", ":", "\n", "        ", "state_action", "=", "random", ".", "randrange", "(", "self", ".", "height", "*", "self", ".", "width", "*", "self", ".", "num_action", ")", "\n", "state", ",", "action", "=", "state_action", "//", "self", ".", "num_action", ",", "state_action", "%", "self", ".", "num_action", "\n", "state", "=", "state", "//", "self", ".", "width", ",", "state", "%", "self", ".", "width", "\n", "if", "not", "(", "(", "state", "[", "0", "]", "==", "self", ".", "low_bound", "and", "state", "[", "1", "]", ">", "0", ")", "or", "self", ".", "action_invalid", "(", "state", ",", "action", ")", ")", ":", "return", "state", ",", "action", "\n", "else", ":", "return", "self", ".", "get_random", "(", ")", "# resample by recursively calling itself", "\n", "", "def", "render", "(", "self", ",", "q_table", ",", "name", "=", "\"\"", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.render": [[55, 84], ["matplotlib.subplots", "matplotlib.subplots", "ax.imshow", "ax.grid", "ax.set_xticks", "ax.set_yticks", "ax.set_xticklabels", "ax.set_yticklabels", "ax.set_frame_on", "cliff walking.GridWorld.to_action_table", "range", "ax.text", "matplotlib.tight_layout", "matplotlib.tight_layout", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.close", "matplotlib.close", "list", "list", "range", "range", "range", "max", "ax.arrow", "datetime.datetime.now().strftime", "cliff walking.GridWorld.flatten_state", "enumerate", "abs", "datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.to_action_table", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state"], ["", "def", "render", "(", "self", ",", "q_table", ",", "name", "=", "\"\"", ")", ":", "\n", "        ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "ax", ".", "imshow", "(", "self", ".", "render_grid", ",", "extent", "=", "(", "0", ",", "self", ".", "width", ",", "self", ".", "height", ",", "0", ")", ",", "interpolation", "=", "\"none\"", ")", "\n", "ax", ".", "grid", "(", "color", "=", "'black'", ",", "linewidth", "=", "2.5", ")", "\n", "ax", ".", "set_xticks", "(", "list", "(", "range", "(", "0", ",", "self", ".", "width", "+", "1", ")", ")", ")", "\n", "ax", ".", "set_yticks", "(", "list", "(", "range", "(", "0", ",", "self", ".", "height", "+", "1", ")", ")", ")", "\n", "ax", ".", "set_xticklabels", "(", "[", "]", ")", "\n", "ax", ".", "set_yticklabels", "(", "[", "]", ")", "\n", "ax", ".", "set_frame_on", "(", "True", ")", "\n", "actions", "=", "self", ".", "to_action_table", "(", "q_table", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "height", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "self", ".", "width", ")", ":", "\n", "                ", "if", "i", "==", "self", ".", "low_bound", "and", "j", ">", "0", ":", "continue", "\n", "displacement", "=", "0.4", "\n", "qs", "=", "q_table", "[", "self", ".", "flatten_state", "(", "(", "i", ",", "j", ")", ")", "]", "\n", "max_q", "=", "max", "(", "qs", ")", "\n", "# note that variable names 'i' and 'j' are already in use", "\n", "actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "qs", ")", "if", "abs", "(", "q", "-", "max_q", ")", "<", "1e-10", "]", "\n", "for", "action", "in", "actions", ":", "\n", "                    ", "center", "=", "(", "j", "+", "0.5", ",", "i", "+", "0.5", ")", "\n", "if", "action", "==", "0", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", ",", "center", "[", "1", "]", "+", "0.05", ",", "0", ",", "-", "1.", "*", "displacement", ")", "# +displacement", "\n", "elif", "action", "==", "1", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", ",", "center", "[", "1", "]", "-", "0.05", ",", "0", ",", "1.", "*", "displacement", ")", "# -displacement", "\n", "elif", "action", "==", "2", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", "-", "0.05", ",", "center", "[", "1", "]", ",", "1.", "*", "displacement", ",", "0", ")", "# -displacement", "\n", "elif", "action", "==", "3", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", "+", "0.05", ",", "center", "[", "1", "]", ",", "-", "1.", "*", "displacement", ",", "0", ")", "# +displacement", "\n", "ax", ".", "arrow", "(", "*", "x_y_dx_dy", ",", "width", "=", "0.05", ",", "head_width", "=", "0.15", ",", "head_length", "=", "0.15", ",", "length_includes_head", "=", "True", ",", "color", "=", "\"tab:red\"", ")", "\n", "", "", "", "ax", ".", "text", "(", "self", ".", "width", "-", "0.5", ",", "self", ".", "height", "-", "0.5", ",", "\"Goal\"", ",", "horizontalalignment", "=", "\"center\"", ",", "verticalalignment", "=", "\"center\"", ",", "fontsize", "=", "13", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "savefig", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d %H:%M:%S\"", ")", "+", "name", "+", "\".pdf\"", ")", "# the file name", "\n", "plt", ".", "close", "(", ")", "\n", "", "def", "to_action_table", "(", "self", ",", "q_table", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.to_action_table": [[84, 87], ["numpy.array().reshape", "numpy.argmax", "numpy.array"], "methods", ["None"], ["", "def", "to_action_table", "(", "self", ",", "q_table", ")", ":", "\n", "        ", "q_table", "=", "np", ".", "array", "(", "q_table", ")", ".", "reshape", "(", "self", ".", "height", ",", "self", ".", "width", ",", "self", ".", "num_action", ")", "\n", "return", "np", ".", "argmax", "(", "q_table", ",", "axis", "=", "2", ")", "\n", "", "def", "loss_to_truth", "(", "self", ",", "q_table", ",", "truth", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.loss_to_truth": [[87, 96], ["enumerate", "cliff walking.GridWorld.unflatten_state", "enumerate", "cliff walking.GridWorld.action_invalid"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.unflatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.action_invalid"], ["", "def", "loss_to_truth", "(", "self", ",", "q_table", ",", "truth", ")", ":", "\n", "        ", "loss", "=", "0.", "\n", "for", "state", ",", "qs", "in", "enumerate", "(", "q_table", ")", ":", "\n", "            ", "unflattened_state", "=", "self", ".", "unflatten_state", "(", "state", ")", "\n", "if", "unflattened_state", "[", "0", "]", "==", "self", ".", "low_bound", "and", "unflattened_state", "[", "1", "]", ">", "0", ":", "continue", "\n", "for", "action", ",", "q", "in", "enumerate", "(", "qs", ")", ":", "\n", "                ", "if", "self", ".", "action_invalid", "(", "unflattened_state", ",", "action", ")", ":", "continue", "\n", "else", ":", "loss", "+=", "(", "q", "-", "truth", "[", "state", "]", "[", "action", "]", ")", "**", "2", "\n", "", "", "return", "loss", "\n", "", "def", "policy_diff_truth", "(", "self", ",", "q_table", ",", "truth", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.policy_diff_truth": [[96, 113], ["enumerate", "cliff walking.GridWorld.unflatten_state", "max", "max", "len", "enumerate", "enumerate", "abs", "abs"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.unflatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "def", "policy_diff_truth", "(", "self", ",", "q_table", ",", "truth", ")", ":", "\n", "        ", "n", ",", "n_wrong", "=", "0", ",", "0", "\n", "for", "state", ",", "qs", "in", "enumerate", "(", "q_table", ")", ":", "\n", "            ", "unflattened_state", "=", "self", ".", "unflatten_state", "(", "state", ")", "\n", "if", "unflattened_state", "[", "0", "]", "==", "self", ".", "low_bound", "and", "unflattened_state", "[", "1", "]", ">", "0", ":", "continue", "\n", "n", "+=", "1", "\n", "# find the optimal actions ", "\n", "optimal_q", "=", "max", "(", "truth", "[", "state", "]", ")", "\n", "optimal_actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "truth", "[", "state", "]", ")", "if", "abs", "(", "q", "-", "optimal_q", ")", "<", "1e-10", "]", "\n", "# find the actions of the current q_table", "\n", "max_q", "=", "max", "(", "qs", ")", "\n", "current_actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "qs", ")", "if", "abs", "(", "q", "-", "max_q", ")", "<", "1e-10", "]", "\n", "num_action", "=", "len", "(", "current_actions", ")", "\n", "for", "action", "in", "current_actions", ":", "\n", "                ", "if", "action", "not", "in", "optimal_actions", ":", "\n", "                    ", "n_wrong", "+=", "1.", "/", "num_action", "\n", "", "", "", "return", "n_wrong", "/", "n", "\n", "", "def", "loss_bellman", "(", "self", ",", "q_table", ",", "gamma", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.loss_bellman": [[113, 124], ["enumerate", "cliff walking.GridWorld.unflatten_state", "enumerate", "cliff walking.GridWorld.step", "cliff walking.GridWorld.action_invalid", "max", "cliff walking.GridWorld.flatten_state"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.unflatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.action_invalid", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state"], ["", "def", "loss_bellman", "(", "self", ",", "q_table", ",", "gamma", ")", ":", "\n", "        ", "loss", "=", "0.", "\n", "for", "state", ",", "qs", "in", "enumerate", "(", "q_table", ")", ":", "\n", "            ", "state", "=", "self", ".", "unflatten_state", "(", "state", ")", "\n", "for", "action", ",", "q", "in", "enumerate", "(", "qs", ")", ":", "\n", "                ", "if", "(", "state", "[", "0", "]", "==", "self", ".", "low_bound", "and", "state", "[", "1", "]", ">", "0", ")", "or", "self", ".", "action_invalid", "(", "state", ",", "action", ")", ":", "continue", "\n", "next_state", ",", "reward", ",", "done", "=", "self", ".", "step", "(", "state", ",", "action", ")", "\n", "target_q_value", "=", "reward", "+", "(", "1.", "-", "done", ")", "*", "gamma", "*", "max", "(", "q_table", "[", "self", ".", "flatten_state", "(", "next_state", ")", "]", ")", "\n", "loss", "+=", "(", "q", "-", "target_q_value", ")", "**", "2", "\n", "", "", "if", "not", "loss", "<", "1e20", ":", "assert", "False", ",", "\"The optimization diverged\"", "\n", "return", "loss", "\n", "", "def", "greedy_total_reward", "(", "self", ",", "q_table", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.greedy_total_reward": [[124, 143], ["cliff walking.GridWorld.reset", "previous_states.append", "grid.flatten_state", "max", "cliff walking.GridWorld.step", "len", "random.choice", "enumerate", "abs"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "greedy_total_reward", "(", "self", ",", "q_table", ")", ":", "\n", "        ", "previous_states", "=", "[", "]", "\n", "state", "=", "self", ".", "reset", "(", ")", "\n", "total_reward", ",", "done", "=", "0.", ",", "0.", "\n", "while", "not", "done", ":", "\n", "# to keep a memory of states that have been previously seen", "\n", "            ", "previous_states", ".", "append", "(", "state", ")", "\n", "flattened_state", "=", "grid", ".", "flatten_state", "(", "state", ")", "\n", "optimal_q", "=", "max", "(", "q_table", "[", "flattened_state", "]", ")", "\n", "optimal_actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "q_table", "[", "flattened_state", "]", ")", "if", "abs", "(", "q", "-", "optimal_q", ")", "<", "1e-10", "]", "\n", "if", "len", "(", "optimal_actions", ")", "==", "1", ":", "action", "=", "optimal_actions", "[", "0", "]", "\n", "else", ":", "action", "=", "random", ".", "choice", "(", "optimal_actions", ")", "\n", "next_state", ",", "reward", ",", "done", "=", "self", ".", "step", "(", "state", ",", "action", ")", "\n", "# we reject loops and assign a negative reward equal to the cliff", "\n", "if", "next_state", "in", "previous_states", ":", "\n", "                ", "reward", "=", "-", "100.", ";", "done", "=", "1.", "\n", "", "total_reward", "+=", "reward", "\n", "state", "=", "next_state", "\n", "", "return", "total_reward", "\n", "", "def", "disable_invalid_action_q", "(", "self", ",", "q_table", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.GridWorld.disable_invalid_action_q": [[143, 149], ["enumerate", "enumerate", "cliff walking.GridWorld.action_invalid", "cliff walking.GridWorld.unflatten_state", "float"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.action_invalid", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.unflatten_state"], ["", "def", "disable_invalid_action_q", "(", "self", ",", "q_table", ")", ":", "\n", "        ", "for", "state", ",", "qs", "in", "enumerate", "(", "q_table", ")", ":", "\n", "            ", "for", "action", ",", "q", "in", "enumerate", "(", "qs", ")", ":", "\n", "                ", "if", "self", ".", "action_invalid", "(", "self", ".", "unflatten_state", "(", "state", ")", ",", "action", ")", ":", "\n", "                    ", "q_table", "[", "state", "]", "[", "action", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "", "", "", "return", "q_table", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.q_step": [[152, 163], ["zip", "len", "zip", "td_errors.append"], "function", ["None"], ["", "", "def", "q_step", "(", "q_table", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ",", "lr", "=", "0.5", ")", ":", "\n", "    ", "td_errors", "=", "[", "]", "\n", "loss", "=", "0.", "\n", "for", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_action", ",", "next_coefficient", "in", "zip", "(", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ")", ":", "\n", "        ", "td_error", "=", "next_coefficient", "*", "q_table", "[", "next_state", "]", "[", "next_action", "]", "+", "reward", "-", "q_table", "[", "state", "]", "[", "action", "]", "\n", "loss", "+=", "td_error", "**", "2", "\n", "td_errors", ".", "append", "(", "td_error", ")", "\n", "", "batch_size", "=", "len", "(", "states", ")", "\n", "for", "td_error", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_action", ",", "next_coefficient", "in", "zip", "(", "td_errors", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ")", ":", "\n", "        ", "q_table", "[", "state", "]", "[", "action", "]", "+=", "lr", "/", "batch_size", "*", "td_error", "\n", "", "return", "loss", "/", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.rg_step": [[164, 175], ["zip", "len", "zip", "td_errors.append"], "function", ["None"], ["", "def", "rg_step", "(", "q_table", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ",", "lr", "=", "0.5", ")", ":", "\n", "    ", "td_errors", "=", "[", "]", "\n", "loss", "=", "0.", "\n", "for", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_action", ",", "next_coefficient", "in", "zip", "(", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ")", ":", "\n", "        ", "td_error", "=", "next_coefficient", "*", "q_table", "[", "next_state", "]", "[", "next_action", "]", "+", "reward", "-", "q_table", "[", "state", "]", "[", "action", "]", "\n", "loss", "+=", "td_error", "**", "2", "\n", "td_errors", ".", "append", "(", "td_error", ")", "\n", "", "batch_size", "=", "len", "(", "states", ")", "\n", "for", "td_error", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_action", ",", "next_coefficient", "in", "zip", "(", "td_errors", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ")", ":", "\n", "        ", "q_table", "[", "state", "]", "[", "action", "]", "+=", "lr", "/", "batch_size", "*", "td_error", ";", "q_table", "[", "next_state", "]", "[", "next_action", "]", "-=", "lr", "/", "batch_size", "*", "next_coefficient", "*", "td_error", "\n", "", "return", "loss", "/", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.1.cliff walking.train_rand": [[176, 186], ["range", "strategy", "grid.get_random", "grid.step", "numpy.argmax", "grid.flatten_state", "grid.flatten_state", "states.append", "actions.append", "next_states.append", "next_actions.append", "next_coefficients.append", "rewards.append"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.get_random", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state"], ["", "def", "train_rand", "(", "q_table", ",", "grid", ",", "strategy", ",", "gamma", "=", "0.99", ",", "lr", "=", "0.5", ",", "batch_size", "=", "1", ")", ":", "\n", "    ", "states", ",", "actions", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ",", "rewards", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "state", ",", "action", "=", "grid", ".", "get_random", "(", ")", "\n", "next_state", ",", "reward", ",", "done", "=", "grid", ".", "step", "(", "state", ",", "action", ")", "\n", "next_coefficient", "=", "(", "1.", "-", "done", ")", "*", "gamma", "\n", "state", ",", "next_state", "=", "grid", ".", "flatten_state", "(", "state", ")", ",", "grid", ".", "flatten_state", "(", "next_state", ")", "\n", "next_action", "=", "np", ".", "argmax", "(", "q_table", "[", "next_state", "]", ")", "\n", "states", ".", "append", "(", "state", ")", ",", "actions", ".", "append", "(", "action", ")", ",", "next_states", ".", "append", "(", "next_state", ")", ",", "next_actions", ".", "append", "(", "next_action", ")", ",", "next_coefficients", ".", "append", "(", "next_coefficient", ")", ",", "rewards", ".", "append", "(", "reward", ")", "\n", "", "strategy", "(", "q_table", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ",", "lr", "=", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.__init__": [[21, 26], ["wetchicken1d.WetChicken1dEnv.seed", "gym.spaces.Box", "gym.spaces.Discrete", "wetchicken1d.WetChicken1dEnv.reset"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "seed", "(", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "0", ",", "1.0", ",", "(", "1", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "action_space", "=", "spaces", ".", "Discrete", "(", "3", ")", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed": [[27, 30], ["gym.utils.seeding.np_random"], "methods", ["None"], ["", "def", "seed", "(", "self", ",", "seed", "=", "None", ")", ":", "\n", "        ", "self", ".", "np_random", ",", "seed", "=", "seeding", ".", "np_random", "(", "seed", ")", "\n", "return", "[", "seed", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv._get_obs": [[31, 35], ["numpy.zeros"], "methods", ["None"], ["", "def", "_get_obs", "(", "self", ")", ":", "\n", "        ", "obs", "=", "np", ".", "zeros", "(", "(", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "obs", "[", "0", "]", "=", "self", ".", "x", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.reset": [[36, 40], ["wetchicken1d.WetChicken1dEnv._get_obs"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv._get_obs"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "x", "=", "0", "\n", "self", ".", "step_n", "=", "0", "\n", "return", "self", ".", "_get_obs", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.step": [[41, 82], ["numpy.zeros", "numpy.random.uniform", "print", "print"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "done", "=", "0", "\n", "\n", "self", ".", "step_n", "+=", "1", "\n", "\n", "obs", "=", "np", ".", "zeros", "(", "(", "1", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "v", "=", "self", ".", "metadata", "[", "'river_velocity'", "]", "\n", "\n", "if", "action", "==", "0", ":", "# drift", "\n", "            ", "deltax", "=", "0", "\n", "", "elif", "action", "==", "1", ":", "# hold", "\n", "            ", "deltax", "=", "self", ".", "metadata", "[", "'deltax_hold'", "]", "\n", "", "elif", "action", "==", "2", ":", "# paddleback", "\n", "            ", "deltax", "=", "self", ".", "metadata", "[", "'deltax_paddleback'", "]", "\n", "", "else", ":", "assert", "False", ",", "\"The action must be one of 0, 1 and 2\"", "\n", "\n", "newx", "=", "self", ".", "x", "+", "v", "+", "deltax", "\n", "\n", "s", "=", "self", ".", "metadata", "[", "'river_turb'", "]", "\n", "turbx", "=", "np", ".", "random", ".", "uniform", "(", "-", "1", "*", "s", ",", "s", ")", "\n", "\n", "finalx", "=", "newx", "+", "turbx", "\n", "\n", "# fell off?", "\n", "if", "finalx", ">=", "self", ".", "metadata", "[", "'river_waterfall_x'", "]", ":", "\n", "            ", "if", "self", ".", "debug_out", ":", "print", "(", "'FELL'", ")", "\n", "finalx", "=", "0.0", "\n", "done", "=", "1", "\n", "", "elif", "finalx", "<", "0.", ":", "\n", "            ", "finalx", "=", "0.", "\n", "", "self", ".", "x", "=", "finalx", "\n", "\n", "reward", "=", "finalx", "\n", "self", ".", "last_obs", "=", "obs", "\n", "\n", "obs", "[", "0", "]", "=", "finalx", "\n", "\n", "if", "self", ".", "debug_out", ":", "\n", "            ", "print", "(", "'self.last_obs, deltax, turbx, obs, reward, done'", ",", "self", ".", "last_obs", ",", "deltax", ",", "turbx", ",", "obs", ",", "reward", ",", "done", ")", "\n", "", "return", "obs", ",", "reward", ",", "done", ",", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.render": [[84, 86], ["None"], "methods", ["None"], ["", "def", "render", "(", "self", ",", "mode", "=", "\"human\"", ")", ":", "\n", "        ", "return", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.main.main": [[12, 31], ["arguments.get_args", "range", "torch.save", "print", "main.set_seeds", "main.generate_dataset", "train.train", "results.append", "Qs.append", "torch.save", "wetchicken1d.WetChicken1dEnv"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.arguments.get_args", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.main.set_seeds", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.main.generate_dataset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.train"], ["\n", "# keep a history of the commands that has been executed", "\n", "with", "open", "(", "\"commandl_history.txt\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y/%m/%d %H:%M:%S\\t\"", ")", "+", "' '", ".", "join", "(", "sys", ".", "argv", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "env", "=", "make_atari", "(", "args", ".", "env", ",", "args", ".", "max_episode_steps", ",", "clip_reward", "=", "(", "not", "(", "args", ".", "no_clip", "or", "args", ".", "transform_Q", ")", ")", "and", "(", "not", "args", ".", "evaluate", ")", ")", "\n", "env", "=", "wrap_atari_dqn", "(", "env", ",", "args", ")", "\n", "\n", "set_global_seeds", "(", "args", ".", "seed", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "if", "args", ".", "evaluate", ":", "\n", "        ", "test", "(", "env", ",", "args", ")", "\n", "env", ".", "close", "(", ")", "\n", "return", "\n", "\n", "", "from", "train", "import", "train", "as", "train", "\n", "train", "(", "env", ",", "args", ")", "\n", "\n", "env", ".", "close", "(", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.main.generate_dataset": [[32, 44], ["env.reset", "range", "random.randrange", "env.step", "dataset_state.append", "dataset_next_state.append", "dataset_action.append", "dataset_reward.append", "dataset_done.append", "env.reset"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["\n", "\n", "", "if", "__name__", "==", "\"__main__\"", ":", "\n", "    ", "main", "(", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.main.set_seeds": [[45, 53], ["torch.manual_seed", "torch.cuda.is_available", "numpy.random.seed", "random.seed", "torch.cuda.manual_seed_all"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.arguments.get_args": [[4, 62], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.is_available"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Deep Q-Learning'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--algorithm'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "# Basic Arguments", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Disable CUDA training'", ")", "\n", "\n", "# Training Arguments", "\n", "parser", ".", "add_argument", "(", "'--max-steps'", ",", "type", "=", "int", ",", "default", "=", "50000000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps to train (equal to actual_frames / 4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--buffer-size'", ",", "type", "=", "int", ",", "default", "=", "1000000", ",", "metavar", "=", "'CAPACITY'", ",", "\n", "help", "=", "'Maximum memory buffer size'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-discard-experience'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly discard half of collected experience data'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-replace-memory'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly replace old experiences by new experiences when the memory replay is full (by default it is first-in-first-out)'", ")", "\n", "parser", ".", "add_argument", "(", "'--update-target'", ",", "type", "=", "int", ",", "default", "=", "8000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Interval of target network update'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-freq'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps between optimization steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "metavar", "=", "'\u03b3'", ",", "\n", "help", "=", "'Discount factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning-start'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'How many steps of the model to collect transitions for before learning starts'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-start'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Start value of epsilon'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-mid'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'Mid value of epsilon (at one million-th step)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-final'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'Final value of epsilon'", ")", "\n", "\n", "# Algorithm Arguments", "\n", "parser", ".", "add_argument", "(", "'--double'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Double Q Learning'", ")", "\n", "parser", ".", "add_argument", "(", "'--dueling'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Dueling Network with Default Evaluation (Avg.) on Advantages'", ")", "\n", "parser", ".", "add_argument", "(", "'--prioritized-replay'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable prioritized experience replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.6", ",", "\n", "help", "=", "'Alpha value for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--ratio-min-prio'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Allowed maximal ratio between the smallest sampling priority and the average priority, which equals the maximal importance sampling weight'", ")", "\n", "parser", ".", "add_argument", "(", "'--prio-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-10", ",", "\n", "help", "=", "'A small number added before computing the priority'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-start'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "\n", "help", "=", "'Start value of beta for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-frames'", ",", "type", "=", "float", ",", "default", "=", "50000000", ",", "\n", "help", "=", "'End step of beta schedule for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--IS-weight-only-smaller'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Divide all importance sampling correction weights by the largest weight, so that they are smaller or equal to one'", ")", "\n", "\n", "# Environment Arguments", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "type", "=", "str", ",", "default", "=", "'PongNoFrameskip-v4'", ",", "\n", "help", "=", "'Environment Name'", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.train.train": [[10, 41], ["model.DQNBase", "copy.deepcopy", "copy.deepcopy.load_state_dict", "copy.deepcopy.parameters", "torch.Adam", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "range", "model.DQNBase.to", "copy.deepcopy.to", "model.DQNBase.state_dict", "model.DQNBase.parameters", "torch.no_grad", "torch.no_grad", "torch.no_grad", "Qs.append", "train.adjust_lr", "train.shuffle_dateset", "train.optimize_loss", "print", "torch.tensor", "torch.tensor", "torch.tensor", "model.DQNBase.cpu().numpy", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "train.test", "performance_results.append", "torch.no_grad", "torch.no_grad", "torch.no_grad", "Qs.append", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "model.DQNBase.cpu().numpy", "numpy.arange", "model.DQNBase.cpu", "model.DQNBase.cpu", "model.DQNBase", "model.DQNBase"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.train.adjust_lr", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.train.shuffle_dateset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.train.optimize_loss", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test"], ["\n", "from", "common", ".", "utils", "import", "epsilon_scheduler", ",", "beta_scheduler", ",", "update_target", ",", "print_log", ",", "load_model", ",", "print_args", "\n", "from", "model", "import", "DQN", "\n", "from", "common", ".", "replay_buffer", "import", "ReplayBuffer", ",", "PrioritizedReplayBuffer", "\n", "#from matplotlib import pyplot", "\n", "\n", "def", "train", "(", "env", ",", "args", ")", ":", "\n", "    ", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "\n", "current_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "target_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "print", "(", "'    Total params: %.2fM'", "%", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "current_model", ".", "parameters", "(", ")", ")", "/", "1000000.0", ")", ")", "\n", "for", "para", "in", "target_model", ".", "parameters", "(", ")", ":", "para", ".", "requires_grad", "=", "False", "\n", "update_target", "(", "current_model", ",", "target_model", ")", "\n", "\n", "epsilon_by_frame", "=", "epsilon_scheduler", "(", "args", ".", "eps_start", ",", "args", ".", "eps_mid", ",", "args", ".", "eps_final", ")", "\n", "beta_by_frame", "=", "beta_scheduler", "(", "args", ".", "beta_start", ",", "args", ".", "beta_frames", ")", "\n", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "replay_buffer", "=", "PrioritizedReplayBuffer", "(", "args", ".", "buffer_size", ",", "args", ".", "alpha", ",", "args", ".", "IS_weight_only_smaller", ",", "allowed_avg_min_ratio", "=", "args", ".", "ratio_min_prio", ")", "\n", "", "else", ":", "\n", "        ", "replay_buffer", "=", "ReplayBuffer", "(", "args", ".", "buffer_size", ")", "\n", "\n", "#args.action_space = env.unwrapped.get_action_meanings()", "\n", "", "args", ".", "init_lives", "=", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "print_args", "(", "args", ")", "\n", "\n", "args", ".", "do_update_target", "=", "False", "\n", "# specify the RL algorithm to use ", "\n", "if", "args", ".", "algorithm", "!=", "\"DQN\"", "and", "args", ".", "algorithm", "!=", "\"Residual\"", "and", "args", ".", "algorithm", "!=", "\"CDQN\"", ":", "\n", "        ", "currentTask", "=", "\"DQN\"", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.train.optimize_loss": [[42, 152], ["len", "state.size", "reward.cpu().numpy", "optimizer.zero_grad", "optimizer.step", "copy.deepcopy", "torch.add", "torch.add", "torch.add", "current_model", "current_model.gather().squeeze", "torch.mse_loss", "loss.mean.mean", "loss.mean.backward", "torch.cat", "torch.cat", "torch.cat", "current_model", "target_model", "numpy.max", "numpy.where", "target_mask.astype.astype", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "torch.cat", "torch.cat", "torch.cat", "current_model", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "os.path.isdir", "os.mkdir", "open", "f.write", "reward.cpu", "current_model.state_dict", "target_model.load_state_dict", "torch.no_grad", "torch.no_grad", "torch.no_grad", "target_model().max", "torch.no_grad", "torch.no_grad", "torch.no_grad", "loss.mean.item", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "next_q_values.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "target_model.cpu().numpy", "numpy.abs", "numpy.abs", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "next_q_values.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "os.path.join", "current_model.gather", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "target_model", "action.unsqueeze", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "target_model.cpu", "torch.cat", "torch.cat", "torch.cat", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "torch.cat", "torch.cat", "torch.cat", "action.unsqueeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "action.unsqueeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.stack", "torch.stack", "torch.stack", "numpy.concatenate", "torch.stack", "torch.stack", "torch.stack", "numpy.concatenate"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.add", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.add", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.add", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["args", ".", "currentTask", "=", "currentTask", "\n", "", "else", ":", "\n", "        ", "currentTask", "=", "args", ".", "algorithm", "\n", "args", ".", "currentTask", "=", "args", ".", "algorithm", "\n", "\n", "# prepare the optimizer", "\n", "", "lr", "=", "args", ".", "lr", "\n", "beta1", "=", "args", ".", "beta1", "\n", "beta2", "=", "args", ".", "beta2", "\n", "parameters", "=", "current_model", ".", "parameters", "\n", "args", ".", "optim", "=", "args", ".", "optim", ".", "lower", "(", ")", "\n", "if", "args", ".", "optim", "==", "'sgd'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "SGD", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "momentum", "=", "beta1", ")", "\n", "", "elif", "args", ".", "optim", "==", "'adam'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "AdamW", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", ".", "startswith", "(", "\"adamb\"", ")", ":", "\n", "        ", "args", ".", "optim", "=", "\"adambelief\"", "\n", "optimizer", "=", "optimizers", ".", "AdamBelief", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", "==", "'laprop'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "LaProp", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "\"The specified optimizer name {} is non-existent\"", ".", "format", "(", "args", ".", "optim", ")", "\n", "\n", "", "print", "(", "currentTask", ")", "\n", "\n", "\n", "reward_list", ",", "length_list", ",", "loss_list", ",", "off_policy_rate_list", ",", "gen_loss_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "clip_reward", "=", "True", "###", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "# the number of parallelized computation is maximally \"arg.train_freq\" to guarantee that the computation order is still consistent with the original method", "\n", "num_task", "=", "args", ".", "train_freq", "\n", "args", ".", "num_task", "=", "num_task", "\n", "episode_rewards", "=", "[", "0.", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "life_lengths", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "envs", "=", "[", "env", "]", "\n", "for", "_i", "in", "range", "(", "num_task", "-", "1", ")", ":", "envs", ".", "append", "(", "copy", ".", "deepcopy", "(", "env", ")", ")", "\n", "for", "env", "in", "envs", ":", "env", ".", "seed", "(", "random", ".", "randrange", "(", "1000000", ")", ")", "\n", "states", "=", "[", "state", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "rewards", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "evaluation_interval", "=", "args", ".", "evaluation_interval", "\n", "data_to_store", "=", "[", "]", "\n", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "prev_step", "=", "0", "\n", "step_idx", "=", "1", "# initialization of step_idx", "\n", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "if", "args", ".", "save_best", ":", "\n", "        ", "recent_performances", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "40", ")", "\n", "recent_models", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "20", ")", "\n", "best_performance", "=", "-", "float", "(", "\"inf\"", ")", "\n", "", "while", "step_idx", "<=", "args", ".", "max_steps", ":", "\n", "# decide the action", "\n", "        ", "epsilons", "=", "(", "epsilon_by_frame", "(", "idx", ")", "for", "idx", "in", "range", "(", "step_idx", ",", "step_idx", "+", "num_task", ")", ")", "if", "step_idx", ">", "args", ".", "learning_start", "else", "(", "1.", "for", "idx", "in", "range", "(", "num_task", ")", ")", "\n", "\n", "tensored_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "state", ".", "_frames", "for", "state", "in", "states", "]", ")", ".", "reshape", "(", "(", "num_task", ",", "-", "1", ")", "+", "image_shape", ")", ")", ".", "to", "(", "args", ".", "device", ")", ".", "float", "(", ")", ".", "div_", "(", "255.", ")", "\n", "actions", ",", "evaluateds", ",", "(", "Qss", ",", "bestActions", ")", "=", "current_model", ".", "act", "(", "tensored_states", ",", "epsilons", ")", "\n", "\n", "for", "_i", ",", "(", "env", ",", "state", ",", "action", ",", "Qs", ",", "bestAction", ",", "reward", ")", "in", "enumerate", "(", "zip", "(", "envs", ",", "states", ",", "actions", ",", "Qss", ",", "bestActions", ",", "rewards", ")", ")", ":", "\n", "\n", "# the environment proceeds by one step (4 frames)", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "if", "clip_reward", ":", "\n", "                ", "raw_reward", ",", "reward", "=", "reward", "\n", "", "else", ":", "\n", "                ", "raw_reward", "=", "reward", "\n", "\n", "", "rewards", "[", "_i", "]", "=", "float", "(", "reward", ")", "\n", "\n", "episode_rewards", "[", "_i", "]", "+=", "raw_reward", "\n", "life_lengths", "[", "_i", "]", "+=", "1", "\n", "\n", "# store the transition into the memory replay", "\n", "if", "not", "args", ".", "randomly_discard_experience", "or", "(", "args", ".", "randomly_discard_experience", "and", "random", ".", "random", "(", ")", ">=", "0.5", ")", ":", "# the data may be randomly discarded", "\n", "                ", "data_to_store", ".", "append", "(", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "float", "(", "done", ")", ")", ")", "\n", "", "if", "data_to_store", ":", "\n", "                ", "for", "data", "in", "data_to_store", ":", "\n", "                    ", "replay_buffer", ".", "add", "(", "*", "data", ")", "\n", "if", "args", ".", "randomly_replace_memory", "and", "len", "(", "replay_buffer", ")", ">=", "args", ".", "buffer_size", ":", "\n", "# probably randomly choose an index to replace ", "\n", "                        ", "replay_buffer", ".", "_next_idx", "=", "random", ".", "randrange", "(", "args", ".", "buffer_size", ")", "\n", "", "", "data_to_store", ".", "clear", "(", ")", "\n", "\n", "# record the performance of a trajectory", "\n", "", "if", "done", ":", "\n", "                ", "length_list", ".", "append", "(", "life_lengths", "[", "_i", "]", ")", "\n", "life_lengths", "[", "_i", "]", "=", "0", "\n", "# only the reward of a real full episode is recorded ", "\n", "if", "env", ".", "unwrapped", ".", "ale", ".", "game_over", "(", ")", "or", "\"TimeLimit.truncated\"", "in", "info", ":", "\n", "                    ", "reward_list", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "if", "not", "args", ".", "silent", ":", "\n", "                        ", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "env", ")", ":", "os", ".", "mkdir", "(", "args", ".", "env", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.txt'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ",", "'a'", ")", "as", "f", ":", "\n", "                            ", "f", ".", "write", "(", "'{:.0f}\\t{}\\n'", ".", "format", "(", "step_idx", "*", "4", ",", "episode_rewards", "[", "_i", "]", ")", ")", "\n", "", "if", "args", ".", "save_best", "and", "step_idx", ">", "args", ".", "learning_start", ":", "\n", "                            ", "recent_performances", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "mean_performance", "=", "np", ".", "mean", "(", "recent_performances", ")", "\n", "if", "best_performance", "<", "mean_performance", "and", "len", "(", "recent_performances", ")", ">=", "40", ":", "\n", "                                ", "assert", "len", "(", "recent_models", ")", "==", "20", "\n", "best_performance", "=", "mean_performance", "\n", "torch", ".", "save", "(", "(", "recent_models", "[", "0", "]", ",", "step_idx", ")", ",", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.pth'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ")", "\n", "", "recent_models", ".", "append", "(", "current_model", ".", "state_dict", "(", ")", ".", "copy", "(", ")", ")", "\n", "", "", "episode_rewards", "[", "_i", "]", "=", "0.", "\n", "", "states", "[", "_i", "]", "=", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "[", "_i", "]", "=", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.train.test": [[153, 199], ["env.reset", "range", "copy.deepcopy", "env.seed", "env.reset", "model.act", "enumerate", "numpy.array", "numpy.mean", "range", "range", "torch.tensor", "torch.tensor", "torch.tensor", "zip", "env.step", "numpy.std", "math.sqrt", "len"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["# optimize", "\n", "", "if", "step_idx", "%", "args", ".", "train_freq", "==", "0", "and", "step_idx", ">", "max", "(", "args", ".", "learning_start", ",", "2", "*", "args", ".", "batch_size", ")", ":", "\n", "                ", "beta", "=", "beta_by_frame", "(", "step_idx", ")", "\n", "loss", ",", "off_policy_rate", "=", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", ")", "\n", "loss_list", ".", "append", "(", "loss", ")", ";", "off_policy_rate_list", ".", "append", "(", "off_policy_rate", ")", "\n", "\n", "# update the target network", "\n", "", "if", "step_idx", "%", "args", ".", "update_target", "==", "0", "and", "currentTask", "!=", "\"Residual\"", ":", "\n", "# we defer the update of the target network to the optimization routine to ensure that the target network is not exactly equal to current network", "\n", "                ", "args", ".", "do_update_target", "=", "True", "\n", "#update_target(current_model, target_model)", "\n", "\n", "# print the statistics", "\n", "", "if", "step_idx", "%", "evaluation_interval", "==", "0", ":", "\n", "# it works only if there is at least one episode to report; otherwise \"evaluation_interval\" is increased", "\n", "                ", "if", "len", "(", "reward_list", ")", ">", "0", ":", "\n", "                    ", "kwargs", "=", "{", "}", "\n", "kwargs", "[", "\"Off-Policy\"", "]", "=", "off_policy_rate_list", "\n", "print_log", "(", "step_idx", ",", "prev_step", ",", "prev_time", ",", "reward_list", ",", "length_list", ",", "loss_list", ",", "args", ",", "'{}{:.0e}{}'", ".", "format", "(", "currentTask", ",", "args", ".", "lr", ",", "args", ".", "comment", ")", ",", "**", "kwargs", ")", "\n", "reward_list", ".", "clear", "(", ")", ";", "length_list", ".", "clear", "(", ")", ";", "loss_list", ".", "clear", "(", ")", "\n", "for", "v", "in", "kwargs", ".", "values", "(", ")", ":", "\n", "                        ", "if", "type", "(", "v", ")", "==", "list", ":", "v", ".", "clear", "(", ")", "\n", "", "prev_step", "=", "step_idx", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "", "else", ":", "\n", "                    ", "evaluation_interval", "+=", "args", ".", "evaluation_interval", "\n", "\n", "", "", "step_idx", "+=", "1", "\n", "\n", "", "", "", "i_count", "=", "0", "\n", "accu1", ",", "accu2", "=", "0.", ",", "0.", "\n", "accu_loss", "=", "0.", "\n", "def", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Calculate loss and optimize\n    \"\"\"", "\n", "global", "i_count", ",", "accu1", ",", "accu2", ",", "accu_loss", "\n", "\n", "# sample data", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "state_next_state", ",", "action_", ",", "reward_", ",", "done", ",", "weights_", ",", "true_weights", ",", "indices", "=", "replay_buffer", ".", "sample", "(", "args", ".", "batch_size", ",", "beta", ")", "\n", "weights", "=", "torch", ".", "from_numpy", "(", "weights_", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "state_next_state", ",", "action_", ",", "reward_", ",", "done", ",", "indices", "=", "replay_buffer", ".", "sample", "(", "args", ".", "batch_size", ")", "\n", "weights", "=", "torch", ".", "ones", "(", "args", ".", "batch_size", ")", ";", "weights_", "=", "weights", ".", "numpy", "(", ")", ";", "true_weights", "=", "weights_", "\n", "weights", "=", "weights", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.train.adjust_lr": [[200, 212], ["round", "round"], "function", ["None"], ["# we move data to GPU in chunks", "\n", "", "state_next_state", "=", "torch", ".", "from_numpy", "(", "state_next_state", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", ".", "float", "(", ")", ".", "div_", "(", "255", ")", "\n", "state", ",", "next_state", "=", "state_next_state", "\n", "action", "=", "torch", ".", "from_numpy", "(", "action_", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "gamma_mul_one_minus_done_", "=", "(", "args", ".", "gamma", "*", "(", "1.", "-", "done", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "if", "args", ".", "currentTask", "==", "\"DQN\"", ":", "\n", "# in some cases these data do not really need to be copied to GPU ", "\n", "        ", "reward", ",", "gamma_mul_one_minus_done", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "(", "reward_", ",", "gamma_mul_one_minus_done_", ")", ")", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "##### start training ##### ", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "# we use \"values\" to refer to Q values for all state-actions, and use \"value\" to refer to Q values for states", "\n", "if", "args", ".", "currentTask", "==", "\"DQN\"", ":", "\n", "        ", "if", "args", ".", "double", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.train.show_moves": [[213, 220], ["move.append", "move.append", "move.append"], "function", ["None"], ["            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q_values", "=", "current_model", "(", "next_state", ")", "\n", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "1", ")", "# **unsqueeze", "\n", "target_next_q_values", "=", "target_model", "(", "next_state", ")", "\n", "next_q_value", "=", "target_next_q_values", ".", "gather", "(", "1", ",", "next_q_action", ")", ".", "squeeze", "(", ")", "\n", "next_q_action", "=", "next_q_action", ".", "squeeze", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.train.shuffle_dateset": [[221, 226], ["list", "random.shuffle", "zip", "zip"], "function", ["None"], ["                ", "next_q_value", ",", "next_q_action", "=", "target_model", "(", "next_state", ")", ".", "max", "(", "1", ")", "\n", "\n", "", "", "expected_q_value", "=", "torch", ".", "addcmul", "(", "reward", ",", "tensor1", "=", "next_q_value", ",", "tensor2", "=", "gamma_mul_one_minus_done", ")", "\n", "q_values", "=", "current_model", "(", "state", ")", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.model.DQNBase.__init__": [[19, 36], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.DQNBase.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["\n", "", "class", "DQNBase", "(", "nn", ".", "Module", ")", ":", "\n", "    ", "\"\"\"\n    Basic DQN\n    \n    parameters\n    ---------\n    env         environment (openai gym)\n    deep        whether to use a deeper convolutional network (bool)\n    \"\"\"", "\n", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "super", "(", "DQNBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "num_actions", "=", "env", ".", "action_space", ".", "n", "\n", "self", ".", "Linear", "=", "Linear", "# We have overridden the \"reset_parameters\" method for a more well-principled initialization", "\n", "\n", "self", ".", "flatten", "=", "Flatten", "(", ")", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.model.DQNBase.forward": [[38, 41], ["model.DQNBase.fc"], "methods", ["None"], ["nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ",", "padding", "=", "0", "if", "self", ".", "input_shape", "[", "1", "]", "!=", "105", "else", "2", ")", ",", "\n", "#nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0 if self.input_shape[1]!=105 else 2),", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.model.DQNBase.act": [[42, 62], ["state.unsqueeze.unsqueeze.dim", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "state.unsqueeze.unsqueeze.unsqueeze", "model.DQNBase.forward().cpu().numpy().squeeze", "numpy.argmax", "state.unsqueeze.unsqueeze.dim", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.DQNBase.forward().cpu().numpy", "numpy.argmax", "numpy.copy", "state.unsqueeze.unsqueeze.size", "model.DQNBase.forward().cpu().numpy", "model.DQNBase.forward().cpu", "model.DQNBase.forward().cpu", "model.DQNBase.forward", "model.DQNBase.forward"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward"], ["nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "for", "module", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "type", "(", "module", ")", "==", "nn", ".", "Conv2d", ":", "init", ".", "kaiming_uniform_", "(", "module", ".", "weight", ".", "data", ",", "nonlinearity", "=", "\"relu\"", ",", "a", "=", "0.", ")", ";", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", ",", "self", ".", "num_actions", ")", "\n", ")", "\n", "self", ".", "out_bias", "=", "self", ".", "fc", "[", "-", "1", "]", ".", "bias", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "**", "kwargs", ")", ":", "\n", "        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.model.Linear.reset_parameters": [[64, 70], ["torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch._calculate_fan_in_and_fan_out", "torch._calculate_fan_in_and_fan_out", "torch._calculate_fan_in_and_fan_out", "torch._calculate_fan_in_and_fan_out", "torch.uniform_", "torch.uniform_", "torch.uniform_", "torch.uniform_", "math.sqrt"], "methods", ["None"], ["        ", "return", "self", ".", "features", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n", "", "def", "act", "(", "self", ",", "state", ",", "epsilon", ",", "**", "kwargs", ")", ":", "\n", "        "]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.main.main": [[9, 32], ["torch.set_num_threads", "arguments.get_args", "common.wrappers.make_atari", "common.wrappers.wrap_atari_dqn", "common.utils.set_global_seeds", "common.wrappers.wrap_atari_dqn.seed", "train", "common.wrappers.wrap_atari_dqn.close", "open", "f.write", "test.test", "common.wrappers.wrap_atari_dqn.close", "datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.arguments.get_args", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.make_atari", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.wrap_atari_dqn", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.set_global_seeds", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.train", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test"], ["def", "main", "(", ")", ":", "\n", "    ", "torch", ".", "set_num_threads", "(", "2", ")", "# we need to constrain the number of threads; it can default to a large value", "\n", "args", "=", "get_args", "(", ")", "\n", "\n", "# keep a history of the commands that has been executed", "\n", "with", "open", "(", "\"commandl_history.txt\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y/%m/%d %H:%M:%S\\t\"", ")", "+", "' '", ".", "join", "(", "sys", ".", "argv", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "env", "=", "make_atari", "(", "args", ".", "env", ",", "args", ".", "max_episode_steps", ",", "clip_reward", "=", "(", "not", "(", "args", ".", "no_clip", "or", "args", ".", "transform_Q", ")", ")", "and", "(", "not", "args", ".", "evaluate", ")", ")", "\n", "env", "=", "wrap_atari_dqn", "(", "env", ",", "args", ")", "\n", "\n", "set_global_seeds", "(", "args", ".", "seed", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "if", "args", ".", "evaluate", ":", "\n", "        ", "test", "(", "env", ",", "args", ")", "\n", "env", ".", "close", "(", ")", "\n", "return", "\n", "\n", "", "from", "train", "import", "train", "as", "train", "\n", "train", "(", "env", ",", "args", ")", "\n", "\n", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.arguments.get_args": [[4, 142], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.is_available"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Deep Q-Learning'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--algorithm'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "# Basic Arguments", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Disable CUDA training'", ")", "\n", "\n", "# Training Arguments", "\n", "parser", ".", "add_argument", "(", "'--max-steps'", ",", "type", "=", "int", ",", "default", "=", "50000000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps to train (equal to actual_frames / 4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--buffer-size'", ",", "type", "=", "int", ",", "default", "=", "1000000", ",", "metavar", "=", "'CAPACITY'", ",", "\n", "help", "=", "'Maximum memory buffer size'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-discard-experience'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly discard half of collected experience data'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-replace-memory'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly replace old experiences by new experiences when the memory replay is full (by default it is first-in-first-out)'", ")", "\n", "parser", ".", "add_argument", "(", "'--update-target'", ",", "type", "=", "int", ",", "default", "=", "8000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Interval of target network update'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-freq'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps between optimization steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "metavar", "=", "'\u03b3'", ",", "\n", "help", "=", "'Discount factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning-start'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'How many steps of the model to collect transitions for before learning starts'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-start'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Start value of epsilon'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-mid'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'Mid value of epsilon (at one million-th step)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-final'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'Final value of epsilon'", ")", "\n", "\n", "# Algorithm Arguments", "\n", "parser", ".", "add_argument", "(", "'--double'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Double Q Learning'", ")", "\n", "parser", ".", "add_argument", "(", "'--dueling'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Dueling Network with Default Evaluation (Avg.) on Advantages'", ")", "\n", "parser", ".", "add_argument", "(", "'--prioritized-replay'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable prioritized experience replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.6", ",", "\n", "help", "=", "'Alpha value for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--ratio-min-prio'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Allowed maximal ratio between the smallest sampling priority and the average priority, which equals the maximal importance sampling weight'", ")", "\n", "parser", ".", "add_argument", "(", "'--prio-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-10", ",", "\n", "help", "=", "'A small number added before computing the priority'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-start'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "\n", "help", "=", "'Start value of beta for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-frames'", ",", "type", "=", "float", ",", "default", "=", "50000000", ",", "\n", "help", "=", "'End step of beta schedule for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--IS-weight-only-smaller'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Divide all importance sampling correction weights by the largest weight, so that they are smaller or equal to one'", ")", "\n", "\n", "# Environment Arguments", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "type", "=", "str", ",", "default", "=", "'PongNoFrameskip-v4'", ",", "\n", "help", "=", "'Environment Name'", ")", "\n", "parser", ".", "add_argument", "(", "'--episode-life'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Whether losing one life is considered as an end of an episode(1) or not(0) from the agent's perspective\"", ")", "\n", "parser", ".", "add_argument", "(", "'--grey'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Change the observation to greyscale (default 1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-stack'", ",", "type", "=", "str", ",", "default", "=", "\"4\"", ",", "\n", "help", "=", "'Number of adjacent observations to stack'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-downscale'", ",", "type", "=", "int", ",", "default", "=", "84", ",", "# we will always crop the frame when it has a height of 250 instead of the default 210 (crop by top 28, bottom 12) ", "\n", "help", "=", "'Downscaling ratio of the frame observation (if <= 10) or image size as the downscaling target (if >10)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-episode-steps'", ",", "type", "=", "int", ",", "default", "=", "20000", ",", "\n", "help", "=", "\"The maximum number of steps allowd before resetting the real episode.\"", ")", "\n", "\n", "# Evaluation Arguments", "\n", "parser", ".", "add_argument", "(", "'--load-model'", ",", "type", "=", "str", ",", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "'Pretrained model names to load (state dict)'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Evaluate only'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-trial'", ",", "type", "=", "int", ",", "default", "=", "400", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluation_interval'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "'Steps for printing statistics'", ")", "\n", "\n", "# Optimization Arguments", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "6.25e-5", ",", "metavar", "=", "'\u03b7'", ",", "\n", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--optim'", ",", "type", "=", "str", ",", "default", "=", "'adam'", ",", "\n", "help", "=", "'Optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--adam-eps'", ",", "type", "=", "float", ",", "default", "=", "1.5e-4", ",", "\n", "help", "=", "'Epsilon of adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu-id'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Which GPU to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta2'", ",", "type", "=", "float", ",", "default", "=", "0.999", ")", "\n", "parser", ".", "add_argument", "(", "'--beta1'", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "'--grad-clip'", ",", "type", "=", "float", ",", "default", "=", "10.", ",", "# when transform-Q is used, it should be 40.", "\n", "help", "=", "\"Gradient clipping norm; 0 corresponds to no gradient clipping\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--silent'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--comment'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "\n", "# A simple option to reproduce the original DQN", "\n", "parser", ".", "add_argument", "(", "'--originalDQN'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To reproduce the original DQN\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--save-best'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To save the model when it performs best during training, averaged over 40 episodes\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "originalDQN", ":", "\n", "        ", "del", "args", ".", "originalDQN", "\n", "# the most important arguments for reproducing the original prioritized dueling DDQN", "\n", "args", ".", "algorithm", "=", "\"DQN\"", "\n", "args", ".", "lr", "=", "6.25e-5", "\n", "args", ".", "adam_eps", "=", "1.5e-4", "\n", "args", ".", "grad_clip", "=", "10.", "\n", "args", ".", "prioritized_replay", "=", "True", "\n", "args", ".", "alpha", "=", "0.6", "\n", "args", ".", "beta_start", "=", "0.4", "\n", "args", ".", "beta_frames", "=", "50000000.", "\n", "args", ".", "auto_init", "=", "False", "\n", "args", ".", "gamma", "=", "0.99", "\n", "args", ".", "double", "=", "True", "\n", "args", ".", "dueling", "=", "True", "\n", "args", ".", "episode_life", "=", "1", "\n", "args", ".", "randomly_replace_memory", "=", "False", "\n", "args", ".", "no_clip", "=", "False", "\n", "args", ".", "transform_Q", "=", "False", "\n", "\n", "", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:{}\"", ".", "format", "(", "args", ".", "gpu_id", ")", "if", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "\n", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.train.train": [[17, 202], ["model.DQN().to", "model.DQN().to", "print", "DQN().to.parameters", "common.utils.update_target", "common.utils.epsilon_scheduler", "common.utils.beta_scheduler", "env.unwrapped.ale.lives", "common.utils.print_args", "args.optim.lower", "print", "env.reset", "range", "time.time", "common.replay_buffer.PrioritizedReplayBuffer", "common.replay_buffer.ReplayBuffer", "torch.SGD", "envs.append", "env.seed", "common.save_agent.Agent_History", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "DQN().to.act", "enumerate", "model.DQN", "model.DQN", "parameters", "optimizers.AdamW", "args.optim.startswith", "range", "range", "copy.deepcopy", "random.randrange", "range", "range", "zip", "env.step", "float", "data_to_store.append", "sum", "parameters", "optimizers.AdamBelief", "range", "range", "range", "common.utils.epsilon_scheduler.", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "reward_record[].append", "value_record[].append", "data_to_store.clear", "length_list.append", "env.reset", "optimizers.LaProp.zero_grad", "common.save_agent.Agent_History.add_agent", "common.utils.beta_scheduler.", "train.compute_td_loss", "loss_list.append", "off_policy_rate_list.append", "parameters", "optimizers.LaProp", "range", "range", "float", "common.replay_buffer.ReplayBuffer.add", "env.unwrapped.ale.game_over", "reward_list.append", "model.compute_value_error", "reward_record[].clear", "value_record[].clear", "copy.deepcopy", "len", "max", "model.auto_initialize", "len", "common.utils.print_log", "reward_list.clear", "length_list.clear", "loss_list.clear", "kwargs.values", "time.time", "p.numel", "parameters", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "max", "print", "random.randrange", "os.path.isdir", "os.mkdir", "open", "f.write", "DQN().to.state_dict", "common.utils.update_target", "DQN().to.parameters", "len", "os.path.isdir", "os.mkdir", "open", "f.write", "os.path.join", "type", "v.clear", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "os.path.join", "common.save_agent.Agent_History.add_training_performance", "numpy.array().reshape", "numpy.array"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.epsilon_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.beta_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_args", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.add_agent", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.compute_td_loss", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.add", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.compute_value_error", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.auto_initialize", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_log", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.save_agent.Agent_History.add_training_performance"], ["    ", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "\n", "current_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "target_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "print", "(", "'    Total params: %.2fM'", "%", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "current_model", ".", "parameters", "(", ")", ")", "/", "1000000.0", ")", ")", "\n", "for", "para", "in", "target_model", ".", "parameters", "(", ")", ":", "para", ".", "requires_grad", "=", "False", "\n", "update_target", "(", "current_model", ",", "target_model", ")", "\n", "\n", "epsilon_by_frame", "=", "epsilon_scheduler", "(", "args", ".", "eps_start", ",", "args", ".", "eps_mid", ",", "args", ".", "eps_final", ")", "\n", "beta_by_frame", "=", "beta_scheduler", "(", "args", ".", "beta_start", ",", "args", ".", "beta_frames", ")", "\n", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "replay_buffer", "=", "PrioritizedReplayBuffer", "(", "args", ".", "buffer_size", ",", "args", ".", "alpha", ",", "args", ".", "IS_weight_only_smaller", ",", "allowed_avg_min_ratio", "=", "args", ".", "ratio_min_prio", ")", "\n", "", "else", ":", "\n", "        ", "replay_buffer", "=", "ReplayBuffer", "(", "args", ".", "buffer_size", ")", "\n", "\n", "#args.action_space = env.unwrapped.get_action_meanings()", "\n", "", "args", ".", "init_lives", "=", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "print_args", "(", "args", ")", "\n", "\n", "args", ".", "do_update_target", "=", "False", "\n", "# specify the RL algorithm to use ", "\n", "if", "args", ".", "algorithm", "!=", "\"DQN\"", "and", "args", ".", "algorithm", "!=", "\"Residual\"", "and", "args", ".", "algorithm", "!=", "\"CDQN\"", ":", "\n", "        ", "currentTask", "=", "\"DQN\"", "\n", "args", ".", "currentTask", "=", "currentTask", "\n", "", "else", ":", "\n", "        ", "currentTask", "=", "args", ".", "algorithm", "\n", "args", ".", "currentTask", "=", "args", ".", "algorithm", "\n", "\n", "# prepare the optimizer", "\n", "", "lr", "=", "args", ".", "lr", "\n", "beta1", "=", "args", ".", "beta1", "\n", "beta2", "=", "args", ".", "beta2", "\n", "parameters", "=", "current_model", ".", "parameters", "\n", "args", ".", "optim", "=", "args", ".", "optim", ".", "lower", "(", ")", "\n", "if", "args", ".", "optim", "==", "'sgd'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "SGD", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "momentum", "=", "beta1", ")", "\n", "", "elif", "args", ".", "optim", "==", "'adam'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "AdamW", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", ".", "startswith", "(", "\"adamb\"", ")", ":", "\n", "        ", "args", ".", "optim", "=", "\"adambelief\"", "\n", "optimizer", "=", "optimizers", ".", "AdamBelief", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", "==", "'laprop'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "LaProp", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "\"The specified optimizer name {} is non-existent\"", ".", "format", "(", "args", ".", "optim", ")", "\n", "\n", "", "print", "(", "currentTask", ")", "\n", "\n", "\n", "reward_list", ",", "length_list", ",", "loss_list", ",", "off_policy_rate_list", ",", "gen_loss_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "clip_reward", "=", "True", "###", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "# the number of parallelized computation is maximally \"arg.train_freq\" to guarantee that the computation order is still consistent with the original method", "\n", "num_task", "=", "args", ".", "train_freq", "\n", "args", ".", "num_task", "=", "num_task", "\n", "episode_rewards", "=", "[", "0.", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "life_lengths", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "envs", "=", "[", "env", "]", "\n", "for", "_i", "in", "range", "(", "num_task", "-", "1", ")", ":", "envs", ".", "append", "(", "copy", ".", "deepcopy", "(", "env", ")", ")", "\n", "for", "env", "in", "envs", ":", "env", ".", "seed", "(", "random", ".", "randrange", "(", "1000000", ")", ")", "\n", "states", "=", "[", "state", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "rewards", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "evaluation_interval", "=", "args", ".", "evaluation_interval", "\n", "data_to_store", "=", "[", "]", "\n", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "prev_step", "=", "0", "\n", "step_idx", "=", "1", "# initialization of step_idx", "\n", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "if", "args", ".", "save_best", ":", "\n", "        ", "recent_performances", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "40", ")", "\n", "recent_models", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "20", ")", "\n", "best_performance", "=", "-", "float", "(", "\"inf\"", ")", "\n", "", "while", "step_idx", "<=", "args", ".", "max_steps", ":", "\n", "# decide the action", "\n", "        ", "epsilons", "=", "(", "epsilon_by_frame", "(", "idx", ")", "for", "idx", "in", "range", "(", "step_idx", ",", "step_idx", "+", "num_task", ")", ")", "if", "step_idx", ">", "args", ".", "learning_start", "else", "(", "1.", "for", "idx", "in", "range", "(", "num_task", ")", ")", "\n", "\n", "tensored_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "state", ".", "_frames", "for", "state", "in", "states", "]", ")", ".", "reshape", "(", "(", "num_task", ",", "-", "1", ")", "+", "image_shape", ")", ")", ".", "to", "(", "args", ".", "device", ")", ".", "float", "(", ")", ".", "div_", "(", "255.", ")", "\n", "actions", ",", "evaluateds", ",", "(", "Qss", ",", "bestActions", ")", "=", "current_model", ".", "act", "(", "tensored_states", ",", "epsilons", ")", "\n", "\n", "for", "_i", ",", "(", "env", ",", "state", ",", "action", ",", "Qs", ",", "bestAction", ",", "reward", ")", "in", "enumerate", "(", "zip", "(", "envs", ",", "states", ",", "actions", ",", "Qss", ",", "bestActions", ",", "rewards", ")", ")", ":", "\n", "\n", "# the environment proceeds by one step (4 frames)", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "if", "clip_reward", ":", "\n", "                ", "raw_reward", ",", "reward", "=", "reward", "\n", "", "else", ":", "\n", "                ", "raw_reward", "=", "reward", "\n", "\n", "", "rewards", "[", "_i", "]", "=", "float", "(", "reward", ")", "\n", "\n", "episode_rewards", "[", "_i", "]", "+=", "raw_reward", "\n", "life_lengths", "[", "_i", "]", "+=", "1", "\n", "\n", "# store the transition into the memory replay", "\n", "if", "not", "args", ".", "randomly_discard_experience", "or", "(", "args", ".", "randomly_discard_experience", "and", "random", ".", "random", "(", ")", ">=", "0.5", ")", ":", "# the data may be randomly discarded", "\n", "                ", "data_to_store", ".", "append", "(", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "float", "(", "done", ")", ")", ")", "\n", "", "if", "data_to_store", ":", "\n", "                ", "for", "data", "in", "data_to_store", ":", "\n", "                    ", "replay_buffer", ".", "add", "(", "*", "data", ")", "\n", "if", "args", ".", "randomly_replace_memory", "and", "len", "(", "replay_buffer", ")", ">=", "args", ".", "buffer_size", ":", "\n", "# probably randomly choose an index to replace ", "\n", "                        ", "replay_buffer", ".", "_next_idx", "=", "random", ".", "randrange", "(", "args", ".", "buffer_size", ")", "\n", "", "", "data_to_store", ".", "clear", "(", ")", "\n", "\n", "# record the performance of a trajectory", "\n", "", "if", "done", ":", "\n", "                ", "length_list", ".", "append", "(", "life_lengths", "[", "_i", "]", ")", "\n", "life_lengths", "[", "_i", "]", "=", "0", "\n", "# only the reward of a real full episode is recorded ", "\n", "if", "env", ".", "unwrapped", ".", "ale", ".", "game_over", "(", ")", "or", "\"TimeLimit.truncated\"", "in", "info", ":", "\n", "                    ", "reward_list", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "if", "not", "args", ".", "silent", ":", "\n", "                        ", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "env", ")", ":", "os", ".", "mkdir", "(", "args", ".", "env", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.txt'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ",", "'a'", ")", "as", "f", ":", "\n", "                            ", "f", ".", "write", "(", "'{:.0f}\\t{}\\n'", ".", "format", "(", "step_idx", "*", "4", ",", "episode_rewards", "[", "_i", "]", ")", ")", "\n", "", "if", "args", ".", "save_best", "and", "step_idx", ">", "args", ".", "learning_start", ":", "\n", "                            ", "recent_performances", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "mean_performance", "=", "np", ".", "mean", "(", "recent_performances", ")", "\n", "if", "best_performance", "<", "mean_performance", "and", "len", "(", "recent_performances", ")", ">=", "40", ":", "\n", "                                ", "assert", "len", "(", "recent_models", ")", "==", "20", "\n", "best_performance", "=", "mean_performance", "\n", "torch", ".", "save", "(", "(", "recent_models", "[", "0", "]", ",", "step_idx", ")", ",", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.pth'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ")", "\n", "", "recent_models", ".", "append", "(", "current_model", ".", "state_dict", "(", ")", ".", "copy", "(", ")", ")", "\n", "", "", "episode_rewards", "[", "_i", "]", "=", "0.", "\n", "", "states", "[", "_i", "]", "=", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "[", "_i", "]", "=", "next_state", "\n", "\n", "# optimize", "\n", "", "if", "step_idx", "%", "args", ".", "train_freq", "==", "0", "and", "step_idx", ">", "max", "(", "args", ".", "learning_start", ",", "2", "*", "args", ".", "batch_size", ")", ":", "\n", "                ", "beta", "=", "beta_by_frame", "(", "step_idx", ")", "\n", "loss", ",", "off_policy_rate", "=", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", ")", "\n", "loss_list", ".", "append", "(", "loss", ")", ";", "off_policy_rate_list", ".", "append", "(", "off_policy_rate", ")", "\n", "\n", "# update the target network", "\n", "", "if", "step_idx", "%", "args", ".", "update_target", "==", "0", "and", "currentTask", "!=", "\"Residual\"", ":", "\n", "# we defer the update of the target network to the optimization routine to ensure that the target network is not exactly equal to current network", "\n", "                ", "args", ".", "do_update_target", "=", "True", "\n", "#update_target(current_model, target_model)", "\n", "\n", "# print the statistics", "\n", "", "if", "step_idx", "%", "evaluation_interval", "==", "0", ":", "\n", "# it works only if there is at least one episode to report; otherwise \"evaluation_interval\" is increased", "\n", "                ", "if", "len", "(", "reward_list", ")", ">", "0", ":", "\n", "                    ", "kwargs", "=", "{", "}", "\n", "kwargs", "[", "\"Off-Policy\"", "]", "=", "off_policy_rate_list", "\n", "print_log", "(", "step_idx", ",", "prev_step", ",", "prev_time", ",", "reward_list", ",", "length_list", ",", "loss_list", ",", "args", ",", "'{}{:.0e}{}'", ".", "format", "(", "currentTask", ",", "args", ".", "lr", ",", "args", ".", "comment", ")", ",", "**", "kwargs", ")", "\n", "reward_list", ".", "clear", "(", ")", ";", "length_list", ".", "clear", "(", ")", ";", "loss_list", ".", "clear", "(", ")", "\n", "for", "v", "in", "kwargs", ".", "values", "(", ")", ":", "\n", "                        ", "if", "type", "(", "v", ")", "==", "list", ":", "v", ".", "clear", "(", ")", "\n", "", "prev_step", "=", "step_idx", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "", "else", ":", "\n", "                    ", "evaluation_interval", "+=", "args", ".", "evaluation_interval", "\n", "\n", "", "", "step_idx", "+=", "1", "\n", "\n", "", "", "", "i_count", "=", "0", "\n", "accu1", ",", "accu2", "=", "0.", ",", "0.", "\n", "accu_loss", "=", "0.", "\n", "def", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Calculate loss and optimize\n    \"\"\"", "\n", "global", "i_count", ",", "accu1", ",", "accu2", ",", "accu_loss", "\n", "\n", "# sample data", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "state_next_state", ",", "action_", ",", "reward_", ",", "done", ",", "weights_", ",", "true_weights", ",", "indices", "=", "replay_buffer", ".", "sample", "(", "args", ".", "batch_size", ",", "beta", ")", "\n", "weights", "=", "torch", ".", "from_numpy", "(", "weights_", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "state_next_state", ",", "action_", ",", "reward_", ",", "done", ",", "indices", "=", "replay_buffer", ".", "sample", "(", "args", ".", "batch_size", ")", "\n", "weights", "=", "torch", ".", "ones", "(", "args", ".", "batch_size", ")", ";", "weights_", "=", "weights", ".", "numpy", "(", ")", ";", "true_weights", "=", "weights_", "\n", "weights", "=", "weights", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "# we move data to GPU in chunks", "\n", "", "state_next_state", "=", "torch", ".", "from_numpy", "(", "state_next_state", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", ".", "float", "(", ")", ".", "div_", "(", "255", ")", "\n", "state", ",", "next_state", "=", "state_next_state", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.train.compute_td_loss": [[207, 404], ["torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "optimizer.zero_grad", "optimizer.step", "numpy.mean", "math.ceil", "replay_buffer.sample", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "replay_buffer.sample", "torch.ones", "torch.ones", "torch.ones", "weights.to.numpy", "weights.to.to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.smooth_l1_loss", "F.smooth_l1_loss.backward", "list", "torch.from_numpy().to().float().div_.view", "current_model", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "list", "torch.from_numpy().to().float().div_.view", "current_model", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "replay_buffer.update_priorities", "replay_buffer.prop_minimum_priorities", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "common.utils.update_target", "print", "done.astype", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model", "current_model.gather().squeeze", "model.inverse_transform_Q", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.addcmul", "torch.addcmul", "torch.addcmul", "current_model", "current_model.gather().squeeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().to().float().div_.size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "current_model.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "target_model", "numpy.where", "target_mask.astype.astype", "numpy.mean", "numpy.clip", "model.transform_backpropagate", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "torch.from_numpy().to().float().div_.size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "current_model.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "numpy.clip", "model.transform_backpropagate", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "current_model.parameters", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model", "[].unsqueeze", "target_model", "target_model.gather().squeeze", "next_q_action.squeeze.squeeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "target_model().max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "numpy.abs", "target_model.gather().squeeze().cpu().numpy", "numpy.max", "model.inverse_transform_Q", "model.transform_Q", "numpy.abs", "numpy.abs", "numpy.where", "numpy.abs", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "model.inverse_transform_Q", "model.transform_Q", "numpy.abs", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "math.sqrt", "open", "f.write", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "numpy.stack", "current_model.gather", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model.gather", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "target_model.cpu().numpy", "numpy.stack", "numpy.stack", "torch.cat", "torch.cat", "torch.cat", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "numpy.stack", "torch.cat", "torch.cat", "torch.cat", "os.path.join", "open", "f.write", "target_model.gather", "target_model", "torch.from_numpy().to.unsqueeze", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "model.transform_Q", "torch.from_numpy().to.unsqueeze", "torch.from_numpy().to.unsqueeze", "target_model.gather().squeeze().cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().to.unsqueeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.argmax", "os.path.join", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model.max", "torch.stack", "torch.stack", "torch.stack", "target_model.cpu", "numpy.concatenate", "torch.stack", "torch.stack", "torch.stack", "numpy.concatenate", "current_model.detach().cpu().numpy", "torch.stack", "torch.stack", "torch.stack", "torch.mse_loss", "target_model.gather().squeeze", "q_values.gather().squeeze.detach", "current_model.detach().cpu", "q_values.gather().squeeze.detach", "target_model.gather", "next_q_action.squeeze.unsqueeze", "current_model.detach"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.update_priorities", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.prop_minimum_priorities", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_backpropagate", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_backpropagate", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["        ", "reward", ",", "gamma_mul_one_minus_done", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "(", "reward_", ",", "gamma_mul_one_minus_done_", ")", ")", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "##### start training ##### ", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "# we use \"values\" to refer to Q values for all state-actions, and use \"value\" to refer to Q values for states", "\n", "if", "args", ".", "currentTask", "==", "\"DQN\"", ":", "\n", "        ", "if", "args", ".", "double", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q_values", "=", "current_model", "(", "next_state", ")", "\n", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "1", ")", "# **unsqueeze", "\n", "target_next_q_values", "=", "target_model", "(", "next_state", ")", "\n", "next_q_value", "=", "target_next_q_values", ".", "gather", "(", "1", ",", "next_q_action", ")", ".", "squeeze", "(", ")", "\n", "next_q_action", "=", "next_q_action", ".", "squeeze", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q_value", ",", "next_q_action", "=", "target_model", "(", "next_state", ")", ".", "max", "(", "1", ")", "\n", "\n", "", "", "expected_q_value", "=", "torch", ".", "addcmul", "(", "reward", ",", "tensor1", "=", "next_q_value", ",", "tensor2", "=", "gamma_mul_one_minus_done", ")", "\n", "q_values", "=", "current_model", "(", "state", ")", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "loss", "=", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ",", "reduction", "=", "'none'", ")", "\n", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "diff", "=", "(", "q_value", ".", "detach", "(", ")", "-", "expected_q_value", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "prios", "=", "np", ".", "abs", "(", "diff", ")", "+", "args", ".", "prio_eps", "#", "\n", "", "loss", "=", "(", "loss", "*", "weights", ")", ".", "mean", "(", ")", "/", "2.", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# we report the mean squared error instead of the Huber loss as the loss", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "report_loss", "=", "(", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ",", "reduction", "=", "'none'", ")", "*", "weights", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "", "if", "args", ".", "currentTask", "==", "\"CDQN\"", ":", "\n", "# compute the current and next state values in a single pass ", "\n", "        ", "size", "=", "list", "(", "state_next_state", ".", "size", "(", ")", ")", "\n", "current_and_next_states", "=", "state_next_state", ".", "view", "(", "[", "-", "1", "]", "+", "size", "[", "2", ":", "]", ")", "\n", "# compute the q values and the gradient", "\n", "all_q_values", "=", "current_model", "(", "current_and_next_states", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_values", ",", "next_q_values", "=", "all_q_values", "[", ":", "args", ".", "batch_size", "]", ",", "all_q_values", "[", "args", ".", "batch_size", ":", "2", "*", "args", ".", "batch_size", "]", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", ",", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "\n", "q_value", ",", "next_q_value", "=", "torch", ".", "stack", "(", "(", "q_value", ",", "next_q_value", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "next_q_values_target", "=", "target_model", "(", "next_state", ")", "\n", "if", "args", ".", "double", ":", "\n", "                ", "next_q_value_target", "=", "next_q_values_target", ".", "gather", "(", "1", ",", "next_q_action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "next_q_value_target", "=", "np", ".", "max", "(", "next_q_values_target", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "expected_q_value_self", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value", "\n", "expected_q_value_target", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value_target", "\n", "target_mask", "=", "(", "np", ".", "abs", "(", "q_value", "-", "expected_q_value_target", ")", ">=", "np", ".", "abs", "(", "q_value", "-", "expected_q_value_self", ")", ")", "\n", "expected_q_value", "=", "np", ".", "where", "(", "target_mask", ",", "expected_q_value_target", ",", "expected_q_value_self", ")", "\n", "target_mask", "=", "target_mask", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "", "diff", "=", "q_value", "-", "expected_q_value", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "prio_diff", "=", "diff", "\n", "prios", "=", "np", ".", "abs", "(", "prio_diff", ")", "+", "args", ".", "prio_eps", "\n", "# the Huber loss is used ", "\n", "", "weighted_diff", "=", "weights_", "*", "diff", "\n", "q_value_grad", "=", "1.", "/", "args", ".", "batch_size", "*", "weighted_diff", "\n", "\n", "all_grads", "=", "torch", ".", "zeros_like", "(", "all_q_values", ")", "\n", "# manually backpropagate the gradient through the term \"expected_q_value\" ", "\n", "next_q_value_grad", "=", "-", "(", "1.", "-", "target_mask", ")", "*", "q_value_grad", "\n", "next_q_value_grad", "=", "next_q_value_grad", "*", "gamma_mul_one_minus_done_", "\n", "grads", "=", "torch", ".", "from_numpy", "(", "np", ".", "concatenate", "(", "[", "q_value_grad", ",", "next_q_value_grad", "]", ",", "axis", "=", "0", ")", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "all_grads", ".", "scatter_", "(", "1", ",", "torch", ".", "cat", "(", "[", "action", ",", "next_q_action", "]", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "1", ")", ",", "grads", ")", "\n", "all_q_values", ".", "backward", "(", "all_grads", ")", "# this method makes it run faster ", "\n", "\n", "report_loss", "=", "np", ".", "dot", "(", "diff", ",", "weights_", "*", "diff", ")", "/", "args", ".", "batch_size", "\n", "\n", "", "if", "args", ".", "currentTask", "==", "\"Residual\"", ":", "\n", "# compute the current and next state values in a single pass ", "\n", "        ", "size", "=", "list", "(", "state_next_state", ".", "size", "(", ")", ")", "\n", "current_and_next_states", "=", "state_next_state", ".", "view", "(", "[", "-", "1", "]", "+", "size", "[", "2", ":", "]", ")", "\n", "# compute the q values and the gradient", "\n", "all_q_values", "=", "current_model", "(", "current_and_next_states", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_values", ",", "next_q_values", "=", "all_q_values", "[", ":", "args", ".", "batch_size", "]", ",", "all_q_values", "[", "args", ".", "batch_size", ":", "2", "*", "args", ".", "batch_size", "]", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", ",", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "\n", "q_value", ",", "next_q_value", "=", "torch", ".", "stack", "(", "(", "q_value", ",", "next_q_value", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "expected_q_value", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value", "\n", "\n", "# then compute the q values and the loss", "\n", "", "diff", "=", "q_value", "-", "expected_q_value", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "prio_diff", "=", "diff", "\n", "prios", "=", "np", ".", "abs", "(", "prio_diff", ")", "+", "args", ".", "prio_eps", "\n", "# the Huber loss is used ", "\n", "", "weighted_diff", "=", "weights_", "*", "diff", "\n", "q_value_grad", "=", "1.", "/", "args", ".", "batch_size", "*", "weighted_diff", "\n", "\n", "all_grads", "=", "torch", ".", "zeros_like", "(", "all_q_values", ")", "\n", "# manually backpropagate the gradient through the term \"expected_q_value\" ", "\n", "next_q_value_grad", "=", "-", "q_value_grad", "\n", "next_q_value_grad", "=", "next_q_value_grad", "*", "gamma_mul_one_minus_done_", "\n", "grads", "=", "torch", ".", "from_numpy", "(", "np", ".", "concatenate", "(", "[", "q_value_grad", ",", "next_q_value_grad", "]", ",", "axis", "=", "0", ")", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "all_grads", ".", "scatter_", "(", "1", ",", "torch", ".", "cat", "(", "[", "action", ",", "next_q_action", "]", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "1", ")", ",", "grads", ")", "\n", "all_q_values", ".", "backward", "(", "all_grads", ")", "# this method makes it run faster ", "\n", "\n", "report_loss", "=", "np", ".", "dot", "(", "diff", ",", "weights_", "*", "diff", ")", "/", "args", ".", "batch_size", "\n", "\n", "", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "replay_buffer", ".", "update_priorities", "(", "indices", ",", "prios", ")", "\n", "# gradient clipping ", "\n", "", "if", "args", ".", "grad_clip", ">", "0.", ":", "\n", "        ", "grad_norm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "current_model", ".", "parameters", "(", ")", ",", "max_norm", "=", "args", ".", "grad_clip", ")", "\n", "accu1", "+=", "grad_norm", "\n", "accu2", "+=", "grad_norm", "**", "2", "\n", "", "if", "args", ".", "do_update_target", ":", "update_target", "(", "current_model", ",", "target_model", ")", ";", "args", ".", "do_update_target", "=", "False", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "off_policy_rate", "=", "np", ".", "mean", "(", "(", "np", ".", "argmax", "(", "q_values", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", "!=", "action_", ")", ".", "astype", "(", "np", ".", "float", ")", "*", "true_weights", ")", "\n", "\n", "i_count", "+=", "1", "\n", "accu_loss", "+=", "report_loss", "\n", "report_period", "=", "math", ".", "ceil", "(", "args", ".", "evaluation_interval", "/", "args", ".", "train_freq", ")", "\n", "if", "i_count", "%", "report_period", "==", "0", "and", "accu1", "!=", "0.", ":", "\n", "        ", "print", "(", "\"gradient norm {:.3f} +- {:.3f}\"", ".", "format", "(", "accu1", "/", "report_period", ",", "math", ".", "sqrt", "(", "accu2", "/", "report_period", "-", "(", "accu1", "/", "report_period", ")", "**", "2", ")", ")", ")", "\n", "accu1", ",", "accu2", "=", "0.", ",", "0.", "\n", "if", "not", "args", ".", "silent", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}mse_{}.txt'", ".", "format", "(", "args", ".", "currentTask", ",", "args", ".", "comment", ")", ")", ",", "'a'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'{:.0f}\\t{}\\n'", ".", "format", "(", "(", "i_count", "*", "args", ".", "train_freq", "+", "args", ".", "learning_start", ")", "*", "4", ",", "accu_loss", "/", "report_period", ")", ")", "\n", "", "", "accu_loss", "=", "0.", "\n", "\n", "", "return", "report_loss", ",", "off_policy_rate", "\n", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.test.test": [[10, 30], ["model.DQN().to", "common.utils.epsilon_scheduler", "torch.load", "torch.load", "print", "model_dict.pop", "DQN().to.load_state_dict", "test.test_whole", "results.append", "len", "model.DQN", "os.path.join", "open", "data_f.write", "open", "data_f.write", "print", "numpy.mean", "numpy.mean", "numpy.std", "math.sqrt", "numpy.std", "math.sqrt", "len", "len"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.epsilon_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test_whole", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN"], ["def", "test", "(", "env", ",", "args", ")", ":", "\n", "    ", "current_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "epsilon_by_frame", "=", "epsilon_scheduler", "(", "args", ".", "eps_start", ",", "args", ".", "eps_mid", ",", "args", ".", "eps_final", ")", "\n", "#current_model.eval()", "\n", "\n", "results", "=", "[", "]", "\n", "for", "filename", "in", "args", ".", "load_model", ":", "\n", "        ", "model_dict", ",", "step_idx", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}.pth'", ".", "format", "(", "filename", ")", ")", ",", "map_location", "=", "args", ".", "device", ")", "\n", "print", "(", "\"load {} at training step {}\"", ".", "format", "(", "filename", ",", "step_idx", ")", ")", "\n", "model_dict", ".", "pop", "(", "\"scale\"", ",", "None", ")", "\n", "current_model", ".", "load_state_dict", "(", "model_dict", ")", "\n", "\n", "mean", ",", "std", "=", "test_whole", "(", "current_model", ",", "env", ",", "args", ",", "filename", ",", "epsilon_by_frame", "(", "step_idx", ")", ",", "num_trial", "=", "args", ".", "num_trial", ")", "#0.01)", "\n", "results", ".", "append", "(", "mean", ")", "\n", "with", "open", "(", "'{}.txt'", ".", "format", "(", "args", ".", "env", ")", ",", "'a'", ")", "as", "data_f", ":", "\n", "            ", "data_f", ".", "write", "(", "'{}: {} +- {}\\n'", ".", "format", "(", "filename", ",", "mean", ",", "std", ")", ")", "\n", "", "", "if", "len", "(", "args", ".", "load_model", ")", ">", "1", ":", "\n", "        ", "with", "open", "(", "'{}.txt'", ".", "format", "(", "args", ".", "env", ")", ",", "'a'", ")", "as", "data_f", ":", "\n", "            ", "data_f", ".", "write", "(", "'{} +- {}\\n'", ".", "format", "(", "np", ".", "mean", "(", "results", ")", ",", "np", ".", "std", "(", "results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "results", ")", ")", ")", ")", "\n", "print", "(", "'{} +- {}'", ".", "format", "(", "np", ".", "mean", "(", "results", ")", ",", "np", ".", "std", "(", "results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "results", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.test.test_whole": [[31, 79], ["min", "range", "len", "numpy.mean", "numpy.mean", "print", "envs.append", "env.seed", "env.reset", "len", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "current_model.act", "enumerate", "reversed", "mark_remove.clear", "numpy.std", "math.sqrt", "copy.deepcopy", "random.randrange", "range", "range", "zip", "env.step", "envs.pop", "states.pop", "episode_lengths.pop", "episode_rewards.pop", "len", "range", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "env.reset", "len", "env.unwrapped.ale.game_over", "reward_results.append", "length_results.append", "torch.from_numpy().to", "torch.from_numpy().to", "mark_remove.append", "torch.from_numpy", "torch.from_numpy", "numpy.array().reshape", "numpy.array", "len"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "", "", "def", "test_whole", "(", "current_model", ",", "env", ",", "args", ",", "idx", ",", "epsilon", ",", "num_parallel", "=", "16", ",", "num_trial", "=", "400", ")", ":", "#16", "\n", "    ", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "envs", "=", "[", "env", "]", "\n", "num_parallel", "=", "min", "(", "num_parallel", ",", "num_trial", ")", "\n", "for", "_i", "in", "range", "(", "num_parallel", "-", "1", ")", ":", "envs", ".", "append", "(", "copy", ".", "deepcopy", "(", "env", ")", ")", "\n", "for", "env", "in", "envs", ":", "env", ".", "seed", "(", "random", ".", "randrange", "(", "1000000", ")", ")", "\n", "states", "=", "[", "env", ".", "reset", "(", ")", "for", "env", "in", "envs", "]", "\n", "episode_rewards", "=", "[", "0.", "for", "_i", "in", "range", "(", "num_parallel", ")", "]", "\n", "episode_lengths", "=", "[", "0", "for", "_i", "in", "range", "(", "num_parallel", ")", "]", "\n", "reward_results", "=", "[", "]", "\n", "length_results", "=", "[", "]", "\n", "\n", "trial", "=", "len", "(", "states", ")", "\n", "mark_remove", "=", "[", "]", "\n", "while", "len", "(", "envs", ")", ">", "0", ":", "\n", "# decide the action", "\n", "        ", "epsilons", "=", "(", "epsilon", "for", "_", "in", "range", "(", "len", "(", "states", ")", ")", ")", "\n", "\n", "tensored_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "state", ".", "_frames", "for", "state", "in", "states", "]", ")", ".", "reshape", "(", "(", "len", "(", "states", ")", ",", "-", "1", ")", "+", "image_shape", ")", ")", ".", "to", "(", "args", ".", "device", ")", ".", "float", "(", ")", ".", "div_", "(", "255.", ")", "\n", "actions", ",", "evaluateds", ",", "(", "Qss", ",", "bestActions", ")", "=", "current_model", ".", "act", "(", "tensored_states", ",", "epsilons", ")", "\n", "for", "_i", ",", "(", "env", ",", "action", ")", "in", "enumerate", "(", "zip", "(", "envs", ",", "actions", ")", ")", ":", "\n", "\n", "# the environment proceeds by one step (4 frames)", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "episode_rewards", "[", "_i", "]", "+=", "reward", "\n", "episode_lengths", "[", "_i", "]", "+=", "1", "\n", "\n", "if", "done", ":", "\n", "                ", "if", "env", ".", "unwrapped", ".", "ale", ".", "game_over", "(", ")", "or", "\"TimeLimit.truncated\"", "in", "info", ":", "\n", "                    ", "reward_results", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", ";", "length_results", ".", "append", "(", "episode_lengths", "[", "_i", "]", ")", "\n", "episode_rewards", "[", "_i", "]", "=", "0.", ";", "episode_lengths", "[", "_i", "]", "=", "0", "\n", "if", "trial", "<", "num_trial", ":", "\n", "                        ", "trial", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "mark_remove", ".", "append", "(", "_i", ")", "\n", "", "", "states", "[", "_i", "]", "=", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "[", "_i", "]", "=", "next_state", "\n", "", "", "for", "_i", "in", "reversed", "(", "mark_remove", ")", ":", "\n", "            ", "envs", ".", "pop", "(", "_i", ")", ";", "states", ".", "pop", "(", "_i", ")", ";", "episode_lengths", ".", "pop", "(", "_i", ")", ";", "episode_rewards", ".", "pop", "(", "_i", ")", "\n", "", "mark_remove", ".", "clear", "(", ")", "\n", "\n", "", "mean_reward", "=", "np", ".", "mean", "(", "reward_results", ")", "\n", "std_reward", "=", "np", ".", "std", "(", "reward_results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "reward_results", ")", ")", "\n", "mean_length", "=", "np", ".", "mean", "(", "length_results", ")", "\n", "print", "(", "\"Test Result - Reward {:.2f}+-{:.2f} Length {:.1f} for {}\"", ".", "format", "(", "mean_reward", ",", "std_reward", ",", "mean_length", ",", "idx", ")", ")", "\n", "return", "mean_reward", ",", "std_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.DQNBase.__init__": [[30, 57], ["torch.Module.__init__", "model.Flatten", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.DQNBase.modules", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "type", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "module.bias.data.zero_", "model.DQNBase._feature_size"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size"], ["        ", "super", "(", "DQNBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "num_actions", "=", "env", ".", "action_space", ".", "n", "\n", "self", ".", "Linear", "=", "Linear", "# We have overridden the \"reset_parameters\" method for a more well-principled initialization", "\n", "\n", "self", ".", "flatten", "=", "Flatten", "(", ")", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ",", "padding", "=", "0", "if", "self", ".", "input_shape", "[", "1", "]", "!=", "105", "else", "2", ")", ",", "\n", "#nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0 if self.input_shape[1]!=105 else 2),", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "for", "module", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "type", "(", "module", ")", "==", "nn", ".", "Conv2d", ":", "init", ".", "kaiming_uniform_", "(", "module", ".", "weight", ".", "data", ",", "nonlinearity", "=", "\"relu\"", ",", "a", "=", "0.", ")", ";", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", ",", "self", ".", "num_actions", ")", "\n", ")", "\n", "self", ".", "out_bias", "=", "self", ".", "fc", "[", "-", "1", "]", ".", "bias", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "**", "kwargs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.DQNBase.forward": [[58, 63], ["model.DQNBase.features", "model.DQNBase.flatten", "model.DQNBase.fc"], "methods", ["None"], ["        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "x", "\n", "\n", "", "def", "_feature_size", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.DQNBase._feature_size": [[64, 66], ["model.DQNBase.features().view().size", "model.DQNBase.features().view", "model.DQNBase.features", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["        ", "return", "self", ".", "features", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n", "", "def", "act", "(", "self", ",", "state", ",", "epsilon", ",", "**", "kwargs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.DQNBase.act": [[67, 93], ["state.unsqueeze.unsqueeze.dim", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "state.unsqueeze.unsqueeze.unsqueeze", "model.DQNBase.forward().cpu().numpy().squeeze", "numpy.argmax", "random.random", "random.randrange", "state.unsqueeze.unsqueeze.dim", "enumerate", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.DQNBase.forward().cpu().numpy", "numpy.argmax", "numpy.copy", "state.unsqueeze.unsqueeze.size", "model.DQNBase.forward().cpu().numpy", "random.random", "random.randrange", "model.DQNBase.forward().cpu", "model.DQNBase.forward().cpu", "model.DQNBase.forward", "model.DQNBase.forward"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward"], ["        ", "\"\"\"\n        Parameters\n        ----------\n        state       torch.Tensor with appropritate device type\n        epsilon     epsilon for epsilon-greedy\n        \"\"\"", "\n", "if", "state", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "state", "=", "state", ".", "unsqueeze", "(", "0", ")", "\n", "q_values", "=", "self", ".", "forward", "(", "state", ",", "**", "kwargs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", "\n", "bestAction", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "0", ")", "\n", "", "if", "random", ".", "random", "(", ")", ">=", "epsilon", ":", "\n", "                ", "action", "=", "bestAction", "\n", "", "else", ":", "\n", "                ", "action", "=", "random", ".", "randrange", "(", "self", ".", "num_actions", ")", "\n", "", "", "elif", "state", ".", "dim", "(", ")", "==", "4", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "q_values", "=", "self", ".", "forward", "(", "state", ",", "**", "kwargs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "bestAction", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "1", ")", "\n", "action", "=", "np", ".", "copy", "(", "bestAction", ")", "\n", "", "for", "i", ",", "e", "in", "enumerate", "(", "epsilon", ")", ":", "\n", "                ", "if", "random", ".", "random", "(", ")", "<", "e", ":", "\n", "                    ", "action", "[", "i", "]", "=", "random", ".", "randrange", "(", "self", ".", "num_actions", ")", "\n", "", "", "", "else", ":", "assert", "False", ",", "\"The input state has an invalid shape {}\"", ".", "format", "(", "state", ".", "size", "(", ")", ")", "\n", "return", "action", ",", "action", "==", "bestAction", ",", "(", "q_values", ",", "bestAction", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.DuelingDQN.__init__": [[100, 128], ["model.DQNBase.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.DuelingDQN.register_buffer", "model.DuelingDQN.fc[].weight.register_hook", "model.DuelingDQN.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DuelingDQN.Linear", "model.DuelingDQN.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DuelingDQN.Linear", "model.DuelingOutput", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zip", "model.DuelingDQN.fc[].weight.zero_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "model.DuelingDQN._feature_size", "model.DuelingDQN._feature_size", "model.DuelingDQN.fc[].parameters", "model.DuelingDQN.advantage[].parameters", "model.DuelingDQN.value[].parameters"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size"], ["        ", "super", "(", "DuelingDQN", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "advantage", "=", "self", ".", "fc", "\n", "self", ".", "value", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", ",", "1", ")", "\n", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", "*", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", "*", "2", ",", "self", ".", "num_actions", "+", "1", ")", ",", "\n", "DuelingOutput", "(", "self", ".", "num_actions", ")", "\n", ")", "\n", "# rewrite the parameters of \"self.advantage\" and \"self.value\" into \"self.fc\" so that they are combined into a single computation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "p", ",", "p_a", ",", "p_v", "in", "zip", "(", "self", ".", "fc", "[", "0", "]", ".", "parameters", "(", ")", ",", "self", ".", "advantage", "[", "0", "]", ".", "parameters", "(", ")", ",", "self", ".", "value", "[", "0", "]", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "p", "[", ":", "512", "]", "=", "p_a", ";", "p", "[", "512", ":", "512", "*", "2", "]", "=", "p_v", "\n", "", "self", ".", "fc", "[", "2", "]", ".", "weight", ".", "zero_", "(", ")", "\n", "self", ".", "fc", "[", "2", "]", ".", "weight", "[", ":", "self", ".", "num_actions", ",", ":", "512", "]", "=", "self", ".", "advantage", "[", "2", "]", ".", "weight", ";", "self", ".", "fc", "[", "2", "]", ".", "weight", "[", "-", "1", ",", "512", ":", "512", "*", "2", "]", "=", "self", ".", "value", "[", "2", "]", ".", "weight", "\n", "self", ".", "fc", "[", "2", "]", ".", "bias", "[", ":", "self", ".", "num_actions", "]", "=", "self", ".", "advantage", "[", "2", "]", ".", "bias", ";", "self", ".", "fc", "[", "2", "]", ".", "bias", "[", "-", "1", "]", "=", "self", ".", "value", "[", "2", "]", ".", "bias", "\n", "del", "self", ".", "value", ",", "self", ".", "advantage", "\n", "# mask the backpropagated gradient on \"self.fc[2].weight\"", "\n", "", "self", ".", "register_buffer", "(", "'grad_mask'", ",", "torch", ".", "zeros", "(", "self", ".", "num_actions", "+", "1", ",", "512", "*", "2", ")", ")", "\n", "self", ".", "grad_mask", "[", ":", "self", ".", "num_actions", ",", ":", "512", "]", "=", "1.", ";", "self", ".", "grad_mask", "[", "-", "1", ",", "512", ":", "512", "*", "2", "]", "=", "1.", "\n", "self", ".", "dueling_grad_hook", "=", "self", ".", "fc", "[", "2", "]", ".", "weight", ".", "register_hook", "(", "lambda", "grad", ":", "self", ".", "grad_mask", "*", "grad", ")", "\n", "\n", "self", ".", "out_bias", "=", "self", ".", "fc", "[", "-", "2", "]", ".", "bias", "\n", "\n", "", "", "class", "DuelingOutput", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.DuelingOutput.__init__": [[130, 143], ["torch.Module.__init__", "model.DuelingOutput.register_buffer", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["        ", "super", "(", "DuelingOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'output_matrix'", ",", "torch", ".", "Tensor", "(", "num_actions", ",", "num_actions", "+", "1", ")", ")", "\n", "# set the \"-advantage.mean(1, keepdim=True)\" term", "\n", "self", ".", "output_matrix", "[", ":", ",", ":", "]", "=", "-", "1.", "/", "num_actions", "\n", "# set the last input dim, the average value, added to all Qs", "\n", "self", ".", "output_matrix", "[", ":", ",", "-", "1", "]", "=", "1.", "\n", "# set the diagonal term", "\n", "for", "i", "in", "range", "(", "num_actions", ")", ":", "\n", "            ", "self", ".", "output_matrix", "[", "i", ",", "i", "]", "=", "(", "num_actions", "-", "1", ")", "/", "num_actions", "\n", "# this complete the definition of \"output_matrix\", which computes \"value + (advantage - advantage.mean(1, keepdim=True)) * rescale \"", "\n", "", "assert", "not", "self", ".", "output_matrix", ".", "requires_grad", "\n", "\n", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.DuelingOutput.forward": [[144, 146], ["torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["None"], ["        ", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "output_matrix", ",", "None", ")", "\n", "\n", "", "", "class", "Flatten", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.Flatten.forward": [[148, 150], ["x.view", "x.size"], "methods", ["None"], ["        ", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n", "", "", "class", "Linear", "(", "nn", ".", "Linear", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.Linear.reset_parameters": [[152, 159], ["torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "model.Linear.bias.data.zero_"], "methods", ["None"], ["        ", "init", ".", "kaiming_uniform_", "(", "self", ".", "weight", ",", "nonlinearity", "=", "\"relu\"", ",", "a", "=", "0.", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "#fan_in, _  = init._calculate_fan_in_and_fan_out(self.weight)", "\n", "#bound = 1./math.sqrt(fan_in)", "\n", "#init.uniform_(self.bias, -bound, bound)", "\n", "            ", "self", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.DQN": [[13, 19], ["model.DuelingDQN", "model.DQNBase"], "function", ["None"], ["    ", "if", "args", ".", "dueling", ":", "\n", "        ", "model", "=", "DuelingDQN", "(", "env", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "DQNBase", "(", "env", ")", "\n", "", "return", "model", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.inverse_transform_Q": [[163, 168], ["type", "Qs.sign", "type", "model.jit_inverse_transform_Q", "numpy.sign", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "math.sqrt", "Qs.abs", "abs"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.jit_inverse_transform_Q"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.jit_inverse_transform_Q": [[169, 172], ["numba.njit", "numpy.sign", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.transform_Q": [[173, 178], ["type", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "type", "model.jit_transform_Q", "numpy.sign", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "math.sqrt", "Qs.abs", "abs"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.jit_transform_Q"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.jit_transform_Q": [[179, 182], ["numba.njit", "numpy.sign", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.d_transform_Q": [[184, 187], ["numba.njit", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.d_inverse_transform_Q": [[189, 192], ["numba.njit", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.transform_backpropagate": [[193, 199], ["numba.njit", "next_q_value_grad.astype", "model.d_transform_Q", "model.d_inverse_transform_Q"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.d_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.d_inverse_transform_Q"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.auto_initialize": [[201, 220], ["common.heuristics.get_initialization_stat", "print", "max", "print"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.get_initialization_stat", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.model.compute_value_error": [[221, 234], ["reversed", "reversed_value_list.reverse", "numpy.array", "numpy.array", "reversed_value_list.append", "numpy.mean", "numpy.mean"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.optimizers.AdamW.__init__": [[21, 36], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.optimizers.AdamW.__setstate__": [[37, 41], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.optimizers.AdamW.step": [[42, 120], ["closure", "math.sqrt", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.data.addcdiv_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "lr", ",", "eps", ",", "amsgrad", "=", "group", "[", "'lr'", "]", ",", "group", "[", "'eps'", "]", ",", "group", "[", "'amsgrad'", "]", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "lr", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'AdamW does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "state", "[", "'PopArt_rescale'", "]", "=", "1.", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "'max_exp_avg_sq'", "in", "state", ":", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# We incorporate the term group['lr'] into the momentum, and define the bias_correction1 such that it respects the possibly moving group['lr']", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "lr", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1. - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "lr", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "*", "sqrt_bias_correction2", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"step_size\" and \"eps\"", "\n", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "rescaling", "=", "state", "[", "'PopArt_rescale'", "]", "\n", "exp_avg", ".", "mul_", "(", "beta1", "*", "rescaling", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "(", "1.", "-", "beta1", ")", "*", "lr", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", "**", "2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "# 'eps' is first multiplied by sqrt_bias_correction2 and then divided by sqrt_bias_correction2", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.optimizers.AdamBelief.__init__": [[136, 151], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamBelief", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.optimizers.AdamBelief.__setstate__": [[152, 156], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.optimizers.AdamBelief.step": [[157, 234], ["closure", "math.sqrt", "exp_avg.mul_().add_", "exp_avg_var.mul_().addcmul_", "p.data.addcdiv_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.max", "max_exp_avg_var.sqrt().add_", "exp_avg_var.sqrt().add_", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_var.mul_", "max_exp_avg_var.sqrt", "exp_avg_var.sqrt"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "lr", ",", "eps", ",", "amsgrad", "=", "group", "[", "'lr'", "]", ",", "group", "[", "'eps'", "]", ",", "group", "[", "'amsgrad'", "]", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "lr", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'AdamW does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_var", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_var'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "'max_exp_avg_var'", "in", "state", ":", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "max_exp_avg_var", "=", "state", "[", "'max_exp_avg_var'", "]", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# We define the bias_correction1 such that it respects the possibly moving \"group['lr']\"", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "lr", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1. - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "lr", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "*", "sqrt_bias_correction2", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"step_size\" and \"eps\"", "\n", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "diff", "=", "grad", "-", "exp_avg", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "(", "1.", "-", "beta1", ")", ")", "\n", "exp_avg_var", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "diff", ",", "diff", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_var", ",", "exp_avg_var", ",", "out", "=", "max_exp_avg_var", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "# 'eps' is first multiplied by sqrt_bias_correction2 and then divided by sqrt_bias_correction2", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", "*", "lr", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.optimizers.LaProp.__init__": [[238, 254], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "4e-4", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-15", ",", "\n", "weight_decay", "=", "0.", ",", "amsgrad", "=", "False", ",", "centered", "=", "False", ")", ":", "\n", "        ", "self", ".", "centered", "=", "centered", "\n", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "LaProp", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.optimizers.LaProp.__setstate__": [[255, 259], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "LaProp", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec5.1, 5.3 and B.2, B.3, B.4.optimizers.LaProp.step": [[260, 349], ["closure", "math.sqrt", "exp_avg_sq.mul_().addcmul_", "denom.addcmul.addcmul.sqrt().add_", "exp_avg.mul_().addcdiv_", "p.data.add_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "exp_mean_avg_sq.mul_().add_", "torch.zeros_like", "exp_avg_sq.mul_", "denom.addcmul.addcmul.addcmul", "torch.max", "denom.addcmul.addcmul.sqrt", "exp_avg.mul_", "exp_mean_avg_sq.mul_"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'LaProp does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "state", "[", "'exp_mean_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "state", "[", "'PopArt_rescale'", "]", "=", "1.", "\n", "state", "[", "'Momentum_rescale'", "]", "=", "1.", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "self", ".", "centered", ":", "\n", "                    ", "exp_mean_avg_sq", "=", "state", "[", "'exp_mean_avg_sq'", "]", "\n", "", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "group", "[", "'lr'", "]", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1 - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "group", "[", "'lr'", "]", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "rescaling", "=", "state", "[", "'PopArt_rescale'", "]", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", "**", "2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "\n", "denom", "=", "exp_avg_sq", "\n", "if", "self", ".", "centered", ":", "\n", "                    ", "exp_mean_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "beta2", ")", "\n", "if", "state", "[", "'step'", "]", ">", "5", ":", "\n", "                        ", "denom", "=", "denom", ".", "addcmul", "(", "exp_mean_avg_sq", ",", "exp_mean_avg_sq", ",", "value", "=", "-", "1.", ")", "\n", "\n", "", "", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "(", "self", ".", "centered", "and", "state", "[", "'step'", "]", "<=", "5", ")", ":", "\n", "# Maintains the maximum of all (centered) 2nd moment running avg. till now", "\n", "                        ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "denom", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", "\n", "\n", "", "", "denom", "=", "denom", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", "*", "sqrt_bias_correction2", ")", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"exp_avg\" and \"eps\"", "\n", "\n", "momentum_rescaling", "=", "state", "[", "'Momentum_rescale'", "]", "\n", "exp_avg", ".", "mul_", "(", "beta1", "*", "momentum_rescaling", ")", ".", "addcdiv_", "(", "grad", ",", "denom", ",", "value", "=", "(", "1.", "-", "beta1", ")", "*", "group", "[", "'lr'", "]", "*", "sqrt_bias_correction2", ")", "\n", "\n", "p", ".", "data", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.3.arguments.get_args": [[4, 141], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.is_available"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Deep Q-Learning'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--algorithm'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "# Basic Arguments", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Disable CUDA training'", ")", "\n", "\n", "# Training Arguments", "\n", "parser", ".", "add_argument", "(", "'--max-steps'", ",", "type", "=", "int", ",", "default", "=", "50000000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps to train (equal to actual_frames / 4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--buffer-size'", ",", "type", "=", "int", ",", "default", "=", "1000000", ",", "metavar", "=", "'CAPACITY'", ",", "\n", "help", "=", "'Maximum memory buffer size'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-discard-experience'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly discard half of collected experience data'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-replace-memory'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly replace old experiences by new experiences when the memory replay is full (by default it is first-in-first-out)'", ")", "\n", "parser", ".", "add_argument", "(", "'--update-target'", ",", "type", "=", "int", ",", "default", "=", "8000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Interval of target network update'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-freq'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps between optimization steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "metavar", "=", "'\u03b3'", ",", "\n", "help", "=", "'Discount factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning-start'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'How many steps of the model to collect transitions for before learning starts'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-start'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Start value of epsilon'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-mid'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'Mid value of epsilon (at one million-th step)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-final'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'Final value of epsilon'", ")", "\n", "\n", "# Algorithm Arguments", "\n", "parser", ".", "add_argument", "(", "'--double'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Double Q Learning'", ")", "\n", "parser", ".", "add_argument", "(", "'--dueling'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Dueling Network with Default Evaluation (Avg.) on Advantages'", ")", "\n", "parser", ".", "add_argument", "(", "'--prioritized-replay'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable prioritized experience replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.6", ",", "\n", "help", "=", "'Alpha value for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--ratio-min-prio'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Allowed maximal ratio between the smallest sampling priority and the average priority, which equals the maximal importance sampling weight'", ")", "\n", "parser", ".", "add_argument", "(", "'--prio-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-10", ",", "\n", "help", "=", "'A small number added before computing the priority'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-start'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "\n", "help", "=", "'Start value of beta for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-frames'", ",", "type", "=", "float", ",", "default", "=", "50000000", ",", "\n", "help", "=", "'End step of beta schedule for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--IS-weight-only-smaller'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Divide all importance sampling correction weights by the largest weight, so that they are smaller or equal to one'", ")", "\n", "\n", "# Environment Arguments", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "type", "=", "str", ",", "default", "=", "'PongNoFrameskip-v4'", ",", "\n", "help", "=", "'Environment Name'", ")", "\n", "parser", ".", "add_argument", "(", "'--episode-life'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Whether losing one life is considered as an end of an episode(1) or not(0) from the agent's perspective\"", ")", "\n", "parser", ".", "add_argument", "(", "'--grey'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Change the observation to greyscale (default 1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-stack'", ",", "type", "=", "str", ",", "default", "=", "\"4\"", ",", "\n", "help", "=", "'Number of adjacent observations to stack'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-downscale'", ",", "type", "=", "int", ",", "default", "=", "84", ",", "# we will always crop the frame when it has a height of 250 instead of the default 210 (crop by top 28, bottom 12) ", "\n", "help", "=", "'Downscaling ratio of the frame observation (if <= 10) or image size as the downscaling target (if >10)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-episode-steps'", ",", "type", "=", "int", ",", "default", "=", "20000", ",", "\n", "help", "=", "\"The maximum number of steps allowd before resetting the real episode.\"", ")", "\n", "\n", "# Evaluation Arguments", "\n", "parser", ".", "add_argument", "(", "'--load-model'", ",", "type", "=", "str", ",", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "'Pretrained model names to load (state dict)'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Evaluate only'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-trial'", ",", "type", "=", "int", ",", "default", "=", "400", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluation_interval'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "'Steps for printing statistics'", ")", "\n", "\n", "# Optimization Arguments", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "6.25e-5", ",", "metavar", "=", "'\u03b7'", ",", "\n", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--optim'", ",", "type", "=", "str", ",", "default", "=", "'adam'", ",", "\n", "help", "=", "'Optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--adam-eps'", ",", "type", "=", "float", ",", "default", "=", "1.5e-4", ",", "\n", "help", "=", "'Epsilon of adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu-id'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Which GPU to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta2'", ",", "type", "=", "float", ",", "default", "=", "0.999", ")", "\n", "parser", ".", "add_argument", "(", "'--beta1'", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "'--grad-clip'", ",", "type", "=", "float", ",", "default", "=", "10.", ",", "# when transform-Q is used, it should be 40.", "\n", "help", "=", "\"Gradient clipping norm; 0 corresponds to no gradient clipping\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--silent'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--comment'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "\n", "# A simple option to reproduce the original DQN", "\n", "parser", ".", "add_argument", "(", "'--originalDQN'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To reproduce the original DQN\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--save-best'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To save the model when it performs best during training, averaged over 40 episodes\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "originalDQN", ":", "\n", "        ", "del", "args", ".", "originalDQN", "\n", "# the most important arguments for reproducing the original prioritized dueling DDQN", "\n", "args", ".", "algorithm", "=", "\"DQN\"", "\n", "args", ".", "lr", "=", "6.25e-5", "\n", "args", ".", "adam_eps", "=", "1.5e-4", "\n", "args", ".", "grad_clip", "=", "10.", "\n", "args", ".", "prioritized_replay", "=", "True", "\n", "args", ".", "alpha", "=", "0.6", "\n", "args", ".", "beta_start", "=", "0.4", "\n", "args", ".", "beta_frames", "=", "50000000.", "\n", "args", ".", "auto_init", "=", "False", "\n", "args", ".", "gamma", "=", "0.99", "\n", "args", ".", "double", "=", "True", "\n", "args", ".", "dueling", "=", "True", "\n", "args", ".", "episode_life", "=", "1", "\n", "args", ".", "randomly_replace_memory", "=", "False", "\n", "args", ".", "no_clip", "=", "False", "\n", "args", ".", "transform_Q", "=", "False", "\n", "\n", "", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:{}\"", ".", "format", "(", "args", ".", "gpu_id", ")", "if", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "\n", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.main.main": [[9, 32], ["torch.set_num_threads", "arguments.get_args", "common.wrappers.make_atari", "common.wrappers.wrap_atari_dqn", "common.utils.set_global_seeds", "common.wrappers.wrap_atari_dqn.seed", "train", "common.wrappers.wrap_atari_dqn.close", "open", "f.write", "test.test", "common.wrappers.wrap_atari_dqn.close", "datetime.datetime.now().strftime", "datetime.datetime.now"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.arguments.get_args", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.make_atari", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.wrappers.wrap_atari_dqn", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.set_global_seeds", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.train", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test"], ["def", "main", "(", ")", ":", "\n", "    ", "torch", ".", "set_num_threads", "(", "2", ")", "# we need to constrain the number of threads; it can default to a large value", "\n", "args", "=", "get_args", "(", ")", "\n", "\n", "# keep a history of the commands that has been executed", "\n", "with", "open", "(", "\"commandl_history.txt\"", ",", "\"a\"", ")", "as", "f", ":", "\n", "        ", "f", ".", "write", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%Y/%m/%d %H:%M:%S\\t\"", ")", "+", "' '", ".", "join", "(", "sys", ".", "argv", ")", "+", "\"\\n\"", ")", "\n", "\n", "", "env", "=", "make_atari", "(", "args", ".", "env", ",", "args", ".", "max_episode_steps", ",", "clip_reward", "=", "(", "not", "(", "args", ".", "no_clip", "or", "args", ".", "transform_Q", ")", ")", "and", "(", "not", "args", ".", "evaluate", ")", ")", "\n", "env", "=", "wrap_atari_dqn", "(", "env", ",", "args", ")", "\n", "\n", "set_global_seeds", "(", "args", ".", "seed", ")", "\n", "env", ".", "seed", "(", "args", ".", "seed", ")", "\n", "\n", "if", "args", ".", "evaluate", ":", "\n", "        ", "test", "(", "env", ",", "args", ")", "\n", "env", ".", "close", "(", ")", "\n", "return", "\n", "\n", "", "from", "train", "import", "train", "as", "train", "\n", "train", "(", "env", ",", "args", ")", "\n", "\n", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.arguments.get_args": [[4, 138], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_mutually_exclusive_group", "parser.add_mutually_exclusive_group.add_argument", "parser.add_mutually_exclusive_group.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.device", "torch.cuda.is_available"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Deep Q-Learning'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--algorithm'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "# Basic Arguments", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch-size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "\n", "help", "=", "'Batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--no-cuda'", ",", "action", "=", "'store_true'", ",", "default", "=", "False", ",", "\n", "help", "=", "'Disable CUDA training'", ")", "\n", "\n", "# Training Arguments", "\n", "parser", ".", "add_argument", "(", "'--max-steps'", ",", "type", "=", "int", ",", "default", "=", "50000000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps to train (equal to actual_frames / 4)'", ")", "\n", "parser", ".", "add_argument", "(", "'--buffer-size'", ",", "type", "=", "int", ",", "default", "=", "1000000", ",", "metavar", "=", "'CAPACITY'", ",", "\n", "help", "=", "'Maximum memory buffer size'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-discard-experience'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly discard half of collected experience data'", ")", "\n", "parser", ".", "add_argument", "(", "'--randomly-replace-memory'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Randomly replace old experiences by new experiences when the memory replay is full (by default it is first-in-first-out)'", ")", "\n", "parser", ".", "add_argument", "(", "'--update-target'", ",", "type", "=", "int", ",", "default", "=", "8000", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Interval of target network update'", ")", "\n", "parser", ".", "add_argument", "(", "'--train-freq'", ",", "type", "=", "int", ",", "default", "=", "4", ",", "metavar", "=", "'STEPS'", ",", "\n", "help", "=", "'Number of steps between optimization steps'", ")", "\n", "parser", ".", "add_argument", "(", "'--gamma'", ",", "type", "=", "float", ",", "default", "=", "0.99", ",", "metavar", "=", "'\u03b3'", ",", "\n", "help", "=", "'Discount factor'", ")", "\n", "parser", ".", "add_argument", "(", "'--learning-start'", ",", "type", "=", "int", ",", "default", "=", "50000", ",", "metavar", "=", "'N'", ",", "\n", "help", "=", "'How many steps of the model to collect transitions for before learning starts'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-start'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "\n", "help", "=", "'Start value of epsilon'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-mid'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "\n", "help", "=", "'Mid value of epsilon (at one million-th step)'", ")", "\n", "parser", ".", "add_argument", "(", "'--eps-final'", ",", "type", "=", "float", ",", "default", "=", "0.01", ",", "\n", "help", "=", "'Final value of epsilon'", ")", "\n", "\n", "# Algorithm Arguments", "\n", "parser", ".", "add_argument", "(", "'--double'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Double Q Learning'", ")", "\n", "parser", ".", "add_argument", "(", "'--dueling'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable Dueling Network with Default Evaluation (Avg.) on Advantages'", ")", "\n", "parser", ".", "add_argument", "(", "'--prioritized-replay'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Enable prioritized experience replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0.6", ",", "\n", "help", "=", "'Alpha value for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--ratio-min-prio'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "\n", "help", "=", "'Allowed maximal ratio between the smallest sampling priority and the average priority, which equals the maximal importance sampling weight'", ")", "\n", "parser", ".", "add_argument", "(", "'--prio-eps'", ",", "type", "=", "float", ",", "default", "=", "1e-10", ",", "\n", "help", "=", "'A small number added before computing the priority'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-start'", ",", "type", "=", "float", ",", "default", "=", "0.4", ",", "\n", "help", "=", "'Start value of beta for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta-frames'", ",", "type", "=", "float", ",", "default", "=", "50000000", ",", "\n", "help", "=", "'End step of beta schedule for prioritized replay'", ")", "\n", "parser", ".", "add_argument", "(", "'--IS-weight-only-smaller'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Divide all importance sampling correction weights by the largest weight, so that they are smaller or equal to one'", ")", "\n", "\n", "# Environment Arguments", "\n", "parser", ".", "add_argument", "(", "'--env'", ",", "type", "=", "str", ",", "default", "=", "'PongNoFrameskip-v4'", ",", "\n", "help", "=", "'Environment Name'", ")", "\n", "parser", ".", "add_argument", "(", "'--episode-life'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "\"Whether losing one life is considered as an end of an episode(1) or not(0) from the agent's perspective\"", ")", "\n", "parser", ".", "add_argument", "(", "'--grey'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'Change the observation to greyscale (default 1)'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-stack'", ",", "type", "=", "str", ",", "default", "=", "\"4\"", ",", "\n", "help", "=", "'Number of adjacent observations to stack'", ")", "\n", "parser", ".", "add_argument", "(", "'--frame-downscale'", ",", "type", "=", "int", ",", "default", "=", "84", ",", "# we will always crop the frame when it has a height of 250 instead of the default 210 (crop by top 28, bottom 12) ", "\n", "help", "=", "'Downscaling ratio of the frame observation (if <= 10) or image size as the downscaling target (if >10)'", ")", "\n", "parser", ".", "add_argument", "(", "'--max-episode-steps'", ",", "type", "=", "int", ",", "default", "=", "20000", ",", "\n", "help", "=", "\"The maximum number of steps allowd before resetting the real episode.\"", ")", "\n", "\n", "# Evaluation Arguments", "\n", "parser", ".", "add_argument", "(", "'--load-model'", ",", "type", "=", "str", ",", "nargs", "=", "\"+\"", ",", "\n", "help", "=", "'Pretrained model names to load (state dict)'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluate'", ",", "action", "=", "'store_true'", ",", "\n", "help", "=", "'Evaluate only'", ")", "\n", "parser", ".", "add_argument", "(", "'--num-trial'", ",", "type", "=", "int", ",", "default", "=", "400", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluation_interval'", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "\n", "help", "=", "'Steps for printing statistics'", ")", "\n", "\n", "# Optimization Arguments", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "6.25e-5", ",", "metavar", "=", "'\u03b7'", ",", "\n", "help", "=", "'Learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--optim'", ",", "type", "=", "str", ",", "default", "=", "'adam'", ",", "\n", "help", "=", "'Optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--adam-eps'", ",", "type", "=", "float", ",", "default", "=", "1.5e-4", ",", "\n", "help", "=", "'Epsilon of adam optimizer'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu-id'", ",", "type", "=", "int", ",", "default", "=", "0", ",", "\n", "help", "=", "'Which GPU to use'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta2'", ",", "type", "=", "float", ",", "default", "=", "0.999", ")", "\n", "parser", ".", "add_argument", "(", "'--beta1'", ",", "type", "=", "float", ",", "default", "=", "0.9", ")", "\n", "parser", ".", "add_argument", "(", "'--grad-clip'", ",", "type", "=", "float", ",", "default", "=", "10.", ",", "# when transform-Q is used, it should be 40.", "\n", "help", "=", "\"Gradient clipping norm; 0 corresponds to no gradient clipping\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--silent'", ",", "action", "=", "'store_true'", ")", "\n", "parser", ".", "add_argument", "(", "'--comment'", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "\n", "# A simple option to reproduce the original DQN", "\n", "parser", ".", "add_argument", "(", "'--originalDQN'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To reproduce the original DQN\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--save-best'", ",", "action", "=", "'store_true'", ",", "help", "=", "\"To save the model when it performs best during training, averaged over 40 episodes\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "originalDQN", ":", "\n", "        ", "del", "args", ".", "originalDQN", "\n", "# the most important arguments for reproducing the original prioritized dueling DDQN", "\n", "args", ".", "algorithm", "=", "\"DQN\"", "\n", "args", ".", "lr", "=", "6.25e-5", "\n", "args", ".", "adam_eps", "=", "1.5e-4", "\n", "args", ".", "grad_clip", "=", "10.", "\n", "args", ".", "prioritized_replay", "=", "True", "\n", "args", ".", "alpha", "=", "0.6", "\n", "args", ".", "beta_start", "=", "0.4", "\n", "args", ".", "beta_frames", "=", "50000000.", "\n", "args", ".", "auto_init", "=", "False", "\n", "args", ".", "gamma", "=", "0.99", "\n", "args", ".", "double", "=", "True", "\n", "args", ".", "dueling", "=", "True", "\n", "args", ".", "episode_life", "=", "1", "\n", "args", ".", "randomly_replace_memory", "=", "False", "\n", "args", ".", "no_clip", "=", "False", "\n", "args", ".", "transform_Q", "=", "False", "\n", "\n", "", "args", ".", "cuda", "=", "not", "args", ".", "no_cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "args", ".", "device", "=", "torch", ".", "device", "(", "\"cuda:{}\"", ".", "format", "(", "args", ".", "gpu_id", ")", "if", "args", ".", "cuda", "else", "\"cpu\"", ")", "\n", "\n", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.train": [[16, 189], ["model.DQN().to", "model.DQN().to", "print", "DQN().to.parameters", "common.utils.update_target", "common.utils.epsilon_scheduler", "common.utils.beta_scheduler", "env.unwrapped.ale.lives", "common.utils.print_args", "args.optim.lower", "print", "env.reset", "range", "time.time", "common.replay_buffer.PrioritizedReplayBuffer", "common.replay_buffer.ReplayBuffer", "torch.SGD", "envs.append", "env.seed", "collections.deque", "collections.deque", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "DQN().to.act", "enumerate", "model.DQN", "model.DQN", "parameters", "optimizers.AdamW", "args.optim.startswith", "range", "range", "copy.deepcopy", "random.randrange", "range", "range", "float", "zip", "env.step", "float", "data_to_store.append", "sum", "parameters", "optimizers.AdamBelief", "common.utils.epsilon_scheduler.", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "data_to_store.clear", "length_list.append", "env.reset", "common.utils.beta_scheduler.", "train.compute_td_loss", "loss_list.append", "off_policy_rate_list.append", "parameters", "optimizers.LaProp", "range", "range", "float", "common.replay_buffer.ReplayBuffer.add", "env.unwrapped.ale.game_over", "reward_list.append", "len", "max", "model.auto_initialize", "len", "common.utils.print_log", "reward_list.clear", "length_list.clear", "loss_list.clear", "kwargs.values", "time.time", "p.numel", "parameters", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "max", "print", "random.randrange", "common.utils.update_target", "DQN().to.parameters", "len", "os.path.isdir", "os.mkdir", "open", "f.write", "collections.deque.append", "numpy.mean", "collections.deque.append", "type", "v.clear", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.save", "torch.save", "torch.save", "DQN().to.state_dict().copy", "numpy.array().reshape", "len", "len", "DQN().to.state_dict", "numpy.array"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.epsilon_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.beta_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_args", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.SumSegmentTree.sum", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.compute_td_loss", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.add", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.auto_initialize", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.print_log", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target"], ["def", "train", "(", "env", ",", "args", ")", ":", "\n", "    ", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "\n", "current_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "target_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "print", "(", "'    Total params: %.2fM'", "%", "(", "sum", "(", "p", ".", "numel", "(", ")", "for", "p", "in", "current_model", ".", "parameters", "(", ")", ")", "/", "1000000.0", ")", ")", "\n", "for", "para", "in", "target_model", ".", "parameters", "(", ")", ":", "para", ".", "requires_grad", "=", "False", "\n", "update_target", "(", "current_model", ",", "target_model", ")", "\n", "\n", "epsilon_by_frame", "=", "epsilon_scheduler", "(", "args", ".", "eps_start", ",", "args", ".", "eps_mid", ",", "args", ".", "eps_final", ")", "\n", "beta_by_frame", "=", "beta_scheduler", "(", "args", ".", "beta_start", ",", "args", ".", "beta_frames", ")", "\n", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "replay_buffer", "=", "PrioritizedReplayBuffer", "(", "args", ".", "buffer_size", ",", "args", ".", "alpha", ",", "args", ".", "IS_weight_only_smaller", ",", "allowed_avg_min_ratio", "=", "args", ".", "ratio_min_prio", ")", "\n", "", "else", ":", "\n", "        ", "replay_buffer", "=", "ReplayBuffer", "(", "args", ".", "buffer_size", ")", "\n", "\n", "#args.action_space = env.unwrapped.get_action_meanings()", "\n", "", "args", ".", "init_lives", "=", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "print_args", "(", "args", ")", "\n", "\n", "args", ".", "do_update_target", "=", "False", "\n", "# specify the RL algorithm to use ", "\n", "if", "args", ".", "algorithm", "!=", "\"DQN\"", "and", "args", ".", "algorithm", "!=", "\"Residual\"", "and", "args", ".", "algorithm", "!=", "\"CDQN\"", ":", "\n", "        ", "currentTask", "=", "\"DQN\"", "\n", "args", ".", "currentTask", "=", "currentTask", "\n", "", "else", ":", "\n", "        ", "currentTask", "=", "args", ".", "algorithm", "\n", "args", ".", "currentTask", "=", "args", ".", "algorithm", "\n", "\n", "# prepare the optimizer", "\n", "", "lr", "=", "args", ".", "lr", "\n", "beta1", "=", "args", ".", "beta1", "\n", "beta2", "=", "args", ".", "beta2", "\n", "parameters", "=", "current_model", ".", "parameters", "\n", "args", ".", "optim", "=", "args", ".", "optim", ".", "lower", "(", ")", "\n", "if", "args", ".", "optim", "==", "'sgd'", ":", "\n", "        ", "optimizer", "=", "optim", ".", "SGD", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "momentum", "=", "beta1", ")", "\n", "", "elif", "args", ".", "optim", "==", "'adam'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "AdamW", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", ".", "startswith", "(", "\"adamb\"", ")", ":", "\n", "        ", "args", ".", "optim", "=", "\"adambelief\"", "\n", "optimizer", "=", "optimizers", ".", "AdamBelief", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "elif", "args", ".", "optim", "==", "'laprop'", ":", "\n", "        ", "optimizer", "=", "optimizers", ".", "LaProp", "(", "parameters", "(", ")", ",", "lr", "=", "lr", ",", "eps", "=", "args", ".", "adam_eps", ",", "betas", "=", "(", "beta1", ",", "beta2", ")", ",", "amsgrad", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "assert", "False", ",", "\"The specified optimizer name {} is non-existent\"", ".", "format", "(", "args", ".", "optim", ")", "\n", "\n", "", "print", "(", "currentTask", ")", "\n", "\n", "\n", "reward_list", ",", "length_list", ",", "loss_list", ",", "off_policy_rate_list", ",", "gen_loss_list", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "clip_reward", "=", "True", "###", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "# the number of parallelized computation is maximally \"arg.train_freq\" to guarantee that the computation order is still consistent with the original method", "\n", "num_task", "=", "args", ".", "train_freq", "\n", "args", ".", "num_task", "=", "num_task", "\n", "episode_rewards", "=", "[", "0.", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "life_lengths", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "envs", "=", "[", "env", "]", "\n", "for", "_i", "in", "range", "(", "num_task", "-", "1", ")", ":", "envs", ".", "append", "(", "copy", ".", "deepcopy", "(", "env", ")", ")", "\n", "for", "env", "in", "envs", ":", "env", ".", "seed", "(", "random", ".", "randrange", "(", "1000000", ")", ")", "\n", "states", "=", "[", "state", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "rewards", "=", "[", "0", "for", "_i", "in", "range", "(", "num_task", ")", "]", "\n", "\n", "evaluation_interval", "=", "args", ".", "evaluation_interval", "\n", "data_to_store", "=", "[", "]", "\n", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "prev_step", "=", "0", "\n", "step_idx", "=", "1", "# initialization of step_idx", "\n", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "if", "args", ".", "save_best", ":", "\n", "        ", "recent_performances", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "40", ")", "\n", "recent_models", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "20", ")", "\n", "best_performance", "=", "-", "float", "(", "\"inf\"", ")", "\n", "", "while", "step_idx", "<=", "args", ".", "max_steps", ":", "\n", "# decide the action", "\n", "        ", "epsilons", "=", "(", "epsilon_by_frame", "(", "idx", ")", "for", "idx", "in", "range", "(", "step_idx", ",", "step_idx", "+", "num_task", ")", ")", "if", "step_idx", ">", "args", ".", "learning_start", "else", "(", "1.", "for", "idx", "in", "range", "(", "num_task", ")", ")", "\n", "\n", "tensored_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "state", ".", "_frames", "for", "state", "in", "states", "]", ")", ".", "reshape", "(", "(", "num_task", ",", "-", "1", ")", "+", "image_shape", ")", ")", ".", "to", "(", "args", ".", "device", ")", ".", "float", "(", ")", ".", "div_", "(", "255.", ")", "\n", "actions", ",", "evaluateds", ",", "(", "Qss", ",", "bestActions", ")", "=", "current_model", ".", "act", "(", "tensored_states", ",", "epsilons", ")", "\n", "\n", "for", "_i", ",", "(", "env", ",", "state", ",", "action", ",", "Qs", ",", "bestAction", ",", "reward", ")", "in", "enumerate", "(", "zip", "(", "envs", ",", "states", ",", "actions", ",", "Qss", ",", "bestActions", ",", "rewards", ")", ")", ":", "\n", "\n", "# the environment proceeds by one step (4 frames)", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "if", "clip_reward", ":", "\n", "                ", "raw_reward", ",", "reward", "=", "reward", "\n", "", "else", ":", "\n", "                ", "raw_reward", "=", "reward", "\n", "\n", "", "rewards", "[", "_i", "]", "=", "float", "(", "reward", ")", "\n", "\n", "episode_rewards", "[", "_i", "]", "+=", "raw_reward", "\n", "life_lengths", "[", "_i", "]", "+=", "1", "\n", "\n", "# store the transition into the memory replay", "\n", "if", "not", "args", ".", "randomly_discard_experience", "or", "(", "args", ".", "randomly_discard_experience", "and", "random", ".", "random", "(", ")", ">=", "0.5", ")", ":", "# the data may be randomly discarded", "\n", "                ", "data_to_store", ".", "append", "(", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "float", "(", "done", ")", ")", ")", "\n", "", "if", "data_to_store", ":", "\n", "                ", "for", "data", "in", "data_to_store", ":", "\n", "                    ", "replay_buffer", ".", "add", "(", "*", "data", ")", "\n", "if", "args", ".", "randomly_replace_memory", "and", "len", "(", "replay_buffer", ")", ">=", "args", ".", "buffer_size", ":", "\n", "# probably randomly choose an index to replace ", "\n", "                        ", "replay_buffer", ".", "_next_idx", "=", "random", ".", "randrange", "(", "args", ".", "buffer_size", ")", "\n", "", "", "data_to_store", ".", "clear", "(", ")", "\n", "\n", "# record the performance of a trajectory", "\n", "", "if", "done", ":", "\n", "                ", "length_list", ".", "append", "(", "life_lengths", "[", "_i", "]", ")", "\n", "life_lengths", "[", "_i", "]", "=", "0", "\n", "# only the reward of a real full episode is recorded ", "\n", "if", "env", ".", "unwrapped", ".", "ale", ".", "game_over", "(", ")", "or", "\"TimeLimit.truncated\"", "in", "info", ":", "\n", "                    ", "reward_list", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "if", "not", "args", ".", "silent", ":", "\n", "                        ", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "env", ")", ":", "os", ".", "mkdir", "(", "args", ".", "env", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.txt'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ",", "'a'", ")", "as", "f", ":", "\n", "                            ", "f", ".", "write", "(", "'{:.0f}\\t{}\\n'", ".", "format", "(", "step_idx", "*", "4", ",", "episode_rewards", "[", "_i", "]", ")", ")", "\n", "", "if", "args", ".", "save_best", "and", "step_idx", ">", "args", ".", "learning_start", ":", "\n", "                            ", "recent_performances", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", "\n", "mean_performance", "=", "np", ".", "mean", "(", "recent_performances", ")", "\n", "if", "best_performance", "<", "mean_performance", "and", "len", "(", "recent_performances", ")", ">=", "40", ":", "\n", "                                ", "assert", "len", "(", "recent_models", ")", "==", "20", "\n", "best_performance", "=", "mean_performance", "\n", "torch", ".", "save", "(", "(", "recent_models", "[", "0", "]", ",", "step_idx", ")", ",", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}_{}.pth'", ".", "format", "(", "currentTask", ",", "args", ".", "comment", ")", ")", ")", "\n", "", "recent_models", ".", "append", "(", "current_model", ".", "state_dict", "(", ")", ".", "copy", "(", ")", ")", "\n", "", "", "episode_rewards", "[", "_i", "]", "=", "0.", "\n", "", "states", "[", "_i", "]", "=", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "[", "_i", "]", "=", "next_state", "\n", "\n", "# optimize", "\n", "", "if", "step_idx", "%", "args", ".", "train_freq", "==", "0", "and", "step_idx", ">", "max", "(", "args", ".", "learning_start", ",", "2", "*", "args", ".", "batch_size", ")", ":", "\n", "                ", "beta", "=", "beta_by_frame", "(", "step_idx", ")", "\n", "loss", ",", "off_policy_rate", "=", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", ")", "\n", "loss_list", ".", "append", "(", "loss", ")", ";", "off_policy_rate_list", ".", "append", "(", "off_policy_rate", ")", "\n", "\n", "# update the target network", "\n", "", "if", "step_idx", "%", "args", ".", "update_target", "==", "0", "and", "currentTask", "!=", "\"Residual\"", ":", "\n", "# we defer the update of the target network to the optimization routine to ensure that the target network is not exactly equal to current network", "\n", "                ", "args", ".", "do_update_target", "=", "True", "\n", "#update_target(current_model, target_model)", "\n", "\n", "# print the statistics", "\n", "", "if", "step_idx", "%", "evaluation_interval", "==", "0", ":", "\n", "# it works only if there is at least one episode to report; otherwise \"evaluation_interval\" is increased", "\n", "                ", "if", "len", "(", "reward_list", ")", ">", "0", ":", "\n", "                    ", "kwargs", "=", "{", "}", "\n", "kwargs", "[", "\"Off-Policy\"", "]", "=", "off_policy_rate_list", "\n", "print_log", "(", "step_idx", ",", "prev_step", ",", "prev_time", ",", "reward_list", ",", "length_list", ",", "loss_list", ",", "args", ",", "'{}{:.0e}{}'", ".", "format", "(", "currentTask", ",", "args", ".", "lr", ",", "args", ".", "comment", ")", ",", "**", "kwargs", ")", "\n", "reward_list", ".", "clear", "(", ")", ";", "length_list", ".", "clear", "(", ")", ";", "loss_list", ".", "clear", "(", ")", "\n", "for", "v", "in", "kwargs", ".", "values", "(", ")", ":", "\n", "                        ", "if", "type", "(", "v", ")", "==", "list", ":", "v", ".", "clear", "(", ")", "\n", "", "prev_step", "=", "step_idx", "\n", "prev_time", "=", "time", ".", "time", "(", ")", "\n", "", "else", ":", "\n", "                    ", "evaluation_interval", "+=", "args", ".", "evaluation_interval", "\n", "\n", "", "", "step_idx", "+=", "1", "\n", "\n", "", "", "", "i_count", "=", "0", "\n", "accu1", ",", "accu2", "=", "0.", ",", "0.", "\n", "accu_loss", "=", "0.", "\n", "def", "compute_td_loss", "(", "current_model", ",", "target_model", ",", "replay_buffer", ",", "optimizer", ",", "args", ",", "beta", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Calculate loss and optimize\n    \"\"\"", "\n", "global", "i_count", ",", "accu1", ",", "accu2", ",", "accu_loss", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.train.compute_td_loss": [[193, 385], ["torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "optimizer.zero_grad", "optimizer.step", "numpy.mean", "math.ceil", "replay_buffer.sample", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "replay_buffer.sample", "torch.ones", "torch.ones", "torch.ones", "weights.to.numpy", "weights.to.to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.smooth_l1_loss", "F.smooth_l1_loss.backward", "list", "torch.from_numpy().to().float().div_.view", "current_model", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "list", "torch.from_numpy().to().float().div_.view", "current_model", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.zeros_like.scatter_", "current_model.backward", "replay_buffer.update_priorities", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "common.utils.update_target", "print", "done.astype", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model", "current_model.gather().squeeze", "model.inverse_transform_Q", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.addcmul", "torch.addcmul", "torch.addcmul", "current_model", "current_model.gather().squeeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.from_numpy().to().float().div_.size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "current_model.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "target_model", "numpy.where", "target_mask.astype.astype", "numpy.clip", "model.transform_backpropagate", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "torch.from_numpy().to().float().div_.size", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model.gather().squeeze", "current_model.max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "numpy.clip", "model.transform_backpropagate", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "numpy.dot", "current_model.parameters", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.no_grad", "torch.no_grad", "torch.no_grad", "current_model", "[].unsqueeze", "target_model", "target_model.gather().squeeze", "next_q_action.squeeze.squeeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "target_model().max", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "torch.stack().cpu().numpy", "numpy.abs", "target_model.gather().squeeze().cpu().numpy", "numpy.max", "model.inverse_transform_Q", "model.transform_Q", "numpy.abs", "numpy.abs", "numpy.where", "numpy.abs", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "model.inverse_transform_Q", "model.transform_Q", "numpy.abs", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "math.sqrt", "open", "f.write", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "numpy.stack", "current_model.gather", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model.gather", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "target_model.cpu().numpy", "numpy.stack", "numpy.stack", "torch.cat", "torch.cat", "torch.cat", "current_model.gather", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "numpy.stack", "torch.cat", "torch.cat", "torch.cat", "target_model.gather", "target_model", "torch.from_numpy().to.unsqueeze", "torch.stack().cpu", "torch.stack().cpu", "torch.stack().cpu", "model.transform_Q", "torch.from_numpy().to.unsqueeze", "torch.from_numpy().to.unsqueeze", "target_model.gather().squeeze().cpu", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy().to.unsqueeze", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "numpy.argmax", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "current_model.max", "torch.stack", "torch.stack", "torch.stack", "target_model.cpu", "numpy.concatenate", "torch.stack", "torch.stack", "torch.stack", "numpy.concatenate", "current_model.detach().cpu().numpy", "torch.stack", "torch.stack", "torch.stack", "torch.mse_loss", "target_model.gather().squeeze", "q_values.gather().squeeze.detach", "current_model.detach().cpu", "q_values.gather().squeeze.detach", "target_model.gather", "next_q_action.squeeze.unsqueeze", "current_model.detach"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.sample", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.PrioritizedReplayBuffer.update_priorities", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.update_target", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_backpropagate", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_backpropagate", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["        ", "state_next_state", ",", "action_", ",", "reward_", ",", "done", ",", "weights_", ",", "true_weights", ",", "indices", "=", "replay_buffer", ".", "sample", "(", "args", ".", "batch_size", ",", "beta", ")", "\n", "weights", "=", "torch", ".", "from_numpy", "(", "weights_", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "", "else", ":", "\n", "        ", "state_next_state", ",", "action_", ",", "reward_", ",", "done", ",", "indices", "=", "replay_buffer", ".", "sample", "(", "args", ".", "batch_size", ")", "\n", "weights", "=", "torch", ".", "ones", "(", "args", ".", "batch_size", ")", ";", "weights_", "=", "weights", ".", "numpy", "(", ")", ";", "true_weights", "=", "weights_", "\n", "weights", "=", "weights", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "\n", "# we move data to GPU in chunks", "\n", "", "state_next_state", "=", "torch", ".", "from_numpy", "(", "state_next_state", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", ".", "float", "(", ")", ".", "div_", "(", "255", ")", "\n", "state", ",", "next_state", "=", "state_next_state", "\n", "action", "=", "torch", ".", "from_numpy", "(", "action_", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "gamma_mul_one_minus_done_", "=", "(", "args", ".", "gamma", "*", "(", "1.", "-", "done", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "if", "args", ".", "currentTask", "==", "\"DQN\"", ":", "\n", "# in some cases these data do not really need to be copied to GPU ", "\n", "        ", "reward", ",", "gamma_mul_one_minus_done", "=", "torch", ".", "from_numpy", "(", "np", ".", "stack", "(", "(", "reward_", ",", "gamma_mul_one_minus_done_", ")", ")", ")", ".", "to", "(", "args", ".", "device", ",", "non_blocking", "=", "True", ")", "\n", "##### start training ##### ", "\n", "", "optimizer", ".", "zero_grad", "(", ")", "\n", "# we use \"values\" to refer to Q values for all state-actions, and use \"value\" to refer to Q values for states", "\n", "if", "args", ".", "currentTask", "==", "\"DQN\"", ":", "\n", "        ", "if", "args", ".", "double", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q_values", "=", "current_model", "(", "next_state", ")", "\n", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "[", "1", "]", ".", "unsqueeze", "(", "1", ")", "# **unsqueeze", "\n", "target_next_q_values", "=", "target_model", "(", "next_state", ")", "\n", "next_q_value", "=", "target_next_q_values", ".", "gather", "(", "1", ",", "next_q_action", ")", ".", "squeeze", "(", ")", "\n", "next_q_action", "=", "next_q_action", ".", "squeeze", "(", ")", "\n", "", "", "else", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "next_q_value", ",", "next_q_action", "=", "target_model", "(", "next_state", ")", ".", "max", "(", "1", ")", "\n", "\n", "", "", "expected_q_value", "=", "torch", ".", "addcmul", "(", "reward", ",", "tensor1", "=", "next_q_value", ",", "tensor2", "=", "gamma_mul_one_minus_done", ")", "\n", "q_values", "=", "current_model", "(", "state", ")", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "\n", "loss", "=", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ",", "reduction", "=", "'none'", ")", "\n", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "diff", "=", "(", "q_value", ".", "detach", "(", ")", "-", "expected_q_value", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "prios", "=", "np", ".", "abs", "(", "diff", ")", "+", "args", ".", "prio_eps", "#", "\n", "", "loss", "=", "(", "loss", "*", "weights", ")", ".", "mean", "(", ")", "/", "2.", "\n", "loss", ".", "backward", "(", ")", "\n", "\n", "# we report the mean squared error instead of the Huber loss as the loss", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "report_loss", "=", "(", "F", ".", "mse_loss", "(", "q_value", ",", "expected_q_value", ",", "reduction", "=", "'none'", ")", "*", "weights", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "", "", "if", "args", ".", "currentTask", "==", "\"CDQN\"", ":", "\n", "# compute the current and next state values in a single pass ", "\n", "        ", "size", "=", "list", "(", "state_next_state", ".", "size", "(", ")", ")", "\n", "current_and_next_states", "=", "state_next_state", ".", "view", "(", "[", "-", "1", "]", "+", "size", "[", "2", ":", "]", ")", "\n", "# compute the q values and the gradient", "\n", "all_q_values", "=", "current_model", "(", "current_and_next_states", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_values", ",", "next_q_values", "=", "all_q_values", "[", ":", "args", ".", "batch_size", "]", ",", "all_q_values", "[", "args", ".", "batch_size", ":", "2", "*", "args", ".", "batch_size", "]", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", ",", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "\n", "q_value", ",", "next_q_value", "=", "torch", ".", "stack", "(", "(", "q_value", ",", "next_q_value", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "next_q_values_target", "=", "target_model", "(", "next_state", ")", "\n", "if", "args", ".", "double", ":", "\n", "                ", "next_q_value_target", "=", "next_q_values_target", ".", "gather", "(", "1", ",", "next_q_action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "                ", "next_q_value_target", "=", "np", ".", "max", "(", "next_q_values_target", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", "\n", "\n", "", "expected_q_value_self", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value", "\n", "expected_q_value_target", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value_target", "\n", "target_mask", "=", "(", "np", ".", "abs", "(", "q_value", "-", "expected_q_value_target", ")", ">=", "np", ".", "abs", "(", "q_value", "-", "expected_q_value_self", ")", ")", "\n", "expected_q_value", "=", "np", ".", "where", "(", "target_mask", ",", "expected_q_value_target", ",", "expected_q_value_self", ")", "\n", "target_mask", "=", "target_mask", ".", "astype", "(", "np", ".", "float32", ")", "\n", "\n", "", "diff", "=", "q_value", "-", "expected_q_value", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "prio_diff", "=", "diff", "\n", "prios", "=", "np", ".", "abs", "(", "prio_diff", ")", "+", "args", ".", "prio_eps", "\n", "# the Huber loss is used ", "\n", "", "weighted_diff", "=", "weights_", "*", "diff", "\n", "q_value_grad", "=", "1.", "/", "args", ".", "batch_size", "*", "weighted_diff", "\n", "\n", "all_grads", "=", "torch", ".", "zeros_like", "(", "all_q_values", ")", "\n", "# manually backpropagate the gradient through the term \"expected_q_value\" ", "\n", "next_q_value_grad", "=", "-", "(", "1.", "-", "target_mask", ")", "*", "q_value_grad", "\n", "next_q_value_grad", "=", "next_q_value_grad", "*", "gamma_mul_one_minus_done_", "\n", "grads", "=", "torch", ".", "from_numpy", "(", "np", ".", "concatenate", "(", "[", "q_value_grad", ",", "next_q_value_grad", "]", ",", "axis", "=", "0", ")", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "all_grads", ".", "scatter_", "(", "1", ",", "torch", ".", "cat", "(", "[", "action", ",", "next_q_action", "]", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "1", ")", ",", "grads", ")", "\n", "all_q_values", ".", "backward", "(", "all_grads", ")", "# this method makes it run faster ", "\n", "\n", "report_loss", "=", "np", ".", "dot", "(", "diff", ",", "weights_", "*", "diff", ")", "/", "args", ".", "batch_size", "\n", "\n", "", "if", "args", ".", "currentTask", "==", "\"Residual\"", ":", "\n", "# compute the current and next state values in a single pass ", "\n", "        ", "size", "=", "list", "(", "state_next_state", ".", "size", "(", ")", ")", "\n", "current_and_next_states", "=", "state_next_state", ".", "view", "(", "[", "-", "1", "]", "+", "size", "[", "2", ":", "]", ")", "\n", "# compute the q values and the gradient", "\n", "all_q_values", "=", "current_model", "(", "current_and_next_states", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_values", ",", "next_q_values", "=", "all_q_values", "[", ":", "args", ".", "batch_size", "]", ",", "all_q_values", "[", "args", ".", "batch_size", ":", "2", "*", "args", ".", "batch_size", "]", "\n", "q_value", "=", "q_values", ".", "gather", "(", "1", ",", "action", ".", "unsqueeze", "(", "1", ")", ")", ".", "squeeze", "(", "1", ")", "\n", "next_q_value", ",", "next_q_action", "=", "next_q_values", ".", "max", "(", "1", ")", "\n", "q_value", ",", "next_q_value", "=", "torch", ".", "stack", "(", "(", "q_value", ",", "next_q_value", ")", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "expected_q_value", "=", "reward_", "+", "gamma_mul_one_minus_done_", "*", "next_q_value", "\n", "\n", "# then compute the q values and the loss", "\n", "", "diff", "=", "q_value", "-", "expected_q_value", "\n", "if", "args", ".", "prioritized_replay", ":", "\n", "            ", "prio_diff", "=", "diff", "\n", "prios", "=", "np", ".", "abs", "(", "prio_diff", ")", "+", "args", ".", "prio_eps", "\n", "# the Huber loss is used ", "\n", "", "weighted_diff", "=", "weights_", "*", "diff", "\n", "q_value_grad", "=", "1.", "/", "args", ".", "batch_size", "*", "weighted_diff", "\n", "\n", "all_grads", "=", "torch", ".", "zeros_like", "(", "all_q_values", ")", "\n", "# manually backpropagate the gradient through the term \"expected_q_value\" ", "\n", "next_q_value_grad", "=", "-", "q_value_grad", "\n", "next_q_value_grad", "=", "next_q_value_grad", "*", "gamma_mul_one_minus_done_", "\n", "grads", "=", "torch", ".", "from_numpy", "(", "np", ".", "concatenate", "(", "[", "q_value_grad", ",", "next_q_value_grad", "]", ",", "axis", "=", "0", ")", ")", ".", "unsqueeze", "(", "1", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "all_grads", ".", "scatter_", "(", "1", ",", "torch", ".", "cat", "(", "[", "action", ",", "next_q_action", "]", ",", "dim", "=", "0", ")", ".", "unsqueeze", "(", "1", ")", ",", "grads", ")", "\n", "all_q_values", ".", "backward", "(", "all_grads", ")", "# this method makes it run faster ", "\n", "\n", "report_loss", "=", "np", ".", "dot", "(", "diff", ",", "weights_", "*", "diff", ")", "/", "args", ".", "batch_size", "\n", "\n", "", "if", "args", ".", "prioritized_replay", ":", "\n", "        ", "replay_buffer", ".", "update_priorities", "(", "indices", ",", "prios", ")", "\n", "# gradient clipping ", "\n", "", "if", "args", ".", "grad_clip", ">", "0.", ":", "\n", "        ", "grad_norm", "=", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "current_model", ".", "parameters", "(", ")", ",", "max_norm", "=", "args", ".", "grad_clip", ")", "\n", "accu1", "+=", "grad_norm", "\n", "accu2", "+=", "grad_norm", "**", "2", "\n", "", "if", "args", ".", "do_update_target", ":", "update_target", "(", "current_model", ",", "target_model", ")", ";", "args", ".", "do_update_target", "=", "False", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "off_policy_rate", "=", "np", ".", "mean", "(", "(", "np", ".", "argmax", "(", "q_values", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "axis", "=", "1", ")", "!=", "action_", ")", ".", "astype", "(", "np", ".", "float", ")", "*", "true_weights", ")", "\n", "\n", "i_count", "+=", "1", "\n", "accu_loss", "+=", "report_loss", "\n", "report_period", "=", "math", ".", "ceil", "(", "args", ".", "evaluation_interval", "/", "args", ".", "train_freq", ")", "\n", "if", "i_count", "%", "report_period", "==", "0", "and", "accu1", "!=", "0.", ":", "\n", "        ", "print", "(", "\"gradient norm {:.3f} +- {:.3f}\"", ".", "format", "(", "accu1", "/", "report_period", ",", "math", ".", "sqrt", "(", "accu2", "/", "report_period", "-", "(", "accu1", "/", "report_period", ")", "**", "2", ")", ")", ")", "\n", "accu1", ",", "accu2", "=", "0.", ",", "0.", "\n", "if", "not", "args", ".", "silent", ":", "\n", "            ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}mse_{}.txt'", ".", "format", "(", "args", ".", "currentTask", ",", "args", ".", "comment", ")", ")", ",", "'a'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "'{:.0f}\\t{}\\n'", ".", "format", "(", "(", "i_count", "*", "args", ".", "train_freq", "+", "args", ".", "learning_start", ")", "*", "4", ",", "accu_loss", "/", "report_period", ")", ")", "\n", "", "", "accu_loss", "=", "0.", "\n", "\n", "", "return", "report_loss", ",", "off_policy_rate", "\n", "\n", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test": [[10, 30], ["model.DQN().to", "common.utils.epsilon_scheduler", "torch.load", "torch.load", "print", "model_dict.pop", "DQN().to.load_state_dict", "test.test_whole", "results.append", "len", "model.DQN", "os.path.join", "common.utils.epsilon_scheduler.", "open", "data_f.write", "open", "data_f.write", "print", "numpy.mean", "numpy.mean", "numpy.std", "math.sqrt", "numpy.std", "math.sqrt", "len", "len"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.utils.epsilon_scheduler", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test_whole", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN"], ["def", "test", "(", "env", ",", "args", ")", ":", "\n", "    ", "current_model", "=", "DQN", "(", "env", ",", "args", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "epsilon_by_frame", "=", "epsilon_scheduler", "(", "args", ".", "eps_start", ",", "args", ".", "eps_mid", ",", "args", ".", "eps_final", ")", "\n", "#current_model.eval()", "\n", "\n", "results", "=", "[", "]", "\n", "for", "filename", "in", "args", ".", "load_model", ":", "\n", "        ", "model_dict", ",", "step_idx", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "args", ".", "env", ",", "'{}.pth'", ".", "format", "(", "filename", ")", ")", ",", "map_location", "=", "args", ".", "device", ")", "\n", "print", "(", "\"load {} at training step {}\"", ".", "format", "(", "filename", ",", "step_idx", ")", ")", "\n", "model_dict", ".", "pop", "(", "\"scale\"", ",", "None", ")", "\n", "current_model", ".", "load_state_dict", "(", "model_dict", ")", "\n", "\n", "mean", ",", "std", "=", "test_whole", "(", "current_model", ",", "env", ",", "args", ",", "filename", ",", "epsilon_by_frame", "(", "step_idx", ")", ",", "num_trial", "=", "args", ".", "num_trial", ")", "#0.01)", "\n", "results", ".", "append", "(", "mean", ")", "\n", "with", "open", "(", "'{}.txt'", ".", "format", "(", "args", ".", "env", ")", ",", "'a'", ")", "as", "data_f", ":", "\n", "            ", "data_f", ".", "write", "(", "'{}: {} +- {}\\n'", ".", "format", "(", "filename", ",", "mean", ",", "std", ")", ")", "\n", "", "", "if", "len", "(", "args", ".", "load_model", ")", ">", "1", ":", "\n", "        ", "with", "open", "(", "'{}.txt'", ".", "format", "(", "args", ".", "env", ")", ",", "'a'", ")", "as", "data_f", ":", "\n", "            ", "data_f", ".", "write", "(", "'{} +- {}\\n'", ".", "format", "(", "np", ".", "mean", "(", "results", ")", ",", "np", ".", "std", "(", "results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "results", ")", ")", ")", ")", "\n", "print", "(", "'{} +- {}'", ".", "format", "(", "np", ".", "mean", "(", "results", ")", ",", "np", ".", "std", "(", "results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "results", ")", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.test.test_whole": [[31, 79], ["min", "range", "len", "numpy.mean", "numpy.mean", "print", "envs.append", "env.seed", "env.reset", "len", "torch.from_numpy().to().float().div_", "torch.from_numpy().to().float().div_", "current_model.act", "enumerate", "reversed", "mark_remove.clear", "numpy.std", "math.sqrt", "copy.deepcopy", "random.randrange", "range", "range", "zip", "env.step", "envs.pop", "states.pop", "episode_lengths.pop", "episode_rewards.pop", "len", "range", "torch.from_numpy().to().float", "torch.from_numpy().to().float", "env.reset", "len", "env.unwrapped.ale.game_over", "reward_results.append", "length_results.append", "torch.from_numpy().to", "torch.from_numpy().to", "mark_remove.append", "torch.from_numpy", "torch.from_numpy", "numpy.array().reshape", "numpy.array", "len"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secA.1.wetchicken1d.WetChicken1dEnv.seed", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "", "", "def", "test_whole", "(", "current_model", ",", "env", ",", "args", ",", "idx", ",", "epsilon", ",", "num_parallel", "=", "16", ",", "num_trial", "=", "400", ")", ":", "#16", "\n", "    ", "image_shape", "=", "env", ".", "observation_space", ".", "shape", "[", "1", ":", "]", "\n", "envs", "=", "[", "env", "]", "\n", "num_parallel", "=", "min", "(", "num_parallel", ",", "num_trial", ")", "\n", "for", "_i", "in", "range", "(", "num_parallel", "-", "1", ")", ":", "envs", ".", "append", "(", "copy", ".", "deepcopy", "(", "env", ")", ")", "\n", "for", "env", "in", "envs", ":", "env", ".", "seed", "(", "random", ".", "randrange", "(", "1000000", ")", ")", "\n", "states", "=", "[", "env", ".", "reset", "(", ")", "for", "env", "in", "envs", "]", "\n", "episode_rewards", "=", "[", "0.", "for", "_i", "in", "range", "(", "num_parallel", ")", "]", "\n", "episode_lengths", "=", "[", "0", "for", "_i", "in", "range", "(", "num_parallel", ")", "]", "\n", "reward_results", "=", "[", "]", "\n", "length_results", "=", "[", "]", "\n", "\n", "trial", "=", "len", "(", "states", ")", "\n", "mark_remove", "=", "[", "]", "\n", "while", "len", "(", "envs", ")", ">", "0", ":", "\n", "# decide the action", "\n", "        ", "epsilons", "=", "(", "epsilon", "for", "_", "in", "range", "(", "len", "(", "states", ")", ")", ")", "\n", "\n", "tensored_states", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "[", "state", ".", "_frames", "for", "state", "in", "states", "]", ")", ".", "reshape", "(", "(", "len", "(", "states", ")", ",", "-", "1", ")", "+", "image_shape", ")", ")", ".", "to", "(", "args", ".", "device", ")", ".", "float", "(", ")", ".", "div_", "(", "255.", ")", "\n", "actions", ",", "evaluateds", ",", "(", "Qss", ",", "bestActions", ")", "=", "current_model", ".", "act", "(", "tensored_states", ",", "epsilons", ")", "\n", "for", "_i", ",", "(", "env", ",", "action", ")", "in", "enumerate", "(", "zip", "(", "envs", ",", "actions", ")", ")", ":", "\n", "\n", "# the environment proceeds by one step (4 frames)", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "episode_rewards", "[", "_i", "]", "+=", "reward", "\n", "episode_lengths", "[", "_i", "]", "+=", "1", "\n", "\n", "if", "done", ":", "\n", "                ", "if", "env", ".", "unwrapped", ".", "ale", ".", "game_over", "(", ")", "or", "\"TimeLimit.truncated\"", "in", "info", ":", "\n", "                    ", "reward_results", ".", "append", "(", "episode_rewards", "[", "_i", "]", ")", ";", "length_results", ".", "append", "(", "episode_lengths", "[", "_i", "]", ")", "\n", "episode_rewards", "[", "_i", "]", "=", "0.", ";", "episode_lengths", "[", "_i", "]", "=", "0", "\n", "if", "trial", "<", "num_trial", ":", "\n", "                        ", "trial", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "mark_remove", ".", "append", "(", "_i", ")", "\n", "", "", "states", "[", "_i", "]", "=", "env", ".", "reset", "(", ")", "\n", "", "else", ":", "\n", "                ", "states", "[", "_i", "]", "=", "next_state", "\n", "", "", "for", "_i", "in", "reversed", "(", "mark_remove", ")", ":", "\n", "            ", "envs", ".", "pop", "(", "_i", ")", ";", "states", ".", "pop", "(", "_i", ")", ";", "episode_lengths", ".", "pop", "(", "_i", ")", ";", "episode_rewards", ".", "pop", "(", "_i", ")", "\n", "", "mark_remove", ".", "clear", "(", ")", "\n", "\n", "", "mean_reward", "=", "np", ".", "mean", "(", "reward_results", ")", "\n", "std_reward", "=", "np", ".", "std", "(", "reward_results", ",", "ddof", "=", "1", ")", "/", "math", ".", "sqrt", "(", "len", "(", "reward_results", ")", ")", "\n", "mean_length", "=", "np", ".", "mean", "(", "length_results", ")", "\n", "print", "(", "\"Test Result - Reward {:.2f}+-{:.2f} Length {:.1f} for {}\"", ".", "format", "(", "mean_reward", ",", "std_reward", ",", "mean_length", ",", "idx", ")", ")", "\n", "return", "mean_reward", ",", "std_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.__init__": [[30, 57], ["torch.Module.__init__", "model.Flatten", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.DQNBase.modules", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DQNBase.Linear", "type", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "module.bias.data.zero_", "model.DQNBase._feature_size"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size"], ["        ", "super", "(", "DQNBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "input_shape", "=", "env", ".", "observation_space", ".", "shape", "\n", "self", ".", "num_actions", "=", "env", ".", "action_space", ".", "n", "\n", "self", ".", "Linear", "=", "Linear", "# We have overridden the \"reset_parameters\" method for a more well-principled initialization", "\n", "\n", "self", ".", "flatten", "=", "Flatten", "(", ")", "\n", "self", ".", "features", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "self", ".", "input_shape", "[", "0", "]", ",", "32", ",", "kernel_size", "=", "8", ",", "stride", "=", "4", ",", "padding", "=", "0", "if", "self", ".", "input_shape", "[", "1", "]", "!=", "105", "else", "2", ")", ",", "\n", "#nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0 if self.input_shape[1]!=105 else 2),", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "32", ",", "64", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Conv2d", "(", "64", ",", "64", ",", "kernel_size", "=", "3", ",", "stride", "=", "1", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", "\n", ")", "\n", "\n", "for", "module", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "type", "(", "module", ")", "==", "nn", ".", "Conv2d", ":", "init", ".", "kaiming_uniform_", "(", "module", ".", "weight", ".", "data", ",", "nonlinearity", "=", "\"relu\"", ",", "a", "=", "0.", ")", ";", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", ",", "self", ".", "num_actions", ")", "\n", ")", "\n", "self", ".", "out_bias", "=", "self", ".", "fc", "[", "-", "1", "]", ".", "bias", "\n", "\n", "", "def", "forward", "(", "self", ",", "x", ",", "**", "kwargs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.forward": [[58, 63], ["model.DQNBase.features", "model.DQNBase.flatten", "model.DQNBase.fc"], "methods", ["None"], ["        ", "x", "=", "self", ".", "features", "(", "x", ")", "\n", "x", "=", "self", ".", "flatten", "(", "x", ")", "\n", "x", "=", "self", ".", "fc", "(", "x", ")", "\n", "return", "x", "\n", "\n", "", "def", "_feature_size", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size": [[64, 66], ["model.DQNBase.features().view().size", "model.DQNBase.features().view", "model.DQNBase.features", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["        ", "return", "self", ".", "features", "(", "torch", ".", "zeros", "(", "1", ",", "*", "self", ".", "input_shape", ")", ")", ".", "view", "(", "1", ",", "-", "1", ")", ".", "size", "(", "1", ")", "\n", "\n", "", "def", "act", "(", "self", ",", "state", ",", "epsilon", ",", "**", "kwargs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase.act": [[67, 93], ["state.unsqueeze.unsqueeze.dim", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "state.unsqueeze.unsqueeze.unsqueeze", "model.DQNBase.forward().cpu().numpy().squeeze", "numpy.argmax", "random.random", "random.randrange", "state.unsqueeze.unsqueeze.dim", "enumerate", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.DQNBase.forward().cpu().numpy", "numpy.argmax", "numpy.copy", "state.unsqueeze.unsqueeze.size", "model.DQNBase.forward().cpu().numpy", "random.random", "random.randrange", "model.DQNBase.forward().cpu", "model.DQNBase.forward().cpu", "model.DQNBase.forward", "model.DQNBase.forward"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward"], ["        ", "\"\"\"\n        Parameters\n        ----------\n        state       torch.Tensor with appropritate device type\n        epsilon     epsilon for epsilon-greedy\n        \"\"\"", "\n", "if", "state", ".", "dim", "(", ")", "==", "3", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "state", "=", "state", ".", "unsqueeze", "(", "0", ")", "\n", "q_values", "=", "self", ".", "forward", "(", "state", ",", "**", "kwargs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", "\n", "bestAction", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "0", ")", "\n", "", "if", "random", ".", "random", "(", ")", ">=", "epsilon", ":", "\n", "                ", "action", "=", "bestAction", "\n", "", "else", ":", "\n", "                ", "action", "=", "random", ".", "randrange", "(", "self", ".", "num_actions", ")", "\n", "", "", "elif", "state", ".", "dim", "(", ")", "==", "4", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "q_values", "=", "self", ".", "forward", "(", "state", ",", "**", "kwargs", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "bestAction", "=", "np", ".", "argmax", "(", "q_values", ",", "axis", "=", "1", ")", "\n", "action", "=", "np", ".", "copy", "(", "bestAction", ")", "\n", "", "for", "i", ",", "e", "in", "enumerate", "(", "epsilon", ")", ":", "\n", "                ", "if", "random", ".", "random", "(", ")", "<", "e", ":", "\n", "                    ", "action", "[", "i", "]", "=", "random", ".", "randrange", "(", "self", ".", "num_actions", ")", "\n", "", "", "", "else", ":", "assert", "False", ",", "\"The input state has an invalid shape {}\"", ".", "format", "(", "state", ".", "size", "(", ")", ")", "\n", "return", "action", ",", "action", "==", "bestAction", ",", "(", "q_values", ",", "bestAction", ")", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DuelingDQN.__init__": [[100, 128], ["model.DQNBase.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Sequential", "model.DuelingDQN.register_buffer", "model.DuelingDQN.fc[].weight.register_hook", "model.DuelingDQN.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DuelingDQN.Linear", "model.DuelingDQN.Linear", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReLU", "model.DuelingDQN.Linear", "model.DuelingOutput", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "zip", "model.DuelingDQN.fc[].weight.zero_", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "model.DuelingDQN._feature_size", "model.DuelingDQN._feature_size", "model.DuelingDQN.fc[].parameters", "model.DuelingDQN.advantage[].parameters", "model.DuelingDQN.value[].parameters"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQNBase._feature_size"], ["        ", "super", "(", "DuelingDQN", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "advantage", "=", "self", ".", "fc", "\n", "self", ".", "value", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", ",", "1", ")", "\n", ")", "\n", "self", ".", "fc", "=", "nn", ".", "Sequential", "(", "\n", "self", ".", "Linear", "(", "self", ".", "_feature_size", "(", ")", ",", "512", "*", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "self", ".", "Linear", "(", "512", "*", "2", ",", "self", ".", "num_actions", "+", "1", ")", ",", "\n", "DuelingOutput", "(", "self", ".", "num_actions", ")", "\n", ")", "\n", "# rewrite the parameters of \"self.advantage\" and \"self.value\" into \"self.fc\" so that they are combined into a single computation", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "p", ",", "p_a", ",", "p_v", "in", "zip", "(", "self", ".", "fc", "[", "0", "]", ".", "parameters", "(", ")", ",", "self", ".", "advantage", "[", "0", "]", ".", "parameters", "(", ")", ",", "self", ".", "value", "[", "0", "]", ".", "parameters", "(", ")", ")", ":", "\n", "                ", "p", "[", ":", "512", "]", "=", "p_a", ";", "p", "[", "512", ":", "512", "*", "2", "]", "=", "p_v", "\n", "", "self", ".", "fc", "[", "2", "]", ".", "weight", ".", "zero_", "(", ")", "\n", "self", ".", "fc", "[", "2", "]", ".", "weight", "[", ":", "self", ".", "num_actions", ",", ":", "512", "]", "=", "self", ".", "advantage", "[", "2", "]", ".", "weight", ";", "self", ".", "fc", "[", "2", "]", ".", "weight", "[", "-", "1", ",", "512", ":", "512", "*", "2", "]", "=", "self", ".", "value", "[", "2", "]", ".", "weight", "\n", "self", ".", "fc", "[", "2", "]", ".", "bias", "[", ":", "self", ".", "num_actions", "]", "=", "self", ".", "advantage", "[", "2", "]", ".", "bias", ";", "self", ".", "fc", "[", "2", "]", ".", "bias", "[", "-", "1", "]", "=", "self", ".", "value", "[", "2", "]", ".", "bias", "\n", "del", "self", ".", "value", ",", "self", ".", "advantage", "\n", "# mask the backpropagated gradient on \"self.fc[2].weight\"", "\n", "", "self", ".", "register_buffer", "(", "'grad_mask'", ",", "torch", ".", "zeros", "(", "self", ".", "num_actions", "+", "1", ",", "512", "*", "2", ")", ")", "\n", "self", ".", "grad_mask", "[", ":", "self", ".", "num_actions", ",", ":", "512", "]", "=", "1.", ";", "self", ".", "grad_mask", "[", "-", "1", ",", "512", ":", "512", "*", "2", "]", "=", "1.", "\n", "self", ".", "dueling_grad_hook", "=", "self", ".", "fc", "[", "2", "]", ".", "weight", ".", "register_hook", "(", "lambda", "grad", ":", "self", ".", "grad_mask", "*", "grad", ")", "\n", "\n", "self", ".", "out_bias", "=", "self", ".", "fc", "[", "-", "2", "]", ".", "bias", "\n", "\n", "", "", "class", "DuelingOutput", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DuelingOutput.__init__": [[130, 143], ["torch.Module.__init__", "model.DuelingOutput.register_buffer", "range", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["        ", "super", "(", "DuelingOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "register_buffer", "(", "'output_matrix'", ",", "torch", ".", "Tensor", "(", "num_actions", ",", "num_actions", "+", "1", ")", ")", "\n", "# set the \"-advantage.mean(1, keepdim=True)\" term", "\n", "self", ".", "output_matrix", "[", ":", ",", ":", "]", "=", "-", "1.", "/", "num_actions", "\n", "# set the last input dim, the average value, added to all Qs", "\n", "self", ".", "output_matrix", "[", ":", ",", "-", "1", "]", "=", "1.", "\n", "# set the diagonal term", "\n", "for", "i", "in", "range", "(", "num_actions", ")", ":", "\n", "            ", "self", ".", "output_matrix", "[", "i", ",", "i", "]", "=", "(", "num_actions", "-", "1", ")", "/", "num_actions", "\n", "# this complete the definition of \"output_matrix\", which computes \"value + (advantage - advantage.mean(1, keepdim=True)) * rescale \"", "\n", "", "assert", "not", "self", ".", "output_matrix", ".", "requires_grad", "\n", "\n", "", "def", "forward", "(", "self", ",", "input", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DuelingOutput.forward": [[144, 146], ["torch.linear", "torch.linear", "torch.linear", "torch.linear"], "methods", ["None"], ["        ", "return", "F", ".", "linear", "(", "input", ",", "self", ".", "output_matrix", ",", "None", ")", "\n", "\n", "", "", "class", "Flatten", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Flatten.forward": [[148, 150], ["x.view", "x.size"], "methods", ["None"], ["        ", "return", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n", "", "", "class", "Linear", "(", "nn", ".", "Linear", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.Linear.reset_parameters": [[152, 159], ["torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "torch.kaiming_uniform_", "model.Linear.bias.data.zero_"], "methods", ["None"], ["        ", "init", ".", "kaiming_uniform_", "(", "self", ".", "weight", ",", "nonlinearity", "=", "\"relu\"", ",", "a", "=", "0.", ")", "\n", "if", "self", ".", "bias", "is", "not", "None", ":", "\n", "#fan_in, _  = init._calculate_fan_in_and_fan_out(self.weight)", "\n", "#bound = 1./math.sqrt(fan_in)", "\n", "#init.uniform_(self.bias, -bound, bound)", "\n", "            ", "self", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.DQN": [[13, 19], ["model.DuelingDQN", "model.DQNBase"], "function", ["None"], ["    ", "if", "args", ".", "dueling", ":", "\n", "        ", "model", "=", "DuelingDQN", "(", "env", ")", "\n", "", "else", ":", "\n", "        ", "model", "=", "DQNBase", "(", "env", ")", "\n", "", "return", "model", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.inverse_transform_Q": [[163, 168], ["type", "Qs.sign", "type", "model.jit_inverse_transform_Q", "numpy.sign", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "math.sqrt", "Qs.abs", "abs"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.jit_inverse_transform_Q"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.jit_inverse_transform_Q": [[169, 172], ["numba.njit", "numpy.sign", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_Q": [[173, 178], ["type", "torch.sign", "torch.sign", "torch.sign", "torch.sign", "type", "model.jit_transform_Q", "numpy.sign", "torch.sqrt", "torch.sqrt", "torch.sqrt", "torch.sqrt", "math.sqrt", "Qs.abs", "abs"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.jit_transform_Q"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.jit_transform_Q": [[179, 182], ["numba.njit", "numpy.sign", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.d_transform_Q": [[184, 187], ["numba.njit", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.d_inverse_transform_Q": [[189, 192], ["numba.njit", "numpy.sqrt", "numpy.abs"], "function", ["None"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.transform_backpropagate": [[193, 199], ["numba.njit", "next_q_value_grad.astype", "model.d_transform_Q", "model.d_inverse_transform_Q"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.d_transform_Q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.d_inverse_transform_Q"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.model.auto_initialize": [[201, 220], ["common.heuristics.get_initialization_stat", "print", "max", "print"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.heuristics.get_initialization_stat", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], []], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.AdamW.__init__": [[21, 36], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamW", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.AdamW.__setstate__": [[37, 41], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.AdamW.step": [[42, 120], ["closure", "math.sqrt", "exp_avg.mul_().add_", "exp_avg_sq.mul_().addcmul_", "p.data.addcdiv_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.max", "max_exp_avg_sq.sqrt().add_", "exp_avg_sq.sqrt().add_", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_sq.mul_", "max_exp_avg_sq.sqrt", "exp_avg_sq.sqrt"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "lr", ",", "eps", ",", "amsgrad", "=", "group", "[", "'lr'", "]", ",", "group", "[", "'eps'", "]", ",", "group", "[", "'amsgrad'", "]", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "lr", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'AdamW does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "state", "[", "'PopArt_rescale'", "]", "=", "1.", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "'max_exp_avg_sq'", "in", "state", ":", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# We incorporate the term group['lr'] into the momentum, and define the bias_correction1 such that it respects the possibly moving group['lr']", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "lr", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1. - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "lr", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "*", "sqrt_bias_correction2", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"step_size\" and \"eps\"", "\n", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "rescaling", "=", "state", "[", "'PopArt_rescale'", "]", "\n", "exp_avg", ".", "mul_", "(", "beta1", "*", "rescaling", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "(", "1.", "-", "beta1", ")", "*", "lr", ")", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", "**", "2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "exp_avg_sq", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "# 'eps' is first multiplied by sqrt_bias_correction2 and then divided by sqrt_bias_correction2", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_sq", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.AdamBelief.__init__": [[136, 151], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "1e-3", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-8", ",", "\n", "weight_decay", "=", "0", ",", "amsgrad", "=", "False", ")", ":", "\n", "        ", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "AdamBelief", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.AdamBelief.__setstate__": [[152, 156], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "AdamW", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.AdamBelief.step": [[157, 234], ["closure", "math.sqrt", "exp_avg.mul_().add_", "exp_avg_var.mul_().addcmul_", "p.data.addcdiv_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.max", "max_exp_avg_var.sqrt().add_", "exp_avg_var.sqrt().add_", "torch.zeros_like", "torch.zeros_like", "exp_avg.mul_", "exp_avg_var.mul_", "max_exp_avg_var.sqrt", "exp_avg_var.sqrt"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "lr", ",", "eps", ",", "amsgrad", "=", "group", "[", "'lr'", "]", ",", "group", "[", "'eps'", "]", ",", "group", "[", "'amsgrad'", "]", "\n", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "lr", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'AdamW does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "", "exp_avg", ",", "exp_avg_var", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_var'", "]", "\n", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "'max_exp_avg_var'", "in", "state", ":", "\n", "                        ", "state", "[", "'max_exp_avg_var'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "max_exp_avg_var", "=", "state", "[", "'max_exp_avg_var'", "]", "\n", "\n", "", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# We define the bias_correction1 such that it respects the possibly moving \"group['lr']\"", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "lr", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1. - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "lr", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "*", "sqrt_bias_correction2", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"step_size\" and \"eps\"", "\n", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "diff", "=", "grad", "-", "exp_avg", "\n", "exp_avg", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "(", "1.", "-", "beta1", ")", ")", "\n", "exp_avg_var", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "diff", ",", "diff", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains the maximum of all 2nd moment running avg. till now", "\n", "                    ", "torch", ".", "max", "(", "max_exp_avg_var", ",", "exp_avg_var", ",", "out", "=", "max_exp_avg_var", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "# 'eps' is first multiplied by sqrt_bias_correction2 and then divided by sqrt_bias_correction2", "\n", "", "else", ":", "\n", "                    ", "denom", "=", "exp_avg_var", ".", "sqrt", "(", ")", ".", "add_", "(", "eps", "*", "sqrt_bias_correction2", ")", "\n", "\n", "", "p", ".", "data", ".", "addcdiv_", "(", "exp_avg", ",", "denom", ",", "value", "=", "-", "step_size", "*", "lr", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__init__": [[238, 254], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__"], ["    ", "def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "4e-4", ",", "betas", "=", "(", "0.9", ",", "0.999", ")", ",", "eps", "=", "1e-15", ",", "\n", "weight_decay", "=", "0.", ",", "amsgrad", "=", "False", ",", "centered", "=", "False", ")", ":", "\n", "        ", "self", ".", "centered", "=", "centered", "\n", "if", "not", "0.0", "<=", "lr", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {}\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "not", "0.0", "<=", "eps", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {}\"", ".", "format", "(", "eps", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "0", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 0: {}\"", ".", "format", "(", "betas", "[", "0", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "betas", "[", "1", "]", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid beta parameter at index 1: {}\"", ".", "format", "(", "betas", "[", "1", "]", ")", ")", "\n", "", "if", "not", "0.0", "<=", "weight_decay", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid weight_decay value: {}\"", ".", "format", "(", "weight_decay", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "betas", "=", "betas", ",", "eps", "=", "eps", ",", "\n", "weight_decay", "=", "weight_decay", ",", "amsgrad", "=", "amsgrad", ")", "\n", "super", "(", "LaProp", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__": [[255, 259], ["super().__setstate__", "group.setdefault"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "super", "(", "LaProp", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "group", ".", "setdefault", "(", "'amsgrad'", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.secE.2.optimizers.LaProp.step": [[260, 349], ["closure", "math.sqrt", "exp_avg_sq.mul_().addcmul_", "denom.addcmul.addcmul.sqrt().add_", "exp_avg.mul_().addcdiv_", "p.data.add_", "p.data.mul_", "RuntimeError", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "exp_mean_avg_sq.mul_().add_", "torch.zeros_like", "exp_avg_sq.mul_", "denom.addcmul.addcmul.addcmul", "torch.max", "denom.addcmul.addcmul.sqrt", "exp_avg.mul_", "exp_mean_avg_sq.mul_"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "\n", "", "if", "group", "[", "'weight_decay'", "]", "!=", "0", ":", "\n", "                    ", "p", ".", "data", ".", "mul_", "(", "1.", "-", "group", "[", "'lr'", "]", "*", "group", "[", "'weight_decay'", "]", ")", "\n", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'LaProp does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "", "amsgrad", "=", "group", "[", "'amsgrad'", "]", "\n", "\n", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "'step'", "not", "in", "state", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'exp_avg'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of learning rates", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "0.", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "0.", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "state", "[", "'exp_mean_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "if", "amsgrad", ":", "\n", "# Maintains max of all exp. moving avg. of sq. grad. values", "\n", "                        ", "state", "[", "'max_exp_avg_sq'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "", "state", "[", "'PopArt_rescale'", "]", "=", "1.", "\n", "state", "[", "'Momentum_rescale'", "]", "=", "1.", "\n", "\n", "", "exp_avg", ",", "exp_avg_sq", "=", "state", "[", "'exp_avg'", "]", ",", "state", "[", "'exp_avg_sq'", "]", "\n", "if", "self", ".", "centered", ":", "\n", "                    ", "exp_mean_avg_sq", "=", "state", "[", "'exp_mean_avg_sq'", "]", "\n", "", "if", "amsgrad", ":", "\n", "                    ", "max_exp_avg_sq", "=", "state", "[", "'max_exp_avg_sq'", "]", "\n", "", "beta1", ",", "beta2", "=", "group", "[", "'betas'", "]", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "state", "[", "'exp_avg_lr_1'", "]", "=", "state", "[", "'exp_avg_lr_1'", "]", "*", "beta1", "+", "(", "1.", "-", "beta1", ")", "*", "group", "[", "'lr'", "]", "\n", "state", "[", "'exp_avg_lr_2'", "]", "=", "state", "[", "'exp_avg_lr_2'", "]", "*", "beta2", "+", "(", "1.", "-", "beta2", ")", "\n", "\n", "#bias_correction1 = state['exp_avg_lr_1'] / group['lr'] if group['lr']!=0. else 1. #1 - beta1 ** state['step']", "\n", "bias_correction2", "=", "state", "[", "'exp_avg_lr_2'", "]", "\n", "\n", "# For convenience, we directly use \"sqrt_bias_correction2\" and \"step_size\" as the following", "\n", "sqrt_bias_correction2", "=", "math", ".", "sqrt", "(", "bias_correction2", ")", "\n", "# when state['exp_avg_lr_1'] is zero, exp_avg should also be zero and it is trivial", "\n", "one_over_bias_correction1", "=", "group", "[", "'lr'", "]", "/", "state", "[", "'exp_avg_lr_1'", "]", "if", "state", "[", "'exp_avg_lr_1'", "]", "!=", "0.", "else", "0.", "\n", "step_size", "=", "one_over_bias_correction1", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "rescaling", "=", "state", "[", "'PopArt_rescale'", "]", "\n", "exp_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", "**", "2", ")", ".", "addcmul_", "(", "grad", ",", "grad", ",", "value", "=", "1.", "-", "beta2", ")", "\n", "\n", "denom", "=", "exp_avg_sq", "\n", "if", "self", ".", "centered", ":", "\n", "                    ", "exp_mean_avg_sq", ".", "mul_", "(", "beta2", "*", "rescaling", ")", ".", "add_", "(", "grad", ",", "alpha", "=", "1.", "-", "beta2", ")", "\n", "if", "state", "[", "'step'", "]", ">", "5", ":", "\n", "                        ", "denom", "=", "denom", ".", "addcmul", "(", "exp_mean_avg_sq", ",", "exp_mean_avg_sq", ",", "value", "=", "-", "1.", ")", "\n", "\n", "", "", "if", "amsgrad", ":", "\n", "                    ", "if", "not", "(", "self", ".", "centered", "and", "state", "[", "'step'", "]", "<=", "5", ")", ":", "\n", "# Maintains the maximum of all (centered) 2nd moment running avg. till now", "\n", "                        ", "torch", ".", "max", "(", "max_exp_avg_sq", ",", "denom", ",", "out", "=", "max_exp_avg_sq", ")", "\n", "# Use the max. for normalizing running avg. of gradient", "\n", "denom", "=", "max_exp_avg_sq", "\n", "\n", "", "", "denom", "=", "denom", ".", "sqrt", "(", ")", ".", "add_", "(", "group", "[", "'eps'", "]", "*", "sqrt_bias_correction2", ")", "# instead of correcting \"denom\" by dividing it, we put the correction factor into \"exp_avg\" and \"eps\"", "\n", "\n", "momentum_rescaling", "=", "state", "[", "'Momentum_rescale'", "]", "\n", "exp_avg", ".", "mul_", "(", "beta1", "*", "momentum_rescaling", ")", ".", "addcdiv_", "(", "grad", ",", "denom", ",", "value", "=", "(", "1.", "-", "beta1", ")", "*", "group", "[", "'lr'", "]", "*", "sqrt_bias_correction2", ")", "\n", "\n", "p", ".", "data", ".", "add_", "(", "exp_avg", ",", "alpha", "=", "-", "step_size", ")", "\n", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.GridWorld.__init__": [[19, 25], ["numpy.ones"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "width", "=", "8", ")", ":", "\n", "        ", "self", ".", "width", "=", "width", "\n", "self", ".", "upper_bound", ",", "self", ".", "right_bound", "=", "self", ".", "height", "-", "1", ",", "self", ".", "width", "-", "1", "\n", "self", ".", "render_grid", "=", "np", ".", "ones", "(", "(", "self", ".", "height", ",", "self", ".", "width", ",", "3", ")", ")", "\n", "self", ".", "render_grid", "[", "0", ",", "0", ":", "]", "=", "self", ".", "cliff_color", "\n", "self", ".", "render_grid", "[", "-", "1", ",", "-", "1", "]", "=", "self", ".", "goal_color", "\n", "", "def", "step", "(", "self", ",", "state", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.GridWorld.step": [[25, 35], ["one-way cliff walking.GridWorld.reset"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset"], ["", "def", "step", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "if", "action", "==", "0", ":", "\n", "            ", "reward", "=", "-", "1.", ";", "done", "=", "1.", "\n", "", "elif", "action", "==", "1", ":", "\n", "            ", "state", "+=", "1", ";", "reward", "=", "2.", "\n", "done", "=", "0.", "if", "state", "<", "self", ".", "right_bound", "else", "1.", "\n", "", "else", ":", "assert", "False", ",", "\"the action is invalid!\"", "\n", "if", "done", ":", "# automatically reset", "\n", "            ", "state", "=", "self", ".", "reset", "(", ")", "\n", "", "return", "state", ",", "reward", ",", "done", "\n", "", "def", "reset", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.GridWorld.reset": [[35, 37], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "0", "\n", "", "def", "flatten_state_action", "(", "self", ",", "state", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.GridWorld.flatten_state_action": [[37, 39], ["None"], "methods", ["None"], ["", "def", "flatten_state_action", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "return", "state", "*", "self", ".", "num_action", "+", "action", "\n", "", "def", "get_random", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.GridWorld.get_random": [[39, 43], ["random.randrange"], "methods", ["None"], ["", "def", "get_random", "(", "self", ")", ":", "\n", "        ", "state_action", "=", "random", ".", "randrange", "(", "(", "self", ".", "width", "-", "1", ")", "*", "self", ".", "num_action", ")", "\n", "state", ",", "action", "=", "state_action", "//", "self", ".", "num_action", ",", "state_action", "%", "self", ".", "num_action", "\n", "return", "state", ",", "action", "\n", "", "def", "render", "(", "self", ",", "q_table", ",", "figsize", ",", "name", "=", "\"\"", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.GridWorld.render": [[43, 69], ["matplotlib.figure", "matplotlib.figure", "matplotlib.gca", "matplotlib.gca", "matplotlib.gca.imshow", "matplotlib.gca.grid", "matplotlib.gca.set_xticks", "matplotlib.gca.set_yticks", "matplotlib.gca.set_xticklabels", "matplotlib.gca.set_yticklabels", "matplotlib.gca.set_frame_on", "one-way cliff walking.GridWorld.to_action_table", "range", "matplotlib.gca.text", "matplotlib.tight_layout", "matplotlib.tight_layout", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.close", "matplotlib.close", "list", "list", "max", "range", "range", "matplotlib.gca.arrow", "enumerate", "datetime.datetime.now().strftime", "abs", "datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.to_action_table", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "def", "render", "(", "self", ",", "q_table", ",", "figsize", ",", "name", "=", "\"\"", ")", ":", "\n", "        ", "plt", ".", "figure", "(", "figsize", "=", "figsize", ")", "\n", "ax", "=", "plt", ".", "gca", "(", ")", "\n", "ax", ".", "imshow", "(", "self", ".", "render_grid", ",", "extent", "=", "(", "0", ",", "self", ".", "width", ",", "self", ".", "height", ",", "0", ")", ",", "interpolation", "=", "\"none\"", ")", "\n", "ax", ".", "grid", "(", "color", "=", "'black'", ",", "linewidth", "=", "2.5", ")", "\n", "ax", ".", "set_xticks", "(", "list", "(", "range", "(", "0", ",", "self", ".", "width", "+", "1", ")", ")", ")", "\n", "ax", ".", "set_yticks", "(", "list", "(", "range", "(", "0", ",", "self", ".", "height", "+", "1", ")", ")", ")", "\n", "ax", ".", "set_xticklabels", "(", "[", "]", ")", "\n", "ax", ".", "set_yticklabels", "(", "[", "]", ")", "\n", "ax", ".", "set_frame_on", "(", "True", ")", "\n", "actions", "=", "self", ".", "to_action_table", "(", "q_table", ")", "\n", "for", "j", "in", "range", "(", "self", ".", "width", "-", "1", ")", ":", "\n", "            ", "i", "=", "1", "\n", "displacement", "=", "0.4", "\n", "qs", "=", "q_table", "[", "i", "]", "\n", "max_q", "=", "max", "(", "qs", ")", "\n", "actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "qs", ")", "if", "abs", "(", "q", "-", "max_q", ")", "<", "1e-10", "]", "\n", "for", "action", "in", "actions", ":", "\n", "                ", "center", "=", "(", "j", "+", "0.5", ",", "i", "+", "0.5", ")", "\n", "if", "action", "==", "0", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", ",", "center", "[", "1", "]", "+", "0.15", ",", "0", ",", "-", "1.", "*", "displacement", ")", "\n", "elif", "action", "==", "1", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", "-", "0.15", ",", "center", "[", "1", "]", ",", "1.", "*", "displacement", ",", "0", ")", "\n", "ax", ".", "arrow", "(", "*", "x_y_dx_dy", ",", "width", "=", "0.05", ",", "head_width", "=", "0.15", ",", "head_length", "=", "0.15", ",", "length_includes_head", "=", "True", ",", "color", "=", "\"tab:red\"", ")", "\n", "", "", "ax", ".", "text", "(", "self", ".", "width", "-", "0.5", ",", "self", ".", "height", "-", "0.5", ",", "\"Goal\"", ",", "horizontalalignment", "=", "\"center\"", ",", "verticalalignment", "=", "\"center\"", ",", "fontsize", "=", "13", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "savefig", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d %H:%M:%S\"", ")", "+", "name", "+", "\".pdf\"", ")", "# the file name", "\n", "plt", ".", "close", "(", ")", "\n", "", "def", "to_action_table", "(", "self", ",", "q_table", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.GridWorld.to_action_table": [[69, 72], ["numpy.array().reshape", "numpy.argmax", "numpy.array"], "methods", ["None"], ["", "def", "to_action_table", "(", "self", ",", "q_table", ")", ":", "\n", "        ", "q_table", "=", "np", ".", "array", "(", "q_table", ")", ".", "reshape", "(", "self", ".", "width", "-", "1", ",", "self", ".", "num_action", ")", "\n", "return", "np", ".", "argmax", "(", "q_table", ",", "axis", "=", "1", ")", "\n", "", "def", "loss_bellman", "(", "self", ",", "q_table", ",", "gamma", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.GridWorld.loss_bellman": [[72, 81], ["enumerate", "enumerate", "one-way cliff walking.GridWorld.step", "max"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "def", "loss_bellman", "(", "self", ",", "q_table", ",", "gamma", ")", ":", "\n", "        ", "loss", "=", "0.", "\n", "for", "state", ",", "qs", "in", "enumerate", "(", "q_table", ")", ":", "\n", "            ", "for", "action", ",", "q", "in", "enumerate", "(", "qs", ")", ":", "\n", "                ", "next_state", ",", "reward", ",", "done", "=", "self", ".", "step", "(", "state", ",", "action", ")", "\n", "target_q_value", "=", "reward", "+", "(", "1.", "-", "done", ")", "*", "gamma", "*", "max", "(", "q_table", "[", "next_state", "]", ")", "\n", "loss", "+=", "(", "q", "-", "target_q_value", ")", "**", "2", "\n", "", "", "if", "not", "loss", "<", "1e20", ":", "assert", "False", ",", "\"The optimization diverged\"", "\n", "return", "loss", "\n", "", "def", "greedy_total_reward", "(", "self", ",", "q_table", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.GridWorld.greedy_total_reward": [[81, 95], ["one-way cliff walking.GridWorld.reset", "max", "one-way cliff walking.GridWorld.step", "len", "random.choice", "enumerate", "abs"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "greedy_total_reward", "(", "self", ",", "q_table", ")", ":", "\n", "        ", "state", "=", "self", ".", "reset", "(", ")", "\n", "total_reward", ",", "done", "=", "0.", ",", "0.", "\n", "while", "not", "done", ":", "\n", "# to keep a memory of states that have been previously seen", "\n", "            ", "optimal_q", "=", "max", "(", "q_table", "[", "state", "]", ")", "\n", "optimal_actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "q_table", "[", "state", "]", ")", "if", "abs", "(", "q", "-", "optimal_q", ")", "<", "1e-10", "]", "\n", "if", "len", "(", "optimal_actions", ")", "==", "1", ":", "action", "=", "optimal_actions", "[", "0", "]", "\n", "else", ":", "action", "=", "random", ".", "choice", "(", "optimal_actions", ")", "\n", "next_state", ",", "reward", ",", "done", "=", "self", ".", "step", "(", "state", ",", "action", ")", "\n", "# we reject loops and assign a negative reward equal to the cliff", "\n", "total_reward", "+=", "reward", "\n", "state", "=", "next_state", "\n", "", "return", "total_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.q_step": [[98, 104], ["None"], "function", ["None"], ["", "", "def", "q_step", "(", "q_table", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_actions", ",", "next_coefficient", ",", "lr", "=", "0.5", ")", ":", "\n", "    ", "next_action", "=", "next_actions", "[", "0", "]", "\n", "td_error", "=", "next_coefficient", "*", "q_table", "[", "next_state", "]", "[", "next_action", "]", "+", "reward", "-", "q_table", "[", "state", "]", "[", "action", "]", "\n", "loss", "=", "td_error", "**", "2", "\n", "q_table", "[", "state", "]", "[", "action", "]", "+=", "lr", "*", "td_error", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.rg_step": [[105, 113], ["len"], "function", ["None"], ["", "def", "rg_step", "(", "q_table", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_actions", ",", "next_coefficient", ",", "lr", "=", "0.5", ")", ":", "\n", "    ", "td_error", "=", "next_coefficient", "*", "q_table", "[", "next_state", "]", "[", "next_actions", "[", "0", "]", "]", "+", "reward", "-", "q_table", "[", "state", "]", "[", "action", "]", "\n", "loss", "=", "td_error", "**", "2", "\n", "q_table", "[", "state", "]", "[", "action", "]", "+=", "lr", "*", "td_error", "\n", "num_of_maximal_next_actions", "=", "len", "(", "next_actions", ")", "\n", "for", "next_action", "in", "next_actions", ":", "# in order to avoid disturbing the policy, we equally modify the actions that have the same Q value ", "\n", "        ", "q_table", "[", "next_state", "]", "[", "next_action", "]", "-=", "lr", "/", "num_of_maximal_next_actions", "*", "next_coefficient", "*", "td_error", "\n", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.train_with_state": [[114, 128], ["grid.step", "max", "strategy", "random.randrange", "max", "random.random", "len", "random.choice", "enumerate", "enumerate", "abs", "abs"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "def", "train_with_state", "(", "state", ",", "q_table", ",", "grid", ",", "strategy", ",", "gamma", "=", "0.99", ",", "lr", "=", "0.5", ",", "epsilon", "=", "0.", ")", ":", "\n", "    ", "if", "epsilon", ">", "0.", "and", "random", ".", "random", "(", ")", "<", "epsilon", ":", "\n", "        ", "action", "=", "random", ".", "randrange", "(", "grid", ".", "num_action", ")", "\n", "", "else", ":", "\n", "        ", "optimal_q", "=", "max", "(", "q_table", "[", "state", "]", ")", "\n", "optimal_actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "q_table", "[", "state", "]", ")", "if", "abs", "(", "q", "-", "optimal_q", ")", "<", "1e-10", "]", "\n", "if", "len", "(", "optimal_actions", ")", "==", "1", ":", "action", "=", "optimal_actions", "[", "0", "]", "\n", "else", ":", "action", "=", "random", ".", "choice", "(", "optimal_actions", ")", "\n", "", "next_state", ",", "reward", ",", "done", "=", "grid", ".", "step", "(", "state", ",", "action", ")", "\n", "next_coefficient", "=", "(", "1.", "-", "done", ")", "*", "gamma", "\n", "next_q", "=", "max", "(", "q_table", "[", "next_state", "]", ")", "\n", "next_actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "q_table", "[", "next_state", "]", ")", "if", "abs", "(", "q", "-", "next_q", ")", "<", "1e-10", "]", "\n", "strategy", "(", "q_table", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_actions", ",", "next_coefficient", ",", "lr", "=", "lr", ")", "\n", "return", "next_state", ",", "reward", ",", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.one-way cliff walking.train_rand": [[129, 135], ["grid.get_random", "grid.step", "numpy.argmax", "strategy"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.get_random", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "train_rand", "(", "q_table", ",", "grid", ",", "strategy", ",", "gamma", "=", "0.99", ",", "lr", "=", "0.5", ")", ":", "\n", "    ", "state", ",", "action", "=", "grid", ".", "get_random", "(", ")", "\n", "next_state", ",", "reward", ",", "done", "=", "grid", ".", "step", "(", "state", ",", "action", ")", "\n", "next_coefficient", "=", "(", "1.", "-", "done", ")", "*", "gamma", "\n", "next_action", "=", "np", ".", "argmax", "(", "q_table", "[", "next_state", "]", ")", "\n", "strategy", "(", "q_table", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "[", "next_action", "]", ",", "next_coefficient", ",", "lr", "=", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.__init__": [[18, 24], ["numpy.ones"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "width", "=", "10", ")", ":", "\n", "        ", "self", ".", "width", "=", "width", "\n", "self", ".", "low_bound", ",", "self", ".", "right_bound", "=", "self", ".", "height", "-", "1", ",", "self", ".", "width", "-", "1", "\n", "self", ".", "render_grid", "=", "np", ".", "ones", "(", "(", "self", ".", "height", ",", "self", ".", "width", ",", "3", ")", ")", "\n", "self", ".", "render_grid", "[", "-", "1", ",", "1", ":", "]", "=", "self", ".", "cliff_color", "\n", "self", ".", "render_grid", "[", "-", "1", ",", "-", "1", "]", "=", "self", ".", "goal_color", "\n", "", "def", "step", "(", "self", ",", "state", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step": [[24, 39], ["None"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "if", "action", "==", "0", "and", "state", "[", "0", "]", ">", "0", ":", "state", "=", "(", "state", "[", "0", "]", "-", "1", ",", "state", "[", "1", "]", ")", "\n", "elif", "action", "==", "1", "and", "state", "[", "0", "]", "<", "self", ".", "low_bound", ":", "state", "=", "(", "state", "[", "0", "]", "+", "1", ",", "state", "[", "1", "]", ")", "\n", "elif", "action", "==", "2", "and", "state", "[", "1", "]", "<", "self", ".", "right_bound", ":", "state", "=", "(", "state", "[", "0", "]", ",", "state", "[", "1", "]", "+", "1", ")", "\n", "elif", "action", "==", "3", "and", "state", "[", "1", "]", ">", "0", ":", "state", "=", "(", "state", "[", "0", "]", ",", "state", "[", "1", "]", "-", "1", ")", "\n", "else", ":", "assert", "False", ",", "\"the action is invalid!\"", "\n", "# prepare reward and done", "\n", "done", "=", "0.", "\n", "reward", "=", "-", "1.", "\n", "# only if the state is at the lowest row, done may be True", "\n", "if", "state", "[", "0", "]", "==", "self", ".", "low_bound", ":", "\n", "            ", "if", "state", "[", "1", "]", ">", "0", ":", "\n", "                ", "done", "=", "1.", "\n", "reward", "=", "0.", "if", "state", "[", "1", "]", "==", "self", ".", "right_bound", "else", "-", "100.", "\n", "", "", "return", "state", ",", "reward", ",", "done", "\n", "", "def", "reset", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset": [[39, 41], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "return", "(", "self", ".", "height", "-", "1", ",", "0", ")", "\n", "", "def", "action_invalid", "(", "self", ",", "state", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.action_invalid": [[41, 43], ["None"], "methods", ["None"], ["", "def", "action_invalid", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "return", "(", "action", "==", "0", "and", "state", "[", "0", "]", "==", "0", ")", "or", "(", "action", "==", "1", "and", "state", "[", "0", "]", "==", "self", ".", "low_bound", ")", "or", "(", "action", "==", "2", "and", "state", "[", "1", "]", "==", "self", ".", "right_bound", ")", "or", "(", "action", "==", "3", "and", "state", "[", "1", "]", "==", "0", ")", "\n", "", "def", "flatten_state", "(", "self", ",", "state", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state": [[43, 45], ["None"], "methods", ["None"], ["", "def", "flatten_state", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "state", "[", "0", "]", "*", "self", ".", "width", "+", "state", "[", "1", "]", "\n", "", "def", "flatten_state_action", "(", "self", ",", "state", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state_action": [[45, 47], ["None"], "methods", ["None"], ["", "def", "flatten_state_action", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "return", "state", "[", "0", "]", "*", "self", ".", "width", "*", "self", ".", "num_action", "+", "state", "[", "1", "]", "*", "self", ".", "num_action", "+", "action", "\n", "", "def", "unflatten_state", "(", "self", ",", "state", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.unflatten_state": [[47, 49], ["None"], "methods", ["None"], ["", "def", "unflatten_state", "(", "self", ",", "state", ")", ":", "\n", "        ", "return", "state", "//", "self", ".", "width", ",", "state", "%", "self", ".", "width", "\n", "", "def", "get_random", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.get_random": [[49, 55], ["random.randrange", "cliff walking.GridWorld.get_random", "cliff walking.GridWorld.action_invalid"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.get_random", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.action_invalid"], ["", "def", "get_random", "(", "self", ")", ":", "\n", "        ", "state_action", "=", "random", ".", "randrange", "(", "self", ".", "height", "*", "self", ".", "width", "*", "self", ".", "num_action", ")", "\n", "state", ",", "action", "=", "state_action", "//", "self", ".", "num_action", ",", "state_action", "%", "self", ".", "num_action", "\n", "state", "=", "state", "//", "self", ".", "width", ",", "state", "%", "self", ".", "width", "\n", "if", "not", "(", "(", "state", "[", "0", "]", "==", "self", ".", "low_bound", "and", "state", "[", "1", "]", ">", "0", ")", "or", "self", ".", "action_invalid", "(", "state", ",", "action", ")", ")", ":", "return", "state", ",", "action", "\n", "else", ":", "return", "self", ".", "get_random", "(", ")", "# resample by recursively calling itself", "\n", "", "def", "render", "(", "self", ",", "q_table", ",", "name", "=", "\"\"", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.render": [[55, 84], ["matplotlib.subplots", "matplotlib.subplots", "ax.imshow", "ax.grid", "ax.set_xticks", "ax.set_yticks", "ax.set_xticklabels", "ax.set_yticklabels", "ax.set_frame_on", "cliff walking.GridWorld.to_action_table", "range", "ax.text", "matplotlib.tight_layout", "matplotlib.tight_layout", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.close", "matplotlib.close", "list", "list", "range", "range", "range", "max", "ax.arrow", "datetime.datetime.now().strftime", "cliff walking.GridWorld.flatten_state", "enumerate", "abs", "datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.to_action_table", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state"], ["", "def", "render", "(", "self", ",", "q_table", ",", "name", "=", "\"\"", ")", ":", "\n", "        ", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "ax", ".", "imshow", "(", "self", ".", "render_grid", ",", "extent", "=", "(", "0", ",", "self", ".", "width", ",", "self", ".", "height", ",", "0", ")", ",", "interpolation", "=", "\"none\"", ")", "\n", "ax", ".", "grid", "(", "color", "=", "'black'", ",", "linewidth", "=", "2.5", ")", "\n", "ax", ".", "set_xticks", "(", "list", "(", "range", "(", "0", ",", "self", ".", "width", "+", "1", ")", ")", ")", "\n", "ax", ".", "set_yticks", "(", "list", "(", "range", "(", "0", ",", "self", ".", "height", "+", "1", ")", ")", ")", "\n", "ax", ".", "set_xticklabels", "(", "[", "]", ")", "\n", "ax", ".", "set_yticklabels", "(", "[", "]", ")", "\n", "ax", ".", "set_frame_on", "(", "True", ")", "\n", "actions", "=", "self", ".", "to_action_table", "(", "q_table", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "height", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "self", ".", "width", ")", ":", "\n", "                ", "if", "i", "==", "self", ".", "low_bound", "and", "j", ">", "0", ":", "continue", "\n", "displacement", "=", "0.4", "\n", "qs", "=", "q_table", "[", "self", ".", "flatten_state", "(", "(", "i", ",", "j", ")", ")", "]", "\n", "max_q", "=", "max", "(", "qs", ")", "\n", "# note that variable names 'i' and 'j' are already in use", "\n", "actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "qs", ")", "if", "abs", "(", "q", "-", "max_q", ")", "<", "1e-10", "]", "\n", "for", "action", "in", "actions", ":", "\n", "                    ", "center", "=", "(", "j", "+", "0.5", ",", "i", "+", "0.5", ")", "\n", "if", "action", "==", "0", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", ",", "center", "[", "1", "]", "+", "0.05", ",", "0", ",", "-", "1.", "*", "displacement", ")", "# +displacement", "\n", "elif", "action", "==", "1", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", ",", "center", "[", "1", "]", "-", "0.05", ",", "0", ",", "1.", "*", "displacement", ")", "# -displacement", "\n", "elif", "action", "==", "2", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", "-", "0.05", ",", "center", "[", "1", "]", ",", "1.", "*", "displacement", ",", "0", ")", "# -displacement", "\n", "elif", "action", "==", "3", ":", "x_y_dx_dy", "=", "(", "center", "[", "0", "]", "+", "0.05", ",", "center", "[", "1", "]", ",", "-", "1.", "*", "displacement", ",", "0", ")", "# +displacement", "\n", "ax", ".", "arrow", "(", "*", "x_y_dx_dy", ",", "width", "=", "0.05", ",", "head_width", "=", "0.15", ",", "head_length", "=", "0.15", ",", "length_includes_head", "=", "True", ",", "color", "=", "\"tab:red\"", ")", "\n", "", "", "", "ax", ".", "text", "(", "self", ".", "width", "-", "0.5", ",", "self", ".", "height", "-", "0.5", ",", "\"Goal\"", ",", "horizontalalignment", "=", "\"center\"", ",", "verticalalignment", "=", "\"center\"", ",", "fontsize", "=", "13", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "savefig", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d %H:%M:%S\"", ")", "+", "name", "+", "\".pdf\"", ")", "# the file name", "\n", "plt", ".", "close", "(", ")", "\n", "", "def", "to_action_table", "(", "self", ",", "q_table", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.to_action_table": [[84, 87], ["numpy.array().reshape", "numpy.argmax", "numpy.array"], "methods", ["None"], ["", "def", "to_action_table", "(", "self", ",", "q_table", ")", ":", "\n", "        ", "q_table", "=", "np", ".", "array", "(", "q_table", ")", ".", "reshape", "(", "self", ".", "height", ",", "self", ".", "width", ",", "self", ".", "num_action", ")", "\n", "return", "np", ".", "argmax", "(", "q_table", ",", "axis", "=", "2", ")", "\n", "", "def", "loss_to_truth", "(", "self", ",", "q_table", ",", "truth", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.loss_to_truth": [[87, 96], ["enumerate", "cliff walking.GridWorld.unflatten_state", "enumerate", "cliff walking.GridWorld.action_invalid"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.unflatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.action_invalid"], ["", "def", "loss_to_truth", "(", "self", ",", "q_table", ",", "truth", ")", ":", "\n", "        ", "loss", "=", "0.", "\n", "for", "state", ",", "qs", "in", "enumerate", "(", "q_table", ")", ":", "\n", "            ", "unflattened_state", "=", "self", ".", "unflatten_state", "(", "state", ")", "\n", "if", "unflattened_state", "[", "0", "]", "==", "self", ".", "low_bound", "and", "unflattened_state", "[", "1", "]", ">", "0", ":", "continue", "\n", "for", "action", ",", "q", "in", "enumerate", "(", "qs", ")", ":", "\n", "                ", "if", "self", ".", "action_invalid", "(", "unflattened_state", ",", "action", ")", ":", "continue", "\n", "else", ":", "loss", "+=", "(", "q", "-", "truth", "[", "state", "]", "[", "action", "]", ")", "**", "2", "\n", "", "", "return", "loss", "\n", "", "def", "policy_diff_truth", "(", "self", ",", "q_table", ",", "truth", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.policy_diff_truth": [[96, 113], ["enumerate", "cliff walking.GridWorld.unflatten_state", "max", "max", "len", "enumerate", "enumerate", "abs", "abs"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.unflatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max"], ["", "def", "policy_diff_truth", "(", "self", ",", "q_table", ",", "truth", ")", ":", "\n", "        ", "n", ",", "n_wrong", "=", "0", ",", "0", "\n", "for", "state", ",", "qs", "in", "enumerate", "(", "q_table", ")", ":", "\n", "            ", "unflattened_state", "=", "self", ".", "unflatten_state", "(", "state", ")", "\n", "if", "unflattened_state", "[", "0", "]", "==", "self", ".", "low_bound", "and", "unflattened_state", "[", "1", "]", ">", "0", ":", "continue", "\n", "n", "+=", "1", "\n", "# find the optimal actions ", "\n", "optimal_q", "=", "max", "(", "truth", "[", "state", "]", ")", "\n", "optimal_actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "truth", "[", "state", "]", ")", "if", "abs", "(", "q", "-", "optimal_q", ")", "<", "1e-10", "]", "\n", "# find the actions of the current q_table", "\n", "max_q", "=", "max", "(", "qs", ")", "\n", "current_actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "qs", ")", "if", "abs", "(", "q", "-", "max_q", ")", "<", "1e-10", "]", "\n", "num_action", "=", "len", "(", "current_actions", ")", "\n", "for", "action", "in", "current_actions", ":", "\n", "                ", "if", "action", "not", "in", "optimal_actions", ":", "\n", "                    ", "n_wrong", "+=", "1.", "/", "num_action", "\n", "", "", "", "return", "n_wrong", "/", "n", "\n", "", "def", "loss_bellman", "(", "self", ",", "q_table", ",", "gamma", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.loss_bellman": [[113, 124], ["enumerate", "cliff walking.GridWorld.unflatten_state", "enumerate", "cliff walking.GridWorld.step", "cliff walking.GridWorld.action_invalid", "max", "cliff walking.GridWorld.flatten_state"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.unflatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.action_invalid", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state"], ["", "def", "loss_bellman", "(", "self", ",", "q_table", ",", "gamma", ")", ":", "\n", "        ", "loss", "=", "0.", "\n", "for", "state", ",", "qs", "in", "enumerate", "(", "q_table", ")", ":", "\n", "            ", "state", "=", "self", ".", "unflatten_state", "(", "state", ")", "\n", "for", "action", ",", "q", "in", "enumerate", "(", "qs", ")", ":", "\n", "                ", "if", "(", "state", "[", "0", "]", "==", "self", ".", "low_bound", "and", "state", "[", "1", "]", ">", "0", ")", "or", "self", ".", "action_invalid", "(", "state", ",", "action", ")", ":", "continue", "\n", "next_state", ",", "reward", ",", "done", "=", "self", ".", "step", "(", "state", ",", "action", ")", "\n", "target_q_value", "=", "reward", "+", "(", "1.", "-", "done", ")", "*", "gamma", "*", "max", "(", "q_table", "[", "self", ".", "flatten_state", "(", "next_state", ")", "]", ")", "\n", "loss", "+=", "(", "q", "-", "target_q_value", ")", "**", "2", "\n", "", "", "if", "not", "loss", "<", "1e20", ":", "assert", "False", ",", "\"The optimization diverged\"", "\n", "return", "loss", "\n", "", "def", "greedy_total_reward", "(", "self", ",", "q_table", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.greedy_total_reward": [[124, 143], ["cliff walking.GridWorld.reset", "previous_states.append", "grid.flatten_state", "max", "cliff walking.GridWorld.step", "len", "random.choice", "enumerate", "abs"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.reset", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.common.replay_buffer.MaxSegmentTree.max", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step"], ["", "def", "greedy_total_reward", "(", "self", ",", "q_table", ")", ":", "\n", "        ", "previous_states", "=", "[", "]", "\n", "state", "=", "self", ".", "reset", "(", ")", "\n", "total_reward", ",", "done", "=", "0.", ",", "0.", "\n", "while", "not", "done", ":", "\n", "# to keep a memory of states that have been previously seen", "\n", "            ", "previous_states", ".", "append", "(", "state", ")", "\n", "flattened_state", "=", "grid", ".", "flatten_state", "(", "state", ")", "\n", "optimal_q", "=", "max", "(", "q_table", "[", "flattened_state", "]", ")", "\n", "optimal_actions", "=", "[", "idx", "for", "idx", ",", "q", "in", "enumerate", "(", "q_table", "[", "flattened_state", "]", ")", "if", "abs", "(", "q", "-", "optimal_q", ")", "<", "1e-10", "]", "\n", "if", "len", "(", "optimal_actions", ")", "==", "1", ":", "action", "=", "optimal_actions", "[", "0", "]", "\n", "else", ":", "action", "=", "random", ".", "choice", "(", "optimal_actions", ")", "\n", "next_state", ",", "reward", ",", "done", "=", "self", ".", "step", "(", "state", ",", "action", ")", "\n", "# we reject loops and assign a negative reward equal to the cliff", "\n", "if", "next_state", "in", "previous_states", ":", "\n", "                ", "reward", "=", "-", "100.", ";", "done", "=", "1.", "\n", "", "total_reward", "+=", "reward", "\n", "state", "=", "next_state", "\n", "", "return", "total_reward", "\n", "", "def", "disable_invalid_action_q", "(", "self", ",", "q_table", ")", ":", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.disable_invalid_action_q": [[143, 149], ["enumerate", "enumerate", "cliff walking.GridWorld.action_invalid", "cliff walking.GridWorld.unflatten_state", "float"], "methods", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.action_invalid", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.unflatten_state"], ["", "def", "disable_invalid_action_q", "(", "self", ",", "q_table", ")", ":", "\n", "        ", "for", "state", ",", "qs", "in", "enumerate", "(", "q_table", ")", ":", "\n", "            ", "for", "action", ",", "q", "in", "enumerate", "(", "qs", ")", ":", "\n", "                ", "if", "self", ".", "action_invalid", "(", "self", ".", "unflatten_state", "(", "state", ")", ",", "action", ")", ":", "\n", "                    ", "q_table", "[", "state", "]", "[", "action", "]", "=", "-", "float", "(", "\"inf\"", ")", "\n", "", "", "", "return", "q_table", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.q_step": [[152, 163], ["zip", "len", "zip", "td_errors.append"], "function", ["None"], ["", "", "def", "q_step", "(", "q_table", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ",", "lr", "=", "0.5", ")", ":", "\n", "    ", "td_errors", "=", "[", "]", "\n", "loss", "=", "0.", "\n", "for", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_action", ",", "next_coefficient", "in", "zip", "(", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ")", ":", "\n", "        ", "td_error", "=", "next_coefficient", "*", "q_table", "[", "next_state", "]", "[", "next_action", "]", "+", "reward", "-", "q_table", "[", "state", "]", "[", "action", "]", "\n", "loss", "+=", "td_error", "**", "2", "\n", "td_errors", ".", "append", "(", "td_error", ")", "\n", "", "batch_size", "=", "len", "(", "states", ")", "\n", "for", "td_error", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_action", ",", "next_coefficient", "in", "zip", "(", "td_errors", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ")", ":", "\n", "        ", "q_table", "[", "state", "]", "[", "action", "]", "+=", "lr", "/", "batch_size", "*", "td_error", "\n", "", "return", "loss", "/", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.rg_step": [[164, 175], ["zip", "len", "zip", "td_errors.append"], "function", ["None"], ["", "def", "rg_step", "(", "q_table", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ",", "lr", "=", "0.5", ")", ":", "\n", "    ", "td_errors", "=", "[", "]", "\n", "loss", "=", "0.", "\n", "for", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_action", ",", "next_coefficient", "in", "zip", "(", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ")", ":", "\n", "        ", "td_error", "=", "next_coefficient", "*", "q_table", "[", "next_state", "]", "[", "next_action", "]", "+", "reward", "-", "q_table", "[", "state", "]", "[", "action", "]", "\n", "loss", "+=", "td_error", "**", "2", "\n", "td_errors", ".", "append", "(", "td_error", ")", "\n", "", "batch_size", "=", "len", "(", "states", ")", "\n", "for", "td_error", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "next_action", ",", "next_coefficient", "in", "zip", "(", "td_errors", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ")", ":", "\n", "        ", "q_table", "[", "state", "]", "[", "action", "]", "+=", "lr", "/", "batch_size", "*", "td_error", ";", "q_table", "[", "next_state", "]", "[", "next_action", "]", "-=", "lr", "/", "batch_size", "*", "next_coefficient", "*", "td_error", "\n", "", "return", "loss", "/", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.train_rand": [[176, 186], ["range", "strategy", "grid.get_random", "grid.step", "numpy.argmax", "grid.flatten_state", "grid.flatten_state", "states.append", "actions.append", "next_states.append", "next_actions.append", "next_coefficients.append", "rewards.append"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.get_random", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.flatten_state"], ["", "def", "train_rand", "(", "q_table", ",", "grid", ",", "strategy", ",", "gamma", "=", "0.99", ",", "lr", "=", "0.5", ",", "batch_size", "=", "1", ")", ":", "\n", "    ", "states", ",", "actions", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ",", "rewards", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "batch_size", ")", ":", "\n", "        ", "state", ",", "action", "=", "grid", ".", "get_random", "(", ")", "\n", "next_state", ",", "reward", ",", "done", "=", "grid", ".", "step", "(", "state", ",", "action", ")", "\n", "next_coefficient", "=", "(", "1.", "-", "done", ")", "*", "gamma", "\n", "state", ",", "next_state", "=", "grid", ".", "flatten_state", "(", "state", ")", ",", "grid", ".", "flatten_state", "(", "next_state", ")", "\n", "next_action", "=", "np", ".", "argmax", "(", "q_table", "[", "next_state", "]", ")", "\n", "states", ".", "append", "(", "state", ")", ",", "actions", ".", "append", "(", "action", ")", ",", "next_states", ".", "append", "(", "next_state", ")", ",", "next_actions", ".", "append", "(", "next_action", ")", ",", "next_coefficients", ".", "append", "(", "next_coefficient", ")", ",", "rewards", ".", "append", "(", "reward", ")", "\n", "", "strategy", "(", "q_table", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "next_actions", ",", "next_coefficients", ",", "lr", "=", "lr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.run": [[198, 224], ["range", "min", "numpy.array().mean", "enumerate", "grid.disable_invalid_action_q", "range", "range", "range", "cliff walking.train_rand", "numpy.array", "grid.loss_bellman", "MSBE_loss[].append", "x[].append", "print", "len", "range", "range", "cliff walking.q_step", "cliff walking.q_step", "cliff walking.q_step", "cliff walking.rg_step", "cliff walking.rg_step", "cliff walking.rg_step"], "function", ["home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.disable_invalid_action_q", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.train_rand", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.GridWorld.loss_bellman", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.q_step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.q_step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.q_step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.rg_step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.rg_step", "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.rg_step"], ["\n", "num_of_trials", "=", "10", "\n", "q_MSBE_loss", ",", "q_true_loss", ",", "q_policy_error", "=", "[", "[", "]", "for", "i", "in", "range", "(", "num_of_trials", ")", "]", ",", "[", "[", "]", "for", "i", "in", "range", "(", "num_of_trials", ")", "]", ",", "[", "[", "]", "for", "i", "in", "range", "(", "num_of_trials", ")", "]", "\n", "q_x", "=", "[", "[", "]", "for", "i", "in", "range", "(", "num_of_trials", ")", "]", "\n", "for", "j", "in", "range", "(", "num_of_trials", ")", ":", "\n", "    ", "q_table", "=", "grid", ".", "disable_invalid_action_q", "(", "[", "[", "0.", "for", "a", "in", "range", "(", "grid", ".", "num_action", ")", "]", "for", "i", "in", "range", "(", "grid", ".", "height", "*", "grid", ".", "width", ")", "]", ")", "\n", "log_scale_x_axis", "=", "0.", "\n", "data_x_coordinate", "=", "10", "**", "log_scale_x_axis", "\n", "for", "i", "in", "range", "(", "1", ",", "10000", "+", "1", ")", ":", "\n", "        ", "train_rand", "(", "q_table", ",", "grid", ",", "q_step", ",", "gamma", "=", "gamma", ",", "lr", "=", "0.5", ",", "batch_size", "=", "1", ")", "\n", "if", "i", ">=", "data_x_coordinate", ":", "\n", "            ", "loss", "=", "grid", ".", "loss_bellman", "(", "q_table", ",", "gamma", "=", "gamma", ")", "\n", "q_MSBE_loss", "[", "j", "]", ".", "append", "(", "loss", ")", ";", "q_true_loss", "[", "j", "]", ".", "append", "(", "grid", ".", "loss_to_truth", "(", "q_table", ",", "truth", ")", ")", ";", "q_policy_error", "[", "j", "]", ".", "append", "(", "grid", ".", "policy_diff_truth", "(", "q_table", ",", "truth", ")", ")", "\n", "q_x", "[", "j", "]", ".", "append", "(", "i", ")", "\n", "while", "i", ">=", "data_x_coordinate", ":", "\n", "                ", "log_scale_x_axis", "+=", "0.005", "\n", "data_x_coordinate", "=", "10", "**", "log_scale_x_axis", "\n", "\n", "", "", "", "", "length", "=", "min", "(", "*", "[", "len", "(", "l", ")", "for", "l", "in", "q_MSBE_loss", "]", ")", "\n", "q_MSBE_loss", ",", "q_true_loss", ",", "q_policy_error", "=", "[", "l", "[", ":", "length", "]", "for", "l", "in", "q_MSBE_loss", "]", ",", "[", "l", "[", ":", "length", "]", "for", "l", "in", "q_true_loss", "]", ",", "[", "l", "[", ":", "length", "]", "for", "l", "in", "q_policy_error", "]", "\n", "q_MSBE_loss", ",", "q_true_loss", ",", "q_policy_error", "=", "np", ".", "array", "(", "q_MSBE_loss", ")", ".", "mean", "(", "axis", "=", "0", ")", ",", "np", ".", "array", "(", "q_true_loss", ")", ".", "mean", "(", "axis", "=", "0", ")", ",", "np", ".", "array", "(", "q_policy_error", ")", ".", "mean", "(", "axis", "=", "0", ")", "\n", "q_x", "=", "q_x", "[", "0", "]", "[", ":", "length", "]", "\n", "for", "q_length", ",", "l", "in", "enumerate", "(", "q_MSBE_loss", ")", ":", "\n", "    ", "if", "l", "<", "1e-5", ":", "break", "\n", "\n", "", "q_x", "=", "q_x", "[", ":", "q_length", "]", "\n", "q_MSBE_loss", ",", "q_true_loss", ",", "q_policy_error", "=", "q_MSBE_loss", "[", ":", "q_length", "]", ",", "q_true_loss", "[", ":", "q_length", "]", ",", "q_policy_error", "[", ":", "q_length", "]", "\n"]], "home.repos.pwc.inspect_result.Z-T-WANG_ConvergentDQN.sec3.2.cliff walking.avg2": [[237, 243], ["numpy.array", "numpy.array", "numpy.mean", "numpy.mean", "len", "array1.reshape", "array2.reshape"], "function", ["None"], ["if", "i", "%", "1000000", "==", "0", ":", "print", "(", "j", ",", "i", ")", "\n", "if", "i", ">=", "data_x_coordinate", ":", "\n", "            ", "loss", "=", "grid", ".", "loss_bellman", "(", "q_table", ",", "gamma", "=", "gamma", ")", "\n", "rg_MSBE_loss", "[", "j", "]", ".", "append", "(", "loss", ")", ";", "rg_true_loss", "[", "j", "]", ".", "append", "(", "grid", ".", "loss_to_truth", "(", "q_table", ",", "truth", ")", ")", ";", "rg_policy_error", "[", "j", "]", ".", "append", "(", "grid", ".", "policy_diff_truth", "(", "q_table", ",", "truth", ")", ")", "\n", "rg_x", "[", "j", "]", ".", "append", "(", "i", ")", "\n", "while", "i", ">=", "data_x_coordinate", ":", "\n", "                ", "log_scale_x_axis", "+=", "0.005", "\n"]]}