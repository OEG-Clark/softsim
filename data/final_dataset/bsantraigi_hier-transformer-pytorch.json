{"home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.HIERTransformer.__init__": [[46, 74], ["torch.nn.modules.module.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "model.PositionalEmbedding", "model.TransformerEncoderLayer", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "model._get_clones", "model.TransformerDecoderLayer", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "model.TransformerDecoder", "model.HIERTransformer._reset_parameters"], "methods", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__init__", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model._get_clones", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.HIERTransformer._reset_parameters"], ["def", "__init__", "(", "self", ",", "d_model", ":", "int", "=", "512", ",", "nhead", ":", "int", "=", "8", ",", "num_encoder_layers", ":", "int", "=", "6", ",", "\n", "num_decoder_layers", ":", "int", "=", "6", ",", "d_word_vec", ":", "int", "=", "512", ",", "dim_feedforward", ":", "int", "=", "2048", ",", "dropout", ":", "float", "=", "0.1", ",", "\n", "activation", ":", "str", "=", "\"relu\"", ",", "custom_encoder", ":", "Optional", "[", "Any", "]", "=", "None", ",", "custom_decoder", ":", "Optional", "[", "Any", "]", "=", "None", ",", "\n", "layer_norm_eps", ":", "float", "=", "1e-5", ",", "vocab_size", ":", "int", "=", "2", ",", "pad_index", ":", "int", "=", "0", ")", "->", "None", ":", "\n", "        ", "super", "(", "HIERTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "# Word Emb", "\n", "self", ".", "word_emb", "=", "torch", ".", "nn", ".", "Embedding", "(", "vocab_size", ",", "d_word_vec", ",", "padding_idx", "=", "pad_index", ")", "\n", "\n", "# Pos Emb", "\n", "self", ".", "post_word_emb", "=", "PositionalEmbedding", "(", "d_model", "=", "d_word_vec", ")", "\n", "\n", "# Encoder", "\n", "encoder_layer", "=", "TransformerEncoderLayer", "(", "d_model", ",", "nhead", ",", "dim_feedforward", ",", "dropout", ",", "activation", ",", "layer_norm_eps", ")", "\n", "encoder_norm", "=", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "self", ".", "enc_layers", "=", "_get_clones", "(", "encoder_layer", ",", "num_encoder_layers", ")", "\n", "self", ".", "num_layers_e", "=", "num_encoder_layers", "\n", "self", ".", "norm_e", "=", "encoder_norm", "\n", "\n", "# Decoder", "\n", "decoder_layer", "=", "TransformerDecoderLayer", "(", "d_model", ",", "nhead", ",", "dim_feedforward", ",", "dropout", ",", "activation", ",", "layer_norm_eps", ")", "\n", "decoder_norm", "=", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "self", ".", "decoder", "=", "TransformerDecoder", "(", "decoder_layer", ",", "num_decoder_layers", ",", "decoder_norm", ")", "\n", "\n", "self", ".", "_reset_parameters", "(", ")", "\n", "\n", "self", ".", "d_model", "=", "d_model", "\n", "self", ".", "nhead", "=", "nhead", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.HIERTransformer.forward": [[75, 156], ["hier_masks.get_hier_encoder_mask", "enumerate", "model.HIERTransformer.decoder", "src.size", "tgt.size", "RuntimeError", "tgt.transpose", "src.transpose", "model.HIERTransformer.word_emb", "model.HIERTransformer.post_word_emb.forward_by_index().transpose", "model.HIERTransformer.norm_e", "model.HIERTransformer.word_emb", "model.HIERTransformer.post_word_emb().transpose", "layer", "layer", "model.HIERTransformer.post_word_emb.forward_by_index", "model.HIERTransformer.post_word_emb().transpose", "model.HIERTransformer.post_word_emb", "tgt.transpose", "model.HIERTransformer.post_word_emb", "layer.transpose"], "methods", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks.get_hier_encoder_mask", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.PositionalEmbedding.forward_by_index"], ["", "def", "forward", "(", "self", ",", "src", ":", "Tensor", ",", "tgt", ":", "Tensor", ",", "utt_indices", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "ct_mask_type", ":", "str", "=", "\"cls\"", ",", "tgt_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "src_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "tgt_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Take in and process masked source/target sequences.\n        Args:\n            src: the sequence to the encoder (required).\n            tgt: the sequence to the decoder (required).\n            src_mask: the additive mask for the src sequence (optional).\n            tgt_mask: the additive mask for the tgt sequence (optional).\n            memory_mask: the additive mask for the encoder output (optional).\n            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).\n            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).\n            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).\n        Shape:\n            - src: :math:`(S, N, E)`.\n            - tgt: :math:`(T, N, E)`.\n            - src_mask: :math:`(S, S)`.\n            - tgt_mask: :math:`(T, T)`.\n            - memory_mask: :math:`(T, S)`.\n            - src_key_padding_mask: :math:`(N, S)`.\n            - tgt_key_padding_mask: :math:`(N, T)`.\n            - memory_key_padding_mask: :math:`(N, S)`.\n            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked\n            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend\n            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``\n            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor\n            is provided, it will be added to the attention weight.\n            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by\n            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero\n            positions will be unchanged. If a BoolTensor is provided, the positions with the\n            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.\n            - output: :math:`(T, N, E)`.\n            Note: Due to the multi-head attention architecture in the transformer model,\n            the output sequence length of a transformer is same as the input sequence\n            (i.e. target) length of the decode.\n            where S is the source sequence length, T is the target sequence length, N is the\n            batch size, E is the feature number\n        Examples:\n            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)\n        \"\"\"", "\n", "\n", "if", "src", ".", "size", "(", "1", ")", "!=", "tgt", ".", "size", "(", "1", ")", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"the batch number of src and tgt must be equal\"", ")", "\n", "\n", "# if src.size(2) != self.d_model or tgt.size(2) != self.d_model:", "\n", "#     raise RuntimeError(\"the feature number of src and tgt must be equal to d_model\")", "\n", "\n", "", "pe_utt_loc", ",", "enc_mask_utt", ",", "enc_mask_ct", ",", "dec_enc_attn_mask", "=", "get_hier_encoder_mask", "(", "tgt", ".", "transpose", "(", "0", ",", "1", ")", ",", "\n", "src", ".", "transpose", "(", "0", ",", "1", ")", ",", "\n", "src_key_padding_mask", ",", "\n", "utt_indices", ",", "\n", "type", "=", "ct_mask_type", ")", "\n", "# memory = self.encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)", "\n", "\n", "# Encoding", "\n", "# memory = src", "\n", "enc_inp", "=", "self", ".", "word_emb", "(", "src", ")", "+", "self", ".", "post_word_emb", ".", "forward_by_index", "(", "pe_utt_loc", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "\n", "for", "i", ",", "layer", "in", "enumerate", "(", "self", ".", "enc_layers", ")", ":", "\n", "            ", "if", "i", "==", "self", ".", "num_layers_e", "//", "2", ":", "\n", "# Positional Embedding for Context Encoder", "\n", "                ", "enc_inp", "=", "enc_inp", "+", "self", ".", "post_word_emb", "(", "enc_inp", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "", "if", "i", "<", "self", ".", "num_layers_e", "//", "2", ":", "\n", "                ", "enc_inp", "=", "layer", "(", "enc_inp", ",", "\n", "src_key_padding_mask", "=", "src_key_padding_mask", ",", "\n", "src_mask", "=", "enc_mask_utt", ")", "\n", "", "else", ":", "\n", "                ", "enc_inp", "=", "layer", "(", "enc_inp", ",", "\n", "src_key_padding_mask", "=", "src_key_padding_mask", ",", "\n", "src_mask", "=", "enc_mask_ct", ")", "\n", "\n", "", "", "if", "self", ".", "norm_e", "is", "not", "None", ":", "\n", "            ", "enc_inp", "=", "self", ".", "norm_e", "(", "enc_inp", ")", "\n", "", "memory", "=", "enc_inp", "\n", "\n", "# Decoding", "\n", "tgt", "=", "self", ".", "word_emb", "(", "tgt", ")", "+", "self", ".", "post_word_emb", "(", "tgt", ".", "transpose", "(", "0", ",", "1", ")", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "output", "=", "self", ".", "decoder", "(", "tgt", ",", "memory", ",", "tgt_mask", "=", "tgt_mask", ",", "memory_mask", "=", "dec_enc_attn_mask", ",", "\n", "tgt_key_padding_mask", "=", "tgt_key_padding_mask", ",", "\n", "memory_key_padding_mask", "=", "memory_key_padding_mask", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.HIERTransformer.generate_square_subsequent_mask": [[157, 164], ["mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill.float().masked_fill().masked_fill", "float", "mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill.float().masked_fill", "torch.triu", "torch.triu", "torch.triu", "torch.triu", "float", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "mask.float().masked_fill().masked_fill.float().masked_fill().masked_fill.float"], "methods", ["None"], ["", "def", "generate_square_subsequent_mask", "(", "self", ",", "sz", ":", "int", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Generate a square mask for the sequence. The masked positions are filled with float('-inf').\n            Unmasked positions are filled with float(0.0).\n        \"\"\"", "\n", "mask", "=", "(", "torch", ".", "triu", "(", "torch", ".", "ones", "(", "sz", ",", "sz", ")", ")", "==", "1", ")", ".", "transpose", "(", "0", ",", "1", ")", "\n", "mask", "=", "mask", ".", "float", "(", ")", ".", "masked_fill", "(", "mask", "==", "0", ",", "float", "(", "'-inf'", ")", ")", ".", "masked_fill", "(", "mask", "==", "1", ",", "float", "(", "0.0", ")", ")", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.HIERTransformer._reset_parameters": [[165, 171], ["model.HIERTransformer.parameters", "p.dim", "torch.nn.init.xavier_uniform_", "torch.nn.init.xavier_uniform_"], "methods", ["None"], ["", "def", "_reset_parameters", "(", "self", ")", ":", "\n", "        ", "r\"\"\"Initiate parameters in the transformer model.\"\"\"", "\n", "\n", "for", "p", "in", "self", ".", "parameters", "(", ")", ":", "\n", "            ", "if", "p", ".", "dim", "(", ")", ">", "1", ":", "\n", "                ", "xavier_uniform_", "(", "p", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.PositionalEmbedding.__init__": [[175, 190], ["super().__init__", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.zeros().float", "torch.arange().float().unsqueeze", "torch.arange().float().unsqueeze", "torch.arange().float().unsqueeze", "torch.arange().float().unsqueeze", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "pe.unsqueeze.unsqueeze.unsqueeze", "model.PositionalEmbedding.register_buffer", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange().float", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "math.log"], "methods", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "d_model", ",", "max_len", "=", "512", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Compute the positional encodings once in log space.", "\n", "pe", "=", "torch", ".", "zeros", "(", "max_len", ",", "d_model", ")", ".", "float", "(", ")", "\n", "pe", ".", "require_grad", "=", "False", "\n", "\n", "position", "=", "torch", ".", "arange", "(", "0", ",", "max_len", ")", ".", "float", "(", ")", ".", "unsqueeze", "(", "1", ")", "\n", "div_term", "=", "(", "torch", ".", "arange", "(", "0", ",", "d_model", ",", "2", ")", ".", "float", "(", ")", "*", "-", "(", "math", ".", "log", "(", "10000.0", ")", "/", "d_model", ")", ")", ".", "exp", "(", ")", "\n", "\n", "pe", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "position", "*", "div_term", ")", "\n", "pe", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "position", "*", "div_term", ")", "\n", "\n", "pe", "=", "pe", ".", "unsqueeze", "(", "0", ")", "\n", "self", ".", "register_buffer", "(", "'pe'", ",", "pe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.PositionalEmbedding.forward": [[191, 194], ["x.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "# Shape of X : [N x L x d] or [N x L]", "\n", "        ", "return", "self", ".", "pe", "[", ":", ",", ":", "x", ".", "size", "(", "1", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.PositionalEmbedding.forward_by_index": [[195, 197], ["model.PositionalEmbedding.pe.expand().gather", "loc.unsqueeze().expand().long", "model.PositionalEmbedding.pe.expand", "loc.unsqueeze().expand", "loc.unsqueeze"], "methods", ["None"], ["", "def", "forward_by_index", "(", "self", ",", "loc", ")", ":", "\n", "        ", "return", "self", ".", "pe", ".", "expand", "(", "loc", ".", "shape", "[", "0", "]", ",", "-", "1", ",", "-", "1", ")", ".", "gather", "(", "1", ",", "loc", ".", "unsqueeze", "(", "2", ")", ".", "expand", "(", "-", "1", ",", "-", "1", ",", "self", ".", "pe", ".", "shape", "[", "2", "]", ")", ".", "long", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerEncoder.__init__": [[213, 218], ["torch.nn.modules.module.Module.__init__", "model._get_clones"], "methods", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__init__", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model._get_clones"], ["def", "__init__", "(", "self", ",", "encoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "encoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerEncoder.forward": [[219, 237], ["mod", "model.TransformerEncoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ":", "Tensor", ",", "mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "src_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Pass the input through the encoder layers in turn.\n        Args:\n            src: the sequence to the encoder (required).\n            mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "output", "=", "src", "\n", "\n", "for", "mod", "in", "self", ".", "layers", ":", "\n", "            ", "output", "=", "mod", "(", "output", ",", "src_mask", "=", "mask", ",", "src_key_padding_mask", "=", "src_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoder.__init__": [[254, 259], ["torch.nn.modules.module.Module.__init__", "model._get_clones"], "methods", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__init__", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model._get_clones"], ["def", "__init__", "(", "self", ",", "decoder_layer", ",", "num_layers", ",", "norm", "=", "None", ")", ":", "\n", "        ", "super", "(", "TransformerDecoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layers", "=", "_get_clones", "(", "decoder_layer", ",", "num_layers", ")", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "norm", "=", "norm", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoder.forward": [[260, 286], ["mod", "model.TransformerDecoder.norm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ":", "Tensor", ",", "memory", ":", "Tensor", ",", "tgt_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "tgt_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "memory_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer in turn.\n        Args:\n            tgt: the sequence to the decoder (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "output", "=", "tgt", "\n", "\n", "for", "mod", "in", "self", ".", "layers", ":", "\n", "            ", "output", "=", "mod", "(", "output", ",", "memory", ",", "tgt_mask", "=", "tgt_mask", ",", "\n", "memory_mask", "=", "memory_mask", ",", "\n", "tgt_key_padding_mask", "=", "tgt_key_padding_mask", ",", "\n", "memory_key_padding_mask", "=", "memory_key_padding_mask", ")", "\n", "\n", "", "if", "self", ".", "norm", "is", "not", "None", ":", "\n", "            ", "output", "=", "self", ".", "norm", "(", "output", ")", "\n", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerEncoderLayer.__init__": [[307, 321], ["torch.nn.modules.module.Module.__init__", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "model._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__init__", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model._get_activation_fn"], ["def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "activation", "=", "\"relu\"", ",", "layer_norm_eps", "=", "1e-5", ")", ":", "\n", "        ", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerEncoderLayer.__setstate__": [[322, 326], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "'activation'", "not", "in", "state", ":", "\n", "            ", "state", "[", "'activation'", "]", "=", "F", ".", "relu", "\n", "", "super", "(", "TransformerEncoderLayer", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerEncoderLayer.forward": [[327, 345], ["src_mask.repeat.repeat.repeat", "model.TransformerEncoderLayer.norm1", "model.TransformerEncoderLayer.linear2", "model.TransformerEncoderLayer.norm2", "model.TransformerEncoderLayer.self_attn", "model.TransformerEncoderLayer.dropout1", "model.TransformerEncoderLayer.dropout", "model.TransformerEncoderLayer.dropout2", "model.TransformerEncoderLayer.activation", "model.TransformerEncoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "src", ":", "Tensor", ",", "src_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "src_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Pass the input through the encoder layer.\n        Args:\n            src: the sequence to the encoder layer (required).\n            src_mask: the mask for the src sequence (optional).\n            src_key_padding_mask: the mask for the src keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "src_mask", "=", "src_mask", ".", "repeat", "(", "self", ".", "self_attn", ".", "num_heads", ",", "1", ",", "1", ")", "\n", "src2", "=", "self", ".", "self_attn", "(", "src", ",", "src", ",", "src", ",", "attn_mask", "=", "src_mask", ",", "\n", "key_padding_mask", "=", "src_key_padding_mask", ")", "[", "0", "]", "\n", "src", "=", "src", "+", "self", ".", "dropout1", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm1", "(", "src", ")", "\n", "src2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "src", ")", ")", ")", ")", "\n", "src", "=", "src", "+", "self", ".", "dropout2", "(", "src2", ")", "\n", "src", "=", "self", ".", "norm2", "(", "src", ")", "\n", "return", "src", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__init__": [[368, 385], ["torch.nn.modules.module.Module.__init__", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.activation.MultiheadAttention", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.linear.Linear", "torch.nn.modules.linear.Linear", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.normalization.LayerNorm", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "torch.nn.modules.dropout.Dropout", "model._get_activation_fn"], "methods", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__init__", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model._get_activation_fn"], ["def", "__init__", "(", "self", ",", "d_model", ",", "nhead", ",", "dim_feedforward", "=", "2048", ",", "dropout", "=", "0.1", ",", "activation", "=", "\"relu\"", ",", "layer_norm_eps", "=", "1e-5", ")", ":", "\n", "        ", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "self", ".", "multihead_attn", "=", "MultiheadAttention", "(", "d_model", ",", "nhead", ",", "dropout", "=", "dropout", ")", "\n", "# Implementation of Feedforward model", "\n", "self", ".", "linear1", "=", "Linear", "(", "d_model", ",", "dim_feedforward", ")", "\n", "self", ".", "dropout", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "linear2", "=", "Linear", "(", "dim_feedforward", ",", "d_model", ")", "\n", "\n", "self", ".", "norm1", "=", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "self", ".", "norm2", "=", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "self", ".", "norm3", "=", "LayerNorm", "(", "d_model", ",", "eps", "=", "layer_norm_eps", ")", "\n", "self", ".", "dropout1", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout2", "=", "Dropout", "(", "dropout", ")", "\n", "self", ".", "dropout3", "=", "Dropout", "(", "dropout", ")", "\n", "\n", "self", ".", "activation", "=", "_get_activation_fn", "(", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__setstate__": [[386, 390], ["super().__setstate__"], "methods", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.__setstate__"], ["", "def", "__setstate__", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "'activation'", "not", "in", "state", ":", "\n", "            ", "state", "[", "'activation'", "]", "=", "F", ".", "relu", "\n", "", "super", "(", "TransformerDecoderLayer", ",", "self", ")", ".", "__setstate__", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model.TransformerDecoderLayer.forward": [[391, 417], ["memory_mask.repeat.repeat.repeat", "model.TransformerDecoderLayer.norm1", "model.TransformerDecoderLayer.norm2", "model.TransformerDecoderLayer.linear2", "model.TransformerDecoderLayer.norm3", "model.TransformerDecoderLayer.self_attn", "model.TransformerDecoderLayer.dropout1", "model.TransformerDecoderLayer.multihead_attn", "model.TransformerDecoderLayer.dropout2", "model.TransformerDecoderLayer.dropout", "model.TransformerDecoderLayer.dropout3", "model.TransformerDecoderLayer.activation", "model.TransformerDecoderLayer.linear1"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "tgt", ":", "Tensor", ",", "memory", ":", "Tensor", ",", "tgt_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "memory_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "\n", "tgt_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ",", "memory_key_padding_mask", ":", "Optional", "[", "Tensor", "]", "=", "None", ")", "->", "Tensor", ":", "\n", "        ", "r\"\"\"Pass the inputs (and mask) through the decoder layer.\n        Args:\n            tgt: the sequence to the decoder layer (required).\n            memory: the sequence from the last layer of the encoder (required).\n            tgt_mask: the mask for the tgt sequence (optional).\n            memory_mask: the mask for the memory sequence (optional).\n            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).\n            memory_key_padding_mask: the mask for the memory keys per batch (optional).\n        Shape:\n            see the docs in Transformer class.\n        \"\"\"", "\n", "memory_mask", "=", "memory_mask", ".", "repeat", "(", "self", ".", "self_attn", ".", "num_heads", ",", "1", ",", "1", ")", "\n", "tgt2", "=", "self", ".", "self_attn", "(", "tgt", ",", "tgt", ",", "tgt", ",", "attn_mask", "=", "tgt_mask", ",", "\n", "key_padding_mask", "=", "tgt_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout1", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm1", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "multihead_attn", "(", "tgt", ",", "memory", ",", "memory", ",", "attn_mask", "=", "memory_mask", ",", "\n", "key_padding_mask", "=", "memory_key_padding_mask", ")", "[", "0", "]", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout2", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm2", "(", "tgt", ")", "\n", "tgt2", "=", "self", ".", "linear2", "(", "self", ".", "dropout", "(", "self", ".", "activation", "(", "self", ".", "linear1", "(", "tgt", ")", ")", ")", ")", "\n", "tgt", "=", "tgt", "+", "self", ".", "dropout3", "(", "tgt2", ")", "\n", "tgt", "=", "self", ".", "norm3", "(", "tgt", ")", "\n", "return", "tgt", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model._get_clones": [[419, 421], ["torch.nn.modules.container.ModuleList", "copy.deepcopy", "range"], "function", ["None"], ["", "", "def", "_get_clones", "(", "module", ",", "N", ")", ":", "\n", "    ", "return", "ModuleList", "(", "[", "copy", ".", "deepcopy", "(", "module", ")", "for", "i", "in", "range", "(", "N", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.model._get_activation_fn": [[423, 430], ["RuntimeError"], "function", ["None"], ["", "def", "_get_activation_fn", "(", "activation", ")", ":", "\n", "    ", "if", "activation", "==", "\"relu\"", ":", "\n", "        ", "return", "F", ".", "relu", "\n", "", "elif", "activation", "==", "\"gelu\"", ":", "\n", "        ", "return", "F", ".", "gelu", "\n", "\n", "", "raise", "RuntimeError", "(", "\"activation should be relu/gelu, not {}\"", ".", "format", "(", "activation", ")", ")", "", "", ""]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks.gen_encoder_ut_mask": [[17, 23], ["hier_masks.gen_encoder_ut_mask._gen_mask_hierarchical"], "function", ["None"], ["def", "gen_encoder_ut_mask", "(", "src_seq", ",", "input_mask", ",", "utt_loc", ")", ":", "\n", "    ", "def", "_gen_mask_hierarchical", "(", "A", ",", "src_pad_mask", ")", ":", "\n", "# A: (bs, 100, 100); 100 is max_len*2 same as input_ids", "\n", "        ", "return", "~", "(", "2", "*", "A", "==", "(", "A", "+", "A", ".", "transpose", "(", "1", ",", "2", ")", ")", ")", ".", "bool", "(", ")", "\n", "", "enc_mask_utt", "=", "_gen_mask_hierarchical", "(", "utt_loc", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "src_seq", ".", "shape", "[", "1", "]", ",", "-", "1", ")", ",", "input_mask", ")", "\n", "return", "enc_mask_utt", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._get_pe_inputs": [[24, 30], ["torch.zeros", "range"], "function", ["None"], ["", "def", "_get_pe_inputs", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", ":", "\n", "    ", "pe_utt_loc", "=", "torch", ".", "zeros", "(", "utt_loc", ".", "shape", ",", "device", "=", "utt_loc", ".", "device", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "utt_loc", ".", "shape", "[", "1", "]", ")", ":", "# time", "\n", "        ", "_logic", "=", "(", "utt_loc", "[", ":", ",", "i", "]", "==", "utt_loc", "[", ":", ",", "i", "-", "1", "]", ")", ".", "float", "(", ")", "\n", "pe_utt_loc", "[", ":", ",", "i", "]", "=", "pe_utt_loc", "[", ":", ",", "i", "-", "1", "]", "+", "_logic", "-", "(", "1", "-", "_logic", ")", "*", "pe_utt_loc", "[", ":", ",", "i", "-", "1", "]", "\n", "", "return", "pe_utt_loc", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._HIER_masks": [[32, 54], ["hier_masks._get_pe_inputs", "hier_masks.gen_encoder_ut_mask", "_x.expand", "_x.expand.transpose", "utt_loc.max"], "function", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._get_pe_inputs", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks.gen_encoder_ut_mask"], ["", "def", "_HIER_masks", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", ":", "\n", "# HT-Encoder", "\n", "    ", "pe_utt_loc", "=", "_get_pe_inputs", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", "\n", "\n", "# UT-MASK", "\n", "enc_mask_utt", "=", "gen_encoder_ut_mask", "(", "src_seq", ",", "input_mask", ",", "utt_loc", ")", "\n", "\n", "# CT-Mask HIER style", "\n", "# Note: The sequence is all utterance followed by kb entries", "\n", "# We attend to the final *utterance*", "\n", "_x", "=", "(", "utt_loc", "==", "(", "utt_loc", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", ".", "values", ")", ")", ".", "unsqueeze", "(", "1", ")", "\n", "final_utt_check", "=", "_x", ".", "expand", "(", "-", "1", ",", "src_seq", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "#     print(utt_loc[0:5])", "\n", "#     print(final_utt_check[0:5, 0, :]*1)", "\n", "\n", "enc_mask_ct", "=", "(", "~", "(", "(", "~", "enc_mask_utt", ")", "|", "final_utt_check", ".", "transpose", "(", "1", ",", "2", ")", ")", ")", "# Real HRED", "\n", "\n", "# For HIER style", "\n", "# dec_enc_attn_mask = input_mask.unsqueeze(1).expand(-1, tgt_seq.shape[1], -1)", "\n", "dec_enc_attn_mask", "=", "(", "~", "_x", ")", ".", "expand", "(", "-", "1", ",", "tgt_seq", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "# print(dec_enc_attn_mask.shape)", "\n", "return", "pe_utt_loc", ",", "enc_mask_utt", ",", "enc_mask_ct", ",", "dec_enc_attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._CLS_masks": [[56, 70], ["hier_masks._get_pe_inputs", "hier_masks.gen_encoder_ut_mask"], "function", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._get_pe_inputs", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks.gen_encoder_ut_mask"], ["", "def", "_CLS_masks", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", ":", "\n", "# HT-Encoder", "\n", "    ", "pe_utt_loc", "=", "_get_pe_inputs", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", "\n", "\n", "# UT-MASK", "\n", "enc_mask_utt", "=", "gen_encoder_ut_mask", "(", "src_seq", ",", "input_mask", ",", "utt_loc", ")", "\n", "\n", "# CT-MASK", "\n", "enc_mask_ct", "=", "(", "(", "pe_utt_loc", "+", "input_mask", ")", "!=", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "src_seq", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "# HIER-CLS style", "\n", "\n", "# For HIER-CLS style", "\n", "dec_enc_attn_mask", "=", "(", "(", "pe_utt_loc", "+", "input_mask", ")", "!=", "0", ")", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "tgt_seq", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "\n", "return", "pe_utt_loc", ",", "enc_mask_utt", ",", "enc_mask_ct", ",", "dec_enc_attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._FULL_masks": [[72, 86], ["hier_masks._get_pe_inputs", "hier_masks.gen_encoder_ut_mask", "input_mask.unsqueeze().expand", "input_mask.unsqueeze().expand", "input_mask.unsqueeze", "input_mask.unsqueeze"], "function", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._get_pe_inputs", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks.gen_encoder_ut_mask"], ["", "def", "_FULL_masks", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", ":", "\n", "# HT-Encoder", "\n", "    ", "pe_utt_loc", "=", "_get_pe_inputs", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", "\n", "\n", "# UT-MASK", "\n", "# enc_mask = input_mask.unsqueeze(1).expand(-1, src_seq.shape[1], -1)", "\n", "enc_mask_utt", "=", "gen_encoder_ut_mask", "(", "src_seq", ",", "input_mask", ",", "utt_loc", ")", "\n", "\n", "# CT-MASK", "\n", "enc_mask_ct", "=", "input_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "src_seq", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "\n", "dec_enc_attn_mask", "=", "input_mask", ".", "unsqueeze", "(", "1", ")", ".", "expand", "(", "-", "1", ",", "tgt_seq", ".", "shape", "[", "1", "]", ",", "-", "1", ")", "\n", "\n", "return", "pe_utt_loc", ",", "enc_mask_utt", ",", "enc_mask_ct", ",", "dec_enc_attn_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks.get_hier_encoder_mask": [[88, 105], ["hier_masks._HIER_masks", "hier_masks._CLS_masks", "hier_masks._FULL_masks"], "function", ["home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._HIER_masks", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._CLS_masks", "home.repos.pwc.inspect_result.bsantraigi_hier-transformer-pytorch.hier_transformer_pytorch.hier_masks._FULL_masks"], ["", "def", "get_hier_encoder_mask", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ",", "type", ":", "str", ")", ":", "\n", "# Padding correction", "\n", "# No token other than padding should attend to padding", "\n", "# But padding needs to attend to padding tokens for numerical stability reasons", "\n", "    ", "utt_loc", "=", "utt_loc", "-", "2", "*", "input_mask", "*", "utt_loc", "\n", "\n", "# CT-Mask type", "\n", "assert", "type", "in", "[", "\"hier\"", ",", "\"cls\"", ",", "\"full\"", "]", "\n", "\n", "if", "type", "==", "\"hier\"", ":", "# HIER: Context through final utterance", "\n", "        ", "return", "_HIER_masks", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", "\n", "", "elif", "type", "==", "\"cls\"", ":", "# HIER-CLS: Context through cls tokens", "\n", "        ", "return", "_CLS_masks", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", "\n", "", "elif", "type", "==", "\"full\"", ":", "# Ut-mask only, CT-mask: Full attention", "\n", "        ", "return", "_FULL_masks", "(", "tgt_seq", ",", "src_seq", ",", "input_mask", ",", "utt_loc", ")", "\n", "\n", "", "return", "None", "", "", ""]]}