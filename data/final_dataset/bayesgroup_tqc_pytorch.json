{"home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.None.main.main": [[19, 84], ["tqc.structures.RescaleAction", "tqc.structures.RescaleAction", "tqc.structures.ReplayBuffer", "tqc.structures.Actor().to", "tqc.structures.Critic().to", "copy.deepcopy", "tqc.trainer.Trainer", "Actor().to.train", "range", "gym.make", "gym.make", "tqc.structures.RescaleAction.reset", "int", "Actor().to.select_action", "tqc.structures.RescaleAction.step", "structures.ReplayBuffer.add", "tqc.structures.Actor", "tqc.structures.Critic", "tqc.trainer.Trainer.train", "print", "evaluations.append", "numpy.save", "numpy.prod().item", "tqc.structures.RescaleAction.reset", "tqc.functions.eval_policy", "tqc.trainer.Trainer.save", "numpy.prod"], "function", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.train", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Actor.select_action", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.ReplayBuffer.add", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.train", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.functions.eval_policy", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save"], ["def", "main", "(", "args", ",", "results_dir", ",", "models_dir", ",", "prefix", ")", ":", "\n", "# --- Init ---", "\n", "\n", "# remove TimeLimit", "\n", "    ", "env", "=", "gym", ".", "make", "(", "args", ".", "env", ")", ".", "unwrapped", "\n", "eval_env", "=", "gym", ".", "make", "(", "args", ".", "env", ")", ".", "unwrapped", "\n", "\n", "env", "=", "RescaleAction", "(", "env", ",", "-", "1.", ",", "1.", ")", "\n", "eval_env", "=", "RescaleAction", "(", "eval_env", ",", "-", "1.", ",", "1.", ")", "\n", "\n", "state_dim", "=", "env", ".", "observation_space", ".", "shape", "[", "0", "]", "\n", "action_dim", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "\n", "replay_buffer", "=", "structures", ".", "ReplayBuffer", "(", "state_dim", ",", "action_dim", ")", "\n", "actor", "=", "Actor", "(", "state_dim", ",", "action_dim", ")", ".", "to", "(", "DEVICE", ")", "\n", "critic", "=", "Critic", "(", "state_dim", ",", "action_dim", ",", "args", ".", "n_quantiles", ",", "args", ".", "n_nets", ")", ".", "to", "(", "DEVICE", ")", "\n", "critic_target", "=", "copy", ".", "deepcopy", "(", "critic", ")", "\n", "\n", "top_quantiles_to_drop", "=", "args", ".", "top_quantiles_to_drop_per_net", "*", "args", ".", "n_nets", "\n", "\n", "trainer", "=", "Trainer", "(", "actor", "=", "actor", ",", "\n", "critic", "=", "critic", ",", "\n", "critic_target", "=", "critic_target", ",", "\n", "top_quantiles_to_drop", "=", "top_quantiles_to_drop", ",", "\n", "discount", "=", "args", ".", "discount", ",", "\n", "tau", "=", "args", ".", "tau", ",", "\n", "target_entropy", "=", "-", "np", ".", "prod", "(", "env", ".", "action_space", ".", "shape", ")", ".", "item", "(", ")", ")", "\n", "\n", "evaluations", "=", "[", "]", "\n", "state", ",", "done", "=", "env", ".", "reset", "(", ")", ",", "False", "\n", "episode_return", "=", "0", "\n", "episode_timesteps", "=", "0", "\n", "episode_num", "=", "0", "\n", "\n", "actor", ".", "train", "(", ")", "\n", "for", "t", "in", "range", "(", "int", "(", "args", ".", "max_timesteps", ")", ")", ":", "\n", "        ", "action", "=", "actor", ".", "select_action", "(", "state", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "episode_timesteps", "+=", "1", "\n", "\n", "replay_buffer", ".", "add", "(", "state", ",", "action", ",", "next_state", ",", "reward", ",", "done", ")", "\n", "\n", "state", "=", "next_state", "\n", "episode_return", "+=", "reward", "\n", "\n", "# Train agent after collecting sufficient data", "\n", "if", "t", ">=", "args", ".", "batch_size", ":", "\n", "            ", "trainer", ".", "train", "(", "replay_buffer", ",", "args", ".", "batch_size", ")", "\n", "\n", "", "if", "done", "or", "episode_timesteps", ">=", "EPISODE_LENGTH", ":", "\n", "# +1 to account for 0 indexing. +0 on ep_timesteps since it will increment +1 even if done=True", "\n", "            ", "print", "(", "f\"Total T: {t + 1} Episode Num: {episode_num + 1} Episode T: {episode_timesteps} Reward: {episode_return:.3f}\"", ")", "\n", "# Reset environment", "\n", "state", ",", "done", "=", "env", ".", "reset", "(", ")", ",", "False", "\n", "\n", "episode_return", "=", "0", "\n", "episode_timesteps", "=", "0", "\n", "episode_num", "+=", "1", "\n", "\n", "# Evaluate episode", "\n", "", "if", "(", "t", "+", "1", ")", "%", "args", ".", "eval_freq", "==", "0", ":", "\n", "            ", "file_name", "=", "f\"{prefix}_{args.env}_{args.seed}\"", "\n", "evaluations", ".", "append", "(", "eval_policy", "(", "actor", ",", "eval_env", ",", "EPISODE_LENGTH", ")", ")", "\n", "np", ".", "save", "(", "results_dir", "/", "file_name", ",", "evaluations", ")", "\n", "if", "args", ".", "save_model", ":", "trainer", ".", "save", "(", "models_dir", "/", "file_name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.__init__": [[8, 37], ["torch.zeros", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "trainer.Trainer.actor.parameters", "trainer.Trainer.critic.parameters"], "methods", ["None"], ["\t", "def", "__init__", "(", "\n", "self", ",", "\n", "*", ",", "\n", "actor", ",", "\n", "critic", ",", "\n", "critic_target", ",", "\n", "discount", ",", "\n", "tau", ",", "\n", "top_quantiles_to_drop", ",", "\n", "target_entropy", ",", "\n", ")", ":", "\n", "\t\t", "self", ".", "actor", "=", "actor", "\n", "self", ".", "critic", "=", "critic", "\n", "self", ".", "critic_target", "=", "critic_target", "\n", "self", ".", "log_alpha", "=", "torch", ".", "zeros", "(", "(", "1", ",", ")", ",", "requires_grad", "=", "True", ",", "device", "=", "DEVICE", ")", "\n", "\n", "# TODO: check hyperparams", "\n", "self", ".", "actor_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "actor", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ")", "\n", "self", ".", "critic_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "critic", ".", "parameters", "(", ")", ",", "lr", "=", "3e-4", ")", "\n", "self", ".", "alpha_optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "[", "self", ".", "log_alpha", "]", ",", "lr", "=", "3e-4", ")", "\n", "\n", "self", ".", "discount", "=", "discount", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "top_quantiles_to_drop", "=", "top_quantiles_to_drop", "\n", "self", ".", "target_entropy", "=", "target_entropy", "\n", "\n", "self", ".", "quantiles_total", "=", "critic", ".", "n_quantiles", "*", "critic", ".", "n_nets", "\n", "\n", "self", ".", "total_it", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.train": [[38, 80], ["replay_buffer.sample", "torch.exp", "trainer.Trainer.critic", "tqc.functions.quantile_huber_loss_f", "trainer.Trainer.actor", "trainer.Trainer.critic_optimizer.zero_grad", "tqc.functions.quantile_huber_loss_f.backward", "trainer.Trainer.critic_optimizer.step", "zip", "trainer.Trainer.actor_optimizer.zero_grad", "actor_loss.backward", "trainer.Trainer.actor_optimizer.step", "trainer.Trainer.alpha_optimizer.zero_grad", "alpha_loss.backward", "trainer.Trainer.alpha_optimizer.step", "torch.no_grad", "trainer.Trainer.actor", "trainer.Trainer.critic_target", "torch.sort", "trainer.Trainer.critic.parameters", "trainer.Trainer.critic_target.parameters", "target_param.data.copy_", "trainer.Trainer.reshape", "trainer.Trainer.critic().mean().mean", "trainer.Trainer.critic().mean", "trainer.Trainer.critic"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.ReplayBuffer.sample", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.functions.quantile_huber_loss_f"], ["", "def", "train", "(", "self", ",", "replay_buffer", ",", "batch_size", "=", "256", ")", ":", "\n", "\t\t", "state", ",", "action", ",", "next_state", ",", "reward", ",", "not_done", "=", "replay_buffer", ".", "sample", "(", "batch_size", ")", "\n", "alpha", "=", "torch", ".", "exp", "(", "self", ".", "log_alpha", ")", "\n", "\n", "# --- Q loss ---", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# get policy action", "\n", "\t\t\t", "new_next_action", ",", "next_log_pi", "=", "self", ".", "actor", "(", "next_state", ")", "\n", "\n", "# compute and cut quantiles at the next state", "\n", "next_z", "=", "self", ".", "critic_target", "(", "next_state", ",", "new_next_action", ")", "# batch x nets x quantiles", "\n", "sorted_z", ",", "_", "=", "torch", ".", "sort", "(", "next_z", ".", "reshape", "(", "batch_size", ",", "-", "1", ")", ")", "\n", "sorted_z_part", "=", "sorted_z", "[", ":", ",", ":", "self", ".", "quantiles_total", "-", "self", ".", "top_quantiles_to_drop", "]", "\n", "\n", "# compute target", "\n", "target", "=", "reward", "+", "not_done", "*", "self", ".", "discount", "*", "(", "sorted_z_part", "-", "alpha", "*", "next_log_pi", ")", "\n", "\n", "", "cur_z", "=", "self", ".", "critic", "(", "state", ",", "action", ")", "\n", "critic_loss", "=", "quantile_huber_loss_f", "(", "cur_z", ",", "target", ")", "\n", "\n", "# --- Policy and alpha loss ---", "\n", "new_action", ",", "log_pi", "=", "self", ".", "actor", "(", "state", ")", "\n", "alpha_loss", "=", "-", "self", ".", "log_alpha", "*", "(", "log_pi", "+", "self", ".", "target_entropy", ")", ".", "detach", "(", ")", ".", "mean", "(", ")", "\n", "actor_loss", "=", "(", "alpha", "*", "log_pi", "-", "self", ".", "critic", "(", "state", ",", "new_action", ")", ".", "mean", "(", "2", ")", ".", "mean", "(", "1", ",", "keepdim", "=", "True", ")", ")", ".", "mean", "(", ")", "\n", "\n", "# --- Update ---", "\n", "self", ".", "critic_optimizer", ".", "zero_grad", "(", ")", "\n", "critic_loss", ".", "backward", "(", ")", "\n", "self", ".", "critic_optimizer", ".", "step", "(", ")", "\n", "\n", "for", "param", ",", "target_param", "in", "zip", "(", "self", ".", "critic", ".", "parameters", "(", ")", ",", "self", ".", "critic_target", ".", "parameters", "(", ")", ")", ":", "\n", "\t\t\t", "target_param", ".", "data", ".", "copy_", "(", "self", ".", "tau", "*", "param", ".", "data", "+", "(", "1", "-", "self", ".", "tau", ")", "*", "target_param", ".", "data", ")", "\n", "\n", "", "self", ".", "actor_optimizer", ".", "zero_grad", "(", ")", "\n", "actor_loss", ".", "backward", "(", ")", "\n", "self", ".", "actor_optimizer", ".", "step", "(", ")", "\n", "\n", "self", ".", "alpha_optimizer", ".", "zero_grad", "(", ")", "\n", "alpha_loss", ".", "backward", "(", ")", "\n", "self", ".", "alpha_optimizer", ".", "step", "(", ")", "\n", "\n", "self", ".", "total_it", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save": [[81, 90], ["str", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "trainer.Trainer.critic.state_dict", "trainer.Trainer.critic_target.state_dict", "trainer.Trainer.critic_optimizer.state_dict", "trainer.Trainer.actor.state_dict", "trainer.Trainer.actor_optimizer.state_dict", "trainer.Trainer.alpha_optimizer.state_dict"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.save"], ["", "def", "save", "(", "self", ",", "filename", ")", ":", "\n", "\t\t", "filename", "=", "str", "(", "filename", ")", "\n", "torch", ".", "save", "(", "self", ".", "critic", ".", "state_dict", "(", ")", ",", "filename", "+", "\"_critic\"", ")", "\n", "torch", ".", "save", "(", "self", ".", "critic_target", ".", "state_dict", "(", ")", ",", "filename", "+", "\"_critic_target\"", ")", "\n", "torch", ".", "save", "(", "self", ".", "critic_optimizer", ".", "state_dict", "(", ")", ",", "filename", "+", "\"_critic_optimizer\"", ")", "\n", "torch", ".", "save", "(", "self", ".", "actor", ".", "state_dict", "(", ")", ",", "filename", "+", "\"_actor\"", ")", "\n", "torch", ".", "save", "(", "self", ".", "actor_optimizer", ".", "state_dict", "(", ")", ",", "filename", "+", "\"_actor_optimizer\"", ")", "\n", "torch", ".", "save", "(", "self", ".", "log_alpha", ",", "filename", "+", "'_log_alpha'", ")", "\n", "torch", ".", "save", "(", "self", ".", "alpha_optimizer", ".", "state_dict", "(", ")", ",", "filename", "+", "\"_alpha_optimizer\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.load": [[91, 100], ["str", "trainer.Trainer.critic.load_state_dict", "trainer.Trainer.critic_target.load_state_dict", "trainer.Trainer.critic_optimizer.load_state_dict", "trainer.Trainer.actor.load_state_dict", "trainer.Trainer.actor_optimizer.load_state_dict", "torch.load", "trainer.Trainer.alpha_optimizer.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.load", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.load", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.load", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.load", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.load", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.load", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.load"], ["", "def", "load", "(", "self", ",", "filename", ")", ":", "\n", "\t\t", "filename", "=", "str", "(", "filename", ")", "\n", "self", ".", "critic", ".", "load_state_dict", "(", "torch", ".", "load", "(", "filename", "+", "\"_critic\"", ")", ")", "\n", "self", ".", "critic_target", ".", "load_state_dict", "(", "torch", ".", "load", "(", "filename", "+", "\"_critic_target\"", ")", ")", "\n", "self", ".", "critic_optimizer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "filename", "+", "\"_critic_optimizer\"", ")", ")", "\n", "self", ".", "actor", ".", "load_state_dict", "(", "torch", ".", "load", "(", "filename", "+", "\"_actor\"", ")", ")", "\n", "self", ".", "actor_optimizer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "filename", "+", "\"_actor_optimizer\"", ")", ")", "\n", "self", ".", "log_alpha", "=", "torch", ".", "load", "(", "filename", "+", "'_log_alpha'", ")", "\n", "self", ".", "alpha_optimizer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "filename", "+", "\"_alpha_optimizer\"", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.functions.eval_policy": [[6, 20], ["policy.eval", "range", "policy.train", "eval_env.reset", "policy.select_action", "eval_env.step"], "function", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.trainer.Trainer.train", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Actor.select_action"], ["def", "eval_policy", "(", "policy", ",", "eval_env", ",", "max_episode_steps", ",", "eval_episodes", "=", "10", ")", ":", "\n", "    ", "policy", ".", "eval", "(", ")", "\n", "avg_reward", "=", "0.", "\n", "for", "_", "in", "range", "(", "eval_episodes", ")", ":", "\n", "        ", "state", ",", "done", "=", "eval_env", ".", "reset", "(", ")", ",", "False", "\n", "t", "=", "0", "\n", "while", "not", "done", "and", "t", "<", "max_episode_steps", ":", "\n", "            ", "action", "=", "policy", ".", "select_action", "(", "state", ")", "\n", "state", ",", "reward", ",", "done", ",", "_", "=", "eval_env", ".", "step", "(", "action", ")", "\n", "avg_reward", "+=", "reward", "\n", "t", "+=", "1", "\n", "", "", "avg_reward", "/=", "eval_episodes", "\n", "policy", ".", "train", "(", ")", "\n", "return", "avg_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.functions.quantile_huber_loss_f": [[22, 33], ["torch.abs", "torch.where", "torch.arange().float", "torch.abs", "torch.arange"], "function", ["None"], ["", "def", "quantile_huber_loss_f", "(", "quantiles", ",", "samples", ")", ":", "\n", "    ", "pairwise_delta", "=", "samples", "[", ":", ",", "None", ",", "None", ",", ":", "]", "-", "quantiles", "[", ":", ",", ":", ",", ":", ",", "None", "]", "# batch x nets x quantiles x samples", "\n", "abs_pairwise_delta", "=", "torch", ".", "abs", "(", "pairwise_delta", ")", "\n", "huber_loss", "=", "torch", ".", "where", "(", "abs_pairwise_delta", ">", "1", ",", "\n", "abs_pairwise_delta", "-", "0.5", ",", "\n", "pairwise_delta", "**", "2", "*", "0.5", ")", "\n", "\n", "n_quantiles", "=", "quantiles", ".", "shape", "[", "2", "]", "\n", "tau", "=", "torch", ".", "arange", "(", "n_quantiles", ",", "device", "=", "DEVICE", ")", ".", "float", "(", ")", "/", "n_quantiles", "+", "1", "/", "2", "/", "n_quantiles", "\n", "loss", "=", "(", "torch", ".", "abs", "(", "tau", "[", "None", ",", "None", ",", ":", ",", "None", "]", "-", "(", "pairwise_delta", "<", "0", ")", ".", "float", "(", ")", ")", "*", "huber_loss", ")", ".", "mean", "(", ")", "\n", "return", "loss", "\n", "", ""]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.RescaleAction.__init__": [[17, 25], ["isinstance", "numpy.less_equal().all", "gym.ActionWrapper.__init__", "gym.spaces.Box", "type", "numpy.zeros", "numpy.zeros", "numpy.less_equal"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "a", ",", "b", ")", ":", "\n", "        ", "assert", "isinstance", "(", "env", ".", "action_space", ",", "spaces", ".", "Box", ")", ",", "(", "\n", "\"expected Box action space, got {}\"", ".", "format", "(", "type", "(", "env", ".", "action_space", ")", ")", ")", "\n", "assert", "np", ".", "less_equal", "(", "a", ",", "b", ")", ".", "all", "(", ")", ",", "(", "a", ",", "b", ")", "\n", "super", "(", "RescaleAction", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "a", "=", "np", ".", "zeros", "(", "env", ".", "action_space", ".", "shape", ",", "dtype", "=", "env", ".", "action_space", ".", "dtype", ")", "+", "a", "\n", "self", ".", "b", "=", "np", ".", "zeros", "(", "env", ".", "action_space", ".", "shape", ",", "dtype", "=", "env", ".", "action_space", ".", "dtype", ")", "+", "b", "\n", "self", ".", "action_space", "=", "spaces", ".", "Box", "(", "low", "=", "a", ",", "high", "=", "b", ",", "shape", "=", "env", ".", "action_space", ".", "shape", ",", "dtype", "=", "env", ".", "action_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.RescaleAction.action": [[26, 34], ["numpy.all", "numpy.all", "numpy.clip", "numpy.greater_equal", "numpy.less_equal"], "methods", ["None"], ["", "def", "action", "(", "self", ",", "action", ")", ":", "\n", "        ", "assert", "np", ".", "all", "(", "np", ".", "greater_equal", "(", "action", ",", "self", ".", "a", ")", ")", ",", "(", "action", ",", "self", ".", "a", ")", "\n", "assert", "np", ".", "all", "(", "np", ".", "less_equal", "(", "action", ",", "self", ".", "b", ")", ")", ",", "(", "action", ",", "self", ".", "b", ")", "\n", "low", "=", "self", ".", "env", ".", "action_space", ".", "low", "\n", "high", "=", "self", ".", "env", ".", "action_space", ".", "high", "\n", "action", "=", "low", "+", "(", "high", "-", "low", ")", "*", "(", "(", "action", "-", "self", ".", "a", ")", "/", "(", "self", ".", "b", "-", "self", ".", "a", ")", ")", "\n", "action", "=", "np", ".", "clip", "(", "action", ",", "low", ",", "high", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Mlp.__init__": [[37, 53], ["torch.nn.Module.__init__", "enumerate", "torch.nn.Linear", "torch.nn.Linear", "structures.Mlp.add_module", "structures.Mlp.fcs.append"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "input_size", ",", "\n", "hidden_sizes", ",", "\n", "output_size", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# TODO: initialization", "\n", "self", ".", "fcs", "=", "[", "]", "\n", "in_size", "=", "input_size", "\n", "for", "i", ",", "next_size", "in", "enumerate", "(", "hidden_sizes", ")", ":", "\n", "            ", "fc", "=", "Linear", "(", "in_size", ",", "next_size", ")", "\n", "self", ".", "add_module", "(", "f'fc{i}'", ",", "fc", ")", "\n", "self", ".", "fcs", ".", "append", "(", "fc", ")", "\n", "in_size", "=", "next_size", "\n", "", "self", ".", "last_fc", "=", "Linear", "(", "in_size", ",", "output_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Mlp.forward": [[54, 60], ["structures.Mlp.last_fc", "torch.nn.functional.relu", "fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "h", "=", "input", "\n", "for", "fc", "in", "self", ".", "fcs", ":", "\n", "            ", "h", "=", "relu", "(", "fc", "(", "h", ")", ")", "\n", "", "output", "=", "self", ".", "last_fc", "(", "h", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.ReplayBuffer.__init__": [[63, 72], ["int", "zip", "setattr", "numpy.empty"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "max_size", "=", "int", "(", "1e6", ")", ")", ":", "\n", "        ", "self", ".", "max_size", "=", "max_size", "\n", "self", ".", "ptr", "=", "0", "\n", "self", ".", "size", "=", "0", "\n", "\n", "self", ".", "transition_names", "=", "(", "'state'", ",", "'action'", ",", "'next_state'", ",", "'reward'", ",", "'not_done'", ")", "\n", "sizes", "=", "(", "state_dim", ",", "action_dim", ",", "state_dim", ",", "1", ",", "1", ")", "\n", "for", "name", ",", "size", "in", "zip", "(", "self", ".", "transition_names", ",", "sizes", ")", ":", "\n", "            ", "setattr", "(", "self", ",", "name", ",", "np", ".", "empty", "(", "(", "max_size", ",", "size", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.ReplayBuffer.add": [[73, 80], ["zip", "min", "getattr"], "methods", ["None"], ["", "", "def", "add", "(", "self", ",", "state", ",", "action", ",", "next_state", ",", "reward", ",", "done", ")", ":", "\n", "        ", "values", "=", "(", "state", ",", "action", ",", "next_state", ",", "reward", ",", "1.", "-", "done", ")", "\n", "for", "name", ",", "value", "in", "zip", "(", "self", ".", "transition_names", ",", "values", ")", ":", "\n", "            ", "getattr", "(", "self", ",", "name", ")", "[", "self", ".", "ptr", "]", "=", "value", "\n", "\n", "", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.ReplayBuffer.sample": [[81, 85], ["numpy.random.randint", "torch.FloatTensor().to", "torch.FloatTensor", "getattr"], "methods", ["None"], ["", "def", "sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "ind", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "names", "=", "self", ".", "transition_names", "\n", "return", "(", "torch", ".", "FloatTensor", "(", "getattr", "(", "self", ",", "name", ")", "[", "ind", "]", ")", ".", "to", "(", "DEVICE", ")", "for", "name", "in", "names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Critic.__init__": [[88, 97], ["torch.nn.Module.__init__", "range", "structures.Mlp", "structures.Critic.add_module", "structures.Critic.nets.append"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ",", "n_quantiles", ",", "n_nets", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "nets", "=", "[", "]", "\n", "self", ".", "n_quantiles", "=", "n_quantiles", "\n", "self", ".", "n_nets", "=", "n_nets", "\n", "for", "i", "in", "range", "(", "n_nets", ")", ":", "\n", "            ", "net", "=", "Mlp", "(", "state_dim", "+", "action_dim", ",", "[", "512", ",", "512", ",", "512", "]", ",", "n_quantiles", ")", "\n", "self", ".", "add_module", "(", "f'qf{i}'", ",", "net", ")", "\n", "self", ".", "nets", ".", "append", "(", "net", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Critic.forward": [[98, 102], ["torch.cat", "torch.stack", "tuple", "net"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "sa", "=", "torch", ".", "cat", "(", "(", "state", ",", "action", ")", ",", "dim", "=", "1", ")", "\n", "quantiles", "=", "torch", ".", "stack", "(", "tuple", "(", "net", "(", "sa", ")", "for", "net", "in", "self", ".", "nets", ")", ",", "dim", "=", "1", ")", "\n", "return", "quantiles", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Actor.__init__": [[105, 109], ["torch.nn.Module.__init__", "structures.Mlp"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.__init__"], ["    ", "def", "__init__", "(", "self", ",", "state_dim", ",", "action_dim", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "action_dim", "=", "action_dim", "\n", "self", ".", "net", "=", "Mlp", "(", "state_dim", ",", "[", "256", ",", "256", "]", ",", "2", "*", "action_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Actor.forward": [[110, 124], ["structures.Actor.net().split", "log_std.clamp.clamp.clamp", "torch.exp", "structures.TanhNormal", "structures.TanhNormal.rsample", "structures.TanhNormal.log_prob", "log_prob.sum.sum.sum", "torch.tanh", "structures.Actor.net"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.rsample", "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.log_prob"], ["", "def", "forward", "(", "self", ",", "obs", ")", ":", "\n", "        ", "mean", ",", "log_std", "=", "self", ".", "net", "(", "obs", ")", ".", "split", "(", "[", "self", ".", "action_dim", ",", "self", ".", "action_dim", "]", ",", "dim", "=", "1", ")", "\n", "log_std", "=", "log_std", ".", "clamp", "(", "*", "LOG_STD_MIN_MAX", ")", "\n", "\n", "if", "self", ".", "training", ":", "\n", "            ", "std", "=", "torch", ".", "exp", "(", "log_std", ")", "\n", "tanh_normal", "=", "TanhNormal", "(", "mean", ",", "std", ")", "\n", "action", ",", "pre_tanh", "=", "tanh_normal", ".", "rsample", "(", ")", "\n", "log_prob", "=", "tanh_normal", ".", "log_prob", "(", "pre_tanh", ")", "\n", "log_prob", "=", "log_prob", ".", "sum", "(", "dim", "=", "1", ",", "keepdim", "=", "True", ")", "\n", "", "else", ":", "# deterministic eval without log_prob computation", "\n", "            ", "action", "=", "torch", ".", "tanh", "(", "mean", ")", "\n", "log_prob", "=", "None", "\n", "", "return", "action", ",", "log_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Actor.select_action": [[125, 130], ["structures.Actor.forward", "action[].cpu().detach().numpy", "torch.FloatTensor().to", "action[].cpu().detach", "torch.FloatTensor", "action[].cpu"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.Actor.forward"], ["", "def", "select_action", "(", "self", ",", "obs", ")", ":", "\n", "        ", "obs", "=", "torch", ".", "FloatTensor", "(", "obs", ")", ".", "to", "(", "DEVICE", ")", "[", "None", ",", ":", "]", "\n", "action", ",", "_", "=", "self", ".", "forward", "(", "obs", ")", "\n", "action", "=", "action", "[", "0", "]", ".", "cpu", "(", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.__init__": [[133, 140], ["torch.distributions.Distribution.__init__", "torch.distributions.Normal", "torch.distributions.Normal", "torch.zeros_like", "torch.ones_like"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.__init__"], ["    ", "def", "__init__", "(", "self", ",", "normal_mean", ",", "normal_std", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "normal_mean", "=", "normal_mean", "\n", "self", ".", "normal_std", "=", "normal_std", "\n", "self", ".", "standard_normal", "=", "Normal", "(", "torch", ".", "zeros_like", "(", "self", ".", "normal_mean", ",", "device", "=", "DEVICE", ")", ",", "\n", "torch", ".", "ones_like", "(", "self", ".", "normal_std", ",", "device", "=", "DEVICE", ")", ")", "\n", "self", ".", "normal", "=", "Normal", "(", "normal_mean", ",", "normal_std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.log_prob": [[141, 145], ["torch.nn.functional.logsigmoid", "structures.TanhNormal.normal.log_prob", "torch.nn.functional.logsigmoid", "numpy.log"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.log_prob"], ["", "def", "log_prob", "(", "self", ",", "pre_tanh", ")", ":", "\n", "        ", "log_det", "=", "2", "*", "np", ".", "log", "(", "2", ")", "+", "logsigmoid", "(", "2", "*", "pre_tanh", ")", "+", "logsigmoid", "(", "-", "2", "*", "pre_tanh", ")", "\n", "result", "=", "self", ".", "normal", ".", "log_prob", "(", "pre_tanh", ")", "-", "log_det", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.TanhNormal.rsample": [[146, 149], ["torch.tanh", "structures.TanhNormal.standard_normal.sample"], "methods", ["home.repos.pwc.inspect_result.bayesgroup_tqc_pytorch.tqc.structures.ReplayBuffer.sample"], ["", "def", "rsample", "(", "self", ")", ":", "\n", "        ", "pretanh", "=", "self", ".", "normal_mean", "+", "self", ".", "normal_std", "*", "self", ".", "standard_normal", ".", "sample", "(", ")", "\n", "return", "torch", ".", "tanh", "(", "pretanh", ")", ",", "pretanh", "\n", "", "", ""]]}