{"home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.test_retrieval.fiq_test": [[24, 128], ["model.eval", "tqdm.tqdm", "numpy.concatenate", "print", "enumerate", "numpy.concatenate", "print", "print", "range", "range", "np.concatenate.dot", "print", "range", "torch.cuda.empty_cache", "len", "tqdm.tqdm", "len", "len", "numpy.linalg.norm", "numpy.linalg.norm", "filtered_ids.append", "enumerate", "len", "len", "len", "len", "enumerate", "len", "testset.get_img", "torch.stack().float", "torch.autograd.Variable().cuda", "model.compose_img_text", "dct_with_representations[].data.cpu().numpy", "testset.get_img_from_split", "torch.stack().float", "torch.autograd.Variable().cuda", "model.extract_img_feature().data.cpu().numpy", "numpy.argsort", "range", "list", "len", "model.extract_img_feature().data.cpu().numpy.cuda", "len", "collections.OrderedDict.fromkeys", "target_caption.split", "things[].append", "things[].append", "len", "torch.stack", "torch.autograd.Variable", "dct_with_representations[].data.cpu", "len", "torch.stack", "torch.autograd.Variable", "model.extract_img_feature().data.cpu", "model.extract_img_feature", "str", "model.extract_img_feature().data.cpu().numpy.cuda", "all_target_captions[].split"], "function", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img_from_split", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_img_feature"], ["def", "fiq_test", "(", "opt", ",", "model", ",", "testset", ")", ":", "\n", "    ", "model", ".", "eval", "(", ")", "\n", "\n", "all_imgs", "=", "[", "]", "\n", "all_queries", "=", "[", "]", "\n", "all_target_captions", "=", "[", "]", "\n", "all_target_ids", "=", "[", "]", "\n", "\n", "imgs", "=", "[", "]", "\n", "mods", "=", "[", "]", "\n", "out", "=", "[", "]", "\n", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "len", "(", "testset", ")", ")", ")", ":", "\n", "        ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "item", "=", "testset", "[", "i", "]", "\n", "imgs", "+=", "[", "testset", ".", "get_img", "(", "item", "[", "'source_img_id'", "]", ")", "]", "\n", "\n", "all_target_captions", "+=", "[", "item", "[", "'target_caption'", "]", "]", "\n", "all_target_ids", "+=", "[", "item", "[", "'target_image_name'", "]", "]", "\n", "\n", "mods", "+=", "[", "item", "[", "'target_caption'", "]", "]", "\n", "\n", "if", "len", "(", "imgs", ")", ">=", "opt", ".", "batch_size", "or", "i", "==", "len", "(", "testset", ")", "-", "1", ":", "\n", "            ", "imgs", "=", "torch", ".", "stack", "(", "imgs", ")", ".", "float", "(", ")", "\n", "imgs", "=", "torch", ".", "autograd", ".", "Variable", "(", "imgs", ")", ".", "cuda", "(", ")", "\n", "\n", "dct_with_representations", "=", "model", ".", "compose_img_text", "(", "imgs", ".", "cuda", "(", ")", ",", "mods", ")", "\n", "f", "=", "dct_with_representations", "[", "\"repres\"", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "all_queries", "+=", "[", "f", "]", "\n", "\n", "imgs", "=", "[", "]", "\n", "mods", "=", "[", "]", "\n", "\n", "", "", "all_queries", "=", "np", ".", "concatenate", "(", "all_queries", ")", "\n", "print", "(", "\"all_queries len:\"", ",", "len", "(", "all_queries", ")", ")", "\n", "\n", "# compute all image features", "\n", "imgs", "=", "[", "]", "\n", "for", "i", ",", "original_image_id", "in", "enumerate", "(", "tqdm", "(", "testset", ".", "all_imgs_from_cat", ")", ")", ":", "\n", "        ", "imgs", "+=", "[", "testset", ".", "get_img_from_split", "(", "original_image_id", ")", "]", "\n", "if", "len", "(", "imgs", ")", ">=", "opt", ".", "batch_size", "or", "i", "==", "len", "(", "testset", ".", "all_imgs_from_cat", ")", "-", "1", ":", "\n", "            ", "imgs", "=", "torch", ".", "stack", "(", "imgs", ")", ".", "float", "(", ")", "\n", "imgs", "=", "torch", ".", "autograd", ".", "Variable", "(", "imgs", ")", ".", "cuda", "(", ")", "\n", "imgs", "=", "model", ".", "extract_img_feature", "(", "imgs", ".", "cuda", "(", ")", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "all_imgs", "+=", "[", "imgs", "]", "\n", "imgs", "=", "[", "]", "\n", "\n", "", "", "all_imgs", "=", "np", ".", "concatenate", "(", "all_imgs", ")", "\n", "\n", "print", "(", "\"all_imgs len:\"", ",", "len", "(", "all_imgs", ")", ")", "\n", "print", "(", "\"all_imgs_from_category len:\"", ",", "len", "(", "testset", ".", "all_imgs_from_cat", ")", ")", "\n", "\n", "# feature normalization", "\n", "for", "i", "in", "range", "(", "all_queries", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "all_queries", "[", "i", ",", ":", "]", "/=", "np", ".", "linalg", ".", "norm", "(", "all_queries", "[", "i", ",", ":", "]", ")", "\n", "", "for", "i", "in", "range", "(", "all_imgs", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "all_imgs", "[", "i", ",", ":", "]", "/=", "np", ".", "linalg", ".", "norm", "(", "all_imgs", "[", "i", ",", ":", "]", ")", "\n", "\n", "", "sims", "=", "all_queries", ".", "dot", "(", "all_imgs", ".", "T", ")", "\n", "print", "(", "\"sims shape: \"", ",", "sims", ".", "shape", ")", "\n", "\n", "nn_result", "=", "[", "np", ".", "argsort", "(", "-", "sims", "[", "i", ",", ":", "]", ")", "[", ":", "150", "]", "for", "i", "in", "range", "(", "sims", ".", "shape", "[", "0", "]", ")", "]", "# take more to remove duplicates", "\n", "nn_result_ids", "=", "[", "[", "testset", ".", "all_imgs_from_cat", "[", "nn", "]", "for", "nn", "in", "nns", "]", "for", "nns", "in", "nn_result", "]", "\n", "\n", "filtered_ids", "=", "[", "]", "\n", "for", "ranking_ids", "in", "nn_result_ids", ":", "\n", "        ", "filtered_id_50", "=", "list", "(", "OrderedDict", ".", "fromkeys", "(", "ranking_ids", ")", ")", "[", ":", "50", "]", "# filter duplicates and preserve order", "\n", "filtered_ids", ".", "append", "(", "filtered_id_50", ")", "\n", "\n", "", "if", "opt", ".", "category_to_train", "==", "'all'", ":", "\n", "        ", "cats_recalls", "=", "{", "'dress'", ":", "{", "'recall'", ":", "0.0", ",", "'num'", ":", "0", "}", ",", "\n", "'shirt'", ":", "{", "'recall'", ":", "0.0", ",", "'num'", ":", "0", "}", ",", "\n", "'toptee'", ":", "{", "'recall'", ":", "0.0", ",", "'num'", ":", "0", "}", "}", "\n", "\n", "things", "=", "{", "}", "\n", "\n", "for", "i", ",", "target_caption", "in", "enumerate", "(", "all_target_captions", ")", ":", "\n", "            ", "clothing", "=", "target_caption", ".", "split", "(", ")", "[", "0", "]", "\n", "if", "clothing", "in", "things", ":", "\n", "                ", "things", "[", "clothing", "]", ".", "append", "(", "{", "'orig_index'", ":", "i", ",", "'target_caption'", ":", "target_caption", "}", ")", "\n", "", "else", ":", "\n", "                ", "things", "[", "clothing", "]", "=", "[", "]", "\n", "things", "[", "clothing", "]", ".", "append", "(", "{", "'orig_index'", ":", "i", ",", "'target_caption'", ":", "target_caption", "}", ")", "\n", "\n", "", "", "cats_recalls", "[", "'dress'", "]", "[", "'num'", "]", "=", "len", "(", "things", "[", "'dress'", "]", ")", "\n", "cats_recalls", "[", "'shirt'", "]", "[", "'num'", "]", "=", "len", "(", "things", "[", "'shirt'", "]", ")", "\n", "cats_recalls", "[", "'toptee'", "]", "[", "'num'", "]", "=", "len", "(", "things", "[", "'toptee'", "]", ")", "\n", "", "else", ":", "\n", "        ", "cats_recalls", "=", "{", "opt", ".", "category_to_train", ":", "{", "'recall'", ":", "0.0", ",", "'num'", ":", "0", "}", "}", "\n", "cats_recalls", "[", "opt", ".", "category_to_train", "]", "[", "'num'", "]", "=", "len", "(", "testset", ")", "\n", "\n", "", "for", "k", "in", "[", "1", ",", "5", ",", "10", ",", "50", ",", "100", "]", ":", "\n", "        ", "for", "i", ",", "nns", "in", "enumerate", "(", "filtered_ids", ")", ":", "\n", "            ", "if", "all_target_ids", "[", "i", "]", "in", "nns", "[", ":", "k", "]", ":", "\n", "                ", "if", "opt", ".", "category_to_train", "==", "'all'", ":", "\n", "                    ", "cats_recalls", "[", "all_target_captions", "[", "i", "]", ".", "split", "(", ")", "[", "0", "]", "]", "[", "'recall'", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "cats_recalls", "[", "opt", ".", "category_to_train", "]", "[", "'recall'", "]", "+=", "1", "\n", "", "", "", "for", "cat", "in", "cats_recalls", ":", "\n", "            ", "cats_recalls", "[", "cat", "]", "[", "'recall'", "]", "/=", "cats_recalls", "[", "cat", "]", "[", "'num'", "]", "\n", "out", "+=", "[", "(", "'recall_top'", "+", "str", "(", "k", ")", "+", "'_correct_'", "+", "cat", ",", "cats_recalls", "[", "cat", "]", "[", "'recall'", "]", ")", "]", "\n", "\n", "", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.test_retrieval.test": [[130, 260], ["model.eval", "testset.get_test_queries", "range", "range", "np.concatenate.dot", "tqdm.tqdm", "numpy.concatenate", "tqdm.tqdm", "numpy.concatenate", "range", "numpy.concatenate", "numpy.concatenate", "numpy.linalg.norm", "numpy.linalg.norm", "enumerate", "enumerate", "len", "torch.cuda.empty_cache", "range", "torch.cuda.empty_cache", "numpy.argsort", "range", "enumerate", "len", "enumerate", "len", "testset.get_img", "torch.stack().float", "torch.autograd.Variable().cuda", "model.compose_img_text", "dct_with_representations[].data.cpu().numpy", "len", "testset.get_img", "torch.stack().float", "torch.autograd.Variable().cuda", "model.extract_img_feature().data.cpu().numpy", "torch.stack().float", "torch.autograd.Variable", "model.compose_img_text", "dct_with_representations[].data.cpu().numpy", "torch.stack().float", "torch.autograd.Variable", "model.extract_img_feature().data.cpu().numpy", "len", "str", "torch.autograd.Variable.cuda", "len", "str", "len", "torch.autograd.Variable.cuda", "len", "type", "torch.from_numpy().float", "torch.stack", "torch.autograd.Variable", "dct_with_representations[].data.cpu", "len", "type", "torch.from_numpy().float", "torch.stack", "torch.autograd.Variable", "model.extract_img_feature().data.cpu", "torch.stack", "dct_with_representations[].data.cpu", "torch.stack", "model.extract_img_feature().data.cpu", "str", "all_target_captions[].split", "all_target_captions[].split", "c.split", "str", "c.split", "str", "torch.from_numpy", "torch.from_numpy", "model.extract_img_feature", "model.extract_img_feature", "torch.autograd.Variable.cuda", "model.extract_img_feature().data.cpu().numpy.cuda"], "function", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.get_test_queries", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_img_feature", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_img_feature"], ["", "def", "test", "(", "opt", ",", "model", ",", "testset", ")", ":", "\n", "    ", "\"\"\"Tests a model over the given testset.\"\"\"", "\n", "model", ".", "eval", "(", ")", "\n", "test_queries", "=", "testset", ".", "get_test_queries", "(", ")", "\n", "\n", "all_imgs", "=", "[", "]", "\n", "all_captions", "=", "[", "]", "\n", "all_queries", "=", "[", "]", "\n", "all_target_captions", "=", "[", "]", "\n", "if", "test_queries", ":", "\n", "        ", "imgs", "=", "[", "]", "\n", "mods", "=", "[", "]", "\n", "for", "t", "in", "tqdm", "(", "test_queries", ")", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "imgs", "+=", "[", "testset", ".", "get_img", "(", "t", "[", "'source_img_id'", "]", ")", "]", "\n", "if", "opt", ".", "use_complete_text_query", ":", "\n", "                ", "if", "opt", ".", "dataset", "==", "'mitstates'", ":", "\n", "                    ", "mods", "+=", "[", "t", "[", "'mod'", "]", "[", "'str'", "]", "+", "\" \"", "+", "t", "[", "\"noun\"", "]", "]", "\n", "", "else", ":", "\n", "                    ", "mods", "+=", "[", "t", "[", "'target_caption'", "]", "]", "\n", "", "", "else", ":", "\n", "                ", "mods", "+=", "[", "t", "[", "'mod'", "]", "[", "'str'", "]", "]", "\n", "\n", "", "if", "len", "(", "imgs", ")", ">=", "opt", ".", "batch_size", "or", "t", "is", "test_queries", "[", "-", "1", "]", ":", "\n", "                ", "if", "'torch'", "not", "in", "str", "(", "type", "(", "imgs", "[", "0", "]", ")", ")", ":", "\n", "                    ", "imgs", "=", "[", "torch", ".", "from_numpy", "(", "d", ")", ".", "float", "(", ")", "for", "d", "in", "imgs", "]", "\n", "", "imgs", "=", "torch", ".", "stack", "(", "imgs", ")", ".", "float", "(", ")", "\n", "imgs", "=", "torch", ".", "autograd", ".", "Variable", "(", "imgs", ")", ".", "cuda", "(", ")", "\n", "dct_with_representations", "=", "model", ".", "compose_img_text", "(", "imgs", ".", "cuda", "(", ")", ",", "mods", ")", "\n", "f", "=", "dct_with_representations", "[", "\"repres\"", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "all_queries", "+=", "[", "f", "]", "\n", "imgs", "=", "[", "]", "\n", "mods", "=", "[", "]", "\n", "", "", "all_queries", "=", "np", ".", "concatenate", "(", "all_queries", ")", "\n", "all_target_captions", "=", "[", "t", "[", "'target_caption'", "]", "for", "t", "in", "test_queries", "]", "\n", "\n", "# compute all image features", "\n", "imgs", "=", "[", "]", "\n", "for", "i", "in", "tqdm", "(", "range", "(", "len", "(", "testset", ".", "imgs", ")", ")", ")", ":", "\n", "            ", "imgs", "+=", "[", "testset", ".", "get_img", "(", "i", ")", "]", "\n", "if", "len", "(", "imgs", ")", ">=", "opt", ".", "batch_size", "or", "i", "==", "len", "(", "testset", ".", "imgs", ")", "-", "1", ":", "\n", "                ", "if", "'torch'", "not", "in", "str", "(", "type", "(", "imgs", "[", "0", "]", ")", ")", ":", "\n", "                    ", "imgs", "=", "[", "torch", ".", "from_numpy", "(", "d", ")", ".", "float", "(", ")", "for", "d", "in", "imgs", "]", "\n", "", "imgs", "=", "torch", ".", "stack", "(", "imgs", ")", ".", "float", "(", ")", "\n", "imgs", "=", "torch", ".", "autograd", ".", "Variable", "(", "imgs", ")", ".", "cuda", "(", ")", "\n", "imgs", "=", "model", ".", "extract_img_feature", "(", "imgs", ".", "cuda", "(", ")", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "all_imgs", "+=", "[", "imgs", "]", "\n", "imgs", "=", "[", "]", "\n", "", "", "all_imgs", "=", "np", ".", "concatenate", "(", "all_imgs", ")", "\n", "all_captions", "=", "[", "img", "[", "'captions'", "]", "[", "0", "]", "for", "img", "in", "testset", ".", "imgs", "]", "\n", "\n", "", "else", ":", "\n", "# use training queries to approximate training retrieval performance", "\n", "        ", "imgs0", "=", "[", "]", "\n", "imgs", "=", "[", "]", "\n", "mods", "=", "[", "]", "\n", "training_approx", "=", "9600", "\n", "for", "i", "in", "range", "(", "training_approx", ")", ":", "\n", "            ", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "item", "=", "testset", "[", "i", "]", "\n", "imgs", "+=", "[", "item", "[", "'source_img_data'", "]", "]", "\n", "if", "opt", ".", "use_complete_text_query", ":", "\n", "                ", "if", "opt", ".", "dataset", "==", "'mitstates'", ":", "\n", "                    ", "mods", "+=", "[", "item", "[", "'mod'", "]", "[", "'str'", "]", "+", "\" \"", "+", "item", "[", "\"noun\"", "]", "]", "\n", "", "else", ":", "\n", "                    ", "mods", "+=", "[", "item", "[", "'target_caption'", "]", "]", "\n", "", "", "else", ":", "\n", "                ", "mods", "+=", "[", "item", "[", "'mod'", "]", "[", "'str'", "]", "]", "\n", "\n", "", "if", "len", "(", "imgs", ")", ">=", "opt", ".", "batch_size", "or", "i", "==", "training_approx", ":", "\n", "                ", "imgs", "=", "torch", ".", "stack", "(", "imgs", ")", ".", "float", "(", ")", "\n", "imgs", "=", "torch", ".", "autograd", ".", "Variable", "(", "imgs", ")", "\n", "dct_with_representations", "=", "model", ".", "compose_img_text", "(", "imgs", ".", "cuda", "(", ")", ",", "mods", ")", "\n", "f", "=", "dct_with_representations", "[", "\"repres\"", "]", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "all_queries", "+=", "[", "f", "]", "\n", "imgs", "=", "[", "]", "\n", "mods", "=", "[", "]", "\n", "", "imgs0", "+=", "[", "item", "[", "'target_img_data'", "]", "]", "\n", "if", "len", "(", "imgs0", ")", ">=", "opt", ".", "batch_size", "or", "i", "==", "training_approx", ":", "\n", "                ", "imgs0", "=", "torch", ".", "stack", "(", "imgs0", ")", ".", "float", "(", ")", "\n", "imgs0", "=", "torch", ".", "autograd", ".", "Variable", "(", "imgs0", ")", "\n", "imgs0", "=", "model", ".", "extract_img_feature", "(", "imgs0", ".", "cuda", "(", ")", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "all_imgs", "+=", "[", "imgs0", "]", "\n", "imgs0", "=", "[", "]", "\n", "", "all_captions", "+=", "[", "item", "[", "'target_caption'", "]", "]", "\n", "all_target_captions", "+=", "[", "item", "[", "'target_caption'", "]", "]", "\n", "", "all_imgs", "=", "np", ".", "concatenate", "(", "all_imgs", ")", "\n", "all_queries", "=", "np", ".", "concatenate", "(", "all_queries", ")", "\n", "\n", "# feature normalization", "\n", "", "for", "i", "in", "range", "(", "all_queries", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "all_queries", "[", "i", ",", ":", "]", "/=", "np", ".", "linalg", ".", "norm", "(", "all_queries", "[", "i", ",", ":", "]", ")", "\n", "", "for", "i", "in", "range", "(", "all_imgs", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "all_imgs", "[", "i", ",", ":", "]", "/=", "np", ".", "linalg", ".", "norm", "(", "all_imgs", "[", "i", ",", ":", "]", ")", "\n", "\n", "# match test queries to target images, get nearest neighbors", "\n", "", "sims", "=", "all_queries", ".", "dot", "(", "all_imgs", ".", "T", ")", "\n", "if", "test_queries", ":", "\n", "        ", "for", "i", ",", "t", "in", "enumerate", "(", "test_queries", ")", ":", "\n", "            ", "sims", "[", "i", ",", "t", "[", "'source_img_id'", "]", "]", "=", "-", "10e10", "# remove query image", "\n", "", "", "nn_result", "=", "[", "np", ".", "argsort", "(", "-", "sims", "[", "i", ",", ":", "]", ")", "[", ":", "110", "]", "for", "i", "in", "range", "(", "sims", ".", "shape", "[", "0", "]", ")", "]", "\n", "\n", "# compute recalls", "\n", "out", "=", "[", "]", "\n", "nn_result", "=", "[", "[", "all_captions", "[", "nn", "]", "for", "nn", "in", "nns", "]", "for", "nns", "in", "nn_result", "]", "\n", "for", "k", "in", "[", "1", ",", "5", ",", "10", ",", "50", ",", "100", "]", ":", "\n", "        ", "r", "=", "0.0", "\n", "for", "i", ",", "nns", "in", "enumerate", "(", "nn_result", ")", ":", "\n", "            ", "if", "all_target_captions", "[", "i", "]", "in", "nns", "[", ":", "k", "]", ":", "\n", "                ", "r", "+=", "1", "\n", "", "", "r", "/=", "len", "(", "nn_result", ")", "\n", "out", "+=", "[", "(", "'recall_top'", "+", "str", "(", "k", ")", "+", "'_correct_composition'", ",", "r", ")", "]", "\n", "\n", "if", "opt", ".", "dataset", "==", "'mitstates'", ":", "\n", "            ", "r", "=", "0.0", "\n", "for", "i", ",", "nns", "in", "enumerate", "(", "nn_result", ")", ":", "\n", "                ", "if", "all_target_captions", "[", "i", "]", ".", "split", "(", ")", "[", "0", "]", "in", "[", "c", ".", "split", "(", ")", "[", "0", "]", "for", "c", "in", "nns", "[", ":", "k", "]", "]", ":", "\n", "                    ", "r", "+=", "1", "\n", "", "", "r", "/=", "len", "(", "nn_result", ")", "\n", "out", "+=", "[", "(", "'recall_top'", "+", "str", "(", "k", ")", "+", "'_correct_adj'", ",", "r", ")", "]", "\n", "\n", "r", "=", "0.0", "\n", "for", "i", ",", "nns", "in", "enumerate", "(", "nn_result", ")", ":", "\n", "                ", "if", "all_target_captions", "[", "i", "]", ".", "split", "(", ")", "[", "1", "]", "in", "[", "c", ".", "split", "(", ")", "[", "1", "]", "for", "c", "in", "nns", "[", ":", "k", "]", "]", ":", "\n", "                    ", "r", "+=", "1", "\n", "", "", "r", "/=", "len", "(", "nn_result", ")", "\n", "out", "+=", "[", "(", "'recall_top'", "+", "str", "(", "k", ")", "+", "'_correct_noun'", ",", "r", ")", "]", "\n", "\n", "", "", "return", "out", "\n", "", ""]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.__init__": [[24, 30], ["object.__init__"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "SimpleVocab", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word2id", "=", "{", "}", "\n", "self", ".", "wordcount", "=", "{", "}", "\n", "self", ".", "word2id", "[", "'<UNK>'", "]", "=", "0", "\n", "self", ".", "wordcount", "[", "'<UNK>'", "]", "=", "9e9", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.tokenize_text": [[31, 35], ["text.encode().decode.encode().decode.encode().decode", "str().lower().translate().strip().split", "text.encode().decode.encode().decode.encode", "str().lower().translate().strip", "str().lower().translate", "str.maketrans", "str().lower", "str"], "methods", ["None"], ["", "def", "tokenize_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "text", "=", "text", ".", "encode", "(", "'ascii'", ",", "'ignore'", ")", ".", "decode", "(", "'ascii'", ")", "\n", "tokens", "=", "str", "(", "text", ")", ".", "lower", "(", ")", ".", "translate", "(", "str", ".", "maketrans", "(", "''", ",", "''", ",", "string", ".", "punctuation", ")", ")", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.add_text_to_vocab": [[36, 43], ["text_model.SimpleVocab.tokenize_text", "len"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.tokenize_text"], ["", "def", "add_text_to_vocab", "(", "self", ",", "text", ")", ":", "\n", "        ", "tokens", "=", "self", ".", "tokenize_text", "(", "text", ")", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "token", "not", "in", "self", ".", "word2id", ":", "\n", "                ", "self", ".", "word2id", "[", "token", "]", "=", "len", "(", "self", ".", "word2id", ")", "\n", "self", ".", "wordcount", "[", "token", "]", "=", "0", "\n", "", "self", ".", "wordcount", "[", "token", "]", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.threshold_rare_words": [[44, 48], ["None"], "methods", ["None"], ["", "", "def", "threshold_rare_words", "(", "self", ",", "wordcount_threshold", "=", "5", ")", ":", "\n", "        ", "for", "w", "in", "self", ".", "word2id", ":", "\n", "            ", "if", "self", ".", "wordcount", "[", "w", "]", "<", "wordcount_threshold", ":", "\n", "                ", "self", ".", "word2id", "[", "w", "]", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.encode_text": [[49, 53], ["text_model.SimpleVocab.tokenize_text", "text_model.SimpleVocab.word2id.get"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.tokenize_text"], ["", "", "", "def", "encode_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "tokens", "=", "self", ".", "tokenize_text", "(", "text", ")", "\n", "x", "=", "[", "self", ".", "word2id", ".", "get", "(", "t", ",", "0", ")", "for", "t", "in", "tokens", "]", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.get_size": [[54, 56], ["len"], "methods", ["None"], ["", "def", "get_size", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "word2id", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.TextLSTMModel.__init__": [[60, 79], ["super().__init__", "text_model.SimpleVocab", "text_model.TextLSTMModel.vocab.get_size", "torch.nn.Embedding", "torch.nn.LSTM", "torch.nn.Sequential", "text_model.TextLSTMModel.vocab.add_text_to_vocab", "torch.nn.Dropout", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.get_size", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.add_text_to_vocab"], ["    ", "def", "__init__", "(", "self", ",", "\n", "texts_to_build_vocab", ",", "\n", "word_embed_dim", "=", "512", ",", "\n", "lstm_hidden_dim", "=", "512", ")", ":", "\n", "\n", "        ", "super", "(", "TextLSTMModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "vocab", "=", "SimpleVocab", "(", ")", "\n", "for", "text", "in", "texts_to_build_vocab", ":", "\n", "            ", "self", ".", "vocab", ".", "add_text_to_vocab", "(", "text", ")", "\n", "", "vocab_size", "=", "self", ".", "vocab", ".", "get_size", "(", ")", "\n", "\n", "self", ".", "word_embed_dim", "=", "word_embed_dim", "\n", "self", ".", "lstm_hidden_dim", "=", "lstm_hidden_dim", "\n", "self", ".", "embedding_layer", "=", "torch", ".", "nn", ".", "Embedding", "(", "vocab_size", ",", "word_embed_dim", ")", "\n", "self", ".", "lstm", "=", "torch", ".", "nn", ".", "LSTM", "(", "word_embed_dim", ",", "lstm_hidden_dim", ")", "\n", "self", ".", "fc_output", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "0.1", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "lstm_hidden_dim", ",", "lstm_hidden_dim", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.TextLSTMModel.forward": [[81, 90], ["text_model.TextLSTMModel.forward_encoded_texts", "type", "type", "type", "type", "type", "type", "text_model.TextLSTMModel.vocab.encode_text"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.TextLSTMModel.forward_encoded_texts", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.SimpleVocab.encode_text"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\" input x: list of strings\"\"\"", "\n", "if", "type", "(", "x", ")", "is", "list", ":", "\n", "            ", "if", "type", "(", "x", "[", "0", "]", ")", "is", "str", "or", "type", "(", "x", "[", "0", "]", ")", "is", "unicode", ":", "\n", "                ", "x", "=", "[", "self", ".", "vocab", ".", "encode_text", "(", "text", ")", "for", "text", "in", "x", "]", "\n", "", "", "assert", "type", "(", "x", ")", "is", "list", "\n", "assert", "type", "(", "x", "[", "0", "]", ")", "is", "list", "\n", "assert", "type", "(", "x", "[", "0", "]", "[", "0", "]", ")", "is", "int", "\n", "return", "self", ".", "forward_encoded_texts", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.TextLSTMModel.forward_encoded_texts": [[91, 114], ["torch.zeros().long", "range", "torch.autograd.Variable().cuda", "text_model.TextLSTMModel.embedding_layer", "text_model.TextLSTMModel.forward_lstm_", "range", "torch.stack", "text_model.TextLSTMModel.fc_output", "len", "len", "torch.tensor", "len", "text_model.TextLSTMModel.append", "torch.zeros", "torch.autograd.Variable", "numpy.max", "len"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.TextLSTMModel.forward_lstm_"], ["", "def", "forward_encoded_texts", "(", "self", ",", "texts", ")", ":", "\n", "# to tensor", "\n", "        ", "lengths", "=", "[", "len", "(", "t", ")", "for", "t", "in", "texts", "]", "\n", "itexts", "=", "torch", ".", "zeros", "(", "(", "np", ".", "max", "(", "lengths", ")", ",", "len", "(", "texts", ")", ")", ")", ".", "long", "(", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "texts", ")", ")", ":", "\n", "            ", "itexts", "[", ":", "lengths", "[", "i", "]", ",", "i", "]", "=", "torch", ".", "tensor", "(", "texts", "[", "i", "]", ")", "\n", "\n", "# embed words", "\n", "", "itexts", "=", "torch", ".", "autograd", ".", "Variable", "(", "itexts", ")", ".", "cuda", "(", ")", "\n", "etexts", "=", "self", ".", "embedding_layer", "(", "itexts", ")", "\n", "\n", "# lstm", "\n", "lstm_output", ",", "_", "=", "self", ".", "forward_lstm_", "(", "etexts", ")", "\n", "\n", "# get last output (using length)", "\n", "text_features", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "texts", ")", ")", ":", "\n", "            ", "text_features", ".", "append", "(", "lstm_output", "[", "lengths", "[", "i", "]", "-", "1", ",", "i", ",", ":", "]", ")", "\n", "\n", "# output", "\n", "", "text_features", "=", "torch", ".", "stack", "(", "text_features", ")", "\n", "text_features", "=", "self", ".", "fc_output", "(", "text_features", ")", "\n", "return", "text_features", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.text_model.TextLSTMModel.forward_lstm_": [[115, 122], ["text_model.TextLSTMModel.lstm", "torch.zeros", "torch.zeros", "first_hidden[].cuda", "first_hidden[].cuda"], "methods", ["None"], ["", "def", "forward_lstm_", "(", "self", ",", "etexts", ")", ":", "\n", "        ", "batch_size", "=", "etexts", ".", "shape", "[", "1", "]", "\n", "first_hidden", "=", "(", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "lstm_hidden_dim", ")", ",", "\n", "torch", ".", "zeros", "(", "1", ",", "batch_size", ",", "self", ".", "lstm_hidden_dim", ")", ")", "\n", "first_hidden", "=", "(", "first_hidden", "[", "0", "]", ".", "cuda", "(", ")", ",", "first_hidden", "[", "1", "]", ".", "cuda", "(", ")", ")", "\n", "lstm_output", ",", "last_hidden", "=", "self", ".", "lstm", "(", "etexts", ",", "first_hidden", ")", "\n", "return", "lstm_output", ",", "last_hidden", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.main.parse_opt": [[42, 67], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_opt", "(", ")", ":", "\n", "    ", "\"\"\"Parses the input arguments.\"\"\"", "\n", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'-f'", ",", "type", "=", "str", ",", "default", "=", "''", ")", "\n", "parser", ".", "add_argument", "(", "'--comment'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--dataset_path'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "'composeAE'", ")", "\n", "parser", ".", "add_argument", "(", "'--image_embed_dim'", ",", "type", "=", "int", ",", "default", "=", "512", ")", "\n", "parser", ".", "add_argument", "(", "'--use_bert'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--use_complete_text_query'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate'", ",", "type", "=", "float", ",", "default", "=", "1e-2", ")", "\n", "parser", ".", "add_argument", "(", "'--learning_rate_decay_frequency'", ",", "type", "=", "int", ",", "default", "=", "9999999", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ")", "\n", "parser", ".", "add_argument", "(", "'--weight_decay'", ",", "type", "=", "float", ",", "default", "=", "1e-6", ")", "\n", "parser", ".", "add_argument", "(", "'--category_to_train'", ",", "type", "=", "str", ",", "default", "=", "'all'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_iters'", ",", "type", "=", "int", ",", "default", "=", "160000", ")", "\n", "parser", ".", "add_argument", "(", "'--loss'", ",", "type", "=", "str", ",", "default", "=", "'soft_triplet'", ")", "\n", "parser", ".", "add_argument", "(", "'--loader_num_workers'", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "'--log_dir'", ",", "type", "=", "str", ",", "default", "=", "'../logs/'", ")", "\n", "parser", ".", "add_argument", "(", "'--test_only'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--model_checkpoint'", ",", "type", "=", "str", ",", "default", "=", "''", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.main.load_dataset": [[69, 144], ["print", "print", "print", "datasets.Fashion200k", "datasets.Fashion200k", "len", "len", "datasets.MITStates", "datasets.MITStates", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "datasets.FashionIQ", "datasets.FashionIQ", "print", "sys.exit", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Compose", "torchvision.transforms.Compose", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize", "torchvision.transforms.Resize", "torchvision.transforms.CenterCrop", "torchvision.transforms.ToTensor", "torchvision.transforms.Normalize"], "function", ["None"], ["", "def", "load_dataset", "(", "opt", ")", ":", "\n", "    ", "\"\"\"Loads the input datasets.\"\"\"", "\n", "print", "(", "'Reading dataset '", ",", "opt", ".", "dataset", ")", "\n", "if", "opt", ".", "dataset", "==", "'fashion200k'", ":", "\n", "        ", "trainset", "=", "datasets", ".", "Fashion200k", "(", "\n", "path", "=", "opt", ".", "dataset_path", ",", "\n", "split", "=", "'train'", ",", "\n", "transform", "=", "torchvision", ".", "transforms", ".", "Compose", "(", "[", "\n", "torchvision", ".", "transforms", ".", "Resize", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "CenterCrop", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "torchvision", ".", "transforms", ".", "Normalize", "(", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "]", ")", ")", "\n", "testset", "=", "datasets", ".", "Fashion200k", "(", "\n", "path", "=", "opt", ".", "dataset_path", ",", "\n", "split", "=", "'test'", ",", "\n", "transform", "=", "torchvision", ".", "transforms", ".", "Compose", "(", "[", "\n", "torchvision", ".", "transforms", ".", "Resize", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "CenterCrop", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "torchvision", ".", "transforms", ".", "Normalize", "(", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "]", ")", ")", "\n", "", "elif", "opt", ".", "dataset", "==", "'mitstates'", ":", "\n", "        ", "trainset", "=", "datasets", ".", "MITStates", "(", "\n", "path", "=", "opt", ".", "dataset_path", ",", "\n", "split", "=", "'train'", ",", "\n", "transform", "=", "torchvision", ".", "transforms", ".", "Compose", "(", "[", "\n", "torchvision", ".", "transforms", ".", "Resize", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "CenterCrop", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "torchvision", ".", "transforms", ".", "Normalize", "(", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "]", ")", ")", "\n", "testset", "=", "datasets", ".", "MITStates", "(", "\n", "path", "=", "opt", ".", "dataset_path", ",", "\n", "split", "=", "'test'", ",", "\n", "transform", "=", "torchvision", ".", "transforms", ".", "Compose", "(", "[", "\n", "torchvision", ".", "transforms", ".", "Resize", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "CenterCrop", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "torchvision", ".", "transforms", ".", "Normalize", "(", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "]", ")", ")", "\n", "", "elif", "opt", ".", "dataset", "==", "'fashionIQ'", ":", "\n", "        ", "trainset", "=", "datasets", ".", "FashionIQ", "(", "\n", "path", "=", "opt", ".", "dataset_path", ",", "\n", "cat_type", "=", "opt", ".", "category_to_train", ",", "\n", "split", "=", "'train'", ",", "\n", "transform", "=", "torchvision", ".", "transforms", ".", "Compose", "(", "[", "\n", "torchvision", ".", "transforms", ".", "Resize", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "CenterCrop", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "torchvision", ".", "transforms", ".", "Normalize", "(", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "]", ")", ")", "\n", "testset", "=", "datasets", ".", "FashionIQ", "(", "\n", "path", "=", "opt", ".", "dataset_path", ",", "\n", "cat_type", "=", "opt", ".", "category_to_train", ",", "\n", "split", "=", "'val'", ",", "\n", "transform", "=", "torchvision", ".", "transforms", ".", "Compose", "(", "[", "\n", "torchvision", ".", "transforms", ".", "Resize", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "CenterCrop", "(", "224", ")", ",", "\n", "torchvision", ".", "transforms", ".", "ToTensor", "(", ")", ",", "\n", "torchvision", ".", "transforms", ".", "Normalize", "(", "[", "0.485", ",", "0.456", ",", "0.406", "]", ",", "\n", "[", "0.229", ",", "0.224", ",", "0.225", "]", ")", "\n", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "'Invalid dataset'", ",", "opt", ".", "dataset", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "print", "(", "'trainset size:'", ",", "len", "(", "trainset", ")", ")", "\n", "print", "(", "'testset size:'", ",", "len", "(", "testset", ")", ")", "\n", "return", "trainset", ",", "testset", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.main.create_model_and_optimizer": [[146, 194], ["print", "img_text_composition_models.RealSpaceConcatAE.cuda", "enumerate", "torch.optim.SGD", "torch.optim.SGD", "img_text_composition_models.TIRG", "enumerate", "img_text_composition_models.ComposeAE", "img_text_composition_models.RealSpaceConcatAE", "img_text_composition_models.RealSpaceConcatAE.img_model.fc.parameters", "img_text_composition_models.RealSpaceConcatAE.img_model.parameters", "img_text_composition_models.RealSpaceConcatAE.parameters", "enumerate", "torch.tensor", "torch.tensor"], "function", ["None"], ["", "def", "create_model_and_optimizer", "(", "opt", ",", "texts", ")", ":", "\n", "    ", "\"\"\"Builds the model and related optimizer.\"\"\"", "\n", "print", "(", "\"Creating model and optimizer for\"", ",", "opt", ".", "model", ")", "\n", "text_embed_dim", "=", "512", "if", "not", "opt", ".", "use_bert", "else", "768", "\n", "\n", "if", "opt", ".", "model", "==", "'tirg'", ":", "\n", "        ", "model", "=", "img_text_composition_models", ".", "TIRG", "(", "texts", ",", "\n", "image_embed_dim", "=", "opt", ".", "image_embed_dim", ",", "\n", "text_embed_dim", "=", "text_embed_dim", ",", "\n", "use_bert", "=", "opt", ".", "use_bert", ",", "\n", "name", "=", "opt", ".", "model", ")", "\n", "", "elif", "opt", ".", "model", "==", "'composeAE'", ":", "\n", "        ", "model", "=", "img_text_composition_models", ".", "ComposeAE", "(", "texts", ",", "\n", "image_embed_dim", "=", "opt", ".", "image_embed_dim", ",", "\n", "text_embed_dim", "=", "text_embed_dim", ",", "\n", "use_bert", "=", "opt", ".", "use_bert", ",", "\n", "name", "=", "opt", ".", "model", ")", "\n", "", "elif", "opt", ".", "model", "==", "'RealSpaceConcatAE'", ":", "\n", "        ", "model", "=", "img_text_composition_models", ".", "RealSpaceConcatAE", "(", "texts", ",", "\n", "image_embed_dim", "=", "opt", ".", "image_embed_dim", ",", "\n", "text_embed_dim", "=", "text_embed_dim", ",", "\n", "use_bert", "=", "opt", ".", "use_bert", ",", "\n", "name", "=", "opt", ".", "model", ")", "\n", "", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "# create optimizer", "\n", "params", "=", "[", "{", "\n", "'params'", ":", "[", "p", "for", "p", "in", "model", ".", "img_model", ".", "fc", ".", "parameters", "(", ")", "]", ",", "\n", "'lr'", ":", "opt", ".", "learning_rate", "\n", "}", ",", "{", "\n", "'params'", ":", "[", "p", "for", "p", "in", "model", ".", "img_model", ".", "parameters", "(", ")", "]", ",", "\n", "'lr'", ":", "0.1", "*", "opt", ".", "learning_rate", "\n", "}", ",", "{", "'params'", ":", "[", "p", "for", "p", "in", "model", ".", "parameters", "(", ")", "]", "}", "]", "\n", "\n", "for", "_", ",", "p1", "in", "enumerate", "(", "params", ")", ":", "# remove duplicated params", "\n", "        ", "for", "_", ",", "p2", "in", "enumerate", "(", "params", ")", ":", "\n", "            ", "if", "p1", "is", "not", "p2", ":", "\n", "                ", "for", "p11", "in", "p1", "[", "'params'", "]", ":", "\n", "                    ", "for", "j", ",", "p22", "in", "enumerate", "(", "p2", "[", "'params'", "]", ")", ":", "\n", "                        ", "if", "p11", "is", "p22", ":", "\n", "                            ", "p2", "[", "'params'", "]", "[", "j", "]", "=", "torch", ".", "tensor", "(", "0.0", ",", "requires_grad", "=", "True", ")", "\n", "\n", "", "", "", "", "", "", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "params", ",", "\n", "lr", "=", "opt", ".", "learning_rate", ",", "\n", "momentum", "=", "0.9", ",", "\n", "weight_decay", "=", "opt", ".", "weight_decay", ")", "\n", "\n", "return", "model", ",", "optimizer", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.main.train_loop": [[196, 336], ["print", "print", "time.time", "torch.nn.MSELoss().cuda", "torch.nn.MSELoss().cuda", "print", "len", "len", "print", "time.time", "logger.add_scalar", "torch.save", "torch.save", "model.train", "trainset.get_loader", "tqdm.tqdm", "torch.nn.MSELoss", "torch.nn.MSELoss", "round", "numpy.mean", "print", "logger.add_scalar", "gc.collect", "numpy.stack", "torch.from_numpy().float", "torch.from_numpy().float", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "numpy.stack", "torch.from_numpy().float", "torch.from_numpy().float", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "model.compute_loss", "sum", "torch.autograd.set_detect_anomaly", "torch.autograd.set_detect_anomaly", "optimizer.zero_grad", "sum.backward", "optimizer.step", "main.train_loop.training_1_iter"], "function", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.get_loader", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.compute_loss", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.MyTripletLossFunc.backward"], ["", "def", "train_loop", "(", "opt", ",", "loss_weights", ",", "logger", ",", "trainset", ",", "testset", ",", "model", ",", "optimizer", ")", ":", "\n", "    ", "\"\"\"Function for train loop\"\"\"", "\n", "print", "(", "'Begin training'", ")", "\n", "print", "(", "len", "(", "trainset", ".", "test_queries", ")", ",", "len", "(", "testset", ".", "test_queries", ")", ")", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "losses_tracking", "=", "{", "}", "\n", "it", "=", "0", "\n", "epoch", "=", "-", "1", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "l2_loss", "=", "torch", ".", "nn", ".", "MSELoss", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "while", "it", "<", "opt", ".", "num_iters", ":", "\n", "        ", "epoch", "+=", "1", "\n", "\n", "# show/log stats", "\n", "print", "(", "'It'", ",", "it", ",", "'epoch'", ",", "epoch", ",", "'Elapsed time'", ",", "round", "(", "time", ".", "time", "(", ")", "-", "tic", ",", "\n", "4", ")", ",", "opt", ".", "comment", ")", "\n", "tic", "=", "time", ".", "time", "(", ")", "\n", "for", "loss_name", "in", "losses_tracking", ":", "\n", "            ", "avg_loss", "=", "np", ".", "mean", "(", "losses_tracking", "[", "loss_name", "]", "[", "-", "len", "(", "trainloader", ")", ":", "]", ")", "\n", "print", "(", "'    Loss'", ",", "loss_name", ",", "round", "(", "avg_loss", ",", "4", ")", ")", "\n", "logger", ".", "add_scalar", "(", "loss_name", ",", "avg_loss", ",", "it", ")", "\n", "", "logger", ".", "add_scalar", "(", "'learning_rate'", ",", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ",", "it", ")", "\n", "\n", "if", "epoch", "%", "1", "==", "0", ":", "\n", "            ", "gc", ".", "collect", "(", ")", "\n", "\n", "# test", "\n", "", "if", "epoch", "%", "3", "==", "1", ":", "\n", "            ", "tests", "=", "[", "]", "\n", "for", "name", ",", "dataset", "in", "[", "(", "'train'", ",", "trainset", ")", ",", "(", "'test'", ",", "testset", ")", "]", ":", "\n", "                ", "if", "opt", ".", "dataset", "==", "'fashionIQ'", ":", "\n", "                    ", "t", "=", "test_retrieval", ".", "fiq_test", "(", "opt", ",", "model", ",", "dataset", ")", "\n", "", "else", ":", "\n", "                    ", "t", "=", "test_retrieval", ".", "test", "(", "opt", ",", "model", ",", "dataset", ")", "\n", "", "tests", "+=", "[", "(", "name", "+", "' '", "+", "metric_name", ",", "metric_value", ")", "\n", "for", "metric_name", ",", "metric_value", "in", "t", "]", "\n", "", "for", "metric_name", ",", "metric_value", "in", "tests", ":", "\n", "                ", "logger", ".", "add_scalar", "(", "metric_name", ",", "metric_value", ",", "it", ")", "\n", "print", "(", "'    '", ",", "metric_name", ",", "round", "(", "metric_value", ",", "4", ")", ")", "\n", "\n", "# save checkpoint", "\n", "", "", "torch", ".", "save", "(", "{", "\n", "'it'", ":", "it", ",", "\n", "'opt'", ":", "opt", ",", "\n", "'model_state_dict'", ":", "model", ".", "state_dict", "(", ")", ",", "\n", "}", ",", "\n", "logger", ".", "file_writer", ".", "get_logdir", "(", ")", "+", "'/latest_checkpoint.pth'", ")", "\n", "\n", "# run training for 1 epoch", "\n", "model", ".", "train", "(", ")", "\n", "trainloader", "=", "trainset", ".", "get_loader", "(", "\n", "batch_size", "=", "opt", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "drop_last", "=", "True", ",", "\n", "num_workers", "=", "opt", ".", "loader_num_workers", ")", "\n", "\n", "def", "training_1_iter", "(", "data", ")", ":", "\n", "            ", "assert", "type", "(", "data", ")", "is", "list", "\n", "img1", "=", "np", ".", "stack", "(", "[", "d", "[", "'source_img_data'", "]", "for", "d", "in", "data", "]", ")", "\n", "img1", "=", "torch", ".", "from_numpy", "(", "img1", ")", ".", "float", "(", ")", "\n", "img1", "=", "torch", ".", "autograd", ".", "Variable", "(", "img1", ")", ".", "cuda", "(", ")", "\n", "\n", "img2", "=", "np", ".", "stack", "(", "[", "d", "[", "'target_img_data'", "]", "for", "d", "in", "data", "]", ")", "\n", "img2", "=", "torch", ".", "from_numpy", "(", "img2", ")", ".", "float", "(", ")", "\n", "img2", "=", "torch", ".", "autograd", ".", "Variable", "(", "img2", ")", ".", "cuda", "(", ")", "\n", "\n", "if", "opt", ".", "use_complete_text_query", ":", "\n", "                ", "if", "opt", ".", "dataset", "==", "'mitstates'", ":", "\n", "                    ", "supp_text", "=", "[", "str", "(", "d", "[", "'noun'", "]", ")", "for", "d", "in", "data", "]", "\n", "mods", "=", "[", "str", "(", "d", "[", "'mod'", "]", "[", "'str'", "]", ")", "for", "d", "in", "data", "]", "\n", "# text_query here means complete_text_query", "\n", "text_query", "=", "[", "adj", "+", "\" \"", "+", "noun", "for", "adj", ",", "noun", "in", "zip", "(", "mods", ",", "supp_text", ")", "]", "\n", "", "else", ":", "\n", "                    ", "text_query", "=", "[", "str", "(", "d", "[", "'target_caption'", "]", ")", "for", "d", "in", "data", "]", "\n", "", "", "else", ":", "\n", "                ", "text_query", "=", "[", "str", "(", "d", "[", "'mod'", "]", "[", "'str'", "]", ")", "for", "d", "in", "data", "]", "\n", "# compute loss", "\n", "", "if", "opt", ".", "loss", "not", "in", "[", "'soft_triplet'", ",", "'batch_based_classification'", "]", ":", "\n", "                ", "print", "(", "'Invalid loss function'", ",", "opt", ".", "loss", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "losses", "=", "[", "]", "\n", "if_soft_triplet", "=", "True", "if", "opt", ".", "loss", "==", "'soft_triplet'", "else", "False", "\n", "loss_value", ",", "dct_with_representations", "=", "model", ".", "compute_loss", "(", "img1", ",", "\n", "text_query", ",", "\n", "img2", ",", "\n", "soft_triplet_loss", "=", "if_soft_triplet", ")", "\n", "\n", "loss_name", "=", "opt", ".", "loss", "\n", "losses", "+=", "[", "(", "loss_name", ",", "loss_weights", "[", "0", "]", ",", "loss_value", ".", "cuda", "(", ")", ")", "]", "\n", "\n", "if", "opt", ".", "model", "==", "'composeAE'", ":", "\n", "                ", "dec_img_loss", "=", "l2_loss", "(", "dct_with_representations", "[", "\"repr_to_compare_with_source\"", "]", ",", "\n", "dct_with_representations", "[", "\"img_features\"", "]", ")", "\n", "dec_text_loss", "=", "l2_loss", "(", "dct_with_representations", "[", "\"repr_to_compare_with_mods\"", "]", ",", "\n", "dct_with_representations", "[", "\"text_features\"", "]", ")", "\n", "\n", "losses", "+=", "[", "(", "\"L2_loss\"", ",", "loss_weights", "[", "1", "]", ",", "dec_img_loss", ".", "cuda", "(", ")", ")", "]", "\n", "losses", "+=", "[", "(", "\"L2_loss_text\"", ",", "loss_weights", "[", "2", "]", ",", "dec_text_loss", ".", "cuda", "(", ")", ")", "]", "\n", "losses", "+=", "[", "(", "\"rot_sym_loss\"", ",", "loss_weights", "[", "3", "]", ",", "dct_with_representations", "[", "\"rot_sym_loss\"", "]", ".", "cuda", "(", ")", ")", "]", "\n", "", "elif", "opt", ".", "model", "==", "'RealSpaceConcatAE'", ":", "\n", "                ", "dec_img_loss", "=", "l2_loss", "(", "dct_with_representations", "[", "\"repr_to_compare_with_source\"", "]", ",", "\n", "dct_with_representations", "[", "\"img_features\"", "]", ")", "\n", "dec_text_loss", "=", "l2_loss", "(", "dct_with_representations", "[", "\"repr_to_compare_with_mods\"", "]", ",", "\n", "dct_with_representations", "[", "\"text_features\"", "]", ")", "\n", "\n", "losses", "+=", "[", "(", "\"L2_loss\"", ",", "loss_weights", "[", "1", "]", ",", "dec_img_loss", ".", "cuda", "(", ")", ")", "]", "\n", "losses", "+=", "[", "(", "\"L2_loss_text\"", ",", "loss_weights", "[", "2", "]", ",", "dec_text_loss", ".", "cuda", "(", ")", ")", "]", "\n", "\n", "", "total_loss", "=", "sum", "(", "[", "\n", "loss_weight", "*", "loss_value", "\n", "for", "loss_name", ",", "loss_weight", ",", "loss_value", "in", "losses", "\n", "]", ")", "\n", "assert", "not", "torch", ".", "isnan", "(", "total_loss", ")", "\n", "losses", "+=", "[", "(", "'total training loss'", ",", "None", ",", "total_loss", ".", "item", "(", ")", ")", "]", "\n", "\n", "# track losses", "\n", "for", "loss_name", ",", "loss_weight", ",", "loss_value", "in", "losses", ":", "\n", "                ", "if", "loss_name", "not", "in", "losses_tracking", ":", "\n", "                    ", "losses_tracking", "[", "loss_name", "]", "=", "[", "]", "\n", "", "losses_tracking", "[", "loss_name", "]", ".", "append", "(", "float", "(", "loss_value", ")", ")", "\n", "\n", "", "torch", ".", "autograd", ".", "set_detect_anomaly", "(", "True", ")", "\n", "\n", "# gradient descendt", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "total_loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "for", "data", "in", "tqdm", "(", "trainloader", ",", "desc", "=", "'Training for epoch '", "+", "str", "(", "epoch", ")", ")", ":", "\n", "            ", "it", "+=", "1", "\n", "training_1_iter", "(", "data", ")", "\n", "\n", "# decay learning rate", "\n", "if", "it", ">=", "opt", ".", "learning_rate_decay_frequency", "and", "it", "%", "opt", ".", "learning_rate_decay_frequency", "==", "0", ":", "\n", "                ", "for", "g", "in", "optimizer", ".", "param_groups", ":", "\n", "                    ", "g", "[", "'lr'", "]", "*=", "0.1", "\n", "\n", "", "", "", "", "print", "(", "'Finished training'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.main.main": [[338, 376], ["main.parse_opt", "print", "parse_opt.__dict__.keys", "datetime.datetime.now().strftime", "os.path.join", "tensorboardX.SummaryWriter", "print", "parse_opt.__dict__.keys", "main.load_dataset", "main.create_model_and_optimizer", "main.train_loop", "tensorboardX.SummaryWriter.close", "print", "tensorboardX.SummaryWriter.file_writer.get_logdir", "tensorboardX.SummaryWriter.add_text", "print", "torch.load", "torch.load", "model.load_state_dict", "model.eval", "str", "datetime.datetime.now", "str", "tensorboardX.SummaryWriter.add_scalar", "print", "socket.gethostname", "trainset.get_all_texts", "test_retrieval.fiq_test", "test_retrieval.test", "round"], "function", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.main.parse_opt", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.main.load_dataset", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.main.create_model_and_optimizer", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.main.train_loop", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_all_texts", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.test_retrieval.fiq_test", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.test_retrieval.test"], ["", "def", "main", "(", ")", ":", "\n", "    ", "opt", "=", "parse_opt", "(", ")", "\n", "print", "(", "'Arguments:'", ")", "\n", "for", "k", "in", "opt", ".", "__dict__", ".", "keys", "(", ")", ":", "\n", "        ", "print", "(", "'    '", ",", "k", ",", "':'", ",", "str", "(", "opt", ".", "__dict__", "[", "k", "]", ")", ")", "\n", "\n", "", "current_time", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%b%d_%H-%M-%S'", ")", "\n", "loss_weights", "=", "[", "1.0", ",", "0.1", ",", "0.1", ",", "0.01", "]", "\n", "logdir", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "log_dir", ",", "current_time", "+", "'_'", "+", "socket", ".", "gethostname", "(", ")", "+", "opt", ".", "comment", ")", "\n", "\n", "logger", "=", "SummaryWriter", "(", "logdir", ")", "\n", "print", "(", "'Log files saved to'", ",", "logger", ".", "file_writer", ".", "get_logdir", "(", ")", ")", "\n", "for", "k", "in", "opt", ".", "__dict__", ".", "keys", "(", ")", ":", "\n", "        ", "logger", ".", "add_text", "(", "k", ",", "str", "(", "opt", ".", "__dict__", "[", "k", "]", ")", ")", "\n", "\n", "", "trainset", ",", "testset", "=", "load_dataset", "(", "opt", ")", "\n", "model", ",", "optimizer", "=", "create_model_and_optimizer", "(", "opt", ",", "[", "t", "for", "t", "in", "trainset", ".", "get_all_texts", "(", ")", "]", ")", "\n", "if", "opt", ".", "test_only", ":", "\n", "        ", "print", "(", "'Doing test only'", ")", "\n", "checkpoint", "=", "torch", ".", "load", "(", "opt", ".", "model_checkpoint", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ")", "\n", "it", "=", "checkpoint", "[", "'it'", "]", "\n", "model", ".", "eval", "(", ")", "\n", "tests", "=", "[", "]", "\n", "it", "=", "0", "\n", "for", "name", ",", "dataset", "in", "[", "(", "'train'", ",", "trainset", ")", ",", "(", "'test'", ",", "testset", ")", "]", ":", "\n", "            ", "if", "opt", ".", "dataset", "==", "'fashionIQ'", ":", "\n", "                ", "t", "=", "test_retrieval", ".", "fiq_test", "(", "opt", ",", "model", ",", "dataset", ")", "\n", "", "else", ":", "\n", "                ", "t", "=", "test_retrieval", ".", "test", "(", "opt", ",", "model", ",", "dataset", ")", "\n", "", "tests", "+=", "[", "(", "name", "+", "' '", "+", "metric_name", ",", "metric_value", ")", "for", "metric_name", ",", "metric_value", "in", "t", "]", "\n", "", "for", "metric_name", ",", "metric_value", "in", "tests", ":", "\n", "            ", "logger", ".", "add_scalar", "(", "metric_name", ",", "metric_value", ",", "it", ")", "\n", "print", "(", "'    '", ",", "metric_name", ",", "round", "(", "metric_value", ",", "4", ")", ")", "\n", "\n", "", "return", "0", "\n", "", "train_loop", "(", "opt", ",", "loss_weights", ",", "logger", ",", "trainset", ",", "testset", ",", "model", ",", "optimizer", ")", "\n", "logger", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.MyTripletLossFunc.__init__": [[56, 60], ["super().__init__", "len"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["  ", "def", "__init__", "(", "self", ",", "triplets", ")", ":", "\n", "    ", "super", "(", "MyTripletLossFunc", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "triplets", "=", "triplets", "\n", "self", ".", "triplet_count", "=", "len", "(", "triplets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.MyTripletLossFunc.forward": [[61, 79], ["torch_functions.MyTripletLossFunc.save_for_backward", "pairwise_distances().cpu().numpy", "torch.FloatTensor", "pairwise_distances().cpu", "numpy.log", "torch_functions.pairwise_distances", "numpy.exp"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.pairwise_distances"], ["", "def", "forward", "(", "self", ",", "features", ")", ":", "\n", "    ", "self", ".", "save_for_backward", "(", "features", ")", "\n", "\n", "self", ".", "distances", "=", "pairwise_distances", "(", "features", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "loss", "=", "0.0", "\n", "triplet_count", "=", "0.0", "\n", "correct_count", "=", "0.0", "\n", "for", "i", ",", "j", ",", "k", "in", "self", ".", "triplets", ":", "\n", "      ", "w", "=", "1.0", "\n", "triplet_count", "+=", "w", "\n", "loss", "+=", "w", "*", "np", ".", "log", "(", "1", "+", "\n", "np", ".", "exp", "(", "self", ".", "distances", "[", "i", ",", "j", "]", "-", "self", ".", "distances", "[", "i", ",", "k", "]", ")", ")", "\n", "if", "self", ".", "distances", "[", "i", ",", "j", "]", "<", "self", ".", "distances", "[", "i", ",", "k", "]", ":", "\n", "        ", "correct_count", "+=", "1", "\n", "\n", "", "", "loss", "/=", "triplet_count", "\n", "return", "torch", ".", "FloatTensor", "(", "(", "loss", ",", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.MyTripletLossFunc.backward": [[80, 103], ["features.cpu().numpy", "grad_features.cpu().numpy", "range", "float", "features.clone", "torch.from_numpy", "features.cpu", "grad_features.cpu", "numpy.exp"], "methods", ["None"], ["", "def", "backward", "(", "self", ",", "grad_output", ")", ":", "\n", "    ", "features", ",", "=", "self", ".", "saved_tensors", "\n", "features_np", "=", "features", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "grad_features", "=", "features", ".", "clone", "(", ")", "*", "0.0", "\n", "grad_features_np", "=", "grad_features", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "for", "i", ",", "j", ",", "k", "in", "self", ".", "triplets", ":", "\n", "      ", "w", "=", "1.0", "\n", "f", "=", "1.0", "-", "1.0", "/", "(", "\n", "1.0", "+", "np", ".", "exp", "(", "self", ".", "distances", "[", "i", ",", "j", "]", "-", "self", ".", "distances", "[", "i", ",", "k", "]", ")", ")", "\n", "grad_features_np", "[", "i", ",", ":", "]", "+=", "w", "*", "f", "*", "(", "\n", "features_np", "[", "i", ",", ":", "]", "-", "features_np", "[", "j", ",", ":", "]", ")", "/", "self", ".", "triplet_count", "\n", "grad_features_np", "[", "j", ",", ":", "]", "+=", "w", "*", "f", "*", "(", "\n", "features_np", "[", "j", ",", ":", "]", "-", "features_np", "[", "i", ",", ":", "]", ")", "/", "self", ".", "triplet_count", "\n", "grad_features_np", "[", "i", ",", ":", "]", "+=", "-", "w", "*", "f", "*", "(", "\n", "features_np", "[", "i", ",", ":", "]", "-", "features_np", "[", "k", ",", ":", "]", ")", "/", "self", ".", "triplet_count", "\n", "grad_features_np", "[", "k", ",", ":", "]", "+=", "-", "w", "*", "f", "*", "(", "\n", "features_np", "[", "k", ",", ":", "]", "-", "features_np", "[", "i", ",", ":", "]", ")", "/", "self", ".", "triplet_count", "\n", "\n", "", "for", "i", "in", "range", "(", "features_np", ".", "shape", "[", "0", "]", ")", ":", "\n", "      ", "grad_features", "[", "i", ",", ":", "]", "=", "torch", ".", "from_numpy", "(", "grad_features_np", "[", "i", ",", ":", "]", ")", "\n", "", "grad_features", "*=", "float", "(", "grad_output", ".", "data", "[", "0", "]", ")", "\n", "return", "grad_features", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.TripletLoss.__init__": [[107, 110], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "pre_layer", "=", "None", ")", ":", "\n", "    ", "super", "(", "TripletLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "pre_layer", "=", "pre_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.TripletLoss.forward": [[111, 116], ["torch_functions.TripletLoss.pre_layer", "torch_functions.MyTripletLossFunc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "triplets", ")", ":", "\n", "    ", "if", "self", ".", "pre_layer", "is", "not", "None", ":", "\n", "      ", "x", "=", "self", ".", "pre_layer", "(", "x", ")", "\n", "", "loss", "=", "MyTripletLossFunc", "(", "triplets", ")", "(", "x", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.NormalizationLayer.__init__": [[120, 125], ["super().__init__", "float", "torch.nn.Parameter", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "normalize_scale", "=", "1.0", ",", "learn_scale", "=", "True", ")", ":", "\n", "    ", "super", "(", "NormalizationLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "norm_s", "=", "float", "(", "normalize_scale", ")", "\n", "if", "learn_scale", ":", "\n", "      ", "self", ".", "norm_s", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "FloatTensor", "(", "(", "self", ".", "norm_s", ",", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.NormalizationLayer.forward": [[126, 129], ["torch.norm().expand_as", "torch.norm"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "    ", "features", "=", "self", ".", "norm_s", "*", "x", "/", "torch", ".", "norm", "(", "x", ",", "dim", "=", "1", ",", "keepdim", "=", "True", ")", ".", "expand_as", "(", "x", ")", "\n", "return", "features", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.torch_functions.pairwise_distances": [[28, 52], ["torch.clamp", "torch.transpose", "torch.transpose", "x_norm.view", "torch.mm"], "function", ["None"], ["def", "pairwise_distances", "(", "x", ",", "y", "=", "None", ")", ":", "\n", "  ", "\"\"\"\n    Input: x is a Nxd matrix\n           y is an optional Mxd matirx\n    Output: dist is a NxM matrix where dist[i,j] is the square norm between\n    x[i,:] and y[j,:]\n            if y is not given then use 'y=x'.\n    i.e. dist[i,j] = ||x[i,:]-y[j,:]||^2\n    source:\n    https://discuss.pytorch.org/t/efficient-distance-matrix-computation/9065/2\n    \"\"\"", "\n", "x_norm", "=", "(", "x", "**", "2", ")", ".", "sum", "(", "1", ")", ".", "view", "(", "-", "1", ",", "1", ")", "\n", "if", "y", "is", "not", "None", ":", "\n", "    ", "y_t", "=", "torch", ".", "transpose", "(", "y", ",", "0", ",", "1", ")", "\n", "y_norm", "=", "(", "y", "**", "2", ")", ".", "sum", "(", "1", ")", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "", "else", ":", "\n", "    ", "y_t", "=", "torch", ".", "transpose", "(", "x", ",", "0", ",", "1", ")", "\n", "y_norm", "=", "x_norm", ".", "view", "(", "1", ",", "-", "1", ")", "\n", "\n", "", "dist", "=", "x_norm", "+", "y_norm", "-", "2.0", "*", "torch", ".", "mm", "(", "x", ",", "y_t", ")", "\n", "# Ensure diagonal is zero if x=y", "\n", "# if y is None:", "\n", "#     dist = dist - torch.diag(dist.diag)", "\n", "return", "torch", ".", "clamp", "(", "dist", ",", "0.0", ",", "np", ".", "inf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ConCatModule.__init__": [[32, 34], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "ConCatModule", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ConCatModule.forward": [[35, 39], ["torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "torch", ".", "cat", "(", "x", ",", "1", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.__init__": [[43, 48], ["super().__init__", "torch_functions.NormalizationLayer", "torch_functions.TripletLoss"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "normalization_layer", "=", "torch_functions", ".", "NormalizationLayer", "(", "\n", "normalize_scale", "=", "4.0", ",", "learn_scale", "=", "True", ")", "\n", "self", ".", "soft_triplet_loss", "=", "torch_functions", ".", "TripletLoss", "(", ")", "\n", "#         self.name = 'model_name'", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.extract_img_feature": [[50, 52], ["None"], "methods", ["None"], ["", "def", "extract_img_feature", "(", "self", ",", "imgs", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.extract_text_feature": [[53, 55], ["None"], "methods", ["None"], ["", "def", "extract_text_feature", "(", "self", ",", "text_query", ",", "use_bert", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.compose_img_text": [[56, 58], ["None"], "methods", ["None"], ["", "def", "compose_img_text", "(", "self", ",", "imgs", ",", "text_query", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.compute_loss": [[59, 92], ["img_text_composition_models.ImgTextCompositionBase.compose_img_text", "img_text_composition_models.ImgTextCompositionBase.normalization_layer", "img_text_composition_models.ImgTextCompositionBase.extract_img_feature", "img_text_composition_models.ImgTextCompositionBase.normalization_layer", "torch.autograd.Variable", "torch.autograd.Variable", "img_text_composition_models.ImgTextCompositionBase.compose_img_text_features", "img_text_composition_models.ImgTextCompositionBase.normalization_layer", "img_text_composition_models.ImgTextCompositionBase.normalization_layer", "torch.cuda.FloatTensor().fill_", "torch.cuda.FloatTensor().fill_", "torch.cuda.FloatTensor().fill_", "torch.cuda.FloatTensor().fill_", "img_text_composition_models.ImgTextCompositionBase.compute_soft_triplet_loss_", "img_text_composition_models.ImgTextCompositionBase.compute_batch_based_classification_loss_", "img_text_composition_models.ImgTextCompositionBase.compute_soft_triplet_loss_", "img_text_composition_models.ImgTextCompositionBase.compute_batch_based_classification_loss_", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_img_feature", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text_features", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.compute_soft_triplet_loss_", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.compute_batch_based_classification_loss_", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.compute_soft_triplet_loss_", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.compute_batch_based_classification_loss_"], ["", "def", "compute_loss", "(", "self", ",", "\n", "imgs_query", ",", "\n", "text_query", ",", "\n", "imgs_target", ",", "\n", "soft_triplet_loss", "=", "True", ")", ":", "\n", "        ", "dct_with_representations", "=", "self", ".", "compose_img_text", "(", "imgs_query", ",", "text_query", ")", "\n", "composed_source_image", "=", "self", ".", "normalization_layer", "(", "dct_with_representations", "[", "\"repres\"", "]", ")", "\n", "target_img_features_non_norm", "=", "self", ".", "extract_img_feature", "(", "imgs_target", ")", "\n", "target_img_features", "=", "self", ".", "normalization_layer", "(", "target_img_features_non_norm", ")", "\n", "assert", "(", "composed_source_image", ".", "shape", "[", "0", "]", "==", "target_img_features", ".", "shape", "[", "0", "]", "and", "\n", "composed_source_image", ".", "shape", "[", "1", "]", "==", "target_img_features", ".", "shape", "[", "1", "]", ")", "\n", "# Get Rot_Sym_Loss", "\n", "if", "self", ".", "name", "==", "'composeAE'", ":", "\n", "            ", "CONJUGATE", "=", "Variable", "(", "torch", ".", "cuda", ".", "FloatTensor", "(", "32", ",", "1", ")", ".", "fill_", "(", "-", "1.0", ")", ",", "requires_grad", "=", "False", ")", "\n", "conjugate_representations", "=", "self", ".", "compose_img_text_features", "(", "target_img_features_non_norm", ",", "dct_with_representations", "[", "\"text_features\"", "]", ",", "CONJUGATE", ")", "\n", "composed_target_image", "=", "self", ".", "normalization_layer", "(", "conjugate_representations", "[", "\"repres\"", "]", ")", "\n", "source_img_features", "=", "self", ".", "normalization_layer", "(", "dct_with_representations", "[", "\"img_features\"", "]", ")", "#img1", "\n", "if", "soft_triplet_loss", ":", "\n", "                ", "dct_with_representations", "[", "\"rot_sym_loss\"", "]", "=", "self", ".", "compute_soft_triplet_loss_", "(", "composed_target_image", ",", "source_img_features", ")", "\n", "", "else", ":", "\n", "                ", "dct_with_representations", "[", "\"rot_sym_loss\"", "]", "=", "self", ".", "compute_batch_based_classification_loss_", "(", "composed_target_image", ",", "\n", "source_img_features", ")", "\n", "", "", "else", ":", "# tirg, RealSpaceConcatAE etc", "\n", "            ", "dct_with_representations", "[", "\"rot_sym_loss\"", "]", "=", "0", "\n", "\n", "", "if", "soft_triplet_loss", ":", "\n", "            ", "return", "self", ".", "compute_soft_triplet_loss_", "(", "composed_source_image", ",", "\n", "target_img_features", ")", ",", "dct_with_representations", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "compute_batch_based_classification_loss_", "(", "composed_source_image", ",", "\n", "target_img_features", ")", ",", "dct_with_representations", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.compute_soft_triplet_loss_": [[93, 107], ["range", "img_text_composition_models.ImgTextCompositionBase.soft_triplet_loss", "list", "list", "len", "range", "numpy.random.shuffle", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range", "range", "len", "len", "range", "len", "triplets_i.append"], "methods", ["None"], ["", "", "def", "compute_soft_triplet_loss_", "(", "self", ",", "mod_img1", ",", "img2", ")", ":", "\n", "        ", "triplets", "=", "[", "]", "\n", "labels", "=", "list", "(", "range", "(", "mod_img1", ".", "shape", "[", "0", "]", ")", ")", "+", "list", "(", "range", "(", "img2", ".", "shape", "[", "0", "]", ")", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "            ", "triplets_i", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "                ", "if", "labels", "[", "i", "]", "==", "labels", "[", "j", "]", "and", "i", "!=", "j", ":", "\n", "                    ", "for", "k", "in", "range", "(", "len", "(", "labels", ")", ")", ":", "\n", "                        ", "if", "labels", "[", "i", "]", "!=", "labels", "[", "k", "]", ":", "\n", "                            ", "triplets_i", ".", "append", "(", "[", "i", ",", "j", ",", "k", "]", ")", "\n", "", "", "", "", "np", ".", "random", ".", "shuffle", "(", "triplets_i", ")", "\n", "triplets", "+=", "triplets_i", "[", ":", "3", "]", "\n", "", "assert", "(", "triplets", "and", "len", "(", "triplets", ")", "<", "2000", ")", "\n", "return", "self", ".", "soft_triplet_loss", "(", "torch", ".", "cat", "(", "[", "mod_img1", ",", "img2", "]", ")", ",", "triplets", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgTextCompositionBase.compute_batch_based_classification_loss_": [[108, 113], ["torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.tensor().long", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.autograd.Variable().cuda", "torch.cross_entropy", "torch.cross_entropy", "img2.transpose", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "torch.autograd.Variable", "range"], "methods", ["None"], ["", "def", "compute_batch_based_classification_loss_", "(", "self", ",", "mod_img1", ",", "img2", ")", ":", "\n", "        ", "x", "=", "torch", ".", "mm", "(", "mod_img1", ",", "img2", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "labels", "=", "torch", ".", "tensor", "(", "range", "(", "x", ".", "shape", "[", "0", "]", ")", ")", ".", "long", "(", ")", "\n", "labels", "=", "torch", ".", "autograd", ".", "Variable", "(", "labels", ")", ".", "cuda", "(", ")", "\n", "return", "F", ".", "cross_entropy", "(", "x", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.__init__": [[117, 137], ["img_text_composition_models.ImgTextCompositionBase.__init__", "torchvision.models.resnet18", "GlobalAvgPool2d", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "text_model.TextLSTMModel", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.adaptive_avg_pool2d", "torch.adaptive_avg_pool2d"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "text_query", ",", "image_embed_dim", ",", "text_embed_dim", ",", "use_bert", ",", "name", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# img model", "\n", "img_model", "=", "torchvision", ".", "models", ".", "resnet18", "(", "pretrained", "=", "True", ")", "\n", "self", ".", "name", "=", "name", "\n", "\n", "class", "GlobalAvgPool2d", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n", "\n", "            ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "                ", "return", "F", ".", "adaptive_avg_pool2d", "(", "x", ",", "(", "1", ",", "1", ")", ")", "\n", "\n", "", "", "img_model", ".", "avgpool", "=", "GlobalAvgPool2d", "(", ")", "\n", "img_model", ".", "fc", "=", "torch", ".", "nn", ".", "Sequential", "(", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", ")", "\n", "self", ".", "img_model", "=", "img_model", "\n", "\n", "# text model", "\n", "self", ".", "text_model", "=", "text_model", ".", "TextLSTMModel", "(", "\n", "texts_to_build_vocab", "=", "text_query", ",", "\n", "word_embed_dim", "=", "text_embed_dim", ",", "\n", "lstm_hidden_dim", "=", "text_embed_dim", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_img_feature": [[138, 140], ["img_text_composition_models.ImgEncoderTextEncoderBase.img_model"], "methods", ["None"], ["", "def", "extract_img_feature", "(", "self", ",", "imgs", ")", ":", "\n", "        ", "return", "self", ".", "img_model", "(", "imgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_text_feature": [[141, 146], ["img_text_composition_models.ImgEncoderTextEncoderBase.text_model", "bc.encode", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy().cuda", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["None"], ["", "def", "extract_text_feature", "(", "self", ",", "text_query", ",", "use_bert", ")", ":", "\n", "        ", "if", "use_bert", ":", "\n", "            ", "text_features", "=", "bc", ".", "encode", "(", "text_query", ")", "\n", "return", "torch", ".", "from_numpy", "(", "text_features", ")", ".", "cuda", "(", ")", "\n", "", "return", "self", ".", "text_model", "(", "text_query", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.TIRG.__init__": [[157, 179], ["img_text_composition_models.ImgEncoderTextEncoderBase.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "img_text_composition_models.ConCatModule", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "img_text_composition_models.ConCatModule", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "text_query", ",", "image_embed_dim", ",", "text_embed_dim", ",", "use_bert", ",", "name", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "text_query", ",", "image_embed_dim", ",", "text_embed_dim", ",", "use_bert", ",", "name", ")", "\n", "\n", "self", ".", "a", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "[", "1.0", ",", "10.0", ",", "1.0", ",", "1.0", "]", ")", ")", "\n", "self", ".", "use_bert", "=", "use_bert", "\n", "\n", "merged_dim", "=", "image_embed_dim", "+", "text_embed_dim", "\n", "\n", "self", ".", "gated_feature_composer", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "ConCatModule", "(", ")", ",", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "merged_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "merged_dim", ",", "image_embed_dim", ")", "\n", ")", "\n", "\n", "self", ".", "res_info_composer", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "ConCatModule", "(", ")", ",", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "merged_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "merged_dim", ",", "merged_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "merged_dim", ",", "image_embed_dim", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.TIRG.compose_img_text": [[181, 186], ["img_text_composition_models.TIRG.extract_img_feature", "img_text_composition_models.TIRG.extract_text_feature", "img_text_composition_models.TIRG.compose_img_text_features"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_img_feature", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_text_feature", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text_features"], ["", "def", "compose_img_text", "(", "self", ",", "imgs", ",", "text_query", ")", ":", "\n", "        ", "img_features", "=", "self", ".", "extract_img_feature", "(", "imgs", ")", "\n", "text_features", "=", "self", ".", "extract_text_feature", "(", "text_query", ",", "self", ".", "use_bert", ")", "\n", "\n", "return", "self", ".", "compose_img_text_features", "(", "img_features", ",", "text_features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.TIRG.compose_img_text_features": [[187, 194], ["img_text_composition_models.TIRG.gated_feature_composer", "img_text_composition_models.TIRG.res_info_composer", "torch.sigmoid", "torch.sigmoid"], "methods", ["None"], ["", "def", "compose_img_text_features", "(", "self", ",", "img_features", ",", "text_features", ")", ":", "\n", "        ", "f1", "=", "self", ".", "gated_feature_composer", "(", "(", "img_features", ",", "text_features", ")", ")", "\n", "f2", "=", "self", ".", "res_info_composer", "(", "(", "img_features", ",", "text_features", ")", ")", "\n", "f", "=", "F", ".", "sigmoid", "(", "f1", ")", "*", "img_features", "*", "self", ".", "a", "[", "0", "]", "+", "f2", "*", "self", ".", "a", "[", "1", "]", "\n", "\n", "dct_with_representations", "=", "{", "\"repres\"", ":", "f", "}", "\n", "return", "dct_with_representations", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ComplexProjectionModule.__init__": [[197, 211], ["super().__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["    ", "def", "__init__", "(", "self", ",", "image_embed_dim", "=", "512", ",", "text_embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "bert_features", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "text_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "text_embed_dim", ",", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", "\n", ")", "\n", "self", ".", "image_features", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "Dropout", "(", "p", "=", "0.5", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ComplexProjectionModule.forward": [[213, 236], ["img_text_composition_models.ComplexProjectionModule.image_features", "img_text_composition_models.ComplexProjectionModule.bert_features", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "x[].unsqueeze", "x1.unsqueeze.unsqueeze.unsqueeze", "x2.unsqueeze.unsqueeze.unsqueeze", "re_score.unsqueeze.unsqueeze.unsqueeze", "im_score.unsqueeze.unsqueeze.unsqueeze", "torch.sin", "torch.sin", "torch.sin", "torch.sin"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x1", "=", "self", ".", "image_features", "(", "x", "[", "0", "]", ")", "\n", "x2", "=", "self", ".", "bert_features", "(", "x", "[", "1", "]", ")", "\n", "# default value of CONJUGATE is 1. Only for rotationally symmetric loss value is -1.", "\n", "# which results in the CONJUGATE of text features in the complex space", "\n", "CONJUGATE", "=", "x", "[", "2", "]", "\n", "num_samples", "=", "x", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "CONJUGATE", "=", "CONJUGATE", "[", ":", "num_samples", "]", "\n", "delta", "=", "x2", "# text as rotation", "\n", "re_delta", "=", "torch", ".", "cos", "(", "delta", ")", "\n", "im_delta", "=", "CONJUGATE", "*", "torch", ".", "sin", "(", "delta", ")", "\n", "\n", "re_score", "=", "x1", "*", "re_delta", "\n", "im_score", "=", "x1", "*", "im_delta", "\n", "\n", "concat_x", "=", "torch", ".", "cat", "(", "[", "re_score", ",", "im_score", "]", ",", "1", ")", "\n", "x0copy", "=", "x", "[", "0", "]", ".", "unsqueeze", "(", "1", ")", "\n", "x1", "=", "x1", ".", "unsqueeze", "(", "1", ")", "\n", "x2", "=", "x2", ".", "unsqueeze", "(", "1", ")", "\n", "re_score", "=", "re_score", ".", "unsqueeze", "(", "1", ")", "\n", "im_score", "=", "im_score", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "return", "concat_x", ",", "x1", ",", "x2", ",", "x0copy", ",", "re_score", ",", "im_score", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.LinearMapping.__init__": [[242, 250], ["super().__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "image_embed_dim", "=", "512", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mapping", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "2", "*", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "2", "*", "image_embed_dim", ",", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.LinearMapping.forward": [[252, 255], ["img_text_composition_models.LinearMapping.mapping"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "theta_linear", "=", "self", ".", "mapping", "(", "x", "[", "0", "]", ")", "\n", "return", "theta_linear", "\n", "", "", "class", "ConvMapping", "(", "torch", ".", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ConvMapping.__init__": [[260, 272], ["super().__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.AdaptiveMaxPool1d", "torch.nn.AdaptiveMaxPool1d", "torch.nn.AdaptiveMaxPool1d", "torch.nn.AdaptiveMaxPool1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "image_embed_dim", "=", "512", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mapping", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "2", "*", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "2", "*", "image_embed_dim", ",", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", "\n", ")", "\n", "# in_channels, output channels", "\n", "self", ".", "conv", "=", "torch", ".", "nn", ".", "Conv1d", "(", "5", ",", "64", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "self", ".", "adaptivepooling", "=", "torch", ".", "nn", ".", "AdaptiveMaxPool1d", "(", "16", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ConvMapping.forward": [[273, 280], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "img_text_composition_models.ConvMapping.conv", "img_text_composition_models.ConvMapping.adaptivepooling", "img_text_composition_models.ConvMapping.reshape", "img_text_composition_models.ConvMapping.mapping"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "concat_features", "=", "torch", ".", "cat", "(", "x", "[", "1", ":", "]", ",", "1", ")", "\n", "concat_x", "=", "self", ".", "conv", "(", "concat_features", ")", "\n", "concat_x", "=", "self", ".", "adaptivepooling", "(", "concat_x", ")", "\n", "final_vec", "=", "concat_x", ".", "reshape", "(", "(", "concat_x", ".", "shape", "[", "0", "]", ",", "1024", ")", ")", "\n", "theta_conv", "=", "self", ".", "mapping", "(", "final_vec", ")", "\n", "return", "theta_conv", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ComposeAE.__init__": [[290, 318], ["img_text_composition_models.ImgEncoderTextEncoderBase.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "img_text_composition_models.ComplexProjectionModule", "img_text_composition_models.LinearMapping", "img_text_composition_models.ComplexProjectionModule", "img_text_composition_models.ConvMapping", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "text_query", ",", "image_embed_dim", ",", "text_embed_dim", ",", "use_bert", ",", "name", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "text_query", ",", "image_embed_dim", ",", "text_embed_dim", ",", "use_bert", ",", "name", ")", "\n", "self", ".", "a", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "[", "1.0", ",", "10.0", ",", "1.0", ",", "1.0", "]", ")", ")", "\n", "self", ".", "use_bert", "=", "use_bert", "\n", "\n", "# merged_dim = image_embed_dim + text_embed_dim", "\n", "\n", "self", ".", "encoderLinear", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "ComplexProjectionModule", "(", ")", ",", "\n", "LinearMapping", "(", ")", "\n", ")", "\n", "self", ".", "encoderWithConv", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "ComplexProjectionModule", "(", ")", ",", "\n", "ConvMapping", "(", ")", "\n", ")", "\n", "self", ".", "decoder", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", "\n", ")", "\n", "self", ".", "txtdecoder", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "text_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "text_embed_dim", ",", "text_embed_dim", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ComposeAE.compose_img_text": [[320, 325], ["img_text_composition_models.ComposeAE.extract_img_feature", "img_text_composition_models.ComposeAE.extract_text_feature", "img_text_composition_models.ComposeAE.compose_img_text_features"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_img_feature", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_text_feature", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text_features"], ["", "def", "compose_img_text", "(", "self", ",", "imgs", ",", "text_query", ")", ":", "\n", "        ", "img_features", "=", "self", ".", "extract_img_feature", "(", "imgs", ")", "\n", "text_features", "=", "self", ".", "extract_text_feature", "(", "text_query", ",", "self", ".", "use_bert", ")", "\n", "\n", "return", "self", ".", "compose_img_text_features", "(", "img_features", ",", "text_features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ComposeAE.compose_img_text_features": [[326, 339], ["torch.autograd.Variable", "torch.autograd.Variable", "img_text_composition_models.ComposeAE.encoderLinear", "img_text_composition_models.ComposeAE.encoderWithConv", "torch.cuda.FloatTensor().fill_", "torch.cuda.FloatTensor().fill_", "torch.cuda.FloatTensor().fill_", "torch.cuda.FloatTensor().fill_", "img_text_composition_models.ComposeAE.decoder", "img_text_composition_models.ComposeAE.txtdecoder", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor", "torch.cuda.FloatTensor"], "methods", ["None"], ["", "def", "compose_img_text_features", "(", "self", ",", "img_features", ",", "text_features", ",", "CONJUGATE", "=", "Variable", "(", "torch", ".", "cuda", ".", "FloatTensor", "(", "32", ",", "1", ")", ".", "fill_", "(", "1.0", ")", ",", "requires_grad", "=", "False", ")", ")", ":", "\n", "        ", "theta_linear", "=", "self", ".", "encoderLinear", "(", "(", "img_features", ",", "text_features", ",", "CONJUGATE", ")", ")", "\n", "theta_conv", "=", "self", ".", "encoderWithConv", "(", "(", "img_features", ",", "text_features", ",", "CONJUGATE", ")", ")", "\n", "theta", "=", "theta_linear", "*", "self", ".", "a", "[", "1", "]", "+", "theta_conv", "*", "self", ".", "a", "[", "0", "]", "\n", "\n", "dct_with_representations", "=", "{", "\"repres\"", ":", "theta", ",", "\n", "\"repr_to_compare_with_source\"", ":", "self", ".", "decoder", "(", "theta", ")", ",", "\n", "\"repr_to_compare_with_mods\"", ":", "self", ".", "txtdecoder", "(", "theta", ")", ",", "\n", "\"img_features\"", ":", "img_features", ",", "\n", "\"text_features\"", ":", "text_features", "\n", "}", "\n", "\n", "return", "dct_with_representations", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealConCatModule.__init__": [[342, 344], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealConCatModule.forward": [[345, 348], ["torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "concat_x", "=", "torch", ".", "cat", "(", "x", ",", "-", "1", ")", "\n", "return", "concat_x", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealLinearMapping.__init__": [[354, 362], ["super().__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "image_embed_dim", "=", "512", ",", "text_embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mapping", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "text_embed_dim", "+", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "text_embed_dim", "+", "image_embed_dim", ",", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealLinearMapping.forward": [[364, 367], ["img_text_composition_models.RealLinearMapping.mapping"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "theta_linear", "=", "self", ".", "mapping", "(", "x", ")", "\n", "return", "theta_linear", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealConvMapping.__init__": [[373, 385], ["super().__init__", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.Conv1d", "torch.nn.AdaptiveMaxPool1d", "torch.nn.AdaptiveMaxPool1d", "torch.nn.AdaptiveMaxPool1d", "torch.nn.AdaptiveMaxPool1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "image_embed_dim", "=", "512", ",", "text_embed_dim", "=", "768", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "mapping", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "text_embed_dim", "+", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "text_embed_dim", "+", "image_embed_dim", ",", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", "\n", ")", "\n", "# in_channels, output channels", "\n", "self", ".", "conv1", "=", "torch", ".", "nn", ".", "Conv1d", "(", "1", ",", "64", ",", "kernel_size", "=", "3", ",", "padding", "=", "1", ")", "\n", "self", ".", "adaptivepooling", "=", "torch", ".", "nn", ".", "AdaptiveMaxPool1d", "(", "20", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealConvMapping.forward": [[386, 392], ["img_text_composition_models.RealConvMapping.conv1", "img_text_composition_models.RealConvMapping.adaptivepooling", "img_text_composition_models.RealConvMapping.reshape", "img_text_composition_models.RealConvMapping.mapping", "x.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "concat_x", "=", "self", ".", "conv1", "(", "x", ".", "unsqueeze", "(", "1", ")", ")", "\n", "concat_x", "=", "self", ".", "adaptivepooling", "(", "concat_x", ")", "\n", "final_vec", "=", "concat_x", ".", "reshape", "(", "(", "concat_x", ".", "shape", "[", "0", "]", ",", "1280", ")", ")", "\n", "theta_conv", "=", "self", ".", "mapping", "(", "final_vec", ")", "\n", "return", "theta_conv", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.__init__": [[402, 430], ["img_text_composition_models.ImgEncoderTextEncoderBase.__init__", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Parameter", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Sequential", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "img_text_composition_models.RealConCatModule", "img_text_composition_models.RealLinearMapping", "img_text_composition_models.RealConCatModule", "img_text_composition_models.RealConvMapping", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.BatchNorm1d", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "text_query", ",", "image_embed_dim", ",", "text_embed_dim", ",", "use_bert", ",", "name", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "text_query", ",", "image_embed_dim", ",", "text_embed_dim", ",", "use_bert", ",", "name", ")", "\n", "self", ".", "a", "=", "torch", ".", "nn", ".", "Parameter", "(", "torch", ".", "tensor", "(", "[", "1.0", ",", "10.0", ",", "1.0", ",", "1.0", "]", ")", ")", "\n", "self", ".", "use_bert", "=", "use_bert", "\n", "\n", "# merged_dim = image_embed_dim + text_embed_dim", "\n", "\n", "self", ".", "encoderLinear", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "RealConCatModule", "(", ")", ",", "\n", "RealLinearMapping", "(", ")", "\n", ")", "\n", "self", ".", "encoderWithConv", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "RealConCatModule", "(", ")", ",", "\n", "RealConvMapping", "(", ")", "\n", ")", "\n", "self", ".", "decoder", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "image_embed_dim", ")", "\n", ")", "\n", "self", ".", "txtdecoder", "=", "torch", ".", "nn", ".", "Sequential", "(", "\n", "torch", ".", "nn", ".", "BatchNorm1d", "(", "image_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "image_embed_dim", ",", "text_embed_dim", ")", ",", "\n", "torch", ".", "nn", ".", "ReLU", "(", ")", ",", "\n", "torch", ".", "nn", ".", "Linear", "(", "text_embed_dim", ",", "text_embed_dim", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text": [[432, 437], ["img_text_composition_models.RealSpaceConcatAE.extract_img_feature", "img_text_composition_models.RealSpaceConcatAE.extract_text_feature", "img_text_composition_models.RealSpaceConcatAE.compose_img_text_features"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_img_feature", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.ImgEncoderTextEncoderBase.extract_text_feature", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text_features"], ["", "def", "compose_img_text", "(", "self", ",", "imgs", ",", "text_query", ")", ":", "\n", "        ", "img_features", "=", "self", ".", "extract_img_feature", "(", "imgs", ")", "\n", "text_features", "=", "self", ".", "extract_text_feature", "(", "text_query", ",", "self", ".", "use_bert", ")", "\n", "\n", "return", "self", ".", "compose_img_text_features", "(", "img_features", ",", "text_features", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.img_text_composition_models.RealSpaceConcatAE.compose_img_text_features": [[438, 451], ["img_text_composition_models.RealSpaceConcatAE.encoderLinear", "img_text_composition_models.RealSpaceConcatAE.encoderWithConv", "img_text_composition_models.RealSpaceConcatAE.decoder", "img_text_composition_models.RealSpaceConcatAE.txtdecoder"], "methods", ["None"], ["", "def", "compose_img_text_features", "(", "self", ",", "img_features", ",", "text_features", ")", ":", "\n", "        ", "theta_linear", "=", "self", ".", "encoderLinear", "(", "(", "img_features", ",", "text_features", ")", ")", "\n", "theta_conv", "=", "self", ".", "encoderWithConv", "(", "(", "img_features", ",", "text_features", ")", ")", "\n", "theta", "=", "theta_linear", "*", "self", ".", "a", "[", "1", "]", "+", "theta_conv", "*", "self", ".", "a", "[", "0", "]", "\n", "\n", "dct_with_representations", "=", "{", "\"repres\"", ":", "theta", ",", "\n", "\"repr_to_compare_with_source\"", ":", "self", ".", "decoder", "(", "theta", ")", ",", "\n", "\"repr_to_compare_with_mods\"", ":", "self", ".", "txtdecoder", "(", "theta", ")", ",", "\n", "\"img_features\"", ":", "img_features", ",", "\n", "\"text_features\"", ":", "text_features", "\n", "}", "\n", "\n", "return", "dct_with_representations", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.__init__": [[32, 36], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "BaseDataset", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "imgs", "=", "[", "]", "\n", "self", ".", "test_queries", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.get_loader": [[37, 49], ["torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader"], "methods", ["None"], ["", "def", "get_loader", "(", "self", ",", "\n", "batch_size", ",", "\n", "shuffle", "=", "False", ",", "\n", "drop_last", "=", "False", ",", "\n", "num_workers", "=", "0", ")", ":", "\n", "        ", "return", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "self", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "drop_last", "=", "drop_last", ",", "\n", "collate_fn", "=", "lambda", "i", ":", "i", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.get_test_queries": [[50, 52], ["None"], "methods", ["None"], ["", "def", "get_test_queries", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "test_queries", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.get_all_texts": [[53, 55], ["None"], "methods", ["None"], ["", "def", "get_all_texts", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.__getitem__": [[56, 58], ["datasets.BaseDataset.generate_random_query_target"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.generate_random_query_target"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "return", "self", ".", "generate_random_query_target", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.generate_random_query_target": [[59, 61], ["None"], "methods", ["None"], ["", "def", "generate_random_query_target", "(", "self", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.BaseDataset.get_img": [[62, 64], ["None"], "methods", ["None"], ["", "def", "get_img", "(", "self", ",", "idx", ",", "raw_img", "=", "False", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.__init__": [[68, 114], ["datasets.BaseDataset.__init__", "print", "s.strip().replace().replace().replace().replace", "print", "len", "datasets.Fashion200k.caption_index_init_", "datasets.Fashion200k.generate_test_queries_", "listdir", "isfile", "open", "f.readlines", "line.split.split.split", "join", "s.strip().replace().replace().replace", "datasets.Fashion200k.__init__.caption_post_process"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.caption_index_init_", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.generate_test_queries_"], ["def", "__init__", "(", "self", ",", "path", ",", "split", "=", "'train'", ",", "transform", "=", "None", ")", ":", "\n", "        ", "super", "(", "Fashion200k", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "img_path", "=", "path", "+", "'/'", "\n", "\n", "# get label files for the split", "\n", "label_path", "=", "path", "+", "'/labels/'", "\n", "from", "os", "import", "listdir", "\n", "from", "os", ".", "path", "import", "isfile", "\n", "from", "os", ".", "path", "import", "join", "\n", "label_files", "=", "[", "\n", "f", "for", "f", "in", "listdir", "(", "label_path", ")", "if", "isfile", "(", "join", "(", "label_path", ",", "f", ")", ")", "\n", "]", "\n", "label_files", "=", "[", "f", "for", "f", "in", "label_files", "if", "split", "in", "f", "]", "\n", "\n", "# read image info from label files", "\n", "self", ".", "imgs", "=", "[", "]", "\n", "\n", "def", "caption_post_process", "(", "s", ")", ":", "\n", "            ", "return", "s", ".", "strip", "(", ")", ".", "replace", "(", "'.'", ",", "\n", "'dotmark'", ")", ".", "replace", "(", "'?'", ",", "'questionmark'", ")", ".", "replace", "(", "\n", "'&'", ",", "'andmark'", ")", ".", "replace", "(", "'*'", ",", "'starmark'", ")", "\n", "\n", "", "for", "filename", "in", "label_files", ":", "\n", "            ", "print", "(", "'read '", "+", "filename", ")", "\n", "with", "open", "(", "label_path", "+", "'/'", "+", "filename", ")", "as", "f", ":", "\n", "                ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "for", "line", "in", "lines", ":", "\n", "                ", "line", "=", "line", ".", "split", "(", "'\t'", ")", "\n", "img", "=", "{", "\n", "'file_path'", ":", "line", "[", "0", "]", ",", "\n", "'detection_score'", ":", "line", "[", "1", "]", ",", "\n", "'captions'", ":", "[", "caption_post_process", "(", "line", "[", "2", "]", ")", "]", ",", "\n", "'split'", ":", "split", ",", "\n", "'modifiable'", ":", "False", "\n", "}", "\n", "self", ".", "imgs", "+=", "[", "img", "]", "\n", "", "", "print", "(", "'Fashion200k:'", ",", "len", "(", "self", ".", "imgs", ")", ",", "'images'", ")", "\n", "\n", "# generate query for training or testing", "\n", "if", "split", "==", "'train'", ":", "\n", "            ", "self", ".", "caption_index_init_", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "generate_test_queries_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.get_different_word": [[115, 126], ["source_caption.split", "target_caption.split"], "methods", ["None"], ["", "", "def", "get_different_word", "(", "self", ",", "source_caption", ",", "target_caption", ")", ":", "\n", "        ", "source_words", "=", "source_caption", ".", "split", "(", ")", "\n", "target_words", "=", "target_caption", ".", "split", "(", ")", "\n", "for", "source_word", "in", "source_words", ":", "\n", "            ", "if", "source_word", "not", "in", "target_words", ":", "\n", "                ", "break", "\n", "", "", "for", "target_word", "in", "target_words", ":", "\n", "            ", "if", "target_word", "not", "in", "source_words", ":", "\n", "                ", "break", "\n", "", "", "mod_str", "=", "'replace '", "+", "source_word", "+", "' with '", "+", "target_word", "\n", "return", "source_word", ",", "target_word", ",", "mod_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.generate_test_queries_": [[127, 148], ["enumerate", "open", "f.readlines", "line.split", "datasets.Fashion200k.get_different_word"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.get_different_word"], ["", "def", "generate_test_queries_", "(", "self", ")", ":", "\n", "        ", "file2imgid", "=", "{", "}", "\n", "for", "i", ",", "img", "in", "enumerate", "(", "self", ".", "imgs", ")", ":", "\n", "            ", "file2imgid", "[", "img", "[", "'file_path'", "]", "]", "=", "i", "\n", "", "with", "open", "(", "self", ".", "img_path", "+", "'/test_queries.txt'", ")", "as", "f", ":", "\n", "            ", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "", "self", ".", "test_queries", "=", "[", "]", "\n", "for", "line", "in", "lines", ":", "\n", "            ", "source_file", ",", "target_file", "=", "line", ".", "split", "(", ")", "\n", "idx", "=", "file2imgid", "[", "source_file", "]", "\n", "target_idx", "=", "file2imgid", "[", "target_file", "]", "\n", "source_caption", "=", "self", ".", "imgs", "[", "idx", "]", "[", "'captions'", "]", "[", "0", "]", "\n", "target_caption", "=", "self", ".", "imgs", "[", "target_idx", "]", "[", "'captions'", "]", "[", "0", "]", "\n", "source_word", ",", "target_word", ",", "mod_str", "=", "self", ".", "get_different_word", "(", "\n", "source_caption", ",", "target_caption", ")", "\n", "self", ".", "test_queries", "+=", "[", "{", "\n", "'source_img_id'", ":", "idx", ",", "\n", "'source_caption'", ":", "source_caption", ",", "\n", "'target_caption'", ":", "target_caption", ",", "\n", "'mod'", ":", "{", "\n", "'str'", ":", "mod_str", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.caption_index_init_": [[151, 195], ["enumerate", "print", "caption2id.keys", "print", "len", "c.split", "caption2imgids[].append", "c.replace", "p.replace().strip.replace().strip.replace().strip", "len", "len", "parent2children_captions[].append", "p.replace().strip.replace().strip.replace", "len"], "methods", ["None"], ["", "", "def", "caption_index_init_", "(", "self", ")", ":", "\n", "        ", "\"\"\" index caption to generate training query-target example on the fly later\"\"\"", "\n", "\n", "# index caption 2 caption_id and caption 2 image_ids", "\n", "caption2id", "=", "{", "}", "\n", "id2caption", "=", "{", "}", "\n", "caption2imgids", "=", "{", "}", "\n", "for", "i", ",", "img", "in", "enumerate", "(", "self", ".", "imgs", ")", ":", "\n", "            ", "for", "c", "in", "img", "[", "'captions'", "]", ":", "\n", "                ", "if", "c", "not", "in", "caption2id", ":", "\n", "                    ", "id2caption", "[", "len", "(", "caption2id", ")", "]", "=", "c", "\n", "caption2id", "[", "c", "]", "=", "len", "(", "caption2id", ")", "\n", "caption2imgids", "[", "c", "]", "=", "[", "]", "\n", "", "caption2imgids", "[", "c", "]", ".", "append", "(", "i", ")", "\n", "", "", "self", ".", "caption2imgids", "=", "caption2imgids", "\n", "print", "(", "len", "(", "caption2imgids", ")", ",", "'unique cations'", ")", "\n", "\n", "# parent captions are 1-word shorter than their children", "\n", "parent2children_captions", "=", "{", "}", "\n", "for", "c", "in", "caption2id", ".", "keys", "(", ")", ":", "\n", "            ", "for", "w", "in", "c", ".", "split", "(", ")", ":", "\n", "                ", "p", "=", "c", ".", "replace", "(", "w", ",", "''", ")", "\n", "p", "=", "p", ".", "replace", "(", "'  '", ",", "' '", ")", ".", "strip", "(", ")", "\n", "if", "p", "not", "in", "parent2children_captions", ":", "\n", "                    ", "parent2children_captions", "[", "p", "]", "=", "[", "]", "\n", "", "if", "c", "not", "in", "parent2children_captions", "[", "p", "]", ":", "\n", "                    ", "parent2children_captions", "[", "p", "]", ".", "append", "(", "c", ")", "\n", "", "", "", "self", ".", "parent2children_captions", "=", "parent2children_captions", "\n", "\n", "# identify parent captions for each image", "\n", "for", "img", "in", "self", ".", "imgs", ":", "\n", "            ", "img", "[", "'modifiable'", "]", "=", "False", "\n", "img", "[", "'parent_captions'", "]", "=", "[", "]", "\n", "", "for", "p", "in", "parent2children_captions", ":", "\n", "            ", "if", "len", "(", "parent2children_captions", "[", "p", "]", ")", ">=", "2", ":", "\n", "                ", "for", "c", "in", "parent2children_captions", "[", "p", "]", ":", "\n", "                    ", "for", "imgid", "in", "caption2imgids", "[", "c", "]", ":", "\n", "                        ", "self", ".", "imgs", "[", "imgid", "]", "[", "'modifiable'", "]", "=", "True", "\n", "self", ".", "imgs", "[", "imgid", "]", "[", "'parent_captions'", "]", "+=", "[", "p", "]", "\n", "", "", "", "", "num_modifiable_imgs", "=", "0", "\n", "for", "img", "in", "self", ".", "imgs", ":", "\n", "            ", "if", "img", "[", "'modifiable'", "]", ":", "\n", "                ", "num_modifiable_imgs", "+=", "1", "\n", "", "", "print", "(", "'Modifiable images'", ",", "num_modifiable_imgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.caption_index_sample_": [[196, 215], ["random.choice", "datasets.Fashion200k.get_different_word", "numpy.random.randint", "random.choice", "random.choice", "len"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.get_different_word"], ["", "def", "caption_index_sample_", "(", "self", ",", "idx", ")", ":", "\n", "        ", "while", "not", "self", ".", "imgs", "[", "idx", "]", "[", "'modifiable'", "]", ":", "\n", "            ", "idx", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "len", "(", "self", ".", "imgs", ")", ")", "\n", "\n", "# find random target image (same parent)", "\n", "", "img", "=", "self", ".", "imgs", "[", "idx", "]", "\n", "while", "True", ":", "\n", "            ", "p", "=", "random", ".", "choice", "(", "img", "[", "'parent_captions'", "]", ")", "\n", "c", "=", "random", ".", "choice", "(", "self", ".", "parent2children_captions", "[", "p", "]", ")", "\n", "if", "c", "not", "in", "img", "[", "'captions'", "]", ":", "\n", "                ", "break", "\n", "", "", "target_idx", "=", "random", ".", "choice", "(", "self", ".", "caption2imgids", "[", "c", "]", ")", "\n", "\n", "# find the word difference between query and target (not in parent caption)", "\n", "source_caption", "=", "self", ".", "imgs", "[", "idx", "]", "[", "'captions'", "]", "[", "0", "]", "\n", "target_caption", "=", "self", ".", "imgs", "[", "target_idx", "]", "[", "'captions'", "]", "[", "0", "]", "\n", "source_word", ",", "target_word", ",", "mod_str", "=", "self", ".", "get_different_word", "(", "\n", "source_caption", ",", "target_caption", ")", "\n", "return", "idx", ",", "target_idx", ",", "source_word", ",", "target_word", ",", "mod_str", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.get_all_texts": [[216, 222], ["texts.append"], "methods", ["None"], ["", "def", "get_all_texts", "(", "self", ")", ":", "\n", "        ", "texts", "=", "[", "]", "\n", "for", "img", "in", "self", ".", "imgs", ":", "\n", "            ", "for", "c", "in", "img", "[", "'captions'", "]", ":", "\n", "                ", "texts", ".", "append", "(", "c", ")", "\n", "", "", "return", "texts", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.__len__": [[223, 225], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "imgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.__getitem__": [[226, 238], ["datasets.Fashion200k.caption_index_sample_", "datasets.Fashion200k.get_img", "datasets.Fashion200k.get_img"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.caption_index_sample_", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "idx", ",", "target_idx", ",", "source_word", ",", "target_word", ",", "mod_str", "=", "self", ".", "caption_index_sample_", "(", "\n", "idx", ")", "\n", "out", "=", "{", "}", "\n", "out", "[", "'source_img_id'", "]", "=", "idx", "\n", "out", "[", "'source_img_data'", "]", "=", "self", ".", "get_img", "(", "idx", ")", "\n", "out", "[", "'source_caption'", "]", "=", "self", ".", "imgs", "[", "idx", "]", "[", "'captions'", "]", "[", "0", "]", "\n", "out", "[", "'target_img_id'", "]", "=", "target_idx", "\n", "out", "[", "'target_img_data'", "]", "=", "self", ".", "get_img", "(", "target_idx", ")", "\n", "out", "[", "'target_caption'", "]", "=", "self", ".", "imgs", "[", "target_idx", "]", "[", "'captions'", "]", "[", "0", "]", "\n", "out", "[", "'mod'", "]", "=", "{", "'str'", ":", "mod_str", "}", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.Fashion200k.get_img": [[239, 249], ["open", "PIL.Image.open", "datasets.Fashion200k.convert", "datasets.Fashion200k.transform"], "methods", ["None"], ["", "def", "get_img", "(", "self", ",", "idx", ",", "raw_img", "=", "False", ")", ":", "\n", "        ", "img_path", "=", "self", ".", "img_path", "+", "self", ".", "imgs", "[", "idx", "]", "[", "'file_path'", "]", "\n", "with", "open", "(", "img_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "img", "=", "PIL", ".", "Image", ".", "open", "(", "f", ")", "\n", "img", "=", "img", ".", "convert", "(", "'RGB'", ")", "\n", "", "if", "raw_img", ":", "\n", "            ", "return", "img", "\n", "", "if", "self", ".", "transform", ":", "\n", "            ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.__init__": [[253, 295], ["datasets.BaseDataset.__init__", "listdir", "datasets.MITStates.caption_index_init_", "f.split", "listdir", "datasets.MITStates.generate_test_queries_", "file_path.endswith"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.caption_index_init_", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.generate_test_queries_"], ["def", "__init__", "(", "self", ",", "path", ",", "split", "=", "'train'", ",", "transform", "=", "None", ")", ":", "\n", "        ", "super", "(", "MITStates", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "path", "=", "path", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "split", "=", "split", "\n", "\n", "self", ".", "imgs", "=", "[", "]", "\n", "test_nouns", "=", "[", "\n", "u'armor'", ",", "u'bracelet'", ",", "u'bush'", ",", "u'camera'", ",", "u'candy'", ",", "u'castle'", ",", "\n", "u'ceramic'", ",", "u'cheese'", ",", "u'clock'", ",", "u'clothes'", ",", "u'coffee'", ",", "u'fan'", ",", "u'fig'", ",", "\n", "u'fish'", ",", "u'foam'", ",", "u'forest'", ",", "u'fruit'", ",", "u'furniture'", ",", "u'garden'", ",", "u'gate'", ",", "\n", "u'glass'", ",", "u'horse'", ",", "u'island'", ",", "u'laptop'", ",", "u'lead'", ",", "u'lightning'", ",", "\n", "u'mirror'", ",", "u'orange'", ",", "u'paint'", ",", "u'persimmon'", ",", "u'plastic'", ",", "u'plate'", ",", "\n", "u'potato'", ",", "u'road'", ",", "u'rubber'", ",", "u'sand'", ",", "u'shell'", ",", "u'sky'", ",", "u'smoke'", ",", "\n", "u'steel'", ",", "u'stream'", ",", "u'table'", ",", "u'tea'", ",", "u'tomato'", ",", "u'vacuum'", ",", "u'wax'", ",", "\n", "u'wheel'", ",", "u'window'", ",", "u'wool'", "\n", "]", "\n", "\n", "from", "os", "import", "listdir", "\n", "for", "f", "in", "listdir", "(", "path", "+", "'/images'", ")", ":", "\n", "            ", "if", "' '", "not", "in", "f", ":", "\n", "                ", "continue", "\n", "", "adj", ",", "noun", "=", "f", ".", "split", "(", ")", "\n", "if", "adj", "==", "'adj'", ":", "\n", "                ", "continue", "\n", "", "if", "split", "==", "'train'", "and", "noun", "in", "test_nouns", ":", "\n", "                ", "continue", "\n", "", "if", "split", "==", "'test'", "and", "noun", "not", "in", "test_nouns", ":", "\n", "                ", "continue", "\n", "\n", "", "for", "file_path", "in", "listdir", "(", "path", "+", "'/images/'", "+", "f", ")", ":", "\n", "                ", "assert", "(", "file_path", ".", "endswith", "(", "'jpg'", ")", ")", "\n", "self", ".", "imgs", "+=", "[", "{", "\n", "'file_path'", ":", "path", "+", "'/images/'", "+", "f", "+", "'/'", "+", "file_path", ",", "\n", "'captions'", ":", "[", "f", "]", ",", "\n", "'adj'", ":", "adj", ",", "\n", "'noun'", ":", "noun", "\n", "}", "]", "\n", "\n", "", "", "self", ".", "caption_index_init_", "(", ")", "\n", "if", "split", "==", "'test'", ":", "\n", "            ", "self", ".", "generate_test_queries_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.get_all_texts": [[296, 301], ["None"], "methods", ["None"], ["", "", "def", "get_all_texts", "(", "self", ")", ":", "\n", "        ", "texts", "=", "[", "]", "\n", "for", "img", "in", "self", ".", "imgs", ":", "\n", "            ", "texts", "+=", "img", "[", "'captions'", "]", "\n", "", "return", "texts", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.__getitem__": [[302, 331], ["datasets.MITStates.get_img", "datasets.MITStates.get_img", "datasets.MITStates.caption_index_sample_", "datasets.MITStates.caption_index_sample_"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.caption_index_sample_", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.caption_index_sample_"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "self", ".", "saved_item", "\n", "", "except", ":", "\n", "            ", "self", ".", "saved_item", "=", "None", "\n", "", "if", "self", ".", "saved_item", "is", "None", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "idx", ",", "target_idx1", "=", "self", ".", "caption_index_sample_", "(", "idx", ")", "\n", "idx", ",", "target_idx2", "=", "self", ".", "caption_index_sample_", "(", "idx", ")", "\n", "if", "self", ".", "imgs", "[", "target_idx1", "]", "[", "'adj'", "]", "!=", "self", ".", "imgs", "[", "target_idx2", "]", "[", "'adj'", "]", ":", "\n", "                    ", "break", "\n", "", "", "idx", ",", "target_idx", "=", "[", "idx", ",", "target_idx1", "]", "\n", "self", ".", "saved_item", "=", "[", "idx", ",", "target_idx2", "]", "\n", "", "else", ":", "\n", "            ", "idx", ",", "target_idx", "=", "self", ".", "saved_item", "\n", "self", ".", "saved_item", "=", "None", "\n", "\n", "", "mod_str", "=", "self", ".", "imgs", "[", "target_idx", "]", "[", "'adj'", "]", "\n", "\n", "return", "{", "\n", "'source_img_id'", ":", "idx", ",", "\n", "'source_img_data'", ":", "self", ".", "get_img", "(", "idx", ")", ",", "\n", "'source_caption'", ":", "self", ".", "imgs", "[", "idx", "]", "[", "'captions'", "]", "[", "0", "]", ",", "\n", "'target_img_id'", ":", "target_idx", ",", "\n", "'target_img_data'", ":", "self", ".", "get_img", "(", "target_idx", ")", ",", "\n", "'noun'", ":", "self", ".", "imgs", "[", "idx", "]", "[", "'noun'", "]", ",", "\n", "'target_caption'", ":", "self", ".", "imgs", "[", "target_idx", "]", "[", "'captions'", "]", "[", "0", "]", ",", "\n", "'mod'", ":", "{", "\n", "'str'", ":", "mod_str", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.caption_index_init_": [[334, 350], ["enumerate", "datasets.MITStates.noun2adjs.items", "datasets.MITStates.caption2imgids[].append", "datasets.MITStates.caption2imgids.keys", "datasets.MITStates.noun2adjs.keys", "datasets.MITStates.noun2adjs[].append", "len"], "methods", ["None"], ["", "def", "caption_index_init_", "(", "self", ")", ":", "\n", "        ", "self", ".", "caption2imgids", "=", "{", "}", "\n", "self", ".", "noun2adjs", "=", "{", "}", "\n", "for", "i", ",", "img", "in", "enumerate", "(", "self", ".", "imgs", ")", ":", "\n", "            ", "cap", "=", "img", "[", "'captions'", "]", "[", "0", "]", "\n", "adj", "=", "img", "[", "'adj'", "]", "\n", "noun", "=", "img", "[", "'noun'", "]", "\n", "if", "cap", "not", "in", "self", ".", "caption2imgids", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "caption2imgids", "[", "cap", "]", "=", "[", "]", "\n", "", "if", "noun", "not", "in", "self", ".", "noun2adjs", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "noun2adjs", "[", "noun", "]", "=", "[", "]", "\n", "", "self", ".", "caption2imgids", "[", "cap", "]", ".", "append", "(", "i", ")", "\n", "if", "adj", "not", "in", "self", ".", "noun2adjs", "[", "noun", "]", ":", "\n", "                ", "self", ".", "noun2adjs", "[", "noun", "]", ".", "append", "(", "adj", ")", "\n", "", "", "for", "noun", ",", "adjs", "in", "self", ".", "noun2adjs", ".", "items", "(", ")", ":", "\n", "            ", "assert", "len", "(", "adjs", ")", ">=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.caption_index_sample_": [[351, 358], ["random.choice", "random.choice"], "methods", ["None"], ["", "", "def", "caption_index_sample_", "(", "self", ",", "idx", ")", ":", "\n", "        ", "noun", "=", "self", ".", "imgs", "[", "idx", "]", "[", "'noun'", "]", "\n", "# adj = self.imgs[idx]['adj']", "\n", "target_adj", "=", "random", ".", "choice", "(", "self", ".", "noun2adjs", "[", "noun", "]", ")", "\n", "target_caption", "=", "target_adj", "+", "' '", "+", "noun", "\n", "target_idx", "=", "random", ".", "choice", "(", "self", ".", "caption2imgids", "[", "target_caption", "]", ")", "\n", "return", "idx", ",", "target_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.generate_test_queries_": [[359, 377], ["enumerate", "print", "len"], "methods", ["None"], ["", "def", "generate_test_queries_", "(", "self", ")", ":", "\n", "        ", "self", ".", "test_queries", "=", "[", "]", "\n", "for", "idx", ",", "img", "in", "enumerate", "(", "self", ".", "imgs", ")", ":", "\n", "            ", "adj", "=", "img", "[", "'adj'", "]", "\n", "noun", "=", "img", "[", "'noun'", "]", "\n", "for", "target_adj", "in", "self", ".", "noun2adjs", "[", "noun", "]", ":", "\n", "                ", "if", "target_adj", "!=", "adj", ":", "\n", "                    ", "mod_str", "=", "target_adj", "\n", "self", ".", "test_queries", "+=", "[", "{", "\n", "'source_img_id'", ":", "idx", ",", "\n", "'source_caption'", ":", "adj", "+", "' '", "+", "noun", ",", "\n", "'target_caption'", ":", "target_adj", "+", "' '", "+", "noun", ",", "\n", "'noun'", ":", "self", ".", "imgs", "[", "idx", "]", "[", "'noun'", "]", ",", "\n", "'mod'", ":", "{", "\n", "'str'", ":", "mod_str", "\n", "}", "\n", "}", "]", "\n", "", "", "", "print", "(", "len", "(", "self", ".", "test_queries", ")", ",", "'test queries'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.__len__": [[378, 380], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "imgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.MITStates.get_img": [[381, 391], ["open", "PIL.Image.open", "datasets.MITStates.convert", "datasets.MITStates.transform"], "methods", ["None"], ["", "def", "get_img", "(", "self", ",", "idx", ",", "raw_img", "=", "False", ")", ":", "\n", "        ", "img_path", "=", "self", ".", "imgs", "[", "idx", "]", "[", "'file_path'", "]", "\n", "with", "open", "(", "img_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "img", "=", "PIL", ".", "Image", ".", "open", "(", "f", ")", "\n", "img", "=", "img", ".", "convert", "(", "'RGB'", ")", "\n", "", "if", "raw_img", ":", "\n", "            ", "return", "img", "\n", "", "if", "self", ".", "transform", ":", "\n", "            ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__": [[395, 459], ["datasets.BaseDataset.__init__", "print", "enumerate", "print", "open", "json.load", "cat.split", "open", "json.load", "open", "json.load", "open", "json.load"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__init__"], ["def", "__init__", "(", "self", ",", "path", ",", "split", "=", "'train'", ",", "cat_type", "=", "'all'", ",", "transform", "=", "None", ")", ":", "\n", "        ", "super", "(", "FashionIQ", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "path", "=", "path", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "split", "=", "split", "\n", "self", ".", "imgs", "=", "[", "]", "\n", "\n", "caps_path", "=", "path", "+", "\"captions/\"", "\n", "\n", "train_caps", "=", "[", "\"cap.dress.train.json\"", ",", "\n", "\"cap.shirt.train.json\"", ",", "\n", "\"cap.toptee.train.json\"", "]", "\n", "\n", "val_caps", "=", "[", "\"cap.dress.val.json\"", ",", "\n", "\"cap.shirt.val.json\"", ",", "\n", "\"cap.toptee.val.json\"", "]", "\n", "\n", "# load all pool of images", "\n", "if", "cat_type", "==", "'all'", ":", "\n", "            ", "self", ".", "all_imgs_from_cat", "=", "[", "]", "\n", "for", "c", "in", "[", "'dress'", ",", "'shirt'", ",", "'toptee'", "]", ":", "\n", "                ", "with", "open", "(", "path", "+", "'image_splits/split.'", "+", "c", "+", "'.'", "+", "split", "+", "'.json'", ")", "as", "f", ":", "\n", "                    ", "self", ".", "all_imgs_from_cat", "+=", "json", ".", "load", "(", "f", ")", "\n", "\n", "#  load splits", "\n", "", "", "", "caps", "=", "[", "]", "\n", "if", "split", "==", "'val'", ":", "\n", "            ", "print", "(", "\"Using \"", "+", "cat_type", "+", "\" val data\"", ")", "\n", "if", "cat_type", "==", "'all'", ":", "\n", "                ", "caps", "=", "val_caps", "\n", "", "else", ":", "\n", "                ", "caps", "=", "[", "\"cap.\"", "+", "cat_type", "+", "\".val.json\"", "]", "\n", "with", "open", "(", "path", "+", "'image_splits/split.'", "+", "cat_type", "+", "'.val.json'", ")", "as", "f", ":", "\n", "                    ", "self", ".", "all_imgs_from_cat", "=", "json", ".", "load", "(", "f", ")", "\n", "", "", "", "elif", "split", "==", "'train'", ":", "\n", "            ", "print", "(", "\"Using \"", "+", "cat_type", "+", "\" train data\"", ")", "\n", "if", "cat_type", "==", "'all'", ":", "\n", "                ", "caps", "=", "train_caps", "\n", "", "else", ":", "\n", "                ", "caps", "=", "[", "\"cap.\"", "+", "cat_type", "+", "\".train.json\"", "]", "\n", "with", "open", "(", "path", "+", "'image_splits/split.'", "+", "cat_type", "+", "'.train.json'", ")", "as", "f", ":", "\n", "                    ", "self", ".", "all_imgs_from_cat", "=", "json", ".", "load", "(", "f", ")", "\n", "\n", "", "", "", "for", "cat", "in", "caps", ":", "\n", "            ", "with", "open", "(", "caps_path", "+", "cat", ")", "as", "f", ":", "\n", "                ", "cap2smth", "=", "json", ".", "load", "(", "f", ")", "\n", "", "cat_name", "=", "cat", ".", "split", "(", "'.'", ")", "[", "1", "]", "\n", "for", "idx", ",", "cap", "in", "enumerate", "(", "cap2smth", ")", ":", "\n", "                ", "if", "cat_type", "==", "'all'", ":", "\n", "                    ", "captions", "=", "(", "cat_name", "+", "' '", "+", "' and it '", ".", "join", "(", "cap", "[", "'captions'", "]", ")", ")", ".", "lower", "(", ")", "\n", "", "else", ":", "\n", "                    ", "captions", "=", "' '", ".", "join", "(", "cap", "[", "'captions'", "]", ")", ".", "lower", "(", ")", "\n", "", "d", "=", "{", "\n", "'source_image_path'", ":", "path", "+", "'all_imgs/'", "+", "cap", "[", "'candidate'", "]", "+", "'.jpg'", ",", "\n", "'captions'", ":", "captions", ",", "\n", "'original_captions'", ":", "cap", "[", "'captions'", "]", ",", "\n", "'candidate_image_name'", ":", "cap", "[", "'candidate'", "]", ",", "\n", "'source_img_id'", ":", "idx", ",", "\n", "}", "\n", "if", "split", "!=", "'real_test'", ":", "\n", "                    ", "d", "[", "'target_image_path'", "]", "=", "path", "+", "'all_imgs/'", "+", "cap", "[", "'target'", "]", "+", "'.jpg'", "\n", "d", "[", "'target_image_name'", "]", "=", "cap", "[", "'target'", "]", "\n", "\n", "", "self", ".", "imgs", "+=", "[", "d", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_all_texts": [[460, 465], ["None"], "methods", ["None"], ["", "", "", "def", "get_all_texts", "(", "self", ")", ":", "\n", "        ", "texts", "=", "[", "]", "\n", "for", "img", "in", "self", ".", "imgs", ":", "\n", "            ", "texts", "+=", "img", "[", "'captions'", "]", "\n", "", "return", "texts", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__getitem__": [[466, 483], ["datasets.FashionIQ.get_img", "datasets.FashionIQ.get_img"], "methods", ["home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img", "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "d", "=", "{", "\n", "'source_img_id'", ":", "idx", ",", "\n", "'source_img_data'", ":", "self", ".", "get_img", "(", "idx", ",", "if_target", "=", "False", ")", ",", "\n", "'target_caption'", ":", "self", ".", "imgs", "[", "idx", "]", "[", "'captions'", "]", ",", "\n", "'original_captions'", ":", "self", ".", "imgs", "[", "idx", "]", "[", "'original_captions'", "]", ",", "\n", "'candidate_image_name'", ":", "self", ".", "imgs", "[", "idx", "]", "[", "'candidate_image_name'", "]", ",", "\n", "'mod'", ":", "{", "\n", "'str'", ":", "self", ".", "imgs", "[", "idx", "]", "[", "'captions'", "]", "\n", "}", "\n", "}", "\n", "\n", "if", "self", ".", "split", "!=", "'real_test'", ":", "\n", "            ", "d", "[", "'target_image_name'", "]", "=", "self", ".", "imgs", "[", "idx", "]", "[", "'target_image_name'", "]", "\n", "d", "[", "'target_img_data'", "]", "=", "self", ".", "get_img", "(", "idx", ",", "if_target", "=", "True", ")", "\n", "\n", "", "return", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.__len__": [[484, 486], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "imgs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img": [[487, 501], ["open", "PIL.Image.open", "datasets.FashionIQ.convert", "datasets.FashionIQ.transform"], "methods", ["None"], ["", "def", "get_img", "(", "self", ",", "idx", ",", "if_target", "=", "False", ")", ":", "\n", "        ", "if", "if_target", ":", "\n", "            ", "img_path", "=", "self", ".", "imgs", "[", "idx", "]", "[", "'target_image_path'", "]", "\n", "", "else", ":", "\n", "            ", "img_path", "=", "self", ".", "imgs", "[", "idx", "]", "[", "'source_image_path'", "]", "\n", "\n", "", "with", "open", "(", "img_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "img", "=", "PIL", ".", "Image", ".", "open", "(", "f", ")", "\n", "img", "=", "img", ".", "convert", "(", "'RGB'", ")", "\n", "\n", "", "if", "self", ".", "transform", ":", "\n", "            ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.ecom-research_ComposeAE.None.datasets.FashionIQ.get_img_from_split": [[502, 513], ["open", "PIL.Image.open", "datasets.FashionIQ.convert", "datasets.FashionIQ.transform"], "methods", ["None"], ["", "def", "get_img_from_split", "(", "self", ",", "original_image_id", ")", ":", "\n", "        ", "img_path", "=", "self", ".", "path", "+", "'all_imgs/'", "+", "original_image_id", "+", "'.jpg'", "\n", "\n", "with", "open", "(", "img_path", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "img", "=", "PIL", ".", "Image", ".", "open", "(", "f", ")", "\n", "img", "=", "img", ".", "convert", "(", "'RGB'", ")", "\n", "\n", "", "if", "self", ".", "transform", ":", "\n", "            ", "img", "=", "self", ".", "transform", "(", "img", ")", "\n", "\n", "", "return", "img", "\n", "", "", ""]]}