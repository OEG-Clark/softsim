{"home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertConfig.__init__": [[34, 81], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "16", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "    ", "\"\"\"Constructs BertConfig.\n\n    Args:\n      vocab_size: Vocabulary size of `inputs_ids` in `BertModel`.\n      hidden_size: Size of the encoder layers and the pooler layer.\n      num_hidden_layers: Number of hidden layers in the Transformer encoder.\n      num_attention_heads: Number of attention heads for each attention layer in\n        the Transformer encoder.\n      intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n        layer in the Transformer encoder.\n      hidden_act: The non-linear activation function (function or string) in the\n        encoder and pooler.\n      hidden_dropout_prob: The dropout probability for all fully connected\n        layers in the embeddings, encoder, and pooler.\n      attention_probs_dropout_prob: The dropout ratio for the attention\n        probabilities.\n      max_position_embeddings: The maximum sequence length that this model might\n        ever be used with. Typically set this to something large just in case\n        (e.g., 512 or 1024 or 2048).\n      type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n        `BertModel`.\n      initializer_range: The stdev of the truncated_normal_initializer for\n        initializing all weight matrices.\n    \"\"\"", "\n", "self", ".", "vocab_size", "=", "vocab_size", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertConfig.from_dict": [[82, 89], ["modeling.BertConfig", "six.iteritems"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "    ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size", "=", "None", ")", "\n", "for", "(", "key", ",", "value", ")", "in", "six", ".", "iteritems", "(", "json_object", ")", ":", "\n", "      ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertConfig.from_json_file": [[90, 96], ["cls.from_dict", "tensorflow.gfile.GFile", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "    ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "json_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "      ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertConfig.to_dict": [[97, 101], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "    ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertConfig.to_json_string": [[102, 105], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "    ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertModel.__init__": [[131, 233], ["copy.deepcopy", "modeling.get_shape_list", "tensorflow.ones", "tensorflow.zeros", "tensorflow.variable_scope", "tensorflow.variable_scope", "modeling.embedding_lookup", "modeling.embedding_postprocessor", "tensorflow.variable_scope", "modeling.create_attention_mask_from_input_mask", "modeling.transformer_model", "tensorflow.variable_scope", "tensorflow.squeeze", "tensorflow.layers.dense", "modeling.get_activation", "modeling.create_initializer"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.embedding_lookup", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.embedding_postprocessor", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_attention_mask_from_input_mask", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.transformer_model", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.squeeze", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.get_activation", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_initializer"], ["def", "__init__", "(", "self", ",", "\n", "config", ",", "\n", "is_training", ",", "\n", "input_ids", ",", "\n", "input_mask", "=", "None", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "use_one_hot_embeddings", "=", "False", ",", "\n", "scope", "=", "None", ")", ":", "\n", "    ", "\"\"\"Constructor for BertModel.\n\n    Args:\n      config: `BertConfig` instance.\n      is_training: bool. true for training model, false for eval model. Controls\n        whether dropout will be applied.\n      input_ids: int32 Tensor of shape [batch_size, seq_length].\n      input_mask: (optional) int32 Tensor of shape [batch_size, seq_length].\n      token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      use_one_hot_embeddings: (optional) bool. Whether to use one-hot word\n        embeddings or tf.embedding_lookup() for the word embeddings.\n      scope: (optional) variable scope. Defaults to \"bert\".\n\n    Raises:\n      ValueError: The config is invalid or one of the input tensor shapes\n        is invalid.\n    \"\"\"", "\n", "config", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "if", "not", "is_training", ":", "\n", "      ", "config", ".", "hidden_dropout_prob", "=", "0.0", "\n", "config", ".", "attention_probs_dropout_prob", "=", "0.0", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ",", "expected_rank", "=", "2", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "\n", "if", "input_mask", "is", "None", ":", "\n", "      ", "input_mask", "=", "tf", ".", "ones", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "      ", "token_type_ids", "=", "tf", ".", "zeros", "(", "shape", "=", "[", "batch_size", ",", "seq_length", "]", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "scope", ",", "default_name", "=", "\"bert\"", ")", ":", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "\"embeddings\"", ")", ":", "\n", "# Perform embedding lookup on the word ids.", "\n", "        ", "(", "self", ".", "embedding_output", ",", "self", ".", "embedding_table", ")", "=", "embedding_lookup", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "vocab_size", "=", "config", ".", "vocab_size", ",", "\n", "embedding_size", "=", "config", ".", "hidden_size", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "use_one_hot_embeddings", ")", "\n", "\n", "# Add positional embeddings and token type embeddings, then layer", "\n", "# normalize and perform dropout.", "\n", "self", ".", "embedding_output", "=", "embedding_postprocessor", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "use_token_type", "=", "True", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "token_type_vocab_size", "=", "config", ".", "type_vocab_size", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "max_position_embeddings", "=", "config", ".", "max_position_embeddings", ",", "\n", "dropout_prob", "=", "config", ".", "hidden_dropout_prob", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"encoder\"", ")", ":", "\n", "# This converts a 2D mask of shape [batch_size, seq_length] to a 3D", "\n", "# mask of shape [batch_size, seq_length, seq_length] which is used", "\n", "# for the attention scores.", "\n", "        ", "attention_mask", "=", "create_attention_mask_from_input_mask", "(", "\n", "input_ids", ",", "input_mask", ")", "\n", "\n", "# Run the stacked transformer.", "\n", "# `sequence_output` shape = [batch_size, seq_length, hidden_size].", "\n", "self", ".", "all_encoder_layers", "=", "transformer_model", "(", "\n", "input_tensor", "=", "self", ".", "embedding_output", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "hidden_size", "=", "config", ".", "hidden_size", ",", "\n", "num_hidden_layers", "=", "config", ".", "num_hidden_layers", ",", "\n", "num_attention_heads", "=", "config", ".", "num_attention_heads", ",", "\n", "intermediate_size", "=", "config", ".", "intermediate_size", ",", "\n", "intermediate_act_fn", "=", "get_activation", "(", "config", ".", "hidden_act", ")", ",", "\n", "hidden_dropout_prob", "=", "config", ".", "hidden_dropout_prob", ",", "\n", "attention_probs_dropout_prob", "=", "config", ".", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "config", ".", "initializer_range", ",", "\n", "do_return_all_layers", "=", "True", ")", "\n", "\n", "", "self", ".", "sequence_output", "=", "self", ".", "all_encoder_layers", "[", "-", "1", "]", "\n", "# The \"pooler\" converts the encoded sequence tensor of shape", "\n", "# [batch_size, seq_length, hidden_size] to a tensor of shape", "\n", "# [batch_size, hidden_size]. This is necessary for segment-level", "\n", "# (or segment-pair-level) classification tasks where we need a fixed", "\n", "# dimensional representation of the segment.", "\n", "with", "tf", ".", "variable_scope", "(", "\"pooler\"", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token. We assume that this has been pre-trained", "\n", "        ", "first_token_tensor", "=", "tf", ".", "squeeze", "(", "self", ".", "sequence_output", "[", ":", ",", "0", ":", "1", ",", ":", "]", ",", "axis", "=", "1", ")", "\n", "self", ".", "pooled_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "first_token_tensor", ",", "\n", "config", ".", "hidden_size", ",", "\n", "activation", "=", "tf", ".", "tanh", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "config", ".", "initializer_range", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertModel.get_pooled_output": [[234, 236], ["None"], "methods", ["None"], ["", "", "", "def", "get_pooled_output", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertModel.get_sequence_output": [[237, 245], ["None"], "methods", ["None"], ["", "def", "get_sequence_output", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gets final hidden layer of encoder.\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the final hidden of the transformer encoder.\n    \"\"\"", "\n", "return", "self", ".", "sequence_output", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertModel.get_all_encoder_layers": [[246, 248], ["None"], "methods", ["None"], ["", "def", "get_all_encoder_layers", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertModel.get_embedding_output": [[249, 259], ["None"], "methods", ["None"], ["", "def", "get_embedding_output", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gets output of the embedding lookup (i.e., input to the transformer).\n\n    Returns:\n      float Tensor of shape [batch_size, seq_length, hidden_size] corresponding\n      to the output of the embedding layer, after summing the word\n      embeddings with the positional embeddings and the token type embeddings,\n      then performing layer normalization. This is the input to the transformer.\n    \"\"\"", "\n", "return", "self", ".", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertModel.get_embedding_table": [[260, 262], ["None"], "methods", ["None"], ["", "def", "get_embedding_table", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "embedding_table", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.gelu": [[264, 278], ["tensorflow.tanh", "numpy.sqrt", "tensorflow.pow"], "function", ["None"], ["", "", "def", "gelu", "(", "x", ")", ":", "\n", "  ", "\"\"\"Gaussian Error Linear Unit.\n\n  This is a smoother version of the RELU.\n  Original paper: https://arxiv.org/abs/1606.08415\n  Args:\n    x: float Tensor to perform activation.\n\n  Returns:\n    `x` with the GELU activation applied.\n  \"\"\"", "\n", "cdf", "=", "0.5", "*", "(", "1.0", "+", "tf", ".", "tanh", "(", "\n", "(", "np", ".", "sqrt", "(", "2", "/", "np", ".", "pi", ")", "*", "(", "x", "+", "0.044715", "*", "tf", ".", "pow", "(", "x", ",", "3", ")", ")", ")", ")", ")", "\n", "return", "x", "*", "cdf", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.get_activation": [[280, 315], ["activation_string.lower", "isinstance", "ValueError"], "function", ["None"], ["", "def", "get_activation", "(", "activation_string", ")", ":", "\n", "  ", "\"\"\"Maps a string to a Python function, e.g., \"relu\" => `tf.nn.relu`.\n\n  Args:\n    activation_string: String name of the activation function.\n\n  Returns:\n    A Python function corresponding to the activation function. If\n    `activation_string` is None, empty, or \"linear\", this will return None.\n    If `activation_string` is not a string, it will return `activation_string`.\n\n  Raises:\n    ValueError: The `activation_string` does not correspond to a known\n      activation.\n  \"\"\"", "\n", "\n", "# We assume that anything that\"s not a string is already an activation", "\n", "# function, so we just return it.", "\n", "if", "not", "isinstance", "(", "activation_string", ",", "six", ".", "string_types", ")", ":", "\n", "    ", "return", "activation_string", "\n", "\n", "", "if", "not", "activation_string", ":", "\n", "    ", "return", "None", "\n", "\n", "", "act", "=", "activation_string", ".", "lower", "(", ")", "\n", "if", "act", "==", "\"linear\"", ":", "\n", "    ", "return", "None", "\n", "", "elif", "act", "==", "\"relu\"", ":", "\n", "    ", "return", "tf", ".", "nn", ".", "relu", "\n", "", "elif", "act", "==", "\"gelu\"", ":", "\n", "    ", "return", "gelu", "\n", "", "elif", "act", "==", "\"tanh\"", ":", "\n", "    ", "return", "tf", ".", "tanh", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Unsupported activation: %s\"", "%", "act", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.get_assignment_map_from_checkpoint": [[317, 342], ["collections.OrderedDict", "tensorflow.train.list_variables", "collections.OrderedDict", "re.match", "re.match.group"], "function", ["None"], ["", "", "def", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", ":", "\n", "  ", "\"\"\"Compute the union of the current variables and checkpoint variables.\"\"\"", "\n", "assignment_map", "=", "{", "}", "\n", "initialized_variable_names", "=", "{", "}", "\n", "\n", "name_to_variable", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "    ", "name", "=", "var", ".", "name", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "name_to_variable", "[", "name", "]", "=", "var", "\n", "\n", "", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "init_checkpoint", ")", "\n", "\n", "assignment_map", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "x", "in", "init_vars", ":", "\n", "    ", "(", "name", ",", "var", ")", "=", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "\n", "if", "name", "not", "in", "name_to_variable", ":", "\n", "      ", "continue", "\n", "", "assignment_map", "[", "name", "]", "=", "name", "\n", "initialized_variable_names", "[", "name", "]", "=", "1", "\n", "initialized_variable_names", "[", "name", "+", "\":0\"", "]", "=", "1", "\n", "\n", "", "return", "(", "assignment_map", ",", "initialized_variable_names", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.dropout": [[344, 360], ["tensorflow.nn.dropout"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.dropout"], ["", "def", "dropout", "(", "input_tensor", ",", "dropout_prob", ")", ":", "\n", "  ", "\"\"\"Perform dropout.\n\n  Args:\n    input_tensor: float Tensor.\n    dropout_prob: Python float. The probability of dropping out a value (NOT of\n      *keeping* a dimension as in `tf.nn.dropout`).\n\n  Returns:\n    A version of `input_tensor` with dropout applied.\n  \"\"\"", "\n", "if", "dropout_prob", "is", "None", "or", "dropout_prob", "==", "0.0", ":", "\n", "    ", "return", "input_tensor", "\n", "\n", "", "output", "=", "tf", ".", "nn", ".", "dropout", "(", "input_tensor", ",", "1.0", "-", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.layer_norm": [[362, 366], ["tensorflow.contrib.layers.layer_norm"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.layer_norm"], ["", "def", "layer_norm", "(", "input_tensor", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Run layer normalization on the last dimension of the tensor.\"\"\"", "\n", "return", "tf", ".", "contrib", ".", "layers", ".", "layer_norm", "(", "\n", "inputs", "=", "input_tensor", ",", "begin_norm_axis", "=", "-", "1", ",", "begin_params_axis", "=", "-", "1", ",", "scope", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.layer_norm_and_dropout": [[368, 373], ["modeling.layer_norm", "modeling.dropout"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.layer_norm", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.dropout"], ["", "def", "layer_norm_and_dropout", "(", "input_tensor", ",", "dropout_prob", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Runs layer normalization followed by dropout.\"\"\"", "\n", "output_tensor", "=", "layer_norm", "(", "input_tensor", ",", "name", ")", "\n", "output_tensor", "=", "dropout", "(", "output_tensor", ",", "dropout_prob", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_initializer": [[375, 378], ["tensorflow.truncated_normal_initializer"], "function", ["None"], ["", "def", "create_initializer", "(", "initializer_range", "=", "0.02", ")", ":", "\n", "  ", "\"\"\"Creates a `truncated_normal_initializer` with the given range.\"\"\"", "\n", "return", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "initializer_range", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.embedding_lookup": [[380, 426], ["tensorflow.get_variable", "tensorflow.reshape", "modeling.get_shape_list", "tensorflow.reshape", "tensorflow.expand_dims", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.gather", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_initializer"], ["", "def", "embedding_lookup", "(", "input_ids", ",", "\n", "vocab_size", ",", "\n", "embedding_size", "=", "128", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "word_embedding_name", "=", "\"word_embeddings\"", ",", "\n", "use_one_hot_embeddings", "=", "False", ")", ":", "\n", "  ", "\"\"\"Looks up words embeddings for id tensor.\n\n  Args:\n    input_ids: int32 Tensor of shape [batch_size, seq_length] containing word\n      ids.\n    vocab_size: int. Size of the embedding vocabulary.\n    embedding_size: int. Width of the word embeddings.\n    initializer_range: float. Embedding initialization range.\n    word_embedding_name: string. Name of the embedding table.\n    use_one_hot_embeddings: bool. If True, use one-hot method for word\n      embeddings. If False, use `tf.gather()`.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, embedding_size].\n  \"\"\"", "\n", "# This function assumes that the input is of shape [batch_size, seq_length,", "\n", "# num_inputs].", "\n", "#", "\n", "# If the input is a 2D tensor of shape [batch_size, seq_length], we", "\n", "# reshape to [batch_size, seq_length, 1].", "\n", "if", "input_ids", ".", "shape", ".", "ndims", "==", "2", ":", "\n", "    ", "input_ids", "=", "tf", ".", "expand_dims", "(", "input_ids", ",", "axis", "=", "[", "-", "1", "]", ")", "\n", "\n", "", "embedding_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "word_embedding_name", ",", "\n", "shape", "=", "[", "vocab_size", ",", "embedding_size", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "flat_input_ids", "=", "tf", ".", "reshape", "(", "input_ids", ",", "[", "-", "1", "]", ")", "\n", "if", "use_one_hot_embeddings", ":", "\n", "    ", "one_hot_input_ids", "=", "tf", ".", "one_hot", "(", "flat_input_ids", ",", "depth", "=", "vocab_size", ")", "\n", "output", "=", "tf", ".", "matmul", "(", "one_hot_input_ids", ",", "embedding_table", ")", "\n", "", "else", ":", "\n", "    ", "output", "=", "tf", ".", "gather", "(", "embedding_table", ",", "flat_input_ids", ")", "\n", "\n", "", "input_shape", "=", "get_shape_list", "(", "input_ids", ")", "\n", "\n", "output", "=", "tf", ".", "reshape", "(", "output", ",", "\n", "input_shape", "[", "0", ":", "-", "1", "]", "+", "[", "input_shape", "[", "-", "1", "]", "*", "embedding_size", "]", ")", "\n", "return", "(", "output", ",", "embedding_table", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.embedding_postprocessor": [[428, 522], ["modeling.get_shape_list", "modeling.layer_norm_and_dropout", "tensorflow.get_variable", "tensorflow.reshape", "tensorflow.one_hot", "tensorflow.matmul", "tensorflow.reshape", "tensorflow.assert_less_equal", "ValueError", "tensorflow.control_dependencies", "tensorflow.get_variable", "tensorflow.slice", "len", "range", "position_broadcast_shape.extend", "tensorflow.reshape", "modeling.create_initializer", "layer_norm_and_dropout.shape.as_list", "position_broadcast_shape.append", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.layer_norm_and_dropout", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_initializer", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_initializer"], ["", "def", "embedding_postprocessor", "(", "input_tensor", ",", "\n", "use_token_type", "=", "False", ",", "\n", "token_type_ids", "=", "None", ",", "\n", "token_type_vocab_size", "=", "16", ",", "\n", "token_type_embedding_name", "=", "\"token_type_embeddings\"", ",", "\n", "use_position_embeddings", "=", "True", ",", "\n", "position_embedding_name", "=", "\"position_embeddings\"", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "dropout_prob", "=", "0.1", ")", ":", "\n", "  ", "\"\"\"Performs various post-processing on a word embedding tensor.\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length,\n      embedding_size].\n    use_token_type: bool. Whether to add embeddings for `token_type_ids`.\n    token_type_ids: (optional) int32 Tensor of shape [batch_size, seq_length].\n      Must be specified if `use_token_type` is True.\n    token_type_vocab_size: int. The vocabulary size of `token_type_ids`.\n    token_type_embedding_name: string. The name of the embedding table variable\n      for token type ids.\n    use_position_embeddings: bool. Whether to add position embeddings for the\n      position of each token in the sequence.\n    position_embedding_name: string. The name of the embedding table variable\n      for positional embeddings.\n    initializer_range: float. Range of the weight initialization.\n    max_position_embeddings: int. Maximum sequence length that might ever be\n      used with this model. This can be longer than the sequence length of\n      input_tensor, but cannot be shorter.\n    dropout_prob: float. Dropout probability applied to the final output tensor.\n\n  Returns:\n    float tensor with same shape as `input_tensor`.\n\n  Raises:\n    ValueError: One of the tensor shapes or input values is invalid.\n  \"\"\"", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "width", "=", "input_shape", "[", "2", "]", "\n", "\n", "output", "=", "input_tensor", "\n", "\n", "if", "use_token_type", ":", "\n", "    ", "if", "token_type_ids", "is", "None", ":", "\n", "      ", "raise", "ValueError", "(", "\"`token_type_ids` must be specified if\"", "\n", "\"`use_token_type` is True.\"", ")", "\n", "", "token_type_table", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "token_type_embedding_name", ",", "\n", "shape", "=", "[", "token_type_vocab_size", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# This vocab will be small so we always do one-hot here, since it is always", "\n", "# faster for a small vocabulary.", "\n", "flat_token_type_ids", "=", "tf", ".", "reshape", "(", "token_type_ids", ",", "[", "-", "1", "]", ")", "\n", "one_hot_ids", "=", "tf", ".", "one_hot", "(", "flat_token_type_ids", ",", "depth", "=", "token_type_vocab_size", ")", "\n", "token_type_embeddings", "=", "tf", ".", "matmul", "(", "one_hot_ids", ",", "token_type_table", ")", "\n", "token_type_embeddings", "=", "tf", ".", "reshape", "(", "token_type_embeddings", ",", "\n", "[", "batch_size", ",", "seq_length", ",", "width", "]", ")", "\n", "output", "+=", "token_type_embeddings", "\n", "\n", "", "if", "use_position_embeddings", ":", "\n", "    ", "assert_op", "=", "tf", ".", "assert_less_equal", "(", "seq_length", ",", "max_position_embeddings", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "assert_op", "]", ")", ":", "\n", "      ", "full_position_embeddings", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "position_embedding_name", ",", "\n", "shape", "=", "[", "max_position_embeddings", ",", "width", "]", ",", "\n", "initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "# Since the position embedding table is a learned variable, we create it", "\n", "# using a (long) sequence length `max_position_embeddings`. The actual", "\n", "# sequence length might be shorter than this, for faster training of", "\n", "# tasks that do not have long sequences.", "\n", "#", "\n", "# So `full_position_embeddings` is effectively an embedding table", "\n", "# for position [0, 1, 2, ..., max_position_embeddings-1], and the current", "\n", "# sequence has positions [0, 1, 2, ... seq_length-1], so we can just", "\n", "# perform a slice.", "\n", "position_embeddings", "=", "tf", ".", "slice", "(", "full_position_embeddings", ",", "[", "0", ",", "0", "]", ",", "\n", "[", "seq_length", ",", "-", "1", "]", ")", "\n", "num_dims", "=", "len", "(", "output", ".", "shape", ".", "as_list", "(", ")", ")", "\n", "\n", "# Only the last two dimensions are relevant (`seq_length` and `width`), so", "\n", "# we broadcast among the first dimensions, which is typically just", "\n", "# the batch size.", "\n", "position_broadcast_shape", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "num_dims", "-", "2", ")", ":", "\n", "        ", "position_broadcast_shape", ".", "append", "(", "1", ")", "\n", "", "position_broadcast_shape", ".", "extend", "(", "[", "seq_length", ",", "width", "]", ")", "\n", "position_embeddings", "=", "tf", ".", "reshape", "(", "position_embeddings", ",", "\n", "position_broadcast_shape", ")", "\n", "output", "+=", "position_embeddings", "\n", "\n", "", "", "output", "=", "layer_norm_and_dropout", "(", "output", ",", "dropout_prob", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_attention_mask_from_input_mask": [[524, 556], ["modeling.get_shape_list", "modeling.get_shape_list", "tensorflow.cast", "tensorflow.ones", "tensorflow.reshape"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list"], ["", "def", "create_attention_mask_from_input_mask", "(", "from_tensor", ",", "to_mask", ")", ":", "\n", "  ", "\"\"\"Create 3D attention mask from a 2D tensor mask.\n\n  Args:\n    from_tensor: 2D or 3D Tensor of shape [batch_size, from_seq_length, ...].\n    to_mask: int32 Tensor of shape [batch_size, to_seq_length].\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length, to_seq_length].\n  \"\"\"", "\n", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "\n", "to_shape", "=", "get_shape_list", "(", "to_mask", ",", "expected_rank", "=", "2", ")", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "\n", "to_mask", "=", "tf", ".", "cast", "(", "\n", "tf", ".", "reshape", "(", "to_mask", ",", "[", "batch_size", ",", "1", ",", "to_seq_length", "]", ")", ",", "tf", ".", "float32", ")", "\n", "\n", "# We don't assume that `from_tensor` is a mask (although it could be). We", "\n", "# don't actually care if we attend *from* padding tokens (only *to* padding)", "\n", "# tokens so we create a tensor of all ones.", "\n", "#", "\n", "# `broadcast_ones` = [batch_size, from_seq_length, 1]", "\n", "broadcast_ones", "=", "tf", ".", "ones", "(", "\n", "shape", "=", "[", "batch_size", ",", "from_seq_length", ",", "1", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Here we broadcast along two dimensions to create the mask.", "\n", "mask", "=", "broadcast_ones", "*", "to_mask", "\n", "\n", "return", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.attention_layer": [[558, 752], ["modeling.get_shape_list", "modeling.get_shape_list", "modeling.reshape_to_matrix", "modeling.reshape_to_matrix", "tensorflow.layers.dense", "tensorflow.layers.dense", "tensorflow.layers.dense", "modeling.attention_layer.transpose_for_scores"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.reshape_to_matrix"], ["", "def", "attention_layer", "(", "from_tensor", ",", "\n", "to_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "num_attention_heads", "=", "1", ",", "\n", "size_per_head", "=", "512", ",", "\n", "query_act", "=", "None", ",", "\n", "key_act", "=", "None", ",", "\n", "value_act", "=", "None", ",", "\n", "attention_probs_dropout_prob", "=", "0.0", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_2d_tensor", "=", "False", ",", "\n", "batch_size", "=", "None", ",", "\n", "from_seq_length", "=", "None", ",", "\n", "to_seq_length", "=", "None", ")", ":", "\n", "  ", "\"\"\"Performs multi-headed attention from `from_tensor` to `to_tensor`.\n\n  This is an implementation of multi-headed attention based on \"Attention\n  is all you Need\". If `from_tensor` and `to_tensor` are the same, then\n  this is self-attention. Each timestep in `from_tensor` attends to the\n  corresponding sequence in `to_tensor`, and returns a fixed-with vector.\n\n  This function first projects `from_tensor` into a \"query\" tensor and\n  `to_tensor` into \"key\" and \"value\" tensors. These are (effectively) a list\n  of tensors of length `num_attention_heads`, where each tensor is of shape\n  [batch_size, seq_length, size_per_head].\n\n  Then, the query and key tensors are dot-producted and scaled. These are\n  softmaxed to obtain attention probabilities. The value tensors are then\n  interpolated by these probabilities, then concatenated back to a single\n  tensor and returned.\n\n  In practice, the multi-headed attention are done with transposes and\n  reshapes rather than actual separate tensors.\n\n  Args:\n    from_tensor: float Tensor of shape [batch_size, from_seq_length,\n      from_width].\n    to_tensor: float Tensor of shape [batch_size, to_seq_length, to_width].\n    attention_mask: (optional) int32 Tensor of shape [batch_size,\n      from_seq_length, to_seq_length]. The values should be 1 or 0. The\n      attention scores will effectively be set to -infinity for any positions in\n      the mask that are 0, and will be unchanged for positions that are 1.\n    num_attention_heads: int. Number of attention heads.\n    size_per_head: int. Size of each attention head.\n    query_act: (optional) Activation function for the query transform.\n    key_act: (optional) Activation function for the key transform.\n    value_act: (optional) Activation function for the value transform.\n    attention_probs_dropout_prob: (optional) float. Dropout probability of the\n      attention probabilities.\n    initializer_range: float. Range of the weight initializer.\n    do_return_2d_tensor: bool. If True, the output will be of shape [batch_size\n      * from_seq_length, num_attention_heads * size_per_head]. If False, the\n      output will be of shape [batch_size, from_seq_length, num_attention_heads\n      * size_per_head].\n    batch_size: (Optional) int. If the input is 2D, this might be the batch size\n      of the 3D version of the `from_tensor` and `to_tensor`.\n    from_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `from_tensor`.\n    to_seq_length: (Optional) If the input is 2D, this might be the seq length\n      of the 3D version of the `to_tensor`.\n\n  Returns:\n    float Tensor of shape [batch_size, from_seq_length,\n      num_attention_heads * size_per_head]. (If `do_return_2d_tensor` is\n      true, this will be of shape [batch_size * from_seq_length,\n      num_attention_heads * size_per_head]).\n\n  Raises:\n    ValueError: Any of the arguments or tensor shapes are invalid.\n  \"\"\"", "\n", "\n", "def", "transpose_for_scores", "(", "input_tensor", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "seq_length", ",", "width", ")", ":", "\n", "    ", "output_tensor", "=", "tf", ".", "reshape", "(", "\n", "input_tensor", ",", "[", "batch_size", ",", "seq_length", ",", "num_attention_heads", ",", "width", "]", ")", "\n", "\n", "output_tensor", "=", "tf", ".", "transpose", "(", "output_tensor", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "return", "output_tensor", "\n", "\n", "", "from_shape", "=", "get_shape_list", "(", "from_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "to_shape", "=", "get_shape_list", "(", "to_tensor", ",", "expected_rank", "=", "[", "2", ",", "3", "]", ")", "\n", "\n", "if", "len", "(", "from_shape", ")", "!=", "len", "(", "to_shape", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The rank of `from_tensor` must match the rank of `to_tensor`.\"", ")", "\n", "\n", "", "if", "len", "(", "from_shape", ")", "==", "3", ":", "\n", "    ", "batch_size", "=", "from_shape", "[", "0", "]", "\n", "from_seq_length", "=", "from_shape", "[", "1", "]", "\n", "to_seq_length", "=", "to_shape", "[", "1", "]", "\n", "", "elif", "len", "(", "from_shape", ")", "==", "2", ":", "\n", "    ", "if", "(", "batch_size", "is", "None", "or", "from_seq_length", "is", "None", "or", "to_seq_length", "is", "None", ")", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "\"When passing in rank 2 tensors to attention_layer, the values \"", "\n", "\"for `batch_size`, `from_seq_length`, and `to_seq_length` \"", "\n", "\"must all be specified.\"", ")", "\n", "\n", "# Scalar dimensions referenced here:", "\n", "#   B = batch size (number of sequences)", "\n", "#   F = `from_tensor` sequence length", "\n", "#   T = `to_tensor` sequence length", "\n", "#   N = `num_attention_heads`", "\n", "#   H = `size_per_head`", "\n", "\n", "", "", "from_tensor_2d", "=", "reshape_to_matrix", "(", "from_tensor", ")", "\n", "to_tensor_2d", "=", "reshape_to_matrix", "(", "to_tensor", ")", "\n", "\n", "# `query_layer` = [B*F, N*H]", "\n", "query_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "from_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "query_act", ",", "\n", "name", "=", "\"query\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `key_layer` = [B*T, N*H]", "\n", "key_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "key_act", ",", "\n", "name", "=", "\"key\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `value_layer` = [B*T, N*H]", "\n", "value_layer", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "to_tensor_2d", ",", "\n", "num_attention_heads", "*", "size_per_head", ",", "\n", "activation", "=", "value_act", ",", "\n", "name", "=", "\"value\"", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# `query_layer` = [B, N, F, H]", "\n", "query_layer", "=", "transpose_for_scores", "(", "query_layer", ",", "batch_size", ",", "\n", "num_attention_heads", ",", "from_seq_length", ",", "\n", "size_per_head", ")", "\n", "\n", "# `key_layer` = [B, N, T, H]", "\n", "key_layer", "=", "transpose_for_scores", "(", "key_layer", ",", "batch_size", ",", "num_attention_heads", ",", "\n", "to_seq_length", ",", "size_per_head", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw", "\n", "# attention scores.", "\n", "# `attention_scores` = [B, N, F, T]", "\n", "attention_scores", "=", "tf", ".", "matmul", "(", "query_layer", ",", "key_layer", ",", "transpose_b", "=", "True", ")", "\n", "attention_scores", "=", "tf", ".", "multiply", "(", "attention_scores", ",", "\n", "1.0", "/", "math", ".", "sqrt", "(", "float", "(", "size_per_head", ")", ")", ")", "\n", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "# `attention_mask` = [B, 1, F, T]", "\n", "    ", "attention_mask", "=", "tf", ".", "expand_dims", "(", "attention_mask", ",", "axis", "=", "[", "1", "]", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "adder", "=", "(", "1.0", "-", "tf", ".", "cast", "(", "attention_mask", ",", "tf", ".", "float32", ")", ")", "*", "-", "10000.0", "\n", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "attention_scores", "+=", "adder", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "# `attention_probs` = [B, N, F, T]", "\n", "", "attention_probs", "=", "tf", ".", "nn", ".", "softmax", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "dropout", "(", "attention_probs", ",", "attention_probs_dropout_prob", ")", "\n", "\n", "# `value_layer` = [B, T, N, H]", "\n", "value_layer", "=", "tf", ".", "reshape", "(", "\n", "value_layer", ",", "\n", "[", "batch_size", ",", "to_seq_length", ",", "num_attention_heads", ",", "size_per_head", "]", ")", "\n", "\n", "# `value_layer` = [B, N, T, H]", "\n", "value_layer", "=", "tf", ".", "transpose", "(", "value_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "# `context_layer` = [B, N, F, H]", "\n", "context_layer", "=", "tf", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "\n", "# `context_layer` = [B, F, N, H]", "\n", "context_layer", "=", "tf", ".", "transpose", "(", "context_layer", ",", "[", "0", ",", "2", ",", "1", ",", "3", "]", ")", "\n", "\n", "if", "do_return_2d_tensor", ":", "\n", "# `context_layer` = [B*F, N*H]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", "*", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "", "else", ":", "\n", "# `context_layer` = [B, F, N*H]", "\n", "    ", "context_layer", "=", "tf", ".", "reshape", "(", "\n", "context_layer", ",", "\n", "[", "batch_size", ",", "from_seq_length", ",", "num_attention_heads", "*", "size_per_head", "]", ")", "\n", "\n", "", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.transformer_model": [[754, 893], ["int", "modeling.get_shape_list", "modeling.reshape_to_matrix", "range", "ValueError", "ValueError", "modeling.reshape_from_matrix", "tensorflow.variable_scope", "modeling.reshape_from_matrix", "final_outputs.append", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.layers.dense", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.dropout", "modeling.layer_norm", "all_layer_outputs.append", "tensorflow.variable_scope", "modeling.attention_layer", "attention_heads.append", "len", "tensorflow.concat", "tensorflow.variable_scope", "tensorflow.layers.dense", "modeling.dropout", "modeling.layer_norm", "modeling.create_initializer", "modeling.create_initializer", "modeling.create_initializer"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.reshape_to_matrix", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.reshape_from_matrix", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.dropout", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.layer_norm", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.attention_layer", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.dropout", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.layer_norm", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_initializer", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_initializer", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.create_initializer"], ["", "def", "transformer_model", "(", "input_tensor", ",", "\n", "attention_mask", "=", "None", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "intermediate_act_fn", "=", "gelu", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "do_return_all_layers", "=", "False", ")", ":", "\n", "  ", "\"\"\"Multi-headed, multi-layer Transformer from \"Attention is All You Need\".\n\n  This is almost an exact implementation of the original Transformer encoder.\n\n  See the original paper:\n  https://arxiv.org/abs/1706.03762\n\n  Also see:\n  https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/models/transformer.py\n\n  Args:\n    input_tensor: float Tensor of shape [batch_size, seq_length, hidden_size].\n    attention_mask: (optional) int32 Tensor of shape [batch_size, seq_length,\n      seq_length], with 1 for positions that can be attended to and 0 in\n      positions that should not be.\n    hidden_size: int. Hidden size of the Transformer.\n    num_hidden_layers: int. Number of layers (blocks) in the Transformer.\n    num_attention_heads: int. Number of attention heads in the Transformer.\n    intermediate_size: int. The size of the \"intermediate\" (a.k.a., feed\n      forward) layer.\n    intermediate_act_fn: function. The non-linear activation function to apply\n      to the output of the intermediate/feed-forward layer.\n    hidden_dropout_prob: float. Dropout probability for the hidden layers.\n    attention_probs_dropout_prob: float. Dropout probability of the attention\n      probabilities.\n    initializer_range: float. Range of the initializer (stddev of truncated\n      normal).\n    do_return_all_layers: Whether to also return all layers or just the final\n      layer.\n\n  Returns:\n    float Tensor of shape [batch_size, seq_length, hidden_size], the final\n    hidden layer of the Transformer.\n\n  Raises:\n    ValueError: A Tensor shape or parameter is invalid.\n  \"\"\"", "\n", "if", "hidden_size", "%", "num_attention_heads", "!=", "0", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "hidden_size", ",", "num_attention_heads", ")", ")", "\n", "\n", "", "attention_head_size", "=", "int", "(", "hidden_size", "/", "num_attention_heads", ")", "\n", "input_shape", "=", "get_shape_list", "(", "input_tensor", ",", "expected_rank", "=", "3", ")", "\n", "batch_size", "=", "input_shape", "[", "0", "]", "\n", "seq_length", "=", "input_shape", "[", "1", "]", "\n", "input_width", "=", "input_shape", "[", "2", "]", "\n", "\n", "# The Transformer performs sum residuals on all layers so the input needs", "\n", "# to be the same as the hidden size.", "\n", "if", "input_width", "!=", "hidden_size", ":", "\n", "    ", "raise", "ValueError", "(", "\"The width of the input tensor (%d) != hidden size (%d)\"", "%", "\n", "(", "input_width", ",", "hidden_size", ")", ")", "\n", "\n", "# We keep the representation as a 2D tensor to avoid re-shaping it back and", "\n", "# forth from a 3D tensor to a 2D tensor. Re-shapes are normally free on", "\n", "# the GPU/CPU but may not be free on the TPU, so we want to minimize them to", "\n", "# help the optimizer.", "\n", "", "prev_output", "=", "reshape_to_matrix", "(", "input_tensor", ")", "\n", "\n", "all_layer_outputs", "=", "[", "]", "\n", "for", "layer_idx", "in", "range", "(", "num_hidden_layers", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "\"layer_%d\"", "%", "layer_idx", ")", ":", "\n", "      ", "layer_input", "=", "prev_output", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"attention\"", ")", ":", "\n", "        ", "attention_heads", "=", "[", "]", "\n", "with", "tf", ".", "variable_scope", "(", "\"self\"", ")", ":", "\n", "          ", "attention_head", "=", "attention_layer", "(", "\n", "from_tensor", "=", "layer_input", ",", "\n", "to_tensor", "=", "layer_input", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "num_attention_heads", "=", "num_attention_heads", ",", "\n", "size_per_head", "=", "attention_head_size", ",", "\n", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", ",", "\n", "initializer_range", "=", "initializer_range", ",", "\n", "do_return_2d_tensor", "=", "True", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "from_seq_length", "=", "seq_length", ",", "\n", "to_seq_length", "=", "seq_length", ")", "\n", "attention_heads", ".", "append", "(", "attention_head", ")", "\n", "\n", "", "attention_output", "=", "None", "\n", "if", "len", "(", "attention_heads", ")", "==", "1", ":", "\n", "          ", "attention_output", "=", "attention_heads", "[", "0", "]", "\n", "", "else", ":", "\n", "# In the case where we have other sequences, we just concatenate", "\n", "# them to the self-attention head before the projection.", "\n", "          ", "attention_output", "=", "tf", ".", "concat", "(", "attention_heads", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Run a linear projection of `hidden_size` then add a residual", "\n", "# with `layer_input`.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "          ", "attention_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "attention_output", "=", "dropout", "(", "attention_output", ",", "hidden_dropout_prob", ")", "\n", "attention_output", "=", "layer_norm", "(", "attention_output", "+", "layer_input", ")", "\n", "\n", "# The activation is only applied to the \"intermediate\" hidden layer.", "\n", "", "", "with", "tf", ".", "variable_scope", "(", "\"intermediate\"", ")", ":", "\n", "        ", "intermediate_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "attention_output", ",", "\n", "intermediate_size", ",", "\n", "activation", "=", "intermediate_act_fn", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "\n", "# Down-project back to `hidden_size` then add the residual.", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"output\"", ")", ":", "\n", "        ", "layer_output", "=", "tf", ".", "layers", ".", "dense", "(", "\n", "intermediate_output", ",", "\n", "hidden_size", ",", "\n", "kernel_initializer", "=", "create_initializer", "(", "initializer_range", ")", ")", "\n", "layer_output", "=", "dropout", "(", "layer_output", ",", "hidden_dropout_prob", ")", "\n", "layer_output", "=", "layer_norm", "(", "layer_output", "+", "attention_output", ")", "\n", "prev_output", "=", "layer_output", "\n", "all_layer_outputs", ".", "append", "(", "layer_output", ")", "\n", "\n", "", "", "", "if", "do_return_all_layers", ":", "\n", "    ", "final_outputs", "=", "[", "]", "\n", "for", "layer_output", "in", "all_layer_outputs", ":", "\n", "      ", "final_output", "=", "reshape_from_matrix", "(", "layer_output", ",", "input_shape", ")", "\n", "final_outputs", ".", "append", "(", "final_output", ")", "\n", "", "return", "final_outputs", "\n", "", "else", ":", "\n", "    ", "final_output", "=", "reshape_from_matrix", "(", "prev_output", ",", "input_shape", ")", "\n", "return", "final_output", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.get_shape_list": [[895, 930], ["tensor.shape.as_list", "enumerate", "tensorflow.shape", "modeling.assert_rank", "non_static_indexes.append"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.assert_rank"], ["", "", "def", "get_shape_list", "(", "tensor", ",", "expected_rank", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Returns a list of the shape of tensor, preferring static dimensions.\n\n  Args:\n    tensor: A tf.Tensor object to find the shape of.\n    expected_rank: (optional) int. The expected rank of `tensor`. If this is\n      specified and the `tensor` has a different rank, and exception will be\n      thrown.\n    name: Optional name of the tensor for the error message.\n\n  Returns:\n    A list of dimensions of the shape of tensor. All static dimensions will\n    be returned as python integers, and dynamic dimensions will be returned\n    as tf.Tensor scalars.\n  \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "    ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "if", "expected_rank", "is", "not", "None", ":", "\n", "    ", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", ")", "\n", "\n", "", "shape", "=", "tensor", ".", "shape", ".", "as_list", "(", ")", "\n", "\n", "non_static_indexes", "=", "[", "]", "\n", "for", "(", "index", ",", "dim", ")", "in", "enumerate", "(", "shape", ")", ":", "\n", "    ", "if", "dim", "is", "None", ":", "\n", "      ", "non_static_indexes", ".", "append", "(", "index", ")", "\n", "\n", "", "", "if", "not", "non_static_indexes", ":", "\n", "    ", "return", "shape", "\n", "\n", "", "dyn_shape", "=", "tf", ".", "shape", "(", "tensor", ")", "\n", "for", "index", "in", "non_static_indexes", ":", "\n", "    ", "shape", "[", "index", "]", "=", "dyn_shape", "[", "index", "]", "\n", "", "return", "shape", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.reshape_to_matrix": [[932, 944], ["tensorflow.reshape", "ValueError"], "function", ["None"], ["", "def", "reshape_to_matrix", "(", "input_tensor", ")", ":", "\n", "  ", "\"\"\"Reshapes a >= rank 2 tensor to a rank 2 tensor (i.e., a matrix).\"\"\"", "\n", "ndims", "=", "input_tensor", ".", "shape", ".", "ndims", "\n", "if", "ndims", "<", "2", ":", "\n", "    ", "raise", "ValueError", "(", "\"Input tensor must have at least rank 2. Shape = %s\"", "%", "\n", "(", "input_tensor", ".", "shape", ")", ")", "\n", "", "if", "ndims", "==", "2", ":", "\n", "    ", "return", "input_tensor", "\n", "\n", "", "width", "=", "input_tensor", ".", "shape", "[", "-", "1", "]", "\n", "output_tensor", "=", "tf", ".", "reshape", "(", "input_tensor", ",", "[", "-", "1", ",", "width", "]", ")", "\n", "return", "output_tensor", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.reshape_from_matrix": [[946, 957], ["modeling.get_shape_list", "tensorflow.reshape", "len"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list"], ["", "def", "reshape_from_matrix", "(", "output_tensor", ",", "orig_shape_list", ")", ":", "\n", "  ", "\"\"\"Reshapes a rank 2 tensor back to its original rank >= 2 tensor.\"\"\"", "\n", "if", "len", "(", "orig_shape_list", ")", "==", "2", ":", "\n", "    ", "return", "output_tensor", "\n", "\n", "", "output_shape", "=", "get_shape_list", "(", "output_tensor", ")", "\n", "\n", "orig_dims", "=", "orig_shape_list", "[", "0", ":", "-", "1", "]", "\n", "width", "=", "output_shape", "[", "-", "1", "]", "\n", "\n", "return", "tf", ".", "reshape", "(", "output_tensor", ",", "orig_dims", "+", "[", "width", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.assert_rank": [[959, 987], ["isinstance", "ValueError", "tensorflow.get_variable_scope", "str", "str"], "function", ["None"], ["", "def", "assert_rank", "(", "tensor", ",", "expected_rank", ",", "name", "=", "None", ")", ":", "\n", "  ", "\"\"\"Raises an exception if the tensor rank is not of the expected rank.\n\n  Args:\n    tensor: A tf.Tensor to check the rank of.\n    expected_rank: Python integer or list of integers, expected rank.\n    name: Optional name of the tensor for the error message.\n\n  Raises:\n    ValueError: If the expected shape doesn't match the actual shape.\n  \"\"\"", "\n", "if", "name", "is", "None", ":", "\n", "    ", "name", "=", "tensor", ".", "name", "\n", "\n", "", "expected_rank_dict", "=", "{", "}", "\n", "if", "isinstance", "(", "expected_rank", ",", "six", ".", "integer_types", ")", ":", "\n", "    ", "expected_rank_dict", "[", "expected_rank", "]", "=", "True", "\n", "", "else", ":", "\n", "    ", "for", "x", "in", "expected_rank", ":", "\n", "      ", "expected_rank_dict", "[", "x", "]", "=", "True", "\n", "\n", "", "", "actual_rank", "=", "tensor", ".", "shape", ".", "ndims", "\n", "if", "actual_rank", "not", "in", "expected_rank_dict", ":", "\n", "    ", "scope_name", "=", "tf", ".", "get_variable_scope", "(", ")", ".", "name", "\n", "raise", "ValueError", "(", "\n", "\"For the tensor `%s` in scope `%s`, the actual rank \"", "\n", "\"`%d` (shape = %s) is not equal to the expected rank `%s`\"", "%", "\n", "(", "name", ",", "scope_name", ",", "actual_rank", ",", "str", "(", "tensor", ".", "shape", ")", ",", "str", "(", "expected_rank", ")", ")", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization.AdamWeightDecayOptimizer.__init__": [[90, 107], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["def", "__init__", "(", "self", ",", "\n", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.0", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "None", ",", "\n", "name", "=", "\"AdamWeightDecayOptimizer\"", ")", ":", "\n", "    ", "\"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"", "\n", "super", "(", "AdamWeightDecayOptimizer", ",", "self", ")", ".", "__init__", "(", "False", ",", "name", ")", "\n", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "weight_decay_rate", "=", "weight_decay_rate", "\n", "self", ".", "beta_1", "=", "beta_1", "\n", "self", ".", "beta_2", "=", "beta_2", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "exclude_from_weight_decay", "=", "exclude_from_weight_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization.AdamWeightDecayOptimizer.apply_gradients": [[108, 158], ["tensorflow.group", "optimization.AdamWeightDecayOptimizer._get_variable_name", "tensorflow.get_variable", "tensorflow.get_variable", "optimization.AdamWeightDecayOptimizer._do_use_weight_decay", "assignments.extend", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "param.shape.as_list", "tensorflow.zeros_initializer", "param.shape.as_list", "tensorflow.zeros_initializer", "tensorflow.square", "tensorflow.sqrt", "param.assign", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer._get_variable_name", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer._do_use_weight_decay", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign"], ["", "def", "apply_gradients", "(", "self", ",", "grads_and_vars", ",", "global_step", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "assignments", "=", "[", "]", "\n", "for", "(", "grad", ",", "param", ")", "in", "grads_and_vars", ":", "\n", "      ", "if", "grad", "is", "None", "or", "param", "is", "None", ":", "\n", "        ", "continue", "\n", "\n", "", "param_name", "=", "self", ".", "_get_variable_name", "(", "param", ".", "name", ")", "\n", "\n", "m", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_m\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "v", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_v\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "# Standard Adam update.", "\n", "next_m", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_1", ",", "m", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_1", ",", "grad", ")", ")", "\n", "next_v", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_2", ",", "v", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_2", ",", "\n", "tf", ".", "square", "(", "grad", ")", ")", ")", "\n", "\n", "update", "=", "next_m", "/", "(", "tf", ".", "sqrt", "(", "next_v", ")", "+", "self", ".", "epsilon", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want ot decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "self", ".", "_do_use_weight_decay", "(", "param_name", ")", ":", "\n", "        ", "update", "+=", "self", ".", "weight_decay_rate", "*", "param", "\n", "\n", "", "update_with_lr", "=", "self", ".", "learning_rate", "*", "update", "\n", "\n", "next_param", "=", "param", "-", "update_with_lr", "\n", "\n", "assignments", ".", "extend", "(", "\n", "[", "param", ".", "assign", "(", "next_param", ")", ",", "\n", "m", ".", "assign", "(", "next_m", ")", ",", "\n", "v", ".", "assign", "(", "next_v", ")", "]", ")", "\n", "", "return", "tf", ".", "group", "(", "*", "assignments", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization.AdamWeightDecayOptimizer._do_use_weight_decay": [[159, 168], ["re.search"], "methods", ["None"], ["", "def", "_do_use_weight_decay", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"", "\n", "if", "not", "self", ".", "weight_decay_rate", ":", "\n", "      ", "return", "False", "\n", "", "if", "self", ".", "exclude_from_weight_decay", ":", "\n", "      ", "for", "r", "in", "self", ".", "exclude_from_weight_decay", ":", "\n", "        ", "if", "re", ".", "search", "(", "r", ",", "param_name", ")", "is", "not", "None", ":", "\n", "          ", "return", "False", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization.AdamWeightDecayOptimizer._get_variable_name": [[169, 175], ["re.match", "re.match.group"], "methods", ["None"], ["", "def", "_get_variable_name", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Get the variable name from the tensor name.\"\"\"", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "param_name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "param_name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "return", "param_name", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization.create_optimizer": [[25, 85], ["tensorflow.train.get_or_create_global_step", "tensorflow.constant", "tensorflow.train.polynomial_decay", "optimization.AdamWeightDecayOptimizer", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.clip_by_global_norm", "tf.contrib.tpu.CrossShardOptimizer.apply_gradients", "tensorflow.group", "tensorflow.cast", "tensorflow.constant", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "tensorflow.contrib.tpu.CrossShardOptimizer", "zip", "tf.train.get_or_create_global_step.assign"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer.apply_gradients", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign"], ["def", "create_optimizer", "(", "loss", ",", "init_lr", ",", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ")", ":", "\n", "  ", "\"\"\"Creates an optimizer training op.\"\"\"", "\n", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", "\n", "\n", "learning_rate", "=", "tf", ".", "constant", "(", "value", "=", "init_lr", ",", "shape", "=", "[", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Implements linear decay of the learning rate.", "\n", "learning_rate", "=", "tf", ".", "train", ".", "polynomial_decay", "(", "\n", "learning_rate", ",", "\n", "global_step", ",", "\n", "num_train_steps", ",", "\n", "end_learning_rate", "=", "0.0", ",", "\n", "power", "=", "1.0", ",", "\n", "cycle", "=", "False", ")", "\n", "\n", "# Implements linear warmup. I.e., if global_step < num_warmup_steps, the", "\n", "# learning rate will be `global_step/num_warmup_steps * init_lr`.", "\n", "if", "num_warmup_steps", ":", "\n", "    ", "global_steps_int", "=", "tf", ".", "cast", "(", "global_step", ",", "tf", ".", "int32", ")", "\n", "warmup_steps_int", "=", "tf", ".", "constant", "(", "num_warmup_steps", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "global_steps_float", "=", "tf", ".", "cast", "(", "global_steps_int", ",", "tf", ".", "float32", ")", "\n", "warmup_steps_float", "=", "tf", ".", "cast", "(", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "\n", "warmup_percent_done", "=", "global_steps_float", "/", "warmup_steps_float", "\n", "warmup_learning_rate", "=", "init_lr", "*", "warmup_percent_done", "\n", "\n", "is_warmup", "=", "tf", ".", "cast", "(", "global_steps_int", "<", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "learning_rate", "=", "(", "\n", "(", "1.0", "-", "is_warmup", ")", "*", "learning_rate", "+", "is_warmup", "*", "warmup_learning_rate", ")", "\n", "\n", "# It is recommended that you use this optimizer for fine tuning, since this", "\n", "# is how the model was trained (note that the Adam m/v variables are NOT", "\n", "# loaded from init_checkpoint.)", "\n", "", "optimizer", "=", "AdamWeightDecayOptimizer", "(", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.01", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "[", "\"LayerNorm\"", ",", "\"layer_norm\"", ",", "\"bias\"", "]", ")", "\n", "\n", "if", "use_tpu", ":", "\n", "    ", "optimizer", "=", "tf", ".", "contrib", ".", "tpu", ".", "CrossShardOptimizer", "(", "optimizer", ")", "\n", "\n", "", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "grads", "=", "tf", ".", "gradients", "(", "loss", ",", "tvars", ")", "\n", "\n", "# This is how the model was pre-trained.", "\n", "(", "grads", ",", "_", ")", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "clip_norm", "=", "1.0", ")", "\n", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "grads", ",", "tvars", ")", ",", "global_step", "=", "global_step", ")", "\n", "\n", "# Normally the global step update is done inside of `apply_gradients`.", "\n", "# However, `AdamWeightDecayOptimizer` doesn't do this. But if you use", "\n", "# a different optimizer, you should probably take this line out.", "\n", "new_global_step", "=", "global_step", "+", "1", "\n", "train_op", "=", "tf", ".", "group", "(", "train_op", ",", "[", "global_step", ".", "assign", "(", "new_global_step", ")", "]", ")", "\n", "return", "train_op", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.InputExample.__init__": [[27, 43], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "    ", "\"\"\"Constructs a InputExample.\n\n    Args:\n      guid: Unique id for the example.\n      text_a: string. The untokenized text of the first sequence. For single\n        sequence tasks, only this sequence must be specified.\n      text_b: (Optional) string. The untokenized text of the second sequence.\n        Only must be specified for sequence pair tasks.\n      label: (Optional) string. The label of the example. This should be\n        specified for train and dev examples, but not for test examples.\n    \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.InputFeatures.__init__": [[48, 65], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "input_ids_a", ",", "\n", "input_mask_a", ",", "\n", "segment_ids_a", ",", "\n", "input_ids_b", ",", "\n", "input_mask_b", ",", "\n", "segment_ids_b", ",", "\n", "label_id", ",", "\n", "is_real_example", "=", "True", ")", ":", "\n", "    ", "self", ".", "input_ids_a", "=", "input_ids_a", "\n", "self", ".", "input_mask_a", "=", "input_mask_a", "\n", "self", ".", "segment_ids_a", "=", "segment_ids_a", "\n", "self", ".", "input_ids_b", "=", "input_ids_b", "\n", "self", ".", "input_mask_b", "=", "input_mask_b", "\n", "self", ".", "segment_ids_b", "=", "segment_ids_b", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "is_real_example", "=", "is_real_example", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.DataProcessor.get_train_examples": [[70, 73], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.DataProcessor.get_dev_examples": [[74, 77], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.DataProcessor.get_test_examples": [[78, 81], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"Gets a collection of `InputExample`s for prediction.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.DataProcessor.get_labels": [[82, 85], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.DataProcessor._read_tsv": [[86, 95], ["tensorflow.gfile.Open", "csv.reader", "lines.append"], "methods", ["None"], ["", "@", "classmethod", "\n", "def", "_read_tsv", "(", "cls", ",", "input_file", ",", "quotechar", "=", "None", ")", ":", "\n", "    ", "\"\"\"Reads a tab separated value file.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "      ", "reader", "=", "csv", ".", "reader", "(", "f", ",", "delimiter", "=", "\"\\t\"", ",", "quotechar", "=", "quotechar", ")", "\n", "lines", "=", "[", "]", "\n", "for", "line", "in", "reader", ":", "\n", "        ", "lines", ".", "append", "(", "line", ")", "\n", "", "return", "lines", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.GLUEProcessor.__init__": [[98, 109], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "train_file", "=", "\"train.tsv\"", "\n", "self", ".", "dev_file", "=", "\"dev.tsv\"", "\n", "self", ".", "test_file", "=", "\"test.tsv\"", "\n", "self", ".", "label_column", "=", "None", "\n", "self", ".", "text_a_column", "=", "None", "\n", "self", ".", "text_b_column", "=", "None", "\n", "self", ".", "contains_header", "=", "True", "\n", "self", ".", "test_text_a_column", "=", "None", "\n", "self", ".", "test_text_b_column", "=", "None", "\n", "self", ".", "test_contains_header", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.GLUEProcessor.get_train_examples": [[110, 114], ["siamese_utils.GLUEProcessor._create_examples", "siamese_utils.GLUEProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.QnliRegressionProcessor._create_examples", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.DataProcessor._read_tsv"], ["", "def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "self", ".", "train_file", ")", ")", ",", "\"train\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.GLUEProcessor.get_dev_examples": [[115, 119], ["siamese_utils.GLUEProcessor._create_examples", "siamese_utils.GLUEProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.QnliRegressionProcessor._create_examples", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.DataProcessor._read_tsv"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "self", ".", "dev_file", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.GLUEProcessor.get_test_examples": [[120, 129], ["siamese_utils.GLUEProcessor._create_examples", "siamese_utils.GLUEProcessor._read_tsv", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.QnliRegressionProcessor._create_examples", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.DataProcessor._read_tsv"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "if", "self", ".", "test_text_a_column", "is", "None", ":", "\n", "      ", "self", ".", "test_text_a_column", "=", "self", ".", "text_a_column", "\n", "", "if", "self", ".", "test_text_b_column", "is", "None", ":", "\n", "      ", "self", ".", "test_text_b_column", "=", "self", ".", "text_b_column", "\n", "\n", "", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_tsv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "self", ".", "test_file", ")", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.GLUEProcessor.get_labels": [[130, 133], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.GLUEProcessor._create_examples": [[134, 175], ["enumerate", "examples.append", "len", "tensorflow.logging.warning", "siamese_utils.InputExample", "len", "tensorflow.logging.warning", "siamese_utils.GLUEProcessor.get_labels", "len", "tensorflow.logging.warning", "len"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.SnliTrainProcessor.get_labels"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "    ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "if", "i", "==", "0", "and", "self", ".", "contains_header", "and", "set_type", "!=", "\"test\"", ":", "\n", "        ", "continue", "\n", "", "if", "i", "==", "0", "and", "self", ".", "test_contains_header", "and", "set_type", "==", "\"test\"", ":", "\n", "        ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "\n", "a_column", "=", "(", "self", ".", "text_a_column", "if", "set_type", "!=", "\"test\"", "else", "\n", "self", ".", "test_text_a_column", ")", "\n", "b_column", "=", "(", "self", ".", "text_b_column", "if", "set_type", "!=", "\"test\"", "else", "\n", "self", ".", "test_text_b_column", ")", "\n", "\n", "# there are some incomplete lines in QNLI", "\n", "if", "len", "(", "line", ")", "<=", "a_column", ":", "\n", "        ", "tf", ".", "logging", ".", "warning", "(", "'Incomplete line, ignored.'", ")", "\n", "continue", "\n", "", "text_a", "=", "line", "[", "a_column", "]", "\n", "\n", "if", "b_column", "is", "not", "None", ":", "\n", "        ", "if", "len", "(", "line", ")", "<=", "b_column", ":", "\n", "          ", "tf", ".", "logging", ".", "warning", "(", "'Incomplete line, ignored.'", ")", "\n", "continue", "\n", "", "text_b", "=", "line", "[", "b_column", "]", "\n", "", "else", ":", "\n", "        ", "text_b", "=", "None", "\n", "\n", "", "if", "set_type", "==", "\"test\"", ":", "\n", "        ", "label", "=", "self", ".", "get_labels", "(", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "if", "len", "(", "line", ")", "<=", "self", ".", "label_column", ":", "\n", "          ", "tf", ".", "logging", ".", "warning", "(", "'Incomplete line, ignored.'", ")", "\n", "continue", "\n", "", "label", "=", "line", "[", "self", ".", "label_column", "]", "\n", "if", "len", "(", "label", ")", "==", "0", ":", "\n", "          ", "raise", "Exception", "\n", "", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.StsbProcessor.__init__": [[178, 183], ["siamese_utils.GLUEProcessor.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "StsbProcessor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "label_column", "=", "9", "\n", "self", ".", "text_a_column", "=", "7", "\n", "self", ".", "text_b_column", "=", "8", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.StsbProcessor.get_labels": [[184, 186], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "return", "[", "0.", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.StsbProcessor._create_examples": [[187, 227], ["enumerate", "examples.append", "len", "tensorflow.logging.warning", "float", "siamese_utils.InputExample", "len", "tensorflow.logging.warning", "siamese_utils.StsbProcessor.get_labels", "len", "tensorflow.logging.warning"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.SnliTrainProcessor.get_labels"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "    ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "if", "i", "==", "0", "and", "self", ".", "contains_header", "and", "set_type", "!=", "\"test\"", ":", "\n", "        ", "continue", "\n", "", "if", "i", "==", "0", "and", "self", ".", "test_contains_header", "and", "set_type", "==", "\"test\"", ":", "\n", "        ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "\n", "a_column", "=", "(", "self", ".", "text_a_column", "if", "set_type", "!=", "\"test\"", "else", "\n", "self", ".", "test_text_a_column", ")", "\n", "b_column", "=", "(", "self", ".", "text_b_column", "if", "set_type", "!=", "\"test\"", "else", "\n", "self", ".", "test_text_b_column", ")", "\n", "\n", "# there are some incomplete lines in QNLI", "\n", "if", "len", "(", "line", ")", "<=", "a_column", ":", "\n", "        ", "tf", ".", "logging", ".", "warning", "(", "'Incomplete line, ignored.'", ")", "\n", "continue", "\n", "", "text_a", "=", "line", "[", "a_column", "]", "\n", "\n", "if", "b_column", "is", "not", "None", ":", "\n", "        ", "if", "len", "(", "line", ")", "<=", "b_column", ":", "\n", "          ", "tf", ".", "logging", ".", "warning", "(", "'Incomplete line, ignored.'", ")", "\n", "continue", "\n", "", "text_b", "=", "line", "[", "b_column", "]", "\n", "", "else", ":", "\n", "        ", "text_b", "=", "None", "\n", "\n", "", "if", "set_type", "==", "\"test\"", ":", "\n", "        ", "label", "=", "self", ".", "get_labels", "(", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "if", "len", "(", "line", ")", "<=", "self", ".", "label_column", ":", "\n", "          ", "tf", ".", "logging", ".", "warning", "(", "'Incomplete line, ignored.'", ")", "\n", "continue", "\n", "", "label", "=", "float", "(", "line", "[", "self", ".", "label_column", "]", ")", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.SickRProcessor.__init__": [[231, 243], ["siamese_utils.StsbProcessor.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "SickRProcessor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "train_file", "=", "\"SICK_train.txt\"", "\n", "self", ".", "dev_file", "=", "\"SICK_trial.txt\"", "\n", "self", ".", "test_file", "=", "\"SICK_test_annotated.txt\"", "\n", "self", ".", "label_column", "=", "3", "\n", "self", ".", "text_a_column", "=", "1", "\n", "self", ".", "text_b_column", "=", "2", "\n", "self", ".", "contains_header", "=", "True", "\n", "self", ".", "test_text_a_column", "=", "None", "\n", "self", ".", "test_text_b_column", "=", "None", "\n", "self", ".", "test_contains_header", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.Sts_12_16_Processor.__init__": [[245, 252], ["siamese_utils.GLUEProcessor.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "Sts_12_16_Processor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "train_file", "=", "\"full.txt\"", "\n", "self", ".", "dev_file", "=", "\"full.txt\"", "\n", "self", ".", "test_file", "=", "\"full.txt\"", "\n", "self", ".", "text_a_column", "=", "0", "\n", "self", ".", "text_b_column", "=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.Sts_12_16_Processor.get_labels": [[253, 255], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "return", "[", "0.", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.Sts_12_16_Processor._create_examples": [[256, 268], ["enumerate", "examples.append", "siamese_utils.Sts_12_16_Processor.get_labels", "siamese_utils.InputExample"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.SnliTrainProcessor.get_labels"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "    ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "text_a", "=", "line", "[", "self", ".", "text_a_column", "]", "\n", "text_b", "=", "line", "[", "self", ".", "text_b_column", "]", "\n", "label", "=", "self", ".", "get_labels", "(", ")", "[", "0", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.QqpProcessor.__init__": [[272, 284], ["siamese_utils.GLUEProcessor.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "QqpProcessor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "train_file", "=", "\"train.tsv\"", "\n", "self", ".", "dev_file", "=", "\"dev.tsv\"", "\n", "self", ".", "test_file", "=", "\"test.tsv\"", "\n", "self", ".", "label_column", "=", "5", "\n", "self", ".", "text_a_column", "=", "3", "\n", "self", ".", "text_b_column", "=", "4", "\n", "self", ".", "contains_header", "=", "True", "\n", "self", ".", "test_text_a_column", "=", "1", "\n", "self", ".", "test_text_b_column", "=", "2", "\n", "self", ".", "test_contains_header", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.QqpProcessor.get_labels": [[285, 288], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "0.", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.MrpcRegressionProcessor.__init__": [[291, 296], ["siamese_utils.StsbProcessor.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "MrpcRegressionProcessor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "label_column", "=", "0", "\n", "self", ".", "text_a_column", "=", "3", "\n", "self", ".", "text_b_column", "=", "4", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.QnliRegressionProcessor.__init__": [[299, 304], ["siamese_utils.GLUEProcessor.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["  ", "def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "QnliRegressionProcessor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "label_column", "=", "-", "1", "\n", "self", ".", "text_a_column", "=", "1", "\n", "self", ".", "text_b_column", "=", "2", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.QnliRegressionProcessor.get_labels": [[305, 307], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "return", "[", "0.", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.QnliRegressionProcessor._create_examples": [[308, 349], ["enumerate", "examples.append", "len", "tensorflow.logging.warning", "siamese_utils.InputExample", "len", "tensorflow.logging.warning", "siamese_utils.QnliRegressionProcessor.get_labels", "len", "tensorflow.logging.warning"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.SnliTrainProcessor.get_labels"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "    ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "      ", "if", "i", "==", "0", "and", "self", ".", "contains_header", "and", "set_type", "!=", "\"test\"", ":", "\n", "        ", "continue", "\n", "", "if", "i", "==", "0", "and", "self", ".", "test_contains_header", "and", "set_type", "==", "\"test\"", ":", "\n", "        ", "continue", "\n", "", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "\n", "a_column", "=", "(", "self", ".", "text_a_column", "if", "set_type", "!=", "\"test\"", "else", "\n", "self", ".", "test_text_a_column", ")", "\n", "b_column", "=", "(", "self", ".", "text_b_column", "if", "set_type", "!=", "\"test\"", "else", "\n", "self", ".", "test_text_b_column", ")", "\n", "\n", "# there are some incomplete lines in QNLI", "\n", "if", "len", "(", "line", ")", "<=", "a_column", ":", "\n", "        ", "tf", ".", "logging", ".", "warning", "(", "'Incomplete line, ignored.'", ")", "\n", "continue", "\n", "", "text_a", "=", "line", "[", "a_column", "]", "\n", "\n", "if", "b_column", "is", "not", "None", ":", "\n", "        ", "if", "len", "(", "line", ")", "<=", "b_column", ":", "\n", "          ", "tf", ".", "logging", ".", "warning", "(", "'Incomplete line, ignored.'", ")", "\n", "continue", "\n", "", "text_b", "=", "line", "[", "b_column", "]", "\n", "", "else", ":", "\n", "        ", "text_b", "=", "None", "\n", "\n", "", "if", "set_type", "==", "\"test\"", ":", "\n", "        ", "label", "=", "self", ".", "get_labels", "(", ")", "[", "0", "]", "\n", "", "else", ":", "\n", "        ", "if", "len", "(", "line", ")", "<=", "self", ".", "label_column", ":", "\n", "          ", "tf", ".", "logging", ".", "warning", "(", "'Incomplete line, ignored.'", ")", "\n", "continue", "\n", "", "label_score_map", "=", "{", "\"not_entailment\"", ":", "0", ",", "\"entailment\"", ":", "1", "}", "\n", "label", "=", "label_score_map", "[", "line", "[", "self", ".", "label_column", "]", "]", "\n", "", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.MnliProcessor.__init__": [[353, 365], ["siamese_utils.GLUEProcessor.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "MnliProcessor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "train_file", "=", "\"train.tsv\"", "\n", "self", ".", "dev_file", "=", "\"dev_matched.tsv\"", "\n", "self", ".", "test_file", "=", "\"test_matched.tsv\"", "\n", "self", ".", "label_column", "=", "10", "\n", "self", ".", "text_a_column", "=", "8", "\n", "self", ".", "text_b_column", "=", "9", "\n", "self", ".", "contains_header", "=", "True", "\n", "self", ".", "test_text_a_column", "=", "None", "\n", "self", ".", "test_text_b_column", "=", "None", "\n", "self", ".", "test_contains_header", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.MnliProcessor.get_labels": [[366, 369], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"contradiction\"", ",", "\"entailment\"", ",", "\"neutral\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.SnliTrainProcessor.__init__": [[373, 382], ["siamese_utils.GLUEProcessor.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "SnliTrainProcessor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "train_file", "=", "\"train.tsv\"", "\n", "self", ".", "dev_file", "=", "\"dev.tsv\"", "\n", "self", ".", "test_file", "=", "\"test.tsv\"", "\n", "self", ".", "label_column", "=", "-", "1", "\n", "self", ".", "text_a_column", "=", "7", "\n", "self", ".", "text_b_column", "=", "8", "\n", "self", ".", "contains_header", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.SnliTrainProcessor.get_labels": [[383, 386], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"contradiction\"", ",", "\"entailment\"", ",", "\"neutral\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.SnliDevTestProcessor.__init__": [[389, 392], ["siamese_utils.SnliTrainProcessor.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "super", "(", "SnliDevTestProcessor", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "label_column", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.get_input_mask_segment": [[393, 423], ["tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "len", "len", "len", "tokenizer.convert_tokens_to_ids.append", "input_mask.append", "segment_ids.append", "len", "len", "len", "random.randint", "len"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_tokens_to_ids"], ["", "", "def", "get_input_mask_segment", "(", "text", ",", "\n", "max_seq_length", ",", "tokenizer", ",", "random_mask", "=", "0", ")", ":", "\n", "  ", "tokens", "=", "tokenizer", ".", "tokenize", "(", "text", ")", "\n", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "if", "len", "(", "tokens", ")", ">", "max_seq_length", "-", "2", ":", "\n", "    ", "tokens", "=", "tokens", "[", "0", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "if", "random_mask", ":", "\n", "    ", "tokens", "[", "random", ".", "randint", "(", "0", ",", "len", "(", "tokens", ")", "-", "1", ")", "]", "=", "\"[MASK]\"", "\n", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens", "+", "[", "\"[SEP]\"", "]", "\n", "segment_ids", "=", "[", "0", "for", "_", "in", "tokens", "]", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "while", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "    ", "input_ids", ".", "append", "(", "0", ")", "\n", "input_mask", ".", "append", "(", "0", ")", "\n", "segment_ids", ".", "append", "(", "0", ")", "\n", "\n", "", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "\n", "return", "(", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.convert_single_example": [[424, 467], ["enumerate", "siamese_utils.get_input_mask_segment", "siamese_utils.get_input_mask_segment", "siamese_utils.InputFeatures", "len", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tokenization.printable_text", "tokenization.printable_text", "str", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.get_input_mask_segment", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.get_input_mask_segment", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.printable_text", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.printable_text"], ["", "def", "convert_single_example", "(", "ex_index", ",", "example", ",", "label_list", ",", "max_seq_length", ",", "\n", "tokenizer", ",", "random_mask", "=", "0", ")", ":", "\n", "  ", "\"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"", "\n", "\n", "label_map", "=", "{", "}", "\n", "for", "(", "i", ",", "label", ")", "in", "enumerate", "(", "label_list", ")", ":", "\n", "    ", "label_map", "[", "label", "]", "=", "i", "\n", "\n", "", "input_ids_a", ",", "input_mask_a", ",", "segment_ids_a", ",", "tokens_a", "=", "get_input_mask_segment", "(", "example", ".", "text_a", ",", "max_seq_length", ",", "tokenizer", ",", "random_mask", ")", "\n", "input_ids_b", ",", "input_mask_b", ",", "segment_ids_b", ",", "tokens_b", "=", "get_input_mask_segment", "(", "example", ".", "text_b", ",", "max_seq_length", ",", "tokenizer", ",", "random_mask", ")", "\n", "\n", "if", "len", "(", "label_list", ")", ">", "1", ":", "\n", "    ", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "", "else", ":", "\n", "    ", "label_id", "=", "example", ".", "label", "\n", "\n", "", "if", "ex_index", "<", "5", ":", "\n", "    ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"tokens_a: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens_a", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"tokens_b: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "tokenization", ".", "printable_text", "(", "x", ")", "for", "x", "in", "tokens_b", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_ids_a: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids_a", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_mask_a: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask_a", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"segment_ids_a: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids_a", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_ids_b: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids_b", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_mask_b: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask_b", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"segment_ids_b: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids_b", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"label: %s (id = %s)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "feature", "=", "InputFeatures", "(", "\n", "input_ids_a", "=", "input_ids_a", ",", "\n", "input_mask_a", "=", "input_mask_a", ",", "\n", "segment_ids_a", "=", "segment_ids_a", ",", "\n", "input_ids_b", "=", "input_ids_b", ",", "\n", "input_mask_b", "=", "input_mask_b", ",", "\n", "segment_ids_b", "=", "segment_ids_b", ",", "\n", "label_id", "=", "label_id", ",", "\n", "is_real_example", "=", "True", ")", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.file_based_convert_examples_to_features": [[469, 514], ["tensorflow.python_io.TFRecordWriter", "enumerate", "tf.python_io.TFRecordWriter.close", "range", "tensorflow.logging.info", "siamese_utils.convert_single_example", "collections.OrderedDict", "siamese_utils.file_based_convert_examples_to_features.create_int_feature"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.convert_single_example"], ["", "def", "file_based_convert_examples_to_features", "(", "\n", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ",", "output_file", ",", "\n", "dupe_factor", ",", "label_min", ",", "label_max", ",", "is_training", "=", "False", ")", ":", "\n", "  ", "\"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"", "\n", "\n", "writer", "=", "tf", ".", "python_io", ".", "TFRecordWriter", "(", "output_file", ")", "\n", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "    ", "if", "ex_index", "%", "10000", "==", "0", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"Writing example %d of %d\"", "%", "(", "ex_index", ",", "len", "(", "examples", ")", ")", ")", "\n", "\n", "", "for", "t", "in", "range", "(", "dupe_factor", "if", "is_training", "else", "1", ")", ":", "\n", "      ", "feature", "=", "convert_single_example", "(", "ex_index", ",", "example", ",", "label_list", ",", "\n", "max_seq_length", ",", "tokenizer", ",", "\n", "random_mask", "=", "0", "if", "t", "==", "0", "else", "1", ")", "\n", "\n", "def", "create_int_feature", "(", "values", ")", ":", "\n", "        ", "f", "=", "tf", ".", "train", ".", "Feature", "(", "int64_list", "=", "tf", ".", "train", ".", "Int64List", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "f", "\n", "\n", "", "def", "create_float_feature", "(", "values", ")", ":", "\n", "        ", "f", "=", "tf", ".", "train", ".", "Feature", "(", "float_list", "=", "tf", ".", "train", ".", "FloatList", "(", "value", "=", "list", "(", "values", ")", ")", ")", "\n", "return", "f", "\n", "\n", "", "features", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "features", "[", "\"input_ids_a\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_ids_a", ")", "\n", "features", "[", "\"input_mask_a\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_mask_a", ")", "\n", "features", "[", "\"segment_ids_a\"", "]", "=", "create_int_feature", "(", "feature", ".", "segment_ids_a", ")", "\n", "\n", "features", "[", "\"input_ids_b\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_ids_b", ")", "\n", "features", "[", "\"input_mask_b\"", "]", "=", "create_int_feature", "(", "feature", ".", "input_mask_b", ")", "\n", "features", "[", "\"segment_ids_b\"", "]", "=", "create_int_feature", "(", "feature", ".", "segment_ids_b", ")", "\n", "\n", "if", "len", "(", "label_list", ")", ">", "1", ":", "\n", "        ", "features", "[", "\"label_ids\"", "]", "=", "create_int_feature", "(", "[", "feature", ".", "label_id", "]", ")", "\n", "", "else", ":", "\n", "        ", "features", "[", "\"label_ids\"", "]", "=", "create_float_feature", "(", "\n", "[", "(", "float", "(", "feature", ".", "label_id", ")", "-", "label_min", ")", "/", "(", "label_max", "-", "label_min", ")", "]", ")", "\n", "", "features", "[", "\"is_real_example\"", "]", "=", "create_int_feature", "(", "\n", "[", "int", "(", "feature", ".", "is_real_example", ")", "]", ")", "\n", "\n", "tf_example", "=", "tf", ".", "train", ".", "Example", "(", "features", "=", "tf", ".", "train", ".", "Features", "(", "feature", "=", "features", ")", ")", "\n", "writer", ".", "write", "(", "tf_example", ".", "SerializeToString", "(", ")", ")", "\n", "\n", "", "", "writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.file_based_input_fn_builder": [[516, 579], ["tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.FixedLenFeature", "tensorflow.parse_single_example", "list", "tensorflow.data.TFRecordDataset", "d.repeat.apply", "tf.parse_single_example.keys", "d.repeat.shuffle", "d.repeat.repeat", "tensorflow.contrib.data.map_and_batch", "tensorflow.to_int32", "siamese_utils.file_based_input_fn_builder._decode_record"], "function", ["None"], ["", "def", "file_based_input_fn_builder", "(", "input_file", ",", "seq_length", ",", "is_training", ",", "\n", "drop_remainder", ",", "is_regression", ")", ":", "\n", "  ", "\"\"\"Creates an `input_fn` closure to be passed to Estimator.\"\"\"", "\n", "\n", "name_to_features", "=", "{", "\n", "\"input_ids_a\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_mask_a\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"segment_ids_a\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_ids_b\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"input_mask_b\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"segment_ids_b\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "seq_length", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"label_ids\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", ",", "\n", "\"is_real_example\"", ":", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "int64", ")", ",", "\n", "}", "\n", "\n", "if", "is_regression", ":", "\n", "    ", "name_to_features", "[", "\"label_ids\"", "]", "=", "tf", ".", "FixedLenFeature", "(", "[", "]", ",", "tf", ".", "float32", ")", "\n", "\n", "", "def", "_decode_record", "(", "record", ",", "name_to_features", ")", ":", "\n", "    ", "\"\"\"Decodes a record to a TensorFlow example.\"\"\"", "\n", "example", "=", "tf", ".", "parse_single_example", "(", "record", ",", "name_to_features", ")", "\n", "\n", "# tf.Example only supports tf.int64, but the TPU only supports tf.int32.", "\n", "# So cast all int64 to int32.", "\n", "for", "name", "in", "list", "(", "example", ".", "keys", "(", ")", ")", ":", "\n", "      ", "t", "=", "example", "[", "name", "]", "\n", "if", "t", ".", "dtype", "==", "tf", ".", "int64", ":", "\n", "        ", "t", "=", "tf", ".", "to_int32", "(", "t", ")", "\n", "", "example", "[", "name", "]", "=", "t", "\n", "\n", "", "return", "example", "\n", "\n", "", "def", "input_fn", "(", "mode", ",", "params", ")", ":", "\n", "    ", "\"\"\"The actual input function.\"\"\"", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "batch_size", "=", "params", "[", "\"train_batch_size\"", "]", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "      ", "batch_size", "=", "params", "[", "\"eval_batch_size\"", "]", "\n", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "PREDICT", ":", "\n", "      ", "batch_size", "=", "params", "[", "\"predict_batch_size\"", "]", "\n", "", "else", ":", "\n", "      ", "raise", "NotImplementedError", "\n", "\n", "# For training, we want a lot of parallel reading and shuffling.", "\n", "# For eval, we want no shuffling and parallel reading doesn't matter.", "\n", "", "d", "=", "tf", ".", "data", ".", "TFRecordDataset", "(", "input_file", ")", "\n", "# if is_training:", "\n", "#   d = d.repeat()", "\n", "#   d = d.shuffle(buffer_size=100)", "\n", "\n", "if", "is_training", ":", "\n", "      ", "d", "=", "d", ".", "shuffle", "(", "buffer_size", "=", "1000", ",", "reshuffle_each_iteration", "=", "True", ")", "\n", "d", "=", "d", ".", "repeat", "(", ")", "\n", "\n", "", "d", "=", "d", ".", "apply", "(", "\n", "tf", ".", "contrib", ".", "data", ".", "map_and_batch", "(", "\n", "lambda", "record", ":", "_decode_record", "(", "record", ",", "name_to_features", ")", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "drop_remainder", "=", "drop_remainder", ")", ")", "\n", "\n", "return", "d", "\n", "\n", "", "return", "input_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization_test.TokenizationTest.test_full_tokenizer": [[28, 50], ["tokenization.FullTokenizer", "os.unlink", "tokenization.FullTokenizer.tokenize", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization_test.TokenizationTest.assertAllEqual", "tempfile.NamedTemporaryFile", "tokenization.FullTokenizer.convert_tokens_to_ids", "vocab_writer.write", "vocab_writer.write"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_tokens_to_ids"], ["  ", "def", "test_full_tokenizer", "(", "self", ")", ":", "\n", "    ", "vocab_tokens", "=", "[", "\n", "\"[UNK]\"", ",", "\"[CLS]\"", ",", "\"[SEP]\"", ",", "\"want\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"wa\"", ",", "\"un\"", ",", "\"runn\"", ",", "\n", "\"##ing\"", ",", "\",\"", "\n", "]", "\n", "with", "tempfile", ".", "NamedTemporaryFile", "(", "delete", "=", "False", ")", "as", "vocab_writer", ":", "\n", "      ", "if", "six", ".", "PY2", ":", "\n", "        ", "vocab_writer", ".", "write", "(", "\"\"", ".", "join", "(", "[", "x", "+", "\"\\n\"", "for", "x", "in", "vocab_tokens", "]", ")", ")", "\n", "", "else", ":", "\n", "        ", "vocab_writer", ".", "write", "(", "\"\"", ".", "join", "(", "\n", "[", "x", "+", "\"\\n\"", "for", "x", "in", "vocab_tokens", "]", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", "\n", "\n", "", "vocab_file", "=", "vocab_writer", ".", "name", "\n", "\n", "", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "vocab_file", ")", "\n", "os", ".", "unlink", "(", "vocab_file", ")", "\n", "\n", "tokens", "=", "tokenizer", ".", "tokenize", "(", "u\"UNwant\\u00E9d,running\"", ")", "\n", "self", ".", "assertAllEqual", "(", "tokens", ",", "[", "\"un\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\",\"", ",", "\"runn\"", ",", "\"##ing\"", "]", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", ",", "[", "7", ",", "4", ",", "5", ",", "10", ",", "8", ",", "9", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization_test.TokenizationTest.test_chinese": [[51, 57], ["tokenization.BasicTokenizer", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.BasicTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "test_chinese", "(", "self", ")", ":", "\n", "    ", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "u\"ah\\u535A\\u63A8zz\"", ")", ",", "\n", "[", "u\"ah\"", ",", "u\"\\u535A\"", ",", "u\"\\u63A8\"", ",", "u\"zz\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization_test.TokenizationTest.test_basic_tokenizer_lower": [[58, 65], ["tokenization.BasicTokenizer", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.BasicTokenizer.tokenize", "tokenization.BasicTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "test_basic_tokenizer_lower", "(", "self", ")", ":", "\n", "    ", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "True", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "u\" \\tHeLLo!how  \\n Are yoU?  \"", ")", ",", "\n", "[", "\"hello\"", ",", "\"!\"", ",", "\"how\"", ",", "\"are\"", ",", "\"you\"", ",", "\"?\"", "]", ")", "\n", "self", ".", "assertAllEqual", "(", "tokenizer", ".", "tokenize", "(", "u\"H\\u00E9llo\"", ")", ",", "[", "\"hello\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization_test.TokenizationTest.test_basic_tokenizer_no_lower": [[66, 72], ["tokenization.BasicTokenizer", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.BasicTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "test_basic_tokenizer_no_lower", "(", "self", ")", ":", "\n", "    ", "tokenizer", "=", "tokenization", ".", "BasicTokenizer", "(", "do_lower_case", "=", "False", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "u\" \\tHeLLo!how  \\n Are yoU?  \"", ")", ",", "\n", "[", "\"HeLLo\"", ",", "\"!\"", ",", "\"how\"", ",", "\"Are\"", ",", "\"yoU\"", ",", "\"?\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization_test.TokenizationTest.test_wordpiece_tokenizer": [[73, 92], ["enumerate", "tokenization.WordpieceTokenizer", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.WordpieceTokenizer.tokenize", "tokenization.WordpieceTokenizer.tokenize", "tokenization.WordpieceTokenizer.tokenize"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "test_wordpiece_tokenizer", "(", "self", ")", ":", "\n", "    ", "vocab_tokens", "=", "[", "\n", "\"[UNK]\"", ",", "\"[CLS]\"", ",", "\"[SEP]\"", ",", "\"want\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"wa\"", ",", "\"un\"", ",", "\"runn\"", ",", "\n", "\"##ing\"", "\n", "]", "\n", "\n", "vocab", "=", "{", "}", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "vocab_tokens", ")", ":", "\n", "      ", "vocab", "[", "token", "]", "=", "i", "\n", "", "tokenizer", "=", "tokenization", ".", "WordpieceTokenizer", "(", "vocab", "=", "vocab", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "tokenizer", ".", "tokenize", "(", "\"\"", ")", ",", "[", "]", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "\"unwanted running\"", ")", ",", "\n", "[", "\"un\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"runn\"", ",", "\"##ing\"", "]", ")", "\n", "\n", "self", ".", "assertAllEqual", "(", "\n", "tokenizer", ".", "tokenize", "(", "\"unwantedX running\"", ")", ",", "[", "\"[UNK]\"", ",", "\"runn\"", ",", "\"##ing\"", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization_test.TokenizationTest.test_convert_tokens_to_ids": [[93, 106], ["enumerate", "tokenization_test.TokenizationTest.assertAllEqual", "tokenization.convert_tokens_to_ids"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_tokens_to_ids"], ["", "def", "test_convert_tokens_to_ids", "(", "self", ")", ":", "\n", "    ", "vocab_tokens", "=", "[", "\n", "\"[UNK]\"", ",", "\"[CLS]\"", ",", "\"[SEP]\"", ",", "\"want\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"wa\"", ",", "\"un\"", ",", "\"runn\"", ",", "\n", "\"##ing\"", "\n", "]", "\n", "\n", "vocab", "=", "{", "}", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "vocab_tokens", ")", ":", "\n", "      ", "vocab", "[", "token", "]", "=", "i", "\n", "\n", "", "self", ".", "assertAllEqual", "(", "\n", "tokenization", ".", "convert_tokens_to_ids", "(", "\n", "vocab", ",", "[", "\"un\"", ",", "\"##want\"", ",", "\"##ed\"", ",", "\"runn\"", ",", "\"##ing\"", "]", ")", ",", "[", "7", ",", "4", ",", "5", ",", "8", ",", "9", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization_test.TokenizationTest.test_is_whitespace": [[107, 116], ["tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace", "tokenization._is_whitespace"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_whitespace", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_whitespace", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_whitespace", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_whitespace", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_whitespace", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_whitespace", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_whitespace"], ["", "def", "test_is_whitespace", "(", "self", ")", ":", "\n", "    ", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\" \"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\"\\t\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\"\\r\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\"\\n\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_whitespace", "(", "u\"\\u00A0\"", ")", ")", "\n", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_whitespace", "(", "u\"A\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_whitespace", "(", "u\"-\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization_test.TokenizationTest.test_is_control": [[117, 125], ["tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization._is_control", "tokenization._is_control", "tokenization._is_control", "tokenization._is_control", "tokenization._is_control", "tokenization._is_control"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_control", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_control", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_control", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_control", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_control", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_control"], ["", "def", "test_is_control", "(", "self", ")", ":", "\n", "    ", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_control", "(", "u\"\\u0005\"", ")", ")", "\n", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\"A\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\" \"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\"\\t\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\"\\r\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_control", "(", "u\"\\U0001F4A9\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization_test.TokenizationTest.test_is_punctuation": [[126, 134], ["tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertTrue", "tokenization_test.TokenizationTest.assertFalse", "tokenization_test.TokenizationTest.assertFalse", "tokenization._is_punctuation", "tokenization._is_punctuation", "tokenization._is_punctuation", "tokenization._is_punctuation", "tokenization._is_punctuation", "tokenization._is_punctuation"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_punctuation", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_punctuation", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_punctuation", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_punctuation", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_punctuation", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_punctuation"], ["", "def", "test_is_punctuation", "(", "self", ")", ":", "\n", "    ", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_punctuation", "(", "u\"-\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_punctuation", "(", "u\"$\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_punctuation", "(", "u\"`\"", ")", ")", "\n", "self", ".", "assertTrue", "(", "tokenization", ".", "_is_punctuation", "(", "u\".\"", ")", ")", "\n", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_punctuation", "(", "u\"A\"", ")", ")", "\n", "self", ".", "assertFalse", "(", "tokenization", ".", "_is_punctuation", "(", "u\" \"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer.__init__": [[110, 127], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["def", "__init__", "(", "self", ",", "\n", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.0", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "None", ",", "\n", "name", "=", "\"AdamWeightDecayOptimizer\"", ")", ":", "\n", "    ", "\"\"\"Constructs a AdamWeightDecayOptimizer.\"\"\"", "\n", "super", "(", "AdamWeightDecayOptimizer", ",", "self", ")", ".", "__init__", "(", "False", ",", "name", ")", "\n", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "weight_decay_rate", "=", "weight_decay_rate", "\n", "self", ".", "beta_1", "=", "beta_1", "\n", "self", ".", "beta_2", "=", "beta_2", "\n", "self", ".", "epsilon", "=", "epsilon", "\n", "self", ".", "exclude_from_weight_decay", "=", "exclude_from_weight_decay", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer.apply_gradients": [[128, 178], ["tensorflow.group", "optimization_bert_flow.AdamWeightDecayOptimizer._get_variable_name", "tensorflow.get_variable", "tensorflow.get_variable", "optimization_bert_flow.AdamWeightDecayOptimizer._do_use_weight_decay", "assignments.extend", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "tensorflow.multiply", "param.shape.as_list", "tensorflow.zeros_initializer", "param.shape.as_list", "tensorflow.zeros_initializer", "tensorflow.square", "tensorflow.sqrt", "param.assign", "tensorflow.get_variable.assign", "tensorflow.get_variable.assign"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer._get_variable_name", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer._do_use_weight_decay", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign"], ["", "def", "apply_gradients", "(", "self", ",", "grads_and_vars", ",", "global_step", "=", "None", ",", "name", "=", "None", ")", ":", "\n", "    ", "\"\"\"See base class.\"\"\"", "\n", "assignments", "=", "[", "]", "\n", "for", "(", "grad", ",", "param", ")", "in", "grads_and_vars", ":", "\n", "      ", "if", "grad", "is", "None", "or", "param", "is", "None", ":", "\n", "        ", "continue", "\n", "\n", "", "param_name", "=", "self", ".", "_get_variable_name", "(", "param", ".", "name", ")", "\n", "\n", "m", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_m\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "v", "=", "tf", ".", "get_variable", "(", "\n", "name", "=", "param_name", "+", "\"/adam_v\"", ",", "\n", "shape", "=", "param", ".", "shape", ".", "as_list", "(", ")", ",", "\n", "dtype", "=", "tf", ".", "float32", ",", "\n", "trainable", "=", "False", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "\n", "# Standard Adam update.", "\n", "next_m", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_1", ",", "m", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_1", ",", "grad", ")", ")", "\n", "next_v", "=", "(", "\n", "tf", ".", "multiply", "(", "self", ".", "beta_2", ",", "v", ")", "+", "tf", ".", "multiply", "(", "1.0", "-", "self", ".", "beta_2", ",", "\n", "tf", ".", "square", "(", "grad", ")", ")", ")", "\n", "\n", "update", "=", "next_m", "/", "(", "tf", ".", "sqrt", "(", "next_v", ")", "+", "self", ".", "epsilon", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want ot decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "self", ".", "_do_use_weight_decay", "(", "param_name", ")", ":", "\n", "        ", "update", "+=", "self", ".", "weight_decay_rate", "*", "param", "\n", "\n", "", "update_with_lr", "=", "self", ".", "learning_rate", "*", "update", "\n", "\n", "next_param", "=", "param", "-", "update_with_lr", "\n", "\n", "assignments", ".", "extend", "(", "\n", "[", "param", ".", "assign", "(", "next_param", ")", ",", "\n", "m", ".", "assign", "(", "next_m", ")", ",", "\n", "v", ".", "assign", "(", "next_v", ")", "]", ")", "\n", "", "return", "tf", ".", "group", "(", "*", "assignments", ",", "name", "=", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer._do_use_weight_decay": [[179, 188], ["re.search"], "methods", ["None"], ["", "def", "_do_use_weight_decay", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Whether to use L2 weight decay for `param_name`.\"\"\"", "\n", "if", "not", "self", ".", "weight_decay_rate", ":", "\n", "      ", "return", "False", "\n", "", "if", "self", ".", "exclude_from_weight_decay", ":", "\n", "      ", "for", "r", "in", "self", ".", "exclude_from_weight_decay", ":", "\n", "        ", "if", "re", ".", "search", "(", "r", ",", "param_name", ")", "is", "not", "None", ":", "\n", "          ", "return", "False", "\n", "", "", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer._get_variable_name": [[189, 195], ["re.match", "re.match.group"], "methods", ["None"], ["", "def", "_get_variable_name", "(", "self", ",", "param_name", ")", ":", "\n", "    ", "\"\"\"Get the variable name from the tensor name.\"\"\"", "\n", "m", "=", "re", ".", "match", "(", "\"^(.*):\\\\d+$\"", ",", "param_name", ")", "\n", "if", "m", "is", "not", "None", ":", "\n", "      ", "param_name", "=", "m", ".", "group", "(", "1", ")", "\n", "", "return", "param_name", "\n", "", "", ""]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.create_optimizer": [[25, 106], ["tensorflow.train.get_or_create_global_step", "tensorflow.constant", "tensorflow.train.polynomial_decay", "tensorflow.cast", "tensorflow.constant", "tensorflow.cast", "tensorflow.cast", "tensorflow.cast", "type", "optimization_bert_flow.AdamWeightDecayOptimizer", "tensorflow.gradients", "tensorflow.clip_by_global_norm", "tf.contrib.tpu.CrossShardOptimizer.apply_gradients", "optimization_bert_flow.AdamWeightDecayOptimizer", "tensorflow.trainable_variables", "tensorflow.gradients", "tensorflow.clip_by_global_norm", "tf.contrib.tpu.CrossShardOptimizer.apply_gradients", "tensorflow.group", "tensorflow.contrib.tpu.CrossShardOptimizer", "zip", "tensorflow.contrib.tpu.CrossShardOptimizer", "zip", "tensorflow.trainable_variables", "tf.train.get_or_create_global_step.assign", "v.name.startswith"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer.apply_gradients", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.AdamWeightDecayOptimizer.apply_gradients", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign"], ["def", "create_optimizer", "(", "loss", ",", "flow_loss", ",", "init_lr", ",", "init_flow_lr", ",", "\n", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", ")", ":", "\n", "  ", "\"\"\"Creates an optimizer training op.\"\"\"", "\n", "global_step", "=", "tf", ".", "train", ".", "get_or_create_global_step", "(", ")", "\n", "\n", "learning_rate", "=", "tf", ".", "constant", "(", "value", "=", "init_lr", ",", "shape", "=", "[", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "# Implements linear decay of the learning rate.", "\n", "learning_rate", "=", "tf", ".", "train", ".", "polynomial_decay", "(", "\n", "learning_rate", ",", "\n", "global_step", ",", "\n", "num_train_steps", ",", "\n", "end_learning_rate", "=", "0.0", ",", "\n", "power", "=", "1.0", ",", "\n", "cycle", "=", "False", ")", "\n", "\n", "# Implements linear warmup. I.e., if global_step < num_warmup_steps, the", "\n", "# learning rate will be `global_step/num_warmup_steps * init_lr`.", "\n", "if", "num_warmup_steps", ":", "\n", "    ", "global_steps_int", "=", "tf", ".", "cast", "(", "global_step", ",", "tf", ".", "int32", ")", "\n", "warmup_steps_int", "=", "tf", ".", "constant", "(", "num_warmup_steps", ",", "dtype", "=", "tf", ".", "int32", ")", "\n", "\n", "global_steps_float", "=", "tf", ".", "cast", "(", "global_steps_int", ",", "tf", ".", "float32", ")", "\n", "warmup_steps_float", "=", "tf", ".", "cast", "(", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "\n", "warmup_percent_done", "=", "global_steps_float", "/", "warmup_steps_float", "\n", "warmup_learning_rate", "=", "init_lr", "*", "warmup_percent_done", "\n", "\n", "is_warmup", "=", "tf", ".", "cast", "(", "global_steps_int", "<", "warmup_steps_int", ",", "tf", ".", "float32", ")", "\n", "learning_rate", "=", "(", "\n", "(", "1.0", "-", "is_warmup", ")", "*", "learning_rate", "+", "is_warmup", "*", "warmup_learning_rate", ")", "\n", "\n", "", "if", "type", "(", "flow_loss", ")", "!=", "\"NoneType\"", ":", "\n", "# bert ", "\n", "# It is recommended that you use this optimizer for fine tuning, since this", "\n", "# is how the model was trained (note that the Adam m/v variables are NOT", "\n", "# loaded from init_checkpoint.)", "\n", "    ", "optimizer", "=", "AdamWeightDecayOptimizer", "(", "\n", "learning_rate", "=", "learning_rate", ",", "\n", "weight_decay_rate", "=", "0.01", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "[", "\"LayerNorm\"", ",", "\"layer_norm\"", ",", "\"bias\"", "]", ")", "\n", "if", "use_tpu", ":", "\n", "      ", "optimizer", "=", "tf", ".", "contrib", ".", "tpu", ".", "CrossShardOptimizer", "(", "optimizer", ")", "\n", "\n", "", "tvars", "=", "[", "v", "for", "v", "in", "tf", ".", "trainable_variables", "(", ")", "if", "not", "v", ".", "name", ".", "startswith", "(", "\"bert/flow\"", ")", "]", "\n", "grads", "=", "tf", ".", "gradients", "(", "loss", ",", "tvars", ")", "\n", "(", "grads", ",", "_", ")", "=", "tf", ".", "clip_by_global_norm", "(", "grads", ",", "clip_norm", "=", "1.0", ")", "\n", "train_op", "=", "optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "grads", ",", "tvars", ")", ",", "global_step", "=", "global_step", ")", "\n", "\n", "########################", "\n", "# flow", "\n", "flow_optimizer", "=", "AdamWeightDecayOptimizer", "(", "\n", "learning_rate", "=", "init_flow_lr", ",", "#learning_rate / init_lr * ", "\n", "weight_decay_rate", "=", "0.01", ",", "\n", "beta_1", "=", "0.9", ",", "\n", "beta_2", "=", "0.999", ",", "\n", "epsilon", "=", "1e-6", ",", "\n", "exclude_from_weight_decay", "=", "[", "\"LayerNorm\"", ",", "\"layer_norm\"", ",", "\"bias\"", "]", ")", "\n", "if", "use_tpu", ":", "\n", "      ", "flow_optimizer", "=", "tf", ".", "contrib", ".", "tpu", ".", "CrossShardOptimizer", "(", "flow_optimizer", ")", "\n", "\n", "", "flow_tvars", "=", "tf", ".", "trainable_variables", "(", "\"bert/flow\"", ")", "\n", "flow_grads", "=", "tf", ".", "gradients", "(", "flow_loss", ",", "flow_tvars", ")", "\n", "(", "flow_grads", ",", "_", ")", "=", "tf", ".", "clip_by_global_norm", "(", "flow_grads", ",", "clip_norm", "=", "1.0", ")", "\n", "flow_train_op", "=", "flow_optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "flow_grads", ",", "flow_tvars", ")", ",", "global_step", "=", "global_step", ")", "\n", "\n", "########################", "\n", "\n", "# Normally the global step update is done inside of `apply_gradients`.", "\n", "# However, `AdamWeightDecayOptimizer` doesn't do this. But if you use", "\n", "# a different optimizer, you should probably take this line out.", "\n", "new_global_step", "=", "global_step", "+", "1", "\n", "train_op", "=", "tf", ".", "group", "(", "train_op", ",", "flow_train_op", ",", "[", "global_step", ".", "assign", "(", "new_global_step", ")", "]", ")", "\n", "return", "train_op", "\n", "", "else", ":", "\n", "    ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.FullTokenizer.__init__": [[164, 169], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "tokenization.FullTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.FullTokenizer.tokenize": [[170, 177], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.FullTokenizer.convert_tokens_to_ids": [[178, 180], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.FullTokenizer.convert_ids_to_tokens": [[181, 183], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer.__init__": [[188, 195], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer.tokenize": [[196, 219], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.whitespace_tokenize", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._run_strip_accents": [[220, 230], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._run_split_on_punc": [[231, 250], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._tokenize_chinese_chars": [[251, 263], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._is_chinese_char": [[264, 285], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "    ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "      ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.BasicTokenizer._clean_text": [[286, 298], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_whitespace", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.__init__": [[303, 307], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "200", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.WordpieceTokenizer.tokenize": [[308, 360], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "      ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "        ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "          ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "            ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "            ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "          ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "        ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.validate_case_matches_checkpoint": [[28, 76], ["re.match", "re.match.group", "ValueError"], "function", ["None"], ["def", "validate_case_matches_checkpoint", "(", "do_lower_case", ",", "init_checkpoint", ")", ":", "\n", "  ", "\"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"", "\n", "\n", "# The casing has to be passed in by the user and there is no explicit check", "\n", "# as to whether it matches the checkpoint. The casing information probably", "\n", "# should have been stored in the bert_config.json file, but it's not, so", "\n", "# we have to heuristically detect it to validate.", "\n", "\n", "if", "not", "init_checkpoint", ":", "\n", "    ", "return", "\n", "\n", "", "m", "=", "re", ".", "match", "(", "\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\"", ",", "init_checkpoint", ")", "\n", "if", "m", "is", "None", ":", "\n", "    ", "return", "\n", "\n", "", "model_name", "=", "m", ".", "group", "(", "1", ")", "\n", "\n", "lower_models", "=", "[", "\n", "\"uncased_L-24_H-1024_A-16\"", ",", "\"uncased_L-12_H-768_A-12\"", ",", "\n", "\"multilingual_L-12_H-768_A-12\"", ",", "\"chinese_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "cased_models", "=", "[", "\n", "\"cased_L-12_H-768_A-12\"", ",", "\"cased_L-24_H-1024_A-16\"", ",", "\n", "\"multi_cased_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "is_bad_config", "=", "False", "\n", "if", "model_name", "in", "lower_models", "and", "not", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"False\"", "\n", "case_name", "=", "\"lowercased\"", "\n", "opposite_flag", "=", "\"True\"", "\n", "\n", "", "if", "model_name", "in", "cased_models", "and", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"True\"", "\n", "case_name", "=", "\"cased\"", "\n", "opposite_flag", "=", "\"False\"", "\n", "\n", "", "if", "is_bad_config", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"", "\n", "\"However, `%s` seems to be a %s model, so you \"", "\n", "\"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"", "\n", "\"how the model was pre-training. If this error is wrong, please \"", "\n", "\"just comment out this check.\"", "%", "(", "actual_flag", ",", "init_checkpoint", ",", "\n", "model_name", ",", "case_name", ",", "opposite_flag", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_to_unicode": [[78, 96], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "convert_to_unicode", "(", "text", ")", ":", "\n", "  ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.printable_text": [[98, 119], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "  ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.load_vocab": [[121, 134], ["collections.OrderedDict", "tensorflow.gfile.GFile", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "  ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "        ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_by_vocab": [[136, 142], ["output.append"], "function", ["None"], ["", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "  ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "    ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_tokens_to_ids": [[144, 146], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_ids_to_tokens": [[148, 150], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.whitespace_tokenize": [[152, 159], ["text.strip.strip", "text.strip.split"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_whitespace": [[362, 372], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_control": [[374, 384], ["unicodedata.category"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "in", "(", "\"Cc\"", ",", "\"Cf\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization._is_punctuation": [[386, 400], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "", ""]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.download_glue_data.download_and_extract": [[44, 52], ["print", "urllib.request.urlretrieve", "os.remove", "print", "zipfile.ZipFile", "zip_ref.extractall"], "function", ["None"], ["def", "download_and_extract", "(", "task", ",", "data_dir", ")", ":", "\n", "    ", "print", "(", "\"Downloading and extracting %s...\"", "%", "task", ")", "\n", "data_file", "=", "\"%s.zip\"", "%", "task", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "TASK2PATH", "[", "task", "]", ",", "data_file", ")", "\n", "with", "zipfile", ".", "ZipFile", "(", "data_file", ")", "as", "zip_ref", ":", "\n", "        ", "zip_ref", ".", "extractall", "(", "data_dir", ")", "\n", "", "os", ".", "remove", "(", "data_file", ")", "\n", "print", "(", "\"\\tCompleted!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.download_glue_data.format_mrpc": [[53, 97], ["print", "os.path.join", "os.path.isfile", "os.path.isfile", "urllib.request.urlretrieve", "print", "os.path.isdir", "os.mkdir", "os.path.join", "os.path.join", "print", "os.path.join", "os.path.join", "urllib.request.urlretrieve", "urllib.request.urlretrieve", "os.path.join", "open", "open", "open", "open", "data_fh.readline", "train_fh.write", "dev_fh.write", "open", "open", "data_fh.readline", "test_fh.write", "enumerate", "os.path.join", "dev_ids.append", "os.path.join", "os.path.join", "row.strip().split", "os.path.join", "row.strip().split", "test_fh.write", "row.strip().split", "dev_fh.write", "train_fh.write", "row.strip", "row.strip", "row.strip"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split"], ["", "def", "format_mrpc", "(", "data_dir", ",", "path_to_data", ")", ":", "\n", "    ", "print", "(", "\"Processing MRPC...\"", ")", "\n", "mrpc_dir", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"MRPC\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "mrpc_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "mrpc_dir", ")", "\n", "", "if", "path_to_data", ":", "\n", "        ", "mrpc_train_file", "=", "os", ".", "path", ".", "join", "(", "path_to_data", ",", "\"msr_paraphrase_train.txt\"", ")", "\n", "mrpc_test_file", "=", "os", ".", "path", ".", "join", "(", "path_to_data", ",", "\"msr_paraphrase_test.txt\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Local MRPC data not specified, downloading data from %s\"", "%", "MRPC_TRAIN", ")", "\n", "mrpc_train_file", "=", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"msr_paraphrase_train.txt\"", ")", "\n", "mrpc_test_file", "=", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"msr_paraphrase_test.txt\"", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MRPC_TRAIN", ",", "mrpc_train_file", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "MRPC_TEST", ",", "mrpc_test_file", ")", "\n", "", "assert", "os", ".", "path", ".", "isfile", "(", "mrpc_train_file", ")", ",", "\"Train data not found at %s\"", "%", "mrpc_train_file", "\n", "assert", "os", ".", "path", ".", "isfile", "(", "mrpc_test_file", ")", ",", "\"Test data not found at %s\"", "%", "mrpc_test_file", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "TASK2PATH", "[", "\"MRPC\"", "]", ",", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"dev_ids.tsv\"", ")", ")", "\n", "\n", "dev_ids", "=", "[", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"dev_ids.tsv\"", ")", ",", "encoding", "=", "\"utf8\"", ")", "as", "ids_fh", ":", "\n", "        ", "for", "row", "in", "ids_fh", ":", "\n", "            ", "dev_ids", ".", "append", "(", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", ")", "\n", "\n", "", "", "with", "open", "(", "mrpc_train_file", ",", "encoding", "=", "\"utf8\"", ")", "as", "data_fh", ",", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"train.tsv\"", ")", ",", "'w'", ",", "encoding", "=", "\"utf8\"", ")", "as", "train_fh", ",", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"dev.tsv\"", ")", ",", "'w'", ",", "encoding", "=", "\"utf8\"", ")", "as", "dev_fh", ":", "\n", "        ", "header", "=", "data_fh", ".", "readline", "(", ")", "\n", "train_fh", ".", "write", "(", "header", ")", "\n", "dev_fh", ".", "write", "(", "header", ")", "\n", "for", "row", "in", "data_fh", ":", "\n", "            ", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", "=", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "[", "id1", ",", "id2", "]", "in", "dev_ids", ":", "\n", "                ", "dev_fh", ".", "write", "(", "\"%s\\t%s\\t%s\\t%s\\t%s\\n\"", "%", "(", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "", "else", ":", "\n", "                ", "train_fh", ".", "write", "(", "\"%s\\t%s\\t%s\\t%s\\t%s\\n\"", "%", "(", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "\n", "", "", "", "with", "open", "(", "mrpc_test_file", ",", "encoding", "=", "\"utf8\"", ")", "as", "data_fh", ",", "open", "(", "os", ".", "path", ".", "join", "(", "mrpc_dir", ",", "\"test.tsv\"", ")", ",", "'w'", ",", "encoding", "=", "\"utf8\"", ")", "as", "test_fh", ":", "\n", "        ", "header", "=", "data_fh", ".", "readline", "(", ")", "\n", "test_fh", ".", "write", "(", "\"index\\t#1 ID\\t#2 ID\\t#1 String\\t#2 String\\n\"", ")", "\n", "for", "idx", ",", "row", "in", "enumerate", "(", "data_fh", ")", ":", "\n", "            ", "label", ",", "id1", ",", "id2", ",", "s1", ",", "s2", "=", "row", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "test_fh", ".", "write", "(", "\"%d\\t%s\\t%s\\t%s\\t%s\\n\"", "%", "(", "idx", ",", "id1", ",", "id2", ",", "s1", ",", "s2", ")", ")", "\n", "", "", "print", "(", "\"\\tCompleted!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.download_glue_data.download_diagnostic": [[98, 106], ["print", "os.path.join", "urllib.request.urlretrieve", "print", "os.path.isdir", "os.mkdir", "os.path.join", "os.path.join"], "function", ["None"], ["", "def", "download_diagnostic", "(", "data_dir", ")", ":", "\n", "    ", "print", "(", "\"Downloading and extracting diagnostic...\"", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"diagnostic\"", ")", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"diagnostic\"", ")", ")", "\n", "", "data_file", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"diagnostic\"", ",", "\"diagnostic.tsv\"", ")", "\n", "urllib", ".", "request", ".", "urlretrieve", "(", "TASK2PATH", "[", "\"diagnostic\"", "]", ",", "data_file", ")", "\n", "print", "(", "\"\\tCompleted!\"", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.download_glue_data.get_tasks": [[107, 117], ["task_names.split.split", "tasks.append"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split"], ["", "def", "get_tasks", "(", "task_names", ")", ":", "\n", "    ", "task_names", "=", "task_names", ".", "split", "(", "','", ")", "\n", "if", "\"all\"", "in", "task_names", ":", "\n", "        ", "tasks", "=", "TASKS", "\n", "", "else", ":", "\n", "        ", "tasks", "=", "[", "]", "\n", "for", "task_name", "in", "task_names", ":", "\n", "            ", "assert", "task_name", "in", "TASKS", ",", "\"Task %s not found!\"", "%", "task_name", "\n", "tasks", ".", "append", "(", "task_name", ")", "\n", "", "", "return", "tasks", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.download_glue_data.main": [[118, 138], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "download_glue_data.get_tasks", "os.path.isdir", "os.mkdir", "download_glue_data.format_mrpc", "download_glue_data.download_diagnostic", "download_glue_data.download_and_extract"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.download_glue_data.get_tasks", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.download_glue_data.format_mrpc", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.download_glue_data.download_diagnostic", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.download_glue_data.download_and_extract"], ["", "def", "main", "(", "arguments", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "help", "=", "'directory to save data to'", ",", "type", "=", "str", ",", "default", "=", "'glue_data'", ")", "\n", "parser", ".", "add_argument", "(", "'--tasks'", ",", "help", "=", "'tasks to download data for as a comma separated string'", ",", "\n", "type", "=", "str", ",", "default", "=", "'all'", ")", "\n", "parser", ".", "add_argument", "(", "'--path_to_mrpc'", ",", "help", "=", "'path to directory containing extracted MRPC data, msr_paraphrase_train.txt and msr_paraphrase_text.txt'", ",", "\n", "type", "=", "str", ",", "default", "=", "''", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", "arguments", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "args", ".", "data_dir", ")", ":", "\n", "        ", "os", ".", "mkdir", "(", "args", ".", "data_dir", ")", "\n", "", "tasks", "=", "get_tasks", "(", "args", ".", "tasks", ")", "\n", "\n", "for", "task", "in", "tasks", ":", "\n", "        ", "if", "task", "==", "'MRPC'", ":", "\n", "            ", "format_mrpc", "(", "args", ".", "data_dir", ",", "args", ".", "path_to_mrpc", ")", "\n", "", "elif", "task", "==", "'diagnostic'", ":", "\n", "            ", "download_diagnostic", "(", "args", ".", "data_dir", ")", "\n", "", "else", ":", "\n", "            ", "download_and_extract", "(", "task", ",", "args", ".", "data_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.run_siamese.get_embedding": [[145, 208], ["modeling.BertModel", "modeling.BertModel.get_sequence_output", "tensorflow.cast", "flow.glow_1x1.Glow", "flow.glow_1x1.Glow.body", "tensorflow.math.reduce_mean", "tensorflow.identity", "modeling.get_shape_list", "tensorflow.reduce_mean", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "modeling.BertModel.get_pooled_output", "FLAGS.sentence_embedding_type.startswith", "open", "flow.glow_1x1.AttrDict", "tensorflow.squeeze", "tensorflow.reshape", "int", "tensorflow.cast", "FLAGS.sentence_embedding_type.startswith", "os.path.join", "json.load", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "int", "tensorflow.cast", "range", "float", "FLAGS.sentence_embedding_type.startswith", "tensorflow.expand_dims", "int", "tensorflow.cast", "range", "tensorflow.concat", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.expand_dims", "tensorflow.reduce_sum", "tensorflow.reduce_sum"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertModel.get_sequence_output", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.body", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertModel.get_pooled_output", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.squeeze"], ["def", "get_embedding", "(", "bert_config", ",", "is_training", ",", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "scope", "=", "None", ")", ":", "\n", "\n", "  ", "model", "=", "modeling", ".", "BertModel", "(", "\n", "config", "=", "bert_config", ",", "\n", "is_training", "=", "is_training", ",", "\n", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "token_type_ids", "=", "segment_ids", ",", "\n", "scope", "=", "scope", ")", "\n", "\n", "if", "FLAGS", ".", "sentence_embedding_type", "==", "\"avg\"", ":", "\n", "    ", "sequence", "=", "model", ".", "get_sequence_output", "(", ")", "# [batch_size, seq_length, hidden_size]", "\n", "input_mask_", "=", "tf", ".", "cast", "(", "tf", ".", "expand_dims", "(", "input_mask", ",", "axis", "=", "-", "1", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "pooled", "=", "tf", ".", "reduce_sum", "(", "sequence", "*", "input_mask_", ",", "axis", "=", "1", ")", "/", "tf", ".", "reduce_sum", "(", "input_mask_", ",", "axis", "=", "1", ")", "\n", "", "elif", "FLAGS", ".", "sentence_embedding_type", "==", "\"cls\"", ":", "\n", "    ", "pooled", "=", "model", ".", "get_pooled_output", "(", ")", "\n", "", "elif", "FLAGS", ".", "sentence_embedding_type", ".", "startswith", "(", "\"avg-last-last-\"", ")", ":", "\n", "    ", "pooled", "=", "0", "\n", "n_last", "=", "int", "(", "FLAGS", ".", "sentence_embedding_type", "[", "-", "1", "]", ")", "\n", "input_mask_", "=", "tf", ".", "cast", "(", "tf", ".", "expand_dims", "(", "input_mask", ",", "axis", "=", "-", "1", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "sequence", "=", "model", ".", "all_encoder_layers", "[", "-", "n_last", "]", "# [batch_size, seq_length, hidden_size]", "\n", "pooled", "+=", "tf", ".", "reduce_sum", "(", "sequence", "*", "input_mask_", ",", "axis", "=", "1", ")", "/", "tf", ".", "reduce_sum", "(", "input_mask_", ",", "axis", "=", "1", ")", "\n", "", "elif", "FLAGS", ".", "sentence_embedding_type", ".", "startswith", "(", "\"avg-last-\"", ")", ":", "\n", "    ", "pooled", "=", "0", "\n", "n_last", "=", "int", "(", "FLAGS", ".", "sentence_embedding_type", "[", "-", "1", "]", ")", "\n", "input_mask_", "=", "tf", ".", "cast", "(", "tf", ".", "expand_dims", "(", "input_mask", ",", "axis", "=", "-", "1", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "for", "i", "in", "range", "(", "n_last", ")", ":", "\n", "      ", "sequence", "=", "model", ".", "all_encoder_layers", "[", "-", "i", "]", "# [batch_size, seq_length, hidden_size]", "\n", "pooled", "+=", "tf", ".", "reduce_sum", "(", "sequence", "*", "input_mask_", ",", "axis", "=", "1", ")", "/", "tf", ".", "reduce_sum", "(", "input_mask_", ",", "axis", "=", "1", ")", "\n", "", "pooled", "/=", "float", "(", "n_last", ")", "\n", "", "elif", "FLAGS", ".", "sentence_embedding_type", ".", "startswith", "(", "\"avg-last-concat-\"", ")", ":", "\n", "    ", "pooled", "=", "[", "]", "\n", "n_last", "=", "int", "(", "FLAGS", ".", "sentence_embedding_type", "[", "-", "1", "]", ")", "\n", "input_mask_", "=", "tf", ".", "cast", "(", "tf", ".", "expand_dims", "(", "input_mask", ",", "axis", "=", "-", "1", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "for", "i", "in", "range", "(", "n_last", ")", ":", "\n", "      ", "sequence", "=", "model", ".", "all_encoder_layers", "[", "-", "i", "]", "# [batch_size, seq_length, hidden_size]", "\n", "pooled", "+=", "[", "tf", ".", "reduce_sum", "(", "sequence", "*", "input_mask_", ",", "axis", "=", "1", ")", "/", "tf", ".", "reduce_sum", "(", "input_mask_", ",", "axis", "=", "1", ")", "]", "\n", "", "pooled", "=", "tf", ".", "concat", "(", "pooled", ",", "axis", "=", "-", "1", ")", "\n", "", "else", ":", "\n", "    ", "raise", "NotImplementedError", "\n", "\n", "# flow", "\n", "", "embedding", "=", "None", "\n", "flow_loss_batch", ",", "flow_loss_example", "=", "None", ",", "None", "\n", "if", "FLAGS", ".", "flow", ":", "\n", "# load model and train config", "\n", "    ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "\"./flow/config\"", ",", "FLAGS", ".", "flow_model_config", "+", "\".json\"", ")", ",", "'r'", ")", "as", "jp", ":", "\n", "        ", "flow_model_config", "=", "AttrDict", "(", "json", ".", "load", "(", "jp", ")", ")", "\n", "", "flow_model_config", ".", "is_training", "=", "is_training", "\n", "flow_model", "=", "Glow", "(", "flow_model_config", ")", "\n", "flow_loss_example", "=", "flow_model", ".", "body", "(", "pooled", ",", "is_training", ")", "# no l2 normalization here any more", "\n", "flow_loss_batch", "=", "tf", ".", "math", ".", "reduce_mean", "(", "flow_loss_example", ")", "\n", "embedding", "=", "tf", ".", "identity", "(", "tf", ".", "squeeze", "(", "flow_model", ".", "z", ",", "[", "1", ",", "2", "]", ")", ")", "# no l2 normalization here any more", "\n", "", "else", ":", "\n", "    ", "embedding", "=", "pooled", "\n", "\n", "", "if", "FLAGS", ".", "low_dim", ">", "0", ":", "\n", "    ", "bsz", ",", "org_dim", "=", "modeling", ".", "get_shape_list", "(", "embedding", ")", "\n", "embedding", "=", "tf", ".", "reduce_mean", "(", "\n", "tf", ".", "reshape", "(", "embedding", ",", "[", "bsz", ",", "FLAGS", ".", "low_dim", ",", "org_dim", "//", "FLAGS", ".", "low_dim", "]", ")", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "return", "embedding", ",", "flow_loss_batch", ",", "flow_loss_example", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.run_siamese.create_model": [[210, 285], ["tensorflow.variable_scope", "run_siamese.get_embedding", "tensorflow.variable_scope", "run_siamese.get_embedding", "tensorflow.variable_scope", "tensorflow.reduce_sum", "tensorflow.multiply", "tensorflow.square", "tensorflow.reduce_mean", "tensorflow.concat", "tensorflow.get_variable", "tensorflow.matmul", "tensorflow.nn.softmax", "tensorflow.argmax", "tensorflow.nn.log_softmax", "tensorflow.one_hot", "tensorflow.reduce_mean", "tensorflow.zeros_like", "tensorflow.zeros_like", "tensorflow.norm", "tensorflow.norm", "tensorflow.reduce_mean", "tensorflow.nn.l2_normalize", "tensorflow.nn.l2_normalize", "tensorflow.reduce_sum", "tensorflow.reduce_mean", "tensorflow.math.abs", "tensorflow.truncated_normal_initializer", "tensorflow.cast"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.run_siamese.get_embedding", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.run_siamese.get_embedding"], ["", "def", "create_model", "(", "bert_config", ",", "is_regression", ",", "\n", "is_training", ",", "\n", "input_ids_a", ",", "input_mask_a", ",", "segment_ids_a", ",", "\n", "input_ids_b", ",", "input_mask_b", ",", "segment_ids_b", ",", "\n", "labels", ",", "num_labels", ")", ":", "\n", "  ", "\"\"\"Creates a classification model.\"\"\"", "\n", "\n", "with", "tf", ".", "variable_scope", "(", "\"bert\"", ")", "as", "scope", ":", "\n", "    ", "embedding_a", ",", "flow_loss_batch_a", ",", "flow_loss_example_a", "=", "get_embedding", "(", "bert_config", ",", "is_training", ",", "\n", "input_ids_a", ",", "input_mask_a", ",", "segment_ids_a", ",", "scope", ")", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"bert\"", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", "as", "scope", ":", "\n", "    ", "embedding_b", ",", "flow_loss_batch_b", ",", "flow_loss_example_b", "=", "get_embedding", "(", "bert_config", ",", "is_training", ",", "\n", "input_ids_b", ",", "input_mask_b", ",", "segment_ids_b", ",", "scope", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"loss\"", ")", ":", "\n", "    ", "cos_similarity", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "multiply", "(", "\n", "tf", ".", "nn", ".", "l2_normalize", "(", "embedding_a", ",", "axis", "=", "-", "1", ")", ",", "\n", "tf", ".", "nn", ".", "l2_normalize", "(", "embedding_b", ",", "axis", "=", "-", "1", ")", ")", ",", "axis", "=", "-", "1", ")", "\n", "if", "is_regression", ":", "\n", "# changing cos_similarity into (cos_similarity + 1)/2.0 ", "\n", "#     leads to large performance decrease in practice", "\n", "      ", "per_example_loss", "=", "tf", ".", "square", "(", "cos_similarity", "-", "labels", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "per_example_loss", ")", "\n", "logits", ",", "predictions", "=", "None", ",", "None", "\n", "", "else", ":", "\n", "      ", "output_layer", "=", "tf", ".", "concat", "(", "[", "\n", "embedding_a", ",", "embedding_b", ",", "tf", ".", "math", ".", "abs", "(", "embedding_a", "-", "embedding_b", ")", "\n", "]", ",", "axis", "=", "-", "1", ")", "\n", "output_size", "=", "output_layer", ".", "shape", "[", "-", "1", "]", ".", "value", "\n", "output_weights", "=", "tf", ".", "get_variable", "(", "\n", "\"output_weights\"", ",", "[", "num_labels", ",", "output_size", "]", ",", "\n", "initializer", "=", "tf", ".", "truncated_normal_initializer", "(", "stddev", "=", "0.02", ")", ")", "\n", "\n", "logits", "=", "tf", ".", "matmul", "(", "output_layer", ",", "output_weights", ",", "transpose_b", "=", "True", ")", "\n", "\n", "probabilities", "=", "tf", ".", "nn", ".", "softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "predictions", "=", "tf", ".", "argmax", "(", "probabilities", ",", "axis", "=", "-", "1", ",", "output_type", "=", "tf", ".", "int32", ")", "\n", "log_probs", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ",", "axis", "=", "-", "1", ")", "\n", "one_hot_labels", "=", "tf", ".", "one_hot", "(", "labels", ",", "depth", "=", "num_labels", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "per_example_loss", "=", "-", "tf", ".", "reduce_sum", "(", "one_hot_labels", "*", "log_probs", ",", "axis", "=", "-", "1", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "per_example_loss", ")", "\n", "\n", "", "if", "FLAGS", ".", "num_examples", "==", "0", ":", "\n", "      ", "per_example_loss", "=", "tf", ".", "zeros_like", "(", "per_example_loss", ")", "\n", "loss", "=", "tf", ".", "zeros_like", "(", "loss", ")", "\n", "", "elif", "FLAGS", ".", "num_examples", ">", "0", ":", "\n", "      ", "per_example_loss", "=", "per_example_loss", "*", "tf", ".", "cast", "(", "labels", ">", "-", "1", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "per_example_loss", ")", "\n", "\n", "", "if", "FLAGS", ".", "l2_penalty", ">", "0", ":", "\n", "      ", "l2_penalty_loss", "=", "tf", ".", "norm", "(", "embedding_a", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "False", ")", "\n", "l2_penalty_loss", "+=", "tf", ".", "norm", "(", "embedding_b", ",", "axis", "=", "-", "1", ",", "keepdims", "=", "False", ")", "\n", "l2_penalty_loss", "*=", "FLAGS", ".", "l2_penalty", "\n", "\n", "per_example_loss", "+=", "l2_penalty_loss", "\n", "loss", "+=", "tf", ".", "reduce_mean", "(", "l2_penalty_loss", ")", "\n", "\n", "", "", "model_output", "=", "{", "\n", "\"loss\"", ":", "loss", ",", "\n", "\"per_example_loss\"", ":", "per_example_loss", ",", "\n", "\"cos_similarity\"", ":", "cos_similarity", ",", "\n", "\"embedding_a\"", ":", "embedding_a", ",", "\n", "\"embedding_b\"", ":", "embedding_b", ",", "\n", "\"logits\"", ":", "logits", ",", "\n", "\"predictions\"", ":", "predictions", ",", "\n", "}", "\n", "\n", "if", "FLAGS", ".", "flow", ":", "\n", "    ", "model_output", "[", "\"flow_example_loss\"", "]", "=", "flow_loss_example_a", "+", "flow_loss_example_b", "\n", "model_output", "[", "\"flow_loss\"", "]", "=", "flow_loss_batch_a", "+", "flow_loss_batch_b", "\n", "\n", "", "return", "model_output", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.run_siamese.model_fn_builder": [[287, 427], ["tensorflow.logging.info", "sorted", "run_siamese.create_model", "tensorflow.trainable_variables", "tensorflow.logging.info", "features.keys", "tensorflow.logging.info", "tensorflow.cast", "tensorflow.ones", "modeling.get_assignment_map_from_checkpoint", "tensorflow.train.init_from_checkpoint", "tensorflow.logging.info", "tensorflow.shape", "optimization_bert_flow.create_optimizer", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.estimator.EstimatorSpec", "optimization.create_optimizer", "tensorflow.estimator.EstimatorSpec", "metric_fn"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.run_siamese.create_model", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.get_assignment_map_from_checkpoint", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.create_optimizer", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.optimization_bert_flow.create_optimizer"], ["", "def", "model_fn_builder", "(", "bert_config", ",", "num_labels", ",", "init_checkpoint", ",", "learning_rate", ",", "\n", "num_train_steps", ",", "num_warmup_steps", ",", "is_regression", ")", ":", "\n", "  ", "\"\"\"Returns `model_fn` closure for Estimator.\"\"\"", "\n", "\n", "def", "model_fn", "(", "features", ",", "labels", ",", "mode", ",", "params", ")", ":", "# pylint: disable=unused-argument", "\n", "    ", "\"\"\"The `model_fn` for Estimator.\"\"\"", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Features ***\"", ")", "\n", "for", "name", "in", "sorted", "(", "features", ".", "keys", "(", ")", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s\"", "%", "(", "name", ",", "features", "[", "name", "]", ".", "shape", ")", ")", "\n", "\n", "", "input_ids_a", "=", "features", "[", "\"input_ids_a\"", "]", "\n", "input_mask_a", "=", "features", "[", "\"input_mask_a\"", "]", "\n", "segment_ids_a", "=", "features", "[", "\"segment_ids_a\"", "]", "\n", "\n", "input_ids_b", "=", "features", "[", "\"input_ids_b\"", "]", "\n", "input_mask_b", "=", "features", "[", "\"input_mask_b\"", "]", "\n", "segment_ids_b", "=", "features", "[", "\"segment_ids_b\"", "]", "\n", "\n", "label_ids", "=", "features", "[", "\"label_ids\"", "]", "\n", "is_real_example", "=", "None", "\n", "if", "\"is_real_example\"", "in", "features", ":", "\n", "      ", "is_real_example", "=", "tf", ".", "cast", "(", "features", "[", "\"is_real_example\"", "]", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "", "else", ":", "\n", "      ", "is_real_example", "=", "tf", ".", "ones", "(", "tf", ".", "shape", "(", "label_ids", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "#### Training or Evaluation", "\n", "", "is_training", "=", "(", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ")", "\n", "\n", "#### Get loss from inputs", "\n", "model_output", "=", "create_model", "(", "\n", "bert_config", ",", "is_regression", ",", "\n", "is_training", ",", "\n", "input_ids_a", ",", "input_mask_a", ",", "segment_ids_a", ",", "\n", "input_ids_b", ",", "input_mask_b", ",", "segment_ids_b", ",", "\n", "label_ids", ",", "\n", "num_labels", ")", "\n", "\n", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "initialized_variable_names", "=", "{", "}", "\n", "if", "init_checkpoint", ":", "\n", "      ", "(", "assignment_map", ",", "initialized_variable_names", "\n", ")", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "init_checkpoint", ")", "\n", "tf", ".", "train", ".", "init_from_checkpoint", "(", "init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "      ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "        ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "\n", "init_string", ")", "\n", "# if \"flow\" in var.name:", "\n", "#   input()", "\n", "\n", "", "output_spec", "=", "None", "\n", "if", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "TRAIN", ":", "\n", "      ", "if", "FLAGS", ".", "flow_loss", ":", "\n", "        ", "train_op", "=", "optimization_bert_flow", ".", "create_optimizer", "(", "\n", "model_output", "[", "\"loss\"", "]", ",", "model_output", "[", "\"flow_loss\"", "]", ",", "\n", "learning_rate", ",", "FLAGS", ".", "flow_learning_rate", ",", "\n", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", "=", "False", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"loss\"", ",", "model_output", "[", "\"loss\"", "]", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"flow_loss\"", ",", "model_output", "[", "\"flow_loss\"", "]", ")", "\n", "\n", "output_spec", "=", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "model_output", "[", "\"loss\"", "]", "+", "model_output", "[", "\"flow_loss\"", "]", ",", "\n", "train_op", "=", "train_op", ")", "\n", "", "else", ":", "\n", "        ", "train_op", "=", "optimization", ".", "create_optimizer", "(", "\n", "model_output", "[", "\"loss\"", "]", ",", "learning_rate", ",", "\n", "num_train_steps", ",", "num_warmup_steps", ",", "use_tpu", "=", "False", ")", "\n", "output_spec", "=", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "model_output", "[", "\"loss\"", "]", ",", "\n", "train_op", "=", "train_op", ")", "\n", "\n", "", "", "elif", "mode", "==", "tf", ".", "estimator", ".", "ModeKeys", ".", "EVAL", ":", "\n", "      ", "def", "metric_fn", "(", "model_output", ",", "label_ids", ",", "is_real_example", ")", ":", "\n", "        ", "predictions", "=", "tf", ".", "argmax", "(", "model_output", "[", "\"logits\"", "]", ",", "axis", "=", "-", "1", ",", "output_type", "=", "tf", ".", "int32", ")", "\n", "accuracy", "=", "tf", ".", "metrics", ".", "accuracy", "(", "\n", "labels", "=", "label_ids", ",", "predictions", "=", "model_output", "[", "\"predictions\"", "]", ",", "\n", "weights", "=", "is_real_example", ")", "\n", "loss", "=", "tf", ".", "metrics", ".", "mean", "(", "\n", "values", "=", "model_output", "[", "\"per_example_loss\"", "]", ",", "weights", "=", "is_real_example", ")", "\n", "metric_output", "=", "{", "\n", "\"eval_accuracy\"", ":", "accuracy", ",", "\n", "\"eval_loss\"", ":", "loss", ",", "\n", "}", "\n", "\n", "if", "\"flow_loss\"", "in", "model_output", ":", "\n", "          ", "metric_output", "[", "\"eval_loss_flow\"", "]", "=", "tf", ".", "metrics", ".", "mean", "(", "values", "=", "model_output", "[", "\"flow_example_loss\"", "]", ",", "weights", "=", "is_real_example", ")", "\n", "metric_output", "[", "\"eval_loss_total\"", "]", "=", "tf", ".", "metrics", ".", "mean", "(", "\n", "values", "=", "model_output", "[", "\"per_example_loss\"", "]", "+", "model_output", "[", "\"flow_example_loss\"", "]", ",", "\n", "weights", "=", "is_real_example", ")", "\n", "\n", "", "return", "metric_output", "\n", "\n", "", "def", "regression_metric_fn", "(", "model_output", ",", "label_ids", ",", "is_real_example", ")", ":", "\n", "        ", "metric_output", "=", "{", "\n", "\"eval_loss\"", ":", "tf", ".", "metrics", ".", "mean", "(", "\n", "values", "=", "model_output", "[", "\"per_example_loss\"", "]", ",", "weights", "=", "is_real_example", ")", ",", "\n", "\"eval_pearsonr\"", ":", "tf", ".", "contrib", ".", "metrics", ".", "streaming_pearson_correlation", "(", "\n", "model_output", "[", "\"cos_similarity\"", "]", ",", "label_ids", ",", "weights", "=", "is_real_example", ")", "\n", "}", "\n", "\n", "# metric_output[\"auc\"] = tf.compat.v1.metrics.auc(", "\n", "#   label_ids, tf.math.maximum(model_output[\"cos_similarity\"], 0), weights=is_real_example, curve='ROC')", "\n", "\n", "if", "\"flow_loss\"", "in", "model_output", ":", "\n", "          ", "metric_output", "[", "\"eval_loss_flow\"", "]", "=", "tf", ".", "metrics", ".", "mean", "(", "values", "=", "model_output", "[", "\"flow_example_loss\"", "]", ",", "weights", "=", "is_real_example", ")", "\n", "metric_output", "[", "\"eval_loss_total\"", "]", "=", "tf", ".", "metrics", ".", "mean", "(", "\n", "values", "=", "model_output", "[", "\"per_example_loss\"", "]", "+", "model_output", "[", "\"flow_example_loss\"", "]", ",", "\n", "weights", "=", "is_real_example", ")", "\n", "\n", "", "return", "metric_output", "\n", "\n", "", "if", "is_regression", ":", "\n", "        ", "metric_fn", "=", "regression_metric_fn", "\n", "\n", "", "eval_metrics", "=", "metric_fn", "(", "model_output", ",", "label_ids", ",", "is_real_example", ")", "\n", "\n", "output_spec", "=", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "loss", "=", "model_output", "[", "\"loss\"", "]", ",", "\n", "eval_metric_ops", "=", "eval_metrics", ")", "\n", "", "else", ":", "\n", "      ", "output_spec", "=", "tf", ".", "estimator", ".", "EstimatorSpec", "(", "\n", "mode", "=", "mode", ",", "\n", "predictions", "=", "{", "\"embedding_a\"", ":", "model_output", "[", "\"embedding_a\"", "]", ",", "\n", "\"embedding_b\"", ":", "model_output", "[", "\"embedding_b\"", "]", "}", "if", "FLAGS", ".", "predict_pool", "else", "{", "\"cos_similarity\"", ":", "model_output", "[", "\"cos_similarity\"", "]", "}", ")", "\n", "", "return", "output_spec", "\n", "\n", "", "return", "model_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.run_siamese.main": [[429, 913], ["tensorflow.logging.set_verbosity", "random.seed", "numpy.random.seed", "tensorflow.compat.v1.set_random_seed", "print", "tensorflow.get_logger", "tokenization.validate_case_matches_checkpoint", "tokenization.FullTokenizer", "modeling.BertConfig.from_json_file", "tensorflow.compat.v1.ConfigProto", "FLAGS.task_name.lower", "processor.get_labels", "tensorflow.logging.info", "tensorflow.gfile.MakeDirs", "tensorflow.estimator.RunConfig", "run_siamese.model_fn_builder", "tensorflow.estimator.Estimator", "tensorflow.logging.info", "tensorflow.logging.info", "ValueError", "tensorflow.logging.set_verbosity", "logging.basicConfig", "sys.path.insert", "tensorflow.logging.info", "ValueError", "FLAGS.task_name.lower.startswith", "os.path.join", "processor.get_train_examples", "int", "int", "int", "random.shuffle", "os.path.join", "tensorflow.gfile.Exists", "tensorflow.io.gfile.rmtree", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "siamese_utils.file_based_input_fn_builder", "processor.get_dev_examples", "len", "os.path.join", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "siamese_utils.file_based_input_fn_builder", "len", "os.path.join", "siamese_utils.file_based_convert_examples_to_features", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "siamese_utils.file_based_input_fn_builder", "run_siamese.main.get_train_input_fn"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.tokenization.validate_case_matches_checkpoint", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.SnliTrainProcessor.get_labels", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.run_siamese.model_fn_builder", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.GLUEProcessor.get_train_examples", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.file_based_input_fn_builder", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.GLUEProcessor.get_dev_examples", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.file_based_input_fn_builder", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.file_based_convert_examples_to_features", "home.repos.pwc.inspect_result.bohanli_BERT-flow.None.siamese_utils.file_based_input_fn_builder"], ["", "def", "main", "(", "_", ")", ":", "\n", "  ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "\n", "# random seed", "\n", "random", ".", "seed", "(", "FLAGS", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "FLAGS", ".", "seed", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "set_random_seed", "(", "FLAGS", ".", "seed", ")", "\n", "print", "(", "\"FLAGS.seed\"", ",", "FLAGS", ".", "seed", ")", "\n", "# input()", "\n", "\n", "# prevent double printing of the tf logs", "\n", "logger", "=", "tf", ".", "get_logger", "(", ")", "\n", "logger", ".", "propagate", "=", "False", "\n", "\n", "# get tokenizer", "\n", "tokenization", ".", "validate_case_matches_checkpoint", "(", "FLAGS", ".", "do_lower_case", ",", "\n", "FLAGS", ".", "init_checkpoint", ")", "\n", "tokenizer", "=", "tokenization", ".", "FullTokenizer", "(", "\n", "vocab_file", "=", "FLAGS", ".", "vocab_file", ",", "do_lower_case", "=", "FLAGS", ".", "do_lower_case", ")", "\n", "\n", "# get bert config", "\n", "bert_config", "=", "modeling", ".", "BertConfig", ".", "from_json_file", "(", "FLAGS", ".", "bert_config_file", ")", "\n", "if", "FLAGS", ".", "max_seq_length", ">", "bert_config", ".", "max_position_embeddings", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"Cannot use sequence length %d because the BERT model \"", "\n", "\"was only trained up to sequence length %d\"", "%", "\n", "(", "FLAGS", ".", "max_seq_length", ",", "bert_config", ".", "max_position_embeddings", ")", ")", "\n", "\n", "# GPU config", "\n", "", "run_config", "=", "tf", ".", "compat", ".", "v1", ".", "ConfigProto", "(", ")", "\n", "if", "FLAGS", ".", "use_xla", ":", "\n", "    ", "run_config", ".", "graph_options", ".", "optimizer_options", ".", "global_jit_level", "=", "tf", ".", "OptimizerOptions", ".", "ON_1", "\n", "\n", "", "run_config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "\n", "if", "FLAGS", ".", "do_senteval", ":", "\n", "# Set up logger", "\n", "    ", "import", "logging", "\n", "tf", ".", "logging", ".", "set_verbosity", "(", "0", ")", "\n", "logging", ".", "basicConfig", "(", "format", "=", "'%(asctime)s : %(message)s'", ",", "level", "=", "logging", ".", "DEBUG", ")", "\n", "\n", "# load senteval", "\n", "import", "sys", "\n", "PATH_TO_SENTEVAL", ",", "PATH_TO_DATA", "=", "'../SentEval'", ",", "'../SentEval/data'", "\n", "sys", ".", "path", ".", "insert", "(", "0", ",", "PATH_TO_SENTEVAL", ")", "\n", "import", "senteval", "\n", "\n", "# model", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running SentEval *****\"", ")", "\n", "with", "tf", ".", "Graph", "(", ")", ".", "as_default", "(", ")", ":", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "\"bert\"", ")", "as", "scope", ":", "\n", "        ", "input_ids", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", ",", "None", "]", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"input_ids\"", ")", "\n", "input_mask", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", ",", "None", "]", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"input_mask\"", ")", "\n", "segment_ids", "=", "tf", ".", "placeholder", "(", "shape", "=", "[", "None", ",", "None", "]", ",", "dtype", "=", "tf", ".", "int32", ",", "name", "=", "\"segment_ids\"", ")", "\n", "\n", "embedding", ",", "flow_loss_batch", ",", "flow_loss_example", "=", "get_embedding", "(", "bert_config", ",", "False", ",", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "scope", "=", "scope", ")", "\n", "embedding", "=", "tf", ".", "nn", ".", "l2_normalize", "(", "embedding", ",", "axis", "=", "-", "1", ")", "\n", "\n", "", "tvars", "=", "tf", ".", "trainable_variables", "(", ")", "\n", "initialized_variable_names", "=", "{", "}", "\n", "if", "FLAGS", ".", "init_checkpoint", ":", "\n", "        ", "(", "assignment_map", ",", "initialized_variable_names", "\n", ")", "=", "modeling", ".", "get_assignment_map_from_checkpoint", "(", "tvars", ",", "FLAGS", ".", "init_checkpoint", ")", "\n", "tf", ".", "train", ".", "init_from_checkpoint", "(", "FLAGS", ".", "init_checkpoint", ",", "assignment_map", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"**** Trainable Variables ****\"", ")", "\n", "for", "var", "in", "tvars", ":", "\n", "        ", "init_string", "=", "\"\"", "\n", "if", "var", ".", "name", "in", "initialized_variable_names", ":", "\n", "          ", "init_string", "=", "\", *INIT_FROM_CKPT*\"", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"  name = %s, shape = %s%s\"", ",", "var", ".", "name", ",", "var", ".", "shape", ",", "\n", "init_string", ")", "\n", "\n", "", "with", "tf", ".", "train", ".", "MonitoredSession", "(", "\n", "session_creator", "=", "tf", ".", "compat", ".", "v1", ".", "train", ".", "ChiefSessionCreator", "(", "config", "=", "run_config", ")", ")", "as", "session", ":", "\n", "\n", "# SentEval prepare and batcher", "\n", "        ", "def", "prepare", "(", "params", ",", "samples", ")", ":", "\n", "          ", "return", "\n", "\n", "", "def", "batcher", "(", "params", ",", "batch", ")", ":", "\n", "          ", "batch_input_ids", ",", "batch_input_mask", ",", "batch_segment_ids", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "sent", "in", "batch", ":", "\n", "            ", "if", "type", "(", "sent", "[", "0", "]", ")", "==", "bytes", ":", "\n", "              ", "sent", "=", "[", "_", ".", "decode", "(", ")", "for", "_", "in", "sent", "]", "\n", "", "text", "=", "' '", ".", "join", "(", "sent", ")", "if", "sent", "!=", "[", "]", "else", "'.'", "\n", "# print(text)", "\n", "\n", "_input_ids", ",", "_input_mask", ",", "_segment_ids", ",", "_tokens", "=", "get_input_mask_segment", "(", "text", ",", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ")", "\n", "batch_input_ids", ".", "append", "(", "_input_ids", ")", "\n", "batch_input_mask", ".", "append", "(", "_input_mask", ")", "\n", "batch_segment_ids", ".", "append", "(", "_segment_ids", ")", "\n", "\n", "", "batch_input_ids", "=", "np", ".", "asarray", "(", "batch_input_ids", ")", "\n", "batch_input_mask", "=", "np", ".", "asarray", "(", "batch_input_mask", ")", "\n", "batch_segment_ids", "=", "np", ".", "asarray", "(", "batch_segment_ids", ")", "\n", "\n", "print", "(", "\".\"", ",", "end", "=", "\"\"", ")", "\n", "\n", "return", "session", ".", "run", "(", "embedding", ",", "\n", "{", "input_ids", ":", "batch_input_ids", ",", "\n", "input_mask", ":", "batch_input_mask", ",", "\n", "segment_ids", ":", "batch_segment_ids", "}", ")", "\n", "\n", "# Set params for SentEval", "\n", "", "params_senteval", "=", "{", "'task_path'", ":", "PATH_TO_DATA", ",", "'usepytorch'", ":", "True", ",", "'kfold'", ":", "5", "}", "\n", "params_senteval", "[", "'classifier'", "]", "=", "{", "'nhid'", ":", "0", ",", "'optim'", ":", "'rmsprop'", ",", "'batch_size'", ":", "128", ",", "\n", "'tenacity'", ":", "3", ",", "'epoch_size'", ":", "2", "}", "\n", "\n", "# main", "\n", "se", "=", "senteval", ".", "engine", ".", "SE", "(", "params_senteval", ",", "batcher", ",", "prepare", ")", "\n", "\n", "# transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16',", "\n", "#                   'MR', 'CR', 'MPQA', 'SUBJ', 'SST2', 'SST5', 'TREC', 'MRPC',", "\n", "#                   'SICKEntailment', 'SICKRelatedness', 'STSBenchmark',", "\n", "#                   'Length', 'WordContent', 'Depth', 'TopConstituents',", "\n", "#                   'BigramShift', 'Tense', 'SubjNumber', 'ObjNumber',", "\n", "#                   'OddManOut', 'CoordinationInversion']", "\n", "#transfer_tasks = ['STS12', 'STS13', 'STS14', 'STS15', 'STS16', 'STSBenchmark', 'SICKRelatedness']", "\n", "#transfer_tasks = ['MR', 'CR', 'SUBJ', 'MPQA', 'SST2', 'TREC', 'MRPC']", "\n", "transfer_tasks", "=", "FLAGS", ".", "senteval_tasks", ".", "split", "(", "\",\"", ")", "\n", "results", "=", "se", ".", "eval", "(", "transfer_tasks", ")", "\n", "from", "collections", "import", "OrderedDict", "\n", "results", "=", "OrderedDict", "(", "results", ")", "\n", "for", "key", "in", "sorted", "(", "results", ")", ":", "\n", "          ", "value", "=", "results", "[", "key", "]", "\n", "if", "key", ".", "startswith", "(", "\"STS\"", ")", ":", "\n", "            ", "print", "(", "\"'\"", "+", "key", "+", "\"':\"", ",", "value", "[", "\"all\"", "]", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "key", ",", "value", ")", "\n", "\n", "", "", "", "", "return", "\n", "\n", "", "processors", "=", "{", "\n", "'sts-b'", ":", "StsbProcessor", ",", "\n", "'sick-r'", ":", "SickRProcessor", ",", "\n", "'mnli'", ":", "MnliProcessor", ",", "\n", "'allnli'", ":", "MnliProcessor", ",", "\n", "'qqp'", ":", "QqpProcessor", ",", "\n", "'sts-12-16'", ":", "Sts_12_16_Processor", ",", "\n", "'sts-12'", ":", "Sts_12_16_Processor", ",", "\n", "'sts-13'", ":", "Sts_12_16_Processor", ",", "\n", "'sts-14'", ":", "Sts_12_16_Processor", ",", "\n", "'sts-15'", ":", "Sts_12_16_Processor", ",", "\n", "'sts-16'", ":", "Sts_12_16_Processor", ",", "\n", "'mrpc-regression'", ":", "MrpcRegressionProcessor", ",", "\n", "'qnli-regression'", ":", "QnliRegressionProcessor", ",", "\n", "}", "\n", "\n", "task_name", "=", "FLAGS", ".", "task_name", ".", "lower", "(", ")", "\n", "if", "task_name", "not", "in", "processors", ":", "\n", "    ", "raise", "ValueError", "(", "\"Task not found: %s\"", "%", "(", "task_name", ")", ")", "\n", "\n", "", "if", "task_name", "==", "'sick-r'", "or", "task_name", ".", "startswith", "(", "\"sts\"", ")", ":", "\n", "    ", "is_regression", "=", "True", "\n", "label_min", ",", "label_max", "=", "0.", ",", "5.", "\n", "", "elif", "task_name", "in", "[", "'qqp'", ",", "'mrpc-regression'", ",", "'qnli-regression'", "]", ":", "\n", "    ", "is_regression", "=", "True", "\n", "label_min", ",", "label_max", "=", "0.", ",", "1.", "\n", "", "else", ":", "\n", "    ", "is_regression", "=", "False", "\n", "label_min", ",", "label_max", "=", "0.", ",", "1.", "\n", "\n", "", "dupe_factor", "=", "FLAGS", ".", "dupe_factor", "\n", "\n", "processor", "=", "processors", "[", "task_name", "]", "(", ")", "\n", "\n", "label_list", "=", "processor", ".", "get_labels", "(", ")", "\n", "\n", "# this block is moved here for calculating the epoch_step for save_checkpoints_steps", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "num_warmup_steps", "=", "None", "\n", "\n", "if", "task_name", "==", "\"allnli\"", ":", "\n", "      ", "FLAGS", ".", "data_dir", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "FLAGS", ".", "data_dir", ")", ",", "\"MNLI\"", ")", "\n", "\n", "", "if", "FLAGS", ".", "do_train", "and", "FLAGS", ".", "num_train_epochs", ">", "1e-6", ":", "\n", "    ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "\n", "if", "task_name", "==", "\"allnli\"", ":", "\n", "      ", "snli_data_dir", "=", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "FLAGS", ".", "data_dir", ")", ",", "\"SNLI\"", ")", "\n", "train_examples", ".", "extend", "(", "SnliTrainProcessor", "(", ")", ".", "get_train_examples", "(", "snli_data_dir", ")", ")", "\n", "train_examples", ".", "extend", "(", "SnliDevTestProcessor", "(", ")", ".", "get_dev_examples", "(", "snli_data_dir", ")", ")", "\n", "train_examples", ".", "extend", "(", "SnliDevTestProcessor", "(", ")", ".", "get_test_examples", "(", "snli_data_dir", ")", ")", "\n", "\n", "", "if", "FLAGS", ".", "use_full_for_training", ":", "\n", "      ", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "predict_examples", "=", "processor", ".", "get_test_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "train_examples", ".", "extend", "(", "eval_examples", "+", "predict_examples", ")", "\n", "\n", "", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "FLAGS", ".", "train_batch_size", "*", "FLAGS", ".", "num_train_epochs", ")", "\n", "num_warmup_steps", "=", "int", "(", "num_train_steps", "*", "FLAGS", ".", "warmup_proportion", ")", "\n", "epoch_step", "=", "int", "(", "len", "(", "train_examples", ")", "/", "FLAGS", ".", "train_batch_size", ")", "\n", "\n", "if", "FLAGS", ".", "num_examples", ">", "0", ":", "\n", "      ", "random", ".", "shuffle", "(", "train_examples", ")", "\n", "for", "i", "in", "range", "(", "FLAGS", ".", "num_examples", ",", "len", "(", "train_examples", ")", ")", ":", "\n", "        ", "train_examples", "[", "i", "]", ".", "label", "=", "-", "10", "\n", "\n", "", "", "random", ".", "shuffle", "(", "train_examples", ")", "\n", "\n", "\n", "# ==== #", "\n", "\n", "", "if", "FLAGS", ".", "early_stopping", ":", "\n", "    ", "save_checkpoints_steps", "=", "epoch_step", "\n", "", "else", ":", "\n", "    ", "save_checkpoints_steps", "=", "FLAGS", ".", "save_checkpoints_steps", "\n", "\n", "", "keep_checkpoint_max", "=", "3", "\n", "save_summary_steps", "=", "log_every_step", "=", "FLAGS", ".", "log_every_step", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"save_checkpoints_steps: %d\"", "%", "save_checkpoints_steps", ")", "\n", "\n", "# make exp dir", "\n", "if", "FLAGS", ".", "exp_name", ":", "\n", "    ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_parent_dir", ",", "FLAGS", ".", "exp_name", ")", "\n", "", "elif", "FLAGS", ".", "exp_name_prefix", ":", "\n", "    ", "output_dir", "=", "os", ".", "path", ".", "join", "(", "FLAGS", ".", "output_parent_dir", ",", "FLAGS", ".", "exp_name_prefix", ")", "\n", "\n", "output_dir", "+=", "\"_t_%s\"", "%", "(", "FLAGS", ".", "task_name", ")", "\n", "output_dir", "+=", "\"_ep_%.2f\"", "%", "(", "FLAGS", ".", "num_train_epochs", ")", "\n", "output_dir", "+=", "\"_lr_%.2e\"", "%", "(", "FLAGS", ".", "learning_rate", ")", "\n", "\n", "if", "FLAGS", ".", "train_batch_size", "!=", "32", ":", "\n", "      ", "output_dir", "+=", "\"_bsz_%d\"", "%", "(", "FLAGS", ".", "train_batch_size", ")", "\n", "\n", "", "if", "FLAGS", ".", "sentence_embedding_type", "!=", "\"avg\"", ":", "\n", "      ", "output_dir", "+=", "\"_e_%s\"", "%", "(", "FLAGS", ".", "sentence_embedding_type", ")", "\n", "\n", "", "if", "FLAGS", ".", "flow", ">", "0", ":", "\n", "      ", "output_dir", "+=", "\"_f_%d%d\"", "%", "(", "FLAGS", ".", "flow", ",", "FLAGS", ".", "flow_loss", ")", "\n", "\n", "if", "FLAGS", ".", "flow_loss", ">", "0", ":", "\n", "        ", "output_dir", "+=", "\"_%.2e\"", "%", "(", "FLAGS", ".", "flow_learning_rate", ")", "\n", "\n", "", "if", "FLAGS", ".", "use_full_for_training", ">", "0", ":", "\n", "        ", "output_dir", "+=", "\"_allsplits\"", "\n", "\n", "", "if", "FLAGS", ".", "flow_model_config", "!=", "\"config_l3_d3_w32\"", ":", "\n", "        ", "output_dir", "+=", "\"_%s\"", "%", "(", "FLAGS", ".", "flow_model_config", ")", "\n", "\n", "", "", "if", "FLAGS", ".", "num_examples", ">", "0", ":", "\n", "      ", "output_dir", "+=", "\"_n_%d\"", "%", "(", "FLAGS", ".", "num_examples", ")", "\n", "\n", "", "if", "FLAGS", ".", "low_dim", ">", "-", "1", ":", "\n", "      ", "output_dir", "+=", "\"_ld_%d\"", "%", "(", "FLAGS", ".", "low_dim", ")", "\n", "\n", "", "if", "FLAGS", ".", "l2_penalty", ">", "0", ":", "\n", "      ", "output_dir", "+=", "\"_l2_%.2e\"", "%", "(", "FLAGS", ".", "l2_penalty", ")", "\n", "\n", "", "", "else", ":", "\n", "    ", "raise", "NotImplementedError", "\n", "\n", "", "if", "tf", ".", "gfile", ".", "Exists", "(", "output_dir", ")", "and", "FLAGS", ".", "do_train", ":", "\n", "    ", "tf", ".", "io", ".", "gfile", ".", "rmtree", "(", "output_dir", ")", "\n", "", "tf", ".", "gfile", ".", "MakeDirs", "(", "output_dir", ")", "\n", "\n", "# set up estimator", "\n", "run_config", "=", "tf", ".", "estimator", ".", "RunConfig", "(", "\n", "model_dir", "=", "output_dir", ",", "\n", "save_summary_steps", "=", "save_summary_steps", ",", "\n", "save_checkpoints_steps", "=", "save_checkpoints_steps", ",", "\n", "keep_checkpoint_max", "=", "keep_checkpoint_max", ",", "\n", "log_step_count_steps", "=", "log_every_step", ",", "\n", "session_config", "=", "run_config", ")", "\n", "\n", "model_fn", "=", "model_fn_builder", "(", "\n", "bert_config", "=", "bert_config", ",", "\n", "num_labels", "=", "len", "(", "label_list", ")", ",", "\n", "init_checkpoint", "=", "FLAGS", ".", "init_checkpoint", ",", "\n", "learning_rate", "=", "FLAGS", ".", "learning_rate", ",", "\n", "num_train_steps", "=", "num_train_steps", ",", "\n", "num_warmup_steps", "=", "num_warmup_steps", ",", "\n", "is_regression", "=", "is_regression", ")", "\n", "\n", "estimator", "=", "tf", ".", "estimator", ".", "Estimator", "(", "\n", "model_fn", "=", "model_fn", ",", "\n", "config", "=", "run_config", ",", "\n", "params", "=", "{", "\n", "'train_batch_size'", ":", "FLAGS", ".", "train_batch_size", ",", "\n", "'eval_batch_size'", ":", "FLAGS", ".", "eval_batch_size", ",", "\n", "'predict_batch_size'", ":", "FLAGS", ".", "predict_batch_size", "}", ")", "\n", "\n", "def", "get_train_input_fn", "(", ")", ":", "\n", "    ", "cached_dir", "=", "FLAGS", ".", "cached_dir", "\n", "if", "not", "cached_dir", ":", "\n", "      ", "cached_dir", "=", "output_dir", "\n", "\n", "", "data_name", "=", "task_name", "\n", "\n", "if", "FLAGS", ".", "num_examples", ">", "0", ":", "\n", "      ", "train_file", "=", "os", ".", "path", ".", "join", "(", "cached_dir", ",", "\n", "data_name", "+", "\"_n_%d\"", "%", "(", "FLAGS", ".", "num_examples", ")", "+", "\"_seed_%d\"", "%", "(", "FLAGS", ".", "seed", ")", "+", "\"_train.tf_record\"", ")", "\n", "", "elif", "FLAGS", ".", "use_full_for_training", ">", "0", ":", "\n", "      ", "train_file", "=", "os", ".", "path", ".", "join", "(", "cached_dir", ",", "data_name", "+", "\"_allsplits.tf_record\"", ")", "\n", "", "else", ":", "\n", "      ", "train_file", "=", "os", ".", "path", ".", "join", "(", "cached_dir", ",", "data_name", "+", "\"_train.tf_record\"", ")", "\n", "\n", "", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "train_file", ")", ":", "\n", "      ", "file_based_convert_examples_to_features", "(", "\n", "train_examples", ",", "label_list", ",", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ",", "train_file", ",", "\n", "dupe_factor", ",", "label_min", ",", "label_max", ",", "\n", "is_training", "=", "True", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "train_batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "\n", "train_input_fn", "=", "file_based_input_fn_builder", "(", "\n", "input_file", "=", "train_file", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "True", ",", "\n", "drop_remainder", "=", "True", ",", "\n", "is_regression", "=", "is_regression", ")", "\n", "\n", "return", "train_input_fn", "\n", "\n", "", "def", "get_eval_input_fn", "(", ")", ":", "\n", "    ", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "num_actual_eval_examples", "=", "len", "(", "eval_examples", ")", "\n", "\n", "cached_dir", "=", "FLAGS", ".", "cached_dir", "\n", "if", "not", "cached_dir", ":", "\n", "      ", "cached_dir", "=", "output_dir", "\n", "", "eval_file", "=", "os", ".", "path", ".", "join", "(", "cached_dir", ",", "task_name", "+", "\"_eval.tf_record\"", ")", "\n", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "eval_file", ")", ":", "\n", "      ", "file_based_convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ",", "eval_file", ",", "\n", "dupe_factor", ",", "label_min", ",", "label_max", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num examples = %d (%d actual, %d padding)\"", ",", "\n", "len", "(", "eval_examples", ")", ",", "num_actual_eval_examples", ",", "\n", "len", "(", "eval_examples", ")", "-", "num_actual_eval_examples", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "eval_batch_size", ")", "\n", "\n", "# This tells the estimator to run through the entire set.", "\n", "eval_drop_remainder", "=", "False", "\n", "eval_input_fn", "=", "file_based_input_fn_builder", "(", "\n", "input_file", "=", "eval_file", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "False", ",", "\n", "drop_remainder", "=", "eval_drop_remainder", ",", "\n", "is_regression", "=", "is_regression", ")", "\n", "\n", "return", "eval_input_fn", "\n", "\n", "", "def", "get_predict_input_fn", "(", ")", ":", "\n", "    ", "predict_examples", "=", "None", "\n", "if", "FLAGS", ".", "do_predict_on_dev", ":", "\n", "      ", "predict_examples", "=", "processor", ".", "get_dev_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "", "elif", "FLAGS", ".", "do_predict_on_full", ":", "\n", "      ", "train_examples", "=", "processor", ".", "get_train_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "predict_examples", "=", "processor", ".", "get_test_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "predict_examples", ".", "extend", "(", "eval_examples", "+", "train_examples", ")", "\n", "", "else", ":", "\n", "      ", "predict_examples", "=", "processor", ".", "get_test_examples", "(", "FLAGS", ".", "data_dir", ")", "\n", "", "num_actual_predict_examples", "=", "len", "(", "predict_examples", ")", "\n", "\n", "cached_dir", "=", "FLAGS", ".", "cached_dir", "\n", "if", "not", "cached_dir", ":", "\n", "      ", "cached_dir", "=", "output_dir", "\n", "", "predict_file", "=", "os", ".", "path", ".", "join", "(", "cached_dir", ",", "task_name", "+", "\"_predict.tf_record\"", ")", "\n", "\n", "file_based_convert_examples_to_features", "(", "\n", "predict_examples", ",", "label_list", ",", "FLAGS", ".", "max_seq_length", ",", "tokenizer", ",", "predict_file", ",", "\n", "dupe_factor", ",", "label_min", ",", "label_max", ")", "\n", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Running prediction*****\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Num examples = %d (%d actual, %d padding)\"", ",", "\n", "len", "(", "predict_examples", ")", ",", "num_actual_predict_examples", ",", "\n", "len", "(", "predict_examples", ")", "-", "num_actual_predict_examples", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"  Batch size = %d\"", ",", "FLAGS", ".", "predict_batch_size", ")", "\n", "\n", "predict_drop_remainder", "=", "False", "\n", "predict_input_fn", "=", "file_based_input_fn_builder", "(", "\n", "input_file", "=", "predict_file", ",", "\n", "seq_length", "=", "FLAGS", ".", "max_seq_length", ",", "\n", "is_training", "=", "False", ",", "\n", "drop_remainder", "=", "predict_drop_remainder", ",", "\n", "is_regression", "=", "is_regression", ")", "\n", "\n", "return", "predict_input_fn", ",", "num_actual_predict_examples", "\n", "\n", "", "eval_steps", "=", "None", "\n", "\n", "if", "FLAGS", ".", "do_train", "and", "FLAGS", ".", "num_train_epochs", ">", "1e-6", ":", "\n", "    ", "train_input_fn", "=", "get_train_input_fn", "(", ")", "\n", "if", "FLAGS", ".", "early_stopping", ":", "\n", "      ", "eval_input_fn", "=", "get_eval_input_fn", "(", ")", "\n", "early_stopping_hook", "=", "tf", ".", "estimator", ".", "experimental", ".", "stop_if_no_decrease_hook", "(", "\n", "estimator", ",", "metric_name", "=", "\"eval_pearsonr\"", ",", "\n", "max_steps_without_decrease", "=", "epoch_step", "//", "2", ",", "run_every_steps", "=", "epoch_step", ",", "run_every_secs", "=", "None", ")", "\n", "train_spec", "=", "tf", ".", "estimator", ".", "TrainSpec", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "num_train_steps", ",", "\n", "hooks", "=", "[", "early_stopping_hook", "]", ")", "\n", "\n", "start_delay_secs", "=", "FLAGS", ".", "start_delay_secs", "\n", "throttle_secs", "=", "FLAGS", ".", "throttle_secs", "\n", "tf", ".", "logging", ".", "info", "(", "\"start_delay_secs: %d; throttle_secs: %d\"", "%", "(", "start_delay_secs", ",", "throttle_secs", ")", ")", "\n", "eval_spec", "=", "tf", ".", "estimator", ".", "EvalSpec", "(", "input_fn", "=", "eval_input_fn", ",", "steps", "=", "eval_steps", ",", "\n", "start_delay_secs", "=", "start_delay_secs", ",", "throttle_secs", "=", "throttle_secs", ")", "\n", "tf", ".", "estimator", ".", "train_and_evaluate", "(", "estimator", ",", "train_spec", ",", "eval_spec", ")", "\n", "", "else", ":", "\n", "      ", "estimator", ".", "train", "(", "input_fn", "=", "train_input_fn", ",", "max_steps", "=", "num_train_steps", ")", "\n", "\n", "", "", "if", "FLAGS", ".", "do_eval", ":", "\n", "    ", "eval_input_fn", "=", "get_eval_input_fn", "(", ")", "\n", "result", "=", "estimator", ".", "evaluate", "(", "input_fn", "=", "eval_input_fn", ",", "steps", "=", "eval_steps", ")", "\n", "\n", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "\"eval_results.txt\"", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n", "", "", "", "if", "FLAGS", ".", "do_predict", ":", "\n", "    ", "predict_input_fn", ",", "num_actual_predict_examples", "=", "get_predict_input_fn", "(", ")", "\n", "checkpoint_path", "=", "None", "\n", "if", "FLAGS", ".", "eval_checkpoint_name", ":", "\n", "      ", "checkpoint_path", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "FLAGS", ".", "eval_checkpoint_name", ")", "\n", "", "result", "=", "estimator", ".", "predict", "(", "input_fn", "=", "predict_input_fn", ",", "\n", "checkpoint_path", "=", "checkpoint_path", ")", "\n", "\n", "def", "round_float_list", "(", "values", ")", ":", "\n", "      ", "values", "=", "[", "round", "(", "float", "(", "x", ")", ",", "6", ")", "for", "x", "in", "values", ".", "flat", "]", "\n", "return", "values", "\n", "\n", "", "fname", "=", "\"\"", "\n", "if", "FLAGS", ".", "do_predict_on_full", ":", "\n", "      ", "fname", "+=", "\"full\"", "\n", "", "elif", "FLAGS", ".", "do_predict_on_dev", ":", "\n", "      ", "fname", "+=", "\"dev\"", "\n", "", "else", ":", "\n", "      ", "fname", "+=", "\"test\"", "\n", "\n", "", "if", "FLAGS", ".", "predict_pool", ":", "\n", "      ", "fname", "+=", "\"_pooled.tsv\"", "\n", "", "else", ":", "\n", "      ", "fname", "+=", "\"_results.tsv\"", "\n", "\n", "", "if", "FLAGS", ".", "eval_checkpoint_name", ":", "\n", "      ", "fname", "=", "FLAGS", ".", "eval_checkpoint_name", "+", "\".\"", "+", "fname", "\n", "", "output_predict_file", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "fname", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "output_predict_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "      ", "num_written_lines", "=", "0", "\n", "tf", ".", "logging", ".", "info", "(", "\"***** Predict results *****\"", ")", "\n", "for", "(", "i", ",", "prediction", ")", "in", "enumerate", "(", "result", ")", ":", "\n", "\n", "        ", "if", "is_regression", ":", "\n", "          ", "if", "FLAGS", ".", "predict_pool", ":", "\n", "            ", "embedding_a", "=", "prediction", "[", "\"embedding_a\"", "]", "\n", "embedding_b", "=", "prediction", "[", "\"embedding_b\"", "]", "\n", "\n", "output_json", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "output_json", "[", "\"embedding_a\"", "]", "=", "round_float_list", "(", "embedding_a", ")", "\n", "output_json", "[", "\"embedding_b\"", "]", "=", "round_float_list", "(", "embedding_b", ")", "\n", "\n", "output_line", "=", "json", ".", "dumps", "(", "output_json", ")", "+", "\"\\n\"", "\n", "", "else", ":", "\n", "            ", "cos_similarity", "=", "prediction", "[", "\"cos_similarity\"", "]", "\n", "if", "i", ">=", "num_actual_predict_examples", ":", "\n", "              ", "break", "\n", "", "output_line", "=", "str", "(", "cos_similarity", ")", "+", "\"\\n\"", "\n", "", "", "else", ":", "\n", "          ", "raise", "NotImplementedError", "\n", "\n", "", "writer", ".", "write", "(", "output_line", ")", "\n", "num_written_lines", "+=", "1", "\n", "", "", "assert", "num_written_lines", "==", "num_actual_predict_examples", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\"*** output_dir ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "output_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list": [[19, 36], ["tensorflow.convert_to_tensor", "tf.convert_to_tensor.get_shape().as_list", "tensorflow.shape", "enumerate", "tensorflow.shape", "ret.append", "tf.convert_to_tensor.get_shape", "tf.convert_to_tensor.get_shape"], "function", ["None"], ["def", "get_shape_list", "(", "x", ")", ":", "\n", "  ", "\"\"\"Return list of dims, statically where possible.\"\"\"", "\n", "x", "=", "tf", ".", "convert_to_tensor", "(", "x", ")", "\n", "\n", "# If unknown rank, return dynamic shape", "\n", "if", "x", ".", "get_shape", "(", ")", ".", "dims", "is", "None", ":", "\n", "    ", "return", "tf", ".", "shape", "(", "x", ")", "\n", "\n", "", "static", "=", "x", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "\n", "shape", "=", "tf", ".", "shape", "(", "x", ")", "\n", "\n", "ret", "=", "[", "]", "\n", "for", "i", ",", "dim", "in", "enumerate", "(", "static", ")", ":", "\n", "    ", "if", "dim", "is", "None", ":", "\n", "      ", "dim", "=", "shape", "[", "i", "]", "\n", "", "ret", ".", "append", "(", "dim", ")", "\n", "", "return", "ret", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_eps": [[37, 40], ["None"], "function", ["None"], ["", "def", "get_eps", "(", "dist", ",", "x", ")", ":", "\n", "  ", "\"\"\"Z = (X - mu) / sigma.\"\"\"", "\n", "return", "(", "x", "-", "dist", ".", "loc", ")", "/", "dist", ".", "scale", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.set_eps": [[42, 45], ["None"], "function", ["None"], ["", "def", "set_eps", "(", "dist", ",", "eps", ")", ":", "\n", "  ", "\"\"\"Z = eps * sigma + mu.\"\"\"", "\n", "return", "eps", "*", "dist", ".", "scale", "+", "dist", ".", "loc", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign": [[48, 53], ["w.assign.assign", "tensorflow.control_dependencies"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign"], ["", "@", "add_arg_scope", "\n", "def", "assign", "(", "w", ",", "initial_value", ")", ":", "\n", "  ", "w", "=", "w", ".", "assign", "(", "initial_value", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "w", "]", ")", ":", "\n", "    ", "return", "w", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_variable_ddi": [[54, 67], ["tensorflow.compat.v1.get_variable", "isinstance", "tensorflow.cond", "glow_ops_1x1.assign", "glow_ops_1x1.assign"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.assign"], ["", "", "@", "add_arg_scope", "\n", "def", "get_variable_ddi", "(", "name", ",", "shape", ",", "initial_value", ",", "dtype", "=", "tf", ".", "float32", ",", "init", "=", "False", ",", "\n", "trainable", "=", "True", ")", ":", "\n", "  ", "\"\"\"Wrapper for data-dependent initialization.\"\"\"", "\n", "# If init is a tf bool: w is assigned dynamically at runtime.", "\n", "# If init is a python bool: then w is determined during graph construction.", "\n", "w", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "name", ",", "shape", ",", "dtype", ",", "None", ",", "trainable", "=", "trainable", ")", "\n", "if", "isinstance", "(", "init", ",", "bool", ")", ":", "\n", "    ", "if", "init", ":", "\n", "      ", "return", "assign", "(", "w", ",", "initial_value", ")", "\n", "", "return", "w", "\n", "", "else", ":", "\n", "    ", "return", "tf", ".", "cond", "(", "init", ",", "lambda", ":", "assign", "(", "w", ",", "initial_value", ")", ",", "lambda", ":", "w", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_dropout": [[68, 82], ["tensorflow.layers.dropout"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.None.modeling.dropout"], ["", "", "@", "add_arg_scope", "\n", "def", "get_dropout", "(", "x", ",", "rate", "=", "0.0", ",", "init", "=", "True", ")", ":", "\n", "  ", "\"\"\"Dropout x with dropout_rate = rate.\n  Apply zero dropout during init or prediction time.\n  Args:\n    x: 4-D Tensor, shape=(NHWC).\n    rate: Dropout rate.\n    init: Initialization.\n  Returns:\n    x: activations after dropout.\n  \"\"\"", "\n", "if", "init", "or", "rate", "==", "0", ":", "\n", "    ", "return", "x", "\n", "", "return", "tf", ".", "layers", ".", "dropout", "(", "x", ",", "rate", "=", "rate", ",", "training", "=", "True", ")", "# TODO", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.default_initializer": [[83, 85], ["tensorflow.random_normal_initializer"], "function", ["None"], ["", "def", "default_initializer", "(", "std", "=", "0.05", ")", ":", "\n", "  ", "return", "tf", ".", "random_normal_initializer", "(", "0.", ",", "std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.actnorm": [[91, 125], ["tensorflow.contrib.framework.python.ops.arg_scope", "tensorflow.compat.v1.variable_scope", "glow_ops_1x1.actnorm_center", "glow_ops_1x1.actnorm_scale", "glow_ops_1x1.actnorm_scale", "glow_ops_1x1.actnorm_center"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.actnorm_center", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.actnorm_scale", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.actnorm_scale", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.actnorm_center"], ["", "@", "add_arg_scope", "\n", "def", "actnorm", "(", "name", ",", "x", ",", "logscale_factor", "=", "3.", ",", "reverse", "=", "False", ",", "init", "=", "False", ",", "\n", "trainable", "=", "True", ")", ":", "\n", "  ", "\"\"\"x_{ij} = s x x_{ij} + b. Per-channel scaling and bias.\n  If init is set to True, the scaling and bias are initialized such\n  that the mean and variance of the output activations of the first minibatch\n  are zero and one respectively.\n  Args:\n    name: variable scope.\n    x: input\n    logscale_factor: Used in actnorm_scale. Optimizes f(ls*s') instead of f(s)\n                     where s' = s / ls. Helps in faster convergence.\n    reverse: forward or reverse operation.\n    init: Whether or not to do data-dependent initialization.\n    trainable:\n  Returns:\n    x: output after adding bias and scaling.\n    objective: log(sum(s))\n  \"\"\"", "\n", "var_arg_scope", "=", "arg_scope", "(", "[", "get_variable_ddi", "]", ",", "trainable", "=", "trainable", ")", "\n", "var_scope", "=", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", "\n", "\n", "with", "var_scope", ",", "var_arg_scope", ":", "\n", "    ", "if", "not", "reverse", ":", "\n", "      ", "x", "=", "actnorm_center", "(", "name", "+", "\"_center\"", ",", "x", ",", "reverse", ",", "init", "=", "init", ")", "\n", "x", ",", "objective", "=", "actnorm_scale", "(", "\n", "name", "+", "\"_scale\"", ",", "x", ",", "logscale_factor", "=", "logscale_factor", ",", "\n", "reverse", "=", "reverse", ",", "init", "=", "init", ")", "\n", "", "else", ":", "\n", "      ", "x", ",", "objective", "=", "actnorm_scale", "(", "\n", "name", "+", "\"_scale\"", ",", "x", ",", "logscale_factor", "=", "logscale_factor", ",", "\n", "reverse", "=", "reverse", ",", "init", "=", "init", ")", "\n", "x", "=", "actnorm_center", "(", "name", "+", "\"_center\"", ",", "x", ",", "reverse", ",", "init", "=", "init", ")", "\n", "", "return", "x", ",", "objective", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.actnorm_center": [[127, 157], ["glow_ops_1x1.get_shape_list", "tensorflow.compat.v1.variable_scope", "len", "tensorflow.reduce_mean", "glow_ops_1x1.get_variable_ddi", "len", "len", "len", "tensorflow.reduce_mean", "glow_ops_1x1.get_variable_ddi"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_variable_ddi", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_variable_ddi"], ["", "", "@", "add_arg_scope", "\n", "def", "actnorm_center", "(", "name", ",", "x", ",", "reverse", "=", "False", ",", "init", "=", "False", ")", ":", "\n", "  ", "\"\"\"Add a bias to x.\n  Initialize such that the output of the first minibatch is zero centered\n  per channel.\n  Args:\n    name: scope\n    x: 2-D or 4-D Tensor.\n    reverse: Forward or backward operation.\n    init: data-dependent initialization.\n  Returns:\n    x_center: (x + b), if reverse is True and (x - b) otherwise.\n  \"\"\"", "\n", "shape", "=", "get_shape_list", "(", "x", ")", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "assert", "len", "(", "shape", ")", "==", "2", "or", "len", "(", "shape", ")", "==", "4", "\n", "if", "len", "(", "shape", ")", "==", "2", ":", "\n", "      ", "x_mean", "=", "tf", ".", "reduce_mean", "(", "x", ",", "[", "0", "]", ",", "keepdims", "=", "True", ")", "\n", "b", "=", "get_variable_ddi", "(", "\"b\"", ",", "(", "1", ",", "shape", "[", "1", "]", ")", ",", "initial_value", "=", "-", "x_mean", ",", "\n", "init", "=", "init", ")", "\n", "", "elif", "len", "(", "shape", ")", "==", "4", ":", "\n", "      ", "x_mean", "=", "tf", ".", "reduce_mean", "(", "x", ",", "[", "0", ",", "1", ",", "2", "]", ",", "keepdims", "=", "True", ")", "\n", "b", "=", "get_variable_ddi", "(", "\n", "\"b\"", ",", "(", "1", ",", "1", ",", "1", ",", "shape", "[", "3", "]", ")", ",", "initial_value", "=", "-", "x_mean", ",", "init", "=", "init", ")", "\n", "\n", "", "if", "not", "reverse", ":", "\n", "      ", "x", "+=", "b", "\n", "", "else", ":", "\n", "      ", "x", "-=", "b", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.actnorm_scale": [[159, 192], ["glow_ops_1x1.get_shape_list", "tensorflow.compat.v1.variable_scope", "glow_ops_1x1.get_variable_ddi", "len", "tensorflow.reduce_mean", "tensorflow.math.log", "tensorflow.reduce_sum", "len", "len", "len", "tensorflow.reduce_mean", "tensorflow.exp", "tensorflow.exp", "tensorflow.sqrt"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_variable_ddi"], ["", "", "@", "add_arg_scope", "\n", "def", "actnorm_scale", "(", "name", ",", "x", ",", "logscale_factor", "=", "3.", ",", "reverse", "=", "False", ",", "init", "=", "False", ")", ":", "\n", "  ", "\"\"\"Per-channel scaling of x.\"\"\"", "\n", "x_shape", "=", "get_shape_list", "(", "x", ")", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "\n", "# Variance initialization logic.", "\n", "    ", "assert", "len", "(", "x_shape", ")", "==", "2", "or", "len", "(", "x_shape", ")", "==", "4", "\n", "if", "len", "(", "x_shape", ")", "==", "2", ":", "\n", "      ", "x_var", "=", "tf", ".", "reduce_mean", "(", "x", "**", "2", ",", "[", "0", "]", ",", "keepdims", "=", "True", ")", "\n", "logdet_factor", "=", "1", "\n", "var_shape", "=", "(", "1", ",", "x_shape", "[", "1", "]", ")", "\n", "", "elif", "len", "(", "x_shape", ")", "==", "4", ":", "\n", "      ", "x_var", "=", "tf", ".", "reduce_mean", "(", "x", "**", "2", ",", "[", "0", ",", "1", ",", "2", "]", ",", "keepdims", "=", "True", ")", "\n", "logdet_factor", "=", "x_shape", "[", "1", "]", "*", "x_shape", "[", "2", "]", "\n", "var_shape", "=", "(", "1", ",", "1", ",", "1", ",", "x_shape", "[", "3", "]", ")", "\n", "\n", "", "init_value", "=", "tf", ".", "math", ".", "log", "(", "1.0", "/", "(", "tf", ".", "sqrt", "(", "x_var", ")", "+", "1e-6", ")", ")", "/", "logscale_factor", "\n", "logs", "=", "get_variable_ddi", "(", "\"logs\"", ",", "var_shape", ",", "initial_value", "=", "init_value", ",", "\n", "init", "=", "init", ")", "\n", "logs", "=", "logs", "*", "logscale_factor", "\n", "\n", "# Function and reverse function.", "\n", "if", "not", "reverse", ":", "\n", "      ", "x", "=", "x", "*", "tf", ".", "exp", "(", "logs", ")", "\n", "", "else", ":", "\n", "      ", "x", "=", "x", "*", "tf", ".", "exp", "(", "-", "logs", ")", "\n", "\n", "# Objective calculation, h * w * sum(log|s|)", "\n", "", "dlogdet", "=", "tf", ".", "reduce_sum", "(", "logs", ")", "*", "logdet_factor", "\n", "if", "reverse", ":", "\n", "      ", "dlogdet", "*=", "-", "1", "\n", "", "return", "x", ",", "dlogdet", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.invertible_1x1_conv": [[197, 278], ["glow_ops_1x1.get_shape_list", "numpy.zeros().astype", "range", "numpy.random.rand", "[].astype", "scipy.linalg.lu", "numpy.diag", "numpy.sign", "numpy.log", "numpy.triu", "tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.get_variable", "numpy.abs", "tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.get_variable", "tensorflow.compat.v1.get_variable", "tensorflow.compat.v1.get_variable", "tensorflow.compat.v1.get_variable", "tensorflow.compat.v1.get_variable", "numpy.tril", "tensorflow.matmul", "numpy.zeros", "tensorflow.reshape", "tensorflow.nn.conv2d", "tensorflow.reshape", "tensorflow.nn.conv2d", "numpy.ones", "tensorflow.eye", "tensorflow.linalg.diag", "tensorflow.matmul", "tensorflow.reduce_sum", "tensorflow.cast", "tensorflow.reshape", "tensorflow.nn.conv2d", "tensorflow.reshape", "tensorflow.nn.conv2d", "tensorflow.linalg.inv", "scipy.linalg.qr", "numpy.transpose", "tensorflow.linalg.inv", "tensorflow.exp"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list"], ["", "", "@", "add_arg_scope", "\n", "def", "invertible_1x1_conv", "(", "name", ",", "x", ",", "reverse", "=", "False", ",", "permutation", "=", "False", ")", ":", "\n", "  ", "\"\"\"1X1 convolution on x.\n  The 1X1 convolution is parametrized as P*L*(U + sign(s)*exp(log(s))) where\n  1. P is a permutation matrix.\n  2. L is a lower triangular matrix with diagonal entries unity.\n  3. U is a upper triangular matrix where the diagonal entries zero.\n  4. s is a vector.\n  sign(s) and P are fixed and the remaining are optimized. P, L, U and s are\n  initialized by the PLU decomposition of a random rotation matrix.\n  Args:\n    name: scope\n    x: Input Tensor.\n    reverse: whether the pass is from z -> x or x -> z.\n  Returns:\n    x_conv: x after a 1X1 convolution is applied on x.\n    objective: sum(log(s))\n  \"\"\"", "\n", "_", ",", "height", ",", "width", ",", "channels", "=", "get_shape_list", "(", "x", ")", "\n", "w_shape", "=", "[", "channels", ",", "channels", "]", "\n", "\n", "if", "permutation", ":", "\n", "    ", "np_w", "=", "np", ".", "zeros", "(", "(", "channels", ",", "channels", ")", ")", ".", "astype", "(", "\"float32\"", ")", "\n", "for", "i", "in", "range", "(", "channels", ")", ":", "\n", "        ", "np_w", "[", "i", "]", "[", "channels", "-", "1", "-", "i", "]", "=", "1.", "\n", "\n", "", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "      ", "w", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"w\"", ",", "initializer", "=", "np_w", ",", "trainable", "=", "False", ")", "\n", "\n", "# If height or width cannot be statically determined then they end up as", "\n", "# tf.int32 tensors, which cannot be directly multiplied with a floating", "\n", "# point tensor without a cast.", "\n", "objective", "=", "0.", "\n", "if", "not", "reverse", ":", "\n", "        ", "w", "=", "tf", ".", "reshape", "(", "w", ",", "[", "1", ",", "1", "]", "+", "w_shape", ")", "\n", "x", "=", "tf", ".", "nn", ".", "conv2d", "(", "x", ",", "w", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ",", "data_format", "=", "\"NHWC\"", ")", "\n", "", "else", ":", "\n", "        ", "w_inv", "=", "tf", ".", "reshape", "(", "tf", ".", "linalg", ".", "inv", "(", "w", ")", ",", "[", "1", ",", "1", "]", "+", "w_shape", ")", "\n", "x", "=", "tf", ".", "nn", ".", "conv2d", "(", "\n", "x", ",", "w_inv", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ",", "data_format", "=", "\"NHWC\"", ")", "\n", "objective", "*=", "-", "1", "\n", "", "", "return", "x", ",", "objective", "\n", "", "else", ":", "\n", "# Random rotation-matrix Q", "\n", "    ", "random_matrix", "=", "np", ".", "random", ".", "rand", "(", "channels", ",", "channels", ")", "\n", "np_w", "=", "scipy", ".", "linalg", ".", "qr", "(", "random_matrix", ")", "[", "0", "]", ".", "astype", "(", "\"float32\"", ")", "\n", "\n", "# Initialize P,L,U and s from the LU decomposition of a random rotation matrix", "\n", "np_p", ",", "np_l", ",", "np_u", "=", "scipy", ".", "linalg", ".", "lu", "(", "np_w", ")", "\n", "np_s", "=", "np", ".", "diag", "(", "np_u", ")", "\n", "np_sign_s", "=", "np", ".", "sign", "(", "np_s", ")", "\n", "np_log_s", "=", "np", ".", "log", "(", "np", ".", "abs", "(", "np_s", ")", ")", "\n", "np_u", "=", "np", ".", "triu", "(", "np_u", ",", "k", "=", "1", ")", "\n", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "      ", "p", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"P\"", ",", "initializer", "=", "np_p", ",", "trainable", "=", "False", ")", "\n", "l", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"L\"", ",", "initializer", "=", "np_l", ")", "\n", "sign_s", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\n", "\"sign_S\"", ",", "initializer", "=", "np_sign_s", ",", "trainable", "=", "False", ")", "\n", "log_s", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"log_S\"", ",", "initializer", "=", "np_log_s", ")", "\n", "u", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"U\"", ",", "initializer", "=", "np_u", ")", "\n", "\n", "# W = P * L * (U + sign_s * exp(log_s))", "\n", "l_mask", "=", "np", ".", "tril", "(", "np", ".", "ones", "(", "[", "channels", ",", "channels", "]", ",", "dtype", "=", "np", ".", "float32", ")", ",", "-", "1", ")", "\n", "l", "=", "l", "*", "l_mask", "+", "tf", ".", "eye", "(", "channels", ",", "channels", ")", "\n", "u", "=", "u", "*", "np", ".", "transpose", "(", "l_mask", ")", "+", "tf", ".", "linalg", ".", "diag", "(", "sign_s", "*", "tf", ".", "exp", "(", "log_s", ")", ")", "\n", "w", "=", "tf", ".", "matmul", "(", "p", ",", "tf", ".", "matmul", "(", "l", ",", "u", ")", ")", "\n", "\n", "# If height or width cannot be statically determined then they end up as", "\n", "# tf.int32 tensors, which cannot be directly multiplied with a floating", "\n", "# point tensor without a cast.", "\n", "objective", "=", "tf", ".", "reduce_sum", "(", "log_s", ")", "*", "tf", ".", "cast", "(", "height", "*", "width", ",", "log_s", ".", "dtype", ")", "\n", "if", "not", "reverse", ":", "\n", "        ", "w", "=", "tf", ".", "reshape", "(", "w", ",", "[", "1", ",", "1", "]", "+", "w_shape", ")", "\n", "x", "=", "tf", ".", "nn", ".", "conv2d", "(", "x", ",", "w", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ",", "data_format", "=", "\"NHWC\"", ")", "\n", "", "else", ":", "\n", "        ", "w_inv", "=", "tf", ".", "reshape", "(", "tf", ".", "linalg", ".", "inv", "(", "w", ")", ",", "[", "1", ",", "1", "]", "+", "w_shape", ")", "\n", "x", "=", "tf", ".", "nn", ".", "conv2d", "(", "\n", "x", ",", "w_inv", ",", "[", "1", ",", "1", ",", "1", ",", "1", "]", ",", "\"SAME\"", ",", "data_format", "=", "\"NHWC\"", ")", "\n", "objective", "*=", "-", "1", "\n", "", "", "return", "x", ",", "objective", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.add_edge_bias": [[284, 305], ["glow_ops_1x1.get_shape_list", "tensorflow.zeros", "tensorflow.pad", "tensorflow.pad", "tensorflow.concat"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list"], ["", "", "def", "add_edge_bias", "(", "x", ",", "filter_size", ")", ":", "\n", "  ", "\"\"\"Pad x and concatenates an edge bias across the depth of x.\n  The edge bias can be thought of as a binary feature which is unity when\n  the filter is being convolved over an edge and zero otherwise.\n  Args:\n    x: Input tensor, shape (NHWC)\n    filter_size: filter_size to determine padding.\n  Returns:\n    x_pad: Input tensor, shape (NHW(c+1))\n  \"\"\"", "\n", "x_shape", "=", "get_shape_list", "(", "x", ")", "\n", "if", "filter_size", "[", "0", "]", "==", "1", "and", "filter_size", "[", "1", "]", "==", "1", ":", "\n", "    ", "return", "x", "\n", "", "a", "=", "(", "filter_size", "[", "0", "]", "-", "1", ")", "//", "2", "# vertical padding size", "\n", "b", "=", "(", "filter_size", "[", "1", "]", "-", "1", ")", "//", "2", "# horizontal padding size", "\n", "padding", "=", "[", "[", "0", ",", "0", "]", ",", "[", "a", ",", "a", "]", ",", "[", "b", ",", "b", "]", ",", "[", "0", ",", "0", "]", "]", "\n", "x_bias", "=", "tf", ".", "zeros", "(", "x_shape", "[", ":", "-", "1", "]", "+", "[", "1", "]", ")", "\n", "\n", "x", "=", "tf", ".", "pad", "(", "x", ",", "padding", ")", "\n", "x_pad", "=", "tf", ".", "pad", "(", "x_bias", ",", "padding", ",", "constant_values", "=", "1", ")", "\n", "return", "tf", ".", "concat", "(", "[", "x", ",", "x_pad", "]", ",", "axis", "=", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv": [[307, 375], ["glow_ops_1x1.get_shape_list", "ValueError", "len", "glow_ops_1x1.add_edge_bias", "NotImplementedError", "glow_ops_1x1.get_shape_list", "tensorflow.compat.v1.variable_scope", "tensorflow.compat.v1.get_variable", "conv_filter", "glow_ops_1x1.default_initializer", "actnorm_func", "tensorflow.compat.v1.get_variable", "tensorflow.compat.v1.get_variable", "tensorflow.exp", "tensorflow.zeros_initializer", "tensorflow.zeros_initializer", "tensorflow.zeros_initializer"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.add_edge_bias", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.default_initializer"], ["", "@", "add_arg_scope", "\n", "def", "conv", "(", "name", ",", "x", ",", "output_channels", ",", "filter_size", "=", "None", ",", "stride", "=", "None", ",", "\n", "logscale_factor", "=", "3.0", ",", "apply_actnorm", "=", "True", ",", "conv_init", "=", "\"default\"", ",", "\n", "dilations", "=", "None", ")", ":", "\n", "  ", "\"\"\"Convolutional layer with edge bias padding and optional actnorm.\n  If x is 5-dimensional, actnorm is applied independently across every\n  time-step.\n  Args:\n    name: variable scope.\n    x: 4-D Tensor or 5-D Tensor of shape NHWC or NTHWC\n    output_channels: Number of output channels.\n    filter_size: list of ints, if None [3, 3] and [2, 3, 3] are defaults for\n                 4-D and 5-D input tensors respectively.\n    stride: list of ints, default stride: 1\n    logscale_factor: see actnorm for parameter meaning.\n    apply_actnorm: if apply_actnorm the activations of the first minibatch\n                   have zero mean and unit variance. Else, there is no scaling\n                   applied.\n    conv_init: default or zeros. default is a normal distribution with 0.05 std.\n    dilations: List of integers, apply dilations.\n  Returns:\n    x: actnorm(conv2d(x))\n  Raises:\n    ValueError: if init is set to \"zeros\" and apply_actnorm is set to True.\n  \"\"\"", "\n", "if", "conv_init", "==", "\"zeros\"", "and", "apply_actnorm", ":", "\n", "    ", "raise", "ValueError", "(", "\"apply_actnorm is unstable when init is set to zeros.\"", ")", "\n", "\n", "", "x_shape", "=", "get_shape_list", "(", "x", ")", "\n", "is_2d", "=", "len", "(", "x_shape", ")", "==", "4", "\n", "num_steps", "=", "x_shape", "[", "1", "]", "\n", "\n", "# set filter_size, stride and in_channels", "\n", "if", "is_2d", ":", "\n", "    ", "if", "filter_size", "is", "None", ":", "\n", "      ", "filter_size", "=", "[", "1", ",", "1", "]", "# filter_size = [3, 3]", "\n", "", "if", "stride", "is", "None", ":", "\n", "      ", "stride", "=", "[", "1", ",", "1", "]", "\n", "", "if", "dilations", "is", "None", ":", "\n", "      ", "dilations", "=", "[", "1", ",", "1", ",", "1", ",", "1", "]", "\n", "", "actnorm_func", "=", "actnorm", "\n", "x", "=", "add_edge_bias", "(", "x", ",", "filter_size", "=", "filter_size", ")", "\n", "conv_filter", "=", "tf", ".", "nn", ".", "conv2d", "\n", "", "else", ":", "\n", "    ", "raise", "NotImplementedError", "(", "'x must be a NHWC 4-D Tensor!'", ")", "\n", "\n", "", "in_channels", "=", "get_shape_list", "(", "x", ")", "[", "-", "1", "]", "\n", "filter_shape", "=", "filter_size", "+", "[", "in_channels", ",", "output_channels", "]", "\n", "stride_shape", "=", "[", "1", "]", "+", "stride", "+", "[", "1", "]", "\n", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "\n", "    ", "if", "conv_init", "==", "\"default\"", ":", "\n", "      ", "initializer", "=", "default_initializer", "(", ")", "\n", "", "elif", "conv_init", "==", "\"zeros\"", ":", "\n", "      ", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", "\n", "\n", "", "w", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"W\"", ",", "filter_shape", ",", "tf", ".", "float32", ",", "initializer", "=", "initializer", ")", "\n", "x", "=", "conv_filter", "(", "x", ",", "w", ",", "stride_shape", ",", "padding", "=", "\"VALID\"", ",", "dilations", "=", "dilations", ")", "\n", "if", "apply_actnorm", ":", "\n", "      ", "x", ",", "_", "=", "actnorm_func", "(", "\"actnorm\"", ",", "x", ",", "logscale_factor", "=", "logscale_factor", ")", "\n", "", "else", ":", "\n", "      ", "x", "+=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"b\"", ",", "[", "1", ",", "1", ",", "1", ",", "output_channels", "]", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "logs", "=", "tf", ".", "compat", ".", "v1", ".", "get_variable", "(", "\"logs\"", ",", "[", "1", ",", "output_channels", "]", ",", "\n", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "x", "*=", "tf", ".", "exp", "(", "logs", "*", "logscale_factor", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv_block": [[377, 427], ["tensorflow.compat.v1.variable_scope", "glow_ops_1x1.get_shape_list", "glow_ops_1x1.conv", "tensorflow.nn.relu", "glow_ops_1x1.get_dropout", "glow_ops_1x1.get_dropout", "len", "NotImplementedError", "glow_ops_1x1.conv", "tensorflow.nn.relu", "glow_ops_1x1.conv", "glow_ops_1x1.conv", "tensorflow.nn.tanh", "tensorflow.nn.sigmoid"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_dropout", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_dropout", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv"], ["", "", "@", "add_arg_scope", "\n", "def", "conv_block", "(", "name", ",", "x", ",", "mid_channels", ",", "dilations", "=", "None", ",", "activation", "=", "\"relu\"", ",", "\n", "dropout", "=", "0.0", ")", ":", "\n", "  ", "\"\"\"2 layer conv block used in the affine coupling layer.\n  Args:\n    name: variable scope.\n    x: 4-D or 5-D Tensor.\n    mid_channels: Output channels of the second layer.\n    dilations: Optional, list of integers.\n    activation: relu or gatu.\n      If relu, the second layer is relu(W*x)\n      If gatu, the second layer is tanh(W1*x) * sigmoid(W2*x)\n    dropout: Dropout probability.\n  Returns:\n    x: 4-D Tensor: Output activations.\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "\n", "    ", "x_shape", "=", "get_shape_list", "(", "x", ")", "\n", "is_2d", "=", "len", "(", "x_shape", ")", "==", "4", "\n", "num_steps", "=", "x_shape", "[", "1", "]", "\n", "if", "is_2d", ":", "\n", "      ", "first_filter", "=", "[", "1", ",", "1", "]", "# first_filter = [3, 3]", "\n", "second_filter", "=", "[", "1", ",", "1", "]", "\n", "", "else", ":", "\n", "      ", "raise", "NotImplementedError", "(", "'x must be a NHWC 4-D Tensor!'", ")", "\n", "\n", "# Edge Padding + conv2d + actnorm + relu:", "\n", "# [output: 512 channels]", "\n", "", "x", "=", "conv", "(", "\"1_1\"", ",", "x", ",", "output_channels", "=", "mid_channels", ",", "filter_size", "=", "first_filter", ",", "\n", "dilations", "=", "dilations", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "x", "=", "get_dropout", "(", "x", ",", "rate", "=", "dropout", ")", "\n", "\n", "# Padding + conv2d + actnorm + activation.", "\n", "# [input, output: 512 channels]", "\n", "if", "activation", "==", "\"relu\"", ":", "\n", "      ", "x", "=", "conv", "(", "\"1_2\"", ",", "x", ",", "output_channels", "=", "mid_channels", ",", "\n", "filter_size", "=", "second_filter", ",", "dilations", "=", "dilations", ")", "\n", "x", "=", "tf", ".", "nn", ".", "relu", "(", "x", ")", "\n", "", "elif", "activation", "==", "\"gatu\"", ":", "\n", "# x = tanh(w1*x) * sigm(w2*x)", "\n", "      ", "x_tanh", "=", "conv", "(", "\"1_tanh\"", ",", "x", ",", "output_channels", "=", "mid_channels", ",", "\n", "filter_size", "=", "second_filter", ",", "dilations", "=", "dilations", ")", "\n", "x_sigm", "=", "conv", "(", "\"1_sigm\"", ",", "x", ",", "output_channels", "=", "mid_channels", ",", "\n", "filter_size", "=", "second_filter", ",", "dilations", "=", "dilations", ")", "\n", "x", "=", "tf", ".", "nn", ".", "tanh", "(", "x_tanh", ")", "*", "tf", ".", "nn", ".", "sigmoid", "(", "x_sigm", ")", "\n", "\n", "", "x", "=", "get_dropout", "(", "x", ",", "rate", "=", "dropout", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv_stack": [[429, 457], ["tensorflow.compat.v1.variable_scope", "glow_ops_1x1.conv_block", "glow_ops_1x1.conv"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv_block", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv"], ["", "", "@", "add_arg_scope", "\n", "def", "conv_stack", "(", "name", ",", "x", ",", "mid_channels", ",", "output_channels", ",", "dilations", "=", "None", ",", "\n", "activation", "=", "\"relu\"", ",", "dropout", "=", "0.0", ")", ":", "\n", "  ", "\"\"\"3-layer convolutional stack.\n  Args:\n    name: variable scope.\n    x: 5-D Tensor.\n    mid_channels: Number of output channels of the first layer.\n    output_channels: Number of output channels.\n    dilations: Dilations to apply in the first 3x3 layer and the last 3x3 layer.\n               By default, apply no dilations.\n    activation: relu or gatu.\n      If relu, the second layer is relu(W*x)\n      If gatu, the second layer is tanh(W1*x) * sigmoid(W2*x)\n    dropout: float, 0.0\n  Returns:\n    output: output of 3 layer conv network.\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "\n", "    ", "x", "=", "conv_block", "(", "\"conv_block\"", ",", "x", ",", "mid_channels", "=", "mid_channels", ",", "\n", "dilations", "=", "dilations", ",", "activation", "=", "activation", ",", "\n", "dropout", "=", "dropout", ")", "\n", "\n", "# Final layer.", "\n", "x", "=", "conv", "(", "\"zeros\"", ",", "x", ",", "apply_actnorm", "=", "False", ",", "conv_init", "=", "\"zeros\"", ",", "\n", "output_channels", "=", "output_channels", ",", "dilations", "=", "dilations", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.additive_coupling": [[459, 487], ["tensorflow.compat.v1.variable_scope", "tensorflow.split", "glow_ops_1x1.conv_stack", "tensorflow.concat", "glow_ops_1x1.get_shape_list"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv_stack", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list"], ["", "@", "add_arg_scope", "\n", "def", "additive_coupling", "(", "name", ",", "x", ",", "mid_channels", "=", "512", ",", "reverse", "=", "False", ",", "\n", "activation", "=", "\"relu\"", ",", "dropout", "=", "0.0", ")", ":", "\n", "  ", "\"\"\"Reversible additive coupling layer.\n  Args:\n    name: variable scope.\n    x: 4-D Tensor, shape=(NHWC).\n    mid_channels: number of channels in the coupling layer.\n    reverse: Forward or reverse operation.\n    activation: \"relu\" or \"gatu\"\n    dropout: default, 0.0\n  Returns:\n    output: 4-D Tensor, shape=(NHWC)\n    objective: 0.0\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "output_channels", "=", "get_shape_list", "(", "x", ")", "[", "-", "1", "]", "//", "2", "\n", "x1", ",", "x2", "=", "tf", ".", "split", "(", "x", ",", "num_or_size_splits", "=", "2", ",", "axis", "=", "-", "1", ")", "\n", "\n", "z1", "=", "x1", "\n", "shift", "=", "conv_stack", "(", "\"nn\"", ",", "x1", ",", "mid_channels", ",", "output_channels", "=", "output_channels", ",", "\n", "activation", "=", "activation", ",", "dropout", "=", "dropout", ")", "\n", "\n", "if", "not", "reverse", ":", "\n", "      ", "z2", "=", "x2", "+", "shift", "\n", "", "else", ":", "\n", "      ", "z2", "=", "x2", "-", "shift", "\n", "", "return", "tf", ".", "concat", "(", "[", "z1", ",", "z2", "]", ",", "axis", "=", "3", ")", ",", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.affine_coupling": [[489, 528], ["tensorflow.compat.v1.variable_scope", "glow_ops_1x1.get_shape_list", "tensorflow.split", "glow_ops_1x1.conv_stack", "tensorflow.nn.sigmoid", "tensorflow.reduce_sum", "tensorflow.math.log", "tensorflow.concat"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv_stack"], ["", "", "@", "add_arg_scope", "\n", "def", "affine_coupling", "(", "name", ",", "x", ",", "mid_channels", "=", "512", ",", "activation", "=", "\"relu\"", ",", "\n", "reverse", "=", "False", ",", "dropout", "=", "0.0", ")", ":", "\n", "  ", "\"\"\"Reversible affine coupling layer.\n  Args:\n    name: variable scope.\n    x: 4-D Tensor.\n    mid_channels: number of channels in the coupling layer.\n    activation: Can be either \"relu\" or \"gatu\".\n    reverse: Forward or reverse operation.\n    dropout: default, 0.0\n  Returns:\n    output: x shifted and scaled by an affine transformation.\n    objective: log-determinant of the jacobian\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "x_shape", "=", "get_shape_list", "(", "x", ")", "\n", "x1", ",", "x2", "=", "tf", ".", "split", "(", "x", ",", "num_or_size_splits", "=", "2", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# scale, shift = NN(x1)", "\n", "# If reverse:", "\n", "# z2 = scale * (x2 + shift)", "\n", "# Else:", "\n", "# z2 = (x2 / scale) - shift", "\n", "z1", "=", "x1", "\n", "log_scale_and_shift", "=", "conv_stack", "(", "\n", "\"nn\"", ",", "x1", ",", "mid_channels", ",", "x_shape", "[", "-", "1", "]", ",", "activation", "=", "activation", ",", "\n", "dropout", "=", "dropout", ")", "\n", "shift", "=", "log_scale_and_shift", "[", ":", ",", ":", ",", ":", ",", "0", ":", ":", "2", "]", "\n", "scale", "=", "tf", ".", "nn", ".", "sigmoid", "(", "log_scale_and_shift", "[", ":", ",", ":", ",", ":", ",", "1", ":", ":", "2", "]", "+", "2.0", ")", "\n", "if", "not", "reverse", ":", "\n", "      ", "z2", "=", "(", "x2", "+", "shift", ")", "*", "scale", "\n", "", "else", ":", "\n", "      ", "z2", "=", "x2", "/", "scale", "-", "shift", "\n", "\n", "", "objective", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "math", ".", "log", "(", "scale", ")", ",", "axis", "=", "[", "1", ",", "2", ",", "3", "]", ")", "\n", "if", "reverse", ":", "\n", "      ", "objective", "*=", "-", "1", "\n", "", "return", "tf", ".", "concat", "(", "[", "z1", ",", "z2", "]", ",", "axis", "=", "3", ")", ",", "objective", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.single_conv_dist": [[533, 550], ["tensorflow.compat.v1.variable_scope", "glow_ops_1x1.get_shape_list", "glow_ops_1x1.conv", "tensorflow.distributions.Normal", "tensorflow.exp"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.conv"], ["", "", "@", "add_arg_scope", "\n", "def", "single_conv_dist", "(", "name", ",", "x", ",", "output_channels", "=", "None", ")", ":", "\n", "  ", "\"\"\"A 1x1 convolution mapping x to a standard normal distribution at init.\n  Args:\n    name: variable scope.\n    x: 4-D Tensor.\n    output_channels: number of channels of the mean and std.\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "x_shape", "=", "get_shape_list", "(", "x", ")", "\n", "if", "output_channels", "is", "None", ":", "\n", "      ", "output_channels", "=", "x_shape", "[", "-", "1", "]", "\n", "", "mean_log_scale", "=", "conv", "(", "\"conv2d\"", ",", "x", ",", "output_channels", "=", "2", "*", "output_channels", ",", "\n", "conv_init", "=", "\"zeros\"", ",", "apply_actnorm", "=", "False", ")", "\n", "mean", "=", "mean_log_scale", "[", ":", ",", ":", ",", ":", ",", "0", ":", ":", "2", "]", "\n", "log_scale", "=", "mean_log_scale", "[", ":", ",", ":", ",", ":", ",", "1", ":", ":", "2", "]", "\n", "return", "tf", ".", "distributions", ".", "Normal", "(", "mean", ",", "tf", ".", "exp", "(", "log_scale", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.revnet_step": [[555, 601], ["tensorflow.compat.v1.variable_scope", "functools.partial", "functools.partial", "op", "functools.partial", "functools.partial", "functools.partial", "functools.partial"], "function", ["None"], ["", "", "@", "add_arg_scope", "\n", "def", "revnet_step", "(", "name", ",", "x", ",", "hparams", ",", "reverse", "=", "True", ")", ":", "\n", "  ", "\"\"\"One step of glow generative flow.\n  Actnorm + invertible 1X1 conv + affine_coupling.\n  Args:\n    name: used for variable scope.\n    x: input\n    hparams: coupling_width is the only hparam that is being used in\n             this function.\n    reverse: forward or reverse pass.\n  Returns:\n    z: Output of one step of reversible flow.\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "if", "hparams", ".", "coupling", "==", "\"additive\"", ":", "\n", "      ", "coupling_layer", "=", "functools", ".", "partial", "(", "\n", "additive_coupling", ",", "name", "=", "\"additive\"", ",", "reverse", "=", "reverse", ",", "\n", "mid_channels", "=", "hparams", ".", "coupling_width", ",", "\n", "activation", "=", "hparams", ".", "activation", ",", "\n", "dropout", "=", "hparams", ".", "coupling_dropout", "if", "hparams", ".", "is_training", "else", "0", ")", "\n", "", "else", ":", "\n", "      ", "coupling_layer", "=", "functools", ".", "partial", "(", "\n", "affine_coupling", ",", "name", "=", "\"affine\"", ",", "reverse", "=", "reverse", ",", "\n", "mid_channels", "=", "hparams", ".", "coupling_width", ",", "\n", "activation", "=", "hparams", ".", "activation", ",", "\n", "dropout", "=", "hparams", ".", "coupling_dropout", "if", "hparams", ".", "is_training", "else", "0", ")", "\n", "\n", "", "if", "\"permutation\"", "in", "hparams", "and", "hparams", "[", "\"permutation\"", "]", "==", "True", ":", "\n", "      ", "ops", "=", "[", "\n", "functools", ".", "partial", "(", "actnorm", ",", "name", "=", "\"actnorm\"", ",", "reverse", "=", "reverse", ")", ",", "\n", "functools", ".", "partial", "(", "invertible_1x1_conv", ",", "name", "=", "\"invertible\"", ",", "reverse", "=", "reverse", ",", "permutation", "=", "True", ")", ",", "\n", "coupling_layer", "]", "\n", "", "else", ":", "\n", "      ", "ops", "=", "[", "\n", "functools", ".", "partial", "(", "actnorm", ",", "name", "=", "\"actnorm\"", ",", "reverse", "=", "reverse", ")", ",", "\n", "functools", ".", "partial", "(", "invertible_1x1_conv", ",", "name", "=", "\"invertible\"", ",", "reverse", "=", "reverse", ")", ",", "\n", "coupling_layer", "]", "\n", "\n", "", "if", "reverse", ":", "\n", "      ", "ops", "=", "ops", "[", ":", ":", "-", "1", "]", "\n", "\n", "", "objective", "=", "0.0", "\n", "for", "op", "in", "ops", ":", "\n", "      ", "x", ",", "curr_obj", "=", "op", "(", "x", "=", "x", ")", "\n", "objective", "+=", "curr_obj", "\n", "", "return", "x", ",", "objective", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.revnet": [[603, 625], ["tensorflow.compat.v1.variable_scope", "numpy.arange", "glow_ops_1x1.revnet_step"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.revnet_step"], ["", "", "def", "revnet", "(", "name", ",", "x", ",", "hparams", ",", "reverse", "=", "True", ")", ":", "\n", "  ", "\"\"\"'hparams.depth' steps of generative flow.\n  Args:\n    name: variable scope for the revnet block.\n    x: 4-D Tensor, shape=(NHWC).\n    hparams: HParams.\n    reverse: bool, forward or backward pass.\n  Returns:\n    x: 4-D Tensor, shape=(NHWC).\n    objective: float.\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "steps", "=", "np", ".", "arange", "(", "hparams", ".", "depth", ")", "\n", "if", "reverse", ":", "\n", "      ", "steps", "=", "steps", "[", ":", ":", "-", "1", "]", "\n", "\n", "", "objective", "=", "0.0", "\n", "for", "step", "in", "steps", ":", "\n", "      ", "x", ",", "curr_obj", "=", "revnet_step", "(", "\n", "\"revnet_step_%d\"", "%", "step", ",", "x", ",", "hparams", ",", "reverse", "=", "reverse", ")", "\n", "objective", "+=", "curr_obj", "\n", "", "return", "x", ",", "objective", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.compute_prior": [[628, 658], ["tensorflow.compat.v1.variable_scope", "glow_ops_1x1.get_shape_list", "tensorflow.zeros", "tensorflow.distributions.Normal", "tensorflow.exp"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list"], ["", "", "@", "add_arg_scope", "\n", "def", "compute_prior", "(", "name", ",", "z", ",", "latent", ",", "hparams", ",", "condition", "=", "False", ",", "state", "=", "None", ",", "\n", "temperature", "=", "1.0", ")", ":", "\n", "  ", "\"\"\"Distribution on z_t conditioned on z_{t-1} and latent.\n  Args:\n    name: variable scope.\n    z: 4-D Tensor.\n    latent: optional,\n            if hparams.latent_dist_encoder == \"pointwise\", this is a list\n            of 4-D Tensors of length hparams.num_cond_latents.\n            else, this is just a 4-D Tensor\n            The first-three dimensions of the latent should be the same as z.\n    hparams: next_frame_glow_hparams.\n    condition: Whether or not to condition the distribution on latent.\n    state: tf.nn.rnn_cell.LSTMStateTuple.\n           the current state of a LSTM used to model the distribution. Used\n           only if hparams.latent_dist_encoder = \"conv_lstm\".\n    temperature: float, temperature with which to sample from the Gaussian.\n  Returns:\n    prior_dist: instance of tfp.distributions.Normal\n    state: Returns updated state.\n  Raises:\n    ValueError: If hparams.latent_dist_encoder is \"pointwise\" and if the shape\n                of latent is different from z.\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "z_shape", "=", "get_shape_list", "(", "z", ")", "\n", "h", "=", "tf", ".", "zeros", "(", "z_shape", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "prior_dist", "=", "tf", ".", "distributions", ".", "Normal", "(", "h", ",", "tf", ".", "exp", "(", "h", ")", ")", "\n", "return", "prior_dist", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split": [[661, 720], ["tensorflow.compat.v1.variable_scope", "tensorflow.split", "glow_ops_1x1.compute_prior", "tensorflow.reduce_sum", "glow_ops_1x1.get_eps", "glow_ops_1x1.compute_prior", "prior_dist.log_prob", "glow_ops_1x1.set_eps", "tensorflow.concat", "prior_dist.sample", "tensorflow.random_normal", "glow_ops_1x1.get_shape_list"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.compute_prior", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_eps", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.compute_prior", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.set_eps", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list"], ["", "", "@", "add_arg_scope", "\n", "def", "split", "(", "name", ",", "x", ",", "reverse", "=", "False", ",", "eps", "=", "None", ",", "eps_std", "=", "None", ",", "cond_latents", "=", "None", ",", "\n", "hparams", "=", "None", ",", "state", "=", "None", ",", "condition", "=", "False", ",", "temperature", "=", "1.0", ")", ":", "\n", "  ", "\"\"\"Splits / concatenates x into x1 and x2 across number of channels.\n  For the forward pass, x2 is assumed be gaussian,\n  i.e P(x2 | x1) ~ N(mu, sigma) where mu and sigma are the outputs of\n  a network conditioned on x1 and optionally on cond_latents.\n  For the reverse pass, x2 is determined from mu(x1) and sigma(x1).\n  This is deterministic/stochastic depending on whether eps is provided.\n  Args:\n    name: variable scope.\n    x: 4-D Tensor, shape (NHWC).\n    reverse: Forward or reverse pass.\n    eps: If eps is provided, x2 is set to be mu(x1) + eps * sigma(x1).\n    eps_std: Sample x2 with the provided eps_std.\n    cond_latents: optionally condition x2 on cond_latents.\n    hparams: next_frame_glow hparams.\n    state: tf.nn.rnn_cell.LSTMStateTuple.. Current state of the LSTM over z_2.\n           Used only when hparams.latent_dist_encoder == \"conv_lstm\"\n    condition: bool, Whether or not to condition the distribution on\n               cond_latents.\n    temperature: Temperature with which to sample from the gaussian.\n  Returns:\n    If reverse:\n      x: 4-D Tensor, concats input and x2 across channels.\n      x2: 4-D Tensor, a sample from N(mu(x1), sigma(x1))\n    Else:\n      x1: 4-D Tensor, Output of the split operation.\n      logpb: log-probability of x2 belonging to mu(x1), sigma(x1)\n      eps: 4-D Tensor, (x2 - mu(x1)) / sigma(x1)\n      x2: 4-D Tensor, Latent representation at the current level.\n    state: Current LSTM state.\n           4-D Tensor, only if hparams.latent_dist_encoder is set to conv_lstm.\n  Raises:\n    ValueError: If latent is provided and shape is not equal to NHW(C/2)\n                where (NHWC) is the size of x.\n  \"\"\"", "\n", "# TODO(mechcoder) Change the return type to be a dict.", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "if", "not", "reverse", ":", "\n", "      ", "x1", ",", "x2", "=", "tf", ".", "split", "(", "x", ",", "num_or_size_splits", "=", "2", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# objective: P(x2|x1) ~N(x2 ; NN(x1))", "\n", "prior_dist", ",", "state", "=", "compute_prior", "(", "\n", "\"prior_on_z2\"", ",", "x1", ",", "cond_latents", ",", "hparams", ",", "condition", ",", "state", "=", "state", ")", "\n", "logpb", "=", "tf", ".", "reduce_sum", "(", "prior_dist", ".", "log_prob", "(", "x2", ")", ",", "axis", "=", "[", "1", ",", "2", ",", "3", "]", ")", "\n", "eps", "=", "get_eps", "(", "prior_dist", ",", "x2", ")", "\n", "return", "x1", ",", "logpb", ",", "eps", ",", "x2", ",", "state", "\n", "", "else", ":", "\n", "      ", "prior_dist", ",", "state", "=", "compute_prior", "(", "\n", "\"prior_on_z2\"", ",", "x", ",", "cond_latents", ",", "hparams", ",", "condition", ",", "state", "=", "state", ",", "\n", "temperature", "=", "temperature", ")", "\n", "if", "eps", "is", "not", "None", ":", "\n", "        ", "x2", "=", "set_eps", "(", "prior_dist", ",", "eps", ")", "\n", "", "elif", "eps_std", "is", "not", "None", ":", "\n", "        ", "x2", "=", "eps_std", "*", "tf", ".", "random_normal", "(", "get_shape_list", "(", "x", ")", ")", "\n", "", "else", ":", "\n", "        ", "x2", "=", "prior_dist", ".", "sample", "(", ")", "\n", "", "return", "tf", ".", "concat", "(", "[", "x", ",", "x2", "]", ",", "3", ")", ",", "x2", ",", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.squeeze": [[722, 756], ["tensorflow.compat.v1.variable_scope", "glow_ops_1x1.get_shape_list", "int", "int", "int", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reshape", "int", "int", "int", "int"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list"], ["", "", "", "@", "add_arg_scope", "\n", "def", "squeeze", "(", "name", ",", "x", ",", "factor", "=", "2", ",", "reverse", "=", "True", ")", ":", "\n", "  ", "\"\"\"Block-wise spatial squeezing of x to increase the number of channels.\n  Args:\n    name: Used for variable scoping.\n    x: 4-D Tensor of shape (batch_size X H X W X C)\n    factor: Factor by which the spatial dimensions should be squeezed.\n    reverse: Squueze or unsqueeze operation.\n  Returns:\n    x: 4-D Tensor of shape (batch_size X (H//factor) X (W//factor) X\n       (cXfactor^2). If reverse is True, then it is factor = (1 / factor)\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "shape", "=", "get_shape_list", "(", "x", ")", "\n", "if", "factor", "==", "1", ":", "\n", "      ", "return", "x", "\n", "", "height", "=", "int", "(", "shape", "[", "1", "]", ")", "\n", "width", "=", "int", "(", "shape", "[", "2", "]", ")", "\n", "n_channels", "=", "int", "(", "shape", "[", "3", "]", ")", "\n", "\n", "if", "not", "reverse", ":", "\n", "      ", "assert", "height", "%", "factor", "==", "0", "and", "width", "%", "factor", "==", "0", "\n", "x", "=", "tf", ".", "reshape", "(", "x", ",", "[", "-", "1", ",", "height", "//", "factor", ",", "factor", ",", "\n", "width", "//", "factor", ",", "factor", ",", "n_channels", "]", ")", "\n", "x", "=", "tf", ".", "transpose", "(", "x", ",", "[", "0", ",", "1", ",", "3", ",", "5", ",", "2", ",", "4", "]", ")", "\n", "x", "=", "tf", ".", "reshape", "(", "x", ",", "[", "-", "1", ",", "height", "//", "factor", ",", "width", "//", "\n", "factor", ",", "n_channels", "*", "factor", "*", "factor", "]", ")", "\n", "", "else", ":", "\n", "      ", "x", "=", "tf", ".", "reshape", "(", "\n", "x", ",", "(", "-", "1", ",", "height", ",", "width", ",", "int", "(", "n_channels", "/", "factor", "**", "2", ")", ",", "factor", ",", "factor", ")", ")", "\n", "x", "=", "tf", ".", "transpose", "(", "x", ",", "[", "0", ",", "1", ",", "4", ",", "2", ",", "5", ",", "3", "]", ")", "\n", "x", "=", "tf", ".", "reshape", "(", "x", ",", "(", "-", "1", ",", "int", "(", "height", "*", "factor", ")", ",", "\n", "int", "(", "width", "*", "factor", ")", ",", "int", "(", "n_channels", "/", "factor", "**", "2", ")", ")", ")", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_cond_latents_at_level": [[758, 765], ["None"], "function", ["None"], ["", "", "def", "get_cond_latents_at_level", "(", "cond_latents", ",", "level", ",", "hparams", ")", ":", "\n", "  ", "\"\"\"Returns a single or list of conditional latents at level 'level'.\"\"\"", "\n", "if", "cond_latents", ":", "\n", "    ", "if", "hparams", ".", "latent_dist_encoder", "in", "[", "\"conv_net\"", ",", "\"conv3d_net\"", "]", ":", "\n", "      ", "return", "[", "cond_latent", "[", "level", "]", "for", "cond_latent", "in", "cond_latents", "]", "\n", "", "elif", "hparams", ".", "latent_dist_encoder", "in", "[", "\"pointwise\"", ",", "\"conv_lstm\"", "]", ":", "\n", "      ", "return", "cond_latents", "[", "level", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.check_cond_latents": [[767, 783], ["isinstance", "int", "len", "ValueError", "len", "ValueError", "len", "len"], "function", ["None"], ["", "", "", "def", "check_cond_latents", "(", "cond_latents", ",", "hparams", ")", ":", "\n", "  ", "\"\"\"Shape checking for cond_latents.\"\"\"", "\n", "if", "cond_latents", "is", "None", ":", "\n", "    ", "return", "\n", "", "if", "not", "isinstance", "(", "cond_latents", "[", "0", "]", ",", "list", ")", ":", "\n", "    ", "cond_latents", "=", "[", "cond_latents", "]", "\n", "", "exp_num_latents", "=", "hparams", ".", "num_cond_latents", "\n", "if", "hparams", ".", "latent_dist_encoder", "==", "\"conv_net\"", ":", "\n", "    ", "exp_num_latents", "+=", "int", "(", "hparams", ".", "cond_first_frame", ")", "\n", "", "if", "len", "(", "cond_latents", ")", "!=", "exp_num_latents", ":", "\n", "    ", "raise", "ValueError", "(", "\"Expected number of cond_latents: %d, got %d\"", "%", "\n", "(", "exp_num_latents", ",", "len", "(", "cond_latents", ")", ")", ")", "\n", "", "for", "cond_latent", "in", "cond_latents", ":", "\n", "    ", "if", "len", "(", "cond_latent", ")", "!=", "hparams", ".", "n_levels", "-", "1", ":", "\n", "      ", "raise", "ValueError", "(", "\"Expected level_latents to be %d, got %d\"", "%", "\n", "(", "hparams", ".", "n_levels", "-", "1", ",", "len", "(", "cond_latent", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.encoder_decoder": [[785, 877], ["tensorflow.compat.v1.variable_scope", "glow_ops_1x1.check_cond_latents", "ValueError", "ValueError", "range", "reversed", "len", "len", "glow_ops_1x1.revnet", "range", "glow_ops_1x1.revnet", "glow_ops_1x1.get_cond_latents_at_level", "glow_ops_1x1.split", "all_eps.append", "all_latents.append", "new_states.append", "glow_ops_1x1.get_cond_latents_at_level", "glow_ops_1x1.split", "new_states.append", "all_latents.append", "len", "len"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.check_cond_latents", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.revnet", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.revnet", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_cond_latents_at_level", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_cond_latents_at_level", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split"], ["", "", "", "@", "add_arg_scope", "\n", "def", "encoder_decoder", "(", "name", ",", "x", ",", "hparams", ",", "eps", "=", "None", ",", "reverse", "=", "False", ",", "\n", "cond_latents", "=", "None", ",", "condition", "=", "False", ",", "states", "=", "None", ",", "\n", "temperature", "=", "1.0", ")", ":", "\n", "  ", "\"\"\"Glow encoder-decoder. n_levels of (Squeeze + Flow + Split.) operations.\n  Args:\n    name: variable scope.\n    x: 4-D Tensor, shape=(NHWC).\n    hparams: HParams.\n    eps: Stores (glow(x) - mu) / sigma during the forward pass.\n         Used only to test if the network is reversible.\n    reverse: Forward or reverse pass.\n    cond_latents: list of lists of tensors.\n                  outer length equals hparams.num_cond_latents\n                  innter length equals hparams.num_levels - 1.\n    condition: If set to True, condition the encoder/decoder on cond_latents.\n    states: LSTM states, used only if hparams.latent_dist_encoder is set\n            to \"conv_lstm.\n    temperature: Temperature set during sampling.\n  Returns:\n    x: If reverse, decoded image, else the encoded glow latent representation.\n    objective: log-likelihood.\n    eps: list of tensors, shape=(num_levels-1).\n         Stores (glow(x) - mu_level(x)) / sigma_level(x)) for each level.\n    all_latents: list of tensors, shape=(num_levels-1).\n                 Latent representations for each level.\n    new_states: list of tensors, shape=(num_levels-1).\n                useful only if hparams.latent_dist_encoder=\"conv_lstm\", returns\n                the current state of each level.\n  \"\"\"", "\n", "# TODO(mechcoder) Change return_type to a dict to be backward compatible.", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "\n", "    ", "if", "states", "and", "len", "(", "states", ")", "!=", "hparams", ".", "n_levels", "-", "1", ":", "\n", "      ", "raise", "ValueError", "(", "\"Expected length of states to be %d, got %d\"", "%", "\n", "(", "hparams", ".", "n_levels", "-", "1", ",", "len", "(", "states", ")", ")", ")", "\n", "", "if", "states", "is", "None", ":", "\n", "      ", "states", "=", "[", "None", "]", "*", "(", "hparams", ".", "n_levels", "-", "1", ")", "\n", "", "if", "eps", "and", "len", "(", "eps", ")", "!=", "hparams", ".", "n_levels", "-", "1", ":", "\n", "      ", "raise", "ValueError", "(", "\"Expected length of eps to be %d, got %d\"", "%", "\n", "(", "hparams", ".", "n_levels", "-", "1", ",", "len", "(", "eps", ")", ")", ")", "\n", "", "if", "eps", "is", "None", ":", "\n", "      ", "eps", "=", "[", "None", "]", "*", "(", "hparams", ".", "n_levels", "-", "1", ")", "\n", "", "check_cond_latents", "(", "cond_latents", ",", "hparams", ")", "\n", "\n", "objective", "=", "0.0", "\n", "all_eps", "=", "[", "]", "\n", "all_latents", "=", "[", "]", "\n", "new_states", "=", "[", "]", "\n", "\n", "if", "not", "reverse", ":", "\n", "# Squeeze + Flow + Split", "\n", "      ", "for", "level", "in", "range", "(", "hparams", ".", "n_levels", ")", ":", "\n", "# x = squeeze(\"squeeze_%d\" % level, x, factor=2, reverse=False)", "\n", "\n", "        ", "x", ",", "obj", "=", "revnet", "(", "\"revnet_%d\"", "%", "level", ",", "x", ",", "hparams", ",", "reverse", "=", "False", ")", "\n", "objective", "+=", "obj", "\n", "\n", "if", "level", "<", "hparams", ".", "n_levels", "-", "1", ":", "\n", "          ", "curr_cond_latents", "=", "get_cond_latents_at_level", "(", "\n", "cond_latents", ",", "level", ",", "hparams", ")", "\n", "x", ",", "obj", ",", "eps", ",", "z", ",", "state", "=", "split", "(", "\"split_%d\"", "%", "level", ",", "x", ",", "reverse", "=", "False", ",", "\n", "cond_latents", "=", "curr_cond_latents", ",", "\n", "condition", "=", "condition", ",", "\n", "hparams", "=", "hparams", ",", "state", "=", "states", "[", "level", "]", ")", "\n", "objective", "+=", "obj", "\n", "all_eps", ".", "append", "(", "eps", ")", "\n", "all_latents", ".", "append", "(", "z", ")", "\n", "new_states", ".", "append", "(", "state", ")", "\n", "\n", "", "", "return", "x", ",", "objective", ",", "all_eps", ",", "all_latents", ",", "new_states", "\n", "\n", "", "else", ":", "\n", "      ", "for", "level", "in", "reversed", "(", "range", "(", "hparams", ".", "n_levels", ")", ")", ":", "\n", "        ", "if", "level", "<", "hparams", ".", "n_levels", "-", "1", ":", "\n", "\n", "          ", "curr_cond_latents", "=", "get_cond_latents_at_level", "(", "\n", "cond_latents", ",", "level", ",", "hparams", ")", "\n", "\n", "x", ",", "latent", ",", "state", "=", "split", "(", "\"split_%d\"", "%", "level", ",", "x", ",", "eps", "=", "eps", "[", "level", "]", ",", "\n", "reverse", "=", "True", ",", "cond_latents", "=", "curr_cond_latents", ",", "\n", "condition", "=", "condition", ",", "hparams", "=", "hparams", ",", "\n", "state", "=", "states", "[", "level", "]", ",", "\n", "temperature", "=", "temperature", ")", "\n", "new_states", ".", "append", "(", "state", ")", "\n", "all_latents", ".", "append", "(", "latent", ")", "\n", "\n", "", "x", ",", "obj", "=", "revnet", "(", "\n", "\"revnet_%d\"", "%", "level", ",", "x", ",", "hparams", "=", "hparams", ",", "reverse", "=", "True", ")", "\n", "objective", "+=", "obj", "\n", "# x = squeeze(\"squeeze_%d\" % level, x, reverse=True)", "\n", "", "return", "x", ",", "objective", ",", "all_latents", "[", ":", ":", "-", "1", "]", ",", "new_states", "[", ":", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.top_prior": [[882, 904], ["tensorflow.compat.v1.variable_scope", "tensorflow.zeros", "tensorflow.distributions.Normal", "tensorflow.exp"], "function", ["None"], ["", "", "", "@", "add_arg_scope", "\n", "def", "top_prior", "(", "name", ",", "z_shape", ",", "learn_prior", "=", "\"normal\"", ",", "temperature", "=", "1.0", ")", ":", "\n", "  ", "\"\"\"Unconditional prior distribution.\n  Args:\n    name: variable scope\n    z_shape: Shape of the mean / scale of the prior distribution.\n    learn_prior: Possible options are \"normal\" and \"single_conv\".\n                 If set to \"single_conv\", the gaussian is parametrized by a\n                 single convolutional layer whose input are an array of zeros\n                 and initialized such that the mean and std are zero and one.\n                 If set to \"normal\", the prior is just a Gaussian with zero\n                 mean and unit variance.\n    temperature: Temperature with which to sample from the Gaussian.\n  Returns:\n    objective: 1-D Tensor shape=(batch_size,) summed across spatial components.\n  Raises:\n    ValueError: If learn_prior not in \"normal\" or \"single_conv\"\n  \"\"\"", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "variable_scope", "(", "name", ",", "reuse", "=", "tf", ".", "compat", ".", "v1", ".", "AUTO_REUSE", ")", ":", "\n", "    ", "h", "=", "tf", ".", "zeros", "(", "z_shape", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "prior_dist", "=", "tf", ".", "distributions", ".", "Normal", "(", "h", ",", "tf", ".", "exp", "(", "h", ")", ")", "\n", "return", "prior_dist", "", "", "", ""]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_init_hook.GlowInitHook.after_create_session": [[12, 22], ["session.run", "tensorflow.compat.v1.train.get_or_create_global_step", "tensorflow.get_collection", "print", "session.run"], "methods", ["None"], ["def", "after_create_session", "(", "self", ",", "session", ",", "coord", ")", ":", "\n", "    ", "del", "coord", "\n", "global_step", "=", "session", ".", "run", "(", "tf", ".", "compat", ".", "v1", ".", "train", ".", "get_or_create_global_step", "(", ")", ")", "\n", "if", "global_step", "==", "0", ":", "\n", "      ", "ddi", "=", "tf", ".", "get_collection", "(", "\"glow_init_op\"", ")", "\n", "# In-case of a multi-GPU system, this just runs the first op in the", "\n", "# collection.", "\n", "print", "(", "ddi", ")", "\n", "if", "ddi", ":", "\n", "        ", "session", ".", "run", "(", "ddi", "[", "0", "]", ")", "\n", "#input()", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.AttrDict.__init__": [[15, 18], ["dict.__init__"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__"], ["  ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "      ", "super", "(", "AttrDict", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "self", ".", "__dict__", "=", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.__init__": [[21, 23], ["None"], "methods", ["None"], ["  ", "def", "__init__", "(", "self", ",", "hparams", ")", ":", "\n", "    ", "self", ".", "hparams", "=", "hparams", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.is_predicting": [[24, 27], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "is_predicting", "(", "self", ")", ":", "\n", "    ", "return", "not", "self", ".", "is_training", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.train_hooks": [[28, 32], ["glow_init_hook.GlowInitHook"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "train_hooks", "(", ")", ":", "\n", "#del hook_context", "\n", "    ", "return", "[", "glow_init_hook", ".", "GlowInitHook", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.top_prior": [[33, 41], ["flow.top_prior", "flow.top_prior"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.top_prior", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.top_prior"], ["", "def", "top_prior", "(", "self", ")", ":", "\n", "    ", "\"\"\"Objective based on the prior over latent z.\n\n    Returns:\n      dist: instance of tfp.distributions.Normal, prior distribution.\n    \"\"\"", "\n", "return", "glow_ops", ".", "top_prior", "(", "\n", "\"top_prior\"", ",", "self", ".", "z_top_shape", ",", "learn_prior", "=", "self", ".", "hparams", ".", "top_prior", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.body": [[42, 51], ["glow_1x1.Glow.objective_tower", "glow_1x1.Glow.objective_tower", "tensorflow.Print", "tensorflow.compat.v1.add_to_collection"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.objective_tower", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.objective_tower"], ["", "def", "body", "(", "self", ",", "features", ",", "is_training", ")", ":", "\n", "    ", "if", "is_training", ":", "\n", "      ", "init_features", "=", "features", "\n", "init_op", "=", "self", ".", "objective_tower", "(", "init_features", ",", "init", "=", "True", ")", "\n", "init_op", "=", "tf", ".", "Print", "(", "\n", "init_op", ",", "[", "init_op", "]", ",", "message", "=", "\"Triggering data-dependent init.\"", ",", "\n", "first_n", "=", "20", ")", "\n", "tf", ".", "compat", ".", "v1", ".", "add_to_collection", "(", "\"glow_init_op\"", ",", "init_op", ")", "\n", "", "return", "self", ".", "objective_tower", "(", "features", ",", "init", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.objective_tower": [[52, 86], ["flow.glow_ops_1x1.get_shape_list", "flow.glow_ops_1x1.get_shape_list", "tensorflow.concat", "tensorflow.contrib.framework.python.ops.arg_scope", "encoder", "flow.glow_ops_1x1.get_shape_list", "flow.glow_ops_1x1.get_shape_list", "glow_1x1.Glow.top_prior", "tensorflow.reduce_sum", "glow_1x1.Glow.log_prob", "numpy.log"], "methods", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.get_shape_list", "home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_1x1.Glow.top_prior"], ["", "def", "objective_tower", "(", "self", ",", "features", ",", "init", "=", "True", ")", ":", "\n", "    ", "\"\"\"Objective in terms of bits-per-pixel. \n    \"\"\"", "\n", "#features = tf.expand_dims(features, [1, 2])", "\n", "features", "=", "features", "[", ":", ",", "tf", ".", "newaxis", ",", "tf", ".", "newaxis", ",", ":", "]", "\n", "x", "=", "features", "\n", "\n", "objective", "=", "0", "\n", "\n", "# The arg_scope call ensures that the actnorm parameters are set such that", "\n", "# the per-channel output activations have zero mean and unit variance", "\n", "# ONLY during the first step. After that the parameters are learned", "\n", "# through optimisation.", "\n", "ops", "=", "[", "glow_ops", ".", "get_variable_ddi", ",", "glow_ops", ".", "actnorm", ",", "glow_ops", ".", "get_dropout", "]", "\n", "with", "arg_scope", "(", "ops", ",", "init", "=", "init", ")", ":", "\n", "      ", "encoder", "=", "glow_ops", ".", "encoder_decoder", "\n", "\n", "self", ".", "z", ",", "encoder_objective", ",", "self", ".", "eps", ",", "_", ",", "_", "=", "encoder", "(", "\n", "\"flow\"", ",", "x", ",", "self", ".", "hparams", ",", "eps", "=", "None", ",", "reverse", "=", "False", ")", "\n", "objective", "+=", "encoder_objective", "\n", "\n", "self", ".", "z_top_shape", "=", "get_shape_list", "(", "self", ".", "z", ")", "\n", "prior_dist", "=", "self", ".", "top_prior", "(", ")", "\n", "prior_objective", "=", "tf", ".", "reduce_sum", "(", "\n", "prior_dist", ".", "log_prob", "(", "self", ".", "z", ")", ",", "axis", "=", "[", "1", ",", "2", ",", "3", "]", ")", "\n", "#self.z_sample = prior_dist.sample()", "\n", "objective", "+=", "prior_objective", "\n", "\n", "# bits per pixel", "\n", "", "_", ",", "h", ",", "w", ",", "c", "=", "get_shape_list", "(", "x", ")", "\n", "objective", "=", "-", "objective", "/", "(", "np", ".", "log", "(", "2", ")", "*", "h", "*", "w", "*", "c", ")", "\n", "\n", "self", ".", "z", "=", "tf", ".", "concat", "(", "self", ".", "eps", "+", "[", "self", ".", "z", "]", ",", "axis", "=", "-", "1", ")", "\n", "return", "objective", "", "", "", ""]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.scripts.eval_stsb.get_pred": [[9, 13], ["open", "float", "f.readlines"], "function", ["None"], ["def", "get_pred", "(", "fpath", ")", ":", "\n", "    ", "with", "open", "(", "fpath", ")", "as", "f", ":", "\n", "        ", "x", "=", "[", "float", "(", "_", ")", "for", "_", "in", "f", ".", "readlines", "(", ")", "]", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.scripts.eval_stsb.get_gt": [[14, 18], ["open", "numpy.asarray", "float", "_.split", "f.readlines", "int"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.flow.glow_ops_1x1.split"], ["", "def", "get_gt", "(", "fpath", ",", "col", ",", "header", "=", "False", ")", ":", "\n", "    ", "with", "open", "(", "fpath", ")", "as", "f", ":", "\n", "        ", "y", "=", "np", ".", "asarray", "(", "[", "float", "(", "_", ".", "split", "(", "'\\t'", ")", "[", "col", "]", ")", "for", "_", "in", "f", ".", "readlines", "(", ")", "[", "int", "(", "header", ")", ":", "]", "]", ")", "\n", "", "return", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.scripts.eval_stsb.get_correlation": [[19, 22], ["print", "print", "eval_stsb.pearson_r", "scipy.stats.spearmanr", "scipy.stats.spearmanr"], "function", ["home.repos.pwc.inspect_result.bohanli_BERT-flow.scripts.eval_stsb.pearson_r"], ["", "def", "get_correlation", "(", "x", ",", "y", ")", ":", "\n", "    ", "print", "(", "\"Pearson: %f\"", "%", "pearson_r", "(", "x", ",", "y", ")", ",", "end", "=", "\", \"", ")", "\n", "print", "(", "\"Spearman: %f\"", "%", "scipy", ".", "stats", ".", "spearmanr", "(", "x", ",", "y", ")", ".", "correlation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.scripts.eval_stsb.get_auc": [[23, 27], ["metrics.roc_curve", "print", "metrics.auc"], "function", ["None"], ["", "def", "get_auc", "(", "pred", ",", "y", ")", ":", "\n", "    ", "from", "sklearn", "import", "metrics", "\n", "fpr", ",", "tpr", ",", "thresholds", "=", "metrics", ".", "roc_curve", "(", "y", ",", "pred", ")", "\n", "print", "(", "\"AUC: %f\"", "%", "metrics", ".", "auc", "(", "fpr", ",", "tpr", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.bohanli_BERT-flow.scripts.eval_stsb.pearson_r": [[28, 32], ["numpy.corrcoef"], "function", ["None"], ["", "def", "pearson_r", "(", "x", ",", "y", ")", ":", "\n", "    ", "\"\"\"Compute Pearson correlation coefficient between two arrays.\"\"\"", "\n", "corr_mat", "=", "np", ".", "corrcoef", "(", "x", ",", "y", ")", "\n", "return", "corr_mat", "[", "0", ",", "1", "]", "\n", "\n"]]}