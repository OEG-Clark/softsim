{"home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__": [[34, 47], ["subfiles", "isfile", "load_pickle", "collections.OrderedDict", "collections.OrderedDict", "join", "join", "join"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "self", ".", "folder_with_cropped_data", "=", "folder_with_cropped_data", "\n", "self", ".", "preprocessed_output_folder", "=", "preprocessed_output_folder", "\n", "self", ".", "list_of_cropped_npz_files", "=", "subfiles", "(", "self", ".", "folder_with_cropped_data", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "\n", "assert", "isfile", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset_properties.pkl\"", ")", ")", ",", "\"folder_with_cropped_data must contain dataset_properties.pkl\"", "\n", "self", ".", "dataset_properties", "=", "load_pickle", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset_properties.pkl\"", ")", ")", "\n", "\n", "self", ".", "plans_per_stage", "=", "OrderedDict", "(", ")", "\n", "self", ".", "plans", "=", "OrderedDict", "(", ")", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "default_plans_identifier", "+", "\"_plans_3D.pkl\"", ")", "\n", "self", ".", "data_identifier", "=", "'nnUNet'", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.get_target_spacing": [[48, 62], ["numpy.percentile", "numpy.vstack"], "methods", ["None"], ["", "def", "get_target_spacing", "(", "self", ")", ":", "\n", "        ", "spacings", "=", "self", ".", "dataset_properties", "[", "'all_spacings'", "]", "\n", "\n", "# target = np.median(np.vstack(spacings), 0)", "\n", "# if target spacing is very anisotropic we may want to not downsample the axis with the worst spacing", "\n", "# uncomment after mystery task submission", "\n", "\"\"\"worst_spacing_axis = np.argmax(target)\n        if max(target) > (2.5 * min(target)):\n            spacings_of_that_axis = np.vstack(spacings)[:, worst_spacing_axis]\n            target_spacing_of_that_axis = np.percentile(spacings_of_that_axis, 5)\n            target[worst_spacing_axis] = target_spacing_of_that_axis\"\"\"", "\n", "\n", "target", "=", "np", ".", "percentile", "(", "np", ".", "vstack", "(", "spacings", ")", ",", "TARGET_SPACING_PERCENTILE", ",", "0", ")", "\n", "return", "target", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_my_plans": [[63, 66], ["open", "pickle.dump"], "methods", ["None"], ["", "def", "save_my_plans", "(", "self", ")", ":", "\n", "        ", "with", "open", "(", "self", ".", "plans_fname", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "plans", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.load_my_plans": [[67, 73], ["open", "pickle.load"], "methods", ["None"], ["", "", "def", "load_my_plans", "(", "self", ")", ":", "\n", "        ", "with", "open", "(", "self", ".", "plans_fname", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "self", ".", "plans", "=", "pickle", ".", "load", "(", "f", ")", "\n", "\n", "", "self", ".", "plans_per_stage", "=", "self", ".", "plans", "[", "'plans_per_stage'", "]", "\n", "self", ".", "dataset_properties", "=", "self", ".", "plans", "[", "'dataset_properties'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_postprocessing": [[74, 122], ["print", "list", "collections.OrderedDict", "print", "collections.OrderedDict", "collections.OrderedDict", "print", "print", "set", "all", "props_per_patient.keys", "props_per_patient.keys", "[].keys", "all_num_voxels.append", "len", "len", "min", "min", "props_per_patient.keys", "tuple", "numpy.percentile"], "methods", ["None"], ["", "def", "determine_postprocessing", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Spoiler: This is unused, postprocessing was removed. Ignore it.\n        :return:\n        \"\"\"", "\n", "print", "(", "\"determining postprocessing...\"", ")", "\n", "\n", "props_per_patient", "=", "self", ".", "dataset_properties", "[", "'segmentation_props_per_patient'", "]", "\n", "\n", "all_region_keys", "=", "[", "i", "for", "k", "in", "props_per_patient", ".", "keys", "(", ")", "for", "i", "in", "props_per_patient", "[", "k", "]", "[", "'only_one_region'", "]", ".", "keys", "(", ")", "]", "\n", "all_region_keys", "=", "list", "(", "set", "(", "all_region_keys", ")", ")", "\n", "\n", "only_keep_largest_connected_component", "=", "OrderedDict", "(", ")", "\n", "\n", "for", "r", "in", "all_region_keys", ":", "\n", "            ", "all_results", "=", "[", "props_per_patient", "[", "k", "]", "[", "'only_one_region'", "]", "[", "r", "]", "for", "k", "in", "props_per_patient", ".", "keys", "(", ")", "]", "\n", "only_keep_largest_connected_component", "[", "tuple", "(", "r", ")", "]", "=", "all", "(", "all_results", ")", "\n", "\n", "", "print", "(", "\"Postprocessing: only_keep_largest_connected_component\"", ",", "only_keep_largest_connected_component", ")", "\n", "\n", "all_classes", "=", "self", ".", "dataset_properties", "[", "'all_classes'", "]", "\n", "classes", "=", "[", "i", "for", "i", "in", "all_classes", "if", "i", ">", "0", "]", "\n", "\n", "props_per_patient", "=", "self", ".", "dataset_properties", "[", "'segmentation_props_per_patient'", "]", "\n", "\n", "min_size_per_class", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "classes", ":", "\n", "            ", "all_num_voxels", "=", "[", "]", "\n", "for", "k", "in", "props_per_patient", ".", "keys", "(", ")", ":", "\n", "                ", "all_num_voxels", ".", "append", "(", "props_per_patient", "[", "k", "]", "[", "'volume_per_class'", "]", "[", "c", "]", ")", "\n", "", "if", "len", "(", "all_num_voxels", ")", ">", "0", ":", "\n", "                ", "min_size_per_class", "[", "c", "]", "=", "np", ".", "percentile", "(", "all_num_voxels", ",", "1", ")", "*", "MIN_SIZE_PER_CLASS_FACTOR", "\n", "", "else", ":", "\n", "                ", "min_size_per_class", "[", "c", "]", "=", "np", ".", "inf", "\n", "\n", "", "", "min_region_size_per_class", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "classes", ":", "\n", "            ", "region_sizes", "=", "[", "l", "for", "k", "in", "props_per_patient", "for", "l", "in", "props_per_patient", "[", "k", "]", "[", "'region_volume_per_class'", "]", "[", "c", "]", "]", "\n", "if", "len", "(", "region_sizes", ")", ">", "0", ":", "\n", "                ", "min_region_size_per_class", "[", "c", "]", "=", "min", "(", "region_sizes", ")", "\n", "# we don't need that line but better safe than sorry, right?", "\n", "min_region_size_per_class", "[", "c", "]", "=", "min", "(", "min_region_size_per_class", "[", "c", "]", ",", "min_size_per_class", "[", "c", "]", ")", "\n", "", "else", ":", "\n", "                ", "min_region_size_per_class", "[", "c", "]", "=", "0", "\n", "\n", "", "", "print", "(", "\"Postprocessing: min_size_per_class\"", ",", "min_size_per_class", ")", "\n", "print", "(", "\"Postprocessing: min_region_size_per_class\"", ",", "min_region_size_per_class", ")", "\n", "return", "only_keep_largest_connected_component", ",", "min_size_per_class", ",", "min_region_size_per_class", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.plan_experiment": [[123, 306], ["numpy.prod", "experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_whether_to_use_mask_for_norm", "print", "len", "experiment_planner_baseline_3DUNet.ExperimentPlanner.get_target_spacing", "numpy.median", "print", "numpy.max", "print", "numpy.min", "print", "print", "list", "experiment_planner_baseline_3DUNet.ExperimentPlanner.plans_per_stage.append", "print", "experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_normalization_scheme", "experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_postprocessing", "experiment_planner_baseline_3DUNet.ExperimentPlanner.save_my_plans", "numpy.round().astype", "numpy.round().astype.mean", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "max", "min", "list", "numpy.vstack", "numpy.vstack", "numpy.vstack", "experiment_planner_baseline_3DUNet.ExperimentPlanner.plan_experiment.get_properties_for_stage"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_whether_to_use_mask_for_norm", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.get_target_spacing", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_normalization_scheme", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_postprocessing", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_my_plans", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption"], ["", "def", "plan_experiment", "(", "self", ")", ":", "\n", "        ", "architecture_input_voxels", "=", "np", ".", "prod", "(", "Generic_UNet", ".", "DEFAULT_PATCH_SIZE_3D", ")", "\n", "\n", "def", "get_properties_for_stage", "(", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ")", ":", "\n", "            ", "\"\"\"\n            Computation of input patch size starts out with the new median shape (in voxels) of a dataset. This is\n            opposed to prior experiments where I based it on the median size in mm. The rationale behind this is that\n            for some organ of interest the acquisition method will most likely be chosen such that the field of view and\n            voxel resolution go hand in hand to show the doctor what they need to see. This assumption may be violated\n            for some modalities with anisotropy (cine MRI) but we will have t live with that. In future experiments I\n            will try to 1) base input patch size match aspect ratio of input size in mm (instead of voxels) and 2) to\n            try to enforce that we see the same 'distance' in all directions (try to maintain equal size in mm of patch)\n            :param current_spacing:\n            :param original_spacing:\n            :param original_shape:\n            :param num_cases:\n            :return:\n            \"\"\"", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "\n", "# the next line is what we had before as a default. The patch size had the same aspect ratio as the median shape of a patient. We swapped t", "\n", "# input_patch_size = new_median_shape", "\n", "\n", "# compute how many voxels are one mm", "\n", "input_patch_size", "=", "1", "/", "np", ".", "array", "(", "current_spacing", ")", "\n", "\n", "# normalize voxels per mm", "\n", "input_patch_size", "/=", "input_patch_size", ".", "mean", "(", ")", "\n", "\n", "# create an isotropic patch of size 512x512x512mm", "\n", "input_patch_size", "*=", "1", "/", "min", "(", "input_patch_size", ")", "*", "512", "# to get a starting value", "\n", "input_patch_size", "=", "np", ".", "round", "(", "input_patch_size", ")", ".", "astype", "(", "int", ")", "\n", "\n", "# clip it to the median shape of the dataset because patches larger then that make not much sense", "\n", "input_patch_size", "=", "[", "min", "(", "i", ",", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "input_patch_size", ",", "new_median_shape", ")", "]", "\n", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "input_patch_size", ",", "\n", "FEATUREMAP_MIN_EDGE_LENGTH_BOTTLENECK", ",", "\n", "Generic_UNet", ".", "MAX_NUMPOOL_3D", ",", "current_spacing", ")", "\n", "\n", "ref", "=", "Generic_UNet", ".", "use_this_for_batch_size_computation_3D", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "Generic_UNet", ".", "BASE_NUM_FEATURES_3D", ",", "\n", "Generic_UNet", ".", "MAX_NUM_FILTERS_3D", ",", "num_modalities", ",", "\n", "num_classes", ",", "\n", "pool_op_kernel_sizes", ")", "\n", "while", "here", ">", "ref", ":", "\n", "                ", "argsrt", "=", "np", ".", "argsort", "(", "new_shp", "/", "new_median_shape", ")", "[", ":", ":", "-", "1", "]", "\n", "pool_fct_per_axis", "=", "np", ".", "prod", "(", "pool_op_kernel_sizes", ",", "0", ")", "\n", "bottleneck_size_per_axis", "=", "new_shp", "/", "pool_fct_per_axis", "\n", "shape_must_be_divisible_by", "=", "[", "shape_must_be_divisible_by", "[", "i", "]", "\n", "if", "bottleneck_size_per_axis", "[", "i", "]", ">", "4", "\n", "else", "shape_must_be_divisible_by", "[", "i", "]", "/", "2", "\n", "for", "i", "in", "range", "(", "len", "(", "bottleneck_size_per_axis", ")", ")", "]", "\n", "new_shp", "[", "argsrt", "[", "0", "]", "]", "-=", "shape_must_be_divisible_by", "[", "argsrt", "[", "0", "]", "]", "\n", "\n", "# we have to recompute numpool now:", "\n", "network_num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "new_shp", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props_poolLateV2", "(", "new_shp", ",", "\n", "FEATUREMAP_MIN_EDGE_LENGTH_BOTTLENECK", ",", "\n", "Generic_UNet", ".", "MAX_NUMPOOL_3D", ",", "current_spacing", ")", "\n", "\n", "here", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "new_shp", ",", "network_num_pool_per_axis", ",", "\n", "Generic_UNet", ".", "BASE_NUM_FEATURES_3D", ",", "\n", "Generic_UNet", ".", "MAX_NUM_FILTERS_3D", ",", "num_modalities", ",", "\n", "num_classes", ",", "pool_op_kernel_sizes", ")", "\n", "print", "(", "new_shp", ")", "\n", "\n", "", "input_patch_size", "=", "new_shp", "\n", "\n", "batch_size", "=", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_3D", "# This is what wirks with 128**3", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "max", "(", "ref", "/", "here", ",", "1", ")", "*", "batch_size", ")", ")", "\n", "\n", "# check if batch size is too large", "\n", "max_batch_size", "=", "np", ".", "round", "(", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ")", ")", ".", "astype", "(", "int", ")", "\n", "max_batch_size", "=", "max", "(", "max_batch_size", ",", "dataset_min_batch_size_cap", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "#Manually modify the batch size to be 8!!!!!", "\n", "batch_size", "=", "4", "\n", "\n", "do_dummy_2D_data_aug", "=", "(", "max", "(", "input_patch_size", ")", "/", "input_patch_size", "[", "0", "]", ")", ">", "RESAMPLING_SEPARATE_Z_ANISOTROPY_THRESHOLD", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_num_pool_per_axis", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'do_dummy_2D_data_aug'", ":", "do_dummy_2D_data_aug", ",", "\n", "'pool_op_kernel_sizes'", ":", "pool_op_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "conv_kernel_sizes", ",", "\n", "}", "\n", "return", "plan", "\n", "\n", "", "use_nonzero_mask_for_normalization", "=", "self", ".", "determine_whether_to_use_mask_for_norm", "(", ")", "\n", "print", "(", "\"Are we using the nonzero maks for normalizaion?\"", ",", "use_nonzero_mask_for_normalization", ")", "\n", "spacings", "=", "self", ".", "dataset_properties", "[", "'all_spacings'", "]", "\n", "sizes", "=", "self", ".", "dataset_properties", "[", "'all_sizes'", "]", "\n", "\n", "all_classes", "=", "self", ".", "dataset_properties", "[", "'all_classes'", "]", "\n", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "\n", "target_spacing", "=", "self", ".", "get_target_spacing", "(", ")", "\n", "new_shapes", "=", "[", "np", ".", "array", "(", "i", ")", "/", "target_spacing", "*", "np", ".", "array", "(", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "spacings", ",", "sizes", ")", "]", "\n", "\n", "# we base our calculations on the median shape of the datasets", "\n", "median_shape", "=", "np", ".", "median", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the median shape of the dataset is \"", ",", "median_shape", ")", "\n", "\n", "max_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the max shape in the dataset is \"", ",", "max_shape", ")", "\n", "min_shape", "=", "np", ".", "min", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the min shape in the dataset is \"", ",", "min_shape", ")", "\n", "\n", "print", "(", "\"we don't want feature maps smaller than \"", ",", "FEATUREMAP_MIN_EDGE_LENGTH_BOTTLENECK", ",", "\" in the bottleneck\"", ")", "\n", "\n", "# how many stages will the image pyramid have?", "\n", "self", ".", "plans_per_stage", "=", "list", "(", ")", "\n", "\n", "self", ".", "plans_per_stage", ".", "append", "(", "get_properties_for_stage", "(", "target_spacing", ",", "target_spacing", ",", "median_shape", ",", "\n", "len", "(", "self", ".", "list_of_cropped_npz_files", ")", ",", "\n", "num_modalities", ",", "len", "(", "all_classes", ")", "+", "1", ")", ")", "\n", "if", "np", ".", "prod", "(", "self", ".", "plans_per_stage", "[", "-", "1", "]", "[", "'median_patient_size_in_voxels'", "]", ")", "/", "architecture_input_voxels", "<", "HOW_MUCH_OF_A_PATIENT_MUST_THE_NETWORK_SEE_AT_STAGE0", ":", "\n", "            ", "more", "=", "False", "\n", "", "else", ":", "\n", "            ", "more", "=", "True", "\n", "\n", "", "if", "more", ":", "\n", "# if we are doing more than one stage then we want the lowest stage to have exactly", "\n", "# HOW_MUCH_OF_A_PATIENT_MUST_THE_NETWORK_SEE_AT_STAGE0 (this is 4 by default so the number of voxels in the", "\n", "# median shape of the lowest stage must be 4 times as much as the network can process at once (128x128x128 by", "\n", "# default). Problem is that we are downsampling higher resolution axes before we start downsampling the", "\n", "# out-of-plane axis. We could probably/maybe do this analytically but I am lazy, so here", "\n", "# we do it the dumb way", "\n", "            ", "lowres_stage_spacing", "=", "deepcopy", "(", "target_spacing", ")", "\n", "num_voxels", "=", "np", ".", "prod", "(", "median_shape", ")", "\n", "while", "num_voxels", ">", "HOW_MUCH_OF_A_PATIENT_MUST_THE_NETWORK_SEE_AT_STAGE0", "*", "architecture_input_voxels", ":", "\n", "                ", "max_spacing", "=", "max", "(", "lowres_stage_spacing", ")", "\n", "if", "np", ".", "any", "(", "(", "max_spacing", "/", "lowres_stage_spacing", ")", ">", "2", ")", ":", "\n", "                    ", "lowres_stage_spacing", "[", "(", "max_spacing", "/", "lowres_stage_spacing", ")", ">", "2", "]", "*=", "1.01", "\n", "", "else", ":", "\n", "                    ", "lowres_stage_spacing", "*=", "1.01", "\n", "", "num_voxels", "=", "np", ".", "prod", "(", "target_spacing", "/", "lowres_stage_spacing", "*", "median_shape", ")", "\n", "\n", "", "new", "=", "get_properties_for_stage", "(", "lowres_stage_spacing", ",", "target_spacing", ",", "median_shape", ",", "\n", "len", "(", "self", ".", "list_of_cropped_npz_files", ")", ",", "\n", "num_modalities", ",", "len", "(", "all_classes", ")", "+", "1", ")", "\n", "\n", "if", "1.5", "*", "np", ".", "prod", "(", "new", "[", "'median_patient_size_in_voxels'", "]", ")", "<", "np", ".", "prod", "(", "self", ".", "plans_per_stage", "[", "0", "]", "[", "'median_patient_size_in_voxels'", "]", ")", ":", "\n", "                ", "self", ".", "plans_per_stage", ".", "append", "(", "new", ")", "\n", "\n", "", "", "self", ".", "plans_per_stage", "=", "self", ".", "plans_per_stage", "[", ":", ":", "-", "1", "]", "\n", "self", ".", "plans_per_stage", "=", "{", "i", ":", "self", ".", "plans_per_stage", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "self", ".", "plans_per_stage", ")", ")", "}", "# convert to dict", "\n", "\n", "print", "(", "self", ".", "plans_per_stage", ")", "\n", "\n", "normalization_schemes", "=", "self", ".", "determine_normalization_scheme", "(", ")", "\n", "only_keep_largest_connected_component", ",", "min_size_per_class", ",", "min_region_size_per_class", "=", "self", ".", "determine_postprocessing", "(", ")", "\n", "\n", "# these are independent of the stage", "\n", "plans", "=", "{", "'num_stages'", ":", "len", "(", "list", "(", "self", ".", "plans_per_stage", ".", "keys", "(", ")", ")", ")", ",", "'num_modalities'", ":", "num_modalities", ",", "\n", "'modalities'", ":", "modalities", ",", "'normalization_schemes'", ":", "normalization_schemes", ",", "\n", "'dataset_properties'", ":", "self", ".", "dataset_properties", ",", "'list_of_npz_files'", ":", "self", ".", "list_of_cropped_npz_files", ",", "\n", "'original_spacings'", ":", "spacings", ",", "'original_sizes'", ":", "sizes", ",", "\n", "'preprocessed_data_folder'", ":", "self", ".", "preprocessed_output_folder", ",", "'num_classes'", ":", "len", "(", "all_classes", ")", ",", "\n", "'all_classes'", ":", "all_classes", ",", "'base_num_features'", ":", "Generic_UNet", ".", "BASE_NUM_FEATURES_3D", ",", "\n", "'use_mask_for_norm'", ":", "use_nonzero_mask_for_normalization", ",", "\n", "'keep_only_largest_region'", ":", "only_keep_largest_connected_component", ",", "\n", "'min_region_size_per_class'", ":", "min_region_size_per_class", ",", "'min_size_per_class'", ":", "min_size_per_class", ",", "\n", "'data_identifier'", ":", "self", ".", "data_identifier", ",", "'plans_per_stage'", ":", "self", ".", "plans_per_stage", "}", "\n", "\n", "self", ".", "plans", "=", "plans", "\n", "self", ".", "save_my_plans", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_normalization_scheme": [[307, 318], ["collections.OrderedDict", "len", "range", "list", "modalities.keys"], "methods", ["None"], ["", "def", "determine_normalization_scheme", "(", "self", ")", ":", "\n", "        ", "schemes", "=", "OrderedDict", "(", ")", "\n", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_modalities", ")", ":", "\n", "            ", "if", "modalities", "[", "i", "]", "==", "\"CT\"", ":", "\n", "                ", "schemes", "[", "i", "]", "=", "\"CT\"", "\n", "", "else", ":", "\n", "                ", "schemes", "[", "i", "]", "=", "\"nonCT\"", "\n", "", "", "return", "schemes", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped": [[319, 322], ["open", "pickle.dump", "join"], "methods", ["None"], ["", "def", "save_properties_of_cropped", "(", "self", ",", "case_identifier", ",", "properties", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "properties", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.load_properties_of_cropped": [[323, 327], ["open", "pickle.load", "join"], "methods", ["None"], ["", "", "def", "load_properties_of_cropped", "(", "self", ",", "case_identifier", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "properties", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_whether_to_use_mask_for_norm": [[328, 361], ["len", "collections.OrderedDict", "range", "list", "print", "nnunet.preprocessing.cropping.get_case_identifier_from_npz", "print", "experiment_planner_baseline_3DUNet.ExperimentPlanner.load_properties_of_cropped", "experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped", "modalities.keys", "experiment_planner_baseline_3DUNet.ExperimentPlanner.dataset_properties[].keys", "all_size_reductions.append", "numpy.median", "print", "print"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_case_identifier_from_npz", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped"], ["", "def", "determine_whether_to_use_mask_for_norm", "(", "self", ")", ":", "\n", "# only use the nonzero mask for normalization of the cropping based on it resulted in a decrease in", "\n", "# image size (this is an indication that the data is something like brats/isles and then we want to", "\n", "# normalize in the brain region only)", "\n", "        ", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "use_nonzero_mask_for_norm", "=", "OrderedDict", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "num_modalities", ")", ":", "\n", "            ", "if", "\"CT\"", "in", "modalities", "[", "i", "]", ":", "\n", "                ", "use_nonzero_mask_for_norm", "[", "i", "]", "=", "False", "\n", "", "else", ":", "\n", "                ", "all_size_reductions", "=", "[", "]", "\n", "for", "k", "in", "self", ".", "dataset_properties", "[", "'size_reductions'", "]", ".", "keys", "(", ")", ":", "\n", "                    ", "all_size_reductions", ".", "append", "(", "self", ".", "dataset_properties", "[", "'size_reductions'", "]", "[", "k", "]", ")", "\n", "\n", "", "if", "np", ".", "median", "(", "all_size_reductions", ")", "<", "3", "/", "4.", ":", "\n", "                    ", "print", "(", "\"using nonzero mask for normalization\"", ")", "\n", "use_nonzero_mask_for_norm", "[", "i", "]", "=", "True", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\"not using nonzero mask for normalization\"", ")", "\n", "use_nonzero_mask_for_norm", "[", "i", "]", "=", "False", "\n", "\n", "# write use_nonzero_mask_for_norm into properties -> will be needed for data augmentation TODO", "\n", "", "", "", "for", "c", "in", "self", ".", "list_of_cropped_npz_files", ":", "\n", "            ", "print", "(", "'c is:'", ",", "c", ")", "\n", "case_identifier", "=", "get_case_identifier_from_npz", "(", "c", ")", "\n", "print", "(", "'case_identifier is:'", ",", "case_identifier", ")", "\n", "properties", "=", "self", ".", "load_properties_of_cropped", "(", "case_identifier", ")", "\n", "properties", "[", "'use_nonzero_mask_for_norm'", "]", "=", "use_nonzero_mask_for_norm", "\n", "self", ".", "save_properties_of_cropped", "(", "case_identifier", ",", "properties", ")", "\n", "", "use_nonzero_mask_for_normalization", "=", "use_nonzero_mask_for_norm", "\n", "return", "use_nonzero_mask_for_normalization", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.write_normalization_scheme_to_patients": [[362, 374], ["nnunet.preprocessing.cropping.get_case_identifier_from_npz", "experiment_planner_baseline_3DUNet.ExperimentPlanner.load_properties_of_cropped", "experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_case_identifier_from_npz", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_properties_of_cropped"], ["", "def", "write_normalization_scheme_to_patients", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This is used for test set preprocessing\n        :return: \n        \"\"\"", "\n", "for", "c", "in", "self", ".", "list_of_cropped_npz_files", ":", "\n", "# print('c is:',c,'!!!!!!!!!')", "\n", "            ", "case_identifier", "=", "get_case_identifier_from_npz", "(", "c", ")", "\n", "# print('case_identifier is:',case_identifier)", "\n", "properties", "=", "self", ".", "load_properties_of_cropped", "(", "case_identifier", ")", "\n", "properties", "[", "'use_nonzero_mask_for_norm'", "]", "=", "self", ".", "plans", "[", "'use_mask_for_norm'", "]", "\n", "self", ".", "save_properties_of_cropped", "(", "case_identifier", ",", "properties", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.run_preprocessing": [[375, 392], ["os.path.isdir", "shutil.copytree", "nnunet.preprocessing.preprocessing.GenericPreprocessor", "nnunet.preprocessing.preprocessing.GenericPreprocessor.run", "join", "shutil.rmtree", "join", "join", "join", "experiment_planner_baseline_3DUNet.ExperimentPlanner.plans_per_stage.values", "isinstance", "isinstance"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.PreprocessorFor2D.run"], ["", "", "def", "run_preprocessing", "(", "self", ",", "num_threads", ")", ":", "\n", "        ", "if", "os", ".", "path", ".", "isdir", "(", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"gt_segmentations\"", ")", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"gt_segmentations\"", ")", ")", "\n", "", "shutil", ".", "copytree", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"gt_segmentations\"", ")", ",", "\n", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"gt_segmentations\"", ")", ")", "\n", "normalization_schemes", "=", "self", ".", "plans", "[", "'normalization_schemes'", "]", "\n", "use_nonzero_mask_for_normalization", "=", "self", ".", "plans", "[", "'use_mask_for_norm'", "]", "\n", "intensityproperties", "=", "self", ".", "plans", "[", "'dataset_properties'", "]", "[", "'intensityproperties'", "]", "\n", "preprocessor", "=", "GenericPreprocessor", "(", "normalization_schemes", ",", "use_nonzero_mask_for_normalization", ",", "\n", "intensityproperties", ")", "\n", "target_spacings", "=", "[", "i", "[", "\"current_spacing\"", "]", "for", "i", "in", "self", ".", "plans_per_stage", ".", "values", "(", ")", "]", "\n", "if", "self", ".", "plans", "[", "'num_stages'", "]", ">", "1", "and", "not", "isinstance", "(", "num_threads", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "num_threads", "=", "(", "8", ",", "num_threads", ")", "\n", "", "elif", "self", ".", "plans", "[", "'num_stages'", "]", "==", "1", "and", "isinstance", "(", "num_threads", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "num_threads", "=", "num_threads", "[", "-", "1", "]", "\n", "", "preprocessor", ".", "run", "(", "target_spacings", ",", "self", ".", "folder_with_cropped_data", ",", "self", ".", "preprocessed_output_folder", ",", "\n", "self", ".", "plans", "[", "'data_identifier'", "]", ",", "num_threads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.split_4d_nifti": [[24, 49], ["SimpleITK.ReadImage", "sitk.ReadImage.GetDimension", "filename.split", "shutil.copy", "batchgenerators.utilities.file_and_folder_operations.join", "RuntimeError", "SimpleITK.GetArrayFromImage", "sitk.ReadImage.GetSpacing", "sitk.ReadImage.GetOrigin", "numpy.array().reshape", "tuple", "tuple", "tuple", "enumerate", "list", "list", "direction[].reshape", "range", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "sitk.GetImageFromArray.SetOrigin", "sitk.GetImageFromArray.SetDirection", "SimpleITK.WriteImage", "numpy.array", "batchgenerators.utilities.file_and_folder_operations.join", "sitk.ReadImage.GetDirection"], "function", ["None"], ["def", "split_4d_nifti", "(", "filename", ",", "output_folder", ")", ":", "\n", "    ", "img_itk", "=", "sitk", ".", "ReadImage", "(", "filename", ")", "\n", "dim", "=", "img_itk", ".", "GetDimension", "(", ")", "\n", "file_base", "=", "filename", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "if", "dim", "==", "3", ":", "\n", "        ", "shutil", ".", "copy", "(", "filename", ",", "join", "(", "output_folder", ",", "file_base", "[", ":", "-", "7", "]", "+", "\"_0000.nii.gz\"", ")", ")", "\n", "return", "\n", "", "elif", "dim", "!=", "4", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Unexpected dimensionality: %d of file %s, cannot split\"", "%", "(", "dim", ",", "filename", ")", ")", "\n", "", "else", ":", "\n", "        ", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img_itk", ")", "\n", "spacing", "=", "img_itk", ".", "GetSpacing", "(", ")", "\n", "origin", "=", "img_itk", ".", "GetOrigin", "(", ")", "\n", "direction", "=", "np", ".", "array", "(", "img_itk", ".", "GetDirection", "(", ")", ")", ".", "reshape", "(", "4", ",", "4", ")", "\n", "# now modify these to remove the fourth dimension", "\n", "spacing", "=", "tuple", "(", "list", "(", "spacing", "[", ":", "-", "1", "]", ")", ")", "\n", "origin", "=", "tuple", "(", "list", "(", "origin", "[", ":", "-", "1", "]", ")", ")", "\n", "direction", "=", "tuple", "(", "direction", "[", ":", "-", "1", ",", ":", "-", "1", "]", ".", "reshape", "(", "-", "1", ")", ")", "\n", "for", "i", ",", "t", "in", "enumerate", "(", "range", "(", "img_npy", ".", "shape", "[", "0", "]", ")", ")", ":", "\n", "            ", "img", "=", "img_npy", "[", "t", "]", "\n", "img_itk_new", "=", "sitk", ".", "GetImageFromArray", "(", "img", ")", "\n", "img_itk_new", ".", "SetSpacing", "(", "spacing", ")", "\n", "img_itk_new", ".", "SetOrigin", "(", "origin", ")", "\n", "img_itk_new", ".", "SetDirection", "(", "direction", ")", "\n", "sitk", ".", "WriteImage", "(", "img_itk_new", ",", "join", "(", "output_folder", ",", "file_base", "[", ":", "-", "7", "]", "+", "\"_%04.0d.nii.gz\"", "%", "i", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.get_pool_and_conv_props_poolLateV2": [[51, 88], ["copy.deepcopy", "max", "len", "common_utils.get_network_numpool", "max", "range", "net_conv_kernel_sizes.append", "common_utils.get_shape_must_be_divisible_by", "common_utils.pad_shape", "all", "net_num_pool_op_kernel_sizes.append", "net_conv_kernel_sizes.append", "range", "range", "zip", "range"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.get_network_numpool", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.get_shape_must_be_divisible_by", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.pad_shape"], ["", "", "", "def", "get_pool_and_conv_props_poolLateV2", "(", "patch_size", ",", "min_feature_map_size", ",", "max_numpool", ",", "spacing", ")", ":", "\n", "    ", "\"\"\"\n\n    :param spacing:\n    :param patch_size:\n    :param min_feature_map_size: min edge length of feature maps in bottleneck\n    :return:\n    \"\"\"", "\n", "initial_spacing", "=", "deepcopy", "(", "spacing", ")", "\n", "reach", "=", "max", "(", "initial_spacing", ")", "\n", "dim", "=", "len", "(", "patch_size", ")", "\n", "\n", "num_pool_per_axis", "=", "get_network_numpool", "(", "patch_size", ",", "max_numpool", ",", "min_feature_map_size", ")", "\n", "\n", "net_num_pool_op_kernel_sizes", "=", "[", "]", "\n", "net_conv_kernel_sizes", "=", "[", "]", "\n", "net_numpool", "=", "max", "(", "num_pool_per_axis", ")", "\n", "\n", "current_spacing", "=", "spacing", "\n", "for", "p", "in", "range", "(", "net_numpool", ")", ":", "\n", "        ", "reached", "=", "[", "current_spacing", "[", "i", "]", "/", "reach", ">", "0.5", "for", "i", "in", "range", "(", "dim", ")", "]", "\n", "pool", "=", "[", "2", "if", "num_pool_per_axis", "[", "i", "]", "+", "p", ">=", "net_numpool", "else", "1", "for", "i", "in", "range", "(", "dim", ")", "]", "\n", "if", "all", "(", "reached", ")", ":", "\n", "            ", "conv", "=", "[", "3", "]", "*", "dim", "\n", "", "else", ":", "\n", "            ", "conv", "=", "[", "3", "if", "not", "reached", "[", "i", "]", "else", "1", "for", "i", "in", "range", "(", "dim", ")", "]", "\n", "", "net_num_pool_op_kernel_sizes", ".", "append", "(", "pool", ")", "\n", "net_conv_kernel_sizes", ".", "append", "(", "conv", ")", "\n", "current_spacing", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "current_spacing", ",", "pool", ")", "]", "\n", "\n", "", "net_conv_kernel_sizes", ".", "append", "(", "[", "3", "]", "*", "dim", ")", "\n", "\n", "must_be_divisible_by", "=", "get_shape_must_be_divisible_by", "(", "num_pool_per_axis", ")", "\n", "patch_size", "=", "pad_shape", "(", "patch_size", ",", "must_be_divisible_by", ")", "\n", "\n", "# we need to add one more conv_kernel_size for the bottleneck. We always use 3x3(x3) conv here", "\n", "return", "num_pool_per_axis", ",", "net_num_pool_op_kernel_sizes", ",", "net_conv_kernel_sizes", ",", "patch_size", ",", "must_be_divisible_by", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.get_pool_and_conv_props": [[90, 155], ["len", "copy.deepcopy", "copy.deepcopy", "common_utils.get_shape_must_be_divisible_by", "common_utils.pad_shape", "conv_kernel_sizes.append", "list", "list", "min", "range", "pool_op_kernel_sizes.append", "conv_kernel_sizes.append", "len", "range", "len", "len", "range", "range", "range"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.get_shape_must_be_divisible_by", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.pad_shape"], ["", "def", "get_pool_and_conv_props", "(", "spacing", ",", "patch_size", ",", "min_feature_map_size", ",", "max_numpool", ")", ":", "\n", "    ", "\"\"\"\n\n    :param spacing:\n    :param patch_size:\n    :param min_feature_map_size: min edge length of feature maps in bottleneck\n    :return:\n    \"\"\"", "\n", "dim", "=", "len", "(", "spacing", ")", "\n", "\n", "current_spacing", "=", "deepcopy", "(", "list", "(", "spacing", ")", ")", "\n", "current_size", "=", "deepcopy", "(", "list", "(", "patch_size", ")", ")", "\n", "\n", "pool_op_kernel_sizes", "=", "[", "]", "\n", "conv_kernel_sizes", "=", "[", "]", "\n", "\n", "num_pool_per_axis", "=", "[", "0", "]", "*", "dim", "\n", "\n", "while", "True", ":", "\n", "# find axes that are within factor 2 of min axis spacing", "\n", "        ", "min_spacing", "=", "min", "(", "current_spacing", ")", "\n", "valid_axes_for_pool", "=", "[", "i", "for", "i", "in", "range", "(", "dim", ")", "if", "current_spacing", "[", "i", "]", "/", "min_spacing", "<", "2", "]", "\n", "axes", "=", "[", "]", "\n", "for", "a", "in", "range", "(", "dim", ")", ":", "\n", "            ", "my_spacing", "=", "current_spacing", "[", "a", "]", "\n", "partners", "=", "[", "i", "for", "i", "in", "range", "(", "dim", ")", "if", "current_spacing", "[", "i", "]", "/", "my_spacing", "<", "2", "and", "my_spacing", "/", "current_spacing", "[", "i", "]", "<", "2", "]", "\n", "if", "len", "(", "partners", ")", ">", "len", "(", "axes", ")", ":", "\n", "                ", "axes", "=", "partners", "\n", "", "", "conv_kernel_size", "=", "[", "3", "if", "i", "in", "axes", "else", "1", "for", "i", "in", "range", "(", "dim", ")", "]", "\n", "\n", "# exclude axes that we cannot pool further because of min_feature_map_size constraint", "\n", "#before = len(valid_axes_for_pool)", "\n", "valid_axes_for_pool", "=", "[", "i", "for", "i", "in", "valid_axes_for_pool", "if", "current_size", "[", "i", "]", ">=", "2", "*", "min_feature_map_size", "]", "\n", "#after = len(valid_axes_for_pool)", "\n", "#if after == 1 and before > 1:", "\n", "#    break", "\n", "\n", "valid_axes_for_pool", "=", "[", "i", "for", "i", "in", "valid_axes_for_pool", "if", "num_pool_per_axis", "[", "i", "]", "<", "max_numpool", "]", "\n", "\n", "if", "len", "(", "valid_axes_for_pool", ")", "==", "0", ":", "\n", "            ", "break", "\n", "\n", "#print(current_spacing, current_size)", "\n", "\n", "", "other_axes", "=", "[", "i", "for", "i", "in", "range", "(", "dim", ")", "if", "i", "not", "in", "valid_axes_for_pool", "]", "\n", "\n", "pool_kernel_sizes", "=", "[", "0", "]", "*", "dim", "\n", "for", "v", "in", "valid_axes_for_pool", ":", "\n", "            ", "pool_kernel_sizes", "[", "v", "]", "=", "2", "\n", "num_pool_per_axis", "[", "v", "]", "+=", "1", "\n", "current_spacing", "[", "v", "]", "*=", "2", "\n", "current_size", "[", "v", "]", "/=", "2", "\n", "", "for", "nv", "in", "other_axes", ":", "\n", "            ", "pool_kernel_sizes", "[", "nv", "]", "=", "1", "\n", "\n", "", "pool_op_kernel_sizes", ".", "append", "(", "pool_kernel_sizes", ")", "\n", "conv_kernel_sizes", ".", "append", "(", "conv_kernel_size", ")", "\n", "#print(conv_kernel_sizes)", "\n", "\n", "", "must_be_divisible_by", "=", "get_shape_must_be_divisible_by", "(", "num_pool_per_axis", ")", "\n", "patch_size", "=", "pad_shape", "(", "patch_size", ",", "must_be_divisible_by", ")", "\n", "\n", "# we need to add one more conv_kernel_size for the bottleneck. We always use 3x3(x3) conv here", "\n", "conv_kernel_sizes", ".", "append", "(", "[", "3", "]", "*", "dim", ")", "\n", "return", "num_pool_per_axis", ",", "pool_op_kernel_sizes", ",", "conv_kernel_sizes", ",", "patch_size", ",", "must_be_divisible_by", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.get_shape_must_be_divisible_by": [[157, 159], ["numpy.array"], "function", ["None"], ["", "def", "get_shape_must_be_divisible_by", "(", "net_numpool_per_axis", ")", ":", "\n", "    ", "return", "2", "**", "np", ".", "array", "(", "net_numpool_per_axis", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.pad_shape": [[161, 180], ["range", "numpy.array().astype", "isinstance", "len", "len", "len", "len", "range", "numpy.array", "len"], "function", ["None"], ["", "def", "pad_shape", "(", "shape", ",", "must_be_divisible_by", ")", ":", "\n", "    ", "\"\"\"\n    pads shape so that it is divisibly by must_be_divisible_by\n    :param shape:\n    :param must_be_divisible_by:\n    :return:\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "must_be_divisible_by", ",", "(", "tuple", ",", "list", ",", "np", ".", "ndarray", ")", ")", ":", "\n", "        ", "must_be_divisible_by", "=", "[", "must_be_divisible_by", "]", "*", "len", "(", "shape", ")", "\n", "", "else", ":", "\n", "        ", "assert", "len", "(", "must_be_divisible_by", ")", "==", "len", "(", "shape", ")", "\n", "\n", "", "new_shp", "=", "[", "shape", "[", "i", "]", "+", "must_be_divisible_by", "[", "i", "]", "-", "shape", "[", "i", "]", "%", "must_be_divisible_by", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "shape", ")", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "shape", ")", ")", ":", "\n", "        ", "if", "shape", "[", "i", "]", "%", "must_be_divisible_by", "[", "i", "]", "==", "0", ":", "\n", "            ", "new_shp", "[", "i", "]", "-=", "must_be_divisible_by", "[", "i", "]", "\n", "", "", "new_shp", "=", "np", ".", "array", "(", "new_shp", ")", ".", "astype", "(", "int", ")", "\n", "return", "new_shp", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.get_network_numpool": [[182, 187], ["numpy.floor().astype", "min", "numpy.floor", "numpy.log", "numpy.log"], "function", ["None"], ["", "def", "get_network_numpool", "(", "patch_size", ",", "maxpool_cap", "=", "Generic_UNet", ".", "MAX_NUMPOOL_3D", ",", "\n", "min_feature_map_size", "=", "FEATUREMAP_MIN_EDGE_LENGTH_BOTTLENECK", ")", ":", "\n", "    ", "network_numpool_per_axis", "=", "np", ".", "floor", "(", "[", "np", ".", "log", "(", "i", "/", "min_feature_map_size", ")", "/", "np", ".", "log", "(", "2", ")", "for", "i", "in", "patch_size", "]", ")", ".", "astype", "(", "int", ")", "\n", "network_numpool_per_axis", "=", "[", "min", "(", "i", ",", "maxpool_cap", ")", "for", "i", "in", "network_numpool_per_axis", "]", "\n", "return", "network_numpool_per_axis", "\n", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.__init__": [[26, 42], ["nnunet.preprocessing.cropping.get_patient_identifiers_from_cropped_files", "isfile", "join", "join", "join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_patient_identifiers_from_cropped_files"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "overwrite", "=", "True", ",", "num_processes", "=", "8", ")", ":", "\n", "        ", "\"\"\"\n        :param folder_with_cropped_data:\n        :param overwrite: If True then precomputed values will not be used and instead recomputed from the data.\n        False will allow loading of precomputed values. This may be dangerous though if some of the code of this class\n        was changed, therefore the default is True.\n        \"\"\"", "\n", "self", ".", "num_processes", "=", "num_processes", "\n", "self", ".", "overwrite", "=", "overwrite", "\n", "self", ".", "folder_with_cropped_data", "=", "folder_with_cropped_data", "\n", "self", ".", "sizes", "=", "self", ".", "spacings", "=", "None", "\n", "self", ".", "patient_identifiers", "=", "get_patient_identifiers_from_cropped_files", "(", "self", ".", "folder_with_cropped_data", ")", "\n", "assert", "isfile", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset.json\"", ")", ")", ",", "\"dataset.json needs to be in folder_with_cropped_data\"", "\n", "self", ".", "props_per_case_file", "=", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"props_per_case.pkl\"", ")", "\n", "self", ".", "intensityproperties_file", "=", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"intensityproperties.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped": [[43, 47], ["open", "pickle.load", "join"], "methods", ["None"], ["", "def", "load_properties_of_cropped", "(", "self", ",", "case_identifier", ")", ":", "\n", "        ", "with", "open", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "properties", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._check_if_all_in_one_region": [[48, 62], ["collections.OrderedDict", "numpy.zeros", "skimage.morphology.label", "tuple", "tuple"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_check_if_all_in_one_region", "(", "args", ")", ":", "\n", "        ", "seg", ",", "regions", "=", "args", "\n", "res", "=", "OrderedDict", "(", ")", "\n", "for", "r", "in", "regions", ":", "\n", "            ", "new_seg", "=", "np", ".", "zeros", "(", "seg", ".", "shape", ")", "\n", "for", "c", "in", "r", ":", "\n", "                ", "new_seg", "[", "seg", "==", "c", "]", "=", "1", "\n", "", "labelmap", ",", "numlabels", "=", "label", "(", "new_seg", ",", "return_num", "=", "True", ")", "\n", "if", "numlabels", "!=", "1", ":", "\n", "                ", "res", "[", "tuple", "(", "r", ")", "]", "=", "False", "\n", "", "else", ":", "\n", "                ", "res", "[", "tuple", "(", "r", ")", "]", "=", "True", "\n", "", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._collect_class_and_region_sizes": [[63, 75], ["collections.OrderedDict", "collections.OrderedDict", "skimage.morphology.label", "range", "numpy.sum", "region_volume_per_class[].append", "numpy.sum"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_collect_class_and_region_sizes", "(", "args", ")", ":", "\n", "        ", "seg", ",", "all_classes", ",", "vol_per_voxel", "=", "args", "\n", "volume_per_class", "=", "OrderedDict", "(", ")", "\n", "region_volume_per_class", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "all_classes", ":", "\n", "            ", "region_volume_per_class", "[", "c", "]", "=", "[", "]", "\n", "volume_per_class", "[", "c", "]", "=", "np", ".", "sum", "(", "seg", "==", "c", ")", "*", "vol_per_voxel", "\n", "labelmap", ",", "numregions", "=", "label", "(", "seg", "==", "c", ",", "return_num", "=", "True", ")", "\n", "for", "l", "in", "range", "(", "1", ",", "numregions", "+", "1", ")", ":", "\n", "                ", "region_volume_per_class", "[", "c", "]", ".", "append", "(", "np", ".", "sum", "(", "labelmap", "==", "l", ")", "*", "vol_per_voxel", ")", "\n", "", "", "return", "volume_per_class", ",", "region_volume_per_class", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._load_seg_analyze_classes": [[76, 104], ["load_pickle", "numpy.prod", "numpy.unique", "list", "list.append", "DatasetAnalyzer.DatasetAnalyzer._check_if_all_in_one_region", "DatasetAnalyzer.DatasetAnalyzer._collect_class_and_region_sizes", "list", "list.append", "numpy.load", "join", "join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._check_if_all_in_one_region", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._collect_class_and_region_sizes"], ["", "def", "_load_seg_analyze_classes", "(", "self", ",", "args", ")", ":", "\n", "        ", "\"\"\"\n        1) what class is in this training case?\n        2) what is the size distribution for each class?\n        3) what is the region size of each class?\n        4) check if all in one region\n        :return:\n        \"\"\"", "\n", "patient_identifier", ",", "all_classes", "=", "args", "\n", "seg", "=", "np", ".", "load", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "patient_identifier", ")", "+", "\".npz\"", ")", "[", "'data'", "]", "[", "-", "1", "]", "\n", "pkl", "=", "load_pickle", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "patient_identifier", ")", "+", "\".pkl\"", ")", "\n", "vol_per_voxel", "=", "np", ".", "prod", "(", "pkl", "[", "'itk_spacing'", "]", ")", "\n", "\n", "# ad 1)", "\n", "unique_classes", "=", "np", ".", "unique", "(", "seg", ")", "\n", "\n", "# 4) check if all in one region", "\n", "regions", "=", "list", "(", ")", "\n", "regions", ".", "append", "(", "list", "(", "all_classes", ")", ")", "\n", "for", "c", "in", "all_classes", ":", "\n", "            ", "regions", ".", "append", "(", "(", "c", ",", ")", ")", "\n", "\n", "", "all_in_one_region", "=", "self", ".", "_check_if_all_in_one_region", "(", "(", "seg", ",", "regions", ")", ")", "\n", "\n", "# 2 & 3) region sizes", "\n", "volume_per_class", ",", "region_sizes", "=", "self", ".", "_collect_class_and_region_sizes", "(", "(", "seg", ",", "all_classes", ",", "vol_per_voxel", ")", ")", "\n", "\n", "return", "unique_classes", ",", "all_in_one_region", ",", "volume_per_class", ",", "region_sizes", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_classes": [[105, 108], ["load_json", "join"], "methods", ["None"], ["", "def", "get_classes", "(", "self", ")", ":", "\n", "        ", "datasetjson", "=", "load_json", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset.json\"", ")", ")", "\n", "return", "datasetjson", "[", "'labels'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.analyse_segmentations": [[109, 135], ["DatasetAnalyzer.DatasetAnalyzer.get_classes", "numpy.array", "multiprocessing.Pool", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "collections.OrderedDict", "zip", "save_pickle", "load_pickle", "int", "isfile", "zip", "dict", "DatasetAnalyzer.DatasetAnalyzer.keys", "len"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_classes"], ["", "def", "analyse_segmentations", "(", "self", ")", ":", "\n", "        ", "class_dct", "=", "self", ".", "get_classes", "(", ")", "\n", "all_classes", "=", "np", ".", "array", "(", "[", "int", "(", "i", ")", "for", "i", "in", "class_dct", ".", "keys", "(", ")", "]", ")", "\n", "all_classes", "=", "all_classes", "[", "all_classes", ">", "0", "]", "# remove background", "\n", "\n", "if", "self", ".", "overwrite", "or", "not", "isfile", "(", "self", ".", "props_per_case_file", ")", ":", "\n", "            ", "p", "=", "Pool", "(", "self", ".", "num_processes", ")", "\n", "res", "=", "p", ".", "map", "(", "self", ".", "_load_seg_analyze_classes", ",", "zip", "(", "self", ".", "patient_identifiers", ",", "\n", "[", "all_classes", "]", "*", "len", "(", "self", ".", "patient_identifiers", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "props_per_patient", "=", "OrderedDict", "(", ")", "\n", "for", "p", ",", "(", "unique_classes", ",", "all_in_one_region", ",", "voxels_per_class", ",", "region_volume_per_class", ")", "in", "zip", "(", "self", ".", "patient_identifiers", ",", "res", ")", ":", "\n", "                ", "props", "=", "dict", "(", ")", "\n", "props", "[", "'has_classes'", "]", "=", "unique_classes", "\n", "props", "[", "'only_one_region'", "]", "=", "all_in_one_region", "\n", "props", "[", "'volume_per_class'", "]", "=", "voxels_per_class", "\n", "props", "[", "'region_volume_per_class'", "]", "=", "region_volume_per_class", "\n", "props_per_patient", "[", "p", "]", "=", "props", "\n", "\n", "", "save_pickle", "(", "props_per_patient", ",", "self", ".", "props_per_case_file", ")", "\n", "", "else", ":", "\n", "            ", "props_per_patient", "=", "load_pickle", "(", "self", ".", "props_per_case_file", ")", "\n", "", "return", "class_dct", ",", "props_per_patient", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_sizes_and_spacings_after_cropping": [[136, 147], ["nnunet.preprocessing.cropping.get_patient_identifiers_from_cropped_files", "print", "DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped", "sizes.append", "spacings.append"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_patient_identifiers_from_cropped_files", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped"], ["", "def", "get_sizes_and_spacings_after_cropping", "(", "self", ")", ":", "\n", "        ", "case_identifiers", "=", "get_patient_identifiers_from_cropped_files", "(", "self", ".", "folder_with_cropped_data", ")", "\n", "print", "(", "'case_identifiers is:'", ",", "case_identifiers", ")", "\n", "sizes", "=", "[", "]", "\n", "spacings", "=", "[", "]", "\n", "for", "c", "in", "case_identifiers", ":", "\n", "            ", "properties", "=", "self", ".", "load_properties_of_cropped", "(", "c", ")", "\n", "sizes", ".", "append", "(", "properties", "[", "\"size_after_cropping\"", "]", ")", "\n", "spacings", ".", "append", "(", "properties", "[", "\"original_spacing\"", "]", ")", "\n", "\n", "", "return", "sizes", ",", "spacings", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_modalities": [[148, 153], ["load_json", "join", "int", "modalities.keys"], "methods", ["None"], ["", "def", "get_modalities", "(", "self", ")", ":", "\n", "        ", "datasetjson", "=", "load_json", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset.json\"", ")", ")", "\n", "modalities", "=", "datasetjson", "[", "\"modality\"", "]", "\n", "modalities", "=", "{", "int", "(", "k", ")", ":", "modalities", "[", "k", "]", "for", "k", "in", "modalities", ".", "keys", "(", ")", "}", "\n", "return", "modalities", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_size_reduction_by_cropping": [[154, 163], ["collections.OrderedDict", "DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped", "numpy.prod", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.load_properties_of_cropped"], ["", "def", "get_size_reduction_by_cropping", "(", "self", ")", ":", "\n", "        ", "size_reduction", "=", "OrderedDict", "(", ")", "\n", "for", "p", "in", "self", ".", "patient_identifiers", ":", "\n", "            ", "props", "=", "self", ".", "load_properties_of_cropped", "(", "p", ")", "\n", "shape_before_crop", "=", "props", "[", "\"original_size_of_raw_data\"", "]", "\n", "shape_after_crop", "=", "props", "[", "'size_after_cropping'", "]", "\n", "size_red", "=", "np", ".", "prod", "(", "shape_after_crop", ")", "/", "np", ".", "prod", "(", "shape_before_crop", ")", "\n", "size_reduction", "[", "p", "]", "=", "size_red", "\n", "", "return", "size_reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._get_voxels_in_foreground": [[164, 171], ["list", "numpy.load", "join"], "methods", ["None"], ["", "def", "_get_voxels_in_foreground", "(", "self", ",", "args", ")", ":", "\n", "        ", "patient_identifier", ",", "modality_id", "=", "args", "\n", "all_data", "=", "np", ".", "load", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "patient_identifier", ")", "+", "\".npz\"", ")", "[", "'data'", "]", "\n", "modality", "=", "all_data", "[", "modality_id", "]", "\n", "mask", "=", "all_data", "[", "-", "1", "]", ">", "0", "\n", "voxels", "=", "list", "(", "modality", "[", "mask", "]", "[", ":", ":", "10", "]", ")", "# no need to take every voxel", "\n", "return", "voxels", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._compute_stats": [[172, 184], ["numpy.median", "numpy.mean", "numpy.std", "numpy.min", "numpy.max", "numpy.percentile", "numpy.percentile", "len"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_compute_stats", "(", "voxels", ")", ":", "\n", "        ", "if", "len", "(", "voxels", ")", "==", "0", ":", "\n", "            ", "return", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", ",", "np", ".", "nan", "\n", "", "median", "=", "np", ".", "median", "(", "voxels", ")", "\n", "mean", "=", "np", ".", "mean", "(", "voxels", ")", "\n", "sd", "=", "np", ".", "std", "(", "voxels", ")", "\n", "mn", "=", "np", ".", "min", "(", "voxels", ")", "\n", "mx", "=", "np", ".", "max", "(", "voxels", ")", "\n", "percentile_99_5", "=", "np", ".", "percentile", "(", "voxels", ",", "99.5", ")", "\n", "percentile_00_5", "=", "np", ".", "percentile", "(", "voxels", ",", "00.5", ")", "\n", "return", "median", ",", "mean", ",", "sd", ",", "mn", ",", "mx", ",", "percentile_99_5", ",", "percentile_00_5", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.collect_intensity_properties": [[185, 228], ["multiprocessing.Pool", "collections.OrderedDict", "range", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "save_pickle", "load_pickle", "isfile", "collections.OrderedDict", "multiprocessing.Pool.map", "DatasetAnalyzer.DatasetAnalyzer._compute_stats", "multiprocessing.Pool.map", "collections.OrderedDict", "enumerate", "zip", "collections.OrderedDict", "len"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer._compute_stats"], ["", "def", "collect_intensity_properties", "(", "self", ",", "num_modalities", ")", ":", "\n", "        ", "if", "self", ".", "overwrite", "or", "not", "isfile", "(", "self", ".", "intensityproperties_file", ")", ":", "\n", "            ", "p", "=", "Pool", "(", "self", ".", "num_processes", ")", "\n", "\n", "results", "=", "OrderedDict", "(", ")", "\n", "for", "mod_id", "in", "range", "(", "num_modalities", ")", ":", "\n", "                ", "results", "[", "mod_id", "]", "=", "OrderedDict", "(", ")", "\n", "v", "=", "p", ".", "map", "(", "self", ".", "_get_voxels_in_foreground", ",", "zip", "(", "self", ".", "patient_identifiers", ",", "\n", "[", "mod_id", "]", "*", "len", "(", "self", ".", "patient_identifiers", ")", ")", ")", "\n", "\n", "w", "=", "[", "]", "\n", "for", "iv", "in", "v", ":", "\n", "                    ", "w", "+=", "iv", "\n", "\n", "", "median", ",", "mean", ",", "sd", ",", "mn", ",", "mx", ",", "percentile_99_5", ",", "percentile_00_5", "=", "self", ".", "_compute_stats", "(", "w", ")", "\n", "\n", "local_props", "=", "p", ".", "map", "(", "self", ".", "_compute_stats", ",", "v", ")", "\n", "props_per_case", "=", "OrderedDict", "(", ")", "\n", "for", "i", ",", "pat", "in", "enumerate", "(", "self", ".", "patient_identifiers", ")", ":", "\n", "                    ", "props_per_case", "[", "pat", "]", "=", "OrderedDict", "(", ")", "\n", "props_per_case", "[", "pat", "]", "[", "'median'", "]", "=", "local_props", "[", "i", "]", "[", "0", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'mean'", "]", "=", "local_props", "[", "i", "]", "[", "1", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'sd'", "]", "=", "local_props", "[", "i", "]", "[", "2", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'mn'", "]", "=", "local_props", "[", "i", "]", "[", "3", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'mx'", "]", "=", "local_props", "[", "i", "]", "[", "4", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'percentile_99_5'", "]", "=", "local_props", "[", "i", "]", "[", "5", "]", "\n", "props_per_case", "[", "pat", "]", "[", "'percentile_00_5'", "]", "=", "local_props", "[", "i", "]", "[", "6", "]", "\n", "\n", "", "results", "[", "mod_id", "]", "[", "'local_props'", "]", "=", "props_per_case", "\n", "results", "[", "mod_id", "]", "[", "'median'", "]", "=", "median", "\n", "results", "[", "mod_id", "]", "[", "'mean'", "]", "=", "mean", "\n", "results", "[", "mod_id", "]", "[", "'sd'", "]", "=", "sd", "\n", "results", "[", "mod_id", "]", "[", "'mn'", "]", "=", "mn", "\n", "results", "[", "mod_id", "]", "[", "'mx'", "]", "=", "mx", "\n", "results", "[", "mod_id", "]", "[", "'percentile_99_5'", "]", "=", "percentile_99_5", "\n", "results", "[", "mod_id", "]", "[", "'percentile_00_5'", "]", "=", "percentile_00_5", "\n", "\n", "", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "save_pickle", "(", "results", ",", "self", ".", "intensityproperties_file", ")", "\n", "", "else", ":", "\n", "            ", "results", "=", "load_pickle", "(", "self", ".", "intensityproperties_file", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.analyze_dataset": [[229, 264], ["DatasetAnalyzer.DatasetAnalyzer.get_sizes_and_spacings_after_cropping", "DatasetAnalyzer.DatasetAnalyzer.analyse_segmentations", "numpy.array", "DatasetAnalyzer.DatasetAnalyzer.get_modalities", "DatasetAnalyzer.DatasetAnalyzer.get_size_reduction_by_cropping", "dict", "save_pickle", "DatasetAnalyzer.DatasetAnalyzer.collect_intensity_properties", "join", "int", "len", "class_dct.keys"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_sizes_and_spacings_after_cropping", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.analyse_segmentations", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_modalities", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.get_size_reduction_by_cropping", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.collect_intensity_properties"], ["", "def", "analyze_dataset", "(", "self", ",", "collect_intensityproperties", "=", "True", ")", ":", "\n", "# get all spacings and sizes", "\n", "        ", "sizes", ",", "spacings", "=", "self", ".", "get_sizes_and_spacings_after_cropping", "(", ")", "\n", "\n", "# get all classes and what classes are in what patients", "\n", "# class min size", "\n", "# region size per class", "\n", "class_dct", ",", "segmentation_props_per_patient", "=", "self", ".", "analyse_segmentations", "(", ")", "\n", "all_classes", "=", "np", ".", "array", "(", "[", "int", "(", "i", ")", "for", "i", "in", "class_dct", ".", "keys", "(", ")", "]", ")", "\n", "all_classes", "=", "all_classes", "[", "all_classes", ">", "0", "]", "\n", "\n", "# modalities", "\n", "modalities", "=", "self", ".", "get_modalities", "(", ")", "\n", "\n", "# collect intensity information", "\n", "if", "collect_intensityproperties", ":", "\n", "            ", "intensityproperties", "=", "self", ".", "collect_intensity_properties", "(", "len", "(", "modalities", ")", ")", "\n", "", "else", ":", "\n", "            ", "intensityproperties", "=", "None", "\n", "\n", "# size reduction by cropping", "\n", "", "size_reductions", "=", "self", ".", "get_size_reduction_by_cropping", "(", ")", "\n", "\n", "dataset_properties", "=", "dict", "(", ")", "\n", "dataset_properties", "[", "'all_sizes'", "]", "=", "sizes", "\n", "dataset_properties", "[", "'all_spacings'", "]", "=", "spacings", "\n", "dataset_properties", "[", "'segmentation_props_per_patient'", "]", "=", "segmentation_props_per_patient", "\n", "dataset_properties", "[", "'class_dct'", "]", "=", "class_dct", "# {int: class name}", "\n", "dataset_properties", "[", "'all_classes'", "]", "=", "all_classes", "\n", "dataset_properties", "[", "'modalities'", "]", "=", "modalities", "# {idx: modality name}", "\n", "dataset_properties", "[", "'intensityproperties'", "]", "=", "intensityproperties", "\n", "dataset_properties", "[", "'size_reductions'", "]", "=", "size_reductions", "# {patient_id: size_reduction}", "\n", "\n", "save_pickle", "(", "dataset_properties", ",", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"dataset_properties.pkl\"", ")", ")", "\n", "return", "dataset_properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.summarize_plans.summarize_plans": [[20, 35], ["load_pickle", "print", "print", "print", "print", "print", "print", "print", "print", "range", "len", "print", "print", "print"], "function", ["None"], ["def", "summarize_plans", "(", "file", ")", ":", "\n", "    ", "plans", "=", "load_pickle", "(", "file", ")", "\n", "print", "(", "\"num_classes: \"", ",", "plans", "[", "'num_classes'", "]", ")", "\n", "print", "(", "\"modalities: \"", ",", "plans", "[", "'modalities'", "]", ")", "\n", "print", "(", "\"use_mask_for_norm\"", ",", "plans", "[", "'use_mask_for_norm'", "]", ")", "\n", "print", "(", "\"keep_only_largest_region\"", ",", "plans", "[", "'keep_only_largest_region'", "]", ")", "\n", "print", "(", "\"min_region_size_per_class\"", ",", "plans", "[", "'min_region_size_per_class'", "]", ")", "\n", "print", "(", "\"min_size_per_class\"", ",", "plans", "[", "'min_size_per_class'", "]", ")", "\n", "print", "(", "\"normalization_schemes\"", ",", "plans", "[", "'normalization_schemes'", "]", ")", "\n", "print", "(", "\"stages...\\n\"", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "plans", "[", "'plans_per_stage'", "]", ")", ")", ":", "\n", "        ", "print", "(", "\"stage: \"", ",", "i", ")", "\n", "print", "(", "plans", "[", "'plans_per_stage'", "]", "[", "i", "]", ")", "\n", "print", "(", "\"\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.summarize_plans.write_plans_to_file": [[37, 61], ["load_pickle", "list", "list.sort", "a[].keys", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "zip", "zip", "plans_file.split", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "str", "plans_file.split", "str", "str", "str", "str"], "function", ["None"], ["", "", "def", "write_plans_to_file", "(", "f", ",", "plans_file", ")", ":", "\n", "    ", "a", "=", "load_pickle", "(", "plans_file", ")", "\n", "stages", "=", "list", "(", "a", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", "\n", "stages", ".", "sort", "(", ")", "\n", "for", "stage", "in", "stages", ":", "\n", "        ", "patch_size_in_mm", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'patch_size'", "]", ",", "\n", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", "]", "\n", "median_patient_size_in_mm", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'median_patient_size_in_voxels'", "]", ",", "\n", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", "]", "\n", "f", ".", "write", "(", "plans_file", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "plans_file", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", "\n", "f", ".", "write", "(", "\";%d\"", "%", "stage", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'batch_size'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'num_pool_per_axis'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'patch_size'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "[", "str", "(", "\"%03.2f\"", "%", "i", ")", "for", "i", "in", "patch_size_in_mm", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'median_patient_size_in_voxels'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "[", "str", "(", "\"%03.2f\"", "%", "i", ")", "for", "i", "in", "median_patient_size_in_mm", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "[", "str", "(", "\"%03.2f\"", "%", "i", ")", "for", "i", "in", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "[", "str", "(", "\"%03.2f\"", "%", "i", ")", "for", "i", "in", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'original_spacing'", "]", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'pool_op_kernel_sizes'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'conv_kernel_sizes'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'data_identifier'", "]", ")", ")", "\n", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.find_classes_in_slice.add_classes_in_slice_info": [[20, 53], ["print", "collections.OrderedDict", "range", "collections.OrderedDict", "open", "pickle.load", "tuple", "collections.OrderedDict", "numpy.sum", "open", "pickle.dump", "numpy.load", "numpy.where", "range", "numpy.sum"], "function", ["None"], ["def", "add_classes_in_slice_info", "(", "args", ")", ":", "\n", "    ", "\"\"\"\n    We need this for 2D dataloader with oversampling. As of now it will detect slices that contain specific classes\n    at run time, meaning it needs to iterate over an entire patient just to extract one slice. That is obviously bad,\n    so we are doing this once beforehand and just give the dataloader the info it needs in the patients pkl file.\n\n    \"\"\"", "\n", "npz_file", ",", "pkl_file", ",", "all_classes", "=", "args", "\n", "seg_map", "=", "np", ".", "load", "(", "npz_file", ")", "[", "'data'", "]", "[", "-", "1", "]", "\n", "with", "open", "(", "pkl_file", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "props", "=", "pickle", ".", "load", "(", "f", ")", "\n", "#if props.get('classes_in_slice_per_axis') is not None:", "\n", "", "print", "(", "pkl_file", ")", "\n", "# this will be a dict of dict where the first dict encodes the axis along which a slice is extracted in its keys.", "\n", "# The second dict (value of first dict) will have all classes as key and as values a list of all slice ids that", "\n", "# contain this class", "\n", "classes_in_slice", "=", "OrderedDict", "(", ")", "\n", "for", "axis", "in", "range", "(", "3", ")", ":", "\n", "        ", "other_axes", "=", "tuple", "(", "[", "i", "for", "i", "in", "range", "(", "3", ")", "if", "i", "!=", "axis", "]", ")", "\n", "classes_in_slice", "[", "axis", "]", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "all_classes", ":", "\n", "            ", "valid_slices", "=", "np", ".", "where", "(", "np", ".", "sum", "(", "seg_map", "==", "c", ",", "axis", "=", "other_axes", ")", ">", "0", ")", "[", "0", "]", "\n", "classes_in_slice", "[", "axis", "]", "[", "c", "]", "=", "valid_slices", "\n", "\n", "", "", "number_of_voxels_per_class", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "all_classes", ":", "\n", "        ", "number_of_voxels_per_class", "[", "c", "]", "=", "np", ".", "sum", "(", "seg_map", "==", "c", ")", "\n", "\n", "", "props", "[", "'classes_in_slice_per_axis'", "]", "=", "classes_in_slice", "\n", "props", "[", "'number_of_voxels_per_class'", "]", "=", "number_of_voxels_per_class", "\n", "\n", "with", "open", "(", "pkl_file", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "pickle", ".", "dump", "(", "props", ",", "f", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.split_4d": [[33, 62], ["join", "join", "isdir", "maybe_mkdir_p", "shutil.copytree", "multiprocessing.Pool", "multiprocessing.Pool.starmap", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "shutil.copy", "shutil.rmtree", "join", "join", "nii_files.sort", "join", "join", "zip", "join", "isdir", "os.mkdir", "os.mkdir", "join", "files.append", "output_dirs.append", "os.listdir", "os.listdir", "i.endswith"], "function", ["None"], ["def", "split_4d", "(", "task_string", ")", ":", "\n", "    ", "base_folder", "=", "join", "(", "raw_dataset_dir", ",", "task_string", ")", "\n", "output_folder", "=", "join", "(", "splitted_4d_output_dir", ",", "task_string", ")", "\n", "\n", "if", "isdir", "(", "output_folder", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "output_folder", ")", "\n", "\n", "", "files", "=", "[", "]", "\n", "output_dirs", "=", "[", "]", "\n", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "for", "subdir", "in", "[", "\"imagesTr\"", ",", "\"imagesTs\"", "]", ":", "\n", "        ", "curr_out_dir", "=", "join", "(", "output_folder", ",", "subdir", ")", "\n", "if", "not", "isdir", "(", "curr_out_dir", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "curr_out_dir", ")", "\n", "", "curr_dir", "=", "join", "(", "base_folder", ",", "subdir", ")", "\n", "nii_files", "=", "[", "join", "(", "curr_dir", ",", "i", ")", "for", "i", "in", "os", ".", "listdir", "(", "curr_dir", ")", "if", "i", ".", "endswith", "(", "\".nii.gz\"", ")", "]", "\n", "nii_files", ".", "sort", "(", ")", "\n", "for", "n", "in", "nii_files", ":", "\n", "            ", "files", ".", "append", "(", "n", ")", "\n", "output_dirs", ".", "append", "(", "curr_out_dir", ")", "\n", "\n", "", "", "shutil", ".", "copytree", "(", "join", "(", "base_folder", ",", "\"labelsTr\"", ")", ",", "join", "(", "output_folder", ",", "\"labelsTr\"", ")", ")", "\n", "\n", "p", "=", "Pool", "(", "8", ")", "\n", "p", ".", "starmap", "(", "split_4d_nifti", ",", "zip", "(", "files", ",", "output_dirs", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "base_folder", ",", "\"dataset.json\"", ")", ",", "output_folder", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.create_lists_from_splitted_dataset": [[64, 81], ["join", "len", "print", "open", "json.load", "d[].keys", "range", "cur_pat.append", "lists.append", "cur_pat.append", "join", "int", "join", "d[].keys", "tr[].split", "str", "tr[].split"], "function", ["None"], ["", "def", "create_lists_from_splitted_dataset", "(", "base_folder_splitted", ")", ":", "\n", "    ", "lists", "=", "[", "]", "\n", "\n", "json_file", "=", "join", "(", "base_folder_splitted", ",", "\"dataset.json\"", ")", "\n", "with", "open", "(", "json_file", ")", "as", "jsn", ":", "\n", "        ", "d", "=", "json", ".", "load", "(", "jsn", ")", "\n", "training_files", "=", "d", "[", "'training'", "]", "\n", "", "num_modalities", "=", "len", "(", "d", "[", "'modality'", "]", ".", "keys", "(", ")", ")", "\n", "for", "tr", "in", "training_files", ":", "\n", "        ", "cur_pat", "=", "[", "]", "\n", "for", "mod", "in", "range", "(", "num_modalities", ")", ":", "\n", "            ", "cur_pat", ".", "append", "(", "join", "(", "base_folder_splitted", ",", "\"imagesTr/\"", ",", "tr", "[", "'image'", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", "+", "\n", "\"_%04.0d.nii.gz\"", "%", "mod", ")", ")", "# Here will appear the problem: in windows, we should use \"\\\" to split the path", "\n", "", "cur_pat", ".", "append", "(", "join", "(", "base_folder_splitted", ",", "\"labelsTr/\"", ",", "tr", "[", "'label'", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", ")", "\n", "lists", ".", "append", "(", "cur_pat", ")", "\n", "", "print", "(", "'Lists is:'", ",", "lists", ")", "\n", "return", "lists", ",", "{", "int", "(", "i", ")", ":", "d", "[", "'modality'", "]", "[", "str", "(", "i", ")", "]", "for", "i", "in", "d", "[", "'modality'", "]", ".", "keys", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.create_lists_from_splitted_dataset_folder": [[83, 94], ["plan_and_preprocess_task.get_caseIDs_from_splitted_dataset_folder", "list_of_lists.append", "subfiles"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.get_caseIDs_from_splitted_dataset_folder"], ["", "def", "create_lists_from_splitted_dataset_folder", "(", "folder", ")", ":", "\n", "    ", "\"\"\"\n    does not rely on dataset.json\n    :param folder:\n    :return:\n    \"\"\"", "\n", "caseIDs", "=", "get_caseIDs_from_splitted_dataset_folder", "(", "folder", ")", "\n", "list_of_lists", "=", "[", "]", "\n", "for", "f", "in", "caseIDs", ":", "\n", "        ", "list_of_lists", ".", "append", "(", "subfiles", "(", "folder", ",", "prefix", "=", "f", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "True", ",", "sort", "=", "True", ")", ")", "\n", "", "return", "list_of_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.get_caseIDs_from_splitted_dataset_folder": [[96, 103], ["subfiles", "numpy.unique"], "function", ["None"], ["", "def", "get_caseIDs_from_splitted_dataset_folder", "(", "folder", ")", ":", "\n", "    ", "files", "=", "subfiles", "(", "folder", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "# all files must be .nii.gz and have 4 digit modality index", "\n", "files", "=", "[", "i", "[", ":", "-", "12", "]", "for", "i", "in", "files", "]", "\n", "# only unique patient ids", "\n", "files", "=", "np", ".", "unique", "(", "files", ")", "\n", "return", "files", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.crop": [[105, 121], ["join", "maybe_mkdir_p", "join", "plan_and_preprocess_task.create_lists_from_splitted_dataset", "nnunet.preprocessing.cropping.ImageCropper", "nnunet.preprocessing.cropping.ImageCropper.run_cropping", "shutil.copy", "isdir", "shutil.rmtree", "maybe_mkdir_p", "join"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.create_lists_from_splitted_dataset", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.run_cropping"], ["", "def", "crop", "(", "task_string", ",", "override", "=", "False", ",", "num_threads", "=", "8", ")", ":", "\n", "    ", "cropped_out_dir", "=", "join", "(", "cropped_output_dir", ",", "task_string", ")", "\n", "maybe_mkdir_p", "(", "cropped_out_dir", ")", "\n", "# print('This is:',cropped_out_dir)", "\n", "\n", "if", "override", "and", "isdir", "(", "cropped_out_dir", ")", ":", "\n", "        ", "shutil", ".", "rmtree", "(", "cropped_out_dir", ")", "\n", "maybe_mkdir_p", "(", "cropped_out_dir", ")", "\n", "# print('That is:', cropped_out_dir)", "\n", "\n", "", "splitted_4d_output_dir_task", "=", "join", "(", "splitted_4d_output_dir", ",", "task_string", ")", "\n", "lists", ",", "_", "=", "create_lists_from_splitted_dataset", "(", "splitted_4d_output_dir_task", ")", "\n", "# print('Nearest cropped_out_dir is:',cropped_out_dir)", "\n", "imgcrop", "=", "ImageCropper", "(", "num_threads", ",", "cropped_out_dir", ")", "\n", "imgcrop", ".", "run_cropping", "(", "lists", ",", "overwrite_existing", "=", "override", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "splitted_4d_output_dir", ",", "task_string", ",", "\"dataset.json\"", ")", ",", "cropped_out_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.analyze_dataset": [[123, 128], ["join", "print", "nnunet.experiment_planning.DatasetAnalyzer.DatasetAnalyzer", "nnunet.experiment_planning.DatasetAnalyzer.DatasetAnalyzer.analyze_dataset"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.analyze_dataset"], ["", "def", "analyze_dataset", "(", "task_string", ",", "override", "=", "False", ",", "collect_intensityproperties", "=", "True", ",", "num_processes", "=", "8", ")", ":", "\n", "    ", "cropped_out_dir", "=", "join", "(", "cropped_output_dir", ",", "task_string", "+", "'/'", ")", "\n", "print", "(", "'cropped_out_dir is:'", ",", "cropped_out_dir", ",", "'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'", ")", "\n", "dataset_analyzer", "=", "DatasetAnalyzer", "(", "cropped_out_dir", ",", "overwrite", "=", "override", ",", "num_processes", "=", "num_processes", ")", "\n", "_", "=", "dataset_analyzer", ".", "analyze_dataset", "(", "collect_intensityproperties", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.plan_and_preprocess": [[130, 174], ["join", "join", "maybe_mkdir_p", "shutil.copy", "shutil.copy", "ExperimentPlanner", "ExperimentPlanner2D.plan_experiment", "ExperimentPlanner2D", "ExperimentPlanner2D.plan_experiment", "join", "join", "ExperimentPlanner2D.run_preprocessing", "ExperimentPlanner2D.run_preprocessing", "multiprocessing.Pool", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "print", "subfiles", "multiprocessing.Pool.map", "subdirs", "numpy.array", "all_classes.append", "zip", "[].find", "s.split", "open", "pickle.load", "i.split"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plan_experiment", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plan_experiment", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.run_preprocessing", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.run_preprocessing"], ["", "def", "plan_and_preprocess", "(", "task_string", ",", "num_threads", "=", "8", ",", "no_preprocessing", "=", "False", ")", ":", "\n", "    ", "from", "nnunet", ".", "experiment_planning", ".", "experiment_planner_baseline_2DUNet", "import", "ExperimentPlanner2D", "\n", "from", "nnunet", ".", "experiment_planning", ".", "experiment_planner_baseline_3DUNet", "import", "ExperimentPlanner", "\n", "\n", "preprocessing_output_dir_this_task_train", "=", "join", "(", "preprocessing_output_dir", ",", "task_string", ")", "\n", "cropped_out_dir", "=", "join", "(", "cropped_output_dir", ",", "task_string", "+", "'/'", ")", "\n", "maybe_mkdir_p", "(", "preprocessing_output_dir_this_task_train", ")", "\n", "\n", "shutil", ".", "copy", "(", "join", "(", "cropped_out_dir", ",", "\"dataset_properties.pkl\"", ")", ",", "preprocessing_output_dir_this_task_train", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "splitted_4d_output_dir", ",", "task_string", ",", "\"dataset.json\"", ")", ",", "preprocessing_output_dir_this_task_train", ")", "\n", "\n", "exp_planner", "=", "ExperimentPlanner", "(", "cropped_out_dir", ",", "preprocessing_output_dir_this_task_train", ")", "\n", "exp_planner", ".", "plan_experiment", "(", ")", "\n", "if", "not", "no_preprocessing", ":", "\n", "        ", "exp_planner", ".", "run_preprocessing", "(", "num_threads", ")", "\n", "\n", "", "exp_planner", "=", "ExperimentPlanner2D", "(", "cropped_out_dir", ",", "preprocessing_output_dir_this_task_train", ")", "\n", "exp_planner", ".", "plan_experiment", "(", ")", "\n", "if", "not", "no_preprocessing", ":", "\n", "        ", "exp_planner", ".", "run_preprocessing", "(", "num_threads", ")", "\n", "\n", "# write which class is in which slice to all training cases (required to speed up 2D Dataloader)", "\n", "# This is done for all data so that if we wanted to use them with 2D we could do so", "\n", "\n", "", "if", "not", "no_preprocessing", ":", "\n", "        ", "p", "=", "Pool", "(", "8", ")", "\n", "\n", "# if there is more than one my_data_identifier (different brnaches) then this code will run for all of them if", "\n", "# they start with the same string. not problematic, but not pretty", "\n", "stages", "=", "[", "i", "for", "i", "in", "subdirs", "(", "preprocessing_output_dir_this_task_train", ",", "join", "=", "True", ",", "sort", "=", "True", ")", "\n", "if", "i", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "find", "(", "\"stage\"", ")", "!=", "-", "1", "]", "\n", "for", "s", "in", "stages", ":", "\n", "            ", "print", "(", "s", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", "\n", "list_of_npz_files", "=", "subfiles", "(", "s", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "list_of_pkl_files", "=", "[", "i", "[", ":", "-", "4", "]", "+", "\".pkl\"", "for", "i", "in", "list_of_npz_files", "]", "\n", "all_classes", "=", "[", "]", "\n", "for", "pk", "in", "list_of_pkl_files", ":", "\n", "                ", "with", "open", "(", "pk", ",", "'rb'", ")", "as", "f", ":", "\n", "                    ", "props", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "all_classes_tmp", "=", "np", ".", "array", "(", "props", "[", "'classes'", "]", ")", "\n", "all_classes", ".", "append", "(", "all_classes_tmp", "[", "all_classes_tmp", ">=", "0", "]", ")", "\n", "", "p", ".", "map", "(", "add_classes_in_slice_info", ",", "zip", "(", "list_of_npz_files", ",", "list_of_pkl_files", ",", "all_classes", ")", ")", "\n", "", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.__init__": [[28, 35], ["nnunet.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.__init__", "batchgenerators.utilities.file_and_folder_operations.join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "folder_with_cropped_data", ",", "preprocessed_output_folder", ")", ":", "\n", "        ", "super", "(", "ExperimentPlanner2D", ",", "self", ")", ".", "__init__", "(", "folder_with_cropped_data", ",", "\n", "preprocessed_output_folder", ")", "\n", "self", ".", "data_identifier", "=", "\"nnUNet_2D\"", "\n", "self", ".", "transpose_forward", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "self", ".", "transpose_backward", "=", "[", "0", ",", "1", ",", "2", "]", "\n", "self", ".", "plans_fname", "=", "join", "(", "self", ".", "preprocessed_output_folder", ",", "default_plans_identifier", "+", "\"_plans_2D.pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.load_my_plans": [[36, 43], ["batchgenerators.utilities.file_and_folder_operations.load_pickle"], "methods", ["None"], ["", "def", "load_my_plans", "(", "self", ")", ":", "\n", "        ", "self", ".", "plans", "=", "load_pickle", "(", "self", ".", "plans_fname", ")", "\n", "\n", "self", ".", "plans_per_stage", "=", "self", ".", "plans", "[", "'plans_per_stage'", "]", "\n", "self", ".", "dataset_properties", "=", "self", ".", "plans", "[", "'dataset_properties'", "]", "\n", "self", ".", "transpose_forward", "=", "self", ".", "plans", "[", "'transpose_forward'", "]", "\n", "self", ".", "transpose_backward", "=", "self", ".", "plans", "[", "'transpose_backward'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plan_experiment": [[44, 155], ["experiment_planner_baseline_2DUNet.ExperimentPlanner2D.determine_whether_to_use_mask_for_norm", "print", "len", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.get_target_spacing", "numpy.array", "numpy.argmax", "numpy.median", "print", "numpy.max", "print", "numpy.min", "print", "print", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plans_per_stage.append", "print", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.determine_normalization_scheme", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.determine_postprocessing", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.save_my_plans", "numpy.round().astype", "nnunet.experiment_planning.common_utils.get_pool_and_conv_props", "nnunet.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption", "int", "numpy.round().astype", "min", "list", "numpy.vstack", "numpy.vstack", "numpy.vstack", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plan_experiment.get_properties_for_stage"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_whether_to_use_mask_for_norm", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.get_target_spacing", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_normalization_scheme", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.determine_postprocessing", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_3DUNet.ExperimentPlanner.save_my_plans", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.common_utils.get_pool_and_conv_props", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption"], ["", "def", "plan_experiment", "(", "self", ")", ":", "\n", "\n", "        ", "def", "get_properties_for_stage", "(", "current_spacing", ",", "original_spacing", ",", "original_shape", ",", "num_cases", ",", "\n", "num_modalities", ",", "num_classes", ",", "transpose_forward", ")", ":", "\n", "            ", "spacing_transposed", "=", "current_spacing", "[", "transpose_forward", "]", "\n", "\n", "new_median_shape", "=", "np", ".", "round", "(", "original_spacing", "/", "current_spacing", "*", "original_shape", ")", ".", "astype", "(", "int", ")", "\n", "\n", "new_median_shape_transposed", "=", "new_median_shape", "[", "transpose_forward", "]", "\n", "\n", "dataset_num_voxels", "=", "np", ".", "prod", "(", "new_median_shape", ")", "*", "num_cases", "\n", "input_patch_size", "=", "new_median_shape_transposed", "[", "1", ":", "]", "\n", "\n", "network_numpool", ",", "net_pool_kernel_sizes", ",", "net_conv_kernel_sizes", ",", "input_patch_size", ",", "shape_must_be_divisible_by", "=", "get_pool_and_conv_props", "(", "spacing_transposed", "[", "1", ":", "]", ",", "input_patch_size", ",", "\n", "FEATUREMAP_MIN_EDGE_LENGTH_BOTTLENECK", ",", "\n", "Generic_UNet", ".", "MAX_NUMPOOL_2D", ")", "\n", "\n", "estimated_gpu_ram_consumption", "=", "Generic_UNet", ".", "compute_approx_vram_consumption", "(", "input_patch_size", ",", "\n", "network_numpool", ",", "\n", "Generic_UNet", ".", "BASE_NUM_FEATURES_2D", ",", "\n", "Generic_UNet", ".", "MAX_FILTERS_2D", ",", "\n", "num_modalities", ",", "num_classes", ",", "\n", "net_pool_kernel_sizes", ")", "\n", "\n", "batch_size", "=", "int", "(", "np", ".", "floor", "(", "Generic_UNet", ".", "use_this_for_batch_size_computation_2D", "/", "\n", "estimated_gpu_ram_consumption", "*", "Generic_UNet", ".", "DEFAULT_BATCH_SIZE_2D", ")", ")", "\n", "if", "batch_size", "<", "dataset_min_batch_size_cap", ":", "\n", "                ", "raise", "RuntimeError", "(", "\"This framework is not made to process patches this large. We will add patch-based \"", "\n", "\"2D networks later. Sorry for the inconvenience\"", ")", "\n", "\n", "# check if batch size is too large (more than 5 % of dataset)", "\n", "", "max_batch_size", "=", "np", ".", "round", "(", "batch_size_covers_max_percent_of_dataset", "*", "dataset_num_voxels", "/", "\n", "np", ".", "prod", "(", "input_patch_size", ")", ")", ".", "astype", "(", "int", ")", "\n", "batch_size", "=", "min", "(", "batch_size", ",", "max_batch_size", ")", "\n", "\n", "plan", "=", "{", "\n", "'batch_size'", ":", "batch_size", ",", "\n", "'num_pool_per_axis'", ":", "network_numpool", ",", "\n", "'patch_size'", ":", "input_patch_size", ",", "\n", "'median_patient_size_in_voxels'", ":", "new_median_shape", ",", "\n", "'current_spacing'", ":", "current_spacing", ",", "\n", "'original_spacing'", ":", "original_spacing", ",", "\n", "'pool_op_kernel_sizes'", ":", "net_pool_kernel_sizes", ",", "\n", "'conv_kernel_sizes'", ":", "net_conv_kernel_sizes", ",", "\n", "'do_dummy_2D_data_aug'", ":", "False", "\n", "}", "\n", "return", "plan", "\n", "\n", "", "use_nonzero_mask_for_normalization", "=", "self", ".", "determine_whether_to_use_mask_for_norm", "(", ")", "\n", "print", "(", "\"Are we using the nonzero maks for normalizaion?\"", ",", "use_nonzero_mask_for_normalization", ")", "\n", "\n", "spacings", "=", "self", ".", "dataset_properties", "[", "'all_spacings'", "]", "\n", "sizes", "=", "self", ".", "dataset_properties", "[", "'all_sizes'", "]", "\n", "all_classes", "=", "self", ".", "dataset_properties", "[", "'all_classes'", "]", "\n", "modalities", "=", "self", ".", "dataset_properties", "[", "'modalities'", "]", "\n", "num_modalities", "=", "len", "(", "list", "(", "modalities", ".", "keys", "(", ")", ")", ")", "\n", "\n", "target_spacing", "=", "self", ".", "get_target_spacing", "(", ")", "\n", "new_shapes", "=", "np", ".", "array", "(", "[", "np", ".", "array", "(", "i", ")", "/", "target_spacing", "*", "np", ".", "array", "(", "j", ")", "for", "i", ",", "j", "in", "zip", "(", "spacings", ",", "sizes", ")", "]", ")", "\n", "\n", "max_spacing_axis", "=", "np", ".", "argmax", "(", "target_spacing", ")", "\n", "remaining_axes", "=", "[", "i", "for", "i", "in", "list", "(", "range", "(", "3", ")", ")", "if", "i", "!=", "max_spacing_axis", "]", "\n", "self", ".", "transpose_forward", "=", "[", "max_spacing_axis", "]", "+", "remaining_axes", "\n", "self", ".", "transpose_backward", "=", "[", "np", ".", "argwhere", "(", "np", ".", "array", "(", "self", ".", "transpose_forward", ")", "==", "i", ")", "[", "0", "]", "[", "0", "]", "for", "i", "in", "range", "(", "3", ")", "]", "\n", "\n", "# we base our calculations on the median shape of the datasets", "\n", "median_shape", "=", "np", ".", "median", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the median shape of the dataset is \"", ",", "median_shape", ")", "\n", "\n", "max_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the max shape in the dataset is \"", ",", "max_shape", ")", "\n", "min_shape", "=", "np", ".", "min", "(", "np", ".", "vstack", "(", "new_shapes", ")", ",", "0", ")", "\n", "print", "(", "\"the min shape in the dataset is \"", ",", "min_shape", ")", "\n", "\n", "print", "(", "\"we don't want feature maps smaller than \"", ",", "FEATUREMAP_MIN_EDGE_LENGTH_BOTTLENECK", ",", "\" in the bottleneck\"", ")", "\n", "\n", "# how many stages will the image pyramid have?", "\n", "self", ".", "plans_per_stage", "=", "[", "]", "\n", "\n", "self", ".", "plans_per_stage", ".", "append", "(", "get_properties_for_stage", "(", "target_spacing", ",", "target_spacing", ",", "median_shape", ",", "\n", "num_cases", "=", "len", "(", "self", ".", "list_of_cropped_npz_files", ")", ",", "\n", "num_modalities", "=", "num_modalities", ",", "\n", "num_classes", "=", "len", "(", "all_classes", ")", "+", "1", ",", "\n", "transpose_forward", "=", "self", ".", "transpose_forward", ")", ",", "\n", ")", "\n", "\n", "print", "(", "self", ".", "plans_per_stage", ")", "\n", "\n", "self", ".", "plans_per_stage", "=", "self", ".", "plans_per_stage", "[", ":", ":", "-", "1", "]", "\n", "self", ".", "plans_per_stage", "=", "{", "i", ":", "self", ".", "plans_per_stage", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "self", ".", "plans_per_stage", ")", ")", "}", "# convert to dict", "\n", "\n", "normalization_schemes", "=", "self", ".", "determine_normalization_scheme", "(", ")", "\n", "only_keep_largest_connected_component", ",", "min_size_per_class", ",", "min_region_size_per_class", "=", "self", ".", "determine_postprocessing", "(", ")", "\n", "\n", "# these are independent of the stage", "\n", "plans", "=", "{", "'num_stages'", ":", "len", "(", "list", "(", "self", ".", "plans_per_stage", ".", "keys", "(", ")", ")", ")", ",", "'num_modalities'", ":", "num_modalities", ",", "\n", "'modalities'", ":", "modalities", ",", "'normalization_schemes'", ":", "normalization_schemes", ",", "\n", "'dataset_properties'", ":", "self", ".", "dataset_properties", ",", "'list_of_npz_files'", ":", "self", ".", "list_of_cropped_npz_files", ",", "\n", "'original_spacings'", ":", "spacings", ",", "'original_sizes'", ":", "sizes", ",", "\n", "'preprocessed_data_folder'", ":", "self", ".", "preprocessed_output_folder", ",", "'num_classes'", ":", "len", "(", "all_classes", ")", ",", "\n", "'all_classes'", ":", "all_classes", ",", "'base_num_features'", ":", "Generic_UNet", ".", "BASE_NUM_FEATURES_3D", ",", "\n", "'use_mask_for_norm'", ":", "use_nonzero_mask_for_normalization", ",", "\n", "'keep_only_largest_region'", ":", "only_keep_largest_connected_component", ",", "\n", "'min_region_size_per_class'", ":", "min_region_size_per_class", ",", "'min_size_per_class'", ":", "min_size_per_class", ",", "\n", "'transpose_forward'", ":", "self", ".", "transpose_forward", ",", "'transpose_backward'", ":", "self", ".", "transpose_backward", ",", "\n", "'data_identifier'", ":", "self", ".", "data_identifier", ",", "'plans_per_stage'", ":", "self", ".", "plans_per_stage", "}", "\n", "\n", "self", ".", "plans", "=", "plans", "\n", "self", ".", "save_my_plans", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.experiment_planner_baseline_2DUNet.ExperimentPlanner2D.run_preprocessing": [[156, 169], ["os.path.isdir", "shutil.copytree", "nnunet.preprocessing.preprocessing.PreprocessorFor2D", "nnunet.preprocessing.preprocessing.PreprocessorFor2D.run", "batchgenerators.utilities.file_and_folder_operations.join", "shutil.rmtree", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "batchgenerators.utilities.file_and_folder_operations.join", "experiment_planner_baseline_2DUNet.ExperimentPlanner2D.plans_per_stage.values"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.PreprocessorFor2D.run"], ["", "def", "run_preprocessing", "(", "self", ",", "num_threads", ")", ":", "\n", "        ", "if", "os", ".", "path", ".", "isdir", "(", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"gt_segmentations\"", ")", ")", ":", "\n", "            ", "shutil", ".", "rmtree", "(", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\"gt_segmentations\"", ")", ")", "\n", "", "shutil", ".", "copytree", "(", "join", "(", "self", ".", "folder_with_cropped_data", ",", "\"gt_segmentations\"", ")", ",", "join", "(", "self", ".", "preprocessed_output_folder", ",", "\n", "\"gt_segmentations\"", ")", ")", "\n", "normalization_schemes", "=", "self", ".", "plans", "[", "'normalization_schemes'", "]", "\n", "use_nonzero_mask_for_normalization", "=", "self", ".", "plans", "[", "'use_mask_for_norm'", "]", "\n", "intensityproperties", "=", "self", ".", "plans", "[", "'dataset_properties'", "]", "[", "'intensityproperties'", "]", "\n", "preprocessor", "=", "PreprocessorFor2D", "(", "normalization_schemes", ",", "use_nonzero_mask_for_normalization", ",", "\n", "intensityproperties", ",", "self", ".", "transpose_forward", "[", "0", "]", ")", "\n", "target_spacings", "=", "[", "i", "[", "\"current_spacing\"", "]", "for", "i", "in", "self", ".", "plans_per_stage", ".", "values", "(", ")", "]", "\n", "preprocessor", ".", "run", "(", "target_spacings", ",", "self", ".", "folder_with_cropped_data", ",", "self", ".", "preprocessed_output_folder", ",", "\n", "self", ".", "plans", "[", "'data_identifier'", "]", ",", "num_threads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.GenericPreprocessor.__init__": [[186, 196], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "normalization_scheme_per_modality", ",", "use_nonzero_mask", ",", "intensityproperties", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param normalization_scheme_per_modality: dict {0:'nonCT'}\n        :param use_nonzero_mask: {0:False}\n        :param intensityproperties:\n        \"\"\"", "\n", "self", ".", "intensityproperties", "=", "intensityproperties", "\n", "self", ".", "normalization_scheme_per_modality", "=", "normalization_scheme_per_modality", "\n", "self", ".", "use_nonzero_mask", "=", "use_nonzero_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.GenericPreprocessor.load_cropped": [[197, 205], ["all_data[].astype", "numpy.load", "open", "pickle.load", "os.path.join", "os.path.join"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "load_cropped", "(", "cropped_output_dir", ",", "case_identifier", ")", ":", "\n", "        ", "all_data", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "cropped_output_dir", ",", "\"%s.npz\"", "%", "case_identifier", ")", ")", "[", "'data'", "]", "\n", "data", "=", "all_data", "[", ":", "-", "1", "]", ".", "astype", "(", "np", ".", "float32", ")", "\n", "seg", "=", "all_data", "[", "-", "1", ":", "]", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "cropped_output_dir", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "properties", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "data", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.GenericPreprocessor.resample_and_normalize": [[206, 261], ["print", "preprocessing.resample_patient", "print", "print", "range", "print", "numpy.array", "numpy.array", "len", "len", "len", "len", "len", "numpy.clip", "numpy.clip", "[].mean", "[].std", "numpy.ones", "[].mean", "[].std"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.resample_patient"], ["", "def", "resample_and_normalize", "(", "self", ",", "data", ",", "target_spacing", ",", "properties", ",", "seg", "=", "None", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "print", "(", "\"before resample:\"", ",", "\"shape\"", ",", "data", ".", "shape", ",", "\"spacing\"", ",", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", ")", "\n", "data", ",", "seg", "=", "resample_patient", "(", "data", ",", "seg", ",", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", ",", "target_spacing", ",", "3", ",", "1", ",", "\n", "force_separate_z", "=", "force_separate_z", ",", "order_z_data", "=", "0", ",", "order_z_seg", "=", "0", ")", "\n", "print", "(", "\"after resample:\"", ",", "\"shape\"", ",", "data", ".", "shape", ",", "\"spacing\"", ",", "target_spacing", ",", "\"\\n\"", ")", "\n", "\n", "if", "seg", "is", "not", "None", ":", "# hippocampus 243 has one voxel with -2 as label. wtf?", "\n", "            ", "seg", "[", "seg", "<", "-", "1", "]", "=", "0", "\n", "\n", "", "properties", "[", "\"size_after_resampling\"", "]", "=", "data", "[", "0", "]", ".", "shape", "\n", "properties", "[", "\"spacing_after_resampling\"", "]", "=", "target_spacing", "\n", "use_nonzero_mask", "=", "self", ".", "use_nonzero_mask", "\n", "\n", "assert", "len", "(", "self", ".", "normalization_scheme_per_modality", ")", "==", "len", "(", "data", ")", ",", "\"self.normalization_scheme_per_modality \"", "\"must have as many entries as data has \"", "\"modalities\"", "\n", "assert", "len", "(", "self", ".", "use_nonzero_mask", ")", "==", "len", "(", "data", ")", ",", "\"self.use_nonzero_mask must have as many entries as data\"", "\" has modalities\"", "\n", "\n", "print", "(", "\"normalization...\"", ")", "\n", "\n", "for", "c", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "scheme", "=", "self", ".", "normalization_scheme_per_modality", "[", "c", "]", "\n", "if", "scheme", "==", "\"CT\"", ":", "\n", "# clip to lb and ub from train data foreground and use foreground mn and sd from training data", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"if there is a CT then we need intensity properties\"", "\n", "mean_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'mean'", "]", "\n", "std_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'sd'", "]", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mean_intensity", ")", "/", "std_intensity", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "elif", "scheme", "==", "\"CT2\"", ":", "\n", "# clip to lb and ub from train data foreground, use mn and sd form each case for normalization", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"if there is a CT then we need intensity properties\"", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "mask", "=", "(", "data", "[", "c", "]", ">", "lower_bound", ")", "&", "(", "data", "[", "c", "]", "<", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "mn", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", "\n", "sd", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mn", ")", "/", "sd", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "else", ":", "\n", "                ", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "mask", "=", "seg", "[", "-", "1", "]", ">=", "0", "\n", "", "else", ":", "\n", "                    ", "mask", "=", "np", ".", "ones", "(", "seg", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "bool", ")", "\n", "", "data", "[", "c", "]", "[", "mask", "]", "=", "(", "data", "[", "c", "]", "[", "mask", "]", "-", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", ")", "/", "(", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "+", "1e-8", ")", "\n", "data", "[", "c", "]", "[", "mask", "==", "0", "]", "=", "0", "\n", "", "", "print", "(", "\"normalization done\"", ")", "\n", "return", "data", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.GenericPreprocessor.preprocess_test_case": [[262, 268], ["nnunet.preprocessing.cropping.ImageCropper.crop_from_list_of_files", "preprocessing.GenericPreprocessor.resample_and_normalize", "data.astype"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.crop_from_list_of_files", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.PreprocessorFor2D.resample_and_normalize"], ["", "def", "preprocess_test_case", "(", "self", ",", "data_files", ",", "target_spacing", ",", "seg_file", "=", "None", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "data", ",", "seg", ",", "properties", "=", "ImageCropper", ".", "crop_from_list_of_files", "(", "data_files", ",", "seg_file", ")", "\n", "\n", "data", ",", "seg", ",", "properties", "=", "self", ".", "resample_and_normalize", "(", "data", ",", "target_spacing", ",", "properties", ",", "seg", ",", "\n", "force_separate_z", "=", "force_separate_z", ")", "\n", "return", "data", ".", "astype", "(", "np", ".", "float32", ")", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.GenericPreprocessor._run_star": [[269, 283], ["preprocessing.GenericPreprocessor.load_cropped", "preprocessing.GenericPreprocessor.resample_and_normalize", "numpy.vstack().astype", "print", "numpy.savez_compressed", "os.path.join", "os.path.join", "open", "pickle.dump", "numpy.vstack", "numpy.vstack().astype.astype", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.GenericPreprocessor.load_cropped", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.PreprocessorFor2D.resample_and_normalize"], ["", "def", "_run_star", "(", "self", ",", "args", ")", ":", "\n", "        ", "target_spacing", ",", "case_identifier", ",", "output_folder_stage", ",", "cropped_output_dir", ",", "force_separate_z", "=", "args", "\n", "\n", "data", ",", "seg", ",", "properties", "=", "self", ".", "load_cropped", "(", "cropped_output_dir", ",", "case_identifier", ")", "\n", "\n", "data", ",", "seg", ",", "properties", "=", "self", ".", "resample_and_normalize", "(", "data", ",", "target_spacing", ",", "\n", "properties", ",", "seg", ",", "force_separate_z", ")", "\n", "\n", "all_data", "=", "np", ".", "vstack", "(", "(", "data", ",", "seg", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "print", "(", "\"saving: \"", ",", "os", ".", "path", ".", "join", "(", "output_folder_stage", ",", "\"%s.npz\"", "%", "case_identifier", ")", ")", "\n", "np", ".", "savez_compressed", "(", "os", ".", "path", ".", "join", "(", "output_folder_stage", ",", "\"%s.npz\"", "%", "case_identifier", ")", ",", "\n", "data", "=", "all_data", ".", "astype", "(", "np", ".", "float32", ")", ")", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "output_folder_stage", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "properties", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.GenericPreprocessor.run": [[284, 318], ["print", "print", "print", "subfiles", "maybe_mkdir_p", "len", "range", "isinstance", "len", "os.path.join", "maybe_mkdir_p", "enumerate", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.map", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "nnunet.preprocessing.cropping.get_case_identifier_from_npz", "all_args.append"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_case_identifier_from_npz"], ["", "", "def", "run", "(", "self", ",", "target_spacings", ",", "input_folder_with_cropped_npz", ",", "output_folder", ",", "data_identifier", "=", "'nnUNetV2'", ",", "num_threads", "=", "8", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        :param target_spacings: list of lists [[1.25, 1.25, 5]]\n        :param input_folder_with_cropped_npz: dim: c, x, y, z | npz_file['data'] np.savez_compressed(fname.npz, data=arr)\n        :param output_folder:\n        :param num_threads:\n        :param force_separate_z: None\n        :return:\n        \"\"\"", "\n", "print", "(", "\"Initializing to run preprocessing\"", ")", "\n", "print", "(", "\"npz folder:\"", ",", "input_folder_with_cropped_npz", ")", "\n", "print", "(", "\"output_folder:\"", ",", "output_folder", ")", "\n", "list_of_cropped_npz_files", "=", "subfiles", "(", "input_folder_with_cropped_npz", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "num_stages", "=", "len", "(", "target_spacings", ")", "\n", "if", "not", "isinstance", "(", "num_threads", ",", "(", "list", ",", "tuple", ",", "np", ".", "ndarray", ")", ")", ":", "\n", "            ", "num_threads", "=", "[", "num_threads", "]", "*", "num_stages", "\n", "\n", "", "assert", "len", "(", "num_threads", ")", "==", "num_stages", "\n", "\n", "for", "i", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "all_args", "=", "[", "]", "\n", "output_folder_stage", "=", "os", ".", "path", ".", "join", "(", "output_folder", ",", "data_identifier", "+", "\"_stage%d\"", "%", "i", ")", "\n", "maybe_mkdir_p", "(", "output_folder_stage", ")", "\n", "spacing", "=", "target_spacings", "[", "i", "]", "\n", "for", "j", ",", "case", "in", "enumerate", "(", "list_of_cropped_npz_files", ")", ":", "\n", "                ", "case_identifier", "=", "get_case_identifier_from_npz", "(", "case", ")", "\n", "args", "=", "spacing", ",", "case_identifier", ",", "output_folder_stage", ",", "input_folder_with_cropped_npz", ",", "force_separate_z", "\n", "all_args", ".", "append", "(", "args", ")", "\n", "", "p", "=", "Pool", "(", "num_threads", "[", "i", "]", ")", "\n", "p", ".", "map", "(", "self", ".", "_run_star", ",", "all_args", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.PreprocessorFor2D.__init__": [[321, 326], ["preprocessing.GenericPreprocessor.__init__"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "normalization_scheme_per_modality", ",", "use_nonzero_mask", ",", "intensityproperties", "=", "None", ",", "\n", "out_of_plane_axis", "=", "0", ")", ":", "\n", "        ", "super", "(", "PreprocessorFor2D", ",", "self", ")", ".", "__init__", "(", "normalization_scheme_per_modality", ",", "use_nonzero_mask", ",", "\n", "intensityproperties", ")", "\n", "self", ".", "out_of_plane_axis", "=", "out_of_plane_axis", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.PreprocessorFor2D.run": [[327, 348], ["print", "print", "print", "subfiles", "maybe_mkdir_p", "len", "range", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.map", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "len", "os.path.join", "maybe_mkdir_p", "enumerate", "nnunet.preprocessing.cropping.get_case_identifier_from_npz", "all_args.append"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_case_identifier_from_npz"], ["", "def", "run", "(", "self", ",", "target_spacings", ",", "input_folder_with_cropped_npz", ",", "output_folder", ",", "data_identifier", "=", "'nnUNetV2'", ",", "num_threads", "=", "8", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "print", "(", "\"Initializing to run preprocessing\"", ")", "\n", "print", "(", "\"npz folder:\"", ",", "input_folder_with_cropped_npz", ")", "\n", "print", "(", "\"output_folder:\"", ",", "output_folder", ")", "\n", "list_of_cropped_npz_files", "=", "subfiles", "(", "input_folder_with_cropped_npz", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "assert", "len", "(", "list_of_cropped_npz_files", ")", "!=", "0", ",", "\"set list of files first\"", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "all_args", "=", "[", "]", "\n", "num_stages", "=", "len", "(", "target_spacings", ")", "\n", "for", "i", "in", "range", "(", "num_stages", ")", ":", "\n", "            ", "output_folder_stage", "=", "os", ".", "path", ".", "join", "(", "output_folder", ",", "data_identifier", "+", "\"_stage%d\"", "%", "i", ")", "\n", "maybe_mkdir_p", "(", "output_folder_stage", ")", "\n", "spacing", "=", "target_spacings", "[", "i", "]", "\n", "for", "j", ",", "case", "in", "enumerate", "(", "list_of_cropped_npz_files", ")", ":", "\n", "                ", "case_identifier", "=", "get_case_identifier_from_npz", "(", "case", ")", "\n", "args", "=", "spacing", ",", "case_identifier", ",", "output_folder_stage", ",", "input_folder_with_cropped_npz", ",", "force_separate_z", "\n", "all_args", ".", "append", "(", "args", ")", "\n", "", "", "p", "=", "Pool", "(", "num_threads", ")", "\n", "p", ".", "map", "(", "self", ".", "_run_star", ",", "all_args", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.PreprocessorFor2D.resample_and_normalize": [[349, 408], ["print", "numpy.array", "preprocessing.resample_patient", "print", "print", "range", "print", "numpy.array", "numpy.array", "len", "len", "len", "len", "len", "numpy.clip", "numpy.clip", "[].mean", "[].std", "numpy.ones", "[].mean", "[].std"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.resample_patient"], ["", "def", "resample_and_normalize", "(", "self", ",", "data", ",", "target_spacing", ",", "properties", ",", "seg", "=", "None", ",", "force_separate_z", "=", "None", ")", ":", "\n", "        ", "print", "(", "\"before resample:\"", ",", "\"shape\"", ",", "data", ".", "shape", ",", "\"spacing\"", ",", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", ")", "\n", "original_spacing", "=", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", "\n", "target_spacing", "[", "self", ".", "out_of_plane_axis", "]", "=", "original_spacing", "[", "self", ".", "out_of_plane_axis", "]", "\n", "\n", "data", ",", "seg", "=", "resample_patient", "(", "data", ",", "seg", ",", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", ",", "target_spacing", ",", "3", ",", "1", ",", "\n", "force_separate_z", "=", "force_separate_z", ",", "order_z_data", "=", "0", ",", "order_z_seg", "=", "0", ")", "\n", "\n", "print", "(", "\"after resample:\"", ",", "\"shape\"", ",", "data", ".", "shape", ",", "\"spacing\"", ",", "target_spacing", ",", "\"\\n\"", ")", "\n", "\n", "if", "seg", "is", "not", "None", ":", "# hippocampus 243 has one voxel with -2 as label. wtf?", "\n", "            ", "seg", "[", "seg", "<", "-", "1", "]", "=", "0", "\n", "\n", "", "properties", "[", "\"size_after_resampling\"", "]", "=", "data", "[", "0", "]", ".", "shape", "\n", "properties", "[", "\"spacing_after_resampling\"", "]", "=", "target_spacing", "\n", "use_nonzero_mask", "=", "self", ".", "use_nonzero_mask", "\n", "\n", "assert", "len", "(", "self", ".", "normalization_scheme_per_modality", ")", "==", "len", "(", "data", ")", ",", "\"self.normalization_scheme_per_modality \"", "\"must have as many entries as data has \"", "\"modalities\"", "\n", "assert", "len", "(", "self", ".", "use_nonzero_mask", ")", "==", "len", "(", "data", ")", ",", "\"self.use_nonzero_mask must have as many entries as data\"", "\" has modalities\"", "\n", "\n", "print", "(", "\"normalization...\"", ")", "\n", "\n", "for", "c", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "            ", "scheme", "=", "self", ".", "normalization_scheme_per_modality", "[", "c", "]", "\n", "if", "scheme", "==", "\"CT\"", ":", "\n", "# clip to lb and ub from train data foreground and use foreground mn and sd from training data", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"if there is a CT then we need intensity properties\"", "\n", "mean_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'mean'", "]", "\n", "std_intensity", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'sd'", "]", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mean_intensity", ")", "/", "std_intensity", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "elif", "scheme", "==", "\"CT2\"", ":", "\n", "# clip to lb and ub from train data foreground, use mn and sd form each case for normalization", "\n", "                ", "assert", "self", ".", "intensityproperties", "is", "not", "None", ",", "\"if there is a CT then we need intensity properties\"", "\n", "lower_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_00_5'", "]", "\n", "upper_bound", "=", "self", ".", "intensityproperties", "[", "c", "]", "[", "'percentile_99_5'", "]", "\n", "mask", "=", "(", "data", "[", "c", "]", ">", "lower_bound", ")", "&", "(", "data", "[", "c", "]", "<", "upper_bound", ")", "\n", "data", "[", "c", "]", "=", "np", ".", "clip", "(", "data", "[", "c", "]", ",", "lower_bound", ",", "upper_bound", ")", "\n", "mn", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", "\n", "sd", "=", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "\n", "data", "[", "c", "]", "=", "(", "data", "[", "c", "]", "-", "mn", ")", "/", "sd", "\n", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "data", "[", "c", "]", "[", "seg", "[", "-", "1", "]", "<", "0", "]", "=", "0", "\n", "", "", "else", ":", "\n", "                ", "if", "use_nonzero_mask", "[", "c", "]", ":", "\n", "                    ", "mask", "=", "seg", "[", "-", "1", "]", ">=", "0", "\n", "", "else", ":", "\n", "                    ", "mask", "=", "np", ".", "ones", "(", "seg", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "bool", ")", "\n", "", "data", "[", "c", "]", "[", "mask", "]", "=", "(", "data", "[", "c", "]", "[", "mask", "]", "-", "data", "[", "c", "]", "[", "mask", "]", ".", "mean", "(", ")", ")", "/", "(", "data", "[", "c", "]", "[", "mask", "]", ".", "std", "(", ")", "+", "1e-8", ")", "\n", "data", "[", "c", "]", "[", "mask", "==", "0", "]", "=", "0", "\n", "", "", "print", "(", "\"normalization done\"", ")", "\n", "return", "data", ",", "seg", ",", "properties", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_do_separate_z": [[26, 29], ["numpy.max", "numpy.min"], "function", ["None"], ["def", "get_do_separate_z", "(", "spacing", ")", ":", "\n", "    ", "do_separate_z", "=", "(", "np", ".", "max", "(", "spacing", ")", "/", "np", ".", "min", "(", "spacing", ")", ")", ">", "RESAMPLING_SEPARATE_Z_ANISOTROPY_THRESHOLD", "\n", "return", "do_separate_z", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_lowres_axis": [[30, 33], ["numpy.where", "max", "numpy.array"], "function", ["None"], ["", "def", "get_lowres_axis", "(", "new_spacing", ")", ":", "\n", "    ", "axis", "=", "np", ".", "where", "(", "max", "(", "new_spacing", ")", "/", "np", ".", "array", "(", "new_spacing", ")", "==", "1", ")", "[", "0", "]", "# find which axis is anisotropic", "\n", "return", "axis", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.resample_patient": [[35, 92], ["numpy.round().astype", "numpy.array", "numpy.array", "preprocessing.get_do_separate_z", "preprocessing.resample_data_or_seg", "preprocessing.resample_data_or_seg", "len", "len", "numpy.round", "preprocessing.get_lowres_axis", "preprocessing.get_lowres_axis", "preprocessing.get_do_separate_z", "preprocessing.get_lowres_axis", "numpy.array", "numpy.array"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.resample_data_or_seg", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.resample_data_or_seg", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_lowres_axis"], ["", "def", "resample_patient", "(", "data", ",", "seg", ",", "original_spacing", ",", "target_spacing", ",", "order_data", "=", "3", ",", "order_seg", "=", "0", ",", "force_separate_z", "=", "False", ",", "\n", "cval_data", "=", "0", ",", "cval_seg", "=", "-", "1", ",", "order_z_data", "=", "0", ",", "order_z_seg", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    :param cval_seg:\n    :param cval_data:\n    :param data:\n    :param seg:\n    :param original_spacing:\n    :param target_spacing:\n    :param order_data:\n    :param order_seg:\n    :param force_separate_z: if None then we dynamically decide how to resample along z, if True/False then always\n    /never resample along z separately\n    :param order_z_seg: only applies if do_separate_z is True\n    :param order_z_data: only applies if do_separate_z is True\n    :return:\n    \"\"\"", "\n", "assert", "not", "(", "(", "data", "is", "None", ")", "and", "(", "seg", "is", "None", ")", ")", "\n", "if", "data", "is", "not", "None", ":", "\n", "        ", "assert", "len", "(", "data", ".", "shape", ")", "==", "4", ",", "\"data must be c x y z\"", "\n", "", "if", "seg", "is", "not", "None", ":", "\n", "        ", "assert", "len", "(", "seg", ".", "shape", ")", "==", "4", ",", "\"seg must be c x y z\"", "\n", "\n", "", "if", "data", "is", "not", "None", ":", "\n", "        ", "shape", "=", "np", ".", "array", "(", "data", "[", "0", "]", ".", "shape", ")", "\n", "", "else", ":", "\n", "        ", "shape", "=", "np", ".", "array", "(", "seg", "[", "0", "]", ".", "shape", ")", "\n", "", "new_shape", "=", "np", ".", "round", "(", "(", "(", "np", ".", "array", "(", "original_spacing", ")", "/", "np", ".", "array", "(", "target_spacing", ")", ")", ".", "astype", "(", "float", ")", "*", "shape", ")", ")", ".", "astype", "(", "int", ")", "\n", "\n", "if", "force_separate_z", "is", "not", "None", ":", "\n", "        ", "do_separate_z", "=", "force_separate_z", "\n", "if", "force_separate_z", ":", "\n", "            ", "axis", "=", "get_lowres_axis", "(", "original_spacing", ")", "\n", "", "else", ":", "\n", "            ", "axis", "=", "None", "\n", "", "", "else", ":", "\n", "        ", "if", "get_do_separate_z", "(", "original_spacing", ")", ":", "\n", "            ", "do_separate_z", "=", "True", "\n", "axis", "=", "get_lowres_axis", "(", "original_spacing", ")", "\n", "", "elif", "get_do_separate_z", "(", "target_spacing", ")", ":", "\n", "            ", "do_separate_z", "=", "True", "\n", "axis", "=", "get_lowres_axis", "(", "target_spacing", ")", "\n", "", "else", ":", "\n", "            ", "do_separate_z", "=", "False", "\n", "axis", "=", "None", "\n", "\n", "", "", "if", "data", "is", "not", "None", ":", "\n", "        ", "data_reshaped", "=", "resample_data_or_seg", "(", "data", ",", "new_shape", ",", "False", ",", "axis", ",", "order_data", ",", "do_separate_z", ",", "cval", "=", "cval_data", ",", "\n", "order_z", "=", "order_z_data", ")", "\n", "", "else", ":", "\n", "        ", "data_reshaped", "=", "None", "\n", "", "if", "seg", "is", "not", "None", ":", "\n", "        ", "seg_reshaped", "=", "resample_data_or_seg", "(", "seg", ",", "new_shape", ",", "True", ",", "axis", ",", "order_seg", ",", "do_separate_z", ",", "cval", "=", "cval_seg", ",", "\n", "order_z", "=", "order_z_seg", ")", "\n", "", "else", ":", "\n", "        ", "seg_reshaped", "=", "None", "\n", "", "return", "data_reshaped", ",", "seg_reshaped", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.resample_data_or_seg": [[94, 183], ["data.astype.astype", "numpy.array", "numpy.array", "numpy.any", "len", "collections.OrderedDict", "np.vstack.astype", "print", "print", "range", "numpy.vstack", "range", "numpy.vstack", "len", "range", "numpy.stack", "np.zeros.append", "numpy.array", "np.vstack.append", "np.stack.append", "float", "float", "float", "np.vstack.append", "numpy.unique", "numpy.zeros", "enumerate", "np.vstack.append", "resize_fn", "resize_fn", "np.stack.append", "np.stack.append", "numpy.round", "resize_fn", "resize_fn", "scipy.ndimage.interpolation.map_coordinates", "scipy.ndimage.interpolation.map_coordinates"], "function", ["None"], ["", "def", "resample_data_or_seg", "(", "data", ",", "new_shape", ",", "is_seg", ",", "axis", "=", "None", ",", "order", "=", "3", ",", "do_separate_z", "=", "False", ",", "cval", "=", "0", ",", "order_z", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    separate_z=True will resample with order 0 along z\n    :param data:\n    :param new_shape:\n    :param is_seg:\n    :param axis:\n    :param order:\n    :param do_separate_z:\n    :param cval:\n    :param order_z: only applies if do_separate_z is True\n    :return:\n    \"\"\"", "\n", "assert", "len", "(", "data", ".", "shape", ")", "==", "4", ",", "\"data must be (c, x, y, z)\"", "\n", "if", "is_seg", ":", "\n", "        ", "resize_fn", "=", "resize_segmentation", "\n", "kwargs", "=", "OrderedDict", "(", ")", "\n", "", "else", ":", "\n", "        ", "resize_fn", "=", "resize", "\n", "kwargs", "=", "{", "'mode'", ":", "'edge'", ",", "'anti_aliasing'", ":", "False", "}", "\n", "", "dtype_data", "=", "data", ".", "dtype", "\n", "data", "=", "data", ".", "astype", "(", "float", ")", "\n", "shape", "=", "np", ".", "array", "(", "data", "[", "0", "]", ".", "shape", ")", "\n", "new_shape", "=", "np", ".", "array", "(", "new_shape", ")", "\n", "if", "np", ".", "any", "(", "shape", "!=", "new_shape", ")", ":", "\n", "        ", "if", "do_separate_z", ":", "\n", "            ", "print", "(", "\"separate z\"", ")", "\n", "assert", "len", "(", "axis", ")", "==", "1", ",", "\"only one anisotropic axis supported\"", "\n", "axis", "=", "axis", "[", "0", "]", "\n", "if", "axis", "==", "0", ":", "\n", "                ", "new_shape_2d", "=", "new_shape", "[", "1", ":", "]", "\n", "", "elif", "axis", "==", "1", ":", "\n", "                ", "new_shape_2d", "=", "new_shape", "[", "[", "0", ",", "2", "]", "]", "\n", "", "else", ":", "\n", "                ", "new_shape_2d", "=", "new_shape", "[", ":", "-", "1", "]", "\n", "\n", "", "reshaped_final_data", "=", "[", "]", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "reshaped_data", "=", "[", "]", "\n", "for", "slice_id", "in", "range", "(", "shape", "[", "axis", "]", ")", ":", "\n", "                    ", "if", "axis", "==", "0", ":", "\n", "                        ", "reshaped_data", ".", "append", "(", "resize_fn", "(", "data", "[", "c", ",", "slice_id", "]", ",", "new_shape_2d", ",", "order", ",", "cval", "=", "cval", ",", "**", "kwargs", ")", ")", "\n", "", "elif", "axis", "==", "1", ":", "\n", "                        ", "reshaped_data", ".", "append", "(", "resize_fn", "(", "data", "[", "c", ",", ":", ",", "slice_id", "]", ",", "new_shape_2d", ",", "order", ",", "cval", "=", "cval", ",", "**", "kwargs", ")", ")", "\n", "", "else", ":", "\n", "                        ", "reshaped_data", ".", "append", "(", "resize_fn", "(", "data", "[", "c", ",", ":", ",", ":", ",", "slice_id", "]", ",", "new_shape_2d", ",", "order", ",", "cval", "=", "cval", ",", "\n", "**", "kwargs", ")", ")", "\n", "", "", "reshaped_data", "=", "np", ".", "stack", "(", "reshaped_data", ",", "axis", ")", "\n", "if", "shape", "[", "axis", "]", "!=", "new_shape", "[", "axis", "]", ":", "\n", "\n", "# The following few lines are blatantly copied and modified from sklearn's resize()", "\n", "                    ", "rows", ",", "cols", ",", "dim", "=", "new_shape", "[", "0", "]", ",", "new_shape", "[", "1", "]", ",", "new_shape", "[", "2", "]", "\n", "orig_rows", ",", "orig_cols", ",", "orig_dim", "=", "reshaped_data", ".", "shape", "#data[c].shape[0], reshaped_data.shape[1], reshaped_data.shape[2]", "\n", "\n", "row_scale", "=", "float", "(", "orig_rows", ")", "/", "rows", "\n", "col_scale", "=", "float", "(", "orig_cols", ")", "/", "cols", "\n", "dim_scale", "=", "float", "(", "orig_dim", ")", "/", "dim", "\n", "\n", "map_rows", ",", "map_cols", ",", "map_dims", "=", "np", ".", "mgrid", "[", ":", "rows", ",", ":", "cols", ",", ":", "dim", "]", "\n", "map_rows", "=", "row_scale", "*", "(", "map_rows", "+", "0.5", ")", "-", "0.5", "\n", "map_cols", "=", "col_scale", "*", "(", "map_cols", "+", "0.5", ")", "-", "0.5", "\n", "map_dims", "=", "dim_scale", "*", "(", "map_dims", "+", "0.5", ")", "-", "0.5", "\n", "\n", "coord_map", "=", "np", ".", "array", "(", "[", "map_rows", ",", "map_cols", ",", "map_dims", "]", ")", "\n", "if", "not", "is_seg", "or", "order_z", "==", "0", ":", "\n", "                        ", "reshaped_final_data", ".", "append", "(", "map_coordinates", "(", "reshaped_data", ",", "coord_map", ",", "order", "=", "order_z", ",", "cval", "=", "cval", ",", "\n", "mode", "=", "'nearest'", ")", "[", "None", "]", ")", "\n", "", "else", ":", "\n", "                        ", "unique_labels", "=", "np", ".", "unique", "(", "reshaped_data", ")", "\n", "reshaped", "=", "np", ".", "zeros", "(", "new_shape", ",", "dtype", "=", "dtype_data", ")", "\n", "\n", "for", "i", ",", "cl", "in", "enumerate", "(", "unique_labels", ")", ":", "\n", "                            ", "reshaped_multihot", "=", "np", ".", "round", "(", "\n", "map_coordinates", "(", "(", "reshaped_data", "==", "cl", ")", ".", "astype", "(", "float", ")", ",", "coord_map", ",", "order", "=", "order_z", ",", "\n", "cval", "=", "cval", ",", "mode", "=", "'nearest'", ")", ")", "\n", "reshaped", "[", "reshaped_multihot", ">=", "0.5", "]", "=", "cl", "\n", "", "reshaped_final_data", ".", "append", "(", "reshaped", "[", "None", "]", ")", "\n", "", "", "else", ":", "\n", "                    ", "reshaped_final_data", ".", "append", "(", "reshaped_data", "[", "None", "]", ")", "\n", "", "", "reshaped_final_data", "=", "np", ".", "vstack", "(", "reshaped_final_data", ")", "\n", "", "else", ":", "\n", "            ", "reshaped", "=", "[", "]", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "reshaped", ".", "append", "(", "resize_fn", "(", "data", "[", "c", "]", ",", "new_shape", ",", "order", ",", "cval", "=", "cval", ",", "**", "kwargs", ")", "[", "None", "]", ")", "\n", "", "reshaped_final_data", "=", "np", ".", "vstack", "(", "reshaped", ")", "\n", "", "return", "reshaped_final_data", ".", "astype", "(", "dtype_data", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"no resampling necessary\"", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.__init__": [[124, 137], ["maybe_mkdir_p"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_threads", ",", "output_folder", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        This one finds a mask of nonzero elements (must be nonzero in all modalities) and crops the image to that mask.\n        In the case of BRaTS and ISLES data this results in a significant reduction in image size\n        :param num_threads:\n        :param output_folder: whete to store the cropped data\n        :param list_of_files:\n        \"\"\"", "\n", "self", ".", "output_folder", "=", "output_folder", "\n", "self", ".", "num_threads", "=", "num_threads", "\n", "\n", "if", "self", ".", "output_folder", "is", "not", "None", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "# print('!!!!!!!output_folder in ImageCropper is:',self.output_folder,'!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.crop": [[139, 152], ["cropping.crop_to_nonzero", "print", "numpy.unique", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.crop_to_nonzero"], ["", "", "@", "staticmethod", "\n", "def", "crop", "(", "data", ",", "properties", ",", "seg", "=", "None", ")", ":", "\n", "        ", "shape_before", "=", "data", ".", "shape", "\n", "data", ",", "seg", ",", "bbox", "=", "crop_to_nonzero", "(", "data", ",", "seg", ",", "nonzero_label", "=", "-", "1", ")", "\n", "shape_after", "=", "data", ".", "shape", "\n", "print", "(", "\"before crop:\"", ",", "shape_before", ",", "\"after crop:\"", ",", "shape_after", ",", "\"spacing:\"", ",", "\n", "np", ".", "array", "(", "properties", "[", "\"original_spacing\"", "]", ")", ",", "\"\\n\"", ")", "\n", "\n", "properties", "[", "\"crop_bbox\"", "]", "=", "bbox", "\n", "properties", "[", "'classes'", "]", "=", "np", ".", "unique", "(", "seg", ")", "\n", "seg", "[", "seg", "<", "-", "1", "]", "=", "0", "\n", "properties", "[", "\"size_after_cropping\"", "]", "=", "data", "[", "0", "]", ".", "shape", "\n", "return", "data", ",", "seg", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.crop_from_list_of_files": [[153, 157], ["cropping.load_case_from_list_of_files", "cropping.ImageCropper.crop"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.load_case_from_list_of_files", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.crop"], ["", "@", "staticmethod", "\n", "def", "crop_from_list_of_files", "(", "data_files", ",", "seg_file", "=", "None", ")", ":", "\n", "        ", "data", ",", "seg", ",", "properties", "=", "load_case_from_list_of_files", "(", "data_files", ",", "seg_file", ")", "\n", "return", "ImageCropper", ".", "crop", "(", "data", ",", "properties", ",", "seg", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.load_crop_save": [[158, 176], ["print", "cropping.ImageCropper.crop_from_list_of_files", "numpy.vstack", "numpy.savez_compressed", "os.path.join", "open", "pickle.dump", "os.path.isfile", "os.path.isfile", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.crop_from_list_of_files"], ["", "def", "load_crop_save", "(", "self", ",", "case", ",", "case_identifier", ",", "overwrite_existing", "=", "False", ")", ":", "\n", "        ", "print", "(", "case_identifier", ")", "\n", "if", "overwrite_existing", "or", "(", "not", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.npz\"", "%", "case_identifier", ")", ")", "\n", "or", "not", "os", ".", "path", ".", "isfile", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ")", ")", ":", "\n", "\n", "            ", "data", ",", "seg", ",", "properties", "=", "self", ".", "crop_from_list_of_files", "(", "case", "[", ":", "-", "1", "]", ",", "case", "[", "-", "1", "]", ")", "\n", "\n", "all_data", "=", "np", ".", "vstack", "(", "(", "data", ",", "seg", ")", ")", "\n", "# print('!!!!!!!case_identifier is:',case_identifier,'&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&&')", "\n", "# print('!!!!!!!output_folder is:',self.output_folder,'#####################################')", "\n", "# print('!!!!!!! Before npz file is saved in:', os.path.join(self.output_folder, \"%s.npz\" % case_identifier),", "\n", "#       '***********************************')", "\n", "np", ".", "savez_compressed", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.npz\"", "%", "case_identifier", ")", ",", "data", "=", "all_data", ")", "\n", "\n", "# print('!!!!!!! After npz file is saved in:',os.path.join(self.output_folder, \"%s.npz\" % case_identifier),'***********************************')", "\n", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "                ", "pickle", ".", "dump", "(", "properties", ",", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper._load_crop_save_star": [[177, 179], ["cropping.ImageCropper.load_crop_save"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.load_crop_save"], ["", "", "", "def", "_load_crop_save_star", "(", "self", ",", "args", ")", ":", "\n", "        ", "return", "self", ".", "load_crop_save", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.get_list_of_cropped_files": [[180, 182], ["subfiles"], "methods", ["None"], ["", "def", "get_list_of_cropped_files", "(", "self", ")", ":", "\n", "        ", "return", "subfiles", "(", "self", ".", "output_folder", ",", "join", "=", "True", ",", "suffix", "=", "\".npz\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.get_patient_identifiers_from_cropped_files": [[183, 185], ["cropping.ImageCropper.get_list_of_cropped_files", "i.split"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.get_list_of_cropped_files"], ["", "def", "get_patient_identifiers_from_cropped_files", "(", "self", ")", ":", "\n", "        ", "return", "[", "i", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "4", "]", "for", "i", "in", "self", ".", "get_list_of_cropped_files", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.run_cropping": [[186, 213], ["os.path.join", "maybe_mkdir_p", "enumerate", "enumerate", "multiprocessing.Pool", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "cropping.get_case_identifier", "list_of_args.append", "shutil.copy"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_case_identifier"], ["", "def", "run_cropping", "(", "self", ",", "list_of_files", ",", "overwrite_existing", "=", "False", ",", "output_folder", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        also copied ground truth nifti segmentation into the preprocessed folder so that we can use them for evaluation\n        on the cluster\n        :param list_of_files: list of list of files [[PATIENTID_TIMESTEP_0000.nii.gz], [PATIENTID_TIMESTEP_0000.nii.gz]]\n        :param overwrite_existing:\n        :param output_folder:\n        :return:\n        \"\"\"", "\n", "if", "output_folder", "is", "not", "None", ":", "\n", "            ", "self", ".", "output_folder", "=", "output_folder", "\n", "\n", "", "output_folder_gt", "=", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"gt_segmentations\"", ")", "\n", "maybe_mkdir_p", "(", "output_folder_gt", ")", "\n", "for", "j", ",", "case", "in", "enumerate", "(", "list_of_files", ")", ":", "\n", "            ", "if", "case", "[", "-", "1", "]", "is", "not", "None", ":", "\n", "                ", "shutil", ".", "copy", "(", "case", "[", "-", "1", "]", ",", "output_folder_gt", ")", "\n", "\n", "", "", "list_of_args", "=", "[", "]", "\n", "for", "j", ",", "case", "in", "enumerate", "(", "list_of_files", ")", ":", "\n", "            ", "case_identifier", "=", "get_case_identifier", "(", "case", ")", "\n", "list_of_args", ".", "append", "(", "(", "case", ",", "case_identifier", ",", "overwrite_existing", ")", ")", "\n", "\n", "", "p", "=", "Pool", "(", "self", ".", "num_threads", ")", "\n", "p", ".", "map", "(", "self", ".", "_load_crop_save_star", ",", "list_of_args", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.load_properties": [[214, 218], ["open", "pickle.load", "os.path.join"], "methods", ["None"], ["", "def", "load_properties", "(", "self", ",", "case_identifier", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "properties", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "return", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.ImageCropper.save_properties": [[219, 222], ["open", "pickle.dump", "os.path.join"], "methods", ["None"], ["", "def", "save_properties", "(", "self", ",", "case_identifier", ",", "properties", ")", ":", "\n", "        ", "with", "open", "(", "os", ".", "path", ".", "join", "(", "self", ".", "output_folder", ",", "\"%s.pkl\"", "%", "case_identifier", ")", ",", "'wb'", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "properties", ",", "f", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.create_nonzero_mask": [[23, 32], ["numpy.zeros", "range", "binary_fill_holes", "len", "len"], "function", ["None"], ["def", "create_nonzero_mask", "(", "data", ")", ":", "\n", "    ", "from", "scipy", ".", "ndimage", "import", "binary_fill_holes", "\n", "assert", "len", "(", "data", ".", "shape", ")", "==", "4", "or", "len", "(", "data", ".", "shape", ")", "==", "3", ",", "\"data must have shape (C, X, Y, Z) or shape (C, X, Y)\"", "\n", "nonzero_mask", "=", "np", ".", "zeros", "(", "data", ".", "shape", "[", "1", ":", "]", ",", "dtype", "=", "bool", ")", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "this_mask", "=", "data", "[", "c", "]", "!=", "0", "\n", "nonzero_mask", "=", "nonzero_mask", "|", "this_mask", "\n", "", "nonzero_mask", "=", "binary_fill_holes", "(", "nonzero_mask", ")", "\n", "return", "nonzero_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_bbox_from_mask": [[34, 43], ["numpy.where", "int", "int", "int", "numpy.min", "int", "numpy.min", "int", "numpy.min", "int", "numpy.max", "numpy.max", "numpy.max"], "function", ["None"], ["", "def", "get_bbox_from_mask", "(", "mask", ",", "outside_value", "=", "0", ")", ":", "\n", "    ", "mask_voxel_coords", "=", "np", ".", "where", "(", "mask", "!=", "outside_value", ")", "\n", "minzidx", "=", "int", "(", "np", ".", "min", "(", "mask_voxel_coords", "[", "0", "]", ")", ")", "\n", "maxzidx", "=", "int", "(", "np", ".", "max", "(", "mask_voxel_coords", "[", "0", "]", ")", ")", "+", "1", "\n", "minxidx", "=", "int", "(", "np", ".", "min", "(", "mask_voxel_coords", "[", "1", "]", ")", ")", "\n", "maxxidx", "=", "int", "(", "np", ".", "max", "(", "mask_voxel_coords", "[", "1", "]", ")", ")", "+", "1", "\n", "minyidx", "=", "int", "(", "np", ".", "min", "(", "mask_voxel_coords", "[", "2", "]", ")", ")", "\n", "maxyidx", "=", "int", "(", "np", ".", "max", "(", "mask_voxel_coords", "[", "2", "]", ")", ")", "+", "1", "\n", "return", "[", "[", "minzidx", ",", "maxzidx", "]", ",", "[", "minxidx", ",", "maxxidx", "]", ",", "[", "minyidx", ",", "maxyidx", "]", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.crop_to_bbox": [[45, 49], ["len", "slice", "slice", "slice"], "function", ["None"], ["", "def", "crop_to_bbox", "(", "image", ",", "bbox", ")", ":", "\n", "    ", "assert", "len", "(", "image", ".", "shape", ")", "==", "3", ",", "\"only supports 3d images\"", "\n", "resizer", "=", "(", "slice", "(", "bbox", "[", "0", "]", "[", "0", "]", ",", "bbox", "[", "0", "]", "[", "1", "]", ")", ",", "slice", "(", "bbox", "[", "1", "]", "[", "0", "]", ",", "bbox", "[", "1", "]", "[", "1", "]", ")", ",", "slice", "(", "bbox", "[", "2", "]", "[", "0", "]", ",", "bbox", "[", "2", "]", "[", "1", "]", ")", ")", "\n", "return", "image", "[", "resizer", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_case_identifier": [[51, 54], ["[].split", "case[].split"], "function", ["None"], ["", "def", "get_case_identifier", "(", "case", ")", ":", "\n", "    ", "case_identifier", "=", "case", "[", "0", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "split", "(", "\".nii.gz\"", ")", "[", "0", "]", "[", ":", "-", "5", "]", "\n", "return", "case_identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_case_identifier_from_npz": [[56, 59], ["case.split"], "function", ["None"], ["", "def", "get_case_identifier_from_npz", "(", "case", ")", ":", "\n", "    ", "case_identifier", "=", "case", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "4", "]", "\n", "return", "case_identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.load_case_from_list_of_files": [[61, 82], ["collections.OrderedDict", "data_itk[].GetOrigin", "data_itk[].GetSpacing", "data_itk[].GetDirection", "numpy.vstack", "isinstance", "isinstance", "SimpleITK.ReadImage", "numpy.array", "numpy.array", "SimpleITK.ReadImage", "[].astype", "np.vstack.astype", "data_itk[].GetSize", "data_itk[].GetSpacing", "SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage"], "function", ["None"], ["", "def", "load_case_from_list_of_files", "(", "data_files", ",", "seg_file", "=", "None", ")", ":", "\n", "    ", "assert", "isinstance", "(", "data_files", ",", "list", ")", "or", "isinstance", "(", "data_files", ",", "tuple", ")", ",", "\"case must be either a list or a tuple\"", "\n", "properties", "=", "OrderedDict", "(", ")", "\n", "data_itk", "=", "[", "sitk", ".", "ReadImage", "(", "f", ")", "for", "f", "in", "data_files", "]", "\n", "\n", "properties", "[", "\"original_size_of_raw_data\"", "]", "=", "np", ".", "array", "(", "data_itk", "[", "0", "]", ".", "GetSize", "(", ")", ")", "[", "[", "2", ",", "1", ",", "0", "]", "]", "\n", "properties", "[", "\"original_spacing\"", "]", "=", "np", ".", "array", "(", "data_itk", "[", "0", "]", ".", "GetSpacing", "(", ")", ")", "[", "[", "2", ",", "1", ",", "0", "]", "]", "\n", "properties", "[", "\"list_of_data_files\"", "]", "=", "data_files", "\n", "properties", "[", "\"seg_file\"", "]", "=", "seg_file", "\n", "\n", "properties", "[", "\"itk_origin\"", "]", "=", "data_itk", "[", "0", "]", ".", "GetOrigin", "(", ")", "\n", "properties", "[", "\"itk_spacing\"", "]", "=", "data_itk", "[", "0", "]", ".", "GetSpacing", "(", ")", "\n", "properties", "[", "\"itk_direction\"", "]", "=", "data_itk", "[", "0", "]", ".", "GetDirection", "(", ")", "\n", "\n", "data_npy", "=", "np", ".", "vstack", "(", "[", "sitk", ".", "GetArrayFromImage", "(", "d", ")", "[", "None", "]", "for", "d", "in", "data_itk", "]", ")", "\n", "if", "seg_file", "is", "not", "None", ":", "\n", "        ", "seg_itk", "=", "sitk", ".", "ReadImage", "(", "seg_file", ")", "\n", "seg_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "seg_itk", ")", "[", "None", "]", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "        ", "seg_npy", "=", "None", "\n", "", "return", "data_npy", ".", "astype", "(", "np", ".", "float32", ")", ",", "seg_npy", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.crop_to_nonzero": [[84, 117], ["cropping.create_nonzero_mask", "cropping.get_bbox_from_mask", "range", "numpy.vstack", "cropping.crop_to_bbox", "cropped_data.append", "range", "numpy.vstack", "cropping.crop_to_bbox", "nonzero_mask.astype.astype", "cropping.crop_to_bbox", "cropped_seg.append"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.create_nonzero_mask", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_bbox_from_mask", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.crop_to_bbox", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.crop_to_bbox", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.crop_to_bbox"], ["", "def", "crop_to_nonzero", "(", "data", ",", "seg", "=", "None", ",", "nonzero_label", "=", "-", "1", ")", ":", "\n", "    ", "\"\"\"\n\n    :param data:\n    :param seg:\n    :param nonzero_label: this will be written into the segmentation map\n    :return:\n    \"\"\"", "\n", "nonzero_mask", "=", "create_nonzero_mask", "(", "data", ")", "\n", "bbox", "=", "get_bbox_from_mask", "(", "nonzero_mask", ",", "0", ")", "\n", "\n", "cropped_data", "=", "[", "]", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "cropped", "=", "crop_to_bbox", "(", "data", "[", "c", "]", ",", "bbox", ")", "\n", "cropped_data", ".", "append", "(", "cropped", "[", "None", "]", ")", "\n", "", "data", "=", "np", ".", "vstack", "(", "cropped_data", ")", "\n", "\n", "if", "seg", "is", "not", "None", ":", "\n", "        ", "cropped_seg", "=", "[", "]", "\n", "for", "c", "in", "range", "(", "seg", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "cropped", "=", "crop_to_bbox", "(", "seg", "[", "c", "]", ",", "bbox", ")", "\n", "cropped_seg", ".", "append", "(", "cropped", "[", "None", "]", ")", "\n", "", "seg", "=", "np", ".", "vstack", "(", "cropped_seg", ")", "\n", "\n", "", "nonzero_mask", "=", "crop_to_bbox", "(", "nonzero_mask", ",", "bbox", ")", "[", "None", "]", "\n", "if", "seg", "is", "not", "None", ":", "\n", "        ", "seg", "[", "(", "seg", "==", "0", ")", "&", "(", "nonzero_mask", "==", "0", ")", "]", "=", "nonzero_label", "\n", "", "else", ":", "\n", "        ", "nonzero_mask", "=", "nonzero_mask", ".", "astype", "(", "int", ")", "\n", "nonzero_mask", "[", "nonzero_mask", "==", "0", "]", "=", "nonzero_label", "\n", "nonzero_mask", "[", "nonzero_mask", ">", "0", "]", "=", "0", "\n", "seg", "=", "nonzero_mask", "\n", "", "return", "data", ",", "seg", ",", "bbox", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.cropping.get_patient_identifiers_from_cropped_files": [[119, 121], ["subfiles", "i.split"], "function", ["None"], ["", "def", "get_patient_identifiers_from_cropped_files", "(", "folder", ")", ":", "\n", "    ", "return", "[", "i", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "4", "]", "for", "i", "in", "subfiles", "(", "folder", ",", "join", "=", "True", ",", "suffix", "=", "\".npz\"", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.ConvDropoutNormNonlin.__init__": [[26, 58], ["torch.nn.Module.__init__", "generic_UNet.ConvDropoutNormNonlin.conv_op", "generic_UNet.ConvDropoutNormNonlin.norm_op", "torch.nn.LeakyReLU", "torch.nn.LeakyReLU", "generic_UNet.ConvDropoutNormNonlin.dropout_op"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_channels", ",", "output_channels", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ")", ":", "\n", "        ", "super", "(", "ConvDropoutNormNonlin", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "self", ".", "conv", "=", "self", ".", "conv_op", "(", "input_channels", ",", "output_channels", ",", "**", "self", ".", "conv_kwargs", ")", "\n", "if", "self", ".", "dropout_op", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "is", "not", "None", "and", "self", ".", "dropout_op_kwargs", "[", "\n", "'p'", "]", ">", "0", ":", "\n", "            ", "self", ".", "dropout", "=", "self", ".", "dropout_op", "(", "**", "self", ".", "dropout_op_kwargs", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "dropout", "=", "None", "\n", "", "self", ".", "instnorm", "=", "self", ".", "norm_op", "(", "output_channels", ",", "**", "self", ".", "norm_op_kwargs", ")", "\n", "self", ".", "lrelu", "=", "nn", ".", "LeakyReLU", "(", "**", "self", ".", "nonlin_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.ConvDropoutNormNonlin.forward": [[59, 64], ["generic_UNet.ConvDropoutNormNonlin.conv", "generic_UNet.ConvDropoutNormNonlin.lrelu", "generic_UNet.ConvDropoutNormNonlin.dropout", "generic_UNet.ConvDropoutNormNonlin.instnorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "conv", "(", "x", ")", "\n", "if", "self", ".", "dropout", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "dropout", "(", "x", ")", "\n", "", "return", "self", ".", "lrelu", "(", "self", ".", "instnorm", "(", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.StackedConvLayers.__init__": [[67, 127], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Sequential", "copy.deepcopy", "generic_UNet.ConvDropoutNormNonlin", "generic_UNet.ConvDropoutNormNonlin", "range"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_feature_channels", ",", "output_feature_channels", ",", "num_convs", ",", "\n", "conv_op", "=", "nn", ".", "Conv2d", ",", "conv_kwargs", "=", "None", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "first_stride", "=", "None", ")", ":", "\n", "        ", "'''\n        stacks ConvDropoutNormLReLU layers. initial_stride will only be applied to first layer in the stack. The other parameters affect all layers\n        :param input_feature_channels:\n        :param output_feature_channels:\n        :param num_convs:\n        :param dilation:\n        :param kernel_size:\n        :param padding:\n        :param dropout:\n        :param initial_stride:\n        :param conv_op:\n        :param norm_op:\n        :param dropout_op:\n        :param inplace:\n        :param neg_slope:\n        :param norm_affine:\n        :param conv_bias:\n        '''", "\n", "self", ".", "input_channels", "=", "input_feature_channels", "\n", "self", ".", "output_channels", "=", "output_feature_channels", "\n", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "            ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "", "if", "conv_kwargs", "is", "None", ":", "\n", "            ", "conv_kwargs", "=", "{", "'kernel_size'", ":", "3", ",", "'stride'", ":", "1", ",", "'padding'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "conv_kwargs", "=", "conv_kwargs", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "\n", "if", "first_stride", "is", "not", "None", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "deepcopy", "(", "conv_kwargs", ")", "\n", "self", ".", "conv_kwargs_first_conv", "[", "'stride'", "]", "=", "first_stride", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv_kwargs_first_conv", "=", "conv_kwargs", "\n", "\n", "", "super", "(", "StackedConvLayers", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "blocks", "=", "nn", ".", "Sequential", "(", "\n", "*", "(", "[", "ConvDropoutNormNonlin", "(", "input_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs_first_conv", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "]", "+", "\n", "[", "ConvDropoutNormNonlin", "(", "output_feature_channels", ",", "output_feature_channels", ",", "self", ".", "conv_op", ",", "\n", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "for", "_", "in", "range", "(", "num_convs", "-", "1", ")", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.StackedConvLayers.forward": [[128, 130], ["generic_UNet.StackedConvLayers.blocks"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "blocks", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.Upsample.__init__": [[141, 147], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", "=", "None", ",", "scale_factor", "=", "None", ",", "mode", "=", "'nearest'", ",", "align_corners", "=", "False", ")", ":", "\n", "        ", "super", "(", "Upsample", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "align_corners", "=", "align_corners", "\n", "self", ".", "mode", "=", "mode", "\n", "self", ".", "scale_factor", "=", "scale_factor", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.Upsample.forward": [[148, 150], ["torch.nn.functional.interpolate", "torch.nn.functional.interpolate"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "nn", ".", "functional", ".", "interpolate", "(", "x", ",", "size", "=", "self", ".", "size", ",", "scale_factor", "=", "self", ".", "scale_factor", ",", "mode", "=", "self", ".", "mode", ",", "align_corners", "=", "self", ".", "align_corners", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.Generic_UNet.__init__": [[169, 450], ["nnunet.network_architecture.initialization.InitWeights_He", "nnunet.network_architecture.neural_network.SegmentationNetwork.__init__", "numpy.prod", "print", "range", "generic_UNet.Generic_UNet.conv_blocks_context.append", "range", "range", "range", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_UNet.Generic_UNet.conv_pad_sizes.append", "generic_UNet.Generic_UNet.conv_blocks_context.append", "int", "torch.nn.Sequential", "torch.nn.Sequential", "generic_UNet.Generic_UNet.conv_blocks_localization.append", "len", "generic_UNet.Generic_UNet.seg_outputs.append", "numpy.cumprod", "torch.nn.ModuleList", "torch.nn.ModuleList", "generic_UNet.Generic_UNet.apply", "ValueError", "generic_UNet.StackedConvLayers", "generic_UNet.Generic_UNet.td.append", "numpy.round", "min", "min", "generic_UNet.StackedConvLayers", "generic_UNet.StackedConvLayers", "generic_UNet.Generic_UNet.tu.append", "generic_UNet.Generic_UNet.tu.append", "torch.nn.Sequential", "torch.nn.Sequential", "conv_op", "numpy.vstack", "generic_UNet.Generic_UNet.upscale_logits_ops.append", "generic_UNet.Generic_UNet.upscale_logits_ops.append", "pool_op", "generic_UNet.Upsample", "transpconv", "generic_UNet.StackedConvLayers", "generic_UNet.StackedConvLayers", "generic_UNet.Upsample", "str", "tuple", "int"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["def", "__init__", "(", "self", ",", "input_channels", ",", "base_num_features", ",", "num_classes", ",", "num_pool", ",", "num_conv_per_stage", "=", "2", ",", "\n", "feat_map_mul_on_downscale", "=", "2", ",", "conv_op", "=", "nn", ".", "Conv2d", ",", "\n", "norm_op", "=", "nn", ".", "BatchNorm2d", ",", "norm_op_kwargs", "=", "None", ",", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", ",", "dropout_op_kwargs", "=", "None", ",", "\n", "nonlin", "=", "nn", ".", "LeakyReLU", ",", "nonlin_kwargs", "=", "None", ",", "deep_supervision", "=", "True", ",", "dropout_in_localization", "=", "False", ",", "\n", "final_nonlin", "=", "softmax_helper", ",", "weightInitializer", "=", "InitWeights_He", "(", "1e-2", ")", ",", "pool_op_kernel_sizes", "=", "None", ",", "\n", "conv_kernel_sizes", "=", "None", ",", "\n", "upscale_logits", "=", "False", ",", "convolutional_pooling", "=", "False", ",", "convolutional_upsampling", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        self.network = Generic_UNet(self.num_input_channels, self.base_num_features, self.num_classes, net_numpool,\n                                    2, 2, conv_op, norm_op, norm_op_kwargs, dropout_op, dropout_op_kwargs,\n                                    net_nonlin, net_nonlin_kwargs, False, False, lambda x: x, InitWeights_He(1e-2),\n                                    self.net_num_pool_op_kernel_sizes, self.net_conv_kernel_sizes, False, True, True)\n        :param input_channels:\n        :param base_num_features:\n        :param num_classes:\n        :param num_pool:\n        :param num_conv_per_stage:\n        :param feat_map_mul_on_downscale:\n        :param conv_op:\n        :param norm_op:\n        :param norm_op_kwargs:\n        :param dropout_op:\n        :param dropout_op_kwargs:\n        :param nonlin:\n        :param nonlin_kwargs:\n        :param deep_supervision:\n        :param dropout_in_localization:\n        :param final_nonlin:\n        :param weightInitializer:\n        :param pool_op_kernel_sizes:\n        :param conv_kernel_sizes:\n        :param upscale_logits:\n        :param convolutional_pooling:\n        :param convolutional_upsampling:\n        \"\"\"", "\n", "\n", "\n", "\"\"\"\n        basically more flexible than v1, architecture is the same\n\n        Does this look complicated? Nah bro. Functionality > usability\n\n        This does everything you need, including world peace.\n\n        Questions? -> f.isensee@dkfz.de\n        \"\"\"", "\n", "super", "(", "Generic_UNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "convolutional_upsampling", "=", "convolutional_upsampling", "\n", "self", ".", "convolutional_pooling", "=", "convolutional_pooling", "\n", "self", ".", "upscale_logits", "=", "upscale_logits", "\n", "if", "nonlin_kwargs", "is", "None", ":", "\n", "             ", "nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "dropout_op_kwargs", "is", "None", ":", "\n", "            ", "dropout_op_kwargs", "=", "{", "'p'", ":", "0.5", ",", "'inplace'", ":", "True", "}", "\n", "", "if", "norm_op_kwargs", "is", "None", ":", "\n", "            ", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", ",", "'momentum'", ":", "0.1", "}", "\n", "\n", "", "self", ".", "conv_kwargs", "=", "{", "'stride'", ":", "1", ",", "'dilation'", ":", "1", ",", "'bias'", ":", "True", "}", "\n", "\n", "self", ".", "nonlin", "=", "nonlin", "\n", "self", ".", "nonlin_kwargs", "=", "nonlin_kwargs", "\n", "self", ".", "dropout_op_kwargs", "=", "dropout_op_kwargs", "\n", "self", ".", "norm_op_kwargs", "=", "norm_op_kwargs", "\n", "self", ".", "weightInitializer", "=", "weightInitializer", "\n", "self", ".", "conv_op", "=", "conv_op", "\n", "self", ".", "norm_op", "=", "norm_op", "\n", "self", ".", "dropout_op", "=", "dropout_op", "\n", "self", ".", "num_classes", "=", "num_classes", "\n", "self", ".", "final_nonlin", "=", "final_nonlin", "\n", "self", ".", "do_ds", "=", "deep_supervision", "\n", "\n", "if", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "            ", "upsample_mode", "=", "'bilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool2d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose2d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "elif", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "upsample_mode", "=", "'trilinear'", "\n", "pool_op", "=", "nn", ".", "MaxPool3d", "\n", "transpconv", "=", "nn", ".", "ConvTranspose3d", "\n", "if", "pool_op_kernel_sizes", "is", "None", ":", "\n", "                ", "pool_op_kernel_sizes", "=", "[", "(", "2", ",", "2", ",", "2", ")", "]", "*", "num_pool", "\n", "", "if", "conv_kernel_sizes", "is", "None", ":", "\n", "                ", "conv_kernel_sizes", "=", "[", "(", "3", ",", "3", ",", "3", ")", "]", "*", "(", "num_pool", "+", "1", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"unknown convolution dimensionality, conv op: %s\"", "%", "str", "(", "conv_op", ")", ")", "\n", "\n", "", "self", ".", "input_shape_must_be_divisible_by", "=", "np", ".", "prod", "(", "pool_op_kernel_sizes", ",", "0", ")", "\n", "self", ".", "pool_op_kernel_sizes", "=", "pool_op_kernel_sizes", "\n", "self", ".", "conv_kernel_sizes", "=", "conv_kernel_sizes", "\n", "\n", "self", ".", "conv_pad_sizes", "=", "[", "]", "\n", "for", "krnl", "in", "self", ".", "conv_kernel_sizes", ":", "\n", "            ", "self", ".", "conv_pad_sizes", ".", "append", "(", "[", "1", "if", "i", "==", "3", "else", "0", "for", "i", "in", "krnl", "]", ")", "\n", "\n", "", "self", ".", "conv_blocks_context", "=", "[", "]", "\n", "self", ".", "conv_blocks_localization", "=", "[", "]", "\n", "self", ".", "td", "=", "[", "]", "\n", "self", ".", "tu", "=", "[", "]", "\n", "self", ".", "seg_outputs", "=", "[", "]", "\n", "\n", "output_features", "=", "base_num_features", "\n", "input_features", "=", "input_channels", "\n", "\n", "print", "(", "'Has entered into the Generic_UNet initialization!'", ")", "\n", "\n", "\"\"\"\n        time means 'd 'as follows:\n        self.convolutional_pooling is: True\n        first_stride of 0 time is: None\n        output_features of 0 time is: 30\n        self.convolutional_pooling is: True\n        first_stride of 1 time is: [2, 2, 1]\n        output_features of 1 time is: 60\n        self.convolutional_pooling is: True\n        first_stride of 2 time is: [2, 2, 1]\n        output_features of 2 time is: 120\n        self.convolutional_pooling is: True\n        first_stride of 3 time is: [2, 2, 2]\n        output_features of 3 time is: 240\n        self.convolutional_pooling is: True\n        first_stride of 4 time is: [2, 2, 2]\n        output_features of 4 time is: 320\n        \"\"\"", "\n", "\n", "for", "d", "in", "range", "(", "num_pool", ")", ":", "\n", "# determine the first stride", "\n", "# print('self.convolutional_pooling is:',self.convolutional_pooling)", "\n", "            ", "if", "d", "!=", "0", "and", "self", ".", "convolutional_pooling", ":", "\n", "                ", "first_stride", "=", "pool_op_kernel_sizes", "[", "d", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "first_stride", "=", "None", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "d", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "d", "]", "\n", "# add convolutions", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "\n", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ",", "\n", "first_stride", ")", ")", "\n", "# print('self.conv_blocks_context of %d time is:'%d, self.conv_blocks_context)", "\n", "# print('first_stride of %d time is:'%d,first_stride)", "\n", "# print('output_features of %d time is:'%d,output_features)", "\n", "\"\"\"\n            class StackedConvLayers(nn.Module):\n            def __init__(self, input_feature_channels, output_feature_channels, num_convs,\n                 conv_op=nn.Conv2d, conv_kwargs=None,\n                 norm_op=nn.BatchNorm2d, norm_op_kwargs=None,\n                 dropout_op=nn.Dropout2d, dropout_op_kwargs=None,\n                 nonlin=nn.LeakyReLU, nonlin_kwargs=None, first_stride=None)\n            \"\"\"", "\n", "if", "not", "self", ".", "convolutional_pooling", ":", "\n", "                ", "self", ".", "td", ".", "append", "(", "pool_op", "(", "pool_op_kernel_sizes", "[", "d", "]", ")", ")", "\n", "", "input_features", "=", "output_features", "\n", "output_features", "=", "int", "(", "np", ".", "round", "(", "output_features", "*", "feat_map_mul_on_downscale", ")", ")", "\n", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "                ", "output_features", "=", "min", "(", "output_features", ",", "self", ".", "MAX_NUM_FILTERS_3D", ")", "\n", "", "else", ":", "\n", "                ", "output_features", "=", "min", "(", "output_features", ",", "self", ".", "MAX_FILTERS_2D", ")", "\n", "\n", "# now the bottleneck.", "\n", "# determine the first stride", "\n", "", "", "if", "self", ".", "convolutional_pooling", ":", "\n", "            ", "first_stride", "=", "pool_op_kernel_sizes", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "first_stride", "=", "None", "\n", "\n", "# the output of the last conv must match the number of features from the skip connection if we are not using", "\n", "# convolutional upsampling. If we use convolutional upsampling then the reduction in feature maps will be", "\n", "# done by the transposed conv", "\n", "", "if", "self", ".", "convolutional_upsampling", ":", "\n", "            ", "final_num_features", "=", "output_features", "\n", "", "else", ":", "\n", "            ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "1", "]", ".", "output_channels", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "num_pool", "]", "\n", "self", ".", "conv_blocks_context", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "input_features", ",", "output_features", ",", "num_conv_per_stage", "-", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ",", "first_stride", ")", ",", "\n", "StackedConvLayers", "(", "output_features", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "\n", "self", ".", "nonlin_kwargs", ")", ")", ")", "\n", "# print('self.conv_blocks_context of final is:',self.conv_blocks_context)", "\n", "\n", "# if we don't want to do dropout in the localization pathway then we set the dropout prob to zero here", "\n", "if", "not", "dropout_in_localization", ":", "\n", "            ", "old_dropout_p", "=", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "\n", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "0.0", "\n", "\n", "# now lets build the localization pathway", "\n", "", "for", "u", "in", "range", "(", "num_pool", ")", ":", "\n", "            ", "nfeatures_from_down", "=", "final_num_features", "\n", "nfeatures_from_skip", "=", "self", ".", "conv_blocks_context", "[", "-", "(", "2", "+", "u", ")", "]", ".", "output_channels", "# self.conv_blocks_context[-1] is bottleneck, so start with -2", "\n", "n_features_after_tu_and_concat", "=", "nfeatures_from_skip", "*", "2", "\n", "\n", "# the first conv reduces the number of features to match those of skip", "\n", "# the following convs work on that number of features", "\n", "# if not convolutional upsampling then the final conv reduces the num of features again", "\n", "if", "u", "!=", "num_pool", "-", "1", "and", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "final_num_features", "=", "self", ".", "conv_blocks_context", "[", "-", "(", "3", "+", "u", ")", "]", ".", "output_channels", "\n", "", "else", ":", "\n", "                ", "final_num_features", "=", "nfeatures_from_skip", "\n", "\n", "", "if", "not", "self", ".", "convolutional_upsampling", ":", "\n", "                ", "self", ".", "tu", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "mode", "=", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "tu", ".", "append", "(", "transpconv", "(", "nfeatures_from_down", ",", "nfeatures_from_skip", ",", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "\n", "pool_op_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", ",", "bias", "=", "False", ")", ")", "\n", "\n", "", "self", ".", "conv_kwargs", "[", "'kernel_size'", "]", "=", "self", ".", "conv_kernel_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "self", ".", "conv_kwargs", "[", "'padding'", "]", "=", "self", ".", "conv_pad_sizes", "[", "-", "(", "u", "+", "1", ")", "]", "\n", "self", ".", "conv_blocks_localization", ".", "append", "(", "nn", ".", "Sequential", "(", "\n", "StackedConvLayers", "(", "n_features_after_tu_and_concat", ",", "nfeatures_from_skip", ",", "num_conv_per_stage", "-", "1", ",", "\n", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "\n", "self", ".", "dropout_op_kwargs", ",", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", ",", "\n", "StackedConvLayers", "(", "nfeatures_from_skip", ",", "final_num_features", ",", "1", ",", "self", ".", "conv_op", ",", "self", ".", "conv_kwargs", ",", "\n", "self", ".", "norm_op", ",", "self", ".", "norm_op_kwargs", ",", "self", ".", "dropout_op", ",", "self", ".", "dropout_op_kwargs", ",", "\n", "self", ".", "nonlin", ",", "self", ".", "nonlin_kwargs", ")", "\n", ")", ")", "\n", "\n", "", "for", "ds", "in", "range", "(", "len", "(", "self", ".", "conv_blocks_localization", ")", ")", ":", "\n", "            ", "self", ".", "seg_outputs", ".", "append", "(", "conv_op", "(", "self", ".", "conv_blocks_localization", "[", "ds", "]", "[", "-", "1", "]", ".", "output_channels", ",", "num_classes", ",", "\n", "1", ",", "1", ",", "0", ",", "1", ",", "1", ",", "False", ")", ")", "\n", "\n", "", "self", ".", "upscale_logits_ops", "=", "[", "]", "\n", "cum_upsample", "=", "np", ".", "cumprod", "(", "np", ".", "vstack", "(", "pool_op_kernel_sizes", ")", ",", "axis", "=", "0", ")", "[", ":", ":", "-", "1", "]", "\n", "for", "usl", "in", "range", "(", "num_pool", "-", "1", ")", ":", "\n", "            ", "if", "self", ".", "upscale_logits", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "Upsample", "(", "scale_factor", "=", "tuple", "(", "[", "int", "(", "i", ")", "for", "i", "in", "cum_upsample", "[", "usl", "+", "1", "]", "]", ")", ",", "\n", "mode", "=", "upsample_mode", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "upscale_logits_ops", ".", "append", "(", "lambda", "x", ":", "x", ")", "\n", "\n", "", "", "if", "not", "dropout_in_localization", ":", "\n", "            ", "self", ".", "dropout_op_kwargs", "[", "'p'", "]", "=", "old_dropout_p", "\n", "\n", "# register all modules properly", "\n", "", "self", ".", "conv_blocks_localization", "=", "nn", ".", "ModuleList", "(", "self", ".", "conv_blocks_localization", ")", "\n", "self", ".", "conv_blocks_context", "=", "nn", ".", "ModuleList", "(", "self", ".", "conv_blocks_context", ")", "\n", "self", ".", "td", "=", "nn", ".", "ModuleList", "(", "self", ".", "td", ")", "\n", "\n", "self", ".", "tu", "=", "nn", ".", "ModuleList", "(", "self", ".", "tu", ")", "\n", "self", ".", "seg_outputs", "=", "nn", ".", "ModuleList", "(", "self", ".", "seg_outputs", ")", "\n", "# print('self.td is:', self.td)", "\n", "# print('self.tu is:', self.tu)", "\n", "# print('self.seg_outputs is:',self.seg_outputs)", "\n", "# print('self.seg_outputs[-1] is:', self.seg_outputs[-1])", "\n", "# print('self.convolutional_upsampling is:',self.convolutional_upsampling)", "\n", "\"\"\"\n        self.td is: ModuleList()\n        self.tu is: ModuleList(\n          (0): ConvTranspose3d(320, 320, kernel_size=[2, 2, 2], stride=[2, 2, 2], bias=False)\n          (1): ConvTranspose3d(320, 240, kernel_size=[2, 2, 2], stride=[2, 2, 2], bias=False)\n          (2): ConvTranspose3d(240, 120, kernel_size=[2, 2, 2], stride=[2, 2, 2], bias=False)\n          (3): ConvTranspose3d(120, 60, kernel_size=[2, 2, 1], stride=[2, 2, 1], bias=False)\n          (4): ConvTranspose3d(60, 30, kernel_size=[2, 2, 1], stride=[2, 2, 1], bias=False)\n        )\n        self.seg_outputs is: ModuleList(\n          (0): Conv3d(320, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n          (1): Conv3d(240, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n          (2): Conv3d(120, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n          (3): Conv3d(60, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n          (4): Conv3d(30, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        )\n        self.seg_outputs[-1] is: Conv3d(30, 3, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n        self.convolutional_upsampling is: True\n        \n        \"\"\"", "\n", "\n", "if", "self", ".", "upscale_logits", ":", "\n", "            ", "self", ".", "upscale_logits_ops", "=", "nn", ".", "ModuleList", "(", "self", ".", "upscale_logits_ops", ")", "# lambda x:x is not a Module so we need to distinguish here", "\n", "\n", "", "if", "self", ".", "weightInitializer", "is", "not", "None", ":", "\n", "            ", "self", ".", "apply", "(", "self", ".", "weightInitializer", ")", "\n", "#self.apply(print_module_training_status)", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.Generic_UNet.forward": [[452, 477], ["range", "range", "skips.append", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "seg_outputs.append", "tuple", "len", "generic_UNet.Generic_UNet.final_nonlin", "i", "zip", "list"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "skips", "=", "[", "]", "\n", "seg_outputs", "=", "[", "]", "\n", "for", "d", "in", "range", "(", "len", "(", "self", ".", "conv_blocks_context", ")", "-", "1", ")", ":", "\n", "            ", "x", "=", "self", ".", "conv_blocks_context", "[", "d", "]", "(", "x", ")", "\n", "skips", ".", "append", "(", "x", ")", "\n", "if", "not", "self", ".", "convolutional_pooling", ":", "\n", "                ", "x", "=", "self", ".", "td", "[", "d", "]", "(", "x", ")", "\n", "\n", "", "", "x", "=", "self", ".", "conv_blocks_context", "[", "-", "1", "]", "(", "x", ")", "\n", "\n", "for", "u", "in", "range", "(", "len", "(", "self", ".", "tu", ")", ")", ":", "\n", "            ", "x", "=", "self", ".", "tu", "[", "u", "]", "(", "x", ")", "\n", "x", "=", "torch", ".", "cat", "(", "(", "x", ",", "skips", "[", "-", "(", "u", "+", "1", ")", "]", ")", ",", "dim", "=", "1", ")", "\n", "x", "=", "self", ".", "conv_blocks_localization", "[", "u", "]", "(", "x", ")", "\n", "seg_outputs", ".", "append", "(", "self", ".", "final_nonlin", "(", "self", ".", "seg_outputs", "[", "u", "]", "(", "x", ")", ")", ")", "\n", "\n", "", "if", "self", ".", "do_ds", ":", "\n", "# result=tuple([seg_outputs[-1]] + [i(j) for i, j in", "\n", "#                                   zip(list(self.upscale_logits_ops)[::-1], seg_outputs[:-1][::-1])])", "\n", "# print('result shape is:',len(result),result[0].shape,result[1].shape,result[2].shape,result[3].shape,result[4].shape)", "\n", "            ", "return", "tuple", "(", "[", "seg_outputs", "[", "-", "1", "]", "]", "+", "[", "i", "(", "j", ")", "for", "i", ",", "j", "in", "\n", "zip", "(", "list", "(", "self", ".", "upscale_logits_ops", ")", "[", ":", ":", "-", "1", "]", ",", "seg_outputs", "[", ":", "-", "1", "]", "[", ":", ":", "-", "1", "]", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "seg_outputs", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.Generic_UNet.compute_approx_vram_consumption": [[478, 511], ["len", "numpy.array", "range", "isinstance", "numpy.array", "range", "min", "numpy.prod", "len", "numpy.prod", "numpy.prod", "numpy.prod"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "compute_approx_vram_consumption", "(", "patch_size", ",", "num_pool_per_axis", ",", "base_num_features", ",", "max_num_features", ",", "\n", "num_modalities", ",", "num_classes", ",", "pool_op_kernel_sizes", ")", ":", "\n", "        ", "\"\"\"\n        This only applies for num_conv_per_stage and convolutional_upsampling=True\n        not real vram consumption. just a constant term to which the vram consumption will be approx proportional\n        (+ offset for parameter storage)\n        :param patch_size:\n        :param num_pool_per_axis:\n        :param base_num_features:\n        :param max_num_features:\n        :return:\n        \"\"\"", "\n", "\n", "if", "not", "isinstance", "(", "num_pool_per_axis", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "num_pool_per_axis", "=", "np", ".", "array", "(", "num_pool_per_axis", ")", "\n", "\n", "", "npool", "=", "len", "(", "pool_op_kernel_sizes", ")", "\n", "\n", "map_size", "=", "np", ".", "array", "(", "patch_size", ")", "\n", "tmp", "=", "5", "*", "np", ".", "prod", "(", "map_size", ")", "*", "base_num_features", "+", "num_modalities", "*", "np", ".", "prod", "(", "map_size", ")", "+", "num_classes", "*", "np", ".", "prod", "(", "map_size", ")", "\n", "\n", "num_feat", "=", "base_num_features", "\n", "\n", "for", "p", "in", "range", "(", "npool", ")", ":", "\n", "            ", "for", "pi", "in", "range", "(", "len", "(", "num_pool_per_axis", ")", ")", ":", "\n", "                ", "map_size", "[", "pi", "]", "/=", "pool_op_kernel_sizes", "[", "p", "]", "[", "pi", "]", "\n", "", "num_feat", "=", "min", "(", "num_feat", "*", "2", ",", "max_num_features", ")", "\n", "num_blocks", "=", "5", "if", "p", "<", "(", "npool", "-", "1", ")", "else", "2", "# 2 + 2 for the convs of encode/decode and 1 for transposed conv", "\n", "tmp", "+=", "num_blocks", "*", "np", ".", "prod", "(", "map_size", ")", "*", "num_feat", "\n", "# print(p, map_size, num_feat, tmp)", "\n", "", "return", "tmp", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.generic_UNet.print_module_training_status": [[132, 138], ["isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "isinstance", "print", "str"], "function", ["None"], ["", "", "def", "print_module_training_status", "(", "module", ")", ":", "\n", "    ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Dropout", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "InstanceNorm1d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "BatchNorm1d", ")", ":", "\n", "        ", "print", "(", "str", "(", "module", ")", ",", "module", ".", "training", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.initialization.InitWeights_He.__init__": [[19, 21], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "neg_slope", "=", "1e-2", ")", ":", "\n", "        ", "self", ".", "neg_slope", "=", "neg_slope", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.initialization.InitWeights_He.__call__": [[22, 27], ["isinstance", "isinstance", "isinstance", "isinstance", "torch.nn.init.kaiming_normal_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "module", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose3d", ")", ":", "\n", "            ", "module", ".", "weight", "=", "nn", ".", "init", ".", "kaiming_normal_", "(", "module", ".", "weight", ",", "a", "=", "1e-2", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", "=", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.initialization.InitWeights_XavierUniform.__init__": [[30, 32], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "gain", "=", "1", ")", ":", "\n", "        ", "self", ".", "gain", "=", "gain", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.initialization.InitWeights_XavierUniform.__call__": [[33, 38], ["isinstance", "isinstance", "isinstance", "isinstance", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "module", ")", ":", "\n", "        ", "if", "isinstance", "(", "module", ",", "nn", ".", "Conv3d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "Conv2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose2d", ")", "or", "isinstance", "(", "module", ",", "nn", ".", "ConvTranspose3d", ")", ":", "\n", "            ", "module", ".", "weight", "=", "nn", ".", "init", ".", "xavier_uniform_", "(", "module", ".", "weight", ",", "self", ".", "gain", ")", "\n", "if", "module", ".", "bias", "is", "not", "None", ":", "\n", "                ", "module", ".", "bias", "=", "nn", ".", "init", ".", "constant_", "(", "module", ".", "bias", ",", "0", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.__init__": [[24, 26], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "NeuralNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device": [[27, 32], ["next", "neural_network.NeuralNetwork.parameters", "next", "neural_network.NeuralNetwork.parameters"], "methods", ["None"], ["", "def", "get_device", "(", "self", ")", ":", "\n", "        ", "if", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", "==", "\"cpu\"", ":", "\n", "            ", "return", "\"cpu\"", "\n", "", "else", ":", "\n", "            ", "return", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", ".", "index", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.set_device": [[33, 38], ["neural_network.NeuralNetwork.cpu", "neural_network.NeuralNetwork.cuda"], "methods", ["None"], ["", "", "def", "set_device", "(", "self", ",", "device", ")", ":", "\n", "        ", "if", "device", "==", "\"cpu\"", ":", "\n", "            ", "self", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "cuda", "(", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.forward": [[39, 41], ["None"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork.__init__": [[44, 50], ["neural_network.NeuralNetwork.__init__"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "input_shape_must_be_divisible_by", "=", "None", "\n", "self", ".", "conv_op", "=", "None", "\n", "self", ".", "num_classes", "=", "None", "\n", "super", "(", "NeuralNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inference_apply_nonlin", "=", "lambda", "x", ":", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork.predict_3D": [[51, 101], ["print", "ValueError", "neural_network.SegmentationNetwork.train", "len", "neural_network.SegmentationNetwork.train", "len", "max", "neural_network.SegmentationNetwork.eval", "neural_network.SegmentationNetwork._internal_predict_3D_3Dconv_tiled", "neural_network.SegmentationNetwork._internal_predict_3D_3Dconv", "RuntimeError", "neural_network.SegmentationNetwork._internal_predict_3D_2Dconv_tiled", "neural_network.SegmentationNetwork._internal_predict_3D_2Dconv"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_3Dconv_tiled", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_3Dconv", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_2Dconv_tiled", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_2Dconv"], ["", "def", "predict_3D", "(", "self", ",", "x", ",", "do_mirroring", ",", "num_repeats", "=", "1", ",", "use_train_mode", "=", "False", ",", "batch_size", "=", "1", ",", "mirror_axes", "=", "(", "0", ",", "1", ",", "2", ")", ",", "\n", "tiled", "=", "False", ",", "tile_in_z", "=", "True", ",", "step", "=", "2", ",", "patch_size", "=", "None", ",", "regions_class_order", "=", "None", ",", "use_gaussian", "=", "False", ",", "\n", "pad_border_mode", "=", "\"edge\"", ",", "pad_kwargs", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        :param x: (c, x, y , z)\n        :param do_mirroring:\n        :param num_repeats:\n        :param use_train_mode:\n        :param batch_size:\n        :param mirror_axes:\n        :param tiled:\n        :param tile_in_z:\n        :param step:\n        :param patch_size:\n        :param regions_class_order:\n        :param use_gaussian:\n        :return:\n        \"\"\"", "\n", "print", "(", "\"debug: mirroring\"", ",", "do_mirroring", ",", "\"mirror_axes\"", ",", "mirror_axes", ")", "\n", "if", "len", "(", "mirror_axes", ")", ">", "0", "and", "max", "(", "mirror_axes", ")", ">", "2", ":", "\n", "            ", "raise", "ValueError", "(", "\"mirror axes. duh\"", ")", "\n", "", "current_mode", "=", "self", ".", "training", "\n", "if", "use_train_mode", "is", "not", "None", "and", "use_train_mode", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "elif", "use_train_mode", "is", "not", "None", "and", "not", "use_train_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", ",", "\"data must have shape (c,x,y,z)\"", "\n", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "if", "tiled", ":", "\n", "                ", "res", "=", "self", ".", "_internal_predict_3D_3Dconv_tiled", "(", "x", ",", "num_repeats", ",", "batch_size", ",", "tile_in_z", ",", "step", ",", "do_mirroring", ",", "\n", "mirror_axes", ",", "patch_size", ",", "regions_class_order", ",", "use_gaussian", ",", "\n", "pad_border_mode", ",", "pad_kwargs", "=", "pad_kwargs", ")", "\n", "", "else", ":", "\n", "                ", "res", "=", "self", ".", "_internal_predict_3D_3Dconv", "(", "x", ",", "do_mirroring", ",", "num_repeats", ",", "patch_size", ",", "batch_size", ",", "\n", "mirror_axes", ",", "regions_class_order", ",", "pad_border_mode", ",", "pad_kwargs", "=", "pad_kwargs", ")", "\n", "", "", "elif", "self", ".", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "            ", "if", "tiled", ":", "\n", "                ", "res", "=", "self", ".", "_internal_predict_3D_2Dconv_tiled", "(", "x", ",", "do_mirroring", ",", "num_repeats", ",", "batch_size", ",", "mirror_axes", ",", "\n", "step", ",", "patch_size", ",", "regions_class_order", ",", "use_gaussian", ",", "\n", "pad_border_mode", ",", "pad_kwargs", "=", "pad_kwargs", ")", "\n", "", "else", ":", "\n", "                ", "res", "=", "self", ".", "_internal_predict_3D_2Dconv", "(", "x", ",", "do_mirroring", ",", "num_repeats", ",", "patch_size", ",", "batch_size", ",", "\n", "mirror_axes", ",", "regions_class_order", ",", "pad_border_mode", ",", "pad_kwargs", "=", "pad_kwargs", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Invalid conv op, cannot determine what dimensionality (2d/3d) the network is\"", ")", "\n", "", "if", "use_train_mode", "is", "not", "None", ":", "\n", "            ", "self", ".", "train", "(", "current_mode", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork.predict_2D": [[102, 130], ["ValueError", "len", "neural_network.SegmentationNetwork.train", "RuntimeError", "neural_network.SegmentationNetwork.train", "len", "max", "neural_network.SegmentationNetwork.eval", "RuntimeError", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv"], ["", "def", "predict_2D", "(", "self", ",", "x", ",", "do_mirroring", ",", "num_repeats", "=", "1", ",", "use_train_mode", "=", "False", ",", "batch_size", "=", "1", ",", "mirror_axes", "=", "(", "0", ",", "1", ")", ",", "\n", "tiled", "=", "False", ",", "step", "=", "2", ",", "patch_size", "=", "None", ",", "regions_class_order", "=", "None", ",", "use_gaussian", "=", "False", ",", "\n", "pad_border_mode", "=", "\"edge\"", ",", "pad_kwargs", "=", "None", ")", ":", "\n", "        ", "if", "len", "(", "mirror_axes", ")", ">", "0", "and", "max", "(", "mirror_axes", ")", ">", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"mirror axes. duh\"", ")", "\n", "", "assert", "len", "(", "x", ".", "shape", ")", "==", "3", ",", "\"data must have shape (c,x,y)\"", "\n", "current_mode", "=", "self", ".", "training", "\n", "if", "use_train_mode", "is", "not", "None", "and", "use_train_mode", ":", "\n", "            ", "self", ".", "train", "(", ")", "\n", "", "elif", "use_train_mode", "is", "not", "None", "and", "not", "use_train_mode", ":", "\n", "            ", "self", ".", "eval", "(", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "", "if", "self", ".", "conv_op", "==", "nn", ".", "Conv3d", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Cannot predict 2d if the network is 3d. Dummy.\"", ")", "\n", "", "elif", "self", ".", "conv_op", "==", "nn", ".", "Conv2d", ":", "\n", "            ", "if", "tiled", ":", "\n", "                ", "res", "=", "self", ".", "_internal_predict_2D_2Dconv_tiled", "(", "x", ",", "num_repeats", ",", "batch_size", ",", "step", ",", "do_mirroring", ",", "\n", "mirror_axes", ",", "patch_size", ",", "regions_class_order", ",", "\n", "use_gaussian", ",", "pad_border_mode", ",", "pad_kwargs", "=", "pad_kwargs", ")", "\n", "", "else", ":", "\n", "                ", "res", "=", "self", ".", "_internal_predict_2D_2Dconv", "(", "x", ",", "do_mirroring", ",", "num_repeats", ",", "None", ",", "batch_size", ",", "mirror_axes", ",", "\n", "regions_class_order", ",", "pad_border_mode", ",", "pad_kwargs", "=", "pad_kwargs", ")", "\n", "", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Invalid conv op, cannot determine what dimensionality (2d/3d) the network is\"", ")", "\n", "", "if", "use_train_mode", "is", "not", "None", ":", "\n", "            ", "self", ".", "train", "(", "current_mode", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D": [[131, 190], ["result_torch.cuda.cuda.detach().cpu().numpy", "torch.no_grad", "torch.from_numpy().float", "torch.zeros().float", "range", "neural_network.SegmentationNetwork.get_device", "x_torch.cuda.cuda.cpu", "x_torch.cuda.cuda.cuda", "neural_network.SegmentationNetwork.get_device", "result_torch.cuda.cuda.cpu", "result_torch.cuda.cuda.cuda", "range", "result_torch.cuda.cuda.detach().cpu", "torch.from_numpy", "neural_network.SegmentationNetwork.get_device", "torch.zeros", "neural_network.SegmentationNetwork.get_device", "len", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "result_torch.cuda.cuda.detach", "list", "neural_network.SegmentationNetwork.", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip"], ["", "def", "_internal_maybe_mirror_and_pred_3D", "(", "self", ",", "x", ",", "num_repeats", ",", "mirror_axes", ",", "do_mirroring", "=", "True", ",", "mult", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "x_torch", "=", "torch", ".", "from_numpy", "(", "x", ")", ".", "float", "(", ")", "\n", "if", "self", ".", "get_device", "(", ")", "==", "\"cpu\"", ":", "\n", "                ", "x_torch", "=", "x_torch", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "                ", "x_torch", "=", "x_torch", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "", "result_torch", "=", "torch", ".", "zeros", "(", "[", "1", ",", "self", ".", "num_classes", "]", "+", "list", "(", "x", ".", "shape", "[", "2", ":", "]", ")", ")", ".", "float", "(", ")", "\n", "if", "self", ".", "get_device", "(", ")", "==", "\"cpu\"", ":", "\n", "                ", "result_torch", "=", "result_torch", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "                ", "result_torch", "=", "result_torch", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "", "num_results", "=", "num_repeats", "\n", "if", "do_mirroring", ":", "\n", "                ", "mirror_idx", "=", "8", "\n", "num_results", "*=", "2", "**", "len", "(", "mirror_axes", ")", "\n", "", "else", ":", "\n", "                ", "mirror_idx", "=", "1", "\n", "\n", "", "for", "i", "in", "range", "(", "num_repeats", ")", ":", "\n", "                ", "for", "m", "in", "range", "(", "mirror_idx", ")", ":", "\n", "                    ", "if", "m", "==", "0", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "x_torch", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "pred", "\n", "\n", "", "if", "m", "==", "1", "and", "(", "2", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "x_torch", ",", "4", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "pred", ",", "4", ")", "\n", "\n", "", "if", "m", "==", "2", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "x_torch", ",", "3", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "pred", ",", "3", ")", "\n", "\n", "", "if", "m", "==", "3", "and", "(", "2", "in", "mirror_axes", ")", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "flip", "(", "x_torch", ",", "4", ")", ",", "3", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "flip", "(", "pred", ",", "4", ")", ",", "3", ")", "\n", "\n", "", "if", "m", "==", "4", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "x_torch", ",", "2", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "pred", ",", "2", ")", "\n", "\n", "", "if", "m", "==", "5", "and", "(", "0", "in", "mirror_axes", ")", "and", "(", "2", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "flip", "(", "x_torch", ",", "4", ")", ",", "2", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "flip", "(", "pred", ",", "4", ")", ",", "2", ")", "\n", "\n", "", "if", "m", "==", "6", "and", "(", "0", "in", "mirror_axes", ")", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "flip", "(", "x_torch", ",", "3", ")", ",", "2", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "flip", "(", "pred", ",", "3", ")", ",", "2", ")", "\n", "\n", "", "if", "m", "==", "7", "and", "(", "0", "in", "mirror_axes", ")", "and", "(", "1", "in", "mirror_axes", ")", "and", "(", "2", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "flip", "(", "flip", "(", "x_torch", ",", "3", ")", ",", "2", ")", ",", "4", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "flip", "(", "flip", "(", "pred", ",", "3", ")", ",", "2", ")", ",", "4", ")", "\n", "\n", "", "", "", "", "if", "mult", "is", "not", "None", ":", "\n", "            ", "result_torch", "[", ":", ",", ":", "]", "*=", "mult", "\n", "\n", "", "return", "result_torch", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_3Dconv_tiled": [[191, 278], ["len", "torch.no_grad", "batchgenerators.augmentations.utils.pad_nd_image", "torch.rand().float", "numpy.zeros", "numpy.zeros", "numpy.array().astype", "numpy.array().astype", "numpy.ceil", "numpy.array", "numpy.round().astype", "numpy.round().astype", "numpy.round().astype", "tuple", "numpy.vstack", "list", "int", "neural_network.SegmentationNetwork.get_device", "a.cuda.cuda.cpu", "a.cuda.cuda.cuda", "neural_network.SegmentationNetwork.size", "numpy.zeros", "scipy.ndimage.filters.gaussian_filter", "numpy.ones", "neural_network.SegmentationNetwork.get_device", "torch.from_numpy().cpu().float", "torch.from_numpy().cuda().float", "softmax_pred.argmax", "numpy.zeros", "enumerate", "torch.rand", "neural_network.SegmentationNetwork.get_device", "list", "list", "numpy.array", "numpy.array", "numpy.round", "numpy.round", "numpy.round", "neural_network.SegmentationNetwork.", "tuple", "scipy.ndimage.filters.gaussian_filter.max", "torch.from_numpy().cpu", "torch.from_numpy().cuda", "range", "range", "numpy.arange", "numpy.arange", "numpy.arange", "slice", "neural_network.SegmentationNetwork.get_device", "neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D", "range", "torch.from_numpy", "torch.from_numpy", "range", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D"], ["", "def", "_internal_predict_3D_3Dconv_tiled", "(", "self", ",", "x", ",", "num_repeats", ",", "BATCH_SIZE", "=", "None", ",", "tile_in_z", "=", "True", ",", "step", "=", "2", ",", "\n", "do_mirroring", "=", "True", ",", "mirror_axes", "=", "(", "0", ",", "1", ",", "2", ")", ",", "patch_size", "=", "None", ",", "\n", "regions_class_order", "=", "None", ",", "use_gaussian", "=", "False", ",", "pad_border_mode", "=", "\"edge\"", ",", "pad_kwargs", "=", "None", ")", ":", "\n", "        ", "\"x must be (c, x, y, z)\"", "\n", "assert", "len", "(", "x", ".", "shape", ")", "==", "4", ",", "\"x must be (c, x, y, z)\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "assert", "patch_size", "is", "not", "None", ",", "\"patch_size cannot be None for tiled prediction\"", "\n", "\n", "data", ",", "slicer", "=", "pad_nd_image", "(", "x", ",", "patch_size", ",", "pad_border_mode", ",", "pad_kwargs", ",", "True", ",", "None", ")", "\n", "\n", "data", "=", "data", "[", "None", "]", "\n", "\n", "if", "BATCH_SIZE", "is", "not", "None", ":", "\n", "                ", "data", "=", "np", ".", "vstack", "(", "[", "data", "]", "*", "BATCH_SIZE", ")", "\n", "\n", "", "input_size", "=", "[", "1", ",", "x", ".", "shape", "[", "0", "]", "]", "+", "list", "(", "patch_size", ")", "\n", "if", "not", "tile_in_z", ":", "\n", "                ", "input_size", "[", "2", "]", "=", "data", ".", "shape", "[", "2", "]", "\n", "patch_size", "[", "0", "]", "=", "data", ".", "shape", "[", "2", "]", "\n", "", "input_size", "=", "[", "int", "(", "i", ")", "for", "i", "in", "input_size", "]", "\n", "\n", "a", "=", "torch", ".", "rand", "(", "input_size", ")", ".", "float", "(", ")", "\n", "if", "self", ".", "get_device", "(", ")", "==", "\"cpu\"", ":", "\n", "                ", "a", "=", "a", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "                ", "a", "=", "a", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "# dummy run to see number of classes", "\n", "", "nb_of_classes", "=", "self", "(", "a", ")", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "result", "=", "np", ".", "zeros", "(", "[", "nb_of_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "2", ":", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "result_numsamples", "=", "np", ".", "zeros", "(", "[", "nb_of_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "2", ":", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "use_gaussian", ":", "\n", "                ", "tmp", "=", "np", ".", "zeros", "(", "patch_size", ")", "\n", "center_coords", "=", "[", "i", "//", "2", "for", "i", "in", "patch_size", "]", "\n", "sigmas", "=", "[", "i", "//", "8", "for", "i", "in", "patch_size", "]", "\n", "tmp", "[", "tuple", "(", "center_coords", ")", "]", "=", "1", "\n", "tmp_smooth", "=", "gaussian_filter", "(", "tmp", ",", "sigmas", ",", "0", ",", "mode", "=", "'constant'", ",", "cval", "=", "0", ")", "\n", "tmp_smooth", "=", "tmp_smooth", "/", "tmp_smooth", ".", "max", "(", ")", "*", "1", "\n", "add", "=", "tmp_smooth", "+", "1e-8", "\n", "", "else", ":", "\n", "                ", "add", "=", "np", ".", "ones", "(", "patch_size", ")", "\n", "\n", "", "if", "self", ".", "get_device", "(", ")", "==", "\"cpu\"", ":", "\n", "                ", "add_torch", "=", "torch", ".", "from_numpy", "(", "add", ")", ".", "cpu", "(", ")", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "                ", "add_torch", "=", "torch", ".", "from_numpy", "(", "add", ")", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ")", ".", "float", "(", ")", "\n", "\n", "", "data_shape", "=", "data", ".", "shape", "\n", "center_coord_start", "=", "np", ".", "array", "(", "[", "i", "//", "2", "for", "i", "in", "patch_size", "]", ")", ".", "astype", "(", "int", ")", "\n", "center_coord_end", "=", "np", ".", "array", "(", "[", "data_shape", "[", "i", "+", "2", "]", "-", "patch_size", "[", "i", "]", "//", "2", "for", "i", "in", "range", "(", "len", "(", "patch_size", ")", ")", "]", ")", ".", "astype", "(", "int", ")", "\n", "num_steps", "=", "np", ".", "ceil", "(", "[", "(", "center_coord_end", "[", "i", "]", "-", "center_coord_start", "[", "i", "]", ")", "/", "(", "patch_size", "[", "i", "]", "/", "step", ")", "for", "i", "in", "range", "(", "3", ")", "]", ")", "\n", "step_size", "=", "np", ".", "array", "(", "[", "(", "center_coord_end", "[", "i", "]", "-", "center_coord_start", "[", "i", "]", ")", "/", "(", "num_steps", "[", "i", "]", "+", "1e-8", ")", "for", "i", "in", "range", "(", "3", ")", "]", ")", "\n", "step_size", "[", "step_size", "==", "0", "]", "=", "9999999", "\n", "xsteps", "=", "np", ".", "round", "(", "np", ".", "arange", "(", "center_coord_start", "[", "0", "]", ",", "center_coord_end", "[", "0", "]", "+", "1e-8", ",", "step_size", "[", "0", "]", ")", ")", ".", "astype", "(", "int", ")", "\n", "ysteps", "=", "np", ".", "round", "(", "np", ".", "arange", "(", "center_coord_start", "[", "1", "]", ",", "center_coord_end", "[", "1", "]", "+", "1e-8", ",", "step_size", "[", "1", "]", ")", ")", ".", "astype", "(", "int", ")", "\n", "zsteps", "=", "np", ".", "round", "(", "np", ".", "arange", "(", "center_coord_start", "[", "2", "]", ",", "center_coord_end", "[", "2", "]", "+", "1e-8", ",", "step_size", "[", "2", "]", ")", ")", ".", "astype", "(", "int", ")", "\n", "\n", "for", "x", "in", "xsteps", ":", "\n", "                ", "lb_x", "=", "x", "-", "patch_size", "[", "0", "]", "//", "2", "\n", "ub_x", "=", "x", "+", "patch_size", "[", "0", "]", "//", "2", "\n", "for", "y", "in", "ysteps", ":", "\n", "                    ", "lb_y", "=", "y", "-", "patch_size", "[", "1", "]", "//", "2", "\n", "ub_y", "=", "y", "+", "patch_size", "[", "1", "]", "//", "2", "\n", "for", "z", "in", "zsteps", ":", "\n", "                        ", "lb_z", "=", "z", "-", "patch_size", "[", "2", "]", "//", "2", "\n", "ub_z", "=", "z", "+", "patch_size", "[", "2", "]", "//", "2", "\n", "result", "[", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", ",", "lb_z", ":", "ub_z", "]", "+=", "self", ".", "_internal_maybe_mirror_and_pred_3D", "(", "data", "[", ":", ",", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", ",", "lb_z", ":", "ub_z", "]", ",", "\n", "num_repeats", ",", "mirror_axes", ",", "do_mirroring", ",", "add_torch", ")", "[", "0", "]", "\n", "result_numsamples", "[", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", ",", "lb_z", ":", "ub_z", "]", "+=", "add", "\n", "\n", "", "", "", "slicer", "=", "tuple", "(", "[", "slice", "(", "0", ",", "result", ".", "shape", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "result", ".", "shape", ")", "-", "(", "len", "(", "slicer", ")", "-", "1", ")", ")", "]", "+", "slicer", "[", "1", ":", "]", ")", "\n", "result", "=", "result", "[", "slicer", "]", "\n", "result_numsamples", "=", "result_numsamples", "[", "slicer", "]", "\n", "\n", "softmax_pred", "=", "result", "/", "result_numsamples", "\n", "\n", "# patient_data = patient_data[:, :old_shape[0], :old_shape[1], :old_shape[2]]", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "                ", "predicted_segmentation", "=", "softmax_pred", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "predicted_segmentation_shp", "=", "softmax_pred", "[", "0", "]", ".", "shape", "\n", "predicted_segmentation", "=", "np", ".", "zeros", "(", "predicted_segmentation_shp", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "regions_class_order", ")", ":", "\n", "                    ", "predicted_segmentation", "[", "softmax_pred", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "", "", "", "return", "predicted_segmentation", ",", "None", ",", "softmax_pred", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv": [[279, 315], ["torch.no_grad", "batchgenerators.augmentations.utils.pad_nd_image", "numpy.zeros", "tuple", "tuple", "numpy.vstack", "neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D", "softmax_pred.argmax", "numpy.zeros", "enumerate", "list", "slice", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D"], ["", "def", "_internal_predict_2D_2Dconv", "(", "self", ",", "x", ",", "do_mirroring", ",", "num_repeats", ",", "min_size", "=", "None", ",", "BATCH_SIZE", "=", "None", ",", "\n", "mirror_axes", "=", "(", "0", ",", "1", ")", ",", "regions_class_order", "=", "None", ",", "pad_border_mode", "=", "\"edge\"", ",", "pad_kwargs", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "_", "=", "None", "\n", "#x, old_shape = pad_patient_2D_incl_c(x, self.input_shape_must_be_divisible_by, min_size)", "\n", "x", ",", "slicer", "=", "pad_nd_image", "(", "x", ",", "min_size", ",", "pad_border_mode", ",", "pad_kwargs", ",", "True", ",", "self", ".", "input_shape_must_be_divisible_by", ")", "\n", "\"\"\"pad_res = []\n            for i in range(x.shape[0]):\n                t, old_shape = pad_patient_2D(x[i], self.input_shape_must_be_divisible_by, None)\n                pad_res.append(t[None])\n\n            x = np.vstack(pad_res)\"\"\"", "\n", "\n", "new_shp", "=", "x", ".", "shape", "\n", "\n", "data", "=", "np", ".", "zeros", "(", "tuple", "(", "[", "1", "]", "+", "list", "(", "new_shp", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "data", "[", "0", "]", "=", "x", "\n", "\n", "if", "BATCH_SIZE", "is", "not", "None", ":", "\n", "                ", "data", "=", "np", ".", "vstack", "(", "[", "data", "]", "*", "BATCH_SIZE", ")", "\n", "\n", "", "result", "=", "self", ".", "_internal_maybe_mirror_and_pred_2D", "(", "data", ",", "num_repeats", ",", "mirror_axes", ",", "do_mirroring", ")", "[", "0", "]", "\n", "\n", "slicer", "=", "tuple", "(", "[", "slice", "(", "0", ",", "result", ".", "shape", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "result", ".", "shape", ")", "-", "(", "len", "(", "slicer", ")", "-", "1", ")", ")", "]", "+", "slicer", "[", "1", ":", "]", ")", "\n", "result", "=", "result", "[", "slicer", "]", "\n", "softmax_pred", "=", "result", "\n", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "                ", "predicted_segmentation", "=", "softmax_pred", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "predicted_segmentation_shp", "=", "softmax_pred", "[", "0", "]", ".", "shape", "\n", "predicted_segmentation", "=", "np", ".", "zeros", "(", "predicted_segmentation_shp", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "regions_class_order", ")", ":", "\n", "                    ", "predicted_segmentation", "[", "softmax_pred", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "", "", "", "return", "predicted_segmentation", ",", "_", ",", "softmax_pred", ",", "_", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_3Dconv": [[316, 346], ["torch.no_grad", "batchgenerators.augmentations.utils.pad_nd_image", "numpy.zeros", "tuple", "tuple", "numpy.vstack", "neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D", "softmax_pred.argmax", "numpy.zeros", "enumerate", "list", "slice", "range", "len", "len"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_3D"], ["", "def", "_internal_predict_3D_3Dconv", "(", "self", ",", "x", ",", "do_mirroring", ",", "num_repeats", ",", "min_size", "=", "None", ",", "BATCH_SIZE", "=", "None", ",", "\n", "mirror_axes", "=", "(", "0", ",", "1", ",", "2", ")", ",", "regions_class_order", "=", "None", ",", "pad_border_mode", "=", "\"edge\"", ",", "\n", "pad_kwargs", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "x", ",", "slicer", "=", "pad_nd_image", "(", "x", ",", "min_size", ",", "pad_border_mode", ",", "pad_kwargs", ",", "True", ",", "self", ".", "input_shape_must_be_divisible_by", ")", "\n", "#x, old_shape = pad_patient_3D_incl_c(x, self.input_shape_must_be_divisible_by, min_size)", "\n", "\n", "new_shp", "=", "x", ".", "shape", "\n", "\n", "data", "=", "np", ".", "zeros", "(", "tuple", "(", "[", "1", "]", "+", "list", "(", "new_shp", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "data", "[", "0", "]", "=", "x", "\n", "\n", "if", "BATCH_SIZE", "is", "not", "None", ":", "\n", "                ", "data", "=", "np", ".", "vstack", "(", "[", "data", "]", "*", "BATCH_SIZE", ")", "\n", "\n", "", "stacked", "=", "self", ".", "_internal_maybe_mirror_and_pred_3D", "(", "data", ",", "num_repeats", ",", "mirror_axes", ",", "do_mirroring", ",", "None", ")", "[", "0", "]", "\n", "\n", "slicer", "=", "tuple", "(", "[", "slice", "(", "0", ",", "stacked", ".", "shape", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "stacked", ".", "shape", ")", "-", "(", "len", "(", "slicer", ")", "-", "1", ")", ")", "]", "+", "slicer", "[", "1", ":", "]", ")", "\n", "stacked", "=", "stacked", "[", "slicer", "]", "\n", "softmax_pred", "=", "stacked", "\n", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "                ", "predicted_segmentation", "=", "softmax_pred", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "predicted_segmentation_shp", "=", "softmax_pred", "[", "0", "]", ".", "shape", "\n", "predicted_segmentation", "=", "np", ".", "zeros", "(", "predicted_segmentation_shp", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "regions_class_order", ")", ":", "\n", "                    ", "predicted_segmentation", "[", "softmax_pred", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "", "", "", "return", "predicted_segmentation", ",", "None", ",", "softmax_pred", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D": [[347, 390], ["result_torch.cuda.cuda.detach().cpu().numpy", "torch.no_grad", "torch.from_numpy().float", "torch.zeros().float", "range", "neural_network.SegmentationNetwork.get_device", "x_torch.cuda.cuda.cpu", "x_torch.cuda.cuda.cuda", "neural_network.SegmentationNetwork.get_device", "result_torch.cuda.cuda.cpu", "result_torch.cuda.cuda.cuda", "range", "result_torch.cuda.cuda.detach().cpu", "torch.from_numpy", "neural_network.SegmentationNetwork.get_device", "torch.zeros", "neural_network.SegmentationNetwork.get_device", "len", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "neural_network.SegmentationNetwork.inference_apply_nonlin", "result_torch.cuda.cuda.detach", "list", "neural_network.SegmentationNetwork.", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "neural_network.SegmentationNetwork.", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip", "nnunet.utilities.tensor_utilities.flip"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip"], ["", "def", "_internal_maybe_mirror_and_pred_2D", "(", "self", ",", "x", ",", "num_repeats", ",", "mirror_axes", ",", "do_mirroring", "=", "True", ",", "mult", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "x_torch", "=", "torch", ".", "from_numpy", "(", "x", ")", ".", "float", "(", ")", "\n", "if", "self", ".", "get_device", "(", ")", "==", "\"cpu\"", ":", "\n", "                ", "x_torch", "=", "x_torch", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "                ", "x_torch", "=", "x_torch", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "", "result_torch", "=", "torch", ".", "zeros", "(", "[", "1", ",", "self", ".", "num_classes", "]", "+", "list", "(", "x", ".", "shape", "[", "2", ":", "]", ")", ")", ".", "float", "(", ")", "\n", "if", "self", ".", "get_device", "(", ")", "==", "\"cpu\"", ":", "\n", "                ", "result_torch", "=", "result_torch", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "                ", "result_torch", "=", "result_torch", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "", "num_results", "=", "num_repeats", "\n", "if", "do_mirroring", ":", "\n", "                ", "mirror_idx", "=", "4", "\n", "num_results", "*=", "2", "**", "len", "(", "mirror_axes", ")", "\n", "", "else", ":", "\n", "                ", "mirror_idx", "=", "1", "\n", "\n", "", "for", "i", "in", "range", "(", "num_repeats", ")", ":", "\n", "                ", "for", "m", "in", "range", "(", "mirror_idx", ")", ":", "\n", "                    ", "if", "m", "==", "0", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "x_torch", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "pred", "\n", "\n", "", "if", "m", "==", "1", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "x_torch", ",", "3", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "pred", ",", "3", ")", "\n", "\n", "", "if", "m", "==", "2", "and", "(", "0", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "x_torch", ",", "2", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "pred", ",", "2", ")", "\n", "\n", "", "if", "m", "==", "3", "and", "(", "0", "in", "mirror_axes", ")", "and", "(", "1", "in", "mirror_axes", ")", ":", "\n", "                        ", "pred", "=", "self", ".", "inference_apply_nonlin", "(", "self", "(", "flip", "(", "flip", "(", "x_torch", ",", "3", ")", ",", "2", ")", ")", ")", "\n", "result_torch", "+=", "1", "/", "num_results", "*", "flip", "(", "flip", "(", "pred", ",", "3", ")", ",", "2", ")", "\n", "\n", "", "", "", "", "if", "mult", "is", "not", "None", ":", "\n", "            ", "result_torch", "[", ":", ",", ":", "]", "*=", "mult", "\n", "\n", "", "return", "result_torch", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled": [[391, 467], ["torch.no_grad", "batchgenerators.augmentations.utils.pad_nd_image", "torch.rand().float", "numpy.zeros", "numpy.zeros", "numpy.array().astype", "numpy.array().astype", "numpy.ceil", "numpy.array", "numpy.round().astype", "numpy.round().astype", "tuple", "numpy.vstack", "list", "int", "neural_network.SegmentationNetwork.get_device", "a.cuda.cuda.cpu", "a.cuda.cuda.cuda", "neural_network.SegmentationNetwork.size", "numpy.zeros", "scipy.ndimage.filters.gaussian_filter", "numpy.ones", "neural_network.SegmentationNetwork.get_device", "torch.from_numpy().cpu().float", "torch.from_numpy().cuda().float", "softmax_pred.argmax", "numpy.zeros", "enumerate", "torch.rand", "neural_network.SegmentationNetwork.get_device", "list", "list", "numpy.array", "numpy.array", "numpy.round", "numpy.round", "neural_network.SegmentationNetwork.", "tuple", "scipy.ndimage.filters.gaussian_filter.max", "torch.from_numpy().cpu", "torch.from_numpy().cuda", "range", "range", "numpy.arange", "numpy.arange", "neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D", "slice", "neural_network.SegmentationNetwork.get_device", "range", "torch.from_numpy", "torch.from_numpy", "range", "len", "len", "len"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_maybe_mirror_and_pred_2D", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.get_device"], ["", "def", "_internal_predict_2D_2Dconv_tiled", "(", "self", ",", "patient_data", ",", "num_repeats", ",", "BATCH_SIZE", "=", "None", ",", "step", "=", "2", ",", "\n", "do_mirroring", "=", "True", ",", "mirror_axes", "=", "(", "0", ",", "1", ")", ",", "patch_size", "=", "None", ",", "regions_class_order", "=", "None", ",", "\n", "use_gaussian", "=", "False", ",", "pad_border_mode", "=", "\"edge\"", ",", "pad_kwargs", "=", "None", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "tile_size", "=", "patch_size", "\n", "assert", "tile_size", "is", "not", "None", ",", "\"patch_size cannot be None for tiled prediction\"", "\n", "# pad images so that their size is a multiple of tile_size", "\n", "data", ",", "slicer", "=", "pad_nd_image", "(", "patient_data", ",", "tile_size", ",", "pad_border_mode", ",", "pad_kwargs", ",", "True", ")", "\n", "\n", "data", "=", "data", "[", "None", "]", "\n", "\n", "if", "BATCH_SIZE", "is", "not", "None", ":", "\n", "                ", "data", "=", "np", ".", "vstack", "(", "[", "data", "]", "*", "BATCH_SIZE", ")", "\n", "\n", "", "input_size", "=", "[", "1", ",", "patient_data", ".", "shape", "[", "0", "]", "]", "+", "list", "(", "tile_size", ")", "\n", "input_size", "=", "[", "int", "(", "i", ")", "for", "i", "in", "input_size", "]", "\n", "a", "=", "torch", ".", "rand", "(", "input_size", ")", ".", "float", "(", ")", "\n", "if", "self", ".", "get_device", "(", ")", "==", "\"cpu\"", ":", "\n", "                ", "a", "=", "a", ".", "cpu", "(", ")", "\n", "", "else", ":", "\n", "                ", "a", "=", "a", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ")", "\n", "\n", "# dummy run to see number of classes", "\n", "", "nb_of_classes", "=", "self", "(", "a", ")", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "result", "=", "np", ".", "zeros", "(", "[", "nb_of_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "2", ":", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "result_numsamples", "=", "np", ".", "zeros", "(", "[", "nb_of_classes", "]", "+", "list", "(", "data", ".", "shape", "[", "2", ":", "]", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "if", "use_gaussian", ":", "\n", "                ", "tmp", "=", "np", ".", "zeros", "(", "tile_size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "center_coords", "=", "[", "i", "//", "2", "for", "i", "in", "tile_size", "]", "\n", "sigmas", "=", "[", "i", "//", "8", "for", "i", "in", "tile_size", "]", "\n", "tmp", "[", "tuple", "(", "center_coords", ")", "]", "=", "1", "\n", "tmp_smooth", "=", "gaussian_filter", "(", "tmp", ",", "sigmas", ",", "0", ",", "mode", "=", "'constant'", ",", "cval", "=", "0", ")", "\n", "tmp_smooth", "=", "tmp_smooth", "/", "tmp_smooth", ".", "max", "(", ")", "*", "1", "\n", "add", "=", "tmp_smooth", "\n", "", "else", ":", "\n", "                ", "add", "=", "np", ".", "ones", "(", "tile_size", ")", "\n", "\n", "", "if", "self", ".", "get_device", "(", ")", "==", "\"cpu\"", ":", "\n", "                ", "add_torch", "=", "torch", ".", "from_numpy", "(", "add", ")", ".", "cpu", "(", ")", ".", "float", "(", ")", "\n", "", "else", ":", "\n", "                ", "add_torch", "=", "torch", ".", "from_numpy", "(", "add", ")", ".", "cuda", "(", "self", ".", "get_device", "(", ")", ")", ".", "float", "(", ")", "\n", "\n", "", "data_shape", "=", "data", ".", "shape", "\n", "center_coord_start", "=", "np", ".", "array", "(", "[", "i", "//", "2", "for", "i", "in", "patch_size", "]", ")", ".", "astype", "(", "int", ")", "\n", "center_coord_end", "=", "np", ".", "array", "(", "[", "data_shape", "[", "i", "+", "2", "]", "-", "patch_size", "[", "i", "]", "//", "2", "for", "i", "in", "range", "(", "len", "(", "patch_size", ")", ")", "]", ")", ".", "astype", "(", "int", ")", "\n", "num_steps", "=", "np", ".", "ceil", "(", "[", "(", "center_coord_end", "[", "i", "]", "-", "center_coord_start", "[", "i", "]", ")", "/", "(", "patch_size", "[", "i", "]", "/", "step", ")", "for", "i", "in", "range", "(", "2", ")", "]", ")", "\n", "step_size", "=", "np", ".", "array", "(", "[", "(", "center_coord_end", "[", "i", "]", "-", "center_coord_start", "[", "i", "]", ")", "/", "(", "num_steps", "[", "i", "]", "+", "1e-8", ")", "for", "i", "in", "range", "(", "2", ")", "]", ")", "\n", "step_size", "[", "step_size", "==", "0", "]", "=", "9999999", "\n", "xsteps", "=", "np", ".", "round", "(", "np", ".", "arange", "(", "center_coord_start", "[", "0", "]", ",", "center_coord_end", "[", "0", "]", "+", "1e-8", ",", "step_size", "[", "0", "]", ")", ")", ".", "astype", "(", "int", ")", "\n", "ysteps", "=", "np", ".", "round", "(", "np", ".", "arange", "(", "center_coord_start", "[", "1", "]", ",", "center_coord_end", "[", "1", "]", "+", "1e-8", ",", "step_size", "[", "1", "]", ")", ")", ".", "astype", "(", "int", ")", "\n", "\n", "for", "x", "in", "xsteps", ":", "\n", "                ", "lb_x", "=", "x", "-", "patch_size", "[", "0", "]", "//", "2", "\n", "ub_x", "=", "x", "+", "patch_size", "[", "0", "]", "//", "2", "\n", "for", "y", "in", "ysteps", ":", "\n", "                    ", "lb_y", "=", "y", "-", "patch_size", "[", "1", "]", "//", "2", "\n", "ub_y", "=", "y", "+", "patch_size", "[", "1", "]", "//", "2", "\n", "result", "[", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", "]", "+=", "self", ".", "_internal_maybe_mirror_and_pred_2D", "(", "data", "[", ":", ",", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", "]", ",", "\n", "num_repeats", ",", "mirror_axes", ",", "do_mirroring", ",", "add_torch", ")", "[", "0", "]", "\n", "result_numsamples", "[", ":", ",", "lb_x", ":", "ub_x", ",", "lb_y", ":", "ub_y", "]", "+=", "add", "\n", "\n", "", "", "slicer", "=", "tuple", "(", "[", "slice", "(", "0", ",", "result", ".", "shape", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "result", ".", "shape", ")", "-", "(", "len", "(", "slicer", ")", "-", "1", ")", ")", "]", "+", "slicer", "[", "1", ":", "]", ")", "\n", "result", "=", "result", "[", "slicer", "]", "\n", "result_numsamples", "=", "result_numsamples", "[", "slicer", "]", "\n", "softmax_pred", "=", "result", "/", "result_numsamples", "\n", "\n", "if", "regions_class_order", "is", "None", ":", "\n", "                ", "predicted_segmentation", "=", "softmax_pred", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "                ", "predicted_segmentation_shp", "=", "softmax_pred", "[", "0", "]", ".", "shape", "\n", "predicted_segmentation", "=", "np", ".", "zeros", "(", "predicted_segmentation_shp", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "regions_class_order", ")", ":", "\n", "                    ", "predicted_segmentation", "[", "softmax_pred", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "", "", "", "return", "predicted_segmentation", ",", "None", ",", "softmax_pred", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_2Dconv": [[468, 482], ["range", "numpy.vstack", "numpy.vstack().transpose", "len", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv", "numpy.vstack.append", "numpy.vstack().transpose.append", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv"], ["", "def", "_internal_predict_3D_2Dconv", "(", "self", ",", "data", ",", "do_mirroring", ",", "num_repeats", ",", "min_size", "=", "None", ",", "BATCH_SIZE", "=", "None", ",", "\n", "mirror_axes", "=", "(", "0", ",", "1", ")", ",", "regions_class_order", "=", "None", ",", "pad_border_mode", "=", "\"edge\"", ",", "pad_kwargs", "=", "None", ")", ":", "\n", "        ", "assert", "len", "(", "data", ".", "shape", ")", "==", "4", ",", "\"data must be c, x, y, z\"", "\n", "predicted_segmentation", "=", "[", "]", "\n", "softmax_pred", "=", "[", "]", "\n", "for", "s", "in", "range", "(", "data", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "pred_seg", ",", "bayesian_predictions", ",", "softmax_pres", ",", "uncertainty", "=", "self", ".", "_internal_predict_2D_2Dconv", "(", "data", "[", ":", ",", "s", "]", ",", "do_mirroring", ",", "num_repeats", ",", "min_size", ",", "BATCH_SIZE", ",", "\n", "mirror_axes", ",", "regions_class_order", ",", "pad_border_mode", ",", "pad_kwargs", ")", "\n", "predicted_segmentation", ".", "append", "(", "pred_seg", "[", "None", "]", ")", "\n", "softmax_pred", ".", "append", "(", "softmax_pres", "[", "None", "]", ")", "\n", "", "predicted_segmentation", "=", "np", ".", "vstack", "(", "predicted_segmentation", ")", "\n", "softmax_pred", "=", "np", ".", "vstack", "(", "softmax_pred", ")", ".", "transpose", "(", "(", "1", ",", "0", ",", "2", ",", "3", ")", ")", "\n", "return", "predicted_segmentation", ",", "None", ",", "softmax_pred", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork.predict_3D_pseudo3D_2Dconv": [[483, 506], ["numpy.array", "numpy.zeros", "numpy.concatenate", "range", "numpy.vstack", "numpy.vstack().transpose", "len", "d.reshape.reshape.reshape", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv", "numpy.vstack.append", "numpy.vstack().transpose.append", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv"], ["", "def", "predict_3D_pseudo3D_2Dconv", "(", "self", ",", "data", ",", "do_mirroring", ",", "num_repeats", ",", "min_size", "=", "None", ",", "BATCH_SIZE", "=", "None", ",", "\n", "mirror_axes", "=", "(", "0", ",", "1", ")", ",", "regions_class_order", "=", "None", ",", "pseudo3D_slices", "=", "5", ")", ":", "\n", "        ", "assert", "len", "(", "data", ".", "shape", ")", "==", "4", ",", "\"data must be c, x, y, z\"", "\n", "assert", "pseudo3D_slices", "%", "2", "==", "1", ",", "\"pseudo3D_slices must be odd\"", "\n", "extra_slices", "=", "(", "pseudo3D_slices", "-", "1", ")", "//", "2", "\n", "shp_for_pad", "=", "np", ".", "array", "(", "data", ".", "shape", ")", "\n", "shp_for_pad", "[", "1", "]", "=", "extra_slices", "\n", "pad", "=", "np", ".", "zeros", "(", "shp_for_pad", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "data", "=", "np", ".", "concatenate", "(", "(", "pad", ",", "data", ",", "pad", ")", ",", "1", ")", "\n", "predicted_segmentation", "=", "[", "]", "\n", "softmax_pred", "=", "[", "]", "\n", "for", "s", "in", "range", "(", "extra_slices", ",", "data", ".", "shape", "[", "1", "]", "-", "extra_slices", ")", ":", "\n", "            ", "d", "=", "data", "[", ":", ",", "(", "s", "-", "extra_slices", ")", ":", "(", "s", "+", "extra_slices", "+", "1", ")", "]", "\n", "d", "=", "d", ".", "reshape", "(", "(", "-", "1", ",", "d", ".", "shape", "[", "-", "2", "]", ",", "d", ".", "shape", "[", "-", "1", "]", ")", ")", "\n", "pred_seg", ",", "bayesian_predictions", ",", "softmax_pres", ",", "uncertainty", "=", "self", ".", "_internal_predict_2D_2Dconv", "(", "d", ",", "do_mirroring", ",", "num_repeats", ",", "min_size", ",", "BATCH_SIZE", ",", "mirror_axes", ",", "\n", "regions_class_order", ")", "\n", "predicted_segmentation", ".", "append", "(", "pred_seg", "[", "None", "]", ")", "\n", "softmax_pred", ".", "append", "(", "softmax_pres", "[", "None", "]", ")", "\n", "", "predicted_segmentation", "=", "np", ".", "vstack", "(", "predicted_segmentation", ")", "\n", "softmax_pred", "=", "np", ".", "vstack", "(", "softmax_pred", ")", ".", "transpose", "(", "(", "1", ",", "0", ",", "2", ",", "3", ")", ")", "\n", "\n", "return", "predicted_segmentation", ",", "None", ",", "softmax_pred", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_3D_2Dconv_tiled": [[507, 523], ["range", "numpy.vstack", "numpy.vstack().transpose", "len", "neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled", "numpy.vstack.append", "numpy.vstack().transpose.append", "numpy.vstack"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork._internal_predict_2D_2Dconv_tiled"], ["", "def", "_internal_predict_3D_2Dconv_tiled", "(", "self", ",", "data", ",", "do_mirroring", ",", "num_repeats", ",", "BATCH_SIZE", "=", "None", ",", "mirror_axes", "=", "(", "0", ",", "1", ")", ",", "\n", "step", "=", "2", ",", "patch_size", "=", "None", ",", "regions_class_order", "=", "None", ",", "use_gaussian", "=", "False", ",", "\n", "pad_border_mode", "=", "\"edge\"", ",", "pad_kwargs", "=", "None", ")", ":", "\n", "        ", "assert", "len", "(", "data", ".", "shape", ")", "==", "4", ",", "\"data must be c, x, y, z\"", "\n", "predicted_segmentation", "=", "[", "]", "\n", "softmax_pred", "=", "[", "]", "\n", "for", "s", "in", "range", "(", "data", ".", "shape", "[", "1", "]", ")", ":", "\n", "            ", "pred_seg", ",", "bayesian_predictions", ",", "softmax_pres", ",", "uncertainty", "=", "self", ".", "_internal_predict_2D_2Dconv_tiled", "(", "data", "[", ":", ",", "s", "]", ",", "num_repeats", ",", "BATCH_SIZE", ",", "step", ",", "do_mirroring", ",", "\n", "mirror_axes", ",", "patch_size", ",", "regions_class_order", ",", "use_gaussian", ",", "\n", "pad_border_mode", "=", "pad_border_mode", ",", "pad_kwargs", "=", "pad_kwargs", ")", "\n", "predicted_segmentation", ".", "append", "(", "pred_seg", "[", "None", "]", ")", "\n", "softmax_pred", ".", "append", "(", "softmax_pres", "[", "None", "]", ")", "\n", "", "predicted_segmentation", "=", "np", ".", "vstack", "(", "predicted_segmentation", ")", "\n", "softmax_pred", "=", "np", ".", "vstack", "(", "softmax_pred", ")", ".", "transpose", "(", "(", "1", ",", "0", ",", "2", ",", "3", ")", ")", "\n", "return", "predicted_segmentation", ",", "None", ",", "softmax_pred", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataset_conversion.LiverTumorSegmentationChallenge.export_segmentations": [[23, 30], ["subfiles", "str", "join", "SimpleITK.ReadImage", "SimpleITK.WriteImage", "join", "n.split"], "function", ["None"], ["def", "export_segmentations", "(", "indir", ",", "outdir", ")", ":", "\n", "    ", "niftis", "=", "subfiles", "(", "indir", ",", "suffix", "=", "'nii.gz'", ",", "join", "=", "False", ")", "\n", "for", "n", "in", "niftis", ":", "\n", "        ", "identifier", "=", "str", "(", "n", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", ")", "\n", "outfname", "=", "join", "(", "outdir", ",", "\"test-segmentation-%s.nii\"", "%", "identifier", ")", "\n", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "indir", ",", "n", ")", ")", "\n", "sitk", ".", "WriteImage", "(", "img", ",", "outfname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataset_conversion.LiverTumorSegmentationChallenge.export_segmentations_postprocess": [[32, 51], ["maybe_mkdir_p", "subfiles", "print", "str", "join", "SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "scipy.ndimage.label", "range", "print", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "join", "sizes.append", "numpy.argmax", "n.split"], "function", ["None"], ["", "", "def", "export_segmentations_postprocess", "(", "indir", ",", "outdir", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "outdir", ")", "\n", "niftis", "=", "subfiles", "(", "indir", ",", "suffix", "=", "'nii.gz'", ",", "join", "=", "False", ")", "\n", "for", "n", "in", "niftis", ":", "\n", "        ", "print", "(", "\"\\n\"", ",", "n", ")", "\n", "identifier", "=", "str", "(", "n", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", ")", "\n", "outfname", "=", "join", "(", "outdir", ",", "\"test-segmentation-%s.nii\"", "%", "identifier", ")", "\n", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "indir", ",", "n", ")", ")", "\n", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img", ")", "\n", "lmap", ",", "num_objects", "=", "label", "(", "(", "img_npy", ">", "0", ")", ".", "astype", "(", "int", ")", ")", "\n", "sizes", "=", "[", "]", "\n", "for", "o", "in", "range", "(", "1", ",", "num_objects", "+", "1", ")", ":", "\n", "            ", "sizes", ".", "append", "(", "(", "lmap", "==", "o", ")", ".", "sum", "(", ")", ")", "\n", "", "mx", "=", "np", ".", "argmax", "(", "sizes", ")", "+", "1", "\n", "print", "(", "sizes", ")", "\n", "img_npy", "[", "lmap", "!=", "mx", "]", "=", "0", "\n", "img_new", "=", "sitk", ".", "GetImageFromArray", "(", "img_npy", ")", "\n", "img_new", ".", "CopyInformation", "(", "img", ")", "\n", "sitk", ".", "WriteImage", "(", "img_new", ",", "outfname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataset_conversion.KidneyTumorSegmentationChallenge.export_segmentations": [[23, 30], ["subfiles", "str", "join", "SimpleITK.ReadImage", "SimpleITK.WriteImage", "join", "n.split"], "function", ["None"], ["def", "export_segmentations", "(", "indir", ",", "outdir", ")", ":", "\n", "    ", "niftis", "=", "subfiles", "(", "indir", ",", "suffix", "=", "'nii.gz'", ",", "join", "=", "False", ")", "\n", "for", "n", "in", "niftis", ":", "\n", "        ", "identifier", "=", "str", "(", "n", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", ")", "\n", "outfname", "=", "join", "(", "outdir", ",", "\"test-segmentation-%s.nii\"", "%", "identifier", ")", "\n", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "indir", ",", "n", ")", ")", "\n", "sitk", ".", "WriteImage", "(", "img", ",", "outfname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataset_conversion.KidneyTumorSegmentationChallenge.export_segmentations_postprocess": [[32, 51], ["maybe_mkdir_p", "subfiles", "print", "str", "join", "SimpleITK.ReadImage", "SimpleITK.GetArrayFromImage", "scipy.ndimage.label", "range", "print", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "join", "sizes.append", "numpy.argmax", "n.split"], "function", ["None"], ["", "", "def", "export_segmentations_postprocess", "(", "indir", ",", "outdir", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "outdir", ")", "\n", "niftis", "=", "subfiles", "(", "indir", ",", "suffix", "=", "'nii.gz'", ",", "join", "=", "False", ")", "\n", "for", "n", "in", "niftis", ":", "\n", "        ", "print", "(", "\"\\n\"", ",", "n", ")", "\n", "identifier", "=", "str", "(", "n", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", "[", ":", "-", "7", "]", ")", "\n", "outfname", "=", "join", "(", "outdir", ",", "\"test-segmentation-%s.nii\"", "%", "identifier", ")", "\n", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "indir", ",", "n", ")", ")", "\n", "img_npy", "=", "sitk", ".", "GetArrayFromImage", "(", "img", ")", "\n", "lmap", ",", "num_objects", "=", "label", "(", "(", "img_npy", ">", "0", ")", ".", "astype", "(", "int", ")", ")", "\n", "sizes", "=", "[", "]", "\n", "for", "o", "in", "range", "(", "1", ",", "num_objects", "+", "1", ")", ":", "\n", "            ", "sizes", ".", "append", "(", "(", "lmap", "==", "o", ")", ".", "sum", "(", ")", ")", "\n", "", "mx", "=", "np", ".", "argmax", "(", "sizes", ")", "+", "1", "\n", "print", "(", "sizes", ")", "\n", "img_npy", "[", "lmap", "!=", "mx", "]", "=", "0", "\n", "img_new", "=", "sitk", ".", "GetImageFromArray", "(", "img_npy", ")", "\n", "img_new", ".", "CopyInformation", "(", "img", ")", "\n", "sitk", ".", "WriteImage", "(", "img_new", ",", "outfname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataset_conversion.AutomaticCardiacDetectionChallenge.convert_to_submission": [[20, 31], ["subfiles", "np.unique", "maybe_mkdir_p", "subfiles", "len", "subfiles.sort", "shutil.copy", "shutil.copy", "join", "join", "join", "join"], "function", ["None"], ["def", "convert_to_submission", "(", "source_dir", ",", "target_dir", ")", ":", "\n", "    ", "niftis", "=", "subfiles", "(", "source_dir", ",", "join", "=", "False", ",", "suffix", "=", "\".nii.gz\"", ")", "\n", "patientids", "=", "np", ".", "unique", "(", "[", "i", "[", ":", "10", "]", "for", "i", "in", "niftis", "]", ")", "\n", "maybe_mkdir_p", "(", "target_dir", ")", "\n", "for", "p", "in", "patientids", ":", "\n", "        ", "files_of_that_patient", "=", "subfiles", "(", "source_dir", ",", "prefix", "=", "p", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "assert", "len", "(", "files_of_that_patient", ")", "\n", "files_of_that_patient", ".", "sort", "(", ")", "\n", "# first is ED, second is ES", "\n", "shutil", ".", "copy", "(", "join", "(", "source_dir", ",", "files_of_that_patient", "[", "0", "]", ")", ",", "join", "(", "target_dir", ",", "p", "+", "\"_ED.nii.gz\"", ")", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "source_dir", ",", "files_of_that_patient", "[", "1", "]", ")", ",", "join", "(", "target_dir", ",", "p", "+", "\"_ES.nii.gz\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataset_conversion.ISBI_MSLesionSegmentationChallenge.convert_to_nii_gz": [[23, 27], ["SimpleITK.ReadImage", "SimpleITK.WriteImage", "os.remove", "os.path.splitext"], "function", ["None"], ["def", "convert_to_nii_gz", "(", "filename", ")", ":", "\n", "    ", "f", "=", "sitk", ".", "ReadImage", "(", "filename", ")", "\n", "sitk", ".", "WriteImage", "(", "f", ",", "os", ".", "path", ".", "splitext", "(", "filename", ")", "[", "0", "]", "+", "\".nii.gz\"", ")", "\n", "os", ".", "remove", "(", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataset_conversion.ISBI_MSLesionSegmentationChallenge.convert_for_submission": [[29, 39], ["subfiles", "maybe_mkdir_p", "f.split", "int", "int", "join", "SimpleITK.ReadImage", "SimpleITK.WriteImage", "join"], "function", ["None"], ["", "def", "convert_for_submission", "(", "source_dir", ",", "target_dir", ")", ":", "\n", "    ", "files", "=", "subfiles", "(", "source_dir", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "maybe_mkdir_p", "(", "target_dir", ")", "\n", "for", "f", "in", "files", ":", "\n", "        ", "splitted", "=", "f", ".", "split", "(", "\"__\"", ")", "\n", "case_id", "=", "int", "(", "splitted", "[", "1", "]", ")", "\n", "timestep", "=", "int", "(", "splitted", "[", "2", "]", "[", ":", "-", "7", "]", ")", "\n", "t", "=", "join", "(", "target_dir", ",", "\"test%02d_%02d_nnUNet.nii\"", "%", "(", "case_id", ",", "timestep", ")", ")", "\n", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "source_dir", ",", "f", ")", ")", "\n", "sitk", ".", "WriteImage", "(", "img", ",", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataset_conversion.Promise2012.export_for_submission": [[20, 33], ["subfiles", "maybe_mkdir_p", "zip", "join", "SimpleITK.ReadImage", "SimpleITK.WriteImage", "join"], "function", ["None"], ["def", "export_for_submission", "(", "source_dir", ",", "target_dir", ")", ":", "\n", "    ", "\"\"\"\n    promise wants mhd :-/\n    :param source_dir:\n    :param target_dir:\n    :return:\n    \"\"\"", "\n", "files", "=", "subfiles", "(", "source_dir", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ")", "\n", "target_files", "=", "[", "join", "(", "target_dir", ",", "i", "[", ":", "-", "7", "]", "+", "\".mhd\"", ")", "for", "i", "in", "files", "]", "\n", "maybe_mkdir_p", "(", "target_dir", ")", "\n", "for", "f", ",", "t", "in", "zip", "(", "files", ",", "target_files", ")", ":", "\n", "        ", "img", "=", "sitk", ".", "ReadImage", "(", "join", "(", "source_dir", ",", "f", ")", ")", "\n", "sitk", ".", "WriteImage", "(", "img", ",", "t", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.run.default_configuration.get_configuration_from_output_folder": [[23, 32], ["folder.startswith", "folder.split", "trainer_and_plans_identifier.split", "len"], "function", ["None"], ["def", "get_configuration_from_output_folder", "(", "folder", ")", ":", "\n", "# split off network_training_output_dir", "\n", "    ", "folder", "=", "folder", "[", "len", "(", "network_training_output_dir", ")", ":", "]", "\n", "if", "folder", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "folder", "=", "folder", "[", "1", ":", "]", "\n", "\n", "", "configuration", ",", "task", ",", "trainer_and_plans_identifier", "=", "folder", ".", "split", "(", "\"/\"", ")", "\n", "trainer", ",", "plans_identifier", "=", "trainer_and_plans_identifier", ".", "split", "(", "\"__\"", ")", "\n", "return", "configuration", ",", "task", ",", "trainer", ",", "plans_identifier", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.run.default_configuration.get_output_folder": [[34, 36], ["join"], "function", ["None"], ["", "def", "get_output_folder", "(", "configuration", ",", "task", ",", "trainer", ",", "plans_identifier", ")", ":", "\n", "    ", "return", "join", "(", "network_training_output_dir", ",", "configuration", ",", "task", ",", "trainer", "+", "\"__\"", "+", "plans_identifier", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.run.default_configuration.get_default_configuration": [[38, 95], ["join", "load_pickle", "list", "nnunet.training.model_restore.recursive_find_trainer", "join", "print", "print", "print", "print", "nnunet.experiment_planning.summarize_plans.summarize_plans", "print", "print", "print", "join", "join", "plans[].keys", "RuntimeError", "print", "print", "join", "len", "join", "len"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.recursive_find_trainer", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.summarize_plans.summarize_plans"], ["", "def", "get_default_configuration", "(", "network", ",", "task", ",", "network_trainer", ",", "plans_identifier", "=", "default_plans_identifier", ",", "\n", "search_in", "=", "(", "nnunet", ".", "__path__", "[", "0", "]", ",", "\"training\"", ",", "\"network_training\"", ")", ",", "\n", "base_module", "=", "'nnunet.training.network_training'", ")", ":", "\n", "    ", "assert", "network", "in", "[", "'2d'", ",", "'3d_lowres'", ",", "'3d_fullres'", ",", "'3d_cascade_fullres'", "]", ",", "\"network can only be one of the following: \\'3d\\', \\'3d_lowres\\', \\'3d_fullres\\', \\'3d_cascade_fullres\\'\"", "\n", "\n", "dataset_directory", "=", "join", "(", "preprocessing_output_dir", ",", "task", ")", "\n", "\n", "if", "network", "==", "'2d'", ":", "\n", "        ", "plans_file", "=", "join", "(", "preprocessing_output_dir", ",", "task", ",", "plans_identifier", "+", "\"_plans_2D.pkl\"", ")", "\n", "", "else", ":", "\n", "        ", "plans_file", "=", "join", "(", "preprocessing_output_dir", ",", "task", "+", "'/'", ",", "plans_identifier", "+", "\"_plans_3D.pkl\"", ")", "\n", "\n", "", "plans", "=", "load_pickle", "(", "plans_file", ")", "\n", "\"\"\"\n    plans_file content:\n    dict_keys(['preprocessed_data_folder', 'min_size_per_class', 'num_classes', 'keep_only_largest_region', 'original_sizes', 'original_spacings', 'all_classes', 'modalities', 'base_num_features', 'min_region_size_per_class', 'data_identifier', 'use_mask_for_norm', 'num_modalities', 'normalization_schemes', 'plans_per_stage', 'num_stages', 'dataset_properties', 'list_of_npz_files'])\n    \"\"\"", "\n", "possible_stages", "=", "list", "(", "plans", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", "\n", "\"\"\"\n     file_data['plans_per_stage'][0]\n     {'pool_op_kernel_sizes': [[2, 2, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'num_pool_per_axis': [5, 5, 4], 'batch_size': 2, 'original_spacing': array([0.78162497, 0.78162497, 3.        ]), 'current_spacing': array([1.5530063, 1.5530063, 3.090903 ]), 'do_dummy_2D_data_aug': False, 'patch_size': array([160, 160,  80]), 'median_patient_size_in_voxels': array([263, 263, 113])}\n     file_data['plans_per_stage'][1]\n     {'pool_op_kernel_sizes': [[2, 2, 1], [2, 2, 1], [2, 2, 2], [2, 2, 2], [2, 2, 2]], 'conv_kernel_sizes': [[3, 3, 1], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3], [3, 3, 3]], 'num_pool_per_axis': [5, 5, 3], 'batch_size': 2, 'original_spacing': array([0.78162497, 0.78162497, 3.        ]), 'current_spacing': array([0.78162497, 0.78162497, 3.        ]), 'do_dummy_2D_data_aug': False, 'patch_size': array([192, 192,  48]), 'median_patient_size_in_voxels': array([523, 523, 116])}\n    \"\"\"", "\n", "\n", "if", "(", "network", "==", "'3d_cascade_fullres'", "or", "network", "==", "\"3d_lowres\"", ")", "and", "len", "(", "possible_stages", ")", "==", "1", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"3d_lowres/3d_cascade_fullres only applies if there is more than one stage. This task does \"", "\n", "\"not require the cascade. Run 3d_fullres instead\"", ")", "\n", "\n", "", "if", "network", "==", "'2d'", "or", "network", "==", "\"3d_lowres\"", ":", "\n", "        ", "stage", "=", "0", "\n", "", "else", ":", "\n", "        ", "stage", "=", "possible_stages", "[", "-", "1", "]", "\n", "\n", "", "trainer_class", "=", "recursive_find_trainer", "(", "[", "join", "(", "*", "search_in", ")", "]", ",", "network_trainer", ",", "\n", "current_module", "=", "base_module", ")", "\n", "\n", "output_folder_name", "=", "join", "(", "network_training_output_dir", "+", "'/'", ",", "network", "+", "'/'", ",", "task", "+", "'/'", ",", "network_trainer", "+", "\"__\"", "+", "plans_identifier", "+", "'/'", ")", "\n", "\n", "print", "(", "\"###############################################\"", ")", "\n", "print", "(", "\"I am running the following nnUNet: %s\"", "%", "network", ")", "\n", "print", "(", "\"My trainer class is: \"", ",", "trainer_class", ")", "\n", "print", "(", "\"For that I will be using the following configuration:\"", ")", "\n", "summarize_plans", "(", "plans_file", ")", "\n", "print", "(", "\"I am using stage %d from these plans\"", "%", "stage", ")", "\n", "\n", "if", "(", "network", "==", "'2d'", "or", "len", "(", "possible_stages", ")", ">", "1", ")", "and", "not", "network", "==", "'3d_lowres'", ":", "\n", "        ", "batch_dice", "=", "True", "\n", "print", "(", "\"I am using batch dice + CE loss\"", ")", "\n", "", "else", ":", "\n", "        ", "batch_dice", "=", "False", "\n", "print", "(", "\"I am using sample dice + CE loss\"", ")", "\n", "\n", "", "print", "(", "\"\\nI am using data from this folder: \"", ",", "join", "(", "dataset_directory", ",", "plans", "[", "'data_identifier'", "]", ")", ")", "\n", "print", "(", "\"###############################################\"", ")", "\n", "return", "plans_file", ",", "output_folder_name", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "trainer_class", "\n", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.recursive_find_trainer": [[23, 43], ["pkgutil.iter_modules", "pkgutil.iter_modules", "importlib.import_module", "hasattr", "getattr", "model_restore.recursive_find_trainer", "join"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.recursive_find_trainer"], ["def", "recursive_find_trainer", "(", "folder", ",", "trainer_name", ",", "current_module", ")", ":", "\n", "    ", "tr", "=", "None", "\n", "for", "importer", ",", "modname", ",", "ispkg", "in", "pkgutil", ".", "iter_modules", "(", "folder", ")", ":", "\n", "# print(modname, ispkg)", "\n", "        ", "if", "not", "ispkg", ":", "\n", "            ", "m", "=", "importlib", ".", "import_module", "(", "current_module", "+", "\".\"", "+", "modname", ")", "\n", "if", "hasattr", "(", "m", ",", "trainer_name", ")", ":", "\n", "                ", "tr", "=", "getattr", "(", "m", ",", "trainer_name", ")", "\n", "break", "\n", "# tr is a class name: 'nnUNetTrainer'", "\n", "\n", "", "", "", "if", "tr", "is", "None", ":", "\n", "        ", "for", "importer", ",", "modname", ",", "ispkg", "in", "pkgutil", ".", "iter_modules", "(", "folder", ")", ":", "\n", "            ", "if", "ispkg", ":", "\n", "                ", "next_current_module", "=", "current_module", "+", "\".\"", "+", "modname", "\n", "tr", "=", "recursive_find_trainer", "(", "[", "join", "(", "folder", "[", "0", "]", ",", "modname", ")", "]", ",", "trainer_name", ",", "current_module", "=", "next_current_module", ")", "\n", "", "if", "tr", "is", "not", "None", ":", "\n", "                ", "break", "\n", "\n", "", "", "", "return", "tr", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.restore_model": [[45, 82], ["load_pickle", "join", "model_restore.recursive_find_trainer", "issubclass", "recursive_find_trainer.", "tr.process_plans", "RuntimeError", "len", "print", "print", "tr.load_checkpoint", "range", "len"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.recursive_find_trainer", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.process_plans", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_checkpoint"], ["", "def", "restore_model", "(", "pkl_file", ",", "checkpoint", "=", "None", ",", "train", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    This is a utility function to load any nnUNet trainer from a pkl. It will recursively search\n    nnunet.trainig.network_training for the file that contains the trainer and instantiate it with the arguments saved in the pkl file. If checkpoint\n    is specified, it will furthermore load the checkpoint file in train/test mode (as specified by train).\n    The pkl file required here is the one that will be saved automatically when calling nnUNetTrainer.save_checkpoint.\n    :param pkl_file:\n    :param checkpoint:\n    :param train:\n    :return:\n    \"\"\"", "\n", "info", "=", "load_pickle", "(", "pkl_file", ")", "\n", "init", "=", "info", "[", "'init'", "]", "\n", "name", "=", "info", "[", "'name'", "]", "\n", "search_in", "=", "join", "(", "nnunet", ".", "__path__", "[", "0", "]", ",", "\"training\"", ",", "\"network_training\"", ")", "\n", "tr", "=", "recursive_find_trainer", "(", "[", "search_in", "]", ",", "name", ",", "current_module", "=", "\"nnunet.training.network_training\"", ")", "\n", "if", "tr", "is", "None", ":", "\n", "        ", "raise", "RuntimeError", "(", "\"Could not find the model trainer specified in checkpoint in nnunet.trainig.network_training. If it \"", "\n", "\"is not located there, please move it or change the code of restore_model. Your model \"", "\n", "\"trainer can be located in any directory within nnunet.trainig.network_training (search is recursive).\"", "\n", "\"\\nDebug info: \\ncheckpoint file: %s\\nName of trainer: %s \"", "%", "(", "checkpoint", ",", "name", ")", ")", "\n", "", "assert", "issubclass", "(", "tr", ",", "nnUNetTrainer", ")", ",", "\"The network trainer was found but is not a subclass of nnUNetTrainer. \"", "\"Please make it so!\"", "\n", "\n", "if", "len", "(", "init", ")", "==", "7", ":", "\n", "        ", "print", "(", "\"warning: this model seems to have been saved with a previous version of nnUNet. Attempting to load it \"", "\n", "\"anyways. Expect the unexpected.\"", ")", "\n", "print", "(", "\"manually editing init args...\"", ")", "\n", "init", "=", "[", "init", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "init", ")", ")", "if", "i", "!=", "2", "]", "\n", "\n", "# init[0] is the plans file. This argument needs to be replaced because the original plans file may not exist", "\n", "# anymore.", "\n", "", "trainer", "=", "tr", "(", "*", "init", ")", "\n", "trainer", ".", "process_plans", "(", "info", "[", "'plans'", "]", ")", "\n", "if", "checkpoint", "is", "not", "None", ":", "\n", "        ", "trainer", ".", "load_checkpoint", "(", "checkpoint", ",", "train", ")", "\n", "", "return", "trainer", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.load_best_model_for_inference": [[84, 88], ["join", "model_restore.restore_model"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.restore_model"], ["", "def", "load_best_model_for_inference", "(", "folder", ")", ":", "\n", "    ", "checkpoint", "=", "join", "(", "folder", ",", "\"model_best.model\"", ")", "\n", "pkl_file", "=", "checkpoint", "+", "\".pkl\"", "\n", "return", "restore_model", "(", "pkl_file", ",", "checkpoint", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.load_model_and_checkpoint_files": [[90, 128], ["isinstance", "model_restore.restore_model", "restore_model.update_fold", "restore_model.initialize", "print", "isdir", "isinstance", "join", "join", "torch.load", "join", "all", "isinstance", "all", "torch.device", "len", "join", "join", "isdir", "join", "print", "subfolders", "print", "ValueError", "torch.cuda.current_device", "isdir", "str", "type"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.restore_model", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.update_fold", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.initialize"], ["", "def", "load_model_and_checkpoint_files", "(", "folder", ",", "folds", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    used for if you need to ensemble the five models of a cross-validation. This will restore the model from the\n    checkpoint in fold 0, load all parameters of the five folds in ram and return both. This will allow for fast\n    switching between parameters (as opposed to loading them form disk each time).\n\n    This is best used for inference and test prediction\n    :param folder:\n    :return:\n    \"\"\"", "\n", "if", "isinstance", "(", "folds", ",", "str", ")", ":", "\n", "        ", "folds", "=", "[", "join", "(", "folder", ",", "\"all\"", ")", "]", "\n", "assert", "isdir", "(", "folds", "[", "0", "]", ")", ",", "\"no output folder for fold %s found\"", "%", "folds", "\n", "", "elif", "isinstance", "(", "folds", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "        ", "if", "len", "(", "folds", ")", "==", "1", "and", "folds", "[", "0", "]", "==", "\"all\"", ":", "\n", "            ", "folds", "=", "[", "join", "(", "folder", ",", "\"all\"", ")", "]", "\n", "", "else", ":", "\n", "            ", "folds", "=", "[", "join", "(", "folder", ",", "\"fold_%d\"", "%", "i", ")", "for", "i", "in", "folds", "]", "\n", "", "assert", "all", "(", "[", "isdir", "(", "i", ")", "for", "i", "in", "folds", "]", ")", ",", "\"list of folds specified but not all output folders are present\"", "\n", "", "elif", "isinstance", "(", "folds", ",", "int", ")", ":", "\n", "        ", "folds", "=", "[", "join", "(", "folder", ",", "\"fold_%d\"", "%", "folds", ")", "]", "\n", "assert", "all", "(", "[", "isdir", "(", "i", ")", "for", "i", "in", "folds", "]", ")", ",", "\"output folder missing for fold %d\"", "%", "folds", "\n", "", "elif", "folds", "is", "None", ":", "\n", "        ", "print", "(", "\"folds is None so we will automatically look for output folders (not using \\'all\\'!)\"", ")", "\n", "folds", "=", "subfolders", "(", "folder", ",", "prefix", "=", "\"fold\"", ")", "\n", "print", "(", "\"found the following folds: \"", ",", "folds", ")", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "\"Unknown value for folds. Type: %s. Expected: list of int, int, str or None\"", ",", "str", "(", "type", "(", "folds", ")", ")", ")", "\n", "\n", "", "trainer", "=", "restore_model", "(", "join", "(", "folds", "[", "0", "]", ",", "\"model_best.model.pkl\"", ")", ")", "\n", "trainer", ".", "output_folder", "=", "folder", "\n", "trainer", ".", "output_folder_base", "=", "folder", "\n", "trainer", ".", "update_fold", "(", "0", ")", "\n", "trainer", ".", "initialize", "(", "False", ")", "\n", "all_best_model_files", "=", "[", "join", "(", "i", ",", "\"model_best.model\"", ")", "for", "i", "in", "folds", "]", "\n", "print", "(", "\"using the following model files: \"", ",", "all_best_model_files", ")", "\n", "all_params", "=", "[", "torch", ".", "load", "(", "i", ",", "map_location", "=", "torch", ".", "device", "(", "'cuda'", ",", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", ")", "for", "i", "in", "all_best_model_files", "]", "\n", "return", "trainer", ",", "all_params", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.cascade_stuff.predict_next_stage.resample_and_save": [[28, 32], ["nnunet.preprocessing.preprocessing.resample_data_or_seg", "nnunet.preprocessing.preprocessing.resample_data_or_seg.argmax", "numpy.savez_compressed", "predicted_new_shape.argmax.astype"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.resample_data_or_seg"], ["def", "resample_and_save", "(", "predicted", ",", "target_shape", ",", "output_file", ")", ":", "\n", "    ", "predicted_new_shape", "=", "resample_data_or_seg", "(", "predicted", ",", "target_shape", ",", "False", ",", "order", "=", "1", ",", "do_separate_z", "=", "False", ",", "cval", "=", "0", ")", "\n", "seg_new_shape", "=", "predicted_new_shape", ".", "argmax", "(", "0", ")", "\n", "np", ".", "savez_compressed", "(", "output_file", ",", "data", "=", "seg_new_shape", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.cascade_stuff.predict_next_stage.predict_next_stage": [[34, 56], ["join", "batchgenerators.utilities.file_and_folder_operations.maybe_mkdir_p", "multiprocessing.Pool", "trainer.dataset_val.keys", "pardir", "print", "trainer.predict_preprocessed_data_return_softmax", "join", "join", "results.append", "i.get", "data_file.split", "numpy.load", "multiprocessing.Pool.starmap_async", "numpy.load", "join.split"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_softmax"], ["", "def", "predict_next_stage", "(", "trainer", ",", "stage_to_be_predicted_folder", ")", ":", "\n", "    ", "output_folder", "=", "join", "(", "pardir", "(", "trainer", ".", "output_folder", ")", ",", "\"pred_next_stage\"", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "\n", "process_manager", "=", "Pool", "(", "2", ")", "\n", "results", "=", "[", "]", "\n", "\n", "for", "pat", "in", "trainer", ".", "dataset_val", ".", "keys", "(", ")", ":", "\n", "        ", "print", "(", "pat", ")", "\n", "data_file", "=", "trainer", ".", "dataset_val", "[", "pat", "]", "[", "'data_file'", "]", "\n", "data_preprocessed", "=", "np", ".", "load", "(", "data_file", ")", "[", "'data'", "]", "[", ":", "-", "1", "]", "\n", "predicted", "=", "trainer", ".", "predict_preprocessed_data_return_softmax", "(", "data_preprocessed", ",", "True", ",", "1", ",", "False", ",", "1", ",", "\n", "trainer", ".", "data_aug_params", "[", "'mirror_axes'", "]", ",", "\n", "True", ",", "True", ",", "2", ",", "trainer", ".", "patch_size", ",", "True", ")", "\n", "data_file_nofolder", "=", "data_file", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "data_file_nextstage", "=", "join", "(", "stage_to_be_predicted_folder", ",", "data_file_nofolder", ")", "\n", "data_nextstage", "=", "np", ".", "load", "(", "data_file_nextstage", ")", "[", "'data'", "]", "\n", "target_shp", "=", "data_nextstage", ".", "shape", "[", "1", ":", "]", "\n", "output_file", "=", "join", "(", "output_folder", ",", "data_file_nextstage", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "4", "]", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "results", ".", "append", "(", "process_manager", ".", "starmap_async", "(", "resample_and_save", ",", "[", "(", "predicted", ",", "target_shp", ",", "output_file", ")", "]", ")", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.pyramid_augmentations.RemoveRandomConnectedComponentFromOneHotEncodingTransform.__init__": [[23, 37], ["isinstance"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "channel_idx", ",", "key", "=", "\"data\"", ",", "p_per_sample", "=", "0.2", ",", "fill_with_other_class_p", "=", "0.25", ",", "\n", "dont_do_if_covers_more_than_X_percent", "=", "0.25", ")", ":", "\n", "        ", "\"\"\"\n        :param dont_do_if_covers_more_than_X_percent: dont_do_if_covers_more_than_X_percent=0.25 is 25\\%!\n        :param channel_idx: can be list or int\n        :param key:\n        \"\"\"", "\n", "self", ".", "dont_do_if_covers_more_than_X_percent", "=", "dont_do_if_covers_more_than_X_percent", "\n", "self", ".", "fill_with_other_class_p", "=", "fill_with_other_class_p", "\n", "self", ".", "p_per_sample", "=", "p_per_sample", "\n", "self", ".", "key", "=", "key", "\n", "if", "not", "isinstance", "(", "channel_idx", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "channel_idx", "=", "[", "channel_idx", "]", "\n", "", "self", ".", "channel_idx", "=", "channel_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.pyramid_augmentations.RemoveRandomConnectedComponentFromOneHotEncodingTransform.__call__": [[38, 66], ["data_dict.get", "range", "numpy.random.uniform", "numpy.copy", "numpy.prod", "skimage.morphology.label", "range", "component_ids.append", "component_sizes.append", "len", "numpy.random.choice", "numpy.sum", "zip", "numpy.random.uniform", "len", "numpy.random.choice"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "data", "=", "data_dict", ".", "get", "(", "self", ".", "key", ")", "\n", "for", "b", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p_per_sample", ":", "\n", "                ", "for", "c", "in", "self", ".", "channel_idx", ":", "\n", "                    ", "workon", "=", "np", ".", "copy", "(", "data", "[", "b", ",", "c", "]", ")", "\n", "num_voxels", "=", "np", ".", "prod", "(", "workon", ".", "shape", ")", "\n", "lab", ",", "num_comp", "=", "label", "(", "workon", ",", "return_num", "=", "True", ")", "\n", "if", "num_comp", ">", "0", ":", "\n", "                        ", "component_ids", "=", "[", "]", "\n", "component_sizes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "1", ",", "num_comp", "+", "1", ")", ":", "\n", "                            ", "component_ids", ".", "append", "(", "i", ")", "\n", "component_sizes", ".", "append", "(", "np", ".", "sum", "(", "lab", "==", "i", ")", ")", "\n", "", "component_ids", "=", "[", "i", "for", "i", ",", "j", "in", "zip", "(", "component_ids", ",", "component_sizes", ")", "if", "j", "<", "num_voxels", "*", "self", ".", "dont_do_if_covers_more_than_X_percent", "]", "\n", "#_ = component_ids.pop(np.argmax(component_sizes))", "\n", "#else:", "\n", "#    component_ids = list(range(1, num_comp + 1))", "\n", "if", "len", "(", "component_ids", ")", ">", "0", ":", "\n", "                            ", "random_component", "=", "np", ".", "random", ".", "choice", "(", "component_ids", ")", "\n", "data", "[", "b", ",", "c", "]", "[", "lab", "==", "random_component", "]", "=", "0", "\n", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "fill_with_other_class_p", ":", "\n", "                                ", "other_ch", "=", "[", "i", "for", "i", "in", "self", ".", "channel_idx", "if", "i", "!=", "c", "]", "\n", "if", "len", "(", "other_ch", ")", ">", "0", ":", "\n", "                                    ", "other_class", "=", "np", ".", "random", ".", "choice", "(", "other_ch", ")", "\n", "data", "[", "b", ",", "other_class", "]", "[", "lab", "==", "random_component", "]", "=", "1", "\n", "", "", "", "", "", "", "", "data_dict", "[", "self", ".", "key", "]", "=", "data", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData.__init__": [[69, 75], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "channel_id", ",", "all_seg_labels", ",", "key_origin", "=", "\"seg\"", ",", "key_target", "=", "\"data\"", ",", "remove_from_origin", "=", "True", ")", ":", "\n", "        ", "self", ".", "remove_from_origin", "=", "remove_from_origin", "\n", "self", ".", "all_seg_labels", "=", "all_seg_labels", "\n", "self", ".", "key_target", "=", "key_target", "\n", "self", ".", "key_origin", "=", "key_origin", "\n", "self", ".", "channel_id", "=", "channel_id", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData.__call__": [[76, 91], ["data_dict.get", "data_dict.get", "numpy.zeros", "enumerate", "numpy.concatenate", "len", "range"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "origin", "=", "data_dict", ".", "get", "(", "self", ".", "key_origin", ")", "\n", "target", "=", "data_dict", ".", "get", "(", "self", ".", "key_target", ")", "\n", "seg", "=", "origin", "[", ":", ",", "self", ".", "channel_id", ":", "self", ".", "channel_id", "+", "1", "]", "\n", "seg_onehot", "=", "np", ".", "zeros", "(", "(", "seg", ".", "shape", "[", "0", "]", ",", "len", "(", "self", ".", "all_seg_labels", ")", ",", "*", "seg", ".", "shape", "[", "2", ":", "]", ")", ",", "dtype", "=", "seg", ".", "dtype", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "all_seg_labels", ")", ":", "\n", "            ", "seg_onehot", "[", ":", ",", "i", "]", "[", "seg", "[", ":", ",", "0", "]", "==", "l", "]", "=", "1", "\n", "", "target", "=", "np", ".", "concatenate", "(", "(", "target", ",", "seg_onehot", ")", ",", "1", ")", "\n", "data_dict", "[", "self", ".", "key_target", "]", "=", "target", "\n", "\n", "if", "self", ".", "remove_from_origin", ":", "\n", "            ", "remaining_channels", "=", "[", "i", "for", "i", "in", "range", "(", "origin", ".", "shape", "[", "1", "]", ")", "if", "i", "!=", "self", ".", "channel_id", "]", "\n", "origin", "=", "origin", "[", ":", ",", "remaining_channels", "]", "\n", "data_dict", "[", "self", ".", "key_origin", "]", "=", "origin", "\n", "", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform.__init__": [[94, 115], ["isinstance", "isinstance"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "channel_idx", ",", "p_per_sample", "=", "0.3", ",", "any_of_these", "=", "(", "binary_dilation", ",", "binary_erosion", ",", "binary_closing", ",", "binary_opening", ")", ",", "\n", "key", "=", "\"data\"", ",", "strel_size", "=", "(", "1", ",", "10", ")", ")", ":", "\n", "        ", "\"\"\"\n\n        :param channel_idx: can be list or int\n        :param p_per_sample:\n        :param any_of_these:\n        :param fill_diff_with_other_class:\n        :param key:\n        :param strel_size:\n        \"\"\"", "\n", "self", ".", "strel_size", "=", "strel_size", "\n", "self", ".", "key", "=", "key", "\n", "self", ".", "any_of_these", "=", "any_of_these", "\n", "self", ".", "p_per_sample", "=", "p_per_sample", "\n", "\n", "assert", "not", "isinstance", "(", "channel_idx", ",", "tuple", ")", ",", "\"b\u00e4h\"", "\n", "\n", "if", "not", "isinstance", "(", "channel_idx", ",", "list", ")", ":", "\n", "            ", "channel_idx", "=", "[", "channel_idx", "]", "\n", "", "self", ".", "channel_idx", "=", "channel_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform.__call__": [[116, 140], ["data_dict.get", "range", "numpy.random.uniform", "copy.deepcopy", "numpy.random.shuffle", "numpy.random.choice", "skimage.morphology.ball", "numpy.copy().astype", "numpy.random.choice.astype", "numpy.random.uniform", "len", "numpy.copy", "numpy.random.choice."], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "data", "=", "data_dict", ".", "get", "(", "self", ".", "key", ")", "\n", "for", "b", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "if", "np", ".", "random", ".", "uniform", "(", ")", "<", "self", ".", "p_per_sample", ":", "\n", "                ", "ch", "=", "deepcopy", "(", "self", ".", "channel_idx", ")", "\n", "np", ".", "random", ".", "shuffle", "(", "ch", ")", "\n", "for", "c", "in", "ch", ":", "\n", "                    ", "operation", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "any_of_these", ")", "\n", "selem", "=", "ball", "(", "np", ".", "random", ".", "uniform", "(", "*", "self", ".", "strel_size", ")", ")", "\n", "workon", "=", "np", ".", "copy", "(", "data", "[", "b", ",", "c", "]", ")", ".", "astype", "(", "int", ")", "\n", "res", "=", "operation", "(", "workon", ",", "selem", ")", ".", "astype", "(", "workon", ".", "dtype", ")", "\n", "data", "[", "b", ",", "c", "]", "=", "res", "\n", "\n", "# if class was added, we need to remove it in ALL other channels to keep one hot encoding", "\n", "# properties", "\n", "# we modify data", "\n", "other_ch", "=", "[", "i", "for", "i", "in", "ch", "if", "i", "!=", "c", "]", "\n", "if", "len", "(", "other_ch", ")", ">", "0", ":", "\n", "                        ", "was_added_mask", "=", "(", "res", "-", "workon", ")", ">", "0", "\n", "for", "oc", "in", "other_ch", ":", "\n", "                            ", "data", "[", "b", ",", "oc", "]", "[", "was_added_mask", "]", "=", "0", "\n", "# if class was removed, leave it at backgound", "\n", "", "", "", "", "", "data_dict", "[", "self", ".", "key", "]", "=", "data", "\n", "return", "data_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.default_data_augmentation.get_patch_size": [[74, 95], ["isinstance", "isinstance", "isinstance", "min", "min", "min", "numpy.array", "numpy.copy", "min", "np.max.astype", "max", "max", "max", "len", "numpy.max", "numpy.max", "numpy.max", "numpy.abs", "numpy.abs", "numpy.abs", "numpy.vstack", "numpy.vstack", "numpy.vstack", "len", "numpy.max", "numpy.vstack", "rotate_coords_3d", "rotate_coords_3d", "rotate_coords_3d", "rotate_coords_2d"], "function", ["None"], ["def", "get_patch_size", "(", "final_patch_size", ",", "rot_x", ",", "rot_y", ",", "rot_z", ",", "scale_range", ")", ":", "\n", "    ", "if", "isinstance", "(", "rot_x", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "rot_x", "=", "max", "(", "np", ".", "abs", "(", "rot_x", ")", ")", "\n", "", "if", "isinstance", "(", "rot_y", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "rot_y", "=", "max", "(", "np", ".", "abs", "(", "rot_y", ")", ")", "\n", "", "if", "isinstance", "(", "rot_z", ",", "(", "tuple", ",", "list", ")", ")", ":", "\n", "        ", "rot_z", "=", "max", "(", "np", ".", "abs", "(", "rot_z", ")", ")", "\n", "", "rot_x", "=", "min", "(", "90", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "rot_x", ")", "\n", "rot_y", "=", "min", "(", "90", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "rot_y", ")", "\n", "rot_z", "=", "min", "(", "90", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "rot_z", ")", "\n", "from", "batchgenerators", ".", "augmentations", ".", "utils", "import", "rotate_coords_3d", ",", "rotate_coords_2d", "\n", "coords", "=", "np", ".", "array", "(", "final_patch_size", ")", "\n", "final_shape", "=", "np", ".", "copy", "(", "coords", ")", "\n", "if", "len", "(", "coords", ")", "==", "3", ":", "\n", "        ", "final_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "(", "rotate_coords_3d", "(", "coords", ",", "rot_x", ",", "0", ",", "0", ")", ",", "final_shape", ")", ")", ",", "0", ")", "\n", "final_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "(", "rotate_coords_3d", "(", "coords", ",", "0", ",", "rot_y", ",", "0", ")", ",", "final_shape", ")", ")", ",", "0", ")", "\n", "final_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "(", "rotate_coords_3d", "(", "coords", ",", "0", ",", "0", ",", "rot_z", ")", ",", "final_shape", ")", ")", ",", "0", ")", "\n", "", "elif", "len", "(", "coords", ")", "==", "2", ":", "\n", "        ", "final_shape", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "(", "rotate_coords_2d", "(", "coords", ",", "rot_x", ")", ",", "final_shape", ")", ")", ",", "0", ")", "\n", "", "final_shape", "/=", "min", "(", "scale_range", ")", "\n", "return", "final_shape", ".", "astype", "(", "int", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.default_data_augmentation.get_default_augmentation": [[97, 175], ["batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "print", "batchgenerators.dataloading.MultiThreadedAugmenter", "print", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose", "print", "batchgenerators.dataloading.MultiThreadedAugmenter", "print", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.SpatialTransform", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.MirrorTransform", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.RenameTransform", "batchgenerators.transforms.NumpyToTensor", "params.get", "params.get", "batchgenerators.transforms.utility_transforms.RemoveLabelTransform", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.RenameTransform", "batchgenerators.transforms.NumpyToTensor", "max", "params.get", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.custom_transforms.Convert3DTo2DTransform", "params.get", "nnunet.training.data_augmentation.custom_transforms.Convert2DTo3DTransform", "batchgenerators.transforms.GammaTransform", "params.get", "nnunet.training.data_augmentation.custom_transforms.MaskTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "params.get", "params.get", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.Compose.append", "batchgenerators.transforms.DataChannelSelectionTransform", "batchgenerators.transforms.SegChannelSelectionTransform", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.MoveSegAsOneHotToData", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "params.get", "nnunet.training.data_augmentation.pyramid_augmentations.ApplyRandomBinaryOperatorTransform", "nnunet.training.data_augmentation.pyramid_augmentations.RemoveRandomConnectedComponentFromOneHotEncodingTransform", "params.get", "params.get", "params.get", "params.get", "params.get", "list", "list", "range", "range", "len", "len", "params.get", "params.get"], "function", ["None"], ["", "def", "get_default_augmentation", "(", "dataloader_train", ",", "dataloader_val", ",", "patch_size", ",", "params", "=", "default_3D_augmentation_params", ",", "border_val_seg", "=", "-", "1", ",", "pin_memory", "=", "True", ",", "\n", "seeds_train", "=", "None", ",", "seeds_val", "=", "None", ")", ":", "\n", "    ", "tr_transforms", "=", "[", "]", "\n", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "# don't do color augmentations while in 2d mode with 3d data because the color channel is overloaded!!", "\n", "", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "Convert3DTo2DTransform", "(", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "SpatialTransform", "(", "\n", "patch_size", ",", "patch_center_dist_from_border", "=", "None", ",", "do_elastic_deform", "=", "params", ".", "get", "(", "\"do_elastic\"", ")", ",", "\n", "alpha", "=", "params", ".", "get", "(", "\"elastic_deform_alpha\"", ")", ",", "sigma", "=", "params", ".", "get", "(", "\"elastic_deform_sigma\"", ")", ",", "\n", "do_rotation", "=", "params", ".", "get", "(", "\"do_rotation\"", ")", ",", "angle_x", "=", "params", ".", "get", "(", "\"rotation_x\"", ")", ",", "angle_y", "=", "params", ".", "get", "(", "\"rotation_y\"", ")", ",", "\n", "angle_z", "=", "params", ".", "get", "(", "\"rotation_z\"", ")", ",", "do_scale", "=", "params", ".", "get", "(", "\"do_scaling\"", ")", ",", "scale", "=", "params", ".", "get", "(", "\"scale_range\"", ")", ",", "\n", "border_mode_data", "=", "params", ".", "get", "(", "\"border_mode_data\"", ")", ",", "border_cval_data", "=", "0", ",", "order_data", "=", "3", ",", "border_mode_seg", "=", "\"constant\"", ",", "border_cval_seg", "=", "border_val_seg", ",", "\n", "order_seg", "=", "1", ",", "random_crop", "=", "params", ".", "get", "(", "\"random_crop\"", ")", ",", "p_el_per_sample", "=", "params", ".", "get", "(", "\"p_eldef\"", ")", ",", "\n", "p_scale_per_sample", "=", "params", ".", "get", "(", "\"p_scale\"", ")", ",", "p_rot_per_sample", "=", "params", ".", "get", "(", "\"p_rot\"", ")", "\n", ")", ")", "\n", "if", "params", ".", "get", "(", "\"dummy_2D\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"dummy_2D\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "Convert2DTo3DTransform", "(", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"do_gamma\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "GammaTransform", "(", "params", ".", "get", "(", "\"gamma_range\"", ")", ",", "False", ",", "True", ",", "retain_stats", "=", "params", ".", "get", "(", "\"gamma_retain_stats\"", ")", ",", "p_per_sample", "=", "params", "[", "\"p_gamma\"", "]", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "MirrorTransform", "(", "params", ".", "get", "(", "\"mirror_axes\"", ")", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "is", "not", "None", ":", "\n", "        ", "mask_was_used_for_normalization", "=", "params", ".", "get", "(", "\"mask_was_used_for_normalization\"", ")", "\n", "tr_transforms", ".", "append", "(", "MaskTransform", "(", "mask_was_used_for_normalization", ",", "mask_idx_in_seg", "=", "0", ",", "set_outside_to", "=", "0", ")", ")", "\n", "\n", "", "tr_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "\n", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "tr_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "if", "params", ".", "get", "(", "\"advanced_pyramid_augmentations\"", ")", "and", "not", "None", "and", "params", ".", "get", "(", "\"advanced_pyramid_augmentations\"", ")", ":", "\n", "            ", "tr_transforms", ".", "append", "(", "ApplyRandomBinaryOperatorTransform", "(", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "p_per_sample", "=", "0.4", ",", "\n", "key", "=", "\"data\"", ",", "\n", "strel_size", "=", "(", "1", ",", "8", ")", ")", ")", "\n", "tr_transforms", ".", "append", "(", "RemoveRandomConnectedComponentFromOneHotEncodingTransform", "(", "channel_idx", "=", "list", "(", "range", "(", "-", "len", "(", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ")", ",", "0", ")", ")", ",", "\n", "key", "=", "\"data\"", ",", "\n", "p_per_sample", "=", "0.2", ",", "\n", "fill_with_other_class_p", "=", "0.0", ",", "\n", "dont_do_if_covers_more_than_X_percent", "=", "0.15", ")", ")", "\n", "\n", "", "", "tr_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "tr_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "tr_transforms", "=", "Compose", "(", "tr_transforms", ")", "\n", "#from batchgenerators.dataloading import SingleThreadedAugmenter", "\n", "#batchgenerator_train = SingleThreadedAugmenter(dataloader_train, tr_transforms)", "\n", "#import IPython;IPython.embed()", "\n", "print", "(", "'********* Next it is going into the batchgenerator_train MultiThreadedAugmenter phase!*********'", ")", "\n", "batchgenerator_train", "=", "MultiThreadedAugmenter", "(", "dataloader_train", ",", "tr_transforms", ",", "params", ".", "get", "(", "'num_threads'", ")", ",", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "seeds", "=", "seeds_train", ",", "pin_memory", "=", "pin_memory", ")", "\n", "print", "(", "'********* Batchgenerator_train MultiThreadedAugmenter phase has been done! *********'", ")", "\n", "val_transforms", "=", "[", "]", "\n", "val_transforms", ".", "append", "(", "RemoveLabelTransform", "(", "-", "1", ",", "0", ")", ")", "\n", "if", "params", ".", "get", "(", "\"selected_data_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "DataChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_data_channels\"", ")", ")", ")", "\n", "", "if", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", "is", "not", "None", ":", "\n", "        ", "val_transforms", ".", "append", "(", "SegChannelSelectionTransform", "(", "params", ".", "get", "(", "\"selected_seg_channels\"", ")", ")", ")", "\n", "\n", "", "if", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", "is", "not", "None", "and", "params", ".", "get", "(", "\"move_last_seg_chanel_to_data\"", ")", ":", "\n", "        ", "val_transforms", ".", "append", "(", "MoveSegAsOneHotToData", "(", "1", ",", "params", ".", "get", "(", "\"all_segmentation_labels\"", ")", ",", "'seg'", ",", "'data'", ")", ")", "\n", "\n", "", "val_transforms", ".", "append", "(", "RenameTransform", "(", "'seg'", ",", "'target'", ",", "True", ")", ")", "\n", "val_transforms", ".", "append", "(", "NumpyToTensor", "(", "[", "'data'", ",", "'target'", "]", ",", "'float'", ")", ")", "\n", "val_transforms", "=", "Compose", "(", "val_transforms", ")", "\n", "\n", "#batchgenerator_val = SingleThreadedAugmenter(dataloader_val, val_transforms)", "\n", "print", "(", "'********* Next it is going into the batchgenerator_val MultiThreadedAugmenter phase!*********'", ")", "\n", "batchgenerator_val", "=", "MultiThreadedAugmenter", "(", "dataloader_val", ",", "val_transforms", ",", "max", "(", "params", ".", "get", "(", "'num_threads'", ")", "//", "2", ",", "1", ")", ",", "params", ".", "get", "(", "\"num_cached_per_thread\"", ")", ",", "seeds", "=", "seeds_val", ",", "pin_memory", "=", "pin_memory", ")", "\n", "print", "(", "'********* Batchgenerator_val MultiThreadedAugmenter phase has been done! *********'", ")", "\n", "return", "batchgenerator_train", ",", "batchgenerator_val", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.RemoveKeyTransform.__init__": [[20, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "key_to_remove", ")", ":", "\n", "        ", "self", ".", "key_to_remove", "=", "key_to_remove", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.RemoveKeyTransform.__call__": [[23, 26], ["data_dict.pop"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "_", "=", "data_dict", ".", "pop", "(", "self", ".", "key_to_remove", ",", "None", ")", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.MaskTransform.__init__": [[29, 45], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "dct_for_where_it_was_used", ",", "mask_idx_in_seg", "=", "1", ",", "set_outside_to", "=", "0", ",", "data_key", "=", "\"data\"", ",", "seg_key", "=", "\"seg\"", ")", ":", "\n", "        ", "\"\"\"\n        data[mask < 0] = 0\n        Sets everything outside the mask to 0. CAREFUL! outside is defined as < 0, not =0 (in the Mask)!!!\n\n        :param dct_for_where_it_was_used:\n        :param mask_idx_in_seg:\n        :param set_outside_to:\n        :param data_key:\n        :param seg_key:\n        \"\"\"", "\n", "self", ".", "dct_for_where_it_was_used", "=", "dct_for_where_it_was_used", "\n", "self", ".", "seg_key", "=", "seg_key", "\n", "self", ".", "data_key", "=", "data_key", "\n", "self", ".", "set_outside_to", "=", "set_outside_to", "\n", "self", ".", "mask_idx_in_seg", "=", "mask_idx_in_seg", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.MaskTransform.__call__": [[46, 58], ["data_dict.get", "data_dict.get", "range", "Warning", "range"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "seg", "=", "data_dict", ".", "get", "(", "self", ".", "seg_key", ")", "\n", "if", "seg", "is", "None", "or", "seg", ".", "shape", "[", "1", "]", "<", "self", ".", "mask_idx_in_seg", ":", "\n", "            ", "raise", "Warning", "(", "\"mask not found, seg may be missing or seg[:, mask_idx_in_seg] may not exist\"", ")", "\n", "", "data", "=", "data_dict", ".", "get", "(", "self", ".", "data_key", ")", "\n", "for", "b", "in", "range", "(", "data", ".", "shape", "[", "0", "]", ")", ":", "\n", "            ", "mask", "=", "seg", "[", "b", ",", "self", ".", "mask_idx_in_seg", "]", "\n", "for", "c", "in", "range", "(", "data", ".", "shape", "[", "1", "]", ")", ":", "\n", "                ", "if", "self", ".", "dct_for_where_it_was_used", "[", "c", "]", ":", "\n", "                    ", "data", "[", "b", ",", "c", "]", "[", "mask", "<", "0", "]", "=", "self", ".", "set_outside_to", "\n", "", "", "", "data_dict", "[", "self", ".", "data_key", "]", "=", "data", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.Convert3DTo2DTransform.__init__": [[81, 83], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.Convert3DTo2DTransform.__call__": [[84, 86], ["custom_transforms.convert_3d_to_2d_generator"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.convert_3d_to_2d_generator"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "return", "convert_3d_to_2d_generator", "(", "data_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.Convert2DTo3DTransform.__init__": [[89, 91], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.Convert2DTo3DTransform.__call__": [[92, 94], ["custom_transforms.convert_2d_to_3d_generator"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.convert_2d_to_3d_generator"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "return", "convert_2d_to_3d_generator", "(", "data_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform.__init__": [[97, 109], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "regions", ",", "seg_key", "=", "\"seg\"", ",", "output_key", "=", "\"seg\"", ",", "seg_channel", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        regions are tuple of tuples where each inner tuple holds the class indices that are merged into one region, example:\n        regions= ((1, 2), (2, )) will result in 2 regions: one covering the region of labels 1&2 and the other just 2\n        :param regions:\n        :param seg_key:\n        :param output_key:\n        \"\"\"", "\n", "self", ".", "seg_channel", "=", "seg_channel", "\n", "self", ".", "output_key", "=", "output_key", "\n", "self", ".", "seg_key", "=", "seg_key", "\n", "self", ".", "regions", "=", "regions", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.ConvertSegmentationToRegionsTransform.__call__": [[110, 124], ["data_dict.get", "len", "list", "numpy.zeros", "range", "range"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "**", "data_dict", ")", ":", "\n", "        ", "seg", "=", "data_dict", ".", "get", "(", "self", ".", "seg_key", ")", "\n", "num_regions", "=", "len", "(", "self", ".", "regions", ")", "\n", "if", "seg", "is", "not", "None", ":", "\n", "            ", "seg_shp", "=", "seg", ".", "shape", "\n", "output_shape", "=", "list", "(", "seg_shp", ")", "\n", "output_shape", "[", "1", "]", "=", "num_regions", "\n", "region_output", "=", "np", ".", "zeros", "(", "output_shape", ",", "dtype", "=", "seg", ".", "dtype", ")", "\n", "for", "b", "in", "range", "(", "seg_shp", "[", "0", "]", ")", ":", "\n", "                ", "for", "r", "in", "range", "(", "num_regions", ")", ":", "\n", "                    ", "for", "l", "in", "self", ".", "regions", "[", "r", "]", ":", "\n", "                        ", "region_output", "[", "b", ",", "r", "]", "[", "seg", "[", "b", ",", "self", ".", "seg_channel", "]", "==", "l", "]", "=", "1", "\n", "", "", "", "data_dict", "[", "self", ".", "output_key", "]", "=", "region_output", "\n", "", "return", "data_dict", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.convert_3d_to_2d_generator": [[60, 68], ["data_dict[].reshape", "data_dict[].reshape"], "function", ["None"], ["", "", "def", "convert_3d_to_2d_generator", "(", "data_dict", ")", ":", "\n", "    ", "shp", "=", "data_dict", "[", "'data'", "]", ".", "shape", "\n", "data_dict", "[", "'data'", "]", "=", "data_dict", "[", "'data'", "]", ".", "reshape", "(", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", "*", "shp", "[", "2", "]", ",", "shp", "[", "3", "]", ",", "shp", "[", "4", "]", ")", ")", "\n", "data_dict", "[", "'orig_shape_data'", "]", "=", "shp", "\n", "shp", "=", "data_dict", "[", "'seg'", "]", ".", "shape", "\n", "data_dict", "[", "'seg'", "]", "=", "data_dict", "[", "'seg'", "]", ".", "reshape", "(", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", "*", "shp", "[", "2", "]", ",", "shp", "[", "3", "]", ",", "shp", "[", "4", "]", ")", ")", "\n", "data_dict", "[", "'orig_shape_seg'", "]", "=", "shp", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.custom_transforms.convert_2d_to_3d_generator": [[70, 78], ["data_dict[].reshape", "data_dict[].reshape"], "function", ["None"], ["", "def", "convert_2d_to_3d_generator", "(", "data_dict", ")", ":", "\n", "    ", "shp", "=", "data_dict", "[", "'orig_shape_data'", "]", "\n", "current_shape", "=", "data_dict", "[", "'data'", "]", ".", "shape", "\n", "data_dict", "[", "'data'", "]", "=", "data_dict", "[", "'data'", "]", ".", "reshape", "(", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", ",", "shp", "[", "2", "]", ",", "current_shape", "[", "-", "2", "]", ",", "current_shape", "[", "-", "1", "]", ")", ")", "\n", "shp", "=", "data_dict", "[", "'orig_shape_seg'", "]", "\n", "current_shape_seg", "=", "data_dict", "[", "'seg'", "]", ".", "shape", "\n", "data_dict", "[", "'seg'", "]", "=", "data_dict", "[", "'seg'", "]", ".", "reshape", "(", "(", "shp", "[", "0", "]", ",", "shp", "[", "1", "]", ",", "shp", "[", "2", "]", ",", "current_shape_seg", "[", "-", "2", "]", ",", "current_shape_seg", "[", "-", "1", "]", ")", ")", "\n", "return", "data_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.__init__": [[27, 100], ["numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        A generic class that can train almost any neural network (RNNs excluded). It provides basic functionality such\n        as the training loop, tracking of training and validation losses (and the target metric if you implement it)\n        Training can be terminated early if the validation loss (or the target metric if implemented) do not improve\n        anymore. This is based on a moving average (MA) of the loss/metric instead of the raw values to get more smooth\n        results.\n\n        What you need to override:\n        - __init__\n        - initialize\n        - run_online_evaluation (optional)\n        - finish_online_evaluation (optional)\n        - validate\n        - predict_test_case\n        \"\"\"", "\n", "np", ".", "random", ".", "seed", "(", "12345", ")", "\n", "torch", ".", "manual_seed", "(", "12345", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "12345", ")", "\n", "self", ".", "fp16", "=", "fp16", "\n", "\n", "if", "deterministic", ":", "\n", "            ", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "", "else", ":", "\n", "            ", "cudnn", ".", "deterministic", "=", "False", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "\n", "################# SET THESE IN self.initialize() ###################################", "\n", "", "self", ".", "network", "=", "None", "\n", "self", ".", "optimizer", "=", "None", "\n", "self", ".", "lr_scheduler", "=", "None", "\n", "self", ".", "tr_gen", "=", "self", ".", "val_gen", "=", "None", "\n", "self", ".", "was_initialized", "=", "False", "\n", "\n", "################# SET THESE IN INIT ################################################", "\n", "self", ".", "output_folder", "=", "None", "\n", "self", ".", "fold", "=", "None", "\n", "self", ".", "loss", "=", "None", "\n", "self", ".", "dataset_directory", "=", "None", "\n", "\n", "################# SET THESE IN LOAD_DATASET OR DO_SPLIT ############################", "\n", "self", ".", "dataset", "=", "None", "# these can be None for inference mode", "\n", "self", ".", "dataset_tr", "=", "self", ".", "dataset_val", "=", "None", "# do not need to be used, they just appear if you are using the suggested load_dataset_and_do_split", "\n", "\n", "################# THESE DO NOT NECESSARILY NEED TO BE MODIFIED #####################", "\n", "self", ".", "patience", "=", "50", "\n", "self", ".", "val_eval_criterion_alpha", "=", "0.9", "# alpha * old + (1-alpha) * new", "\n", "# if this is too low then the moving average will be too noisy and the training may terminate early. If it is", "\n", "# too high the training will take forever", "\n", "self", ".", "train_loss_MA_alpha", "=", "0.93", "# alpha * old + (1-alpha) * new", "\n", "self", ".", "train_loss_MA_eps", "=", "5e-4", "# new MA must be at least this much better (smaller)", "\n", "self", ".", "save_every", "=", "50", "\n", "self", ".", "save_latest_only", "=", "True", "\n", "self", ".", "max_num_epochs", "=", "1000", "\n", "self", ".", "num_batches_per_epoch", "=", "250", "\n", "self", ".", "num_val_batches_per_epoch", "=", "50", "\n", "self", ".", "also_val_in_tr_mode", "=", "False", "\n", "self", ".", "lr_threshold", "=", "1e-6", "# the network will not terminate training if the lr is still above this threshold", "\n", "\n", "################# LEAVE THESE ALONE ################################################", "\n", "self", ".", "val_eval_criterion_MA", "=", "None", "\n", "self", ".", "train_loss_MA", "=", "None", "\n", "self", ".", "best_val_eval_criterion_MA", "=", "None", "\n", "self", ".", "best_MA_tr_loss_for_patience", "=", "None", "\n", "self", ".", "best_epoch_based_on_MA_tr_loss", "=", "None", "\n", "self", ".", "all_tr_losses", "=", "[", "]", "\n", "self", ".", "all_val_losses", "=", "[", "]", "\n", "self", ".", "all_val_losses_tr_mode", "=", "[", "]", "\n", "self", ".", "all_val_eval_metrics", "=", "[", "]", "# does not have to be used", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "log_file", "=", "None", "\n", "self", ".", "deterministic", "=", "deterministic", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.initialize": [[102, 117], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "initialize", "(", "self", ",", "training", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        create self.output_folder\n\n        modify self.output_folder if you are doing cross-validation (one folder per fold)\n\n        set self.tr_gen and self.val_gen\n        \n        set self.network, self.optimizer and self.lr_scheduler\n        \n        finally set self.was_initialized to True\n        :param training:\n        :return:\n        \"\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_dataset": [[118, 121], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "load_dataset", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.do_split": [[122, 161], ["join", "load_pickle", "list.sort", "list.sort", "collections.OrderedDict", "collections.OrderedDict", "isfile", "network_trainer.NetworkTrainer.print_to_log_file", "numpy.sort", "sklearn.model_selection.KFold", "enumerate", "save_pickle", "list", "list", "sklearn.model_selection.KFold.split", "load_pickle.append", "network_trainer.NetworkTrainer.dataset.keys", "network_trainer.NetworkTrainer.dataset.keys", "numpy.array", "numpy.array", "collections.OrderedDict"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file"], ["", "def", "do_split", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This is a suggestion for if your dataset is a dictionary (my personal standard)\n        :return:\n        \"\"\"", "\n", "splits_file", "=", "join", "(", "self", ".", "dataset_directory", ",", "\"splits_final.pkl\"", ")", "\n", "if", "not", "isfile", "(", "splits_file", ")", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"Creating new split...\"", ")", "\n", "splits", "=", "[", "]", "\n", "all_keys_sorted", "=", "np", ".", "sort", "(", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", ")", "\n", "kfold", "=", "KFold", "(", "n_splits", "=", "5", ",", "shuffle", "=", "True", ",", "random_state", "=", "12345", ")", "\n", "for", "i", ",", "(", "train_idx", ",", "test_idx", ")", "in", "enumerate", "(", "kfold", ".", "split", "(", "all_keys_sorted", ")", ")", ":", "\n", "                ", "train_keys", "=", "np", ".", "array", "(", "all_keys_sorted", ")", "[", "train_idx", "]", "\n", "test_keys", "=", "np", ".", "array", "(", "all_keys_sorted", ")", "[", "test_idx", "]", "\n", "splits", ".", "append", "(", "OrderedDict", "(", ")", ")", "\n", "splits", "[", "-", "1", "]", "[", "'train'", "]", "=", "train_keys", "\n", "splits", "[", "-", "1", "]", "[", "'val'", "]", "=", "test_keys", "\n", "", "save_pickle", "(", "splits", ",", "splits_file", ")", "\n", "\n", "", "splits", "=", "load_pickle", "(", "splits_file", ")", "\n", "\n", "if", "self", ".", "fold", "==", "\"all\"", ":", "\n", "            ", "tr_keys", "=", "val_keys", "=", "list", "(", "self", ".", "dataset", ".", "keys", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "tr_keys", "=", "splits", "[", "self", ".", "fold", "]", "[", "'train'", "]", "\n", "val_keys", "=", "splits", "[", "self", ".", "fold", "]", "[", "'val'", "]", "\n", "\n", "", "tr_keys", ".", "sort", "(", ")", "\n", "val_keys", ".", "sort", "(", ")", "\n", "# print('tr_keys is:',tr_keys)", "\n", "# print('val_keys is:',val_keys)", "\n", "\n", "self", ".", "dataset_tr", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "tr_keys", ":", "\n", "            ", "self", ".", "dataset_tr", "[", "i", "]", "=", "self", ".", "dataset", "[", "i", "]", "\n", "\n", "", "self", ".", "dataset_val", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "val_keys", ":", "\n", "            ", "self", ".", "dataset_val", "[", "i", "]", "=", "self", ".", "dataset", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.plot_progress": [[162, 198], ["matplotlib.rc", "matplotlib.rc", "matplotlib.rc", "matplotlib.rc", "matplotlib.figure", "matplotlib.figure", "matplotlib.figure.add_subplot", "plt.figure.add_subplot.twinx", "list", "plt.figure.add_subplot.plot", "plt.figure.add_subplot.plot", "plt.figure.add_subplot.set_xlabel", "plt.figure.add_subplot.set_ylabel", "fig.add_subplot.twinx.set_ylabel", "plt.figure.add_subplot.legend", "fig.add_subplot.twinx.legend", "matplotlib.figure.savefig", "matplotlib.close", "matplotlib.close", "range", "len", "plt.figure.add_subplot.plot", "len", "len", "fig.add_subplot.twinx.plot", "join", "network_trainer.NetworkTrainer.print_to_log_file", "sys.exc_info"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file"], ["", "", "def", "plot_progress", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Should probably by improved\n        :return:\n        \"\"\"", "\n", "try", ":", "\n", "            ", "font", "=", "{", "'weight'", ":", "'normal'", ",", "\n", "'size'", ":", "18", "}", "\n", "\n", "matplotlib", ".", "rc", "(", "'font'", ",", "**", "font", ")", "\n", "\n", "fig", "=", "plt", ".", "figure", "(", "figsize", "=", "(", "30", ",", "24", ")", ")", "\n", "ax", "=", "fig", ".", "add_subplot", "(", "111", ")", "\n", "ax2", "=", "ax", ".", "twinx", "(", ")", "\n", "\n", "x_values", "=", "list", "(", "range", "(", "self", ".", "epoch", "+", "1", ")", ")", "\n", "\n", "ax", ".", "plot", "(", "x_values", ",", "self", ".", "all_tr_losses", ",", "color", "=", "'b'", ",", "ls", "=", "'-'", ",", "label", "=", "\"loss_tr\"", ")", "\n", "\n", "ax", ".", "plot", "(", "x_values", ",", "self", ".", "all_val_losses", ",", "color", "=", "'r'", ",", "ls", "=", "'-'", ",", "label", "=", "\"loss_val, train=False\"", ")", "\n", "\n", "if", "len", "(", "self", ".", "all_val_losses_tr_mode", ")", ">", "0", ":", "\n", "                ", "ax", ".", "plot", "(", "x_values", ",", "self", ".", "all_val_losses_tr_mode", ",", "color", "=", "'g'", ",", "ls", "=", "'-'", ",", "label", "=", "\"loss_val, train=True\"", ")", "\n", "", "if", "len", "(", "self", ".", "all_val_eval_metrics", ")", "==", "len", "(", "self", ".", "all_val_losses", ")", ":", "\n", "                ", "ax2", ".", "plot", "(", "x_values", ",", "self", ".", "all_val_eval_metrics", ",", "color", "=", "'g'", ",", "ls", "=", "'--'", ",", "label", "=", "\"evaluation metric\"", ")", "\n", "\n", "", "ax", ".", "set_xlabel", "(", "\"epoch\"", ")", "\n", "ax", ".", "set_ylabel", "(", "\"loss\"", ")", "\n", "ax2", ".", "set_ylabel", "(", "\"evaluation metric\"", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "ax2", ".", "legend", "(", "loc", "=", "9", ")", "\n", "\n", "fig", ".", "savefig", "(", "join", "(", "self", ".", "output_folder", ",", "\"progress.png\"", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "", "except", "IOError", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"failed to plot: \"", ",", "sys", ".", "exc_info", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file": [[199, 231], ["time.time.time", "datetime.datetime.datetime.fromtimestamp", "maybe_mkdir_p", "datetime.datetime.datetime.now", "join", "print", "open", "f.write", "open", "f.write", "print", "time.time.sleep", "f.write", "f.write", "sys.exc_info", "str", "datetime.datetime.datetime.fromtimestamp"], "methods", ["None"], ["", "", "def", "print_to_log_file", "(", "self", ",", "*", "args", ",", "also_print_to_console", "=", "True", ",", "add_timestamp", "=", "True", ")", ":", "\n", "\n", "        ", "timestamp", "=", "time", "(", ")", "\n", "dt_object", "=", "datetime", ".", "fromtimestamp", "(", "timestamp", ")", "\n", "\n", "if", "add_timestamp", ":", "\n", "            ", "args", "=", "(", "\"%s:\"", "%", "dt_object", ",", "*", "args", ")", "\n", "\n", "", "if", "self", ".", "log_file", "is", "None", ":", "\n", "            ", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "timestamp", "=", "datetime", ".", "now", "(", ")", "\n", "self", ".", "log_file", "=", "join", "(", "self", ".", "output_folder", ",", "\"training_log_%d_%d_%d_%02.0d_%02.0d_%02.0d.txt\"", "%", "\n", "(", "timestamp", ".", "year", ",", "timestamp", ".", "month", ",", "timestamp", ".", "day", ",", "timestamp", ".", "hour", ",", "timestamp", ".", "minute", ",", "timestamp", ".", "second", ")", ")", "\n", "with", "open", "(", "self", ".", "log_file", ",", "'w'", ")", "as", "f", ":", "\n", "                ", "f", ".", "write", "(", "\"Starting... \\n\"", ")", "\n", "", "", "successful", "=", "False", "\n", "max_attempts", "=", "5", "\n", "ctr", "=", "0", "\n", "while", "not", "successful", "and", "ctr", "<", "max_attempts", ":", "\n", "            ", "try", ":", "\n", "                ", "with", "open", "(", "self", ".", "log_file", ",", "'a+'", ")", "as", "f", ":", "\n", "                    ", "for", "a", "in", "args", ":", "\n", "                        ", "f", ".", "write", "(", "str", "(", "a", ")", ")", "\n", "f", ".", "write", "(", "\" \"", ")", "\n", "", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "", "successful", "=", "True", "\n", "", "except", "IOError", ":", "\n", "                ", "print", "(", "\"%s: failed to log: \"", "%", "datetime", ".", "fromtimestamp", "(", "timestamp", ")", ",", "sys", ".", "exc_info", "(", ")", ")", "\n", "sleep", "(", "0.5", ")", "\n", "ctr", "+=", "1", "\n", "", "", "if", "also_print_to_console", ":", "\n", "            ", "print", "(", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.save_checkpoint": [[232, 257], ["time.time.time", "network_trainer.NetworkTrainer.network.state_dict", "network_trainer.NetworkTrainer.keys", "network_trainer.NetworkTrainer.print_to_log_file", "torch.save", "torch.save", "torch.save", "torch.save", "network_trainer.NetworkTrainer.print_to_log_file", "state_dict[].cpu", "network_trainer.NetworkTrainer.lr_scheduler.state_dict", "network_trainer.NetworkTrainer.keys", "network_trainer.NetworkTrainer.optimizer.state_dict", "isinstance", "time.time.time"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file"], ["", "", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "start_time", "=", "time", "(", ")", "\n", "state_dict", "=", "self", ".", "network", ".", "state_dict", "(", ")", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "state_dict", "[", "key", "]", "=", "state_dict", "[", "key", "]", ".", "cpu", "(", ")", "\n", "", "lr_sched_state_dct", "=", "None", "\n", "if", "self", ".", "lr_scheduler", "is", "not", "None", "and", "not", "isinstance", "(", "self", ".", "lr_scheduler", ",", "lr_scheduler", ".", "ReduceLROnPlateau", ")", ":", "\n", "            ", "lr_sched_state_dct", "=", "self", ".", "lr_scheduler", ".", "state_dict", "(", ")", "\n", "for", "key", "in", "lr_sched_state_dct", ".", "keys", "(", ")", ":", "\n", "                ", "lr_sched_state_dct", "[", "key", "]", "=", "lr_sched_state_dct", "[", "key", "]", "\n", "", "", "if", "save_optimizer", ":", "\n", "            ", "optimizer_state_dict", "=", "self", ".", "optimizer", ".", "state_dict", "(", ")", "\n", "", "else", ":", "\n", "            ", "optimizer_state_dict", "=", "None", "\n", "\n", "", "self", ".", "print_to_log_file", "(", "\"saving checkpoint...\"", ")", "\n", "torch", ".", "save", "(", "{", "\n", "'epoch'", ":", "self", ".", "epoch", "+", "1", ",", "\n", "'state_dict'", ":", "state_dict", ",", "\n", "'optimizer_state_dict'", ":", "optimizer_state_dict", ",", "\n", "'lr_scheduler_state_dict'", ":", "lr_sched_state_dct", ",", "\n", "'plot_stuff'", ":", "(", "self", ".", "all_tr_losses", ",", "self", ".", "all_val_losses", ",", "self", ".", "all_val_losses_tr_mode", ",", "\n", "self", ".", "all_val_eval_metrics", ")", "}", ",", "\n", "fname", ")", "\n", "self", ".", "print_to_log_file", "(", "\"done, saving took %.2f seconds\"", "%", "(", "time", "(", ")", "-", "start_time", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_best_checkpoint": [[258, 262], ["network_trainer.NetworkTrainer.load_checkpoint", "RuntimeError", "join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_checkpoint"], ["", "def", "load_best_checkpoint", "(", "self", ",", "train", "=", "True", ")", ":", "\n", "        ", "if", "self", ".", "fold", "is", "None", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"Cannot load best checkpoint if self.fold is None\"", ")", "\n", "", "self", ".", "load_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_best.model\"", ")", ",", "train", "=", "train", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_latest_checkpoint": [[263, 274], ["isfile", "isfile", "network_trainer.NetworkTrainer.load_checkpoint", "join", "network_trainer.NetworkTrainer.load_checkpoint", "join", "network_trainer.NetworkTrainer.load_checkpoint", "len", "network_trainer.NetworkTrainer.load_best_checkpoint", "int", "join", "join", "join", "os.listdir", "numpy.argmax", "i.endswith", "[].split", "i.find", "i.split"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_checkpoint", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_checkpoint", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_checkpoint", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_best_checkpoint"], ["", "def", "load_latest_checkpoint", "(", "self", ",", "train", "=", "True", ")", ":", "\n", "        ", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_final_checkpoint.model\"", ")", ")", ":", "\n", "            ", "return", "self", ".", "load_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_final_checkpoint.model\"", ")", ",", "train", "=", "train", ")", "\n", "", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ")", ":", "\n", "            ", "return", "self", ".", "load_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ",", "train", "=", "train", ")", "\n", "", "all_checkpoints", "=", "[", "i", "for", "i", "in", "os", ".", "listdir", "(", "self", ".", "output_folder", ")", "if", "i", ".", "endswith", "(", "\".model\"", ")", "and", "i", ".", "find", "(", "\"_ep_\"", ")", "!=", "-", "1", "]", "\n", "if", "len", "(", "all_checkpoints", ")", "==", "0", ":", "\n", "            ", "return", "self", ".", "load_best_checkpoint", "(", "train", "=", "train", ")", "\n", "", "corresponding_epochs", "=", "[", "int", "(", "i", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ".", "split", "(", "\".\"", ")", "[", "0", "]", ")", "for", "i", "in", "all_checkpoints", "]", "\n", "checkpoint", "=", "all_checkpoints", "[", "np", ".", "argmax", "(", "corresponding_epochs", ")", "]", "\n", "self", ".", "load_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "checkpoint", ")", ",", "train", "=", "train", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_checkpoint": [[275, 281], ["network_trainer.NetworkTrainer.print_to_log_file", "torch.load", "torch.load", "torch.load", "torch.load", "network_trainer.NetworkTrainer.load_checkpoint_ram", "network_trainer.NetworkTrainer.initialize", "torch.device", "torch.device", "torch.device", "torch.device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device", "torch.cuda.current_device"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_checkpoint_ram", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.initialize"], ["", "def", "load_checkpoint", "(", "self", ",", "fname", ",", "train", "=", "True", ")", ":", "\n", "        ", "self", ".", "print_to_log_file", "(", "\"loading checkpoint\"", ",", "fname", ",", "\"train=\"", ",", "train", ")", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "self", ".", "initialize", "(", "train", ")", "\n", "", "saved_model", "=", "torch", ".", "load", "(", "fname", ",", "map_location", "=", "torch", ".", "device", "(", "'cuda'", ",", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", ")", "\n", "self", ".", "load_checkpoint_ram", "(", "saved_model", ",", "train", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_checkpoint_ram": [[282, 311], ["collections.OrderedDict", "list", "saved_model[].items", "network_trainer.NetworkTrainer.network.load_state_dict", "network_trainer.NetworkTrainer.initialize", "network_trainer.NetworkTrainer.network.state_dict().keys", "network_trainer.NetworkTrainer.optimizer.load_state_dict", "network_trainer.NetworkTrainer.lr_scheduler.load_state_dict", "network_trainer.NetworkTrainer.network.state_dict", "isinstance"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.initialize"], ["", "def", "load_checkpoint_ram", "(", "self", ",", "saved_model", ",", "train", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        used for if the checkpoint is already in ram\n        :param saved_model:\n        :param train:\n        :return:\n        \"\"\"", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "self", ".", "initialize", "(", "train", ")", "\n", "\n", "", "new_state_dict", "=", "OrderedDict", "(", ")", "\n", "curr_state_dict_keys", "=", "list", "(", "self", ".", "network", ".", "state_dict", "(", ")", ".", "keys", "(", ")", ")", "\n", "# if state dict comes form nn.DataParallel but we use non-parallel model here then the state dict keys do not", "\n", "# match. Use heuristic to make it match", "\n", "for", "k", ",", "value", "in", "saved_model", "[", "'state_dict'", "]", ".", "items", "(", ")", ":", "\n", "            ", "key", "=", "k", "\n", "if", "key", "not", "in", "curr_state_dict_keys", ":", "\n", "                ", "key", "=", "key", "[", "7", ":", "]", "\n", "", "new_state_dict", "[", "key", "]", "=", "value", "\n", "", "self", ".", "network", ".", "load_state_dict", "(", "new_state_dict", ")", "\n", "self", ".", "epoch", "=", "saved_model", "[", "'epoch'", "]", "\n", "if", "train", ":", "\n", "            ", "optimizer_state_dict", "=", "saved_model", "[", "'optimizer_state_dict'", "]", "\n", "if", "optimizer_state_dict", "is", "not", "None", ":", "\n", "                ", "self", ".", "optimizer", ".", "load_state_dict", "(", "optimizer_state_dict", ")", "\n", "", "if", "self", ".", "lr_scheduler", "is", "not", "None", "and", "not", "isinstance", "(", "self", ".", "lr_scheduler", ",", "lr_scheduler", ".", "ReduceLROnPlateau", ")", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "load_state_dict", "(", "saved_model", "[", "'lr_scheduler_state_dict'", "]", ")", "\n", "\n", "", "", "self", ".", "all_tr_losses", ",", "self", ".", "all_val_losses", ",", "self", ".", "all_val_losses_tr_mode", ",", "self", ".", "all_val_eval_metrics", "=", "saved_model", "[", "'plot_stuff'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer._maybe_init_amp": [[312, 319], ["amp.initialize", "network_trainer.NetworkTrainer.print_to_log_file"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.initialize", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file"], ["", "def", "_maybe_init_amp", "(", "self", ")", ":", "\n", "# we use fp16 for training only, not inference", "\n", "        ", "if", "self", ".", "fp16", ":", "\n", "            ", "if", "amp", "is", "not", "None", ":", "\n", "                ", "self", ".", "network", ",", "self", ".", "optimizer", "=", "amp", ".", "initialize", "(", "self", ".", "network", ",", "self", ".", "optimizer", ",", "opt_level", "=", "\"O1\"", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"WARNING: FP16 training was requested but nvidia apex is not installed. \"", "\n", "\"Install it from https://github.com/NVIDIA/apex\"", ")", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.run_training": [[321, 393], ["network_trainer.NetworkTrainer._maybe_init_amp", "maybe_mkdir_p", "network_trainer.NetworkTrainer.save_checkpoint", "isfile", "isfile", "_warnings.warn", "network_trainer.NetworkTrainer.initialize", "network_trainer.NetworkTrainer.print_to_log_file", "time.time.time", "network_trainer.NetworkTrainer.network.train", "range", "network_trainer.NetworkTrainer.all_tr_losses.append", "network_trainer.NetworkTrainer.print_to_log_file", "time.time.time", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.update_train_loss_MA", "network_trainer.NetworkTrainer.on_epoch_end", "join", "join", "os.remove", "join", "os.remove", "network_trainer.NetworkTrainer.run_iteration", "network_trainer.NetworkTrainer.data.cpu().numpy", "train_losses_epoch.append", "numpy.mean", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "network_trainer.NetworkTrainer.network.eval", "range", "network_trainer.NetworkTrainer.all_val_losses.append", "network_trainer.NetworkTrainer.print_to_log_file", "join", "join", "network_trainer.NetworkTrainer.run_iteration", "val_losses.append", "numpy.mean", "network_trainer.NetworkTrainer.network.train", "range", "network_trainer.NetworkTrainer.all_val_losses_tr_mode.append", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.data.cpu", "network_trainer.NetworkTrainer.data.cpu().numpy", "network_trainer.NetworkTrainer.run_iteration", "val_losses.append", "numpy.mean", "network_trainer.NetworkTrainer.data.cpu().numpy", "network_trainer.NetworkTrainer.data.cpu", "network_trainer.NetworkTrainer.data.cpu"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer._maybe_init_amp", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.save_checkpoint", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.initialize", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.update_train_loss_MA", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.on_epoch_end", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.run_iteration", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.run_iteration", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.run_iteration"], ["", "", "", "def", "run_training", "(", "self", ")", ":", "\n", "        ", "self", ".", "_maybe_init_amp", "(", ")", "\n", "\n", "if", "cudnn", ".", "benchmark", "and", "cudnn", ".", "deterministic", ":", "\n", "            ", "warn", "(", "\"torch.backends.cudnn.deterministic is True indicating a deterministic training is desired. \"", "\n", "\"But torch.backends.cudnn.benchmark is True as well and this will prevent deterministic training! \"", "\n", "\"If you want deterministic then set benchmark=False\"", ")", "\n", "\n", "", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "not", "self", ".", "was_initialized", ":", "\n", "            ", "self", ".", "initialize", "(", "True", ")", "\n", "\n", "", "while", "self", ".", "epoch", "<", "self", ".", "max_num_epochs", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"\\nepoch: \"", ",", "self", ".", "epoch", ")", "\n", "epoch_start_time", "=", "time", "(", ")", "\n", "train_losses_epoch", "=", "[", "]", "\n", "\n", "# train one epoch", "\n", "# print('before gpu printing!')", "\n", "self", ".", "network", ".", "train", "(", ")", "\n", "# sleep(2)", "\n", "# print('After gpu printing!')", "\n", "for", "b", "in", "range", "(", "self", ".", "num_batches_per_epoch", ")", ":", "\n", "# for b in range(2):", "\n", "                ", "l", "=", "self", ".", "run_iteration", "(", "self", ".", "tr_gen", ",", "True", ")", "\n", "l_cpu", "=", "l", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "train_losses_epoch", ".", "append", "(", "l_cpu", ")", "\n", "\n", "", "self", ".", "all_tr_losses", ".", "append", "(", "np", ".", "mean", "(", "train_losses_epoch", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"train loss : %.4f\"", "%", "self", ".", "all_tr_losses", "[", "-", "1", "]", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# validation with train=False", "\n", "                ", "self", ".", "network", ".", "eval", "(", ")", "\n", "val_losses", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "self", ".", "num_val_batches_per_epoch", ")", ":", "\n", "# for b in range(2):", "\n", "                    ", "l", "=", "self", ".", "run_iteration", "(", "self", ".", "val_gen", ",", "False", ",", "True", ")", "\n", "val_losses", ".", "append", "(", "l", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "self", ".", "all_val_losses", ".", "append", "(", "np", ".", "mean", "(", "val_losses", ")", ")", "\n", "# print('Using this val loss!')", "\n", "self", ".", "print_to_log_file", "(", "\"val loss (train=False): %.4f\"", "%", "self", ".", "all_val_losses", "[", "-", "1", "]", ")", "\n", "\n", "if", "self", ".", "also_val_in_tr_mode", ":", "\n", "                    ", "self", ".", "network", ".", "train", "(", ")", "\n", "# validation with train=True", "\n", "val_losses", "=", "[", "]", "\n", "for", "b", "in", "range", "(", "self", ".", "num_val_batches_per_epoch", ")", ":", "\n", "                        ", "l", "=", "self", ".", "run_iteration", "(", "self", ".", "val_gen", ",", "False", ")", "\n", "val_losses", ".", "append", "(", "l", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "", "self", ".", "all_val_losses_tr_mode", ".", "append", "(", "np", ".", "mean", "(", "val_losses", ")", ")", "\n", "# print('Using also_val_in_tr_mode val loss!')", "\n", "self", ".", "print_to_log_file", "(", "\"val loss (train=True): %.4f\"", "%", "self", ".", "all_val_losses_tr_mode", "[", "-", "1", "]", ")", "\n", "\n", "", "", "epoch_end_time", "=", "time", "(", ")", "\n", "self", ".", "print_to_log_file", "(", "\"This epoch took %f s\"", "%", "(", "epoch_end_time", "-", "epoch_start_time", ")", ")", "\n", "\n", "self", ".", "update_train_loss_MA", "(", ")", "# needed for lr scheduler and stopping of training", "\n", "\n", "continue_training", "=", "self", ".", "on_epoch_end", "(", ")", "\n", "if", "not", "continue_training", ":", "\n", "# allows for early stopping", "\n", "                ", "break", "\n", "\n", "", "self", ".", "epoch", "+=", "1", "\n", "", "self", ".", "save_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_final_checkpoint.model\"", ")", ")", "\n", "# now we can delete latest as it will be identical with final", "\n", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ")", ":", "\n", "            ", "os", ".", "remove", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ")", "\n", "", "if", "isfile", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model.pkl\"", ")", ")", ":", "\n", "            ", "os", ".", "remove", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model.pkl\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.maybe_update_lr": [[394, 405], ["network_trainer.NetworkTrainer.print_to_log_file", "isinstance", "isinstance", "network_trainer.NetworkTrainer.lr_scheduler.step", "network_trainer.NetworkTrainer.lr_scheduler.step", "str"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file"], ["", "", "def", "maybe_update_lr", "(", "self", ")", ":", "\n", "# maybe update learning rate", "\n", "        ", "if", "self", ".", "lr_scheduler", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "self", ".", "lr_scheduler", ",", "(", "lr_scheduler", ".", "ReduceLROnPlateau", ",", "lr_scheduler", ".", "_LRScheduler", ")", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "lr_scheduler", ",", "lr_scheduler", ".", "ReduceLROnPlateau", ")", ":", "\n", "# lr scheduler is updated with moving average val loss. should be more robust", "\n", "                ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "train_loss_MA", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "lr_scheduler", ".", "step", "(", "self", ".", "epoch", "+", "1", ")", "\n", "", "", "self", ".", "print_to_log_file", "(", "\"lr is now (scheduler) %s\"", "%", "str", "(", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.maybe_save_checkpoint": [[406, 417], ["network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.save_checkpoint", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.save_checkpoint", "join", "join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.save_checkpoint", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.save_checkpoint"], ["", "def", "maybe_save_checkpoint", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Saves a checkpoint every save_ever epochs.\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "epoch", "%", "self", ".", "save_every", "==", "(", "self", ".", "save_every", "-", "1", ")", ":", "\n", "            ", "self", ".", "print_to_log_file", "(", "\"saving scheduled checkpoint file...\"", ")", "\n", "if", "not", "self", ".", "save_latest_only", ":", "\n", "                ", "self", ".", "save_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_ep_%03.0d.model\"", "%", "(", "self", ".", "epoch", "+", "1", ")", ")", ")", "\n", "", "self", ".", "save_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_latest.model\"", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"done\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.update_eval_criterion_MA": [[418, 442], ["len", "len"], "methods", ["None"], ["", "", "def", "update_eval_criterion_MA", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        If self.all_val_eval_metrics is unused (len=0) then we fall back to using -self.all_val_losses for the MA to determine early stopping\n        (not a minimization, but a maximization of a metric and therefore the - in the latter case)\n        :return:\n        \"\"\"", "\n", "if", "self", ".", "val_eval_criterion_MA", "is", "None", ":", "\n", "            ", "if", "len", "(", "self", ".", "all_val_eval_metrics", ")", "==", "0", ":", "\n", "                ", "self", ".", "val_eval_criterion_MA", "=", "-", "self", ".", "all_val_losses", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "val_eval_criterion_MA", "=", "self", ".", "all_val_eval_metrics", "[", "-", "1", "]", "\n", "", "", "else", ":", "\n", "            ", "if", "len", "(", "self", ".", "all_val_eval_metrics", ")", "==", "0", ":", "\n", "                ", "\"\"\"\n                We here use alpha * old - (1 - alpha) * new because new in this case is the vlaidation loss and lower \n                is better, so we need to negate it. \n                \"\"\"", "\n", "self", ".", "val_eval_criterion_MA", "=", "self", ".", "val_eval_criterion_alpha", "*", "self", ".", "val_eval_criterion_MA", "-", "(", "\n", "1", "-", "self", ".", "val_eval_criterion_alpha", ")", "*", "self", ".", "all_val_losses", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "val_eval_criterion_MA", "=", "self", ".", "val_eval_criterion_alpha", "*", "self", ".", "val_eval_criterion_MA", "+", "(", "\n", "1", "-", "self", ".", "val_eval_criterion_alpha", ")", "*", "self", ".", "all_val_eval_metrics", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.manage_patience": [[443, 492], ["network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.save_checkpoint", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.print_to_log_file", "join", "network_trainer.NetworkTrainer.print_to_log_file", "network_trainer.NetworkTrainer.print_to_log_file"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.save_checkpoint", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file"], ["", "", "", "def", "manage_patience", "(", "self", ")", ":", "\n", "# update patience", "\n", "        ", "continue_training", "=", "True", "\n", "if", "self", ".", "patience", "is", "not", "None", ":", "\n", "# if best_MA_tr_loss_for_patience and best_epoch_based_on_MA_tr_loss were not yet initialized,", "\n", "# initialize them", "\n", "            ", "if", "self", ".", "best_MA_tr_loss_for_patience", "is", "None", ":", "\n", "                ", "self", ".", "best_MA_tr_loss_for_patience", "=", "self", ".", "train_loss_MA", "\n", "\n", "", "if", "self", ".", "best_epoch_based_on_MA_tr_loss", "is", "None", ":", "\n", "                ", "self", ".", "best_epoch_based_on_MA_tr_loss", "=", "self", ".", "epoch", "\n", "\n", "", "if", "self", ".", "best_val_eval_criterion_MA", "is", "None", ":", "\n", "                ", "self", ".", "best_val_eval_criterion_MA", "=", "self", ".", "val_eval_criterion_MA", "\n", "\n", "# check if the current epoch is the best one according to moving average of validation criterion. If so", "\n", "# then save 'best' model", "\n", "# Do not use this for validation. This is intended for test set prediction only.", "\n", "", "self", ".", "print_to_log_file", "(", "\"current best_val_eval_criterion_MA is %.4f0\"", "%", "self", ".", "best_val_eval_criterion_MA", ")", "\n", "self", ".", "print_to_log_file", "(", "\"current val_eval_criterion_MA is %.4f\"", "%", "self", ".", "val_eval_criterion_MA", ")", "\n", "\n", "if", "self", ".", "val_eval_criterion_MA", ">", "self", ".", "best_val_eval_criterion_MA", ":", "\n", "                ", "self", ".", "best_val_eval_criterion_MA", "=", "self", ".", "val_eval_criterion_MA", "\n", "self", ".", "print_to_log_file", "(", "\"saving best epoch checkpoint...\"", ")", "\n", "self", ".", "save_checkpoint", "(", "join", "(", "self", ".", "output_folder", ",", "\"model_best.model\"", ")", ")", "\n", "\n", "# Now see if the moving average of the train loss has improved. If yes then reset patience, else", "\n", "# increase patience", "\n", "", "if", "self", ".", "train_loss_MA", "+", "self", ".", "train_loss_MA_eps", "<", "self", ".", "best_MA_tr_loss_for_patience", ":", "\n", "                ", "self", ".", "best_MA_tr_loss_for_patience", "=", "self", ".", "train_loss_MA", "\n", "self", ".", "best_epoch_based_on_MA_tr_loss", "=", "self", ".", "epoch", "\n", "self", ".", "print_to_log_file", "(", "\"New best epoch (train loss MA): %03.4f\"", "%", "self", ".", "best_MA_tr_loss_for_patience", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"No improvement: current train MA %03.4f, best: %03.4f, eps is %03.4f\"", "%", "\n", "(", "self", ".", "train_loss_MA", ",", "self", ".", "best_MA_tr_loss_for_patience", ",", "self", ".", "train_loss_MA_eps", ")", ")", "\n", "\n", "# if patience has reached its maximum then finish training (provided lr is low enough)", "\n", "", "if", "self", ".", "epoch", "-", "self", ".", "best_epoch_based_on_MA_tr_loss", ">", "self", ".", "patience", ":", "\n", "                ", "if", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", ">", "self", ".", "lr_threshold", ":", "\n", "                    ", "self", ".", "print_to_log_file", "(", "\"My patience ended, but I believe I need more time (lr > 1e-6)\"", ")", "\n", "self", ".", "best_epoch_based_on_MA_tr_loss", "=", "self", ".", "epoch", "-", "self", ".", "patience", "//", "2", "\n", "", "else", ":", "\n", "                    ", "self", ".", "print_to_log_file", "(", "\"My patience ended\"", ")", "\n", "continue_training", "=", "False", "\n", "", "", "else", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\n", "\"Patience: %d/%d\"", "%", "(", "self", ".", "epoch", "-", "self", ".", "best_epoch_based_on_MA_tr_loss", ",", "self", ".", "patience", ")", ")", "\n", "\n", "", "", "return", "continue_training", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.on_epoch_end": [[493, 507], ["network_trainer.NetworkTrainer.finish_online_evaluation", "network_trainer.NetworkTrainer.plot_progress", "network_trainer.NetworkTrainer.maybe_update_lr", "network_trainer.NetworkTrainer.maybe_save_checkpoint", "network_trainer.NetworkTrainer.update_eval_criterion_MA", "network_trainer.NetworkTrainer.manage_patience"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.finish_online_evaluation", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.plot_progress", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.maybe_update_lr", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.maybe_save_checkpoint", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.update_eval_criterion_MA", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.manage_patience"], ["", "def", "on_epoch_end", "(", "self", ")", ":", "\n", "        ", "self", ".", "finish_online_evaluation", "(", ")", "# does not have to do anything, but can be used to update self.all_val_eval_", "\n", "# metrics", "\n", "\n", "self", ".", "plot_progress", "(", ")", "\n", "\n", "self", ".", "maybe_update_lr", "(", ")", "\n", "\n", "self", ".", "maybe_save_checkpoint", "(", ")", "\n", "\n", "self", ".", "update_eval_criterion_MA", "(", ")", "\n", "\n", "continue_training", "=", "self", ".", "manage_patience", "(", ")", "\n", "return", "continue_training", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.update_train_loss_MA": [[508, 514], ["None"], "methods", ["None"], ["", "def", "update_train_loss_MA", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "train_loss_MA", "is", "None", ":", "\n", "            ", "self", ".", "train_loss_MA", "=", "self", ".", "all_tr_losses", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "train_loss_MA", "=", "self", ".", "train_loss_MA_alpha", "*", "self", ".", "train_loss_MA", "+", "(", "1", "-", "self", ".", "train_loss_MA_alpha", ")", "*", "self", ".", "all_tr_losses", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.run_iteration": [[515, 621], ["next", "torch.from_numpy().float.cuda", "torch.from_numpy().float.cuda", "torch.from_numpy().float.cuda", "torch.from_numpy().float.cuda", "list", "list.append", "range", "network_trainer.NetworkTrainer.optimizer.zero_grad", "network_trainer.NetworkTrainer.network", "network_trainer.NetworkTrainer.loss", "isinstance", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "isinstance", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.from_numpy().float", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "torch.nn.functional.interpolate", "list.append", "network_trainer.NetworkTrainer.run_online_evaluation", "network_trainer.NetworkTrainer.optimizer.step", "torch.round_", "torch.round_", "torch.round_", "torch.round_", "network_trainer.NetworkTrainer.backward", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "amp.scale_loss", "scaled_loss.backward"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.run_online_evaluation"], ["", "", "def", "run_iteration", "(", "self", ",", "data_generator", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ":", "\n", "        ", "data_dict", "=", "next", "(", "data_generator", ")", "\n", "data", "=", "data_dict", "[", "'data'", "]", "\n", "target", "=", "data_dict", "[", "'target'", "]", "\n", "# print('target unique is:',torch.unique(target))", "\n", "\n", "# print('data type is:',(data.shape))#torch.tensor", "\n", "# print('target type is:',(target.shape))#torch.tensor", "\n", "\"\"\"\n        data shape is: torch.Size([8, 1, 192, 192, 48])\n        target shape is: torch.Size([8, 1, 192, 192, 48])\n        data type is: torch.float32\n        target type is: torch.float32\n        \"\"\"", "\n", "\n", "if", "not", "isinstance", "(", "data", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "data", "=", "torch", ".", "from_numpy", "(", "data", ")", ".", "float", "(", ")", "\n", "", "if", "not", "isinstance", "(", "target", ",", "torch", ".", "Tensor", ")", ":", "\n", "            ", "target", "=", "torch", ".", "from_numpy", "(", "target", ")", ".", "float", "(", ")", "\n", "\n", "# target_list=target[np.newaxis,:]", "\n", "# target_list.append(target)", "\n", "# new_shape=[[8, 1, 96, 96, 48],[8, 1, 48, 48, 48],[8, 1, 24, 24, 24],[8, 1, 12, 12, 12]]", "\n", "# if self.do_supervision:", "\n", "\n", "# new_shape = [[4, 1, 48, 48, 48],[4, 1, 12, 12, 12]]", "\n", "", "new_shape", "=", "[", "[", "96", ",", "96", ",", "48", "]", ",", "[", "48", ",", "48", ",", "48", "]", ",", "[", "24", ",", "24", ",", "24", "]", ",", "[", "12", ",", "12", ",", "12", "]", "]", "\n", "data", "=", "data", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "target", "=", "target", ".", "cuda", "(", "non_blocking", "=", "True", ")", "\n", "\n", "target_list", "=", "list", "(", ")", "\n", "target_list", ".", "append", "(", "target", ")", "\n", "\n", "for", "i", "in", "range", "(", "4", ")", ":", "\n", "            ", "target_temp", "=", "torch", ".", "nn", ".", "functional", ".", "interpolate", "(", "target", ",", "size", "=", "new_shape", "[", "i", "]", ",", "mode", "=", "'trilinear'", ",", "\n", "align_corners", "=", "True", ")", "\n", "# print('target_temp shape is:',target_temp.shape)", "\n", "target_list", ".", "append", "(", "torch", ".", "round_", "(", "target_temp", ")", ")", "\n", "\n", "# target_cpu_np=target.data.cpu().numpy()", "\n", "# del target", "\n", "# # target.data.cpu()", "\n", "# # print('After cpu conversion, target is cuda?',target.is_cuda)", "\n", "# # target.numpy()", "\n", "# for i in range(2):", "\n", "#     data_cpu=resize_segmentation(target_cpu_np,new_shape[i])", "\n", "#     data_tensor=torch.from_numpy(data_cpu).float()", "\n", "#     # data_gpu=data_tensor.cuda(non_blocking=True)", "\n", "#     # print('data_gpu is cuda?',data_gpu.is_cuda)", "\n", "#     #data_gpu is cuda? True", "\n", "#     target_list.append(data_tensor.cuda(non_blocking=True))", "\n", "# print('target_list %d is cuda?'%i,target_list[i+1].is_cuda)", "\n", "# if not isinstance(target_list[i+1], torch.Tensor):", "\n", "#     target_list[i+1] = torch.from_numpy(target_list[i]).float()", "\n", "# print('target_list %d shape is:'%(i+1),target_list[i+1].shape)", "\n", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n", "output", "=", "self", ".", "network", "(", "data", ")", "\n", "# print('output is cuda?',output[0].is_cuda)", "\n", "# target_layers=output.clone()", "\n", "# for i in range(len(output)):", "\n", "#     print('output of %d shape is:'%i, output[i].shape)", "\n", "#     new_shape=output[i].shape", "\n", "\n", "\n", "\"\"\"\n        output of 0 shape is: torch.Size([8, 3, 192, 192, 48])\n        output of 1 shape is: torch.Size([8, 3, 96, 96, 48])\n        output of 2 shape is: torch.Size([8, 3, 48, 48, 48])\n        output of 3 shape is: torch.Size([8, 3, 24, 24, 24])\n        output of 4 shape is: torch.Size([8, 3, 12, 12, 12])\n        \"\"\"", "\n", "# print('output shape is:',output.shape)", "\n", "# output shape is: [8,3,192,192,48] when batch size is 8 and labels are [0,1,2]", "\n", "l", "=", "self", ".", "loss", "(", "output", ",", "target_list", ")", "\n", "# print('loss shape is:',l.shape)", "\n", "# print('run_online_evaluation is:',run_online_evaluation)", "\n", "# print('output[0] shape is:',output[0].shape)", "\n", "# print('target shape is:',target.shape)", "\n", "# print('target unique is:',torch.unique(target))", "\n", "# print('target[1] unique is:', torch.unique(target_list[1]))", "\n", "# print('target[2] unique is:', torch.unique(target_list[2]))", "\n", "# print('target[3] unique is:', torch.unique(target_list[3]))", "\n", "\"\"\"\n        run_online_evaluation is: False\n        output[0] shape is: torch.Size([4, 3, 192, 192, 48])\n        target shape is: torch.Size([4, 1, 192, 192, 48])\n        target unique is: tensor([0., 1., 2.], device='cuda:2')\n        target[1] unique is: tensor([0., 1., 2.], device='cuda:2')\n        target[2] unique is: tensor([0., 1., 2.], device='cuda:2')\n        target[3] unique is: tensor([0., 1., 2.], device='cuda:2')\n        \"\"\"", "\n", "if", "run_online_evaluation", ":", "\n", "            ", "self", ".", "run_online_evaluation", "(", "output", "[", "0", "]", ",", "target", ")", "# if do_supervision, we should set output[0], otherwise we should use output directly.", "\n", "\n", "", "if", "do_backprop", ":", "\n", "            ", "if", "not", "self", ".", "fp16", "or", "amp", "is", "None", ":", "\n", "                ", "l", ".", "backward", "(", ")", "\n", "", "else", ":", "\n", "                ", "with", "amp", ".", "scale_loss", "(", "l", ",", "self", ".", "optimizer", ")", "as", "scaled_loss", ":", "\n", "                    ", "scaled_loss", ".", "backward", "(", ")", "\n", "", "", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "return", "l", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.run_online_evaluation": [[622, 630], ["None"], "methods", ["None"], ["", "def", "run_online_evaluation", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Can be implemented, does not have to\n        :param output_torch:\n        :param target_npy:\n        :return:\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.finish_online_evaluation": [[631, 637], ["None"], "methods", ["None"], ["", "def", "finish_online_evaluation", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Can be implemented, does not have to\n        :return:\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.validate": [[638, 641], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "validate", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.find_lr": [[642, 693], ["network_trainer.NetworkTrainer._maybe_init_amp", "range", "matplotlib.figure", "matplotlib.figure", "matplotlib.xscale", "matplotlib.xscale", "matplotlib.plot", "matplotlib.plot", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.close", "matplotlib.close", "losses.append", "log_lrs.append", "join", "network_trainer.NetworkTrainer.run_iteration().data.item", "math.log10", "network_trainer.NetworkTrainer.run_iteration"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer._maybe_init_amp", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.run_iteration"], ["", "def", "find_lr", "(", "self", ",", "num_iters", "=", "1000", ",", "init_value", "=", "1e-6", ",", "final_value", "=", "10.", ",", "beta", "=", "0.98", ")", ":", "\n", "        ", "\"\"\"\n        stolen and adapted from here: https://sgugger.github.io/how-do-you-find-a-good-learning-rate.html\n        :param num_iters:\n        :param init_value:\n        :param final_value:\n        :param beta:\n        :return:\n        \"\"\"", "\n", "import", "math", "\n", "self", ".", "_maybe_init_amp", "(", ")", "\n", "mult", "=", "(", "final_value", "/", "init_value", ")", "**", "(", "1", "/", "num_iters", ")", "\n", "lr", "=", "init_value", "\n", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "lr", "\n", "avg_loss", "=", "0.", "\n", "best_loss", "=", "0.", "\n", "losses", "=", "[", "]", "\n", "log_lrs", "=", "[", "]", "\n", "\n", "for", "batch_num", "in", "range", "(", "1", ",", "num_iters", "+", "1", ")", ":", "\n", "# +1 because this one here is not designed to have negative loss...", "\n", "            ", "loss", "=", "self", ".", "run_iteration", "(", "self", ".", "tr_gen", ",", "do_backprop", "=", "True", ",", "run_online_evaluation", "=", "False", ")", ".", "data", ".", "item", "(", ")", "+", "1", "\n", "\n", "# Compute the smoothed loss", "\n", "avg_loss", "=", "beta", "*", "avg_loss", "+", "(", "1", "-", "beta", ")", "*", "loss", "\n", "smoothed_loss", "=", "avg_loss", "/", "(", "1", "-", "beta", "**", "batch_num", ")", "\n", "\n", "# Stop if the loss is exploding", "\n", "if", "batch_num", ">", "1", "and", "smoothed_loss", ">", "4", "*", "best_loss", ":", "\n", "                ", "break", "\n", "\n", "# Record the best loss", "\n", "", "if", "smoothed_loss", "<", "best_loss", "or", "batch_num", "==", "1", ":", "\n", "                ", "best_loss", "=", "smoothed_loss", "\n", "\n", "# Store the values", "\n", "", "losses", ".", "append", "(", "smoothed_loss", ")", "\n", "log_lrs", ".", "append", "(", "math", ".", "log10", "(", "lr", ")", ")", "\n", "\n", "# Update the lr for the next step", "\n", "lr", "*=", "mult", "\n", "self", ".", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "=", "lr", "\n", "\n", "", "import", "matplotlib", ".", "pyplot", "as", "plt", "\n", "lrs", "=", "[", "10", "**", "i", "for", "i", "in", "log_lrs", "]", "\n", "fig", "=", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "xscale", "(", "'log'", ")", "\n", "plt", ".", "plot", "(", "lrs", "[", "10", ":", "-", "5", "]", ",", "losses", "[", "10", ":", "-", "5", "]", ")", "\n", "plt", ".", "savefig", "(", "join", "(", "self", ".", "output_folder", ",", "\"lr_finder.png\"", ")", ")", "\n", "plt", ".", "close", "(", ")", "\n", "return", "log_lrs", ",", "losses", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.__init__": [[29, 111], ["nnunet.training.network_training.network_trainer.NetworkTrainer.__init__", "nnunet.training.loss_functions.dice_loss.DC_and_CE_loss", "nnUNetTrainer.nnUNetTrainer.update_fold", "isdir", "join", "collections.OrderedDict"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.update_fold"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param deterministic:\n        :param fold: can be either [0 ... 5) for cross-validation, 'all' to train on all available training data or\n        None if you wish to load some checkpoint and do inference only\n        :param plans_file: the pkl file generated by preprocessing. This file will determine all design choices\n        :param subfolder_with_preprocessed_data: must be a subfolder of dataset_directory (just the name of the folder,\n        not the entire path). This is where the preprocessed data lies that will be used for network training. We made\n        this explicitly available so that differently preprocessed data can coexist and the user can choose what to use.\n        Can be None if you are doing inference only.\n        :param output_folder: where to store parameters, plot progress and to the validation\n        :param dataset_directory: the parent directory in which the preprocessed Task data is stored. This is required\n        because the split information is stored in this directory. For running prediction only this input is not\n        required and may be set to None\n        :param batch_dice: compute dice loss for each sample and average over all samples in the batch or pretend the\n        batch is a pseudo volume?\n        :param stage: The plans file may contain several stages (used for lowres / highres / pyramid). Stage must be\n        specified for training:\n        if stage 1 exists then stage 1 is the high resolution stage, otherwise it's 0\n        :param unpack_data: if False, npz preprocessed data will not be unpacked to npy. This consumes less space but\n        is considerably slower! Running unpack_data=False with 2d should never be done!\n\n        IMPORTANT: If you inherit from nnUNetTrainer and the init args change then you need to redefine self.init_args\n        in your init accordingly. Otherwise checkpoints won't load properly!\n        \"\"\"", "\n", "super", "(", "nnUNetTrainer", ",", "self", ")", ".", "__init__", "(", "deterministic", ",", "fp16", ")", "\n", "self", ".", "unpack_data", "=", "unpack_data", "\n", "self", ".", "init_args", "=", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "fp16", ")", "\n", "# set through arguments from init", "\n", "self", ".", "stage", "=", "stage", "\n", "self", ".", "experiment_name", "=", "self", ".", "__class__", ".", "__name__", "\n", "self", ".", "plans_file", "=", "plans_file", "\n", "self", ".", "output_folder", "=", "output_folder", "\n", "self", ".", "dataset_directory", "=", "dataset_directory", "\n", "self", ".", "output_folder_base", "=", "self", ".", "output_folder", "\n", "self", ".", "fold", "=", "fold", "\n", "\n", "self", ".", "plans", "=", "None", "\n", "\n", "# if we are running inference only then the self.dataset_directory is set (due to checkpoint loading) but it", "\n", "# irrelevant", "\n", "if", "self", ".", "dataset_directory", "is", "not", "None", "and", "isdir", "(", "self", ".", "dataset_directory", ")", ":", "\n", "            ", "self", ".", "gt_niftis_folder", "=", "join", "(", "self", ".", "dataset_directory", ",", "\"gt_segmentations\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "gt_niftis_folder", "=", "None", "\n", "\n", "", "self", ".", "folder_with_preprocessed_data", "=", "None", "\n", "\n", "# set in self.initialize()", "\n", "\n", "self", ".", "dl_tr", "=", "self", ".", "dl_val", "=", "None", "\n", "self", ".", "num_input_channels", "=", "self", ".", "num_classes", "=", "self", ".", "net_pool_per_axis", "=", "self", ".", "patch_size", "=", "self", ".", "batch_size", "=", "self", ".", "threeD", "=", "self", ".", "base_num_features", "=", "self", ".", "intensity_properties", "=", "self", ".", "normalization_schemes", "=", "self", ".", "net_num_pool_op_kernel_sizes", "=", "self", ".", "net_conv_kernel_sizes", "=", "None", "# loaded automatically from plans_file", "\n", "self", ".", "basic_generator_patch_size", "=", "self", ".", "data_aug_params", "=", "None", "\n", "\n", "self", ".", "batch_dice", "=", "batch_dice", "\n", "self", ".", "loss", "=", "DC_and_CE_loss", "(", "{", "'batch_dice'", ":", "self", ".", "batch_dice", ",", "'smooth'", ":", "1e-5", ",", "'smooth_in_nom'", ":", "True", ",", "\n", "'do_bg'", ":", "False", ",", "'rebalance_weights'", ":", "None", ",", "'background_weight'", ":", "1", "}", ",", "OrderedDict", "(", ")", ")", "\n", "\n", "self", ".", "online_eval_foreground_dc", "=", "[", "]", "\n", "self", ".", "online_eval_tp", "=", "[", "]", "\n", "self", ".", "online_eval_fp", "=", "[", "]", "\n", "self", ".", "online_eval_fn", "=", "[", "]", "\n", "\n", "self", ".", "classes", "=", "self", ".", "do_dummy_2D_aug", "=", "self", ".", "use_mask_for_norm", "=", "self", ".", "only_keep_largest_connected_component", "=", "self", ".", "min_region_size_per_class", "=", "self", ".", "min_size_per_class", "=", "None", "\n", "\n", "self", ".", "inference_pad_border_mode", "=", "\"constant\"", "\n", "self", ".", "inference_pad_kwargs", "=", "{", "'constant_values'", ":", "0", "}", "\n", "\n", "self", ".", "update_fold", "(", "fold", ")", "\n", "self", ".", "pad_all_sides", "=", "None", "\n", "\n", "self", ".", "lr_scheduler_eps", "=", "1e-3", "\n", "self", ".", "lr_scheduler_patience", "=", "30", "\n", "self", ".", "initial_lr", "=", "3e-4", "\n", "self", ".", "weight_decay", "=", "3e-5", "\n", "\n", "self", ".", "oversample_foreground_percent", "=", "0.33", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.update_fold": [[112, 130], ["isinstance", "nnUNetTrainer.nnUNetTrainer.output_folder.endswith", "join", "nnUNetTrainer.nnUNetTrainer.output_folder.endswith", "join", "str", "str", "str", "str"], "methods", ["None"], ["", "def", "update_fold", "(", "self", ",", "fold", ")", ":", "\n", "        ", "\"\"\"\n        used to swap between folds for inference (ensemble of models from cross-validation)\n        DO NOT USE DURING TRAINING AS THIS WILL NOT UPDATE THE DATASET SPLIT AND THE DATA AUGMENTATION GENERATORS\n        :param fold:\n        :return:\n        \"\"\"", "\n", "if", "fold", "is", "not", "None", ":", "\n", "            ", "if", "isinstance", "(", "fold", ",", "str", ")", ":", "\n", "                ", "assert", "fold", "==", "\"all\"", ",", "\"if self.fold is a string then it must be \\'all\\'\"", "\n", "if", "self", ".", "output_folder", ".", "endswith", "(", "\"%s\"", "%", "str", "(", "self", ".", "fold", ")", ")", ":", "\n", "                    ", "self", ".", "output_folder", "=", "self", ".", "output_folder_base", "\n", "", "self", ".", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "\"%s\"", "%", "str", "(", "fold", ")", ")", "\n", "", "else", ":", "\n", "                ", "if", "self", ".", "output_folder", ".", "endswith", "(", "\"fold_%s\"", "%", "str", "(", "self", ".", "fold", ")", ")", ":", "\n", "                    ", "self", ".", "output_folder", "=", "self", ".", "output_folder_base", "\n", "", "self", ".", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "\"fold_%s\"", "%", "str", "(", "fold", ")", ")", "\n", "", "self", ".", "fold", "=", "fold", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.setup_DA_params": [[131, 166], ["nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "numpy.array", "nnunet.training.data_augmentation.default_data_augmentation.get_patch_size", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "max", "min", "list"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.default_data_augmentation.get_patch_size", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file"], ["", "", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "threeD", ":", "\n", "            ", "self", ".", "data_aug_params", "=", "default_3D_augmentation_params", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "                ", "self", ".", "data_aug_params", "[", "\"dummy_2D\"", "]", "=", "True", "\n", "self", ".", "print_to_log_file", "(", "\"Using dummy2d data augmentation\"", ")", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_alpha\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_alpha\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"elastic_deform_sigma\"", "]", "=", "default_2D_augmentation_params", "[", "\"elastic_deform_sigma\"", "]", "\n", "self", ".", "data_aug_params", "[", "\"rotation_x\"", "]", "=", "default_2D_augmentation_params", "[", "\"rotation_x\"", "]", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "do_dummy_2D_aug", "=", "False", "\n", "if", "max", "(", "self", ".", "patch_size", ")", "/", "min", "(", "self", ".", "patch_size", ")", ">", "1.5", ":", "\n", "                ", "default_2D_augmentation_params", "[", "'rotation_x'", "]", "=", "(", "-", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ",", "15.", "/", "360", "*", "2.", "*", "np", ".", "pi", ")", "\n", "", "self", ".", "data_aug_params", "=", "default_2D_augmentation_params", "\n", "", "self", ".", "data_aug_params", "[", "\"mask_was_used_for_normalization\"", "]", "=", "self", ".", "use_mask_for_norm", "\n", "\n", "if", "self", ".", "do_dummy_2D_aug", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", "[", "1", ":", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "self", ".", "basic_generator_patch_size", "=", "np", ".", "array", "(", "[", "self", ".", "patch_size", "[", "0", "]", "]", "+", "list", "(", "self", ".", "basic_generator_patch_size", ")", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "basic_generator_patch_size", "=", "get_patch_size", "(", "self", ".", "patch_size", ",", "self", ".", "data_aug_params", "[", "'rotation_x'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_y'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'rotation_z'", "]", ",", "\n", "self", ".", "data_aug_params", "[", "'scale_range'", "]", ")", "\n", "patch_size_for_spatialtransform", "=", "self", ".", "patch_size", "\n", "\n", "", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", "]", "\n", "self", ".", "data_aug_params", "[", "'patch_size_for_spatialtransform'", "]", "=", "patch_size_for_spatialtransform", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.initialize": [[167, 213], ["print", "maybe_mkdir_p", "nnUNetTrainer.nnUNetTrainer.process_plans", "nnUNetTrainer.nnUNetTrainer.setup_DA_params", "join", "print", "nnUNetTrainer.nnUNetTrainer.initialize_network_optimizer_and_scheduler", "nnUNetTrainer.nnUNetTrainer.load_plans_file", "print", "nnUNetTrainer.nnUNetTrainer.get_basic_generators", "print", "print", "nnunet.training.data_augmentation.default_data_augmentation.get_default_augmentation", "print", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "str", "str", "nnUNetTrainer.nnUNetTrainer.dataset_tr.keys", "nnUNetTrainer.nnUNetTrainer.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.process_plans", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.initialize_network_optimizer_and_scheduler", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.get_basic_generators", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.default_data_augmentation.get_default_augmentation", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.unpack_dataset", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        For prediction of test cases just set training=False, this will prevent loading of training data and\n        training batchgenerator initialization\n        :param training:\n        :return:\n        \"\"\"", "\n", "print", "(", "'output_folder is:'", ",", "self", ".", "output_folder", ")", "\n", "maybe_mkdir_p", "(", "self", ".", "output_folder", ")", "\n", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "            ", "self", ".", "load_plans_file", "(", ")", "\n", "# print('self.plans.keys() is:',self.plans.keys())", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", ")", "\n", "print", "(", "'self.folder_with_preprocessed_data is:'", ",", "self", ".", "folder_with_preprocessed_data", ")", "\n", "if", "training", ":", "\n", "            ", "print", "(", "'************Now it is going into the phase of get_basic_generator! *********'", ")", "\n", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "print", "(", "'************Now get_basic_generator has been done! *********'", ")", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "self", ".", "print_to_log_file", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "print_to_log_file", "(", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "", "print", "(", "'************Now it is going into the phase of get_default_augmentation*********'", ")", "\n", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_default_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "\n", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ")", "\n", "print", "(", "'************Now get_default_augmentation has been done! *********'", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ",", "\n", "also_print_to_console", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "pass", "\n", "", "self", ".", "initialize_network_optimizer_and_scheduler", "(", ")", "\n", "#assert isinstance(self.network, (SegmentationNetwork, nn.DataParallel))", "\n", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.initialize_network_optimizer_and_scheduler": [[214, 260], ["print", "len", "torch.cuda.set_device", "nnunet.network_architecture.generic_UNet.Generic_UNet", "tensorwatch.draw_model", "tensorwatch.draw_model.save", "print", "torch.optim.Adam", "torch.optim.lr_scheduler.ReduceLROnPlateau", "nnUNetTrainer.nnUNetTrainer.network.cuda", "nnunet.network_architecture.initialization.InitWeights_He", "nnUNetTrainer.nnUNetTrainer.network.parameters"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.NeuralNetwork.set_device"], ["", "def", "initialize_network_optimizer_and_scheduler", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This is specific to the U-Net and must be adapted for other network architectures\n        :return:\n        \"\"\"", "\n", "#self.print_to_log_file(self.net_num_pool_op_kernel_sizes)", "\n", "#self.print_to_log_file(self.net_conv_kernel_sizes)", "\n", "print", "(", "'Has entered into the initialize_network_optimizer_and_scheduler\uff01'", ")", "\n", "net_numpool", "=", "len", "(", "self", ".", "net_num_pool_op_kernel_sizes", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv3d", "\n", "dropout_op", "=", "nn", ".", "Dropout3d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm3d", "\n", "", "else", ":", "\n", "            ", "conv_op", "=", "nn", ".", "Conv2d", "\n", "dropout_op", "=", "nn", ".", "Dropout2d", "\n", "norm_op", "=", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "norm_op_kwargs", "=", "{", "'eps'", ":", "1e-5", ",", "'affine'", ":", "True", "}", "\n", "dropout_op_kwargs", "=", "{", "'p'", ":", "0", ",", "'inplace'", ":", "True", "}", "\n", "net_nonlin", "=", "nn", ".", "LeakyReLU", "\n", "net_nonlin_kwargs", "=", "{", "'negative_slope'", ":", "1e-2", ",", "'inplace'", ":", "True", "}", "\n", "torch", ".", "cuda", ".", "set_device", "(", "0", ")", "\n", "\n", "self", ".", "do_supervision", "=", "False", "\n", "self", ".", "network", "=", "Generic_UNet", "(", "self", ".", "num_input_channels", ",", "self", ".", "base_num_features", ",", "self", ".", "num_classes", ",", "net_numpool", ",", "\n", "2", ",", "2", ",", "conv_op", ",", "norm_op", ",", "norm_op_kwargs", ",", "dropout_op", ",", "dropout_op_kwargs", ",", "\n", "net_nonlin", ",", "net_nonlin_kwargs", ",", "self", ".", "do_supervision", ",", "False", ",", "lambda", "x", ":", "x", ",", "InitWeights_He", "(", "1e-2", ")", ",", "\n", "self", ".", "net_num_pool_op_kernel_sizes", ",", "self", ".", "net_conv_kernel_sizes", ",", "False", ",", "True", ",", "True", ")", "\n", "\n", "# print('self.network is:',self.network)", "\n", "# Here!!! Modify it to be parallel model", "\n", "# self.network=nn.DataParallel(self.network,device_ids=[0,1,2,3])", "\n", "g", "=", "tw", ".", "draw_model", "(", "self", ".", "network", ",", "[", "8", ",", "1", ",", "128", ",", "128", ",", "128", "]", ")", "\n", "g", ".", "save", "(", "'/cache/WenshuaiZhao/ProjectFiles/NNUnet/nnunet/model_structure.png'", ")", "\n", "print", "(", "'draw model has been done!'", ")", "\n", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "network", ".", "parameters", "(", ")", ",", "self", ".", "initial_lr", ",", "weight_decay", "=", "self", ".", "weight_decay", ",", "amsgrad", "=", "True", ")", "\n", "# Here!!! optimizer is also need to be parallel", "\n", "# self.optimizer=nn.DataParallel(self.optimizer,device_ids=[0,1,2,3])", "\n", "\n", "self", ".", "lr_scheduler", "=", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "self", ".", "optimizer", ",", "mode", "=", "'min'", ",", "factor", "=", "0.2", ",", "patience", "=", "self", ".", "lr_scheduler_patience", ",", "\n", "verbose", "=", "True", ",", "threshold", "=", "self", ".", "lr_scheduler_eps", ",", "threshold_mode", "=", "\"abs\"", ")", "\n", "self", ".", "network", ".", "cuda", "(", ")", "\n", "self", ".", "network", ".", "inference_apply_nonlin", "=", "softmax_helper", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.run_training": [[261, 279], ["collections.OrderedDict", "nnUNetTrainer.nnUNetTrainer.__dir__", "save_json", "shutil.copy", "super().run_training", "join", "join", "k.startswith", "callable", "str", "getattr", "getattr"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.run_training"], ["", "def", "run_training", "(", "self", ")", ":", "\n", "        ", "dct", "=", "OrderedDict", "(", ")", "\n", "for", "k", "in", "self", ".", "__dir__", "(", ")", ":", "\n", "            ", "if", "not", "k", ".", "startswith", "(", "\"__\"", ")", ":", "\n", "                ", "if", "not", "callable", "(", "getattr", "(", "self", ",", "k", ")", ")", ":", "\n", "                    ", "dct", "[", "k", "]", "=", "str", "(", "getattr", "(", "self", ",", "k", ")", ")", "\n", "", "", "", "del", "dct", "[", "'plans'", "]", "\n", "del", "dct", "[", "'intensity_properties'", "]", "\n", "del", "dct", "[", "'dataset'", "]", "\n", "del", "dct", "[", "'dataset_tr'", "]", "\n", "del", "dct", "[", "'dataset_val'", "]", "\n", "save_json", "(", "dct", ",", "join", "(", "self", ".", "output_folder", ",", "\"debug.json\"", ")", ")", "\n", "\n", "import", "shutil", "\n", "\n", "shutil", ".", "copy", "(", "self", ".", "plans_file", ",", "join", "(", "self", ".", "output_folder_base", ",", "\"plans.pkl\"", ")", ")", "\n", "\n", "super", "(", "nnUNetTrainer", ",", "self", ")", ".", "run_training", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file": [[280, 286], ["load_pickle"], "methods", ["None"], ["", "def", "load_plans_file", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This is what actually configures the entire experiment. The plans file is generated by experiment planning\n        :return:\n        \"\"\"", "\n", "self", ".", "plans", "=", "load_pickle", "(", "self", ".", "plans_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.process_plans": [[287, 322], ["numpy.array().astype", "len", "len", "list", "numpy.array", "len", "RuntimeError", "list", "plans[].keys", "plans[].keys", "str"], "methods", ["None"], ["", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "if", "self", ".", "stage", "is", "None", ":", "\n", "            ", "assert", "len", "(", "list", "(", "plans", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", ")", "==", "1", ",", "\"If self.stage is None then there can be only one stage in the plans file. That seems to not be the \"", "\"case. Please specify which stage of the cascade must be trained\"", "\n", "self", ".", "stage", "=", "list", "(", "plans", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", "[", "0", "]", "\n", "\n", "", "self", ".", "plans", "=", "plans", "\n", "\n", "stage_plans", "=", "self", ".", "plans", "[", "'plans_per_stage'", "]", "[", "self", ".", "stage", "]", "\n", "self", ".", "batch_size", "=", "stage_plans", "[", "'batch_size'", "]", "\n", "self", ".", "net_pool_per_axis", "=", "stage_plans", "[", "'num_pool_per_axis'", "]", "\n", "self", ".", "patch_size", "=", "np", ".", "array", "(", "stage_plans", "[", "'patch_size'", "]", ")", ".", "astype", "(", "int", ")", "\n", "self", ".", "do_dummy_2D_aug", "=", "stage_plans", "[", "'do_dummy_2D_data_aug'", "]", "\n", "self", ".", "net_num_pool_op_kernel_sizes", "=", "stage_plans", "[", "'pool_op_kernel_sizes'", "]", "\n", "self", ".", "net_conv_kernel_sizes", "=", "stage_plans", "[", "'conv_kernel_sizes'", "]", "\n", "\n", "self", ".", "pad_all_sides", "=", "None", "# self.patch_size", "\n", "self", ".", "intensity_properties", "=", "plans", "[", "'dataset_properties'", "]", "[", "'intensityproperties'", "]", "\n", "self", ".", "normalization_schemes", "=", "plans", "[", "'normalization_schemes'", "]", "\n", "self", ".", "base_num_features", "=", "plans", "[", "'base_num_features'", "]", "\n", "self", ".", "num_input_channels", "=", "plans", "[", "'num_modalities'", "]", "\n", "self", ".", "num_classes", "=", "plans", "[", "'num_classes'", "]", "+", "1", "# background is no longer in num_classes", "\n", "self", ".", "classes", "=", "plans", "[", "'all_classes'", "]", "\n", "self", ".", "use_mask_for_norm", "=", "plans", "[", "'use_mask_for_norm'", "]", "\n", "self", ".", "only_keep_largest_connected_component", "=", "plans", "[", "'keep_only_largest_region'", "]", "\n", "self", ".", "min_region_size_per_class", "=", "plans", "[", "'min_region_size_per_class'", "]", "\n", "self", ".", "min_size_per_class", "=", "None", "# DONT USE THIS. plans['min_size_per_class']", "\n", "\n", "if", "len", "(", "self", ".", "patch_size", ")", "==", "2", ":", "\n", "            ", "self", ".", "threeD", "=", "False", "\n", "", "elif", "len", "(", "self", ".", "patch_size", ")", "==", "3", ":", "\n", "            ", "self", ".", "threeD", "=", "True", "\n", "", "else", ":", "\n", "            ", "raise", "RuntimeError", "(", "\"invalid patch size in plans file: %s\"", "%", "str", "(", "self", ".", "patch_size", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.load_dataset": [[323, 325], ["nnunet.training.dataloading.dataset_loading.load_dataset"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.load_dataset"], ["", "", "def", "load_dataset", "(", "self", ")", ":", "\n", "        ", "self", ".", "dataset", "=", "load_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.get_basic_generators": [[326, 362], ["nnUNetTrainer.nnUNetTrainer.load_dataset", "nnUNetTrainer.nnUNetTrainer.do_split", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader2D", "nnunet.training.dataloading.dataset_loading.DataLoader2D", "nnUNetTrainer.nnUNetTrainer.plans.get", "nnUNetTrainer.nnUNetTrainer.plans.get"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split"], ["", "def", "get_basic_generators", "(", "self", ")", ":", "\n", "        ", "self", ".", "load_dataset", "(", ")", "\n", "\"\"\"\n        def load_dataset(folder):\n        # we don't load the actual data but instead return the filename to the np file. the properties are loaded though\n        case_identifiers = get_case_identifiers(folder)\n        case_identifiers.sort()\n        dataset = OrderedDict()\n        for c in case_identifiers:\n            dataset[c] = OrderedDict()\n            dataset[c]['data_file'] = join(folder, \"%s.npz\"%c)\n            with open(join(folder, \"%s.pkl\"%c), 'rb') as f:\n                dataset[c]['properties'] = pickle.load(f)\n            if dataset[c].get('seg_from_prev_stage_file') is not None:\n                dataset[c]['seg_from_prev_stage_file'] = join(folder, \"%s_segs.npz\"%c)\n        return dataset\n        \"\"\"", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "dl_tr", "=", "DataLoader3D", "(", "self", ".", "dataset_tr", ",", "self", ".", "basic_generator_patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "False", ",", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "dl_val", "=", "DataLoader3D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "False", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "", "else", ":", "\n", "            ", "dl_tr", "=", "DataLoader2D", "(", "self", ".", "dataset_tr", ",", "self", ".", "basic_generator_patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "transpose", "=", "self", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "dl_val", "=", "DataLoader2D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "transpose", "=", "self", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ",", "\n", "pad_mode", "=", "\"constant\"", ",", "pad_sides", "=", "self", ".", "pad_all_sides", ")", "\n", "", "return", "dl_tr", ",", "dl_val", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.preprocess_patient": [[363, 380], ["PreprocessorFor2D.preprocess_test_case", "GenericPreprocessor", "PreprocessorFor2D"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.GenericPreprocessor.preprocess_test_case"], ["", "def", "preprocess_patient", "(", "self", ",", "input_files", ")", ":", "\n", "        ", "\"\"\"\n        Used to predict new unseen data. Not used for the preprocessing of the training/test data\n        :param input_files:\n        :return:\n        \"\"\"", "\n", "from", "nnunet", ".", "preprocessing", ".", "preprocessing", "import", "GenericPreprocessor", ",", "PreprocessorFor2D", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "preprocessor", "=", "GenericPreprocessor", "(", "self", ".", "normalization_schemes", ",", "self", ".", "use_mask_for_norm", ",", "\n", "self", ".", "intensity_properties", ")", "\n", "", "else", ":", "\n", "            ", "preprocessor", "=", "PreprocessorFor2D", "(", "self", ".", "normalization_schemes", ",", "self", ".", "use_mask_for_norm", ",", "\n", "self", ".", "intensity_properties", ")", "\n", "\n", "", "d", ",", "s", ",", "properties", "=", "preprocessor", ".", "preprocess_test_case", "(", "input_files", ",", "\n", "self", ".", "plans", "[", "'plans_per_stage'", "]", "[", "self", ".", "stage", "]", "[", "'current_spacing'", "]", ")", "\n", "return", "d", ",", "s", ",", "properties", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.preprocess_predict_nifti": [[381, 398], ["print", "nnUNetTrainer.nnUNetTrainer.preprocess_patient", "print", "nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_softmax", "print", "nnunet.inference.segmentation_export.save_segmentation_nifti_from_softmax", "print"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.preprocess_patient", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_softmax", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.segmentation_export.save_segmentation_nifti_from_softmax"], ["", "def", "preprocess_predict_nifti", "(", "self", ",", "input_files", ",", "output_file", "=", "None", ",", "softmax_ouput_file", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        Use this to predict new data\n        :param input_files:\n        :param output_file:\n        :param softmax_ouput_file:\n        :return:\n        \"\"\"", "\n", "print", "(", "\"preprocessing...\"", ")", "\n", "d", ",", "s", ",", "properties", "=", "self", ".", "preprocess_patient", "(", "input_files", ")", "\n", "print", "(", "\"predicting...\"", ")", "\n", "pred", "=", "self", ".", "predict_preprocessed_data_return_softmax", "(", "d", ",", "True", ",", "1", ",", "False", ",", "1", ",", "(", "0", ",", "1", ",", "2", ")", ",", "True", ",", "True", ",", "2", ",", "\n", "self", ".", "patch_size", ",", "True", ")", "# TODO use da params for mirror", "\n", "print", "(", "\"resampling to original spacing and nifti export...\"", ")", "\n", "save_segmentation_nifti_from_softmax", "(", "pred", ",", "output_file", ",", "properties", ",", "3", ",", "None", ",", "None", ",", "None", ",", "softmax_ouput_file", ",", "\n", "None", ")", "\n", "print", "(", "\"done\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_softmax": [[399, 422], ["isinstance", "nnUNetTrainer.nnUNetTrainer.network.predict_3D"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_architecture.neural_network.SegmentationNetwork.predict_3D"], ["", "def", "predict_preprocessed_data_return_softmax", "(", "self", ",", "data", ",", "do_mirroring", ",", "num_repeats", ",", "use_train_mode", ",", "batch_size", ",", "\n", "mirror_axes", ",", "tiled", ",", "tile_in_z", ",", "step", ",", "min_size", ",", "use_gaussian", ")", ":", "\n", "        ", "\"\"\"\n        Don't use this. If you need softmax output, use preprocess_predict_nifti and set softmax_output_file.\n        :param data:\n        :param do_mirroring:\n        :param num_repeats:\n        :param use_train_mode:\n        :param batch_size:\n        :param mirror_axes:\n        :param tiled:\n        :param tile_in_z:\n        :param step:\n        :param min_size:\n        :param use_gaussian:\n        :param use_temporal:\n        :return:\n        \"\"\"", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "(", "SegmentationNetwork", ",", "nn", ".", "DataParallel", ")", ")", "\n", "return", "self", ".", "network", ".", "predict_3D", "(", "data", ",", "do_mirroring", ",", "num_repeats", ",", "use_train_mode", ",", "batch_size", ",", "mirror_axes", ",", "\n", "tiled", ",", "tile_in_z", ",", "step", ",", "min_size", ",", "use_gaussian", "=", "use_gaussian", ",", "\n", "pad_border_mode", "=", "self", ".", "inference_pad_border_mode", ",", "\n", "pad_kwargs", "=", "self", ".", "inference_pad_kwargs", ")", "[", "2", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.validate": [[423, 558], ["join", "maybe_mkdir_p", "multiprocessing.Pool", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "nnUNetTrainer.nnUNetTrainer.dataset_val.keys", "print", "nnunet.evaluation.evaluator.aggregate_scores", "nnUNetTrainer.nnUNetTrainer.load_dataset", "nnUNetTrainer.nnUNetTrainer.do_split", "print", "pred_gt_tuples.append", "i.get", "nnUNetTrainer.nnUNetTrainer.dataset_directory.split", "collections.OrderedDict", "list", "write_json", "nnUNetTrainer.nnUNetTrainer.plans.get", "print", "nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_softmax", "results.append", "list", "join", "collections.OrderedDict.keys", "float", "join", "[].split", "isfile", "numpy.load", "data.transpose.transpose.transpose", "join.argmax", "nnUNetTrainer.nnUNetTrainer.plans.get", "join.transpose", "join", "numpy.prod", "numpy.save", "join", "multiprocessing.Pool.starmap_async", "join", "join", "range", "join", "int", "nnunet.evaluation.metrics.ConfusionMatrix", "nnunet.evaluation.metrics.ConfusionMatrix.compute", "join", "str", "int", "collections.OrderedDict.keys", "collections.OrderedDict.keys", "collections.OrderedDict.keys", "join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.aggregate_scores", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_softmax", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.compute"], ["", "def", "validate", "(", "self", ",", "do_mirroring", "=", "True", ",", "use_train_mode", "=", "False", ",", "tiled", "=", "True", ",", "step", "=", "2", ",", "save_softmax", "=", "True", ",", "\n", "use_gaussian", "=", "True", ",", "compute_global_dice", "=", "True", ",", "override", "=", "True", ",", "validation_folder_name", "=", "'validation'", ")", ":", "\n", "        ", "\"\"\"\n        2018_12_05: I added global accumulation of TP, FP and FN for the validation in here. This is because I believe\n        that selecting models is easier when computing the Dice globally instead of independently for each case and\n        then averaging over cases. The Lung dataset in particular is very unstable because of the small size of the\n        Lung Lesions. My theory is that even though the global Dice is different than the acutal target metric it is\n        still a good enough substitute that allows us to get a lot more stable results when rerunning the same\n        experiment twice. FYI: computer vision community uses the global jaccard for the evaluation of Cityscapes etc,\n        not the per-image jaccard averaged over images.\n        The reason I am accumulating TP/FP/FN here and not from the nifti files (which are used by our Evaluator) is\n        that all predictions made here will have identical voxel spacing whereas voxel spacings in the nifti files\n        will be different (which we could compensate for by using the volume per voxel but that would require the\n        evaluator to understand spacings which is does not at this point)\n\n        :param do_mirroring:\n        :param use_train_mode:\n        :param mirror_axes:\n        :param tiled:\n        :param tile_in_z:\n        :param step:\n        :param use_nifti:\n        :param save_softmax:\n        :param use_gaussian:\n        :param use_temporal_models:\n        :return:\n        \"\"\"", "\n", "assert", "self", ".", "was_initialized", ",", "\"must initialize, ideally with checkpoint (or train first)\"", "\n", "if", "self", ".", "dataset_val", "is", "None", ":", "\n", "            ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "validation_folder_name", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "\n", "if", "do_mirroring", ":", "\n", "            ", "mirror_axes", "=", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", "\n", "", "else", ":", "\n", "            ", "mirror_axes", "=", "(", ")", "\n", "\n", "", "pred_gt_tuples", "=", "[", "]", "\n", "\n", "export_pool", "=", "Pool", "(", "4", ")", "\n", "results", "=", "[", "]", "\n", "global_tp", "=", "OrderedDict", "(", ")", "\n", "global_fp", "=", "OrderedDict", "(", ")", "\n", "global_fn", "=", "OrderedDict", "(", ")", "\n", "\n", "for", "k", "in", "self", ".", "dataset_val", ".", "keys", "(", ")", ":", "\n", "            ", "print", "(", "k", ")", "\n", "properties", "=", "self", ".", "dataset", "[", "k", "]", "[", "'properties'", "]", "\n", "fname", "=", "properties", "[", "'list_of_data_files'", "]", "[", "0", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "12", "]", "\n", "if", "override", "or", "(", "not", "isfile", "(", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ")", ")", ":", "\n", "                ", "data", "=", "np", ".", "load", "(", "self", ".", "dataset", "[", "k", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "\n", "transpose_forward", "=", "self", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", "\n", "if", "transpose_forward", "is", "not", "None", ":", "\n", "                    ", "data", "=", "data", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "transpose_forward", "]", ")", "\n", "\n", "", "print", "(", "k", ",", "data", ".", "shape", ")", "\n", "data", "[", "-", "1", "]", "[", "data", "[", "-", "1", "]", "==", "-", "1", "]", "=", "0", "\n", "\n", "softmax_pred", "=", "self", ".", "predict_preprocessed_data_return_softmax", "(", "data", "[", ":", "-", "1", "]", ",", "do_mirroring", ",", "1", ",", "\n", "use_train_mode", ",", "1", ",", "mirror_axes", ",", "tiled", ",", "\n", "True", ",", "step", ",", "self", ".", "patch_size", ",", "\n", "use_gaussian", "=", "use_gaussian", ")", "\n", "\n", "if", "compute_global_dice", ":", "\n", "                    ", "predicted_segmentation", "=", "softmax_pred", ".", "argmax", "(", "0", ")", "\n", "gt_segmentation", "=", "data", "[", "-", "1", "]", "\n", "labels", "=", "properties", "[", "'classes'", "]", "\n", "labels", "=", "[", "int", "(", "i", ")", "for", "i", "in", "labels", "if", "i", ">", "0", "]", "\n", "for", "l", "in", "labels", ":", "\n", "                        ", "if", "l", "not", "in", "global_fn", ".", "keys", "(", ")", ":", "\n", "                            ", "global_fn", "[", "l", "]", "=", "0", "\n", "", "if", "l", "not", "in", "global_fp", ".", "keys", "(", ")", ":", "\n", "                            ", "global_fp", "[", "l", "]", "=", "0", "\n", "", "if", "l", "not", "in", "global_tp", ".", "keys", "(", ")", ":", "\n", "                            ", "global_tp", "[", "l", "]", "=", "0", "\n", "", "conf", "=", "ConfusionMatrix", "(", "(", "predicted_segmentation", "==", "l", ")", ".", "astype", "(", "int", ")", ",", "(", "gt_segmentation", "==", "l", ")", ".", "astype", "(", "int", ")", ")", "\n", "conf", ".", "compute", "(", ")", "\n", "global_fn", "[", "l", "]", "+=", "conf", ".", "fn", "\n", "global_fp", "[", "l", "]", "+=", "conf", ".", "fp", "\n", "global_tp", "[", "l", "]", "+=", "conf", ".", "tp", "\n", "\n", "", "", "if", "transpose_forward", "is", "not", "None", ":", "\n", "                    ", "transpose_backward", "=", "self", ".", "plans", ".", "get", "(", "'transpose_backward'", ")", "\n", "softmax_pred", "=", "softmax_pred", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "transpose_backward", "]", ")", "\n", "\n", "", "if", "save_softmax", ":", "\n", "                    ", "softmax_fname", "=", "join", "(", "output_folder", ",", "fname", "+", "\".npz\"", ")", "\n", "", "else", ":", "\n", "                    ", "softmax_fname", "=", "None", "\n", "\n", "", "\"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n                larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n                communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n                enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n                patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n                then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n                filename or np.ndarray and will handle this automatically\"\"\"", "\n", "if", "np", ".", "prod", "(", "softmax_pred", ".", "shape", ")", ">", "(", "2e9", "/", "4", "*", "0.9", ")", ":", "# *0.9 just to be save", "\n", "                    ", "np", ".", "save", "(", "join", "(", "output_folder", ",", "fname", "+", "\".npy\"", ")", ",", "softmax_pred", ")", "\n", "softmax_pred", "=", "join", "(", "output_folder", ",", "fname", "+", "\".npy\"", ")", "\n", "", "results", ".", "append", "(", "export_pool", ".", "starmap_async", "(", "save_segmentation_nifti_from_softmax", ",", "\n", "(", "(", "softmax_pred", ",", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "properties", ",", "3", ",", "None", ",", "None", ",", "None", ",", "softmax_fname", ",", "None", ")", ",", "\n", ")", "\n", ")", "\n", ")", "\n", "#save_segmentation_nifti_from_softmax(softmax_pred, join(output_folder, fname + \".nii.gz\"),", "\n", "#                                               properties, 3, None, None,", "\n", "#                                               None,", "\n", "#                                               softmax_fname,", "\n", "#                                               None)", "\n", "\n", "", "pred_gt_tuples", ".", "append", "(", "[", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "join", "(", "self", ".", "gt_niftis_folder", ",", "fname", "+", "\".nii.gz\"", ")", "]", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "print", "(", "\"finished prediction, now evaluating...\"", ")", "\n", "\n", "task", "=", "self", ".", "dataset_directory", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "job_name", "=", "self", ".", "experiment_name", "\n", "_", "=", "aggregate_scores", "(", "pred_gt_tuples", ",", "labels", "=", "list", "(", "range", "(", "self", ".", "num_classes", ")", ")", ",", "\n", "json_output_file", "=", "join", "(", "output_folder", ",", "\"summary.json\"", ")", ",", "\n", "json_name", "=", "job_name", "+", "\" val tiled %s\"", "%", "(", "str", "(", "tiled", ")", ")", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "\n", "json_task", "=", "task", ",", "num_threads", "=", "3", ")", "\n", "if", "compute_global_dice", ":", "\n", "            ", "global_dice", "=", "OrderedDict", "(", ")", "\n", "all_labels", "=", "list", "(", "global_fn", ".", "keys", "(", ")", ")", "\n", "for", "l", "in", "all_labels", ":", "\n", "                ", "global_dice", "[", "int", "(", "l", ")", "]", "=", "float", "(", "2", "*", "global_tp", "[", "l", "]", "/", "(", "2", "*", "global_tp", "[", "l", "]", "+", "global_fn", "[", "l", "]", "+", "global_fp", "[", "l", "]", ")", ")", "\n", "", "write_json", "(", "global_dice", ",", "join", "(", "output_folder", ",", "\"global_dice.json\"", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.run_online_evaluation": [[559, 585], ["torch.no_grad", "nnunet.utilities.nd_softmax.softmax_helper", "nnunet.utilities.nd_softmax.softmax_helper.argmax", "tuple", "torch.zeros().to", "torch.zeros().to", "torch.zeros().to", "range", "tp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu().numpy", "fp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu().numpy", "fn_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu().numpy", "nnUNetTrainer.nnUNetTrainer.online_eval_foreground_dc.append", "nnUNetTrainer.nnUNetTrainer.online_eval_tp.append", "nnUNetTrainer.nnUNetTrainer.online_eval_fp.append", "nnUNetTrainer.nnUNetTrainer.online_eval_fn.append", "range", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "list", "list", "list", "list", "len", "torch.zeros", "torch.zeros", "torch.zeros", "tp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu", "fp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu", "fn_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach().cpu", "tp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach", "fp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach", "fn_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum().detach", "tp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum", "fp_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum", "fn_hard.sum().detach().cpu().numpy.sum().detach().cpu().numpy.sum"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.nd_softmax.softmax_helper", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor"], ["", "", "def", "run_online_evaluation", "(", "self", ",", "output", ",", "target", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "num_classes", "=", "output", ".", "shape", "[", "1", "]", "\n", "output_softmax", "=", "softmax_helper", "(", "output", ")", "\n", "output_seg", "=", "output_softmax", ".", "argmax", "(", "1", ")", "\n", "\n", "# print('num_classes is:',num_classes)", "\n", "# print('unique output_seg is:',torch.unique(output_seg))", "\n", "target", "=", "target", "[", ":", ",", "0", "]", "\n", "axes", "=", "tuple", "(", "range", "(", "1", ",", "len", "(", "target", ".", "shape", ")", ")", ")", "\n", "tp_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "fp_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "fn_hard", "=", "torch", ".", "zeros", "(", "(", "target", ".", "shape", "[", "0", "]", ",", "num_classes", "-", "1", ")", ")", ".", "to", "(", "output_seg", ".", "device", ".", "index", ")", "\n", "for", "c", "in", "range", "(", "1", ",", "num_classes", ")", ":", "\n", "                ", "tp_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "==", "c", ")", ".", "float", "(", ")", "*", "(", "target", "==", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "fp_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "==", "c", ")", ".", "float", "(", ")", "*", "(", "target", "!=", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "fn_hard", "[", ":", ",", "c", "-", "1", "]", "=", "sum_tensor", "(", "(", "output_seg", "!=", "c", ")", ".", "float", "(", ")", "*", "(", "target", "==", "c", ")", ".", "float", "(", ")", ",", "axes", "=", "axes", ")", "\n", "\n", "", "tp_hard", "=", "tp_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "fp_hard", "=", "fp_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "fn_hard", "=", "fn_hard", ".", "sum", "(", "0", ",", "keepdim", "=", "False", ")", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "self", ".", "online_eval_foreground_dc", ".", "append", "(", "list", "(", "(", "2", "*", "tp_hard", ")", "/", "(", "2", "*", "tp_hard", "+", "fp_hard", "+", "fn_hard", "+", "1e-8", ")", ")", ")", "\n", "self", ".", "online_eval_tp", ".", "append", "(", "list", "(", "tp_hard", ")", ")", "\n", "self", ".", "online_eval_fp", ".", "append", "(", "list", "(", "fp_hard", ")", ")", "\n", "self", ".", "online_eval_fn", ".", "append", "(", "list", "(", "fn_hard", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.finish_online_evaluation": [[586, 604], ["numpy.sum", "numpy.sum", "numpy.sum", "nnUNetTrainer.nnUNetTrainer.all_val_eval_metrics.append", "nnUNetTrainer.nnUNetTrainer.print_to_log_file", "numpy.mean", "str", "numpy.isnan", "zip"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file"], ["", "", "def", "finish_online_evaluation", "(", "self", ")", ":", "\n", "        ", "self", ".", "online_eval_tp", "=", "np", ".", "sum", "(", "self", ".", "online_eval_tp", ",", "0", ")", "\n", "self", ".", "online_eval_fp", "=", "np", ".", "sum", "(", "self", ".", "online_eval_fp", ",", "0", ")", "\n", "self", ".", "online_eval_fn", "=", "np", ".", "sum", "(", "self", ".", "online_eval_fn", ",", "0", ")", "\n", "\n", "\n", "global_dc_per_class", "=", "[", "i", "for", "i", "in", "[", "2", "*", "i", "/", "(", "2", "*", "i", "+", "j", "+", "k", ")", "for", "i", ",", "j", ",", "k", "in", "\n", "zip", "(", "self", ".", "online_eval_tp", ",", "self", ".", "online_eval_fp", ",", "self", ".", "online_eval_fn", ")", "]", "\n", "if", "not", "np", ".", "isnan", "(", "i", ")", "]", "\n", "# print('global_dc_per_class is:',global_dc_per_class)", "\n", "self", ".", "all_val_eval_metrics", ".", "append", "(", "np", ".", "mean", "(", "global_dc_per_class", ")", ")", "\n", "\n", "self", ".", "print_to_log_file", "(", "\"Val glob dc per class:\"", ",", "str", "(", "global_dc_per_class", ")", ")", "\n", "\n", "self", ".", "online_eval_foreground_dc", "=", "[", "]", "\n", "self", ".", "online_eval_tp", "=", "[", "]", "\n", "self", ".", "online_eval_fp", "=", "[", "]", "\n", "self", ".", "online_eval_fn", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.save_checkpoint": [[605, 614], ["super().save_checkpoint", "collections.OrderedDict", "str", "write_pickle"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.save_checkpoint"], ["", "def", "save_checkpoint", "(", "self", ",", "fname", ",", "save_optimizer", "=", "True", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainer", ",", "self", ")", ".", "save_checkpoint", "(", "fname", ",", "save_optimizer", ")", "\n", "info", "=", "OrderedDict", "(", ")", "\n", "info", "[", "'init'", "]", "=", "self", ".", "init_args", "\n", "info", "[", "'name'", "]", "=", "self", ".", "__class__", ".", "__name__", "\n", "info", "[", "'class'", "]", "=", "str", "(", "self", ".", "__class__", ")", "\n", "info", "[", "'plans'", "]", "=", "self", ".", "plans", "\n", "\n", "write_pickle", "(", "info", ",", "fname", "+", "\".pkl\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.__init__": [[19, 43], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.__init__", "join", "join", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.output_folder.split", "[].split", "isdir", "RuntimeError", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.output_folder.split"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "previous_trainer", "=", "\"nnUNetTrainer\"", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerCascadeFullRes", ",", "self", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "\n", "batch_dice", ",", "stage", ",", "unpack_data", ",", "deterministic", ",", "fp16", ")", "\n", "self", ".", "init_args", "=", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "unpack_data", ",", "\n", "deterministic", ",", "previous_trainer", ",", "fp16", ")", "\n", "\n", "if", "self", ".", "output_folder", "is", "not", "None", ":", "\n", "            ", "task", "=", "self", ".", "output_folder", ".", "split", "(", "\"/\"", ")", "[", "-", "3", "]", "\n", "plans_identifier", "=", "self", ".", "output_folder", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", ".", "split", "(", "\"__\"", ")", "[", "-", "1", "]", "\n", "\n", "folder_with_segs_prev_stage", "=", "join", "(", "network_training_output_dir", ",", "\"3d_lowres\"", ",", "\n", "task", ",", "previous_trainer", "+", "\"__\"", "+", "plans_identifier", ",", "\"pred_next_stage\"", ")", "\n", "if", "not", "isdir", "(", "folder_with_segs_prev_stage", ")", ":", "\n", "                ", "raise", "RuntimeError", "(", "\n", "\"Cannot run final stage of cascade. Run corresponding 3d_lowres first and predict the \"", "\n", "\"segmentations for the next stage\"", ")", "\n", "", "self", ".", "folder_with_segs_from_prev_stage", "=", "folder_with_segs_prev_stage", "\n", "# Do not put segs_prev_stage into self.output_folder as we need to unpack them for performance and we", "\n", "# don't want to do that in self.output_folder because that one is located on some network drive.", "\n", "self", ".", "folder_with_segs_from_prev_stage_for_train", "=", "join", "(", "self", ".", "dataset_directory", ",", "\"segs_prev_stage\"", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "folder_with_segs_from_prev_stage", "=", "None", "\n", "self", ".", "folder_with_segs_from_prev_stage_for_train", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split": [[44, 57], ["super().do_split", "join", "isfile", "join", "join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split"], ["", "", "def", "do_split", "(", "self", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerCascadeFullRes", ",", "self", ")", ".", "do_split", "(", ")", "\n", "for", "k", "in", "self", ".", "dataset", ":", "\n", "            ", "self", ".", "dataset", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "assert", "isfile", "(", "self", ".", "dataset", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", ")", ",", "\"seg from prev stage missing: %s\"", "%", "(", "self", ".", "dataset", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", ")", "\n", "", "for", "k", "in", "self", ".", "dataset_val", ":", "\n", "            ", "self", ".", "dataset_val", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "", "for", "k", "in", "self", ".", "dataset_tr", ":", "\n", "            ", "self", ".", "dataset_tr", "[", "k", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.get_basic_generators": [[58, 69], ["nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.load_dataset", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split", "nnunet.training.dataloading.dataset_loading.DataLoader3D", "nnunet.training.dataloading.dataset_loading.DataLoader3D"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split"], ["", "", "def", "get_basic_generators", "(", "self", ")", ":", "\n", "        ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "if", "self", ".", "threeD", ":", "\n", "            ", "dl_tr", "=", "DataLoader3D", "(", "self", ".", "dataset_tr", ",", "self", ".", "basic_generator_patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "\n", "True", ",", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ")", "\n", "dl_val", "=", "DataLoader3D", "(", "self", ".", "dataset_val", ",", "self", ".", "patch_size", ",", "self", ".", "patch_size", ",", "self", ".", "batch_size", ",", "True", ",", "\n", "oversample_foreground_percent", "=", "self", ".", "oversample_foreground_percent", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "\n", "", "return", "dl_tr", ",", "dl_val", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.process_plans": [[70, 73], ["super().process_plans"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.process_plans"], ["", "def", "process_plans", "(", "self", ",", "plans", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerCascadeFullRes", ",", "self", ")", ".", "process_plans", "(", "plans", ")", "\n", "self", ".", "num_input_channels", "+=", "(", "self", ".", "num_classes", "-", "1", ")", "# for seg from prev stage", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params": [[74, 80], ["super().setup_DA_params", "list", "range"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params"], ["", "def", "setup_DA_params", "(", "self", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerCascadeFullRes", ",", "self", ")", ".", "setup_DA_params", "(", ")", "\n", "self", ".", "data_aug_params", "[", "'selected_seg_channels'", "]", "=", "[", "0", ",", "1", "]", "\n", "self", ".", "data_aug_params", "[", "'all_segmentation_labels'", "]", "=", "list", "(", "range", "(", "1", ",", "self", ".", "num_classes", ")", ")", "\n", "self", ".", "data_aug_params", "[", "'move_last_seg_chanel_to_data'", "]", "=", "True", "\n", "self", ".", "data_aug_params", "[", "'advanced_pyramid_augmentations'", "]", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.initialize": [[81, 140], ["nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.process_plans", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params", "join", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.initialize_network_optimizer_and_scheduler", "isinstance", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.load_plans_file", "isdir", "maybe_mkdir_p", "subfiles", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params", "shutil.rmtree", "shutil.copy", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.get_basic_generators", "nnunet.training.data_augmentation.default_data_augmentation.get_default_augmentation", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.print_to_log_file", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.print_to_log_file", "print", "nnunet.training.dataloading.dataset_loading.unpack_dataset", "print", "print", "str", "str", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.dataset_tr.keys", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.dataset_val.keys"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.process_plans", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.initialize_network_optimizer_and_scheduler", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.load_plans_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.setup_DA_params", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.unpack_dataset", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.get_basic_generators", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.data_augmentation.default_data_augmentation.get_default_augmentation", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.print_to_log_file", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.unpack_dataset"], ["", "def", "initialize", "(", "self", ",", "training", "=", "True", ",", "force_load_plans", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        For prediction of test cases just set training=False, this will prevent loading of training data and\n        training batchgenerator initialization\n        :param training:\n        :return:\n        \"\"\"", "\n", "if", "force_load_plans", "or", "(", "self", ".", "plans", "is", "None", ")", ":", "\n", "            ", "self", ".", "load_plans_file", "(", ")", "\n", "\n", "", "self", ".", "process_plans", "(", "self", ".", "plans", ")", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "self", ".", "folder_with_preprocessed_data", "=", "join", "(", "self", ".", "dataset_directory", "+", "'/'", ",", "self", ".", "plans", "[", "'data_identifier'", "]", "+", "\n", "\"_stage%d\"", "%", "self", ".", "stage", "+", "'/'", ")", "\n", "if", "training", ":", "\n", "# copy segs from prev stage to separate folder and extract them", "\n", "\n", "# If we don't do this then we need to make sure to manually delete the folder if we want to update", "\n", "# segs_from_prev_stage. I will probably forget to do so, so I leave this in as a safeguard", "\n", "            ", "if", "isdir", "(", "self", ".", "folder_with_segs_from_prev_stage_for_train", ")", ":", "\n", "                ", "shutil", ".", "rmtree", "(", "self", ".", "folder_with_segs_from_prev_stage_for_train", ")", "\n", "\n", "", "maybe_mkdir_p", "(", "self", ".", "folder_with_segs_from_prev_stage_for_train", ")", "\n", "segs_from_prev_stage_files", "=", "subfiles", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "suffix", "=", "'.npz'", ")", "\n", "for", "s", "in", "segs_from_prev_stage_files", ":", "\n", "                ", "shutil", ".", "copy", "(", "s", ",", "self", ".", "folder_with_segs_from_prev_stage_for_train", ")", "\n", "\n", "# if we don't do this then performance is shit", "\n", "", "if", "self", ".", "unpack_data", ":", "\n", "                ", "unpack_dataset", "(", "self", ".", "folder_with_segs_from_prev_stage_for_train", ")", "\n", "\n", "", "self", ".", "folder_with_segs_from_prev_stage", "=", "self", ".", "folder_with_segs_from_prev_stage_for_train", "\n", "\n", "self", ".", "setup_DA_params", "(", ")", "\n", "\n", "if", "self", ".", "folder_with_preprocessed_data", "is", "not", "None", ":", "\n", "                ", "self", ".", "dl_tr", ",", "self", ".", "dl_val", "=", "self", ".", "get_basic_generators", "(", ")", "\n", "\n", "if", "self", ".", "unpack_data", ":", "\n", "                    ", "print", "(", "\"unpacking dataset\"", ")", "\n", "unpack_dataset", "(", "self", ".", "folder_with_preprocessed_data", ")", "\n", "print", "(", "\"done\"", ")", "\n", "", "else", ":", "\n", "                    ", "print", "(", "\n", "\"INFO: Not unpacking data! Training may be slow due to that. Pray you are not using 2d or you \"", "\n", "\"will wait all winter for your model to finish!\"", ")", "\n", "\n", "", "self", ".", "tr_gen", ",", "self", ".", "val_gen", "=", "get_default_augmentation", "(", "self", ".", "dl_tr", ",", "self", ".", "dl_val", ",", "\n", "self", ".", "data_aug_params", "[", "'patch_size_for_spatialtransform'", "]", ",", "\n", "self", ".", "data_aug_params", ")", "\n", "self", ".", "print_to_log_file", "(", "\"TRAINING KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_tr", ".", "keys", "(", ")", ")", ")", ")", "\n", "self", ".", "print_to_log_file", "(", "\"VALIDATION KEYS:\\n %s\"", "%", "(", "str", "(", "self", ".", "dataset_val", ".", "keys", "(", ")", ")", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "pass", "\n", "", "self", ".", "initialize_network_optimizer_and_scheduler", "(", ")", "\n", "assert", "isinstance", "(", "self", ".", "network", ",", "SegmentationNetwork", ")", "\n", "self", ".", "was_initialized", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.validate": [[141, 235], ["join", "maybe_mkdir_p", "multiprocessing.pool.Pool", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.dataset_val.keys", "nnunet.evaluation.evaluator.aggregate_scores", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.load_dataset", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.plans.get", "print", "numpy.concatenate", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.predict_preprocessed_data_return_softmax", "results.append", "pred_gt_tuples.append", "i.get", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.dataset_directory.split", "numpy.load", "data.transpose.transpose.transpose", "seg_from_prev_stage.transpose.transpose.transpose", "nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.plans.get", "softmax_pred.transpose.transpose.transpose", "join", "numpy.prod", "numpy.save", "multiprocessing.pool.Pool.starmap_async", "list", "join", "numpy.load", "nnunet.utilities.one_hot_encoding.to_one_hot", "[].split", "join", "join", "range", "join", "range", "join"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.aggregate_scores", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.load_dataset", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainerCascadeFullRes.nnUNetTrainerCascadeFullRes.do_split", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_softmax", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.one_hot_encoding.to_one_hot"], ["", "def", "validate", "(", "self", ",", "do_mirroring", "=", "True", ",", "use_train_mode", "=", "False", ",", "tiled", "=", "True", ",", "step", "=", "2", ",", "save_softmax", "=", "True", ",", "\n", "use_gaussian", "=", "True", ",", "validation_folder_name", "=", "'validation'", ")", ":", "\n", "        ", "\"\"\"\n\n        :param do_mirroring:\n        :param use_train_mode:\n        :param mirror_axes:\n        :param tiled:\n        :param tile_in_z:\n        :param step:\n        :param use_nifti:\n        :param save_softmax:\n        :param use_gaussian:\n        :param use_temporal_models:\n        :return:\n        \"\"\"", "\n", "assert", "self", ".", "was_initialized", ",", "\"must initialize, ideally with checkpoint (or train first)\"", "\n", "if", "self", ".", "dataset_val", "is", "None", ":", "\n", "            ", "self", ".", "load_dataset", "(", ")", "\n", "self", ".", "do_split", "(", ")", "\n", "\n", "", "output_folder", "=", "join", "(", "self", ".", "output_folder", ",", "validation_folder_name", ")", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "\n", "if", "do_mirroring", ":", "\n", "            ", "mirror_axes", "=", "self", ".", "data_aug_params", "[", "'mirror_axes'", "]", "\n", "", "else", ":", "\n", "            ", "mirror_axes", "=", "(", ")", "\n", "\n", "", "pred_gt_tuples", "=", "[", "]", "\n", "\n", "process_manager", "=", "Pool", "(", "2", ")", "\n", "results", "=", "[", "]", "\n", "\n", "for", "k", "in", "self", ".", "dataset_val", ".", "keys", "(", ")", ":", "\n", "            ", "properties", "=", "self", ".", "dataset", "[", "k", "]", "[", "'properties'", "]", "\n", "data", "=", "np", ".", "load", "(", "self", ".", "dataset", "[", "k", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "\n", "# concat segmentation of previous step", "\n", "seg_from_prev_stage", "=", "np", ".", "load", "(", "join", "(", "self", ".", "folder_with_segs_from_prev_stage", ",", "\n", "k", "+", "\"_segFromPrevStage.npz\"", ")", ")", "[", "'data'", "]", "[", "None", "]", "\n", "\n", "transpose_forward", "=", "self", ".", "plans", ".", "get", "(", "'transpose_forward'", ")", "\n", "if", "transpose_forward", "is", "not", "None", ":", "\n", "                ", "data", "=", "data", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "transpose_forward", "]", ")", "\n", "seg_from_prev_stage", "=", "seg_from_prev_stage", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "transpose_forward", "]", ")", "\n", "\n", "", "print", "(", "data", ".", "shape", ")", "\n", "data", "[", "-", "1", "]", "[", "data", "[", "-", "1", "]", "==", "-", "1", "]", "=", "0", "\n", "data_for_net", "=", "np", ".", "concatenate", "(", "(", "data", "[", ":", "-", "1", "]", ",", "to_one_hot", "(", "seg_from_prev_stage", "[", "0", "]", ",", "range", "(", "1", ",", "self", ".", "num_classes", ")", ")", ")", ")", "\n", "softmax_pred", "=", "self", ".", "predict_preprocessed_data_return_softmax", "(", "data_for_net", ",", "do_mirroring", ",", "1", ",", "\n", "use_train_mode", ",", "1", ",", "mirror_axes", ",", "tiled", ",", "\n", "True", ",", "step", ",", "self", ".", "patch_size", ",", "\n", "use_gaussian", "=", "use_gaussian", ")", "\n", "\n", "if", "transpose_forward", "is", "not", "None", ":", "\n", "                ", "transpose_backward", "=", "self", ".", "plans", ".", "get", "(", "'transpose_backward'", ")", "\n", "softmax_pred", "=", "softmax_pred", ".", "transpose", "(", "[", "0", "]", "+", "[", "i", "+", "1", "for", "i", "in", "transpose_backward", "]", ")", "\n", "\n", "", "fname", "=", "properties", "[", "'list_of_data_files'", "]", "[", "0", "]", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "[", ":", "-", "12", "]", "\n", "\n", "if", "save_softmax", ":", "\n", "                ", "softmax_fname", "=", "join", "(", "output_folder", ",", "fname", "+", "\".npz\"", ")", "\n", "", "else", ":", "\n", "                ", "softmax_fname", "=", "None", "\n", "\n", "", "\"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n            larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n            communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n            enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n            patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n            then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n            filename or np.ndarray and will handle this automatically\"\"\"", "\n", "if", "np", ".", "prod", "(", "softmax_pred", ".", "shape", ")", ">", "(", "2e9", "/", "4", "*", "0.9", ")", ":", "# *0.9 just to be save", "\n", "                ", "np", ".", "save", "(", "fname", "+", "\".npy\"", ",", "softmax_pred", ")", "\n", "softmax_pred", "=", "fname", "+", "\".npy\"", "\n", "", "results", ".", "append", "(", "process_manager", ".", "starmap_async", "(", "save_segmentation_nifti_from_softmax", ",", "\n", "(", "(", "softmax_pred", ",", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "properties", ",", "1", ",", "None", ",", "None", ",", "None", ",", "softmax_fname", ",", "None", ")", ",", "\n", ")", "\n", ")", "\n", ")", "\n", "\n", "pred_gt_tuples", ".", "append", "(", "[", "join", "(", "output_folder", ",", "fname", "+", "\".nii.gz\"", ")", ",", "\n", "join", "(", "self", ".", "gt_niftis_folder", ",", "fname", "+", "\".nii.gz\"", ")", "]", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "\n", "task", "=", "self", ".", "dataset_directory", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", "\n", "job_name", "=", "self", ".", "experiment_name", "\n", "_", "=", "aggregate_scores", "(", "pred_gt_tuples", ",", "labels", "=", "list", "(", "range", "(", "self", ".", "num_classes", ")", ")", ",", "\n", "json_output_file", "=", "join", "(", "output_folder", ",", "\"summary.json\"", ")", ",", "json_name", "=", "job_name", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "json_description", "=", "\"\"", ",", "\n", "json_task", "=", "task", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.nnUNet_variants.nnUNetTrainerCE.nnUNetTrainerCE.__init__": [[6, 11], ["nnunet.training.network_training.nnUNetTrainer.nnUNetTrainer.__init__", "nnunet.training.loss_functions.dice_loss.CrossentropyND"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "plans_file", ",", "fold", ",", "output_folder", "=", "None", ",", "dataset_directory", "=", "None", ",", "batch_dice", "=", "True", ",", "stage", "=", "None", ",", "\n", "unpack_data", "=", "True", ",", "deterministic", "=", "True", ",", "fp16", "=", "False", ")", ":", "\n", "        ", "super", "(", "nnUNetTrainerCE", ",", "self", ")", ".", "__init__", "(", "plans_file", ",", "fold", ",", "output_folder", ",", "dataset_directory", ",", "batch_dice", ",", "stage", ",", "\n", "unpack_data", ",", "deterministic", ",", "fp16", ")", "\n", "self", ".", "loss", "=", "CrossentropyND", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.ND_Crossentropy.CrossentropyND.forward": [[22, 40], ["target.view.view.long", "inp.transpose.transpose.contiguous", "inp.transpose.transpose.view", "target.view.view.view", "super().forward", "inp.transpose.transpose.size", "len", "inp.transpose.transpose.transpose"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.dice_loss.DC_and_CE_loss.forward"], ["def", "forward", "(", "self", ",", "inp", ",", "target", ")", ":", "\n", "        ", "target", "=", "target", ".", "long", "(", ")", "\n", "num_classes", "=", "inp", ".", "size", "(", ")", "[", "1", "]", "\n", "\n", "i0", "=", "1", "\n", "i1", "=", "2", "\n", "\n", "while", "i1", "<", "len", "(", "inp", ".", "shape", ")", ":", "# this is ugly but torch only allows to transpose two axes at once", "\n", "            ", "inp", "=", "inp", ".", "transpose", "(", "i0", ",", "i1", ")", "\n", "i0", "+=", "1", "\n", "i1", "+=", "1", "\n", "\n", "", "inp", "=", "inp", ".", "contiguous", "(", ")", "\n", "inp", "=", "inp", ".", "view", "(", "-", "1", ",", "num_classes", ")", "\n", "\n", "target", "=", "target", ".", "view", "(", "-", "1", ",", ")", "\n", "\n", "return", "super", "(", "CrossentropyND", ",", "self", ")", ".", "forward", "(", "inp", ",", "target", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.dice_loss.SoftDiceLoss.__init__": [[23, 51], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "smooth", "=", "1.", ",", "apply_nonlin", "=", "None", ",", "batch_dice", "=", "False", ",", "do_bg", "=", "True", ",", "smooth_in_nom", "=", "True", ",", "\n", "background_weight", "=", "1", ",", "rebalance_weights", "=", "None", ",", "square_nominator", "=", "False", ",", "square_denom", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        hahaa no documentation for you today\n        :param smooth:\n        :param apply_nonlin:\n        :param batch_dice:\n        :param do_bg:\n        :param smooth_in_nom:\n        :param background_weight:\n        :param rebalance_weights:\n        \"\"\"", "\n", "super", "(", "SoftDiceLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "square_denom", "=", "square_denom", "\n", "self", ".", "square_nominator", "=", "square_nominator", "\n", "if", "not", "do_bg", ":", "\n", "            ", "assert", "background_weight", "==", "1", ",", "\"if there is no bg, then set background weight to 1 you dummy\"", "\n", "", "self", ".", "rebalance_weights", "=", "rebalance_weights", "\n", "self", ".", "background_weight", "=", "background_weight", "\n", "if", "smooth_in_nom", ":", "\n", "            ", "self", ".", "smooth_in_nom", "=", "smooth", "\n", "", "else", ":", "\n", "            ", "self", ".", "smooth_in_nom", "=", "0", "\n", "", "self", ".", "do_bg", "=", "do_bg", "\n", "self", ".", "batch_dice", "=", "batch_dice", "\n", "self", ".", "apply_nonlin", "=", "apply_nonlin", "\n", "self", ".", "smooth", "=", "smooth", "\n", "self", ".", "y_onehot", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.dice_loss.SoftDiceLoss.forward": [[52, 108], ["torch.zeros", "y_onehot.cuda.cuda.scatter_", "torch.no_grad", "y.view.view.long", "dice_loss.SoftDiceLoss.apply_nonlin", "len", "len", "y.view.view.view", "y_onehot.cuda.cuda.cuda", "dice_loss.soft_dice", "dice_loss.soft_dice_per_batch_2", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.dice_loss.soft_dice", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.dice_loss.soft_dice_per_batch_2"], ["", "def", "forward", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "y", "=", "y", ".", "long", "(", ")", "\n", "", "shp_x", "=", "x", ".", "shape", "\n", "# print('x shape is:',shp_x)", "\n", "shp_y", "=", "y", ".", "shape", "\n", "# print('y shape is:',shp_y)", "\n", "#y shape is: torch.Size([8, 1, 192, 192, 48])", "\n", "# nonlin maybe mean NONLINEARITY!", "\n", "if", "self", ".", "apply_nonlin", "is", "not", "None", ":", "\n", "            ", "x", "=", "self", ".", "apply_nonlin", "(", "x", ")", "\n", "", "if", "len", "(", "shp_x", ")", "!=", "len", "(", "shp_y", ")", ":", "\n", "            ", "y", "=", "y", ".", "view", "(", "(", "shp_y", "[", "0", "]", ",", "1", ",", "*", "shp_y", "[", "1", ":", "]", ")", ")", "\n", "# print('After apply nonlin, x shape is:',x.shape)", "\n", "#After apply nonlin, x shape is: torch.Size([8, 3, 192, 192, 48])", "\n", "# output shape is: [8,3,192,192,48] when batch size is 8 and labels are [0,1,2]", "\n", "# now x and y should have shape (B, C, X, Y(, Z))) and (B, 1, X, Y(, Z))), respectively", "\n", "", "y_onehot", "=", "torch", ".", "zeros", "(", "shp_x", ")", "\n", "if", "x", ".", "device", ".", "type", "==", "\"cuda\"", ":", "\n", "            ", "y_onehot", "=", "y_onehot", ".", "cuda", "(", "x", ".", "device", ".", "index", ")", "\n", "", "y_onehot", ".", "scatter_", "(", "1", ",", "y", ",", "1", ")", "\n", "if", "not", "self", ".", "do_bg", ":", "\n", "            ", "x", "=", "x", "[", ":", ",", "1", ":", "]", "# This means to reduce the first 0 dimension of the shape of output x, to remove background prediction", "\n", "# x is the probability output, so its range is between [0,1]", "\n", "y_onehot", "=", "y_onehot", "[", ":", ",", "1", ":", "]", "\n", "# print('y_onehot shape is:',y_onehot.shape)", "\n", "# y_onehot shape is: torch.Size([8, 2, 192, 192, 48])", "\n", "# print('The last version of x shape is:',x.shape)", "\n", "#The last version of x shape is: torch.Size([8, 2, 192, 192, 48])", "\n", "# print('x max is:', torch.max(x))", "\n", "# x max is: tensor(1.0000, device='cuda:4', grad_fn=<MaxBackward1>)", "\n", "# print('x min is:', torch.min(x))", "\n", "# x min is: tensor(3.1973e-07, device='cuda:4', grad_fn=<MinBackward1>)", "\n", "# print('y_onehot max is:', torch.max(y_onehot))", "\n", "#y_onehot max is: tensor(1., device='cuda:4')", "\n", "#x max is: tensor(1.0000, device='cuda:4', grad_fn=<MaxBackward1>)", "\n", "# print('y_onehot min is:', torch.min(y_onehot))", "\n", "#y_onehot min is: tensor(0., device='cuda:4')", "\n", "\n", "\n", "", "if", "not", "self", ".", "batch_dice", ":", "\n", "            ", "if", "self", ".", "background_weight", "!=", "1", "or", "(", "self", ".", "rebalance_weights", "is", "not", "None", ")", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\"nah son\"", ")", "\n", "\n", "", "l", "=", "soft_dice", "(", "x", ",", "y_onehot", ",", "self", ".", "smooth", ",", "self", ".", "smooth_in_nom", ",", "self", ".", "square_nominator", ",", "self", ".", "square_denom", ")", "\n", "# print('Using soft_dice!')", "\n", "", "else", ":", "\n", "\n", "            ", "l", "=", "soft_dice_per_batch_2", "(", "x", ",", "y_onehot", ",", "self", ".", "smooth", ",", "self", ".", "smooth_in_nom", ",", "\n", "background_weight", "=", "self", ".", "background_weight", ",", "\n", "rebalance_weights", "=", "self", ".", "rebalance_weights", ")", "\n", "# print('Using soft_dice_per_batch_2!')", "\n", "# Here we use the soft_dice_per_batch_2", "\n", "# print('dc shape is:',l.size())", "\n", "# dc_loss is: tensor(-0.0282, device='cuda:4', grad_fn=<MeanBackward0>)", "\n", "", "return", "l", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.dice_loss.DC_and_CE_loss.__init__": [[193, 198], ["torch.nn.Module.__init__", "nnunet.training.loss_functions.ND_Crossentropy.CrossentropyND", "dice_loss.SoftDiceLoss"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "soft_dice_kwargs", ",", "ce_kwargs", ",", "aggregate", "=", "\"sum\"", ")", ":", "\n", "        ", "super", "(", "DC_and_CE_loss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "aggregate", "=", "aggregate", "\n", "self", ".", "ce", "=", "CrossentropyND", "(", "**", "ce_kwargs", ")", "\n", "self", ".", "dc", "=", "SoftDiceLoss", "(", "apply_nonlin", "=", "softmax_helper", ",", "**", "soft_dice_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.dice_loss.DC_and_CE_loss.forward": [[199, 244], ["torch.tensor().to", "nnunet.training.loss_functions.ND_Crossentropy.CrossentropyND", "list", "list", "isinstance", "torch.cuda.current_device", "range", "dice_loss.DC_and_CE_loss.dc", "nnunet.training.loss_functions.ND_Crossentropy.CrossentropyND.", "torch.tensor", "len", "list.append", "list.append", "NotImplementedError", "NotImplementedError", "dice_loss.DC_and_CE_loss.dc", "nnunet.training.loss_functions.ND_Crossentropy.CrossentropyND."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "net_output", ",", "target", ")", ":", "\n", "# print('target shape is:',target.shape)", "\n", "#target shape is: torch.Size([8, 1, 192, 192, 48])", "\n", "        ", "ce_weights", "=", "torch", ".", "tensor", "(", "[", "0.28", ",", "0.28", ",", "0.44", "]", ")", ".", "to", "(", "torch", ".", "cuda", ".", "current_device", "(", ")", ")", "\n", "ce_1", "=", "CrossentropyND", "(", "weight", "=", "ce_weights", ")", "\n", "# dc_loss = self.dc(net_output, target)", "\n", "# # ce_loss = self.ce(net_output, target)", "\n", "# ce1_loss = ce_1(net_output, target)", "\n", "# target_layers=list()", "\n", "dc_loss_layers", "=", "list", "(", ")", "\n", "ce_loss_layers", "=", "list", "(", ")", "\n", "\n", "if", "isinstance", "(", "target", ",", "list", ")", ":", "\n", "# print('The target is list!')", "\n", "\n", "            ", "for", "i", "in", "range", "(", "len", "(", "target", ")", ")", ":", "\n", "# print('net_output[%d] is cuda?'%(2*i),net_output[2*i].is_cuda)", "\n", "# print('target[%d] is cuda?' % (i), target[i].is_cuda)", "\n", "# print('target %d shape is:'%i,target[i].shape)", "\n", "# print('net_output %d shape is:'%(2*i),net_output[2*i].shape)", "\n", "# print('net_output[%d] shape is:'%i,net_output[i].shape)", "\n", "# print('target[%d] shape is:' % i, target[i].shape)", "\n", "                ", "dc_loss_layers", ".", "append", "(", "self", ".", "dc", "(", "net_output", "[", "i", "]", ",", "target", "[", "i", "]", ")", ")", "\n", "ce_loss_layers", ".", "append", "(", "ce_1", "(", "net_output", "[", "i", "]", ",", "target", "[", "i", "]", ")", ")", "\n", "\n", "", "dc_loss", "=", "dc_loss_layers", "[", "0", "]", "*", "0.6", "+", "dc_loss_layers", "[", "1", "]", "*", "0.1", "+", "dc_loss_layers", "[", "2", "]", "*", "0.1", "+", "dc_loss_layers", "[", "3", "]", "*", "0.1", "+", "dc_loss_layers", "[", "4", "]", "*", "0.1", "\n", "ce_loss", "=", "ce_loss_layers", "[", "0", "]", "*", "0.6", "+", "ce_loss_layers", "[", "1", "]", "*", "0.1", "+", "ce_loss_layers", "[", "2", "]", "*", "0.1", "+", "ce_loss_layers", "[", "3", "]", "*", "0.1", "+", "ce_loss_layers", "[", "4", "]", "*", "0.1", "\n", "# print('Final dc_loss is:',dc_loss)", "\n", "# print('Final ce_loss is:',ce_loss)", "\n", "if", "self", ".", "aggregate", "==", "\"sum\"", ":", "\n", "                ", "result", "=", "ce_loss", "+", "dc_loss", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\"nah son\"", ")", "# reserved for other stuff (later)", "\n", "", "return", "result", "\n", "", "else", ":", "\n", "# print('Target is not list!')", "\n", "            ", "dc_loss", "=", "self", ".", "dc", "(", "net_output", ",", "target", ")", "\n", "ce_loss", "=", "ce_1", "(", "net_output", ",", "target", ")", "\n", "if", "self", ".", "aggregate", "==", "\"sum\"", ":", "\n", "                ", "result", "=", "ce_loss", "+", "dc_loss", "\n", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "\"nah son\"", ")", "# reserved for other stuff (later)", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.dice_loss.soft_dice_per_batch_2": [[110, 174], ["tuple", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "torch.ones", "weights.cuda.cuda", "torch.from_numpy().float", "len", "list", "rebalance_weights.cuda.cuda", "torch.pow", "torch.pow", "range", "torch.from_numpy", "len", "torch.log", "torch.log", "net_output.size"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor"], ["", "", "def", "soft_dice_per_batch_2", "(", "net_output", ",", "gt", ",", "smooth", "=", "1.", ",", "smooth_in_nom", "=", "1.", ",", "background_weight", "=", "1", ",", "rebalance_weights", "=", "None", ",", "\n", "square_nominator", "=", "False", ",", "square_denom", "=", "False", ")", ":", "\n", "    ", "if", "rebalance_weights", "is", "not", "None", "and", "len", "(", "rebalance_weights", ")", "!=", "gt", ".", "shape", "[", "1", "]", ":", "\n", "        ", "rebalance_weights", "=", "rebalance_weights", "[", "1", ":", "]", "# this is the case when use_bg=False", "\n", "# print('\\nrebalance_weights is:',rebalance_weights)", "\n", "#rebalance_weights is: None", "\n", "", "axes", "=", "tuple", "(", "[", "0", "]", "+", "list", "(", "range", "(", "2", ",", "len", "(", "net_output", ".", "size", "(", ")", ")", ")", ")", ")", "\n", "# print('\\naxes is:',axes)", "\n", "#axes is: (0, 2, 3, 4)", "\n", "# print('\\nnet_output shape is:',net_output.shape)", "\n", "# print('\\ngt shape is:',gt.shape)", "\n", "#net_output shape is: torch.Size([8, 2, 192, 192, 48])", "\n", "#gt shape is: torch.Size([8, 2, 192, 192, 48])", "\n", "tp", "=", "sum_tensor", "(", "net_output", "*", "gt", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "# print('\\ntp is:',tp)", "\n", "#tp shape is: torch.Size([2])", "\n", "#tp is: tensor([62684.4570, 82510.1562], device='cuda:4', grad_fn=<SumBackward2>)", "\n", "fn", "=", "sum_tensor", "(", "(", "1", "-", "net_output", ")", "*", "gt", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "# print('\\nfn is:',fn)", "\n", "#fn shape is: torch.Size([2])", "\n", "#fn is: tensor([195664.5312, 103144.8438], device='cuda:4', grad_fn=<SumBackward2>)", "\n", "fp", "=", "sum_tensor", "(", "net_output", "*", "(", "1", "-", "gt", ")", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "# print('\\nfp is:',fp)", "\n", "#fp shape is: torch.Size([2])", "\n", "#fp is: tensor([3610596., 6475380.], device='cuda:4', grad_fn=<SumBackward2>)", "\n", "weights", "=", "torch", ".", "ones", "(", "tp", ".", "shape", ")", "\n", "# print('\\nweights shape is:',weights.shape)", "\n", "#weights shape is: torch.Size([2])", "\n", "weights", "[", "0", "]", "=", "background_weight", "\n", "# print('\\nbackground_weight is:',background_weight)", "\n", "#background_weight is: 1", "\n", "if", "net_output", ".", "device", ".", "type", "==", "\"cuda\"", ":", "\n", "        ", "weights", "=", "weights", ".", "cuda", "(", "net_output", ".", "device", ".", "index", ")", "\n", "", "if", "rebalance_weights", "is", "not", "None", ":", "\n", "        ", "rebalance_weights", "=", "torch", ".", "from_numpy", "(", "rebalance_weights", ")", ".", "float", "(", ")", "\n", "if", "net_output", ".", "device", ".", "type", "==", "\"cuda\"", ":", "\n", "            ", "rebalance_weights", "=", "rebalance_weights", ".", "cuda", "(", "net_output", ".", "device", ".", "index", ")", "\n", "", "tp", "=", "tp", "*", "rebalance_weights", "\n", "fn", "=", "fn", "*", "rebalance_weights", "\n", "\n", "", "nominator", "=", "tp", "\n", "\n", "if", "square_nominator", ":", "\n", "        ", "nominator", "=", "nominator", "**", "2", "\n", "\n", "", "if", "square_denom", ":", "\n", "        ", "denom", "=", "2", "*", "tp", "**", "2", "+", "fp", "**", "2", "+", "fn", "**", "2", "\n", "", "else", ":", "\n", "        ", "denom", "=", "2", "*", "tp", "+", "fp", "+", "fn", "\n", "\n", "# result_1=(- ((2 * nominator + smooth_in_nom) / (denom + smooth)) * weights)", "\n", "# print('\\nresult_1 is:',result_1)", "\n", "#result_1 is: tensor([-0.0616, -0.0038], device='cuda:4', grad_fn=<MulBackward0>)", "\n", "", "dice_1", "=", "(", "(", "(", "2", "*", "nominator", "+", "smooth_in_nom", ")", "/", "(", "denom", "+", "smooth", ")", ")", "*", "weights", ")", "\n", "# print('\\ndice_1 is:',dice_1)", "\n", "result_1", "=", "torch", ".", "pow", "(", "(", "-", "torch", ".", "log", "(", "dice_1", "[", "0", "]", ")", ")", ",", "0.3", ")", "*", "0.4", "+", "torch", ".", "pow", "(", "(", "-", "torch", ".", "log", "(", "dice_1", "[", "1", "]", ")", ")", ",", "0.3", ")", "*", "0.6", "\n", "# print('\\nresult_1 is:',result_1)", "\n", "\n", "\n", "# result = (- ((2 * nominator + smooth_in_nom) / (denom + smooth)) * weights).mean()", "\n", "# print('\\nresult is:',result)", "\n", "# result is: tensor(-0.0327, device='cuda:4', grad_fn= < MeanBackward0 >)", "\n", "# Here we should notice that the soft dice is set as negative.", "\n", "return", "result_1", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.loss_functions.dice_loss.soft_dice": [[176, 188], ["tuple", "range", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "nnunet.utilities.tensor_utilities.sum_tensor", "len", "net_output.size"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor"], ["", "def", "soft_dice", "(", "net_output", ",", "gt", ",", "smooth", "=", "1.", ",", "smooth_in_nom", "=", "1.", ",", "square_nominator", "=", "False", ",", "square_denom", "=", "False", ")", ":", "\n", "    ", "axes", "=", "tuple", "(", "range", "(", "2", ",", "len", "(", "net_output", ".", "size", "(", ")", ")", ")", ")", "\n", "if", "square_nominator", ":", "\n", "        ", "intersect", "=", "sum_tensor", "(", "net_output", "*", "gt", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "intersect", "=", "sum_tensor", "(", "(", "net_output", "*", "gt", ")", "**", "2", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "", "if", "square_denom", ":", "\n", "        ", "denom", "=", "sum_tensor", "(", "net_output", "**", "2", "+", "gt", "**", "2", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "", "else", ":", "\n", "        ", "denom", "=", "sum_tensor", "(", "net_output", "+", "gt", ",", "axes", ",", "keepdim", "=", "False", ")", "\n", "", "result", "=", "(", "-", "(", "(", "2", "*", "intersect", "+", "smooth_in_nom", ")", "/", "(", "denom", "+", "smooth", ")", ")", ")", ".", "mean", "(", ")", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.DataLoader3D.__init__": [[148, 202], ["batchgenerators.dataloading.SlimDataLoaderBase.__init__", "list", "collections.OrderedDict", "dataset_loading.DataLoader3D._data.keys", "isinstance", "numpy.array", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "patch_size", ",", "final_patch_size", ",", "batch_size", ",", "has_prev_stage", "=", "False", ",", "\n", "oversample_foreground_percent", "=", "0.0", ",", "memmap_mode", "=", "\"r\"", ",", "pad_mode", "=", "\"edge\"", ",", "pad_kwargs_data", "=", "None", ",", "\n", "pad_sides", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n       dl_tr = DataLoader3D(self.dataset_tr, self.basic_generator_patch_size, self.patch_size, self.batch_size,\n                                 False, oversample_foreground_percent=self.oversample_foreground_percent,\n                                 pad_mode=\"constant\", pad_sides=self.pad_all_sides)\n        \"\"\"", "\n", "\n", "\n", "\"\"\"\n        This is the basic data loader for 3D networks. It uses preprocessed data as produced by my (Fabian) preprocessing.\n        You can load the data with load_dataset(folder) where folder is the folder where the npz files are located. If there\n        are only npz files present in that folder, the data loader will unpack them on the fly. This may take a while\n        and increase CPU usage. Therefore, I advise you to call unpack_dataset(folder) first, which will unpack all npz\n        to npy. Don't forget to call delete_npy(folder) after you are done with training?\n        Why all the hassle? Well the decathlon dataset is huge. Using npy for everything will consume >1 TB and that is uncool\n        given that I (Fabian) will have to store that permanently on /datasets and my local computer. With htis strategy all\n        data is stored in a compressed format (factor 10 smaller) and only unpacked when needed.\n        :param data: get this with load_dataset(folder, stage=0). Plug the return value in here and you are g2g (good to go)\n        :param patch_size: what patch size will this data loader return? it is common practice to first load larger\n        patches so that a central crop after data augmentation can be done to reduce border artifacts. If unsure, use\n        get_patch_size() from data_augmentation.default_data_augmentation\n        :param final_patch_size: what will the patch finally be cropped to (after data augmentation)? this is the patch\n        size that goes into your network. We need this here because we will pad patients in here so that patches at the\n        border of patients are sampled properly\n        :param batch_size:\n        :param num_batches: how many batches will the data loader produce before stopping? None=endless\n        :param seed:\n        :param stage: ignore this (Fabian only)\n        :param random: Sample keys randomly; CAREFUL! non-random sampling requires batch_size=1, otherwise you will iterate batch_size times over the dataset\n        :param oversample_foreground: half the batch will be forced to contain at least some foreground (equal prob for each of the foreground classes)\n        \"\"\"", "\n", "super", "(", "DataLoader3D", ",", "self", ")", ".", "__init__", "(", "data", ",", "batch_size", ",", "None", ")", "\n", "if", "pad_kwargs_data", "is", "None", ":", "\n", "            ", "pad_kwargs_data", "=", "OrderedDict", "(", ")", "\n", "", "self", ".", "pad_kwargs_data", "=", "pad_kwargs_data", "\n", "self", ".", "pad_mode", "=", "pad_mode", "\n", "self", ".", "oversample_foreground_percent", "=", "oversample_foreground_percent", "\n", "self", ".", "final_patch_size", "=", "final_patch_size", "\n", "self", ".", "has_prev_stage", "=", "has_prev_stage", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "list_of_keys", "=", "list", "(", "self", ".", "_data", ".", "keys", "(", ")", ")", "\n", "# need_to_pad denotes by how much we need to pad the data so that if we sample a patch of size final_patch_size", "\n", "# (which is what the network will get) these patches will also cover the border of the patients", "\n", "self", ".", "need_to_pad", "=", "(", "np", ".", "array", "(", "patch_size", ")", "-", "np", ".", "array", "(", "final_patch_size", ")", ")", ".", "astype", "(", "int", ")", "\n", "if", "pad_sides", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "pad_sides", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "pad_sides", "=", "np", ".", "array", "(", "pad_sides", ")", "\n", "", "self", ".", "need_to_pad", "+=", "pad_sides", "\n", "", "self", ".", "memmap_mode", "=", "memmap_mode", "\n", "self", ".", "num_channels", "=", "None", "\n", "self", ".", "pad_sides", "=", "pad_sides", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.DataLoader3D.get_do_oversample": [[203, 205], ["round"], "methods", ["None"], ["", "def", "get_do_oversample", "(", "self", ",", "batch_idx", ")", ":", "\n", "        ", "return", "not", "batch_idx", "<", "round", "(", "self", ".", "batch_size", "*", "(", "1", "-", "self", ".", "oversample_foreground_percent", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.DataLoader3D.generate_train_batch": [[206, 364], ["numpy.random.choice", "enumerate", "numpy.vstack", "numpy.vstack", "dataset_loading.DataLoader3D.get_do_oversample", "case_properties.append", "isfile", "range", "max", "min", "max", "min", "max", "min", "numpy.pad", "numpy.pad", "numpy.vstack.append", "numpy.vstack.append", "numpy.load", "isfile", "numpy.random.choice", "all", "numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "numpy.array", "numpy.argwhere", "numpy.pad", "numpy.concatenate", "numpy.load", "len", "numpy.random.choice", "len", "max", "max", "max", "numpy.random.randint", "numpy.random.randint", "numpy.random.randint", "numpy.load", "str", "str", "max", "max", "max", "max", "max", "max", "numpy.load", "zip", "numpy.random.choice", "min", "min", "min", "min", "min", "min", "max", "max", "max", "len", "min", "min", "min"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.DataLoader2D.get_do_oversample"], ["", "def", "generate_train_batch", "(", "self", ")", ":", "\n", "        ", "selected_keys", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "list_of_keys", ",", "self", ".", "batch_size", ",", "True", ",", "None", ")", "\n", "# print('selected_keys is:',selected_keys)", "\n", "data", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "case_properties", "=", "[", "]", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "selected_keys", ")", ":", "\n", "# oversampling foreground will improve stability of model training, especially if many patches are empty", "\n", "# (Lung for example)", "\n", "            ", "if", "self", ".", "get_do_oversample", "(", "j", ")", ":", "\n", "                ", "force_fg", "=", "True", "\n", "", "else", ":", "\n", "                ", "force_fg", "=", "False", "\n", "\n", "", "case_properties", ".", "append", "(", "self", ".", "_data", "[", "i", "]", "[", "'properties'", "]", ")", "\n", "# cases are stores as npz, but we require unpack_dataset to be run. This will decompress them into npy", "\n", "# which is much faster to access", "\n", "if", "isfile", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ")", ":", "\n", "                ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ",", "self", ".", "memmap_mode", ")", "\n", "", "else", ":", "\n", "                ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", ")", "[", "'data'", "]", "\n", "\n", "# If we are doing the cascade then we will also need to load the segmentation of the previous stage and", "\n", "# concatenate it. Here it will be concatenates to the segmentation because the augmentations need to be", "\n", "# applied to it in segmentation mode. Later in the data augmentation we move it from the segmentations to", "\n", "# the last channel of the data", "\n", "", "if", "self", ".", "has_prev_stage", ":", "\n", "                ", "if", "isfile", "(", "self", ".", "_data", "[", "i", "]", "[", "'seg_from_prev_stage_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ")", ":", "\n", "                    ", "segs_from_previous_stage", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'seg_from_prev_stage_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ",", "\n", "mmap_mode", "=", "self", ".", "memmap_mode", ")", "[", "None", "]", "\n", "", "else", ":", "\n", "                    ", "segs_from_previous_stage", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'seg_from_prev_stage_file'", "]", ")", "[", "'data'", "]", "[", "None", "]", "\n", "# we theoretically support several possible previsous segmentations from which only one is sampled. But", "\n", "# in practice this feature was never used so it's always only one segmentation", "\n", "", "seg_key", "=", "np", ".", "random", ".", "choice", "(", "segs_from_previous_stage", ".", "shape", "[", "0", "]", ")", "\n", "seg_from_previous_stage", "=", "segs_from_previous_stage", "[", "seg_key", ":", "seg_key", "+", "1", "]", "\n", "assert", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "seg_from_previous_stage", ".", "shape", "[", "1", ":", "]", ",", "case_all_data", ".", "shape", "[", "1", ":", "]", ")", "]", ")", ",", "\"seg_from_previous_stage does not match the shape of case_all_data: %s vs %s\"", "%", "(", "str", "(", "seg_from_previous_stage", ".", "shape", "[", "1", ":", "]", ")", ",", "str", "(", "case_all_data", ".", "shape", "[", "1", ":", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "seg_from_previous_stage", "=", "None", "\n", "\n", "# do you trust me? You better do. Otherwise you'll have to go through this mess and honestly there are", "\n", "# better things you could do right now", "\n", "\n", "# (above) documentation of the day. Nice. Even myself coming back 1 months later I have not friggin idea", "\n", "# what's going on. I keep the above documentation just for fun but attempt to make things clearer now", "\n", "\n", "", "need_to_pad", "=", "self", ".", "need_to_pad", "\n", "for", "d", "in", "range", "(", "3", ")", ":", "\n", "# if case_all_data.shape + need_to_pad is still < patch size we need to pad more! We pad on both sides", "\n", "# always", "\n", "                ", "if", "need_to_pad", "[", "d", "]", "+", "case_all_data", ".", "shape", "[", "d", "+", "1", "]", "<", "self", ".", "patch_size", "[", "d", "]", ":", "\n", "                    ", "need_to_pad", "[", "d", "]", "=", "self", ".", "patch_size", "[", "d", "]", "-", "case_all_data", ".", "shape", "[", "d", "+", "1", "]", "\n", "\n", "# we can now choose the bbox from -need_to_pad // 2 to shape - patch_size + need_to_pad // 2. Here we", "\n", "# define what the upper and lower bound can be to then sample form them with np.random.randint", "\n", "", "", "shape", "=", "case_all_data", ".", "shape", "[", "1", ":", "]", "\n", "lb_x", "=", "-", "need_to_pad", "[", "0", "]", "//", "2", "\n", "ub_x", "=", "shape", "[", "0", "]", "+", "need_to_pad", "[", "0", "]", "//", "2", "+", "need_to_pad", "[", "0", "]", "%", "2", "-", "self", ".", "patch_size", "[", "0", "]", "\n", "lb_y", "=", "-", "need_to_pad", "[", "1", "]", "//", "2", "\n", "ub_y", "=", "shape", "[", "1", "]", "+", "need_to_pad", "[", "1", "]", "//", "2", "+", "need_to_pad", "[", "1", "]", "%", "2", "-", "self", ".", "patch_size", "[", "1", "]", "\n", "lb_z", "=", "-", "need_to_pad", "[", "2", "]", "//", "2", "\n", "ub_z", "=", "shape", "[", "2", "]", "+", "need_to_pad", "[", "2", "]", "//", "2", "+", "need_to_pad", "[", "2", "]", "%", "2", "-", "self", ".", "patch_size", "[", "2", "]", "\n", "\n", "# if not force_fg then we can just sample the bbox randomly from lb and ub. Else we need to make sure we get", "\n", "# at least one of the foreground classes in the patch", "\n", "if", "not", "force_fg", ":", "\n", "                ", "bbox_x_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_x", ",", "ub_x", "+", "1", ")", "\n", "bbox_y_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_y", ",", "ub_y", "+", "1", ")", "\n", "bbox_z_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_z", ",", "ub_z", "+", "1", ")", "\n", "", "else", ":", "\n", "# this saves us a np.unique. Preprocessing already did that for all cases. Neat.", "\n", "                ", "foreground_classes", "=", "np", ".", "array", "(", "self", ".", "_data", "[", "i", "]", "[", "'properties'", "]", "[", "'classes'", "]", ")", "\n", "foreground_classes", "=", "foreground_classes", "[", "foreground_classes", ">", "0", "]", "\n", "if", "len", "(", "foreground_classes", ")", "==", "0", ":", "\n", "                    ", "selected_class", "=", "0", "\n", "", "else", ":", "\n", "                    ", "selected_class", "=", "np", ".", "random", ".", "choice", "(", "foreground_classes", ")", "\n", "", "voxels_of_that_class", "=", "np", ".", "argwhere", "(", "case_all_data", "[", "-", "1", "]", "==", "selected_class", ")", "\n", "\n", "# voxels_of_that_class should always contain some voxels we chose selected_class from the classes that", "\n", "# are actually present in the case. There is however one slight chance that foreground_classes includes", "\n", "# a class that is not anymore present in the preprocessed data. That is because np.unique is called on", "\n", "# the data in their original resolution but the data/segmentations here are present in some resampled", "\n", "# resolution. In extremely rare cases (and if the ground truth was bad = single annoteted pixels) then", "\n", "# a class can get lost in the preprocessing. This is not a problem at all because that segmentation was", "\n", "# most likely faulty anyways but it may lead to crashes here.", "\n", "if", "len", "(", "voxels_of_that_class", ")", "!=", "0", ":", "\n", "                    ", "selected_voxel", "=", "voxels_of_that_class", "[", "np", ".", "random", ".", "choice", "(", "len", "(", "voxels_of_that_class", ")", ")", "]", "\n", "# selected voxel is center voxel. Subtract half the patch size to get lower bbox voxel.", "\n", "# Make sure it is within the bounds of lb and ub", "\n", "bbox_x_lb", "=", "max", "(", "lb_x", ",", "selected_voxel", "[", "0", "]", "-", "self", ".", "patch_size", "[", "0", "]", "//", "2", ")", "\n", "bbox_y_lb", "=", "max", "(", "lb_y", ",", "selected_voxel", "[", "1", "]", "-", "self", ".", "patch_size", "[", "1", "]", "//", "2", ")", "\n", "bbox_z_lb", "=", "max", "(", "lb_z", ",", "selected_voxel", "[", "2", "]", "-", "self", ".", "patch_size", "[", "2", "]", "//", "2", ")", "\n", "", "else", ":", "\n", "# If the selected class is indeed not present then we fall back to random cropping. We can do that", "\n", "# because this case is extremely rare.", "\n", "                    ", "bbox_x_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_x", ",", "ub_x", "+", "1", ")", "\n", "bbox_y_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_y", ",", "ub_y", "+", "1", ")", "\n", "bbox_z_lb", "=", "np", ".", "random", ".", "randint", "(", "lb_z", ",", "ub_z", "+", "1", ")", "\n", "\n", "", "", "bbox_x_ub", "=", "bbox_x_lb", "+", "self", ".", "patch_size", "[", "0", "]", "\n", "bbox_y_ub", "=", "bbox_y_lb", "+", "self", ".", "patch_size", "[", "1", "]", "\n", "bbox_z_ub", "=", "bbox_z_lb", "+", "self", ".", "patch_size", "[", "2", "]", "\n", "\n", "# whoever wrote this knew what he was doing (hint: it was me). We first crop the data to the region of the", "\n", "# bbox that actually lies within the data. This will result in a smaller array which is then faster to pad.", "\n", "# valid_bbox is just the coord that lied within the data cube. It will be padded to match the patch size", "\n", "# later", "\n", "valid_bbox_x_lb", "=", "max", "(", "0", ",", "bbox_x_lb", ")", "\n", "valid_bbox_x_ub", "=", "min", "(", "shape", "[", "0", "]", ",", "bbox_x_ub", ")", "\n", "valid_bbox_y_lb", "=", "max", "(", "0", ",", "bbox_y_lb", ")", "\n", "valid_bbox_y_ub", "=", "min", "(", "shape", "[", "1", "]", ",", "bbox_y_ub", ")", "\n", "valid_bbox_z_lb", "=", "max", "(", "0", ",", "bbox_z_lb", ")", "\n", "valid_bbox_z_ub", "=", "min", "(", "shape", "[", "2", "]", ",", "bbox_z_ub", ")", "\n", "\n", "# At this point you might ask yourself why we would treat seg differently from seg_from_previous_stage.", "\n", "# Why not just concatenate them here and forget about the if statements? Well that's because segneeds to", "\n", "# be padded with -1 constant whereas seg_from_previous_stage needs to be padded with 0s (we could also", "\n", "# remove label -1 in the data augmentation but this way it is less error prone)", "\n", "\n", "case_all_data", "=", "case_all_data", "[", ":", ",", "valid_bbox_x_lb", ":", "valid_bbox_x_ub", ",", "\n", "valid_bbox_y_lb", ":", "valid_bbox_y_ub", ",", "\n", "valid_bbox_z_lb", ":", "valid_bbox_z_ub", "]", "\n", "if", "seg_from_previous_stage", "is", "not", "None", ":", "\n", "                ", "seg_from_previous_stage", "=", "seg_from_previous_stage", "[", ":", ",", "valid_bbox_x_lb", ":", "valid_bbox_x_ub", ",", "\n", "valid_bbox_y_lb", ":", "valid_bbox_y_ub", ",", "\n", "valid_bbox_z_lb", ":", "valid_bbox_z_ub", "]", "\n", "\n", "", "case_all_data_donly", "=", "np", ".", "pad", "(", "case_all_data", "[", ":", "-", "1", "]", ",", "(", "(", "0", ",", "0", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_x_lb", ")", ",", "max", "(", "bbox_x_ub", "-", "shape", "[", "0", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_y_lb", ")", ",", "max", "(", "bbox_y_ub", "-", "shape", "[", "1", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_z_lb", ")", ",", "max", "(", "bbox_z_ub", "-", "shape", "[", "2", "]", ",", "0", ")", ")", ")", ",", "\n", "self", ".", "pad_mode", ",", "**", "self", ".", "pad_kwargs_data", ")", "\n", "\n", "case_all_data_segonly", "=", "np", ".", "pad", "(", "case_all_data", "[", "-", "1", ":", "]", ",", "(", "(", "0", ",", "0", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_x_lb", ")", ",", "max", "(", "bbox_x_ub", "-", "shape", "[", "0", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_y_lb", ")", ",", "max", "(", "bbox_y_ub", "-", "shape", "[", "1", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_z_lb", ")", ",", "max", "(", "bbox_z_ub", "-", "shape", "[", "2", "]", ",", "0", ")", ")", ")", ",", "\n", "'constant'", ",", "**", "{", "'constant_values'", ":", "-", "1", "}", ")", "\n", "if", "seg_from_previous_stage", "is", "not", "None", ":", "\n", "                ", "seg_from_previous_stage", "=", "np", ".", "pad", "(", "seg_from_previous_stage", ",", "(", "(", "0", ",", "0", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_x_lb", ")", ",", "max", "(", "bbox_x_ub", "-", "shape", "[", "0", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_y_lb", ")", ",", "max", "(", "bbox_y_ub", "-", "shape", "[", "1", "]", ",", "0", ")", ")", ",", "\n", "(", "-", "min", "(", "0", ",", "bbox_z_lb", ")", ",", "max", "(", "bbox_z_ub", "-", "shape", "[", "2", "]", ",", "0", ")", ")", ")", ",", "\n", "'constant'", ",", "**", "{", "'constant_values'", ":", "0", "}", ")", "\n", "case_all_data_segonly", "=", "np", ".", "concatenate", "(", "(", "case_all_data_segonly", ",", "seg_from_previous_stage", ")", ",", "0", ")", "\n", "\n", "", "data", ".", "append", "(", "case_all_data_donly", "[", "None", "]", ")", "\n", "seg", ".", "append", "(", "case_all_data_segonly", "[", "None", "]", ")", "\n", "\n", "# this is bad. Slow. Better preallocate data and set with np.zeros(). But this way we don't have to know how", "\n", "# many color channels data has and how many seg channels there are", "\n", "", "data", "=", "np", ".", "vstack", "(", "data", ")", "\n", "seg", "=", "np", ".", "vstack", "(", "seg", ")", "\n", "\n", "return", "{", "'data'", ":", "data", ",", "'seg'", ":", "seg", ",", "'properties'", ":", "case_properties", ",", "'keys'", ":", "selected_keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.DataLoader2D.__init__": [[367, 414], ["batchgenerators.dataloading.SlimDataLoaderBase.__init__", "list", "collections.OrderedDict", "isinstance", "dataset_loading.DataLoader2D._data.keys", "numpy.array", "numpy.array", "isinstance", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "data", ",", "patch_size", ",", "final_patch_size", ",", "batch_size", ",", "transpose", "=", "None", ",", "\n", "oversample_foreground_percent", "=", "0.0", ",", "memmap_mode", "=", "\"r\"", ",", "pseudo_3d_slices", "=", "1", ",", "pad_mode", "=", "\"edge\"", ",", "\n", "pad_kwargs_data", "=", "None", ",", "pad_sides", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        This is the basic data loader for 2D networks. It uses preprocessed data as produced by my (Fabian) preprocessing.\n        You can load the data with load_dataset(folder) where folder is the folder where the npz files are located. If there\n        are only npz files present in that folder, the data loader will unpack them on the fly. This may take a while\n        and increase CPU usage. Therefore, I advise you to call unpack_dataset(folder) first, which will unpack all npz\n        to npy. Don't forget to call delete_npy(folder) after you are done with training?\n        Why all the hassle? Well the decathlon dataset is huge. Using npy for everything will consume >1 TB and that is uncool\n        given that I (Fabian) will have to store that permanently on /datasets and my local computer. With htis strategy all\n        data is stored in a compressed format (factor 10 smaller) and only unpacked when needed.\n        :param data: get this with load_dataset(folder, stage=0). Plug the return value in here and you are g2g (good to go)\n        :param patch_size: what patch size will this data loader return? it is common practice to first load larger\n        patches so that a central crop after data augmentation can be done to reduce border artifacts. If unsure, use\n        get_patch_size() from data_augmentation.default_data_augmentation\n        :param final_patch_size: what will the patch finally be cropped to (after data augmentation)? this is the patch\n        size that goes into your network. We need this here because we will pad patients in here so that patches at the\n        border of patients are sampled properly\n        :param batch_size:\n        :param num_batches: how many batches will the data loader produce before stopping? None=endless\n        :param seed:\n        :param stage: ignore this (Fabian only)\n        :param transpose: ignore this\n        :param random: sample randomly; CAREFUL! non-random sampling requires batch_size=1, otherwise you will iterate batch_size times over the dataset\n        :param pseudo_3d_slices: 7 = 3 below and 3 above the center slice\n        \"\"\"", "\n", "super", "(", "DataLoader2D", ",", "self", ")", ".", "__init__", "(", "data", ",", "batch_size", ",", "None", ")", "\n", "if", "pad_kwargs_data", "is", "None", ":", "\n", "            ", "pad_kwargs_data", "=", "OrderedDict", "(", ")", "\n", "", "self", ".", "pad_kwargs_data", "=", "pad_kwargs_data", "\n", "self", ".", "pad_mode", "=", "pad_mode", "\n", "self", ".", "pseudo_3d_slices", "=", "pseudo_3d_slices", "\n", "self", ".", "oversample_foreground_percent", "=", "oversample_foreground_percent", "\n", "self", ".", "final_patch_size", "=", "final_patch_size", "\n", "if", "transpose", "is", "not", "None", ":", "\n", "            ", "assert", "isinstance", "(", "transpose", ",", "(", "list", ",", "tuple", ")", ")", ",", "\"Transpose must be either None or be a tuple/list representing the new axis order (3 ints)\"", "\n", "", "self", ".", "transpose", "=", "transpose", "\n", "self", ".", "patch_size", "=", "patch_size", "\n", "self", ".", "list_of_keys", "=", "list", "(", "self", ".", "_data", ".", "keys", "(", ")", ")", "\n", "self", ".", "need_to_pad", "=", "np", ".", "array", "(", "patch_size", ")", "-", "np", ".", "array", "(", "final_patch_size", ")", "\n", "self", ".", "memmap_mode", "=", "memmap_mode", "\n", "if", "pad_sides", "is", "not", "None", ":", "\n", "            ", "if", "not", "isinstance", "(", "pad_sides", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "pad_sides", "=", "np", ".", "array", "(", "pad_sides", ")", "\n", "", "self", ".", "need_to_pad", "+=", "pad_sides", "\n", "", "self", ".", "pad_sides", "=", "pad_sides", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.DataLoader2D.all_possible_slices": [[415, 427], ["sorted", "range", "slices.append"], "methods", ["None"], ["", "@", "property", "\n", "def", "all_possible_slices", "(", "self", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "self", ".", "_all_possible_slices", "\n", "", "except", "AttributeError", ":", "\n", "            ", "slices", "=", "[", "]", "\n", "for", "key", "in", "sorted", "(", "self", ".", "list_of_keys", ")", ":", "\n", "                ", "shape", "=", "self", ".", "_data", "[", "key", "]", "[", "\"properties\"", "]", "[", "\"size_after_resampling\"", "]", "\n", "for", "s", "in", "range", "(", "shape", "[", "0", "]", ")", ":", "\n", "                    ", "slices", ".", "append", "(", "(", "key", ",", "s", ")", ")", "\n", "", "", "self", ".", "_all_possible_slices", "=", "slices", "\n", "return", "slices", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.DataLoader2D.get_do_oversample": [[428, 430], ["round"], "methods", ["None"], ["", "", "def", "get_do_oversample", "(", "self", ",", "batch_idx", ")", ":", "\n", "        ", "return", "not", "batch_idx", "<", "round", "(", "self", ".", "batch_size", "*", "(", "1", "-", "self", ".", "oversample_foreground_percent", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.DataLoader2D.generate_train_batch": [[431, 544], ["numpy.random.choice", "enumerate", "numpy.vstack", "numpy.vstack", "case_properties.append", "dataset_loading.DataLoader2D.get_do_oversample", "numpy.vstack.append", "numpy.vstack.append", "isfile", "numpy.load", "len", "numpy.random.choice", "properties.get", "numpy.unique", "max", "min", "numpy.concatenate.reshape", "numpy.concatenate", "numpy.any", "numpy.array", "numpy.any", "numpy.any", "batchgenerators.augmentations.utils.pad_nd_image", "batchgenerators.augmentations.utils.pad_nd_image", "batchgenerators.augmentations.utils.random_crop_2D_image_batched", "numpy.load", "numpy.random.choice", "len", "numpy.random.choice", "numpy.random.choice", "numpy.concatenate.transpose", "numpy.array", "numpy.concatenate", "numpy.array", "numpy.concatenate", "numpy.max", "numpy.max", "numpy.vstack", "tuple", "dataset_loading.crop_2D_image_force_fg", "len", "numpy.where", "numpy.vstack", "numpy.vstack", "tuple", "numpy.sum", "numpy.zeros", "numpy.zeros", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "range"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.DataLoader2D.get_do_oversample", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.crop_2D_image_force_fg"], ["", "def", "generate_train_batch", "(", "self", ")", ":", "\n", "        ", "selected_keys", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "list_of_keys", ",", "self", ".", "batch_size", ",", "True", ",", "None", ")", "\n", "data", "=", "[", "]", "\n", "seg", "=", "[", "]", "\n", "case_properties", "=", "[", "]", "\n", "for", "j", ",", "i", "in", "enumerate", "(", "selected_keys", ")", ":", "\n", "            ", "properties", "=", "self", ".", "_data", "[", "i", "]", "[", "'properties'", "]", "\n", "case_properties", ".", "append", "(", "properties", ")", "\n", "\n", "if", "self", ".", "get_do_oversample", "(", "j", ")", ":", "\n", "                ", "force_fg", "=", "True", "\n", "", "else", ":", "\n", "                ", "force_fg", "=", "False", "\n", "\n", "", "if", "not", "isfile", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ")", ":", "\n", "# lets hope you know what you're doing", "\n", "                ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npz\"", ")", "[", "'data'", "]", "\n", "", "else", ":", "\n", "                ", "case_all_data", "=", "np", ".", "load", "(", "self", ".", "_data", "[", "i", "]", "[", "'data_file'", "]", "[", ":", "-", "4", "]", "+", "\".npy\"", ",", "self", ".", "memmap_mode", ")", "\n", "\n", "# this is for when there is just a 2d slice in case_all_data (2d support)", "\n", "", "if", "len", "(", "case_all_data", ".", "shape", ")", "==", "3", ":", "\n", "                ", "case_all_data", "=", "case_all_data", "[", ":", ",", "None", "]", "\n", "\n", "", "if", "self", ".", "transpose", "is", "not", "None", ":", "\n", "                ", "leading_axis", "=", "self", ".", "transpose", "[", "0", "]", "\n", "", "else", ":", "\n", "                ", "leading_axis", "=", "0", "\n", "\n", "", "if", "not", "force_fg", ":", "\n", "                ", "random_slice", "=", "np", ".", "random", ".", "choice", "(", "case_all_data", ".", "shape", "[", "leading_axis", "+", "1", "]", ")", "\n", "", "else", ":", "\n", "# select one class, then select a slice that contains that class", "\n", "                ", "classes_in_slice_per_axis", "=", "properties", ".", "get", "(", "\"classes_in_slice_per_axis\"", ")", "\n", "possible_classes", "=", "np", ".", "unique", "(", "properties", "[", "'classes'", "]", ")", "\n", "possible_classes", "=", "possible_classes", "[", "possible_classes", ">", "0", "]", "\n", "if", "len", "(", "possible_classes", ")", ">", "0", "and", "not", "(", "0", "in", "possible_classes", ".", "shape", ")", ":", "\n", "                    ", "selected_class", "=", "np", ".", "random", ".", "choice", "(", "possible_classes", ")", "\n", "", "else", ":", "\n", "                    ", "selected_class", "=", "0", "\n", "", "if", "classes_in_slice_per_axis", "is", "not", "None", ":", "\n", "                    ", "valid_slices", "=", "classes_in_slice_per_axis", "[", "leading_axis", "]", "[", "selected_class", "]", "\n", "", "else", ":", "\n", "                    ", "valid_slices", "=", "np", ".", "where", "(", "np", ".", "sum", "(", "case_all_data", "[", "-", "1", "]", "==", "selected_class", ",", "axis", "=", "[", "i", "for", "i", "in", "range", "(", "3", ")", "if", "i", "!=", "leading_axis", "]", ")", ")", "[", "0", "]", "\n", "", "if", "len", "(", "valid_slices", ")", "!=", "0", ":", "\n", "                    ", "random_slice", "=", "np", ".", "random", ".", "choice", "(", "valid_slices", ")", "\n", "", "else", ":", "\n", "                    ", "random_slice", "=", "np", ".", "random", ".", "choice", "(", "case_all_data", ".", "shape", "[", "leading_axis", "+", "1", "]", ")", "\n", "\n", "", "", "if", "self", ".", "pseudo_3d_slices", "==", "1", ":", "\n", "                ", "if", "leading_axis", "==", "0", ":", "\n", "                    ", "case_all_data", "=", "case_all_data", "[", ":", ",", "random_slice", "]", "\n", "", "elif", "leading_axis", "==", "1", ":", "\n", "                    ", "case_all_data", "=", "case_all_data", "[", ":", ",", ":", ",", "random_slice", "]", "\n", "", "else", ":", "\n", "                    ", "case_all_data", "=", "case_all_data", "[", ":", ",", ":", ",", ":", ",", "random_slice", "]", "\n", "", "if", "self", ".", "transpose", "is", "not", "None", "and", "self", ".", "transpose", "[", "1", "]", ">", "self", ".", "transpose", "[", "2", "]", ":", "\n", "                    ", "case_all_data", "=", "case_all_data", ".", "transpose", "(", "0", ",", "2", ",", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "assert", "leading_axis", "==", "0", ",", "\"pseudo_3d_slices works only without transpose for now!\"", "\n", "mn", "=", "random_slice", "-", "(", "self", ".", "pseudo_3d_slices", "-", "1", ")", "//", "2", "\n", "mx", "=", "random_slice", "+", "(", "self", ".", "pseudo_3d_slices", "-", "1", ")", "//", "2", "+", "1", "\n", "valid_mn", "=", "max", "(", "mn", ",", "0", ")", "\n", "valid_mx", "=", "min", "(", "mx", ",", "case_all_data", ".", "shape", "[", "1", "]", ")", "\n", "case_all_seg", "=", "case_all_data", "[", "-", "1", ":", "]", "\n", "case_all_data", "=", "case_all_data", "[", ":", "-", "1", "]", "\n", "case_all_data", "=", "case_all_data", "[", ":", ",", "valid_mn", ":", "valid_mx", "]", "\n", "case_all_seg", "=", "case_all_seg", "[", ":", ",", "random_slice", "]", "\n", "need_to_pad_below", "=", "valid_mn", "-", "mn", "\n", "need_to_pad_above", "=", "mx", "-", "valid_mx", "\n", "if", "need_to_pad_below", ">", "0", ":", "\n", "                    ", "shp_for_pad", "=", "np", ".", "array", "(", "case_all_data", ".", "shape", ")", "\n", "shp_for_pad", "[", "1", "]", "=", "need_to_pad_below", "\n", "case_all_data", "=", "np", ".", "concatenate", "(", "(", "np", ".", "zeros", "(", "shp_for_pad", ")", ",", "case_all_data", ")", ",", "1", ")", "\n", "", "if", "need_to_pad_above", ">", "0", ":", "\n", "                    ", "shp_for_pad", "=", "np", ".", "array", "(", "case_all_data", ".", "shape", ")", "\n", "shp_for_pad", "[", "1", "]", "=", "need_to_pad_above", "\n", "case_all_data", "=", "np", ".", "concatenate", "(", "(", "case_all_data", ",", "np", ".", "zeros", "(", "shp_for_pad", ")", ")", ",", "1", ")", "\n", "", "case_all_data", "=", "case_all_data", ".", "reshape", "(", "(", "-", "1", ",", "case_all_data", ".", "shape", "[", "-", "2", "]", ",", "case_all_data", ".", "shape", "[", "-", "1", "]", ")", ")", "\n", "case_all_data", "=", "np", ".", "concatenate", "(", "(", "case_all_data", ",", "case_all_seg", ")", ",", "0", ")", "\n", "\n", "", "num_seg", "=", "1", "\n", "\n", "# why we need this is a little complicated. It has to do with downstream random cropping during data", "\n", "# augmentation. Basically we will rotate the patch and then to a center crop of size self.final_patch_size.", "\n", "# Depending on the rotation, scaling and elastic deformation parameters, self.patch_size has to be large", "\n", "# enough to prevent border artifacts from being present in the final patch", "\n", "new_shp", "=", "None", "\n", "if", "np", ".", "any", "(", "self", ".", "need_to_pad", ")", ">", "0", ":", "\n", "                ", "new_shp", "=", "np", ".", "array", "(", "case_all_data", ".", "shape", "[", "1", ":", "]", "+", "self", ".", "need_to_pad", ")", "\n", "if", "np", ".", "any", "(", "new_shp", "-", "np", ".", "array", "(", "self", ".", "patch_size", ")", "<", "0", ")", ":", "\n", "                    ", "new_shp", "=", "np", ".", "max", "(", "np", ".", "vstack", "(", "(", "new_shp", "[", "None", "]", ",", "np", ".", "array", "(", "self", ".", "patch_size", ")", "[", "None", "]", ")", ")", ",", "0", ")", "\n", "", "", "else", ":", "\n", "                ", "if", "np", ".", "any", "(", "np", ".", "array", "(", "case_all_data", ".", "shape", "[", "1", ":", "]", ")", "-", "np", ".", "array", "(", "self", ".", "patch_size", ")", "<", "0", ")", ":", "\n", "                    ", "new_shp", "=", "np", ".", "max", "(", "\n", "np", ".", "vstack", "(", "(", "np", ".", "array", "(", "case_all_data", ".", "shape", "[", "1", ":", "]", ")", "[", "None", "]", ",", "np", ".", "array", "(", "self", ".", "patch_size", ")", "[", "None", "]", ")", ")", ",", "0", ")", "\n", "", "", "if", "new_shp", "is", "not", "None", ":", "\n", "                ", "case_all_data_donly", "=", "pad_nd_image", "(", "case_all_data", "[", ":", "-", "num_seg", "]", ",", "new_shp", ",", "self", ".", "pad_mode", ",", "kwargs", "=", "self", ".", "pad_kwargs_data", ")", "\n", "case_all_data_segnonly", "=", "pad_nd_image", "(", "case_all_data", "[", "-", "num_seg", ":", "]", ",", "new_shp", ",", "'constant'", ",", "kwargs", "=", "{", "'constant_values'", ":", "-", "1", "}", ")", "\n", "case_all_data", "=", "np", ".", "vstack", "(", "(", "case_all_data_donly", ",", "case_all_data_segnonly", ")", ")", "[", "None", "]", "\n", "", "else", ":", "\n", "                ", "case_all_data", "=", "case_all_data", "[", "None", "]", "\n", "\n", "", "if", "not", "force_fg", ":", "\n", "                ", "case_all_data", "=", "random_crop_2D_image_batched", "(", "case_all_data", ",", "tuple", "(", "self", ".", "patch_size", ")", ")", "\n", "", "else", ":", "\n", "                ", "case_all_data", "=", "crop_2D_image_force_fg", "(", "case_all_data", "[", "0", "]", ",", "tuple", "(", "self", ".", "patch_size", ")", ",", "selected_class", ")", "[", "None", "]", "\n", "", "data", ".", "append", "(", "case_all_data", "[", ":", ",", ":", "-", "num_seg", "]", ")", "\n", "seg", ".", "append", "(", "case_all_data", "[", ":", ",", "-", "num_seg", ":", "]", ")", "\n", "", "data", "=", "np", ".", "vstack", "(", "data", ")", "\n", "seg", "=", "np", ".", "vstack", "(", "seg", ")", "\n", "keys", "=", "selected_keys", "\n", "return", "{", "'data'", ":", "data", ",", "'seg'", ":", "seg", ",", "'properties'", ":", "case_properties", ",", "\"keys\"", ":", "keys", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.get_case_identifiers": [[24, 27], ["os.listdir", "i.endswith", "i.find"], "function", ["None"], ["def", "get_case_identifiers", "(", "folder", ")", ":", "\n", "    ", "case_identifiers", "=", "[", "i", "[", ":", "-", "4", "]", "for", "i", "in", "os", ".", "listdir", "(", "folder", ")", "if", "i", ".", "endswith", "(", "\"npz\"", ")", "and", "(", "i", ".", "find", "(", "\"segFromPrevStage\"", ")", "==", "-", "1", ")", "]", "\n", "return", "case_identifiers", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.get_case_identifiers_from_raw_folder": [[29, 32], ["numpy.unique", "os.listdir", "i.endswith", "i.find"], "function", ["None"], ["", "def", "get_case_identifiers_from_raw_folder", "(", "folder", ")", ":", "\n", "    ", "case_identifiers", "=", "np", ".", "unique", "(", "[", "i", "[", ":", "-", "12", "]", "for", "i", "in", "os", ".", "listdir", "(", "folder", ")", "if", "i", ".", "endswith", "(", "\".nii.gz\"", ")", "and", "(", "i", ".", "find", "(", "\"segFromPrevStage\"", ")", "==", "-", "1", ")", "]", ")", "\n", "return", "case_identifiers", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.convert_to_npy": [[34, 43], ["isinstance", "isfile", "numpy.save", "numpy.load"], "function", ["None"], ["", "def", "convert_to_npy", "(", "args", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "args", ",", "tuple", ")", ":", "\n", "        ", "key", "=", "\"data\"", "\n", "npz_file", "=", "args", "\n", "", "else", ":", "\n", "        ", "npz_file", ",", "key", "=", "args", "\n", "", "if", "not", "isfile", "(", "npz_file", "[", ":", "-", "3", "]", "+", "\"npy\"", ")", ":", "\n", "        ", "a", "=", "np", ".", "load", "(", "npz_file", ")", "[", "key", "]", "\n", "np", ".", "save", "(", "npz_file", "[", ":", "-", "3", "]", "+", "\"npy\"", ",", "a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.save_as_npz": [[45, 53], ["numpy.load", "numpy.savez_compressed", "isinstance"], "function", ["None"], ["", "", "def", "save_as_npz", "(", "args", ")", ":", "\n", "    ", "if", "not", "isinstance", "(", "args", ",", "tuple", ")", ":", "\n", "        ", "key", "=", "\"data\"", "\n", "npy_file", "=", "args", "\n", "", "else", ":", "\n", "        ", "npy_file", ",", "key", "=", "args", "\n", "", "d", "=", "np", ".", "load", "(", "npy_file", ")", "\n", "np", ".", "savez_compressed", "(", "npy_file", "[", ":", "-", "3", "]", "+", "\"npz\"", ",", "**", "{", "key", ":", "d", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.unpack_dataset": [[55, 68], ["multiprocessing.Pool", "subfiles", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "zip", "len"], "function", ["None"], ["", "def", "unpack_dataset", "(", "folder", ",", "threads", "=", "8", ",", "key", "=", "\"data\"", ")", ":", "\n", "    ", "\"\"\"\n    unpacks all npz files in a folder to npy (whatever you want to have unpacked must be saved unter key)\n    :param folder:\n    :param threads:\n    :param key:\n    :return:\n    \"\"\"", "\n", "p", "=", "Pool", "(", "threads", ")", "\n", "npz_files", "=", "subfiles", "(", "folder", ",", "True", ",", "None", ",", "\".npz\"", ",", "True", ")", "\n", "p", ".", "map", "(", "convert_to_npy", ",", "zip", "(", "npz_files", ",", "[", "key", "]", "*", "len", "(", "npz_files", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.pack_dataset": [[70, 76], ["multiprocessing.Pool", "subfiles", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "zip", "len"], "function", ["None"], ["", "def", "pack_dataset", "(", "folder", ",", "threads", "=", "8", ",", "key", "=", "\"data\"", ")", ":", "\n", "    ", "p", "=", "Pool", "(", "threads", ")", "\n", "npy_files", "=", "subfiles", "(", "folder", ",", "True", ",", "None", ",", "\".npy\"", ",", "True", ")", "\n", "p", ".", "map", "(", "save_as_npz", ",", "zip", "(", "npy_files", ",", "[", "key", "]", "*", "len", "(", "npy_files", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.delete_npy": [[78, 84], ["dataset_loading.get_case_identifiers", "join", "os.remove", "isfile"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.get_case_identifiers"], ["", "def", "delete_npy", "(", "folder", ")", ":", "\n", "    ", "case_identifiers", "=", "get_case_identifiers", "(", "folder", ")", "\n", "npy_files", "=", "[", "join", "(", "folder", ",", "i", "+", "\".npy\"", ")", "for", "i", "in", "case_identifiers", "]", "\n", "npy_files", "=", "[", "i", "for", "i", "in", "npy_files", "if", "isfile", "(", "i", ")", "]", "\n", "for", "n", "in", "npy_files", ":", "\n", "        ", "os", ".", "remove", "(", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.load_dataset": [[86, 99], ["dataset_loading.get_case_identifiers", "get_case_identifiers.sort", "collections.OrderedDict", "collections.OrderedDict", "join", "open", "pickle.load", "dataset[].get", "join", "join"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.get_case_identifiers"], ["", "", "def", "load_dataset", "(", "folder", ")", ":", "\n", "# we don't load the actual data but instead return the filename to the np file. the properties are loaded though", "\n", "    ", "case_identifiers", "=", "get_case_identifiers", "(", "folder", ")", "\n", "case_identifiers", ".", "sort", "(", ")", "\n", "dataset", "=", "OrderedDict", "(", ")", "\n", "for", "c", "in", "case_identifiers", ":", "\n", "        ", "dataset", "[", "c", "]", "=", "OrderedDict", "(", ")", "\n", "dataset", "[", "c", "]", "[", "'data_file'", "]", "=", "join", "(", "folder", ",", "\"%s.npz\"", "%", "c", ")", "\n", "with", "open", "(", "join", "(", "folder", ",", "\"%s.pkl\"", "%", "c", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "dataset", "[", "c", "]", "[", "'properties'", "]", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "if", "dataset", "[", "c", "]", ".", "get", "(", "'seg_from_prev_stage_file'", ")", "is", "not", "None", ":", "\n", "            ", "dataset", "[", "c", "]", "[", "'seg_from_prev_stage_file'", "]", "=", "join", "(", "folder", ",", "\"%s_segs.npz\"", "%", "c", ")", "\n", "", "", "return", "dataset", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.dataloading.dataset_loading.crop_2D_image_force_fg": [[101, 145], ["numpy.unique", "numpy.array", "numpy.any", "numpy.array", "range", "type", "numpy.random.choice", "numpy.where", "max", "min", "len", "len", "numpy.array", "numpy.random.random_integers", "numpy.random.random_integers", "len", "len", "numpy.random.choice"], "function", ["None"], ["", "def", "crop_2D_image_force_fg", "(", "img", ",", "crop_size", ",", "force_class", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    img must be [c, x, y]\n    img[-1] must be the segmentation with segmentation>0 being foreground\n    :param img:\n    :param crop_size:\n    :return:\n    \"\"\"", "\n", "if", "type", "(", "crop_size", ")", "not", "in", "(", "tuple", ",", "list", ")", ":", "\n", "        ", "crop_size", "=", "[", "crop_size", "]", "*", "(", "len", "(", "img", ".", "shape", ")", "-", "1", ")", "\n", "", "else", ":", "\n", "        ", "assert", "len", "(", "crop_size", ")", "==", "(", "len", "(", "\n", "img", ".", "shape", ")", "-", "1", ")", ",", "\"If you provide a list/tuple as center crop make sure it has the same len as your data has dims (3d)\"", "\n", "\n", "# we need to find the center coords that we can crop to without exceeding the image border", "\n", "", "lb_x", "=", "crop_size", "[", "0", "]", "//", "2", "\n", "ub_x", "=", "img", ".", "shape", "[", "1", "]", "-", "crop_size", "[", "0", "]", "//", "2", "-", "crop_size", "[", "0", "]", "%", "2", "\n", "lb_y", "=", "crop_size", "[", "1", "]", "//", "2", "\n", "ub_y", "=", "img", ".", "shape", "[", "2", "]", "-", "crop_size", "[", "1", "]", "//", "2", "-", "crop_size", "[", "1", "]", "%", "2", "\n", "\n", "foreground_classes", "=", "np", ".", "unique", "(", "img", "[", "-", "1", "]", ")", "\n", "foreground_classes", "=", "foreground_classes", "[", "foreground_classes", ">", "0", "]", "\n", "if", "len", "(", "foreground_classes", ")", "==", "0", "or", "(", "0", "in", "foreground_classes", ".", "shape", ")", ":", "\n", "        ", "foreground_classes", "=", "[", "0", "]", "\n", "\n", "", "if", "force_class", "is", "None", "or", "force_class", "not", "in", "foreground_classes", ":", "\n", "        ", "chosen_class", "=", "np", ".", "random", ".", "choice", "(", "foreground_classes", ")", "\n", "", "else", ":", "\n", "        ", "chosen_class", "=", "force_class", "\n", "", "foreground_voxels", "=", "np", ".", "array", "(", "np", ".", "where", "(", "img", "[", "-", "1", "]", "==", "chosen_class", ")", ")", "\n", "if", "np", ".", "any", "(", "np", ".", "array", "(", "foreground_voxels", ".", "shape", ")", "==", "0", ")", ":", "\n", "        ", "selected_center_voxel", "=", "(", "np", ".", "random", ".", "random_integers", "(", "lb_x", ",", "ub_x", ")", ",", "\n", "np", ".", "random", ".", "random_integers", "(", "lb_y", ",", "ub_y", ")", ")", "\n", "", "else", ":", "\n", "        ", "selected_center_voxel", "=", "foreground_voxels", "[", ":", ",", "np", ".", "random", ".", "choice", "(", "foreground_voxels", ".", "shape", "[", "1", "]", ")", "]", "\n", "\n", "", "selected_center_voxel", "=", "np", ".", "array", "(", "selected_center_voxel", ")", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "        ", "selected_center_voxel", "[", "i", "]", "=", "max", "(", "crop_size", "[", "i", "]", "//", "2", ",", "selected_center_voxel", "[", "i", "]", ")", "\n", "selected_center_voxel", "[", "i", "]", "=", "min", "(", "img", ".", "shape", "[", "i", "+", "1", "]", "-", "crop_size", "[", "i", "]", "//", "2", "-", "crop_size", "[", "i", "]", "%", "2", ",", "selected_center_voxel", "[", "i", "]", ")", "\n", "\n", "", "result", "=", "img", "[", ":", ",", "(", "selected_center_voxel", "[", "0", "]", "-", "crop_size", "[", "0", "]", "//", "2", ")", ":", "(", "selected_center_voxel", "[", "0", "]", "+", "crop_size", "[", "0", "]", "//", "2", "+", "crop_size", "[", "0", "]", "%", "2", ")", ",", "\n", "(", "selected_center_voxel", "[", "1", "]", "-", "crop_size", "[", "1", "]", "//", "2", ")", ":", "(", "selected_center_voxel", "[", "1", "]", "+", "crop_size", "[", "1", "]", "//", "2", "+", "crop_size", "[", "1", "]", "%", "2", ")", "]", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.predict.predict_save_to_queue": [[32, 80], ["enumerate", "q.put", "len", "print", "print", "print", "preprocess_fn", "print", "q.put", "SimpleITK.GetArrayFromImage", "SimpleITK.GetArrayFromImage", "all", "batchgenerators.augmentations.utils.resize_segmentation", "nnunet.utilities.one_hot_encoding.to_one_hot", "numpy.vstack().astype", "numpy.prod", "print", "numpy.save", "print", "traceback.print_exc", "errors_in.append", "isfile", "segs_from_prev_stage[].endswith", "SimpleITK.ReadImage", "SimpleITK.ReadImage", "numpy.vstack", "zip"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.one_hot_encoding.to_one_hot"], ["def", "predict_save_to_queue", "(", "preprocess_fn", ",", "q", ",", "list_of_lists", ",", "output_files", ",", "segs_from_prev_stage", ",", "classes", ")", ":", "\n", "    ", "errors_in", "=", "[", "]", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "list_of_lists", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "output_file", "=", "output_files", "[", "i", "]", "\n", "d", ",", "_", ",", "dct", "=", "preprocess_fn", "(", "l", ")", "\n", "if", "segs_from_prev_stage", "[", "i", "]", "is", "not", "None", ":", "\n", "                ", "assert", "isfile", "(", "segs_from_prev_stage", "[", "i", "]", ")", "and", "segs_from_prev_stage", "[", "i", "]", ".", "endswith", "(", "\".nii.gz\"", ")", ",", "\"segs_from_prev_stage\"", "\" must point to a \"", "\"segmentation file\"", "\n", "seg_prev", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "segs_from_prev_stage", "[", "i", "]", ")", ")", "\n", "# check to see if shapes match", "\n", "img", "=", "sitk", ".", "GetArrayFromImage", "(", "sitk", ".", "ReadImage", "(", "l", "[", "0", "]", ")", ")", "\n", "assert", "all", "(", "[", "i", "==", "j", "for", "i", ",", "j", "in", "zip", "(", "seg_prev", ".", "shape", ",", "img", ".", "shape", ")", "]", ")", ",", "\"image and segmentation from previous \"", "\"stage don't have the same pixel array \"", "\"shape! image: %s, seg_prev: %s\"", "%", "(", "l", "[", "0", "]", ",", "segs_from_prev_stage", "[", "i", "]", ")", "\n", "seg_reshaped", "=", "resize_segmentation", "(", "seg_prev", ",", "d", ".", "shape", "[", "1", ":", "]", ",", "order", "=", "1", ",", "cval", "=", "0", ")", "\n", "seg_reshaped", "=", "to_one_hot", "(", "seg_reshaped", ",", "classes", ")", "\n", "d", "=", "np", ".", "vstack", "(", "(", "d", ",", "seg_reshaped", ")", ")", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "\"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n            larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n            communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n            enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n            patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n            then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n            filename or np.ndarray and will handle this automatically\"\"\"", "\n", "print", "(", "d", ".", "shape", ")", "\n", "if", "np", ".", "prod", "(", "d", ".", "shape", ")", ">", "(", "2e9", "/", "4", "*", "0.9", ")", ":", "# *0.9 just to be save, 4 because float32 is 4 bytes", "\n", "                ", "print", "(", "\n", "\"This output is too large for python process-process communication. \"", "\n", "\"Saving output temporarily to disk\"", ")", "\n", "np", ".", "save", "(", "output_file", "[", ":", "-", "7", "]", "+", "\".npy\"", ",", "d", ")", "\n", "d", "=", "output_file", "[", ":", "-", "7", "]", "+", "\".npy\"", "\n", "", "q", ".", "put", "(", "(", "output_file", ",", "(", "d", ",", "dct", ")", ")", ")", "\n", "", "except", "KeyboardInterrupt", ":", "\n", "            ", "raise", "KeyboardInterrupt", "\n", "", "except", ":", "\n", "            ", "print", "(", "\"error in\"", ",", "l", ")", "\n", "import", "traceback", "\n", "traceback", ".", "print_exc", "(", ")", "\n", "errors_in", ".", "append", "(", "l", ")", "\n", "", "", "q", ".", "put", "(", "\"end\"", ")", "\n", "if", "len", "(", "errors_in", ")", ">", "0", ":", "\n", "        ", "print", "(", "\"There were some errors in the following cases:\"", ",", "errors_in", ")", "\n", "print", "(", "\"These cases were ignored.\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"This worker has ended successfully, no errors to report\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.predict.preprocess_multithreaded": [[82, 116], ["list", "isinstance", "multiprocessing.Queue", "range", "range", "multiprocessing.Process", "multiprocessing.Process.start", "processes.append", "multiprocessing.Queue.close", "len", "multiprocessing.Queue.get", "p.is_alive", "p.join", "p.terminate"], "function", ["None"], ["", "", "def", "preprocess_multithreaded", "(", "trainer", ",", "list_of_lists", ",", "output_files", ",", "num_processes", "=", "2", ",", "segs_from_prev_stage", "=", "None", ")", ":", "\n", "    ", "if", "segs_from_prev_stage", "is", "None", ":", "\n", "        ", "segs_from_prev_stage", "=", "[", "None", "]", "*", "len", "(", "list_of_lists", ")", "\n", "\n", "", "classes", "=", "list", "(", "range", "(", "1", ",", "trainer", ".", "num_classes", ")", ")", "\n", "assert", "isinstance", "(", "trainer", ",", "nnUNetTrainer", ")", "\n", "q", "=", "Queue", "(", "1", ")", "\n", "processes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_processes", ")", ":", "\n", "        ", "pr", "=", "Process", "(", "target", "=", "predict_save_to_queue", ",", "args", "=", "(", "trainer", ".", "preprocess_patient", ",", "q", ",", "\n", "list_of_lists", "[", "i", ":", ":", "num_processes", "]", ",", "\n", "output_files", "[", "i", ":", ":", "num_processes", "]", ",", "\n", "segs_from_prev_stage", "[", "i", ":", ":", "num_processes", "]", ",", "\n", "classes", ")", ")", "\n", "pr", ".", "start", "(", ")", "\n", "processes", ".", "append", "(", "pr", ")", "\n", "\n", "", "try", ":", "\n", "        ", "end_ctr", "=", "0", "\n", "while", "end_ctr", "!=", "num_processes", ":", "\n", "            ", "item", "=", "q", ".", "get", "(", ")", "\n", "if", "item", "==", "\"end\"", ":", "\n", "                ", "end_ctr", "+=", "1", "\n", "continue", "\n", "", "else", ":", "\n", "                ", "yield", "item", "\n", "\n", "", "", "", "finally", ":", "\n", "        ", "for", "p", "in", "processes", ":", "\n", "            ", "if", "p", ".", "is_alive", "(", ")", ":", "\n", "                ", "p", ".", "terminate", "(", ")", "# this should not happen but better safe than sorry right", "\n", "", "p", ".", "join", "(", ")", "\n", "\n", "", "q", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.predict.predict_cases": [[118, 199], ["multiprocessing.Pool", "print", "torch.cuda.empty_cache", "print", "nnunet.training.model_restore.load_model_and_checkpoint_files", "print", "predict.preprocess_multithreaded", "print", "len", "len", "os.path.split", "cleaned_output_files.append", "print", "print", "isinstance", "print", "numpy.vstack", "numpy.mean", "results.append", "i.get", "len", "len", "len", "maybe_mkdir_p", "f.endswith", "os.path.splitext", "join", "len", "len", "numpy.load", "os.remove", "trainer.load_checkpoint_ram", "np.vstack.append", "numpy.prod", "print", "numpy.save", "multiprocessing.Pool.starmap_async", "enumerate", "isfile", "trainer.predict_preprocessed_data_return_softmax"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.training.model_restore.load_model_and_checkpoint_files", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.predict.preprocess_multithreaded", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.network_trainer.NetworkTrainer.load_checkpoint_ram", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.network_training.nnUNetTrainer.nnUNetTrainer.predict_preprocessed_data_return_softmax"], ["", "", "def", "predict_cases", "(", "model", ",", "list_of_lists", ",", "output_filenames", ",", "folds", ",", "save_npz", ",", "num_threads_preprocessing", ",", "\n", "num_threads_nifti_save", ",", "segs_from_prev_stage", "=", "None", ",", "do_tta", "=", "True", ",", "\n", "overwrite_existing", "=", "False", ")", ":", "\n", "\n", "    ", "assert", "len", "(", "list_of_lists", ")", "==", "len", "(", "output_filenames", ")", "\n", "if", "segs_from_prev_stage", "is", "not", "None", ":", "assert", "len", "(", "segs_from_prev_stage", ")", "==", "len", "(", "output_filenames", ")", "\n", "\n", "prman", "=", "Pool", "(", "num_threads_nifti_save", ")", "\n", "results", "=", "[", "]", "\n", "\n", "cleaned_output_files", "=", "[", "]", "\n", "for", "o", "in", "output_filenames", ":", "\n", "        ", "dr", ",", "f", "=", "os", ".", "path", ".", "split", "(", "o", ")", "\n", "if", "len", "(", "dr", ")", ">", "0", ":", "\n", "            ", "maybe_mkdir_p", "(", "dr", ")", "\n", "", "if", "not", "f", ".", "endswith", "(", "\".nii.gz\"", ")", ":", "\n", "            ", "f", ",", "_", "=", "os", ".", "path", ".", "splitext", "(", "f", ")", "\n", "f", "=", "f", "+", "\".nii.gz\"", "\n", "", "cleaned_output_files", ".", "append", "(", "join", "(", "dr", ",", "f", ")", ")", "\n", "\n", "", "if", "not", "overwrite_existing", ":", "\n", "        ", "print", "(", "\"number of cases:\"", ",", "len", "(", "list_of_lists", ")", ")", "\n", "not_done_idx", "=", "[", "i", "for", "i", ",", "j", "in", "enumerate", "(", "cleaned_output_files", ")", "if", "not", "isfile", "(", "j", ")", "]", "\n", "\n", "cleaned_output_files", "=", "[", "cleaned_output_files", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "list_of_lists", "=", "[", "list_of_lists", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "if", "segs_from_prev_stage", "is", "not", "None", ":", "\n", "            ", "segs_from_prev_stage", "=", "[", "segs_from_prev_stage", "[", "i", "]", "for", "i", "in", "not_done_idx", "]", "\n", "\n", "", "print", "(", "\"number of cases that still need to be predicted:\"", ",", "len", "(", "cleaned_output_files", ")", ")", "\n", "\n", "", "print", "(", "\"emptying cuda cache\"", ")", "\n", "torch", ".", "cuda", ".", "empty_cache", "(", ")", "\n", "\n", "print", "(", "\"loading parameters for folds,\"", ",", "folds", ")", "\n", "trainer", ",", "params", "=", "load_model_and_checkpoint_files", "(", "model", ",", "folds", ")", "\n", "\n", "print", "(", "\"starting preprocessing generator\"", ")", "\n", "preprocessing", "=", "preprocess_multithreaded", "(", "trainer", ",", "list_of_lists", ",", "cleaned_output_files", ",", "num_threads_preprocessing", ",", "segs_from_prev_stage", ")", "\n", "print", "(", "\"starting prediction...\"", ")", "\n", "for", "preprocessed", "in", "preprocessing", ":", "\n", "        ", "output_filename", ",", "(", "d", ",", "dct", ")", "=", "preprocessed", "\n", "if", "isinstance", "(", "d", ",", "str", ")", ":", "\n", "            ", "data", "=", "np", ".", "load", "(", "d", ")", "\n", "os", ".", "remove", "(", "d", ")", "\n", "d", "=", "data", "\n", "\n", "", "print", "(", "\"predicting\"", ",", "output_filename", ")", "\n", "\n", "softmax", "=", "[", "]", "\n", "for", "p", "in", "params", ":", "\n", "            ", "trainer", ".", "load_checkpoint_ram", "(", "p", ",", "False", ")", "\n", "softmax", ".", "append", "(", "trainer", ".", "predict_preprocessed_data_return_softmax", "(", "d", ",", "do_tta", ",", "1", ",", "False", ",", "1", ",", "\n", "trainer", ".", "data_aug_params", "[", "'mirror_axes'", "]", ",", "\n", "True", ",", "True", ",", "2", ",", "trainer", ".", "patch_size", ",", "True", ")", "[", "None", "]", ")", "\n", "\n", "", "softmax", "=", "np", ".", "vstack", "(", "softmax", ")", "\n", "softmax_mean", "=", "np", ".", "mean", "(", "softmax", ",", "0", ")", "\n", "\n", "if", "save_npz", ":", "\n", "            ", "npz_file", "=", "output_filename", "[", ":", "-", "7", "]", "+", "\".npz\"", "\n", "", "else", ":", "\n", "            ", "npz_file", "=", "None", "\n", "\n", "", "\"\"\"There is a problem with python process communication that prevents us from communicating obejcts \n        larger than 2 GB between processes (basically when the length of the pickle string that will be sent is \n        communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long \n        enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually \n        patching system python code. We circumvent that problem here by saving softmax_pred to a npy file that will \n        then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either \n        filename or np.ndarray and will handle this automatically\"\"\"", "\n", "if", "np", ".", "prod", "(", "softmax_mean", ".", "shape", ")", ">", "(", "2e9", "/", "4", "*", "0.9", ")", ":", "# *0.9 just to be save", "\n", "            ", "print", "(", "\"This output is too large for python process-process communication. Saving output temporarily to disk\"", ")", "\n", "np", ".", "save", "(", "output_filename", "[", ":", "-", "7", "]", "+", "\".npy\"", ",", "softmax_mean", ")", "\n", "softmax_mean", "=", "output_filename", "[", ":", "-", "7", "]", "+", "\".npy\"", "\n", "\n", "", "results", ".", "append", "(", "prman", ".", "starmap_async", "(", "save_segmentation_nifti_from_softmax", ",", "\n", "(", "(", "softmax_mean", ",", "output_filename", ",", "dct", ",", "1", ",", "None", ",", "None", ",", "None", ",", "npz_file", ")", ",", ")", "\n", ")", ")", "\n", "\n", "", "_", "=", "[", "i", ".", "get", "(", ")", "for", "i", "in", "results", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.predict.predict_from_folder": [[201, 239], ["maybe_mkdir_p", "shutil.copy", "nnunet.experiment_planning.plan_and_preprocess_task.get_caseIDs_from_splitted_dataset_folder", "subfiles", "predict.predict_cases", "join", "join", "isdir", "all", "join", "join", "isfile", "i[].startswith", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.experiment_planning.plan_and_preprocess_task.get_caseIDs_from_splitted_dataset_folder", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.predict.predict_cases"], ["", "def", "predict_from_folder", "(", "model", ",", "input_folder", ",", "output_folder", ",", "folds", ",", "save_npz", ",", "num_threads_preprocessing", ",", "\n", "num_threads_nifti_save", ",", "lowres_segmentations", ",", "part_id", ",", "num_parts", ",", "tta", ",", "\n", "overwrite_existing", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    here we use the standard naming scheme to generate list_of_lists and output_files needed by predict_cases\n    :param model:\n    :param input_folder:\n    :param output_folder:\n    :param folds:\n    :param save_npz:\n    :param num_threads_preprocessing:\n    :param num_threads_nifti_save:\n    :param lowres_segmentations:\n    :param part_id:\n    :param num_parts:\n    :param tta:\n    :return:\n    \"\"\"", "\n", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "shutil", ".", "copy", "(", "join", "(", "model", ",", "'plans.pkl'", ")", ",", "output_folder", ")", "\n", "\n", "case_ids", "=", "get_caseIDs_from_splitted_dataset_folder", "(", "input_folder", ")", "\n", "output_files", "=", "[", "join", "(", "output_folder", ",", "i", "+", "\".nii.gz\"", ")", "for", "i", "in", "case_ids", "]", "\n", "all_files", "=", "subfiles", "(", "input_folder", ",", "suffix", "=", "\".nii.gz\"", ",", "join", "=", "False", ",", "sort", "=", "True", ")", "\n", "list_of_lists", "=", "[", "[", "join", "(", "input_folder", ",", "i", ")", "for", "i", "in", "all_files", "if", "i", "[", ":", "len", "(", "j", ")", "]", ".", "startswith", "(", "j", ")", "and", "\n", "len", "(", "i", ")", "==", "(", "len", "(", "j", ")", "+", "12", ")", "]", "for", "j", "in", "case_ids", "]", "\n", "\n", "if", "lowres_segmentations", "is", "not", "None", ":", "\n", "        ", "assert", "isdir", "(", "lowres_segmentations", ")", ",", "\"if lowres_segmentations is not None then it must point to a directory\"", "\n", "lowres_segmentations", "=", "[", "join", "(", "lowres_segmentations", ",", "i", "+", "\".nii.gz\"", ")", "for", "i", "in", "case_ids", "]", "\n", "assert", "all", "(", "[", "isfile", "(", "i", ")", "for", "i", "in", "lowres_segmentations", "]", ")", ",", "\"not all lowres_segmentations files are present. \"", "\"(I was searching for case_id.nii.gz in that folder)\"", "\n", "lowres_segmentations", "=", "lowres_segmentations", "[", "part_id", ":", ":", "num_parts", "]", "\n", "", "else", ":", "\n", "        ", "lowres_segmentations", "=", "None", "\n", "", "return", "predict_cases", "(", "model", ",", "list_of_lists", "[", "part_id", ":", ":", "num_parts", "]", ",", "output_files", "[", "part_id", ":", ":", "num_parts", "]", ",", "folds", ",", "save_npz", ",", "\n", "num_threads_preprocessing", ",", "num_threads_nifti_save", ",", "lowres_segmentations", ",", "\n", "tta", ",", "overwrite_existing", "=", "overwrite_existing", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.segmentation_export.save_segmentation_nifti_from_softmax": [[22, 133], ["isinstance", "dct.get", "dct.get", "numpy.any", "dct.get", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "sitk.GetImageFromArray.SetOrigin", "sitk.GetImageFromArray.SetDirection", "SimpleITK.WriteImage", "isfile", "copy.deepcopy", "numpy.load", "os.remove", "print", "nnunet.preprocessing.preprocessing.resample_data_or_seg", "numpy.savez_compressed", "save_pickle", "seg_old_spacing.argmax.argmax", "numpy.zeros", "enumerate", "numpy.zeros", "range", "seg_postprogess_fn", "seg_postprogess_fn.astype", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.SetSpacing", "sitk.GetImageFromArray.SetOrigin", "sitk.GetImageFromArray.SetDirection", "SimpleITK.WriteImage", "numpy.array", "numpy.array", "nnunet.preprocessing.preprocessing.get_do_separate_z", "numpy.min", "numpy.copy", "np.zeros.astype", "dct.get", "nnunet.preprocessing.preprocessing.get_lowres_axis", "nnunet.preprocessing.preprocessing.get_do_separate_z", "nnunet.preprocessing.preprocessing.get_lowres_axis", "seg_old_spacing.argmax.astype", "dct.get", "dct.get", "nnunet.preprocessing.preprocessing.get_lowres_axis", "dct.get", "dct.get"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.resample_data_or_seg", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_do_separate_z", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_lowres_axis", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.preprocessing.preprocessing.get_lowres_axis"], ["def", "save_segmentation_nifti_from_softmax", "(", "segmentation_softmax", ",", "out_fname", ",", "dct", ",", "order", "=", "1", ",", "region_class_order", "=", "None", ",", "\n", "seg_postprogess_fn", "=", "None", ",", "seg_postprocess_args", "=", "None", ",", "resampled_npz_fname", "=", "None", ",", "\n", "non_postprocessed_fname", "=", "None", ",", "force_separate_z", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    This is a utility for writing segmentations to nifto and npz. It requires the data to have been preprocessed by\n    GenericPreprocessor because it depends on the property dictionary output (dct) to know the geometry of the original\n    data. segmentation_softmax does not have to have the same size in pixels as the original data, it will be\n    resampled to match that. This is generally useful because the spacings our networks operate on are most of the time\n    not the native spacings of the image data.\n    If seg_postprogess_fn is not None then seg_postprogess_fnseg_postprogess_fn(segmentation, *seg_postprocess_args)\n    will be called before nifto export\n    There is a problem with python process communication that prevents us from communicating obejcts\n    larger than 2 GB between processes (basically when the length of the pickle string that will be sent is\n    communicated by the multiprocessing.Pipe object then the placeholder (\\%i I think) does not allow for long\n    enough strings (lol). This could be fixed by changing i to l (for long) but that would require manually\n    patching system python code.) We circumvent that problem here by saving softmax_pred to a npy file that will\n    then be read (and finally deleted) by the Process. save_segmentation_nifti_from_softmax can take either\n    filename or np.ndarray for segmentation_softmax and will handle this automatically\n    :param segmentation_softmax:\n    :param out_fname:\n    :param dct:\n    :param order:\n    :param region_class_order:\n    :param seg_postprogess_fn:\n    :param seg_postprocess_args:\n    :param resampled_npz_fname:\n    :param non_postprocessed_fname:\n    :param force_separate_z: if None then we dynamically decide how to resample along z, if True/False then always\n    /never resample along z separately. Do not touch unless you know what you are doing\n    :return:\n    \"\"\"", "\n", "if", "isinstance", "(", "segmentation_softmax", ",", "str", ")", ":", "\n", "        ", "assert", "isfile", "(", "segmentation_softmax", ")", ",", "\"If isinstance(segmentation_softmax, str) then \"", "\"isfile(segmentation_softmax) must be True\"", "\n", "del_file", "=", "deepcopy", "(", "segmentation_softmax", ")", "\n", "segmentation_softmax", "=", "np", ".", "load", "(", "segmentation_softmax", ")", "\n", "os", ".", "remove", "(", "del_file", ")", "\n", "\n", "# first resample, then put result into bbox of cropping, then save", "\n", "", "current_shape", "=", "segmentation_softmax", ".", "shape", "\n", "shape_original_after_cropping", "=", "dct", ".", "get", "(", "'size_after_cropping'", ")", "\n", "shape_original_before_cropping", "=", "dct", ".", "get", "(", "'original_size_of_raw_data'", ")", "\n", "# current_spacing = dct.get('spacing_after_resampling')", "\n", "# original_spacing = dct.get('original_spacing')", "\n", "\n", "if", "np", ".", "any", "(", "np", ".", "array", "(", "current_shape", ")", "!=", "np", ".", "array", "(", "shape_original_after_cropping", ")", ")", ":", "\n", "        ", "if", "force_separate_z", "is", "None", ":", "\n", "            ", "if", "get_do_separate_z", "(", "dct", ".", "get", "(", "'original_spacing'", ")", ")", ":", "\n", "                ", "do_separate_z", "=", "True", "\n", "lowres_axis", "=", "get_lowres_axis", "(", "dct", ".", "get", "(", "'original_spacing'", ")", ")", "\n", "", "elif", "get_do_separate_z", "(", "dct", ".", "get", "(", "'spacing_after_resampling'", ")", ")", ":", "\n", "                ", "do_separate_z", "=", "True", "\n", "lowres_axis", "=", "get_lowres_axis", "(", "dct", ".", "get", "(", "'spacing_after_resampling'", ")", ")", "\n", "", "else", ":", "\n", "                ", "do_separate_z", "=", "False", "\n", "lowres_axis", "=", "None", "\n", "", "", "else", ":", "\n", "            ", "do_separate_z", "=", "force_separate_z", "\n", "if", "do_separate_z", ":", "\n", "                ", "lowres_axis", "=", "get_lowres_axis", "(", "dct", ".", "get", "(", "'original_spacing'", ")", ")", "\n", "", "else", ":", "\n", "                ", "lowres_axis", "=", "None", "\n", "\n", "", "", "print", "(", "\"separate z:\"", ",", "do_separate_z", ",", "\"lowres axis\"", ",", "lowres_axis", ")", "\n", "seg_old_spacing", "=", "resample_data_or_seg", "(", "segmentation_softmax", ",", "shape_original_after_cropping", ",", "is_seg", "=", "False", ",", "\n", "axis", "=", "lowres_axis", ",", "order", "=", "order", ",", "do_separate_z", "=", "do_separate_z", ",", "cval", "=", "0", ")", "\n", "#seg_old_spacing = resize_softmax_output(segmentation_softmax, shape_original_after_cropping, order=order)", "\n", "", "else", ":", "\n", "        ", "seg_old_spacing", "=", "segmentation_softmax", "\n", "\n", "", "if", "resampled_npz_fname", "is", "not", "None", ":", "\n", "        ", "np", ".", "savez_compressed", "(", "resampled_npz_fname", ",", "softmax", "=", "seg_old_spacing", ".", "astype", "(", "np", ".", "float16", ")", ")", "\n", "save_pickle", "(", "dct", ",", "resampled_npz_fname", "[", ":", "-", "4", "]", "+", "\".pkl\"", ")", "\n", "\n", "", "if", "region_class_order", "is", "None", ":", "\n", "        ", "seg_old_spacing", "=", "seg_old_spacing", ".", "argmax", "(", "0", ")", "\n", "", "else", ":", "\n", "        ", "seg_old_spacing_final", "=", "np", ".", "zeros", "(", "seg_old_spacing", ".", "shape", "[", "1", ":", "]", ")", "\n", "for", "i", ",", "c", "in", "enumerate", "(", "region_class_order", ")", ":", "\n", "            ", "seg_old_spacing_final", "[", "seg_old_spacing", "[", "i", "]", ">", "0.5", "]", "=", "c", "\n", "", "seg_old_spacing", "=", "seg_old_spacing_final", "\n", "\n", "", "bbox", "=", "dct", ".", "get", "(", "'crop_bbox'", ")", "\n", "\n", "if", "bbox", "is", "not", "None", ":", "\n", "        ", "seg_old_size", "=", "np", ".", "zeros", "(", "shape_original_before_cropping", ")", "\n", "for", "c", "in", "range", "(", "3", ")", ":", "\n", "            ", "bbox", "[", "c", "]", "[", "1", "]", "=", "np", ".", "min", "(", "(", "bbox", "[", "c", "]", "[", "0", "]", "+", "seg_old_spacing", ".", "shape", "[", "c", "]", ",", "shape_original_before_cropping", "[", "c", "]", ")", ")", "\n", "", "seg_old_size", "[", "bbox", "[", "0", "]", "[", "0", "]", ":", "bbox", "[", "0", "]", "[", "1", "]", ",", "\n", "bbox", "[", "1", "]", "[", "0", "]", ":", "bbox", "[", "1", "]", "[", "1", "]", ",", "\n", "bbox", "[", "2", "]", "[", "0", "]", ":", "bbox", "[", "2", "]", "[", "1", "]", "]", "=", "seg_old_spacing", "\n", "", "else", ":", "\n", "        ", "seg_old_size", "=", "seg_old_spacing", "\n", "\n", "", "if", "seg_postprogess_fn", "is", "not", "None", ":", "\n", "        ", "seg_old_size_postprocessed", "=", "seg_postprogess_fn", "(", "np", ".", "copy", "(", "seg_old_size", ")", ",", "*", "seg_postprocess_args", ")", "\n", "", "else", ":", "\n", "        ", "seg_old_size_postprocessed", "=", "seg_old_size", "\n", "\n", "", "seg_resized_itk", "=", "sitk", ".", "GetImageFromArray", "(", "seg_old_size_postprocessed", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "seg_resized_itk", ".", "SetSpacing", "(", "dct", "[", "'itk_spacing'", "]", ")", "\n", "seg_resized_itk", ".", "SetOrigin", "(", "dct", "[", "'itk_origin'", "]", ")", "\n", "seg_resized_itk", ".", "SetDirection", "(", "dct", "[", "'itk_direction'", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "seg_resized_itk", ",", "out_fname", ")", "\n", "\n", "if", "(", "non_postprocessed_fname", "is", "not", "None", ")", "and", "(", "seg_postprogess_fn", "is", "not", "None", ")", ":", "\n", "        ", "seg_resized_itk", "=", "sitk", ".", "GetImageFromArray", "(", "seg_old_size", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "seg_resized_itk", ".", "SetSpacing", "(", "dct", "[", "'itk_spacing'", "]", ")", "\n", "seg_resized_itk", ".", "SetOrigin", "(", "dct", "[", "'itk_origin'", "]", ")", "\n", "seg_resized_itk", ".", "SetDirection", "(", "dct", "[", "'itk_direction'", "]", ")", "\n", "sitk", ".", "WriteImage", "(", "seg_resized_itk", ",", "non_postprocessed_fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.ensemble_predictions.merge_files": [[21, 29], ["numpy.vstack", "numpy.mean", "load_pickle", "nnunet.inference.segmentation_export.save_segmentation_nifti_from_softmax", "isfile", "numpy.load"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.segmentation_export.save_segmentation_nifti_from_softmax"], ["def", "merge_files", "(", "args", ")", ":", "\n", "    ", "files", ",", "properties_file", ",", "out_file", ",", "only_keep_largest_connected_component", ",", "min_region_size_per_class", ",", "override", "=", "args", "\n", "if", "override", "or", "not", "isfile", "(", "out_file", ")", ":", "\n", "        ", "softmax", "=", "[", "np", ".", "load", "(", "f", ")", "[", "'softmax'", "]", "[", "None", "]", "for", "f", "in", "files", "]", "\n", "softmax", "=", "np", ".", "vstack", "(", "softmax", ")", "\n", "softmax", "=", "np", ".", "mean", "(", "softmax", ",", "0", ")", "\n", "props", "=", "load_pickle", "(", "properties_file", ")", "\n", "save_segmentation_nifti_from_softmax", "(", "softmax", ",", "out_file", ",", "props", ",", "1", ",", "None", ",", "None", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.ensemble_predictions.merge": [[31, 62], ["maybe_mkdir_p", "numpy.unique", "load_pickle", "multiprocessing.Pool", "multiprocessing.Pool.map", "multiprocessing.Pool.close", "multiprocessing.Pool.join", "subfiles", "all", "all", "files.append", "property_files.append", "out_files.append", "join", "zip", "join", "join", "isfile", "isfile", "join", "len", "len", "len", "join", "join"], "function", ["None"], ["", "", "def", "merge", "(", "folders", ",", "output_folder", ",", "threads", ",", "override", "=", "True", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "output_folder", ")", "\n", "\n", "patient_ids", "=", "[", "subfiles", "(", "i", ",", "suffix", "=", "\".npz\"", ",", "join", "=", "False", ")", "for", "i", "in", "folders", "]", "\n", "patient_ids", "=", "[", "i", "for", "j", "in", "patient_ids", "for", "i", "in", "j", "]", "\n", "patient_ids", "=", "[", "i", "[", ":", "-", "4", "]", "for", "i", "in", "patient_ids", "]", "\n", "patient_ids", "=", "np", ".", "unique", "(", "patient_ids", ")", "\n", "\n", "for", "f", "in", "folders", ":", "\n", "        ", "assert", "all", "(", "[", "isfile", "(", "join", "(", "f", ",", "i", "+", "\".npz\"", ")", ")", "for", "i", "in", "patient_ids", "]", ")", ",", "\"Not all patient npz are available in \"", "\"all folders\"", "\n", "assert", "all", "(", "[", "isfile", "(", "join", "(", "f", ",", "i", "+", "\".pkl\"", ")", ")", "for", "i", "in", "patient_ids", "]", ")", ",", "\"Not all patient pkl are available in \"", "\"all folders\"", "\n", "\n", "", "files", "=", "[", "]", "\n", "property_files", "=", "[", "]", "\n", "out_files", "=", "[", "]", "\n", "for", "p", "in", "patient_ids", ":", "\n", "        ", "files", ".", "append", "(", "[", "join", "(", "f", ",", "p", "+", "\".npz\"", ")", "for", "f", "in", "folders", "]", ")", "\n", "property_files", ".", "append", "(", "join", "(", "folders", "[", "0", "]", ",", "p", "+", "\".pkl\"", ")", ")", "\n", "out_files", ".", "append", "(", "join", "(", "output_folder", ",", "p", "+", "\".nii.gz\"", ")", ")", "\n", "\n", "", "plans", "=", "load_pickle", "(", "join", "(", "folders", "[", "0", "]", ",", "\"plans.pkl\"", ")", ")", "\n", "\n", "only_keep_largest_connected_component", ",", "min_region_size_per_class", "=", "plans", "[", "'keep_only_largest_region'", "]", ",", "plans", "[", "'min_region_size_per_class'", "]", "\n", "p", "=", "Pool", "(", "threads", ")", "\n", "p", ".", "map", "(", "merge_files", ",", "zip", "(", "files", ",", "property_files", ",", "out_files", ",", "[", "only_keep_largest_connected_component", "]", "*", "len", "(", "out_files", ")", ",", "\n", "[", "min_region_size_per_class", "]", "*", "len", "(", "out_files", ")", ",", "[", "override", "]", "*", "len", "(", "out_files", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.__init__": [[27, 40], ["metrics.ConfusionMatrix.set_reference", "metrics.ConfusionMatrix.set_test"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test"], ["    ", "def", "__init__", "(", "self", ",", "test", "=", "None", ",", "reference", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "tp", "=", "None", "\n", "self", ".", "fp", "=", "None", "\n", "self", ".", "tn", "=", "None", "\n", "self", ".", "fn", "=", "None", "\n", "self", ".", "size", "=", "None", "\n", "self", ".", "reference_empty", "=", "None", "\n", "self", ".", "reference_full", "=", "None", "\n", "self", ".", "test_empty", "=", "None", "\n", "self", ".", "test_full", "=", "None", "\n", "self", ".", "set_reference", "(", "reference", ")", "\n", "self", ".", "set_test", "(", "test", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.set_test": [[41, 45], ["metrics.ConfusionMatrix.reset"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.reset"], ["", "def", "set_test", "(", "self", ",", "test", ")", ":", "\n", "\n", "        ", "self", ".", "test", "=", "test", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.set_reference": [[46, 50], ["metrics.ConfusionMatrix.reset"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.reset"], ["", "def", "set_reference", "(", "self", ",", "reference", ")", ":", "\n", "\n", "        ", "self", ".", "reference", "=", "reference", "\n", "self", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.reset": [[51, 62], ["None"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\n", "        ", "self", ".", "tp", "=", "None", "\n", "self", ".", "fp", "=", "None", "\n", "self", ".", "tn", "=", "None", "\n", "self", ".", "fn", "=", "None", "\n", "self", ".", "size", "=", "None", "\n", "self", ".", "test_empty", "=", "None", "\n", "self", ".", "test_full", "=", "None", "\n", "self", ".", "reference_empty", "=", "None", "\n", "self", ".", "reference_full", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.compute": [[63, 79], ["metrics.assert_shape", "int", "int", "int", "int", "int", "numpy.all", "numpy.all", "ValueError", "numpy.product", "numpy.any", "numpy.any"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.assert_shape"], ["", "def", "compute", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "test", "is", "None", "or", "self", ".", "reference", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"'test' and 'reference' must both be set to compute confusion matrix.\"", ")", "\n", "\n", "", "assert_shape", "(", "self", ".", "test", ",", "self", ".", "reference", ")", "\n", "\n", "self", ".", "tp", "=", "int", "(", "(", "(", "self", ".", "test", "!=", "0", ")", "*", "(", "self", ".", "reference", "!=", "0", ")", ")", ".", "sum", "(", ")", ")", "\n", "self", ".", "fp", "=", "int", "(", "(", "(", "self", ".", "test", "!=", "0", ")", "*", "(", "self", ".", "reference", "==", "0", ")", ")", ".", "sum", "(", ")", ")", "\n", "self", ".", "tn", "=", "int", "(", "(", "(", "self", ".", "test", "==", "0", ")", "*", "(", "self", ".", "reference", "==", "0", ")", ")", ".", "sum", "(", ")", ")", "\n", "self", ".", "fn", "=", "int", "(", "(", "(", "self", ".", "test", "==", "0", ")", "*", "(", "self", ".", "reference", "!=", "0", ")", ")", ".", "sum", "(", ")", ")", "\n", "self", ".", "size", "=", "int", "(", "np", ".", "product", "(", "self", ".", "reference", ".", "shape", ")", ")", "\n", "self", ".", "test_empty", "=", "not", "np", ".", "any", "(", "self", ".", "test", ")", "\n", "self", ".", "test_full", "=", "np", ".", "all", "(", "self", ".", "test", ")", "\n", "self", ".", "reference_empty", "=", "not", "np", ".", "any", "(", "self", ".", "reference", ")", "\n", "self", ".", "reference_full", "=", "np", ".", "all", "(", "self", ".", "reference", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix": [[80, 88], ["metrics.ConfusionMatrix.compute"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.compute"], ["", "def", "get_matrix", "(", "self", ")", ":", "\n", "\n", "        ", "for", "entry", "in", "(", "self", ".", "tp", ",", "self", ".", "fp", ",", "self", ".", "tn", ",", "self", ".", "fn", ")", ":", "\n", "            ", "if", "entry", "is", "None", ":", "\n", "                ", "self", ".", "compute", "(", ")", "\n", "break", "\n", "\n", "", "", "return", "self", ".", "tp", ",", "self", ".", "fp", ",", "self", ".", "tn", ",", "self", ".", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_size": [[89, 94], ["metrics.ConfusionMatrix.compute"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.compute"], ["", "def", "get_size", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "size", "is", "None", ":", "\n", "            ", "self", ".", "compute", "(", ")", "\n", "", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence": [[95, 103], ["metrics.ConfusionMatrix.compute"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.compute"], ["", "def", "get_existence", "(", "self", ")", ":", "\n", "\n", "        ", "for", "case", "in", "(", "self", ".", "test_empty", ",", "self", ".", "test_full", ",", "self", ".", "reference_empty", ",", "self", ".", "reference_full", ")", ":", "\n", "            ", "if", "case", "is", "None", ":", "\n", "                ", "self", ".", "compute", "(", ")", "\n", "break", "\n", "\n", "", "", "return", "self", ".", "test_empty", ",", "self", ".", "test_full", ",", "self", ".", "reference_empty", ",", "self", ".", "reference_full", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.assert_shape": [[19, 23], ["None"], "function", ["None"], ["def", "assert_shape", "(", "test", ",", "reference", ")", ":", "\n", "\n", "    ", "assert", "test", ".", "shape", "==", "reference", ".", "shape", ",", "\"Shape mismatch: {} and {}\"", ".", "format", "(", "\n", "test", ".", "shape", ",", "reference", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.dice": [[105, 121], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "", "def", "dice", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"2TP / (2TP + FP + FN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "and", "reference_empty", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "2.", "*", "tp", "/", "(", "2", "*", "tp", "+", "fp", "+", "fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.jaccard": [[123, 139], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "jaccard", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP / (TP + FP + FN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "and", "reference_empty", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "tp", "/", "(", "tp", "+", "fp", "+", "fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.precision": [[141, 157], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "precision", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP / (TP + FP)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "tp", "/", "(", "tp", "+", "fp", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.sensitivity": [[159, 175], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "sensitivity", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP / (TP + FN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "reference_empty", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "tp", "/", "(", "tp", "+", "fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.recall": [[177, 181], ["metrics.sensitivity"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.sensitivity"], ["", "def", "recall", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP / (TP + FN)\"\"\"", "\n", "\n", "return", "sensitivity", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.specificity": [[183, 199], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "specificity", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN / (TN + FP)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "tn", "/", "(", "tn", "+", "fp", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.accuracy": [[201, 210], ["metrics.ConfusionMatrix.get_matrix", "float", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "accuracy", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"(TP + TN) / (TP + FP + FN + TN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "float", "(", "(", "tp", "+", "tn", ")", "/", "(", "tp", "+", "fp", "+", "tn", "+", "fn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.fscore": [[212, 220], ["metrics.precision", "metrics.recall"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.precision", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.recall"], ["", "def", "fscore", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "beta", "=", "1.", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"(1 + b^2) * TP / ((1 + b^2) * TP + b^2 * FN + FP)\"\"\"", "\n", "\n", "precision_", "=", "precision", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "recall_", "=", "recall", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n", "return", "(", "1", "+", "beta", "*", "beta", ")", "*", "precision_", "*", "recall_", "/", "(", "(", "beta", "*", "beta", "*", "precision_", ")", "+", "recall_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.false_positive_rate": [[222, 226], ["metrics.specificity"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.specificity"], ["", "def", "false_positive_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"FP / (FP + TN)\"\"\"", "\n", "\n", "return", "1", "-", "specificity", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.false_omission_rate": [[228, 244], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix.get_existence", "float", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "false_omission_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"FN / (TN + FN)\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0.", "\n", "\n", "", "", "return", "float", "(", "fn", "/", "(", "fn", "+", "tn", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.false_negative_rate": [[246, 250], ["metrics.sensitivity"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.sensitivity"], ["", "def", "false_negative_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"FN / (TP + FN)\"\"\"", "\n", "\n", "return", "1", "-", "sensitivity", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.true_negative_rate": [[252, 256], ["metrics.specificity"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.specificity"], ["", "def", "true_negative_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN / (TN + FP)\"\"\"", "\n", "\n", "return", "specificity", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.false_discovery_rate": [[258, 262], ["metrics.precision"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.precision"], ["", "def", "false_discovery_rate", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"FP / (TP + FP)\"\"\"", "\n", "\n", "return", "1", "-", "precision", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.negative_predictive_value": [[264, 268], ["metrics.false_omission_rate"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.false_omission_rate"], ["", "def", "negative_predictive_value", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN / (TN + FN)\"\"\"", "\n", "\n", "return", "1", "-", "false_omission_rate", "(", "test", ",", "reference", ",", "confusion_matrix", ",", "nan_for_nonexisting", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.total_positives_test": [[270, 279], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "total_positives_test", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP + FP\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "tp", "+", "fp", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.total_negatives_test": [[281, 290], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "total_negatives_test", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN + FN\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "tn", "+", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.total_positives_reference": [[292, 301], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "total_positives_reference", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TP + FN\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "tp", "+", "fn", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.total_negatives_reference": [[303, 312], ["metrics.ConfusionMatrix.get_matrix", "metrics.ConfusionMatrix"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_matrix"], ["", "def", "total_negatives_reference", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "**", "kwargs", ")", ":", "\n", "    ", "\"\"\"TN + FP\"\"\"", "\n", "\n", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "tp", ",", "fp", ",", "tn", ",", "fn", "=", "confusion_matrix", ".", "get_matrix", "(", ")", "\n", "\n", "return", "tn", "+", "fp", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.hausdorff_distance": [[314, 330], ["metrics.ConfusionMatrix.get_existence", "medpy.metric.hd", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "hausdorff_distance", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "voxel_spacing", "=", "None", ",", "connectivity", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "\n", "    ", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "or", "test_full", "or", "reference_empty", "or", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n", "", "", "test", ",", "reference", "=", "confusion_matrix", ".", "test", ",", "confusion_matrix", ".", "reference", "\n", "\n", "return", "metric", ".", "hd", "(", "test", ",", "reference", ",", "voxel_spacing", ",", "connectivity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.hausdorff_distance_95": [[332, 348], ["metrics.ConfusionMatrix.get_existence", "medpy.metric.hd95", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "hausdorff_distance_95", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "voxel_spacing", "=", "None", ",", "connectivity", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "\n", "    ", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "or", "test_full", "or", "reference_empty", "or", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n", "", "", "test", ",", "reference", "=", "confusion_matrix", ".", "test", ",", "confusion_matrix", ".", "reference", "\n", "\n", "return", "metric", ".", "hd95", "(", "test", ",", "reference", ",", "voxel_spacing", ",", "connectivity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.avg_surface_distance": [[350, 366], ["metrics.ConfusionMatrix.get_existence", "medpy.metric.asd", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "avg_surface_distance", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "voxel_spacing", "=", "None", ",", "connectivity", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "\n", "    ", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "or", "test_full", "or", "reference_empty", "or", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n", "", "", "test", ",", "reference", "=", "confusion_matrix", ".", "test", ",", "confusion_matrix", ".", "reference", "\n", "\n", "return", "metric", ".", "asd", "(", "test", ",", "reference", ",", "voxel_spacing", ",", "connectivity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.avg_surface_distance_symmetric": [[368, 384], ["metrics.ConfusionMatrix.get_existence", "medpy.metric.assd", "metrics.ConfusionMatrix", "float"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.metrics.ConfusionMatrix.get_existence"], ["", "def", "avg_surface_distance_symmetric", "(", "test", "=", "None", ",", "reference", "=", "None", ",", "confusion_matrix", "=", "None", ",", "nan_for_nonexisting", "=", "True", ",", "voxel_spacing", "=", "None", ",", "connectivity", "=", "1", ",", "**", "kwargs", ")", ":", "\n", "\n", "    ", "if", "confusion_matrix", "is", "None", ":", "\n", "        ", "confusion_matrix", "=", "ConfusionMatrix", "(", "test", ",", "reference", ")", "\n", "\n", "", "test_empty", ",", "test_full", ",", "reference_empty", ",", "reference_full", "=", "confusion_matrix", ".", "get_existence", "(", ")", "\n", "\n", "if", "test_empty", "or", "test_full", "or", "reference_empty", "or", "reference_full", ":", "\n", "        ", "if", "nan_for_nonexisting", ":", "\n", "            ", "return", "float", "(", "\"NaN\"", ")", "\n", "", "else", ":", "\n", "            ", "return", "0", "\n", "\n", "", "", "test", ",", "reference", "=", "confusion_matrix", ".", "test", ",", "confusion_matrix", ".", "reference", "\n", "\n", "return", "metric", ".", "assd", "(", "test", ",", "reference", ",", "voxel_spacing", ",", "connectivity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.collect_results_files.crawl_and_copy": [[20, 41], ["batchgenerators.utilities.file_and_folder_operations.subdirs", "batchgenerators.utilities.file_and_folder_operations.subfiles", "current_folder.find", "collect_results_files.crawl_and_copy", "i.endswith", "shutil.copy", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.collect_results_files.crawl_and_copy"], ["def", "crawl_and_copy", "(", "current_folder", ",", "out_folder", ",", "prefix", "=", "\"fabian_\"", ",", "suffix", "=", "\"ummary.json\"", ")", ":", "\n", "    ", "\"\"\"\n    This script will run recursively through all subfolders of current_folder and copy all files that end with\n    suffix with some automatically generated prefix into out_folder\n    :param current_folder:\n    :param out_folder:\n    :param prefix:\n    :return:\n    \"\"\"", "\n", "s", "=", "subdirs", "(", "current_folder", ",", "join", "=", "False", ")", "\n", "f", "=", "subfiles", "(", "current_folder", ",", "join", "=", "False", ")", "\n", "f", "=", "[", "i", "for", "i", "in", "f", "if", "i", ".", "endswith", "(", "suffix", ")", "]", "\n", "if", "current_folder", ".", "find", "(", "\"fold0\"", ")", "!=", "-", "1", ":", "\n", "        ", "for", "fl", "in", "f", ":", "\n", "            ", "shutil", ".", "copy", "(", "os", ".", "path", ".", "join", "(", "current_folder", ",", "fl", ")", ",", "os", ".", "path", ".", "join", "(", "out_folder", ",", "prefix", "+", "fl", ")", ")", "\n", "", "", "for", "su", "in", "s", ":", "\n", "        ", "if", "prefix", "==", "\"\"", ":", "\n", "            ", "add", "=", "su", "\n", "", "else", ":", "\n", "            ", "add", "=", "\"__\"", "+", "su", "\n", "", "crawl_and_copy", "(", "os", ".", "path", ".", "join", "(", "current_folder", ",", "su", ")", ",", "out_folder", ",", "prefix", "=", "prefix", "+", "add", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.add_mean_dice_to_json.foreground_mean": [[21, 40], ["numpy.array", "[].get", "[].keys", "collections.OrderedDict", "open", "json.load", "[].pop", "numpy.nanmean", "open", "json.dump", "int", "[].keys", "str"], "function", ["None"], ["def", "foreground_mean", "(", "filename", ")", ":", "\n", "    ", "with", "open", "(", "filename", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "res", "=", "json", ".", "load", "(", "f", ")", "\n", "", "class_ids", "=", "np", ".", "array", "(", "[", "int", "(", "i", ")", "for", "i", "in", "res", "[", "'results'", "]", "[", "'mean'", "]", ".", "keys", "(", ")", "if", "(", "i", "!=", "'mean'", ")", "]", ")", "\n", "class_ids", "=", "class_ids", "[", "class_ids", "!=", "0", "]", "\n", "class_ids", "=", "class_ids", "[", "class_ids", "!=", "-", "1", "]", "\n", "class_ids", "=", "class_ids", "[", "class_ids", "!=", "99", "]", "\n", "\n", "tmp", "=", "res", "[", "'results'", "]", "[", "'mean'", "]", ".", "get", "(", "'99'", ")", "\n", "if", "tmp", "is", "not", "None", ":", "\n", "        ", "_", "=", "res", "[", "'results'", "]", "[", "'mean'", "]", ".", "pop", "(", "'99'", ")", "\n", "\n", "", "metrics", "=", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "'1'", "]", ".", "keys", "(", ")", "\n", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "\"mean\"", "]", "=", "OrderedDict", "(", ")", "\n", "for", "m", "in", "metrics", ":", "\n", "        ", "foreground_values", "=", "[", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "str", "(", "i", ")", "]", "[", "m", "]", "for", "i", "in", "class_ids", "]", "\n", "res", "[", "'results'", "]", "[", "'mean'", "]", "[", "\"mean\"", "]", "[", "m", "]", "=", "np", ".", "nanmean", "(", "foreground_values", ")", "\n", "", "with", "open", "(", "filename", ",", "'w'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "res", ",", "f", ",", "indent", "=", "4", ",", "sort_keys", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.add_mean_dice_to_json.run_in_folder": [[41, 46], ["batchgenerators.utilities.file_and_folder_operations.subfiles", "add_mean_dice_to_json.foreground_mean", "[].startswith", "i.endswith", "i.split"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.add_mean_dice_to_json.foreground_mean"], ["", "", "def", "run_in_folder", "(", "folder", ")", ":", "\n", "    ", "json_files", "=", "subfiles", "(", "folder", ",", "True", ",", "None", ",", "\".json\"", ",", "True", ")", "\n", "json_files", "=", "[", "i", "for", "i", "in", "json_files", "if", "not", "i", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ".", "startswith", "(", "\".\"", ")", "and", "not", "i", ".", "endswith", "(", "\"_globalMean.json\"", ")", "]", "# stupid mac", "\n", "for", "j", "in", "json_files", ":", "\n", "        ", "foreground_mean", "(", "j", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.__init__": [[62, 100], ["nnunet.evaluation.metrics.ConfusionMatrix", "evaluator.Evaluator.set_reference", "evaluator.Evaluator.set_test", "evaluator.Evaluator.set_labels", "evaluator.Evaluator.metrics.append", "evaluator.Evaluator.metrics.append", "evaluator.Evaluator.advanced_metrics.append", "evaluator.Evaluator.advanced_metrics.append", "evaluator.Evaluator.construct_labels"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.set_labels", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.construct_labels"], ["def", "__init__", "(", "self", ",", "\n", "test", "=", "None", ",", "\n", "reference", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "metrics", "=", "None", ",", "\n", "advanced_metrics", "=", "None", ",", "\n", "nan_for_nonexisting", "=", "True", ")", ":", "\n", "\n", "        ", "self", ".", "test", "=", "None", "\n", "self", ".", "reference", "=", "None", "\n", "self", ".", "confusion_matrix", "=", "ConfusionMatrix", "(", ")", "\n", "self", ".", "labels", "=", "None", "\n", "self", ".", "nan_for_nonexisting", "=", "nan_for_nonexisting", "\n", "self", ".", "result", "=", "None", "\n", "\n", "self", ".", "metrics", "=", "[", "]", "\n", "if", "metrics", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "default_metrics", ":", "\n", "                ", "self", ".", "metrics", ".", "append", "(", "m", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "m", "in", "metrics", ":", "\n", "                ", "self", ".", "metrics", ".", "append", "(", "m", ")", "\n", "\n", "", "", "self", ".", "advanced_metrics", "=", "[", "]", "\n", "if", "advanced_metrics", "is", "None", ":", "\n", "            ", "for", "m", "in", "self", ".", "default_advanced_metrics", ":", "\n", "                ", "self", ".", "advanced_metrics", ".", "append", "(", "m", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "m", "in", "advanced_metrics", ":", "\n", "                ", "self", ".", "advanced_metrics", ".", "append", "(", "m", ")", "\n", "\n", "", "", "self", ".", "set_reference", "(", "reference", ")", "\n", "self", ".", "set_test", "(", "test", ")", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_labels", "(", "labels", ")", "\n", "", "else", ":", "\n", "            ", "if", "test", "is", "not", "None", "and", "reference", "is", "not", "None", ":", "\n", "                ", "self", ".", "construct_labels", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.set_test": [[101, 105], ["None"], "methods", ["None"], ["", "", "", "def", "set_test", "(", "self", ",", "test", ")", ":", "\n", "        ", "\"\"\"Set the test segmentation.\"\"\"", "\n", "\n", "self", ".", "test", "=", "test", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.set_reference": [[106, 110], ["None"], "methods", ["None"], ["", "def", "set_reference", "(", "self", ",", "reference", ")", ":", "\n", "        ", "\"\"\"Set the reference segmentation.\"\"\"", "\n", "\n", "self", ".", "reference", "=", "reference", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.set_labels": [[111, 126], ["isinstance", "collections.OrderedDict", "isinstance", "list", "isinstance", "isinstance", "TypeError", "type"], "methods", ["None"], ["", "def", "set_labels", "(", "self", ",", "labels", ")", ":", "\n", "        ", "\"\"\"Set the labels.\n        :param labels= may be a dictionary (int->str), a set (of ints), a tuple (of ints) or a list (of ints). Labels\n        will only have names if you pass a dictionary\"\"\"", "\n", "\n", "if", "isinstance", "(", "labels", ",", "dict", ")", ":", "\n", "            ", "self", ".", "labels", "=", "collections", ".", "OrderedDict", "(", "labels", ")", "\n", "", "elif", "isinstance", "(", "labels", ",", "set", ")", ":", "\n", "            ", "self", ".", "labels", "=", "list", "(", "labels", ")", "\n", "", "elif", "isinstance", "(", "labels", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "self", ".", "labels", "=", "[", "i", "for", "i", "in", "labels", "]", "\n", "", "elif", "isinstance", "(", "labels", ",", "(", "list", ",", "tuple", ")", ")", ":", "\n", "            ", "self", ".", "labels", "=", "labels", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "\"Can only handle dict, list, tuple, set & numpy array, but input is of type {}\"", ".", "format", "(", "type", "(", "labels", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.construct_labels": [[127, 138], ["list", "ValueError", "map", "numpy.unique", "numpy.union1d", "numpy.unique", "numpy.unique", "int"], "methods", ["None"], ["", "", "def", "construct_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Construct label set from unique entries in segmentations.\"\"\"", "\n", "\n", "if", "self", ".", "test", "is", "None", "and", "self", ".", "reference", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"No test or reference segmentations.\"", ")", "\n", "", "elif", "self", ".", "test", "is", "None", ":", "\n", "            ", "labels", "=", "np", ".", "unique", "(", "self", ".", "reference", ")", "\n", "", "else", ":", "\n", "            ", "labels", "=", "np", ".", "union1d", "(", "np", ".", "unique", "(", "self", ".", "test", ")", ",", "\n", "np", ".", "unique", "(", "self", ".", "reference", ")", ")", "\n", "", "self", ".", "labels", "=", "list", "(", "map", "(", "lambda", "x", ":", "int", "(", "x", ")", ",", "labels", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.set_metrics": [[139, 148], ["isinstance", "list", "isinstance", "TypeError", "type"], "methods", ["None"], ["", "def", "set_metrics", "(", "self", ",", "metrics", ")", ":", "\n", "        ", "\"\"\"Set evaluation metrics\"\"\"", "\n", "\n", "if", "isinstance", "(", "metrics", ",", "set", ")", ":", "\n", "            ", "self", ".", "metrics", "=", "list", "(", "metrics", ")", "\n", "", "elif", "isinstance", "(", "metrics", ",", "(", "list", ",", "tuple", ",", "np", ".", "ndarray", ")", ")", ":", "\n", "            ", "self", ".", "metrics", "=", "metrics", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "\"Can only handle list, tuple, set & numpy array, but input is of type {}\"", ".", "format", "(", "type", "(", "metrics", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.add_metric": [[149, 153], ["evaluator.Evaluator.metrics.append"], "methods", ["None"], ["", "", "def", "add_metric", "(", "self", ",", "metric", ")", ":", "\n", "\n", "        ", "if", "metric", "not", "in", "self", ".", "metrics", ":", "\n", "            ", "self", ".", "metrics", ".", "append", "(", "metric", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.evaluate": [[154, 228], ["evaluator.Evaluator.metrics.sort", "inspect.getouterframes", "collections.OrderedDict", "isinstance", "evaluator.Evaluator.set_test", "evaluator.Evaluator.set_reference", "ValueError", "evaluator.Evaluator.construct_labels", "inspect.currentframe", "evaluator.Evaluator.labels.items", "enumerate", "str", "collections.OrderedDict", "str", "collections.OrderedDict", "evaluator.Evaluator.confusion_matrix.set_test", "evaluator.Evaluator.confusion_matrix.set_reference", "NotImplementedError", "hasattr", "evaluator.Evaluator.confusion_matrix.set_test", "evaluator.Evaluator.confusion_matrix.set_reference", "evaluator.Evaluator.confusion_matrix.set_test", "evaluator.Evaluator.confusion_matrix.set_reference"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.construct_labels", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference"], ["", "", "def", "evaluate", "(", "self", ",", "test", "=", "None", ",", "reference", "=", "None", ",", "advanced", "=", "False", ",", "**", "metric_kwargs", ")", ":", "\n", "        ", "\"\"\"Compute metrics for segmentations.\"\"\"", "\n", "if", "test", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_test", "(", "test", ")", "\n", "\n", "", "if", "reference", "is", "not", "None", ":", "\n", "            ", "self", ".", "set_reference", "(", "reference", ")", "\n", "\n", "", "if", "self", ".", "test", "is", "None", "or", "self", ".", "reference", "is", "None", ":", "\n", "            ", "raise", "ValueError", "(", "\"Need both test and reference segmentations.\"", ")", "\n", "\n", "", "if", "self", ".", "labels", "is", "None", ":", "\n", "            ", "self", ".", "construct_labels", "(", ")", "\n", "\n", "", "self", ".", "metrics", ".", "sort", "(", ")", "\n", "\n", "# get functions for evaluation", "\n", "# somewhat convoluted, but allows users to define additonal metrics", "\n", "# on the fly, e.g. inside an IPython console", "\n", "_funcs", "=", "{", "m", ":", "ALL_METRICS", "[", "m", "]", "for", "m", "in", "self", ".", "metrics", "+", "self", ".", "advanced_metrics", "}", "\n", "frames", "=", "inspect", ".", "getouterframes", "(", "inspect", ".", "currentframe", "(", ")", ")", "\n", "for", "metric", "in", "self", ".", "metrics", ":", "\n", "            ", "for", "f", "in", "frames", ":", "\n", "                ", "if", "metric", "in", "f", "[", "0", "]", ".", "f_locals", ":", "\n", "                    ", "_funcs", "[", "metric", "]", "=", "f", "[", "0", "]", ".", "f_locals", "[", "metric", "]", "\n", "break", "\n", "", "", "else", ":", "\n", "                ", "if", "metric", "in", "_funcs", ":", "\n", "                    ", "continue", "\n", "", "else", ":", "\n", "                    ", "raise", "NotImplementedError", "(", "\n", "\"Metric {} not implemented.\"", ".", "format", "(", "metric", ")", ")", "\n", "\n", "# get results", "\n", "", "", "", "self", ".", "result", "=", "OrderedDict", "(", ")", "\n", "\n", "eval_metrics", "=", "self", ".", "metrics", "\n", "if", "advanced", ":", "\n", "            ", "eval_metrics", "+=", "self", ".", "advanced_metrics", "\n", "\n", "", "if", "isinstance", "(", "self", ".", "labels", ",", "dict", ")", ":", "\n", "\n", "            ", "for", "label", ",", "name", "in", "self", ".", "labels", ".", "items", "(", ")", ":", "\n", "                ", "k", "=", "str", "(", "name", ")", "\n", "self", ".", "result", "[", "k", "]", "=", "OrderedDict", "(", ")", "\n", "if", "not", "hasattr", "(", "label", ",", "\"__iter__\"", ")", ":", "\n", "                    ", "self", ".", "confusion_matrix", ".", "set_test", "(", "self", ".", "test", "==", "label", ")", "\n", "self", ".", "confusion_matrix", ".", "set_reference", "(", "self", ".", "reference", "==", "label", ")", "\n", "", "else", ":", "\n", "                    ", "current_test", "=", "0", "\n", "current_reference", "=", "0", "\n", "for", "l", "in", "label", ":", "\n", "                        ", "current_test", "+=", "(", "self", ".", "test", "==", "l", ")", "\n", "current_reference", "+=", "(", "self", ".", "reference", "==", "l", ")", "\n", "", "self", ".", "confusion_matrix", ".", "set_test", "(", "current_test", ")", "\n", "self", ".", "confusion_matrix", ".", "set_reference", "(", "current_reference", ")", "\n", "", "for", "metric", "in", "eval_metrics", ":", "\n", "                    ", "self", ".", "result", "[", "k", "]", "[", "metric", "]", "=", "_funcs", "[", "metric", "]", "(", "confusion_matrix", "=", "self", ".", "confusion_matrix", ",", "\n", "nan_for_nonexisting", "=", "self", ".", "nan_for_nonexisting", ",", "\n", "**", "metric_kwargs", ")", "\n", "\n", "", "", "", "else", ":", "\n", "\n", "            ", "for", "i", ",", "l", "in", "enumerate", "(", "self", ".", "labels", ")", ":", "\n", "                ", "k", "=", "str", "(", "l", ")", "\n", "self", ".", "result", "[", "k", "]", "=", "OrderedDict", "(", ")", "\n", "self", ".", "confusion_matrix", ".", "set_test", "(", "self", ".", "test", "==", "l", ")", "\n", "self", ".", "confusion_matrix", ".", "set_reference", "(", "self", ".", "reference", "==", "l", ")", "\n", "for", "metric", "in", "eval_metrics", ":", "\n", "                    ", "self", ".", "result", "[", "k", "]", "[", "metric", "]", "=", "_funcs", "[", "metric", "]", "(", "confusion_matrix", "=", "self", ".", "confusion_matrix", ",", "\n", "nan_for_nonexisting", "=", "self", ".", "nan_for_nonexisting", ",", "\n", "**", "metric_kwargs", ")", "\n", "\n", "", "", "", "return", "self", ".", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.to_dict": [[229, 234], ["evaluator.Evaluator.evaluate"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.evaluate"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "\n", "        ", "if", "self", ".", "result", "is", "None", ":", "\n", "            ", "self", ".", "evaluate", "(", ")", "\n", "", "return", "self", ".", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.to_array": [[235, 255], ["sorted", "numpy.zeros", "isinstance", "evaluator.Evaluator.result[].keys", "enumerate", "enumerate", "len", "len", "evaluator.Evaluator.labels.keys", "enumerate", "enumerate", "list", "evaluator.Evaluator.result.keys"], "methods", ["None"], ["", "def", "to_array", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return result as numpy array (labels x metrics).\"\"\"", "\n", "\n", "if", "self", ".", "result", "is", "None", ":", "\n", "            ", "self", ".", "evaluate", "\n", "\n", "", "result_metrics", "=", "sorted", "(", "self", ".", "result", "[", "list", "(", "self", ".", "result", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ".", "keys", "(", ")", ")", "\n", "\n", "a", "=", "np", ".", "zeros", "(", "(", "len", "(", "self", ".", "labels", ")", ",", "len", "(", "result_metrics", ")", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "labels", ",", "dict", ")", ":", "\n", "            ", "for", "i", ",", "label", "in", "enumerate", "(", "self", ".", "labels", ".", "keys", "(", ")", ")", ":", "\n", "                ", "for", "j", ",", "metric", "in", "enumerate", "(", "result_metrics", ")", ":", "\n", "                    ", "a", "[", "i", "]", "[", "j", "]", "=", "self", ".", "result", "[", "self", ".", "labels", "[", "label", "]", "]", "[", "metric", "]", "\n", "", "", "", "else", ":", "\n", "            ", "for", "i", ",", "label", "in", "enumerate", "(", "self", ".", "labels", ")", ":", "\n", "                ", "for", "j", ",", "metric", "in", "enumerate", "(", "result_metrics", ")", ":", "\n", "                    ", "a", "[", "i", "]", "[", "j", "]", "=", "self", ".", "result", "[", "label", "]", "[", "metric", "]", "\n", "\n", "", "", "", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.to_pandas": [[256, 269], ["evaluator.Evaluator.to_array", "isinstance", "sorted", "pandas.DataFrame", "list", "evaluator.Evaluator.result[].keys", "evaluator.Evaluator.labels.values", "list", "evaluator.Evaluator.result.keys"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.to_array"], ["", "def", "to_pandas", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return result as pandas DataFrame.\"\"\"", "\n", "\n", "a", "=", "self", ".", "to_array", "(", ")", "\n", "\n", "if", "isinstance", "(", "self", ".", "labels", ",", "dict", ")", ":", "\n", "            ", "labels", "=", "list", "(", "self", ".", "labels", ".", "values", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "labels", "=", "self", ".", "labels", "\n", "\n", "", "result_metrics", "=", "sorted", "(", "self", ".", "result", "[", "list", "(", "self", ".", "result", ".", "keys", "(", ")", ")", "[", "0", "]", "]", ".", "keys", "(", ")", ")", "\n", "\n", "return", "pd", ".", "DataFrame", "(", "a", ",", "index", "=", "labels", ",", "columns", "=", "result_metrics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__": [[273, 278], ["evaluator.Evaluator.__init__"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "self", ".", "test_nifti", "=", "None", "\n", "self", ".", "reference_nifti", "=", "None", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "__init__", "(", "*", "args", ",", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test": [[279, 288], ["SimpleITK.ReadImage", "evaluator.Evaluator.set_test", "evaluator.Evaluator.set_test", "SimpleITK.GetArrayFromImage"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test"], ["", "def", "set_test", "(", "self", ",", "test", ")", ":", "\n", "        ", "\"\"\"Set the test segmentation.\"\"\"", "\n", "\n", "if", "test", "is", "not", "None", ":", "\n", "            ", "self", ".", "test_nifti", "=", "sitk", ".", "ReadImage", "(", "test", ")", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "set_test", "(", "sitk", ".", "GetArrayFromImage", "(", "self", ".", "test_nifti", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "test_nifti", "=", "None", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "set_test", "(", "test", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference": [[289, 298], ["SimpleITK.ReadImage", "evaluator.Evaluator.set_reference", "evaluator.Evaluator.set_reference", "SimpleITK.GetArrayFromImage"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference"], ["", "", "def", "set_reference", "(", "self", ",", "reference", ")", ":", "\n", "        ", "\"\"\"Set the reference segmentation.\"\"\"", "\n", "\n", "if", "reference", "is", "not", "None", ":", "\n", "            ", "self", ".", "reference_nifti", "=", "sitk", ".", "ReadImage", "(", "reference", ")", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "set_reference", "(", "sitk", ".", "GetArrayFromImage", "(", "self", ".", "reference_nifti", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "reference_nifti", "=", "None", "\n", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "set_reference", "(", "reference", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.evaluate": [[299, 306], ["evaluator.Evaluator.evaluate", "numpy.array", "evaluator.NiftiEvaluator.test_nifti.GetSpacing"], "methods", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.evaluate"], ["", "", "def", "evaluate", "(", "self", ",", "test", "=", "None", ",", "reference", "=", "None", ",", "voxel_spacing", "=", "None", ",", "**", "metric_kwargs", ")", ":", "\n", "\n", "        ", "if", "voxel_spacing", "is", "None", ":", "\n", "            ", "voxel_spacing", "=", "np", ".", "array", "(", "self", ".", "test_nifti", ".", "GetSpacing", "(", ")", ")", "[", ":", ":", "-", "1", "]", "\n", "metric_kwargs", "[", "\"voxel_spacing\"", "]", "=", "voxel_spacing", "\n", "\n", "", "return", "super", "(", "NiftiEvaluator", ",", "self", ")", ".", "evaluate", "(", "test", ",", "reference", ",", "**", "metric_kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.run_evaluation": [[308, 321], ["evaluator.set_test", "evaluator.set_reference", "evaluator.evaluate", "evaluator.construct_labels", "type", "type"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_test", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.set_reference", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.NiftiEvaluator.evaluate", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.construct_labels"], ["", "", "def", "run_evaluation", "(", "args", ")", ":", "\n", "    ", "test", ",", "ref", ",", "evaluator", ",", "metric_kwargs", "=", "args", "\n", "# evaluate", "\n", "evaluator", ".", "set_test", "(", "test", ")", "\n", "evaluator", ".", "set_reference", "(", "ref", ")", "\n", "if", "evaluator", ".", "labels", "is", "None", ":", "\n", "        ", "evaluator", ".", "construct_labels", "(", ")", "\n", "", "current_scores", "=", "evaluator", ".", "evaluate", "(", "**", "metric_kwargs", ")", "\n", "if", "type", "(", "test", ")", "==", "str", ":", "\n", "        ", "current_scores", "[", "\"test\"", "]", "=", "test", "\n", "", "if", "type", "(", "ref", ")", "==", "str", ":", "\n", "        ", "current_scores", "[", "\"reference\"", "]", "=", "ref", "\n", "", "return", "current_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.aggregate_scores": [[323, 403], ["collections.OrderedDict", "collections.OrderedDict", "multiprocessing.pool.Pool", "multiprocessing.pool.Pool.map", "multiprocessing.pool.Pool.close", "multiprocessing.pool.Pool.join", "range", "type", "evaluator.", "evaluator.set_labels", "zip", "len", "all_scores[].append", "all_res[].items", "collections.OrderedDict", "datetime.datetime.today", "str", "batchgenerators.utilities.file_and_folder_operations.save_json", "score_dict.items", "hashlib.md5().hexdigest", "len", "len", "collections.OrderedDict", "[].append", "float", "float", "numpy.nanmean", "numpy.mean", "hashlib.md5", "json.dumps().encode", "json.dumps"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.Evaluator.set_labels"], ["", "def", "aggregate_scores", "(", "test_ref_pairs", ",", "\n", "evaluator", "=", "NiftiEvaluator", ",", "\n", "labels", "=", "None", ",", "\n", "nanmean", "=", "True", ",", "\n", "json_output_file", "=", "None", ",", "\n", "json_name", "=", "\"\"", ",", "\n", "json_description", "=", "\"\"", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "\n", "json_task", "=", "\"\"", ",", "\n", "num_threads", "=", "2", ",", "\n", "**", "metric_kwargs", ")", ":", "\n", "    ", "\"\"\"\n    test = predicted image\n    :param test_ref_pairs:\n    :param evaluator:\n    :param labels: must be a dict of int-> str or a list of int\n    :param nanmean:\n    :param json_output_file:\n    :param json_name:\n    :param json_description:\n    :param json_author:\n    :param json_task:\n    :param metric_kwargs:\n    :return:\n    \"\"\"", "\n", "\n", "if", "type", "(", "evaluator", ")", "==", "type", ":", "\n", "        ", "evaluator", "=", "evaluator", "(", ")", "\n", "\n", "", "if", "labels", "is", "not", "None", ":", "\n", "        ", "evaluator", ".", "set_labels", "(", "labels", ")", "\n", "\n", "", "all_scores", "=", "OrderedDict", "(", ")", "\n", "all_scores", "[", "\"all\"", "]", "=", "[", "]", "\n", "all_scores", "[", "\"mean\"", "]", "=", "OrderedDict", "(", ")", "\n", "\n", "test", "=", "[", "i", "[", "0", "]", "for", "i", "in", "test_ref_pairs", "]", "\n", "ref", "=", "[", "i", "[", "1", "]", "for", "i", "in", "test_ref_pairs", "]", "\n", "p", "=", "Pool", "(", "num_threads", ")", "\n", "all_res", "=", "p", ".", "map", "(", "run_evaluation", ",", "zip", "(", "test", ",", "ref", ",", "[", "evaluator", "]", "*", "len", "(", "ref", ")", ",", "[", "metric_kwargs", "]", "*", "len", "(", "ref", ")", ")", ")", "\n", "p", ".", "close", "(", ")", "\n", "p", ".", "join", "(", ")", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "all_res", ")", ")", ":", "\n", "        ", "all_scores", "[", "\"all\"", "]", ".", "append", "(", "all_res", "[", "i", "]", ")", "\n", "\n", "# append score list for mean", "\n", "for", "label", ",", "score_dict", "in", "all_res", "[", "i", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "label", "in", "(", "\"test\"", ",", "\"reference\"", ")", ":", "\n", "                ", "continue", "\n", "", "if", "label", "not", "in", "all_scores", "[", "\"mean\"", "]", ":", "\n", "                ", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "=", "OrderedDict", "(", ")", "\n", "", "for", "score", ",", "value", "in", "score_dict", ".", "items", "(", ")", ":", "\n", "                ", "if", "score", "not", "in", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", ":", "\n", "                    ", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", "=", "[", "]", "\n", "", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", ".", "append", "(", "value", ")", "\n", "\n", "", "", "", "for", "label", "in", "all_scores", "[", "\"mean\"", "]", ":", "\n", "        ", "for", "score", "in", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", ":", "\n", "            ", "if", "nanmean", ":", "\n", "                ", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", "=", "float", "(", "np", ".", "nanmean", "(", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", ")", ")", "\n", "", "else", ":", "\n", "                ", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", "=", "float", "(", "np", ".", "mean", "(", "all_scores", "[", "\"mean\"", "]", "[", "label", "]", "[", "score", "]", ")", ")", "\n", "\n", "# save to file if desired", "\n", "# we create a hopefully unique id by hashing the entire output dictionary", "\n", "", "", "", "if", "json_output_file", "is", "not", "None", ":", "\n", "        ", "json_dict", "=", "OrderedDict", "(", ")", "\n", "json_dict", "[", "\"name\"", "]", "=", "json_name", "\n", "json_dict", "[", "\"description\"", "]", "=", "json_description", "\n", "timestamp", "=", "datetime", ".", "today", "(", ")", "\n", "json_dict", "[", "\"timestamp\"", "]", "=", "str", "(", "timestamp", ")", "\n", "json_dict", "[", "\"task\"", "]", "=", "json_task", "\n", "json_dict", "[", "\"author\"", "]", "=", "json_author", "\n", "json_dict", "[", "\"results\"", "]", "=", "all_scores", "\n", "json_dict", "[", "\"id\"", "]", "=", "hashlib", ".", "md5", "(", "json", ".", "dumps", "(", "json_dict", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "hexdigest", "(", ")", "[", ":", "12", "]", "\n", "save_json", "(", "json_dict", ",", "json_output_file", ")", "\n", "\n", "\n", "", "return", "all_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.evaluator.aggregate_scores_for_experiment": [[405, 446], ["numpy.load", "np.load.mean", "collections.OrderedDict", "range", "collections.OrderedDict", "datetime.datetime.today", "str", "list", "results.append", "enumerate", "hashlib.md5().hexdigest", "open", "json.dump", "open.close", "map", "collections.OrderedDict", "collections.OrderedDict", "collections.OrderedDict", "enumerate", "range", "float", "float", "hashlib.md5", "json.dumps().encode", "json.dumps"], "function", ["None"], ["", "def", "aggregate_scores_for_experiment", "(", "score_file", ",", "\n", "labels", "=", "None", ",", "\n", "metrics", "=", "Evaluator", ".", "default_metrics", ",", "\n", "nanmean", "=", "True", ",", "\n", "json_output_file", "=", "None", ",", "\n", "json_name", "=", "\"\"", ",", "\n", "json_description", "=", "\"\"", ",", "\n", "json_author", "=", "\"Fabian\"", ",", "\n", "json_task", "=", "\"\"", ")", ":", "\n", "\n", "    ", "scores", "=", "np", ".", "load", "(", "score_file", ")", "\n", "scores_mean", "=", "scores", ".", "mean", "(", "0", ")", "\n", "if", "labels", "is", "None", ":", "\n", "        ", "labels", "=", "list", "(", "map", "(", "str", ",", "range", "(", "scores", ".", "shape", "[", "1", "]", ")", ")", ")", "\n", "\n", "", "results", "=", "[", "]", "\n", "results_mean", "=", "OrderedDict", "(", ")", "\n", "for", "i", "in", "range", "(", "scores", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "results", ".", "append", "(", "OrderedDict", "(", ")", ")", "\n", "for", "l", ",", "label", "in", "enumerate", "(", "labels", ")", ":", "\n", "            ", "results", "[", "-", "1", "]", "[", "label", "]", "=", "OrderedDict", "(", ")", "\n", "results_mean", "[", "label", "]", "=", "OrderedDict", "(", ")", "\n", "for", "m", ",", "metric", "in", "enumerate", "(", "metrics", ")", ":", "\n", "                ", "results", "[", "-", "1", "]", "[", "label", "]", "[", "metric", "]", "=", "float", "(", "scores", "[", "i", "]", "[", "l", "]", "[", "m", "]", ")", "\n", "results_mean", "[", "label", "]", "[", "metric", "]", "=", "float", "(", "scores_mean", "[", "l", "]", "[", "m", "]", ")", "\n", "\n", "", "", "", "json_dict", "=", "OrderedDict", "(", ")", "\n", "json_dict", "[", "\"name\"", "]", "=", "json_name", "\n", "json_dict", "[", "\"description\"", "]", "=", "json_description", "\n", "timestamp", "=", "datetime", ".", "today", "(", ")", "\n", "json_dict", "[", "\"timestamp\"", "]", "=", "str", "(", "timestamp", ")", "\n", "json_dict", "[", "\"task\"", "]", "=", "json_task", "\n", "json_dict", "[", "\"author\"", "]", "=", "json_author", "\n", "json_dict", "[", "\"results\"", "]", "=", "{", "\"all\"", ":", "results", ",", "\"mean\"", ":", "results_mean", "}", "\n", "json_dict", "[", "\"id\"", "]", "=", "hashlib", ".", "md5", "(", "json", ".", "dumps", "(", "json_dict", ")", ".", "encode", "(", "\"utf-8\"", ")", ")", ".", "hexdigest", "(", ")", "[", ":", "12", "]", "\n", "if", "json_output_file", "is", "not", "None", ":", "\n", "        ", "json_output_file", "=", "open", "(", "json_output_file", ",", "\"w\"", ")", "\n", "json", ".", "dump", "(", "json_dict", ",", "json_output_file", ",", "indent", "=", "4", ",", "separators", "=", "(", "\",\"", ",", "\": \"", ")", ")", "\n", "json_output_file", ".", "close", "(", ")", "\n", "\n", "", "return", "json_dict", "\n", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.model_selection.figure_out_what_to_submit.copy_nifti_and_convert_to_uint8": [[25, 31], ["SimpleITK.ReadImage", "SimpleITK.GetImageFromArray", "sitk.GetImageFromArray.CopyInformation", "SimpleITK.WriteImage", "SimpleITK.GetArrayFromImage().astype", "SimpleITK.GetArrayFromImage"], "function", ["None"], ["def", "copy_nifti_and_convert_to_uint8", "(", "args", ")", ":", "\n", "    ", "source_file", ",", "target_file", "=", "args", "\n", "i", "=", "sitk", ".", "ReadImage", "(", "source_file", ")", "\n", "j", "=", "sitk", ".", "GetImageFromArray", "(", "sitk", ".", "GetArrayFromImage", "(", "i", ")", ".", "astype", "(", "np", ".", "uint8", ")", ")", "\n", "j", ".", "CopyInformation", "(", "i", ")", "\n", "sitk", ".", "WriteImage", "(", "j", ",", "target_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.model_selection.summarize_results_in_one_json.summarize": [[22, 98], ["join", "maybe_mkdir_p", "list", "len", "range", "int", "subfolders", "join", "subdirs", "isdir", "join", "len", "print", "trainer.startswith", "join", "join", "join", "subfolders", "collections.OrderedDict", "isdir", "join", "join", "join", "metrics_tmp.keys", "collections.OrderedDict.keys", "collections.OrderedDict", "collections.OrderedDict", "save_json", "save_json", "nnunet.evaluation.add_mean_dice_to_json.foreground_mean", "nnunet.evaluation.add_mean_dice_to_json.foreground_mean", "isdir", "isdir", "join", "isfile", "print", "metrics_tmp[].keys", "metrics[].keys", "join", "join", "join", "join", "isdir", "join", "load_json", "collections.OrderedDict.get", "collections.OrderedDict", "[].append", "numpy.mean", "join", "metrics[].get", "len", "len"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.add_mean_dice_to_json.foreground_mean", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.evaluation.add_mean_dice_to_json.foreground_mean"], ["def", "summarize", "(", "tasks", ",", "models", "=", "(", "'2d'", ",", "'3d_lowres'", ",", "'3d_fullres'", ",", "'3d_cascade_fullres'", ")", ",", "\n", "output_dir", "=", "join", "(", "network_training_output_dir", ",", "\"summary_jsons\"", ")", ",", "folds", "=", "(", "0", ",", "1", ",", "2", ",", "3", ",", "4", ")", ")", ":", "\n", "    ", "maybe_mkdir_p", "(", "output_dir", ")", "\n", "\n", "if", "len", "(", "tasks", ")", "==", "1", "and", "tasks", "[", "0", "]", "==", "\"all\"", ":", "\n", "        ", "tasks", "=", "list", "(", "range", "(", "100", ")", ")", "\n", "", "else", ":", "\n", "        ", "tasks", "=", "[", "int", "(", "i", ")", "for", "i", "in", "tasks", "]", "\n", "\n", "", "for", "model", "in", "models", ":", "\n", "        ", "for", "t", "in", "tasks", ":", "\n", "            ", "if", "not", "isdir", "(", "join", "(", "network_training_output_dir", ",", "model", ")", ")", ":", "\n", "                ", "continue", "\n", "", "task_name", "=", "subfolders", "(", "join", "(", "network_training_output_dir", ",", "model", ")", ",", "prefix", "=", "\"Task%02.0d\"", "%", "t", ",", "join", "=", "False", ")", "\n", "if", "len", "(", "task_name", ")", "!=", "1", ":", "\n", "                ", "print", "(", "\"did not find unique output folder for network %s and task %s\"", "%", "(", "model", ",", "t", ")", ")", "\n", "continue", "\n", "", "task_name", "=", "task_name", "[", "0", "]", "\n", "out_dir_task", "=", "join", "(", "network_training_output_dir", ",", "model", ",", "task_name", ")", "\n", "\n", "model_trainers", "=", "subdirs", "(", "out_dir_task", ",", "join", "=", "False", ")", "\n", "for", "trainer", "in", "model_trainers", ":", "\n", "                ", "if", "trainer", ".", "startswith", "(", "\"fold\"", ")", ":", "\n", "                    ", "continue", "\n", "", "out_dir", "=", "join", "(", "out_dir_task", ",", "trainer", ")", "\n", "\n", "validation_folders", "=", "[", "]", "\n", "for", "fld", "in", "folds", ":", "\n", "                    ", "d", "=", "join", "(", "out_dir", ",", "\"fold%d\"", "%", "fld", ")", "\n", "if", "not", "isdir", "(", "d", ")", ":", "\n", "                        ", "d", "=", "join", "(", "out_dir", ",", "\"fold_%d\"", "%", "fld", ")", "\n", "if", "not", "isdir", "(", "d", ")", ":", "\n", "                            ", "break", "\n", "", "", "validation_folders", "+=", "subfolders", "(", "d", ",", "prefix", "=", "\"validation\"", ",", "join", "=", "False", ")", "\n", "\n", "", "for", "v", "in", "validation_folders", ":", "\n", "                    ", "ok", "=", "True", "\n", "metrics", "=", "OrderedDict", "(", ")", "\n", "for", "fld", "in", "folds", ":", "\n", "                        ", "d", "=", "join", "(", "out_dir", ",", "\"fold%d\"", "%", "fld", ")", "\n", "if", "not", "isdir", "(", "d", ")", ":", "\n", "                            ", "d", "=", "join", "(", "out_dir", ",", "\"fold_%d\"", "%", "fld", ")", "\n", "if", "not", "isdir", "(", "d", ")", ":", "\n", "                                ", "ok", "=", "False", "\n", "break", "\n", "", "", "validation_folder", "=", "join", "(", "d", ",", "v", ")", "\n", "\n", "if", "not", "isfile", "(", "join", "(", "validation_folder", ",", "\"summary.json\"", ")", ")", ":", "\n", "                            ", "print", "(", "\"summary.json missing for net %s task %s fold %d\"", "%", "(", "model", ",", "task_name", ",", "fld", ")", ")", "\n", "ok", "=", "False", "\n", "break", "\n", "\n", "", "metrics_tmp", "=", "load_json", "(", "join", "(", "validation_folder", ",", "\"summary.json\"", ")", ")", "[", "\"results\"", "]", "[", "\"mean\"", "]", "\n", "for", "l", "in", "metrics_tmp", ".", "keys", "(", ")", ":", "\n", "                            ", "if", "metrics", ".", "get", "(", "l", ")", "is", "None", ":", "\n", "                                ", "metrics", "[", "l", "]", "=", "OrderedDict", "(", ")", "\n", "", "for", "m", "in", "metrics_tmp", "[", "l", "]", ".", "keys", "(", ")", ":", "\n", "                                ", "if", "metrics", "[", "l", "]", ".", "get", "(", "m", ")", "is", "None", ":", "\n", "                                    ", "metrics", "[", "l", "]", "[", "m", "]", "=", "[", "]", "\n", "", "metrics", "[", "l", "]", "[", "m", "]", ".", "append", "(", "metrics_tmp", "[", "l", "]", "[", "m", "]", ")", "\n", "", "", "", "if", "ok", ":", "\n", "                        ", "for", "l", "in", "metrics", ".", "keys", "(", ")", ":", "\n", "                            ", "for", "m", "in", "metrics", "[", "l", "]", ".", "keys", "(", ")", ":", "\n", "                                ", "assert", "len", "(", "metrics", "[", "l", "]", "[", "m", "]", ")", "==", "len", "(", "folds", ")", "\n", "metrics", "[", "l", "]", "[", "m", "]", "=", "np", ".", "mean", "(", "metrics", "[", "l", "]", "[", "m", "]", ")", "\n", "", "", "json_out", "=", "OrderedDict", "(", ")", "\n", "json_out", "[", "\"results\"", "]", "=", "OrderedDict", "(", ")", "\n", "json_out", "[", "\"results\"", "]", "[", "\"mean\"", "]", "=", "metrics", "\n", "json_out", "[", "\"task\"", "]", "=", "task_name", "\n", "json_out", "[", "\"description\"", "]", "=", "model", "+", "\" \"", "+", "task_name", "+", "\" all folds summary\"", "\n", "json_out", "[", "\"name\"", "]", "=", "model", "+", "\" \"", "+", "task_name", "+", "\" all folds summary\"", "\n", "json_out", "[", "\"experiment_name\"", "]", "=", "model", "\n", "save_json", "(", "json_out", ",", "join", "(", "out_dir", ",", "\"summary_allFolds__%s.json\"", "%", "v", ")", ")", "\n", "save_json", "(", "json_out", ",", "join", "(", "output_dir", ",", "\"%s__%s__%s__%s.json\"", "%", "(", "task_name", ",", "model", ",", "trainer", ",", "v", ")", ")", ")", "\n", "foreground_mean", "(", "join", "(", "out_dir", ",", "\"summary_allFolds__%s.json\"", "%", "v", ")", ")", "\n", "foreground_mean", "(", "join", "(", "output_dir", ",", "\"%s__%s__%s__%s.json\"", "%", "(", "task_name", ",", "model", ",", "trainer", ",", "v", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.model_selection.summarize_results_with_plans.list_to_string": [[22, 27], ["None"], "function", ["None"], ["def", "list_to_string", "(", "l", ",", "delim", "=", "\",\"", ")", ":", "\n", "    ", "st", "=", "\"%03.3f\"", "%", "l", "[", "0", "]", "\n", "for", "i", "in", "l", "[", "1", ":", "]", ":", "\n", "        ", "st", "+=", "delim", "+", "\"%03.3f\"", "%", "i", "\n", "", "return", "st", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.model_selection.summarize_results_with_plans.write_plans_to_file": [[29, 54], ["load_pickle", "list", "list.sort", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "f.write", "a[].keys", "f.write", "f.write", "f.write", "zip", "zip", "str", "str", "str", "summarize_results_with_plans.list_to_string", "str", "summarize_results_with_plans.list_to_string", "summarize_results_with_plans.list_to_string", "summarize_results_with_plans.list_to_string", "str", "str", "plans_file.split", "plans_file.split"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.model_selection.summarize_results_with_plans.list_to_string", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.model_selection.summarize_results_with_plans.list_to_string", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.model_selection.summarize_results_with_plans.list_to_string", "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.model_selection.summarize_results_with_plans.list_to_string"], ["", "def", "write_plans_to_file", "(", "f", ",", "plans_file", ",", "stage", "=", "0", ",", "do_linebreak_at_end", "=", "True", ",", "override_name", "=", "None", ")", ":", "\n", "    ", "a", "=", "load_pickle", "(", "plans_file", ")", "\n", "stages", "=", "list", "(", "a", "[", "'plans_per_stage'", "]", ".", "keys", "(", ")", ")", "\n", "stages", ".", "sort", "(", ")", "\n", "patch_size_in_mm", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'patch_size'", "]", ",", "\n", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", "]", "\n", "median_patient_size_in_mm", "=", "[", "i", "*", "j", "for", "i", ",", "j", "in", "zip", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'median_patient_size_in_voxels'", "]", ",", "\n", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", "]", "\n", "if", "override_name", "is", "None", ":", "\n", "        ", "f", ".", "write", "(", "plans_file", ".", "split", "(", "\"/\"", ")", "[", "-", "2", "]", "+", "\"__\"", "+", "plans_file", ".", "split", "(", "\"/\"", ")", "[", "-", "1", "]", ")", "\n", "", "else", ":", "\n", "        ", "f", ".", "write", "(", "override_name", ")", "\n", "", "f", ".", "write", "(", "\";%d\"", "%", "stage", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'batch_size'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'num_pool_per_axis'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'patch_size'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "list_to_string", "(", "patch_size_in_mm", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'median_patient_size_in_voxels'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "list_to_string", "(", "median_patient_size_in_mm", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "list_to_string", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'current_spacing'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "list_to_string", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'original_spacing'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'pool_op_kernel_sizes'", "]", ")", ")", "\n", "f", ".", "write", "(", "\";%s\"", "%", "str", "(", "a", "[", "'plans_per_stage'", "]", "[", "stages", "[", "stage", "]", "]", "[", "'conv_kernel_sizes'", "]", ")", ")", "\n", "if", "do_linebreak_at_end", ":", "\n", "        ", "f", ".", "write", "(", "\"\\n\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.model_selection.ensemble.merge": [[25, 33], ["isfile", "load_pickle", "numpy.mean", "nnunet.inference.segmentation_export.save_segmentation_nifti_from_softmax", "numpy.load", "numpy.load"], "function", ["home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.inference.segmentation_export.save_segmentation_nifti_from_softmax"], ["def", "merge", "(", "args", ")", ":", "\n", "    ", "file1", ",", "file2", ",", "properties_file", ",", "out_file", "=", "args", "\n", "if", "not", "isfile", "(", "out_file", ")", ":", "\n", "        ", "res1", "=", "np", ".", "load", "(", "file1", ")", "[", "'softmax'", "]", "\n", "res2", "=", "np", ".", "load", "(", "file2", ")", "[", "'softmax'", "]", "\n", "props", "=", "load_pickle", "(", "properties_file", ")", "\n", "mn", "=", "np", ".", "mean", "(", "(", "res1", ",", "res2", ")", ",", "0", ")", "\n", "save_segmentation_nifti_from_softmax", "(", "mn", ",", "out_file", ",", "props", ",", "1", ",", "None", ",", "None", ",", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.online_evaluation_metrics.hard_dice": [[19, 52], ["isinstance", "isinstance", "output.detach().cpu().numpy.argmax", "numpy.arange", "range", "output.detach().cpu().numpy.detach().cpu().numpy", "target.detach().cpu().numpy.detach().cpu().numpy", "all_tp.append", "all_fp.append", "all_fn.append", "all_fg_dc.append", "tp.append", "fp.append", "fn.append", "output.detach().cpu().numpy.detach().cpu", "target.detach().cpu().numpy.detach().cpu", "numpy.sum", "numpy.sum", "numpy.sum", "zip", "output.detach().cpu().numpy.detach", "target.detach().cpu().numpy.detach"], "function", ["None"], ["def", "hard_dice", "(", "output", ",", "target", ")", ":", "\n", "    ", "if", "isinstance", "(", "output", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "output", "=", "output", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "if", "isinstance", "(", "target", ",", "torch", ".", "Tensor", ")", ":", "\n", "        ", "target", "=", "target", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "", "target", "=", "target", "[", ":", ",", "0", "]", "\n", "# target is not one hot encoded, output is", "\n", "# target must be the CPU segemtnation, not tensor. output is pytorch tensor", "\n", "num_classes", "=", "output", ".", "shape", "[", "1", "]", "\n", "output", "=", "output", ".", "argmax", "(", "1", ")", "\n", "foreground_classes", "=", "np", ".", "arange", "(", "1", ",", "num_classes", ")", "\n", "all_tp", "=", "[", "]", "\n", "all_fp", "=", "[", "]", "\n", "all_fn", "=", "[", "]", "\n", "all_fg_dc", "=", "[", "]", "\n", "for", "s", "in", "range", "(", "target", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "tp", "=", "[", "]", "\n", "fp", "=", "[", "]", "\n", "fn", "=", "[", "]", "\n", "for", "c", "in", "foreground_classes", ":", "\n", "            ", "t_is_c", "=", "target", "[", "s", "]", "==", "c", "\n", "o_is_c", "=", "output", "[", "s", "]", "==", "c", "\n", "t_is_not_c", "=", "target", "[", "s", "]", "!=", "c", "\n", "o_is_not_c", "=", "output", "[", "s", "]", "!=", "c", "\n", "tp", ".", "append", "(", "np", ".", "sum", "(", "o_is_c", "&", "t_is_c", ")", ")", "\n", "fp", ".", "append", "(", "np", ".", "sum", "(", "o_is_c", "&", "t_is_not_c", ")", ")", "\n", "fn", ".", "append", "(", "np", ".", "sum", "(", "o_is_not_c", "&", "t_is_c", ")", ")", "\n", "", "foreground_dice", "=", "[", "2", "*", "i", "/", "(", "2", "*", "i", "+", "j", "+", "k", "+", "1e-8", ")", "for", "i", ",", "j", ",", "k", "in", "zip", "(", "tp", ",", "fp", ",", "fn", ")", "]", "\n", "all_tp", ".", "append", "(", "tp", ")", "\n", "all_fp", ".", "append", "(", "fp", ")", "\n", "all_fn", ".", "append", "(", "fn", ")", "\n", "all_fg_dc", ".", "append", "(", "foreground_dice", ")", "\n", "", "return", "all_fg_dc", ",", "all_tp", ",", "all_fp", ",", "all_fn", "\n", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.sum_tensor": [[19, 28], ["numpy.unique().astype", "sorted", "numpy.unique", "inp.sum.sum", "inp.sum.sum", "int", "int"], "function", ["None"], ["def", "sum_tensor", "(", "inp", ",", "axes", ",", "keepdim", "=", "False", ")", ":", "\n", "    ", "axes", "=", "np", ".", "unique", "(", "axes", ")", ".", "astype", "(", "int", ")", "\n", "if", "keepdim", ":", "\n", "        ", "for", "ax", "in", "axes", ":", "\n", "            ", "inp", "=", "inp", ".", "sum", "(", "int", "(", "ax", ")", ",", "keepdim", "=", "True", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "ax", "in", "sorted", "(", "axes", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "inp", "=", "inp", ".", "sum", "(", "int", "(", "ax", ")", ")", "\n", "", "", "return", "inp", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.mean_tensor": [[30, 39], ["numpy.unique().astype", "sorted", "numpy.unique", "inp.mean.mean", "inp.mean.mean", "int", "int"], "function", ["None"], ["", "def", "mean_tensor", "(", "inp", ",", "axes", ",", "keepdim", "=", "False", ")", ":", "\n", "    ", "axes", "=", "np", ".", "unique", "(", "axes", ")", ".", "astype", "(", "int", ")", "\n", "if", "keepdim", ":", "\n", "        ", "for", "ax", "in", "axes", ":", "\n", "            ", "inp", "=", "inp", ".", "mean", "(", "int", "(", "ax", ")", ",", "keepdim", "=", "True", ")", "\n", "", "", "else", ":", "\n", "        ", "for", "ax", "in", "sorted", "(", "axes", ",", "reverse", "=", "True", ")", ":", "\n", "            ", "inp", "=", "inp", ".", "mean", "(", "int", "(", "ax", ")", ")", "\n", "", "", "return", "inp", "\n", "\n"]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.tensor_utilities.flip": [[41, 52], ["torch.arange", "x.dim", "slice", "x.size", "tuple"], "function", ["None"], ["", "def", "flip", "(", "x", ",", "dim", ")", ":", "\n", "    ", "\"\"\"\n    flips the tensor at dimension dim (mirroring!)\n    :param x:\n    :param dim:\n    :return:\n    \"\"\"", "\n", "indices", "=", "[", "slice", "(", "None", ")", "]", "*", "x", ".", "dim", "(", ")", "\n", "indices", "[", "dim", "]", "=", "torch", ".", "arange", "(", "x", ".", "size", "(", "dim", ")", "-", "1", ",", "-", "1", ",", "-", "1", ",", "\n", "dtype", "=", "torch", ".", "long", ",", "device", "=", "x", ".", "device", ")", "\n", "return", "x", "[", "tuple", "(", "indices", ")", "]", "\n", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.one_hot_encoding.to_one_hot": [[18, 25], ["numpy.zeros", "enumerate", "numpy.unique", "len"], "function", ["None"], ["def", "to_one_hot", "(", "seg", ",", "all_seg_labels", "=", "None", ")", ":", "\n", "    ", "if", "all_seg_labels", "is", "None", ":", "\n", "        ", "all_seg_labels", "=", "np", ".", "unique", "(", "seg", ")", "\n", "", "result", "=", "np", ".", "zeros", "(", "(", "len", "(", "all_seg_labels", ")", ",", "*", "seg", ".", "shape", ")", ",", "dtype", "=", "seg", ".", "dtype", ")", "\n", "for", "i", ",", "l", "in", "enumerate", "(", "all_seg_labels", ")", ":", "\n", "        ", "result", "[", "i", "]", "[", "seg", "==", "l", "]", "=", "1", "\n", "", "return", "result", "\n", "", ""]], "home.repos.pwc.inspect_result.LINGYUNFDU_MSSU-Net.utilities.nd_softmax.softmax_helper": [[18, 24], ["x.size", "[].repeat", "torch.exp", "torch.exp.sum().repeat", "range", "len", "x.max", "torch.exp.sum", "x.size"], "function", ["None"], ["def", "softmax_helper", "(", "x", ")", ":", "\n", "    ", "rpt", "=", "[", "1", "for", "_", "in", "range", "(", "len", "(", "x", ".", "size", "(", ")", ")", ")", "]", "\n", "rpt", "[", "1", "]", "=", "x", ".", "size", "(", "1", ")", "\n", "x_max", "=", "x", ".", "max", "(", "1", ",", "keepdim", "=", "True", ")", "[", "0", "]", ".", "repeat", "(", "*", "rpt", ")", "\n", "e_x", "=", "torch", ".", "exp", "(", "x", "-", "x_max", ")", "\n", "return", "e_x", "/", "e_x", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", ".", "repeat", "(", "*", "rpt", ")", "\n", "", ""]]}