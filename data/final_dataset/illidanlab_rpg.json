{"home.repos.pwc.inspect_result.illidanlab_rpg.utils.test_utils.MockReplayBuffer.__init__": [[30, 35], ["tensorflow.variable_scope", "mock.Mock", "mock.Mock"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "with", "tf", ".", "variable_scope", "(", "'MockReplayBuffer'", ",", "reuse", "=", "tf", ".", "AUTO_REUSE", ")", ":", "\n", "      ", "self", ".", "add", "=", "mock", ".", "Mock", "(", ")", "\n", "self", ".", "memory", "=", "mock", ".", "Mock", "(", ")", "\n", "self", ".", "memory", ".", "add_count", "=", "0", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.__init__": [[97, 164], ["isinstance", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "circular_replay_buffer.OutOfGraphReplayBuffer._create_storage", "numpy.array", "numpy.zeros", "numpy.array", "ValueError", "str", "str", "math.pow", "range"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._create_storage"], ["def", "__init__", "(", "self", ",", "\n", "observation_shape", ",", "\n", "stack_size", ",", "\n", "replay_capacity", ",", "\n", "batch_size", ",", "\n", "update_horizon", "=", "1", ",", "\n", "gamma", "=", "0.99", ",", "\n", "max_sample_attempts", "=", "MAX_SAMPLE_ATTEMPTS", ",", "\n", "extra_storage_types", "=", "None", ",", "\n", "observation_dtype", "=", "np", ".", "uint8", ")", ":", "\n", "    ", "\"\"\"Initializes OutOfGraphReplayBuffer.\n\n    Args:\n      observation_shape: tuple of ints.\n      stack_size: int, number of frames to use in state stack.\n      replay_capacity: int, number of transitions to keep in memory.\n      batch_size: int.\n      update_horizon: int, length of update ('n' in n-step update).\n      gamma: int, the discount factor.\n      max_sample_attempts: int, the maximum number of attempts allowed to\n        get a sample.\n      extra_storage_types: list of ReplayElements defining the type of the extra\n        contents that will be stored and returned by sample_transition_batch.\n      observation_dtype: np.dtype, type of the observations. Defaults to\n        np.uint8 for Atari 2600.\n\n    Raises:\n      ValueError: If replay_capacity is too small to hold at least one\n        transition.\n    \"\"\"", "\n", "assert", "isinstance", "(", "observation_shape", ",", "tuple", ")", "\n", "if", "replay_capacity", "<", "update_horizon", "+", "stack_size", ":", "\n", "      ", "raise", "ValueError", "(", "'There is not enough capacity to cover '", "\n", "'update_horizon and stack_size.'", ")", "\n", "\n", "", "tf", ".", "logging", ".", "info", "(", "\n", "'Creating a %s replay memory with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t observation_shape: %s'", ",", "str", "(", "observation_shape", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t observation_dtype: %s'", ",", "str", "(", "observation_dtype", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t stack_size: %d'", ",", "stack_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t replay_capacity: %d'", ",", "replay_capacity", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t batch_size: %d'", ",", "batch_size", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_horizon: %d'", ",", "update_horizon", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t gamma: %f'", ",", "gamma", ")", "\n", "\n", "self", ".", "_observation_shape", "=", "observation_shape", "\n", "self", ".", "_stack_size", "=", "stack_size", "\n", "self", ".", "_state_shape", "=", "self", ".", "_observation_shape", "+", "(", "self", ".", "_stack_size", ",", ")", "\n", "self", ".", "_replay_capacity", "=", "replay_capacity", "\n", "self", ".", "_batch_size", "=", "batch_size", "\n", "self", ".", "_update_horizon", "=", "update_horizon", "\n", "self", ".", "_gamma", "=", "gamma", "\n", "self", ".", "_observation_dtype", "=", "observation_dtype", "\n", "self", ".", "_max_sample_attempts", "=", "max_sample_attempts", "\n", "if", "extra_storage_types", ":", "\n", "      ", "self", ".", "_extra_storage_types", "=", "extra_storage_types", "\n", "", "else", ":", "\n", "      ", "self", ".", "_extra_storage_types", "=", "[", "]", "\n", "", "self", ".", "_create_storage", "(", ")", "\n", "self", ".", "add_count", "=", "np", ".", "array", "(", "0", ")", "\n", "self", ".", "invalid_range", "=", "np", ".", "zeros", "(", "(", "self", ".", "_stack_size", ")", ")", "\n", "# When the horizon is > 1, we compute the sum of discounted rewards as a dot", "\n", "# product using the precomputed vector <gamma^0, gamma^1, ..., gamma^{n-1}>.", "\n", "self", ".", "_cumulative_discount_vector", "=", "np", ".", "array", "(", "\n", "[", "math", ".", "pow", "(", "self", ".", "_gamma", ",", "n", ")", "for", "n", "in", "range", "(", "update_horizon", ")", "]", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._create_storage": [[165, 173], ["circular_replay_buffer.OutOfGraphReplayBuffer.get_storage_signature", "numpy.empty", "list"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_storage_signature"], ["", "def", "_create_storage", "(", "self", ")", ":", "\n", "    ", "\"\"\"Creates the numpy arrays used to store transitions.\n    \"\"\"", "\n", "self", ".", "_store", "=", "{", "}", "\n", "for", "storage_element", "in", "self", ".", "get_storage_signature", "(", ")", ":", "\n", "      ", "array_shape", "=", "[", "self", ".", "_replay_capacity", "]", "+", "list", "(", "storage_element", ".", "shape", ")", "\n", "self", ".", "_store", "[", "storage_element", ".", "name", "]", "=", "np", ".", "empty", "(", "\n", "array_shape", ",", "dtype", "=", "storage_element", ".", "type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_add_args_signature": [[174, 184], ["circular_replay_buffer.OutOfGraphReplayBuffer.get_storage_signature"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_storage_signature"], ["", "", "def", "get_add_args_signature", "(", "self", ")", ":", "\n", "    ", "\"\"\"The signature of the add function.\n\n    Note - Derived classes may return a different signature.\n\n    Returns:\n      list of ReplayElements defining the type of the argument signature needed\n        by the add function.\n    \"\"\"", "\n", "return", "self", ".", "get_storage_signature", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_storage_signature": [[185, 204], ["ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "storage_elements.append"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "get_storage_signature", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns a default list of elements to be stored in this replay memory.\n\n    Note - Derived classes may return a different signature.\n\n    Returns:\n      list of ReplayElements defining the type of the contents stored.\n    \"\"\"", "\n", "storage_elements", "=", "[", "\n", "ReplayElement", "(", "'observation'", ",", "self", ".", "_observation_shape", ",", "\n", "self", ".", "_observation_dtype", ")", ",", "\n", "ReplayElement", "(", "'action'", ",", "(", ")", ",", "np", ".", "int32", ")", ",", "\n", "ReplayElement", "(", "'reward'", ",", "(", ")", ",", "np", ".", "float32", ")", ",", "\n", "ReplayElement", "(", "'terminal'", ",", "(", ")", ",", "np", ".", "uint8", ")", "\n", "]", "\n", "\n", "for", "extra_replay_element", "in", "self", ".", "_extra_storage_types", ":", "\n", "      ", "storage_elements", ".", "append", "(", "extra_replay_element", ")", "\n", "", "return", "storage_elements", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._add_zero_transition": [[205, 213], ["circular_replay_buffer.OutOfGraphReplayBuffer.get_add_args_signature", "circular_replay_buffer.OutOfGraphReplayBuffer._add", "zero_transition.append", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_add_args_signature", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer._add", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "_add_zero_transition", "(", "self", ")", ":", "\n", "    ", "\"\"\"Adds a padding transition filled with zeros (Used in episode beginnings).\n    \"\"\"", "\n", "zero_transition", "=", "[", "]", "\n", "for", "element_type", "in", "self", ".", "get_add_args_signature", "(", ")", ":", "\n", "      ", "zero_transition", ".", "append", "(", "\n", "np", ".", "zeros", "(", "element_type", ".", "shape", ",", "dtype", "=", "element_type", ".", "type", ")", ")", "\n", "", "self", ".", "_add", "(", "*", "zero_transition", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.add": [[214, 241], ["circular_replay_buffer.OutOfGraphReplayBuffer._check_add_types", "circular_replay_buffer.OutOfGraphReplayBuffer._add", "circular_replay_buffer.OutOfGraphReplayBuffer.is_empty", "range", "circular_replay_buffer.OutOfGraphReplayBuffer._add_zero_transition", "circular_replay_buffer.OutOfGraphReplayBuffer.cursor"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._check_add_types", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer._add", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_empty", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._add_zero_transition", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor"], ["", "def", "add", "(", "self", ",", "observation", ",", "action", ",", "reward", ",", "terminal", ",", "*", "args", ")", ":", "\n", "    ", "\"\"\"Adds a transition to the replay memory.\n\n    This function checks the types and handles the padding at the beginning of\n    an episode. Then it calls the _add function.\n\n    Since the next_observation in the transition will be the observation added\n    next there is no need to pass it.\n\n    If the replay memory is at capacity the oldest transition will be discarded.\n\n    Args:\n      observation: np.array with shape observation_shape.\n      action: int, the action in the transition.\n      reward: float, the reward received in the transition.\n      terminal: A uint8 acting as a boolean indicating whether the transition\n                was terminal (1) or not (0).\n      *args: extra contents with shapes and dtypes according to\n        extra_storage_types.\n    \"\"\"", "\n", "self", ".", "_check_add_types", "(", "observation", ",", "action", ",", "reward", ",", "terminal", ",", "*", "args", ")", "\n", "if", "self", ".", "is_empty", "(", ")", "or", "self", ".", "_store", "[", "'terminal'", "]", "[", "self", ".", "cursor", "(", ")", "-", "1", "]", "==", "1", ":", "\n", "      ", "for", "_", "in", "range", "(", "self", ".", "_stack_size", "-", "1", ")", ":", "\n", "# Child classes can rely on the padding transitions being filled with", "\n", "# zeros. This is useful when there is a priority argument.", "\n", "        ", "self", ".", "_add_zero_transition", "(", ")", "\n", "", "", "self", ".", "_add", "(", "observation", ",", "action", ",", "reward", ",", "terminal", ",", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._add": [[242, 258], ["circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "zip", "circular_replay_buffer.invalid_range", "circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "circular_replay_buffer.OutOfGraphReplayBuffer.get_add_args_signature"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.invalid_range", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_add_args_signature"], ["", "def", "_add", "(", "self", ",", "*", "args", ")", ":", "\n", "    ", "\"\"\"Internal add method to add to the storage arrays.\n\n    Args:\n      *args: All the elements in a transition.\n    \"\"\"", "\n", "cursor", "=", "self", ".", "cursor", "(", ")", "\n", "\n", "arg_names", "=", "[", "e", ".", "name", "for", "e", "in", "self", ".", "get_add_args_signature", "(", ")", "]", "\n", "for", "arg_name", ",", "arg", "in", "zip", "(", "arg_names", ",", "args", ")", ":", "\n", "      ", "self", ".", "_store", "[", "arg_name", "]", "[", "cursor", "]", "=", "arg", "\n", "\n", "", "self", ".", "add_count", "+=", "1", "\n", "self", ".", "invalid_range", "=", "invalid_range", "(", "\n", "self", ".", "cursor", "(", ")", ",", "self", ".", "_replay_capacity", ",", "self", ".", "_stack_size", ",", "\n", "self", ".", "_update_horizon", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._check_add_types": [[259, 284], ["zip", "len", "len", "ValueError", "circular_replay_buffer.OutOfGraphReplayBuffer.get_add_args_signature", "isinstance", "tuple", "circular_replay_buffer.OutOfGraphReplayBuffer.get_add_args_signature", "ValueError", "len", "len", "isinstance", "isinstance", "tuple", "circular_replay_buffer.OutOfGraphReplayBuffer.get_add_args_signature", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_add_args_signature", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_add_args_signature", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_add_args_signature"], ["", "def", "_check_add_types", "(", "self", ",", "*", "args", ")", ":", "\n", "    ", "\"\"\"Checks if args passed to the add method match those of the storage.\n\n    Args:\n      *args: Args whose types need to be validated.\n\n    Raises:\n      ValueError: If args have wrong shape or dtype.\n    \"\"\"", "\n", "if", "len", "(", "args", ")", "!=", "len", "(", "self", ".", "get_add_args_signature", "(", ")", ")", ":", "\n", "      ", "raise", "ValueError", "(", "'Add expects {} elements, received {}'", ".", "format", "(", "\n", "len", "(", "self", ".", "get_add_args_signature", "(", ")", ")", ",", "len", "(", "args", ")", ")", ")", "\n", "", "for", "arg_element", ",", "store_element", "in", "zip", "(", "args", ",", "self", ".", "get_add_args_signature", "(", ")", ")", ":", "\n", "      ", "if", "isinstance", "(", "arg_element", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "arg_shape", "=", "arg_element", ".", "shape", "\n", "", "elif", "isinstance", "(", "arg_element", ",", "tuple", ")", "or", "isinstance", "(", "arg_element", ",", "list", ")", ":", "\n", "# TODO(b/80536437). This is not efficient when arg_element is a list.", "\n", "        ", "arg_shape", "=", "np", ".", "array", "(", "arg_element", ")", ".", "shape", "\n", "", "else", ":", "\n", "# Assume it is scalar.", "\n", "        ", "arg_shape", "=", "tuple", "(", ")", "\n", "", "store_element_shape", "=", "tuple", "(", "store_element", ".", "shape", ")", "\n", "if", "arg_shape", "!=", "store_element_shape", ":", "\n", "        ", "raise", "ValueError", "(", "'arg has shape {}, expected {}'", ".", "format", "(", "\n", "arg_shape", ",", "store_element_shape", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_empty": [[285, 288], ["None"], "methods", ["None"], ["", "", "", "def", "is_empty", "(", "self", ")", ":", "\n", "    ", "\"\"\"Is the Replay Buffer empty?\"\"\"", "\n", "return", "self", ".", "add_count", "==", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_full": [[289, 292], ["None"], "methods", ["None"], ["", "def", "is_full", "(", "self", ")", ":", "\n", "    ", "\"\"\"Is the Replay Buffer full?\"\"\"", "\n", "return", "self", ".", "add_count", ">=", "self", ".", "_replay_capacity", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor": [[293, 296], ["None"], "methods", ["None"], ["", "def", "cursor", "(", "self", ")", ":", "\n", "    ", "\"\"\"Index to the location where the next transition will be written.\"\"\"", "\n", "return", "self", ".", "add_count", "%", "self", ".", "_replay_capacity", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_range": [[297, 326], ["circular_replay_buffer.OutOfGraphReplayBuffer.is_full", "circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "range"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_full", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor"], ["", "def", "get_range", "(", "self", ",", "array", ",", "start_index", ",", "end_index", ")", ":", "\n", "    ", "\"\"\"Returns the range of array at the index handling wraparound if necessary.\n\n    Args:\n      array: np.array, the array to get the stack from.\n      start_index: int, index to the start of the range to be returned. Range\n        will wraparound if start_index is smaller than 0.\n      end_index: int, exclusive end index. Range will wraparound if end_index\n        exceeds replay_capacity.\n\n    Returns:\n      np.array, with shape [end_index - start_index, array.shape[1:]].\n    \"\"\"", "\n", "assert", "end_index", ">", "start_index", ",", "'end_index must be larger than start_index'", "\n", "assert", "end_index", ">=", "0", "\n", "assert", "start_index", "<", "self", ".", "_replay_capacity", "\n", "if", "not", "self", ".", "is_full", "(", ")", ":", "\n", "      ", "assert", "end_index", "<=", "self", ".", "cursor", "(", ")", ",", "(", "\n", "'Index {} has not been added.'", ".", "format", "(", "start_index", ")", ")", "\n", "\n", "# Fast slice read when there is no wraparound.", "\n", "", "if", "start_index", "%", "self", ".", "_replay_capacity", "<", "end_index", "%", "self", ".", "_replay_capacity", ":", "\n", "      ", "return_array", "=", "array", "[", "start_index", ":", "end_index", ",", "...", "]", "\n", "# Slow list read.", "\n", "", "else", ":", "\n", "      ", "indices", "=", "[", "(", "start_index", "+", "i", ")", "%", "self", ".", "_replay_capacity", "\n", "for", "i", "in", "range", "(", "end_index", "-", "start_index", ")", "]", "\n", "return_array", "=", "array", "[", "indices", ",", "...", "]", "\n", "", "return", "return_array", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_observation_stack": [[327, 332], ["circular_replay_buffer.OutOfGraphReplayBuffer.get_range", "numpy.moveaxis"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_range"], ["", "def", "get_observation_stack", "(", "self", ",", "index", ")", ":", "\n", "    ", "state", "=", "self", ".", "get_range", "(", "self", ".", "_store", "[", "'observation'", "]", ",", "\n", "index", "-", "self", ".", "_stack_size", "+", "1", ",", "index", "+", "1", ")", "\n", "# The stacking axis is 0 but the agent expects as the last axis.", "\n", "return", "np", ".", "moveaxis", "(", "state", ",", "0", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_terminal_stack": [[333, 336], ["circular_replay_buffer.OutOfGraphReplayBuffer.get_range"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_range"], ["", "def", "get_terminal_stack", "(", "self", ",", "index", ")", ":", "\n", "    ", "return", "self", ".", "get_range", "(", "self", ".", "_store", "[", "'terminal'", "]", ",", "index", "-", "self", ".", "_stack_size", "+", "1", ",", "\n", "index", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_valid_transition": [[337, 371], ["[].any", "circular_replay_buffer.OutOfGraphReplayBuffer.is_full", "set", "circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "circular_replay_buffer.OutOfGraphReplayBuffer.get_terminal_stack"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_full", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.set", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_terminal_stack"], ["", "def", "is_valid_transition", "(", "self", ",", "index", ")", ":", "\n", "    ", "\"\"\"Checks if the index contains a valid transition.\n\n    Checks for collisions with the end of episodes and the current position\n    of the cursor.\n\n    Args:\n      index: int, the index to the state in the transition.\n\n    Returns:\n      Is the index valid: Boolean.\n\n    \"\"\"", "\n", "# Check the index is in the valid range", "\n", "if", "index", "<", "0", "or", "index", ">=", "self", ".", "_replay_capacity", ":", "\n", "      ", "return", "False", "\n", "", "if", "not", "self", ".", "is_full", "(", ")", ":", "\n", "# The indices and next_indices must be smaller than the cursor.", "\n", "      ", "if", "index", ">=", "self", ".", "cursor", "(", ")", "-", "self", ".", "_update_horizon", ":", "\n", "        ", "return", "False", "\n", "# The first few indices contain the padding states of the first episode.", "\n", "", "if", "index", "<", "self", ".", "_stack_size", "-", "1", ":", "\n", "        ", "return", "False", "\n", "\n", "# Skip transitions that straddle the cursor.", "\n", "", "", "if", "index", "in", "set", "(", "self", ".", "invalid_range", ")", ":", "\n", "      ", "return", "False", "\n", "\n", "# If there are terminal flags in any other frame other than the last one", "\n", "# the stack is not valid, so don't sample it.", "\n", "", "if", "self", ".", "get_terminal_stack", "(", "index", ")", "[", ":", "-", "1", "]", ".", "any", "(", ")", ":", "\n", "      ", "return", "False", "\n", "\n", "", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._create_batch_arrays": [[372, 391], ["circular_replay_buffer.OutOfGraphReplayBuffer.get_transition_elements", "tuple", "batch_arrays.append", "numpy.empty"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_transition_elements", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "_create_batch_arrays", "(", "self", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Create a tuple of arrays with the type of get_transition_elements.\n\n    When using the WrappedReplayBuffer with staging enabled it is important to\n    create new arrays every sample because StaginArea keeps a pointer to the\n    returned arrays.\n\n    Args:\n      batch_size: (int) number of transitions returned. If None the default\n        batch_size will be used.\n\n    Returns:\n      Tuple of np.arrays with the shape and type of get_transition_elements.\n    \"\"\"", "\n", "transition_elements", "=", "self", ".", "get_transition_elements", "(", "batch_size", ")", "\n", "batch_arrays", "=", "[", "]", "\n", "for", "element", "in", "transition_elements", ":", "\n", "      ", "batch_arrays", ".", "append", "(", "np", ".", "empty", "(", "element", ".", "shape", ",", "dtype", "=", "element", ".", "type", ")", ")", "\n", "", "return", "tuple", "(", "batch_arrays", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.sample_index_batch": [[392, 433], ["circular_replay_buffer.OutOfGraphReplayBuffer.is_full", "circular_replay_buffer.OutOfGraphReplayBuffer.is_valid_transition", "len", "RuntimeError", "circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "RuntimeError", "len", "numpy.random.randint", "indices.append", "len", "circular_replay_buffer.OutOfGraphReplayBuffer.cursor"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_full", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_valid_transition", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor"], ["", "def", "sample_index_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Returns a batch of valid indices sampled uniformly.\n\n    Args:\n      batch_size: int, number of indices returned.\n\n    Returns:\n      list of ints, a batch of valid indices sampled uniformly.\n\n    Raises:\n      RuntimeError: If the batch was not constructed after maximum number of\n        tries.\n    \"\"\"", "\n", "if", "self", ".", "is_full", "(", ")", ":", "\n", "# add_count >= self._replay_capacity > self._stack_size", "\n", "      ", "min_id", "=", "self", ".", "cursor", "(", ")", "-", "self", ".", "_replay_capacity", "+", "self", ".", "_stack_size", "-", "1", "\n", "max_id", "=", "self", ".", "cursor", "(", ")", "-", "self", ".", "_update_horizon", "\n", "", "else", ":", "\n", "# add_count < self._replay_capacity", "\n", "      ", "min_id", "=", "self", ".", "_stack_size", "-", "1", "\n", "max_id", "=", "self", ".", "cursor", "(", ")", "-", "self", ".", "_update_horizon", "\n", "if", "max_id", "<=", "min_id", ":", "\n", "        ", "raise", "RuntimeError", "(", "'Cannot sample a batch with fewer than stack size '", "\n", "'({}) + update_horizon ({}) transitions.'", ".", "\n", "format", "(", "self", ".", "_stack_size", ",", "self", ".", "_update_horizon", ")", ")", "\n", "\n", "", "", "indices", "=", "[", "]", "\n", "attempt_count", "=", "0", "\n", "while", "(", "len", "(", "indices", ")", "<", "batch_size", "and", "\n", "attempt_count", "<", "self", ".", "_max_sample_attempts", ")", ":", "\n", "      ", "attempt_count", "+=", "1", "\n", "index", "=", "np", ".", "random", ".", "randint", "(", "min_id", ",", "max_id", ")", "%", "self", ".", "_replay_capacity", "\n", "if", "self", ".", "is_valid_transition", "(", "index", ")", ":", "\n", "        ", "indices", ".", "append", "(", "index", ")", "\n", "", "", "if", "len", "(", "indices", ")", "!=", "batch_size", ":", "\n", "      ", "raise", "RuntimeError", "(", "\n", "'Max sample attempts: Tried {} times but only sampled {}'", "\n", "' valid indices. Batch size is {}'", ".", "\n", "format", "(", "self", ".", "_max_sample_attempts", ",", "len", "(", "indices", ")", ",", "batch_size", ")", ")", "\n", "\n", "", "return", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.sample_transition_batch": [[434, 510], ["circular_replay_buffer.OutOfGraphReplayBuffer.get_transition_elements", "circular_replay_buffer.OutOfGraphReplayBuffer._create_batch_arrays", "enumerate", "circular_replay_buffer.OutOfGraphReplayBuffer.sample_index_batch", "len", "trajectory_terminals.any", "circular_replay_buffer.OutOfGraphReplayBuffer.get_range", "zip", "len", "len", "range", "numpy.argmax", "circular_replay_buffer.OutOfGraphReplayBuffer.get_observation_stack", "trajectory_terminals.astype", "trajectory_discount_vector.dot", "circular_replay_buffer.OutOfGraphReplayBuffer.get_observation_stack", "circular_replay_buffer.OutOfGraphReplayBuffer._store.keys"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_transition_elements", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._create_batch_arrays", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.sample_index_batch", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_range", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_observation_stack", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_observation_stack"], ["", "def", "sample_transition_batch", "(", "self", ",", "batch_size", "=", "None", ",", "indices", "=", "None", ")", ":", "\n", "    ", "\"\"\"Returns a batch of transitions (including any extra contents).\n\n    If get_transition_elements has been overridden and defines elements not\n    stored in self._store, an empty array will be returned and it will be\n    left to the child class to fill it. For example, for the child class\n    OutOfGraphPrioritizedReplayBuffer, the contents of the\n    sampling_probabilities are stored separately in a sum tree.\n\n    When the transition is terminal next_state_batch has undefined contents.\n\n    NOTE: This transition contains the indices of the sampled elements. These\n    are only valid during the call to sample_transition_batch, i.e. they may\n    be used by subclasses of this replay buffer but may point to different data\n    as soon as sampling is done.\n\n    Args:\n      batch_size: int, number of transitions returned. If None, the default\n        batch_size will be used.\n      indices: None or list of ints, the indices of every transition in the\n        batch. If None, sample the indices uniformly.\n\n    Returns:\n      transition_batch: tuple of np.arrays with the shape and type as in\n        get_transition_elements().\n\n    Raises:\n      ValueError: If an element to be sampled is missing from the replay buffer.\n    \"\"\"", "\n", "if", "batch_size", "is", "None", ":", "\n", "      ", "batch_size", "=", "self", ".", "_batch_size", "\n", "", "if", "indices", "is", "None", ":", "\n", "      ", "indices", "=", "self", ".", "sample_index_batch", "(", "batch_size", ")", "\n", "", "assert", "len", "(", "indices", ")", "==", "batch_size", "\n", "\n", "transition_elements", "=", "self", ".", "get_transition_elements", "(", "batch_size", ")", "\n", "batch_arrays", "=", "self", ".", "_create_batch_arrays", "(", "batch_size", ")", "\n", "for", "batch_element", ",", "state_index", "in", "enumerate", "(", "indices", ")", ":", "\n", "      ", "trajectory_indices", "=", "[", "(", "state_index", "+", "j", ")", "%", "self", ".", "_replay_capacity", "\n", "for", "j", "in", "range", "(", "self", ".", "_update_horizon", ")", "]", "\n", "trajectory_terminals", "=", "self", ".", "_store", "[", "'terminal'", "]", "[", "trajectory_indices", "]", "\n", "is_terminal_transition", "=", "trajectory_terminals", ".", "any", "(", ")", "\n", "if", "not", "is_terminal_transition", ":", "\n", "        ", "trajectory_length", "=", "self", ".", "_update_horizon", "\n", "", "else", ":", "\n", "# np.argmax of a bool array returns the index of the first True.", "\n", "        ", "trajectory_length", "=", "np", ".", "argmax", "(", "trajectory_terminals", ".", "astype", "(", "np", ".", "bool", ")", ",", "\n", "0", ")", "+", "1", "\n", "", "next_state_index", "=", "state_index", "+", "trajectory_length", "\n", "trajectory_discount_vector", "=", "(", "\n", "self", ".", "_cumulative_discount_vector", "[", ":", "trajectory_length", "]", ")", "\n", "trajectory_rewards", "=", "self", ".", "get_range", "(", "self", ".", "_store", "[", "'reward'", "]", ",", "state_index", ",", "\n", "next_state_index", ")", "\n", "\n", "# Fill the contents of each array in the sampled batch.", "\n", "assert", "len", "(", "transition_elements", ")", "==", "len", "(", "batch_arrays", ")", "\n", "for", "element_array", ",", "element", "in", "zip", "(", "batch_arrays", ",", "transition_elements", ")", ":", "\n", "        ", "if", "element", ".", "name", "==", "'state'", ":", "\n", "          ", "element_array", "[", "batch_element", "]", "=", "self", ".", "get_observation_stack", "(", "state_index", ")", "\n", "", "elif", "element", ".", "name", "==", "'reward'", ":", "\n", "# cumpute the discounted sum of rewards in the trajectory.", "\n", "          ", "element_array", "[", "batch_element", "]", "=", "trajectory_discount_vector", ".", "dot", "(", "\n", "trajectory_rewards", ")", "\n", "", "elif", "element", ".", "name", "==", "'next_state'", ":", "\n", "          ", "element_array", "[", "batch_element", "]", "=", "self", ".", "get_observation_stack", "(", "\n", "(", "next_state_index", ")", "%", "self", ".", "_replay_capacity", ")", "\n", "", "elif", "element", ".", "name", "==", "'terminal'", ":", "\n", "          ", "element_array", "[", "batch_element", "]", "=", "is_terminal_transition", "\n", "", "elif", "element", ".", "name", "==", "'indices'", ":", "\n", "          ", "element_array", "[", "batch_element", "]", "=", "state_index", "\n", "", "elif", "element", ".", "name", "in", "self", ".", "_store", ".", "keys", "(", ")", ":", "\n", "          ", "element_array", "[", "batch_element", "]", "=", "(", "\n", "self", ".", "_store", "[", "element", ".", "name", "]", "[", "state_index", "]", ")", "\n", "# We assume the other elements are filled in by the subclass.", "\n", "\n", "", "", "", "return", "batch_arrays", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.get_transition_elements": [[511, 537], ["ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "ReplayElement", "transition_elements.append", "ReplayElement", "tuple"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "get_transition_elements", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "    ", "\"\"\"Returns a 'type signature' for sample_transition_batch.\n\n    Args:\n      batch_size: int, number of transitions returned. If None, the default\n        batch_size will be used.\n    Returns:\n      signature: A namedtuple describing the method's return type signature.\n    \"\"\"", "\n", "batch_size", "=", "self", ".", "_batch_size", "if", "batch_size", "is", "None", "else", "batch_size", "\n", "\n", "transition_elements", "=", "[", "\n", "ReplayElement", "(", "'state'", ",", "(", "batch_size", ",", ")", "+", "self", ".", "_state_shape", ",", "\n", "self", ".", "_observation_dtype", ")", ",", "\n", "ReplayElement", "(", "'action'", ",", "(", "batch_size", ",", ")", ",", "np", ".", "int32", ")", ",", "\n", "ReplayElement", "(", "'reward'", ",", "(", "batch_size", ",", ")", ",", "np", ".", "float32", ")", ",", "\n", "ReplayElement", "(", "'next_state'", ",", "(", "batch_size", ",", ")", "+", "self", ".", "_state_shape", ",", "\n", "self", ".", "_observation_dtype", ")", ",", "\n", "ReplayElement", "(", "'terminal'", ",", "(", "batch_size", ",", ")", ",", "np", ".", "uint8", ")", ",", "\n", "ReplayElement", "(", "'indices'", ",", "(", "batch_size", ",", ")", ",", "np", ".", "int32", ")", "\n", "]", "\n", "for", "element", "in", "self", ".", "_extra_storage_types", ":", "\n", "      ", "transition_elements", ".", "append", "(", "\n", "ReplayElement", "(", "element", ".", "name", ",", "(", "batch_size", ",", ")", "+", "tuple", "(", "element", ".", "shape", ")", ",", "\n", "element", ".", "type", ")", ")", "\n", "", "return", "transition_elements", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._generate_filename": [[538, 540], ["os.path.join"], "methods", ["None"], ["", "def", "_generate_filename", "(", "self", ",", "checkpoint_dir", ",", "name", ",", "suffix", ")", ":", "\n", "    ", "return", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'{}_ckpt.{}.gz'", ".", "format", "(", "name", ",", "suffix", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._return_checkpointable_elements": [[541, 556], ["circular_replay_buffer.OutOfGraphReplayBuffer.__dict__.items", "circular_replay_buffer.OutOfGraphReplayBuffer._store.items", "member_name.startswith"], "methods", ["None"], ["", "def", "_return_checkpointable_elements", "(", "self", ")", ":", "\n", "    ", "\"\"\"Return the dict of elements of the class for checkpointing.\n\n    Returns:\n      checkpointable_elements: dict containing all non private (starting with\n      _) members + all the arrays inside self._store.\n    \"\"\"", "\n", "checkpointable_elements", "=", "{", "}", "\n", "for", "member_name", ",", "member", "in", "self", ".", "__dict__", ".", "items", "(", ")", ":", "\n", "      ", "if", "member_name", "==", "'_store'", ":", "\n", "        ", "for", "array_name", ",", "array", "in", "self", ".", "_store", ".", "items", "(", ")", ":", "\n", "          ", "checkpointable_elements", "[", "STORE_FILENAME_PREFIX", "+", "array_name", "]", "=", "array", "\n", "", "", "elif", "not", "member_name", ".", "startswith", "(", "'_'", ")", ":", "\n", "        ", "checkpointable_elements", "[", "member_name", "]", "=", "member", "\n", "", "", "return", "checkpointable_elements", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.save": [[557, 600], ["circular_replay_buffer.OutOfGraphReplayBuffer._return_checkpointable_elements", "tensorflow.gfile.Exists", "circular_replay_buffer.OutOfGraphReplayBuffer._generate_filename", "tensorflow.gfile.Open", "circular_replay_buffer.OutOfGraphReplayBuffer._generate_filename", "gzip.GzipFile", "attr.startswith", "tensorflow.gfile.Remove", "numpy.save", "isinstance", "numpy.save", "pickle.dump", "len"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._return_checkpointable_elements", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save"], ["", "def", "save", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ")", ":", "\n", "    ", "\"\"\"Save the OutOfGraphReplayBuffer attributes into a file.\n\n    This method will save all the replay buffer's state in a single file.\n\n    Args:\n      checkpoint_dir: str, the directory where numpy checkpoint files should be\n        saved.\n      iteration_number: int, iteration_number to use as a suffix in naming\n        numpy checkpoint files.\n    \"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "checkpoint_dir", ")", ":", "\n", "      ", "return", "\n", "\n", "", "checkpointable_elements", "=", "self", ".", "_return_checkpointable_elements", "(", ")", "\n", "\n", "for", "attr", "in", "checkpointable_elements", ":", "\n", "      ", "filename", "=", "self", ".", "_generate_filename", "(", "checkpoint_dir", ",", "attr", ",", "iteration_number", ")", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "'wb'", ")", "as", "f", ":", "\n", "        ", "with", "gzip", ".", "GzipFile", "(", "fileobj", "=", "f", ")", "as", "outfile", ":", "\n", "# Checkpoint the np arrays in self._store with np.save instead of", "\n", "# pickling the dictionary is critical for file size and performance.", "\n", "# STORE_FILENAME_PREFIX indicates that the variable is contained in", "\n", "# self._store.", "\n", "          ", "if", "attr", ".", "startswith", "(", "STORE_FILENAME_PREFIX", ")", ":", "\n", "            ", "array_name", "=", "attr", "[", "len", "(", "STORE_FILENAME_PREFIX", ")", ":", "]", "\n", "np", ".", "save", "(", "outfile", ",", "self", ".", "_store", "[", "array_name", "]", ",", "allow_pickle", "=", "False", ")", "\n", "# Some numpy arrays might not be part of storage", "\n", "", "elif", "isinstance", "(", "self", ".", "__dict__", "[", "attr", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "np", ".", "save", "(", "outfile", ",", "self", ".", "__dict__", "[", "attr", "]", ",", "allow_pickle", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "pickle", ".", "dump", "(", "self", ".", "__dict__", "[", "attr", "]", ",", "outfile", ")", "\n", "\n", "# After writing a checkpoint file, we garbage collect the checkpoint file", "\n", "# that is four versions old.", "\n", "", "", "", "stale_iteration_number", "=", "iteration_number", "-", "CHECKPOINT_DURATION", "\n", "if", "stale_iteration_number", ">=", "0", ":", "\n", "        ", "stale_filename", "=", "self", ".", "_generate_filename", "(", "checkpoint_dir", ",", "attr", ",", "\n", "stale_iteration_number", ")", "\n", "try", ":", "\n", "          ", "tf", ".", "gfile", ".", "Remove", "(", "stale_filename", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "          ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.load": [[601, 633], ["circular_replay_buffer.OutOfGraphReplayBuffer._return_checkpointable_elements", "circular_replay_buffer.OutOfGraphReplayBuffer._generate_filename", "circular_replay_buffer.OutOfGraphReplayBuffer._generate_filename", "tensorflow.gfile.Exists", "tensorflow.errors.NotFoundError", "tensorflow.gfile.Open", "gzip.GzipFile", "attr.startswith", "numpy.load", "isinstance", "numpy.load", "pickle.load", "len"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer._return_checkpointable_elements", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load"], ["", "", "", "", "def", "load", "(", "self", ",", "checkpoint_dir", ",", "suffix", ")", ":", "\n", "    ", "\"\"\"Restores the object from bundle_dictionary and numpy checkpoints.\n\n    Args:\n      checkpoint_dir: str, the directory where to read the numpy checkpointed\n        files from.\n      suffix: str, the suffix to use in numpy checkpoint files.\n\n    Raises:\n      NotFoundError: If not all expected files are found in directory.\n    \"\"\"", "\n", "save_elements", "=", "self", ".", "_return_checkpointable_elements", "(", ")", "\n", "# We will first make sure we have all the necessary files available to avoid", "\n", "# loading a partially-specified (i.e. corrupted) replay buffer.", "\n", "for", "attr", "in", "save_elements", ":", "\n", "      ", "filename", "=", "self", ".", "_generate_filename", "(", "checkpoint_dir", ",", "attr", ",", "suffix", ")", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "filename", ")", ":", "\n", "        ", "raise", "tf", ".", "errors", ".", "NotFoundError", "(", "None", ",", "None", ",", "\n", "'Missing file: {}'", ".", "format", "(", "filename", ")", ")", "\n", "# If we've reached this point then we have verified that all expected files", "\n", "# are available.", "\n", "", "", "for", "attr", "in", "save_elements", ":", "\n", "      ", "filename", "=", "self", ".", "_generate_filename", "(", "checkpoint_dir", ",", "attr", ",", "suffix", ")", "\n", "with", "tf", ".", "gfile", ".", "Open", "(", "filename", ",", "'rb'", ")", "as", "f", ":", "\n", "        ", "with", "gzip", ".", "GzipFile", "(", "fileobj", "=", "f", ")", "as", "infile", ":", "\n", "          ", "if", "attr", ".", "startswith", "(", "STORE_FILENAME_PREFIX", ")", ":", "\n", "            ", "array_name", "=", "attr", "[", "len", "(", "STORE_FILENAME_PREFIX", ")", ":", "]", "\n", "self", ".", "_store", "[", "array_name", "]", "=", "np", ".", "load", "(", "infile", ",", "allow_pickle", "=", "False", ")", "\n", "", "elif", "isinstance", "(", "self", ".", "__dict__", "[", "attr", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "self", ".", "__dict__", "[", "attr", "]", "=", "np", ".", "load", "(", "infile", ",", "allow_pickle", "=", "False", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "__dict__", "[", "attr", "]", "=", "pickle", ".", "load", "(", "infile", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.__init__": [[649, 707], ["circular_replay_buffer.WrappedReplayBuffer.create_sampling_ops", "ValueError", "ValueError", "ValueError", "circular_replay_buffer.OutOfGraphReplayBuffer"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.create_sampling_ops"], ["def", "__init__", "(", "self", ",", "\n", "observation_shape", ",", "\n", "stack_size", ",", "\n", "use_staging", "=", "True", ",", "\n", "replay_capacity", "=", "1000000", ",", "\n", "batch_size", "=", "32", ",", "\n", "update_horizon", "=", "1", ",", "\n", "gamma", "=", "0.99", ",", "\n", "wrapped_memory", "=", "None", ",", "\n", "max_sample_attempts", "=", "MAX_SAMPLE_ATTEMPTS", ",", "\n", "extra_storage_types", "=", "None", ",", "\n", "observation_dtype", "=", "np", ".", "uint8", ")", ":", "\n", "    ", "\"\"\"Initializes WrappedReplayBuffer.\n\n    Args:\n      observation_shape: tuple of ints.\n      stack_size: int, number of frames to use in state stack.\n      use_staging: bool, when True it would use a staging area to prefetch\n        the next sampling batch.\n      replay_capacity: int, number of transitions to keep in memory.\n      batch_size: int.\n      update_horizon: int, length of update ('n' in n-step update).\n      gamma: int, the discount factor.\n      wrapped_memory: The 'inner' memory data structure. If None,\n        it creates the standard DQN replay memory.\n      max_sample_attempts: int, the maximum number of attempts allowed to\n        get a sample.\n      extra_storage_types: list of ReplayElements defining the type of the extra\n        contents that will be stored and returned by sample_transition_batch.\n      observation_dtype: np.dtype, type of the observations. Defaults to\n        np.uint8 for Atari 2600.\n\n    Raises:\n      ValueError: If update_horizon is not positive.\n      ValueError: If discount factor is not in [0, 1].\n    \"\"\"", "\n", "if", "replay_capacity", "<", "update_horizon", "+", "1", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "'Update horizon ({}) should be significantly smaller '", "\n", "'than replay capacity ({}).'", ".", "format", "(", "update_horizon", ",", "replay_capacity", ")", ")", "\n", "", "if", "not", "update_horizon", ">=", "1", ":", "\n", "      ", "raise", "ValueError", "(", "'Update horizon must be positive.'", ")", "\n", "", "if", "not", "0.0", "<=", "gamma", "<=", "1.0", ":", "\n", "      ", "raise", "ValueError", "(", "'Discount factor (gamma) must be in [0, 1].'", ")", "\n", "\n", "", "self", ".", "batch_size", "=", "batch_size", "\n", "\n", "# Mainly used to allow subclasses to pass self.memory.", "\n", "if", "wrapped_memory", "is", "not", "None", ":", "\n", "      ", "self", ".", "memory", "=", "wrapped_memory", "\n", "", "else", ":", "\n", "      ", "self", ".", "memory", "=", "OutOfGraphReplayBuffer", "(", "\n", "observation_shape", ",", "stack_size", ",", "replay_capacity", ",", "batch_size", ",", "\n", "update_horizon", ",", "gamma", ",", "max_sample_attempts", ",", "\n", "observation_dtype", "=", "observation_dtype", ",", "\n", "extra_storage_types", "=", "extra_storage_types", ")", "\n", "\n", "", "self", ".", "create_sampling_ops", "(", "use_staging", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.add": [[708, 726], ["circular_replay_buffer.WrappedReplayBuffer.memory.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "def", "add", "(", "self", ",", "observation", ",", "action", ",", "reward", ",", "terminal", ",", "*", "args", ")", ":", "\n", "    ", "\"\"\"Adds a transition to the replay memory.\n\n    Since the next_observation in the transition will be the observation added\n    next there is no need to pass it.\n\n    If the replay memory is at capacity the oldest transition will be discarded.\n\n    Args:\n      observation: np.array with shape observation_shape.\n      action: int, the action in the transition.\n      reward: float, the reward received in the transition.\n      terminal: A uint8 acting as a boolean indicating whether the transition\n                was terminal (1) or not (0).\n      *args: extra contents with shapes and dtypes according to\n        extra_storage_types.\n    \"\"\"", "\n", "self", ".", "memory", ".", "add", "(", "observation", ",", "action", ",", "reward", ",", "terminal", ",", "*", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.create_sampling_ops": [[727, 750], ["tensorflow.name_scope", "tensorflow.device", "circular_replay_buffer.WrappedReplayBuffer.memory.get_transition_elements", "tensorflow.py_func", "circular_replay_buffer.WrappedReplayBuffer._set_transition_shape", "circular_replay_buffer.WrappedReplayBuffer.unpack_transition", "circular_replay_buffer.WrappedReplayBuffer._set_up_staging", "circular_replay_buffer.WrappedReplayBuffer._set_transition_shape"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_transition_elements", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer._set_transition_shape", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.unpack_transition", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer._set_up_staging", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer._set_transition_shape"], ["", "def", "create_sampling_ops", "(", "self", ",", "use_staging", ")", ":", "\n", "    ", "\"\"\"Creates the ops necessary to sample from the replay buffer.\n\n    Creates the transition dictionary containing the sampling tensors.\n\n    Args:\n      use_staging: bool, when True it would use a staging area to prefetch\n        the next sampling batch.\n    \"\"\"", "\n", "with", "tf", ".", "name_scope", "(", "'sample_replay'", ")", ":", "\n", "      ", "with", "tf", ".", "device", "(", "'/cpu:*'", ")", ":", "\n", "        ", "transition_type", "=", "self", ".", "memory", ".", "get_transition_elements", "(", ")", "\n", "transition_tensors", "=", "tf", ".", "py_func", "(", "\n", "self", ".", "memory", ".", "sample_transition_batch", ",", "[", "]", ",", "\n", "[", "return_entry", ".", "type", "for", "return_entry", "in", "transition_type", "]", ",", "\n", "name", "=", "'replay_sample_py_func'", ")", "\n", "self", ".", "_set_transition_shape", "(", "transition_tensors", ",", "transition_type", ")", "\n", "if", "use_staging", ":", "\n", "          ", "transition_tensors", "=", "self", ".", "_set_up_staging", "(", "transition_tensors", ")", "\n", "self", ".", "_set_transition_shape", "(", "transition_tensors", ",", "transition_type", ")", "\n", "\n", "# Unpack sample transition into member variables.", "\n", "", "self", ".", "unpack_transition", "(", "transition_tensors", ",", "transition_type", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer._set_transition_shape": [[751, 761], ["zip", "element.set_shape"], "methods", ["None"], ["", "", "", "def", "_set_transition_shape", "(", "self", ",", "transition", ",", "transition_type", ")", ":", "\n", "    ", "\"\"\"Set shape for each element in the transition.\n\n    Args:\n      transition: tuple of tf.Tensors.\n      transition_type: tuple of ReplayElements descriving the shapes of the\n        respective tensors.\n    \"\"\"", "\n", "for", "element", ",", "element_type", "in", "zip", "(", "transition", ",", "transition_type", ")", ":", "\n", "      ", "element", ".", "set_shape", "(", "element_type", ".", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer._set_up_staging": [[762, 795], ["circular_replay_buffer.WrappedReplayBuffer.memory.get_transition_elements", "tensorflow.contrib.staging.StagingArea", "tensorflow.contrib.staging.StagingArea.put", "tensorflow.cond", "tensorflow.equal", "tensorflow.control_dependencies", "tensorflow.contrib.staging.StagingArea.get", "tensorflow.contrib.staging.StagingArea.size", "tensorflow.contrib.staging.StagingArea.put"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_transition_elements", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.get", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.size"], ["", "", "def", "_set_up_staging", "(", "self", ",", "transition", ")", ":", "\n", "    ", "\"\"\"Sets up staging ops for prefetching the next transition.\n\n    This allows us to hide the py_func latency. To do so we use a staging area\n    to pre-fetch the next batch of transitions.\n\n    Args:\n      transition: tuple of tf.Tensors with shape\n        memory.get_transition_elements().\n\n    Returns:\n      prefetched_transition: tuple of tf.Tensors with shape\n        memory.get_transition_elements() that have been previously prefetched.\n    \"\"\"", "\n", "transition_type", "=", "self", ".", "memory", ".", "get_transition_elements", "(", ")", "\n", "\n", "# Create the staging area in CPU.", "\n", "prefetch_area", "=", "tf", ".", "contrib", ".", "staging", ".", "StagingArea", "(", "\n", "[", "shape_with_type", ".", "type", "for", "shape_with_type", "in", "transition_type", "]", ")", "\n", "\n", "# Store prefetch op for tests, but keep it private -- users should not be", "\n", "# calling _prefetch_batch.", "\n", "self", ".", "_prefetch_batch", "=", "prefetch_area", ".", "put", "(", "transition", ")", "\n", "initial_prefetch", "=", "tf", ".", "cond", "(", "\n", "tf", ".", "equal", "(", "prefetch_area", ".", "size", "(", ")", ",", "0", ")", ",", "\n", "lambda", ":", "prefetch_area", ".", "put", "(", "transition", ")", ",", "tf", ".", "no_op", ")", "\n", "\n", "# Every time a transition is sampled self.prefetch_batch will be", "\n", "# called. If the staging area is empty, two put ops will be called.", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "self", ".", "_prefetch_batch", ",", "initial_prefetch", "]", ")", ":", "\n", "      ", "prefetched_transition", "=", "prefetch_area", ".", "get", "(", ")", "\n", "\n", "", "return", "prefetched_transition", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.unpack_transition": [[796, 815], ["collections.OrderedDict", "zip"], "methods", ["None"], ["", "def", "unpack_transition", "(", "self", ",", "transition_tensors", ",", "transition_type", ")", ":", "\n", "    ", "\"\"\"Unpacks the given transition into member variables.\n\n    Args:\n      transition_tensors: tuple of tf.Tensors.\n      transition_type: tuple of ReplayElements matching transition_tensors.\n    \"\"\"", "\n", "self", ".", "transition", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "for", "element", ",", "element_type", "in", "zip", "(", "transition_tensors", ",", "transition_type", ")", ":", "\n", "      ", "self", ".", "transition", "[", "element_type", ".", "name", "]", "=", "element", "\n", "\n", "# TODO(bellemare): These are legacy and should probably be removed in", "\n", "# future versions.", "\n", "", "self", ".", "states", "=", "self", ".", "transition", "[", "'state'", "]", "\n", "self", ".", "actions", "=", "self", ".", "transition", "[", "'action'", "]", "\n", "self", ".", "rewards", "=", "self", ".", "transition", "[", "'reward'", "]", "\n", "self", ".", "next_states", "=", "self", ".", "transition", "[", "'next_state'", "]", "\n", "self", ".", "terminals", "=", "self", ".", "transition", "[", "'terminal'", "]", "\n", "self", ".", "indices", "=", "self", ".", "transition", "[", "'indices'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save": [[816, 826], ["circular_replay_buffer.WrappedReplayBuffer.memory.save"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save"], ["", "def", "save", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ")", ":", "\n", "    ", "\"\"\"Save the underlying replay buffer's contents in a file.\n\n    Args:\n      checkpoint_dir: str, the directory where to read the numpy checkpointed\n        files from.\n      iteration_number: int, the iteration_number to use as a suffix in naming\n        numpy checkpoint files.\n    \"\"\"", "\n", "self", ".", "memory", ".", "save", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load": [[827, 836], ["circular_replay_buffer.WrappedReplayBuffer.memory.load"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load"], ["", "def", "load", "(", "self", ",", "checkpoint_dir", ",", "suffix", ")", ":", "\n", "    ", "\"\"\"Loads the replay buffer's state from a saved file.\n\n    Args:\n      checkpoint_dir: str, the directory where to read the numpy checkpointed\n        files from.\n      suffix: str, the suffix to use in numpy checkpoint files.\n    \"\"\"", "\n", "self", ".", "memory", ".", "load", "(", "checkpoint_dir", ",", "suffix", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.invalid_range": [[54, 79], ["numpy.array", "range"], "function", ["None"], ["def", "invalid_range", "(", "cursor", ",", "replay_capacity", ",", "stack_size", ",", "update_horizon", ")", ":", "\n", "  ", "\"\"\"Returns a array with the indices of cursor-related invalid transitions.\n\n  There are update_horizon + stack_size invalid indices:\n    - The update_horizon indices before the cursor, because we do not have a\n      valid N-step transition (including the next state).\n    - The stack_size indices on or immediately after the cursor.\n  If N = update_horizon, K = stack_size, and the cursor is at c, invalid\n  indices are:\n    c - N, c - N + 1, ..., c, c + 1, ..., c + K - 1.\n\n  It handles special cases in a circular buffer in the beginning and the end.\n\n  Args:\n    cursor: int, the position of the cursor.\n    replay_capacity: int, the size of the replay memory.\n    stack_size: int, the size of the stacks returned by the replay memory.\n    update_horizon: int, the agent's update horizon.\n  Returns:\n    np.array of size stack_size with the invalid indices.\n  \"\"\"", "\n", "assert", "cursor", "<", "replay_capacity", "\n", "return", "np", ".", "array", "(", "\n", "[", "(", "cursor", "-", "update_horizon", "+", "i", ")", "%", "replay_capacity", "\n", "for", "i", "in", "range", "(", "stack_size", "+", "update_horizon", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.__init__": [[43, 81], ["dopamine.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.__init__", "dopamine.replay_memory.sum_tree.SumTree"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.__init__"], ["def", "__init__", "(", "self", ",", "\n", "observation_shape", ",", "\n", "stack_size", ",", "\n", "replay_capacity", ",", "\n", "batch_size", ",", "\n", "update_horizon", "=", "1", ",", "\n", "gamma", "=", "0.99", ",", "\n", "max_sample_attempts", "=", "circular_replay_buffer", ".", "MAX_SAMPLE_ATTEMPTS", ",", "\n", "extra_storage_types", "=", "None", ",", "\n", "observation_dtype", "=", "np", ".", "uint8", ")", ":", "\n", "    ", "\"\"\"Initializes OutOfGraphPrioritizedReplayBuffer.\n\n    Args:\n      observation_shape: tuple of ints.\n      stack_size: int, number of frames to use in state stack.\n      replay_capacity: int, number of transitions to keep in memory.\n      batch_size: int.\n      update_horizon: int, length of update ('n' in n-step update).\n      gamma: int, the discount factor.\n      max_sample_attempts: int, the maximum number of attempts allowed to\n        get a sample.\n      extra_storage_types: list of ReplayElements defining the type of the extra\n        contents that will be stored and returned by sample_transition_batch.\n      observation_dtype: np.dtype, type of the observations. Defaults to\n        np.uint8 for Atari 2600.\n    \"\"\"", "\n", "super", "(", "OutOfGraphPrioritizedReplayBuffer", ",", "self", ")", ".", "__init__", "(", "\n", "observation_shape", "=", "observation_shape", ",", "\n", "stack_size", "=", "stack_size", ",", "\n", "replay_capacity", "=", "replay_capacity", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "update_horizon", "=", "update_horizon", ",", "\n", "gamma", "=", "gamma", ",", "\n", "max_sample_attempts", "=", "max_sample_attempts", ",", "\n", "extra_storage_types", "=", "extra_storage_types", ",", "\n", "observation_dtype", "=", "observation_dtype", ")", "\n", "\n", "self", ".", "sum_tree", "=", "sum_tree", ".", "SumTree", "(", "replay_capacity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_add_args_signature": [[82, 98], ["super().get_add_args_signature", "dopamine.replay_memory.circular_replay_buffer.ReplayElement"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_add_args_signature"], ["", "def", "get_add_args_signature", "(", "self", ")", ":", "\n", "    ", "\"\"\"The signature of the add function.\n\n    The signature is the same as the one for OutOfGraphReplayBuffer, with an\n    added priority.\n\n    Returns:\n      list of ReplayElements defining the type of the argument signature needed\n        by the add function.\n    \"\"\"", "\n", "parent_add_signature", "=", "super", "(", "OutOfGraphPrioritizedReplayBuffer", ",", "\n", "self", ")", ".", "get_add_args_signature", "(", ")", "\n", "add_signature", "=", "parent_add_signature", "+", "[", "\n", "ReplayElement", "(", "'priority'", ",", "(", ")", ",", "np", ".", "float32", ")", "\n", "]", "\n", "return", "add_signature", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer._add": [[99, 123], ["enumerate", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.sum_tree.set", "super()._add", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_add_args_signature", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.cursor", "parent_add_args.append"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.set", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer._add", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_add_args_signature", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.cursor", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "_add", "(", "self", ",", "*", "args", ")", ":", "\n", "    ", "\"\"\"Internal add method to add to the underlying memory arrays.\n\n    The arguments need to match add_arg_signature.\n\n    If priority is none, it is set to the maximum priority ever seen.\n\n    Args:\n      *args: All the elements in a transition.\n    \"\"\"", "\n", "# Use Schaul et al.'s (2015) scheme of setting the priority of new elements", "\n", "# to the maximum priority so far.", "\n", "parent_add_args", "=", "[", "]", "\n", "# Picks out 'priority' from arguments and passes the other arguments to the", "\n", "# parent method.", "\n", "for", "i", ",", "element", "in", "enumerate", "(", "self", ".", "get_add_args_signature", "(", ")", ")", ":", "\n", "      ", "if", "element", ".", "name", "==", "'priority'", ":", "\n", "        ", "priority", "=", "args", "[", "i", "]", "\n", "", "else", ":", "\n", "        ", "parent_add_args", ".", "append", "(", "args", "[", "i", "]", ")", "\n", "\n", "", "", "self", ".", "sum_tree", ".", "set", "(", "self", ".", "cursor", "(", ")", ",", "priority", ")", "\n", "\n", "super", "(", "OutOfGraphPrioritizedReplayBuffer", ",", "self", ")", ".", "_add", "(", "*", "parent_add_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.sample_index_batch": [[124, 154], ["prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.sum_tree.stratified_sample", "range", "len", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.is_valid_transition", "RuntimeError", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.sum_tree.sample", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.is_valid_transition"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.stratified_sample", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_valid_transition", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.sample", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.OutOfGraphReplayBuffer.is_valid_transition"], ["", "def", "sample_index_batch", "(", "self", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Returns a batch of valid indices sampled as in Schaul et al. (2015).\n\n    Args:\n      batch_size: int, number of indices returned.\n\n    Returns:\n      list of ints, a batch of valid indices sampled uniformly.\n\n    Raises:\n      Exception: If the batch was not constructed after maximum number of tries.\n    \"\"\"", "\n", "# Sample stratified indices. Some of them might be invalid.", "\n", "indices", "=", "self", ".", "sum_tree", ".", "stratified_sample", "(", "batch_size", ")", "\n", "allowed_attempts", "=", "self", ".", "_max_sample_attempts", "\n", "for", "i", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "      ", "if", "not", "self", ".", "is_valid_transition", "(", "indices", "[", "i", "]", ")", ":", "\n", "        ", "if", "allowed_attempts", "==", "0", ":", "\n", "          ", "raise", "RuntimeError", "(", "\n", "'Max saple attempsts: Tried {} times but only sampled {}'", "\n", "' valid indices. Batch size is {}'", ".", "\n", "format", "(", "self", ".", "_max_sample_attempts", ",", "i", ",", "batch_size", ")", ")", "\n", "", "index", "=", "indices", "[", "i", "]", "\n", "while", "not", "self", ".", "is_valid_transition", "(", "index", ")", "and", "allowed_attempts", ">", "0", ":", "\n", "# If index i is not valid keep sampling others. Note that this", "\n", "# is not stratified.", "\n", "          ", "index", "=", "self", ".", "sum_tree", ".", "sample", "(", ")", "\n", "allowed_attempts", "-=", "1", "\n", "", "indices", "[", "i", "]", "=", "index", "\n", "", "", "return", "indices", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.sample_transition_batch": [[155, 184], ["super().sample_transition_batch", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_transition_elements", "transition_names.index", "transition_names.index", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_priority"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.sample_transition_batch", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_transition_elements", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_priority"], ["", "def", "sample_transition_batch", "(", "self", ",", "batch_size", "=", "None", ",", "indices", "=", "None", ")", ":", "\n", "    ", "\"\"\"Returns a batch of transitions with extra storage and the priorities.\n\n    The extra storage are defined through the extra_storage_types constructor\n    argument.\n\n    When the transition is terminal next_state_batch has undefined contents.\n\n    Args:\n      batch_size: int, number of transitions returned. If None, the default\n        batch_size will be used.\n      indices: None or list of ints, the indices of every transition in the\n        batch. If None, sample the indices uniformly.\n\n    Returns:\n      transition_batch: tuple of np.arrays with the shape and type as in\n        get_transition_elements().\n    \"\"\"", "\n", "transition", "=", "(", "super", "(", "OutOfGraphPrioritizedReplayBuffer", ",", "self", ")", ".", "\n", "sample_transition_batch", "(", "batch_size", ",", "indices", ")", ")", "\n", "transition_elements", "=", "self", ".", "get_transition_elements", "(", "batch_size", ")", "\n", "transition_names", "=", "[", "e", ".", "name", "for", "e", "in", "transition_elements", "]", "\n", "probabilities_index", "=", "transition_names", ".", "index", "(", "'sampling_probabilities'", ")", "\n", "indices_index", "=", "transition_names", ".", "index", "(", "'indices'", ")", "\n", "indices", "=", "transition", "[", "indices_index", "]", "\n", "# The parent returned an empty array for the probabilities. Fill it with the", "\n", "# contents of the sum tree.", "\n", "transition", "[", "probabilities_index", "]", "[", ":", "]", "=", "self", ".", "get_priority", "(", "indices", ")", "\n", "return", "transition", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.set_priority": [[185, 197], ["zip", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.sum_tree.set"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.set"], ["", "def", "set_priority", "(", "self", ",", "indices", ",", "priorities", ")", ":", "\n", "    ", "\"\"\"Sets the priority of the given elements according to Schaul et al.\n\n    Args:\n      indices: np.array with dtype int32, of indices in range\n        [0, replay_capacity).\n      priorities: float, the corresponding priorities.\n    \"\"\"", "\n", "assert", "indices", ".", "dtype", "==", "np", ".", "int32", ",", "(", "'Indices must be integers, '", "\n", "'given: {}'", ".", "format", "(", "indices", ".", "dtype", ")", ")", "\n", "for", "index", ",", "priority", "in", "zip", "(", "indices", ",", "priorities", ")", ":", "\n", "      ", "self", ".", "sum_tree", ".", "set", "(", "index", ",", "priority", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_priority": [[198, 218], ["len", "numpy.empty", "enumerate", "prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.sum_tree.get"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.get"], ["", "", "def", "get_priority", "(", "self", ",", "indices", ")", ":", "\n", "    ", "\"\"\"Fetches the priorities correspond to a batch of memory indices.\n\n    For any memory location not yet used, the corresponding priority is 0.\n\n    Args:\n      indices: np.array with dtype int32, of indices in range\n        [0, replay_capacity).\n\n    Returns:\n      priorities: float, the corresponding priorities.\n    \"\"\"", "\n", "assert", "indices", ".", "shape", ",", "'Indices must be an array.'", "\n", "assert", "indices", ".", "dtype", "==", "np", ".", "int32", ",", "(", "'Indices must be int32s, '", "\n", "'given: {}'", ".", "format", "(", "indices", ".", "dtype", ")", ")", "\n", "batch_size", "=", "len", "(", "indices", ")", "\n", "priority_batch", "=", "np", ".", "empty", "(", "(", "batch_size", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "for", "i", ",", "memory_index", "in", "enumerate", "(", "indices", ")", ":", "\n", "      ", "priority_batch", "[", "i", "]", "=", "self", ".", "sum_tree", ".", "get", "(", "memory_index", ")", "\n", "", "return", "priority_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_transition_elements": [[219, 235], ["super().get_transition_elements", "dopamine.replay_memory.circular_replay_buffer.ReplayElement"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer.get_transition_elements"], ["", "def", "get_transition_elements", "(", "self", ",", "batch_size", "=", "None", ")", ":", "\n", "    ", "\"\"\"Returns a 'type signature' for sample_transition_batch.\n\n    Args:\n      batch_size: int, number of transitions returned. If None, the default\n        batch_size will be used.\n    Returns:\n      signature: A namedtuple describing the method's return type signature.\n    \"\"\"", "\n", "parent_transition_type", "=", "(", "\n", "super", "(", "OutOfGraphPrioritizedReplayBuffer", ",", "\n", "self", ")", ".", "get_transition_elements", "(", "batch_size", ")", ")", "\n", "probablilities_type", "=", "[", "\n", "ReplayElement", "(", "'sampling_probabilities'", ",", "(", "batch_size", ",", ")", ",", "np", ".", "float32", ")", "\n", "]", "\n", "return", "parent_transition_type", "+", "probablilities_type", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.WrappedPrioritizedReplayBuffer.__init__": [[252, 299], ["prioritized_replay_buffer.OutOfGraphPrioritizedReplayBuffer", "dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer.__init__"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.__init__"], ["def", "__init__", "(", "self", ",", "\n", "observation_shape", ",", "\n", "stack_size", ",", "\n", "use_staging", "=", "True", ",", "\n", "replay_capacity", "=", "1000000", ",", "\n", "batch_size", "=", "32", ",", "\n", "update_horizon", "=", "1", ",", "\n", "gamma", "=", "0.99", ",", "\n", "max_sample_attempts", "=", "circular_replay_buffer", ".", "MAX_SAMPLE_ATTEMPTS", ",", "\n", "extra_storage_types", "=", "None", ",", "\n", "observation_dtype", "=", "np", ".", "uint8", ")", ":", "\n", "    ", "\"\"\"Initializes WrappedPrioritizedReplayBuffer.\n\n    Args:\n      observation_shape: tuple of ints.\n      stack_size: int, number of frames to use in state stack.\n      use_staging: bool, when True it would use a staging area to prefetch\n        the next sampling batch.\n      replay_capacity: int, number of transitions to keep in memory.\n      batch_size: int.\n      update_horizon: int, length of update ('n' in n-step update).\n      gamma: int, the discount factor.\n      max_sample_attempts: int, the maximum number of attempts allowed to\n        get a sample.\n      extra_storage_types: list of ReplayElements defining the type of the extra\n        contents that will be stored and returned by sample_transition_batch.\n      observation_dtype: np.dtype, type of the observations. Defaults to\n        np.uint8 for Atari 2600.\n\n    Raises:\n      ValueError: If update_horizon is not positive.\n      ValueError: If discount factor is not in [0, 1].\n    \"\"\"", "\n", "memory", "=", "OutOfGraphPrioritizedReplayBuffer", "(", "\n", "observation_shape", ",", "stack_size", ",", "replay_capacity", ",", "batch_size", ",", "\n", "update_horizon", ",", "gamma", ",", "max_sample_attempts", ",", "\n", "extra_storage_types", "=", "extra_storage_types", ")", "\n", "super", "(", "WrappedPrioritizedReplayBuffer", ",", "self", ")", ".", "__init__", "(", "\n", "observation_shape", ",", "\n", "stack_size", ",", "\n", "use_staging", ",", "\n", "replay_capacity", ",", "\n", "batch_size", ",", "\n", "update_horizon", ",", "\n", "gamma", ",", "\n", "wrapped_memory", "=", "memory", ",", "\n", "extra_storage_types", "=", "extra_storage_types", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.WrappedPrioritizedReplayBuffer.tf_set_priority": [[300, 313], ["tensorflow.py_func"], "methods", ["None"], ["", "def", "tf_set_priority", "(", "self", ",", "indices", ",", "priorities", ")", ":", "\n", "    ", "\"\"\"Sets the priorities for the given indices.\n\n    Args:\n      indices: tf.Tensor with dtype int32 and shape [n].\n      priorities: tf.Tensor with dtype float and shape [n].\n\n    Returns:\n       A tf op setting the priorities for prioritized sampling.\n    \"\"\"", "\n", "return", "tf", ".", "py_func", "(", "\n", "self", ".", "memory", ".", "set_priority", ",", "[", "indices", ",", "priorities", "]", ",", "[", "]", ",", "\n", "name", "=", "'prioritized_replay_set_priority_py_func'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.WrappedPrioritizedReplayBuffer.tf_get_priority": [[314, 328], ["tensorflow.py_func"], "methods", ["None"], ["", "def", "tf_get_priority", "(", "self", ",", "indices", ")", ":", "\n", "    ", "\"\"\"Gets the priorities for the given indices.\n\n    Args:\n      indices: tf.Tensor with dtype int32 and shape [n].\n\n    Returns:\n      priorities: tf.Tensor with dtype float and shape [n], the priorities at\n        the indices.\n    \"\"\"", "\n", "return", "tf", ".", "py_func", "(", "\n", "self", ".", "memory", ".", "get_priority", ",", "[", "indices", "]", ",", "\n", "tf", ".", "float32", ",", "\n", "name", "=", "'prioritized_replay_get_priority_py_func'", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.__init__": [[65, 90], ["isinstance", "int", "range", "ValueError", "math.ceil", "numpy.zeros", "sum_tree.SumTree.nodes.append", "numpy.log2"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["def", "__init__", "(", "self", ",", "capacity", ")", ":", "\n", "    ", "\"\"\"Creates the sum tree data structure for the given replay capacity.\n\n    Args:\n      capacity: int, the maximum number of elements that can be stored in this\n        data structure.\n\n    Raises:\n      ValueError: If requested capacity is not positive.\n    \"\"\"", "\n", "assert", "isinstance", "(", "capacity", ",", "int", ")", "\n", "if", "capacity", "<=", "0", ":", "\n", "      ", "raise", "ValueError", "(", "'Sum tree capacity should be positive. Got: {}'", ".", "\n", "format", "(", "capacity", ")", ")", "\n", "\n", "", "self", ".", "nodes", "=", "[", "]", "\n", "tree_depth", "=", "int", "(", "math", ".", "ceil", "(", "np", ".", "log2", "(", "capacity", ")", ")", ")", "\n", "level_size", "=", "1", "\n", "for", "_", "in", "range", "(", "tree_depth", "+", "1", ")", ":", "\n", "      ", "nodes_at_this_depth", "=", "np", ".", "zeros", "(", "level_size", ")", "\n", "self", ".", "nodes", ".", "append", "(", "nodes_at_this_depth", ")", "\n", "\n", "level_size", "*=", "2", "\n", "\n", "", "self", ".", "max_recorded_priority", "=", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree._total_priority": [[91, 98], ["None"], "methods", ["None"], ["", "def", "_total_priority", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns the sum of all priorities stored in this sum tree.\n\n    Returns:\n      float, sum of priorities stored in this sum tree.\n    \"\"\"", "\n", "return", "self", ".", "nodes", "[", "0", "]", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.sample": [[99, 142], ["sum_tree.SumTree._total_priority", "sum_tree.SumTree._total_priority", "Exception", "ValueError", "random.random"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree._total_priority", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree._total_priority"], ["", "def", "sample", "(", "self", ",", "query_value", "=", "None", ")", ":", "\n", "    ", "\"\"\"Samples an element from the sum tree.\n\n    Each element has probability p_i / sum_j p_j of being picked, where p_i is\n    the (positive) value associated with node i (possibly unnormalized).\n\n    Args:\n      query_value: float in [0, 1], used as the random value to select a\n      sample. If None, will select one randomly in [0, 1).\n\n    Returns:\n      int, a random element from the sum tree.\n\n    Raises:\n      Exception: If the sum tree is empty (i.e. its node values sum to 0), or if\n        the supplied query_value is larger than the total sum.\n    \"\"\"", "\n", "if", "self", ".", "_total_priority", "(", ")", "==", "0.0", ":", "\n", "      ", "raise", "Exception", "(", "'Cannot sample from an empty sum tree.'", ")", "\n", "\n", "", "if", "query_value", "and", "(", "query_value", "<", "0.", "or", "query_value", ">", "1.", ")", ":", "\n", "      ", "raise", "ValueError", "(", "'query_value must be in [0, 1].'", ")", "\n", "\n", "# Sample a value in range [0, R), where R is the value stored at the root.", "\n", "", "query_value", "=", "random", ".", "random", "(", ")", "if", "query_value", "is", "None", "else", "query_value", "\n", "query_value", "*=", "self", ".", "_total_priority", "(", ")", "\n", "\n", "# Now traverse the sum tree.", "\n", "node_index", "=", "0", "\n", "for", "nodes_at_this_depth", "in", "self", ".", "nodes", "[", "1", ":", "]", ":", "\n", "# Compute children of previous depth's node.", "\n", "      ", "left_child", "=", "node_index", "*", "2", "\n", "\n", "left_sum", "=", "nodes_at_this_depth", "[", "left_child", "]", "\n", "# Each subtree describes a range [0, a), where a is its value.", "\n", "if", "query_value", "<", "left_sum", ":", "# Recurse into left subtree.", "\n", "        ", "node_index", "=", "left_child", "\n", "", "else", ":", "# Recurse into right subtree.", "\n", "        ", "node_index", "=", "left_child", "+", "1", "\n", "# Adjust query to be relative to right subtree.", "\n", "query_value", "-=", "left_sum", "\n", "\n", "", "", "return", "node_index", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.stratified_sample": [[143, 167], ["numpy.linspace", "sum_tree.SumTree._total_priority", "Exception", "len", "random.uniform", "sum_tree.SumTree.sample", "range"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree._total_priority", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.sample"], ["", "def", "stratified_sample", "(", "self", ",", "batch_size", ")", ":", "\n", "    ", "\"\"\"Performs stratified sampling using the sum tree.\n\n    Let R be the value at the root (total value of sum tree). This method will\n    divide [0, R) into batch_size segments, pick a random number from each of\n    those segments, and use that random number to sample from the sum_tree. This\n    is as specified in Schaul et al. (2015).\n\n    Args:\n      batch_size: int, the number of strata to use.\n    Returns:\n      list of batch_size elements sampled from the sum tree.\n\n    Raises:\n      Exception: If the sum tree is empty (i.e. its node values sum to 0).\n    \"\"\"", "\n", "if", "self", ".", "_total_priority", "(", ")", "==", "0.0", ":", "\n", "      ", "raise", "Exception", "(", "'Cannot sample from an empty sum tree.'", ")", "\n", "\n", "", "bounds", "=", "np", ".", "linspace", "(", "0.", ",", "1.", ",", "batch_size", "+", "1", ")", "\n", "assert", "len", "(", "bounds", ")", "==", "batch_size", "+", "1", "\n", "segments", "=", "[", "(", "bounds", "[", "i", "]", ",", "bounds", "[", "i", "+", "1", "]", ")", "for", "i", "in", "range", "(", "batch_size", ")", "]", "\n", "query_values", "=", "[", "random", ".", "uniform", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", "for", "x", "in", "segments", "]", "\n", "return", "[", "self", ".", "sample", "(", "query_value", "=", "x", ")", "for", "x", "in", "query_values", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.get": [[168, 177], ["None"], "methods", ["None"], ["", "def", "get", "(", "self", ",", "node_index", ")", ":", "\n", "    ", "\"\"\"Returns the value of the leaf node corresponding to the index.\n\n    Args:\n      node_index: The index of the leaf node.\n    Returns:\n      The value of the leaf node.\n    \"\"\"", "\n", "return", "self", ".", "nodes", "[", "-", "1", "]", "[", "node_index", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.set": [[178, 205], ["max", "reversed", "ValueError"], "methods", ["None"], ["", "def", "set", "(", "self", ",", "node_index", ",", "value", ")", ":", "\n", "    ", "\"\"\"Sets the value of a leaf node and updates internal nodes accordingly.\n\n    This operation takes O(log(capacity)).\n    Args:\n      node_index: int, the index of the leaf node to be updated.\n      value: float, the value which we assign to the node. This value must be\n        nonnegative. Setting value = 0 will cause the element to never be\n        sampled.\n\n    Raises:\n      ValueError: If the given value is negative.\n    \"\"\"", "\n", "if", "value", "<", "0.0", ":", "\n", "      ", "raise", "ValueError", "(", "'Sum tree values should be nonnegative. Got {}'", ".", "\n", "format", "(", "value", ")", ")", "\n", "", "self", ".", "max_recorded_priority", "=", "max", "(", "value", ",", "self", ".", "max_recorded_priority", ")", "\n", "\n", "delta_value", "=", "value", "-", "self", ".", "nodes", "[", "-", "1", "]", "[", "node_index", "]", "\n", "\n", "# Now traverse back the tree, adjusting all sums along the way.", "\n", "for", "nodes_at_this_depth", "in", "reversed", "(", "self", ".", "nodes", ")", ":", "\n", "# Note: Adding a delta leads to some tolerable numerical inaccuracies.", "\n", "      ", "nodes_at_this_depth", "[", "node_index", "]", "+=", "delta_value", "\n", "node_index", "//=", "2", "\n", "\n", "", "assert", "node_index", "==", "0", ",", "(", "'Sum tree traversal failed, final node index '", "\n", "'is not 0.'", ")", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.ReplayBufferRegular.__init__": [[24, 31], ["collections.deque", "random.seed"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "buffer_size", ",", "random_seed", "=", "1234", ")", ":", "\n", "        ", "self", ".", "buffer_size", "=", "buffer_size", "\n", "self", ".", "count", "=", "0", "\n", "# Right side of deque contains newest experience", "\n", "self", ".", "buffer", "=", "deque", "(", ")", "\n", "random", ".", "seed", "(", "random_seed", ")", "\n", "self", ".", "ptr", ",", "self", ".", "path_start_idx", "=", "0", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.ReplayBufferRegular.add": [[32, 38], ["agent_utils.ReplayBufferRegular.buffer.append"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "add", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "terminal", ")", ":", "\n", "        ", "experience", "=", "[", "state", ",", "action", ",", "reward", ",", "terminal", "]", "\n", "assert", "self", ".", "count", "<", "self", ".", "buffer_size", "\n", "self", ".", "buffer", ".", "append", "(", "experience", ")", "\n", "self", ".", "count", "+=", "1", "\n", "self", ".", "ptr", "+=", "1", "\n", "# else:", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.ReplayBufferRegular.get_sample": [[44, 47], ["agent_utils.ReplayBufferRegular.buffer.popleft"], "methods", ["None"], ["", "def", "get_sample", "(", "self", ")", ":", "\n", "        ", "self", ".", "count", "-=", "1", "\n", "return", "self", ".", "buffer", ".", "popleft", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.ReplayBufferRegular.size": [[48, 50], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.ReplayBufferRegular.clear": [[51, 56], ["agent_utils.ReplayBufferRegular.buffer.clear"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.clear"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "buffer", ".", "clear", "(", ")", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "ptr", "=", "0", "\n", "self", ".", "path_start_idx", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.collect_trajectory": [[4, 16], ["agent.replay_buffer.clear", "agent.replay_buffer.add", "agent.replay_buffer.add", "agent.replay_buffer.size", "agent.replay_buffer.get_sample", "agent._store_transition"], "function", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.clear", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.size", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.get_sample", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition"], ["def", "collect_trajectory", "(", "agent", ",", "reward", ")", ":", "\n", "    ", "\"\"\"for pong \"\"\"", "\n", "if", "reward", "<", "0", ":", "\n", "        ", "agent", ".", "replay_buffer", ".", "clear", "(", ")", "\n", "", "elif", "reward", ">", "0", ":", "\n", "        ", "agent", ".", "replay_buffer", ".", "add", "(", "agent", ".", "_last_observation", ",", "agent", ".", "action", ",", "reward", ",", "False", ")", "\n", "while", "agent", ".", "replay_buffer", ".", "size", "(", ")", ">", "0", ":", "\n", "            ", "experience", "=", "agent", ".", "replay_buffer", ".", "get_sample", "(", ")", "\n", "state", ",", "action", ",", "reward", ",", "_", "=", "experience", "\n", "agent", ".", "_store_transition", "(", "state", ",", "action", ",", "reward", ",", "False", ")", "\n", "", "", "else", ":", "\n", "        ", "agent", ".", "replay_buffer", ".", "add", "(", "agent", ".", "_last_observation", ",", "agent", ".", "action", ",", "reward", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent.__init__": [[69, 198], ["tensorflow.train.RMSPropOptimizer", "isinstance", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tuple", "math.pow", "lpg_agent.ReplayBufferRegular", "tensorflow.train.Saver", "tensorflow.device", "numpy.zeros", "tensorflow.placeholder", "lpg_agent.LPGAgent._build_replay_buffer", "lpg_agent.LPGAgent._build_networks", "lpg_agent.LPGAgent._build_train_op", "tensorflow.summary.merge_all"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_replay_buffer", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_networks", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_train_op"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "game_name", "=", "\"Pong\"", ",", "\n", "observation_shape", "=", "NATURE_DQN_OBSERVATION_SHAPE", ",", "\n", "observation_dtype", "=", "NATURE_DQN_DTYPE", ",", "\n", "stack_size", "=", "NATURE_DQN_STACK_SIZE", ",", "\n", "gamma", "=", "0.99", ",", "\n", "update_horizon", "=", "1", ",", "\n", "min_replay_history", "=", "20000", ",", "\n", "update_period", "=", "4", ",", "\n", "epsilon_fn", "=", "linearly_decaying_epsilon", ",", "\n", "epsilon_train", "=", "0.01", ",", "\n", "epsilon_eval", "=", "0.001", ",", "\n", "epsilon_decay_period", "=", "250000", ",", "\n", "margin", "=", "1", ",", "\n", "tf_device", "=", "'/cpu:*'", ",", "\n", "use_staging", "=", "True", ",", "\n", "max_tf_checkpoints_to_keep", "=", "3", ",", "\n", "optimizer", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "\n", "decay", "=", "0.95", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "0.00001", ",", "\n", "centered", "=", "True", ")", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "        ", "\"\"\"Initializes the agent and constructs the components of its graph.\n\n        Args:\n          sess: `tf.Session`, for executing ops.\n          num_actions: int, number of actions the agent can take at any state.\n          observation_shape: tuple of ints describing the observation shape.\n          observation_dtype: tf.DType, specifies the type of the observations. Note\n            that if your inputs are continuous, you should set this to tf.float32.\n          stack_size: int, number of frames to use in state stack.\n          gamma: float, discount factor with the usual RL meaning.\n          update_horizon: int, horizon at which updates are performed, the 'n' in\n            n-step update.\n          min_replay_history: int, number of transitions that should be experienced\n            before the agent begins training its value function.\n          update_period: int, period between DQN updates.\n          target_update_period: int, update period for the target network.\n          epsilon_fn: function expecting 4 parameters:\n            (decay_period, step, warmup_steps, epsilon). This function should return\n            the epsilon value used for exploration during training.\n          epsilon_train: float, the value to which the agent's epsilon is eventually\n            decayed during training.\n          epsilon_eval: float, epsilon used when evaluating the agent.\n          epsilon_decay_period: int, length of the epsilon decay schedule.\n          tf_device: str, Tensorflow device on which the agent's graph is executed.\n          use_staging: bool, when True use a staging area to prefetch the next\n            training batch, speeding training up by about 30%.\n          max_tf_checkpoints_to_keep: int, the number of TensorFlow checkpoints to\n            keep.\n          optimizer: `tf.train.Optimizer`, for training the value function.\n          summary_writer: SummaryWriter object for outputting training statistics.\n            Summary writing disabled if set to None.\n          summary_writing_frequency: int, frequency with which summaries will be\n            written. Lower values will result in slower training.\n        \"\"\"", "\n", "assert", "isinstance", "(", "observation_shape", ",", "tuple", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Creating %s agent with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t gamma: %f'", ",", "gamma", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_horizon: %f'", ",", "update_horizon", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t min_replay_history: %d'", ",", "min_replay_history", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_period: %d'", ",", "update_period", ")", "\n", "# tf.logging.info('\\t random_seed: %d', random_seed)", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_train: %f'", ",", "epsilon_train", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_eval: %f'", ",", "epsilon_eval", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_decay_period: %d'", ",", "epsilon_decay_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t tf_device: %s'", ",", "tf_device", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t use_staging: %s'", ",", "use_staging", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t optimizer: %s'", ",", "optimizer", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t game: %s'", ",", "game_name", ")", "\n", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "self", ".", "observation_shape", "=", "tuple", "(", "observation_shape", ")", "\n", "self", ".", "observation_dtype", "=", "observation_dtype", "\n", "self", ".", "stack_size", "=", "stack_size", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "update_horizon", "=", "update_horizon", "\n", "self", ".", "cumulative_gamma", "=", "math", ".", "pow", "(", "gamma", ",", "update_horizon", ")", "\n", "self", ".", "min_replay_history", "=", "min_replay_history", "\n", "self", ".", "epsilon_fn", "=", "epsilon_fn", "\n", "self", ".", "epsilon_train", "=", "epsilon_train", "\n", "self", ".", "epsilon_eval", "=", "epsilon_eval", "\n", "self", ".", "epsilon_decay_period", "=", "epsilon_decay_period", "\n", "self", ".", "update_period", "=", "update_period", "\n", "self", ".", "eval_mode", "=", "False", "\n", "self", ".", "training_steps", "=", "0", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "summary_writer", "=", "summary_writer", "\n", "self", ".", "summary_writing_frequency", "=", "summary_writing_frequency", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "start_training", "=", "1000", "\n", "# todo task specific FOR PONG IS 1000 IF THIS IS TOO SMALL WE END UP WITH A DETERMINISTIC POLICY QUCKKLY", "\n", "self", ".", "highest_reward", "=", "6", "# todo task specific", "\n", "self", ".", "isPrinted", "=", "False", "\n", "self", ".", "current_replay_size", "=", "0", "\n", "self", ".", "epsilon_current", "=", "1", "\n", "\n", "with", "tf", ".", "device", "(", "tf_device", ")", ":", "\n", "# Create a placeholder for the state input to the DQN network.", "\n", "# The last axis indicates the number of consecutive frames stacked.", "\n", "            ", "state_shape", "=", "(", "1", ",", ")", "+", "self", ".", "observation_shape", "+", "(", "stack_size", ",", ")", "\n", "self", ".", "state", "=", "np", ".", "zeros", "(", "state_shape", ")", "\n", "self", ".", "state_ph", "=", "tf", ".", "placeholder", "(", "self", ".", "observation_dtype", ",", "state_shape", ",", "\n", "name", "=", "'state_ph'", ")", "\n", "self", ".", "_replay", "=", "self", ".", "_build_replay_buffer", "(", "use_staging", ")", "\n", "\n", "self", ".", "_build_networks", "(", ")", "\n", "\n", "self", ".", "_train_op", "=", "self", ".", "_build_train_op", "(", ")", "\n", "\n", "", "self", ".", "replay_buffer", "=", "ReplayBufferRegular", "(", "100000", ")", "\n", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "# All tf.summaries should have been defined prior to running this.", "\n", "            ", "self", ".", "_merged_summaries", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "", "self", ".", "_sess", "=", "sess", "\n", "self", ".", "_saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "max_tf_checkpoints_to_keep", ")", "\n", "\n", "# Variables to be initialized by the agent once it interacts with the", "\n", "# environment.", "\n", "self", ".", "_observation", "=", "None", "\n", "self", ".", "_last_observation", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._get_network_type": [[199, 206], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the type of the outputs of a Q value network.\n\n        Returns:\n          net_type: _network_type object defining the outputs of the network.\n        \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "'DQN_network'", ",", "[", "'q_values'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._network_template": [[207, 225], ["tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "slim.fully_connected", "slim.fully_connected", "lpg_agent.LPGAgent._get_network_type"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Builds the convolutional network used to compute the agent's Q-values.\n\n        Args:\n          state: `tf.Tensor`, contains the agent's current state.\n\n        Returns:\n          net: _network_type object containing the tensors output by the network.\n        \"\"\"", "\n", "net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "net", "=", "tf", ".", "div", "(", "net", ",", "255.", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "net", ",", "512", ")", "\n", "q_values", "=", "slim", ".", "fully_connected", "(", "net", ",", "self", ".", "num_actions", ",", "activation_fn", "=", "None", ")", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._build_networks": [[226, 252], ["tensorflow.make_template", "lpg_agent.LPGAgent.online_convnet", "lpg_agent.LPGAgent.online_convnet", "tensorflow.nn.log_softmax", "tensorflow.distributions.Categorical().sample", "tensorflow.argmax", "tensorflow.distributions.Categorical"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.sample"], ["", "def", "_build_networks", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds the Q-value network computations needed for acting and training.\n\n        These are:\n          self.online_convnet: For computing the current state's Q-values.\n          self.target_convnet: For computing the next state's target Q-values.\n          self._net_outputs: The actual Q-values.\n          self._q_argmax: The action maximizing the current state's Q-values.\n          self._replay_net_outputs: The replayed states' Q-values.\n          self._replay_next_target_net_outputs: The replayed next states' target\n            Q-values (see Mnih et al., 2015 for details).\n        \"\"\"", "\n", "# Calling online_convnet will generate a new graph as defined in", "\n", "# self._get_network_template using whatever input is passed, but will always", "\n", "# share the same weights.", "\n", "self", ".", "online_convnet", "=", "tf", ".", "make_template", "(", "'Online'", ",", "self", ".", "_network_template", ")", "\n", "\n", "self", ".", "_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "state_ph", ")", "\n", "# using a deep network, but may affect performance with a linear", "\n", "# approximation scheme.", "\n", "self", ".", "_q_argmax", "=", "tf", ".", "argmax", "(", "self", ".", "_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "\n", "self", ".", "_replay_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "states", ")", "\n", "# treat self._net_outputs.q_values as logits", "\n", "self", ".", "logsoftmaxprob", "=", "tf", ".", "nn", ".", "log_softmax", "(", "self", ".", "_net_outputs", ".", "q_values", ")", "\n", "self", ".", "sample", "=", "Categorical", "(", "logits", "=", "self", ".", "logsoftmaxprob", ")", ".", "sample", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._build_replay_buffer": [[254, 271], ["dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer"], "methods", ["None"], ["", "def", "_build_replay_buffer", "(", "self", ",", "use_staging", ")", ":", "\n", "        ", "\"\"\"Creates the replay buffer used by the agent.\n\n        Args:\n          use_staging: bool, if True, uses a staging area to prefetch data for\n            faster training.\n\n        Returns:\n          A WrapperReplayBuffer object.\n        \"\"\"", "\n", "return", "circular_replay_buffer", ".", "WrappedReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "observation_dtype", "=", "self", ".", "observation_dtype", ".", "as_numpy_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._build_train_op": [[272, 292], ["tensorflow.one_hot", "tensorflow.nn.log_softmax", "tensorflow.reduce_mean", "lpg_agent.LPGAgent.optimizer.minimize", "tensorflow.reduce_sum", "tensorflow.variable_scope", "tensorflow.summary.scalar"], "methods", ["None"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds a training op.\n\n        Returns:\n          train_op: An op performing one step of training from replay data.\n        \"\"\"", "\n", "\n", "replay_action_one_hot", "=", "tf", ".", "one_hot", "(", "\n", "self", ".", "_replay", ".", "actions", ",", "self", ".", "num_actions", ",", "1.", ",", "0.", ",", "name", "=", "'action_one_hot'", ")", "\n", "logits", "=", "self", ".", "_replay_net_outputs", ".", "q_values", "\n", "self", ".", "logsoftmaxprob", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ")", "\n", "self", ".", "neglogprob", "=", "-", "tf", ".", "reduce_sum", "(", "self", ".", "logsoftmaxprob", "*", "replay_action_one_hot", ",", "axis", "=", "1", ")", "\n", "# self.temp_loss = self.neglogprob # * self.y_pl", "\n", "loss", "=", "self", ".", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "neglogprob", ")", "\n", "self", ".", "replay_action_one_hot", "=", "replay_action_one_hot", "\n", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "                ", "tf", ".", "summary", ".", "scalar", "(", "'hingeLoss'", ",", "loss", ")", "\n", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent.begin_episode": [[293, 310], ["lpg_agent.LPGAgent._reset_state", "lpg_agent.LPGAgent._record_observation", "lpg_agent.LPGAgent._select_action", "lpg_agent.LPGAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._reset_state", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step"], ["", "def", "begin_episode", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Returns the agent's first action for this episode.\n\n        Args:\n          observation: numpy array, the environment's initial observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_reset_state", "(", ")", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent.step": [[311, 349], ["lpg_agent.LPGAgent._record_observation", "lpg_agent.LPGAgent._select_action", "isinstance", "lpg_agent.LPGAgent._train_step", "collect_trajectory", "ValueError"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step", "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.collect_trajectory"], ["", "def", "step", "(", "self", ",", "reward", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records the most recent transition and returns the agent's next action.\n\n        We store the observation of the last time step since we want to store it\n        with the reward.\n\n        Args:\n          reward: float, the reward received from the agent's most recent action.\n          observation: numpy array, the most recent observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_last_observation", "=", "self", ".", "_observation", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "# if reward < 0:", "\n", "#     self.replay_buffer.clear()", "\n", "# elif reward > 0:", "\n", "#     self.replay_buffer.add(self._last_observation, self.action, reward, False)", "\n", "#     while self.replay_buffer.size() > 0:", "\n", "#         experience = self.replay_buffer.get_sample()", "\n", "#         state, action, reward, _ = experience", "\n", "#         self._store_transition(state, action, reward, False)", "\n", "# else:", "\n", "#     self.replay_buffer.add(self._last_observation, self.action, reward, False)", "\n", "            ", "if", "self", ".", "game_name", "in", "[", "\"Pong\"", "]", ":", "\n", "                ", "collect_trajectory", "(", "self", ",", "reward", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"collection wrong trajectory\"", ")", "\n", "\n", "", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "if", "isinstance", "(", "self", ".", "action", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "pass", "\n", "", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent.end_episode": [[350, 364], ["collect_trajectory", "ValueError"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.collect_trajectory"], ["", "def", "end_episode", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Signals the end of the episode to the agent.\n\n        We store the observation of the current time step, which is the last\n        observation of the episode.\n\n        Args:\n          reward: float, the last reward from the environment.\n        \"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "if", "self", ".", "game_name", "in", "[", "\"Pong\"", "]", ":", "\n", "                ", "collect_trajectory", "(", "self", ",", "reward", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"collection wrong trajectory\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._select_action_training": [[365, 368], ["lpg_agent.LPGAgent._sess.run"], "methods", ["None"], ["", "", "", "def", "_select_action_training", "(", "self", ")", ":", "\n", "        ", "\"\"\"Use EPG to select action during training, \"\"\"", "\n", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "sample", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "[", "0", "]", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._select_action": [[369, 408], ["lpg_agent.LPGAgent._select_action_training", "lpg_agent.LPGAgent._sess.run", "random.random", "random.randint", "lpg_agent.LPGAgent._sess.run"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._select_action_training"], ["", "def", "_select_action", "(", "self", ")", ":", "\n", "        ", "\"\"\"Select an action from the set of available actions.\n\n        Chooses an action randomly with probability self._calculate_epsilon(), and\n        otherwise acts greedily according to the current Q-value estimates.\n\n        Returns:\n           int, the selected action.\n        \"\"\"", "\n", "exploration", "=", "\"Randomexplore\"", "\n", "if", "exploration", "==", "\"EPG\"", ":", "\n", "            ", "self", ".", "epsilon_current", "=", "0", "\n", "self", ".", "current_replay_size", "=", "self", ".", "_replay", ".", "memory", ".", "add_count", "\n", "if", "self", ".", "eval_mode", ":", "\n", "                ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "", "return", "self", ".", "_select_action_training", "(", ")", "\n", "\n", "", "elif", "exploration", "==", "\"Randomexplore\"", ":", "\n", "# epsilon greedy explore.", "\n", "# epsilon = self.epsilon_eval if self.eval_mode else self.epsilon_fn(", "\n", "#     self.epsilon_decay_period,", "\n", "#     self.training_steps,", "\n", "#     self.min_replay_history,", "\n", "#     self.epsilon_train)", "\n", "            ", "if", "self", ".", "training_steps", "<", "self", ".", "min_replay_history", ":", "\n", "                ", "epsilon", "=", "1", "\n", "", "else", ":", "\n", "                ", "epsilon", "=", "self", ".", "epsilon_train", "\n", "", "if", "self", ".", "eval_mode", ":", "\n", "                ", "epsilon", "=", "self", ".", "epsilon_eval", "\n", "", "self", ".", "epsilon_current", "=", "epsilon", "\n", "self", ".", "current_replay_size", "=", "self", ".", "_replay", ".", "memory", ".", "add_count", "\n", "#", "\n", "if", "random", ".", "random", "(", ")", "<=", "epsilon", ":", "\n", "# Choose a random action with probability epsilon.", "\n", "                ", "return", "random", ".", "randint", "(", "0", ",", "self", ".", "num_actions", "-", "1", ")", "\n", "", "else", ":", "\n", "# Choose the action with highest Q-value at the current state.", "\n", "                ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._train_step": [[410, 448], ["print", "lpg_agent.LPGAgent._sess.run", "lpg_agent.LPGAgent._sess.run", "lpg_agent.LPGAgent.summary_writer.add_summary"], "methods", ["None"], ["", "", "", "def", "_train_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Runs a single training step.\n\n        Runs a training op if both:\n          (1) A minimum number of frames have been added to the replay buffer.\n          (2) `training_steps` is a multiple of `update_period`.\n\n        Also, syncs weights from online to target network if training steps is a\n        multiple of target update period.\n        \"\"\"", "\n", "# Run a train op at the rate of self.update_period if enough training steps", "\n", "# have been run. This matches the Nature DQN behaviour.", "\n", "\n", "if", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ":", "\n", "            ", "if", "self", ".", "training_steps", "%", "self", ".", "update_period", "==", "0", ":", "\n", "\n", "# debug checked.", "\n", "# _, neglogprob, logsoftmaxprob, \\", "\n", "# actor_loss, replay_action_one_hot = self._sess.run([self._train_op,", "\n", "#                                                    self.neglogprob,", "\n", "#                                                    self.logsoftmaxprob,", "\n", "#                                                    self.actor_loss,", "\n", "#                                                    self.replay_action_one_hot])", "\n", "                ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_op", ")", "\n", "if", "(", "self", ".", "summary_writer", "is", "not", "None", "and", "\n", "self", ".", "training_steps", ">", "0", "and", "\n", "self", ".", "training_steps", "%", "self", ".", "summary_writing_frequency", "==", "0", ")", ":", "\n", "                    ", "summary", "=", "self", ".", "_sess", ".", "run", "(", "self", ".", "_merged_summaries", ")", "\n", "self", ".", "summary_writer", ".", "add_summary", "(", "summary", ",", "self", ".", "training_steps", ")", "\n", "\n", "# if self.training_steps % self.target_update_period == 0:", "\n", "#     self._sess.run(self._sync_qt_ops)", "\n", "\n", "", "", "", "self", ".", "training_steps", "+=", "1", "\n", "\n", "if", "(", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ")", "and", "self", ".", "isPrinted", "is", "False", ":", "\n", "            ", "print", "(", "\"start training at {}\"", ".", "format", "(", "self", ".", "training_steps", ")", ")", "\n", "self", ".", "isPrinted", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._record_observation": [[449, 466], ["numpy.reshape", "numpy.reshape", "numpy.roll"], "methods", ["None"], ["", "", "def", "_record_observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records an observation and update state.\n\n        Extracts a frame from the observation vector and overwrites the oldest\n        frame in the state buffer.\n\n        Args:\n          observation: numpy array, an observation from the environment.\n        \"\"\"", "\n", "# Set current observation. We do the reshaping to handle environments", "\n", "# without frame stacking.", "\n", "observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "self", ".", "_observation", "=", "observation", "[", "...", ",", "0", "]", "\n", "self", ".", "_observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "# Swap out the oldest frame with the current frame.", "\n", "self", ".", "state", "=", "np", ".", "roll", "(", "self", ".", "state", ",", "-", "1", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "state", "[", "0", ",", "...", ",", "-", "1", "]", "=", "self", ".", "_observation", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._store_transition": [[467, 484], ["lpg_agent.LPGAgent._replay.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "def", "_store_transition", "(", "self", ",", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", ":", "\n", "        ", "\"\"\"Stores an experienced transition.\n\n        Executes a tf session and executes replay buffer ops in order to store the\n        following tuple in the replay buffer:\n          (last_observation, action, reward, is_terminal).\n\n        Pedantically speaking, this does not actually store an entire transition\n        since the next state is recorded on the following time step.\n\n        Args:\n          last_observation: numpy array, last observation.\n          action: int, the action taken.\n          reward: float, the reward.\n          is_terminal: bool, indicating if the current state is a terminal state.\n        \"\"\"", "\n", "self", ".", "_replay", ".", "add", "(", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent._reset_state": [[485, 488], ["lpg_agent.LPGAgent.state.fill"], "methods", ["None"], ["", "def", "_reset_state", "(", "self", ")", ":", "\n", "        ", "\"\"\"Resets the agent state by filling it with zeros.\"\"\"", "\n", "self", ".", "state", ".", "fill", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent.bundle_and_checkpoint": [[489, 519], ["lpg_agent.LPGAgent._saver.save", "lpg_agent.LPGAgent._replay.save", "tensorflow.gfile.Exists", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save"], ["", "def", "bundle_and_checkpoint", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ")", ":", "\n", "        ", "\"\"\"Returns a self-contained bundle of the agent's state.\n\n        This is used for checkpointing. It will return a dictionary containing all\n        non-TensorFlow objects (to be saved into a file by the caller), and it saves\n        all TensorFlow objects into a checkpoint file.\n\n        Args:\n          checkpoint_dir: str, directory where TensorFlow objects will be saved.\n          iteration_number: int, iteration number to use for naming the checkpoint\n            file.\n\n        Returns:\n          A dict containing additional Python objects to be checkpointed by the\n            experiment. If the checkpoint directory does not exist, returns None.\n        \"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "checkpoint_dir", ")", ":", "\n", "            ", "return", "None", "\n", "# Call the Tensorflow saver to checkpoint the graph.", "\n", "", "self", ".", "_saver", ".", "save", "(", "\n", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'tf_ckpt'", ")", ",", "\n", "global_step", "=", "iteration_number", ")", "\n", "# Checkpoint the out-of-graph replay buffer.", "\n", "self", ".", "_replay", ".", "save", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "bundle_dictionary", "=", "{", "}", "\n", "bundle_dictionary", "[", "'state'", "]", "=", "self", ".", "state", "\n", "bundle_dictionary", "[", "'eval_mode'", "]", "=", "self", ".", "eval_mode", "\n", "bundle_dictionary", "[", "'training_steps'", "]", "=", "self", ".", "training_steps", "\n", "return", "bundle_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.LPGAgent.unbundle": [[520, 552], ["lpg_agent.LPGAgent._saver.restore", "lpg_agent.LPGAgent._replay.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load"], ["", "def", "unbundle", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ",", "bundle_dictionary", ")", ":", "\n", "        ", "\"\"\"Restores the agent from a checkpoint.\n\n        Restores the agent's Python objects to those specified in bundle_dictionary,\n        and restores the TensorFlow objects to those specified in the\n        checkpoint_dir. If the checkpoint_dir does not exist, will not reset the\n          agent's state.\n\n        Args:\n          checkpoint_dir: str, path to the checkpoint saved by tf.Save.\n          iteration_number: int, checkpoint version, used when restoring replay\n            buffer.\n          bundle_dictionary: dict, containing additional Python objects owned by\n            the agent.\n\n        Returns:\n          bool, True if unbundling was successful.\n        \"\"\"", "\n", "try", ":", "\n", "# self._replay.load() will throw a NotFoundError if it does not find all", "\n", "# the necessary files, in which case we abort the process & return False.", "\n", "            ", "self", ".", "_replay", ".", "load", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "            ", "return", "False", "\n", "", "for", "key", "in", "self", ".", "__dict__", ":", "\n", "            ", "if", "key", "in", "bundle_dictionary", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "bundle_dictionary", "[", "key", "]", "\n", "# Restore the agent's TensorFlow graph.", "\n", "", "", "self", ".", "_saver", ".", "restore", "(", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "\n", "'tf_ckpt-{}'", ".", "format", "(", "iteration_number", ")", ")", ")", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.ReplayBufferRegular.__init__": [[559, 566], ["collections.deque", "random.seed"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "buffer_size", ",", "random_seed", "=", "1234", ")", ":", "\n", "        ", "self", ".", "buffer_size", "=", "buffer_size", "\n", "self", ".", "count", "=", "0", "\n", "# Right side of deque contains newest experience", "\n", "self", ".", "buffer", "=", "deque", "(", ")", "\n", "random", ".", "seed", "(", "random_seed", ")", "\n", "self", ".", "ptr", ",", "self", ".", "path_start_idx", "=", "0", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.ReplayBufferRegular.add": [[567, 573], ["lpg_agent.ReplayBufferRegular.buffer.append"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "add", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "terminal", ")", ":", "\n", "        ", "experience", "=", "[", "state", ",", "action", ",", "reward", ",", "terminal", "]", "\n", "assert", "self", ".", "count", "<", "self", ".", "buffer_size", "\n", "self", ".", "buffer", ".", "append", "(", "experience", ")", "\n", "self", ".", "count", "+=", "1", "\n", "self", ".", "ptr", "+=", "1", "\n", "# else:", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.ReplayBufferRegular.get_sample": [[579, 582], ["lpg_agent.ReplayBufferRegular.buffer.popleft"], "methods", ["None"], ["", "def", "get_sample", "(", "self", ")", ":", "\n", "        ", "self", ".", "count", "-=", "1", "\n", "return", "self", ".", "buffer", ".", "popleft", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.ReplayBufferRegular.size": [[583, 585], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.ReplayBufferRegular.clear": [[586, 591], ["lpg_agent.ReplayBufferRegular.buffer.clear"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.clear"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "buffer", ".", "clear", "(", ")", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "ptr", "=", "0", "\n", "self", ".", "path_start_idx", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.lpg.lpg_agent.linearly_decaying_epsilon": [[41, 63], ["numpy.clip"], "function", ["None"], ["def", "linearly_decaying_epsilon", "(", "decay_period", ",", "step", ",", "warmup_steps", ",", "epsilon", ")", ":", "\n", "    ", "\"\"\"Returns the current epsilon for the agent's epsilon-greedy policy.\n\n    This follows the Nature DQN schedule of a linearly decaying epsilon (Mnih et\n    al., 2015). The schedule is as follows:\n      Begin at 1. until warmup_steps steps have been taken; then\n      Linearly decay epsilon from 1. to epsilon in decay_period steps; and then\n      Use epsilon from there on.\n\n    Args:\n      decay_period: float, the period over which epsilon is decayed.\n      step: int, the number of training steps completed so far.\n      warmup_steps: int, the number of steps taken before epsilon is decayed.\n      epsilon: float, the final value to which to decay the epsilon parameter.\n\n    Returns:\n      A float, the current epsilon value computed according to the schedule.\n    \"\"\"", "\n", "steps_left", "=", "decay_period", "+", "warmup_steps", "-", "step", "\n", "bonus", "=", "(", "1.0", "-", "epsilon", ")", "*", "steps_left", "/", "decay_period", "\n", "bonus", "=", "np", ".", "clip", "(", "bonus", ",", "0.", ",", "1.", "-", "epsilon", ")", "\n", "return", "epsilon", "+", "bonus", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.__init__": [[44, 97], ["dopamine.agents.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent.__init__"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.__init__"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "kappa", "=", "1.0", ",", "\n", "num_tau_samples", "=", "32", ",", "\n", "num_tau_prime_samples", "=", "32", ",", "\n", "num_quantile_samples", "=", "32", ",", "\n", "quantile_embedding_dim", "=", "64", ",", "\n", "double_dqn", "=", "False", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "        ", "\"\"\"Initializes the agent and constructs the Graph.\n\n        Most of this constructor's parameters are IQN-specific hyperparameters whose\n        values are taken from Dabney et al. (2018).\n\n        Args:\n          sess: `tf.Session` object for running associated ops.\n          num_actions: int, number of actions the agent can take at any state.\n          kappa: float, Huber loss cutoff.\n          num_tau_samples: int, number of online quantile samples for loss\n            estimation.\n          num_tau_prime_samples: int, number of target quantile samples for loss\n            estimation.\n          num_quantile_samples: int, number of quantile samples for computing\n            Q-values.\n          quantile_embedding_dim: int, embedding dimension for the quantile input.\n          double_dqn: boolean, whether to perform double DQN style learning\n            as described in Van Hasselt et al.: https://arxiv.org/abs/1509.06461.\n          summary_writer: SummaryWriter object for outputting training statistics.\n            Summary writing disabled if set to None.\n          summary_writing_frequency: int, frequency with which summaries will be\n            written. Lower values will result in slower training.\n        \"\"\"", "\n", "self", ".", "kappa", "=", "kappa", "\n", "# num_tau_samples = N below equation (3) in the paper.", "\n", "self", ".", "num_tau_samples", "=", "num_tau_samples", "\n", "# num_tau_prime_samples = N' below equation (3) in the paper.", "\n", "self", ".", "num_tau_prime_samples", "=", "num_tau_prime_samples", "\n", "# num_quantile_samples = k below equation (3) in the paper.", "\n", "self", ".", "num_quantile_samples", "=", "num_quantile_samples", "\n", "# quantile_embedding_dim = n above equation (4) in the paper.", "\n", "self", ".", "quantile_embedding_dim", "=", "quantile_embedding_dim", "\n", "# option to perform double dqn.", "\n", "self", ".", "double_dqn", "=", "double_dqn", "\n", "\n", "super", "(", "ImplicitQuantileRPGAgent", ",", "self", ")", ".", "__init__", "(", "\n", "sess", "=", "sess", ",", "\n", "num_actions", "=", "num_actions", ",", "\n", "summary_writer", "=", "summary_writer", ",", "\n", "summary_writing_frequency", "=", "summary_writing_frequency", ")", "\n", "\n", "self", ".", "start_training", "=", "1000", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._get_network_type_rpg": [[99, 107], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type_rpg", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the type of the outputs of a value distribution network.\n\n        Returns:\n          net_type: _network_type object defining the outputs of the network.\n        \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "'c51_network'", ",", "\n", "[", "'q_values'", ",", "'logits'", ",", "'probabilities'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._network_template_rpg": [[108, 141], ["slim.variance_scaling_initializer", "tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "slim.fully_connected", "slim.fully_connected", "tensorflow.reshape", "tensorflow.contrib.layers.softmax", "tensorflow.reduce_sum", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._get_network_type_rpg", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._get_network_type_rpg"], ["", "def", "_network_template_rpg", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Builds a convolutional network that outputs Q-value distributions.\n\n        Args:\n          state: `tf.Tensor`, contains the agent's current state.\n\n        Returns:\n          net: _network_type object containing the tensors output by the network.\n        \"\"\"", "\n", "weights_initializer", "=", "slim", ".", "variance_scaling_initializer", "(", "\n", "factor", "=", "1.0", "/", "np", ".", "sqrt", "(", "3.0", ")", ",", "mode", "=", "'FAN_IN'", ",", "uniform", "=", "True", ")", "\n", "\n", "net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "net", "=", "tf", ".", "div", "(", "net", ",", "255.", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "\n", "net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "\n", "net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "\n", "net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "512", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "\n", "self", ".", "num_actions", "*", "self", ".", "_num_atoms", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "\n", "logits", "=", "tf", ".", "reshape", "(", "net", ",", "[", "-", "1", ",", "self", ".", "num_actions", ",", "self", ".", "_num_atoms", "]", ")", "\n", "probabilities", "=", "tf", ".", "contrib", ".", "layers", ".", "softmax", "(", "logits", ")", "\n", "q_values", "=", "tf", ".", "reduce_sum", "(", "self", ".", "_support", "*", "probabilities", ",", "axis", "=", "2", ")", "\n", "return", "self", ".", "_get_network_type_rpg", "(", ")", "(", "q_values", ",", "logits", ",", "probabilities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._build_networks_rpg": [[142, 148], ["tensorflow.make_template", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.rpg_convnet", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.rpg_convnet", "tensorflow.argmax"], "methods", ["None"], ["", "def", "_build_networks_rpg", "(", "self", ")", ":", "\n", "# RPG learning net.", "\n", "        ", "self", ".", "rpg_convnet", "=", "tf", ".", "make_template", "(", "'RPG'", ",", "self", ".", "_network_template_rpg", ")", "\n", "self", ".", "_rpg_net_outputs", "=", "self", ".", "rpg_convnet", "(", "self", ".", "state_ph", ")", "\n", "self", ".", "_q_argmax_rpg", "=", "tf", ".", "argmax", "(", "self", ".", "_rpg_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "self", ".", "_replay_rpg_net_outputs", "=", "self", ".", "rpg_convnet", "(", "self", ".", "_replay_opt", ".", "states", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._build_train_op_rpg": [[149, 167], ["tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.maximum", "tensorflow.reduce_mean", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.optimizer_rpg.minimize", "tensorflow.reshape", "tensorflow.ones"], "methods", ["None"], ["", "def", "_build_train_op_rpg", "(", "self", ")", ":", "\n", "# RPG loss", "\n", "        ", "replay_action_one_hot", "=", "tf", ".", "one_hot", "(", "\n", "self", ".", "_replay_opt", ".", "actions", ",", "self", ".", "num_actions", ",", "1.", ",", "0.", ",", "name", "=", "'action_one_hot_rpg'", ")", "\n", "replay_chosen_q", "=", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "_replay_rpg_net_outputs", ".", "q_values", "*", "replay_action_one_hot", ",", "\n", "reduction_indices", "=", "1", ",", "\n", "name", "=", "'replay_chosen_q_rpg'", ")", "\n", "margin", "=", "1", "\n", "qvalue", "=", "self", ".", "_replay_rpg_net_outputs", ".", "q_values", "\n", "# debug self.temp_action_one_hot = replay_action_one_hot", "\n", "self", ".", "temp_qvalue", "=", "qvalue", "\n", "self", ".", "temp1", "=", "(", "qvalue", "+", "margin", ")", "*", "(", "1", "-", "replay_action_one_hot", ")", "+", "qvalue", "*", "replay_action_one_hot", "\n", "self", ".", "temp2", "=", "-", "(", "tf", ".", "reshape", "(", "replay_chosen_q", ",", "[", "-", "1", ",", "1", "]", ")", "*", "tf", ".", "ones", "(", "[", "1", ",", "self", ".", "num_actions", "]", ")", ")", "*", "(", "(", "1", "-", "replay_action_one_hot", ")", "+", "(", "replay_action_one_hot", ")", ")", "\n", "self", ".", "hingeloss", "=", "tf", ".", "maximum", "(", "0.0", ",", "self", ".", "temp1", "+", "self", ".", "temp2", ")", "\n", "rpg_loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "hingeloss", ")", "\n", "return", "self", ".", "optimizer_rpg", ".", "minimize", "(", "rpg_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._get_network_type": [[168, 176], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the type of the outputs of the implicit quantile network.\n\n        Returns:\n          _network_type object defining the outputs of the network.\n        \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "\n", "'iqn_network'", ",", "[", "'quantile_values'", ",", "'quantiles'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._network_template": [[177, 233], ["slim.variance_scaling_initializer", "tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "tensorflow.tile", "tensorflow.random_uniform", "tensorflow.tile", "tensorflow.constant", "tensorflow.cos", "slim.fully_connected", "tensorflow.multiply", "slim.fully_connected", "slim.fully_connected", "slim.flatten.get_shape().as_list", "slim.flatten.get_shape().as_list", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._get_network_type", "tensorflow.cast", "numpy.sqrt", "slim.flatten.get_shape", "slim.flatten.get_shape", "tensorflow.range"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ",", "num_quantiles", ")", ":", "\n", "        ", "r\"\"\"Builds an Implicit Quantile ConvNet.\n\n        Takes state and quantile as inputs and outputs state-action quantile values.\n\n        Args:\n          state: A `tf.placeholder` for the RL state.\n          num_quantiles: int, number of quantile inputs.\n\n        Returns:\n          _network_type object containing quantile value outputs of the network.\n        \"\"\"", "\n", "\n", "weights_initializer", "=", "slim", ".", "variance_scaling_initializer", "(", "\n", "factor", "=", "1.0", "/", "np", ".", "sqrt", "(", "3.0", ")", ",", "mode", "=", "'FAN_IN'", ",", "uniform", "=", "True", ")", "\n", "\n", "state_net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "state_net", "=", "tf", ".", "div", "(", "state_net", ",", "255.", ")", "\n", "state_net", "=", "slim", ".", "conv2d", "(", "\n", "state_net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "state_net", "=", "slim", ".", "conv2d", "(", "\n", "state_net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "state_net", "=", "slim", ".", "conv2d", "(", "\n", "state_net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "state_net", "=", "slim", ".", "flatten", "(", "state_net", ")", "\n", "state_net_size", "=", "state_net", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "state_net_tiled", "=", "tf", ".", "tile", "(", "state_net", ",", "[", "num_quantiles", ",", "1", "]", ")", "\n", "\n", "batch_size", "=", "state_net", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "0", "]", "\n", "quantiles_shape", "=", "[", "num_quantiles", "*", "batch_size", ",", "1", "]", "\n", "quantiles", "=", "tf", ".", "random_uniform", "(", "\n", "quantiles_shape", ",", "minval", "=", "0", ",", "maxval", "=", "1", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "quantile_net", "=", "tf", ".", "tile", "(", "quantiles", ",", "[", "1", ",", "self", ".", "quantile_embedding_dim", "]", ")", "\n", "pi", "=", "tf", ".", "constant", "(", "math", ".", "pi", ")", "\n", "quantile_net", "=", "tf", ".", "cast", "(", "tf", ".", "range", "(", "\n", "1", ",", "self", ".", "quantile_embedding_dim", "+", "1", ",", "1", ")", ",", "tf", ".", "float32", ")", "*", "pi", "*", "quantile_net", "\n", "quantile_net", "=", "tf", ".", "cos", "(", "quantile_net", ")", "\n", "quantile_net", "=", "slim", ".", "fully_connected", "(", "quantile_net", ",", "state_net_size", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "# Hadamard product.", "\n", "net", "=", "tf", ".", "multiply", "(", "state_net_tiled", ",", "quantile_net", ")", "\n", "\n", "net", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "512", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "quantile_values", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "\n", "self", ".", "num_actions", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "quantile_values", "=", "quantile_values", ",", "\n", "quantiles", "=", "quantiles", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._build_networks": [[234, 300], ["tensorflow.make_template", "tensorflow.make_template", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.online_convnet", "tensorflow.reduce_mean", "tensorflow.argmax", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.online_convnet", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.target_convnet", "tensorflow.reshape", "tensorflow.squeeze", "tensorflow.argmax", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.online_convnet", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.target_convnet", "tensorflow.reduce_mean"], "methods", ["None"], ["", "def", "_build_networks", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds the IQN computations needed for acting and training.\n\n        These are:\n          self.online_convnet: For computing the current state's quantile values.\n          self.target_convnet: For computing the next state's target quantile\n            values.\n          self._net_outputs: The actual quantile values.\n          self._q_argmax: The action maximizing the current state's Q-values.\n          self._replay_net_outputs: The replayed states' quantile values.\n          self._replay_next_target_net_outputs: The replayed next states' target\n            quantile values.\n        \"\"\"", "\n", "# Calling online_convnet will generate a new graph as defined in", "\n", "# self._get_network_template using whatever input is passed, but will always", "\n", "# share the same weights.", "\n", "self", ".", "online_convnet", "=", "tf", ".", "make_template", "(", "'Online'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "target_convnet", "=", "tf", ".", "make_template", "(", "'Target'", ",", "self", ".", "_network_template", ")", "\n", "\n", "# Compute the Q-values which are used for action selection in the current", "\n", "# state.", "\n", "self", ".", "_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "state_ph", ",", "\n", "self", ".", "num_quantile_samples", ")", "\n", "# Shape of self._net_outputs.quantile_values:", "\n", "# num_quantile_samples x num_actions.", "\n", "# e.g. if num_actions is 2, it might look something like this:", "\n", "# Vals for Quantile .2  Vals for Quantile .4  Vals for Quantile .6", "\n", "#    [[0.1, 0.5],         [0.15, -0.3],          [0.15, -0.2]]", "\n", "# Q-values = [(0.1 + 0.15 + 0.15)/3, (0.5 + 0.15 + -0.2)/3].", "\n", "self", ".", "_q_values", "=", "tf", ".", "reduce_mean", "(", "self", ".", "_net_outputs", ".", "quantile_values", ",", "axis", "=", "0", ")", "\n", "self", ".", "_q_argmax", "=", "tf", ".", "argmax", "(", "self", ".", "_q_values", ",", "axis", "=", "0", ")", "\n", "\n", "self", ".", "_replay_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "states", ",", "\n", "self", ".", "num_tau_samples", ")", "\n", "# Shape: (num_tau_samples x batch_size) x num_actions.", "\n", "self", ".", "_replay_net_quantile_values", "=", "self", ".", "_replay_net_outputs", ".", "quantile_values", "\n", "self", ".", "_replay_net_quantiles", "=", "self", ".", "_replay_net_outputs", ".", "quantiles", "\n", "\n", "# Do the same for next states in the replay buffer.", "\n", "self", ".", "_replay_net_target_outputs", "=", "self", ".", "target_convnet", "(", "\n", "self", ".", "_replay", ".", "next_states", ",", "self", ".", "num_tau_prime_samples", ")", "\n", "# Shape: (num_tau_prime_samples x batch_size) x num_actions.", "\n", "vals", "=", "self", ".", "_replay_net_target_outputs", ".", "quantile_values", "\n", "self", ".", "_replay_net_target_quantile_values", "=", "vals", "\n", "\n", "# Compute Q-values which are used for action selection for the next states", "\n", "# in the replay buffer. Compute the argmax over the Q-values.", "\n", "if", "self", ".", "double_dqn", ":", "\n", "            ", "outputs_action", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "next_states", ",", "\n", "self", ".", "num_quantile_samples", ")", "\n", "", "else", ":", "\n", "            ", "outputs_action", "=", "self", ".", "target_convnet", "(", "self", ".", "_replay", ".", "next_states", ",", "\n", "self", ".", "num_quantile_samples", ")", "\n", "\n", "# Shape: (num_quantile_samples x batch_size) x num_actions.", "\n", "", "target_quantile_values_action", "=", "outputs_action", ".", "quantile_values", "\n", "# Shape: num_quantile_samples x batch_size x num_actions.", "\n", "target_quantile_values_action", "=", "tf", ".", "reshape", "(", "target_quantile_values_action", ",", "\n", "[", "self", ".", "num_quantile_samples", ",", "\n", "self", ".", "_replay", ".", "batch_size", ",", "\n", "self", ".", "num_actions", "]", ")", "\n", "# Shape: batch_size x num_actions.", "\n", "self", ".", "_replay_net_target_q_values", "=", "tf", ".", "squeeze", "(", "tf", ".", "reduce_mean", "(", "\n", "target_quantile_values_action", ",", "axis", "=", "0", ")", ")", "\n", "self", ".", "_replay_next_qt_argmax", "=", "tf", ".", "argmax", "(", "\n", "self", ".", "_replay_net_target_q_values", ",", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._build_target_quantile_values_op": [[301, 340], ["tensorflow.tile", "tensorflow.tile", "tensorflow.tile", "tensorflow.cast", "tensorflow.concat", "tensorflow.shape", "tensorflow.to_float", "tensorflow.gather_nd", "tensorflow.range"], "methods", ["None"], ["", "def", "_build_target_quantile_values_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Build an op used as a target for return values at given quantiles.\n\n        Returns:\n          An op calculating the target quantile return.\n        \"\"\"", "\n", "batch_size", "=", "tf", ".", "shape", "(", "self", ".", "_replay", ".", "rewards", ")", "[", "0", "]", "\n", "# Shape of rewards: (num_tau_prime_samples x batch_size) x 1.", "\n", "rewards", "=", "self", ".", "_replay", ".", "rewards", "[", ":", ",", "None", "]", "\n", "rewards", "=", "tf", ".", "tile", "(", "rewards", ",", "[", "self", ".", "num_tau_prime_samples", ",", "1", "]", ")", "\n", "\n", "is_terminal_multiplier", "=", "1.", "-", "tf", ".", "to_float", "(", "self", ".", "_replay", ".", "terminals", ")", "\n", "# Incorporate terminal state to discount factor.", "\n", "# size of gamma_with_terminal: (num_tau_prime_samples x batch_size) x 1.", "\n", "gamma_with_terminal", "=", "self", ".", "cumulative_gamma", "*", "is_terminal_multiplier", "\n", "gamma_with_terminal", "=", "tf", ".", "tile", "(", "gamma_with_terminal", "[", ":", ",", "None", "]", ",", "\n", "[", "self", ".", "num_tau_prime_samples", ",", "1", "]", ")", "\n", "\n", "# Get the indices of the maximium Q-value across the action dimension.", "\n", "# Shape of replay_next_qt_argmax: (num_tau_prime_samples x batch_size) x 1.", "\n", "\n", "replay_next_qt_argmax", "=", "tf", ".", "tile", "(", "\n", "self", ".", "_replay_next_qt_argmax", "[", ":", ",", "None", "]", ",", "[", "self", ".", "num_tau_prime_samples", ",", "1", "]", ")", "\n", "\n", "# Shape of batch_indices: (num_tau_prime_samples x batch_size) x 1.", "\n", "batch_indices", "=", "tf", ".", "cast", "(", "tf", ".", "range", "(", "\n", "self", ".", "num_tau_prime_samples", "*", "batch_size", ")", "[", ":", ",", "None", "]", ",", "tf", ".", "int64", ")", "\n", "\n", "# Shape of batch_indexed_target_values:", "\n", "# (num_tau_prime_samples x batch_size) x 2.", "\n", "batch_indexed_target_values", "=", "tf", ".", "concat", "(", "\n", "[", "batch_indices", ",", "replay_next_qt_argmax", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# Shape of next_target_values: (num_tau_prime_samples x batch_size) x 1.", "\n", "target_quantile_values", "=", "tf", ".", "gather_nd", "(", "\n", "self", ".", "_replay_net_target_quantile_values", ",", "\n", "batch_indexed_target_values", ")", "[", ":", ",", "None", "]", "\n", "\n", "return", "rewards", "+", "gamma_with_terminal", "*", "target_quantile_values", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._build_train_op": [[341, 432], ["tensorflow.stop_gradient", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.tile", "tensorflow.concat", "tensorflow.gather_nd", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.to_float", "tensorflow.reduce_sum", "tensorflow.reduce_mean", "tensorflow.no_op", "tensorflow.shape", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent._build_target_quantile_values_op", "tensorflow.range", "tensorflow.tile", "tensorflow.control_dependencies", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.abs", "tensorflow.abs", "implicit_quantilerpg_agent.ImplicitQuantileRPGAgent.optimizer.minimize", "tensorflow.reduce_mean", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.reduce_mean", "tensorflow.abs", "tensorflow.abs", "tensorflow.stop_gradient", "tensorflow.reduce_mean", "tensorflow.to_float"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantile.implicit_quantile_agent.ImplicitQuantileAgent._build_target_quantile_values_op"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds a training op.\n\n        Returns:\n          train_op: An op performing one step of training from replay data.\n        \"\"\"", "\n", "batch_size", "=", "tf", ".", "shape", "(", "self", ".", "_replay", ".", "rewards", ")", "[", "0", "]", "\n", "\n", "target_quantile_values", "=", "tf", ".", "stop_gradient", "(", "\n", "self", ".", "_build_target_quantile_values_op", "(", ")", ")", "\n", "# Reshape to self.num_tau_prime_samples x batch_size x 1 since this is", "\n", "# the manner in which the target_quantile_values are tiled.", "\n", "target_quantile_values", "=", "tf", ".", "reshape", "(", "target_quantile_values", ",", "\n", "[", "self", ".", "num_tau_prime_samples", ",", "\n", "batch_size", ",", "1", "]", ")", "\n", "# Transpose dimensions so that the dimensionality is batch_size x", "\n", "# self.num_tau_prime_samples x 1 to prepare for computation of", "\n", "# Bellman errors.", "\n", "# Final shape of target_quantile_values:", "\n", "# batch_size x num_tau_prime_samples x 1.", "\n", "target_quantile_values", "=", "tf", ".", "transpose", "(", "target_quantile_values", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "# Shape of indices: (num_tau_samples x batch_size) x 1.", "\n", "# Expand dimension by one so that it can be used to index into all the", "\n", "# quantiles when using the tf.gather_nd function (see below).", "\n", "indices", "=", "tf", ".", "range", "(", "self", ".", "num_tau_samples", "*", "batch_size", ")", "[", ":", ",", "None", "]", "\n", "\n", "# Expand the dimension by one so that it can be used to index into all the", "\n", "# quantiles when using the tf.gather_nd function (see below).", "\n", "reshaped_actions", "=", "self", ".", "_replay", ".", "actions", "[", ":", ",", "None", "]", "\n", "reshaped_actions", "=", "tf", ".", "tile", "(", "reshaped_actions", ",", "[", "self", ".", "num_tau_samples", ",", "1", "]", ")", "\n", "# Shape of reshaped_actions: (num_tau_samples x batch_size) x 2.", "\n", "reshaped_actions", "=", "tf", ".", "concat", "(", "[", "indices", ",", "reshaped_actions", "]", ",", "axis", "=", "1", ")", "\n", "\n", "chosen_action_quantile_values", "=", "tf", ".", "gather_nd", "(", "\n", "self", ".", "_replay_net_quantile_values", ",", "reshaped_actions", ")", "\n", "# Reshape to self.num_tau_samples x batch_size x 1 since this is the manner", "\n", "# in which the quantile values are tiled.", "\n", "chosen_action_quantile_values", "=", "tf", ".", "reshape", "(", "chosen_action_quantile_values", ",", "\n", "[", "self", ".", "num_tau_samples", ",", "\n", "batch_size", ",", "1", "]", ")", "\n", "# Transpose dimensions so that the dimensionality is batch_size x", "\n", "# self.num_tau_samples x 1 to prepare for computation of", "\n", "# Bellman errors.", "\n", "# Final shape of chosen_action_quantile_values:", "\n", "# batch_size x num_tau_samples x 1.", "\n", "chosen_action_quantile_values", "=", "tf", ".", "transpose", "(", "\n", "chosen_action_quantile_values", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "# Shape of bellman_erors and huber_loss:", "\n", "# batch_size x num_tau_prime_samples x num_tau_samples x 1.", "\n", "bellman_errors", "=", "target_quantile_values", "[", "\n", ":", ",", ":", ",", "None", ",", ":", "]", "-", "chosen_action_quantile_values", "[", ":", ",", "None", ",", ":", ",", ":", "]", "\n", "# The huber loss (see Section 2.3 of the paper) is defined via two cases:", "\n", "# case_one: |bellman_errors| <= kappa", "\n", "# case_two: |bellman_errors| > kappa", "\n", "huber_loss_case_one", "=", "tf", ".", "to_float", "(", "\n", "tf", ".", "abs", "(", "bellman_errors", ")", "<=", "self", ".", "kappa", ")", "*", "0.5", "*", "bellman_errors", "**", "2", "\n", "huber_loss_case_two", "=", "tf", ".", "to_float", "(", "\n", "tf", ".", "abs", "(", "bellman_errors", ")", ">", "self", ".", "kappa", ")", "*", "self", ".", "kappa", "*", "(", "\n", "tf", ".", "abs", "(", "bellman_errors", ")", "-", "0.5", "*", "self", ".", "kappa", ")", "\n", "huber_loss", "=", "huber_loss_case_one", "+", "huber_loss_case_two", "\n", "\n", "# Reshape replay_quantiles to batch_size x num_tau_samples x 1", "\n", "replay_quantiles", "=", "tf", ".", "reshape", "(", "\n", "self", ".", "_replay_net_quantiles", ",", "[", "self", ".", "num_tau_samples", ",", "batch_size", ",", "1", "]", ")", "\n", "replay_quantiles", "=", "tf", ".", "transpose", "(", "replay_quantiles", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "# Tile by num_tau_prime_samples along a new dimension. Shape is now", "\n", "# batch_size x num_tau_prime_samples x num_tau_samples x 1.", "\n", "# These quantiles will be used for computation of the quantile huber loss", "\n", "# below (see section 2.3 of the paper).", "\n", "replay_quantiles", "=", "tf", ".", "to_float", "(", "tf", ".", "tile", "(", "\n", "replay_quantiles", "[", ":", ",", "None", ",", ":", ",", ":", "]", ",", "[", "1", ",", "self", ".", "num_tau_prime_samples", ",", "1", ",", "1", "]", ")", ")", "\n", "# Shape: batch_size x num_tau_prime_samples x num_tau_samples x 1.", "\n", "quantile_huber_loss", "=", "(", "tf", ".", "abs", "(", "replay_quantiles", "-", "tf", ".", "stop_gradient", "(", "\n", "tf", ".", "to_float", "(", "bellman_errors", "<", "0", ")", ")", ")", "*", "huber_loss", ")", "/", "self", ".", "kappa", "\n", "# Sum over current quantile value (num_tau_samples) dimension,", "\n", "# average over target quantile value (num_tau_prime_samples) dimension.", "\n", "# Shape: batch_size x num_tau_prime_samples x 1.", "\n", "loss", "=", "tf", ".", "reduce_sum", "(", "quantile_huber_loss", ",", "axis", "=", "2", ")", "\n", "# Shape: batch_size x 1.", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "loss", ",", "axis", "=", "1", ")", "\n", "\n", "# TODO(kumasaurabh): Add prioritized replay functionality here.", "\n", "update_priorities_op", "=", "tf", ".", "no_op", "(", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "update_priorities_op", "]", ")", ":", "\n", "            ", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "                    ", "tf", ".", "summary", ".", "scalar", "(", "'QuantileLoss'", ",", "tf", ".", "reduce_mean", "(", "loss", ")", ")", "\n", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "tf", ".", "reduce_mean", "(", "loss", ")", ")", ",", "tf", ".", "reduce_mean", "(", "loss", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbow.rainbow_agent.RainbowAgent.__init__": [[58, 147], ["tensorflow.train.AdamOptimizer", "float", "tensorflow.linspace", "dopamine.agents.dqn.dqn_agent.DQNAgent.__init__"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.__init__"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "observation_shape", "=", "dqn_agent", ".", "NATURE_DQN_OBSERVATION_SHAPE", ",", "\n", "observation_dtype", "=", "dqn_agent", ".", "NATURE_DQN_DTYPE", ",", "\n", "stack_size", "=", "dqn_agent", ".", "NATURE_DQN_STACK_SIZE", ",", "\n", "num_atoms", "=", "51", ",", "\n", "vmax", "=", "10.", ",", "\n", "gamma", "=", "0.99", ",", "\n", "update_horizon", "=", "1", ",", "\n", "min_replay_history", "=", "20000", ",", "\n", "update_period", "=", "4", ",", "\n", "target_update_period", "=", "8000", ",", "\n", "epsilon_fn", "=", "dqn_agent", ".", "linearly_decaying_epsilon", ",", "\n", "epsilon_train", "=", "0.01", ",", "\n", "epsilon_eval", "=", "0.001", ",", "\n", "epsilon_decay_period", "=", "250000", ",", "\n", "replay_scheme", "=", "'prioritized'", ",", "\n", "tf_device", "=", "'/cpu:*'", ",", "\n", "use_staging", "=", "True", ",", "\n", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "epsilon", "=", "0.0003125", ")", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "    ", "\"\"\"Initializes the agent and constructs the components of its graph.\n\n    Args:\n      sess: `tf.Session`, for executing ops.\n      num_actions: int, number of actions the agent can take at any state.\n      observation_shape: tuple of ints or an int. If single int, the observation\n        is assumed to be a 2D square.\n      observation_dtype: tf.DType, specifies the type of the observations. Note\n        that if your inputs are continuous, you should set this to tf.float32.\n      stack_size: int, number of frames to use in state stack.\n      num_atoms: int, the number of buckets of the value function distribution.\n      vmax: float, the value distribution support is [-vmax, vmax].\n      gamma: float, discount factor with the usual RL meaning.\n      update_horizon: int, horizon at which updates are performed, the 'n' in\n        n-step update.\n      min_replay_history: int, number of transitions that should be experienced\n        before the agent begins training its value function.\n      update_period: int, period between DQN updates.\n      target_update_period: int, update period for the target network.\n      epsilon_fn: function expecting 4 parameters:\n        (decay_period, step, warmup_steps, epsilon). This function should return\n        the epsilon value used for exploration during training.\n      epsilon_train: float, the value to which the agent's epsilon is eventually\n        decayed during training.\n      epsilon_eval: float, epsilon used when evaluating the agent.\n      epsilon_decay_period: int, length of the epsilon decay schedule.\n      replay_scheme: str, 'prioritized' or 'uniform', the sampling scheme of the\n        replay memory.\n      tf_device: str, Tensorflow device on which the agent's graph is executed.\n      use_staging: bool, when True use a staging area to prefetch the next\n        training batch, speeding training up by about 30%.\n      optimizer: `tf.train.Optimizer`, for training the value function.\n      summary_writer: SummaryWriter object for outputting training statistics.\n        Summary writing disabled if set to None.\n      summary_writing_frequency: int, frequency with which summaries will be\n        written. Lower values will result in slower training.\n    \"\"\"", "\n", "# We need this because some tools convert round floats into ints.", "\n", "vmax", "=", "float", "(", "vmax", ")", "\n", "self", ".", "_num_atoms", "=", "num_atoms", "\n", "self", ".", "_support", "=", "tf", ".", "linspace", "(", "-", "vmax", ",", "vmax", ",", "num_atoms", ")", "\n", "self", ".", "_replay_scheme", "=", "replay_scheme", "\n", "# TODO(b/110897128): Make agent optimizer attribute private.", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "\n", "super", "(", "RainbowAgent", ",", "self", ")", ".", "__init__", "(", "\n", "sess", "=", "sess", ",", "\n", "num_actions", "=", "num_actions", ",", "\n", "observation_shape", "=", "observation_shape", ",", "\n", "observation_dtype", "=", "observation_dtype", ",", "\n", "stack_size", "=", "stack_size", ",", "\n", "gamma", "=", "gamma", ",", "\n", "update_horizon", "=", "update_horizon", ",", "\n", "min_replay_history", "=", "min_replay_history", ",", "\n", "update_period", "=", "update_period", ",", "\n", "target_update_period", "=", "target_update_period", ",", "\n", "epsilon_fn", "=", "epsilon_fn", ",", "\n", "epsilon_train", "=", "epsilon_train", ",", "\n", "epsilon_eval", "=", "epsilon_eval", ",", "\n", "epsilon_decay_period", "=", "epsilon_decay_period", ",", "\n", "tf_device", "=", "tf_device", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "optimizer", "=", "self", ".", "optimizer", ",", "\n", "summary_writer", "=", "summary_writer", ",", "\n", "summary_writing_frequency", "=", "summary_writing_frequency", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbow.rainbow_agent.RainbowAgent._get_network_type": [[148, 156], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns the type of the outputs of a value distribution network.\n\n    Returns:\n      net_type: _network_type object defining the outputs of the network.\n    \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "'c51_network'", ",", "\n", "[", "'q_values'", ",", "'logits'", ",", "'probabilities'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbow.rainbow_agent.RainbowAgent._network_template": [[157, 190], ["slim.variance_scaling_initializer", "tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "slim.fully_connected", "slim.fully_connected", "tensorflow.reshape", "tensorflow.contrib.layers.softmax", "tensorflow.reduce_sum", "rainbow_agent.RainbowAgent._get_network_type", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ")", ":", "\n", "    ", "\"\"\"Builds a convolutional network that outputs Q-value distributions.\n\n    Args:\n      state: `tf.Tensor`, contains the agent's current state.\n\n    Returns:\n      net: _network_type object containing the tensors output by the network.\n    \"\"\"", "\n", "weights_initializer", "=", "slim", ".", "variance_scaling_initializer", "(", "\n", "factor", "=", "1.0", "/", "np", ".", "sqrt", "(", "3.0", ")", ",", "mode", "=", "'FAN_IN'", ",", "uniform", "=", "True", ")", "\n", "\n", "net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "net", "=", "tf", ".", "div", "(", "net", ",", "255.", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "\n", "net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "\n", "net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "\n", "net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "512", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "\n", "self", ".", "num_actions", "*", "self", ".", "_num_atoms", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "\n", "logits", "=", "tf", ".", "reshape", "(", "net", ",", "[", "-", "1", ",", "self", ".", "num_actions", ",", "self", ".", "_num_atoms", "]", ")", "\n", "probabilities", "=", "tf", ".", "contrib", ".", "layers", ".", "softmax", "(", "logits", ")", "\n", "q_values", "=", "tf", ".", "reduce_sum", "(", "self", ".", "_support", "*", "probabilities", ",", "axis", "=", "2", ")", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "q_values", ",", "logits", ",", "probabilities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbow.rainbow_agent.RainbowAgent._build_replay_buffer": [[191, 212], ["dopamine.replay_memory.prioritized_replay_buffer.WrappedPrioritizedReplayBuffer", "ValueError"], "methods", ["None"], ["", "def", "_build_replay_buffer", "(", "self", ",", "use_staging", ")", ":", "\n", "    ", "\"\"\"Creates the replay buffer used by the agent.\n\n    Args:\n      use_staging: bool, if True, uses a staging area to prefetch data for\n        faster training.\n\n    Returns:\n      A `WrappedPrioritizedReplayBuffer` object.\n\n    Raises:\n      ValueError: if given an invalid replay scheme.\n    \"\"\"", "\n", "if", "self", ".", "_replay_scheme", "not", "in", "[", "'uniform'", ",", "'prioritized'", "]", ":", "\n", "      ", "raise", "ValueError", "(", "'Invalid replay scheme: {}'", ".", "format", "(", "self", ".", "_replay_scheme", ")", ")", "\n", "", "return", "prioritized_replay_buffer", ".", "WrappedPrioritizedReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbow.rainbow_agent.RainbowAgent._build_target_distribution": [[213, 265], ["tensorflow.tile", "tensorflow.reshape", "tensorflow.concat", "tensorflow.gather_nd", "rainbow_agent.project_distribution", "tensorflow.cast", "tensorflow.argmax", "tensorflow.range", "tensorflow.to_int64"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.project_distribution"], ["", "def", "_build_target_distribution", "(", "self", ")", ":", "\n", "    ", "\"\"\"Builds the C51 target distribution as per Bellemare et al. (2017).\n\n    First, we compute the support of the Bellman target, r + gamma Z'. Where Z'\n    is the support of the next state distribution:\n\n      * Evenly spaced in [-vmax, vmax] if the current state is nonterminal;\n      * 0 otherwise (duplicated num_atoms times).\n\n    Second, we compute the next-state probabilities, corresponding to the action\n    with highest expected value.\n\n    Finally we project the Bellman target (support + probabilities) onto the\n    original support.\n\n    Returns:\n      target_distribution: tf.tensor, the target distribution from the replay.\n    \"\"\"", "\n", "batch_size", "=", "self", ".", "_replay", ".", "batch_size", "\n", "\n", "# size of rewards: batch_size x 1", "\n", "rewards", "=", "self", ".", "_replay", ".", "rewards", "[", ":", ",", "None", "]", "\n", "\n", "# size of tiled_support: batch_size x num_atoms", "\n", "tiled_support", "=", "tf", ".", "tile", "(", "self", ".", "_support", ",", "[", "batch_size", "]", ")", "\n", "tiled_support", "=", "tf", ".", "reshape", "(", "tiled_support", ",", "[", "batch_size", ",", "self", ".", "_num_atoms", "]", ")", "\n", "\n", "# size of target_support: batch_size x num_atoms", "\n", "\n", "is_terminal_multiplier", "=", "1.", "-", "tf", ".", "cast", "(", "self", ".", "_replay", ".", "terminals", ",", "tf", ".", "float32", ")", "\n", "# Incorporate terminal state to discount factor.", "\n", "# size of gamma_with_terminal: batch_size x 1", "\n", "gamma_with_terminal", "=", "self", ".", "cumulative_gamma", "*", "is_terminal_multiplier", "\n", "gamma_with_terminal", "=", "gamma_with_terminal", "[", ":", ",", "None", "]", "\n", "\n", "target_support", "=", "rewards", "+", "gamma_with_terminal", "*", "tiled_support", "\n", "\n", "# size of next_qt_argmax: 1 x batch_size", "\n", "next_qt_argmax", "=", "tf", ".", "argmax", "(", "\n", "self", ".", "_replay_next_target_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", ":", ",", "None", "]", "\n", "batch_indices", "=", "tf", ".", "range", "(", "tf", ".", "to_int64", "(", "batch_size", ")", ")", "[", ":", ",", "None", "]", "\n", "# size of next_qt_argmax: batch_size x 2", "\n", "batch_indexed_next_qt_argmax", "=", "tf", ".", "concat", "(", "\n", "[", "batch_indices", ",", "next_qt_argmax", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# size of next_probabilities: batch_size x num_atoms", "\n", "next_probabilities", "=", "tf", ".", "gather_nd", "(", "\n", "self", ".", "_replay_next_target_net_outputs", ".", "probabilities", ",", "\n", "batch_indexed_next_qt_argmax", ")", "\n", "\n", "return", "project_distribution", "(", "target_support", ",", "next_probabilities", ",", "\n", "self", ".", "_support", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbow.rainbow_agent.RainbowAgent._build_train_op": [[266, 318], ["tensorflow.stop_gradient", "tensorflow.concat", "tensorflow.gather_nd", "tensorflow.nn.softmax_cross_entropy_with_logits", "rainbow_agent.RainbowAgent._build_target_distribution", "tensorflow.range", "tensorflow.reduce_max", "rainbow_agent.RainbowAgent._replay.tf_set_priority", "tensorflow.no_op", "tensorflow.control_dependencies", "tensorflow.sqrt", "tensorflow.sqrt", "rainbow_agent.RainbowAgent.optimizer.minimize", "tensorflow.shape", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.reduce_mean", "tensorflow.reduce_mean"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_target_distribution", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.WrappedPrioritizedReplayBuffer.tf_set_priority"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "    ", "\"\"\"Builds a training op.\n\n    Returns:\n      train_op: An op performing one step of training from replay data.\n    \"\"\"", "\n", "target_distribution", "=", "tf", ".", "stop_gradient", "(", "self", ".", "_build_target_distribution", "(", ")", ")", "\n", "\n", "# size of indices: batch_size x 1.", "\n", "indices", "=", "tf", ".", "range", "(", "tf", ".", "shape", "(", "self", ".", "_replay_net_outputs", ".", "logits", ")", "[", "0", "]", ")", "[", ":", ",", "None", "]", "\n", "# size of reshaped_actions: batch_size x 2.", "\n", "reshaped_actions", "=", "tf", ".", "concat", "(", "[", "indices", ",", "self", ".", "_replay", ".", "actions", "[", ":", ",", "None", "]", "]", ",", "1", ")", "\n", "# For each element of the batch, fetch the logits for its selected action.", "\n", "chosen_action_logits", "=", "tf", ".", "gather_nd", "(", "self", ".", "_replay_net_outputs", ".", "logits", ",", "\n", "reshaped_actions", ")", "\n", "\n", "loss", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "\n", "labels", "=", "target_distribution", ",", "\n", "logits", "=", "chosen_action_logits", ")", "\n", "\n", "if", "self", ".", "_replay_scheme", "==", "'prioritized'", ":", "\n", "# The original prioritized experience replay uses a linear exponent", "\n", "# schedule 0.4 -> 1.0. Comparing the schedule to a fixed exponent of 0.5", "\n", "# on 5 games (Asterix, Pong, Q*Bert, Seaquest, Space Invaders) suggested", "\n", "# a fixed exponent actually performs better, except on Pong.", "\n", "      ", "probs", "=", "self", ".", "_replay", ".", "transition", "[", "'sampling_probabilities'", "]", "\n", "loss_weights", "=", "1.0", "/", "tf", ".", "sqrt", "(", "probs", "+", "1e-10", ")", "\n", "loss_weights", "/=", "tf", ".", "reduce_max", "(", "loss_weights", ")", "\n", "\n", "# Rainbow and prioritized replay are parametrized by an exponent alpha,", "\n", "# but in both cases it is set to 0.5 - for simplicity's sake we leave it", "\n", "# as is here, using the more direct tf.sqrt(). Taking the square root", "\n", "# \"makes sense\", as we are dealing with a squared loss.", "\n", "# Add a small nonzero value to the loss to avoid 0 priority items. While", "\n", "# technically this may be okay, setting all items to 0 priority will cause", "\n", "# troubles, and also result in 1.0 / 0.0 = NaN correction terms.", "\n", "update_priorities_op", "=", "self", ".", "_replay", ".", "tf_set_priority", "(", "\n", "self", ".", "_replay", ".", "indices", ",", "tf", ".", "sqrt", "(", "loss", "+", "1e-10", ")", ")", "\n", "\n", "# Weight the loss by the inverse priorities.", "\n", "loss", "=", "loss_weights", "*", "loss", "\n", "", "else", ":", "\n", "      ", "update_priorities_op", "=", "tf", ".", "no_op", "(", ")", "\n", "\n", "", "with", "tf", ".", "control_dependencies", "(", "[", "update_priorities_op", "]", ")", ":", "\n", "      ", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "          ", "tf", ".", "summary", ".", "scalar", "(", "'CrossEntropyLoss'", ",", "tf", ".", "reduce_mean", "(", "loss", ")", ")", "\n", "# Schaul et al. reports a slightly different rule, where 1/N is also", "\n", "# exponentiated by beta. Not doing so seems more reasonable, and did not", "\n", "# impact performance in our experiments.", "\n", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "tf", ".", "reduce_mean", "(", "loss", ")", ")", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbow.rainbow_agent.RainbowAgent._store_transition": [[319, 348], ["rainbow_agent.RainbowAgent._replay.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "", "def", "_store_transition", "(", "self", ",", "\n", "last_observation", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "is_terminal", ",", "\n", "priority", "=", "None", ")", ":", "\n", "    ", "\"\"\"Stores a transition when in training mode.\n\n    Executes a tf session and executes replay buffer ops in order to store the\n    following tuple in the replay buffer (last_observation, action, reward,\n    is_terminal, priority).\n\n    Args:\n      last_observation: Last observation, type determined via observation_type\n        parameter in the replay_memory constructor.\n      action: An integer, the action taken.\n      reward: A float, the reward.\n      is_terminal: Boolean indicating if the current state is a terminal state.\n      priority: Float. Priority of sampling the transition. If None, the default\n        priority will be used. If replay scheme is uniform, the default priority\n        is 1. If the replay scheme is prioritized, the default priority is the\n        maximum ever seen [Schaul et al., 2015].\n    \"\"\"", "\n", "if", "priority", "is", "None", ":", "\n", "      ", "priority", "=", "(", "1.", "if", "self", ".", "_replay_scheme", "==", "'uniform'", "else", "\n", "self", ".", "_replay", ".", "memory", ".", "sum_tree", ".", "max_recorded_priority", ")", "\n", "\n", "", "if", "not", "self", ".", "eval_mode", ":", "\n", "      ", "self", ".", "_replay", ".", "add", "(", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ",", "priority", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbow.rainbow_agent.project_distribution": [[350, 505], ["supports.shape.assert_is_compatible_with", "supports[].shape.assert_is_compatible_with", "target_support.shape.assert_has_rank", "validate_deps.append", "validate_deps.append", "validate_deps.append", "validate_deps.append", "validate_deps.append", "tensorflow.control_dependencies", "tensorflow.tile", "tensorflow.tile", "tensorflow.reshape", "tensorflow.abs", "tensorflow.clip_by_value", "tensorflow.reduce_sum", "tensorflow.reshape", "tensorflow.Assert", "tensorflow.Assert", "tensorflow.Assert", "tensorflow.Assert", "tensorflow.Assert", "tensorflow.shape", "tensorflow.shape", "tensorflow.clip_by_value", "tensorflow.reduce_all", "tensorflow.reduce_all", "tensorflow.equal", "tensorflow.reduce_all", "tensorflow.reduce_all", "tensorflow.equal", "tensorflow.equal", "tensorflow.size", "tensorflow.equal", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.size"], ["", "", "", "def", "project_distribution", "(", "supports", ",", "weights", ",", "target_support", ",", "\n", "validate_args", "=", "False", ")", ":", "\n", "  ", "\"\"\"Projects a batch of (support, weights) onto target_support.\n\n  Based on equation (7) in (Bellemare et al., 2017):\n    https://arxiv.org/abs/1707.06887\n  In the rest of the comments we will refer to this equation simply as Eq7.\n\n  This code is not easy to digest, so we will use a running example to clarify\n  what is going on, with the following sample inputs:\n\n    * supports =       [[0, 2, 4, 6, 8],\n                        [1, 3, 4, 5, 6]]\n    * weights =        [[0.1, 0.6, 0.1, 0.1, 0.1],\n                        [0.1, 0.2, 0.5, 0.1, 0.1]]\n    * target_support = [4, 5, 6, 7, 8]\n\n  In the code below, comments preceded with 'Ex:' will be referencing the above\n  values.\n\n  Args:\n    supports: Tensor of shape (batch_size, num_dims) defining supports for the\n      distribution.\n    weights: Tensor of shape (batch_size, num_dims) defining weights on the\n      original support points. Although for the CategoricalDQN agent these\n      weights are probabilities, it is not required that they are.\n    target_support: Tensor of shape (num_dims) defining support of the projected\n      distribution. The values must be monotonically increasing. Vmin and Vmax\n      will be inferred from the first and last elements of this tensor,\n      respectively. The values in this tensor must be equally spaced.\n    validate_args: Whether we will verify the contents of the\n      target_support parameter.\n\n  Returns:\n    A Tensor of shape (batch_size, num_dims) with the projection of a batch of\n    (support, weights) onto target_support.\n\n  Raises:\n    ValueError: If target_support has no dimensions, or if shapes of supports,\n      weights, and target_support are incompatible.\n  \"\"\"", "\n", "target_support_deltas", "=", "target_support", "[", "1", ":", "]", "-", "target_support", "[", ":", "-", "1", "]", "\n", "# delta_z = `\\Delta z` in Eq7.", "\n", "delta_z", "=", "target_support_deltas", "[", "0", "]", "\n", "validate_deps", "=", "[", "]", "\n", "supports", ".", "shape", ".", "assert_is_compatible_with", "(", "weights", ".", "shape", ")", "\n", "supports", "[", "0", "]", ".", "shape", ".", "assert_is_compatible_with", "(", "target_support", ".", "shape", ")", "\n", "target_support", ".", "shape", ".", "assert_has_rank", "(", "1", ")", "\n", "if", "validate_args", ":", "\n", "# Assert that supports and weights have the same shapes.", "\n", "    ", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "\n", "tf", ".", "reduce_all", "(", "tf", ".", "equal", "(", "tf", ".", "shape", "(", "supports", ")", ",", "tf", ".", "shape", "(", "weights", ")", ")", ")", ",", "\n", "[", "supports", ",", "weights", "]", ")", ")", "\n", "# Assert that elements of supports and target_support have the same shape.", "\n", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "\n", "tf", ".", "reduce_all", "(", "\n", "tf", ".", "equal", "(", "tf", ".", "shape", "(", "supports", ")", "[", "1", "]", ",", "tf", ".", "shape", "(", "target_support", ")", ")", ")", ",", "\n", "[", "supports", ",", "target_support", "]", ")", ")", "\n", "# Assert that target_support has a single dimension.", "\n", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "\n", "tf", ".", "equal", "(", "tf", ".", "size", "(", "tf", ".", "shape", "(", "target_support", ")", ")", ",", "1", ")", ",", "[", "target_support", "]", ")", ")", "\n", "# Assert that the target_support is monotonically increasing.", "\n", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "tf", ".", "reduce_all", "(", "target_support_deltas", ">", "0", ")", ",", "[", "target_support", "]", ")", ")", "\n", "# Assert that the values in target_support are equally spaced.", "\n", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "\n", "tf", ".", "reduce_all", "(", "tf", ".", "equal", "(", "target_support_deltas", ",", "delta_z", ")", ")", ",", "\n", "[", "target_support", "]", ")", ")", "\n", "\n", "", "with", "tf", ".", "control_dependencies", "(", "validate_deps", ")", ":", "\n", "# Ex: `v_min, v_max = 4, 8`.", "\n", "    ", "v_min", ",", "v_max", "=", "target_support", "[", "0", "]", ",", "target_support", "[", "-", "1", "]", "\n", "# Ex: `batch_size = 2`.", "\n", "batch_size", "=", "tf", ".", "shape", "(", "supports", ")", "[", "0", "]", "\n", "# `N` in Eq7.", "\n", "# Ex: `num_dims = 5`.", "\n", "num_dims", "=", "tf", ".", "shape", "(", "target_support", ")", "[", "0", "]", "\n", "# clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.", "\n", "# Ex: `clipped_support = [[[ 4.  4.  4.  6.  8.]]", "\n", "#                         [[ 4.  4.  4.  5.  6.]]]`.", "\n", "clipped_support", "=", "tf", ".", "clip_by_value", "(", "supports", ",", "v_min", ",", "v_max", ")", "[", ":", ",", "None", ",", ":", "]", "\n", "# Ex: `tiled_support = [[[[ 4.  4.  4.  6.  8.]", "\n", "#                         [ 4.  4.  4.  6.  8.]", "\n", "#                         [ 4.  4.  4.  6.  8.]", "\n", "#                         [ 4.  4.  4.  6.  8.]", "\n", "#                         [ 4.  4.  4.  6.  8.]]", "\n", "#                        [[ 4.  4.  4.  5.  6.]", "\n", "#                         [ 4.  4.  4.  5.  6.]", "\n", "#                         [ 4.  4.  4.  5.  6.]", "\n", "#                         [ 4.  4.  4.  5.  6.]", "\n", "#                         [ 4.  4.  4.  5.  6.]]]]`.", "\n", "tiled_support", "=", "tf", ".", "tile", "(", "[", "clipped_support", "]", ",", "[", "1", ",", "1", ",", "num_dims", ",", "1", "]", ")", "\n", "# Ex: `reshaped_target_support = [[[ 4.]", "\n", "#                                  [ 5.]", "\n", "#                                  [ 6.]", "\n", "#                                  [ 7.]", "\n", "#                                  [ 8.]]", "\n", "#                                 [[ 4.]", "\n", "#                                  [ 5.]", "\n", "#                                  [ 6.]", "\n", "#                                  [ 7.]", "\n", "#                                  [ 8.]]]`.", "\n", "reshaped_target_support", "=", "tf", ".", "tile", "(", "target_support", "[", ":", ",", "None", "]", ",", "[", "batch_size", ",", "1", "]", ")", "\n", "reshaped_target_support", "=", "tf", ".", "reshape", "(", "reshaped_target_support", ",", "\n", "[", "batch_size", ",", "num_dims", ",", "1", "]", ")", "\n", "# numerator = `|clipped_support - z_i|` in Eq7.", "\n", "# Ex: `numerator = [[[[ 0.  0.  0.  2.  4.]", "\n", "#                     [ 1.  1.  1.  1.  3.]", "\n", "#                     [ 2.  2.  2.  0.  2.]", "\n", "#                     [ 3.  3.  3.  1.  1.]", "\n", "#                     [ 4.  4.  4.  2.  0.]]", "\n", "#                    [[ 0.  0.  0.  1.  2.]", "\n", "#                     [ 1.  1.  1.  0.  1.]", "\n", "#                     [ 2.  2.  2.  1.  0.]", "\n", "#                     [ 3.  3.  3.  2.  1.]", "\n", "#                     [ 4.  4.  4.  3.  2.]]]]`.", "\n", "numerator", "=", "tf", ".", "abs", "(", "tiled_support", "-", "reshaped_target_support", ")", "\n", "quotient", "=", "1", "-", "(", "numerator", "/", "delta_z", ")", "\n", "# clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.", "\n", "# Ex: `clipped_quotient = [[[[ 1.  1.  1.  0.  0.]", "\n", "#                            [ 0.  0.  0.  0.  0.]", "\n", "#                            [ 0.  0.  0.  1.  0.]", "\n", "#                            [ 0.  0.  0.  0.  0.]", "\n", "#                            [ 0.  0.  0.  0.  1.]]", "\n", "#                           [[ 1.  1.  1.  0.  0.]", "\n", "#                            [ 0.  0.  0.  1.  0.]", "\n", "#                            [ 0.  0.  0.  0.  1.]", "\n", "#                            [ 0.  0.  0.  0.  0.]", "\n", "#                            [ 0.  0.  0.  0.  0.]]]]`.", "\n", "clipped_quotient", "=", "tf", ".", "clip_by_value", "(", "quotient", ",", "0", ",", "1", ")", "\n", "# Ex: `weights = [[ 0.1  0.6  0.1  0.1  0.1]", "\n", "#                 [ 0.1  0.2  0.5  0.1  0.1]]`.", "\n", "weights", "=", "weights", "[", ":", ",", "None", ",", ":", "]", "\n", "# inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x', \\pi(x'))`", "\n", "# in Eq7.", "\n", "# Ex: `inner_prod = [[[[ 0.1  0.6  0.1  0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.1 0. ]", "\n", "#                      [ 0.   0.   0.   0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.  0.1]]", "\n", "#                     [[ 0.1  0.2  0.5  0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.1 0. ]", "\n", "#                      [ 0.   0.   0.   0.  0.1]", "\n", "#                      [ 0.   0.   0.   0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.  0. ]]]]`.", "\n", "inner_prod", "=", "clipped_quotient", "*", "weights", "\n", "# Ex: `projection = [[ 0.8 0.0 0.1 0.0 0.1]", "\n", "#                    [ 0.8 0.1 0.1 0.0 0.0]]`.", "\n", "projection", "=", "tf", ".", "reduce_sum", "(", "inner_prod", ",", "3", ")", "\n", "projection", "=", "tf", ".", "reshape", "(", "projection", ",", "[", "batch_size", ",", "num_dims", "]", ")", "\n", "return", "projection", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent.__init__": [[70, 198], ["tensorflow.train.RMSPropOptimizer", "tensorflow.train.RMSPropOptimizer", "isinstance", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tuple", "math.pow", "epg_agent.ReplayBufferRegular", "tensorflow.train.Saver", "tensorflow.train.Saver", "tensorflow.device", "tensorflow.device", "numpy.zeros", "tensorflow.placeholder", "tensorflow.placeholder", "epg_agent.EPGAgent._build_replay_buffer", "epg_agent.EPGAgent._build_networks", "epg_agent.EPGAgent._build_train_op", "tensorflow.summary.merge_all", "tensorflow.summary.merge_all"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_replay_buffer", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_networks", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_train_op"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "game_name", "=", "\"Pong\"", ",", "\n", "observation_shape", "=", "NATURE_DQN_OBSERVATION_SHAPE", ",", "\n", "observation_dtype", "=", "NATURE_DQN_DTYPE", ",", "\n", "stack_size", "=", "NATURE_DQN_STACK_SIZE", ",", "\n", "gamma", "=", "0.99", ",", "\n", "update_horizon", "=", "1", ",", "\n", "min_replay_history", "=", "20000", ",", "\n", "update_period", "=", "4", ",", "\n", "epsilon_fn", "=", "linearly_decaying_epsilon", ",", "\n", "epsilon_train", "=", "0.01", ",", "\n", "epsilon_eval", "=", "0.001", ",", "\n", "epsilon_decay_period", "=", "250000", ",", "\n", "margin", "=", "1", ",", "\n", "tf_device", "=", "'/cpu:*'", ",", "\n", "use_staging", "=", "True", ",", "\n", "max_tf_checkpoints_to_keep", "=", "3", ",", "\n", "optimizer", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "\n", "decay", "=", "0.95", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "0.00001", ",", "\n", "centered", "=", "True", ")", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "        ", "\"\"\"Initializes the agent and constructs the components of its graph.\n\n        Args:\n          sess: `tf.Session`, for executing ops.\n          num_actions: int, number of actions the agent can take at any state.\n          observation_shape: tuple of ints describing the observation shape.\n          observation_dtype: tf.DType, specifies the type of the observations. Note\n            that if your inputs are continuous, you should set this to tf.float32.\n          stack_size: int, number of frames to use in state stack.\n          gamma: float, discount factor with the usual RL meaning.\n          update_horizon: int, horizon at which updates are performed, the 'n' in\n            n-step update.\n          min_replay_history: int, number of transitions that should be experienced\n            before the agent begins training its value function.\n          update_period: int, period between DQN updates.\n          target_update_period: int, update period for the target network.\n          epsilon_fn: function expecting 4 parameters:\n            (decay_period, step, warmup_steps, epsilon). This function should return\n            the epsilon value used for exploration during training.\n          epsilon_train: float, the value to which the agent's epsilon is eventually\n            decayed during training.\n          epsilon_eval: float, epsilon used when evaluating the agent.\n          epsilon_decay_period: int, length of the epsilon decay schedule.\n          tf_device: str, Tensorflow device on which the agent's graph is executed.\n          use_staging: bool, when True use a staging area to prefetch the next\n            training batch, speeding training up by about 30%.\n          max_tf_checkpoints_to_keep: int, the number of TensorFlow checkpoints to\n            keep.\n          optimizer: `tf.train.Optimizer`, for training the value function.\n          summary_writer: SummaryWriter object for outputting training statistics.\n            Summary writing disabled if set to None.\n          summary_writing_frequency: int, frequency with which summaries will be\n            written. Lower values will result in slower training.\n        \"\"\"", "\n", "assert", "isinstance", "(", "observation_shape", ",", "tuple", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Creating %s agent with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t gamma: %f'", ",", "gamma", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_horizon: %f'", ",", "update_horizon", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t min_replay_history: %d'", ",", "min_replay_history", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_period: %d'", ",", "update_period", ")", "\n", "# tf.logging.info('\\t random_seed: %d', random_seed)", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_train: %f'", ",", "epsilon_train", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_eval: %f'", ",", "epsilon_eval", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_decay_period: %d'", ",", "epsilon_decay_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t tf_device: %s'", ",", "tf_device", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t use_staging: %s'", ",", "use_staging", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t optimizer: %s'", ",", "optimizer", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t game: %s'", ",", "game_name", ")", "\n", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "self", ".", "observation_shape", "=", "tuple", "(", "observation_shape", ")", "\n", "self", ".", "observation_dtype", "=", "observation_dtype", "\n", "self", ".", "stack_size", "=", "stack_size", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "update_horizon", "=", "update_horizon", "\n", "self", ".", "cumulative_gamma", "=", "math", ".", "pow", "(", "gamma", ",", "update_horizon", ")", "\n", "self", ".", "min_replay_history", "=", "min_replay_history", "\n", "self", ".", "epsilon_fn", "=", "epsilon_fn", "\n", "self", ".", "epsilon_train", "=", "epsilon_train", "\n", "self", ".", "epsilon_eval", "=", "epsilon_eval", "\n", "self", ".", "epsilon_decay_period", "=", "epsilon_decay_period", "\n", "self", ".", "update_period", "=", "update_period", "\n", "self", ".", "eval_mode", "=", "False", "\n", "self", ".", "training_steps", "=", "0", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "summary_writer", "=", "summary_writer", "\n", "self", ".", "summary_writing_frequency", "=", "summary_writing_frequency", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "start_training", "=", "1000", "# todo task specific PONG IS 1000", "\n", "self", ".", "highest_reward", "=", "6", "# todo task specific", "\n", "self", ".", "isPrinted", "=", "False", "\n", "self", ".", "current_replay_size", "=", "0", "\n", "self", ".", "epsilon_current", "=", "1", "\n", "\n", "with", "tf", ".", "device", "(", "tf_device", ")", ":", "\n", "# Create a placeholder for the state input to the DQN network.", "\n", "# The last axis indicates the number of consecutive frames stacked.", "\n", "            ", "state_shape", "=", "(", "1", ",", ")", "+", "self", ".", "observation_shape", "+", "(", "stack_size", ",", ")", "\n", "self", ".", "state", "=", "np", ".", "zeros", "(", "state_shape", ")", "\n", "self", ".", "state_ph", "=", "tf", ".", "placeholder", "(", "self", ".", "observation_dtype", ",", "state_shape", ",", "\n", "name", "=", "'state_ph'", ")", "\n", "self", ".", "_replay", "=", "self", ".", "_build_replay_buffer", "(", "use_staging", ")", "\n", "\n", "self", ".", "_build_networks", "(", ")", "\n", "\n", "self", ".", "_train_op", "=", "self", ".", "_build_train_op", "(", ")", "\n", "\n", "", "self", ".", "replay_buffer", "=", "ReplayBufferRegular", "(", "100000", ")", "\n", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "# All tf.summaries should have been defined prior to running this.", "\n", "            ", "self", ".", "_merged_summaries", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "", "self", ".", "_sess", "=", "sess", "\n", "self", ".", "_saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "max_tf_checkpoints_to_keep", ")", "\n", "\n", "# Variables to be initialized by the agent once it interacts with the", "\n", "# environment.", "\n", "self", ".", "_observation", "=", "None", "\n", "self", ".", "_last_observation", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._get_network_type": [[199, 206], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the type of the outputs of a Q value network.\n\n        Returns:\n          net_type: _network_type object defining the outputs of the network.\n        \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "'DQN_network'", ",", "[", "'q_values'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._network_template": [[207, 225], ["tensorflow.cast", "tensorflow.cast", "tensorflow.div", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "slim.fully_connected", "slim.fully_connected", "epg_agent.EPGAgent._get_network_type"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Builds the convolutional network used to compute the agent's Q-values.\n\n        Args:\n          state: `tf.Tensor`, contains the agent's current state.\n\n        Returns:\n          net: _network_type object containing the tensors output by the network.\n        \"\"\"", "\n", "net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "net", "=", "tf", ".", "div", "(", "net", ",", "255.", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "net", ",", "512", ")", "\n", "q_values", "=", "slim", ".", "fully_connected", "(", "net", ",", "self", ".", "num_actions", ",", "activation_fn", "=", "None", ")", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._build_networks": [[226, 252], ["tensorflow.make_template", "tensorflow.make_template", "epg_agent.EPGAgent.online_convnet", "epg_agent.EPGAgent.online_convnet", "tensorflow.nn.log_softmax", "tensorflow.nn.log_softmax", "tensorflow.distributions.Categorical().sample", "tensorflow.distributions.Categorical().sample", "tensorflow.distributions.Categorical", "tensorflow.distributions.Categorical"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.sample", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.sample"], ["", "def", "_build_networks", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds the Q-value network computations needed for acting and training.\n\n        These are:\n          self.online_convnet: For computing the current state's Q-values.\n          self.target_convnet: For computing the next state's target Q-values.\n          self._net_outputs: The actual Q-values.\n          self._q_argmax: The action maximizing the current state's Q-values.\n          self._replay_net_outputs: The replayed states' Q-values.\n          self._replay_next_target_net_outputs: The replayed next states' target\n            Q-values (see Mnih et al., 2015 for details).\n        \"\"\"", "\n", "# Calling online_convnet will generate a new graph as defined in", "\n", "# self._get_network_template using whatever input is passed, but will always", "\n", "# share the same weights.", "\n", "self", ".", "online_convnet", "=", "tf", ".", "make_template", "(", "'Online'", ",", "self", ".", "_network_template", ")", "\n", "\n", "self", ".", "_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "state_ph", ")", "\n", "# using a deep network, but may affect performance with a linear", "\n", "# approximation scheme.", "\n", "# self._q_argmax = tf.argmax(self._net_outputs.q_values, axis=1)[0]", "\n", "\n", "self", ".", "_replay_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "states", ")", "\n", "# treat self._net_outputs.q_values as logits", "\n", "self", ".", "logsoftmaxprob", "=", "tf", ".", "nn", ".", "log_softmax", "(", "self", ".", "_net_outputs", ".", "q_values", ")", "\n", "self", ".", "sample", "=", "Categorical", "(", "logits", "=", "self", ".", "logsoftmaxprob", ")", ".", "sample", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._build_replay_buffer": [[254, 271], ["dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer"], "methods", ["None"], ["", "def", "_build_replay_buffer", "(", "self", ",", "use_staging", ")", ":", "\n", "        ", "\"\"\"Creates the replay buffer used by the agent.\n\n        Args:\n          use_staging: bool, if True, uses a staging area to prefetch data for\n            faster training.\n\n        Returns:\n          A WrapperReplayBuffer object.\n        \"\"\"", "\n", "return", "circular_replay_buffer", ".", "WrappedReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "observation_dtype", "=", "self", ".", "observation_dtype", ".", "as_numpy_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._build_train_op": [[272, 292], ["tensorflow.one_hot", "tensorflow.one_hot", "tensorflow.nn.log_softmax", "tensorflow.nn.log_softmax", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "epg_agent.EPGAgent.optimizer.minimize", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.variable_scope", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.summary.scalar"], "methods", ["None"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds a training op.\n\n        Returns:\n          train_op: An op performing one step of training from replay data.\n        \"\"\"", "\n", "\n", "replay_action_one_hot", "=", "tf", ".", "one_hot", "(", "\n", "self", ".", "_replay", ".", "actions", ",", "self", ".", "num_actions", ",", "1.", ",", "0.", ",", "name", "=", "'action_one_hot'", ")", "\n", "logits", "=", "self", ".", "_replay_net_outputs", ".", "q_values", "\n", "self", ".", "logsoftmaxprob", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ")", "\n", "self", ".", "neglogprob", "=", "-", "tf", ".", "reduce_sum", "(", "self", ".", "logsoftmaxprob", "*", "replay_action_one_hot", ",", "axis", "=", "1", ")", "\n", "# self.temp_loss = self.neglogprob # * self.y_pl", "\n", "loss", "=", "self", ".", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "neglogprob", ")", "\n", "self", ".", "replay_action_one_hot", "=", "replay_action_one_hot", "\n", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "                ", "tf", ".", "summary", ".", "scalar", "(", "'hingeLoss'", ",", "loss", ")", "\n", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent.begin_episode": [[293, 310], ["epg_agent.EPGAgent._reset_state", "epg_agent.EPGAgent._record_observation", "epg_agent.EPGAgent._select_action", "epg_agent.EPGAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._reset_state", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step"], ["", "def", "begin_episode", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Returns the agent's first action for this episode.\n\n        Args:\n          observation: numpy array, the environment's initial observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_reset_state", "(", ")", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent.step": [[311, 339], ["epg_agent.EPGAgent._record_observation", "epg_agent.EPGAgent._select_action", "isinstance", "epg_agent.EPGAgent._train_step", "collect_trajectory", "ValueError"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step", "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.collect_trajectory"], ["", "def", "step", "(", "self", ",", "reward", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records the most recent transition and returns the agent's next action.\n\n        We store the observation of the last time step since we want to store it\n        with the reward.\n\n        Args:\n          reward: float, the reward received from the agent's most recent action.\n          observation: numpy array, the most recent observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_last_observation", "=", "self", ".", "_observation", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "if", "self", ".", "game_name", "in", "[", "\"Pong\"", "]", ":", "\n", "                ", "collect_trajectory", "(", "self", ",", "reward", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"collection wrong trajectory\"", ")", "\n", "\n", "", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "if", "isinstance", "(", "self", ".", "action", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "pass", "\n", "", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent.end_episode": [[340, 354], ["collect_trajectory", "ValueError"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.collect_trajectory"], ["", "def", "end_episode", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Signals the end of the episode to the agent.\n\n        We store the observation of the current time step, which is the last\n        observation of the episode.\n\n        Args:\n          reward: float, the last reward from the environment.\n        \"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "if", "self", ".", "game_name", "in", "[", "\"Pong\"", "]", ":", "\n", "                ", "collect_trajectory", "(", "self", ",", "reward", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"collection wrong trajectory\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._select_action": [[355, 368], ["epg_agent.EPGAgent._sess.run"], "methods", ["None"], ["", "", "", "def", "_select_action", "(", "self", ")", ":", "\n", "        ", "\"\"\"Select an action from the set of available actions.\n\n        Chooses an action randomly with probability self._calculate_epsilon(), and\n        otherwise acts greedily according to the current Q-value estimates.\n\n        Returns:\n           int, the selected action.\n        \"\"\"", "\n", "\n", "self", ".", "epsilon_current", "=", "self", ".", "training_steps", "\n", "self", ".", "current_replay_size", "=", "self", ".", "_replay", ".", "memory", ".", "add_count", "\n", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "sample", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "[", "0", "]", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._train_step": [[370, 408], ["print", "epg_agent.EPGAgent._sess.run", "epg_agent.EPGAgent._sess.run", "epg_agent.EPGAgent.summary_writer.add_summary"], "methods", ["None"], ["", "def", "_train_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Runs a single training step.\n\n        Runs a training op if both:\n          (1) A minimum number of frames have been added to the replay buffer.\n          (2) `training_steps` is a multiple of `update_period`.\n\n        Also, syncs weights from online to target network if training steps is a\n        multiple of target update period.\n        \"\"\"", "\n", "# Run a train op at the rate of self.update_period if enough training steps", "\n", "# have been run. This matches the Nature DQN behaviour.", "\n", "\n", "if", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ":", "\n", "            ", "if", "self", ".", "training_steps", "%", "self", ".", "update_period", "==", "0", ":", "\n", "\n", "# debug checked.", "\n", "# _, neglogprob, logsoftmaxprob, \\", "\n", "# actor_loss, replay_action_one_hot = self._sess.run([self._train_op,", "\n", "#                                                    self.neglogprob,", "\n", "#                                                    self.logsoftmaxprob,", "\n", "#                                                    self.actor_loss,", "\n", "#                                                    self.replay_action_one_hot])", "\n", "                ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_op", ")", "\n", "if", "(", "self", ".", "summary_writer", "is", "not", "None", "and", "\n", "self", ".", "training_steps", ">", "0", "and", "\n", "self", ".", "training_steps", "%", "self", ".", "summary_writing_frequency", "==", "0", ")", ":", "\n", "                    ", "summary", "=", "self", ".", "_sess", ".", "run", "(", "self", ".", "_merged_summaries", ")", "\n", "self", ".", "summary_writer", ".", "add_summary", "(", "summary", ",", "self", ".", "training_steps", ")", "\n", "\n", "# if self.training_steps % self.target_update_period == 0:", "\n", "#     self._sess.run(self._sync_qt_ops)", "\n", "\n", "", "", "", "self", ".", "training_steps", "+=", "1", "\n", "\n", "if", "(", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ")", "and", "self", ".", "isPrinted", "is", "False", ":", "\n", "            ", "print", "(", "\"start training at {}\"", ".", "format", "(", "self", ".", "training_steps", ")", ")", "\n", "self", ".", "isPrinted", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._record_observation": [[409, 426], ["numpy.reshape", "numpy.reshape", "numpy.roll"], "methods", ["None"], ["", "", "def", "_record_observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records an observation and update state.\n\n        Extracts a frame from the observation vector and overwrites the oldest\n        frame in the state buffer.\n\n        Args:\n          observation: numpy array, an observation from the environment.\n        \"\"\"", "\n", "# Set current observation. We do the reshaping to handle environments", "\n", "# without frame stacking.", "\n", "observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "self", ".", "_observation", "=", "observation", "[", "...", ",", "0", "]", "\n", "self", ".", "_observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "# Swap out the oldest frame with the current frame.", "\n", "self", ".", "state", "=", "np", ".", "roll", "(", "self", ".", "state", ",", "-", "1", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "state", "[", "0", ",", "...", ",", "-", "1", "]", "=", "self", ".", "_observation", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._store_transition": [[427, 444], ["epg_agent.EPGAgent._replay.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "def", "_store_transition", "(", "self", ",", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", ":", "\n", "        ", "\"\"\"Stores an experienced transition.\n\n        Executes a tf session and executes replay buffer ops in order to store the\n        following tuple in the replay buffer:\n          (last_observation, action, reward, is_terminal).\n\n        Pedantically speaking, this does not actually store an entire transition\n        since the next state is recorded on the following time step.\n\n        Args:\n          last_observation: numpy array, last observation.\n          action: int, the action taken.\n          reward: float, the reward.\n          is_terminal: bool, indicating if the current state is a terminal state.\n        \"\"\"", "\n", "self", ".", "_replay", ".", "add", "(", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent._reset_state": [[445, 448], ["epg_agent.EPGAgent.state.fill"], "methods", ["None"], ["", "def", "_reset_state", "(", "self", ")", ":", "\n", "        ", "\"\"\"Resets the agent state by filling it with zeros.\"\"\"", "\n", "self", ".", "state", ".", "fill", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent.bundle_and_checkpoint": [[449, 479], ["epg_agent.EPGAgent._saver.save", "epg_agent.EPGAgent._replay.save", "tensorflow.gfile.Exists", "tensorflow.gfile.Exists", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save"], ["", "def", "bundle_and_checkpoint", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ")", ":", "\n", "        ", "\"\"\"Returns a self-contained bundle of the agent's state.\n\n        This is used for checkpointing. It will return a dictionary containing all\n        non-TensorFlow objects (to be saved into a file by the caller), and it saves\n        all TensorFlow objects into a checkpoint file.\n\n        Args:\n          checkpoint_dir: str, directory where TensorFlow objects will be saved.\n          iteration_number: int, iteration number to use for naming the checkpoint\n            file.\n\n        Returns:\n          A dict containing additional Python objects to be checkpointed by the\n            experiment. If the checkpoint directory does not exist, returns None.\n        \"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "checkpoint_dir", ")", ":", "\n", "            ", "return", "None", "\n", "# Call the Tensorflow saver to checkpoint the graph.", "\n", "", "self", ".", "_saver", ".", "save", "(", "\n", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'tf_ckpt'", ")", ",", "\n", "global_step", "=", "iteration_number", ")", "\n", "# Checkpoint the out-of-graph replay buffer.", "\n", "self", ".", "_replay", ".", "save", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "bundle_dictionary", "=", "{", "}", "\n", "bundle_dictionary", "[", "'state'", "]", "=", "self", ".", "state", "\n", "bundle_dictionary", "[", "'eval_mode'", "]", "=", "self", ".", "eval_mode", "\n", "bundle_dictionary", "[", "'training_steps'", "]", "=", "self", ".", "training_steps", "\n", "return", "bundle_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.EPGAgent.unbundle": [[480, 512], ["epg_agent.EPGAgent._saver.restore", "epg_agent.EPGAgent._replay.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load"], ["", "def", "unbundle", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ",", "bundle_dictionary", ")", ":", "\n", "        ", "\"\"\"Restores the agent from a checkpoint.\n\n        Restores the agent's Python objects to those specified in bundle_dictionary,\n        and restores the TensorFlow objects to those specified in the\n        checkpoint_dir. If the checkpoint_dir does not exist, will not reset the\n          agent's state.\n\n        Args:\n          checkpoint_dir: str, path to the checkpoint saved by tf.Save.\n          iteration_number: int, checkpoint version, used when restoring replay\n            buffer.\n          bundle_dictionary: dict, containing additional Python objects owned by\n            the agent.\n\n        Returns:\n          bool, True if unbundling was successful.\n        \"\"\"", "\n", "try", ":", "\n", "# self._replay.load() will throw a NotFoundError if it does not find all", "\n", "# the necessary files, in which case we abort the process & return False.", "\n", "            ", "self", ".", "_replay", ".", "load", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "            ", "return", "False", "\n", "", "for", "key", "in", "self", ".", "__dict__", ":", "\n", "            ", "if", "key", "in", "bundle_dictionary", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "bundle_dictionary", "[", "key", "]", "\n", "# Restore the agent's TensorFlow graph.", "\n", "", "", "self", ".", "_saver", ".", "restore", "(", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "\n", "'tf_ckpt-{}'", ".", "format", "(", "iteration_number", ")", ")", ")", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.ReplayBufferRegular.__init__": [[519, 526], ["collections.deque", "random.seed"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "buffer_size", ",", "random_seed", "=", "1234", ")", ":", "\n", "        ", "self", ".", "buffer_size", "=", "buffer_size", "\n", "self", ".", "count", "=", "0", "\n", "# Right side of deque contains newest experience", "\n", "self", ".", "buffer", "=", "deque", "(", ")", "\n", "random", ".", "seed", "(", "random_seed", ")", "\n", "self", ".", "ptr", ",", "self", ".", "path_start_idx", "=", "0", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.ReplayBufferRegular.add": [[527, 533], ["epg_agent.ReplayBufferRegular.buffer.append"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "add", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "terminal", ")", ":", "\n", "        ", "experience", "=", "[", "state", ",", "action", ",", "reward", ",", "terminal", "]", "\n", "assert", "self", ".", "count", "<", "self", ".", "buffer_size", "\n", "self", ".", "buffer", ".", "append", "(", "experience", ")", "\n", "self", ".", "count", "+=", "1", "\n", "self", ".", "ptr", "+=", "1", "\n", "# else:", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.ReplayBufferRegular.get_sample": [[539, 542], ["epg_agent.ReplayBufferRegular.buffer.popleft"], "methods", ["None"], ["", "def", "get_sample", "(", "self", ")", ":", "\n", "        ", "self", ".", "count", "-=", "1", "\n", "return", "self", ".", "buffer", ".", "popleft", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.ReplayBufferRegular.size": [[543, 545], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.ReplayBufferRegular.clear": [[546, 551], ["epg_agent.ReplayBufferRegular.buffer.clear"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.clear"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "buffer", ".", "clear", "(", ")", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "ptr", "=", "0", "\n", "self", ".", "path_start_idx", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.epg.epg_agent.linearly_decaying_epsilon": [[42, 64], ["numpy.clip"], "function", ["None"], ["def", "linearly_decaying_epsilon", "(", "decay_period", ",", "step", ",", "warmup_steps", ",", "epsilon", ")", ":", "\n", "    ", "\"\"\"Returns the current epsilon for the agent's epsilon-greedy policy.\n\n    This follows the Nature DQN schedule of a linearly decaying epsilon (Mnih et\n    al., 2015). The schedule is as follows:\n      Begin at 1. until warmup_steps steps have been taken; then\n      Linearly decay epsilon from 1. to epsilon in decay_period steps; and then\n      Use epsilon from there on.\n\n    Args:\n      decay_period: float, the period over which epsilon is decayed.\n      step: int, the number of training steps completed so far.\n      warmup_steps: int, the number of steps taken before epsilon is decayed.\n      epsilon: float, the final value to which to decay the epsilon parameter.\n\n    Returns:\n      A float, the current epsilon value computed according to the schedule.\n    \"\"\"", "\n", "steps_left", "=", "decay_period", "+", "warmup_steps", "-", "step", "\n", "bonus", "=", "(", "1.0", "-", "epsilon", ")", "*", "steps_left", "/", "decay_period", "\n", "bonus", "=", "np", ".", "clip", "(", "bonus", ",", "0.", ",", "1.", "-", "epsilon", ")", "\n", "return", "epsilon", "+", "bonus", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantile.implicit_quantile_agent.ImplicitQuantileAgent.__init__": [[43, 94], ["dopamine.agents.rainbow.rainbow_agent.RainbowAgent.__init__"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.__init__"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "kappa", "=", "1.0", ",", "\n", "num_tau_samples", "=", "32", ",", "\n", "num_tau_prime_samples", "=", "32", ",", "\n", "num_quantile_samples", "=", "32", ",", "\n", "quantile_embedding_dim", "=", "64", ",", "\n", "double_dqn", "=", "False", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "    ", "\"\"\"Initializes the agent and constructs the Graph.\n\n    Most of this constructor's parameters are IQN-specific hyperparameters whose\n    values are taken from Dabney et al. (2018).\n\n    Args:\n      sess: `tf.Session` object for running associated ops.\n      num_actions: int, number of actions the agent can take at any state.\n      kappa: float, Huber loss cutoff.\n      num_tau_samples: int, number of online quantile samples for loss\n        estimation.\n      num_tau_prime_samples: int, number of target quantile samples for loss\n        estimation.\n      num_quantile_samples: int, number of quantile samples for computing\n        Q-values.\n      quantile_embedding_dim: int, embedding dimension for the quantile input.\n      double_dqn: boolean, whether to perform double DQN style learning\n        as described in Van Hasselt et al.: https://arxiv.org/abs/1509.06461.\n      summary_writer: SummaryWriter object for outputting training statistics.\n        Summary writing disabled if set to None.\n      summary_writing_frequency: int, frequency with which summaries will be\n        written. Lower values will result in slower training.\n    \"\"\"", "\n", "self", ".", "kappa", "=", "kappa", "\n", "# num_tau_samples = N below equation (3) in the paper.", "\n", "self", ".", "num_tau_samples", "=", "num_tau_samples", "\n", "# num_tau_prime_samples = N' below equation (3) in the paper.", "\n", "self", ".", "num_tau_prime_samples", "=", "num_tau_prime_samples", "\n", "# num_quantile_samples = k below equation (3) in the paper.", "\n", "self", ".", "num_quantile_samples", "=", "num_quantile_samples", "\n", "# quantile_embedding_dim = n above equation (4) in the paper.", "\n", "self", ".", "quantile_embedding_dim", "=", "quantile_embedding_dim", "\n", "# option to perform double dqn.", "\n", "self", ".", "double_dqn", "=", "double_dqn", "\n", "\n", "super", "(", "ImplicitQuantileAgent", ",", "self", ")", ".", "__init__", "(", "\n", "sess", "=", "sess", ",", "\n", "num_actions", "=", "num_actions", ",", "\n", "summary_writer", "=", "summary_writer", ",", "\n", "summary_writing_frequency", "=", "summary_writing_frequency", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantile.implicit_quantile_agent.ImplicitQuantileAgent._get_network_type": [[95, 103], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns the type of the outputs of the implicit quantile network.\n\n    Returns:\n      _network_type object defining the outputs of the network.\n    \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "\n", "'iqn_network'", ",", "[", "'quantile_values'", ",", "'quantiles'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantile.implicit_quantile_agent.ImplicitQuantileAgent._network_template": [[104, 160], ["slim.variance_scaling_initializer", "tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "tensorflow.tile", "tensorflow.random_uniform", "tensorflow.tile", "tensorflow.constant", "tensorflow.cos", "slim.fully_connected", "tensorflow.multiply", "slim.fully_connected", "slim.fully_connected", "slim.flatten.get_shape().as_list", "slim.flatten.get_shape().as_list", "implicit_quantile_agent.ImplicitQuantileAgent._get_network_type", "tensorflow.cast", "numpy.sqrt", "slim.flatten.get_shape", "slim.flatten.get_shape", "tensorflow.range"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ",", "num_quantiles", ")", ":", "\n", "    ", "r\"\"\"Builds an Implicit Quantile ConvNet.\n\n    Takes state and quantile as inputs and outputs state-action quantile values.\n\n    Args:\n      state: A `tf.placeholder` for the RL state.\n      num_quantiles: int, number of quantile inputs.\n\n    Returns:\n      _network_type object containing quantile value outputs of the network.\n    \"\"\"", "\n", "\n", "weights_initializer", "=", "slim", ".", "variance_scaling_initializer", "(", "\n", "factor", "=", "1.0", "/", "np", ".", "sqrt", "(", "3.0", ")", ",", "mode", "=", "'FAN_IN'", ",", "uniform", "=", "True", ")", "\n", "\n", "state_net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "state_net", "=", "tf", ".", "div", "(", "state_net", ",", "255.", ")", "\n", "state_net", "=", "slim", ".", "conv2d", "(", "\n", "state_net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "state_net", "=", "slim", ".", "conv2d", "(", "\n", "state_net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "state_net", "=", "slim", ".", "conv2d", "(", "\n", "state_net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "state_net", "=", "slim", ".", "flatten", "(", "state_net", ")", "\n", "state_net_size", "=", "state_net", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "-", "1", "]", "\n", "state_net_tiled", "=", "tf", ".", "tile", "(", "state_net", ",", "[", "num_quantiles", ",", "1", "]", ")", "\n", "\n", "batch_size", "=", "state_net", ".", "get_shape", "(", ")", ".", "as_list", "(", ")", "[", "0", "]", "\n", "quantiles_shape", "=", "[", "num_quantiles", "*", "batch_size", ",", "1", "]", "\n", "quantiles", "=", "tf", ".", "random_uniform", "(", "\n", "quantiles_shape", ",", "minval", "=", "0", ",", "maxval", "=", "1", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "\n", "quantile_net", "=", "tf", ".", "tile", "(", "quantiles", ",", "[", "1", ",", "self", ".", "quantile_embedding_dim", "]", ")", "\n", "pi", "=", "tf", ".", "constant", "(", "math", ".", "pi", ")", "\n", "quantile_net", "=", "tf", ".", "cast", "(", "tf", ".", "range", "(", "\n", "1", ",", "self", ".", "quantile_embedding_dim", "+", "1", ",", "1", ")", ",", "tf", ".", "float32", ")", "*", "pi", "*", "quantile_net", "\n", "quantile_net", "=", "tf", ".", "cos", "(", "quantile_net", ")", "\n", "quantile_net", "=", "slim", ".", "fully_connected", "(", "quantile_net", ",", "state_net_size", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "# Hadamard product.", "\n", "net", "=", "tf", ".", "multiply", "(", "state_net_tiled", ",", "quantile_net", ")", "\n", "\n", "net", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "512", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "quantile_values", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "\n", "self", ".", "num_actions", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "quantile_values", "=", "quantile_values", ",", "\n", "quantiles", "=", "quantiles", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantile.implicit_quantile_agent.ImplicitQuantileAgent._build_networks": [[161, 227], ["tensorflow.make_template", "tensorflow.make_template", "implicit_quantile_agent.ImplicitQuantileAgent.online_convnet", "tensorflow.reduce_mean", "tensorflow.argmax", "implicit_quantile_agent.ImplicitQuantileAgent.online_convnet", "implicit_quantile_agent.ImplicitQuantileAgent.target_convnet", "tensorflow.reshape", "tensorflow.squeeze", "tensorflow.argmax", "implicit_quantile_agent.ImplicitQuantileAgent.online_convnet", "implicit_quantile_agent.ImplicitQuantileAgent.target_convnet", "tensorflow.reduce_mean"], "methods", ["None"], ["", "def", "_build_networks", "(", "self", ")", ":", "\n", "    ", "\"\"\"Builds the IQN computations needed for acting and training.\n\n    These are:\n      self.online_convnet: For computing the current state's quantile values.\n      self.target_convnet: For computing the next state's target quantile\n        values.\n      self._net_outputs: The actual quantile values.\n      self._q_argmax: The action maximizing the current state's Q-values.\n      self._replay_net_outputs: The replayed states' quantile values.\n      self._replay_next_target_net_outputs: The replayed next states' target\n        quantile values.\n    \"\"\"", "\n", "# Calling online_convnet will generate a new graph as defined in", "\n", "# self._get_network_template using whatever input is passed, but will always", "\n", "# share the same weights.", "\n", "self", ".", "online_convnet", "=", "tf", ".", "make_template", "(", "'Online'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "target_convnet", "=", "tf", ".", "make_template", "(", "'Target'", ",", "self", ".", "_network_template", ")", "\n", "\n", "# Compute the Q-values which are used for action selection in the current", "\n", "# state.", "\n", "self", ".", "_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "state_ph", ",", "\n", "self", ".", "num_quantile_samples", ")", "\n", "# Shape of self._net_outputs.quantile_values:", "\n", "# num_quantile_samples x num_actions.", "\n", "# e.g. if num_actions is 2, it might look something like this:", "\n", "# Vals for Quantile .2  Vals for Quantile .4  Vals for Quantile .6", "\n", "#    [[0.1, 0.5],         [0.15, -0.3],          [0.15, -0.2]]", "\n", "# Q-values = [(0.1 + 0.15 + 0.15)/3, (0.5 + 0.15 + -0.2)/3].", "\n", "self", ".", "_q_values", "=", "tf", ".", "reduce_mean", "(", "self", ".", "_net_outputs", ".", "quantile_values", ",", "axis", "=", "0", ")", "\n", "self", ".", "_q_argmax", "=", "tf", ".", "argmax", "(", "self", ".", "_q_values", ",", "axis", "=", "0", ")", "\n", "\n", "self", ".", "_replay_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "states", ",", "\n", "self", ".", "num_tau_samples", ")", "\n", "# Shape: (num_tau_samples x batch_size) x num_actions.", "\n", "self", ".", "_replay_net_quantile_values", "=", "self", ".", "_replay_net_outputs", ".", "quantile_values", "\n", "self", ".", "_replay_net_quantiles", "=", "self", ".", "_replay_net_outputs", ".", "quantiles", "\n", "\n", "# Do the same for next states in the replay buffer.", "\n", "self", ".", "_replay_net_target_outputs", "=", "self", ".", "target_convnet", "(", "\n", "self", ".", "_replay", ".", "next_states", ",", "self", ".", "num_tau_prime_samples", ")", "\n", "# Shape: (num_tau_prime_samples x batch_size) x num_actions.", "\n", "vals", "=", "self", ".", "_replay_net_target_outputs", ".", "quantile_values", "\n", "self", ".", "_replay_net_target_quantile_values", "=", "vals", "\n", "\n", "# Compute Q-values which are used for action selection for the next states", "\n", "# in the replay buffer. Compute the argmax over the Q-values.", "\n", "if", "self", ".", "double_dqn", ":", "\n", "      ", "outputs_action", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "next_states", ",", "\n", "self", ".", "num_quantile_samples", ")", "\n", "", "else", ":", "\n", "      ", "outputs_action", "=", "self", ".", "target_convnet", "(", "self", ".", "_replay", ".", "next_states", ",", "\n", "self", ".", "num_quantile_samples", ")", "\n", "\n", "# Shape: (num_quantile_samples x batch_size) x num_actions.", "\n", "", "target_quantile_values_action", "=", "outputs_action", ".", "quantile_values", "\n", "# Shape: num_quantile_samples x batch_size x num_actions.", "\n", "target_quantile_values_action", "=", "tf", ".", "reshape", "(", "target_quantile_values_action", ",", "\n", "[", "self", ".", "num_quantile_samples", ",", "\n", "self", ".", "_replay", ".", "batch_size", ",", "\n", "self", ".", "num_actions", "]", ")", "\n", "# Shape: batch_size x num_actions.", "\n", "self", ".", "_replay_net_target_q_values", "=", "tf", ".", "squeeze", "(", "tf", ".", "reduce_mean", "(", "\n", "target_quantile_values_action", ",", "axis", "=", "0", ")", ")", "\n", "self", ".", "_replay_next_qt_argmax", "=", "tf", ".", "argmax", "(", "\n", "self", ".", "_replay_net_target_q_values", ",", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantile.implicit_quantile_agent.ImplicitQuantileAgent._build_target_quantile_values_op": [[228, 267], ["tensorflow.tile", "tensorflow.tile", "tensorflow.tile", "tensorflow.cast", "tensorflow.concat", "tensorflow.shape", "tensorflow.to_float", "tensorflow.gather_nd", "tensorflow.range"], "methods", ["None"], ["", "def", "_build_target_quantile_values_op", "(", "self", ")", ":", "\n", "    ", "\"\"\"Build an op used as a target for return values at given quantiles.\n\n    Returns:\n      An op calculating the target quantile return.\n    \"\"\"", "\n", "batch_size", "=", "tf", ".", "shape", "(", "self", ".", "_replay", ".", "rewards", ")", "[", "0", "]", "\n", "# Shape of rewards: (num_tau_prime_samples x batch_size) x 1.", "\n", "rewards", "=", "self", ".", "_replay", ".", "rewards", "[", ":", ",", "None", "]", "\n", "rewards", "=", "tf", ".", "tile", "(", "rewards", ",", "[", "self", ".", "num_tau_prime_samples", ",", "1", "]", ")", "\n", "\n", "is_terminal_multiplier", "=", "1.", "-", "tf", ".", "to_float", "(", "self", ".", "_replay", ".", "terminals", ")", "\n", "# Incorporate terminal state to discount factor.", "\n", "# size of gamma_with_terminal: (num_tau_prime_samples x batch_size) x 1.", "\n", "gamma_with_terminal", "=", "self", ".", "cumulative_gamma", "*", "is_terminal_multiplier", "\n", "gamma_with_terminal", "=", "tf", ".", "tile", "(", "gamma_with_terminal", "[", ":", ",", "None", "]", ",", "\n", "[", "self", ".", "num_tau_prime_samples", ",", "1", "]", ")", "\n", "\n", "# Get the indices of the maximium Q-value across the action dimension.", "\n", "# Shape of replay_next_qt_argmax: (num_tau_prime_samples x batch_size) x 1.", "\n", "\n", "replay_next_qt_argmax", "=", "tf", ".", "tile", "(", "\n", "self", ".", "_replay_next_qt_argmax", "[", ":", ",", "None", "]", ",", "[", "self", ".", "num_tau_prime_samples", ",", "1", "]", ")", "\n", "\n", "# Shape of batch_indices: (num_tau_prime_samples x batch_size) x 1.", "\n", "batch_indices", "=", "tf", ".", "cast", "(", "tf", ".", "range", "(", "\n", "self", ".", "num_tau_prime_samples", "*", "batch_size", ")", "[", ":", ",", "None", "]", ",", "tf", ".", "int64", ")", "\n", "\n", "# Shape of batch_indexed_target_values:", "\n", "# (num_tau_prime_samples x batch_size) x 2.", "\n", "batch_indexed_target_values", "=", "tf", ".", "concat", "(", "\n", "[", "batch_indices", ",", "replay_next_qt_argmax", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# Shape of next_target_values: (num_tau_prime_samples x batch_size) x 1.", "\n", "target_quantile_values", "=", "tf", ".", "gather_nd", "(", "\n", "self", ".", "_replay_net_target_quantile_values", ",", "\n", "batch_indexed_target_values", ")", "[", ":", ",", "None", "]", "\n", "\n", "return", "rewards", "+", "gamma_with_terminal", "*", "target_quantile_values", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantile.implicit_quantile_agent.ImplicitQuantileAgent._build_train_op": [[268, 359], ["tensorflow.stop_gradient", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.tile", "tensorflow.concat", "tensorflow.gather_nd", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.reshape", "tensorflow.transpose", "tensorflow.to_float", "tensorflow.reduce_sum", "tensorflow.reduce_mean", "tensorflow.no_op", "tensorflow.shape", "implicit_quantile_agent.ImplicitQuantileAgent._build_target_quantile_values_op", "tensorflow.range", "tensorflow.tile", "tensorflow.control_dependencies", "tensorflow.to_float", "tensorflow.to_float", "tensorflow.abs", "tensorflow.abs", "implicit_quantile_agent.ImplicitQuantileAgent.optimizer.minimize", "tensorflow.reduce_mean", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.reduce_mean", "tensorflow.abs", "tensorflow.abs", "tensorflow.stop_gradient", "tensorflow.reduce_mean", "tensorflow.to_float"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.implicit_quantile.implicit_quantile_agent.ImplicitQuantileAgent._build_target_quantile_values_op"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "    ", "\"\"\"Builds a training op.\n\n    Returns:\n      train_op: An op performing one step of training from replay data.\n    \"\"\"", "\n", "batch_size", "=", "tf", ".", "shape", "(", "self", ".", "_replay", ".", "rewards", ")", "[", "0", "]", "\n", "\n", "target_quantile_values", "=", "tf", ".", "stop_gradient", "(", "\n", "self", ".", "_build_target_quantile_values_op", "(", ")", ")", "\n", "# Reshape to self.num_tau_prime_samples x batch_size x 1 since this is", "\n", "# the manner in which the target_quantile_values are tiled.", "\n", "target_quantile_values", "=", "tf", ".", "reshape", "(", "target_quantile_values", ",", "\n", "[", "self", ".", "num_tau_prime_samples", ",", "\n", "batch_size", ",", "1", "]", ")", "\n", "# Transpose dimensions so that the dimensionality is batch_size x", "\n", "# self.num_tau_prime_samples x 1 to prepare for computation of", "\n", "# Bellman errors.", "\n", "# Final shape of target_quantile_values:", "\n", "# batch_size x num_tau_prime_samples x 1.", "\n", "target_quantile_values", "=", "tf", ".", "transpose", "(", "target_quantile_values", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "# Shape of indices: (num_tau_samples x batch_size) x 1.", "\n", "# Expand dimension by one so that it can be used to index into all the", "\n", "# quantiles when using the tf.gather_nd function (see below).", "\n", "indices", "=", "tf", ".", "range", "(", "self", ".", "num_tau_samples", "*", "batch_size", ")", "[", ":", ",", "None", "]", "\n", "\n", "# Expand the dimension by one so that it can be used to index into all the", "\n", "# quantiles when using the tf.gather_nd function (see below).", "\n", "reshaped_actions", "=", "self", ".", "_replay", ".", "actions", "[", ":", ",", "None", "]", "\n", "reshaped_actions", "=", "tf", ".", "tile", "(", "reshaped_actions", ",", "[", "self", ".", "num_tau_samples", ",", "1", "]", ")", "\n", "# Shape of reshaped_actions: (num_tau_samples x batch_size) x 2.", "\n", "reshaped_actions", "=", "tf", ".", "concat", "(", "[", "indices", ",", "reshaped_actions", "]", ",", "axis", "=", "1", ")", "\n", "\n", "chosen_action_quantile_values", "=", "tf", ".", "gather_nd", "(", "\n", "self", ".", "_replay_net_quantile_values", ",", "reshaped_actions", ")", "\n", "# Reshape to self.num_tau_samples x batch_size x 1 since this is the manner", "\n", "# in which the quantile values are tiled.", "\n", "chosen_action_quantile_values", "=", "tf", ".", "reshape", "(", "chosen_action_quantile_values", ",", "\n", "[", "self", ".", "num_tau_samples", ",", "\n", "batch_size", ",", "1", "]", ")", "\n", "# Transpose dimensions so that the dimensionality is batch_size x", "\n", "# self.num_tau_samples x 1 to prepare for computation of", "\n", "# Bellman errors.", "\n", "# Final shape of chosen_action_quantile_values:", "\n", "# batch_size x num_tau_samples x 1.", "\n", "chosen_action_quantile_values", "=", "tf", ".", "transpose", "(", "\n", "chosen_action_quantile_values", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "# Shape of bellman_erors and huber_loss:", "\n", "# batch_size x num_tau_prime_samples x num_tau_samples x 1.", "\n", "bellman_errors", "=", "target_quantile_values", "[", "\n", ":", ",", ":", ",", "None", ",", ":", "]", "-", "chosen_action_quantile_values", "[", ":", ",", "None", ",", ":", ",", ":", "]", "\n", "# The huber loss (see Section 2.3 of the paper) is defined via two cases:", "\n", "# case_one: |bellman_errors| <= kappa", "\n", "# case_two: |bellman_errors| > kappa", "\n", "huber_loss_case_one", "=", "tf", ".", "to_float", "(", "\n", "tf", ".", "abs", "(", "bellman_errors", ")", "<=", "self", ".", "kappa", ")", "*", "0.5", "*", "bellman_errors", "**", "2", "\n", "huber_loss_case_two", "=", "tf", ".", "to_float", "(", "\n", "tf", ".", "abs", "(", "bellman_errors", ")", ">", "self", ".", "kappa", ")", "*", "self", ".", "kappa", "*", "(", "\n", "tf", ".", "abs", "(", "bellman_errors", ")", "-", "0.5", "*", "self", ".", "kappa", ")", "\n", "huber_loss", "=", "huber_loss_case_one", "+", "huber_loss_case_two", "\n", "\n", "# Reshape replay_quantiles to batch_size x num_tau_samples x 1", "\n", "replay_quantiles", "=", "tf", ".", "reshape", "(", "\n", "self", ".", "_replay_net_quantiles", ",", "[", "self", ".", "num_tau_samples", ",", "batch_size", ",", "1", "]", ")", "\n", "replay_quantiles", "=", "tf", ".", "transpose", "(", "replay_quantiles", ",", "[", "1", ",", "0", ",", "2", "]", ")", "\n", "\n", "# Tile by num_tau_prime_samples along a new dimension. Shape is now", "\n", "# batch_size x num_tau_prime_samples x num_tau_samples x 1.", "\n", "# These quantiles will be used for computation of the quantile huber loss", "\n", "# below (see section 2.3 of the paper).", "\n", "replay_quantiles", "=", "tf", ".", "to_float", "(", "tf", ".", "tile", "(", "\n", "replay_quantiles", "[", ":", ",", "None", ",", ":", ",", ":", "]", ",", "[", "1", ",", "self", ".", "num_tau_prime_samples", ",", "1", ",", "1", "]", ")", ")", "\n", "# Shape: batch_size x num_tau_prime_samples x num_tau_samples x 1.", "\n", "quantile_huber_loss", "=", "(", "tf", ".", "abs", "(", "replay_quantiles", "-", "tf", ".", "stop_gradient", "(", "\n", "tf", ".", "to_float", "(", "bellman_errors", "<", "0", ")", ")", ")", "*", "huber_loss", ")", "/", "self", ".", "kappa", "\n", "# Sum over current quantile value (num_tau_samples) dimension,", "\n", "# average over target quantile value (num_tau_prime_samples) dimension.", "\n", "# Shape: batch_size x num_tau_prime_samples x 1.", "\n", "loss", "=", "tf", ".", "reduce_sum", "(", "quantile_huber_loss", ",", "axis", "=", "2", ")", "\n", "# Shape: batch_size x 1.", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "loss", ",", "axis", "=", "1", ")", "\n", "\n", "# TODO(kumasaurabh): Add prioritized replay functionality here.", "\n", "update_priorities_op", "=", "tf", ".", "no_op", "(", ")", "\n", "with", "tf", ".", "control_dependencies", "(", "[", "update_priorities_op", "]", ")", ":", "\n", "      ", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "        ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "          ", "tf", ".", "summary", ".", "scalar", "(", "'QuantileLoss'", ",", "tf", ".", "reduce_mean", "(", "loss", ")", ")", "\n", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "tf", ".", "reduce_mean", "(", "loss", ")", ")", ",", "tf", ".", "reduce_mean", "(", "loss", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent.__init__": [[69, 200], ["tensorflow.train.RMSPropOptimizer", "isinstance", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tuple", "math.pow", "rpg_agent.ReplayBufferRegular", "tensorflow.train.Saver", "tensorflow.device", "numpy.zeros", "tensorflow.placeholder", "rpg_agent.RPGAgent._build_replay_buffer", "rpg_agent.RPGAgent._build_networks", "rpg_agent.RPGAgent._build_train_op", "tensorflow.summary.merge_all"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_replay_buffer", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_networks", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_train_op"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "game_name", "=", "\"Pong\"", ",", "\n", "observation_shape", "=", "NATURE_DQN_OBSERVATION_SHAPE", ",", "\n", "observation_dtype", "=", "NATURE_DQN_DTYPE", ",", "\n", "stack_size", "=", "NATURE_DQN_STACK_SIZE", ",", "\n", "gamma", "=", "0.99", ",", "\n", "update_horizon", "=", "1", ",", "\n", "min_replay_history", "=", "20000", ",", "\n", "update_period", "=", "4", ",", "\n", "epsilon_fn", "=", "linearly_decaying_epsilon", ",", "\n", "epsilon_train", "=", "0.01", ",", "\n", "epsilon_eval", "=", "0.001", ",", "\n", "epsilon_decay_period", "=", "250000", ",", "\n", "margin", "=", "1", ",", "\n", "tf_device", "=", "'/cpu:*'", ",", "\n", "use_staging", "=", "True", ",", "\n", "max_tf_checkpoints_to_keep", "=", "3", ",", "\n", "optimizer", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "\n", "decay", "=", "0.95", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "0.00001", ",", "\n", "centered", "=", "True", ")", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "        ", "\"\"\"Initializes the agent and constructs the components of its graph.\n\n        Args:\n          sess: `tf.Session`, for executing ops.\n          num_actions: int, number of actions the agent can take at any state.\n          observation_shape: tuple of ints describing the observation shape.\n          observation_dtype: tf.DType, specifies the type of the observations. Note\n            that if your inputs are continuous, you should set this to tf.float32.\n          stack_size: int, number of frames to use in state stack.\n          gamma: float, discount factor with the usual RL meaning.\n          update_horizon: int, horizon at which updates are performed, the 'n' in\n            n-step update.\n          min_replay_history: int, number of transitions that should be experienced\n            before the agent begins training its value function.\n          update_period: int, period between DQN updates.\n          target_update_period: int, update period for the target network.\n          epsilon_fn: function expecting 4 parameters:\n            (decay_period, step, warmup_steps, epsilon). This function should return\n            the epsilon value used for exploration during training.\n          epsilon_train: float, the value to which the agent's epsilon is eventually\n            decayed during training.\n          epsilon_eval: float, epsilon used when evaluating the agent.\n          epsilon_decay_period: int, length of the epsilon decay schedule.\n          tf_device: str, Tensorflow device on which the agent's graph is executed.\n          use_staging: bool, when True use a staging area to prefetch the next\n            training batch, speeding training up by about 30%.\n          max_tf_checkpoints_to_keep: int, the number of TensorFlow checkpoints to\n            keep.\n          optimizer: `tf.train.Optimizer`, for training the value function.\n          summary_writer: SummaryWriter object for outputting training statistics.\n            Summary writing disabled if set to None.\n          summary_writing_frequency: int, frequency with which summaries will be\n            written. Lower values will result in slower training.\n        \"\"\"", "\n", "assert", "isinstance", "(", "observation_shape", ",", "tuple", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Creating %s agent with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t gamma: %f'", ",", "gamma", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_horizon: %f'", ",", "update_horizon", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t min_replay_history: %d'", ",", "min_replay_history", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_period: %d'", ",", "update_period", ")", "\n", "# tf.logging.info('\\t random_seed: %d', random_seed)", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_train: %f'", ",", "epsilon_train", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_eval: %f'", ",", "epsilon_eval", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_decay_period: %d'", ",", "epsilon_decay_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t tf_device: %s'", ",", "tf_device", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t use_staging: %s'", ",", "use_staging", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t optimizer: %s'", ",", "optimizer", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t game: %s'", ",", "game_name", ")", "\n", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "self", ".", "observation_shape", "=", "tuple", "(", "observation_shape", ")", "\n", "self", ".", "observation_dtype", "=", "observation_dtype", "\n", "self", ".", "stack_size", "=", "stack_size", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "update_horizon", "=", "update_horizon", "\n", "self", ".", "cumulative_gamma", "=", "math", ".", "pow", "(", "gamma", ",", "update_horizon", ")", "\n", "self", ".", "min_replay_history", "=", "min_replay_history", "\n", "self", ".", "epsilon_fn", "=", "epsilon_fn", "\n", "self", ".", "epsilon_train", "=", "epsilon_train", "\n", "self", ".", "epsilon_eval", "=", "epsilon_eval", "\n", "self", ".", "epsilon_decay_period", "=", "epsilon_decay_period", "\n", "self", ".", "update_period", "=", "update_period", "\n", "self", ".", "eval_mode", "=", "False", "\n", "self", ".", "training_steps", "=", "0", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "summary_writer", "=", "summary_writer", "\n", "self", ".", "summary_writing_frequency", "=", "summary_writing_frequency", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "start_training", "=", "1000", "# todo task specific", "\n", "self", ".", "highest_reward", "=", "6", "# todo task specific", "\n", "self", ".", "exploration_strategy", "=", "\"NonEGP\"", "# NonEPG for random explore.", "\n", "# todo EPG exploration failed since when hinge loss is small, it will optimize the cross entropy,", "\n", "# which deviate the hing loss.", "\n", "self", ".", "isPrinted", "=", "False", "\n", "self", ".", "current_replay_size", "=", "0", "\n", "self", ".", "epsilon_current", "=", "1", "\n", "\n", "with", "tf", ".", "device", "(", "tf_device", ")", ":", "\n", "# Create a placeholder for the state input to the DQN network.", "\n", "# The last axis indicates the number of consecutive frames stacked.", "\n", "            ", "state_shape", "=", "(", "1", ",", ")", "+", "self", ".", "observation_shape", "+", "(", "stack_size", ",", ")", "\n", "self", ".", "state", "=", "np", ".", "zeros", "(", "state_shape", ")", "\n", "self", ".", "state_ph", "=", "tf", ".", "placeholder", "(", "self", ".", "observation_dtype", ",", "state_shape", ",", "\n", "name", "=", "'state_ph'", ")", "\n", "self", ".", "_replay", "=", "self", ".", "_build_replay_buffer", "(", "use_staging", ")", "\n", "\n", "self", ".", "_build_networks", "(", ")", "\n", "\n", "self", ".", "_train_op", "=", "self", ".", "_build_train_op", "(", ")", "\n", "\n", "", "self", ".", "replay_buffer", "=", "ReplayBufferRegular", "(", "100000", ")", "\n", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "# All tf.summaries should have been defined prior to running this.", "\n", "            ", "self", ".", "_merged_summaries", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "", "self", ".", "_sess", "=", "sess", "\n", "self", ".", "_saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "max_tf_checkpoints_to_keep", ")", "\n", "\n", "# Variables to be initialized by the agent once it interacts with the", "\n", "# environment.", "\n", "self", ".", "_observation", "=", "None", "\n", "self", ".", "_last_observation", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._get_network_type": [[201, 208], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the type of the outputs of a Q value network.\n\n        Returns:\n          net_type: _network_type object defining the outputs of the network.\n        \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "'DQN_network'", ",", "[", "'q_values'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._network_template": [[209, 227], ["tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "slim.fully_connected", "slim.fully_connected", "rpg_agent.RPGAgent._get_network_type"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Builds the convolutional network used to compute the agent's Q-values.\n\n        Args:\n          state: `tf.Tensor`, contains the agent's current state.\n\n        Returns:\n          net: _network_type object containing the tensors output by the network.\n        \"\"\"", "\n", "net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "net", "=", "tf", ".", "div", "(", "net", ",", "255.", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "net", ",", "512", ")", "\n", "q_values", "=", "slim", ".", "fully_connected", "(", "net", ",", "self", ".", "num_actions", ",", "activation_fn", "=", "None", ")", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._build_networks": [[228, 256], ["tensorflow.make_template", "tensorflow.make_template", "rpg_agent.RPGAgent.online_convnet", "rpg_agent.RPGAgent.online_convnet", "rpg_agent.RPGAgent.target_convnet", "tensorflow.nn.log_softmax", "tensorflow.distributions.Categorical().sample", "tensorflow.argmax", "tensorflow.distributions.Categorical"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.sample"], ["", "def", "_build_networks", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds the Q-value network computations needed for acting and training.\n\n        These are:\n          self.online_convnet: For computing the current state's Q-values.\n          self.target_convnet: For computing the next state's target Q-values.\n          self._net_outputs: The actual Q-values.\n          self._q_argmax: The action maximizing the current state's Q-values.\n          self._replay_net_outputs: The replayed states' Q-values.\n          self._replay_next_target_net_outputs: The replayed next states' target\n            Q-values (see Mnih et al., 2015 for details).\n        \"\"\"", "\n", "# Calling online_convnet will generate a new graph as defined in", "\n", "# self._get_network_template using whatever input is passed, but will always", "\n", "# share the same weights.", "\n", "self", ".", "online_convnet", "=", "tf", ".", "make_template", "(", "'Online'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "target_convnet", "=", "tf", ".", "make_template", "(", "'Target'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "state_ph", ")", "\n", "# TODO(bellemare): Ties should be broken. They are unlikely to happen when", "\n", "# using a deep network, but may affect performance with a linear", "\n", "# approximation scheme.", "\n", "self", ".", "_q_argmax", "=", "tf", ".", "argmax", "(", "self", ".", "_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "\n", "self", ".", "_replay_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "states", ")", "\n", "self", ".", "_replay_next_target_net_outputs", "=", "self", ".", "target_convnet", "(", "\n", "self", ".", "_replay", ".", "next_states", ")", "\n", "self", ".", "logsoftmaxprob", "=", "tf", ".", "nn", ".", "log_softmax", "(", "self", ".", "_net_outputs", ".", "q_values", ")", "\n", "self", ".", "sample", "=", "Categorical", "(", "logits", "=", "self", ".", "logsoftmaxprob", ")", ".", "sample", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._build_replay_buffer": [[257, 274], ["dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer"], "methods", ["None"], ["", "def", "_build_replay_buffer", "(", "self", ",", "use_staging", ")", ":", "\n", "        ", "\"\"\"Creates the replay buffer used by the agent.\n\n        Args:\n          use_staging: bool, if True, uses a staging area to prefetch data for\n            faster training.\n\n        Returns:\n          A WrapperReplayBuffer object.\n        \"\"\"", "\n", "return", "circular_replay_buffer", ".", "WrappedReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "observation_dtype", "=", "self", ".", "observation_dtype", ".", "as_numpy_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._build_train_op": [[275, 318], ["tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.maximum", "tensorflow.reduce_mean", "rpg_agent.RPGAgent.optimizer.minimize", "tensorflow.nn.log_softmax", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.reshape", "tensorflow.ones", "tensorflow.summary.scalar"], "methods", ["None"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds a training op.\n\n        Returns:\n          train_op: An op performing one step of training from replay data.\n        \"\"\"", "\n", "margin", "=", "1", "\n", "\n", "replay_action_one_hot", "=", "tf", ".", "one_hot", "(", "\n", "self", ".", "_replay", ".", "actions", ",", "self", ".", "num_actions", ",", "1.", ",", "0.", ",", "name", "=", "'action_one_hot'", ")", "\n", "qvalue", "=", "self", ".", "_replay_net_outputs", ".", "q_values", "\n", "replay_chosen_q", "=", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "_replay_net_outputs", ".", "q_values", "*", "replay_action_one_hot", ",", "\n", "reduction_indices", "=", "1", ",", "\n", "name", "=", "'replay_chosen_q'", ")", "\n", "self", ".", "temp_action_one_hot", "=", "replay_action_one_hot", "\n", "self", ".", "temp_qvalue", "=", "qvalue", "\n", "\n", "# Q_j + c - Q_* = temp1 + temp2", "\n", "# temp1 = [Q_j + c, Q_*]", "\n", "# temp2 = [-Q_*, -Q_*]", "\n", "self", ".", "temp1", "=", "(", "qvalue", "+", "margin", ")", "*", "(", "1", "-", "replay_action_one_hot", ")", "+", "qvalue", "*", "replay_action_one_hot", "\n", "self", ".", "temp2", "=", "-", "(", "tf", ".", "reshape", "(", "replay_chosen_q", ",", "[", "-", "1", ",", "1", "]", ")", "*", "tf", ".", "ones", "(", "[", "1", ",", "self", ".", "num_actions", "]", ")", ")", "*", "(", "(", "1", "-", "replay_action_one_hot", ")", "+", "(", "replay_action_one_hot", ")", ")", "\n", "self", ".", "hingeloss", "=", "tf", ".", "maximum", "(", "0.0", ",", "self", ".", "temp1", "+", "self", ".", "temp2", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "hingeloss", ")", "\n", "\n", "if", "self", ".", "exploration_strategy", "==", "\"EPG\"", ":", "\n", "            ", "logits", "=", "qvalue", "\n", "self", ".", "logsoftmaxprob", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ")", "\n", "self", ".", "neglogprob", "=", "-", "tf", ".", "reduce_sum", "(", "self", ".", "logsoftmaxprob", "*", "replay_action_one_hot", ",", "axis", "=", "1", ")", "\n", "self", ".", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "neglogprob", ")", "\n", "loss", "=", "self", ".", "actor_loss", "+", "loss", "\n", "\n", "# target = tf.stop_gradient(self._build_target_q_op())", "\n", "# loss = tf.losses.huber_loss(", "\n", "#     target, replay_chosen_q, reduction=tf.losses.Reduction.NONE)", "\n", "", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "                ", "tf", ".", "summary", ".", "scalar", "(", "'hingeLoss'", ",", "loss", ")", "\n", "if", "self", ".", "exploration_strategy", "==", "\"EPG\"", ":", "\n", "                    ", "tf", ".", "summary", ".", "scalar", "(", "'actorloss'", ",", "self", ".", "actor_loss", ")", "\n", "", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent.begin_episode": [[319, 336], ["rpg_agent.RPGAgent._reset_state", "rpg_agent.RPGAgent._record_observation", "rpg_agent.RPGAgent._select_action", "rpg_agent.RPGAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._reset_state", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step"], ["", "def", "begin_episode", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Returns the agent's first action for this episode.\n\n        Args:\n          observation: numpy array, the environment's initial observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_reset_state", "(", ")", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent.step": [[337, 363], ["rpg_agent.RPGAgent._record_observation", "rpg_agent.RPGAgent._select_action", "rpg_agent.RPGAgent._train_step", "collect_trajectory", "ValueError"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step", "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.collect_trajectory"], ["", "def", "step", "(", "self", ",", "reward", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records the most recent transition and returns the agent's next action.\n\n        We store the observation of the last time step since we want to store it\n        with the reward.\n\n        Args:\n          reward: float, the reward received from the agent's most recent action.\n          observation: numpy array, the most recent observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_last_observation", "=", "self", ".", "_observation", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "if", "self", ".", "game_name", "in", "[", "\"Pong\"", "]", ":", "\n", "                ", "collect_trajectory", "(", "self", ",", "reward", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"collection wrong trajectory\"", ")", "\n", "\n", "", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent.end_episode": [[364, 378], ["collect_trajectory", "ValueError"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.collect_trajectory"], ["", "def", "end_episode", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Signals the end of the episode to the agent.\n\n        We store the observation of the current time step, which is the last\n        observation of the episode.\n\n        Args:\n          reward: float, the last reward from the environment.\n        \"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "if", "self", ".", "game_name", "in", "[", "\"Pong\"", "]", ":", "\n", "                ", "collect_trajectory", "(", "self", ",", "reward", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"collection wrong trajectory\"", ")", "\n", "# if reward < 0:", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._select_action_training": [[390, 393], ["rpg_agent.RPGAgent._sess.run"], "methods", ["None"], ["", "", "", "def", "_select_action_training", "(", "self", ")", ":", "\n", "        ", "\"\"\"Use EPG to select action during training, \"\"\"", "\n", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "sample", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "[", "0", "]", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._select_action": [[394, 432], ["rpg_agent.RPGAgent._select_action_training", "rpg_agent.RPGAgent._sess.run", "random.random", "random.randint", "rpg_agent.RPGAgent._sess.run"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._select_action_training"], ["", "def", "_select_action", "(", "self", ")", ":", "\n", "        ", "\"\"\"Select an action from the set of available actions.\n\n        Chooses an action randomly with probability self._calculate_epsilon(), and\n        otherwise acts greedily according to the current Q-value estimates.\n\n        Returns:\n           int, the selected action.\n        \"\"\"", "\n", "# epsilon = self.epsilon_eval if self.eval_mode else self.epsilon_fn(", "\n", "#     self.epsilon_decay_period,", "\n", "#     self.training_steps,", "\n", "#     self.min_replay_history,", "\n", "#     self.epsilon_train)", "\n", "\n", "exploration", "=", "self", ".", "exploration_strategy", "# default random explore", "\n", "if", "exploration", "==", "\"EPG\"", ":", "\n", "            ", "self", ".", "epsilon_current", "=", "0", "\n", "self", ".", "current_replay_size", "=", "self", ".", "_replay", ".", "memory", ".", "add_count", "\n", "if", "self", ".", "eval_mode", ":", "\n", "                ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "", "return", "self", ".", "_select_action_training", "(", ")", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "training_steps", "<", "self", ".", "min_replay_history", ":", "\n", "                ", "epsilon", "=", "1", "\n", "", "else", ":", "\n", "                ", "epsilon", "=", "self", ".", "epsilon_train", "\n", "", "if", "self", ".", "eval_mode", ":", "\n", "                ", "epsilon", "=", "self", ".", "epsilon_eval", "\n", "\n", "", "self", ".", "epsilon_current", "=", "epsilon", "\n", "self", ".", "current_replay_size", "=", "self", ".", "_replay", ".", "memory", ".", "add_count", "\n", "if", "random", ".", "random", "(", ")", "<=", "epsilon", ":", "\n", "# Choose a random action with probability epsilon.", "\n", "                ", "return", "random", ".", "randint", "(", "0", ",", "self", ".", "num_actions", "-", "1", ")", "\n", "", "else", ":", "\n", "# Choose the action with highest Q-value at the current state.", "\n", "                ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._train_step": [[433, 471], ["print", "rpg_agent.RPGAgent._sess.run", "rpg_agent.RPGAgent._sess.run", "rpg_agent.RPGAgent.summary_writer.add_summary"], "methods", ["None"], ["", "", "", "def", "_train_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Runs a single training step.\n\n        Runs a training op if both:\n          (1) A minimum number of frames have been added to the replay buffer.\n          (2) `training_steps` is a multiple of `update_period`.\n\n        Also, syncs weights from online to target network if training steps is a\n        multiple of target update period.\n        \"\"\"", "\n", "# Run a train op at the rate of self.update_period if enough training steps", "\n", "# have been run. This matches the Nature DQN behaviour.", "\n", "\n", "if", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ":", "\n", "            ", "if", "self", ".", "training_steps", "%", "self", ".", "update_period", "==", "0", ":", "\n", "\n", "# # debug checked.", "\n", "# _, temp1, temp2, taction, tqvalue, hingloss = self._sess.run([self._train_op,", "\n", "#                                             self.temp1,", "\n", "#                                             self.temp2,", "\n", "#                                             self.temp_action_one_hot,", "\n", "#                                             self.temp_qvalue,", "\n", "#                                             self.hingeloss])", "\n", "                ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_op", ")", "\n", "if", "(", "self", ".", "summary_writer", "is", "not", "None", "and", "\n", "self", ".", "training_steps", ">", "0", "and", "\n", "self", ".", "training_steps", "%", "self", ".", "summary_writing_frequency", "==", "0", ")", ":", "\n", "                    ", "summary", "=", "self", ".", "_sess", ".", "run", "(", "self", ".", "_merged_summaries", ")", "\n", "self", ".", "summary_writer", ".", "add_summary", "(", "summary", ",", "self", ".", "training_steps", ")", "\n", "\n", "# if self.training_steps % self.target_update_period == 0:", "\n", "#     self._sess.run(self._sync_qt_ops)", "\n", "\n", "", "", "", "self", ".", "training_steps", "+=", "1", "\n", "\n", "if", "(", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ")", "and", "self", ".", "isPrinted", "is", "False", ":", "\n", "            ", "print", "(", "\"start training at {}\"", ".", "format", "(", "self", ".", "training_steps", ")", ")", "\n", "self", ".", "isPrinted", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._record_observation": [[472, 489], ["numpy.reshape", "numpy.reshape", "numpy.roll"], "methods", ["None"], ["", "", "def", "_record_observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records an observation and update state.\n\n        Extracts a frame from the observation vector and overwrites the oldest\n        frame in the state buffer.\n\n        Args:\n          observation: numpy array, an observation from the environment.\n        \"\"\"", "\n", "# Set current observation. We do the reshaping to handle environments", "\n", "# without frame stacking.", "\n", "observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "self", ".", "_observation", "=", "observation", "[", "...", ",", "0", "]", "\n", "self", ".", "_observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "# Swap out the oldest frame with the current frame.", "\n", "self", ".", "state", "=", "np", ".", "roll", "(", "self", ".", "state", ",", "-", "1", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "state", "[", "0", ",", "...", ",", "-", "1", "]", "=", "self", ".", "_observation", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._store_transition": [[490, 507], ["rpg_agent.RPGAgent._replay.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "def", "_store_transition", "(", "self", ",", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", ":", "\n", "        ", "\"\"\"Stores an experienced transition.\n\n        Executes a tf session and executes replay buffer ops in order to store the\n        following tuple in the replay buffer:\n          (last_observation, action, reward, is_terminal).\n\n        Pedantically speaking, this does not actually store an entire transition\n        since the next state is recorded on the following time step.\n\n        Args:\n          last_observation: numpy array, last observation.\n          action: int, the action taken.\n          reward: float, the reward.\n          is_terminal: bool, indicating if the current state is a terminal state.\n        \"\"\"", "\n", "self", ".", "_replay", ".", "add", "(", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent._reset_state": [[508, 511], ["rpg_agent.RPGAgent.state.fill"], "methods", ["None"], ["", "def", "_reset_state", "(", "self", ")", ":", "\n", "        ", "\"\"\"Resets the agent state by filling it with zeros.\"\"\"", "\n", "self", ".", "state", ".", "fill", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent.bundle_and_checkpoint": [[512, 542], ["rpg_agent.RPGAgent._saver.save", "rpg_agent.RPGAgent._replay.save", "tensorflow.gfile.Exists", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save"], ["", "def", "bundle_and_checkpoint", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ")", ":", "\n", "        ", "\"\"\"Returns a self-contained bundle of the agent's state.\n\n        This is used for checkpointing. It will return a dictionary containing all\n        non-TensorFlow objects (to be saved into a file by the caller), and it saves\n        all TensorFlow objects into a checkpoint file.\n\n        Args:\n          checkpoint_dir: str, directory where TensorFlow objects will be saved.\n          iteration_number: int, iteration number to use for naming the checkpoint\n            file.\n\n        Returns:\n          A dict containing additional Python objects to be checkpointed by the\n            experiment. If the checkpoint directory does not exist, returns None.\n        \"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "checkpoint_dir", ")", ":", "\n", "            ", "return", "None", "\n", "# Call the Tensorflow saver to checkpoint the graph.", "\n", "", "self", ".", "_saver", ".", "save", "(", "\n", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'tf_ckpt'", ")", ",", "\n", "global_step", "=", "iteration_number", ")", "\n", "# Checkpoint the out-of-graph replay buffer.", "\n", "self", ".", "_replay", ".", "save", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "bundle_dictionary", "=", "{", "}", "\n", "bundle_dictionary", "[", "'state'", "]", "=", "self", ".", "state", "\n", "bundle_dictionary", "[", "'eval_mode'", "]", "=", "self", ".", "eval_mode", "\n", "bundle_dictionary", "[", "'training_steps'", "]", "=", "self", ".", "training_steps", "\n", "return", "bundle_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.RPGAgent.unbundle": [[543, 575], ["rpg_agent.RPGAgent._saver.restore", "rpg_agent.RPGAgent._replay.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load"], ["", "def", "unbundle", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ",", "bundle_dictionary", ")", ":", "\n", "        ", "\"\"\"Restores the agent from a checkpoint.\n\n        Restores the agent's Python objects to those specified in bundle_dictionary,\n        and restores the TensorFlow objects to those specified in the\n        checkpoint_dir. If the checkpoint_dir does not exist, will not reset the\n          agent's state.\n\n        Args:\n          checkpoint_dir: str, path to the checkpoint saved by tf.Save.\n          iteration_number: int, checkpoint version, used when restoring replay\n            buffer.\n          bundle_dictionary: dict, containing additional Python objects owned by\n            the agent.\n\n        Returns:\n          bool, True if unbundling was successful.\n        \"\"\"", "\n", "try", ":", "\n", "# self._replay.load() will throw a NotFoundError if it does not find all", "\n", "# the necessary files, in which case we abort the process & return False.", "\n", "            ", "self", ".", "_replay", ".", "load", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "            ", "return", "False", "\n", "", "for", "key", "in", "self", ".", "__dict__", ":", "\n", "            ", "if", "key", "in", "bundle_dictionary", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "bundle_dictionary", "[", "key", "]", "\n", "# Restore the agent's TensorFlow graph.", "\n", "", "", "self", ".", "_saver", ".", "restore", "(", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "\n", "'tf_ckpt-{}'", ".", "format", "(", "iteration_number", ")", ")", ")", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.ReplayBufferRegular.__init__": [[582, 589], ["collections.deque", "random.seed"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "buffer_size", ",", "random_seed", "=", "1234", ")", ":", "\n", "        ", "self", ".", "buffer_size", "=", "buffer_size", "\n", "self", ".", "count", "=", "0", "\n", "# Right side of deque contains newest experience", "\n", "self", ".", "buffer", "=", "deque", "(", ")", "\n", "random", ".", "seed", "(", "random_seed", ")", "\n", "self", ".", "ptr", ",", "self", ".", "path_start_idx", "=", "0", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.ReplayBufferRegular.add": [[590, 596], ["rpg_agent.ReplayBufferRegular.buffer.append"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "add", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "terminal", ")", ":", "\n", "        ", "experience", "=", "[", "state", ",", "action", ",", "reward", ",", "terminal", "]", "\n", "assert", "self", ".", "count", "<", "self", ".", "buffer_size", "\n", "self", ".", "buffer", ".", "append", "(", "experience", ")", "\n", "self", ".", "count", "+=", "1", "\n", "self", ".", "ptr", "+=", "1", "\n", "# else:", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.ReplayBufferRegular.get_sample": [[602, 605], ["rpg_agent.ReplayBufferRegular.buffer.popleft"], "methods", ["None"], ["", "def", "get_sample", "(", "self", ")", ":", "\n", "        ", "self", ".", "count", "-=", "1", "\n", "return", "self", ".", "buffer", ".", "popleft", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.ReplayBufferRegular.size": [[606, 608], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.ReplayBufferRegular.clear": [[609, 614], ["rpg_agent.ReplayBufferRegular.buffer.clear"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.clear"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "buffer", ".", "clear", "(", ")", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "ptr", "=", "0", "\n", "self", ".", "path_start_idx", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.rpg.rpg_agent.linearly_decaying_epsilon": [[41, 63], ["numpy.clip"], "function", ["None"], ["def", "linearly_decaying_epsilon", "(", "decay_period", ",", "step", ",", "warmup_steps", ",", "epsilon", ")", ":", "\n", "    ", "\"\"\"Returns the current epsilon for the agent's epsilon-greedy policy.\n\n    This follows the Nature DQN schedule of a linearly decaying epsilon (Mnih et\n    al., 2015). The schedule is as follows:\n      Begin at 1. until warmup_steps steps have been taken; then\n      Linearly decay epsilon from 1. to epsilon in decay_period steps; and then\n      Use epsilon from there on.\n\n    Args:\n      decay_period: float, the period over which epsilon is decayed.\n      step: int, the number of training steps completed so far.\n      warmup_steps: int, the number of steps taken before epsilon is decayed.\n      epsilon: float, the final value to which to decay the epsilon parameter.\n\n    Returns:\n      A float, the current epsilon value computed according to the schedule.\n    \"\"\"", "\n", "steps_left", "=", "decay_period", "+", "warmup_steps", "-", "step", "\n", "bonus", "=", "(", "1.0", "-", "epsilon", ")", "*", "steps_left", "/", "decay_period", "\n", "bonus", "=", "np", ".", "clip", "(", "bonus", ",", "0.", ",", "1.", "-", "epsilon", ")", "\n", "return", "epsilon", "+", "bonus", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent.__init__": [[69, 204], ["tensorflow.train.RMSPropOptimizer", "isinstance", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tuple", "math.pow", "tensorflow.train.RMSPropOptimizer", "repg_agent.ReplayBufferRegular", "tensorflow.train.Saver", "tensorflow.device", "numpy.zeros", "tensorflow.placeholder", "repg_agent.REPGAgent._build_replay_buffer", "repg_agent.REPGAgent._build_networks", "repg_agent.REPGAgent._build_train_op", "tensorflow.summary.merge_all"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_replay_buffer", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_networks", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_train_op"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "game_name", "=", "\"Pong\"", ",", "\n", "observation_shape", "=", "NATURE_DQN_OBSERVATION_SHAPE", ",", "\n", "observation_dtype", "=", "NATURE_DQN_DTYPE", ",", "\n", "stack_size", "=", "NATURE_DQN_STACK_SIZE", ",", "\n", "gamma", "=", "0.99", ",", "\n", "update_horizon", "=", "1", ",", "\n", "min_replay_history", "=", "20000", ",", "\n", "update_period", "=", "4", ",", "\n", "epsilon_fn", "=", "linearly_decaying_epsilon", ",", "\n", "epsilon_train", "=", "0.01", ",", "\n", "epsilon_eval", "=", "0.001", ",", "\n", "epsilon_decay_period", "=", "250000", ",", "\n", "margin", "=", "1", ",", "\n", "tf_device", "=", "'/cpu:*'", ",", "\n", "use_staging", "=", "True", ",", "\n", "max_tf_checkpoints_to_keep", "=", "3", ",", "\n", "optimizer", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "\n", "decay", "=", "0.95", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "0.00001", ",", "\n", "centered", "=", "True", ")", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "        ", "\"\"\"Initializes the agent and constructs the components of its graph.\n\n        Args:\n          sess: `tf.Session`, for executing ops.\n          num_actions: int, number of actions the agent can take at any state.\n          observation_shape: tuple of ints describing the observation shape.\n          observation_dtype: tf.DType, specifies the type of the observations. Note\n            that if your inputs are continuous, you should set this to tf.float32.\n          stack_size: int, number of frames to use in state stack.\n          gamma: float, discount factor with the usual RL meaning.\n          update_horizon: int, horizon at which updates are performed, the 'n' in\n            n-step update.\n          min_replay_history: int, number of transitions that should be experienced\n            before the agent begins training its value function.\n          update_period: int, period between DQN updates.\n          target_update_period: int, update period for the target network.\n          epsilon_fn: function expecting 4 parameters:\n            (decay_period, step, warmup_steps, epsilon). This function should return\n            the epsilon value used for exploration during training.\n          epsilon_train: float, the value to which the agent's epsilon is eventually\n            decayed during training.\n          epsilon_eval: float, epsilon used when evaluating the agent.\n          epsilon_decay_period: int, length of the epsilon decay schedule.\n          tf_device: str, Tensorflow device on which the agent's graph is executed.\n          use_staging: bool, when True use a staging area to prefetch the next\n            training batch, speeding training up by about 30%.\n          max_tf_checkpoints_to_keep: int, the number of TensorFlow checkpoints to\n            keep.\n          optimizer: `tf.train.Optimizer`, for training the value function.\n          summary_writer: SummaryWriter object for outputting training statistics.\n            Summary writing disabled if set to None.\n          summary_writing_frequency: int, frequency with which summaries will be\n            written. Lower values will result in slower training.\n        \"\"\"", "\n", "assert", "isinstance", "(", "observation_shape", ",", "tuple", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Creating %s agent with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t gamma: %f'", ",", "gamma", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_horizon: %f'", ",", "update_horizon", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t min_replay_history: %d'", ",", "min_replay_history", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_period: %d'", ",", "update_period", ")", "\n", "# tf.logging.info('\\t random_seed: %d', random_seed)", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_train: %f'", ",", "epsilon_train", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_eval: %f'", ",", "epsilon_eval", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_decay_period: %d'", ",", "epsilon_decay_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t tf_device: %s'", ",", "tf_device", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t use_staging: %s'", ",", "use_staging", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t optimizer: %s'", ",", "optimizer", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t game: %s'", ",", "game_name", ")", "\n", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "self", ".", "observation_shape", "=", "tuple", "(", "observation_shape", ")", "\n", "self", ".", "observation_dtype", "=", "observation_dtype", "\n", "self", ".", "stack_size", "=", "stack_size", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "update_horizon", "=", "update_horizon", "\n", "self", ".", "cumulative_gamma", "=", "math", ".", "pow", "(", "gamma", ",", "update_horizon", ")", "\n", "self", ".", "min_replay_history", "=", "min_replay_history", "\n", "self", ".", "epsilon_fn", "=", "epsilon_fn", "\n", "self", ".", "epsilon_train", "=", "epsilon_train", "\n", "self", ".", "epsilon_eval", "=", "epsilon_eval", "\n", "self", ".", "epsilon_decay_period", "=", "epsilon_decay_period", "\n", "self", ".", "update_period", "=", "update_period", "\n", "self", ".", "eval_mode", "=", "False", "\n", "self", ".", "training_steps", "=", "0", "\n", "self", ".", "optimizer", "=", "optimizer", "# optimizer for RPG", "\n", "self", ".", "optimizer_exp", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "\n", "decay", "=", "0.95", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "0.00001", ",", "\n", "centered", "=", "True", ")", "# optimizer for EPG", "\n", "self", ".", "summary_writer", "=", "summary_writer", "\n", "self", ".", "summary_writing_frequency", "=", "summary_writing_frequency", "\n", "self", ".", "margin", "=", "margin", "\n", "self", ".", "start_training", "=", "1000", "# todo task specific", "\n", "self", ".", "highest_reward", "=", "6", "# todo task specific", "\n", "# which deviate the hing loss.", "\n", "self", ".", "isPrinted", "=", "False", "\n", "self", ".", "current_replay_size", "=", "0", "\n", "self", ".", "epsilon_current", "=", "1", "\n", "\n", "with", "tf", ".", "device", "(", "tf_device", ")", ":", "\n", "# Create a placeholder for the state input to the DQN network.", "\n", "# The last axis indicates the number of consecutive frames stacked.", "\n", "            ", "state_shape", "=", "(", "1", ",", ")", "+", "self", ".", "observation_shape", "+", "(", "stack_size", ",", ")", "\n", "self", ".", "state", "=", "np", ".", "zeros", "(", "state_shape", ")", "\n", "self", ".", "state_ph", "=", "tf", ".", "placeholder", "(", "self", ".", "observation_dtype", ",", "state_shape", ",", "\n", "name", "=", "'state_ph'", ")", "\n", "self", ".", "_replay", "=", "self", ".", "_build_replay_buffer", "(", "use_staging", ")", "\n", "\n", "self", ".", "_build_networks", "(", ")", "\n", "\n", "self", ".", "_train_op", ",", "self", ".", "_train_exp_op", "=", "self", ".", "_build_train_op", "(", ")", "\n", "\n", "", "self", ".", "replay_buffer", "=", "ReplayBufferRegular", "(", "100000", ")", "\n", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "# All tf.summaries should have been defined prior to running this.", "\n", "            ", "self", ".", "_merged_summaries", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "", "self", ".", "_sess", "=", "sess", "\n", "self", ".", "_saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "max_tf_checkpoints_to_keep", ")", "\n", "\n", "# Variables to be initialized by the agent once it interacts with the", "\n", "# environment.", "\n", "self", ".", "_observation", "=", "None", "\n", "self", ".", "_last_observation", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._get_network_type": [[205, 212], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the type of the outputs of a Q value network.\n\n        Returns:\n          net_type: _network_type object defining the outputs of the network.\n        \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "'DQN_network'", ",", "[", "'q_values'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._network_template": [[213, 231], ["tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "slim.fully_connected", "slim.fully_connected", "repg_agent.REPGAgent._get_network_type"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Builds the convolutional network used to compute the agent's Q-values.\n\n        Args:\n          state: `tf.Tensor`, contains the agent's current state.\n\n        Returns:\n          net: _network_type object containing the tensors output by the network.\n        \"\"\"", "\n", "net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "net", "=", "tf", ".", "div", "(", "net", ",", "255.", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "net", ",", "512", ")", "\n", "q_values", "=", "slim", ".", "fully_connected", "(", "net", ",", "self", ".", "num_actions", ",", "activation_fn", "=", "None", ")", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._build_networks": [[232, 264], ["tensorflow.make_template", "tensorflow.make_template", "repg_agent.REPGAgent.online_convnet", "repg_agent.REPGAgent.explore_convnet", "repg_agent.REPGAgent.online_convnet", "repg_agent.REPGAgent.explore_convnet", "tensorflow.nn.log_softmax", "tensorflow.distributions.Categorical().sample", "tensorflow.argmax", "tensorflow.distributions.Categorical"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.sum_tree.SumTree.sample"], ["", "def", "_build_networks", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds the Q-value network computations needed for acting and training.\n\n        These are:\n          self.online_convnet: For computing the current state's Q-values.\n          self.target_convnet: For computing the next state's target Q-values.\n          self._net_outputs: The actual Q-values.\n          self._q_argmax: The action maximizing the current state's Q-values.\n          self._replay_net_outputs: The replayed states' Q-values.\n          self._replay_next_target_net_outputs: The replayed next states' target\n            Q-values (see Mnih et al., 2015 for details).\n        \"\"\"", "\n", "# Calling online_convnet will generate a new graph as defined in", "\n", "# self._get_network_template using whatever input is passed, but will always", "\n", "# share the same weights.", "\n", "self", ".", "online_convnet", "=", "tf", ".", "make_template", "(", "'Online'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "explore_convnet", "=", "tf", ".", "make_template", "(", "'Explore'", ",", "self", ".", "_network_template", ")", "\n", "\n", "self", ".", "_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "state_ph", ")", "\n", "self", ".", "_exp_net_outputs", "=", "self", ".", "explore_convnet", "(", "self", ".", "state_ph", ")", "\n", "\n", "# TODO(bellemare): Ties should be broken. They are unlikely to happen when", "\n", "# using a deep network, but may affect performance with a linear", "\n", "# approximation scheme.", "\n", "self", ".", "_q_argmax", "=", "tf", ".", "argmax", "(", "self", ".", "_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "\n", "self", ".", "_replay_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "states", ")", "\n", "self", ".", "_replay_exp_net_outputs", "=", "self", ".", "explore_convnet", "(", "\n", "self", ".", "_replay", ".", "states", ")", "\n", "\n", "self", ".", "logsoftmaxprob", "=", "tf", ".", "nn", ".", "log_softmax", "(", "self", ".", "_exp_net_outputs", ".", "q_values", ")", "\n", "self", ".", "sample", "=", "Categorical", "(", "logits", "=", "self", ".", "logsoftmaxprob", ")", ".", "sample", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._build_replay_buffer": [[265, 282], ["dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer"], "methods", ["None"], ["", "def", "_build_replay_buffer", "(", "self", ",", "use_staging", ")", ":", "\n", "        ", "\"\"\"Creates the replay buffer used by the agent.\n\n        Args:\n          use_staging: bool, if True, uses a staging area to prefetch data for\n            faster training.\n\n        Returns:\n          A WrapperReplayBuffer object.\n        \"\"\"", "\n", "return", "circular_replay_buffer", ".", "WrappedReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "observation_dtype", "=", "self", ".", "observation_dtype", ".", "as_numpy_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._build_train_op": [[283, 329], ["tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.maximum", "tensorflow.reduce_mean", "tensorflow.nn.log_softmax", "tensorflow.reduce_mean", "tensorflow.reduce_sum", "repg_agent.REPGAgent.optimizer.minimize", "repg_agent.REPGAgent.optimizer_exp.minimize", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.reshape", "tensorflow.ones", "tensorflow.summary.scalar"], "methods", ["None"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds a training op.\n\n        Returns:\n          train_op: An op performing one step of training from replay data.\n        \"\"\"", "\n", "\n", "# for hinge loss", "\n", "margin", "=", "1", "\n", "\n", "replay_action_one_hot", "=", "tf", ".", "one_hot", "(", "\n", "self", ".", "_replay", ".", "actions", ",", "self", ".", "num_actions", ",", "1.", ",", "0.", ",", "name", "=", "'action_one_hot'", ")", "\n", "qvalue", "=", "self", ".", "_replay_net_outputs", ".", "q_values", "\n", "replay_chosen_q", "=", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "_replay_net_outputs", ".", "q_values", "*", "replay_action_one_hot", ",", "\n", "reduction_indices", "=", "1", ",", "\n", "name", "=", "'replay_chosen_q'", ")", "\n", "self", ".", "temp_action_one_hot", "=", "replay_action_one_hot", "\n", "self", ".", "temp_qvalue", "=", "qvalue", "\n", "\n", "# Q_j + c - Q_* = temp1 + temp2", "\n", "# temp1 = [Q_j + c, Q_*]", "\n", "# temp2 = [-Q_*, -Q_*]", "\n", "self", ".", "temp1", "=", "(", "qvalue", "+", "margin", ")", "*", "(", "1", "-", "replay_action_one_hot", ")", "+", "qvalue", "*", "replay_action_one_hot", "\n", "self", ".", "temp2", "=", "-", "(", "tf", ".", "reshape", "(", "replay_chosen_q", ",", "[", "-", "1", ",", "1", "]", ")", "*", "tf", ".", "ones", "(", "[", "1", ",", "self", ".", "num_actions", "]", ")", ")", "*", "(", "(", "1", "-", "replay_action_one_hot", ")", "+", "(", "replay_action_one_hot", ")", ")", "\n", "self", ".", "hingeloss", "=", "tf", ".", "maximum", "(", "0.0", ",", "self", ".", "temp1", "+", "self", ".", "temp2", ")", "\n", "loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "hingeloss", ")", "\n", "\n", "# for cross entropy loss", "\n", "logits", "=", "self", ".", "_replay_exp_net_outputs", ".", "q_values", "\n", "self", ".", "logsoftmaxprob", "=", "tf", ".", "nn", ".", "log_softmax", "(", "logits", ")", "\n", "self", ".", "neglogprob", "=", "-", "tf", ".", "reduce_sum", "(", "self", ".", "logsoftmaxprob", "*", "replay_action_one_hot", ",", "axis", "=", "1", ")", "\n", "# self.temp_loss = self.neglogprob # * self.y_pl", "\n", "self", ".", "actor_loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "neglogprob", ")", "\n", "self", ".", "replay_action_one_hot", "=", "replay_action_one_hot", "\n", "\n", "# target = tf.stop_gradient(self._build_target_q_op())", "\n", "# loss = tf.losses.huber_loss(", "\n", "#     target, replay_chosen_q, reduction=tf.losses.Reduction.NONE)", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "                ", "tf", ".", "summary", ".", "scalar", "(", "'hingeLoss'", ",", "loss", ")", "\n", "if", "self", ".", "exploration_strategy", "==", "\"EPG\"", ":", "\n", "                    ", "tf", ".", "summary", ".", "scalar", "(", "'actorloss'", ",", "self", ".", "actor_loss", ")", "\n", "", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "loss", ")", ",", "self", ".", "optimizer_exp", ".", "minimize", "(", "self", ".", "actor_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent.begin_episode": [[330, 347], ["repg_agent.REPGAgent._reset_state", "repg_agent.REPGAgent._record_observation", "repg_agent.REPGAgent._select_action", "repg_agent.REPGAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._reset_state", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step"], ["", "def", "begin_episode", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Returns the agent's first action for this episode.\n\n        Args:\n          observation: numpy array, the environment's initial observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_reset_state", "(", ")", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent.step": [[348, 374], ["repg_agent.REPGAgent._record_observation", "repg_agent.REPGAgent._select_action", "repg_agent.REPGAgent._train_step", "collect_trajectory", "ValueError"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step", "home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.collect_trajectory"], ["", "def", "step", "(", "self", ",", "reward", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records the most recent transition and returns the agent's next action.\n\n        We store the observation of the last time step since we want to store it\n        with the reward.\n\n        Args:\n          reward: float, the reward received from the agent's most recent action.\n          observation: numpy array, the most recent observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_last_observation", "=", "self", ".", "_observation", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "if", "self", ".", "game_name", "in", "[", "\"Pong\"", "]", ":", "\n", "                ", "collect_trajectory", "(", "self", ",", "reward", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"collection wrong trajectory\"", ")", "\n", "\n", "", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent.end_episode": [[375, 389], ["collect_trajectory", "ValueError"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.agents.agent_utils.collect_trajectory"], ["", "def", "end_episode", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Signals the end of the episode to the agent.\n\n        We store the observation of the current time step, which is the last\n        observation of the episode.\n\n        Args:\n          reward: float, the last reward from the environment.\n        \"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "if", "self", ".", "game_name", "in", "[", "\"Pong\"", "]", ":", "\n", "                ", "collect_trajectory", "(", "self", ",", "reward", ")", "\n", "", "else", ":", "\n", "                ", "raise", "ValueError", "(", "\"collection wrong trajectory\"", ")", "\n", "# if reward < 0:", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._select_action_training": [[401, 404], ["repg_agent.REPGAgent._sess.run"], "methods", ["None"], ["", "", "", "def", "_select_action_training", "(", "self", ")", ":", "\n", "        ", "\"\"\"Use EPG to select action during training, \"\"\"", "\n", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "sample", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "[", "0", "]", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._select_action": [[405, 425], ["repg_agent.REPGAgent._select_action_training", "repg_agent.REPGAgent._sess.run"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._select_action_training"], ["", "def", "_select_action", "(", "self", ")", ":", "\n", "        ", "\"\"\"Select an action from the set of available actions.\n\n        Chooses an action randomly with probability self._calculate_epsilon(), and\n        otherwise acts greedily according to the current Q-value estimates.\n\n        Returns:\n           int, the selected action.\n        \"\"\"", "\n", "# epsilon = self.epsilon_eval if self.eval_mode else self.epsilon_fn(", "\n", "#     self.epsilon_decay_period,", "\n", "#     self.training_steps,", "\n", "#     self.min_replay_history,", "\n", "#     self.epsilon_train)", "\n", "\n", "self", ".", "epsilon_current", "=", "0", "\n", "self", ".", "current_replay_size", "=", "self", ".", "_replay", ".", "memory", ".", "add_count", "\n", "if", "self", ".", "eval_mode", ":", "\n", "            ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "", "return", "self", ".", "_select_action_training", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._train_step": [[426, 465], ["print", "repg_agent.REPGAgent._sess.run", "repg_agent.REPGAgent._sess.run", "repg_agent.REPGAgent._sess.run", "repg_agent.REPGAgent.summary_writer.add_summary"], "methods", ["None"], ["", "def", "_train_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Runs a single training step.\n\n        Runs a training op if both:\n          (1) A minimum number of frames have been added to the replay buffer.\n          (2) `training_steps` is a multiple of `update_period`.\n\n        Also, syncs weights from online to target network if training steps is a\n        multiple of target update period.\n        \"\"\"", "\n", "# Run a train op at the rate of self.update_period if enough training steps", "\n", "# have been run. This matches the Nature DQN behaviour.", "\n", "\n", "if", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ":", "\n", "            ", "if", "self", ".", "training_steps", "%", "self", ".", "update_period", "==", "0", ":", "\n", "\n", "# # debug checked.", "\n", "# _, temp1, temp2, taction, tqvalue, hingloss = self._sess.run([self._train_op,", "\n", "#                                             self.temp1,", "\n", "#                                             self.temp2,", "\n", "#                                             self.temp_action_one_hot,", "\n", "#                                             self.temp_qvalue,", "\n", "#                                             self.hingeloss])", "\n", "                ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_op", ")", "\n", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_exp_op", ")", "\n", "if", "(", "self", ".", "summary_writer", "is", "not", "None", "and", "\n", "self", ".", "training_steps", ">", "0", "and", "\n", "self", ".", "training_steps", "%", "self", ".", "summary_writing_frequency", "==", "0", ")", ":", "\n", "                    ", "summary", "=", "self", ".", "_sess", ".", "run", "(", "self", ".", "_merged_summaries", ")", "\n", "self", ".", "summary_writer", ".", "add_summary", "(", "summary", ",", "self", ".", "training_steps", ")", "\n", "\n", "# if self.training_steps % self.target_update_period == 0:", "\n", "#     self._sess.run(self._sync_qt_ops)", "\n", "\n", "", "", "", "self", ".", "training_steps", "+=", "1", "\n", "\n", "if", "(", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ")", "and", "self", ".", "isPrinted", "is", "False", ":", "\n", "            ", "print", "(", "\"start training at {}\"", ".", "format", "(", "self", ".", "training_steps", ")", ")", "\n", "self", ".", "isPrinted", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._record_observation": [[466, 483], ["numpy.reshape", "numpy.reshape", "numpy.roll"], "methods", ["None"], ["", "", "def", "_record_observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records an observation and update state.\n\n        Extracts a frame from the observation vector and overwrites the oldest\n        frame in the state buffer.\n\n        Args:\n          observation: numpy array, an observation from the environment.\n        \"\"\"", "\n", "# Set current observation. We do the reshaping to handle environments", "\n", "# without frame stacking.", "\n", "observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "self", ".", "_observation", "=", "observation", "[", "...", ",", "0", "]", "\n", "self", ".", "_observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "# Swap out the oldest frame with the current frame.", "\n", "self", ".", "state", "=", "np", ".", "roll", "(", "self", ".", "state", ",", "-", "1", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "state", "[", "0", ",", "...", ",", "-", "1", "]", "=", "self", ".", "_observation", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._store_transition": [[484, 501], ["repg_agent.REPGAgent._replay.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "def", "_store_transition", "(", "self", ",", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", ":", "\n", "        ", "\"\"\"Stores an experienced transition.\n\n        Executes a tf session and executes replay buffer ops in order to store the\n        following tuple in the replay buffer:\n          (last_observation, action, reward, is_terminal).\n\n        Pedantically speaking, this does not actually store an entire transition\n        since the next state is recorded on the following time step.\n\n        Args:\n          last_observation: numpy array, last observation.\n          action: int, the action taken.\n          reward: float, the reward.\n          is_terminal: bool, indicating if the current state is a terminal state.\n        \"\"\"", "\n", "self", ".", "_replay", ".", "add", "(", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent._reset_state": [[502, 505], ["repg_agent.REPGAgent.state.fill"], "methods", ["None"], ["", "def", "_reset_state", "(", "self", ")", ":", "\n", "        ", "\"\"\"Resets the agent state by filling it with zeros.\"\"\"", "\n", "self", ".", "state", ".", "fill", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent.bundle_and_checkpoint": [[506, 536], ["repg_agent.REPGAgent._saver.save", "repg_agent.REPGAgent._replay.save", "tensorflow.gfile.Exists", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save"], ["", "def", "bundle_and_checkpoint", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ")", ":", "\n", "        ", "\"\"\"Returns a self-contained bundle of the agent's state.\n\n        This is used for checkpointing. It will return a dictionary containing all\n        non-TensorFlow objects (to be saved into a file by the caller), and it saves\n        all TensorFlow objects into a checkpoint file.\n\n        Args:\n          checkpoint_dir: str, directory where TensorFlow objects will be saved.\n          iteration_number: int, iteration number to use for naming the checkpoint\n            file.\n\n        Returns:\n          A dict containing additional Python objects to be checkpointed by the\n            experiment. If the checkpoint directory does not exist, returns None.\n        \"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "checkpoint_dir", ")", ":", "\n", "            ", "return", "None", "\n", "# Call the Tensorflow saver to checkpoint the graph.", "\n", "", "self", ".", "_saver", ".", "save", "(", "\n", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'tf_ckpt'", ")", ",", "\n", "global_step", "=", "iteration_number", ")", "\n", "# Checkpoint the out-of-graph replay buffer.", "\n", "self", ".", "_replay", ".", "save", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "bundle_dictionary", "=", "{", "}", "\n", "bundle_dictionary", "[", "'state'", "]", "=", "self", ".", "state", "\n", "bundle_dictionary", "[", "'eval_mode'", "]", "=", "self", ".", "eval_mode", "\n", "bundle_dictionary", "[", "'training_steps'", "]", "=", "self", ".", "training_steps", "\n", "return", "bundle_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.REPGAgent.unbundle": [[537, 569], ["repg_agent.REPGAgent._saver.restore", "repg_agent.REPGAgent._replay.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load"], ["", "def", "unbundle", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ",", "bundle_dictionary", ")", ":", "\n", "        ", "\"\"\"Restores the agent from a checkpoint.\n\n        Restores the agent's Python objects to those specified in bundle_dictionary,\n        and restores the TensorFlow objects to those specified in the\n        checkpoint_dir. If the checkpoint_dir does not exist, will not reset the\n          agent's state.\n\n        Args:\n          checkpoint_dir: str, path to the checkpoint saved by tf.Save.\n          iteration_number: int, checkpoint version, used when restoring replay\n            buffer.\n          bundle_dictionary: dict, containing additional Python objects owned by\n            the agent.\n\n        Returns:\n          bool, True if unbundling was successful.\n        \"\"\"", "\n", "try", ":", "\n", "# self._replay.load() will throw a NotFoundError if it does not find all", "\n", "# the necessary files, in which case we abort the process & return False.", "\n", "            ", "self", ".", "_replay", ".", "load", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "            ", "return", "False", "\n", "", "for", "key", "in", "self", ".", "__dict__", ":", "\n", "            ", "if", "key", "in", "bundle_dictionary", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "bundle_dictionary", "[", "key", "]", "\n", "# Restore the agent's TensorFlow graph.", "\n", "", "", "self", ".", "_saver", ".", "restore", "(", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "\n", "'tf_ckpt-{}'", ".", "format", "(", "iteration_number", ")", ")", ")", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.__init__": [[576, 583], ["collections.deque", "random.seed"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "buffer_size", ",", "random_seed", "=", "1234", ")", ":", "\n", "        ", "self", ".", "buffer_size", "=", "buffer_size", "\n", "self", ".", "count", "=", "0", "\n", "# Right side of deque contains newest experience", "\n", "self", ".", "buffer", "=", "deque", "(", ")", "\n", "random", ".", "seed", "(", "random_seed", ")", "\n", "self", ".", "ptr", ",", "self", ".", "path_start_idx", "=", "0", ",", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add": [[584, 590], ["repg_agent.ReplayBufferRegular.buffer.append"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "add", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "terminal", ")", ":", "\n", "        ", "experience", "=", "[", "state", ",", "action", ",", "reward", ",", "terminal", "]", "\n", "assert", "self", ".", "count", "<", "self", ".", "buffer_size", "\n", "self", ".", "buffer", ".", "append", "(", "experience", ")", "\n", "self", ".", "count", "+=", "1", "\n", "self", ".", "ptr", "+=", "1", "\n", "# else:", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.get_sample": [[596, 599], ["repg_agent.ReplayBufferRegular.buffer.popleft"], "methods", ["None"], ["", "def", "get_sample", "(", "self", ")", ":", "\n", "        ", "self", ".", "count", "-=", "1", "\n", "return", "self", ".", "buffer", ".", "popleft", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.size": [[600, 602], ["None"], "methods", ["None"], ["", "def", "size", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.clear": [[603, 608], ["repg_agent.ReplayBufferRegular.buffer.clear"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.clear"], ["", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "buffer", ".", "clear", "(", ")", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "ptr", "=", "0", "\n", "self", ".", "path_start_idx", "=", "0", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.linearly_decaying_epsilon": [[41, 63], ["numpy.clip"], "function", ["None"], ["def", "linearly_decaying_epsilon", "(", "decay_period", ",", "step", ",", "warmup_steps", ",", "epsilon", ")", ":", "\n", "    ", "\"\"\"Returns the current epsilon for the agent's epsilon-greedy policy.\n\n    This follows the Nature DQN schedule of a linearly decaying epsilon (Mnih et\n    al., 2015). The schedule is as follows:\n      Begin at 1. until warmup_steps steps have been taken; then\n      Linearly decay epsilon from 1. to epsilon in decay_period steps; and then\n      Use epsilon from there on.\n\n    Args:\n      decay_period: float, the period over which epsilon is decayed.\n      step: int, the number of training steps completed so far.\n      warmup_steps: int, the number of steps taken before epsilon is decayed.\n      epsilon: float, the final value to which to decay the epsilon parameter.\n\n    Returns:\n      A float, the current epsilon value computed according to the schedule.\n    \"\"\"", "\n", "steps_left", "=", "decay_period", "+", "warmup_steps", "-", "step", "\n", "bonus", "=", "(", "1.0", "-", "epsilon", ")", "*", "steps_left", "/", "decay_period", "\n", "bonus", "=", "np", ".", "clip", "(", "bonus", ",", "0.", ",", "1.", "-", "epsilon", ")", "\n", "return", "epsilon", "+", "bonus", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent.__init__": [[70, 189], ["tensorflow.train.RMSPropOptimizer", "isinstance", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tuple", "math.pow", "tensorflow.train.Saver", "tensorflow.device", "numpy.zeros", "tensorflow.placeholder", "dqn_agent.DQNAgent._build_replay_buffer", "dqn_agent.DQNAgent._build_networks", "dqn_agent.DQNAgent._build_train_op", "dqn_agent.DQNAgent._build_sync_op", "tensorflow.summary.merge_all"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_replay_buffer", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_networks", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_train_op", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_sync_op"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "observation_shape", "=", "NATURE_DQN_OBSERVATION_SHAPE", ",", "\n", "observation_dtype", "=", "NATURE_DQN_DTYPE", ",", "\n", "stack_size", "=", "NATURE_DQN_STACK_SIZE", ",", "\n", "gamma", "=", "0.99", ",", "\n", "update_horizon", "=", "1", ",", "\n", "min_replay_history", "=", "20000", ",", "\n", "update_period", "=", "4", ",", "\n", "target_update_period", "=", "8000", ",", "\n", "epsilon_fn", "=", "linearly_decaying_epsilon", ",", "\n", "epsilon_train", "=", "0.01", ",", "\n", "epsilon_eval", "=", "0.001", ",", "\n", "epsilon_decay_period", "=", "250000", ",", "\n", "tf_device", "=", "'/cpu:*'", ",", "\n", "use_staging", "=", "True", ",", "\n", "max_tf_checkpoints_to_keep", "=", "3", ",", "\n", "optimizer", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "\n", "decay", "=", "0.95", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "0.00001", ",", "\n", "centered", "=", "True", ")", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "    ", "\"\"\"Initializes the agent and constructs the components of its graph.\n\n    Args:\n      sess: `tf.Session`, for executing ops.\n      num_actions: int, number of actions the agent can take at any state.\n      observation_shape: tuple of ints describing the observation shape.\n      observation_dtype: tf.DType, specifies the type of the observations. Note\n        that if your inputs are continuous, you should set this to tf.float32.\n      stack_size: int, number of frames to use in state stack.\n      gamma: float, discount factor with the usual RL meaning.\n      update_horizon: int, horizon at which updates are performed, the 'n' in\n        n-step update.\n      min_replay_history: int, number of transitions that should be experienced\n        before the agent begins training its value function.\n      update_period: int, period between DQN updates.\n      target_update_period: int, update period for the target network.\n      epsilon_fn: function expecting 4 parameters:\n        (decay_period, step, warmup_steps, epsilon). This function should return\n        the epsilon value used for exploration during training.\n      epsilon_train: float, the value to which the agent's epsilon is eventually\n        decayed during training.\n      epsilon_eval: float, epsilon used when evaluating the agent.\n      epsilon_decay_period: int, length of the epsilon decay schedule.\n      tf_device: str, Tensorflow device on which the agent's graph is executed.\n      use_staging: bool, when True use a staging area to prefetch the next\n        training batch, speeding training up by about 30%.\n      max_tf_checkpoints_to_keep: int, the number of TensorFlow checkpoints to\n        keep.\n      optimizer: `tf.train.Optimizer`, for training the value function.\n      summary_writer: SummaryWriter object for outputting training statistics.\n        Summary writing disabled if set to None.\n      summary_writing_frequency: int, frequency with which summaries will be\n        written. Lower values will result in slower training.\n    \"\"\"", "\n", "assert", "isinstance", "(", "observation_shape", ",", "tuple", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Creating %s agent with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t gamma: %f'", ",", "gamma", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_horizon: %f'", ",", "update_horizon", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t min_replay_history: %d'", ",", "min_replay_history", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_period: %d'", ",", "update_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t target_update_period: %d'", ",", "target_update_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_train: %f'", ",", "epsilon_train", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_eval: %f'", ",", "epsilon_eval", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_decay_period: %d'", ",", "epsilon_decay_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t tf_device: %s'", ",", "tf_device", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t use_staging: %s'", ",", "use_staging", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t optimizer: %s'", ",", "optimizer", ")", "\n", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "self", ".", "observation_shape", "=", "tuple", "(", "observation_shape", ")", "\n", "self", ".", "observation_dtype", "=", "observation_dtype", "\n", "self", ".", "stack_size", "=", "stack_size", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "update_horizon", "=", "update_horizon", "\n", "self", ".", "cumulative_gamma", "=", "math", ".", "pow", "(", "gamma", ",", "update_horizon", ")", "\n", "self", ".", "min_replay_history", "=", "min_replay_history", "\n", "self", ".", "target_update_period", "=", "target_update_period", "\n", "self", ".", "epsilon_fn", "=", "epsilon_fn", "\n", "self", ".", "epsilon_train", "=", "epsilon_train", "\n", "self", ".", "epsilon_eval", "=", "epsilon_eval", "\n", "self", ".", "epsilon_decay_period", "=", "epsilon_decay_period", "\n", "self", ".", "update_period", "=", "update_period", "\n", "self", ".", "eval_mode", "=", "False", "\n", "self", ".", "training_steps", "=", "0", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "summary_writer", "=", "summary_writer", "\n", "self", ".", "summary_writing_frequency", "=", "summary_writing_frequency", "\n", "\n", "with", "tf", ".", "device", "(", "tf_device", ")", ":", "\n", "# Create a placeholder for the state input to the DQN network.", "\n", "# The last axis indicates the number of consecutive frames stacked.", "\n", "      ", "state_shape", "=", "(", "1", ",", ")", "+", "self", ".", "observation_shape", "+", "(", "stack_size", ",", ")", "\n", "self", ".", "state", "=", "np", ".", "zeros", "(", "state_shape", ")", "\n", "self", ".", "state_ph", "=", "tf", ".", "placeholder", "(", "self", ".", "observation_dtype", ",", "state_shape", ",", "\n", "name", "=", "'state_ph'", ")", "\n", "self", ".", "_replay", "=", "self", ".", "_build_replay_buffer", "(", "use_staging", ")", "\n", "\n", "self", ".", "_build_networks", "(", ")", "\n", "\n", "self", ".", "_train_op", "=", "self", ".", "_build_train_op", "(", ")", "\n", "self", ".", "_sync_qt_ops", "=", "self", ".", "_build_sync_op", "(", ")", "\n", "\n", "", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "# All tf.summaries should have been defined prior to running this.", "\n", "      ", "self", ".", "_merged_summaries", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "", "self", ".", "_sess", "=", "sess", "\n", "self", ".", "_saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "max_tf_checkpoints_to_keep", ")", "\n", "\n", "# Variables to be initialized by the agent once it interacts with the", "\n", "# environment.", "\n", "self", ".", "_observation", "=", "None", "\n", "self", ".", "_last_observation", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._get_network_type": [[190, 197], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns the type of the outputs of a Q value network.\n\n    Returns:\n      net_type: _network_type object defining the outputs of the network.\n    \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "'DQN_network'", ",", "[", "'q_values'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._network_template": [[198, 216], ["tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "slim.fully_connected", "slim.fully_connected", "dqn_agent.DQNAgent._get_network_type"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ")", ":", "\n", "    ", "\"\"\"Builds the convolutional network used to compute the agent's Q-values.\n\n    Args:\n      state: `tf.Tensor`, contains the agent's current state.\n\n    Returns:\n      net: _network_type object containing the tensors output by the network.\n    \"\"\"", "\n", "net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "net", "=", "tf", ".", "div", "(", "net", ",", "255.", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "net", ",", "512", ")", "\n", "q_values", "=", "slim", ".", "fully_connected", "(", "net", ",", "self", ".", "num_actions", ",", "activation_fn", "=", "None", ")", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._build_networks": [[217, 243], ["tensorflow.make_template", "tensorflow.make_template", "dqn_agent.DQNAgent.online_convnet", "dqn_agent.DQNAgent.online_convnet", "dqn_agent.DQNAgent.target_convnet", "tensorflow.argmax"], "methods", ["None"], ["", "def", "_build_networks", "(", "self", ")", ":", "\n", "    ", "\"\"\"Builds the Q-value network computations needed for acting and training.\n\n    These are:\n      self.online_convnet: For computing the current state's Q-values.\n      self.target_convnet: For computing the next state's target Q-values.\n      self._net_outputs: The actual Q-values.\n      self._q_argmax: The action maximizing the current state's Q-values.\n      self._replay_net_outputs: The replayed states' Q-values.\n      self._replay_next_target_net_outputs: The replayed next states' target\n        Q-values (see Mnih et al., 2015 for details).\n    \"\"\"", "\n", "# Calling online_convnet will generate a new graph as defined in", "\n", "# self._get_network_template using whatever input is passed, but will always", "\n", "# share the same weights.", "\n", "self", ".", "online_convnet", "=", "tf", ".", "make_template", "(", "'Online'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "target_convnet", "=", "tf", ".", "make_template", "(", "'Target'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "state_ph", ")", "\n", "# TODO(bellemare): Ties should be broken. They are unlikely to happen when", "\n", "# using a deep network, but may affect performance with a linear", "\n", "# approximation scheme.", "\n", "self", ".", "_q_argmax", "=", "tf", ".", "argmax", "(", "self", ".", "_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "\n", "self", ".", "_replay_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "states", ")", "\n", "self", ".", "_replay_next_target_net_outputs", "=", "self", ".", "target_convnet", "(", "\n", "self", ".", "_replay", ".", "next_states", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._build_replay_buffer": [[244, 261], ["dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer"], "methods", ["None"], ["", "def", "_build_replay_buffer", "(", "self", ",", "use_staging", ")", ":", "\n", "    ", "\"\"\"Creates the replay buffer used by the agent.\n\n    Args:\n      use_staging: bool, if True, uses a staging area to prefetch data for\n        faster training.\n\n    Returns:\n      A WrapperReplayBuffer object.\n    \"\"\"", "\n", "return", "circular_replay_buffer", ".", "WrappedReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "observation_dtype", "=", "self", ".", "observation_dtype", ".", "as_numpy_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._build_target_q_op": [[262, 280], ["tensorflow.reduce_max", "tensorflow.cast"], "methods", ["None"], ["", "def", "_build_target_q_op", "(", "self", ")", ":", "\n", "    ", "\"\"\"Build an op used as a target for the Q-value.\n\n    Returns:\n      target_q_op: An op calculating the Q-value.\n    \"\"\"", "\n", "# Get the maximum Q-value across the actions dimension.", "\n", "replay_next_qt_max", "=", "tf", ".", "reduce_max", "(", "\n", "self", ".", "_replay_next_target_net_outputs", ".", "q_values", ",", "1", ")", "\n", "# Calculate the Bellman target value.", "\n", "#   Q_t = R_t + \\gamma^N * Q'_t+1", "\n", "# where,", "\n", "#   Q'_t+1 = \\argmax_a Q(S_t+1, a)", "\n", "#          (or) 0 if S_t is a terminal state,", "\n", "# and", "\n", "#   N is the update horizon (by default, N=1).", "\n", "return", "self", ".", "_replay", ".", "rewards", "+", "self", ".", "cumulative_gamma", "*", "replay_next_qt_max", "*", "(", "\n", "1.", "-", "tf", ".", "cast", "(", "self", ".", "_replay", ".", "terminals", ",", "tf", ".", "float32", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._build_train_op": [[281, 301], ["tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.stop_gradient", "tensorflow.losses.huber_loss", "dqn_agent.DQNAgent.optimizer.minimize", "dqn_agent.DQNAgent._build_target_q_op", "tensorflow.reduce_mean", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.reduce_mean"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_target_q_op"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "    ", "\"\"\"Builds a training op.\n\n    Returns:\n      train_op: An op performing one step of training from replay data.\n    \"\"\"", "\n", "replay_action_one_hot", "=", "tf", ".", "one_hot", "(", "\n", "self", ".", "_replay", ".", "actions", ",", "self", ".", "num_actions", ",", "1.", ",", "0.", ",", "name", "=", "'action_one_hot'", ")", "\n", "replay_chosen_q", "=", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "_replay_net_outputs", ".", "q_values", "*", "replay_action_one_hot", ",", "\n", "reduction_indices", "=", "1", ",", "\n", "name", "=", "'replay_chosen_q'", ")", "\n", "\n", "target", "=", "tf", ".", "stop_gradient", "(", "self", ".", "_build_target_q_op", "(", ")", ")", "\n", "loss", "=", "tf", ".", "losses", ".", "huber_loss", "(", "\n", "target", ",", "replay_chosen_q", ",", "reduction", "=", "tf", ".", "losses", ".", "Reduction", ".", "NONE", ")", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "      ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "        ", "tf", ".", "summary", ".", "scalar", "(", "'HuberLoss'", ",", "tf", ".", "reduce_mean", "(", "loss", ")", ")", "\n", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "tf", ".", "reduce_mean", "(", "loss", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._build_sync_op": [[302, 318], ["tensorflow.get_collection", "tensorflow.get_collection", "zip", "sync_qt_ops.append", "w_target.assign"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "_build_sync_op", "(", "self", ")", ":", "\n", "    ", "\"\"\"Builds ops for assigning weights from online to target network.\n\n    Returns:\n      ops: A list of ops assigning weights from online to target network.\n    \"\"\"", "\n", "# Get trainable variables from online and target DQNs", "\n", "sync_qt_ops", "=", "[", "]", "\n", "trainables_online", "=", "tf", ".", "get_collection", "(", "\n", "tf", ".", "GraphKeys", ".", "TRAINABLE_VARIABLES", ",", "scope", "=", "'Online'", ")", "\n", "trainables_target", "=", "tf", ".", "get_collection", "(", "\n", "tf", ".", "GraphKeys", ".", "TRAINABLE_VARIABLES", ",", "scope", "=", "'Target'", ")", "\n", "for", "(", "w_online", ",", "w_target", ")", "in", "zip", "(", "trainables_online", ",", "trainables_target", ")", ":", "\n", "# Assign weights from online to target network.", "\n", "      ", "sync_qt_ops", ".", "append", "(", "w_target", ".", "assign", "(", "w_online", ",", "use_locking", "=", "True", ")", ")", "\n", "", "return", "sync_qt_ops", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent.begin_episode": [[319, 336], ["dqn_agent.DQNAgent._reset_state", "dqn_agent.DQNAgent._record_observation", "dqn_agent.DQNAgent._select_action", "dqn_agent.DQNAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._reset_state", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step"], ["", "def", "begin_episode", "(", "self", ",", "observation", ")", ":", "\n", "    ", "\"\"\"Returns the agent's first action for this episode.\n\n    Args:\n      observation: numpy array, the environment's initial observation.\n\n    Returns:\n      int, the selected action.\n    \"\"\"", "\n", "self", ".", "_reset_state", "(", ")", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "      ", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent.step": [[337, 359], ["dqn_agent.DQNAgent._record_observation", "dqn_agent.DQNAgent._select_action", "dqn_agent.DQNAgent._store_transition", "dqn_agent.DQNAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step"], ["", "def", "step", "(", "self", ",", "reward", ",", "observation", ")", ":", "\n", "    ", "\"\"\"Records the most recent transition and returns the agent's next action.\n\n    We store the observation of the last time step since we want to store it\n    with the reward.\n\n    Args:\n      reward: float, the reward received from the agent's most recent action.\n      observation: numpy array, the most recent observation.\n\n    Returns:\n      int, the selected action.\n    \"\"\"", "\n", "self", ".", "_last_observation", "=", "self", ".", "_observation", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "      ", "self", ".", "_store_transition", "(", "self", ".", "_last_observation", ",", "self", ".", "action", ",", "reward", ",", "False", ")", "\n", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent.end_episode": [[360, 371], ["dqn_agent.DQNAgent._store_transition"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition"], ["", "def", "end_episode", "(", "self", ",", "reward", ")", ":", "\n", "    ", "\"\"\"Signals the end of the episode to the agent.\n\n    We store the observation of the current time step, which is the last\n    observation of the episode.\n\n    Args:\n      reward: float, the last reward from the environment.\n    \"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "      ", "self", ".", "_store_transition", "(", "self", ".", "_observation", ",", "self", ".", "action", ",", "reward", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._select_action": [[372, 392], ["dqn_agent.DQNAgent.epsilon_fn", "random.random", "random.randint", "dqn_agent.DQNAgent._sess.run"], "methods", ["None"], ["", "", "def", "_select_action", "(", "self", ")", ":", "\n", "    ", "\"\"\"Select an action from the set of available actions.\n\n    Chooses an action randomly with probability self._calculate_epsilon(), and\n    otherwise acts greedily according to the current Q-value estimates.\n\n    Returns:\n       int, the selected action.\n    \"\"\"", "\n", "epsilon", "=", "self", ".", "epsilon_eval", "if", "self", ".", "eval_mode", "else", "self", ".", "epsilon_fn", "(", "\n", "self", ".", "epsilon_decay_period", ",", "\n", "self", ".", "training_steps", ",", "\n", "self", ".", "min_replay_history", ",", "\n", "self", ".", "epsilon_train", ")", "\n", "if", "random", ".", "random", "(", ")", "<=", "epsilon", ":", "\n", "# Choose a random action with probability epsilon.", "\n", "      ", "return", "random", ".", "randint", "(", "0", ",", "self", ".", "num_actions", "-", "1", ")", "\n", "", "else", ":", "\n", "# Choose the action with highest Q-value at the current state.", "\n", "      ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._train_step": [[393, 418], ["dqn_agent.DQNAgent._sess.run", "dqn_agent.DQNAgent._sess.run", "dqn_agent.DQNAgent._sess.run", "dqn_agent.DQNAgent.summary_writer.add_summary"], "methods", ["None"], ["", "", "def", "_train_step", "(", "self", ")", ":", "\n", "    ", "\"\"\"Runs a single training step.\n\n    Runs a training op if both:\n      (1) A minimum number of frames have been added to the replay buffer.\n      (2) `training_steps` is a multiple of `update_period`.\n\n    Also, syncs weights from online to target network if training steps is a\n    multiple of target update period.\n    \"\"\"", "\n", "# Run a train op at the rate of self.update_period if enough training steps", "\n", "# have been run. This matches the Nature DQN behaviour.", "\n", "if", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "min_replay_history", ":", "\n", "      ", "if", "self", ".", "training_steps", "%", "self", ".", "update_period", "==", "0", ":", "\n", "        ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_op", ")", "\n", "if", "(", "self", ".", "summary_writer", "is", "not", "None", "and", "\n", "self", ".", "training_steps", ">", "0", "and", "\n", "self", ".", "training_steps", "%", "self", ".", "summary_writing_frequency", "==", "0", ")", ":", "\n", "          ", "summary", "=", "self", ".", "_sess", ".", "run", "(", "self", ".", "_merged_summaries", ")", "\n", "self", ".", "summary_writer", ".", "add_summary", "(", "summary", ",", "self", ".", "training_steps", ")", "\n", "\n", "", "", "if", "self", ".", "training_steps", "%", "self", ".", "target_update_period", "==", "0", ":", "\n", "        ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_sync_qt_ops", ")", "\n", "\n", "", "", "self", ".", "training_steps", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._record_observation": [[419, 436], ["numpy.reshape", "numpy.reshape", "numpy.roll"], "methods", ["None"], ["", "def", "_record_observation", "(", "self", ",", "observation", ")", ":", "\n", "    ", "\"\"\"Records an observation and update state.\n\n    Extracts a frame from the observation vector and overwrites the oldest\n    frame in the state buffer.\n\n    Args:\n      observation: numpy array, an observation from the environment.\n    \"\"\"", "\n", "# Set current observation. We do the reshaping to handle environments", "\n", "# without frame stacking.", "\n", "observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "self", ".", "_observation", "=", "observation", "[", "...", ",", "0", "]", "\n", "self", ".", "_observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "# Swap out the oldest frame with the current frame.", "\n", "self", ".", "state", "=", "np", ".", "roll", "(", "self", ".", "state", ",", "-", "1", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "state", "[", "0", ",", "...", ",", "-", "1", "]", "=", "self", ".", "_observation", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._store_transition": [[437, 454], ["dqn_agent.DQNAgent._replay.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "def", "_store_transition", "(", "self", ",", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", ":", "\n", "    ", "\"\"\"Stores an experienced transition.\n\n    Executes a tf session and executes replay buffer ops in order to store the\n    following tuple in the replay buffer:\n      (last_observation, action, reward, is_terminal).\n\n    Pedantically speaking, this does not actually store an entire transition\n    since the next state is recorded on the following time step.\n\n    Args:\n      last_observation: numpy array, last observation.\n      action: int, the action taken.\n      reward: float, the reward.\n      is_terminal: bool, indicating if the current state is a terminal state.\n    \"\"\"", "\n", "self", ".", "_replay", ".", "add", "(", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent._reset_state": [[455, 458], ["dqn_agent.DQNAgent.state.fill"], "methods", ["None"], ["", "def", "_reset_state", "(", "self", ")", ":", "\n", "    ", "\"\"\"Resets the agent state by filling it with zeros.\"\"\"", "\n", "self", ".", "state", ".", "fill", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent.bundle_and_checkpoint": [[459, 489], ["dqn_agent.DQNAgent._saver.save", "dqn_agent.DQNAgent._replay.save", "tensorflow.gfile.Exists", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save"], ["", "def", "bundle_and_checkpoint", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ")", ":", "\n", "    ", "\"\"\"Returns a self-contained bundle of the agent's state.\n\n    This is used for checkpointing. It will return a dictionary containing all\n    non-TensorFlow objects (to be saved into a file by the caller), and it saves\n    all TensorFlow objects into a checkpoint file.\n\n    Args:\n      checkpoint_dir: str, directory where TensorFlow objects will be saved.\n      iteration_number: int, iteration number to use for naming the checkpoint\n        file.\n\n    Returns:\n      A dict containing additional Python objects to be checkpointed by the\n        experiment. If the checkpoint directory does not exist, returns None.\n    \"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "checkpoint_dir", ")", ":", "\n", "      ", "return", "None", "\n", "# Call the Tensorflow saver to checkpoint the graph.", "\n", "", "self", ".", "_saver", ".", "save", "(", "\n", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'tf_ckpt'", ")", ",", "\n", "global_step", "=", "iteration_number", ")", "\n", "# Checkpoint the out-of-graph replay buffer.", "\n", "self", ".", "_replay", ".", "save", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "bundle_dictionary", "=", "{", "}", "\n", "bundle_dictionary", "[", "'state'", "]", "=", "self", ".", "state", "\n", "bundle_dictionary", "[", "'eval_mode'", "]", "=", "self", ".", "eval_mode", "\n", "bundle_dictionary", "[", "'training_steps'", "]", "=", "self", ".", "training_steps", "\n", "return", "bundle_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.DQNAgent.unbundle": [[490, 522], ["dqn_agent.DQNAgent._saver.restore", "dqn_agent.DQNAgent._replay.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load"], ["", "def", "unbundle", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ",", "bundle_dictionary", ")", ":", "\n", "    ", "\"\"\"Restores the agent from a checkpoint.\n\n    Restores the agent's Python objects to those specified in bundle_dictionary,\n    and restores the TensorFlow objects to those specified in the\n    checkpoint_dir. If the checkpoint_dir does not exist, will not reset the\n      agent's state.\n\n    Args:\n      checkpoint_dir: str, path to the checkpoint saved by tf.Save.\n      iteration_number: int, checkpoint version, used when restoring replay\n        buffer.\n      bundle_dictionary: dict, containing additional Python objects owned by\n        the agent.\n\n    Returns:\n      bool, True if unbundling was successful.\n    \"\"\"", "\n", "try", ":", "\n", "# self._replay.load() will throw a NotFoundError if it does not find all", "\n", "# the necessary files, in which case we abort the process & return False.", "\n", "      ", "self", ".", "_replay", ".", "load", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "      ", "return", "False", "\n", "", "for", "key", "in", "self", ".", "__dict__", ":", "\n", "      ", "if", "key", "in", "bundle_dictionary", ":", "\n", "        ", "self", ".", "__dict__", "[", "key", "]", "=", "bundle_dictionary", "[", "key", "]", "\n", "# Restore the agent's TensorFlow graph.", "\n", "", "", "self", ".", "_saver", ".", "restore", "(", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "\n", "'tf_ckpt-{}'", ".", "format", "(", "iteration_number", ")", ")", ")", "\n", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqn.dqn_agent.linearly_decaying_epsilon": [[42, 64], ["numpy.clip"], "function", ["None"], ["def", "linearly_decaying_epsilon", "(", "decay_period", ",", "step", ",", "warmup_steps", ",", "epsilon", ")", ":", "\n", "  ", "\"\"\"Returns the current epsilon for the agent's epsilon-greedy policy.\n\n  This follows the Nature DQN schedule of a linearly decaying epsilon (Mnih et\n  al., 2015). The schedule is as follows:\n    Begin at 1. until warmup_steps steps have been taken; then\n    Linearly decay epsilon from 1. to epsilon in decay_period steps; and then\n    Use epsilon from there on.\n\n  Args:\n    decay_period: float, the period over which epsilon is decayed.\n    step: int, the number of training steps completed so far.\n    warmup_steps: int, the number of steps taken before epsilon is decayed.\n    epsilon: float, the final value to which to decay the epsilon parameter.\n\n  Returns:\n    A float, the current epsilon value computed according to the schedule.\n  \"\"\"", "\n", "steps_left", "=", "decay_period", "+", "warmup_steps", "-", "step", "\n", "bonus", "=", "(", "1.0", "-", "epsilon", ")", "*", "steps_left", "/", "decay_period", "\n", "bonus", "=", "np", ".", "clip", "(", "bonus", ",", "0.", ",", "1.", "-", "epsilon", ")", "\n", "return", "epsilon", "+", "bonus", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent.__init__": [[58, 162], ["tensorflow.train.AdamOptimizer", "float", "tensorflow.linspace", "tensorflow.train.RMSPropOptimizer", "dopamine.agents.dqn.dqn_agent.DQNAgent.__init__", "ReplayBufferRegular", "tensorflow.device", "rainbowrpg_agent.RainbowRPGAgent._build_replay_buffer_opt", "rainbowrpg_agent.RainbowRPGAgent._build_networks_rpg", "rainbowrpg_agent.RainbowRPGAgent._build_train_op_rpg"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.__init__", "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_replay_buffer_opt", "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_networks_rpg", "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_train_op_rpg"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "observation_shape", "=", "dqn_agent", ".", "NATURE_DQN_OBSERVATION_SHAPE", ",", "\n", "observation_dtype", "=", "dqn_agent", ".", "NATURE_DQN_DTYPE", ",", "\n", "stack_size", "=", "dqn_agent", ".", "NATURE_DQN_STACK_SIZE", ",", "\n", "num_atoms", "=", "51", ",", "\n", "vmax", "=", "10.", ",", "\n", "gamma", "=", "0.99", ",", "\n", "update_horizon", "=", "1", ",", "\n", "min_replay_history", "=", "20000", ",", "\n", "update_period", "=", "4", ",", "\n", "target_update_period", "=", "8000", ",", "\n", "epsilon_fn", "=", "dqn_agent", ".", "linearly_decaying_epsilon", ",", "\n", "epsilon_train", "=", "0.01", ",", "\n", "epsilon_eval", "=", "0.001", ",", "\n", "epsilon_decay_period", "=", "250000", ",", "\n", "replay_scheme", "=", "'prioritized'", ",", "\n", "tf_device", "=", "'/gpu:*'", ",", "\n", "use_staging", "=", "True", ",", "\n", "optimizer", "=", "tf", ".", "train", ".", "AdamOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "epsilon", "=", "0.0003125", ")", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "        ", "\"\"\"Initializes the agent and constructs the components of its graph.\n\n        Args:\n          sess: `tf.Session`, for executing ops.\n          num_actions: int, number of actions the agent can take at any state.\n          observation_shape: tuple of ints or an int. If single int, the observation\n            is assumed to be a 2D square.\n          observation_dtype: tf.DType, specifies the type of the observations. Note\n            that if your inputs are continuous, you should set this to tf.float32.\n          stack_size: int, number of frames to use in state stack.\n          num_atoms: int, the number of buckets of the value function distribution.\n          vmax: float, the value distribution support is [-vmax, vmax].\n          gamma: float, discount factor with the usual RL meaning.\n          update_horizon: int, horizon at which updates are performed, the 'n' in\n            n-step update.\n          min_replay_history: int, number of transitions that should be experienced\n            before the agent begins training its value function.\n          update_period: int, period between DQN updates.\n          target_update_period: int, update period for the target network.\n          epsilon_fn: function expecting 4 parameters:\n            (decay_period, step, warmup_steps, epsilon). This function should return\n            the epsilon value used for exploration during training.\n          epsilon_train: float, the value to which the agent's epsilon is eventually\n            decayed during training.\n          epsilon_eval: float, epsilon used when evaluating the agent.\n          epsilon_decay_period: int, length of the epsilon decay schedule.\n          replay_scheme: str, 'prioritized' or 'uniform', the sampling scheme of the\n            replay memory.\n          tf_device: str, Tensorflow device on which the agent's graph is executed.\n          use_staging: bool, when True use a staging area to prefetch the next\n            training batch, speeding training up by about 30%.\n          optimizer: `tf.train.Optimizer`, for training the value function.\n          summary_writer: SummaryWriter object for outputting training statistics.\n            Summary writing disabled if set to None.\n          summary_writing_frequency: int, frequency with which summaries will be\n            written. Lower values will result in slower training.\n        \"\"\"", "\n", "# We need this because some tools convert round floats into ints.", "\n", "vmax", "=", "float", "(", "vmax", ")", "\n", "self", ".", "_num_atoms", "=", "num_atoms", "\n", "self", ".", "_support", "=", "tf", ".", "linspace", "(", "-", "vmax", ",", "vmax", ",", "num_atoms", ")", "\n", "self", ".", "_replay_scheme", "=", "replay_scheme", "\n", "# TODO(b/110897128): Make agent optimizer attribute private.", "\n", "self", ".", "optimizer", "=", "optimizer", "\n", "self", ".", "optimizer_rpg", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "\n", "decay", "=", "0.95", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "0.00001", ",", "\n", "centered", "=", "True", ")", "# optimizer for RPG'=", "\n", "self", ".", "start_training", "=", "1000", "\n", "\n", "super", "(", "RainbowRPGAgent", ",", "self", ")", ".", "__init__", "(", "\n", "sess", "=", "sess", ",", "\n", "num_actions", "=", "num_actions", ",", "\n", "observation_shape", "=", "observation_shape", ",", "\n", "observation_dtype", "=", "observation_dtype", ",", "\n", "stack_size", "=", "stack_size", ",", "\n", "gamma", "=", "gamma", ",", "\n", "update_horizon", "=", "update_horizon", ",", "\n", "min_replay_history", "=", "min_replay_history", ",", "\n", "update_period", "=", "update_period", ",", "\n", "target_update_period", "=", "target_update_period", ",", "\n", "epsilon_fn", "=", "epsilon_fn", ",", "\n", "epsilon_train", "=", "epsilon_train", ",", "\n", "epsilon_eval", "=", "epsilon_eval", ",", "\n", "epsilon_decay_period", "=", "epsilon_decay_period", ",", "\n", "tf_device", "=", "tf_device", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "optimizer", "=", "self", ".", "optimizer", ",", "\n", "summary_writer", "=", "summary_writer", ",", "\n", "summary_writing_frequency", "=", "summary_writing_frequency", ")", "\n", "\n", "with", "tf", ".", "device", "(", "tf_device", ")", ":", "\n", "            ", "self", ".", "_replay_opt", "=", "self", ".", "_build_replay_buffer_opt", "(", "use_staging", ")", "\n", "self", ".", "_build_networks_rpg", "(", ")", "\n", "self", ".", "_train_op_rpg", "=", "self", ".", "_build_train_op_rpg", "(", ")", "\n", "\n", "# replay buffer for rpg. only store good trajectories.", "\n", "", "self", ".", "replay_buffer_temp", "=", "ReplayBufferRegular", "(", "100000", ")", "# temporarily", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._get_network_type": [[163, 171], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the type of the outputs of a value distribution network.\n\n        Returns:\n          net_type: _network_type object defining the outputs of the network.\n        \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "'c51_network'", ",", "\n", "[", "'q_values'", ",", "'logits'", ",", "'probabilities'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._network_template": [[172, 205], ["slim.variance_scaling_initializer", "tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "slim.fully_connected", "slim.fully_connected", "tensorflow.reshape", "tensorflow.contrib.layers.softmax", "tensorflow.reduce_sum", "rainbowrpg_agent.RainbowRPGAgent._get_network_type", "numpy.sqrt"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Builds a convolutional network that outputs Q-value distributions.\n\n        Args:\n          state: `tf.Tensor`, contains the agent's current state.\n\n        Returns:\n          net: _network_type object containing the tensors output by the network.\n        \"\"\"", "\n", "weights_initializer", "=", "slim", ".", "variance_scaling_initializer", "(", "\n", "factor", "=", "1.0", "/", "np", ".", "sqrt", "(", "3.0", ")", ",", "mode", "=", "'FAN_IN'", ",", "uniform", "=", "True", ")", "\n", "\n", "net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "net", "=", "tf", ".", "div", "(", "net", ",", "255.", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "\n", "net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "\n", "net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "\n", "net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "512", ",", "weights_initializer", "=", "weights_initializer", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "\n", "net", ",", "\n", "self", ".", "num_actions", "*", "self", ".", "_num_atoms", ",", "\n", "activation_fn", "=", "None", ",", "\n", "weights_initializer", "=", "weights_initializer", ")", "\n", "\n", "logits", "=", "tf", ".", "reshape", "(", "net", ",", "[", "-", "1", ",", "self", ".", "num_actions", ",", "self", ".", "_num_atoms", "]", ")", "\n", "probabilities", "=", "tf", ".", "contrib", ".", "layers", ".", "softmax", "(", "logits", ")", "\n", "q_values", "=", "tf", ".", "reduce_sum", "(", "self", ".", "_support", "*", "probabilities", ",", "axis", "=", "2", ")", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "q_values", ",", "logits", ",", "probabilities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_replay_buffer": [[206, 227], ["dopamine.replay_memory.prioritized_replay_buffer.WrappedPrioritizedReplayBuffer", "ValueError"], "methods", ["None"], ["", "def", "_build_replay_buffer", "(", "self", ",", "use_staging", ")", ":", "\n", "        ", "\"\"\"Creates the replay buffer used by the agent.\n\n        Args:\n          use_staging: bool, if True, uses a staging area to prefetch data for\n            faster training.\n\n        Returns:\n          A `WrappedPrioritizedReplayBuffer` object.\n\n        Raises:\n          ValueError: if given an invalid replay scheme.\n        \"\"\"", "\n", "if", "self", ".", "_replay_scheme", "not", "in", "[", "'uniform'", ",", "'prioritized'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid replay scheme: {}'", ".", "format", "(", "self", ".", "_replay_scheme", ")", ")", "\n", "", "return", "prioritized_replay_buffer", ".", "WrappedPrioritizedReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_replay_buffer_opt": [[228, 245], ["dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer"], "methods", ["None"], ["", "def", "_build_replay_buffer_opt", "(", "self", ",", "use_staging", ")", ":", "\n", "        ", "\"\"\"Creates the replay buffer used by the agent.\n\n        Args:\n          use_staging: bool, if True, uses a staging area to prefetch data for\n            faster training.\n\n        Returns:\n          A WrapperReplayBuffer object.\n        \"\"\"", "\n", "return", "circular_replay_buffer", ".", "WrappedReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "observation_dtype", "=", "self", ".", "observation_dtype", ".", "as_numpy_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_target_distribution": [[246, 298], ["tensorflow.tile", "tensorflow.reshape", "tensorflow.concat", "tensorflow.gather_nd", "rainbowrpg_agent.project_distribution", "tensorflow.cast", "tensorflow.argmax", "tensorflow.range", "tensorflow.to_int64"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.project_distribution"], ["", "def", "_build_target_distribution", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds the C51 target distribution as per Bellemare et al. (2017).\n\n        First, we compute the support of the Bellman target, r + gamma Z'. Where Z'\n        is the support of the next state distribution:\n\n          * Evenly spaced in [-vmax, vmax] if the current state is nonterminal;\n          * 0 otherwise (duplicated num_atoms times).\n\n        Second, we compute the next-state probabilities, corresponding to the action\n        with highest expected value.\n\n        Finally we project the Bellman target (support + probabilities) onto the\n        original support.\n\n        Returns:\n          target_distribution: tf.tensor, the target distribution from the replay.\n        \"\"\"", "\n", "batch_size", "=", "self", ".", "_replay", ".", "batch_size", "\n", "\n", "# size of rewards: batch_size x 1", "\n", "rewards", "=", "self", ".", "_replay", ".", "rewards", "[", ":", ",", "None", "]", "\n", "\n", "# size of tiled_support: batch_size x num_atoms", "\n", "tiled_support", "=", "tf", ".", "tile", "(", "self", ".", "_support", ",", "[", "batch_size", "]", ")", "\n", "tiled_support", "=", "tf", ".", "reshape", "(", "tiled_support", ",", "[", "batch_size", ",", "self", ".", "_num_atoms", "]", ")", "\n", "\n", "# size of target_support: batch_size x num_atoms", "\n", "\n", "is_terminal_multiplier", "=", "1.", "-", "tf", ".", "cast", "(", "self", ".", "_replay", ".", "terminals", ",", "tf", ".", "float32", ")", "\n", "# Incorporate terminal state to discount factor.", "\n", "# size of gamma_with_terminal: batch_size x 1", "\n", "gamma_with_terminal", "=", "self", ".", "cumulative_gamma", "*", "is_terminal_multiplier", "\n", "gamma_with_terminal", "=", "gamma_with_terminal", "[", ":", ",", "None", "]", "\n", "\n", "target_support", "=", "rewards", "+", "gamma_with_terminal", "*", "tiled_support", "\n", "\n", "# size of next_qt_argmax: 1 x batch_size", "\n", "next_qt_argmax", "=", "tf", ".", "argmax", "(", "\n", "self", ".", "_replay_next_target_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", ":", ",", "None", "]", "\n", "batch_indices", "=", "tf", ".", "range", "(", "tf", ".", "to_int64", "(", "batch_size", ")", ")", "[", ":", ",", "None", "]", "\n", "# size of next_qt_argmax: batch_size x 2", "\n", "batch_indexed_next_qt_argmax", "=", "tf", ".", "concat", "(", "\n", "[", "batch_indices", ",", "next_qt_argmax", "]", ",", "axis", "=", "1", ")", "\n", "\n", "# size of next_probabilities: batch_size x num_atoms", "\n", "next_probabilities", "=", "tf", ".", "gather_nd", "(", "\n", "self", ".", "_replay_next_target_net_outputs", ".", "probabilities", ",", "\n", "batch_indexed_next_qt_argmax", ")", "\n", "\n", "return", "project_distribution", "(", "target_support", ",", "next_probabilities", ",", "\n", "self", ".", "_support", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_networks": [[299, 327], ["tensorflow.make_template", "tensorflow.make_template", "rainbowrpg_agent.RainbowRPGAgent.online_convnet", "rainbowrpg_agent.RainbowRPGAgent.online_convnet", "rainbowrpg_agent.RainbowRPGAgent.target_convnet", "tensorflow.argmax"], "methods", ["None"], ["", "def", "_build_networks", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds the Q-value network computations needed for acting and training.\n\n        These are:\n          self.online_convnet: For computing the current state's Q-values.\n          self.target_convnet: For computing the next state's target Q-values.\n          self._net_outputs: The actual Q-values.\n          self._q_argmax: The action maximizing the current state's Q-values.\n          self._replay_net_outputs: The replayed states' Q-values.\n          self._replay_next_target_net_outputs: The replayed next states' target\n            Q-values (see Mnih et al., 2015 for details).\n        \"\"\"", "\n", "# Calling online_convnet will generate a new graph as defined in", "\n", "# self._get_network_template using whatever input is passed, but will always", "\n", "# share the same weights.", "\n", "# DQN explore net.", "\n", "self", ".", "online_convnet", "=", "tf", ".", "make_template", "(", "'Online'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "target_convnet", "=", "tf", ".", "make_template", "(", "'Target'", ",", "self", ".", "_network_template", ")", "\n", "\n", "self", ".", "_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "state_ph", ")", "\n", "# TODO(bellemare): Ties should be broken. They are unlikely to happen when", "\n", "# using a deep network, but may affect performance with a linear", "\n", "# approximation scheme.", "\n", "self", ".", "_q_argmax", "=", "tf", ".", "argmax", "(", "self", ".", "_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "\n", "self", ".", "_replay_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "states", ")", "\n", "self", ".", "_replay_next_target_net_outputs", "=", "self", ".", "target_convnet", "(", "\n", "self", ".", "_replay", ".", "next_states", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_networks_rpg": [[328, 334], ["tensorflow.make_template", "rainbowrpg_agent.RainbowRPGAgent.rpg_convnet", "rainbowrpg_agent.RainbowRPGAgent.rpg_convnet", "tensorflow.argmax"], "methods", ["None"], ["", "def", "_build_networks_rpg", "(", "self", ")", ":", "\n", "# RPG learning net.", "\n", "        ", "self", ".", "rpg_convnet", "=", "tf", ".", "make_template", "(", "'RPG'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "_rpg_net_outputs", "=", "self", ".", "rpg_convnet", "(", "self", ".", "state_ph", ")", "\n", "self", ".", "_q_argmax_rpg", "=", "tf", ".", "argmax", "(", "self", ".", "_rpg_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "self", ".", "_replay_rpg_net_outputs", "=", "self", ".", "rpg_convnet", "(", "self", ".", "_replay_opt", ".", "states", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_train_op": [[335, 387], ["tensorflow.stop_gradient", "tensorflow.concat", "tensorflow.gather_nd", "tensorflow.nn.softmax_cross_entropy_with_logits", "rainbowrpg_agent.RainbowRPGAgent._build_target_distribution", "tensorflow.range", "tensorflow.reduce_max", "rainbowrpg_agent.RainbowRPGAgent._replay.tf_set_priority", "tensorflow.no_op", "tensorflow.control_dependencies", "tensorflow.sqrt", "tensorflow.sqrt", "rainbowrpg_agent.RainbowRPGAgent.optimizer.minimize", "tensorflow.shape", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.reduce_mean", "tensorflow.reduce_mean"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_target_distribution", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.prioritized_replay_buffer.WrappedPrioritizedReplayBuffer.tf_set_priority"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds a training op.\n\n        Returns:\n          train_op: An op performing one step of training from replay data.\n        \"\"\"", "\n", "target_distribution", "=", "tf", ".", "stop_gradient", "(", "self", ".", "_build_target_distribution", "(", ")", ")", "\n", "\n", "# size of indices: batch_size x 1.", "\n", "indices", "=", "tf", ".", "range", "(", "tf", ".", "shape", "(", "self", ".", "_replay_net_outputs", ".", "logits", ")", "[", "0", "]", ")", "[", ":", ",", "None", "]", "\n", "# size of reshaped_actions: batch_size x 2.", "\n", "reshaped_actions", "=", "tf", ".", "concat", "(", "[", "indices", ",", "self", ".", "_replay", ".", "actions", "[", ":", ",", "None", "]", "]", ",", "1", ")", "\n", "# For each element of the batch, fetch the logits for its selected action.", "\n", "chosen_action_logits", "=", "tf", ".", "gather_nd", "(", "self", ".", "_replay_net_outputs", ".", "logits", ",", "\n", "reshaped_actions", ")", "\n", "\n", "loss", "=", "tf", ".", "nn", ".", "softmax_cross_entropy_with_logits", "(", "\n", "labels", "=", "target_distribution", ",", "\n", "logits", "=", "chosen_action_logits", ")", "\n", "\n", "if", "self", ".", "_replay_scheme", "==", "'prioritized'", ":", "\n", "# The original prioritized experience replay uses a linear exponent", "\n", "# schedule 0.4 -> 1.0. Comparing the schedule to a fixed exponent of 0.5", "\n", "# on 5 games (Asterix, Pong, Q*Bert, Seaquest, Space Invaders) suggested", "\n", "# a fixed exponent actually performs better, except on Pong.", "\n", "            ", "probs", "=", "self", ".", "_replay", ".", "transition", "[", "'sampling_probabilities'", "]", "\n", "loss_weights", "=", "1.0", "/", "tf", ".", "sqrt", "(", "probs", "+", "1e-10", ")", "\n", "loss_weights", "/=", "tf", ".", "reduce_max", "(", "loss_weights", ")", "\n", "\n", "# Rainbow and prioritized replay are parametrized by an exponent alpha,", "\n", "# but in both cases it is set to 0.5 - for simplicity's sake we leave it", "\n", "# as is here, using the more direct tf.sqrt(). Taking the square root", "\n", "# \"makes sense\", as we are dealing with a squared loss.", "\n", "# Add a small nonzero value to the loss to avoid 0 priority items. While", "\n", "# technically this may be okay, setting all items to 0 priority will cause", "\n", "# troubles, and also result in 1.0 / 0.0 = NaN correction terms.", "\n", "update_priorities_op", "=", "self", ".", "_replay", ".", "tf_set_priority", "(", "\n", "self", ".", "_replay", ".", "indices", ",", "tf", ".", "sqrt", "(", "loss", "+", "1e-10", ")", ")", "\n", "\n", "# Weight the loss by the inverse priorities.", "\n", "loss", "=", "loss_weights", "*", "loss", "\n", "", "else", ":", "\n", "            ", "update_priorities_op", "=", "tf", ".", "no_op", "(", ")", "\n", "\n", "", "with", "tf", ".", "control_dependencies", "(", "[", "update_priorities_op", "]", ")", ":", "\n", "            ", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "                ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "                    ", "tf", ".", "summary", ".", "scalar", "(", "'CrossEntropyLoss'", ",", "tf", ".", "reduce_mean", "(", "loss", ")", ")", "\n", "# Schaul et al. reports a slightly different rule, where 1/N is also", "\n", "# exponentiated by beta. Not doing so seems more reasonable, and did not", "\n", "# impact performance in our experiments.", "\n", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "tf", ".", "reduce_mean", "(", "loss", ")", ")", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._build_train_op_rpg": [[388, 406], ["tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.maximum", "tensorflow.reduce_mean", "rainbowrpg_agent.RainbowRPGAgent.optimizer_rpg.minimize", "tensorflow.reshape", "tensorflow.ones"], "methods", ["None"], ["", "", "def", "_build_train_op_rpg", "(", "self", ")", ":", "\n", "# RPG loss", "\n", "        ", "replay_action_one_hot", "=", "tf", ".", "one_hot", "(", "\n", "self", ".", "_replay_opt", ".", "actions", ",", "self", ".", "num_actions", ",", "1.", ",", "0.", ",", "name", "=", "'action_one_hot_rpg'", ")", "\n", "replay_chosen_q", "=", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "_replay_rpg_net_outputs", ".", "q_values", "*", "replay_action_one_hot", ",", "\n", "reduction_indices", "=", "1", ",", "\n", "name", "=", "'replay_chosen_q_rpg'", ")", "\n", "margin", "=", "1", "\n", "qvalue", "=", "self", ".", "_replay_rpg_net_outputs", ".", "q_values", "\n", "# debug self.temp_action_one_hot = replay_action_one_hot", "\n", "self", ".", "temp_qvalue", "=", "qvalue", "\n", "self", ".", "temp1", "=", "(", "qvalue", "+", "margin", ")", "*", "(", "1", "-", "replay_action_one_hot", ")", "+", "qvalue", "*", "replay_action_one_hot", "\n", "self", ".", "temp2", "=", "-", "(", "tf", ".", "reshape", "(", "replay_chosen_q", ",", "[", "-", "1", ",", "1", "]", ")", "*", "tf", ".", "ones", "(", "[", "1", ",", "self", ".", "num_actions", "]", ")", ")", "*", "(", "(", "1", "-", "replay_action_one_hot", ")", "+", "(", "replay_action_one_hot", ")", ")", "\n", "self", ".", "hingeloss", "=", "tf", ".", "maximum", "(", "0.0", ",", "self", ".", "temp1", "+", "self", ".", "temp2", ")", "\n", "rpg_loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "hingeloss", ")", "\n", "return", "self", ".", "optimizer_rpg", ".", "minimize", "(", "rpg_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._store_transition": [[407, 436], ["rainbowrpg_agent.RainbowRPGAgent._replay.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "def", "_store_transition", "(", "self", ",", "\n", "last_observation", ",", "\n", "action", ",", "\n", "reward", ",", "\n", "is_terminal", ",", "\n", "priority", "=", "None", ")", ":", "\n", "        ", "\"\"\"Stores a transition when in training mode.\n\n        Executes a tf session and executes replay buffer ops in order to store the\n        following tuple in the replay buffer (last_observation, action, reward,\n        is_terminal, priority).\n\n        Args:\n          last_observation: Last observation, type determined via observation_type\n            parameter in the replay_memory constructor.\n          action: An integer, the action taken.\n          reward: A float, the reward.\n          is_terminal: Boolean indicating if the current state is a terminal state.\n          priority: Float. Priority of sampling the transition. If None, the default\n            priority will be used. If replay scheme is uniform, the default priority\n            is 1. If the replay scheme is prioritized, the default priority is the\n            maximum ever seen [Schaul et al., 2015].\n        \"\"\"", "\n", "if", "priority", "is", "None", ":", "\n", "            ", "priority", "=", "(", "1.", "if", "self", ".", "_replay_scheme", "==", "'uniform'", "else", "\n", "self", ".", "_replay", ".", "memory", ".", "sum_tree", ".", "max_recorded_priority", ")", "\n", "\n", "", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_replay", ".", "add", "(", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ",", "priority", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent.step": [[437, 460], ["rainbowrpg_agent.RainbowRPGAgent._record_observation", "rainbowrpg_agent.RainbowRPGAgent._select_action", "rainbowrpg_agent.RainbowRPGAgent._store_transition", "rainbowrpg_agent.RainbowRPGAgent.replay_buffer_temp.add", "rainbowrpg_agent.RainbowRPGAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step"], ["", "", "def", "step", "(", "self", ",", "reward", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records the most recent transition and returns the agent's next action.\n\n        We store the observation of the last time step since we want to store it\n        with the reward.\n\n        Args:\n          reward: float, the reward received from the agent's most recent action.\n          observation: numpy array, the most recent observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_last_observation", "=", "self", ".", "_observation", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_store_transition", "(", "self", ".", "_last_observation", ",", "self", ".", "action", ",", "reward", ",", "False", ")", "\n", "self", ".", "replay_buffer_temp", ".", "add", "(", "self", ".", "_last_observation", ",", "self", ".", "action", ",", "reward", ",", "False", ")", "\n", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent.end_episode": [[461, 473], ["rainbowrpg_agent.RainbowRPGAgent.replay_buffer_temp.clear", "rainbowrpg_agent.RainbowRPGAgent._store_transition"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.clear", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition"], ["", "def", "end_episode", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Signals the end of the episode to the agent.\n\n        We store the observation of the current time step, which is the last\n        observation of the episode.\n\n        Args:\n          reward: float, the last reward from the environment.\n        \"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "replay_buffer_temp", ".", "clear", "(", ")", "# this episode is not optimal", "\n", "self", ".", "_store_transition", "(", "self", ".", "_observation", ",", "self", ".", "action", ",", "reward", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent.end_episode_": [[474, 489], ["rainbowrpg_agent.RainbowRPGAgent._store_transition", "rainbowrpg_agent.RainbowRPGAgent.replay_buffer_temp.add", "rainbowrpg_agent.RainbowRPGAgent.replay_buffer_temp.get_sample", "rainbowrpg_agent.RainbowRPGAgent._replay_opt.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.get_sample", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "", "def", "end_episode_", "(", "self", ",", "reward", ",", "total_reward", ",", "step_number", ")", ":", "\n", "        ", "\"\"\" This episodes is optimal trajectory \"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "# for DQN", "\n", "            ", "self", ".", "_store_transition", "(", "self", ".", "_observation", ",", "self", ".", "action", ",", "reward", ",", "True", ")", "\n", "\n", "# replay buffer for RPG.", "\n", "self", ".", "replay_buffer_temp", ".", "add", "(", "self", ".", "_observation", ",", "self", ".", "action", ",", "reward", ",", "True", ")", "\n", "count", "=", "step_number", "\n", "while", "count", ">", "0", ":", "\n", "                ", "experience", "=", "self", ".", "replay_buffer_temp", ".", "get_sample", "(", ")", "\n", "state", ",", "action", ",", "reward", ",", "_", "=", "experience", "\n", "count", "-=", "1", "\n", "# self.replay_buffer_opt.add(state, action, reward, False)", "\n", "self", ".", "_replay_opt", ".", "add", "(", "state", ",", "action", ",", "reward", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._select_action": [[490, 514], ["rainbowrpg_agent.RainbowRPGAgent.epsilon_fn", "rainbowrpg_agent.RainbowRPGAgent._sess.run", "random.random", "random.randint", "rainbowrpg_agent.RainbowRPGAgent._sess.run"], "methods", ["None"], ["", "", "", "def", "_select_action", "(", "self", ")", ":", "\n", "        ", "\"\"\"Select an action from the set of available actions.\n\n        Chooses an action randomly with probability self._calculate_epsilon(), and\n        otherwise acts greedily according to the current Q-value estimates.\n\n        Returns:\n           int, the selected action.\n        \"\"\"", "\n", "if", "self", ".", "eval_mode", "is", "not", "True", ":", "\n", "            ", "epsilon", "=", "self", ".", "epsilon_fn", "(", "\n", "self", ".", "epsilon_decay_period", ",", "\n", "self", ".", "training_steps", ",", "\n", "self", ".", "min_replay_history", ",", "\n", "self", ".", "epsilon_train", ")", "\n", "if", "random", ".", "random", "(", ")", "<=", "epsilon", ":", "\n", "# Choose a random action with probability epsilon.", "\n", "                ", "return", "random", ".", "randint", "(", "0", ",", "self", ".", "num_actions", "-", "1", ")", "\n", "", "else", ":", "\n", "# Choose the action with highest Q-value at the current state.", "\n", "                ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "", "", "else", ":", "\n", "# evaluation mode: use rpg.", "\n", "            ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax_rpg", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent._train_step": [[515, 543], ["rainbowrpg_agent.RainbowRPGAgent._sess.run", "rainbowrpg_agent.RainbowRPGAgent._sess.run", "rainbowrpg_agent.RainbowRPGAgent._sess.run", "rainbowrpg_agent.RainbowRPGAgent._sess.run", "rainbowrpg_agent.RainbowRPGAgent.summary_writer.add_summary"], "methods", ["None"], ["", "", "def", "_train_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Runs a single training step.\n\n        Runs a training op if both:\n          (1) A minimum number of frames have been added to the replay buffer.\n          (2) `training_steps` is a multiple of `update_period`.\n\n        Also, syncs weights from online to target network if training steps is a\n        multiple of target update period.\n        \"\"\"", "\n", "# Run a train op at the rate of self.update_period if enough training steps", "\n", "# have been run. This matches the Nature DQN behaviour.", "\n", "if", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "min_replay_history", ":", "\n", "            ", "if", "self", ".", "training_steps", "%", "self", ".", "update_period", "==", "0", ":", "\n", "                ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_op", ")", "\n", "if", "self", ".", "_replay_opt", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ":", "\n", "                    ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_op_rpg", ")", "\n", "\n", "", "if", "(", "self", ".", "summary_writer", "is", "not", "None", "and", "\n", "self", ".", "training_steps", ">", "0", "and", "\n", "self", ".", "training_steps", "%", "self", ".", "summary_writing_frequency", "==", "0", ")", ":", "\n", "                    ", "summary", "=", "self", ".", "_sess", ".", "run", "(", "self", ".", "_merged_summaries", ")", "\n", "self", ".", "summary_writer", ".", "add_summary", "(", "summary", ",", "self", ".", "training_steps", ")", "\n", "\n", "", "", "if", "self", ".", "training_steps", "%", "self", ".", "target_update_period", "==", "0", ":", "\n", "                ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_sync_qt_ops", ")", "\n", "\n", "", "", "self", ".", "training_steps", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.rainbowrpg.rainbowrpg_agent.project_distribution": [[545, 700], ["supports.shape.assert_is_compatible_with", "supports[].shape.assert_is_compatible_with", "target_support.shape.assert_has_rank", "validate_deps.append", "validate_deps.append", "validate_deps.append", "validate_deps.append", "validate_deps.append", "tensorflow.control_dependencies", "tensorflow.tile", "tensorflow.tile", "tensorflow.reshape", "tensorflow.abs", "tensorflow.clip_by_value", "tensorflow.reduce_sum", "tensorflow.reshape", "tensorflow.Assert", "tensorflow.Assert", "tensorflow.Assert", "tensorflow.Assert", "tensorflow.Assert", "tensorflow.shape", "tensorflow.shape", "tensorflow.clip_by_value", "tensorflow.reduce_all", "tensorflow.reduce_all", "tensorflow.equal", "tensorflow.reduce_all", "tensorflow.reduce_all", "tensorflow.equal", "tensorflow.equal", "tensorflow.size", "tensorflow.equal", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape", "tensorflow.shape"], "function", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.size"], ["", "", "def", "project_distribution", "(", "supports", ",", "weights", ",", "target_support", ",", "\n", "validate_args", "=", "False", ")", ":", "\n", "    ", "\"\"\"Projects a batch of (support, weights) onto target_support.\n\n    Based on equation (7) in (Bellemare et al., 2017):\n      https://arxiv.org/abs/1707.06887\n    In the rest of the comments we will refer to this equation simply as Eq7.\n\n    This code is not easy to digest, so we will use a running example to clarify\n    what is going on, with the following sample inputs:\n\n      * supports =       [[0, 2, 4, 6, 8],\n                          [1, 3, 4, 5, 6]]\n      * weights =        [[0.1, 0.6, 0.1, 0.1, 0.1],\n                          [0.1, 0.2, 0.5, 0.1, 0.1]]\n      * target_support = [4, 5, 6, 7, 8]\n\n    In the code below, comments preceded with 'Ex:' will be referencing the above\n    values.\n\n    Args:\n      supports: Tensor of shape (batch_size, num_dims) defining supports for the\n        distribution.\n      weights: Tensor of shape (batch_size, num_dims) defining weights on the\n        original support points. Although for the CategoricalDQN agent these\n        weights are probabilities, it is not required that they are.\n      target_support: Tensor of shape (num_dims) defining support of the projected\n        distribution. The values must be monotonically increasing. Vmin and Vmax\n        will be inferred from the first and last elements of this tensor,\n        respectively. The values in this tensor must be equally spaced.\n      validate_args: Whether we will verify the contents of the\n        target_support parameter.\n\n    Returns:\n      A Tensor of shape (batch_size, num_dims) with the projection of a batch of\n      (support, weights) onto target_support.\n\n    Raises:\n      ValueError: If target_support has no dimensions, or if shapes of supports,\n        weights, and target_support are incompatible.\n    \"\"\"", "\n", "target_support_deltas", "=", "target_support", "[", "1", ":", "]", "-", "target_support", "[", ":", "-", "1", "]", "\n", "# delta_z = `\\Delta z` in Eq7.", "\n", "delta_z", "=", "target_support_deltas", "[", "0", "]", "\n", "validate_deps", "=", "[", "]", "\n", "supports", ".", "shape", ".", "assert_is_compatible_with", "(", "weights", ".", "shape", ")", "\n", "supports", "[", "0", "]", ".", "shape", ".", "assert_is_compatible_with", "(", "target_support", ".", "shape", ")", "\n", "target_support", ".", "shape", ".", "assert_has_rank", "(", "1", ")", "\n", "if", "validate_args", ":", "\n", "# Assert that supports and weights have the same shapes.", "\n", "        ", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "\n", "tf", ".", "reduce_all", "(", "tf", ".", "equal", "(", "tf", ".", "shape", "(", "supports", ")", ",", "tf", ".", "shape", "(", "weights", ")", ")", ")", ",", "\n", "[", "supports", ",", "weights", "]", ")", ")", "\n", "# Assert that elements of supports and target_support have the same shape.", "\n", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "\n", "tf", ".", "reduce_all", "(", "\n", "tf", ".", "equal", "(", "tf", ".", "shape", "(", "supports", ")", "[", "1", "]", ",", "tf", ".", "shape", "(", "target_support", ")", ")", ")", ",", "\n", "[", "supports", ",", "target_support", "]", ")", ")", "\n", "# Assert that target_support has a single dimension.", "\n", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "\n", "tf", ".", "equal", "(", "tf", ".", "size", "(", "tf", ".", "shape", "(", "target_support", ")", ")", ",", "1", ")", ",", "[", "target_support", "]", ")", ")", "\n", "# Assert that the target_support is monotonically increasing.", "\n", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "tf", ".", "reduce_all", "(", "target_support_deltas", ">", "0", ")", ",", "[", "target_support", "]", ")", ")", "\n", "# Assert that the values in target_support are equally spaced.", "\n", "validate_deps", ".", "append", "(", "\n", "tf", ".", "Assert", "(", "\n", "tf", ".", "reduce_all", "(", "tf", ".", "equal", "(", "target_support_deltas", ",", "delta_z", ")", ")", ",", "\n", "[", "target_support", "]", ")", ")", "\n", "\n", "", "with", "tf", ".", "control_dependencies", "(", "validate_deps", ")", ":", "\n", "# Ex: `v_min, v_max = 4, 8`.", "\n", "        ", "v_min", ",", "v_max", "=", "target_support", "[", "0", "]", ",", "target_support", "[", "-", "1", "]", "\n", "# Ex: `batch_size = 2`.", "\n", "batch_size", "=", "tf", ".", "shape", "(", "supports", ")", "[", "0", "]", "\n", "# `N` in Eq7.", "\n", "# Ex: `num_dims = 5`.", "\n", "num_dims", "=", "tf", ".", "shape", "(", "target_support", ")", "[", "0", "]", "\n", "# clipped_support = `[\\hat{T}_{z_j}]^{V_max}_{V_min}` in Eq7.", "\n", "# Ex: `clipped_support = [[[ 4.  4.  4.  6.  8.]]", "\n", "#                         [[ 4.  4.  4.  5.  6.]]]`.", "\n", "clipped_support", "=", "tf", ".", "clip_by_value", "(", "supports", ",", "v_min", ",", "v_max", ")", "[", ":", ",", "None", ",", ":", "]", "\n", "# Ex: `tiled_support = [[[[ 4.  4.  4.  6.  8.]", "\n", "#                         [ 4.  4.  4.  6.  8.]", "\n", "#                         [ 4.  4.  4.  6.  8.]", "\n", "#                         [ 4.  4.  4.  6.  8.]", "\n", "#                         [ 4.  4.  4.  6.  8.]]", "\n", "#                        [[ 4.  4.  4.  5.  6.]", "\n", "#                         [ 4.  4.  4.  5.  6.]", "\n", "#                         [ 4.  4.  4.  5.  6.]", "\n", "#                         [ 4.  4.  4.  5.  6.]", "\n", "#                         [ 4.  4.  4.  5.  6.]]]]`.", "\n", "tiled_support", "=", "tf", ".", "tile", "(", "[", "clipped_support", "]", ",", "[", "1", ",", "1", ",", "num_dims", ",", "1", "]", ")", "\n", "# Ex: `reshaped_target_support = [[[ 4.]", "\n", "#                                  [ 5.]", "\n", "#                                  [ 6.]", "\n", "#                                  [ 7.]", "\n", "#                                  [ 8.]]", "\n", "#                                 [[ 4.]", "\n", "#                                  [ 5.]", "\n", "#                                  [ 6.]", "\n", "#                                  [ 7.]", "\n", "#                                  [ 8.]]]`.", "\n", "reshaped_target_support", "=", "tf", ".", "tile", "(", "target_support", "[", ":", ",", "None", "]", ",", "[", "batch_size", ",", "1", "]", ")", "\n", "reshaped_target_support", "=", "tf", ".", "reshape", "(", "reshaped_target_support", ",", "\n", "[", "batch_size", ",", "num_dims", ",", "1", "]", ")", "\n", "# numerator = `|clipped_support - z_i|` in Eq7.", "\n", "# Ex: `numerator = [[[[ 0.  0.  0.  2.  4.]", "\n", "#                     [ 1.  1.  1.  1.  3.]", "\n", "#                     [ 2.  2.  2.  0.  2.]", "\n", "#                     [ 3.  3.  3.  1.  1.]", "\n", "#                     [ 4.  4.  4.  2.  0.]]", "\n", "#                    [[ 0.  0.  0.  1.  2.]", "\n", "#                     [ 1.  1.  1.  0.  1.]", "\n", "#                     [ 2.  2.  2.  1.  0.]", "\n", "#                     [ 3.  3.  3.  2.  1.]", "\n", "#                     [ 4.  4.  4.  3.  2.]]]]`.", "\n", "numerator", "=", "tf", ".", "abs", "(", "tiled_support", "-", "reshaped_target_support", ")", "\n", "quotient", "=", "1", "-", "(", "numerator", "/", "delta_z", ")", "\n", "# clipped_quotient = `[1 - numerator / (\\Delta z)]_0^1` in Eq7.", "\n", "# Ex: `clipped_quotient = [[[[ 1.  1.  1.  0.  0.]", "\n", "#                            [ 0.  0.  0.  0.  0.]", "\n", "#                            [ 0.  0.  0.  1.  0.]", "\n", "#                            [ 0.  0.  0.  0.  0.]", "\n", "#                            [ 0.  0.  0.  0.  1.]]", "\n", "#                           [[ 1.  1.  1.  0.  0.]", "\n", "#                            [ 0.  0.  0.  1.  0.]", "\n", "#                            [ 0.  0.  0.  0.  1.]", "\n", "#                            [ 0.  0.  0.  0.  0.]", "\n", "#                            [ 0.  0.  0.  0.  0.]]]]`.", "\n", "clipped_quotient", "=", "tf", ".", "clip_by_value", "(", "quotient", ",", "0", ",", "1", ")", "\n", "# Ex: `weights = [[ 0.1  0.6  0.1  0.1  0.1]", "\n", "#                 [ 0.1  0.2  0.5  0.1  0.1]]`.", "\n", "weights", "=", "weights", "[", ":", ",", "None", ",", ":", "]", "\n", "# inner_prod = `\\sum_{j=0}^{N-1} clipped_quotient * p_j(x', \\pi(x'))`", "\n", "# in Eq7.", "\n", "# Ex: `inner_prod = [[[[ 0.1  0.6  0.1  0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.1 0. ]", "\n", "#                      [ 0.   0.   0.   0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.  0.1]]", "\n", "#                     [[ 0.1  0.2  0.5  0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.1 0. ]", "\n", "#                      [ 0.   0.   0.   0.  0.1]", "\n", "#                      [ 0.   0.   0.   0.  0. ]", "\n", "#                      [ 0.   0.   0.   0.  0. ]]]]`.", "\n", "inner_prod", "=", "clipped_quotient", "*", "weights", "\n", "# Ex: `projection = [[ 0.8 0.0 0.1 0.0 0.1]", "\n", "#                    [ 0.8 0.1 0.1 0.0 0.0]]`.", "\n", "projection", "=", "tf", ".", "reduce_sum", "(", "inner_prod", ",", "3", ")", "\n", "projection", "=", "tf", ".", "reshape", "(", "projection", ",", "[", "batch_size", ",", "num_dims", "]", ")", "\n", "return", "projection", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.__init__": [[67, 197], ["tensorflow.train.RMSPropOptimizer", "isinstance", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tuple", "math.pow", "tensorflow.train.RMSPropOptimizer", "ReplayBufferRegular", "tensorflow.train.Saver", "tensorflow.device", "numpy.zeros", "tensorflow.placeholder", "dqnrpg_agent.DQNRPGAgent._build_replay_buffer", "dqnrpg_agent.DQNRPGAgent._build_replay_buffer", "dqnrpg_agent.DQNRPGAgent._build_networks", "dqnrpg_agent.DQNRPGAgent._build_train_op", "dqnrpg_agent.DQNRPGAgent._build_sync_op", "tensorflow.summary.merge_all"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_replay_buffer", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_replay_buffer", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_networks", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_train_op", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_sync_op"], ["def", "__init__", "(", "self", ",", "\n", "sess", ",", "\n", "num_actions", ",", "\n", "observation_shape", "=", "NATURE_DQN_OBSERVATION_SHAPE", ",", "\n", "observation_dtype", "=", "NATURE_DQN_DTYPE", ",", "\n", "stack_size", "=", "NATURE_DQN_STACK_SIZE", ",", "\n", "gamma", "=", "0.99", ",", "\n", "update_horizon", "=", "1", ",", "\n", "min_replay_history", "=", "20000", ",", "\n", "update_period", "=", "4", ",", "\n", "target_update_period", "=", "8000", ",", "\n", "epsilon_fn", "=", "linearly_decaying_epsilon", ",", "\n", "epsilon_train", "=", "0.01", ",", "\n", "epsilon_eval", "=", "0.001", ",", "\n", "epsilon_decay_period", "=", "250000", ",", "\n", "tf_device", "=", "'/cpu:*'", ",", "\n", "use_staging", "=", "True", ",", "\n", "max_tf_checkpoints_to_keep", "=", "3", ",", "\n", "optimizer", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "\n", "decay", "=", "0.95", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "0.00001", ",", "\n", "centered", "=", "True", ")", ",", "\n", "summary_writer", "=", "None", ",", "\n", "summary_writing_frequency", "=", "500", ")", ":", "\n", "        ", "\"\"\"Initializes the agent and constructs the components of its graph.\n\n        Args:\n          sess: `tf.Session`, for executing ops.\n          num_actions: int, number of actions the agent can take at any state.\n          observation_shape: tuple of ints describing the observation shape.\n          observation_dtype: tf.DType, specifies the type of the observations. Note\n            that if your inputs are continuous, you should set this to tf.float32.\n          stack_size: int, number of frames to use in state stack.\n          gamma: float, discount factor with the usual RL meaning.\n          update_horizon: int, horizon at which updates are performed, the 'n' in\n            n-step update.\n          min_replay_history: int, number of transitions that should be experienced\n            before the agent begins training its value function.\n          update_period: int, period between DQN updates.\n          target_update_period: int, update period for the target network.\n          epsilon_fn: function expecting 4 parameters:\n            (decay_period, step, warmup_steps, epsilon). This function should return\n            the epsilon value used for exploration during training.\n          epsilon_train: float, the value to which the agent's epsilon is eventually\n            decayed during training.\n          epsilon_eval: float, epsilon used when evaluating the agent.\n          epsilon_decay_period: int, length of the epsilon decay schedule.\n          tf_device: str, Tensorflow device on which the agent's graph is executed.\n          use_staging: bool, when True use a staging area to prefetch the next\n            training batch, speeding training up by about 30%.\n          max_tf_checkpoints_to_keep: int, the number of TensorFlow checkpoints to\n            keep.\n          optimizer: `tf.train.Optimizer`, for training the value function.\n          summary_writer: SummaryWriter object for outputting training statistics.\n            Summary writing disabled if set to None.\n          summary_writing_frequency: int, frequency with which summaries will be\n            written. Lower values will result in slower training.\n        \"\"\"", "\n", "assert", "isinstance", "(", "observation_shape", ",", "tuple", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Creating %s agent with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t gamma: %f'", ",", "gamma", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_horizon: %f'", ",", "update_horizon", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t min_replay_history: %d'", ",", "min_replay_history", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t update_period: %d'", ",", "update_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t target_update_period: %d'", ",", "target_update_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_train: %f'", ",", "epsilon_train", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_eval: %f'", ",", "epsilon_eval", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t epsilon_decay_period: %d'", ",", "epsilon_decay_period", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t tf_device: %s'", ",", "tf_device", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t use_staging: %s'", ",", "use_staging", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t optimizer: %s'", ",", "optimizer", ")", "\n", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "self", ".", "observation_shape", "=", "tuple", "(", "observation_shape", ")", "\n", "self", ".", "observation_dtype", "=", "observation_dtype", "\n", "self", ".", "stack_size", "=", "stack_size", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "update_horizon", "=", "update_horizon", "\n", "self", ".", "cumulative_gamma", "=", "math", ".", "pow", "(", "gamma", ",", "update_horizon", ")", "\n", "self", ".", "min_replay_history", "=", "min_replay_history", "\n", "self", ".", "target_update_period", "=", "target_update_period", "\n", "self", ".", "epsilon_fn", "=", "epsilon_fn", "\n", "self", ".", "epsilon_train", "=", "epsilon_train", "\n", "self", ".", "epsilon_eval", "=", "epsilon_eval", "\n", "self", ".", "epsilon_decay_period", "=", "epsilon_decay_period", "\n", "self", ".", "update_period", "=", "update_period", "\n", "self", ".", "eval_mode", "=", "False", "\n", "self", ".", "training_steps", "=", "0", "\n", "self", ".", "optimizer", "=", "optimizer", "# DQN optimizer.", "\n", "self", ".", "optimizer_rpg", "=", "tf", ".", "train", ".", "RMSPropOptimizer", "(", "\n", "learning_rate", "=", "0.00025", ",", "\n", "decay", "=", "0.95", ",", "\n", "momentum", "=", "0.0", ",", "\n", "epsilon", "=", "0.00001", ",", "\n", "centered", "=", "True", ")", "# optimizer for RPG", "\n", "self", ".", "summary_writer", "=", "summary_writer", "\n", "self", ".", "summary_writing_frequency", "=", "summary_writing_frequency", "\n", "self", ".", "start_training", "=", "1000", "# todo task specific", "\n", "\n", "with", "tf", ".", "device", "(", "tf_device", ")", ":", "\n", "# Create a placeholder for the state input to the DQN network.", "\n", "# The last axis indicates the number of consecutive frames stacked.", "\n", "            ", "state_shape", "=", "(", "1", ",", ")", "+", "self", ".", "observation_shape", "+", "(", "stack_size", ",", ")", "\n", "self", ".", "state", "=", "np", ".", "zeros", "(", "state_shape", ")", "\n", "self", ".", "state_ph", "=", "tf", ".", "placeholder", "(", "self", ".", "observation_dtype", ",", "state_shape", ",", "\n", "name", "=", "'state_ph'", ")", "\n", "self", ".", "_replay", "=", "self", ".", "_build_replay_buffer", "(", "use_staging", ")", "\n", "self", ".", "_replay_opt", "=", "self", ".", "_build_replay_buffer", "(", "use_staging", ")", "# store optimal trajectory", "\n", "self", ".", "_build_networks", "(", ")", "\n", "\n", "self", ".", "_train_op", ",", "self", ".", "_train_op_rpg", "=", "self", ".", "_build_train_op", "(", ")", "\n", "self", ".", "_sync_qt_ops", "=", "self", ".", "_build_sync_op", "(", ")", "\n", "\n", "# replay buffer for rpg. only store good trajectories.", "\n", "", "self", ".", "replay_buffer_temp", "=", "ReplayBufferRegular", "(", "100000", ")", "# temporarily", "\n", "\n", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "# All tf.summaries should have been defined prior to running this.", "\n", "            ", "self", ".", "_merged_summaries", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "", "self", ".", "_sess", "=", "sess", "\n", "self", ".", "_saver", "=", "tf", ".", "train", ".", "Saver", "(", "max_to_keep", "=", "max_tf_checkpoints_to_keep", ")", "\n", "\n", "# Variables to be initialized by the agent once it interacts with the", "\n", "# environment.", "\n", "self", ".", "_observation", "=", "None", "\n", "self", ".", "_last_observation", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type": [[198, 205], ["collections.namedtuple"], "methods", ["None"], ["", "def", "_get_network_type", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns the type of the outputs of a Q value network.\n\n        Returns:\n          net_type: _network_type object defining the outputs of the network.\n        \"\"\"", "\n", "return", "collections", ".", "namedtuple", "(", "'DQN_network'", ",", "[", "'q_values'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._network_template": [[206, 224], ["tensorflow.cast", "tensorflow.div", "slim.conv2d", "slim.conv2d", "slim.conv2d", "slim.flatten", "slim.fully_connected", "slim.fully_connected", "dqnrpg_agent.DQNRPGAgent._get_network_type"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._get_network_type"], ["", "def", "_network_template", "(", "self", ",", "state", ")", ":", "\n", "        ", "\"\"\"Builds the convolutional network used to compute the agent's Q-values.\n\n        Args:\n          state: `tf.Tensor`, contains the agent's current state.\n\n        Returns:\n          net: _network_type object containing the tensors output by the network.\n        \"\"\"", "\n", "net", "=", "tf", ".", "cast", "(", "state", ",", "tf", ".", "float32", ")", "\n", "net", "=", "tf", ".", "div", "(", "net", ",", "255.", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "32", ",", "[", "8", ",", "8", "]", ",", "stride", "=", "4", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "4", ",", "4", "]", ",", "stride", "=", "2", ")", "\n", "net", "=", "slim", ".", "conv2d", "(", "net", ",", "64", ",", "[", "3", ",", "3", "]", ",", "stride", "=", "1", ")", "\n", "net", "=", "slim", ".", "flatten", "(", "net", ")", "\n", "net", "=", "slim", ".", "fully_connected", "(", "net", ",", "512", ")", "\n", "q_values", "=", "slim", ".", "fully_connected", "(", "net", ",", "self", ".", "num_actions", ",", "activation_fn", "=", "None", ")", "\n", "return", "self", ".", "_get_network_type", "(", ")", "(", "q_values", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_networks": [[225, 259], ["tensorflow.make_template", "tensorflow.make_template", "dqnrpg_agent.DQNRPGAgent.online_convnet", "dqnrpg_agent.DQNRPGAgent.online_convnet", "dqnrpg_agent.DQNRPGAgent.target_convnet", "tensorflow.make_template", "dqnrpg_agent.DQNRPGAgent.rpg_convnet", "dqnrpg_agent.DQNRPGAgent.rpg_convnet", "tensorflow.argmax", "tensorflow.argmax"], "methods", ["None"], ["", "def", "_build_networks", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds the Q-value network computations needed for acting and training.\n\n        These are:\n          self.online_convnet: For computing the current state's Q-values.\n          self.target_convnet: For computing the next state's target Q-values.\n          self._net_outputs: The actual Q-values.\n          self._q_argmax: The action maximizing the current state's Q-values.\n          self._replay_net_outputs: The replayed states' Q-values.\n          self._replay_next_target_net_outputs: The replayed next states' target\n            Q-values (see Mnih et al., 2015 for details).\n        \"\"\"", "\n", "# Calling online_convnet will generate a new graph as defined in", "\n", "# self._get_network_template using whatever input is passed, but will always", "\n", "# share the same weights.", "\n", "# DQN explore net.", "\n", "self", ".", "online_convnet", "=", "tf", ".", "make_template", "(", "'Online'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "target_convnet", "=", "tf", ".", "make_template", "(", "'Target'", ",", "self", ".", "_network_template", ")", "\n", "\n", "self", ".", "_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "state_ph", ")", "\n", "# TODO(bellemare): Ties should be broken. They are unlikely to happen when", "\n", "# using a deep network, but may affect performance with a linear", "\n", "# approximation scheme.", "\n", "self", ".", "_q_argmax", "=", "tf", ".", "argmax", "(", "self", ".", "_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "\n", "self", ".", "_replay_net_outputs", "=", "self", ".", "online_convnet", "(", "self", ".", "_replay", ".", "states", ")", "\n", "self", ".", "_replay_next_target_net_outputs", "=", "self", ".", "target_convnet", "(", "\n", "self", ".", "_replay", ".", "next_states", ")", "\n", "\n", "# RPG learning net.", "\n", "self", ".", "rpg_convnet", "=", "tf", ".", "make_template", "(", "'RPG'", ",", "self", ".", "_network_template", ")", "\n", "self", ".", "_rpg_net_outputs", "=", "self", ".", "rpg_convnet", "(", "self", ".", "state_ph", ")", "\n", "self", ".", "_q_argmax_rpg", "=", "tf", ".", "argmax", "(", "self", ".", "_rpg_net_outputs", ".", "q_values", ",", "axis", "=", "1", ")", "[", "0", "]", "\n", "self", ".", "_replay_rpg_net_outputs", "=", "self", ".", "rpg_convnet", "(", "self", ".", "_replay_opt", ".", "states", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_replay_buffer": [[263, 280], ["dopamine.replay_memory.circular_replay_buffer.WrappedReplayBuffer"], "methods", ["None"], ["", "def", "_build_replay_buffer", "(", "self", ",", "use_staging", ")", ":", "\n", "        ", "\"\"\"Creates the replay buffer used by the agent.\n\n        Args:\n          use_staging: bool, if True, uses a staging area to prefetch data for\n            faster training.\n\n        Returns:\n          A WrapperReplayBuffer object.\n        \"\"\"", "\n", "return", "circular_replay_buffer", ".", "WrappedReplayBuffer", "(", "\n", "observation_shape", "=", "self", ".", "observation_shape", ",", "\n", "stack_size", "=", "self", ".", "stack_size", ",", "\n", "use_staging", "=", "use_staging", ",", "\n", "update_horizon", "=", "self", ".", "update_horizon", ",", "\n", "gamma", "=", "self", ".", "gamma", ",", "\n", "observation_dtype", "=", "self", ".", "observation_dtype", ".", "as_numpy_dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_target_q_op": [[281, 299], ["tensorflow.reduce_max", "tensorflow.cast"], "methods", ["None"], ["", "def", "_build_target_q_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Build an op used as a target for the Q-value.\n\n        Returns:\n          target_q_op: An op calculating the Q-value.\n        \"\"\"", "\n", "# Get the maximum Q-value across the actions dimension.", "\n", "replay_next_qt_max", "=", "tf", ".", "reduce_max", "(", "\n", "self", ".", "_replay_next_target_net_outputs", ".", "q_values", ",", "1", ")", "\n", "# Calculate the Bellman target value.", "\n", "#   Q_t = R_t + \\gamma^N * Q'_t+1", "\n", "# where,", "\n", "#   Q'_t+1 = \\argmax_a Q(S_t+1, a)", "\n", "#          (or) 0 if S_t is a terminal state,", "\n", "# and", "\n", "#   N is the update horizon (by default, N=1).", "\n", "return", "self", ".", "_replay", ".", "rewards", "+", "self", ".", "cumulative_gamma", "*", "replay_next_qt_max", "*", "(", "\n", "1.", "-", "tf", ".", "cast", "(", "self", ".", "_replay", ".", "terminals", ",", "tf", ".", "float32", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_train_op": [[300, 340], ["tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.maximum", "tensorflow.reduce_mean", "tensorflow.one_hot", "tensorflow.reduce_sum", "tensorflow.stop_gradient", "tensorflow.losses.huber_loss", "tensorflow.reduce_mean", "dqnrpg_agent.DQNRPGAgent._build_target_q_op", "dqnrpg_agent.DQNRPGAgent.optimizer.minimize", "dqnrpg_agent.DQNRPGAgent.optimizer_rpg.minimize", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.reshape", "tensorflow.ones"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_target_q_op"], ["", "def", "_build_train_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds a training op.\n\n        Returns:\n          train_op: An op performing one step of training from replay data.\n        \"\"\"", "\n", "\n", "# RPG loss", "\n", "replay_opt_action_one_hot", "=", "tf", ".", "one_hot", "(", "\n", "self", ".", "_replay_opt", ".", "actions", ",", "self", ".", "num_actions", ",", "1.", ",", "0.", ",", "name", "=", "'action_one_hot_rpg'", ")", "\n", "replay_chosen_q_rpg", "=", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "_replay_rpg_net_outputs", ".", "q_values", "*", "replay_opt_action_one_hot", ",", "\n", "reduction_indices", "=", "1", ",", "\n", "name", "=", "'replay_chosen_q_rpg'", ")", "\n", "margin", "=", "1", "\n", "qvalue", "=", "self", ".", "_replay_rpg_net_outputs", ".", "q_values", "\n", "# debug self.temp_action_one_hot = replay_action_one_hot", "\n", "# self.temp_qvalue = qvalue", "\n", "self", ".", "temp1", "=", "(", "qvalue", "+", "margin", ")", "*", "(", "1", "-", "replay_opt_action_one_hot", ")", "+", "qvalue", "*", "replay_opt_action_one_hot", "\n", "self", ".", "temp2", "=", "-", "(", "tf", ".", "reshape", "(", "replay_chosen_q_rpg", ",", "[", "-", "1", ",", "1", "]", ")", "*", "tf", ".", "ones", "(", "[", "1", ",", "self", ".", "num_actions", "]", ")", ")", "*", "(", "(", "1", "-", "replay_opt_action_one_hot", ")", "+", "(", "replay_opt_action_one_hot", ")", ")", "\n", "self", ".", "hingeloss", "=", "tf", ".", "maximum", "(", "0.0", ",", "self", ".", "temp1", "+", "self", ".", "temp2", ")", "\n", "rpg_loss", "=", "tf", ".", "reduce_mean", "(", "self", ".", "hingeloss", ")", "\n", "\n", "# DQN loss", "\n", "replay_action_one_hot", "=", "tf", ".", "one_hot", "(", "\n", "self", ".", "_replay", ".", "actions", ",", "self", ".", "num_actions", ",", "1.", ",", "0.", ",", "name", "=", "'action_one_hot'", ")", "\n", "replay_chosen_q", "=", "tf", ".", "reduce_sum", "(", "\n", "self", ".", "_replay_net_outputs", ".", "q_values", "*", "replay_action_one_hot", ",", "\n", "reduction_indices", "=", "1", ",", "\n", "name", "=", "'replay_chosen_q'", ")", "\n", "target", "=", "tf", ".", "stop_gradient", "(", "self", ".", "_build_target_q_op", "(", ")", ")", "\n", "loss", "=", "tf", ".", "losses", ".", "huber_loss", "(", "\n", "target", ",", "replay_chosen_q", ",", "reduction", "=", "tf", ".", "losses", ".", "Reduction", ".", "NONE", ")", "\n", "mean_loss", "=", "tf", ".", "reduce_mean", "(", "loss", ")", "\n", "if", "self", ".", "summary_writer", "is", "not", "None", ":", "\n", "            ", "with", "tf", ".", "variable_scope", "(", "'Losses'", ")", ":", "\n", "                ", "tf", ".", "summary", ".", "scalar", "(", "'HuberLoss'", ",", "mean_loss", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "\"hingeLossRPG\"", ",", "rpg_loss", ")", "\n", "", "", "return", "self", ".", "optimizer", ".", "minimize", "(", "mean_loss", ")", ",", "self", ".", "optimizer_rpg", ".", "minimize", "(", "rpg_loss", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._build_sync_op": [[341, 357], ["tensorflow.get_collection", "tensorflow.get_collection", "zip", "sync_qt_ops.append", "w_target.assign"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "_build_sync_op", "(", "self", ")", ":", "\n", "        ", "\"\"\"Builds ops for assigning weights from online to target network.\n\n        Returns:\n          ops: A list of ops assigning weights from online to target network.\n        \"\"\"", "\n", "# Get trainable variables from online and target DQNs", "\n", "sync_qt_ops", "=", "[", "]", "\n", "trainables_online", "=", "tf", ".", "get_collection", "(", "\n", "tf", ".", "GraphKeys", ".", "TRAINABLE_VARIABLES", ",", "scope", "=", "'Online'", ")", "\n", "trainables_target", "=", "tf", ".", "get_collection", "(", "\n", "tf", ".", "GraphKeys", ".", "TRAINABLE_VARIABLES", ",", "scope", "=", "'Target'", ")", "\n", "for", "(", "w_online", ",", "w_target", ")", "in", "zip", "(", "trainables_online", ",", "trainables_target", ")", ":", "\n", "# Assign weights from online to target network.", "\n", "            ", "sync_qt_ops", ".", "append", "(", "w_target", ".", "assign", "(", "w_online", ",", "use_locking", "=", "True", ")", ")", "\n", "", "return", "sync_qt_ops", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.begin_episode": [[358, 375], ["dqnrpg_agent.DQNRPGAgent._reset_state", "dqnrpg_agent.DQNRPGAgent._record_observation", "dqnrpg_agent.DQNRPGAgent._select_action", "dqnrpg_agent.DQNRPGAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._reset_state", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step"], ["", "def", "begin_episode", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Returns the agent's first action for this episode.\n\n        Args:\n          observation: numpy array, the environment's initial observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_reset_state", "(", ")", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.step": [[376, 399], ["dqnrpg_agent.DQNRPGAgent._record_observation", "dqnrpg_agent.DQNRPGAgent._select_action", "dqnrpg_agent.DQNRPGAgent._store_transition", "dqnrpg_agent.DQNRPGAgent.replay_buffer_temp.add", "dqnrpg_agent.DQNRPGAgent._train_step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step"], ["", "def", "step", "(", "self", ",", "reward", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records the most recent transition and returns the agent's next action.\n\n        We store the observation of the last time step since we want to store it\n        with the reward.\n\n        Args:\n          reward: float, the reward received from the agent's most recent action.\n          observation: numpy array, the most recent observation.\n\n        Returns:\n          int, the selected action.\n        \"\"\"", "\n", "self", ".", "_last_observation", "=", "self", ".", "_observation", "\n", "self", ".", "_record_observation", "(", "observation", ")", "\n", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "_store_transition", "(", "self", ".", "_last_observation", ",", "self", ".", "action", ",", "reward", ",", "False", ")", "\n", "self", ".", "replay_buffer_temp", ".", "add", "(", "self", ".", "_last_observation", ",", "self", ".", "action", ",", "reward", ",", "False", ")", "\n", "self", ".", "_train_step", "(", ")", "\n", "\n", "", "self", ".", "action", "=", "self", ".", "_select_action", "(", ")", "\n", "return", "self", ".", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.end_episode": [[400, 412], ["dqnrpg_agent.DQNRPGAgent.replay_buffer_temp.clear", "dqnrpg_agent.DQNRPGAgent._store_transition"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.clear", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition"], ["", "def", "end_episode", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"Signals the end of the episode to the agent.\n\n        We store the observation of the current time step, which is the last\n        observation of the episode.\n\n        Args:\n          reward: float, the last reward from the environment.\n        \"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "            ", "self", ".", "replay_buffer_temp", ".", "clear", "(", ")", "# this episode is not optimal", "\n", "self", ".", "_store_transition", "(", "self", ".", "_observation", ",", "self", ".", "action", ",", "reward", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.end_episode_": [[413, 428], ["dqnrpg_agent.DQNRPGAgent._store_transition", "dqnrpg_agent.DQNRPGAgent.replay_buffer_temp.add", "dqnrpg_agent.DQNRPGAgent.replay_buffer_temp.get_sample", "dqnrpg_agent.DQNRPGAgent._replay_opt.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.get_sample", "home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "", "def", "end_episode_", "(", "self", ",", "reward", ",", "total_reward", ",", "step_number", ")", ":", "\n", "        ", "\"\"\" This episodes is optimal trajectory \"\"\"", "\n", "if", "not", "self", ".", "eval_mode", ":", "\n", "# for DQN", "\n", "            ", "self", ".", "_store_transition", "(", "self", ".", "_observation", ",", "self", ".", "action", ",", "reward", ",", "True", ")", "\n", "\n", "# replay buffer for RPG.", "\n", "self", ".", "replay_buffer_temp", ".", "add", "(", "self", ".", "_observation", ",", "self", ".", "action", ",", "reward", ",", "True", ")", "\n", "count", "=", "step_number", "\n", "while", "count", ">", "0", ":", "\n", "                ", "experience", "=", "self", ".", "replay_buffer_temp", ".", "get_sample", "(", ")", "\n", "state", ",", "action", ",", "reward", ",", "_", "=", "experience", "\n", "count", "-=", "1", "\n", "# self.replay_buffer_opt.add(state, action, reward, False)", "\n", "self", ".", "_replay_opt", ".", "add", "(", "state", ",", "action", ",", "reward", ",", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._select_action": [[429, 453], ["dqnrpg_agent.DQNRPGAgent.epsilon_fn", "dqnrpg_agent.DQNRPGAgent._sess.run", "random.random", "random.randint", "dqnrpg_agent.DQNRPGAgent._sess.run"], "methods", ["None"], ["", "", "", "def", "_select_action", "(", "self", ")", ":", "\n", "        ", "\"\"\"Select an action from the set of available actions.\n\n        Chooses an action randomly with probability self._calculate_epsilon(), and\n        otherwise acts greedily according to the current Q-value estimates.\n\n        Returns:\n           int, the selected action.\n        \"\"\"", "\n", "if", "self", ".", "eval_mode", "is", "not", "True", ":", "\n", "            ", "epsilon", "=", "self", ".", "epsilon_fn", "(", "\n", "self", ".", "epsilon_decay_period", ",", "\n", "self", ".", "training_steps", ",", "\n", "self", ".", "min_replay_history", ",", "\n", "self", ".", "epsilon_train", ")", "\n", "if", "random", ".", "random", "(", ")", "<=", "epsilon", ":", "\n", "# Choose a random action with probability epsilon.", "\n", "                ", "return", "random", ".", "randint", "(", "0", ",", "self", ".", "num_actions", "-", "1", ")", "\n", "", "else", ":", "\n", "# Choose the action with highest Q-value at the current state.", "\n", "                ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "", "", "else", ":", "\n", "# evaluation mode: use rpg.", "\n", "            ", "return", "self", ".", "_sess", ".", "run", "(", "self", ".", "_q_argmax_rpg", ",", "{", "self", ".", "state_ph", ":", "self", ".", "state", "}", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._train_step": [[454, 482], ["dqnrpg_agent.DQNRPGAgent._sess.run", "dqnrpg_agent.DQNRPGAgent._sess.run", "dqnrpg_agent.DQNRPGAgent._sess.run", "dqnrpg_agent.DQNRPGAgent._sess.run", "dqnrpg_agent.DQNRPGAgent.summary_writer.add_summary"], "methods", ["None"], ["", "", "def", "_train_step", "(", "self", ")", ":", "\n", "        ", "\"\"\"Runs a single training step.\n\n        Runs a training op if both:\n          (1) A minimum number of frames have been added to the replay buffer.\n          (2) `training_steps` is a multiple of `update_period`.\n\n        Also, syncs weights from online to target network if training steps is a\n        multiple of target update period.\n        \"\"\"", "\n", "# Run a train op at the rate of self.update_period if enough training steps", "\n", "# have been run. This matches the Nature DQN behaviour.", "\n", "if", "self", ".", "_replay", ".", "memory", ".", "add_count", ">", "self", ".", "min_replay_history", ":", "\n", "            ", "if", "self", ".", "training_steps", "%", "self", ".", "update_period", "==", "0", ":", "\n", "                ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_op", ")", "\n", "if", "self", ".", "_replay_opt", ".", "memory", ".", "add_count", ">", "self", ".", "start_training", ":", "\n", "                    ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_train_op_rpg", ")", "\n", "\n", "", "if", "(", "self", ".", "summary_writer", "is", "not", "None", "and", "\n", "self", ".", "training_steps", ">", "0", "and", "\n", "self", ".", "training_steps", "%", "self", ".", "summary_writing_frequency", "==", "0", ")", ":", "\n", "                    ", "summary", "=", "self", ".", "_sess", ".", "run", "(", "self", ".", "_merged_summaries", ")", "\n", "self", ".", "summary_writer", ".", "add_summary", "(", "summary", ",", "self", ".", "training_steps", ")", "\n", "\n", "", "", "if", "self", ".", "training_steps", "%", "self", ".", "target_update_period", "==", "0", ":", "\n", "                ", "self", ".", "_sess", ".", "run", "(", "self", ".", "_sync_qt_ops", ")", "\n", "\n", "", "", "self", ".", "training_steps", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._record_observation": [[483, 500], ["numpy.reshape", "numpy.reshape", "numpy.roll"], "methods", ["None"], ["", "def", "_record_observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Records an observation and update state.\n\n        Extracts a frame from the observation vector and overwrites the oldest\n        frame in the state buffer.\n\n        Args:\n          observation: numpy array, an observation from the environment.\n        \"\"\"", "\n", "# Set current observation. We do the reshaping to handle environments", "\n", "# without frame stacking.", "\n", "observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "self", ".", "_observation", "=", "observation", "[", "...", ",", "0", "]", "\n", "self", ".", "_observation", "=", "np", ".", "reshape", "(", "observation", ",", "self", ".", "observation_shape", ")", "\n", "# Swap out the oldest frame with the current frame.", "\n", "self", ".", "state", "=", "np", ".", "roll", "(", "self", ".", "state", ",", "-", "1", ",", "axis", "=", "-", "1", ")", "\n", "self", ".", "state", "[", "0", ",", "...", ",", "-", "1", "]", "=", "self", ".", "_observation", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._store_transition": [[501, 518], ["dqnrpg_agent.DQNRPGAgent._replay.add"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.repg.repg_agent.ReplayBufferRegular.add"], ["", "def", "_store_transition", "(", "self", ",", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", ":", "\n", "        ", "\"\"\"Stores an experienced transition.\n\n        Executes a tf session and executes replay buffer ops in order to store the\n        following tuple in the replay buffer:\n          (last_observation, action, reward, is_terminal).\n\n        Pedantically speaking, this does not actually store an entire transition\n        since the next state is recorded on the following time step.\n\n        Args:\n          last_observation: numpy array, last observation.\n          action: int, the action taken.\n          reward: float, the reward.\n          is_terminal: bool, indicating if the current state is a terminal state.\n        \"\"\"", "\n", "self", ".", "_replay", ".", "add", "(", "last_observation", ",", "action", ",", "reward", ",", "is_terminal", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent._reset_state": [[519, 522], ["dqnrpg_agent.DQNRPGAgent.state.fill"], "methods", ["None"], ["", "def", "_reset_state", "(", "self", ")", ":", "\n", "        ", "\"\"\"Resets the agent state by filling it with zeros.\"\"\"", "\n", "self", ".", "state", ".", "fill", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.bundle_and_checkpoint": [[523, 553], ["dqnrpg_agent.DQNRPGAgent._saver.save", "dqnrpg_agent.DQNRPGAgent._replay.save", "tensorflow.gfile.Exists", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save", "home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.save"], ["", "def", "bundle_and_checkpoint", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ")", ":", "\n", "        ", "\"\"\"Returns a self-contained bundle of the agent's state.\n\n        This is used for checkpointing. It will return a dictionary containing all\n        non-TensorFlow objects (to be saved into a file by the caller), and it saves\n        all TensorFlow objects into a checkpoint file.\n\n        Args:\n          checkpoint_dir: str, directory where TensorFlow objects will be saved.\n          iteration_number: int, iteration number to use for naming the checkpoint\n            file.\n\n        Returns:\n          A dict containing additional Python objects to be checkpointed by the\n            experiment. If the checkpoint directory does not exist, returns None.\n        \"\"\"", "\n", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "checkpoint_dir", ")", ":", "\n", "            ", "return", "None", "\n", "# Call the Tensorflow saver to checkpoint the graph.", "\n", "", "self", ".", "_saver", ".", "save", "(", "\n", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'tf_ckpt'", ")", ",", "\n", "global_step", "=", "iteration_number", ")", "\n", "# Checkpoint the out-of-graph replay buffer.", "\n", "self", ".", "_replay", ".", "save", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "bundle_dictionary", "=", "{", "}", "\n", "bundle_dictionary", "[", "'state'", "]", "=", "self", ".", "state", "\n", "bundle_dictionary", "[", "'eval_mode'", "]", "=", "self", ".", "eval_mode", "\n", "bundle_dictionary", "[", "'training_steps'", "]", "=", "self", ".", "training_steps", "\n", "return", "bundle_dictionary", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.unbundle": [[554, 586], ["dqnrpg_agent.DQNRPGAgent._saver.restore", "dqnrpg_agent.DQNRPGAgent._replay.load", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load"], ["", "def", "unbundle", "(", "self", ",", "checkpoint_dir", ",", "iteration_number", ",", "bundle_dictionary", ")", ":", "\n", "        ", "\"\"\"Restores the agent from a checkpoint.\n\n        Restores the agent's Python objects to those specified in bundle_dictionary,\n        and restores the TensorFlow objects to those specified in the\n        checkpoint_dir. If the checkpoint_dir does not exist, will not reset the\n          agent's state.\n\n        Args:\n          checkpoint_dir: str, path to the checkpoint saved by tf.Save.\n          iteration_number: int, checkpoint version, used when restoring replay\n            buffer.\n          bundle_dictionary: dict, containing additional Python objects owned by\n            the agent.\n\n        Returns:\n          bool, True if unbundling was successful.\n        \"\"\"", "\n", "try", ":", "\n", "# self._replay.load() will throw a NotFoundError if it does not find all", "\n", "# the necessary files, in which case we abort the process & return False.", "\n", "            ", "self", ".", "_replay", ".", "load", "(", "checkpoint_dir", ",", "iteration_number", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "            ", "return", "False", "\n", "", "for", "key", "in", "self", ".", "__dict__", ":", "\n", "            ", "if", "key", "in", "bundle_dictionary", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "bundle_dictionary", "[", "key", "]", "\n", "# Restore the agent's TensorFlow graph.", "\n", "", "", "self", ".", "_saver", ".", "restore", "(", "self", ".", "_sess", ",", "\n", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "\n", "'tf_ckpt-{}'", ".", "format", "(", "iteration_number", ")", ")", ")", "\n", "return", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.linearly_decaying_epsilon": [[39, 61], ["numpy.clip"], "function", ["None"], ["def", "linearly_decaying_epsilon", "(", "decay_period", ",", "step", ",", "warmup_steps", ",", "epsilon", ")", ":", "\n", "    ", "\"\"\"Returns the current epsilon for the agent's epsilon-greedy policy.\n\n    This follows the Nature DQN schedule of a linearly decaying epsilon (Mnih et\n    al., 2015). The schedule is as follows:\n      Begin at 1. until warmup_steps steps have been taken; then\n      Linearly decay epsilon from 1. to epsilon in decay_period steps; and then\n      Use epsilon from there on.\n\n    Args:\n      decay_period: float, the period over which epsilon is decayed.\n      step: int, the number of training steps completed so far.\n      warmup_steps: int, the number of steps taken before epsilon is decayed.\n      epsilon: float, the final value to which to decay the epsilon parameter.\n\n    Returns:\n      A float, the current epsilon value computed according to the schedule.\n    \"\"\"", "\n", "steps_left", "=", "decay_period", "+", "warmup_steps", "-", "step", "\n", "bonus", "=", "(", "1.0", "-", "epsilon", ")", "*", "steps_left", "/", "decay_period", "\n", "bonus", "=", "np", ".", "clip", "(", "bonus", ",", "0.", ",", "1.", "-", "epsilon", ")", "\n", "return", "epsilon", "+", "bonus", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.train.create_agent": [[78, 129], ["dopamine.agents.dqn.dqn_agent.DQNAgent", "dopamine.agents.rainbow.rainbow_agent.RainbowAgent", "dopamine.agents.implicit_quantile.implicit_quantile_agent.ImplicitQuantileAgent", "dopamine.agents.rpg.rpg_agent.RPGAgent", "dopamine.agents.epg.epg_agent.EPGAgent", "dopamine.agents.lpg.lpg_agent.LPGAgent", "dopamine.agents.repg.repg_agent.REPGAgent", "dopamine.agents.dqnrpg.dqnrpg_agent.DQNRPGAgent", "dopamine.agents.rainbowrpg.rainbowrpg_agent.RainbowRPGAgent", "dopamine.agents.implicit_quantilerpg.implicit_quantilerpg_agent.ImplicitQuantileRPGAgent", "ValueError"], "function", ["None"], ["def", "create_agent", "(", "sess", ",", "environment", ",", "summary_writer", "=", "None", ")", ":", "\n", "  ", "\"\"\"Creates a DQN agent.\n\n  Args:\n    sess: A `tf.Session` object for running associated ops.\n    environment: An Atari 2600 Gym environment.\n    summary_writer: A Tensorflow summary writer to pass to the agent\n      for in-agent training statistics in Tensorboard.\n\n  Returns:\n    agent: An RL agent.\n\n  Raises:\n    ValueError: If `agent_name` is not in supported list.\n  \"\"\"", "\n", "if", "not", "FLAGS", ".", "debug_mode", ":", "\n", "    ", "summary_writer", "=", "None", "\n", "", "if", "FLAGS", ".", "agent_name", "==", "'dqn'", ":", "\n", "    ", "return", "dqn_agent", ".", "DQNAgent", "(", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "elif", "FLAGS", ".", "agent_name", "==", "'rainbow'", ":", "\n", "    ", "return", "rainbow_agent", ".", "RainbowAgent", "(", "\n", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "elif", "FLAGS", ".", "agent_name", "==", "'implicit_quantile'", ":", "\n", "    ", "return", "implicit_quantile_agent", ".", "ImplicitQuantileAgent", "(", "\n", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "elif", "FLAGS", ".", "agent_name", "==", "'rpg'", ":", "\n", "    ", "return", "rpg_agent", ".", "RPGAgent", "(", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "elif", "FLAGS", ".", "agent_name", "==", "'epg'", ":", "\n", "    ", "return", "epg_agent", ".", "EPGAgent", "(", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "elif", "FLAGS", ".", "agent_name", "==", "'lpg'", ":", "\n", "    ", "return", "lpg_agent", ".", "LPGAgent", "(", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "elif", "FLAGS", ".", "agent_name", "==", "'repg'", ":", "\n", "    ", "return", "repg_agent", ".", "REPGAgent", "(", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "elif", "FLAGS", ".", "agent_name", "==", "'dqnrpg'", ":", "\n", "    ", "return", "dqnrpg_agent", ".", "DQNRPGAgent", "(", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "elif", "FLAGS", ".", "agent_name", "==", "'rainbowrpg'", ":", "\n", "    ", "return", "rainbowrpg_agent", ".", "RainbowRPGAgent", "(", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "elif", "FLAGS", ".", "agent_name", "==", "'implicit_quantilerpg'", ":", "\n", "    ", "return", "implicit_quantilerpg_agent", ".", "ImplicitQuantileRPGAgent", "(", "sess", ",", "num_actions", "=", "environment", ".", "action_space", ".", "n", ",", "\n", "summary_writer", "=", "summary_writer", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "'Unknown agent: {}'", ".", "format", "(", "FLAGS", ".", "agent_name", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.train.create_runner": [[131, 155], ["dopamine.atari.run_experiment.Runner", "dopamine.atari.run_experiment.TrainRunner", "ValueError"], "function", ["None"], ["", "", "def", "create_runner", "(", "base_dir", ",", "create_agent_fn", ",", "random_seed", ",", "agent_name", ",", "game_name", ",", "num_iterations", ")", ":", "\n", "  ", "\"\"\"Creates an experiment Runner.\n\n  Args:\n    base_dir: str, base directory for hosting all subdirectories.\n    create_agent_fn: A function that takes as args a Tensorflow session and an\n     Atari 2600 Gym environment, and returns an agent.\n\n  Returns:\n    runner: A `run_experiment.Runner` like object.\n\n  Raises:\n    ValueError: When an unknown schedule is encountered.\n  \"\"\"", "\n", "assert", "base_dir", "is", "not", "None", "\n", "# Continuously runs training and evaluation until max num_iterations is hit.", "\n", "if", "FLAGS", ".", "schedule", "==", "'continuous_train_and_eval'", ":", "\n", "    ", "return", "run_experiment", ".", "Runner", "(", "base_dir", ",", "create_agent_fn", ",", "random_seed", ",", "\n", "agent_name", ",", "game_name", ",", "num_iterations", ")", "\n", "# Continuously runs training until max num_iterations is hit.", "\n", "", "elif", "FLAGS", ".", "schedule", "==", "'continuous_train'", ":", "\n", "    ", "return", "run_experiment", ".", "TrainRunner", "(", "base_dir", ",", "create_agent_fn", ")", "\n", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "'Unknown schedule: {}'", ".", "format", "(", "FLAGS", ".", "schedule", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.train.launch_experiment": [[157, 171], ["dopamine.atari.run_experiment.load_gin_configs", "create_runner_fn.run_experiment", "train.create_runner", "train.create_agent"], "function", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.load_gin_configs", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner.run_experiment", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.train.create_runner", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.train.create_agent"], ["", "", "def", "launch_experiment", "(", "create_runner_fn", ",", "create_agent_fn", ")", ":", "\n", "  ", "\"\"\"Launches the experiment.\n\n  Args:\n    create_runner_fn: A function that takes as args a base directory and a\n      function for creating an agent and returns a `Runner`-like object.\n    create_agent_fn: A function that takes as args a Tensorflow session and an\n     Atari 2600 Gym environment, and returns an agent.\n  \"\"\"", "\n", "run_experiment", ".", "load_gin_configs", "(", "FLAGS", ".", "gin_files", ",", "FLAGS", ".", "gin_bindings", ")", "\n", "runner", "=", "create_runner_fn", "(", "FLAGS", ".", "base_dir", ",", "create_agent_fn", ",", "\n", "FLAGS", ".", "random_seed", ",", "FLAGS", ".", "agent_name", ",", "\n", "FLAGS", ".", "game_name", ",", "FLAGS", ".", "num_iterations", ")", "\n", "runner", ".", "run_experiment", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.train.main": [[173, 181], ["tensorflow.logging.set_verbosity", "train.launch_experiment"], "function", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.train.launch_experiment"], ["", "def", "main", "(", "unused_argv", ")", ":", "\n", "  ", "\"\"\"Main method.\n\n  Args:\n    unused_argv: Arguments (unused).\n  \"\"\"", "\n", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "launch_experiment", "(", "create_runner", ",", "create_agent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing.__init__": [[50, 86], ["ValueError", "ValueError", "numpy.empty", "numpy.empty"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "environment", ",", "frame_skip", "=", "4", ",", "terminal_on_life_loss", "=", "False", ",", "\n", "screen_size", "=", "84", ")", ":", "\n", "    ", "\"\"\"Constructor for an Atari 2600 preprocessor.\n\n    Args:\n      environment: Gym environment whose observations are preprocessed.\n      frame_skip: int, the frequency at which the agent experiences the game.\n      terminal_on_life_loss: bool, If True, the step() method returns\n        is_terminal=True whenever a life is lost. See Mnih et al. 2015.\n      screen_size: int, size of a resized Atari 2600 frame.\n\n    Raises:\n      ValueError: if frame_skip or screen_size are not strictly positive.\n    \"\"\"", "\n", "if", "frame_skip", "<=", "0", ":", "\n", "      ", "raise", "ValueError", "(", "'Frame skip should be strictly positive, got {}'", ".", "\n", "format", "(", "frame_skip", ")", ")", "\n", "", "if", "screen_size", "<=", "0", ":", "\n", "      ", "raise", "ValueError", "(", "'Target screen size should be strictly positive, got {}'", ".", "\n", "format", "(", "screen_size", ")", ")", "\n", "\n", "", "self", ".", "environment", "=", "environment", "\n", "self", ".", "terminal_on_life_loss", "=", "terminal_on_life_loss", "\n", "self", ".", "frame_skip", "=", "frame_skip", "\n", "self", ".", "screen_size", "=", "screen_size", "\n", "\n", "obs_dims", "=", "self", ".", "environment", ".", "observation_space", "\n", "# Stores temporary observations used for pooling over two successive", "\n", "# frames.", "\n", "self", ".", "screen_buffer", "=", "[", "\n", "np", ".", "empty", "(", "(", "obs_dims", ".", "shape", "[", "0", "]", ",", "obs_dims", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "np", ".", "uint8", ")", ",", "\n", "np", ".", "empty", "(", "(", "obs_dims", ".", "shape", "[", "0", "]", ",", "obs_dims", ".", "shape", "[", "1", "]", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "]", "\n", "\n", "self", ".", "game_over", "=", "False", "\n", "self", ".", "lives", "=", "0", "# Will need to be set by reset().", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing.observation_space": [[87, 93], ["gym.spaces.box.Box"], "methods", ["None"], ["", "@", "property", "\n", "def", "observation_space", "(", "self", ")", ":", "\n", "# Return the observation space adjusted to match the shape of the processed", "\n", "# observations.", "\n", "    ", "return", "Box", "(", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "self", ".", "screen_size", ",", "self", ".", "screen_size", ",", "1", ")", ",", "\n", "dtype", "=", "np", ".", "uint8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing.action_space": [[94, 97], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "action_space", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "environment", ".", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing.reward_range": [[98, 101], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "reward_range", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "environment", ".", "reward_range", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing.metadata": [[102, 105], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "metadata", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "environment", ".", "metadata", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing.reset": [[106, 118], ["preprocessing.AtariPreprocessing.environment.reset", "preprocessing.AtariPreprocessing.environment.ale.lives", "preprocessing.AtariPreprocessing._fetch_grayscale_observation", "preprocessing.AtariPreprocessing.screen_buffer[].fill", "preprocessing.AtariPreprocessing._pool_and_resize"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.reset", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing._fetch_grayscale_observation", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing._pool_and_resize"], ["", "def", "reset", "(", "self", ")", ":", "\n", "    ", "\"\"\"Resets the environment.\n\n    Returns:\n      observation: numpy array, the initial observation emitted by the\n        environment.\n    \"\"\"", "\n", "self", ".", "environment", ".", "reset", "(", ")", "\n", "self", ".", "lives", "=", "self", ".", "environment", ".", "ale", ".", "lives", "(", ")", "\n", "self", ".", "_fetch_grayscale_observation", "(", "self", ".", "screen_buffer", "[", "0", "]", ")", "\n", "self", ".", "screen_buffer", "[", "1", "]", ".", "fill", "(", "0", ")", "\n", "return", "self", ".", "_pool_and_resize", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing.render": [[119, 135], ["preprocessing.AtariPreprocessing.environment.render"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing.render"], ["", "def", "render", "(", "self", ",", "mode", ")", ":", "\n", "    ", "\"\"\"Renders the current screen, before preprocessing.\n\n    This calls the Gym API's render() method.\n\n    Args:\n      mode: Mode argument for the environment's render() method.\n        Valid values (str) are:\n          'rgb_array': returns the raw ALE image.\n          'human': renders to display via the Gym renderer.\n\n    Returns:\n      if mode='rgb_array': numpy array, the most recent screen.\n      if mode='human': bool, whether the rendering was successful.\n    \"\"\"", "\n", "return", "self", ".", "environment", ".", "render", "(", "mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing.step": [[136, 184], ["range", "preprocessing.AtariPreprocessing._pool_and_resize", "preprocessing.AtariPreprocessing.environment.step", "preprocessing.AtariPreprocessing.environment.ale.lives", "preprocessing.AtariPreprocessing._fetch_grayscale_observation"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing._pool_and_resize", "home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.step", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing._fetch_grayscale_observation"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "    ", "\"\"\"Applies the given action in the environment.\n\n    Remarks:\n\n      * If a terminal state (from life loss or episode end) is reached, this may\n        execute fewer than self.frame_skip steps in the environment.\n      * Furthermore, in this case the returned observation may not contain valid\n        image data and should be ignored.\n\n    Args:\n      action: The action to be executed.\n\n    Returns:\n      observation: numpy array, the observation following the action.\n      reward: float, the reward following the action.\n      is_terminal: bool, whether the environment has reached a terminal state.\n        This is true when a life is lost and terminal_on_life_loss, or when the\n        episode is over.\n      info: Gym API's info data structure.\n    \"\"\"", "\n", "accumulated_reward", "=", "0.", "\n", "\n", "for", "time_step", "in", "range", "(", "self", ".", "frame_skip", ")", ":", "\n", "# We bypass the Gym observation altogether and directly fetch the", "\n", "# grayscale image from the ALE. This is a little faster.", "\n", "      ", "_", ",", "reward", ",", "game_over", ",", "info", "=", "self", ".", "environment", ".", "step", "(", "action", ")", "\n", "accumulated_reward", "+=", "reward", "\n", "\n", "if", "self", ".", "terminal_on_life_loss", ":", "\n", "        ", "new_lives", "=", "self", ".", "environment", ".", "ale", ".", "lives", "(", ")", "\n", "is_terminal", "=", "game_over", "or", "new_lives", "<", "self", ".", "lives", "\n", "self", ".", "lives", "=", "new_lives", "\n", "", "else", ":", "\n", "        ", "is_terminal", "=", "game_over", "\n", "\n", "", "if", "is_terminal", ":", "\n", "        ", "break", "\n", "# We max-pool over the last two frames, in grayscale.", "\n", "", "elif", "time_step", ">=", "self", ".", "frame_skip", "-", "2", ":", "\n", "        ", "t", "=", "time_step", "-", "(", "self", ".", "frame_skip", "-", "2", ")", "\n", "self", ".", "_fetch_grayscale_observation", "(", "self", ".", "screen_buffer", "[", "t", "]", ")", "\n", "\n", "# Pool the last two observations.", "\n", "", "", "observation", "=", "self", ".", "_pool_and_resize", "(", ")", "\n", "\n", "self", ".", "game_over", "=", "game_over", "\n", "return", "observation", ",", "accumulated_reward", ",", "is_terminal", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing._fetch_grayscale_observation": [[185, 198], ["preprocessing.AtariPreprocessing.environment.ale.getScreenGrayscale"], "methods", ["None"], ["", "def", "_fetch_grayscale_observation", "(", "self", ",", "output", ")", ":", "\n", "    ", "\"\"\"Returns the current observation in grayscale.\n\n    The returned observation is stored in 'output'.\n\n    Args:\n      output: numpy array, screen buffer to hold the returned observation.\n\n    Returns:\n      observation: numpy array, the current observation in grayscale.\n    \"\"\"", "\n", "self", ".", "environment", ".", "ale", ".", "getScreenGrayscale", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.preprocessing.AtariPreprocessing._pool_and_resize": [[199, 217], ["cv2.resize", "numpy.asarray", "numpy.expand_dims", "numpy.maximum"], "methods", ["None"], ["", "def", "_pool_and_resize", "(", "self", ")", ":", "\n", "    ", "\"\"\"Transforms two frames into a Nature DQN observation.\n\n    For efficiency, the transformation is done in-place in self.screen_buffer.\n\n    Returns:\n      transformed_screen: numpy array, pooled, resized screen.\n    \"\"\"", "\n", "# Pool if there are enough screens to do so.", "\n", "if", "self", ".", "frame_skip", ">", "1", ":", "\n", "      ", "np", ".", "maximum", "(", "self", ".", "screen_buffer", "[", "0", "]", ",", "self", ".", "screen_buffer", "[", "1", "]", ",", "\n", "out", "=", "self", ".", "screen_buffer", "[", "0", "]", ")", "\n", "\n", "", "transformed_image", "=", "cv2", ".", "resize", "(", "self", ".", "screen_buffer", "[", "0", "]", ",", "\n", "(", "self", ".", "screen_size", ",", "self", ".", "screen_size", ")", ",", "\n", "interpolation", "=", "cv2", ".", "INTER_AREA", ")", "\n", "int_image", "=", "np", ".", "asarray", "(", "transformed_image", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "return", "np", ".", "expand_dims", "(", "int_image", ",", "axis", "=", "2", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner.__init__": [[109, 194], ["run_experiment.Runner._create_directories", "tensorflow.summary.FileWriter", "create_environment_fn", "tensorflow.set_random_seed", "tensorflow.ConfigProto", "tensorflow.Session", "create_agent_fn", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "run_experiment.Runner._summary_writer.add_graph", "run_experiment.Runner._sess.run", "run_experiment.Runner._initialize_checkpointer_and_maybe_resume", "tensorflow.global_variables_initializer", "tensorflow.get_default_graph"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._create_directories", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._initialize_checkpointer_and_maybe_resume"], ["def", "__init__", "(", "self", ",", "\n", "base_dir", ",", "\n", "create_agent_fn", ",", "\n", "random_seed", ",", "\n", "agent_name", ",", "\n", "game_name", ",", "\n", "num_iterations", ",", "\n", "create_environment_fn", "=", "create_atari_environment", ",", "\n", "sticky_actions", "=", "True", ",", "\n", "checkpoint_file_prefix", "=", "'ckpt'", ",", "\n", "logging_file_prefix", "=", "'log'", ",", "\n", "log_every_n", "=", "1", ",", "\n", "training_steps", "=", "250000", ",", "\n", "evaluation_steps", "=", "125000", ",", "\n", "max_steps_per_episode", "=", "27000", ")", ":", "\n", "    ", "\"\"\"Initialize the Runner object in charge of running a full experiment.\n\n    Args:\n      base_dir: str, the base directory to host all required sub-directories.\n      create_agent_fn: A function that takes as args a Tensorflow session and an\n        Atari 2600 Gym environment, and returns an agent.\n      create_environment_fn: A function which receives a game name and creates\n        an Atari 2600 Gym environment.\n      game_name: str, name of the Atari 2600 domain to run.\n      sticky_actions: bool, whether to enable sticky actions in the environment.\n      checkpoint_file_prefix: str, the prefix to use for checkpoint files.\n      logging_file_prefix: str, prefix to use for the log files.\n      log_every_n: int, the frequency for writing logs.\n      num_iterations: int, the iteration number threshold (must be greater than\n        start_iteration).\n      training_steps: int, the number of training steps to perform.\n      evaluation_steps: int, the number of evaluation steps to perform.\n      max_steps_per_episode: int, maximum number of steps after which an episode\n        terminates.\n\n    This constructor will take the following actions:\n    - Initialize an environment.\n    - Initialize a `tf.Session`.\n    - Initialize a logger.\n    - Initialize an agent.\n    - Reload from the latest checkpoint, if available, and initialize the\n      Checkpointer object.\n    \"\"\"", "\n", "assert", "base_dir", "is", "not", "None", "\n", "assert", "game_name", "is", "not", "None", "\n", "self", ".", "_logging_file_prefix", "=", "logging_file_prefix", "\n", "self", ".", "_log_every_n", "=", "log_every_n", "\n", "self", ".", "_num_iterations", "=", "num_iterations", "\n", "self", ".", "_training_steps", "=", "training_steps", "\n", "self", ".", "_evaluation_steps", "=", "evaluation_steps", "\n", "self", ".", "_max_steps_per_episode", "=", "max_steps_per_episode", "\n", "self", ".", "_base_dir", "=", "base_dir", "\n", "self", ".", "_create_directories", "(", ")", "\n", "self", ".", "_summary_writer", "=", "tf", ".", "summary", ".", "FileWriter", "(", "self", ".", "_base_dir", ")", "\n", "self", ".", "average_reward_eval", "=", "-", "100", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "agent_name", "=", "agent_name", "\n", "\n", "self", ".", "_environment", "=", "create_environment_fn", "(", "game_name", ",", "sticky_actions", ")", "\n", "# Set up a session and initialize variables.", "\n", "tf", ".", "set_random_seed", "(", "random_seed", ")", "\n", "tfconfig", "=", "tf", ".", "ConfigProto", "(", "allow_soft_placement", "=", "True", ")", "\n", "tfconfig", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "self", ".", "_sess", "=", "tf", ".", "Session", "(", "''", ",", "\n", "config", "=", "tfconfig", ")", "\n", "# self._sess = tf.Session('',", "\n", "#                         config=tf.ConfigProto(allow_soft_placement=True))", "\n", "self", ".", "_agent", "=", "create_agent_fn", "(", "self", ".", "_sess", ",", "self", ".", "_environment", ",", "\n", "summary_writer", "=", "self", ".", "_summary_writer", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Running %s with the following parameters:'", ",", "\n", "self", ".", "__class__", ".", "__name__", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t random_seed: %s'", ",", "random_seed", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t num_iterations: %s'", ",", "num_iterations", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t training_steps: %s'", ",", "training_steps", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t sticky_actions: %s'", ",", "sticky_actions", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'\\t game_name: %s'", ",", "game_name", ")", "\n", "# self._sess = tf.Session('',", "\n", "#                         config=tf.ConfigProto(allow_soft_placement=True))", "\n", "# self._agent = create_agent_fn(self._sess, self._environment,", "\n", "#                               summary_writer=self._summary_writer)", "\n", "\n", "self", ".", "_summary_writer", ".", "add_graph", "(", "graph", "=", "tf", ".", "get_default_graph", "(", ")", ")", "\n", "self", ".", "_sess", ".", "run", "(", "tf", ".", "global_variables_initializer", "(", ")", ")", "\n", "\n", "self", ".", "_initialize_checkpointer_and_maybe_resume", "(", "checkpoint_file_prefix", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._create_directories": [[201, 205], ["os.path.join", "dopamine.common.logger.Logger", "os.path.join"], "methods", ["None"], ["", "def", "_create_directories", "(", "self", ")", ":", "\n", "    ", "\"\"\"Create necessary sub-directories.\"\"\"", "\n", "self", ".", "_checkpoint_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_base_dir", ",", "'checkpoints'", ")", "\n", "self", ".", "_logger", "=", "logger", ".", "Logger", "(", "os", ".", "path", ".", "join", "(", "self", ".", "_base_dir", ",", "'logs'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._initialize_checkpointer_and_maybe_resume": [[206, 246], ["dopamine.common.checkpointer.Checkpointer", "dopamine.common.checkpointer.get_latest_checkpoint_number", "run_experiment.Runner._checkpointer.load_checkpoint", "run_experiment.Runner._agent.unbundle", "tensorflow.logging.info"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.get_latest_checkpoint_number", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer.load_checkpoint", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.unbundle"], ["", "def", "_initialize_checkpointer_and_maybe_resume", "(", "self", ",", "checkpoint_file_prefix", ")", ":", "\n", "    ", "\"\"\"Reloads the latest checkpoint if it exists.\n\n    This method will first create a `Checkpointer` object and then call\n    `checkpointer.get_latest_checkpoint_number` to determine if there is a valid\n    checkpoint in self._checkpoint_dir, and what the largest file number is.\n    If a valid checkpoint file is found, it will load the bundled data from this\n    file and will pass it to the agent for it to reload its data.\n    If the agent is able to successfully unbundle, this method will verify that\n    the unbundled data contains the keys,'logs' and 'current_iteration'. It will\n    then load the `Logger`'s data from the bundle, and will return the iteration\n    number keyed by 'current_iteration' as one of the return values (along with\n    the `Checkpointer` object).\n\n    Args:\n      checkpoint_file_prefix: str, the checkpoint file prefix.\n\n    Returns:\n      start_iteration: int, the iteration number to start the experiment from.\n      experiment_checkpointer: `Checkpointer` object for the experiment.\n    \"\"\"", "\n", "# self._checkpoint_dir = base_dir + \"/checkpoints\"", "\n", "self", ".", "_checkpointer", "=", "checkpointer", ".", "Checkpointer", "(", "self", ".", "_checkpoint_dir", ",", "\n", "checkpoint_file_prefix", ")", "\n", "self", ".", "_start_iteration", "=", "0", "\n", "# Check if checkpoint exists. Note that the existence of checkpoint 0 means", "\n", "# that we have finished iteration 0 (so we will start from iteration 1).", "\n", "latest_checkpoint_version", "=", "checkpointer", ".", "get_latest_checkpoint_number", "(", "\n", "self", ".", "_checkpoint_dir", ")", "\n", "if", "latest_checkpoint_version", ">=", "0", ":", "\n", "      ", "experiment_data", "=", "self", ".", "_checkpointer", ".", "load_checkpoint", "(", "\n", "latest_checkpoint_version", ")", "\n", "if", "self", ".", "_agent", ".", "unbundle", "(", "\n", "self", ".", "_checkpoint_dir", ",", "latest_checkpoint_version", ",", "experiment_data", ")", ":", "\n", "        ", "assert", "'logs'", "in", "experiment_data", "\n", "assert", "'current_iteration'", "in", "experiment_data", "\n", "self", ".", "_logger", ".", "data", "=", "experiment_data", "[", "'logs'", "]", "\n", "self", ".", "_start_iteration", "=", "experiment_data", "[", "'current_iteration'", "]", "+", "1", "\n", "tf", ".", "logging", ".", "info", "(", "'Reloaded checkpoint and will start from iteration %d'", ",", "\n", "self", ".", "_start_iteration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner.restore_checkpoints": [[247, 250], ["tensorflow.train.Saver", "tensorflow.train.Saver.restore", "os.path.join"], "methods", ["None"], ["", "", "", "def", "restore_checkpoints", "(", "self", ",", "restore_dir", ",", "filename", ")", ":", "\n", "    ", "saver", "=", "tf", ".", "train", ".", "Saver", "(", ")", "\n", "saver", ".", "restore", "(", "self", ".", "_sess", ",", "os", ".", "path", ".", "join", "(", "restore_dir", ",", "filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._initialize_episode": [[251, 259], ["run_experiment.Runner._environment.reset", "run_experiment.Runner._agent.begin_episode"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.reset", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.begin_episode"], ["", "def", "_initialize_episode", "(", "self", ")", ":", "\n", "    ", "\"\"\"Initialization for a new episode.\n\n    Returns:\n      action: int, the initial action chosen by the agent.\n    \"\"\"", "\n", "initial_observation", "=", "self", ".", "_environment", ".", "reset", "(", ")", "\n", "return", "self", ".", "_agent", ".", "begin_episode", "(", "initial_observation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_one_step": [[260, 272], ["run_experiment.Runner._environment.step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.step"], ["", "def", "_run_one_step", "(", "self", ",", "action", ")", ":", "\n", "    ", "\"\"\"Executes a single step in the environment.\n\n    Args:\n      action: int, the action to perform in the environment.\n\n    Returns:\n      The observation, reward, and is_terminal values returned from the\n        environment.\n    \"\"\"", "\n", "observation", ",", "reward", ",", "is_terminal", ",", "_", "=", "self", ".", "_environment", ".", "step", "(", "action", ")", "\n", "return", "observation", ",", "reward", ",", "is_terminal", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._end_episode": [[273, 280], ["run_experiment.Runner._agent.end_episode"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.end_episode"], ["", "def", "_end_episode", "(", "self", ",", "reward", ")", ":", "\n", "    ", "\"\"\"Finalizes an episode run.\n\n    Args:\n      reward: float, the last reward from the environment.\n    \"\"\"", "\n", "self", ".", "_agent", ".", "end_episode", "(", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._end_episode_store": [[281, 291], ["run_experiment.Runner._agent.end_episode_", "run_experiment.Runner._agent.end_episode"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.end_episode_", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.end_episode"], ["", "def", "_end_episode_store", "(", "self", ",", "reward", ",", "total_reward", ",", "step_number", ",", "is_opt", ")", ":", "\n", "    ", "\"\"\"Finalizes an episode run and store optimal trajectories.\n\n    Args:\n      reward: float, the last reward from the environment.\n    \"\"\"", "\n", "if", "is_opt", ":", "# if it is optimal trajectories, then store it.", "\n", "      ", "self", ".", "_agent", ".", "end_episode_", "(", "reward", ",", "total_reward", ",", "step_number", ")", "\n", "", "else", ":", "# else only store for DQN", "\n", "      ", "self", ".", "_agent", ".", "end_episode", "(", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_one_episode": [[292, 334], ["run_experiment.Runner._initialize_episode", "run_experiment.Runner._run_one_step", "run_experiment.Runner._end_episode_store", "run_experiment.Runner._end_episode", "run_experiment.Runner._agent.end_episode", "run_experiment.Runner._agent.begin_episode", "run_experiment.Runner._agent.step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._initialize_episode", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_one_step", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._end_episode_store", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._end_episode", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.end_episode", "home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.begin_episode", "home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.step"], ["", "", "def", "_run_one_episode", "(", "self", ")", ":", "\n", "    ", "\"\"\"Executes a full trajectory of the agent interacting with the environment.\n\n    Returns:\n      The number of steps taken and the total reward.\n    \"\"\"", "\n", "step_number", "=", "0", "\n", "total_reward", "=", "0.", "\n", "\n", "action", "=", "self", ".", "_initialize_episode", "(", ")", "\n", "is_terminal", "=", "False", "\n", "\n", "# Keep interacting until we reach a terminal state.", "\n", "while", "True", ":", "\n", "      ", "observation", ",", "reward", ",", "is_terminal", "=", "self", ".", "_run_one_step", "(", "action", ")", "\n", "\n", "total_reward", "+=", "reward", "\n", "step_number", "+=", "1", "\n", "\n", "# Perform reward clipping.", "\n", "# reward = np.clip(reward, -1, 1)  todo", "\n", "\n", "if", "(", "self", ".", "_environment", ".", "game_over", "or", "\n", "step_number", "==", "self", ".", "_max_steps_per_episode", ")", ":", "\n", "# Stop the run loop once we reach the true end of episode.", "\n", "        ", "break", "\n", "", "elif", "is_terminal", ":", "\n", "# If we lose a life but the episode is not over, signal an artificial", "\n", "# end of episode to the agent.", "\n", "        ", "self", ".", "_agent", ".", "end_episode", "(", "reward", ")", "\n", "action", "=", "self", ".", "_agent", ".", "begin_episode", "(", "observation", ")", "\n", "", "else", ":", "\n", "        ", "action", "=", "self", ".", "_agent", ".", "step", "(", "reward", ",", "observation", ")", "\n", "", "", "if", "self", ".", "agent_name", "in", "RPG_AGENTS", ":", "\n", "      ", "is_opt", "=", "False", "\n", "if", "total_reward", ">=", "episodic_return", "[", "self", ".", "game_name", "]", ":", "\n", "        ", "is_opt", "=", "True", "\n", "", "self", ".", "_end_episode_store", "(", "reward", ",", "total_reward", ",", "step_number", ",", "is_opt", ")", "\n", "", "else", ":", "\n", "      ", "self", ".", "_end_episode", "(", "reward", ")", "\n", "\n", "", "return", "step_number", ",", "total_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_one_phase": [[335, 386], ["run_experiment.Runner._run_one_episode", "statistics.append", "sys.stdout.write", "sys.stdout.flush", "sys.stdout.write", "sys.stdout.write"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_one_episode", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "_run_one_phase", "(", "self", ",", "min_steps", ",", "statistics", ",", "run_mode_str", ")", ":", "\n", "    ", "\"\"\"Runs the agent/environment loop until a desired number of steps.\n\n    We follow the Machado et al., 2017 convention of running full episodes,\n    and terminating once we've run a minimum number of steps.\n\n    Args:\n      min_steps: int, minimum number of steps to generate in this phase.\n      statistics: `IterationStatistics` object which records the experimental\n        results.\n      run_mode_str: str, describes the run mode for this agent.\n\n    Returns:\n      Tuple containing the number of steps taken in this phase (int), the sum of\n        returns (float), and the number of episodes performed (int).\n    \"\"\"", "\n", "step_count", "=", "0", "\n", "num_episodes", "=", "0", "\n", "sum_returns", "=", "0.", "\n", "num_good_trajs", "=", "0", "\n", "good_traj_label", "=", "0", "\n", "while", "step_count", "<", "min_steps", ":", "\n", "      ", "episode_length", ",", "episode_return", "=", "self", ".", "_run_one_episode", "(", ")", "\n", "\n", "good_traj_label", "=", "0", "\n", "if", "episode_return", ">=", "episodic_return", "[", "self", ".", "game_name", "]", ":", "\n", "        ", "good_traj_label", "=", "1", "\n", "num_good_trajs", "+=", "1", "\n", "", "statistics", ".", "append", "(", "{", "\n", "'{}_episode_lengths'", ".", "format", "(", "run_mode_str", ")", ":", "episode_length", ",", "\n", "'{}_episode_returns'", ".", "format", "(", "run_mode_str", ")", ":", "episode_return", ",", "\n", "'{}_episode_goodtraj'", ".", "format", "(", "run_mode_str", ")", ":", "good_traj_label", "\n", "}", ")", "\n", "step_count", "+=", "episode_length", "\n", "sum_returns", "+=", "episode_return", "\n", "num_episodes", "+=", "1", "\n", "# We use sys.stdout.write instead of tf.logging so as to flush frequently", "\n", "# without generating a line break.", "\n", "\n", "if", "self", ".", "agent_name", "in", "[", "'rpg'", ",", "'repg'", "]", ":", "\n", "        ", "sys", ".", "stdout", ".", "write", "(", "'epsilon: {} '", ".", "format", "(", "self", ".", "_agent", ".", "epsilon_current", ")", "+", "\n", "'replaysize {}\\r'", ".", "format", "(", "self", ".", "_agent", ".", "current_replay_size", ")", ")", "\n", "", "elif", "self", ".", "agent_name", "in", "RPG_AGENTS", ":", "\n", "        ", "sys", ".", "stdout", ".", "write", "(", "'Opt replay size: {} '", ".", "format", "(", "self", ".", "_agent", ".", "_replay_opt", ".", "memory", ".", "add_count", ")", ")", "\n", "\n", "", "sys", ".", "stdout", ".", "write", "(", "'Steps executed: {} '", ".", "format", "(", "step_count", ")", "+", "\n", "'Episode length: {} '", ".", "format", "(", "episode_length", ")", "+", "\n", "'Return: {}'", ".", "format", "(", "episode_return", ")", "+", "\n", "'Good traj?: {}\\r'", ".", "format", "(", "good_traj_label", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "", "return", "step_count", ",", "sum_returns", ",", "num_episodes", ",", "num_good_trajs", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_train_phase": [[387, 413], ["time.time", "run_experiment.Runner._run_one_phase", "statistics.append", "statistics.append", "tensorflow.logging.info", "tensorflow.logging.info", "time.time"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_one_phase", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "_run_train_phase", "(", "self", ",", "statistics", ",", "eval_mode", "=", "False", ")", ":", "\n", "    ", "\"\"\"Run training phase.\n\n    Args:\n      statistics: `IterationStatistics` object which records the experimental\n        results. Note - This object is modified by this method.\n\n    Returns:\n      num_episodes: int, The number of episodes run in this phase.\n      average_reward: The average reward generated in this phase.\n    \"\"\"", "\n", "# Perform the training phase, during which the agent learns.", "\n", "self", ".", "_agent", ".", "eval_mode", "=", "eval_mode", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "number_steps", ",", "sum_returns", ",", "num_episodes", ",", "num_good_trajs", "=", "self", ".", "_run_one_phase", "(", "\n", "self", ".", "_training_steps", ",", "statistics", ",", "'train'", ")", "\n", "average_return", "=", "sum_returns", "/", "num_episodes", "if", "num_episodes", ">", "0", "else", "0.0", "\n", "statistics", ".", "append", "(", "{", "'train_average_return'", ":", "average_return", "}", ")", "\n", "average_good_trajs", "=", "num_good_trajs", "/", "num_episodes", "if", "num_episodes", ">", "0", "else", "0.0", "\n", "statistics", ".", "append", "(", "{", "'train_average_goodtraj'", ":", "average_good_trajs", "}", ")", "\n", "time_delta", "=", "time", ".", "time", "(", ")", "-", "start_time", "\n", "tf", ".", "logging", ".", "info", "(", "'Average undiscounted return per training episode: %.2f'", ",", "\n", "average_return", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Average training steps per second: %.2f'", ",", "\n", "number_steps", "/", "time_delta", ")", "\n", "return", "num_episodes", ",", "average_return", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_eval_phase": [[414, 434], ["run_experiment.Runner._run_one_phase", "tensorflow.logging.info", "statistics.append"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_one_phase", "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "_run_eval_phase", "(", "self", ",", "statistics", ")", ":", "\n", "    ", "\"\"\"Run evaluation phase.\n\n    Args:\n      statistics: `IterationStatistics` object which records the experimental\n        results. Note - This object is modified by this method.\n\n    Returns:\n      num_episodes: int, The number of episodes run in this phase.\n      average_reward: float, The average reward generated in this phase.\n    \"\"\"", "\n", "# Perform the evaluation phase -- no learning.", "\n", "self", ".", "_agent", ".", "eval_mode", "=", "True", "\n", "_", ",", "sum_returns", ",", "num_episodes", ",", "_", "=", "self", ".", "_run_one_phase", "(", "\n", "self", ".", "_evaluation_steps", ",", "statistics", ",", "'eval'", ")", "\n", "average_return", "=", "sum_returns", "/", "num_episodes", "if", "num_episodes", ">", "0", "else", "0.0", "\n", "tf", ".", "logging", ".", "info", "(", "'Average undiscounted return per evaluation episode: %.2f'", ",", "\n", "average_return", ")", "\n", "statistics", ".", "append", "(", "{", "'eval_average_return'", ":", "average_return", "}", ")", "\n", "return", "num_episodes", ",", "average_return", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_one_iteration": [[435, 475], ["dopamine.common.iteration_statistics.IterationStatistics", "tensorflow.logging.info", "run_experiment.Runner._run_train_phase", "run_experiment.Runner._save_tensorboard_summaries", "print", "run_experiment.Runner._run_eval_phase"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_train_phase", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.TrainRunner._save_tensorboard_summaries", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_eval_phase"], ["", "def", "_run_one_iteration", "(", "self", ",", "iteration", ")", ":", "\n", "    ", "\"\"\"Runs one iteration of agent/environment interaction.\n\n    An iteration involves running several episodes until a certain number of\n    steps are obtained. The interleaving of train/eval phases implemented here\n    are to match the implementation of (Mnih et al., 2015).\n\n    Args:\n      iteration: int, current iteration number, used as a global_step for saving\n        Tensorboard summaries.\n\n    Returns:\n      A dict containing summary statistics for this iteration.\n    \"\"\"", "\n", "statistics", "=", "iteration_statistics", ".", "IterationStatistics", "(", ")", "\n", "tf", ".", "logging", ".", "info", "(", "'Starting iteration %d'", ",", "iteration", ")", "\n", "\n", "train_eval_mode", "=", "False", "\n", "# if self.game_name == \"Pong\":", "\n", "if", "self", ".", "average_reward_eval", ">=", "episodic_return_switch", "[", "self", ".", "game_name", "]", ":", "\n", "      ", "train_eval_mode", "=", "True", "\n", "print", "(", "\"Stop training at iteration {}\"", ".", "format", "(", "iteration", ")", ")", "\n", "\n", "", "num_episodes_train", ",", "average_reward_train", "=", "self", ".", "_run_train_phase", "(", "\n", "statistics", ",", "train_eval_mode", ")", "\n", "# else:", "\n", "#   # don't train, only for evaluation.", "\n", "#   num_episodes_train, average_reward_train = 0, 0", "\n", "\n", "if", "self", ".", "agent_name", "in", "RPG_AGENTS", "and", "self", ".", "_agent", ".", "_replay_opt", ".", "memory", ".", "add_count", "==", "0", ":", "\n", "      ", "num_episodes_eval", ",", "average_reward_eval", "=", "-", "10000", ",", "-", "10000", "\n", "# if we didn't train rpg, don't waste time evaluate it.", "\n", "", "else", ":", "\n", "      ", "num_episodes_eval", ",", "average_reward_eval", "=", "self", ".", "_run_eval_phase", "(", "\n", "statistics", ")", "\n", "", "self", ".", "average_reward_eval", "=", "average_reward_eval", "\n", "self", ".", "_save_tensorboard_summaries", "(", "iteration", ",", "num_episodes_train", ",", "\n", "average_reward_train", ",", "num_episodes_eval", ",", "\n", "average_reward_eval", ")", "\n", "return", "statistics", ".", "data_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._save_tensorboard_summaries": [[476, 501], ["tensorflow.Summary", "run_experiment.Runner._summary_writer.add_summary", "tensorflow.Summary.Value", "tensorflow.Summary.Value", "tensorflow.Summary.Value", "tensorflow.Summary.Value"], "methods", ["None"], ["", "def", "_save_tensorboard_summaries", "(", "self", ",", "iteration", ",", "\n", "num_episodes_train", ",", "\n", "average_reward_train", ",", "\n", "num_episodes_eval", ",", "\n", "average_reward_eval", ")", ":", "\n", "    ", "\"\"\"Save statistics as tensorboard summaries.\n\n    Args:\n      iteration: int, The current iteration number.\n      num_episodes_train: int, number of training episodes run.\n      average_reward_train: float, The average training reward.\n      num_episodes_eval: int, number of evaluation episodes run.\n      average_reward_eval: float, The average evaluation reward.\n    \"\"\"", "\n", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "[", "\n", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "'Train/NumEpisodes'", ",", "\n", "simple_value", "=", "num_episodes_train", ")", ",", "\n", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "'Train/AverageReturns'", ",", "\n", "simple_value", "=", "average_reward_train", ")", ",", "\n", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "'Eval/NumEpisodes'", ",", "\n", "simple_value", "=", "num_episodes_eval", ")", ",", "\n", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "'Eval/AverageReturns'", ",", "\n", "simple_value", "=", "average_reward_eval", ")", "\n", "]", ")", "\n", "self", ".", "_summary_writer", ".", "add_summary", "(", "summary", ",", "iteration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._log_experiment": [[502, 512], ["run_experiment.Runner._logger.log_to_file"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.logger.Logger.log_to_file"], ["", "def", "_log_experiment", "(", "self", ",", "iteration", ",", "statistics", ")", ":", "\n", "    ", "\"\"\"Records the results of the current iteration.\n\n    Args:\n      iteration: int, iteration number.\n      statistics: `IterationStatistics` object containing statistics to log.\n    \"\"\"", "\n", "self", ".", "_logger", "[", "'iteration_{:d}'", ".", "format", "(", "iteration", ")", "]", "=", "statistics", "\n", "if", "iteration", "%", "self", ".", "_log_every_n", "==", "0", ":", "\n", "      ", "self", ".", "_logger", ".", "log_to_file", "(", "self", ".", "_logging_file_prefix", ",", "iteration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._checkpoint_experiment": [[513, 525], ["run_experiment.Runner._agent.bundle_and_checkpoint", "run_experiment.Runner._checkpointer.save_checkpoint"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.dqnrpg.dqnrpg_agent.DQNRPGAgent.bundle_and_checkpoint", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer.save_checkpoint"], ["", "", "def", "_checkpoint_experiment", "(", "self", ",", "iteration", ")", ":", "\n", "    ", "\"\"\"Checkpoint experiment data.\n\n    Args:\n      iteration: int, iteration number for checkpointing.\n    \"\"\"", "\n", "experiment_data", "=", "self", ".", "_agent", ".", "bundle_and_checkpoint", "(", "self", ".", "_checkpoint_dir", ",", "\n", "iteration", ")", "\n", "if", "experiment_data", ":", "\n", "      ", "experiment_data", "[", "'current_iteration'", "]", "=", "iteration", "\n", "experiment_data", "[", "'logs'", "]", "=", "self", ".", "_logger", ".", "data", "\n", "self", ".", "_checkpointer", ".", "save_checkpoint", "(", "iteration", ",", "experiment_data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner.run_experiment": [[526, 538], ["tensorflow.logging.info", "range", "tensorflow.logging.warning", "run_experiment.Runner._run_one_iteration", "run_experiment.Runner._log_experiment", "run_experiment.Runner._checkpoint_experiment"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.TrainRunner._run_one_iteration", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._log_experiment", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._checkpoint_experiment"], ["", "", "def", "run_experiment", "(", "self", ")", ":", "\n", "    ", "\"\"\"Runs a full experiment, spread over multiple iterations.\"\"\"", "\n", "tf", ".", "logging", ".", "info", "(", "'Beginning training...'", ")", "\n", "if", "self", ".", "_num_iterations", "<=", "self", ".", "_start_iteration", ":", "\n", "      ", "tf", ".", "logging", ".", "warning", "(", "'num_iterations (%d) < start_iteration(%d)'", ",", "\n", "self", ".", "_num_iterations", ",", "self", ".", "_start_iteration", ")", "\n", "return", "\n", "\n", "", "for", "iteration", "in", "range", "(", "self", ".", "_start_iteration", ",", "self", ".", "_num_iterations", ")", ":", "\n", "      ", "statistics", "=", "self", ".", "_run_one_iteration", "(", "iteration", ")", "\n", "self", ".", "_log_experiment", "(", "iteration", ",", "statistics", ")", "\n", "self", ".", "_checkpoint_experiment", "(", "iteration", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.TrainRunner.__init__": [[549, 561], ["tensorflow.logging.info", "run_experiment.Runner.__init__"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.__init__"], ["def", "__init__", "(", "self", ",", "base_dir", ",", "create_agent_fn", ")", ":", "\n", "    ", "\"\"\"Initialize the TrainRunner object in charge of running a full experiment.\n\n    Args:\n      base_dir: str, the base directory to host all required sub-directories.\n      create_agent_fn: A function that takes as args a Tensorflow session and an\n        Atari 2600 Gym environment, and returns an agent.\n    \"\"\"", "\n", "tf", ".", "logging", ".", "info", "(", "'Creating TrainRunner ...'", ")", "\n", "super", "(", "TrainRunner", ",", "self", ")", ".", "__init__", "(", "\n", "base_dir", "=", "base_dir", ",", "create_agent_fn", "=", "create_agent_fn", ")", "\n", "self", ".", "_agent", ".", "eval_mode", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.TrainRunner._run_one_iteration": [[562, 583], ["dopamine.common.iteration_statistics.IterationStatistics", "run_experiment.TrainRunner._run_train_phase", "run_experiment.TrainRunner._save_tensorboard_summaries"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.Runner._run_train_phase", "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.TrainRunner._save_tensorboard_summaries"], ["", "def", "_run_one_iteration", "(", "self", ",", "iteration", ")", ":", "\n", "    ", "\"\"\"Runs one iteration of agent/environment interaction.\n\n    An iteration involves running several episodes until a certain number of\n    steps are obtained. This method differs from the `_run_one_iteration` method\n    in the base `Runner` class in that it only runs the train phase.\n\n    Args:\n      iteration: int, current iteration number, used as a global_step for saving\n        Tensorboard summaries.\n\n    Returns:\n      A dict containing summary statistics for this iteration.\n    \"\"\"", "\n", "statistics", "=", "iteration_statistics", ".", "IterationStatistics", "(", ")", "\n", "num_episodes_train", ",", "average_reward_train", "=", "self", ".", "_run_train_phase", "(", "\n", "statistics", ")", "\n", "\n", "self", ".", "_save_tensorboard_summaries", "(", "iteration", ",", "num_episodes_train", ",", "\n", "average_reward_train", ")", "\n", "return", "statistics", ".", "data_lists", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.TrainRunner._save_tensorboard_summaries": [[584, 593], ["tensorflow.Summary", "run_experiment.TrainRunner._summary_writer.add_summary", "tensorflow.Summary.Value", "tensorflow.Summary.Value"], "methods", ["None"], ["", "def", "_save_tensorboard_summaries", "(", "self", ",", "iteration", ",", "num_episodes", ",", "\n", "average_reward", ")", ":", "\n", "    ", "\"\"\"Save statistics as tensorboard summaries.\"\"\"", "\n", "summary", "=", "tf", ".", "Summary", "(", "value", "=", "[", "\n", "tf", ".", "Summary", ".", "Value", "(", "tag", "=", "'Train/NumEpisodes'", ",", "simple_value", "=", "num_episodes", ")", ",", "\n", "tf", ".", "Summary", ".", "Value", "(", "\n", "tag", "=", "'Train/AverageReturns'", ",", "simple_value", "=", "average_reward", ")", ",", "\n", "]", ")", "\n", "self", ".", "_summary_writer", ".", "add_summary", "(", "summary", ",", "iteration", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.load_gin_configs": [[41, 53], ["gin.parse_config_files_and_bindings"], "function", ["None"], ["def", "load_gin_configs", "(", "gin_files", ",", "gin_bindings", ")", ":", "\n", "  ", "\"\"\"Loads gin configuration files.\n\n  Args:\n    gin_files: list, of paths to the gin configuration files for this\n      experiment.\n    gin_bindings: list, of gin parameter bindings to override the values in\n      the config files.\n  \"\"\"", "\n", "gin", ".", "parse_config_files_and_bindings", "(", "gin_files", ",", "\n", "bindings", "=", "gin_bindings", ",", "\n", "skip_unknown", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.atari.run_experiment.create_atari_environment": [[55, 88], ["gym.make", "dopamine.atari.preprocessing.AtariPreprocessing"], "function", ["None"], ["", "def", "create_atari_environment", "(", "game_name", ",", "sticky_actions", "=", "True", ")", ":", "\n", "  ", "\"\"\"Wraps an Atari 2600 Gym environment with some basic preprocessing.\n\n  This preprocessing matches the guidelines proposed in Machado et al. (2017),\n  \"Revisiting the Arcade Learning Environment: Evaluation Protocols and Open\n  Problems for General Agents\".\n\n  The created environment is the Gym wrapper around the Arcade Learning\n  Environment.\n\n  The main choice available to the user is whether to use sticky actions or not.\n  Sticky actions, as prescribed by Machado et al., cause actions to persist\n  with some probability (0.25) when a new command is sent to the ALE. This\n  can be viewed as introducing a mild form of stochasticity in the environment.\n  We use them by default.\n\n  Args:\n    game_name: str, the name of the Atari 2600 domain.\n    sticky_actions: bool, whether to use sticky_actions as per Machado et al.\n\n  Returns:\n    An Atari 2600 environment with some standard preprocessing.\n  \"\"\"", "\n", "game_version", "=", "'v0'", "if", "sticky_actions", "else", "'v4'", "\n", "full_game_name", "=", "'{}NoFrameskip-{}'", ".", "format", "(", "game_name", ",", "game_version", ")", "\n", "env", "=", "gym", ".", "make", "(", "full_game_name", ")", "\n", "# Strip out the TimeLimit wrapper from Gym, which caps us at 100k frames. We", "\n", "# handle this time limit internally instead, which lets us cap at 108k frames", "\n", "# (30 minutes). The TimeLimit wrapper also plays poorly with saving and", "\n", "# restoring states.", "\n", "env", "=", "env", ".", "env", "\n", "env", "=", "preprocessing", ".", "AtariPreprocessing", "(", "env", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.__init__": [[37, 39], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "data_lists", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append": [[40, 50], ["data_pairs.items", "iteration_statistics.IterationStatistics.data_lists[].append"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.iteration_statistics.IterationStatistics.append"], ["", "def", "append", "(", "self", ",", "data_pairs", ")", ":", "\n", "    ", "\"\"\"Add the given values to their corresponding key-indexed lists.\n\n    Args:\n      data_pairs: A dictionary of key-value pairs to be recorded.\n    \"\"\"", "\n", "for", "key", ",", "value", "in", "data_pairs", ".", "items", "(", ")", ":", "\n", "      ", "if", "key", "not", "in", "self", ".", "data_lists", ":", "\n", "        ", "self", ".", "data_lists", "[", "key", "]", "=", "[", "]", "\n", "", "self", ".", "data_lists", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.logger.Logger.__init__": [[32, 59], ["tensorflow.logging.info", "tensorflow.gfile.MakeDirs", "tensorflow.gfile.Exists", "tensorflow.logging.warning"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "logging_dir", ")", ":", "\n", "    ", "\"\"\"Initializes Logger.\n\n    Args:\n      logging_dir: str, Directory to which logs are written.\n    \"\"\"", "\n", "# Dict used by logger to store data.", "\n", "self", ".", "data", "=", "{", "}", "\n", "self", ".", "_logging_enabled", "=", "True", "\n", "\n", "if", "not", "logging_dir", ":", "\n", "      ", "tf", ".", "logging", ".", "info", "(", "'Logging directory not specified, will not log.'", ")", "\n", "self", ".", "_logging_enabled", "=", "False", "\n", "return", "\n", "# Try to create logging directory.", "\n", "", "try", ":", "\n", "      ", "tf", ".", "gfile", ".", "MakeDirs", "(", "logging_dir", ")", "\n", "", "except", "tf", ".", "errors", ".", "PermissionDeniedError", ":", "\n", "# If it already exists, ignore exception.", "\n", "      ", "pass", "\n", "", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "logging_dir", ")", ":", "\n", "      ", "tf", ".", "logging", ".", "warning", "(", "\n", "'Could not create directory %s, logging will be disabled.'", ",", "\n", "logging_dir", ")", "\n", "self", ".", "_logging_enabled", "=", "False", "\n", "return", "\n", "", "self", ".", "_logging_dir", "=", "logging_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.logger.Logger.__setitem__": [[60, 71], ["None"], "methods", ["None"], ["", "def", "__setitem__", "(", "self", ",", "key", ",", "value", ")", ":", "\n", "    ", "\"\"\"This method will set an entry at key with value in the dictionary.\n\n    It will effectively overwrite any previous data at the same key.\n\n    Args:\n      key: str, indicating key where to write the entry.\n      value: A python object to store.\n    \"\"\"", "\n", "if", "self", ".", "_logging_enabled", ":", "\n", "      ", "self", ".", "data", "[", "key", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.logger.Logger._generate_filename": [[72, 75], ["os.path.join"], "methods", ["None"], ["", "", "def", "_generate_filename", "(", "self", ",", "filename_prefix", ",", "iteration_number", ")", ":", "\n", "    ", "filename", "=", "'{}_{}'", ".", "format", "(", "filename_prefix", ",", "iteration_number", ")", "\n", "return", "os", ".", "path", ".", "join", "(", "self", ".", "_logging_dir", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.logger.Logger.log_to_file": [[76, 102], ["logger.Logger._generate_filename", "tensorflow.logging.warning", "tensorflow.gfile.GFile", "pickle.dump", "logger.Logger._generate_filename", "tensorflow.gfile.Remove"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename"], ["", "def", "log_to_file", "(", "self", ",", "filename_prefix", ",", "iteration_number", ")", ":", "\n", "    ", "\"\"\"Save the pickled dictionary to a file.\n\n    Args:\n      filename_prefix: str, name of the file to use (without iteration\n        number).\n      iteration_number: int, the iteration number, appended to the end of\n        filename_prefix.\n    \"\"\"", "\n", "if", "not", "self", ".", "_logging_enabled", ":", "\n", "      ", "tf", ".", "logging", ".", "warning", "(", "'Logging is disabled.'", ")", "\n", "return", "\n", "", "log_file", "=", "self", ".", "_generate_filename", "(", "filename_prefix", ",", "iteration_number", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "log_file", ",", "'w'", ")", "as", "fout", ":", "\n", "      ", "pickle", ".", "dump", "(", "self", ".", "data", ",", "fout", ",", "protocol", "=", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "# After writing a checkpoint file, we garbage collect the log file", "\n", "# that is CHECKPOINT_DURATION versions old.", "\n", "", "stale_iteration_number", "=", "iteration_number", "-", "CHECKPOINT_DURATION", "\n", "if", "stale_iteration_number", ">=", "0", ":", "\n", "      ", "stale_file", "=", "self", ".", "_generate_filename", "(", "filename_prefix", ",", "\n", "stale_iteration_number", ")", "\n", "try", ":", "\n", "        ", "tf", ".", "gfile", ".", "Remove", "(", "stale_file", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "# Ignore if file not found.", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.logger.Logger.is_logging_enabled": [[103, 106], ["None"], "methods", ["None"], ["", "", "", "def", "is_logging_enabled", "(", "self", ")", ":", "\n", "    ", "\"\"\"Return if logging is enabled.\"\"\"", "\n", "return", "self", ".", "_logging_enabled", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer.__init__": [[84, 107], ["ValueError", "tensorflow.gfile.MakeDirs", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "base_directory", ",", "checkpoint_file_prefix", "=", "'ckpt'", ",", "\n", "checkpoint_frequency", "=", "1", ")", ":", "\n", "    ", "\"\"\"Initializes Checkpointer.\n\n    Args:\n      base_directory: str, directory where all checkpoints are saved/loaded.\n      checkpoint_file_prefix: str, prefix to use for naming checkpoint files.\n      checkpoint_frequency: int, the frequency at which to checkpoint.\n\n    Raises:\n      ValueError: if base_directory is empty, or not creatable.\n    \"\"\"", "\n", "if", "not", "base_directory", ":", "\n", "      ", "raise", "ValueError", "(", "'No path provided to Checkpointer.'", ")", "\n", "", "self", ".", "_checkpoint_file_prefix", "=", "checkpoint_file_prefix", "\n", "self", ".", "_checkpoint_frequency", "=", "checkpoint_frequency", "\n", "self", ".", "_base_directory", "=", "base_directory", "\n", "try", ":", "\n", "      ", "tf", ".", "gfile", ".", "MakeDirs", "(", "base_directory", ")", "\n", "", "except", "tf", ".", "errors", ".", "PermissionDeniedError", ":", "\n", "# We catch the PermissionDeniedError and issue a more useful exception.", "\n", "      ", "raise", "ValueError", "(", "'Unable to create checkpoint path: {}.'", ".", "format", "(", "\n", "base_directory", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename": [[108, 112], ["os.path.join"], "methods", ["None"], ["", "", "def", "_generate_filename", "(", "self", ",", "file_prefix", ",", "iteration_number", ")", ":", "\n", "    ", "\"\"\"Returns a checkpoint filename from prefix and iteration number.\"\"\"", "\n", "filename", "=", "'{}.{}'", ".", "format", "(", "file_prefix", ",", "iteration_number", ")", "\n", "return", "os", ".", "path", ".", "join", "(", "self", ".", "_base_directory", ",", "filename", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._save_data_to_file": [[113, 117], ["tensorflow.gfile.GFile", "pickle.dump"], "methods", ["None"], ["", "def", "_save_data_to_file", "(", "self", ",", "data", ",", "filename", ")", ":", "\n", "    ", "\"\"\"Saves the given 'data' object to a file.\"\"\"", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "filename", ",", "'w'", ")", "as", "fout", ":", "\n", "      ", "pickle", ".", "dump", "(", "data", ",", "fout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer.save_checkpoint": [[118, 138], ["checkpointer.Checkpointer._generate_filename", "checkpointer.Checkpointer._save_data_to_file", "checkpointer.Checkpointer._generate_filename", "checkpointer.Checkpointer._clean_up_old_checkpoints", "tensorflow.gfile.GFile", "fout.write"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._save_data_to_file", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._clean_up_old_checkpoints"], ["", "", "def", "save_checkpoint", "(", "self", ",", "iteration_number", ",", "data", ")", ":", "\n", "    ", "\"\"\"Saves a new checkpoint at the current iteration_number.\n\n    Args:\n      iteration_number: int, the current iteration number for this checkpoint.\n      data: Any (picklable) python object containing the data to store in the\n        checkpoint.\n    \"\"\"", "\n", "if", "iteration_number", "%", "self", ".", "_checkpoint_frequency", "!=", "0", ":", "\n", "      ", "return", "\n", "\n", "", "filename", "=", "self", ".", "_generate_filename", "(", "self", ".", "_checkpoint_file_prefix", ",", "\n", "iteration_number", ")", "\n", "self", ".", "_save_data_to_file", "(", "data", ",", "filename", ")", "\n", "filename", "=", "self", ".", "_generate_filename", "(", "'sentinel_checkpoint_complete'", ",", "\n", "iteration_number", ")", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "filename", ",", "'wb'", ")", "as", "fout", ":", "\n", "      ", "fout", ".", "write", "(", "'done'", ")", "\n", "\n", "", "self", ".", "_clean_up_old_checkpoints", "(", "iteration_number", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._clean_up_old_checkpoints": [[139, 158], ["checkpointer.Checkpointer._generate_filename", "checkpointer.Checkpointer._generate_filename", "tensorflow.gfile.Remove", "tensorflow.gfile.Remove", "tensorflow.logging.info"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename"], ["", "def", "_clean_up_old_checkpoints", "(", "self", ",", "iteration_number", ")", ":", "\n", "    ", "\"\"\"Removes sufficiently old checkpoints.\"\"\"", "\n", "# After writing a the checkpoint and sentinel file, we garbage collect files", "\n", "# that are CHECKPOINT_DURATION * self._checkpoint_frequency versions old.", "\n", "stale_iteration_number", "=", "iteration_number", "-", "(", "self", ".", "_checkpoint_frequency", "*", "\n", "CHECKPOINT_DURATION", ")", "\n", "\n", "if", "stale_iteration_number", ">=", "0", ":", "\n", "      ", "stale_file", "=", "self", ".", "_generate_filename", "(", "self", ".", "_checkpoint_file_prefix", ",", "\n", "stale_iteration_number", ")", "\n", "stale_sentinel", "=", "self", ".", "_generate_filename", "(", "'sentinel_checkpoint_complete'", ",", "\n", "stale_iteration_number", ")", "\n", "try", ":", "\n", "        ", "tf", ".", "gfile", ".", "Remove", "(", "stale_file", ")", "\n", "tf", ".", "gfile", ".", "Remove", "(", "stale_sentinel", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "# Ignore if file not found.", "\n", "        ", "tf", ".", "logging", ".", "info", "(", "'Unable to remove {} or {}.'", ".", "format", "(", "stale_file", ",", "\n", "stale_sentinel", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._load_data_from_file": [[159, 164], ["tensorflow.gfile.Exists", "tensorflow.gfile.GFile", "pickle.load"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.replay_memory.circular_replay_buffer.WrappedReplayBuffer.load"], ["", "", "", "def", "_load_data_from_file", "(", "self", ",", "filename", ")", ":", "\n", "    ", "if", "not", "tf", ".", "gfile", ".", "Exists", "(", "filename", ")", ":", "\n", "      ", "return", "None", "\n", "", "with", "tf", ".", "gfile", ".", "GFile", "(", "filename", ",", "'rb'", ")", "as", "fin", ":", "\n", "      ", "return", "pickle", ".", "load", "(", "fin", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer.load_checkpoint": [[165, 178], ["checkpointer.Checkpointer._generate_filename", "checkpointer.Checkpointer._load_data_from_file"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._generate_filename", "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.Checkpointer._load_data_from_file"], ["", "", "def", "load_checkpoint", "(", "self", ",", "iteration_number", ")", ":", "\n", "    ", "\"\"\"Tries to reload a checkpoint at the selected iteration number.\n\n    Args:\n      iteration_number: The checkpoint iteration number to try to load.\n\n    Returns:\n      If the checkpoint files exist, two unpickled objects that were passed in\n        as data to save_checkpoint; returns None if the files do not exist.\n    \"\"\"", "\n", "checkpoint_file", "=", "self", ".", "_generate_filename", "(", "self", ".", "_checkpoint_file_prefix", ",", "\n", "iteration_number", ")", "\n", "return", "self", ".", "_load_data_from_file", "(", "checkpoint_file", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.illidanlab_rpg.common.checkpointer.get_latest_checkpoint_number": [[57, 78], ["os.path.join", "int", "tensorflow.gfile.Glob", "max", "checkpointer.get_latest_checkpoint_number.extract_iteration"], "function", ["None"], ["def", "get_latest_checkpoint_number", "(", "base_directory", ")", ":", "\n", "  ", "\"\"\"Returns the version number of the latest completed checkpoint.\n\n  Args:\n    base_directory: str, directory in which to look for checkpoint files.\n\n  Returns:\n    int, the iteration number of the latest checkpoint, or -1 if none was found.\n  \"\"\"", "\n", "glob", "=", "os", ".", "path", ".", "join", "(", "base_directory", ",", "'sentinel_checkpoint_complete.*'", ")", "\n", "def", "extract_iteration", "(", "x", ")", ":", "\n", "    ", "return", "int", "(", "x", "[", "x", ".", "rfind", "(", "'.'", ")", "+", "1", ":", "]", ")", "\n", "", "try", ":", "\n", "    ", "checkpoint_files", "=", "tf", ".", "gfile", ".", "Glob", "(", "glob", ")", "\n", "", "except", "tf", ".", "errors", ".", "NotFoundError", ":", "\n", "    ", "return", "-", "1", "\n", "", "try", ":", "\n", "    ", "latest_iteration", "=", "max", "(", "extract_iteration", "(", "x", ")", "for", "x", "in", "checkpoint_files", ")", "\n", "return", "latest_iteration", "\n", "", "except", "ValueError", ":", "\n", "    ", "return", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.__init__": [[28, 31], ["None"], "methods", ["None"], ["import", "numpy", "as", "np", "\n", "import", "gin", ".", "tf", "\n", "import", "cv2", "\n", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.observation_space": [[32, 35], ["None"], "methods", ["None"], ["\n", "@", "gin", ".", "configurable", "\n", "class", "AtariPreprocessing", "(", "object", ")", ":", "\n", "  "]], "home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.action_space": [[36, 39], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.reward_range": [[40, 43], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.metadata": [[44, 47], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.reset": [[48, 50], ["preprocessing.GymPreprocessing.environment.reset"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.reset"], ["\n", "\n", "def", "__init__", "(", "self", ",", "environment", ",", "frame_skip", "=", "4", ",", "terminal_on_life_loss", "=", "False", ",", "\n"]], "home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.step": [[51, 55], ["preprocessing.GymPreprocessing.environment.step"], "methods", ["home.repos.pwc.inspect_result.illidanlab_rpg.gym.preprocessing.GymPreprocessing.step"], ["screen_size", "=", "84", ")", ":", "\n", "    "]]}