{"home.repos.pwc.inspect_result.kk7nc_Text_Classification.Data.Download_WOS.download_and_extract": [[38, 63], ["os.path.join", "os.path.abspath", "os.path.exists", "os.makedirs", "DATA_URL.split", "os.path.exists", "urllib.urlretrieve", "print", "tarfile.open().extractall", "sys.stdout.write", "sys.stdout.flush", "tarfile.open", "float", "float"], "function", ["None"], ["def", "download_and_extract", "(", ")", ":", "\n", "    ", "\"\"\"\n    Download and extract the WOS datasets\n    :return: None\n    \"\"\"", "\n", "dest_directory", "=", "DATA_DIR", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "dest_directory", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "dest_directory", ")", "\n", "", "filename", "=", "DATA_URL", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "\n", "filepath", "=", "os", ".", "path", ".", "join", "(", "dest_directory", ",", "filename", ")", "\n", "\n", "\n", "path", "=", "os", ".", "path", ".", "abspath", "(", "dest_directory", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "filepath", ")", ":", "\n", "        ", "def", "_progress", "(", "count", ",", "block_size", ",", "total_size", ")", ":", "\n", "            ", "sys", ".", "stdout", ".", "write", "(", "'\\rDownloading %s %.2f%%'", "%", "(", "filename", ",", "\n", "float", "(", "count", "*", "block_size", ")", "/", "float", "(", "total_size", ")", "*", "100.0", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "filepath", ",", "_", "=", "urllib", ".", "urlretrieve", "(", "DATA_URL", ",", "filepath", ",", "reporthook", "=", "_progress", ")", "\n", "\n", "print", "(", "'Downloaded'", ",", "filename", ")", "\n", "\n", "tarfile", ".", "open", "(", "filepath", ",", "'r'", ")", ".", "extractall", "(", "dest_directory", ")", "\n", "", "return", "path", "\n", "", ""]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Data.Download_Glove.download_and_extract": [[39, 79], ["os.path.join", "print", "os.path.abspath", "os.path.exists", "os.makedirs", "DATA_URL.split", "os.path.exists", "urllib.urlretrieve", "zipfile.ZipFile", "zipfile.ZipFile.extractall", "zipfile.ZipFile.close", "sys.stdout.write", "sys.stdout.flush", "print", "exit", "float", "float"], "function", ["None"], ["def", "download_and_extract", "(", "data", "=", "'Wikipedia'", ")", ":", "\n", "    ", "\"\"\"\n    Download and extract the GloVe\n    :return: None\n    \"\"\"", "\n", "\n", "if", "data", "==", "'Wikipedia'", ":", "\n", "        ", "DATA_URL", "=", "'http://nlp.stanford.edu/data/glove.6B.zip'", "\n", "", "elif", "data", "==", "'Common_Crawl_840B'", ":", "\n", "        ", "DATA_URL", "=", "'http://nlp.stanford.edu/data/wordvecs/glove.840B.300d.zip'", "\n", "", "elif", "data", "==", "'Common_Crawl_42B'", ":", "\n", "        ", "DATA_URL", "=", "'http://nlp.stanford.edu/data/wordvecs/glove.42B.300d.zip'", "\n", "", "elif", "data", "==", "'Twitter'", ":", "\n", "        ", "DATA_URL", "=", "'http://nlp.stanford.edu/data/wordvecs/glove.twitter.27B.zip'", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"prameter should be Twitter, Common_Crawl_42B, Common_Crawl_840B, or Wikipedia\"", ")", "\n", "exit", "(", "0", ")", "\n", "\n", "\n", "", "dest_directory", "=", "DATA_DIR", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "dest_directory", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "dest_directory", ")", "\n", "", "filename", "=", "DATA_URL", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "\n", "filepath", "=", "os", ".", "path", ".", "join", "(", "dest_directory", ",", "filename", ")", "\n", "print", "(", "filepath", ")", "\n", "\n", "path", "=", "os", ".", "path", ".", "abspath", "(", "dest_directory", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "filepath", ")", ":", "\n", "        ", "def", "_progress", "(", "count", ",", "block_size", ",", "total_size", ")", ":", "\n", "            ", "sys", ".", "stdout", ".", "write", "(", "'\\rDownloading %s %.2f%%'", "%", "(", "filename", ",", "\n", "float", "(", "count", "*", "block_size", ")", "/", "float", "(", "total_size", ")", "*", "100.0", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "\n", "", "filepath", ",", "_", "=", "urllib", ".", "urlretrieve", "(", "DATA_URL", ",", "filepath", ")", "#, reporthook=_progress)", "\n", "\n", "\n", "zip_ref", "=", "zipfile", ".", "ZipFile", "(", "filepath", ",", "'r'", ")", "\n", "zip_ref", ".", "extractall", "(", "DATA_DIR", ")", "\n", "zip_ref", ".", "close", "(", ")", "\n", "", "return", "path", "\n", "", ""]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.CNN.loadData_Tokenizer": [[12, 41], ["numpy.random.seed", "numpy.concatenate", "numpy.array", "keras.preprocessing.text.Tokenizer", "keras.preprocessing.text.Tokenizer.fit_on_texts", "keras.preprocessing.text.Tokenizer.texts_to_sequences", "keras.preprocessing.sequence.pad_sequences", "print", "numpy.arange", "print", "open", "open.close", "print", "line.split", "len", "numpy.asarray", "len", "len", "len"], "function", ["None"], ["def", "loadData_Tokenizer", "(", "X_train", ",", "X_test", ",", "MAX_NB_WORDS", "=", "75000", ",", "MAX_SEQUENCE_LENGTH", "=", "1000", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "7", ")", "\n", "text", "=", "np", ".", "concatenate", "(", "(", "X_train", ",", "X_test", ")", ",", "axis", "=", "0", ")", "\n", "text", "=", "np", ".", "array", "(", "text", ")", "\n", "tokenizer", "=", "Tokenizer", "(", "num_words", "=", "MAX_NB_WORDS", ")", "\n", "tokenizer", ".", "fit_on_texts", "(", "text", ")", "\n", "sequences", "=", "tokenizer", ".", "texts_to_sequences", "(", "text", ")", "\n", "word_index", "=", "tokenizer", ".", "word_index", "\n", "text", "=", "pad_sequences", "(", "sequences", ",", "maxlen", "=", "MAX_SEQUENCE_LENGTH", ")", "\n", "print", "(", "'Found %s unique tokens.'", "%", "len", "(", "word_index", ")", ")", "\n", "indices", "=", "np", ".", "arange", "(", "text", ".", "shape", "[", "0", "]", ")", "\n", "# np.random.shuffle(indices)", "\n", "text", "=", "text", "[", "indices", "]", "\n", "print", "(", "text", ".", "shape", ")", "\n", "X_train", "=", "text", "[", "0", ":", "len", "(", "X_train", ")", ",", "]", "\n", "X_test", "=", "text", "[", "len", "(", "X_train", ")", ":", ",", "]", "\n", "embeddings_index", "=", "{", "}", "\n", "f", "=", "open", "(", "\".\\glove.6B.100d.txt\"", ",", "encoding", "=", "\"utf8\"", ")", "## GloVe file which could be download https://nlp.stanford.edu/projects/glove/", "\n", "for", "line", "in", "f", ":", "\n", "        ", "values", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "values", "[", "0", "]", "\n", "try", ":", "\n", "            ", "coefs", "=", "np", ".", "asarray", "(", "values", "[", "1", ":", "]", ",", "dtype", "=", "'float32'", ")", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "", "embeddings_index", "[", "word", "]", "=", "coefs", "\n", "", "f", ".", "close", "(", ")", "\n", "print", "(", "'Total %s word vectors.'", "%", "len", "(", "embeddings_index", ")", ")", "\n", "return", "(", "X_train", ",", "X_test", ",", "word_index", ",", "embeddings_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.CNN.Build_Model_CNN_Text": [[44, 114], ["keras.models.Sequential", "numpy.random.random", "word_index.items", "keras.layers.Embedding", "print", "range", "keras.layers.Input", "keras.layers.Embedding.", "keras.models.Model", "keras.models.Model.compile", "embeddings_index.get", "filter_sizes.append", "keras.layers.Reshape", "convs.append", "keras.layers.merge.Concatenate", "keras.layers.Conv2D", "keras.layers.AveragePooling2D", "keras.layers.Conv2D", "keras.layers.AveragePooling2D", "keras.layers.Dropout", "keras.layers.Flatten", "keras.layers.Dense", "keras.layers.Dropout", "keras.layers.Dense", "len", "keras.layers.Conv2D", "keras.layers.AveragePooling2D", "len", "len", "len", "print", "exit", "str", "str", "len", "len"], "function", ["None"], ["", "def", "Build_Model_CNN_Text", "(", "word_index", ",", "embeddings_index", ",", "nclasses", ",", "MAX_SEQUENCE_LENGTH", "=", "500", ",", "EMBEDDING_DIM", "=", "100", ",", "dropout", "=", "0.5", ")", ":", "\n", "\n", "    ", "\"\"\"\n        def buildModel_CNN(word_index, embeddings_index, nclasses, MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n        word_index in word index ,\n        embeddings_index is embeddings index, look at data_helper.py\n        nClasses is number of classes,\n        MAX_SEQUENCE_LENGTH is maximum lenght of text sequences,\n        EMBEDDING_DIM is an int value for dimention of word embedding look at data_helper.py\n    \"\"\"", "\n", "\n", "model", "=", "Sequential", "(", ")", "\n", "embedding_matrix", "=", "np", ".", "random", ".", "random", "(", "(", "len", "(", "word_index", ")", "+", "1", ",", "EMBEDDING_DIM", ")", ")", "\n", "for", "word", ",", "i", "in", "word_index", ".", "items", "(", ")", ":", "\n", "        ", "embedding_vector", "=", "embeddings_index", ".", "get", "(", "word", ")", "\n", "if", "embedding_vector", "is", "not", "None", ":", "\n", "# words not found in embedding index will be all-zeros.", "\n", "            ", "if", "len", "(", "embedding_matrix", "[", "i", "]", ")", "!=", "len", "(", "embedding_vector", ")", ":", "\n", "                ", "print", "(", "\"could not broadcast input array from shape\"", ",", "str", "(", "len", "(", "embedding_matrix", "[", "i", "]", ")", ")", ",", "\n", "\"into shape\"", ",", "str", "(", "len", "(", "embedding_vector", ")", ")", ",", "\" Please make sure your\"", "\n", "\" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\"", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "embedding_matrix", "[", "i", "]", "=", "embedding_vector", "\n", "\n", "", "", "embedding_layer", "=", "Embedding", "(", "len", "(", "word_index", ")", "+", "1", ",", "\n", "EMBEDDING_DIM", ",", "\n", "weights", "=", "[", "embedding_matrix", "]", ",", "\n", "input_length", "=", "MAX_SEQUENCE_LENGTH", ",", "\n", "trainable", "=", "True", ")", "\n", "\n", "# applying a more complex convolutional approach", "\n", "convs", "=", "[", "]", "\n", "filter_sizes", "=", "[", "]", "\n", "layer", "=", "5", "\n", "print", "(", "\"Filter  \"", ",", "layer", ")", "\n", "for", "fl", "in", "range", "(", "0", ",", "layer", ")", ":", "\n", "        ", "filter_sizes", ".", "append", "(", "(", "fl", "+", "2", ",", "fl", "+", "2", ")", ")", "\n", "\n", "", "node", "=", "128", "\n", "sequence_input", "=", "Input", "(", "shape", "=", "(", "MAX_SEQUENCE_LENGTH", ",", ")", ",", "dtype", "=", "'int32'", ")", "\n", "embedded_sequences", "=", "embedding_layer", "(", "sequence_input", ")", "\n", "emb", "=", "Reshape", "(", "(", "500", ",", "10", ",", "10", ")", ",", "input_shape", "=", "(", "500", ",", "100", ")", ")", "(", "embedded_sequences", ")", "\n", "\n", "for", "fsz", "in", "filter_sizes", ":", "\n", "        ", "l_conv", "=", "Conv2D", "(", "node", ",", "padding", "=", "\"same\"", ",", "kernel_size", "=", "fsz", ",", "activation", "=", "'relu'", ")", "(", "emb", ")", "\n", "l_pool", "=", "AveragePooling2D", "(", "pool_size", "=", "(", "5", ",", "1", ")", ",", "padding", "=", "\"same\"", ")", "(", "l_conv", ")", "\n", "#l_pool = Dropout(0.25)(l_pool)", "\n", "convs", ".", "append", "(", "l_pool", ")", "\n", "\n", "", "l_merge", "=", "Concatenate", "(", "axis", "=", "1", ")", "(", "convs", ")", "\n", "l_cov1", "=", "Conv2D", "(", "node", ",", "(", "5", ",", "5", ")", ",", "padding", "=", "\"same\"", ",", "activation", "=", "'relu'", ")", "(", "l_merge", ")", "\n", "l_cov1", "=", "AveragePooling2D", "(", "pool_size", "=", "(", "5", ",", "2", ")", ",", "padding", "=", "\"same\"", ")", "(", "l_cov1", ")", "\n", "l_cov2", "=", "Conv2D", "(", "node", ",", "(", "5", ",", "5", ")", ",", "padding", "=", "\"same\"", ",", "activation", "=", "'relu'", ")", "(", "l_cov1", ")", "\n", "l_pool2", "=", "AveragePooling2D", "(", "pool_size", "=", "(", "5", ",", "2", ")", ",", "padding", "=", "\"same\"", ")", "(", "l_cov2", ")", "\n", "l_cov2", "=", "Dropout", "(", "dropout", ")", "(", "l_pool2", ")", "\n", "l_flat", "=", "Flatten", "(", ")", "(", "l_cov2", ")", "\n", "l_dense", "=", "Dense", "(", "128", ",", "activation", "=", "'relu'", ")", "(", "l_flat", ")", "\n", "l_dense", "=", "Dropout", "(", "dropout", ")", "(", "l_dense", ")", "\n", "\n", "preds", "=", "Dense", "(", "nclasses", ",", "activation", "=", "'softmax'", ")", "(", "l_dense", ")", "\n", "model", "=", "Model", "(", "sequence_input", ",", "preds", ")", "\n", "\n", "model", ".", "compile", "(", "loss", "=", "'sparse_categorical_crossentropy'", ",", "\n", "optimizer", "=", "'adam'", ",", "\n", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "\n", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.RCNN.loadData_Tokenizer": [[14, 43], ["numpy.random.seed", "numpy.concatenate", "numpy.array", "keras.preprocessing.text.Tokenizer", "keras.preprocessing.text.Tokenizer.fit_on_texts", "keras.preprocessing.text.Tokenizer.texts_to_sequences", "keras.preprocessing.sequence.pad_sequences", "print", "numpy.arange", "print", "open", "open.close", "print", "line.split", "len", "numpy.asarray", "len", "len", "len"], "function", ["None"], ["def", "loadData_Tokenizer", "(", "X_train", ",", "X_test", ",", "MAX_NB_WORDS", "=", "75000", ",", "MAX_SEQUENCE_LENGTH", "=", "500", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "7", ")", "\n", "text", "=", "np", ".", "concatenate", "(", "(", "X_train", ",", "X_test", ")", ",", "axis", "=", "0", ")", "\n", "text", "=", "np", ".", "array", "(", "text", ")", "\n", "tokenizer", "=", "Tokenizer", "(", "num_words", "=", "MAX_NB_WORDS", ")", "\n", "tokenizer", ".", "fit_on_texts", "(", "text", ")", "\n", "sequences", "=", "tokenizer", ".", "texts_to_sequences", "(", "text", ")", "\n", "word_index", "=", "tokenizer", ".", "word_index", "\n", "text", "=", "pad_sequences", "(", "sequences", ",", "maxlen", "=", "MAX_SEQUENCE_LENGTH", ")", "\n", "print", "(", "'Found %s unique tokens.'", "%", "len", "(", "word_index", ")", ")", "\n", "indices", "=", "np", ".", "arange", "(", "text", ".", "shape", "[", "0", "]", ")", "\n", "# np.random.shuffle(indices)", "\n", "text", "=", "text", "[", "indices", "]", "\n", "print", "(", "text", ".", "shape", ")", "\n", "X_train", "=", "text", "[", "0", ":", "len", "(", "X_train", ")", ",", "]", "\n", "X_test", "=", "text", "[", "len", "(", "X_train", ")", ":", ",", "]", "\n", "embeddings_index", "=", "{", "}", "\n", "f", "=", "open", "(", "\".\\glove.6B.100d.txt\"", ",", "encoding", "=", "\"utf8\"", ")", "\n", "for", "line", "in", "f", ":", "\n", "        ", "values", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "values", "[", "0", "]", "\n", "try", ":", "\n", "            ", "coefs", "=", "np", ".", "asarray", "(", "values", "[", "1", ":", "]", ",", "dtype", "=", "'float32'", ")", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "", "embeddings_index", "[", "word", "]", "=", "coefs", "\n", "", "f", ".", "close", "(", ")", "\n", "print", "(", "'Total %s word vectors.'", "%", "len", "(", "embeddings_index", ")", ")", "\n", "return", "(", "X_train", ",", "X_test", ",", "word_index", ",", "embeddings_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.RCNN.Build_Model_RCNN_Text": [[45, 95], ["numpy.random.random", "word_index.items", "keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "embeddings_index.get", "keras.layers.Embedding", "keras.layers.Dropout", "keras.layers.Conv1D", "keras.layers.MaxPooling1D", "keras.layers.Conv1D", "keras.layers.MaxPooling1D", "keras.layers.Conv1D", "keras.layers.MaxPooling1D", "keras.layers.Conv1D", "keras.layers.MaxPooling1D", "keras.layers.LSTM", "keras.layers.LSTM", "keras.layers.LSTM", "keras.layers.LSTM", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Activation", "len", "len", "len", "print", "exit", "len", "str", "str", "len", "len"], "function", ["None"], ["", "def", "Build_Model_RCNN_Text", "(", "word_index", ",", "embeddings_index", ",", "nclasses", ",", "MAX_SEQUENCE_LENGTH", "=", "500", ",", "EMBEDDING_DIM", "=", "100", ")", ":", "\n", "\n", "    ", "kernel_size", "=", "2", "\n", "filters", "=", "256", "\n", "pool_size", "=", "2", "\n", "gru_node", "=", "256", "\n", "\n", "embedding_matrix", "=", "np", ".", "random", ".", "random", "(", "(", "len", "(", "word_index", ")", "+", "1", ",", "EMBEDDING_DIM", ")", ")", "\n", "for", "word", ",", "i", "in", "word_index", ".", "items", "(", ")", ":", "\n", "        ", "embedding_vector", "=", "embeddings_index", ".", "get", "(", "word", ")", "\n", "if", "embedding_vector", "is", "not", "None", ":", "\n", "# words not found in embedding index will be all-zeros.", "\n", "            ", "if", "len", "(", "embedding_matrix", "[", "i", "]", ")", "!=", "len", "(", "embedding_vector", ")", ":", "\n", "                ", "print", "(", "\"could not broadcast input array from shape\"", ",", "str", "(", "len", "(", "embedding_matrix", "[", "i", "]", ")", ")", ",", "\n", "\"into shape\"", ",", "str", "(", "len", "(", "embedding_vector", ")", ")", ",", "\" Please make sure your\"", "\n", "\" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\"", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "embedding_matrix", "[", "i", "]", "=", "embedding_vector", "\n", "\n", "\n", "\n", "", "", "model", "=", "Sequential", "(", ")", "\n", "model", ".", "add", "(", "Embedding", "(", "len", "(", "word_index", ")", "+", "1", ",", "\n", "EMBEDDING_DIM", ",", "\n", "weights", "=", "[", "embedding_matrix", "]", ",", "\n", "input_length", "=", "MAX_SEQUENCE_LENGTH", ",", "\n", "trainable", "=", "True", ")", ")", "\n", "model", ".", "add", "(", "Dropout", "(", "0.25", ")", ")", "\n", "model", ".", "add", "(", "Conv1D", "(", "filters", ",", "kernel_size", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "MaxPooling1D", "(", "pool_size", "=", "pool_size", ")", ")", "\n", "model", ".", "add", "(", "Conv1D", "(", "filters", ",", "kernel_size", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "MaxPooling1D", "(", "pool_size", "=", "pool_size", ")", ")", "\n", "model", ".", "add", "(", "Conv1D", "(", "filters", ",", "kernel_size", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "MaxPooling1D", "(", "pool_size", "=", "pool_size", ")", ")", "\n", "model", ".", "add", "(", "Conv1D", "(", "filters", ",", "kernel_size", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "MaxPooling1D", "(", "pool_size", "=", "pool_size", ")", ")", "\n", "model", ".", "add", "(", "LSTM", "(", "gru_node", ",", "return_sequences", "=", "True", ",", "recurrent_dropout", "=", "0.2", ")", ")", "\n", "model", ".", "add", "(", "LSTM", "(", "gru_node", ",", "return_sequences", "=", "True", ",", "recurrent_dropout", "=", "0.2", ")", ")", "\n", "model", ".", "add", "(", "LSTM", "(", "gru_node", ",", "return_sequences", "=", "True", ",", "recurrent_dropout", "=", "0.2", ")", ")", "\n", "model", ".", "add", "(", "LSTM", "(", "gru_node", ",", "recurrent_dropout", "=", "0.2", ")", ")", "\n", "model", ".", "add", "(", "Dense", "(", "1024", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "Dense", "(", "nclasses", ")", ")", "\n", "model", ".", "add", "(", "Activation", "(", "'softmax'", ")", ")", "\n", "\n", "model", ".", "compile", "(", "loss", "=", "'sparse_categorical_crossentropy'", ",", "\n", "optimizer", "=", "'adam'", ",", "\n", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.CRF.word2features": [[7, 49], ["word.lower", "word.isupper", "word.istitle", "word.isdigit", "features.update", "features.update", "len", "word1.lower", "word1.istitle", "word1.isupper", "word1.lower", "word1.istitle", "word1.isupper"], "function", ["None"], ["def", "word2features", "(", "sent", ",", "i", ")", ":", "\n", "    ", "word", "=", "sent", "[", "i", "]", "[", "0", "]", "\n", "postag", "=", "sent", "[", "i", "]", "[", "1", "]", "\n", "\n", "features", "=", "{", "\n", "'bias'", ":", "1.0", ",", "\n", "'word.lower()'", ":", "word", ".", "lower", "(", ")", ",", "\n", "'word[-3:]'", ":", "word", "[", "-", "3", ":", "]", ",", "\n", "'word[-2:]'", ":", "word", "[", "-", "2", ":", "]", ",", "\n", "'word.isupper()'", ":", "word", ".", "isupper", "(", ")", ",", "\n", "'word.istitle()'", ":", "word", ".", "istitle", "(", ")", ",", "\n", "'word.isdigit()'", ":", "word", ".", "isdigit", "(", ")", ",", "\n", "'postag'", ":", "postag", ",", "\n", "'postag[:2]'", ":", "postag", "[", ":", "2", "]", ",", "\n", "}", "\n", "if", "i", ">", "0", ":", "\n", "        ", "word1", "=", "sent", "[", "i", "-", "1", "]", "[", "0", "]", "\n", "postag1", "=", "sent", "[", "i", "-", "1", "]", "[", "1", "]", "\n", "features", ".", "update", "(", "{", "\n", "'-1:word.lower()'", ":", "word1", ".", "lower", "(", ")", ",", "\n", "'-1:word.istitle()'", ":", "word1", ".", "istitle", "(", ")", ",", "\n", "'-1:word.isupper()'", ":", "word1", ".", "isupper", "(", ")", ",", "\n", "'-1:postag'", ":", "postag1", ",", "\n", "'-1:postag[:2]'", ":", "postag1", "[", ":", "2", "]", ",", "\n", "}", ")", "\n", "", "else", ":", "\n", "        ", "features", "[", "'BOS'", "]", "=", "True", "\n", "\n", "", "if", "i", "<", "len", "(", "sent", ")", "-", "1", ":", "\n", "        ", "word1", "=", "sent", "[", "i", "+", "1", "]", "[", "0", "]", "\n", "postag1", "=", "sent", "[", "i", "+", "1", "]", "[", "1", "]", "\n", "features", ".", "update", "(", "{", "\n", "'+1:word.lower()'", ":", "word1", ".", "lower", "(", ")", ",", "\n", "'+1:word.istitle()'", ":", "word1", ".", "istitle", "(", ")", ",", "\n", "'+1:word.isupper()'", ":", "word1", ".", "isupper", "(", ")", ",", "\n", "'+1:postag'", ":", "postag1", ",", "\n", "'+1:postag[:2]'", ":", "postag1", "[", ":", "2", "]", ",", "\n", "}", ")", "\n", "", "else", ":", "\n", "        ", "features", "[", "'EOS'", "]", "=", "True", "\n", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.CRF.sent2features": [[51, 53], ["CRF.word2features", "range", "len"], "function", ["home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.CRF.word2features"], ["", "def", "sent2features", "(", "sent", ")", ":", "\n", "    ", "return", "[", "word2features", "(", "sent", ",", "i", ")", "for", "i", "in", "range", "(", "len", "(", "sent", ")", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.CRF.sent2labels": [[54, 56], ["None"], "function", ["None"], ["", "def", "sent2labels", "(", "sent", ")", ":", "\n", "    ", "return", "[", "label", "for", "token", ",", "postag", ",", "label", "in", "sent", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.CRF.sent2tokens": [[57, 59], ["None"], "function", ["None"], ["", "def", "sent2tokens", "(", "sent", ")", ":", "\n", "    ", "return", "[", "token", "for", "token", ",", "postag", ",", "label", "in", "sent", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.DNN.TFIDF": [[8, 14], ["sklearn.feature_extraction.text.TfidfVectorizer", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform().toarray", "sklearn.feature_extraction.text.TfidfVectorizer.transform().toarray", "print", "str", "sklearn.feature_extraction.text.TfidfVectorizer.fit_transform", "sklearn.feature_extraction.text.TfidfVectorizer.transform", "numpy.array"], "function", ["None"], ["def", "TFIDF", "(", "X_train", ",", "X_test", ",", "MAX_NB_WORDS", "=", "75000", ")", ":", "\n", "    ", "vectorizer_x", "=", "TfidfVectorizer", "(", "max_features", "=", "MAX_NB_WORDS", ")", "\n", "X_train", "=", "vectorizer_x", ".", "fit_transform", "(", "X_train", ")", ".", "toarray", "(", ")", "\n", "X_test", "=", "vectorizer_x", ".", "transform", "(", "X_test", ")", ".", "toarray", "(", ")", "\n", "print", "(", "\"tf-idf with\"", ",", "str", "(", "np", ".", "array", "(", "X_train", ")", ".", "shape", "[", "1", "]", ")", ",", "\"features\"", ")", "\n", "return", "(", "X_train", ",", "X_test", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.DNN.Build_Model_DNN_Text": [[16, 39], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "range", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Dense", "keras.layers.Dropout", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dropout"], "function", ["None"], ["", "def", "Build_Model_DNN_Text", "(", "shape", ",", "nClasses", ",", "dropout", "=", "0.5", ")", ":", "\n", "    ", "\"\"\"\n    buildModel_DNN_Tex(shape, nClasses,dropout)\n    Build Deep neural networks Model for text classification\n    Shape is input feature space\n    nClasses is number of classes\n    \"\"\"", "\n", "model", "=", "Sequential", "(", ")", "\n", "node", "=", "512", "# number of nodes", "\n", "nLayers", "=", "4", "# number of  hidden layer", "\n", "\n", "model", ".", "add", "(", "Dense", "(", "node", ",", "input_dim", "=", "shape", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "Dropout", "(", "dropout", ")", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "nLayers", ")", ":", "\n", "        ", "model", ".", "add", "(", "Dense", "(", "node", ",", "input_dim", "=", "node", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "Dropout", "(", "dropout", ")", ")", "\n", "", "model", ".", "add", "(", "Dense", "(", "nClasses", ",", "activation", "=", "'softmax'", ")", ")", "\n", "\n", "model", ".", "compile", "(", "loss", "=", "'sparse_categorical_crossentropy'", ",", "\n", "optimizer", "=", "'adam'", ",", "\n", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.RNN.loadData_Tokenizer": [[12, 42], ["numpy.random.seed", "numpy.concatenate", "numpy.array", "keras.preprocessing.text.Tokenizer", "keras.preprocessing.text.Tokenizer.fit_on_texts", "keras.preprocessing.text.Tokenizer.texts_to_sequences", "keras.preprocessing.sequence.pad_sequences", "print", "numpy.arange", "print", "open", "open.close", "print", "line.split", "len", "numpy.asarray", "len", "len", "len"], "function", ["None"], ["def", "loadData_Tokenizer", "(", "X_train", ",", "X_test", ",", "MAX_NB_WORDS", "=", "75000", ",", "MAX_SEQUENCE_LENGTH", "=", "500", ")", ":", "\n", "    ", "np", ".", "random", ".", "seed", "(", "7", ")", "\n", "text", "=", "np", ".", "concatenate", "(", "(", "X_train", ",", "X_test", ")", ",", "axis", "=", "0", ")", "\n", "text", "=", "np", ".", "array", "(", "text", ")", "\n", "tokenizer", "=", "Tokenizer", "(", "num_words", "=", "MAX_NB_WORDS", ")", "\n", "tokenizer", ".", "fit_on_texts", "(", "text", ")", "\n", "sequences", "=", "tokenizer", ".", "texts_to_sequences", "(", "text", ")", "\n", "word_index", "=", "tokenizer", ".", "word_index", "\n", "text", "=", "pad_sequences", "(", "sequences", ",", "maxlen", "=", "MAX_SEQUENCE_LENGTH", ")", "\n", "print", "(", "'Found %s unique tokens.'", "%", "len", "(", "word_index", ")", ")", "\n", "indices", "=", "np", ".", "arange", "(", "text", ".", "shape", "[", "0", "]", ")", "\n", "# np.random.shuffle(indices)", "\n", "text", "=", "text", "[", "indices", "]", "\n", "print", "(", "text", ".", "shape", ")", "\n", "X_train", "=", "text", "[", "0", ":", "len", "(", "X_train", ")", ",", "]", "\n", "X_test", "=", "text", "[", "len", "(", "X_train", ")", ":", ",", "]", "\n", "embeddings_index", "=", "{", "}", "\n", "f", "=", "open", "(", "\"C:\\\\Users\\\\kamran\\\\Documents\\\\GitHub\\\\RMDL\\\\Examples\\\\Glove\\\\glove.6B.50d.txt\"", ",", "encoding", "=", "\"utf8\"", ")", "\n", "for", "line", "in", "f", ":", "\n", "\n", "        ", "values", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "values", "[", "0", "]", "\n", "try", ":", "\n", "            ", "coefs", "=", "np", ".", "asarray", "(", "values", "[", "1", ":", "]", ",", "dtype", "=", "'float32'", ")", "\n", "", "except", ":", "\n", "            ", "pass", "\n", "", "embeddings_index", "[", "word", "]", "=", "coefs", "\n", "", "f", ".", "close", "(", ")", "\n", "print", "(", "'Total %s word vectors.'", "%", "len", "(", "embeddings_index", ")", ")", "\n", "return", "(", "X_train", ",", "X_test", ",", "word_index", ",", "embeddings_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.code.RNN.Build_Model_RNN_Text": [[45, 89], ["keras.models.Sequential", "numpy.random.random", "word_index.items", "keras.models.Sequential.add", "print", "range", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "embeddings_index.get", "keras.layers.Embedding", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.layers.GRU", "keras.layers.Dense", "keras.layers.GRU", "keras.layers.Dropout", "len", "len", "len", "print", "exit", "len", "str", "str", "len", "len"], "function", ["None"], ["", "def", "Build_Model_RNN_Text", "(", "word_index", ",", "embeddings_index", ",", "nclasses", ",", "MAX_SEQUENCE_LENGTH", "=", "500", ",", "EMBEDDING_DIM", "=", "50", ",", "dropout", "=", "0.5", ")", ":", "\n", "    ", "\"\"\"\n    def buildModel_RNN(word_index, embeddings_index, nclasses,  MAX_SEQUENCE_LENGTH=500, EMBEDDING_DIM=50, dropout=0.5):\n    word_index in word index ,\n    embeddings_index is embeddings index, look at data_helper.py\n    nClasses is number of classes,\n    MAX_SEQUENCE_LENGTH is maximum lenght of text sequences\n    \"\"\"", "\n", "\n", "model", "=", "Sequential", "(", ")", "\n", "hidden_layer", "=", "3", "\n", "gru_node", "=", "256", "\n", "\n", "embedding_matrix", "=", "np", ".", "random", ".", "random", "(", "(", "len", "(", "word_index", ")", "+", "1", ",", "EMBEDDING_DIM", ")", ")", "\n", "for", "word", ",", "i", "in", "word_index", ".", "items", "(", ")", ":", "\n", "        ", "embedding_vector", "=", "embeddings_index", ".", "get", "(", "word", ")", "\n", "if", "embedding_vector", "is", "not", "None", ":", "\n", "# words not found in embedding index will be all-zeros.", "\n", "            ", "if", "len", "(", "embedding_matrix", "[", "i", "]", ")", "!=", "len", "(", "embedding_vector", ")", ":", "\n", "                ", "print", "(", "\"could not broadcast input array from shape\"", ",", "str", "(", "len", "(", "embedding_matrix", "[", "i", "]", ")", ")", ",", "\n", "\"into shape\"", ",", "str", "(", "len", "(", "embedding_vector", ")", ")", ",", "\" Please make sure your\"", "\n", "\" EMBEDDING_DIM is equal to embedding_vector file ,GloVe,\"", ")", "\n", "exit", "(", "1", ")", "\n", "", "embedding_matrix", "[", "i", "]", "=", "embedding_vector", "\n", "", "", "model", ".", "add", "(", "Embedding", "(", "len", "(", "word_index", ")", "+", "1", ",", "\n", "EMBEDDING_DIM", ",", "\n", "weights", "=", "[", "embedding_matrix", "]", ",", "\n", "input_length", "=", "MAX_SEQUENCE_LENGTH", ",", "\n", "trainable", "=", "True", ")", ")", "\n", "\n", "\n", "print", "(", "gru_node", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "hidden_layer", ")", ":", "\n", "        ", "model", ".", "add", "(", "GRU", "(", "gru_node", ",", "return_sequences", "=", "True", ",", "recurrent_dropout", "=", "0.2", ")", ")", "\n", "model", ".", "add", "(", "Dropout", "(", "dropout", ")", ")", "\n", "", "model", ".", "add", "(", "GRU", "(", "gru_node", ",", "recurrent_dropout", "=", "0.2", ")", ")", "\n", "#model.add(Dense(, activation='relu'))", "\n", "model", ".", "add", "(", "Dense", "(", "nclasses", ",", "activation", "=", "'softmax'", ")", ")", "\n", "\n", "\n", "model", ".", "compile", "(", "loss", "=", "'sparse_categorical_crossentropy'", ",", "\n", "optimizer", "=", "'adam'", ",", "\n", "metrics", "=", "[", "'accuracy'", "]", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierRNN.AttLayer.__init__": [[126, 130], ["keras.initializations.get", "keras.engine.topology.Layer.__init__"], "methods", ["home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierHATT.AttLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "init", "=", "initializations", ".", "get", "(", "'normal'", ")", "\n", "#self.input_spec = [InputSpec(ndim=3)]", "\n", "super", "(", "AttLayer", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierRNN.AttLayer.build": [[131, 138], ["textClassifierRNN.AttLayer.init", "super().build", "len"], "methods", ["home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierHATT.AttLayer.build"], ["", "def", "build", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "assert", "len", "(", "input_shape", ")", "==", "3", "\n", "#self.W = self.init((input_shape[-1],1))", "\n", "self", ".", "W", "=", "self", ".", "init", "(", "(", "input_shape", "[", "-", "1", "]", ",", ")", ")", "\n", "#self.input_spec = [InputSpec(shape=input_shape)]", "\n", "self", ".", "trainable_weights", "=", "[", "self", ".", "W", "]", "\n", "super", "(", "AttLayer", ",", "self", ")", ".", "build", "(", "input_shape", ")", "# be sure you call this somewhere!", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierRNN.AttLayer.call": [[139, 147], ["keras.backend.tanh", "keras.backend.exp", "weighted_input.sum", "keras.backend.dot", "keras.backend.sum().dimshuffle", "weights.dimshuffle", "keras.backend.sum"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "\n", "        ", "eij", "=", "K", ".", "tanh", "(", "K", ".", "dot", "(", "x", ",", "self", ".", "W", ")", ")", "\n", "\n", "ai", "=", "K", ".", "exp", "(", "eij", ")", "\n", "weights", "=", "ai", "/", "K", ".", "sum", "(", "ai", ",", "axis", "=", "1", ")", ".", "dimshuffle", "(", "0", ",", "'x'", ")", "\n", "\n", "weighted_input", "=", "x", "*", "weights", ".", "dimshuffle", "(", "0", ",", "1", ",", "'x'", ")", "\n", "return", "weighted_input", ".", "sum", "(", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierRNN.AttLayer.get_output_shape_for": [[148, 150], ["None"], "methods", ["None"], ["", "def", "get_output_shape_for", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "return", "(", "input_shape", "[", "0", "]", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierRNN.clean_str": [[35, 44], ["re.sub", "re.sub", "re.sub", "re.sub.strip().lower", "re.sub.strip"], "function", ["None"], ["def", "clean_str", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Tokenization/string cleaning for dataset\n    Every dataset is lower cased except\n    \"\"\"", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\\\\"", ",", "\"\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'\"", ",", "\"\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\\"\"", ",", "\"\"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierHATT.AttLayer.__init__": [[161, 165], ["keras.initializations.get", "keras.engine.topology.Layer.__init__"], "methods", ["home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierHATT.AttLayer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "init", "=", "initializations", ".", "get", "(", "'normal'", ")", "\n", "#self.input_spec = [InputSpec(ndim=3)]", "\n", "super", "(", "AttLayer", ",", "self", ")", ".", "__init__", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierHATT.AttLayer.build": [[166, 173], ["textClassifierHATT.AttLayer.init", "super().build", "len"], "methods", ["home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierHATT.AttLayer.build"], ["", "def", "build", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "assert", "len", "(", "input_shape", ")", "==", "3", "\n", "#self.W = self.init((input_shape[-1],1))", "\n", "self", ".", "W", "=", "self", ".", "init", "(", "(", "input_shape", "[", "-", "1", "]", ",", ")", ")", "\n", "#self.input_spec = [InputSpec(shape=input_shape)]", "\n", "self", ".", "trainable_weights", "=", "[", "self", ".", "W", "]", "\n", "super", "(", "AttLayer", ",", "self", ")", ".", "build", "(", "input_shape", ")", "# be sure you call this somewhere!", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierHATT.AttLayer.call": [[174, 182], ["keras.backend.tanh", "keras.backend.exp", "weighted_input.sum", "keras.backend.dot", "keras.backend.sum().dimshuffle", "weights.dimshuffle", "keras.backend.sum"], "methods", ["None"], ["", "def", "call", "(", "self", ",", "x", ",", "mask", "=", "None", ")", ":", "\n", "        ", "eij", "=", "K", ".", "tanh", "(", "K", ".", "dot", "(", "x", ",", "self", ".", "W", ")", ")", "\n", "\n", "ai", "=", "K", ".", "exp", "(", "eij", ")", "\n", "weights", "=", "ai", "/", "K", ".", "sum", "(", "ai", ",", "axis", "=", "1", ")", ".", "dimshuffle", "(", "0", ",", "'x'", ")", "\n", "\n", "weighted_input", "=", "x", "*", "weights", ".", "dimshuffle", "(", "0", ",", "1", ",", "'x'", ")", "\n", "return", "weighted_input", ".", "sum", "(", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierHATT.AttLayer.get_output_shape_for": [[183, 185], ["None"], "methods", ["None"], ["", "def", "get_output_shape_for", "(", "self", ",", "input_shape", ")", ":", "\n", "        ", "return", "(", "input_shape", "[", "0", "]", ",", "input_shape", "[", "-", "1", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierHATT.clean_str": [[36, 45], ["re.sub", "re.sub", "re.sub", "re.sub.strip().lower", "re.sub.strip"], "function", ["None"], ["def", "clean_str", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Tokenization/string cleaning for dataset\n    Every dataset is lower cased except\n    \"\"\"", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\\\\"", ",", "\"\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'\"", ",", "\"\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\\"\"", ",", "\"\"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.kk7nc_Text_Classification.Hierarchical_Attention_Networks.textClassifierConv.clean_str": [[31, 40], ["re.sub", "re.sub", "re.sub", "re.sub.strip().lower", "re.sub.strip"], "function", ["None"], ["def", "clean_str", "(", "string", ")", ":", "\n", "    ", "\"\"\"\n    Tokenization/string cleaning for dataset\n    Every dataset is lower cased except\n    \"\"\"", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\\\\"", ",", "\"\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'\"", ",", "\"\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\\"\"", ",", "\"\"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "\n"]]}