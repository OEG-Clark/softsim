{"home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.trainer.train_actor": [[27, 38], ["range", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log.gather", "optimizer[].zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "optimizer[].step", "Pi[].parameters", "td_error.detach"], "function", ["home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.step"], ["def", "train_actor", "(", "Experience", ",", "Pi", ",", "td_error", ",", "device", ",", "optimizer", ")", ":", "\n", "    ", "S", ",", "A", ",", "R", ",", "S_Prime", "=", "Experience", "\n", "for", "i", "in", "range", "(", "n_agents", ")", ":", "\n", "        ", "log_dist", "=", "torch", ".", "log", "(", "Pi", "[", "i", "]", "(", "S", "[", ":", ",", "i", "]", ")", ")", "\n", "log_pi_a", "=", "log_dist", ".", "gather", "(", "-", "1", ",", "A", "[", ":", ",", ":", ",", "i", "]", ")", "\n", "loss", "=", "-", "(", "td_error", ".", "detach", "(", ")", "*", "log_pi_a", ")", ".", "sum", "(", ")", "\n", "\n", "optimizer", "[", "i", "]", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "Pi", "[", "i", "]", ".", "parameters", "(", ")", ",", "10", ")", "\n", "optimizer", "[", "i", "]", ".", "step", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.trainer.train_critic": [[39, 58], ["V.train", "VTarget.eval", "S.reshape", "S_Prime.reshape", "R.sum.sum", "V", "VTarget", "optimizer.zero_grad", "loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "targets.detach", "V.parameters", "VTarget.load_state_dict", "V.state_dict"], "function", ["home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.trainer.train", "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.step"], ["", "", "def", "train_critic", "(", "t", ",", "Experience", ",", "V", ",", "VTarget", ",", "optimizer", ")", ":", "\n", "    ", "V", ".", "train", "(", ")", "\n", "VTarget", ".", "eval", "(", ")", "\n", "S", ",", "A", ",", "R", ",", "S_Prime", "=", "Experience", "\n", "State", "=", "S", ".", "reshape", "(", "S", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "State_Prime", "=", "S_Prime", ".", "reshape", "(", "S_Prime", ".", "shape", "[", "0", "]", ",", "-", "1", ")", "\n", "R", "=", "R", ".", "sum", "(", "-", "1", ")", "\n", "v", "=", "V", "(", "State", ")", "\n", "vtarget", "=", "VTarget", "(", "State_Prime", ")", "\n", "targets", "=", "R", "+", "gamma", "*", "vtarget", "\n", "td_error", "=", "targets", ".", "detach", "(", ")", "-", "v", "\n", "loss", "=", "(", "td_error", "**", "2", ")", ".", "sum", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "V", ".", "parameters", "(", ")", ",", "10", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "if", "t", ">", "0", "and", "t", "%", "10", "==", "0", ":", "\n", "        ", "VTarget", ".", "load_state_dict", "(", "V", ".", "state_dict", "(", ")", ")", "\n", "", "return", "td_error", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.trainer.train": [[59, 115], ["Environment.ENV", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.device", "torch.device", "torch.device", "torch.device", "network.QCritic().to", "copy.deepcopy", "copy.deepcopy.load_state_dict", "network.ReplayBuffer", "torch.Adam", "range", "network.QActor().to", "QCritic().to.state_dict", "torch.Adam", "torch.optim.lr_scheduler.CosineAnnealingLR", "QCritic().to.parameters", "range", "Environment.ENV.reset", "numpy.copy", "range", "print", "network.ReplayBuffer.make_batch", "trainer.train_critic", "trainer.train_actor", "range", "range", "torch.save", "torch.save", "torch.save", "torch.save", "range", "network.QCritic", "actors[].parameters", "range", "range", "actors[].q_layer.static_on", "np.copy.copy", "range", "numpy.array", "Environment.ENV.step", "network.ReplayBuffer.put_data", "Scheduler[].step", "torch.save", "torch.save", "torch.save", "torch.save", "QCritic().to.state_dict", "network.QActor", "np.array.append", "actors[].state_dict", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.from_numpy().unsqueeze().to", "torch.distributions.Categorical().sample().item", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.from_numpy().unsqueeze", "torch.distributions.Categorical().sample", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.distributions.Categorical"], "function", ["home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.reset", "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.network.ReplayBuffer.make_batch", "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.trainer.train_critic", "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.trainer.train_actor", "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.step", "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.network.ReplayBuffer.put_data", "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.step"], ["", "def", "train", "(", ")", ":", "\n", "    ", "env", "=", "ENV", "(", ")", "\n", "use_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "use_cuda", "else", "\"cpu\"", ")", "\n", "actors", "=", "[", "QActor", "(", ")", ".", "to", "(", "device", ")", "for", "_", "in", "range", "(", "n_agents", ")", "]", "\n", "critic", "=", "QCritic", "(", ")", ".", "to", "(", "device", ")", "\n", "critic_target", "=", "copy", ".", "deepcopy", "(", "critic", ")", "\n", "critic_target", ".", "load_state_dict", "(", "critic", ".", "state_dict", "(", ")", ")", "\n", "replay", "=", "ReplayBuffer", "(", "device", ")", "\n", "Qoptimizer", "=", "[", "optim", ".", "Adam", "(", "actors", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "a_lr", ")", "for", "i", "in", "range", "(", "n_agents", ")", "]", "\n", "Scheduler", "=", "[", "CosineAnnealingLR", "(", "Qoptimizer", "[", "i", "]", ",", "T_max", "=", "n_epochs", ")", "for", "i", "in", "range", "(", "n_agents", ")", "]", "\n", "Voptimizer", "=", "optim", ".", "Adam", "(", "critic", ".", "parameters", "(", ")", ",", "lr", "=", "c_lr", ")", "\n", "\n", "if", "static", ":", "\n", "        ", "for", "i", "in", "range", "(", "n_agents", ")", ":", "\n", "\n", "            ", "actors", "[", "i", "]", ".", "q_layer", ".", "static_on", "(", "wires_per_block", "=", "wires_per_block", ")", "\n", "\n", "", "", "for", "epoch", "in", "range", "(", "1", ",", "n_epochs", "+", "1", ")", ":", "\n", "\n", "        ", "s", "=", "env", ".", "reset", "(", ")", "\n", "s_prime", "=", "np", ".", "copy", "(", "s", ")", "\n", "score", "=", "0", "\n", "for", "t", "in", "range", "(", "env", ".", "T_MAX", ")", ":", "\n", "\n", "            ", "s", "=", "s_prime", ".", "copy", "(", ")", "\n", "a", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "n_agents", ")", ":", "\n", "                ", "action_dist", "=", "actors", "[", "i", "]", "(", "torch", ".", "from_numpy", "(", "s", "[", "i", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "device", ",", "dtype", "=", "torch", ".", "float", ")", ")", "\n", "a", ".", "append", "(", "Categorical", "(", "action_dist", ")", ".", "sample", "(", ")", ".", "item", "(", ")", ")", "\n", "", "a", "=", "np", ".", "array", "(", "a", ")", "\n", "r", ",", "s_prime", ",", "done", "=", "env", ".", "step", "(", "a", ")", "\n", "transition", "=", "[", "s", ",", "a", ",", "r", ",", "s_prime", "]", "\n", "replay", ".", "put_data", "(", "transition", ")", "\n", "score", "+=", "r", "\n", "", "print", "(", "f'Epoch {epoch}/ Reward\\t'", ",", "score", ")", "\n", "\n", "experience", "=", "replay", ".", "make_batch", "(", ")", "\n", "td_error", "=", "train_critic", "(", "t", "=", "epoch", ",", "\n", "Experience", "=", "experience", ",", "\n", "V", "=", "critic", ",", "\n", "VTarget", "=", "critic_target", ",", "\n", "optimizer", "=", "Voptimizer", ")", "\n", "\n", "train_actor", "(", "Experience", "=", "experience", ",", "\n", "Pi", "=", "actors", ",", "\n", "td_error", "=", "td_error", ",", "\n", "device", "=", "device", ",", "\n", "optimizer", "=", "Qoptimizer", ")", "\n", "\n", "for", "i", "in", "range", "(", "n_agents", ")", ":", "\n", "            ", "Scheduler", "[", "i", "]", ".", "step", "(", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "n_agents", ")", ":", "\n", "            ", "torch", ".", "save", "(", "actors", "[", "i", "]", ".", "state_dict", "(", ")", ",", "'./Qagent.pkl'", ")", "\n", "", "torch", ".", "save", "(", "critic", ".", "state_dict", "(", ")", ",", "'./Qcritic.pkl'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.network.QActor.__init__": [[59, 66], ["torchquantum.QuantumModule.__init__", "torchquantum.QuantumDevice", "torchquantum.QuantumDevice", "torchquantum.GeneralEncoder", "torchquantum.GeneralEncoder", "network.QActor.QLayer", "torchquantum.MeasureAll", "torchquantum.MeasureAll"], "methods", ["home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.__init__"], ["", "", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_wires", "=", "4", "\n", "self", ".", "q_device", "=", "tq", ".", "QuantumDevice", "(", "n_wires", "=", "self", ".", "n_wires", ")", "\n", "self", ".", "encoder", "=", "tq", ".", "GeneralEncoder", "(", "tq", ".", "encoder_op_list_name_dict", "[", "'4_ry'", "]", ")", "\n", "self", ".", "q_layer", "=", "self", ".", "QLayer", "(", ")", "\n", "self", ".", "measure", "=", "tq", ".", "MeasureAll", "(", "tq", ".", "PauliZ", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.network.QActor.forward": [[67, 82], ["torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "network.QActor.qiskit_processor.process_parameterized", "network.QActor.encoder", "network.QActor.q_layer", "network.QActor.measure"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "use_qiskit", "=", "False", ")", ":", "\n", "# bsz = x.shape[0]", "\n", "# x = F.avg_pool2d(x, 6).view(bsz, 16)", "\n", "\n", "        ", "if", "use_qiskit", ":", "\n", "            ", "x", "=", "self", ".", "qiskit_processor", ".", "process_parameterized", "(", "self", ".", "q_device", ",", "self", ".", "encoder", ",", "self", ".", "q_layer", ",", "self", ".", "measure", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder", "(", "self", ".", "q_device", ",", "x", ")", "\n", "self", ".", "q_layer", "(", "self", ".", "q_device", ")", "\n", "x", "=", "self", ".", "measure", "(", "self", ".", "q_device", ")", "\n", "\n", "# x = x.reshape(bsz, 2, 2).sum(-1).squeeze()", "\n", "", "x", "=", "F", ".", "softmax", "(", "x", "*", "4", ",", "dim", "=", "1", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.network.QCritic.__init__": [[126, 133], ["torchquantum.QuantumModule.__init__", "torchquantum.QuantumDevice", "torchquantum.QuantumDevice", "torchquantum.GeneralEncoder", "torchquantum.GeneralEncoder", "network.QCritic.QLayer", "torchquantum.MeasureAll", "torchquantum.MeasureAll"], "methods", ["home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.__init__"], ["", "", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "n_wires", "=", "4", "\n", "self", ".", "q_device", "=", "tq", ".", "QuantumDevice", "(", "n_wires", "=", "self", ".", "n_wires", ")", "\n", "self", ".", "encoder", "=", "tq", ".", "GeneralEncoder", "(", "tq", ".", "encoder_op_list_name_dict", "[", "'4x4_ryzxy'", "]", ")", "\n", "self", ".", "q_layer", "=", "self", ".", "QLayer", "(", ")", "\n", "self", ".", "measure", "=", "tq", ".", "MeasureAll", "(", "tq", ".", "PauliZ", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.network.QCritic.forward": [[134, 146], ["network.QCritic.qiskit_processor.process_parameterized", "network.QCritic.encoder", "network.QCritic.q_layer", "network.QCritic.measure", "network.QCritic.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "use_qiskit", "=", "False", ")", ":", "\n", "# bsz = x.shape[0]", "\n", "# x = F.avg_pool2d(x, 6).view(bsz, 16)", "\n", "\n", "        ", "if", "use_qiskit", ":", "\n", "            ", "x", "=", "self", ".", "qiskit_processor", ".", "process_parameterized", "(", "self", ".", "q_device", ",", "self", ".", "encoder", ",", "self", ".", "q_layer", ",", "self", ".", "measure", ",", "x", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "encoder", "(", "self", ".", "q_device", ",", "x", ")", "\n", "self", ".", "q_layer", "(", "self", ".", "q_device", ")", "\n", "x", "=", "self", ".", "measure", "(", "self", ".", "q_device", ")", "\n", "", "x", "=", "x", ".", "sum", "(", "-", "1", ")", "*", "20", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.network.ReplayBuffer.__init__": [[148, 151], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "device", ")", ":", "\n", "        ", "self", ".", "data", "=", "[", "]", "\n", "self", ".", "device", "=", "device", "\n", "", "def", "put_data", "(", "self", ",", "transition", ")", ":", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.network.ReplayBuffer.put_data": [[151, 153], ["network.ReplayBuffer.data.append"], "methods", ["None"], ["", "def", "put_data", "(", "self", ",", "transition", ")", ":", "\n", "        ", "self", ".", "data", ".", "append", "(", "transition", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.network.ReplayBuffer.make_batch": [[154, 167], ["s_lst.append", "a_lst.append", "r_lst.append", "s_prime_lst.append", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor().to", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "make_batch", "(", "self", ")", ":", "\n", "        ", "s_lst", ",", "a_lst", ",", "r_lst", ",", "s_prime_lst", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "transition", "in", "self", ".", "data", ":", "\n", "            ", "s", ",", "a", ",", "r", ",", "s_prime", "=", "transition", "\n", "s_lst", ".", "append", "(", "s", ")", "\n", "a_lst", ".", "append", "(", "[", "a", "]", ")", "\n", "r_lst", ".", "append", "(", "[", "r", "]", ")", "\n", "s_prime_lst", ".", "append", "(", "s_prime", ")", "\n", "\n", "", "s", ",", "a", ",", "r", ",", "s_prime", "=", "torch", ".", "tensor", "(", "s_lst", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "self", ".", "device", ")", ",", "torch", ".", "tensor", "(", "a_lst", ",", "dtype", "=", "torch", ".", "int64", ")", ".", "to", "(", "self", ".", "device", ")", ",", "torch", ".", "tensor", "(", "r_lst", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "self", ".", "device", ")", ",", "torch", ".", "tensor", "(", "s_prime_lst", ",", "dtype", "=", "torch", ".", "float", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "data", "=", "[", "]", "\n", "return", "s", ",", "a", ",", "r", ",", "s_prime", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.__init__": [[5, 20], ["numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "numpy.array"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "A_EDGE", "=", "0.3", "\n", "self", ".", "ACTION_SPACE", "=", "np", ".", "array", "(", "[", "0.1", ",", "0.2", "]", ")", "\n", "self", ".", "DEPARTURE_CLOUD", "=", "np", ".", "array", "(", "[", "1", ",", "1", "]", ")", "*", "0.3", "\n", "self", ".", "CAPACITY_EDGE", "=", "np", ".", "array", "(", "[", "1", ",", "1", ",", "1", ",", "1", "]", ")", "\n", "self", ".", "CAPACITY_CLOUD", "=", "np", ".", "array", "(", "[", "1", ",", "1", "]", ")", "\n", "self", ".", "N_AGENTS", "=", "4", "\n", "self", ".", "N_CLOUD", "=", "2", "\n", "self", ".", "T_MAX", "=", "30", "\n", "self", ".", "time", "=", "0", "\n", "self", ".", "q_prev_edge", "=", "np", ".", "array", "(", "[", "0.8", ",", "0.8", ",", "0.2", ",", "0.2", "]", ")", "\n", "self", ".", "q_curr_edge", "=", "np", ".", "array", "(", "[", "0.8", ",", "0.8", ",", "0.2", ",", "0.2", "]", ")", "\n", "self", ".", "q_curr_cloud", "=", "np", ".", "array", "(", "[", "0.5", ",", "0.5", "]", ")", "\n", "self", ".", "action_dim", "=", "4", "\n", "self", ".", "state_dim", "=", "4", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.get_state": [[21, 23], ["numpy.array", "numpy.hstack", "range"], "methods", ["None"], ["", "def", "get_state", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "array", "(", "[", "np", ".", "hstack", "(", "[", "self", ".", "q_prev_edge", "[", "i", "]", ",", "self", ".", "q_curr_edge", "[", "i", "]", ",", "self", ".", "q_curr_cloud", "]", ")", "for", "i", "in", "range", "(", "self", ".", "N_AGENTS", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.step": [[24, 72], ["numpy.copy", "numpy.zeros", "numpy.zeros", "range", "numpy.clip", "numpy.array", "numpy.clip", "Environment.ENV.get_state", "numpy.random.rand", "numpy.abs", "numpy.abs", "numpy.abs", "numpy.abs", "Environment.ENV.q_loss_cloud.sum", "Environment.ENV.q_stall_cloud.sum", "range"], "methods", ["home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.get_state"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "        ", "self", ".", "q_prev_edge", "=", "np", ".", "copy", "(", "self", ".", "q_curr_edge", ")", "\n", "self", ".", "indicator_edge", "=", "np", ".", "zeros", "(", "self", ".", "N_AGENTS", ")", "\n", "self", ".", "actions", "=", "np", ".", "zeros", "(", "self", ".", "N_AGENTS", ")", "\n", "\n", "for", "n", "in", "range", "(", "self", ".", "N_AGENTS", ")", ":", "\n", "            ", "if", "actions", "[", "n", "]", "==", "0", ":", "\n", "                ", "self", ".", "indicator_edge", "[", "n", "]", "=", "0", "\n", "self", ".", "actions", "[", "n", "]", "=", "self", ".", "ACTION_SPACE", "[", "0", "]", "\n", "", "elif", "actions", "[", "n", "]", "==", "1", ":", "\n", "                ", "self", ".", "indicator_edge", "[", "n", "]", "=", "0", "\n", "self", ".", "actions", "[", "n", "]", "=", "self", ".", "ACTION_SPACE", "[", "1", "]", "\n", "", "elif", "actions", "[", "n", "]", "==", "2", ":", "\n", "                ", "self", ".", "indicator_edge", "[", "n", "]", "=", "1", "\n", "self", ".", "actions", "[", "n", "]", "=", "self", ".", "ACTION_SPACE", "[", "0", "]", "\n", "", "elif", "actions", "[", "n", "]", "==", "3", ":", "\n", "                ", "self", ".", "indicator_edge", "[", "n", "]", "=", "1", "\n", "self", ".", "actions", "[", "n", "]", "=", "self", ".", "ACTION_SPACE", "[", "1", "]", "\n", "\n", "\n", "", "", "self", ".", "arrival_edge", "=", "np", ".", "random", ".", "rand", "(", "self", ".", "N_AGENTS", ")", "*", "self", ".", "A_EDGE", "\n", "self", ".", "q_curr_edge", "=", "self", ".", "q_curr_edge", "+", "self", ".", "arrival_edge", "-", "self", ".", "actions", "\n", "self", ".", "q_loss_edge", "=", "(", "self", ".", "q_curr_edge", ">", "self", ".", "CAPACITY_EDGE", ")", "*", "np", ".", "abs", "(", "self", ".", "q_curr_edge", "-", "self", ".", "CAPACITY_EDGE", ")", "\n", "self", ".", "q_stall_edge", "=", "(", "self", ".", "q_curr_edge", "<", "0", ")", "*", "np", ".", "abs", "(", "self", ".", "q_curr_edge", ")", "\n", "self", ".", "q_curr_edge", "=", "np", ".", "clip", "(", "self", ".", "q_curr_edge", ",", "0", ",", "1", ")", "\n", "\n", "self", ".", "actions", "=", "self", ".", "actions", "-", "self", ".", "q_stall_edge", "\n", "\n", "self", ".", "arrival_cloud", "=", "np", ".", "array", "(", "[", "(", "(", "self", ".", "indicator_edge", "==", "i", ")", "*", "self", ".", "actions", ")", ".", "sum", "(", ")", "for", "i", "in", "range", "(", "self", ".", "N_CLOUD", ")", "]", ")", "\n", "self", ".", "q_curr_cloud", "=", "self", ".", "q_curr_cloud", "+", "self", ".", "arrival_cloud", "-", "self", ".", "DEPARTURE_CLOUD", "\n", "self", ".", "q_loss_cloud", "=", "(", "self", ".", "q_curr_cloud", ">", "self", ".", "CAPACITY_CLOUD", ")", "*", "np", ".", "abs", "(", "self", ".", "q_curr_cloud", "-", "self", ".", "CAPACITY_CLOUD", ")", "\n", "self", ".", "q_stall_cloud", "=", "(", "self", ".", "q_curr_cloud", "<", "0", ")", "*", "np", ".", "abs", "(", "self", ".", "q_curr_cloud", ")", "\n", "self", ".", "departure_cloud", "=", "self", ".", "DEPARTURE_CLOUD", "-", "self", ".", "q_stall_cloud", "\n", "self", ".", "q_curr_cloud", "=", "np", ".", "clip", "(", "self", ".", "q_curr_cloud", ",", "0", ",", "1", ")", "\n", "\n", "\n", "comm_r", "=", "1", "*", "self", ".", "q_loss_cloud", ".", "sum", "(", ")", "+", "4", "*", "self", ".", "q_stall_cloud", ".", "sum", "(", ")", "\n", "rewards", "=", "-", "comm_r", "\n", "\n", "next_state", "=", "self", ".", "get_state", "(", ")", "\n", "self", ".", "time", "+=", "1", "\n", "\n", "if", "self", ".", "time", ">=", "self", ".", "T_MAX", ":", "\n", "            ", "done", "=", "True", "\n", "", "else", ":", "\n", "            ", "done", "=", "False", "\n", "\n", "", "return", "rewards", ",", "next_state", ",", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.reset": [[73, 78], ["numpy.array", "numpy.array", "numpy.array", "Environment.ENV.get_state"], "methods", ["home.repos.pwc.inspect_result.wonjoon-yun_quantum-multi-agent-reinforcement-learning.None.Environment.ENV.get_state"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "q_prev_edge", "=", "np", ".", "array", "(", "[", "0.8", ",", "0.8", ",", "0.2", ",", "0.2", "]", ")", "\n", "self", ".", "q_curr_edge", "=", "np", ".", "array", "(", "[", "0.8", ",", "0.8", ",", "0.2", ",", "0.2", "]", ")", "\n", "self", ".", "q_curr_cloud", "=", "np", ".", "array", "(", "[", "0.5", ",", "0.5", "]", ")", "\n", "return", "self", ".", "get_state", "(", ")", "\n", "", "", ""]]}