{"home.repos.pwc.inspect_result.waste-wood_e-care.None.causal_reasoning.read_gold_data": [[6, 10], ["jsonlines.open"], "function", ["None"], ["def", "read_gold_data", "(", "path", ")", ":", "\n", "    ", "fi", "=", "jsonlines", ".", "open", "(", "path", ",", "'r'", ")", "\n", "labels", "=", "{", "line", "[", "'index'", "]", ":", "line", "[", "'label'", "]", "for", "line", "in", "fi", "}", "\n", "return", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.None.causal_reasoning.evaluation_metrics": [[12, 27], ["predictions.items", "len", "KeyError"], "function", ["None"], ["", "def", "evaluation_metrics", "(", "gold", ",", "predictions", ")", ":", "\n", "    ", "count", "=", "0", "\n", "for", "key", ",", "value", "in", "predictions", ".", "items", "(", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "gold_label", "=", "gold", "[", "key", "]", "\n", "", "except", ":", "\n", "            ", "raise", "KeyError", "(", "'{} is not a correct index in the dataset.'", ")", "\n", "\n", "", "if", "gold_label", "==", "value", ":", "\n", "            ", "count", "+=", "1", "\n", "", "else", ":", "\n", "            ", "continue", "\n", "\n", "", "", "accuracy", "=", "count", "/", "len", "(", "gold", ")", "\n", "return", "accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.None.causal_reasoning.main": [[29, 41], ["json.load", "causal_reasoning.read_gold_data", "open", "causal_reasoning.evaluation_metrics", "json.dump", "print", "open"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.None.conceptual_explanation_generation.read_gold_data", "home.repos.pwc.inspect_result.waste-wood_e-care.None.causal_reasoning.evaluation_metrics"], ["", "def", "main", "(", ")", ":", "\n", "    ", "prediction_file", "=", "sys", ".", "argv", "[", "1", "]", "\n", "gold_file", "=", "sys", ".", "argv", "[", "2", "]", "\n", "\n", "predictions", "=", "json", ".", "load", "(", "open", "(", "prediction_file", ",", "'r'", ")", ")", "\n", "gold_labels", "=", "read_gold_data", "(", "gold_file", ")", "\n", "\n", "fo", "=", "open", "(", "'./prediction_causal_reasoning.json'", ",", "'w'", ")", "\n", "\n", "accuracy", "=", "evaluation_metrics", "(", "gold_labels", ",", "predictions", ")", "\n", "json", ".", "dump", "(", "{", "\"accuracy\"", ":", "accuracy", "}", ",", "fo", ")", "\n", "print", "(", "\"[Accuracy]: {}\"", ".", "format", "(", "accuracy", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.None.conceptual_explanation_generation.read_gold_data": [[8, 12], ["jsonlines.open"], "function", ["None"], ["def", "read_gold_data", "(", "path", ")", ":", "\n", "    ", "fi", "=", "jsonlines", ".", "open", "(", "path", ",", "'r'", ")", "\n", "explanations", "=", "{", "line", "[", "'index'", "]", ":", "line", "[", "'conceptual_explanation'", "]", "for", "line", "in", "fi", "}", "\n", "return", "explanations", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.None.conceptual_explanation_generation.evaluation_bleu": [[14, 27], ["nltk.bleu", "len", "KeyError"], "function", ["None"], ["", "def", "evaluation_bleu", "(", "golds", ",", "predictions", ")", ":", "\n", "    ", "bleu_socre", "=", "0", "\n", "for", "key", "in", "predictions", ":", "\n", "        ", "prediction", "=", "predictions", "[", "key", "]", "\n", "try", ":", "\n", "            ", "gold", "=", "golds", "[", "key", "]", "\n", "", "except", ":", "\n", "            ", "raise", "KeyError", "(", "'{} is not a correct index in e-CARE dataset'", ".", "format", "(", "key", ")", ")", "\n", "\n", "", "bleu_socre", "+=", "bleu", "(", "[", "gold", "]", ",", "prediction", ")", "\n", "\n", "", "avg_bleu", "=", "bleu_socre", "/", "len", "(", "golds", ")", "\n", "return", "avg_bleu", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.None.conceptual_explanation_generation.evaluation_rouge": [[29, 48], ["rouge.Rouge", "len", "rouge.Rouge.get_scores", "KeyError"], "function", ["None"], ["", "def", "evaluation_rouge", "(", "golds", ",", "predictions", ")", ":", "\n", "    ", "rouge_l", "=", "0", "\n", "rouge", "=", "Rouge", "(", ")", "\n", "\n", "for", "key", "in", "predictions", ":", "\n", "        ", "prediction", "=", "predictions", "[", "key", "]", "\n", "try", ":", "\n", "            ", "gold", "=", "golds", "[", "key", "]", "\n", "", "except", ":", "\n", "            ", "raise", "KeyError", "(", "'{} is not a correct index in e-CARE dataset'", ".", "format", "(", "key", ")", ")", "\n", "\n", "", "try", ":", "\n", "            ", "scores", "=", "rouge", ".", "get_scores", "(", "prediction", ",", "gold", ")", "\n", "rouge_l", "+=", "scores", "[", "0", "]", "[", "'rouge-l'", "]", "[", "'r'", "]", "\n", "", "except", ":", "\n", "            ", "continue", "\n", "\n", "", "", "avg_rougel", "=", "rouge_l", "/", "len", "(", "golds", ")", "\n", "return", "avg_rougel", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.None.conceptual_explanation_generation.main": [[50, 65], ["json.load", "conceptual_explanation_generation.read_gold_data", "conceptual_explanation_generation.evaluation_bleu", "conceptual_explanation_generation.evaluation_rouge", "open", "json.dump", "print", "print", "open"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.None.conceptual_explanation_generation.read_gold_data", "home.repos.pwc.inspect_result.waste-wood_e-care.None.conceptual_explanation_generation.evaluation_bleu", "home.repos.pwc.inspect_result.waste-wood_e-care.None.conceptual_explanation_generation.evaluation_rouge"], ["", "def", "main", "(", ")", ":", "\n", "    ", "prediction_file", "=", "sys", ".", "argv", "[", "1", "]", "\n", "gold_file", "=", "sys", ".", "argv", "[", "2", "]", "\n", "\n", "predictions", "=", "json", ".", "load", "(", "open", "(", "prediction_file", ",", "'r'", ")", ")", "\n", "gold_labels", "=", "read_gold_data", "(", "gold_file", ")", "\n", "\n", "bleu_score", "=", "evaluation_bleu", "(", "gold_labels", ",", "predictions", ")", "\n", "rouge_l", "=", "evaluation_rouge", "(", "gold_labels", ",", "predictions", ")", "\n", "\n", "fo", "=", "open", "(", "'./evaluation_metrics_conceptual_explanation_generation.json'", ",", "'w'", ")", "\n", "\n", "json", ".", "dump", "(", "{", "\"bleu\"", ":", "bleu_score", ",", "\"rouge-l\"", ":", "rouge_l", "}", ",", "fo", ")", "\n", "print", "(", "\"[Average BLEU]: {}\"", ".", "format", "(", "bleu_score", ")", ")", "\n", "print", "(", "\"[Rouge-l]: {}\"", ".", "format", "(", "rouge_l", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_multi_task.gpt2_multi_task.__init__": [[24, 29], ["torch.Module.__init__", "transformers.GPT2LMHeadModel.from_pretrained", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.waste-wood_e-care.model.discriminate_model.pretrained_model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hps", ")", ":", "\n", "        ", "super", "(", "gpt2_multi_task", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hps", "=", "hps", "\n", "self", ".", "model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "self", ".", "model", ".", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_multi_task.gpt2_multi_task.forward": [[30, 119], ["gpt2_multi_task.gpt2_multi_task.model", "gpt2_multi_task.gpt2_multi_task.linear().squeeze", "gpt2_multi_task.gpt2_multi_task.model.generate", "range", "generated_text.eq", "range", "gpt2_multi_task.gpt2_multi_task.linear().squeeze", "gpt2_multi_task.gpt2_multi_task.model", "gpt2_multi_task.gpt2_multi_task.linear().squeeze", "gpt2_multi_task.gpt2_multi_task.linear", "[].view", "s.view", "s.argmax", "index[].eq", "len", "range", "range", "generated_text[].unsqueeze", "range", "ttmp.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "eos_mask[].sum().item", "gpt2_multi_task.gpt2_multi_task.linear", "gpt2_multi_task.gpt2_multi_task.linear", "len", "tmp.unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "eos_mask[].sum().item", "enumerate", "range", "ttmp.unsqueeze", "eos_mask[].sum", "t.item", "tmp.unsqueeze", "eos_mask[].sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "pos", ",", "mode", "=", "'train'", ",", "token_type_ids", "=", "None", ")", ":", "\n", "# if mode == 'train':", "\n", "\n", "        ", "if", "mode", "==", "'train'", ":", "\n", "# pdb.set_trace()", "\n", "            ", "outputs", "=", "self", ".", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "output_hidden_states", "=", "True", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "hidden_state", "=", "outputs", ".", "hidden_states", "[", "-", "1", "]", "\n", "# pos = pos.squeeze().unsqueeze(0)", "\n", "hidden_state", "=", "hidden_state", "[", "range", "(", "hidden_state", ".", "shape", "[", "0", "]", ")", ",", "pos", ",", ":", "]", "\n", "logits", "=", "self", ".", "linear", "(", "hidden_state", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "gen_logits", "=", "outputs", ".", "logits", "\n", "return", "logits", ",", "gen_logits", "\n", "\n", "", "if", "mode", "==", "'test'", ":", "\n", "            ", "output_dict", "=", "self", ".", "model", ".", "generate", "(", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "max_length", "=", "self", ".", "hps", ".", "length", "+", "input_ids", ".", "shape", "[", "1", "]", ",", "\n", "num_beams", "=", "self", ".", "hps", ".", "beam_size", ",", "\n", "early_stopping", "=", "self", ".", "hps", ".", "early_stopping", ",", "\n", "# do_sample=self.hps.do_sample,", "\n", "no_repeat_ngram_size", "=", "self", ".", "hps", ".", "no_repeat_ngram_size", ",", "\n", "repetition_penalty", "=", "self", ".", "hps", ".", "repetition_penalty", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", "return_dict_in_generate", "=", "True", ",", "\n", "output_scores", "=", "True", "\n", ")", "\n", "\n", "if", "self", ".", "hps", ".", "mode", "==", "'generate_discriminate'", ":", "\n", "# pdb.set_trace()", "\n", "                ", "scores", "=", "output_dict", ".", "scores", "\n", "hiddens", "=", "output_dict", ".", "hidden_states", "\n", "\n", "hiddens", "=", "[", "h", "[", "-", "1", "]", "[", ":", ",", "-", "1", ",", ":", "]", ".", "view", "(", "input_ids", ".", "shape", "[", "0", "]", ",", "self", ".", "hps", ".", "beam_size", ",", "-", "1", ")", "for", "h", "in", "hiddens", "]", "\n", "scores", "=", "[", "s", ".", "view", "(", "input_ids", ".", "shape", "[", "0", "]", ",", "self", ".", "hps", ".", "beam_size", ",", "-", "1", ")", "for", "s", "in", "scores", "]", "\n", "index", "=", "[", "s", ".", "argmax", "(", "-", "1", ")", "for", "s", "in", "scores", "]", "\n", "\n", "generated_text", "=", "output_dict", ".", "sequences", "[", ":", ",", "input_ids", ".", "shape", "[", "1", "]", ":", "]", "\n", "index", "=", "[", "index", "[", "i", "]", ".", "eq", "(", "generated_text", "[", ":", ",", "i", "]", ".", "unsqueeze", "(", "1", ")", ")", "for", "i", "in", "range", "(", "len", "(", "index", ")", ")", "]", "\n", "selected", "=", "None", "\n", "# pdb.set_trace()", "\n", "for", "i", "in", "range", "(", "len", "(", "index", ")", ")", ":", "\n", "                    ", "ttmp", "=", "None", "\n", "for", "j", "in", "range", "(", "index", "[", "i", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                        ", "tmp", "=", "hiddens", "[", "i", "]", "[", "j", "]", "[", "index", "[", "i", "]", "[", "j", "]", "]", "\n", "if", "tmp", ".", "shape", "[", "0", "]", "==", "0", ":", "\n", "                            ", "tmp", "=", "hiddens", "[", "i", "]", "[", "j", "]", "[", "0", "]", "\n", "", "else", ":", "\n", "                            ", "tmp", "=", "tmp", "[", "0", "]", "\n", "", "ttmp", "=", "tmp", ".", "unsqueeze", "(", "0", ")", "if", "ttmp", "is", "None", "else", "torch", ".", "cat", "(", "(", "ttmp", ",", "tmp", ".", "unsqueeze", "(", "0", ")", ")", ",", "0", ")", "\n", "", "selected", "=", "ttmp", ".", "unsqueeze", "(", "1", ")", "if", "selected", "is", "None", "else", "torch", ".", "cat", "(", "(", "selected", ",", "ttmp", ".", "unsqueeze", "(", "1", ")", ")", ",", "1", ")", "\n", "\n", "# pdb.set_trace()", "\n", "# eos_mask = generated_text.eq(self.model.config.eos_token_id)", "\n", "", "eos_mask", "=", "generated_text", ".", "eq", "(", "13", ")", "\n", "\n", "gen_ids", "=", "generated_text", "\n", "for", "i", "in", "range", "(", "eos_mask", ".", "shape", "[", "0", "]", ")", ":", "\n", "                    ", "flag", "=", "False", "\n", "if", "eos_mask", "[", "i", "]", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "1", ":", "\n", "                        ", "continue", "\n", "", "elif", "eos_mask", "[", "i", "]", ".", "sum", "(", ")", ".", "item", "(", ")", "==", "0", ":", "\n", "                        ", "eos_mask", "[", "i", "]", "[", "-", "1", "]", "=", "True", "\n", "continue", "\n", "", "else", ":", "\n", "                        ", "for", "j", ",", "t", "in", "enumerate", "(", "eos_mask", "[", "i", "]", ")", ":", "\n", "                            ", "if", "t", ".", "item", "(", ")", ":", "\n", "                                ", "if", "flag", ":", "\n", "                                    ", "eos_mask", "[", "i", "]", "[", "j", "]", "=", "False", "\n", "", "flag", "=", "True", "\n", "", "else", ":", "\n", "                                ", "continue", "\n", "\n", "# pdb.set_trace()", "\n", "# selected = selected.view(eos_mask.shape[0], eos_mask.shape[1], -1)[eos_mask, :]", "\n", "", "", "", "", "selected", "=", "selected", "[", "eos_mask", ",", ":", "]", "\n", "logits", "=", "self", ".", "linear", "(", "selected", ")", ".", "squeeze", "(", ")", "\n", "gen_ids", "=", "generated_text", "\n", "\n", "", "else", ":", "\n", "# pdb.set_trace()", "\n", "                ", "outputs", "=", "self", ".", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "output_hidden_states", "=", "True", ")", "\n", "hidden_state", "=", "outputs", ".", "hidden_states", "[", "-", "1", "]", "\n", "# pos = pos.squeeze().unsqueeze(0)", "\n", "hidden_state", "=", "hidden_state", "[", "range", "(", "hidden_state", ".", "shape", "[", "0", "]", ")", ",", "pos", ",", ":", "]", "\n", "logits", "=", "self", ".", "linear", "(", "hidden_state", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "# logits = torch.cat((logits[::2].unsqueeze(1), logits[1::2].unsqueeze(1)), 1)", "\n", "gen_ids", "=", "output_dict", ".", "sequences", "[", ":", ",", "input_ids", ".", "shape", "[", "1", "]", ":", "]", "\n", "\n", "", "return", "logits", ",", "gen_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_multi_task.tokenization": [[121, 198], ["transformers.GPT2Tokenizer.from_pretrained", "GPT2Tokenizer.from_pretrained.", "GPT2Tokenizer.from_pretrained.", "range", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "range", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "pos1.append", "loss_label.append", "torch.LongTensor.append", "torch.LongTensor.append", "pos2.append", "max", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "inputs.append", "inputs.append", "inputs.append", "inputs.append", "len", "len", "max", "len", "len", "max", "len", "range", "range", "range", "range", "range", "range", "len", "range", "range", "len"], "function", ["None"], ["", "", "", "def", "tokenization", "(", "data", ",", "hps", ")", ":", "\n", "    ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "tokenizer", ".", "pad_token", "=", "tokenizer", ".", "eos_token", "\n", "\n", "inputs", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "pos1", "=", "[", "]", "\n", "pos2", "=", "[", "]", "\n", "loss_label", "=", "[", "]", "\n", "truth", "=", "[", "]", "\n", "# token_type_ids = []", "\n", "\n", "for", "example", "in", "data", ":", "\n", "        ", "if", "example", "[", "'ask-for'", "]", "==", "'cause'", ":", "\n", "            ", "inputs", ".", "append", "(", "[", "example", "[", "'alternative1'", "]", ",", "example", "[", "'premise'", "]", "]", ")", "\n", "inputs", ".", "append", "(", "[", "example", "[", "'alternative2'", "]", ",", "example", "[", "'premise'", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "inputs", ".", "append", "(", "[", "example", "[", "'premise'", "]", ",", "example", "[", "'alternative1'", "]", "]", ")", "\n", "inputs", ".", "append", "(", "[", "example", "[", "'premise'", "]", ",", "example", "[", "'alternative2'", "]", "]", ")", "\n", "", "truth", "+=", "[", "example", "[", "'general_truth'", "]", "]", "*", "2", "\n", "labels", "+=", "[", "0", ",", "1", "]", "if", "example", "[", "'label'", "]", "==", "1", "else", "[", "1", ",", "0", "]", "\n", "\n", "", "outputs", "=", "tokenizer", "(", "inputs", ",", "return_length", "=", "True", ")", "\n", "input_ids", "=", "outputs", "[", "'input_ids'", "]", "\n", "attention_mask", "=", "outputs", "[", "'attention_mask'", "]", "\n", "length", "=", "outputs", "[", "'length'", "]", "\n", "# max_length = max(length)", "\n", "\n", "truth_output", "=", "tokenizer", "(", "truth", ",", "return_length", "=", "True", ")", "\n", "truth_ids", "=", "truth_output", "[", "'input_ids'", "]", "\n", "truth_mask", "=", "truth_output", "[", "'attention_mask'", "]", "\n", "truth_length", "=", "truth_output", "[", "'length'", "]", "\n", "\n", "premise_ids", "=", "[", "]", "\n", "premise_mask", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "input_ids", ")", ")", ":", "\n", "        ", "pos1", ".", "append", "(", "len", "(", "input_ids", "[", "i", "]", ")", ")", "\n", "input_ids", "[", "i", "]", "+=", "[", "50256", "]", "\n", "attention_mask", "[", "i", "]", "+=", "[", "1", "]", "\n", "loss_label", ".", "append", "(", "[", "-", "100", "for", "_", "in", "range", "(", "len", "(", "input_ids", "[", "i", "]", ")", ")", "]", ")", "\n", "# token_type_ids += [0 for _ in range(len(input_ids[i]))]", "\n", "\n", "gap1", "=", "max", "(", "length", ")", "+", "1", "-", "len", "(", "input_ids", "[", "i", "]", ")", "\n", "premise_ids", ".", "append", "(", "input_ids", "[", "i", "]", "+", "[", "50256", "for", "_", "in", "range", "(", "gap1", ")", "]", ")", "\n", "premise_mask", ".", "append", "(", "attention_mask", "[", "i", "]", "+", "[", "0", "for", "_", "in", "range", "(", "gap1", ")", "]", ")", "\n", "\n", "input_ids", "[", "i", "]", "+=", "truth_ids", "[", "i", "]", "\n", "attention_mask", "[", "i", "]", "+=", "truth_mask", "[", "i", "]", "\n", "# token_type_ids += [1 for _ in range(len(truth_ids[i]))]", "\n", "pos2", ".", "append", "(", "len", "(", "input_ids", "[", "i", "]", ")", "-", "1", ")", "\n", "\n", "loss_label", "[", "i", "]", "+=", "truth_ids", "[", "i", "]", "\n", "\n", "gap2", "=", "max", "(", "truth_length", ")", "-", "len", "(", "truth_ids", "[", "i", "]", ")", "\n", "truth_ids", "[", "i", "]", "+=", "[", "50256", "for", "_", "in", "range", "(", "gap2", ")", "]", "\n", "\n", "", "truth_ids", "=", "torch", ".", "LongTensor", "(", "truth_ids", ")", "\n", "\n", "max_length", "=", "max", "(", "[", "length", "[", "k", "]", "+", "truth_length", "[", "k", "]", "for", "k", "in", "range", "(", "len", "(", "length", ")", ")", "]", ")", "+", "2", "\n", "for", "i", "in", "range", "(", "len", "(", "input_ids", ")", ")", ":", "\n", "        ", "gap", "=", "max_length", "-", "len", "(", "input_ids", "[", "i", "]", ")", "\n", "input_ids", "[", "i", "]", "+=", "[", "50256", "for", "_", "in", "range", "(", "gap", ")", "]", "\n", "attention_mask", "[", "i", "]", "+=", "[", "0", "for", "_", "in", "range", "(", "gap", ")", "]", "\n", "# token_type_ids += [0 for _ in range(gap)]", "\n", "loss_label", "[", "i", "]", "+=", "[", "-", "100", "for", "_", "in", "range", "(", "gap", ")", "]", "\n", "\n", "# premise_outputs = tokenizer(inputs, padding=True)", "\n", "", "premise_ids", "=", "torch", ".", "LongTensor", "(", "premise_ids", ")", "\n", "premise_mask", "=", "torch", ".", "LongTensor", "(", "premise_mask", ")", "\n", "\n", "if", "hps", ".", "mode", "==", "'discriminate_generate'", ":", "\n", "        ", "pos", "=", "pos1", "\n", "", "else", ":", "\n", "        ", "pos", "=", "pos2", "\n", "", "return", "torch", ".", "LongTensor", "(", "input_ids", ")", ",", "torch", ".", "LongTensor", "(", "attention_mask", ")", ",", "torch", ".", "LongTensor", "(", "pos", ")", ",", "torch", ".", "LongTensor", "(", "\n", "labels", ")", ",", "torch", ".", "LongTensor", "(", "loss_label", ")", ",", "premise_ids", ",", "premise_mask", ",", "truth_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_multi_task.evaluate": [[200, 362], ["transformers.GPT2Tokenizer.from_pretrained", "rouge.Rouge", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "range", "datetime.datetime.now().strftime", "open", "csv.writer", "csv.writer.writerows", "copy.deepcopy", "optimizer.zero_grad", "model", "loss_function", "loss_function.cpu().item", "tmp_labels[].unsqueeze", "tmp_labels[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat.argmax", "gen_logits[].unsqueeze", "gen_logits[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "loss_label[].unsqueeze", "loss_label[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "g[].contiguous", "l[].contiguous", "loss_function2", "total_loss.backward", "len", "tuple", "tmp_labels.float", "g[].contiguous.view", "l[].contiguous.view", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.eval", "copy.deepcopy.state_dict", "model", "torch.sum", "torch.sum", "torch.sum", "attack_embedding.pow().unsqueeze.pow().unsqueeze", "copy.deepcopy.load_state_dict", "copy.deepcopy.eval", "copy.deepcopy.", "loss_function().item", "tmp_labels.cpu().numpy().tolist", "logits.cpu().tolist", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax.cpu().tolist", "attack_logits.cpu().tolist", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax.cpu().tolist", "gen_ids[].unsqueeze", "gen_ids[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "tmp_labels[].unsqueeze", "tmp_labels[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "range", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax", "torch.argmax", "torch.argmax", "datetime.datetime.now", "len", "len", "len", "len", "len", "len", "len", "len", "len", "loss_function.cpu", "g[].contiguous.size", "attack_embedding.pow().unsqueeze.pow", "GPT2Tokenizer.from_pretrained.decode", "GPT2Tokenizer.from_pretrained.decode", "nltk.bleu", "nltk.bleu", "nltk.bleu", "nltk.bleu", "rouge.Rouge.get_scores", "term.cuda", "range", "range", "attack_embedding.pow().unsqueeze.pow", "loss_function", "tmp_labels.cpu().numpy", "logits.cpu", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax.cpu", "attack_logits.cpu", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax.cpu", "generated.cpu().tolist", "truth[].cpu().tolist", "range", "tmp_labels.float", "range", "torch.cat.argmax", "len", "tmp_labels.cpu", "generated.cpu", "truth[].cpu", "generated_text[].split", "generated_text[].split", "generated_text[].split", "generated_text[].split", "generated_text[].split"], "function", ["None"], ["", "def", "evaluate", "(", "hps", ",", "model", ",", "dataloader", ",", "loss_function", ",", "loss_function2", ",", "optimizer", ")", ":", "\n", "\n", "    ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "# predictions = []", "\n", "labels", "=", "[", "]", "\n", "predict_labels", ",", "attack_predict_labels", "=", "[", "]", ",", "[", "]", "\n", "loss", "=", "0", "\n", "attack_loss", "=", "0", "\n", "\n", "bleu1", ",", "bleu2", ",", "bleu3", ",", "bleu4", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "rouge1p", ",", "rouge1r", ",", "rouge1f", ",", "rouge2p", ",", "rouge2r", ",", "rouge2f", ",", "rougelp", ",", "rougelr", ",", "rougelf", "=", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "\n", "rouge", "=", "Rouge", "(", ")", "\n", "output_text", "=", "[", "]", "\n", "\n", "# model.eval()", "\n", "# pdb.set_trace()", "\n", "for", "batch", "in", "dataloader", ":", "\n", "        ", "attack_model", "=", "copy", ".", "deepcopy", "(", "model", ")", "\n", "\n", "# attack_model.eval()", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "hps", ".", "cuda", ":", "\n", "            ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "ids", ",", "mask", ",", "pos", ",", "tmp_labels", ",", "loss_label", ",", "input_ids", ",", "attention_mask", ",", "truth", "=", "batch", "\n", "dis_logits", ",", "gen_logits", "=", "model", "(", "ids", ",", "attention_mask", "=", "mask", ",", "pos", "=", "pos", ",", "mode", "=", "'train'", ")", "\n", "\n", "# predictions += logits.cpu().tolist()", "\n", "tmp_loss", "=", "loss_function", "(", "dis_logits", ",", "tmp_labels", ".", "float", "(", ")", ")", "\n", "loss", "+=", "tmp_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "\n", "\n", "t_1", "=", "tmp_labels", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "t_2", "=", "tmp_labels", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "t_", "=", "torch", ".", "cat", "(", "(", "t_1", ",", "t_2", ")", ",", "1", ")", "\n", "t_index", "=", "t_", ".", "argmax", "(", "1", ")", "\n", "\n", "g1", "=", "gen_logits", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "g2", "=", "gen_logits", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "g", "=", "torch", ".", "cat", "(", "(", "g1", ",", "g2", ")", ",", "1", ")", "\n", "\n", "l1", "=", "loss_label", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "l2", "=", "loss_label", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "l", "=", "torch", ".", "cat", "(", "(", "l1", ",", "l2", ")", ",", "1", ")", "\n", "\n", "g", "=", "g", "[", "range", "(", "g", ".", "shape", "[", "0", "]", ")", ",", "t_index", ",", ":", "]", "\n", "l", "=", "l", "[", "range", "(", "l", ".", "shape", "[", "0", "]", ")", ",", "t_index", ",", ":", "]", "\n", "\n", "# pdb.set_trace()", "\n", "\n", "shift_logits", "=", "g", "[", "...", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "shift_labels", "=", "l", "[", "...", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "\n", "loss_gen", "=", "loss_function2", "(", "shift_logits", ".", "view", "(", "-", "1", ",", "shift_logits", ".", "size", "(", "-", "1", ")", ")", ",", "shift_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "hps", ".", "alpha", "*", "tmp_loss", "+", "(", "1", "-", "hps", ".", "alpha", ")", "*", "loss_gen", "\n", "total_loss", ".", "backward", "(", ")", "\n", "embedding_grad", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'params'", "]", "[", "0", "]", ".", "grad", "\n", "# attack", "\n", "\n", "\n", "# optimizer.zero_grad()", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# pdb.set_trace()", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "state_dict", "=", "attack_model", ".", "state_dict", "(", ")", "\n", "logits", ",", "gen_ids", "=", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "pos", "=", "pos", ",", "mode", "=", "'test'", ")", "\n", "# embedding_grad = F.softmax(embedding_grad, 1)", "\n", "attack_embedding", "=", "torch", ".", "sum", "(", "embedding_grad", "*", "embedding_grad", ",", "-", "1", ")", "\n", "attack_embedding", "=", "attack_embedding", ".", "pow", "(", "0.5", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "attack_embedding", "=", "attack_embedding", ".", "pow", "(", "-", "1", ")", "*", "embedding_grad", "\n", "state_dict", "[", "'model.transformer.wte.weight'", "]", "+=", "hps", ".", "attack_rate", "*", "attack_embedding", "\n", "attack_model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "attack_model", ".", "eval", "(", ")", "\n", "attack_logits", ",", "_", "=", "attack_model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "pos", "=", "pos", ",", "mode", "=", "'test'", ")", "\n", "\n", "attack_loss", "+=", "loss_function", "(", "attack_logits", ",", "tmp_labels", ".", "float", "(", ")", ")", ".", "item", "(", ")", "\n", "labels", "+=", "tmp_labels", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "# generated = gen_ids[:, input_ids.shape[1]:]", "\n", "tmp_predict", "=", "logits", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "a1", "=", "torch", ".", "FloatTensor", "(", "tmp_predict", "[", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a2", "=", "torch", ".", "FloatTensor", "(", "tmp_predict", "[", "1", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a", "=", "torch", ".", "cat", "(", "(", "a1", ",", "a2", ")", ",", "dim", "=", "1", ")", "\n", "predict_label", "=", "torch", ".", "argmax", "(", "a", ",", "1", ")", "\n", "predict_labels", "+=", "predict_label", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "attack_tmp_predict", "=", "attack_logits", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "a_t1", "=", "torch", ".", "FloatTensor", "(", "attack_tmp_predict", "[", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a_t2", "=", "torch", ".", "FloatTensor", "(", "attack_tmp_predict", "[", "1", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a_t", "=", "torch", ".", "cat", "(", "(", "a_t1", ",", "a_t2", ")", ",", "1", ")", "\n", "attack_predict_label", "=", "torch", ".", "argmax", "(", "a_t", ",", "1", ")", "\n", "attack_predict_labels", "+=", "attack_predict_label", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "g1", "=", "gen_ids", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "g2", "=", "gen_ids", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "g", "=", "torch", ".", "cat", "(", "(", "g1", ",", "g2", ")", ",", "1", ")", "\n", "\n", "t_1", "=", "tmp_labels", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "t_2", "=", "tmp_labels", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "t", "=", "torch", ".", "cat", "(", "(", "t_1", ",", "t_2", ")", ",", "1", ")", "\n", "\n", "# generated = g[range(g.shape[0]), predict_label]", "\n", "generated", "=", "g", "[", "range", "(", "g", ".", "shape", "[", "0", "]", ")", ",", "t", ".", "argmax", "(", "1", ")", "]", "\n", "\n", "generated_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "\n", "generated", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "]", "\n", "\n", "gold_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "\n", "truth", "[", ":", ":", "2", "]", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "]", "\n", "# input_text = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in input_ids]", "\n", "output_text", "+=", "[", "[", "gold_text", "[", "i", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", "]", "for", "i", "in", "range", "(", "len", "(", "gold_text", ")", ")", "]", "\n", "for", "i", "in", "range", "(", "generated", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "bleu1", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "1", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "bleu2", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "0", ",", "1", ",", "0", ",", "0", "]", ")", "\n", "bleu3", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "0", ",", "0", ",", "1", ",", "0", "]", ")", "\n", "bleu4", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "0", ",", "0", ",", "0", ",", "1", "]", ")", "\n", "\n", "scores", "=", "rouge", ".", "get_scores", "(", "generated_text", "[", "i", "]", ",", "gold_text", "[", "i", "]", ")", "\n", "rouge1", "=", "scores", "[", "0", "]", "[", "'rouge-1'", "]", "\n", "rouge1f", "+=", "rouge1", "[", "'f'", "]", "\n", "rougelp", "+=", "rouge1", "[", "'p'", "]", "\n", "rouge1r", "+=", "rouge1", "[", "'r'", "]", "\n", "\n", "rouge2", "=", "scores", "[", "0", "]", "[", "'rouge-2'", "]", "\n", "rouge2f", "+=", "rouge2", "[", "'f'", "]", "\n", "rouge1p", "+=", "rouge2", "[", "'p'", "]", "\n", "rouge2r", "+=", "rouge2", "[", "'r'", "]", "\n", "\n", "rougel", "=", "scores", "[", "0", "]", "[", "'rouge-l'", "]", "\n", "rougelf", "+=", "rougel", "[", "'f'", "]", "\n", "rougelp", "+=", "rougel", "[", "'p'", "]", "\n", "rougelr", "+=", "rougel", "[", "'r'", "]", "\n", "\n", "# predict_labels = predict_labels.cpu().tolist()", "\n", "# pdb.set_trace()", "\n", "", "", "", "t_a1", "=", "torch", ".", "FloatTensor", "(", "labels", "[", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a2", "=", "torch", ".", "FloatTensor", "(", "labels", "[", "1", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a", "=", "torch", ".", "cat", "(", "(", "t_a1", ",", "t_a2", ")", ",", "dim", "=", "1", ")", "\n", "true_labels", "=", "torch", ".", "argmax", "(", "t_a", ",", "1", ")", ".", "tolist", "(", ")", "\n", "count", "=", "0", "\n", "attack_count", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "predict_labels", ")", ")", ":", "\n", "        ", "if", "predict_labels", "[", "i", "]", "==", "true_labels", "[", "i", "]", "and", "attack_predict_labels", "[", "i", "]", "==", "true_labels", "[", "i", "]", ":", "\n", "            ", "count", "+=", "1", "\n", "attack_count", "+=", "1", "\n", "", "elif", "predict_labels", "[", "i", "]", "==", "true_labels", "[", "i", "]", ":", "\n", "            ", "count", "+=", "1", "\n", "", "elif", "attack_predict_labels", "[", "i", "]", "==", "true_labels", "[", "i", "]", ":", "\n", "            ", "attack_count", "+=", "1", "\n", "", "else", ":", "\n", "            ", "continue", "\n", "# pdb.set_trace()", "\n", "", "", "nowtime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "fo", "=", "open", "(", "hps", ".", "output_dir", "+", "'/gpt2_predict_'", "+", "nowtime", "+", "'.csv'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "writer", "=", "csv", ".", "writer", "(", "fo", ")", "\n", "writer", ".", "writerows", "(", "output_text", ")", "\n", "\n", "# num_instances = int(((len(data_loader)-1) * hps.batch_size + truth_ids.shape[0]) / 2)", "\n", "\n", "return", "count", "/", "len", "(", "true_labels", ")", ",", "bleu1", "/", "len", "(", "true_labels", ")", ",", "bleu2", "/", "len", "(", "true_labels", ")", ",", "bleu3", "/", "len", "(", "\n", "true_labels", ")", ",", "bleu4", "/", "len", "(", "true_labels", ")", ",", "rouge1r", "/", "len", "(", "true_labels", ")", ",", "rouge2r", "/", "len", "(", "true_labels", ")", ",", "rougelr", "/", "len", "(", "\n", "true_labels", ")", ",", "loss", ",", "attack_count", "/", "len", "(", "true_labels", ")", ",", "attack_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_multi_task.compute_ppl": [[364, 394], ["transformers.GPT2Tokenizer.from_pretrained", "torch.exp", "torch.exp", "torch.exp", "torch.exp.item", "GPT2Tokenizer.from_pretrained.", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "GPT2Tokenizer.from_pretrained.", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.no_grad", "torch.no_grad", "torch.no_grad", "lls.append", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "model.model", "torch.stack", "torch.stack", "torch.stack", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.ones", "torch.ones", "torch.ones", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "function", ["None"], ["", "def", "compute_ppl", "(", "hps", ",", "model", ",", "data", ")", ":", "\n", "# pdb.set_trace()", "\n", "    ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "lls", "=", "[", "]", "\n", "total_length", "=", "0", "\n", "for", "example", "in", "data", ":", "\n", "        ", "if", "example", "[", "'ask-for'", "]", "==", "'cause'", ":", "\n", "            ", "input_text", "=", "(", "example", "[", "'alternative1'", "]", "+", "' '", "+", "example", "[", "'premise'", "]", ")", "if", "example", "[", "'label'", "]", "==", "0", "else", "(", "example", "[", "'alternative2'", "]", "+", "' '", "+", "example", "[", "'premise'", "]", ")", "\n", "", "else", ":", "\n", "            ", "input_text", "=", "(", "example", "[", "'premise'", "]", "+", "' '", "+", "example", "[", "'alternative1'", "]", ")", "if", "example", "[", "'label'", "]", "==", "0", "else", "(", "example", "[", "'premise'", "]", "+", "' '", "+", "example", "[", "'alternative2'", "]", ")", "\n", "", "truth", "=", "example", "[", "'general_truth'", "]", "\n", "inputs", "=", "tokenizer", "(", "input_text", ")", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "inputs", "[", "'input_ids'", "]", "+", "[", "50256", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "attention_mask", "=", "torch", ".", "LongTensor", "(", "inputs", "[", "'attention_mask'", "]", "+", "[", "1", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "label_inputs", "=", "tokenizer", "(", "truth", ")", "\n", "label_ids", "=", "torch", ".", "LongTensor", "(", "label_inputs", "[", "'input_ids'", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "length", "=", "label_ids", ".", "shape", "[", "1", "]", "\n", "total_length", "+=", "length", "\n", "\n", "# label_mask = torch.LongTensor(label_inputs['attention_mask']).unsqueeze(0).cuda()", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "attention_mask", ",", "torch", ".", "ones", "(", "1", ",", "label_ids", ".", "shape", "[", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ",", "1", ")", "\n", "label_ids", "=", "torch", ".", "cat", "(", "(", "torch", ".", "LongTensor", "(", "[", "-", "100", "]", "*", "input_ids", ".", "shape", "[", "1", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", ",", "label_ids", ")", ",", "1", ")", "\n", "input_ids", "=", "torch", ".", "cat", "(", "(", "input_ids", ",", "label_ids", "[", ":", ",", "input_ids", ".", "shape", "[", "1", "]", ":", "]", ")", ",", "1", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "loss", "=", "model", ".", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "labels", "=", "label_ids", ")", "[", "0", "]", "\n", "lls", ".", "append", "(", "loss", "*", "length", ")", "\n", "# pdb.set_trace()", "\n", "", "", "ppl", "=", "torch", ".", "exp", "(", "torch", ".", "stack", "(", "lls", ")", ".", "sum", "(", ")", "/", "total_length", ")", "\n", "\n", "return", "ppl", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_multi_task.main": [[396, 611], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.utils.define_logger", "os.path.join", "logging.FileHandler", "logging.FileHandler.setFormatter", "logger.addHandler", "logger.info", "logger.info", "utils.utils.load_data", "utils.utils.load_data", "utils.utils.load_data", "logger.info", "gpt2_multi_task.tokenization", "gpt2_multi_task.tokenization", "gpt2_multi_task.tokenization", "logger.info", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "logger.info", "gpt2_multi_task.gpt2_multi_task", "transformers.AdamW", "torch.BCEWithLogitsLoss", "torch.CrossEntropyLoss", "logger.info", "range", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "os.path.join", "os.path.join", "os.path.join", "filter", "nn.DataParallel.cuda", "logger.info", "tqdm.trange", "zip", "nn.DataParallel.parameters", "int", "len", "torch.DataParallel", "len", "transformers.AdamW.zero_grad", "nn.DataParallel.train", "nn.DataParallel.", "nn.BCEWithLogitsLoss.", "label[].unsqueeze", "label[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat.argmax", "gen_logits[].unsqueeze", "gen_logits[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "loss_label[].unsqueeze", "loss_label[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "g[].contiguous", "l[].contiguous", "nn.CrossEntropyLoss.", "loss.item", "tqdm.trange.set_postfix", "loss.backward", "transformers.AdamW.step", "parser.parse_args.gpu.split", "tuple", "label.float", "g[].contiguous.view", "l[].contiguous.view", "nn.DataParallel.eval", "print", "logger.info", "gpt2_multi_task.evaluate", "utils.utils.compute_ppl", "logger.info", "print", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "g[].contiguous.size", "logger.info", "logger.info", "gpt2_multi_task.evaluate", "utils.utils.compute_ppl", "logger.info", "print", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "term.cuda", "range", "range"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.define_logger", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.tokenization", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.tokenization", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.tokenization", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.evaluate", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.compute_ppl", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.evaluate", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.compute_ppl"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'xCAR'", ")", "\n", "\n", "# Data Paths", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "'./data/'", ",", "help", "=", "'The dataset directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--model_dir'", ",", "type", "=", "str", ",", "default", "=", "'../../huggingface_transformers/gpt2/'", ",", "\n", "help", "=", "'The pretrained model directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/saved_model'", ",", "help", "=", "'The model saving directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/log'", ",", "help", "=", "'The training log directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--apex_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/log'", ",", "help", "=", "'The apex directory'", ")", "\n", "\n", "# Data names", "\n", "parser", ".", "add_argument", "(", "'--train'", ",", "type", "=", "str", ",", "default", "=", "'train.pkl'", ",", "help", "=", "'The train data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--dev'", ",", "type", "=", "str", ",", "default", "=", "'dev.pkl'", ",", "help", "=", "'The dev data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--test'", ",", "type", "=", "str", ",", "default", "=", "'test.pkl'", ",", "help", "=", "'The test data directory'", ")", "\n", "\n", "# Model Settings", "\n", "parser", ".", "add_argument", "(", "'--model_name'", ",", "type", "=", "str", ",", "default", "=", "'gpt2'", ",", "help", "=", "'Pretrained model name'", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'Whether to use gpu for training'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "str", ",", "default", "=", "'0'", ",", "help", "=", "'Gpu ids for training'", ")", "\n", "# parser.add_argument('--apex', type=bool, default=False, help='Whether to use half precision')", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "'batch_size for training and evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--shuffle'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'whether to shuffle training data'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "'training iterations'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluation_step'", ",", "type", "=", "int", ",", "default", "=", "2", ",", "\n", "help", "=", "'when training for some steps, start evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'the learning rate of training'", ")", "\n", "parser", ".", "add_argument", "(", "'--set_seed'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'Whether to fix the random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1024", ",", "help", "=", "'fix the random seed for reproducible'", ")", "\n", "parser", ".", "add_argument", "(", "'--patient'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'the patient of early-stopping'", ")", "\n", "parser", ".", "add_argument", "(", "'--length'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "'the max length of generated text'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/output_examples'", ")", "\n", "parser", ".", "add_argument", "(", "'--hyp_only'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--alpha'", ",", "type", "=", "float", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "'--mode'", ",", "type", "=", "str", ",", "default", "=", "'discriminate_generate'", ")", "\n", "# parser.add_argument('--length', type=int, default=22)", "\n", "parser", ".", "add_argument", "(", "'--beam_size'", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "'--no_repeat_ngram_size'", ",", "type", "=", "int", ",", "default", "=", "3", ")", "\n", "parser", ".", "add_argument", "(", "'--repetition_penalty'", ",", "type", "=", "float", ",", "default", "=", "1.5", ")", "\n", "parser", ".", "add_argument", "(", "'--early_stopping'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--do_sample'", ",", "type", "=", "bool", ",", "default", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "'--attack_rate'", ",", "type", "=", "float", ",", "default", "=", "0.015", ")", "\n", "\n", "# parsing the hyper-parameters from command line and define logger", "\n", "hps", "=", "parser", ".", "parse_args", "(", ")", "\n", "logger", ",", "formatter", "=", "define_logger", "(", ")", "\n", "# nowtime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')", "\n", "log_path", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "log_dir", ",", "hps", ".", "mode", "+", "'_'", "+", "hps", ".", "model_name", "+", "'.txt'", ")", "\n", "\n", "file_handler", "=", "logging", ".", "FileHandler", "(", "log_path", ")", "\n", "file_handler", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "\n", "# fix random seed", "\n", "if", "hps", ".", "set_seed", ":", "\n", "        ", "random", ".", "seed", "(", "hps", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "hps", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "hps", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "hps", ".", "seed", ")", "\n", "\n", "# load data", "\n", "# logger.info(\"[Pytorch] %s\", torch.)", "\n", "", "logger", ".", "info", "(", "\"[INFO] Loading Data\"", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Mode:\\t{}\"", ".", "format", "(", "hps", ".", "mode", ")", ")", "\n", "train_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "train", ")", ")", "\n", "dev_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "dev", ")", ")", "\n", "test_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "test", ")", ")", "\n", "\n", "# Tokenization", "\n", "logger", ".", "info", "(", "\"[INFO] Tokenization and Padding for Data\"", ")", "\n", "train_ids", ",", "train_mask", ",", "train_pos", ",", "train_labels", ",", "train_loss_labels", ",", "_", ",", "_", ",", "_", "=", "tokenization", "(", "train_data", ",", "hps", ")", "\n", "dev", ",", "dev_mask", ",", "dev_pos", ",", "dev_labels", ",", "dev_loss_labels", ",", "dev_premise_ids", ",", "dev_premise_mask", ",", "dev_truth", "=", "tokenization", "(", "dev_data", ",", "hps", ")", "\n", "test", ",", "test_mask", ",", "test_pos", ",", "test_labels", ",", "test_loss_labels", ",", "test_premise_ids", ",", "test_premise_mask", ",", "test_truth", "=", "tokenization", "(", "test_data", ",", "hps", ")", "\n", "\n", "# Dataset and DataLoader", "\n", "logger", ".", "info", "(", "\"[INFO] Creating Dataset and splitting batch for data\"", ")", "\n", "TRAIN", "=", "TensorDataset", "(", "train_ids", ",", "train_mask", ",", "train_pos", ",", "train_labels", ",", "train_loss_labels", ")", "\n", "DEV", "=", "TensorDataset", "(", "dev", ",", "dev_mask", ",", "dev_pos", ",", "dev_labels", ",", "dev_loss_labels", ",", "dev_premise_ids", ",", "dev_premise_mask", ",", "dev_truth", ")", "\n", "TEST", "=", "TensorDataset", "(", "test", ",", "test_mask", ",", "test_pos", ",", "test_labels", ",", "test_loss_labels", ",", "test_premise_ids", ",", "test_premise_mask", ",", "test_truth", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "TRAIN", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "DEV", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "TEST", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "\n", "# initialize model, optimizer, loss_function", "\n", "logger", ".", "info", "(", "'[INFO] Loading pretrained model, setting optimizer and loss function'", ")", "\n", "\n", "model", "=", "gpt2_multi_task", "(", "hps", ")", "\n", "\n", "optimizer", "=", "AdamW", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "lr", "=", "hps", ".", "lr", ")", "\n", "loss_function", "=", "nn", ".", "BCEWithLogitsLoss", "(", "reduction", "=", "'mean'", ")", "\n", "loss_function2", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'mean'", ",", "ignore_index", "=", "-", "100", ")", "\n", "\n", "# Multi-Gpu training", "\n", "if", "hps", ".", "cuda", ":", "\n", "        ", "gpu_ids", "=", "[", "int", "(", "x", ")", "for", "x", "in", "hps", ".", "gpu", ".", "split", "(", "' '", ")", "]", "\n", "model", ".", "cuda", "(", "gpu_ids", "[", "0", "]", ")", "\n", "if", "len", "(", "gpu_ids", ")", ">", "1", ":", "\n", "            ", "model", "=", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "gpu_ids", ")", "\n", "\n", "# training", "\n", "", "", "logger", ".", "info", "(", "\"[INFO] Start Training\"", ")", "\n", "step", "=", "0", "\n", "patient", "=", "0", "\n", "best_accuracy", "=", "0", "\n", "stop_train", "=", "False", "\n", "\n", "for", "epoch", "in", "range", "(", "hps", ".", "epochs", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'[Epoch] {}'", ".", "format", "(", "epoch", ")", ")", "\n", "t", "=", "trange", "(", "len", "(", "train_dataloader", ")", ")", "\n", "epoch_step", "=", "0", "\n", "total_loss", "=", "0", "\n", "for", "i", ",", "batch", "in", "zip", "(", "t", ",", "train_dataloader", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "if", "hps", ".", "cuda", ":", "\n", "                ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "input_ids", ",", "input_mask", ",", "pos", ",", "label", ",", "loss_label", "=", "batch", "\n", "\n", "dis_logits", ",", "gen_logits", "=", "model", "(", "input_ids", ",", "attention_mask", "=", "input_mask", ",", "pos", "=", "pos", ")", "\n", "# pdb.set_trace()", "\n", "loss_dis", "=", "loss_function", "(", "dis_logits", ",", "label", ".", "float", "(", ")", ")", "\n", "\n", "# a1 = dis_logits[::2].unsqueeze(1)", "\n", "# a2 = dis_logits[1::2].unsqueeze(1)", "\n", "# a = torch.cat((a1, a2), 1)", "\n", "# index = a.argmax(1)", "\n", "\n", "t_1", "=", "label", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "t_2", "=", "label", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "t_", "=", "torch", ".", "cat", "(", "(", "t_1", ",", "t_2", ")", ",", "1", ")", "\n", "t_index", "=", "t_", ".", "argmax", "(", "1", ")", "\n", "\n", "g1", "=", "gen_logits", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "g2", "=", "gen_logits", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "g", "=", "torch", ".", "cat", "(", "(", "g1", ",", "g2", ")", ",", "1", ")", "\n", "\n", "l1", "=", "loss_label", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "l2", "=", "loss_label", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "l", "=", "torch", ".", "cat", "(", "(", "l1", ",", "l2", ")", ",", "1", ")", "\n", "\n", "g", "=", "g", "[", "range", "(", "g", ".", "shape", "[", "0", "]", ")", ",", "t_index", ",", ":", "]", "\n", "l", "=", "l", "[", "range", "(", "l", ".", "shape", "[", "0", "]", ")", ",", "t_index", ",", ":", "]", "\n", "\n", "# pdb.set_trace()", "\n", "\n", "shift_logits", "=", "g", "[", "...", ",", ":", "-", "1", ",", ":", "]", ".", "contiguous", "(", ")", "\n", "shift_labels", "=", "l", "[", "...", ",", "1", ":", "]", ".", "contiguous", "(", ")", "\n", "\n", "loss_gen", "=", "loss_function2", "(", "shift_logits", ".", "view", "(", "-", "1", ",", "shift_logits", ".", "size", "(", "-", "1", ")", ")", ",", "shift_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "loss", "=", "hps", ".", "alpha", "*", "loss_dis", "+", "(", "1", "-", "hps", ".", "alpha", ")", "*", "loss_gen", "\n", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "t", ".", "set_postfix", "(", "avg_loss", "=", "'{}'", ".", "format", "(", "total_loss", "/", "(", "epoch_step", "+", "1", ")", ")", ")", "\n", "epoch_step", "+=", "1", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "step", "%", "hps", ".", "evaluation_step", "==", "0", "and", "step", "!=", "0", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "\n", "# with torch.no_grad():", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Dev Evaluation] Start Evaluation on Dev Set\"", ")", "\n", "evaluate_output", "=", "evaluate", "(", "hps", ",", "model", ",", "dev_dataloader", ",", "loss_function", ",", "loss_function2", ",", "optimizer", ")", "\n", "dev_ppl", "=", "compute_ppl", "(", "hps", ",", "model", ",", "dev_data", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Perplexity: \\t{}\"", ".", "format", "(", "dev_ppl", ")", ")", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Accuracy: \\t{}\"", ".", "format", "(", "evaluate_output", "[", "0", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[DEV Metrics] Dev Attack Accuracy: \\t{}\"", ".", "format", "(", "evaluate_output", "[", "-", "2", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"[Dev Metrics] Dev BLEU:\\t({}, {}, {}, {})\"", ".", "format", "(", "evaluate_output", "[", "1", "]", ",", "evaluate_output", "[", "2", "]", ",", "\n", "evaluate_output", "[", "3", "]", ",", "evaluate_output", "[", "4", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Discriminate Loss: \\t{}\"", ".", "format", "(", "evaluate_output", "[", "-", "3", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Discriminate Attack Loss: \\t{}\"", ".", "format", "(", "evaluate_output", "[", "-", "1", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"[Dev Metrics] Dev Rouge Recall:\\t({}, {}, {})\"", ".", "format", "(", "evaluate_output", "[", "5", "]", ",", "evaluate_output", "[", "6", "]", ",", "\n", "evaluate_output", "[", "7", "]", ")", ")", "\n", "\n", "if", "evaluate_output", "[", "0", "]", ">=", "best_accuracy", ":", "\n", "                    ", "patient", "=", "0", "\n", "best_accuracy", "=", "evaluate_output", "[", "0", "]", "\n", "logger", ".", "info", "(", "\"[Saving] Saving Model to {}\"", ".", "format", "(", "hps", ".", "save_dir", ")", ")", "\n", "# torch.save(model, os.path.join(hps.save_dir, '{}_{}'.format('generated', hps.model_name)))", "\n", "logger", ".", "info", "(", "\"[Test Evaluation] Start Evaluation on Test Set\"", ")", "\n", "\n", "test_output", "=", "evaluate", "(", "hps", ",", "model", ",", "test_dataloader", ",", "loss_function", ",", "loss_function2", ",", "optimizer", ")", "\n", "test_ppl", "=", "compute_ppl", "(", "hps", ",", "model", ",", "test_data", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Perplexity: \\t{}\"", ".", "format", "(", "test_ppl", ")", ")", "\n", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Accuracy: \\t{}\"", ".", "format", "(", "test_output", "[", "0", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Attack Accuracy: \\t{}\"", ".", "format", "(", "test_output", "[", "-", "2", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test BLEU:\\t({}, {}, {}, {})\"", ".", "format", "(", "test_output", "[", "1", "]", ",", "test_output", "[", "2", "]", ",", "\n", "test_output", "[", "3", "]", ",", "\n", "test_output", "[", "4", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Discriminate Loss: \\t{}\"", ".", "format", "(", "test_output", "[", "-", "3", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Discriminate Attack Loss: \\t{}\"", ".", "format", "(", "test_output", "[", "-", "1", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"[Test Metrics] Test Rouge Recall:\\t({}, {}, {})\"", ".", "format", "(", "test_output", "[", "5", "]", ",", "test_output", "[", "6", "]", ",", "\n", "test_output", "[", "7", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "patient", "+=", "1", "\n", "\n", "", "logger", ".", "info", "(", "\"[Patient] {}\"", ".", "format", "(", "patient", ")", ")", "\n", "\n", "if", "patient", ">=", "hps", ".", "patient", ":", "\n", "                    ", "logger", ".", "info", "(", "\"[INFO] Stopping Training by Early Stopping\"", ")", "\n", "stop_train", "=", "True", "\n", "break", "\n", "", "", "step", "+=", "1", "\n", "\n", "", "if", "stop_train", ":", "\n", "            ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_generate.main": [[18, 190], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.utils.define_logger", "datetime.datetime.now().strftime", "os.path.join", "logging.FileHandler", "logging.FileHandler.setFormatter", "logger.addHandler", "logger.info", "utils.utils.load_data", "utils.utils.load_data", "utils.utils.load_data", "logger.info", "utils.utils.tokenize_gen", "utils.utils.tokenize_gen", "utils.utils.tokenize_gen", "logger.info", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "logger.info", "transformers.GPT2LMHeadModel.from_pretrained", "transformers.AdamW", "logger.info", "range", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "os.path.join", "os.path.join", "os.path.join", "filter", "nn.DataParallel.cuda", "logger.info", "tqdm.trange", "zip", "datetime.datetime.now", "nn.DataParallel.parameters", "int", "len", "torch.DataParallel", "len", "transformers.AdamW.zero_grad", "nn.DataParallel.train", "torch.ones().long", "torch.ones().long", "torch.sum().squeeze().tolist", "torch.sum().squeeze().tolist", "range", "nn.DataParallel.", "loss.item", "tqdm.trange.set_postfix", "loss.backward", "transformers.AdamW.step", "parser.parse_args.gpu.split", "tuple", "nn.DataParallel.eval", "torch.ones", "torch.ones", "torch.sum().squeeze", "torch.sum().squeeze", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "torch.cat", "torch.cat", "torch.no_grad", "torch.no_grad", "print", "logger.info", "utils.utils.gpt2_evaluate", "print", "logger.info", "logger.info", "utils.utils.compute_ppl", "logger.info", "logger.info", "term.cuda", "logger.info", "logger.info", "utils.utils.gpt2_evaluate", "print", "logger.info", "logger.info", "utils.utils.compute_ppl", "logger.info", "logger.info", "torch.sum", "torch.sum", "torch.cat", "torch.cat", "torch.cat().unsqueeze", "torch.cat().unsqueeze", "input_labels_mask.cpu", "torch.cat", "torch.cat"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.define_logger", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.tokenize_gen", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.tokenize_gen", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.tokenize_gen", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.gpt2_evaluate", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.compute_ppl", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.gpt2_evaluate", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.compute_ppl"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'xCAR'", ")", "\n", "\n", "# Data Paths", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "'./data/'", ",", "help", "=", "'The dataset directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--model_dir'", ",", "type", "=", "str", ",", "default", "=", "'../../huggingface_transformers/gpt2/'", ",", "\n", "help", "=", "'The pretrained model directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/saved_model'", ",", "help", "=", "'The model saving directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/log'", ",", "help", "=", "'The training log directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--apex_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/log'", ",", "help", "=", "'The apex directory'", ")", "\n", "\n", "# Data names", "\n", "parser", ".", "add_argument", "(", "'--train'", ",", "type", "=", "str", ",", "default", "=", "'train_gen.pkl'", ",", "help", "=", "'The train data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--dev'", ",", "type", "=", "str", ",", "default", "=", "'dev_gen.pkl'", ",", "help", "=", "'The dev data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--test'", ",", "type", "=", "str", ",", "default", "=", "'test_gen.pkl'", ",", "help", "=", "'The test data directory'", ")", "\n", "\n", "# Model Settings", "\n", "parser", ".", "add_argument", "(", "'--model_name'", ",", "type", "=", "str", ",", "default", "=", "'gpt2'", ",", "help", "=", "'Pretrained model name'", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'Whether to use gpu for training'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "str", ",", "default", "=", "'0'", ",", "help", "=", "'Gpu ids for training'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "32", ",", "help", "=", "'batch_size for training and evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--shuffle'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'whether to shuffle training data'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "'training iterations'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluation_step'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'when training for some steps, start evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'the learning rate of training'", ")", "\n", "parser", ".", "add_argument", "(", "'--set_seed'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'Whether to fix the random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1024", ",", "help", "=", "'fix the random seed for reproducible'", ")", "\n", "parser", ".", "add_argument", "(", "'--patient'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'the patient of early-stopping'", ")", "\n", "parser", ".", "add_argument", "(", "'--length'", ",", "type", "=", "int", ",", "default", "=", "22", ",", "help", "=", "'the max length of generated text'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/output_examples'", ")", "\n", "\n", "# parsing the hyper-parameters from command line and define logger", "\n", "hps", "=", "parser", ".", "parse_args", "(", ")", "\n", "logger", ",", "formatter", "=", "define_logger", "(", ")", "\n", "nowtime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "log_path", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "log_dir", ",", "'generated_'", "+", "hps", ".", "model_name", "+", "'.txt'", ")", "\n", "file_handler", "=", "logging", ".", "FileHandler", "(", "log_path", ")", "\n", "file_handler", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "\n", "# fix random seed", "\n", "if", "hps", ".", "set_seed", ":", "\n", "        ", "random", ".", "seed", "(", "hps", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "hps", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "hps", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "hps", ".", "seed", ")", "\n", "\n", "# load data", "\n", "# logger.info(\"[Pytorch] %s\", torch.)", "\n", "", "logger", ".", "info", "(", "\"[INFO] Loading Data\"", ")", "\n", "train_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "train", ")", ")", "\n", "dev_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "dev", ")", ")", "\n", "test_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "test", ")", ")", "\n", "\n", "# Tokenization", "\n", "logger", ".", "info", "(", "\"[INFO] Tokenization and Padding for Data\"", ")", "\n", "train_ids", ",", "train_mask", ",", "train_seg_ids", ",", "train_label_ids", ",", "train_label_mask", ",", "_", ",", "_", ",", "_", ",", "_", "=", "tokenize_gen", "(", "train_data", ",", "hps", ")", "\n", "_", ",", "_", ",", "_", ",", "dev_label_ids", ",", "dev_label_mask", ",", "dev_label_seg_ids", ",", "dev_premise_ids", ",", "dev_premise_mask", ",", "dev_premise_seg_ids", "=", "tokenize_gen", "(", "dev_data", ",", "hps", ")", "\n", "_", ",", "_", ",", "_", ",", "test_label_ids", ",", "test_label_mask", ",", "test_label_seg_ids", ",", "test_premise_ids", ",", "test_premise_mask", ",", "test_premise_seg_ids", "=", "tokenize_gen", "(", "test_data", ",", "hps", ")", "\n", "\n", "# Dataset and DataLoader", "\n", "logger", ".", "info", "(", "\"[INFO] Creating Dataset and splitting batch for data\"", ")", "\n", "TRAIN", "=", "TensorDataset", "(", "train_ids", ",", "train_mask", ",", "train_seg_ids", ",", "train_label_ids", ",", "train_label_mask", ")", "\n", "DEV", "=", "TensorDataset", "(", "dev_label_ids", ",", "dev_label_mask", ",", "dev_label_seg_ids", ",", "dev_premise_ids", ",", "dev_premise_mask", ",", "dev_premise_seg_ids", ")", "\n", "TEST", "=", "TensorDataset", "(", "test_label_ids", ",", "test_label_mask", ",", "test_label_seg_ids", ",", "test_premise_ids", ",", "test_premise_mask", ",", "test_premise_seg_ids", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "TRAIN", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "DEV", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "TEST", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "\n", "# initialize model, optimizer, loss_function", "\n", "logger", ".", "info", "(", "'[INFO] Loading pretrained model, setting optimizer and loss function'", ")", "\n", "\n", "# model = gpt2_generate(hps)", "\n", "model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "\n", "optimizer", "=", "AdamW", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "lr", "=", "hps", ".", "lr", ")", "\n", "\n", "# Multi-Gpu training", "\n", "if", "hps", ".", "cuda", ":", "\n", "        ", "gpu_ids", "=", "[", "int", "(", "x", ")", "for", "x", "in", "hps", ".", "gpu", ".", "split", "(", "' '", ")", "]", "\n", "model", ".", "cuda", "(", "gpu_ids", "[", "0", "]", ")", "\n", "if", "len", "(", "gpu_ids", ")", ">", "1", ":", "\n", "            ", "model", "=", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "gpu_ids", ")", "\n", "\n", "# training", "\n", "", "", "logger", ".", "info", "(", "\"[INFO] Start Training\"", ")", "\n", "step", "=", "0", "\n", "patient", "=", "0", "\n", "best_accuracy", "=", "0", "\n", "stop_train", "=", "False", "\n", "\n", "for", "epoch", "in", "range", "(", "hps", ".", "epochs", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'[Epoch] {}'", ".", "format", "(", "epoch", ")", ")", "\n", "t", "=", "trange", "(", "len", "(", "train_dataloader", ")", ")", "\n", "epoch_step", "=", "0", "\n", "total_loss", "=", "0", "\n", "for", "i", ",", "batch", "in", "zip", "(", "t", ",", "train_dataloader", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "if", "hps", ".", "cuda", ":", "\n", "                ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "input_ids", ",", "input_mask", ",", "input_seg_ids", ",", "input_labels", ",", "input_labels_mask", "=", "batch", "\n", "# pdb.set_trace()", "\n", "tmp", "=", "torch", ".", "ones", "(", "input_labels_mask", ".", "shape", ")", ".", "long", "(", ")", "\n", "count_mask_length", "=", "torch", ".", "sum", "(", "tmp", "==", "input_labels_mask", ".", "cpu", "(", ")", ",", "1", ")", ".", "squeeze", "(", ")", ".", "tolist", "(", ")", "\n", "true_labels", "=", "None", "\n", "for", "j", "in", "range", "(", "input_ids", ".", "shape", "[", "0", "]", ")", ":", "\n", "                ", "if", "true_labels", "is", "None", ":", "\n", "# true_labels = torch.cat((torch.ones(count_mask_length[j]).long(), input_ids[j, count_mask_length[j]:].cpu())).unsqueeze(0)", "\n", "                    ", "true_labels", "=", "torch", ".", "cat", "(", "(", "input_ids", "[", "j", ",", ":", "-", "count_mask_length", "[", "j", "]", "]", "*", "0", "-", "100", ",", "input_ids", "[", "j", ",", "-", "count_mask_length", "[", "j", "]", ":", "]", ")", ")", ".", "unsqueeze", "(", "0", ")", "\n", "", "else", ":", "\n", "# true_labels = torch.cat((true_labels, torch.cat((torch.ones(count_mask_length[j]).long(), input_ids[j, count_mask_length[j]:].cpu())).unsqueeze(0)), 0)", "\n", "                    ", "true_labels", "=", "torch", ".", "cat", "(", "(", "true_labels", ",", "torch", ".", "cat", "(", "(", "input_ids", "[", "j", ",", ":", "-", "count_mask_length", "[", "j", "]", "]", "*", "0", "-", "100", ",", "input_ids", "[", "j", ",", "-", "count_mask_length", "[", "j", "]", ":", "]", ")", ")", ".", "unsqueeze", "(", "0", ")", ")", ",", "0", ")", "\n", "\n", "# true_labels = true_labels.cuda()", "\n", "\n", "\n", "# loss = model(input_ids=input_ids, attention_mask=input_mask, token_type_ids=input_seg_ids, true_labels=true_labels, mode='train')[0]", "\n", "# pdb.set_trace()", "\n", "", "", "output", "=", "model", "(", "input_ids", "=", "input_ids", ",", "attention_mask", "=", "input_mask", ",", "token_type_ids", "=", "input_seg_ids", ",", "labels", "=", "true_labels", ")", "\n", "loss", "=", "output", "[", "0", "]", "\n", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "t", ".", "set_postfix", "(", "avg_loss", "=", "'{}'", ".", "format", "(", "total_loss", "/", "(", "epoch_step", "+", "1", ")", ")", ")", "\n", "epoch_step", "+=", "1", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "step", "%", "hps", ".", "evaluation_step", "==", "0", "and", "step", "!=", "0", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Dev Evaluation] Start Evaluation on Dev Set\"", ")", "\n", "dev_bleu1", ",", "dev_bleu2", ",", "dev_bleu3", ",", "dev_bleu4", ",", "dev_rouge1", ",", "dev_rouge2", ",", "dev_rougel", "=", "gpt2_evaluate", "(", "model", ",", "hps", ".", "length", ",", "dev_dataloader", ",", "hps", ")", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev BLEU: \\t({}, {}, {}, {})\"", ".", "format", "(", "dev_bleu1", ",", "dev_bleu2", ",", "dev_bleu3", ",", "dev_bleu4", ")", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Rouge: \\t({}, {}, {})\"", ".", "format", "(", "dev_rouge1", ",", "dev_rouge2", ",", "dev_rougel", ")", ")", "\n", "\n", "dev_ppl", "=", "compute_ppl", "(", "hps", ",", "model", ",", "dev_data", ")", "\n", "logger", ".", "info", "(", "'[PPL] Model PerPlexity On Dev Set is {}'", ".", "format", "(", "dev_ppl", ")", ")", "\n", "\n", "if", "dev_bleu1", "+", "dev_rouge1", ">=", "best_accuracy", ":", "\n", "                        ", "patient", "=", "0", "\n", "best_accuracy", "=", "dev_bleu1", "+", "dev_rouge1", "\n", "logger", ".", "info", "(", "\"[Saving] Saving Model to {}\"", ".", "format", "(", "hps", ".", "save_dir", ")", ")", "\n", "# torch.save(model, os.path.join(hps.save_dir, '{}_{}'.format('generated', hps.model_name)))", "\n", "logger", ".", "info", "(", "\"[Test Evaluation] Start Evaluation on Test Set\"", ")", "\n", "\n", "test_bleu1", ",", "test_bleu2", ",", "test_bleu3", ",", "test_bleu4", ",", "test_rouge1", ",", "test_rouge2", ",", "test_rougel", "=", "gpt2_evaluate", "(", "model", ",", "hps", ".", "length", ",", "test_dataloader", ",", "hps", ")", "\n", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[TEST Metrics] Test BLEU: \\t({}, {}, {}, {})\"", ".", "format", "(", "test_bleu1", ",", "test_bleu2", ",", "test_bleu3", ",", "test_bleu4", ")", ")", "\n", "logger", ".", "info", "(", "\"[TEST Metrics] Test Rouge: \\t({}, {}, {})\"", ".", "format", "(", "test_rouge1", ",", "test_rouge2", ",", "test_rougel", ")", ")", "\n", "test_ppl", "=", "compute_ppl", "(", "hps", ",", "model", ",", "test_data", ")", "\n", "logger", ".", "info", "(", "'[PPL] Model PerPlexity On Test Set is {}'", ".", "format", "(", "test_ppl", ")", ")", "\n", "", "else", ":", "\n", "                        ", "patient", "+=", "1", "\n", "\n", "", "logger", ".", "info", "(", "\"[Patient] {}\"", ".", "format", "(", "patient", ")", ")", "\n", "\n", "if", "patient", ">=", "hps", ".", "patient", ":", "\n", "                        ", "logger", ".", "info", "(", "\"[INFO] Stopping Training by Early Stopping\"", ")", "\n", "stop_train", "=", "True", "\n", "break", "\n", "", "", "", "step", "+=", "1", "\n", "\n", "", "if", "stop_train", ":", "\n", "            ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.roberta.__init__": [[19, 26], ["torch.Module.__init__", "transformers.RobertaModel.from_pretrained", "transformers.RobertaConfig.from_pretrained", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax"], "methods", ["home.repos.pwc.inspect_result.waste-wood_e-care.model.discriminate_model.pretrained_model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hps", ")", ":", "\n", "        ", "super", "(", "roberta", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model", "=", "RobertaModel", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "config", "=", "RobertaConfig", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "hps", "=", "hps", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.roberta.forward": [[27, 39], ["adversarial_filtering.roberta.model", "adversarial_filtering.roberta.linear().squeeze", "scores[].unsqueeze", "scores[].unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "adversarial_filtering.roberta.softmax", "adversarial_filtering.roberta.linear"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "mode", "=", "'train'", ")", ":", "\n", "        ", "output", "=", "self", ".", "model", "(", "input_ids", "=", "input_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "cls_token", "=", "output", "[", "1", "]", "\n", "scores", "=", "self", ".", "linear", "(", "cls_token", ")", ".", "squeeze", "(", "1", ")", "\n", "if", "mode", "==", "'train'", ":", "\n", "            ", "alternative1", "=", "scores", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "alternative2", "=", "scores", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", "\n", "alternatives", "=", "torch", ".", "cat", "(", "(", "alternative1", ",", "alternative2", ")", ",", "1", ")", "\n", "probs", "=", "self", ".", "softmax", "(", "alternatives", ")", "\n", "", "else", ":", "\n", "            ", "probs", "=", "None", "\n", "", "return", "probs", ",", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.data_process": [[41, 48], ["utils.utils.load_data", "utils.utils.load_data", "utils.utils.load_data", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data"], ["", "", "def", "data_process", "(", "path", ")", ":", "\n", "    ", "train_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'train.pkl'", ")", ")", "\n", "test_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'test.pkl'", ")", ")", "\n", "dev_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "path", ",", "'dev.pkl'", ")", ")", "\n", "\n", "data", "=", "train_data", "+", "test_data", "+", "dev_data", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.split_data": [[50, 58], ["range", "len", "t.append", "d.append"], "function", ["None"], ["", "def", "split_data", "(", "data", ")", ":", "\n", "    ", "t", ",", "d", "=", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "data", ")", ")", ":", "\n", "        ", "if", "i", "%", "10", "<", "7", ":", "\n", "            ", "t", ".", "append", "(", "data", "[", "i", "]", ")", "\n", "", "else", ":", "\n", "            ", "d", ".", "append", "(", "data", "[", "i", "]", ")", "\n", "", "", "return", "t", ",", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.shuffle_data": [[61, 123], ["random.seed", "random.shuffle", "adversarial_filtering.split_data", "adversarial_filtering.split_data", "adversarial_filtering.split_data", "adversarial_filtering.split_data", "random.shuffle", "random.shuffle", "train.append", "ask_fors.append", "truths.append", "labels.append", "cause_label_0.append", "effects.append", "right_causes.append", "wrong_causes.append", "causes.append", "right_effects.append", "wrong_effects.append", "cause_label_1.append", "len", "len", "len", "effect_label_0.append", "effect_label_1.append", "len"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.split_data", "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.split_data", "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.split_data", "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.split_data"], ["", "def", "shuffle_data", "(", "data", ",", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "shuffle", "(", "data", ")", "\n", "\n", "cause_label_0", ",", "cause_label_1", ",", "effect_label_0", ",", "effect_label_1", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "train", ",", "dev", "=", "[", "]", ",", "[", "]", "\n", "\n", "for", "d", "in", "data", ":", "\n", "        ", "if", "d", "[", "'label'", "]", "==", "0", "and", "d", "[", "'ask-for'", "]", "==", "'cause'", ":", "\n", "            ", "cause_label_0", ".", "append", "(", "d", ")", "\n", "", "elif", "d", "[", "'label'", "]", "==", "1", "and", "d", "[", "'ask-for'", "]", "==", "'cause'", ":", "\n", "            ", "cause_label_1", ".", "append", "(", "d", ")", "\n", "", "elif", "d", "[", "'label'", "]", "==", "0", ":", "\n", "            ", "effect_label_0", ".", "append", "(", "d", ")", "\n", "", "else", ":", "\n", "            ", "effect_label_1", ".", "append", "(", "d", ")", "\n", "\n", "", "", "t", ",", "d", "=", "split_data", "(", "cause_label_0", ")", "\n", "train", "+=", "t", "\n", "dev", "+=", "d", "\n", "\n", "t", ",", "d", "=", "split_data", "(", "cause_label_1", ")", "\n", "train", "+=", "t", "\n", "dev", "+=", "d", "\n", "\n", "t", ",", "d", "=", "split_data", "(", "effect_label_0", ")", "\n", "train", "+=", "t", "\n", "dev", "+=", "d", "\n", "\n", "t", ",", "d", "=", "split_data", "(", "effect_label_1", ")", "\n", "train", "+=", "t", "\n", "dev", "+=", "d", "\n", "\n", "random", ".", "shuffle", "(", "train", ")", "\n", "random", ".", "shuffle", "(", "dev", ")", "\n", "\n", "if", "len", "(", "train", ")", "%", "2", "!=", "0", "and", "len", "(", "dev", ")", "%", "2", "!=", "0", ":", "\n", "        ", "train", ".", "append", "(", "dev", "[", "-", "1", "]", ")", "\n", "dev", "=", "dev", "[", ":", "-", "1", "]", "\n", "", "elif", "len", "(", "train", ")", "%", "2", "!=", "0", ":", "\n", "        ", "train", "=", "train", "[", ":", "-", "1", "]", "\n", "", "elif", "len", "(", "dev", ")", "%", "2", "!=", "0", ":", "\n", "        ", "dev", "=", "dev", "[", ":", "-", "1", "]", "\n", "\n", "", "right_causes", ",", "right_effects", ",", "wrong_causes", ",", "wrong_effects", ",", "causes", ",", "effects", ",", "ask_fors", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "truths", ",", "labels", "=", "[", "]", ",", "[", "]", "\n", "for", "example", "in", "dev", ":", "\n", "        ", "ask_for", "=", "example", "[", "'ask-for'", "]", "\n", "if", "ask_for", "==", "'cause'", ":", "\n", "            ", "effects", ".", "append", "(", "example", "[", "'premise'", "]", ")", "\n", "right_causes", ".", "append", "(", "example", "[", "'alternative1'", "]", "if", "example", "[", "'label'", "]", "==", "0", "else", "example", "[", "'alternative2'", "]", ")", "\n", "wrong_causes", ".", "append", "(", "example", "[", "'alternative2'", "]", "if", "example", "[", "'label'", "]", "==", "0", "else", "example", "[", "'alternative1'", "]", ")", "\n", "", "else", ":", "\n", "            ", "causes", ".", "append", "(", "example", "[", "'premise'", "]", ")", "\n", "right_effects", ".", "append", "(", "example", "[", "'alternative1'", "]", "if", "example", "[", "'label'", "]", "==", "0", "else", "example", "[", "'alternative2'", "]", ")", "\n", "wrong_effects", ".", "append", "(", "example", "[", "'alternative2'", "]", "if", "example", "[", "'label'", "]", "==", "0", "else", "example", "[", "'alternative1'", "]", ")", "\n", "", "ask_fors", ".", "append", "(", "ask_for", ")", "\n", "truths", ".", "append", "(", "example", "[", "'general_truth'", "]", ")", "\n", "labels", ".", "append", "(", "example", "[", "'label'", "]", ")", "\n", "\n", "", "return", "train", ",", "dev", ",", "right_causes", ",", "right_effects", ",", "wrong_causes", ",", "wrong_effects", ",", "causes", ",", "effects", ",", "ask_fors", ",", "truths", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.training": [[125, 190], ["range", "logger.info", "tqdm.trange", "zip", "len", "optimizer.zero_grad", "model.train", "model", "loss_function", "loss_function.item", "tqdm.trange.set_postfix", "loss_function.backward", "optimizer.step", "tuple", "labels.float", "model.eval", "torch.no_grad", "torch.no_grad", "print", "logger.info", "utils.utils.evaluation", "print", "logger.info", "logger.info", "logger.info", "term.cuda", "logger.info", "logger.info"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.evaluation"], ["", "def", "training", "(", "train", ",", "dev", ",", "model", ",", "loss_function", ",", "optimizer", ",", "hps", ")", ":", "\n", "    ", "step", "=", "0", "\n", "patient", "=", "0", "\n", "best_accuracy", "=", "0", "\n", "stop_train", "=", "False", "\n", "final_model", "=", "None", "\n", "for", "epoch", "in", "range", "(", "hps", ".", "epochs", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'[Epoch] {}'", ".", "format", "(", "epoch", ")", ")", "\n", "t", "=", "trange", "(", "len", "(", "train", ")", ")", "\n", "epoch_step", "=", "0", "\n", "total_loss", "=", "0", "\n", "for", "i", ",", "batch", "in", "zip", "(", "t", ",", "train", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "if", "hps", ".", "cuda", ":", "\n", "                ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", "0", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "sent", ",", "atten_mask", ",", "labels", "=", "batch", "\n", "probs", ",", "scores", "=", "model", "(", "sent", ",", "atten_mask", ")", "\n", "\n", "# labels1 = labels[::2].unsqueeze(1)", "\n", "# labels2 = labels[1::2].unsqueeze(1)", "\n", "# labels = torch.cat((labels1, labels2), 1)", "\n", "# labels = torch.argmax(labels, 1)", "\n", "loss", "=", "loss_function", "(", "scores", ",", "labels", ".", "float", "(", ")", ")", "\n", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "t", ".", "set_postfix", "(", "avg_loss", "=", "'{}'", ".", "format", "(", "total_loss", "/", "(", "epoch_step", "+", "1", ")", ")", ")", "\n", "epoch_step", "+=", "1", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "step", "%", "hps", ".", "evaluation_step", "==", "0", "and", "step", "!=", "0", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Dev Evaluation] Start Evaluation on Dev Set\"", ")", "\n", "\n", "dev_accu", ",", "dev_loss", "=", "evaluation", "(", "hps", ",", "dev", ",", "model", ",", "loss_function", ",", "mode", "=", "'filtering'", ")", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Accuracy: \\t{}\"", ".", "format", "(", "dev_accu", ")", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Loss: \\t{}\"", ".", "format", "(", "dev_loss", ")", ")", "\n", "\n", "if", "dev_accu", ">=", "best_accuracy", ":", "\n", "                        ", "patient", "=", "0", "\n", "best_accuracy", "=", "dev_accu", "\n", "logger", ".", "info", "(", "\"[Saving] Saving Model to {}\"", ".", "format", "(", "hps", ".", "save_dir", ")", ")", "\n", "final_model", "=", "model", "\n", "# torch.save(model, os.path.join(hps.save_dir, '{}_{}'.format(hps.model_name, dev_accu)))", "\n", "", "else", ":", "\n", "                        ", "patient", "+=", "1", "\n", "\n", "", "logger", ".", "info", "(", "\"[Patient] {}\"", ".", "format", "(", "patient", ")", ")", "\n", "\n", "if", "patient", ">=", "hps", ".", "patient", ":", "\n", "                        ", "logger", ".", "info", "(", "\"[INFO] Stopping Training by Early Stopping\"", ")", "\n", "stop_train", "=", "True", "\n", "break", "\n", "", "", "", "step", "+=", "1", "\n", "\n", "", "if", "stop_train", ":", "\n", "            ", "break", "\n", "", "", "return", "final_model", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.adversarial_filtering.adversarial_filtering": [[192, 293], ["module.eval", "zip", "enumerate", "range", "tqdm.trange", "random.shuffle", "tokenizer", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "candidate_deltas.cpu().tolist.cpu().tolist", "random.random", "len", "len", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "module", "list", "list", "len", "list", "list", "candidates.append", "random.choice", "candidates.append", "h_negatives.remove", "h_negatives.remove", "tuple", "torch.cat", "torch.cat", "positive.cpu().item", "candidate_deltas.cpu().tolist.cpu", "filter", "filter", "filter", "len", "filter", "len", "dev_adversarial_filtered.append", "dev_adversarial_filtered.append", "dev_adversarial_filtered.append", "dev_adversarial_filtered.append", "len", "range", "range", "range", "range", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "term.cuda", "positive.cpu", "len", "len", "len", "len", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count", "ask_fors[].count"], "function", ["None"], ["", "def", "adversarial_filtering", "(", "module", ",", "ask_fors", ",", "causes", ",", "effects", ",", "right_causes", ",", "right_effects", ",", "wrong_causes", ",", "wrong_effects", ",", "\n", "t", ",", "tokenizer", ",", "truths", ",", "labels", ",", "iterations", ",", "hps", ")", ":", "\n", "    ", "module", ".", "eval", "(", ")", "\n", "candidates", "=", "[", "]", "\n", "count", "=", "0", "\n", "for", "i", ",", "ask_for", ",", "tq", "in", "zip", "(", "range", "(", "len", "(", "ask_fors", ")", ")", ",", "ask_fors", ",", "trange", "(", "len", "(", "ask_fors", ")", ")", ")", ":", "\n", "        ", "if", "ask_for", "==", "'cause'", ":", "\n", "            ", "h_positive", "=", "[", "right_causes", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", ",", "effects", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", "]", "\n", "h_negative", "=", "[", "wrong_causes", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", ",", "effects", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", "]", "\n", "h_negatives", "=", "[", "[", "cause", ",", "effects", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", "]", "for", "cause", "in", "wrong_causes", "]", "\n", "h_negatives", "+=", "[", "[", "cause", ",", "effects", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", "]", "for", "cause", "in", "(", "right_causes", "[", ":", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", "+", "right_causes", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "+", "2", ":", "]", ")", "]", "\n", "while", "h_positive", "in", "h_negatives", ":", "\n", "                ", "h_negatives", ".", "remove", "(", "h_positive", ")", "\n", "", "", "else", ":", "\n", "            ", "h_positive", "=", "[", "causes", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", ",", "right_effects", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", "]", "\n", "h_negative", "=", "[", "causes", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", ",", "wrong_effects", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", "]", "\n", "h_negatives", "=", "[", "[", "causes", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", ",", "effect", "]", "for", "effect", "in", "wrong_effects", "]", "\n", "h_negatives", "+=", "[", "[", "causes", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", ",", "effect", "]", "for", "effect", "in", "(", "right_effects", "[", ":", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "]", "+", "right_effects", "[", "ask_fors", "[", ":", "i", "]", ".", "count", "(", "ask_for", ")", "+", "2", ":", "]", ")", "]", "\n", "# h_negatives.remove(h_positive)", "\n", "while", "h_positive", "in", "h_negatives", ":", "\n", "                ", "h_negatives", ".", "remove", "(", "h_positive", ")", "\n", "", "", "random", ".", "shuffle", "(", "h_negatives", ")", "\n", "h_negatives", "=", "h_negatives", "[", ":", "len", "(", "h_negatives", ")", "//", "4", "]", "\n", "input1", "=", "tokenizer", "(", "[", "h_positive", "]", "+", "[", "h_negative", "]", "+", "h_negatives", ",", "padding", "=", "True", ")", "\n", "input_ids", ",", "attention_mask", "=", "torch", ".", "LongTensor", "(", "input1", "[", "'input_ids'", "]", ")", ",", "torch", ".", "LongTensor", "(", "input1", "[", "'attention_mask'", "]", ")", "\n", "dataset", "=", "TensorDataset", "(", "input_ids", ",", "attention_mask", ")", "\n", "data_loader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "drop_last", "=", "False", ",", "shuffle", "=", "False", ")", "\n", "# print(len(data_loader))", "\n", "scores", "=", "None", "\n", "for", "batch", "in", "data_loader", ":", "\n", "            ", "if", "hps", ".", "cuda", ":", "\n", "                ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", "0", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "ids", ",", "mask", "=", "batch", "\n", "# print('IDS shape: {}'.format(ids.shape))", "\n", "_", ",", "score", "=", "module", "(", "ids", ",", "mask", ",", "mode", "=", "'filtering'", ")", "\n", "scores", "=", "score", "if", "scores", "is", "None", "else", "torch", ".", "cat", "(", "(", "scores", ",", "score", ")", ",", "0", ")", "\n", "\n", "", "positive", "=", "scores", "[", "0", "]", "\n", "negative", "=", "scores", "[", "1", "]", "\n", "negatives", "=", "scores", "[", "2", ":", "]", "\n", "delta", "=", "(", "positive", "-", "negative", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "candidate_deltas", "=", "-", "1", "*", "(", "negatives", "-", "positive", ".", "cpu", "(", ")", ".", "item", "(", ")", ")", "\n", "candidate_deltas", "=", "candidate_deltas", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "if", "delta", ">=", "0", ":", "\n", "            ", "indexes", "=", "list", "(", "filter", "(", "lambda", "x", ":", "candidate_deltas", "[", "x", "]", "<", "0", ",", "range", "(", "len", "(", "candidate_deltas", ")", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "indexes", "=", "list", "(", "filter", "(", "lambda", "x", ":", "candidate_deltas", "[", "x", "]", "<", "delta", ",", "range", "(", "len", "(", "candidate_deltas", ")", ")", ")", ")", "\n", "\n", "", "if", "len", "(", "indexes", ")", "==", "0", ":", "\n", "            ", "indexes", "=", "list", "(", "filter", "(", "lambda", "x", ":", "candidate_deltas", "[", "x", "]", "<", "delta", ",", "range", "(", "len", "(", "candidate_deltas", ")", ")", ")", ")", "\n", "", "if", "len", "(", "indexes", ")", "==", "0", "or", "iterations", ">=", "12", ":", "\n", "            ", "indexes", "=", "list", "(", "filter", "(", "lambda", "x", ":", "candidate_deltas", "[", "x", "]", "<", "delta", "/", "2", ",", "range", "(", "len", "(", "candidate_deltas", ")", ")", ")", ")", "\n", "", "r", "=", "random", ".", "random", "(", ")", "\n", "if", "t", "<", "r", "or", "delta", "<", "0", "or", "len", "(", "indexes", ")", "<=", "0", ":", "\n", "            ", "candidates", ".", "append", "(", "[", "h_negative", "[", "0", "]", "]", "+", "h_positive", "if", "ask_for", "==", "'cause'", "else", "h_positive", "+", "[", "h_negative", "[", "1", "]", "]", ")", "\n", "", "else", ":", "\n", "            ", "index", "=", "choice", "(", "indexes", ")", "\n", "count", "+=", "1", "\n", "candidates", ".", "append", "(", "[", "h_negatives", "[", "index", "]", "[", "0", "]", "]", "+", "h_positive", "if", "ask_for", "==", "'cause'", "else", "h_positive", "+", "[", "h_negatives", "[", "index", "]", "[", "1", "]", "]", ")", "\n", "", "", "dev_adversarial_filtered", "=", "[", "]", "\n", "for", "k", ",", "example", "in", "enumerate", "(", "candidates", ")", ":", "\n", "        ", "if", "ask_fors", "[", "k", "]", "==", "'cause'", ":", "\n", "            ", "if", "labels", "[", "k", "]", "==", "0", ":", "\n", "                ", "dev_adversarial_filtered", ".", "append", "(", "{", "'index'", ":", "k", ",", "\n", "'general_truth'", ":", "truths", "[", "k", "]", ",", "\n", "'ask-for'", ":", "ask_fors", "[", "k", "]", ",", "\n", "'premise'", ":", "candidates", "[", "k", "]", "[", "2", "]", ",", "\n", "'alternative1'", ":", "candidates", "[", "k", "]", "[", "1", "]", ",", "\n", "'alternative2'", ":", "candidates", "[", "k", "]", "[", "0", "]", ",", "\n", "'label'", ":", "labels", "[", "k", "]", "\n", "}", ")", "\n", "", "else", ":", "\n", "                ", "dev_adversarial_filtered", ".", "append", "(", "{", "'index'", ":", "k", ",", "\n", "'general_truth'", ":", "truths", "[", "k", "]", ",", "\n", "'ask-for'", ":", "ask_fors", "[", "k", "]", ",", "\n", "'premise'", ":", "candidates", "[", "k", "]", "[", "2", "]", ",", "\n", "'alternative1'", ":", "candidates", "[", "k", "]", "[", "0", "]", ",", "\n", "'alternative2'", ":", "candidates", "[", "k", "]", "[", "1", "]", ",", "\n", "'label'", ":", "labels", "[", "k", "]", "\n", "}", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "labels", "[", "k", "]", "==", "0", ":", "\n", "                ", "dev_adversarial_filtered", ".", "append", "(", "{", "'index'", ":", "k", ",", "\n", "'general_truth'", ":", "truths", "[", "k", "]", ",", "\n", "'ask-for'", ":", "ask_fors", "[", "k", "]", ",", "\n", "'premise'", ":", "candidates", "[", "k", "]", "[", "0", "]", ",", "\n", "'alternative1'", ":", "candidates", "[", "k", "]", "[", "1", "]", ",", "\n", "'alternative2'", ":", "candidates", "[", "k", "]", "[", "2", "]", ",", "\n", "'label'", ":", "labels", "[", "k", "]", "\n", "}", ")", "\n", "", "else", ":", "\n", "                ", "dev_adversarial_filtered", ".", "append", "(", "{", "'index'", ":", "k", ",", "\n", "'general_truth'", ":", "truths", "[", "k", "]", ",", "\n", "'ask-for'", ":", "ask_fors", "[", "k", "]", ",", "\n", "'premise'", ":", "candidates", "[", "k", "]", "[", "0", "]", ",", "\n", "'alternative1'", ":", "candidates", "[", "k", "]", "[", "2", "]", ",", "\n", "'alternative2'", ":", "candidates", "[", "k", "]", "[", "1", "]", ",", "\n", "'label'", ":", "labels", "[", "k", "]", "\n", "}", ")", "\n", "", "", "", "return", "dev_adversarial_filtered", ",", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.gpt2_discriminate.__init__": [[22, 27], ["torch.Module.__init__", "transformers.GPT2LMHeadModel.from_pretrained", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.waste-wood_e-care.model.discriminate_model.pretrained_model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hps", ")", ":", "\n", "        ", "super", "(", "gpt2_discriminate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hps", "=", "hps", "\n", "self", ".", "model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "self", ".", "model", ".", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.gpt2_discriminate.forward": [[28, 36], ["gpt2_discriminate.gpt2_discriminate.model", "pos.squeeze().unsqueeze.squeeze().unsqueeze.squeeze().unsqueeze", "hidden_state[].squeeze", "gpt2_discriminate.gpt2_discriminate.linear().squeeze", "pos.squeeze().unsqueeze.squeeze().unsqueeze.squeeze", "gpt2_discriminate.gpt2_discriminate.linear", "range"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "pos", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "output_hidden_states", "=", "True", ")", "\n", "# pdb.set_trace()", "\n", "hidden_state", "=", "outputs", ".", "hidden_states", "[", "-", "1", "]", "\n", "pos", "=", "pos", ".", "squeeze", "(", ")", ".", "unsqueeze", "(", "0", ")", "\n", "hidden_state", "=", "hidden_state", "[", "range", "(", "hidden_state", ".", "shape", "[", "0", "]", ")", ",", "pos", ",", ":", "]", ".", "squeeze", "(", "0", ")", "\n", "logits", "=", "self", ".", "linear", "(", "hidden_state", ")", ".", "squeeze", "(", "-", "1", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.tokenization": [[39, 68], ["transformers.GPT2Tokenizer.from_pretrained", "GPT2Tokenizer.from_pretrained.", "max", "range", "len", "pos.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "len", "inputs.append", "inputs.append", "inputs.append", "inputs.append", "len", "range", "range"], "function", ["None"], ["", "", "def", "tokenization", "(", "data", ",", "hps", ")", ":", "\n", "    ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "\n", "inputs", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "pos", "=", "[", "]", "\n", "\n", "for", "example", "in", "data", ":", "\n", "        ", "if", "not", "hps", ".", "hyp_only", ":", "\n", "            ", "if", "example", "[", "'ask-for'", "]", "==", "'cause'", ":", "\n", "                ", "inputs", ".", "append", "(", "[", "example", "[", "'alternative1'", "]", ",", "example", "[", "'premise'", "]", "]", ")", "\n", "inputs", ".", "append", "(", "[", "example", "[", "'alternative2'", "]", ",", "example", "[", "'premise'", "]", "]", ")", "\n", "", "else", ":", "\n", "                ", "inputs", ".", "append", "(", "[", "example", "[", "'premise'", "]", ",", "example", "[", "'alternative1'", "]", "]", ")", "\n", "inputs", ".", "append", "(", "[", "example", "[", "'premise'", "]", ",", "example", "[", "'alternative2'", "]", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "inputs", "+=", "[", "example", "[", "'alternative1'", "]", ",", "example", "[", "'alternative2'", "]", "]", "\n", "", "labels", "+=", "[", "0", ",", "1", "]", "if", "example", "[", "'label'", "]", "==", "1", "else", "[", "1", ",", "0", "]", "\n", "", "outputs", "=", "tokenizer", "(", "inputs", ",", "return_length", "=", "True", ")", "\n", "input_ids", "=", "outputs", "[", "'input_ids'", "]", "\n", "attention_mask", "=", "outputs", "[", "'attention_mask'", "]", "\n", "length", "=", "outputs", "[", "'length'", "]", "\n", "max_length", "=", "max", "(", "length", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "input_ids", ")", ")", ":", "\n", "        ", "gap", "=", "max_length", "-", "len", "(", "input_ids", "[", "i", "]", ")", "+", "1", "\n", "pos", ".", "append", "(", "len", "(", "input_ids", "[", "i", "]", ")", ")", "\n", "input_ids", "[", "i", "]", "+=", "[", "50256", "for", "_", "in", "range", "(", "gap", ")", "]", "\n", "attention_mask", "[", "i", "]", "+=", "[", "1", "]", "+", "[", "0", "for", "_", "in", "range", "(", "gap", "-", "1", ")", "]", "\n", "", "return", "torch", ".", "LongTensor", "(", "input_ids", ")", ",", "torch", ".", "LongTensor", "(", "attention_mask", ")", ",", "torch", ".", "LongTensor", "(", "pos", ")", ",", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.evaluate": [[71, 144], ["torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.argmax", "torch.argmax", "torch.argmax", "range", "copy.deepcopy", "optimizer.zero_grad", "model", "loss_function", "loss_function.item", "loss_function.backward", "len", "tuple", "tmp_labels.float", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.eval", "model", "model.cpu().tolist", "loss_function", "loss_function.item", "tmp_labels.cpu().numpy().tolist", "copy.deepcopy.state_dict", "torch.sum", "torch.sum", "torch.sum", "attack_embedding.pow().unsqueeze.pow().unsqueeze", "copy.deepcopy.load_state_dict", "copy.deepcopy.eval", "copy.deepcopy.", "attack_model.cpu().tolist", "loss_function().item", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax", "torch.argmax", "torch.argmax", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax", "torch.argmax", "torch.argmax", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "len", "len", "tmp_labels.float", "attack_embedding.pow().unsqueeze.pow", "term.cuda", "model.cpu", "tmp_labels.cpu().numpy", "attack_embedding.pow().unsqueeze.pow", "attack_model.cpu", "loss_function", "tmp_labels.float", "tmp_labels.cpu"], "function", ["None"], ["", "def", "evaluate", "(", "hps", ",", "model", ",", "dataloader", ",", "loss_function", ",", "optimizer", ")", ":", "\n", "    ", "predictions", ",", "attack_predictions", "=", "[", "]", ",", "[", "]", "\n", "labels", "=", "[", "]", "\n", "loss", "=", "0", "\n", "attack_loss", "=", "0", "\n", "\n", "# model.eval()", "\n", "for", "batch", "in", "dataloader", ":", "\n", "        ", "attack_model", "=", "copy", ".", "deepcopy", "(", "model", ")", "\n", "# attack_model.eval()", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "if", "hps", ".", "cuda", ":", "\n", "            ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "input_ids", ",", "attention_mask", ",", "pos", ",", "tmp_labels", "=", "batch", "\n", "logits", "=", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "pos", "=", "pos", ")", "\n", "\n", "# predictions += logits.cpu().tolist()", "\n", "tmp_loss", "=", "loss_function", "(", "logits", ",", "tmp_labels", ".", "float", "(", ")", ")", "\n", "loss", "+=", "tmp_loss", ".", "item", "(", ")", "\n", "# labels += tmp_labels.cpu().numpy().tolist()", "\n", "\n", "tmp_loss", ".", "backward", "(", ")", "\n", "embedding_grad", "=", "optimizer", ".", "param_groups", "[", "0", "]", "[", "'params'", "]", "[", "0", "]", ".", "grad", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "logits", "=", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "pos", "=", "pos", ")", "\n", "predictions", "+=", "logits", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "tmp_loss", "=", "loss_function", "(", "logits", ",", "tmp_labels", ".", "float", "(", ")", ")", "\n", "loss", "+=", "tmp_loss", ".", "item", "(", ")", "\n", "labels", "+=", "tmp_labels", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "state_dict", "=", "attack_model", ".", "state_dict", "(", ")", "\n", "# embedding_grad = F.softmax(embedding_grad, 1)", "\n", "attack_embedding", "=", "torch", ".", "sum", "(", "embedding_grad", "*", "embedding_grad", ",", "-", "1", ")", "\n", "attack_embedding", "=", "attack_embedding", ".", "pow", "(", "0.5", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "attack_embedding", "=", "attack_embedding", ".", "pow", "(", "-", "1", ")", "*", "embedding_grad", "\n", "state_dict", "[", "'model.transformer.wte.weight'", "]", "+=", "hps", ".", "attack_rate", "*", "attack_embedding", "\n", "attack_model", ".", "load_state_dict", "(", "state_dict", ")", "\n", "# pdb.set_trace()", "\n", "attack_model", ".", "eval", "(", ")", "\n", "attack_logits", "=", "attack_model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "pos", "=", "pos", ")", "\n", "attack_predictions", "+=", "attack_logits", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "attack_loss", "+=", "loss_function", "(", "attack_logits", ",", "tmp_labels", ".", "float", "(", ")", ")", ".", "item", "(", ")", "\n", "\n", "\n", "", "", "a1", "=", "torch", ".", "FloatTensor", "(", "predictions", "[", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a2", "=", "torch", ".", "FloatTensor", "(", "predictions", "[", "1", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a", "=", "torch", ".", "cat", "(", "(", "a1", ",", "a2", ")", ",", "dim", "=", "1", ")", "\n", "predict_labels", "=", "torch", ".", "argmax", "(", "a", ",", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "t_a1", "=", "torch", ".", "FloatTensor", "(", "labels", "[", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a2", "=", "torch", ".", "FloatTensor", "(", "labels", "[", "1", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a", "=", "torch", ".", "cat", "(", "(", "t_a1", ",", "t_a2", ")", ",", "dim", "=", "1", ")", "\n", "true_labels", "=", "torch", ".", "argmax", "(", "t_a", ",", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "a_t1", "=", "torch", ".", "FloatTensor", "(", "attack_predictions", "[", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a_t2", "=", "torch", ".", "FloatTensor", "(", "attack_predictions", "[", "1", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a_t", "=", "torch", ".", "cat", "(", "(", "a_t1", ",", "a_t2", ")", ",", "1", ")", "\n", "attack_predict_labels", "=", "torch", ".", "argmax", "(", "a_t", ",", "1", ")", "\n", "\n", "count", ",", "attack_count", "=", "0", ",", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "predict_labels", ")", ")", ":", "\n", "        ", "if", "predict_labels", "[", "i", "]", "==", "true_labels", "[", "i", "]", "and", "attack_predict_labels", "[", "i", "]", "==", "true_labels", "[", "i", "]", ":", "\n", "            ", "count", "+=", "1", "\n", "attack_count", "+=", "1", "\n", "", "elif", "predict_labels", "[", "i", "]", "==", "true_labels", "[", "i", "]", ":", "\n", "            ", "count", "+=", "1", "\n", "", "elif", "attack_predict_labels", "[", "i", "]", "==", "true_labels", "[", "i", "]", ":", "\n", "            ", "attack_count", "+=", "1", "\n", "", "else", ":", "\n", "            ", "continue", "\n", "", "", "return", "count", "/", "len", "(", "true_labels", ")", ",", "loss", ",", "attack_count", "/", "len", "(", "true_labels", ")", ",", "attack_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.main": [[147, 311], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.utils.define_logger", "logging.FileHandler", "logging.FileHandler.setFormatter", "logger.addHandler", "logger.info", "logger.info", "utils.utils.load_data", "utils.utils.load_data", "utils.utils.load_data", "logger.info", "gpt2_discriminate.tokenization", "gpt2_discriminate.tokenization", "gpt2_discriminate.tokenization", "logger.info", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "logger.info", "gpt2_discriminate.gpt2_discriminate", "transformers.AdamW", "torch.BCEWithLogitsLoss", "logger.info", "range", "os.path.join", "os.path.join", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "os.path.join", "os.path.join", "os.path.join", "filter", "nn.DataParallel.cuda", "logger.info", "tqdm.trange", "zip", "nn.DataParallel.parameters", "int", "len", "torch.DataParallel", "len", "transformers.AdamW.zero_grad", "nn.DataParallel.train", "nn.DataParallel.", "nn.BCEWithLogitsLoss.", "loss_function.item", "tqdm.trange.set_postfix", "loss_function.backward", "transformers.AdamW.step", "parser.parse_args.gpu.split", "tuple", "label.float", "nn.DataParallel.eval", "logger.info", "gpt2_discriminate.evaluate", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "gpt2_discriminate.evaluate", "print", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "term.cuda"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.define_logger", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.tokenization", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.tokenization", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.tokenization", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.evaluate", "home.repos.pwc.inspect_result.waste-wood_e-care.code.gpt2_discriminate.evaluate"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'xCAR'", ")", "\n", "\n", "# Data Paths", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "'./data/'", ",", "help", "=", "'The dataset directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--model_dir'", ",", "type", "=", "str", ",", "default", "=", "'../../huggingface_transformers/gpt2/'", ",", "\n", "help", "=", "'The pretrained model directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/saved_model'", ",", "help", "=", "'The model saving directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/log'", ",", "help", "=", "'The training log directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--apex_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/log'", ",", "help", "=", "'The apex directory'", ")", "\n", "\n", "# Data names", "\n", "parser", ".", "add_argument", "(", "'--train'", ",", "type", "=", "str", ",", "default", "=", "'train.pkl'", ",", "help", "=", "'The train data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--dev'", ",", "type", "=", "str", ",", "default", "=", "'dev.pkl'", ",", "help", "=", "'The dev data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--test'", ",", "type", "=", "str", ",", "default", "=", "'test.pkl'", ",", "help", "=", "'The test data directory'", ")", "\n", "\n", "# Model Settings", "\n", "parser", ".", "add_argument", "(", "'--model_name'", ",", "type", "=", "str", ",", "default", "=", "'gpt2'", ",", "help", "=", "'Pretrained model name'", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'Whether to use gpu for training'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "str", ",", "default", "=", "'0'", ",", "help", "=", "'Gpu ids for training'", ")", "\n", "# parser.add_argument('--apex', type=bool, default=False, help='Whether to use half precision')", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "'batch_size for training and evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--shuffle'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'whether to shuffle training data'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "'training iterations'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluation_step'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "\n", "help", "=", "'when training for some steps, start evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'the learning rate of training'", ")", "\n", "parser", ".", "add_argument", "(", "'--set_seed'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'Whether to fix the random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1024", ",", "help", "=", "'fix the random seed for reproducible'", ")", "\n", "parser", ".", "add_argument", "(", "'--patient'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'the patient of early-stopping'", ")", "\n", "parser", ".", "add_argument", "(", "'--length'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "help", "=", "'the max length of generated text'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/output_examples'", ")", "\n", "parser", ".", "add_argument", "(", "'--hyp_only'", ",", "type", "=", "bool", ",", "default", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'--attack_rate'", ",", "type", "=", "float", ",", "default", "=", "0.015", ")", "\n", "\n", "# parsing the hyper-parameters from command line and define logger", "\n", "hps", "=", "parser", ".", "parse_args", "(", ")", "\n", "logger", ",", "formatter", "=", "define_logger", "(", ")", "\n", "# nowtime = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')", "\n", "if", "hps", ".", "hyp_only", ":", "\n", "        ", "log_path", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "log_dir", ",", "'discriminate_'", "+", "hps", ".", "model_name", "+", "'_hyp_only.txt'", ")", "\n", "", "else", ":", "\n", "        ", "log_path", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "log_dir", ",", "'discriminate_'", "+", "hps", ".", "model_name", "+", "'.txt'", ")", "\n", "", "file_handler", "=", "logging", ".", "FileHandler", "(", "log_path", ")", "\n", "file_handler", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "\n", "# fix random seed", "\n", "if", "hps", ".", "set_seed", ":", "\n", "        ", "random", ".", "seed", "(", "hps", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "hps", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "hps", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "hps", ".", "seed", ")", "\n", "\n", "# load data", "\n", "# logger.info(\"[Pytorch] %s\", torch.)", "\n", "", "logger", ".", "info", "(", "\"[INFO] Loading Data\"", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Hypothesis Only:\\t{}\"", ".", "format", "(", "hps", ".", "hyp_only", ")", ")", "\n", "train_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "train", ")", ")", "\n", "dev_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "dev", ")", ")", "\n", "test_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "test", ")", ")", "\n", "\n", "# Tokenization", "\n", "logger", ".", "info", "(", "\"[INFO] Tokenization and Padding for Data\"", ")", "\n", "train_ids", ",", "train_mask", ",", "train_pos", ",", "train_labels", "=", "tokenization", "(", "train_data", ",", "hps", ")", "\n", "dev_ids", ",", "dev_mask", ",", "dev_pos", ",", "dev_labels", "=", "tokenization", "(", "dev_data", ",", "hps", ")", "\n", "test_ids", ",", "test_mask", ",", "test_pos", ",", "test_labels", "=", "tokenization", "(", "test_data", ",", "hps", ")", "\n", "\n", "# Dataset and DataLoader", "\n", "logger", ".", "info", "(", "\"[INFO] Creating Dataset and splitting batch for data\"", ")", "\n", "TRAIN", "=", "TensorDataset", "(", "train_ids", ",", "train_mask", ",", "train_pos", ",", "train_labels", ")", "\n", "DEV", "=", "TensorDataset", "(", "dev_ids", ",", "dev_mask", ",", "dev_pos", ",", "dev_labels", ")", "\n", "TEST", "=", "TensorDataset", "(", "test_ids", ",", "test_mask", ",", "test_pos", ",", "test_labels", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "TRAIN", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "DEV", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "TEST", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "\n", "# initialize model, optimizer, loss_function", "\n", "logger", ".", "info", "(", "'[INFO] Loading pretrained model, setting optimizer and loss function'", ")", "\n", "\n", "model", "=", "gpt2_discriminate", "(", "hps", ")", "\n", "\n", "optimizer", "=", "AdamW", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "lr", "=", "hps", ".", "lr", ")", "\n", "loss_function", "=", "nn", ".", "BCEWithLogitsLoss", "(", "reduction", "=", "'mean'", ")", "\n", "\n", "# Multi-Gpu training", "\n", "if", "hps", ".", "cuda", ":", "\n", "        ", "gpu_ids", "=", "[", "int", "(", "x", ")", "for", "x", "in", "hps", ".", "gpu", ".", "split", "(", "' '", ")", "]", "\n", "model", ".", "cuda", "(", "gpu_ids", "[", "0", "]", ")", "\n", "if", "len", "(", "gpu_ids", ")", ">", "1", ":", "\n", "            ", "model", "=", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "gpu_ids", ")", "\n", "\n", "# training", "\n", "", "", "logger", ".", "info", "(", "\"[INFO] Start Training\"", ")", "\n", "step", "=", "0", "\n", "patient", "=", "0", "\n", "best_accuracy", "=", "0", "\n", "stop_train", "=", "False", "\n", "\n", "for", "epoch", "in", "range", "(", "hps", ".", "epochs", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'[Epoch] {}'", ".", "format", "(", "epoch", ")", ")", "\n", "t", "=", "trange", "(", "len", "(", "train_dataloader", ")", ")", "\n", "epoch_step", "=", "0", "\n", "total_loss", "=", "0", "\n", "for", "i", ",", "batch", "in", "zip", "(", "t", ",", "train_dataloader", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "if", "hps", ".", "cuda", ":", "\n", "                ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "input_ids", ",", "input_mask", ",", "pos", ",", "label", "=", "batch", "\n", "\n", "logits", "=", "model", "(", "input_ids", ",", "attention_mask", "=", "input_mask", ",", "pos", "=", "pos", ")", "\n", "# pdb.set_trace()", "\n", "loss", "=", "loss_function", "(", "logits", ",", "label", ".", "float", "(", ")", ")", "\n", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "t", ".", "set_postfix", "(", "avg_loss", "=", "'{}'", ".", "format", "(", "total_loss", "/", "(", "epoch_step", "+", "1", ")", ")", ")", "\n", "epoch_step", "+=", "1", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "step", "%", "hps", ".", "evaluation_step", "==", "0", "and", "step", "!=", "0", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "\n", "# with torch.no_grad():", "\n", "# print('\\n')", "\n", "logger", ".", "info", "(", "\"[Dev Evaluation] Start Evaluation on Dev Set\"", ")", "\n", "evaluation_output", "=", "evaluate", "(", "hps", ",", "model", ",", "dev_dataloader", ",", "loss_function", ",", "optimizer", ")", "\n", "# print('\\n')", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Accuracy: \\t{}\"", ".", "format", "(", "evaluation_output", "[", "0", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Attack Accuracy: \\t{}\"", ".", "format", "(", "evaluation_output", "[", "2", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Loss: \\t{}\"", ".", "format", "(", "evaluation_output", "[", "1", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Attack Loss: \\t{}\"", ".", "format", "(", "evaluation_output", "[", "3", "]", ")", ")", "\n", "\n", "\n", "if", "evaluation_output", "[", "0", "]", ">=", "best_accuracy", ":", "\n", "                    ", "patient", "=", "0", "\n", "best_accuracy", "=", "evaluation_output", "[", "0", "]", "\n", "logger", ".", "info", "(", "\"[Saving] Saving Model to {}\"", ".", "format", "(", "hps", ".", "save_dir", ")", ")", "\n", "# torch.save(model, os.path.join(hps.save_dir, '{}_{}'.format('generated', hps.model_name)))", "\n", "logger", ".", "info", "(", "\"[Test Evaluation] Start Evaluation on Test Set\"", ")", "\n", "\n", "evaluation_output", "=", "evaluate", "(", "hps", ",", "model", ",", "test_dataloader", ",", "loss_function", ",", "optimizer", ")", "\n", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Accuracy: \\t{}\"", ".", "format", "(", "evaluation_output", "[", "0", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Attack Accuracy: \\t{}\"", ".", "format", "(", "evaluation_output", "[", "2", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Loss: \\t{}\"", ".", "format", "(", "evaluation_output", "[", "1", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Attack Loss: \\t{}\"", ".", "format", "(", "evaluation_output", "[", "3", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "patient", "+=", "1", "\n", "\n", "", "logger", ".", "info", "(", "\"[Patient] {}\"", ".", "format", "(", "patient", ")", ")", "\n", "\n", "if", "patient", ">=", "hps", ".", "patient", ":", "\n", "                    ", "logger", ".", "info", "(", "\"[INFO] Stopping Training by Early Stopping\"", ")", "\n", "stop_train", "=", "True", "\n", "break", "\n", "", "", "step", "+=", "1", "\n", "\n", "", "if", "stop_train", ":", "\n", "            ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.code.train_discriminate.main": [[17, 193], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "utils.utils.define_logger", "datetime.datetime.now().strftime", "logging.FileHandler", "logging.FileHandler.setFormatter", "logger.addHandler", "logger.info", "logger.info", "logger.info", "utils.utils.load_data", "utils.utils.load_data", "utils.utils.load_data", "logger.info", "utils.utils.quick_tokenize", "utils.utils.quick_tokenize", "utils.utils.quick_tokenize", "logger.info", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "logger.info", "model.discriminate_model.pretrained_model", "transformers.AdamW", "logger.info", "range", "os.path.join", "os.path.join", "random.seed", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed", "torch.cuda.manual_seed", "os.path.join", "os.path.join", "os.path.join", "filter", "torch.CrossEntropyLoss", "torch.BCEWithLogitsLoss", "nn.DataParallel.cuda", "logger.info", "tqdm.trange", "zip", "datetime.datetime.now", "nn.DataParallel.parameters", "int", "len", "torch.DataParallel", "len", "transformers.AdamW.zero_grad", "nn.DataParallel.train", "nn.DataParallel.", "loss_function.item", "tqdm.trange.set_postfix", "loss_function.backward", "transformers.AdamW.step", "parser.parse_args.gpu.split", "tuple", "nn.BCEWithLogitsLoss.", "nn.BCEWithLogitsLoss.", "nn.DataParallel.eval", "model.squeeze", "labels.float", "torch.no_grad", "torch.no_grad", "print", "logger.info", "logger.info", "logger.info", "term.cuda", "utils.utils.evaluation", "print", "logger.info", "logger.info", "utils.utils.evaluation", "print", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "torch.save", "torch.save", "torch.save", "torch.save", "utils.utils.evaluation", "print", "logger.info", "logger.info", "utils.utils.evaluation", "print", "logger.info", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.define_logger", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.quick_tokenize", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.quick_tokenize", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.quick_tokenize", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.evaluation", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.evaluation", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.evaluation", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.evaluation"], ["def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'xCAR'", ")", "\n", "\n", "# Data Paths", "\n", "parser", ".", "add_argument", "(", "'--data_dir'", ",", "type", "=", "str", ",", "default", "=", "'./data/final_data/data/'", ",", "help", "=", "'The dataset directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--model_dir'", ",", "type", "=", "str", ",", "default", "=", "'../../huggingface_transformers/xlnet-base-cased/'", ",", "\n", "help", "=", "'The pretrained model directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/saved_model'", ",", "help", "=", "'The model saving directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--log_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/log'", ",", "help", "=", "'The training log directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--apex_dir'", ",", "type", "=", "str", ",", "default", "=", "'./output/log'", ",", "help", "=", "'The apex directory'", ")", "\n", "\n", "# Data names", "\n", "parser", ".", "add_argument", "(", "'--train'", ",", "type", "=", "str", ",", "default", "=", "'train.pkl'", ",", "help", "=", "'The train data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--dev'", ",", "type", "=", "str", ",", "default", "=", "'dev.pkl'", ",", "help", "=", "'The dev data directory'", ")", "\n", "parser", ".", "add_argument", "(", "'--test'", ",", "type", "=", "str", ",", "default", "=", "'test.pkl'", ",", "help", "=", "'The test data directory'", ")", "\n", "\n", "# Model Settings", "\n", "parser", ".", "add_argument", "(", "'--model_name'", ",", "type", "=", "str", ",", "default", "=", "'xlnet'", ",", "help", "=", "'Pretrained model name'", ")", "\n", "parser", ".", "add_argument", "(", "'--data_name'", ",", "type", "=", "str", ",", "default", "=", "'copa'", ")", "\n", "parser", ".", "add_argument", "(", "'--cuda'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'Whether to use gpu for training'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu'", ",", "type", "=", "str", ",", "default", "=", "'0'", ",", "help", "=", "'Gpu ids for training'", ")", "\n", "# parser.add_argument('--apex', type=bool, default=False, help='Whether to use half precision')", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'batch_size for training and evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--shuffle'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "'whether to shuffle training data'", ")", "\n", "parser", ".", "add_argument", "(", "'--epochs'", ",", "type", "=", "int", ",", "default", "=", "200", ",", "help", "=", "'training iterations'", ")", "\n", "parser", ".", "add_argument", "(", "'--evaluation_step'", ",", "type", "=", "int", ",", "default", "=", "20", ",", "\n", "help", "=", "'when training for some steps, start evaluation'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "1e-5", ",", "help", "=", "'the learning rate of training'", ")", "\n", "parser", ".", "add_argument", "(", "'--set_seed'", ",", "type", "=", "bool", ",", "default", "=", "True", ",", "help", "=", "'Whether to fix the random seed'", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1024", ",", "help", "=", "'fix the random seed for reproducible'", ")", "\n", "parser", ".", "add_argument", "(", "'--patient'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'the patient of early-stopping'", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_func'", ",", "type", "=", "str", ",", "default", "=", "'BCE'", ",", "help", "=", "\"loss function of output\"", ")", "\n", "parser", ".", "add_argument", "(", "'--hyp_only'", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "\"If set True, Only send hypothesis into model\"", ")", "\n", "# parser.add_argument('--warmup_proportion', type=float, default=0.1, help='warmup settings')", "\n", "\n", "# parsing the hyper-parameters from command line and define logger", "\n", "hps", "=", "parser", ".", "parse_args", "(", ")", "\n", "logger", ",", "formatter", "=", "define_logger", "(", ")", "\n", "nowtime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "if", "hps", ".", "hyp_only", ":", "\n", "        ", "log_path", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "log_dir", ",", "'discriminate_'", "+", "hps", ".", "model_name", "+", "'_hyp'", "+", "'_{}.txt'", ".", "format", "(", "nowtime", ")", ")", "\n", "", "else", ":", "\n", "        ", "log_path", "=", "os", ".", "path", ".", "join", "(", "hps", ".", "log_dir", ",", "'discriminate_'", "+", "hps", ".", "model_name", "+", "'_{}.txt'", ".", "format", "(", "nowtime", ")", ")", "\n", "", "file_handler", "=", "logging", ".", "FileHandler", "(", "log_path", ")", "\n", "file_handler", ".", "setFormatter", "(", "formatter", ")", "\n", "logger", ".", "addHandler", "(", "file_handler", ")", "\n", "\n", "# fix random seed", "\n", "if", "hps", ".", "set_seed", ":", "\n", "        ", "random", ".", "seed", "(", "hps", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "hps", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "hps", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "hps", ".", "seed", ")", "\n", "\n", "# load data", "\n", "# logger.info(\"[Pytorch] %s\", torch.)", "\n", "", "logger", ".", "info", "(", "\"[MODEL] {}\"", ".", "format", "(", "hps", ".", "model_name", ")", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Loading Data\"", ")", "\n", "logger", ".", "info", "(", "\"[INFO] Hypothesis Only: {}\"", ".", "format", "(", "hps", ".", "hyp_only", ")", ")", "\n", "train_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "train", ")", ")", "\n", "dev_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "dev", ")", ")", "\n", "test_data", "=", "load_data", "(", "os", ".", "path", ".", "join", "(", "hps", ".", "data_dir", ",", "hps", ".", "test", ")", ")", "\n", "\n", "# Tokenization", "\n", "logger", ".", "info", "(", "\"[INFO] Tokenization and Padding for Data\"", ")", "\n", "train_ids", ",", "train_mask", ",", "train_seg_ids", ",", "train_labels", ",", "train_length", "=", "quick_tokenize", "(", "train_data", ",", "hps", ")", "\n", "dev_ids", ",", "dev_mask", ",", "dev_seg_ids", ",", "dev_labels", ",", "dev_length", "=", "quick_tokenize", "(", "dev_data", ",", "hps", ")", "\n", "test_ids", ",", "test_mask", ",", "test_seg_ids", ",", "test_labels", ",", "test_length", "=", "quick_tokenize", "(", "test_data", ",", "hps", ")", "\n", "\n", "# Dataset and DataLoader", "\n", "logger", ".", "info", "(", "\"[INFO] Creating Dataset and splitting batch for data\"", ")", "\n", "TRAIN", "=", "TensorDataset", "(", "train_ids", ",", "train_seg_ids", ",", "train_mask", ",", "train_labels", ",", "train_length", ")", "\n", "DEV", "=", "TensorDataset", "(", "dev_ids", ",", "dev_seg_ids", ",", "dev_mask", ",", "dev_labels", ",", "dev_length", ")", "\n", "TEST", "=", "TensorDataset", "(", "test_ids", ",", "test_seg_ids", ",", "test_mask", ",", "test_labels", ",", "test_length", ")", "\n", "train_dataloader", "=", "DataLoader", "(", "TRAIN", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "DEV", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "TEST", ",", "batch_size", "=", "hps", ".", "batch_size", ",", "shuffle", "=", "hps", ".", "shuffle", ",", "drop_last", "=", "False", ")", "\n", "\n", "# initialize model, optimizer, loss_function", "\n", "logger", ".", "info", "(", "'[INFO] Loading pretrained model, setting optimizer and loss function'", ")", "\n", "model", "=", "pretrained_model", "(", "hps", ")", "\n", "optimizer", "=", "AdamW", "(", "filter", "(", "lambda", "p", ":", "p", ".", "requires_grad", ",", "model", ".", "parameters", "(", ")", ")", ",", "lr", "=", "hps", ".", "lr", ")", "\n", "if", "hps", ".", "loss_func", "==", "\"CrossEntropy\"", ":", "\n", "        ", "loss_function", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'mean'", ")", "\n", "", "else", ":", "\n", "        ", "loss_function", "=", "nn", ".", "BCEWithLogitsLoss", "(", "reduction", "=", "'mean'", ")", "\n", "\n", "# Multi-Gpu training", "\n", "", "if", "hps", ".", "cuda", ":", "\n", "        ", "gpu_ids", "=", "[", "int", "(", "x", ")", "for", "x", "in", "hps", ".", "gpu", ".", "split", "(", "' '", ")", "]", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "if", "len", "(", "gpu_ids", ")", ">", "1", ":", "\n", "            ", "model", "=", "nn", ".", "DataParallel", "(", "model", ",", "device_ids", "=", "gpu_ids", ")", "\n", "\n", "# training", "\n", "", "", "logger", ".", "info", "(", "\"[INFO] Start Training\"", ")", "\n", "step", "=", "0", "\n", "patient", "=", "0", "\n", "best_accuracy", "=", "0", "\n", "stop_train", "=", "False", "\n", "\n", "for", "epoch", "in", "range", "(", "hps", ".", "epochs", ")", ":", "\n", "        ", "logger", ".", "info", "(", "'[Epoch] {}'", ".", "format", "(", "epoch", ")", ")", "\n", "t", "=", "trange", "(", "len", "(", "train_dataloader", ")", ")", "\n", "epoch_step", "=", "0", "\n", "total_loss", "=", "0", "\n", "for", "i", ",", "batch", "in", "zip", "(", "t", ",", "train_dataloader", ")", ":", "\n", "            ", "optimizer", ".", "zero_grad", "(", ")", "\n", "model", ".", "train", "(", ")", "\n", "if", "hps", ".", "cuda", ":", "\n", "                ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "sent", ",", "seg_id", ",", "atten_mask", ",", "labels", ",", "length", "=", "batch", "\n", "probs", "=", "model", "(", "sent", ",", "atten_mask", ",", "seg_ids", "=", "seg_id", ",", "length", "=", "length", ")", "\n", "\n", "if", "hps", ".", "loss_func", "==", "'CrossEntropy'", ":", "\n", "                ", "loss", "=", "loss_function", "(", "probs", ",", "labels", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "loss_function", "(", "probs", ".", "squeeze", "(", "1", ")", ",", "labels", ".", "float", "(", ")", ")", "\n", "\n", "", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "t", ".", "set_postfix", "(", "avg_loss", "=", "'{}'", ".", "format", "(", "total_loss", "/", "(", "epoch_step", "+", "1", ")", ")", ")", "\n", "epoch_step", "+=", "1", "\n", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "\n", "if", "step", "%", "hps", ".", "evaluation_step", "==", "0", "and", "step", "!=", "0", ":", "\n", "                ", "model", ".", "eval", "(", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                    ", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Dev Evaluation] Strain Evaluation on Dev Set\"", ")", "\n", "if", "hps", ".", "loss_func", "==", "'CrossEntropy'", ":", "\n", "                        ", "dev_accu", ",", "dev_exact_accu", ",", "dev_loss", "=", "evaluation", "(", "hps", ",", "dev_dataloader", ",", "model", ",", "loss_function", ")", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Soft Accuracy: \\t{}\"", ".", "format", "(", "dev_accu", ")", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Exact Accuracy: \\t{}\"", ".", "format", "(", "dev_exact_accu", ")", ")", "\n", "", "else", ":", "\n", "                        ", "dev_accu", ",", "dev_loss", "=", "evaluation", "(", "hps", ",", "dev_dataloader", ",", "model", ",", "loss_function", ")", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Accuracy: \\t{}\"", ".", "format", "(", "dev_accu", ")", ")", "\n", "", "logger", ".", "info", "(", "\"[Dev Metrics] Dev Loss: \\t{}\"", ".", "format", "(", "dev_loss", ")", ")", "\n", "\n", "if", "dev_accu", ">=", "best_accuracy", ":", "\n", "                        ", "patient", "=", "0", "\n", "best_accuracy", "=", "dev_accu", "\n", "logger", ".", "info", "(", "\"[Saving] Saving Model to {}\"", ".", "format", "(", "hps", ".", "save_dir", ")", ")", "\n", "if", "hps", ".", "hyp_only", ":", "\n", "                            ", "torch", ".", "save", "(", "model", ",", "os", ".", "path", ".", "join", "(", "hps", ".", "save_dir", ",", "'discriminate_'", "+", "hps", ".", "model_name", "+", "'_hyp'", ")", ")", "\n", "", "else", ":", "\n", "                            ", "torch", ".", "save", "(", "model", ",", "os", ".", "path", ".", "join", "(", "hps", ".", "save_dir", ",", "'discriminate_'", "+", "hps", ".", "model_name", ")", ")", "\n", "", "logger", ".", "info", "(", "\"[Test Evaluation] Strain Evaluation on Test Set\"", ")", "\n", "if", "hps", ".", "loss_func", "==", "'CrossEntropy'", ":", "\n", "                            ", "te_soft_accu", ",", "te_exact_accu", ",", "te_loss", "=", "evaluation", "(", "hps", ",", "test_dataloader", ",", "model", ",", "loss_function", ")", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Soft Accuracy: \\t{}\"", ".", "format", "(", "te_soft_accu", ")", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Exact Accuracy: \\t{}\"", ".", "format", "(", "te_exact_accu", ")", ")", "\n", "", "else", ":", "\n", "                            ", "te_accu", ",", "te_loss", "=", "evaluation", "(", "hps", ",", "test_dataloader", ",", "model", ",", "loss_function", ")", "\n", "print", "(", "'\\n'", ")", "\n", "logger", ".", "info", "(", "\"[Test Metrics] Test Accuracy: \\t{}\"", ".", "format", "(", "te_accu", ")", ")", "\n", "", "logger", ".", "info", "(", "\"[Test Metrics] Test Loss: \\t{}\"", ".", "format", "(", "te_loss", ")", ")", "\n", "", "else", ":", "\n", "                        ", "patient", "+=", "1", "\n", "\n", "", "logger", ".", "info", "(", "\"[Patient] {}\"", ".", "format", "(", "patient", ")", ")", "\n", "\n", "if", "patient", ">=", "hps", ".", "patient", ":", "\n", "                        ", "logger", ".", "info", "(", "\"[INFO] Stopping Training by Early Stopping\"", ")", "\n", "stop_train", "=", "True", "\n", "break", "\n", "", "", "", "step", "+=", "1", "\n", "\n", "", "if", "stop_train", ":", "\n", "            ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.tokenize_data": [[21, 66], ["RobertaTokenizer.from_pretrained._convert_token_to_id", "RobertaTokenizer.from_pretrained._convert_token_to_id", "RobertaTokenizer.from_pretrained._convert_token_to_id", "transformers.BertTokenizer.from_pretrained", "RobertaTokenizer.from_pretrained.convert_tokens_to_ids", "RobertaTokenizer.from_pretrained.convert_tokens_to_ids", "RobertaTokenizer.from_pretrained.convert_tokens_to_ids", "max", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "transformers.RobertaTokenizer.from_pretrained", "RobertaTokenizer.from_pretrained._tokenize", "RobertaTokenizer.from_pretrained._tokenize", "RobertaTokenizer.from_pretrained._tokenize", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "function", ["None"], ["def", "tokenize_data", "(", "data", ",", "model_path", ",", "model_name", ")", ":", "\n", "# tokenizer = BertTokenizer(vocab_file=model_path+'/'+'vocab.txt')", "\n", "    ", "if", "model_name", "==", "'bert'", ":", "\n", "        ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "model_path", ")", "\n", "", "elif", "model_name", "==", "'roberta'", ":", "\n", "        ", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "model_path", ")", "\n", "\n", "# unique ids", "\n", "", "cls_id", "=", "tokenizer", ".", "_convert_token_to_id", "(", "'[CLS]'", ")", "\n", "sep_id", "=", "tokenizer", ".", "_convert_token_to_id", "(", "'[SEP]'", ")", "\n", "pad_id", "=", "tokenizer", ".", "_convert_token_to_id", "(", "'[PAD]'", ")", "\n", "\n", "labels", "=", "[", "]", "\n", "instances", "=", "[", "]", "\n", "segments", "=", "[", "]", "\n", "\n", "max_length", "=", "0", "\n", "\n", "# tokenization", "\n", "for", "example", "in", "data", ":", "\n", "        ", "premise", ",", "a1", ",", "a2", "=", "example", "[", "'premise'", "]", ",", "example", "[", "'alternative1'", "]", ",", "example", "[", "'alternative2'", "]", "\n", "premise_id", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "_tokenize", "(", "premise", ")", ")", "\n", "a1_id", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "_tokenize", "(", "a1", ")", ")", "\n", "a2_id", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokenizer", ".", "_tokenize", "(", "a2", ")", ")", "\n", "max_length", "=", "max", "(", "max_length", ",", "len", "(", "premise_id", "+", "a1_id", ")", "+", "3", ",", "len", "(", "premise_id", "+", "a2_id", ")", "+", "3", ")", "\n", "if", "example", "[", "'ask-for'", "]", "==", "'cause'", ":", "\n", "            ", "instance1", "=", "[", "cls_id", "]", "+", "a1_id", "+", "[", "sep_id", "]", "+", "premise_id", "+", "[", "sep_id", "]", "\n", "seg1", "=", "[", "0", "]", "*", "(", "len", "(", "a1_id", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "premise_id", ")", "+", "1", ")", "\n", "instance2", "=", "[", "cls_id", "]", "+", "a2_id", "+", "[", "sep_id", "]", "+", "premise_id", "+", "[", "sep_id", "]", "\n", "seg2", "=", "[", "0", "]", "*", "(", "len", "(", "a2_id", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "premise_id", ")", "+", "1", ")", "\n", "", "else", ":", "\n", "            ", "instance1", "=", "[", "cls_id", "]", "+", "premise_id", "+", "[", "sep_id", "]", "+", "a1_id", "+", "[", "sep_id", "]", "\n", "seg1", "=", "[", "0", "]", "*", "(", "len", "(", "premise_id", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "a1_id", ")", "+", "1", ")", "\n", "instance2", "=", "[", "cls_id", "]", "+", "premise_id", "+", "[", "sep_id", "]", "+", "a2_id", "+", "[", "sep_id", "]", "\n", "seg2", "=", "[", "0", "]", "*", "(", "len", "(", "premise_id", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "a2_id", ")", "+", "1", ")", "\n", "", "instances", "+=", "[", "instance1", ",", "instance2", "]", "\n", "segments", "+=", "[", "seg1", ",", "seg2", "]", "\n", "labels", "+=", "[", "0", ",", "1", "]", "if", "example", "[", "'label'", "]", "==", "1", "else", "[", "1", ",", "0", "]", "\n", "\n", "# padding", "\n", "", "segments", "=", "[", "seg", "+", "[", "0", "]", "*", "(", "max_length", "-", "len", "(", "seg", ")", ")", "for", "seg", "in", "segments", "]", "\n", "attention_mask", "=", "[", "[", "1", "]", "*", "len", "(", "instance", ")", "+", "[", "0", "]", "*", "(", "max_length", "-", "len", "(", "instance", ")", ")", "for", "instance", "in", "instances", "]", "\n", "instances", "=", "[", "instance", "+", "[", "pad_id", "]", "*", "(", "max_length", "-", "len", "(", "instance", ")", ")", "for", "instance", "in", "instances", "]", "\n", "\n", "return", "torch", ".", "LongTensor", "(", "instances", ")", ",", "torch", ".", "LongTensor", "(", "attention_mask", ")", ",", "torch", ".", "LongTensor", "(", "segments", ")", ",", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.tokenize_multi_choices": [[68, 114], ["XLNetTokenizer.from_pretrained.", "transformers.BertTokenizer.from_pretrained", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "transformers.RobertaTokenizer.from_pretrained", "labels.append", "instances.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "transformers.AlbertTokenizer.from_pretrained", "transformers.OpenAIGPTTokenizer.from_pretrained", "transformers.GPT2Tokenizer.from_pretrained", "range", "transformers.BartTokenizer.from_pretrained", "transformers.XLNetTokenizer.from_pretrained", "len"], "function", ["None"], ["", "def", "tokenize_multi_choices", "(", "data", ",", "hps", ")", ":", "\n", "# load pretrained tokenizer", "\n", "    ", "if", "hps", ".", "model_name", "==", "'bert'", ":", "\n", "        ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'roberta'", ":", "\n", "        ", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'albert'", ":", "\n", "        ", "tokenizer", "=", "AlbertTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'gpt'", ":", "\n", "        ", "tokenizer", "=", "OpenAIGPTTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ",", "unk_token", "=", "\"<unk>\"", ")", "\n", "tokenizer", ".", "pad_token", "=", "tokenizer", ".", "unk_token", "\n", "", "elif", "hps", ".", "model_name", "==", "'gpt2'", ":", "\n", "        ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "tokenizer", ".", "pad_token", "=", "tokenizer", ".", "unk_token", "\n", "", "elif", "hps", ".", "model_name", "==", "'bart'", ":", "\n", "        ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "else", ":", "\n", "        ", "tokenizer", "=", "XLNetTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "\n", "", "instances", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "\n", "# pdb.set_trace()", "\n", "for", "example", "in", "data", ":", "\n", "        ", "if", "hps", ".", "data_name", "==", "'because'", "or", "hps", ".", "data_name", "==", "'event_storyline'", ":", "\n", "            ", "premise", ",", "hypothesis", "=", "example", "[", "'premise'", "]", ",", "example", "[", "'hypothesis'", "]", "\n", "instance", "=", "[", "premise", ",", "hypothesis", "]", "\n", "labels", ".", "append", "(", "example", "[", "'label'", "]", ")", "\n", "instances", ".", "append", "(", "instance", ")", "\n", "", "elif", "hps", ".", "data_name", "==", "'commonsenseqa'", ":", "\n", "            ", "premise", ",", "alternatives", "=", "example", "[", "'premise'", "]", ",", "example", "[", "'alternatives'", "]", "\n", "label", "=", "example", "[", "'label'", "]", "\n", "tmp_instances", "=", "[", "[", "premise", ",", "alternative", "]", "for", "alternative", "in", "alternatives", "]", "\n", "tmp_labels", "=", "[", "0", "for", "_", "in", "range", "(", "len", "(", "alternatives", ")", ")", "]", "\n", "tmp_labels", "[", "label", "]", "=", "1", "\n", "labels", "+=", "tmp_labels", "\n", "instances", "+=", "tmp_instances", "\n", "\n", "", "", "outputs", "=", "tokenizer", "(", "instances", ",", "padding", "=", "True", ",", "return_token_type_ids", "=", "True", ",", "return_length", "=", "True", ")", "\n", "input_ids", "=", "outputs", "[", "'input_ids'", "]", "\n", "attention_mask", "=", "outputs", "[", "'attention_mask'", "]", "\n", "token_type_ids", "=", "outputs", "[", "'token_type_ids'", "]", "\n", "length", "=", "outputs", "[", "'length'", "]", "\n", "\n", "return", "torch", ".", "LongTensor", "(", "input_ids", ")", ",", "torch", ".", "LongTensor", "(", "attention_mask", ")", ",", "torch", ".", "LongTensor", "(", "token_type_ids", ")", ",", "torch", ".", "LongTensor", "(", "labels", ")", ",", "torch", ".", "LongTensor", "(", "length", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.quick_tokenize": [[116, 165], ["XLNetTokenizer.from_pretrained.", "transformers.BertTokenizer.from_pretrained", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "transformers.RobertaTokenizer.from_pretrained", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "transformers.AlbertTokenizer.from_pretrained", "transformers.OpenAIGPTTokenizer.from_pretrained", "transformers.GPT2Tokenizer.from_pretrained", "transformers.BartTokenizer.from_pretrained", "transformers.XLNetTokenizer.from_pretrained"], "function", ["None"], ["", "def", "quick_tokenize", "(", "data", ",", "hps", ")", ":", "\n", "# load pretrained tokenizer", "\n", "    ", "if", "hps", ".", "model_name", "==", "'bert'", ":", "\n", "        ", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'roberta'", ":", "\n", "        ", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'albert'", ":", "\n", "        ", "tokenizer", "=", "AlbertTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'gpt'", ":", "\n", "        ", "tokenizer", "=", "OpenAIGPTTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ",", "unk_token", "=", "\"<unk>\"", ")", "\n", "tokenizer", ".", "pad_token", "=", "tokenizer", ".", "unk_token", "\n", "", "elif", "hps", ".", "model_name", "==", "'gpt2'", ":", "\n", "        ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "tokenizer", ".", "pad_token", "=", "tokenizer", ".", "unk_token", "\n", "", "elif", "hps", ".", "model_name", "==", "'bart'", ":", "\n", "        ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "else", ":", "\n", "        ", "tokenizer", "=", "XLNetTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "\n", "", "instances", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "for", "example", "in", "data", ":", "\n", "        ", "premise", ",", "a1", ",", "a2", "=", "example", "[", "'premise'", "]", ",", "example", "[", "'alternative1'", "]", ",", "example", "[", "'alternative2'", "]", "\n", "\n", "if", "example", "[", "'ask-for'", "]", "==", "'cause'", ":", "\n", "            ", "if", "not", "hps", ".", "hyp_only", ":", "\n", "                ", "instance1", "=", "[", "a1", ",", "premise", "]", "\n", "instance2", "=", "[", "a2", ",", "premise", "]", "\n", "", "else", ":", "\n", "                ", "instance1", "=", "a1", "\n", "instance2", "=", "a2", "\n", "", "", "else", ":", "\n", "            ", "if", "not", "hps", ".", "hyp_only", ":", "\n", "                ", "instance1", "=", "[", "premise", ",", "a1", "]", "\n", "instance2", "=", "[", "premise", ",", "a2", "]", "\n", "", "else", ":", "\n", "                ", "instance1", "=", "a1", "\n", "instance2", "=", "a2", "\n", "", "", "labels", "+=", "[", "0", ",", "1", "]", "if", "example", "[", "'label'", "]", "==", "1", "else", "[", "1", ",", "0", "]", "\n", "instances", "+=", "[", "instance1", ",", "instance2", "]", "\n", "\n", "", "outputs", "=", "tokenizer", "(", "instances", ",", "padding", "=", "True", ",", "return_token_type_ids", "=", "True", ",", "return_length", "=", "True", ")", "\n", "input_ids", "=", "outputs", "[", "'input_ids'", "]", "\n", "attention_mask", "=", "outputs", "[", "'attention_mask'", "]", "\n", "token_type_ids", "=", "outputs", "[", "'token_type_ids'", "]", "\n", "length", "=", "outputs", "[", "'length'", "]", "\n", "\n", "return", "torch", ".", "LongTensor", "(", "input_ids", ")", ",", "torch", ".", "LongTensor", "(", "attention_mask", ")", ",", "torch", ".", "LongTensor", "(", "token_type_ids", ")", ",", "torch", ".", "LongTensor", "(", "labels", ")", ",", "torch", ".", "LongTensor", "(", "length", ")", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.tokenize_multi_task": [[167, 197], ["transformers.RobertaTokenizer.from_pretrained", "RobertaTokenizer.from_pretrained.", "RobertaTokenizer.from_pretrained.", "RobertaTokenizer.from_pretrained.", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "truths.append", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "instances1.append", "instances2.append", "instances1.append", "instances2.append"], "function", ["None"], ["", "def", "tokenize_multi_task", "(", "hps", ",", "data", ")", ":", "\n", "    ", "tokenizer", "=", "RobertaTokenizer", ".", "from_pretrained", "(", "hps", ".", "discriminate_model_dir", ")", "\n", "instances1", "=", "[", "]", "\n", "instances2", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "truths", "=", "[", "]", "\n", "\n", "for", "example", "in", "data", ":", "\n", "        ", "truth", ",", "premise", ",", "a1", ",", "a2", "=", "example", "[", "'general_truth'", "]", ",", "example", "[", "'premise'", "]", ",", "example", "[", "'alternative1'", "]", ",", "example", "[", "\n", "'alternative2'", "]", "\n", "truths", ".", "append", "(", "truth", ")", "\n", "if", "example", "[", "'ask-for'", "]", "==", "'cause'", ":", "\n", "            ", "instances1", ".", "append", "(", "[", "a1", ",", "premise", "]", ")", "\n", "instances2", ".", "append", "(", "[", "a2", ",", "premise", "]", ")", "\n", "", "else", ":", "\n", "            ", "instances1", ".", "append", "(", "[", "premise", ",", "a1", "]", ")", "\n", "instances2", ".", "append", "(", "[", "premise", ",", "a2", "]", ")", "\n", "", "labels", "+=", "[", "example", "[", "'label'", "]", "]", "\n", "\n", "", "outputs1", "=", "tokenizer", "(", "instances1", ",", "padding", "=", "True", ")", "\n", "outputs2", "=", "tokenizer", "(", "instances2", ",", "padding", "=", "True", ")", "\n", "outputs_truth", "=", "tokenizer", "(", "truths", ",", "padding", "=", "True", ")", "\n", "input_ids1", "=", "torch", ".", "LongTensor", "(", "outputs1", "[", "'input_ids'", "]", ")", "\n", "input_ids2", "=", "torch", ".", "LongTensor", "(", "outputs2", "[", "'input_ids'", "]", ")", "\n", "truth_ids", "=", "torch", ".", "LongTensor", "(", "outputs_truth", "[", "'input_ids'", "]", ")", "\n", "mask1", "=", "torch", ".", "LongTensor", "(", "outputs1", "[", "'attention_mask'", "]", ")", "\n", "mask2", "=", "torch", ".", "LongTensor", "(", "outputs2", "[", "'attention_mask'", "]", ")", "\n", "mask_truth", "=", "torch", ".", "LongTensor", "(", "outputs_truth", "[", "'attention_mask'", "]", ")", "\n", "\n", "return", "input_ids1", ",", "input_ids2", ",", "truth_ids", "[", ":", ",", "1", ":", "]", ",", "mask1", ",", "mask2", ",", "mask_truth", "[", ":", ",", "1", ":", "]", ",", "torch", ".", "LongTensor", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.compute_ppl": [[199, 251], ["torch.exp.item", "transformers.GPT2Tokenizer.from_pretrained", "torch.exp", "torch.exp", "torch.exp", "transformers.BartTokenizer.from_pretrained", "torch.exp", "torch.exp", "torch.exp", "BartTokenizer.from_pretrained.", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "BartTokenizer.from_pretrained.", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "BartTokenizer.from_pretrained.", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "BartTokenizer.from_pretrained.", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.no_grad", "torch.no_grad", "torch.no_grad", "lls.append", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.no_grad", "torch.no_grad", "torch.no_grad", "lls.append", "torch.stack().sum", "torch.stack().sum", "torch.stack().sum", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "torch.LongTensor().unsqueeze().cuda", "model", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "model", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor().unsqueeze", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.ones", "torch.ones", "torch.ones", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor"], "function", ["None"], ["", "def", "compute_ppl", "(", "hps", ",", "model", ",", "data", ")", ":", "\n", "# device = 'cuda'", "\n", "    ", "if", "hps", ".", "model_name", "==", "'gpt2'", ":", "\n", "        ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "lls", "=", "[", "]", "\n", "total_length", "=", "0", "\n", "for", "example", "in", "data", ":", "\n", "            ", "input_text", "=", "example", "[", "'cause'", "]", "+", "' '", "+", "example", "[", "'effect'", "]", "\n", "truth", "=", "example", "[", "'general_truth'", "]", "\n", "inputs", "=", "tokenizer", "(", "input_text", ")", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "inputs", "[", "'input_ids'", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "attention_mask", "=", "torch", ".", "LongTensor", "(", "inputs", "[", "'attention_mask'", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "label_inputs", "=", "tokenizer", "(", "truth", ")", "\n", "label_ids", "=", "torch", ".", "LongTensor", "(", "label_inputs", "[", "'input_ids'", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "length", "=", "label_ids", ".", "shape", "[", "1", "]", "\n", "total_length", "+=", "length", "\n", "\n", "# label_mask = torch.LongTensor(label_inputs['attention_mask']).unsqueeze(0).cuda()", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "attention_mask", ",", "torch", ".", "ones", "(", "1", ",", "label_ids", ".", "shape", "[", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ",", "1", ")", "\n", "label_ids", "=", "torch", ".", "cat", "(", "(", "torch", ".", "LongTensor", "(", "[", "-", "100", "]", "*", "input_ids", ".", "shape", "[", "1", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", ",", "label_ids", ")", ",", "1", ")", "\n", "input_ids", "=", "torch", ".", "cat", "(", "(", "input_ids", ",", "label_ids", "[", ":", ",", "input_ids", ".", "shape", "[", "1", "]", ":", "]", ")", ",", "1", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "loss", "=", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "labels", "=", "label_ids", ")", "[", "0", "]", "\n", "lls", ".", "append", "(", "loss", "*", "length", ")", "\n", "\n", "", "", "ppl", "=", "torch", ".", "exp", "(", "torch", ".", "stack", "(", "lls", ")", ".", "sum", "(", ")", "/", "total_length", ")", "\n", "\n", "", "else", ":", "\n", "        ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "lls", "=", "[", "]", "\n", "total_length", "=", "0", "\n", "for", "example", "in", "data", ":", "\n", "            ", "input_text", "=", "example", "[", "'cause'", "]", "+", "' '", "+", "example", "[", "'effect'", "]", "\n", "truth", "=", "example", "[", "'general_truth'", "]", "\n", "inputs", "=", "tokenizer", "(", "input_text", ")", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "inputs", "[", "'input_ids'", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "attention_mask", "=", "torch", ".", "LongTensor", "(", "inputs", "[", "'attention_mask'", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "label_inputs", "=", "tokenizer", "(", "truth", ")", "\n", "label_ids", "=", "torch", ".", "LongTensor", "(", "label_inputs", "[", "'input_ids'", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "length", "=", "label_ids", ".", "shape", "[", "1", "]", "\n", "total_length", "+=", "length", "\n", "label_mask", "=", "torch", ".", "LongTensor", "(", "label_inputs", "[", "'attention_mask'", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "cuda", "(", ")", "\n", "# attention_mask = torch.cat((attention_mask, torch.ones(1, label_ids.shape[1]).long().cuda()), 1)", "\n", "# label_ids = torch.cat((torch.LongTensor([-100]*input_ids.shape[1]).unsqueeze(0).cuda(), label_ids), 1)", "\n", "# input_ids = torch.cat((input_ids, label_ids[:, input_ids.shape[1]:]), 1)", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "loss", "=", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "decoder_input_ids", "=", "label_ids", ",", "decoder_attention_mask", "=", "label_mask", ",", "labels", "=", "label_ids", ")", "[", "0", "]", "\n", "lls", ".", "append", "(", "loss", "*", "length", ")", "\n", "\n", "", "", "ppl", "=", "torch", ".", "exp", "(", "torch", ".", "stack", "(", "lls", ")", ".", "sum", "(", ")", "/", "total_length", ")", "\n", "\n", "", "return", "ppl", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.evaluate_multi_task": [[253, 304], ["transformers.BartTokenizer.from_pretrained", "zip", "tqdm.trange", "model", "torch.cat", "torch.cat", "torch.cat", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax.cpu().tolist", "torch.cat", "torch.cat", "torch.cat", "torch.argmax().cpu().tolist", "torch.argmax().cpu().tolist", "torch.argmax().cpu().tolist", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model", "range", "len", "tuple", "tuple", "len", "BartTokenizer.from_pretrained.decode", "BartTokenizer.from_pretrained.decode", "len", "nltk.bleu", "nltk.bleu", "nltk.bleu", "nltk.bleu", "scores[].unsqueeze", "scores[].unsqueeze", "torch.argmax.cpu", "labels[].unsqueeze", "labels[].unsqueeze", "torch.argmax().cpu", "torch.argmax().cpu", "torch.argmax().cpu", "input_ids[].unsqueeze", "input_ids[].unsqueeze", "attention_mask[].unsqueeze", "attention_mask[].unsqueeze", "model.tolist", "decoder_ids[].tolist", "term.cuda", "term.cuda", "range", "range", "len", "torch.argmax", "torch.argmax", "torch.argmax"], "function", ["None"], ["", "def", "evaluate_multi_task", "(", "model", ",", "dataloader_input", ",", "dataloader_output", ",", "hps", ")", ":", "\n", "    ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "bleu1", ",", "bleu2", ",", "bleu3", ",", "bleu4", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "count", "=", "0", "\n", "for", "batch1", ",", "batch2", ",", "t", "in", "zip", "(", "dataloader_input", ",", "dataloader_output", ",", "trange", "(", "len", "(", "dataloader_input", ")", ")", ")", ":", "\n", "        ", "if", "hps", ".", "cuda", ":", "\n", "            ", "batch1", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch1", ")", "\n", "batch2", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch2", ")", "\n", "\n", "", "input_ids", ",", "attention_mask", ",", "labels", "=", "batch1", "\n", "decoder_ids", ",", "decoder_mask", "=", "batch2", "\n", "scores", ",", "_", "=", "model", "(", "input_ids", ",", "\n", "attention_mask", ",", "\n", "decoder_ids", ",", "\n", "decoder_mask", ",", "\n", "labels", ",", "\n", "mode", "=", "'train'", ")", "\n", "scores", "=", "torch", ".", "cat", "(", "(", "scores", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", ",", "scores", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", ")", ",", "1", ")", "\n", "index", "=", "torch", ".", "argmax", "(", "scores", ",", "1", ")", "\n", "predict_labels", "=", "index", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "labels", "=", "torch", ".", "cat", "(", "(", "labels", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", ",", "labels", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", ")", ",", "1", ")", "\n", "labels", "=", "torch", ".", "argmax", "(", "labels", ",", "1", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "for", "k", "in", "range", "(", "len", "(", "predict_labels", ")", ")", ":", "\n", "            ", "if", "labels", "[", "k", "]", "==", "predict_labels", "[", "k", "]", ":", "\n", "                ", "count", "+=", "1", "\n", "", "else", ":", "\n", "                ", "continue", "\n", "\n", "", "", "input_ids", "=", "torch", ".", "cat", "(", "(", "input_ids", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", ",", "input_ids", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", ")", ",", "1", ")", "\n", "input_ids", "=", "input_ids", "[", "range", "(", "input_ids", ".", "shape", "[", "0", "]", ")", ",", "index", ",", ":", "]", "\n", "attention_mask", "=", "torch", ".", "cat", "(", "(", "attention_mask", "[", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", ",", "attention_mask", "[", "1", ":", ":", "2", "]", ".", "unsqueeze", "(", "1", ")", ")", ",", "1", ")", "\n", "attention_mask", "=", "attention_mask", "[", "range", "(", "attention_mask", ".", "shape", "[", "0", "]", ")", ",", "index", ",", ":", "]", "\n", "\n", "# for i in range(input_ids.shape[0]):", "\n", "gen_ids", "=", "model", "(", "input_ids", ",", "\n", "attention_mask", ",", "\n", "decoder_ids", ",", "\n", "decoder_mask", ",", "\n", "labels", ",", "\n", "mode", "=", "'generate'", ")", "\n", "generated_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "gen_ids", ".", "tolist", "(", ")", "]", "\n", "gold_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "decoder_ids", "[", ":", ":", "2", ",", ":", "]", ".", "tolist", "(", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "generated_text", ")", ")", ":", "\n", "            ", "bleu1", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ",", "[", "1", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "bleu2", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ",", "[", "0", ",", "1", ",", "0", ",", "0", "]", ")", "\n", "bleu3", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ",", "[", "0", ",", "0", ",", "1", ",", "0", "]", ")", "\n", "bleu4", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ",", "[", "0", ",", "0", ",", "0", ",", "1", "]", ")", "\n", "\n", "", "", "num_instances", "=", "(", "len", "(", "dataloader_output", ")", "-", "1", ")", "*", "hps", ".", "batch_size", "//", "2", "+", "input_ids", ".", "shape", "[", "0", "]", "\n", "return", "count", "/", "num_instances", ",", "bleu1", "/", "num_instances", ",", "bleu2", "/", "num_instances", ",", "bleu3", "/", "num_instances", ",", "bleu4", "/", "num_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.load_data": [[341, 344], ["json.laods", "open"], "function", ["None"], ["", "def", "load_data", "(", "path", ")", ":", "\n", "    ", "data", "=", "[", "json", ".", "laods", "(", "line", ")", "for", "line", "in", "open", "(", "path", ",", "'r'", ")", "]", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.evaluation": [[346, 451], ["model.eval", "range", "model().squeeze.squeeze().cpu().tolist", "loss_function().item", "tmp_labels.cpu().numpy().tolist", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "len", "tuple", "model().squeeze", "model", "torch.sigmoid().tolist", "torch.sigmoid().tolist", "torch.sigmoid().tolist", "enumerate", "len", "model().squeeze.squeeze().cpu", "loss_function", "tmp_labels.cpu().numpy", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sigmoid().tolist", "torch.sigmoid().tolist", "torch.sigmoid().tolist", "range", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.FloatTensor().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "torch.argmax().tolist", "term.cuda", "model", "tmp_labels.float", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "len", "model().squeeze.squeeze", "tmp_labels.cpu", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "function", ["None"], ["", "def", "evaluation", "(", "hps", ",", "dataloader", ",", "model", ",", "loss_function", ",", "mode", "=", "'train'", ")", ":", "\n", "    ", "predictions", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "loss", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "for", "batch", "in", "dataloader", ":", "\n", "        ", "if", "hps", ".", "cuda", ":", "\n", "            ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "if", "mode", "==", "'train'", ":", "\n", "            ", "sent", ",", "seg_id", ",", "atten_mask", ",", "tmp_labels", ",", "tmp_length", "=", "batch", "\n", "probs", "=", "model", "(", "sent", ",", "atten_mask", ",", "seg_ids", "=", "seg_id", ",", "length", "=", "tmp_length", ")", ".", "squeeze", "(", ")", "\n", "", "else", ":", "\n", "            ", "sent", ",", "atten_mask", ",", "tmp_labels", "=", "batch", "\n", "_", ",", "probs", "=", "model", "(", "sent", ",", "atten_mask", ")", "\n", "# sent, seg_id, atten_mask, tmp_labels, tmp_length = batch", "\n", "# probs = model(sent, atten_mask, seg_ids=seg_id, length=tmp_length)", "\n", "\n", "# if hps.loss_func == \"CrossEntropy\":", "\n", "#     # predictions += torch.argmax(probs, 1).cpu().numpy().tolist()", "\n", "#     predictions += torch.argmax(torch.cat((probs[::2].unsqueeze(1), probs[1::2].unsqueeze(1)), 1), 1).cpu().tolist()", "\n", "#     labels += tmp_labels.cpu().tolist()", "\n", "#     loss += loss_function(probs, tmp_labels.float()).item()", "\n", "# else:", "\n", "", "predictions", "+=", "probs", ".", "squeeze", "(", ")", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "\n", "loss", "+=", "loss_function", "(", "probs", ",", "tmp_labels", ".", "float", "(", ")", ")", ".", "item", "(", ")", "\n", "labels", "+=", "tmp_labels", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ".", "tolist", "(", ")", "\n", "\n", "# if hps.loss_func == 'CrossEntropy':", "\n", "#     count = 0", "\n", "#     for i in range(len(predictions)):", "\n", "#         if predictions[i] == labels[i]:", "\n", "#             count += 1", "\n", "#         else:", "\n", "#             continue", "\n", "\n", "#     return count/len(labels), loss", "\n", "# else:", "\n", "", "if", "hps", ".", "data_name", "==", "'commonsenseqa'", ":", "\n", "        ", "a1", "=", "torch", ".", "FloatTensor", "(", "predictions", "[", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a2", "=", "torch", ".", "FloatTensor", "(", "predictions", "[", "1", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a3", "=", "torch", ".", "FloatTensor", "(", "predictions", "[", "2", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a4", "=", "torch", ".", "FloatTensor", "(", "predictions", "[", "3", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a5", "=", "torch", ".", "FloatTensor", "(", "predictions", "[", "4", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a", "=", "torch", ".", "cat", "(", "(", "a1", ",", "a2", ",", "a3", ",", "a4", ",", "a5", ")", ",", "dim", "=", "1", ")", "\n", "\n", "t_a1", "=", "torch", ".", "FloatTensor", "(", "labels", "[", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a2", "=", "torch", ".", "FloatTensor", "(", "labels", "[", "1", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a3", "=", "torch", ".", "FloatTensor", "(", "labels", "[", "2", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a4", "=", "torch", ".", "FloatTensor", "(", "labels", "[", "3", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a5", "=", "torch", ".", "FloatTensor", "(", "labels", "[", "4", ":", ":", "5", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a", "=", "torch", ".", "cat", "(", "(", "t_a1", ",", "t_a2", ",", "t_a3", ",", "t_a4", ",", "t_a5", ")", ",", "dim", "=", "1", ")", "\n", "predict_labels", "=", "torch", ".", "argmax", "(", "a", ",", "1", ")", ".", "tolist", "(", ")", "\n", "true_labels", "=", "torch", ".", "argmax", "(", "t_a", ",", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "", "elif", "hps", ".", "data_name", "==", "'because'", ":", "\n", "# softmax = nn.Softmax(1)", "\n", "        ", "a", "=", "predictions", "\n", "t_a", "=", "labels", "\n", "predict_labels", "=", "torch", ".", "sigmoid", "(", "torch", ".", "FloatTensor", "(", "a", ")", ")", ".", "tolist", "(", ")", "\n", "true_labels", "=", "t_a", "\n", "for", "k", ",", "p", "in", "enumerate", "(", "predict_labels", ")", ":", "\n", "            ", "if", "p", ">=", "0.5", ":", "\n", "                ", "predict_labels", "[", "k", "]", "=", "1", "\n", "", "else", ":", "\n", "                ", "predict_labels", "[", "k", "]", "=", "0", "\n", "\n", "", "", "", "elif", "hps", ".", "data_name", "==", "'event_storyline'", ":", "\n", "        ", "a", "=", "predictions", "\n", "predict_labels", "=", "torch", ".", "sigmoid", "(", "torch", ".", "FloatTensor", "(", "a", ")", ")", ".", "tolist", "(", ")", "\n", "predict_labels", "=", "[", "1", "if", "p", ">=", "0.5", "else", "0", "for", "p", "in", "predict_labels", "]", "\n", "t_a", "=", "labels", "\n", "tp", ",", "tn", ",", "fp", ",", "fn", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "for", "k", "in", "range", "(", "len", "(", "t_a", ")", ")", ":", "\n", "            ", "if", "labels", "[", "k", "]", "==", "1", "and", "predict_labels", "[", "k", "]", "==", "1", ":", "\n", "                ", "tp", "+=", "1", "\n", "", "elif", "labels", "[", "k", "]", "==", "1", "and", "predict_labels", "[", "k", "]", "==", "0", ":", "\n", "                ", "fn", "+=", "1", "\n", "", "elif", "labels", "[", "k", "]", "==", "0", "and", "predict_labels", "[", "k", "]", "==", "1", ":", "\n", "                ", "fp", "+=", "1", "\n", "", "else", ":", "\n", "                ", "tn", "+=", "1", "\n", "", "", "precision", "=", "tp", "/", "(", "tp", "+", "fp", ")", "\n", "recall", "=", "tp", "/", "(", "tp", "+", "fn", ")", "\n", "f1", "=", "2", "*", "precision", "*", "recall", "/", "(", "precision", "+", "recall", ")", "\n", "return", "f1", ",", "0", "\n", "\n", "", "else", ":", "\n", "        ", "a1", "=", "torch", ".", "FloatTensor", "(", "predictions", "[", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a2", "=", "torch", ".", "FloatTensor", "(", "predictions", "[", "1", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "a", "=", "torch", ".", "cat", "(", "(", "a1", ",", "a2", ")", ",", "dim", "=", "1", ")", "\n", "t_a1", "=", "torch", ".", "FloatTensor", "(", "labels", "[", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a2", "=", "torch", ".", "FloatTensor", "(", "labels", "[", "1", ":", ":", "2", "]", ")", ".", "unsqueeze", "(", "1", ")", "\n", "t_a", "=", "torch", ".", "cat", "(", "(", "t_a1", ",", "t_a2", ")", ",", "dim", "=", "1", ")", "\n", "predict_labels", "=", "torch", ".", "argmax", "(", "a", ",", "1", ")", ".", "tolist", "(", ")", "\n", "true_labels", "=", "torch", ".", "argmax", "(", "t_a", ",", "1", ")", ".", "tolist", "(", ")", "\n", "\n", "\n", "", "count", "=", "0", "\n", "for", "i", "in", "range", "(", "len", "(", "predict_labels", ")", ")", ":", "\n", "        ", "if", "predict_labels", "[", "i", "]", "==", "true_labels", "[", "i", "]", ":", "\n", "            ", "count", "+=", "1", "\n", "", "else", ":", "\n", "            ", "continue", "\n", "", "", "return", "count", "/", "len", "(", "true_labels", ")", ",", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.define_logger": [[453, 463], ["logging.getLogger", "logging.Formatter", "logging.StreamHandler", "logging.StreamHandler.setLevel", "logging.getLogger.addHandler", "logging.getLogger.setLevel"], "function", ["None"], ["", "def", "define_logger", "(", ")", ":", "\n", "    ", "logger", "=", "logging", ".", "getLogger", "(", "'Discriminate logger'", ")", "\n", "formatter", "=", "logging", ".", "Formatter", "(", "'%(asctime)s %(levelname)-8s: %(message)s'", ")", "\n", "console_handler", "=", "logging", ".", "StreamHandler", "(", "sys", ".", "stdout", ")", "\n", "console_handler", ".", "formatter", "=", "formatter", "\n", "console_handler", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "logger", ".", "addHandler", "(", "console_handler", ")", "\n", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "\n", "return", "logger", ",", "formatter", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.tokenize_gen": [[465, 517], ["transformers.BartTokenizer.from_pretrained", "GPT2Tokenizer.from_pretrained.", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "GPT2Tokenizer.from_pretrained.", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "transformers.GPT2Tokenizer.from_pretrained", "inputs.append", "labels.append", "GPT2Tokenizer.from_pretrained.", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "GPT2Tokenizer.from_pretrained.", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "GPT2Tokenizer.from_pretrained.", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "inputs.append", "premise.append", "labels.append"], "function", ["None"], ["", "def", "tokenize_gen", "(", "data", ",", "hps", ")", ":", "\n", "    ", "if", "hps", ".", "model_name", "==", "'bart'", ":", "\n", "        ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'gpt2'", ":", "\n", "        ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "tokenizer", ".", "pad_token", "=", "tokenizer", ".", "unk_token", "\n", "", "else", ":", "\n", "        ", "tokenizer", "=", "None", "\n", "\n", "", "inputs", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "premise", "=", "[", "]", "\n", "for", "example", "in", "data", ":", "\n", "        ", "if", "hps", ".", "model_name", "==", "'bart'", ":", "\n", "            ", "seq1", "=", "example", "[", "'cause'", "]", "+", "example", "[", "'effect'", "]", "\n", "seq2", "=", "example", "[", "'general_truth'", "]", "\n", "inputs", ".", "append", "(", "seq1", ")", "\n", "labels", ".", "append", "(", "seq2", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'gpt2'", ":", "\n", "            ", "inputs", ".", "append", "(", "[", "example", "[", "'cause'", "]", "+", "' '", "+", "example", "[", "'effect'", "]", ",", "example", "[", "'general_truth'", "]", "]", ")", "\n", "premise", ".", "append", "(", "example", "[", "'cause'", "]", "+", "' '", "+", "example", "[", "'effect'", "]", ")", "\n", "labels", ".", "append", "(", "example", "[", "'general_truth'", "]", ")", "\n", "", "else", ":", "\n", "            ", "return", "\n", "\n", "", "", "if", "hps", ".", "model_name", "==", "'bart'", ":", "\n", "        ", "outputs", "=", "tokenizer", "(", "inputs", ",", "padding", "=", "True", ")", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "outputs", "[", "'input_ids'", "]", ")", "\n", "input_attention_mask", "=", "torch", ".", "LongTensor", "(", "outputs", "[", "'attention_mask'", "]", ")", "\n", "label_output", "=", "tokenizer", "(", "labels", ",", "padding", "=", "True", ")", "\n", "label_ids", "=", "torch", ".", "LongTensor", "(", "label_output", "[", "'input_ids'", "]", ")", "\n", "label_attention_mask", "=", "torch", ".", "LongTensor", "(", "label_output", "[", "'attention_mask'", "]", ")", "\n", "\n", "return", "input_ids", ",", "input_attention_mask", ",", "label_ids", ",", "label_attention_mask", "\n", "\n", "", "elif", "hps", ".", "model_name", "==", "'gpt2'", ":", "\n", "        ", "evaluate_outputs", "=", "tokenizer", "(", "labels", ",", "padding", "=", "True", ",", "return_token_type_ids", "=", "True", ")", "\n", "labels_ids", "=", "torch", ".", "LongTensor", "(", "evaluate_outputs", "[", "'input_ids'", "]", ")", "\n", "labels_mask", "=", "torch", ".", "LongTensor", "(", "evaluate_outputs", "[", "'attention_mask'", "]", ")", "\n", "labels_seg_id", "=", "torch", ".", "LongTensor", "(", "evaluate_outputs", "[", "'token_type_ids'", "]", ")", "\n", "\n", "tokenizer", ".", "padding_side", "=", "'left'", "\n", "outputs", "=", "tokenizer", "(", "inputs", ",", "padding", "=", "True", ",", "return_token_type_ids", "=", "True", ")", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "outputs", "[", "'input_ids'", "]", ")", "\n", "input_attention_mask", "=", "torch", ".", "LongTensor", "(", "outputs", "[", "'attention_mask'", "]", ")", "\n", "input_seg_id", "=", "torch", ".", "LongTensor", "(", "outputs", "[", "'token_type_ids'", "]", ")", "\n", "\n", "premise_outputs", "=", "tokenizer", "(", "premise", ",", "padding", "=", "True", ",", "return_token_type_ids", "=", "True", ")", "\n", "premise_ids", "=", "torch", ".", "LongTensor", "(", "premise_outputs", "[", "'input_ids'", "]", ")", "\n", "premise_mask", "=", "torch", ".", "LongTensor", "(", "premise_outputs", "[", "'attention_mask'", "]", ")", "\n", "premise_seg_ids", "=", "torch", ".", "LongTensor", "(", "premise_outputs", "[", "'token_type_ids'", "]", ")", "\n", "return", "input_ids", ",", "input_attention_mask", ",", "input_seg_id", ",", "labels_ids", ",", "labels_mask", ",", "labels_seg_id", ",", "premise_ids", ",", "premise_mask", ",", "premise_seg_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.evaluation_bart": [[519, 546], ["transformers.BartTokenizer.from_pretrained", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "sum", "tuple", "model", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "BartTokenizer.from_pretrained.convert_ids_to_tokens", "BartTokenizer.from_pretrained.convert_ids_to_tokens", "BartTokenizer.from_pretrained.convert_tokens_to_string", "BartTokenizer.from_pretrained.convert_tokens_to_string", "utils.remove_special_tokens", "utils.remove_special_tokens", "len", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.argmax().unsqueeze.tolist", "range", "range", "nltk.bleu", "term.cuda", "torch.argmax", "torch.argmax", "torch.argmax", "range", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.remove_special_tokens", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.remove_special_tokens"], ["", "", "def", "evaluation_bart", "(", "dataloader", ",", "model", ",", "hps", ")", ":", "\n", "    ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "score", "=", "0", "\n", "for", "batch", "in", "dataloader", ":", "\n", "        ", "if", "hps", ".", "cuda", ":", "\n", "            ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "input_ids", ",", "input_mask", ",", "labels", ",", "label_mask", "=", "batch", "\n", "predict_id", "=", "torch", ".", "zeros", "(", "[", "input_ids", ".", "shape", "[", "0", "]", ",", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "decoder_ids", "=", "torch", ".", "zeros", "(", "[", "input_ids", ".", "shape", "[", "0", "]", ",", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "while", "decoder_ids", ".", "shape", "[", "1", "]", "<", "35", "and", "predict_id", ".", "tolist", "(", ")", "not", "in", "[", "[", "[", "2", "]", ",", "[", "2", "]", "]", ",", "[", "[", "1", "]", ",", "[", "1", "]", "]", "]", ":", "\n", "            ", "output", "=", "model", "(", "input_ids", ",", "input_mask", "=", "input_mask", ",", "decoder_ids", "=", "decoder_ids", ",", "mode", "=", "'test'", ")", "\n", "predict_id", "=", "torch", ".", "argmax", "(", "output", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", ",", "-", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "decoder_ids", "=", "torch", ".", "cat", "(", "(", "decoder_ids", ",", "predict_id", ")", ",", "-", "1", ")", "\n", "\n", "", "label_tokens", "=", "[", "tokenizer", ".", "convert_ids_to_tokens", "(", "labels", "[", "i", "]", ")", "for", "i", "in", "range", "(", "labels", ".", "shape", "[", "0", "]", ")", "]", "\n", "predict_tokens", "=", "[", "tokenizer", ".", "convert_ids_to_tokens", "(", "decoder_ids", "[", "i", "]", ")", "for", "i", "in", "range", "(", "decoder_ids", ".", "shape", "[", "0", "]", ")", "]", "\n", "references", "=", "[", "tokenizer", ".", "convert_tokens_to_string", "(", "tokens", ")", "for", "tokens", "in", "label_tokens", "]", "\n", "hypothesis", "=", "[", "tokenizer", ".", "convert_tokens_to_string", "(", "tokens", ")", "for", "tokens", "in", "predict_tokens", "]", "\n", "references", "=", "[", "remove_special_tokens", "(", "text", ")", "for", "text", "in", "references", "]", "\n", "hypothesis", "=", "[", "remove_special_tokens", "(", "text", ")", "for", "text", "in", "hypothesis", "]", "\n", "\n", "score", "+=", "sum", "(", "[", "bleu", "(", "[", "references", "[", "i", "]", "]", ",", "hypothesis", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "references", ")", ")", "]", ")", "\n", "\n", "", "return", "score", "/", "len", "(", "dataloader", ")", "/", "hps", ".", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.evaluate_gpt2": [[548, 577], ["transformers.GPT2Tokenizer.from_pretrained", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "torch.zeros().long().cuda", "sum", "tuple", "model", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.argmax().unsqueeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "GPT2Tokenizer.from_pretrained.convert_ids_to_tokens", "GPT2Tokenizer.from_pretrained.convert_ids_to_tokens", "GPT2Tokenizer.from_pretrained.convert_tokens_to_string", "GPT2Tokenizer.from_pretrained.convert_tokens_to_string", "utils.remove_special_tokens", "utils.remove_special_tokens", "len", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.argmax().unsqueeze.tolist", "range", "range", "nltk.bleu", "term.cuda", "torch.argmax", "torch.argmax", "torch.argmax", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "range", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.remove_special_tokens", "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.remove_special_tokens"], ["", "def", "evaluate_gpt2", "(", "dataloader", ",", "model", ",", "hps", ")", ":", "\n", "    ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "score", "=", "0", "\n", "for", "batch", "in", "dataloader", ":", "\n", "        ", "if", "hps", ".", "cuda", ":", "\n", "            ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "gen_ids", ",", "gen_mask", ",", "_", ",", "premise_ids", ",", "premise_mask", ",", "premise_token_type_ids", "=", "batch", "\n", "decode_ids", "=", "torch", ".", "zeros", "(", "[", "premise_ids", ".", "shape", "[", "0", "]", ",", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "predict_id", "=", "torch", ".", "zeros", "(", "[", "premise_ids", ".", "shape", "[", "0", "]", ",", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", "\n", "\n", "while", "decode_ids", ".", "shape", "[", "1", "]", "<=", "35", "and", "predict_id", ".", "tolist", "(", ")", "!=", "(", "torch", ".", "ones", "(", "[", "hps", ".", "batch_size", ",", "1", "]", ")", ".", "long", "(", ")", "*", "50256", ")", ".", "tolist", "(", ")", ":", "\n", "            ", "output", "=", "model", "(", "premise_ids", ",", "premise_mask", ",", "token_type_ids", "=", "premise_token_type_ids", ",", "mode", "=", "'test'", ")", "\n", "predict_id", "=", "torch", ".", "argmax", "(", "output", "[", "1", "]", "[", ":", ",", "-", "1", ",", ":", "]", ",", "-", "1", ")", ".", "unsqueeze", "(", "1", ")", "\n", "decode_ids", "=", "torch", ".", "cat", "(", "(", "decode_ids", ",", "predict_id", ")", ",", "-", "1", ")", "\n", "premise_ids", "=", "torch", ".", "cat", "(", "(", "premise_ids", ",", "predict_id", ")", ",", "-", "1", ")", "\n", "premise_mask", "=", "torch", ".", "cat", "(", "(", "premise_mask", ",", "torch", ".", "ones", "(", "[", "premise_mask", ".", "shape", "[", "0", "]", ",", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ",", "-", "1", ")", "\n", "premise_token_type_ids", "=", "torch", ".", "cat", "(", "(", "premise_token_type_ids", ",", "torch", ".", "ones", "(", "[", "premise_token_type_ids", ".", "shape", "[", "0", "]", ",", "1", "]", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ",", "-", "1", ")", "\n", "\n", "", "label_tokens", "=", "[", "tokenizer", ".", "convert_ids_to_tokens", "(", "gen_ids", "[", "i", "]", ")", "for", "i", "in", "range", "(", "gen_ids", ".", "shape", "[", "0", "]", ")", "]", "\n", "predict_tokens", "=", "[", "tokenizer", ".", "convert_ids_to_tokens", "(", "decode_ids", "[", "i", "]", "[", "1", ":", "]", ")", "for", "i", "in", "range", "(", "decode_ids", ".", "shape", "[", "0", "]", ")", "]", "\n", "references", "=", "[", "tokenizer", ".", "convert_tokens_to_string", "(", "tokens", ")", "for", "tokens", "in", "label_tokens", "]", "\n", "hypothesis", "=", "[", "tokenizer", ".", "convert_tokens_to_string", "(", "tokens", ")", "for", "tokens", "in", "predict_tokens", "]", "\n", "references", "=", "[", "remove_special_tokens", "(", "text", ")", "for", "text", "in", "references", "]", "\n", "hypothesis", "=", "[", "remove_special_tokens", "(", "text", ")", "for", "text", "in", "hypothesis", "]", "\n", "\n", "score", "+=", "sum", "(", "[", "bleu", "(", "[", "references", "[", "i", "]", "]", ",", "hypothesis", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "references", ")", ")", "]", ")", "\n", "\n", "", "return", "score", "/", "len", "(", "dataloader", ")", "/", "hps", ".", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.remove_special_tokens": [[579, 581], ["text.replace().replace().replace().replace().replace", "text.replace().replace().replace().replace", "text.replace().replace().replace", "text.replace().replace", "text.replace"], "function", ["None"], ["", "def", "remove_special_tokens", "(", "text", ")", ":", "\n", "    ", "return", "text", ".", "replace", "(", "'<s>'", ",", "''", ")", ".", "replace", "(", "'</s>'", ",", "''", ")", ".", "replace", "(", "'<pad>'", ",", "''", ")", ".", "replace", "(", "'<unk>'", ",", "''", ")", ".", "replace", "(", "'<|endoftext|>'", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.top_k_logits": [[584, 590], ["torch.topk", "torch.topk", "torch.topk", "values[].unsqueeze", "torch.where", "torch.where", "torch.where", "torch.ones_like", "torch.ones_like", "torch.ones_like"], "function", ["None"], ["", "def", "top_k_logits", "(", "logits", ",", "k", ")", ":", "\n", "    ", "if", "k", "==", "0", ":", "\n", "        ", "return", "logits", "\n", "", "values", ",", "_", "=", "torch", ".", "topk", "(", "logits", ",", "k", ")", "\n", "min_values", "=", "values", "[", ":", ",", "-", "1", "]", ".", "unsqueeze", "(", "1", ")", "\n", "return", "torch", ".", "where", "(", "logits", "<", "min_values", ",", "torch", ".", "ones_like", "(", "logits", ",", "dtype", "=", "logits", ".", "dtype", ")", "*", "-", "1e10", ",", "logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.sample_sequence": [[618, 655], ["torch.full", "torch.full", "torch.full", "torch.no_grad", "torch.no_grad", "torch.no_grad", "tqdm.trange", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "utils.top_k_logits", "torch.softmax", "torch.cat", "torch.cat", "torch.cat", "model", "model", "torch.multinomial", "torch.multinomial", "torch.multinomial", "torch.topk", "torch.topk", "torch.topk", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.ones().long().cuda", "torch.ones().long().cuda", "torch.ones().long().cuda", "[].unsqueeze", "torch.ones().long", "torch.ones().long", "torch.ones().long", "torch.ones", "torch.ones", "torch.ones"], "function", ["home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.top_k_logits"], ["", "def", "sample_sequence", "(", "model", ",", "length", ",", "start_token", "=", "None", ",", "batch_size", "=", "None", ",", "context", "=", "None", ",", "temperature", "=", "0.7", ",", "top_k", "=", "40", ",", "\n", "device", "=", "'cuda'", ",", "sample", "=", "True", ",", "attention_mask", "=", "None", ",", "input_type", "=", "'ids'", ")", ":", "\n", "    ", "if", "start_token", "is", "None", ":", "\n", "        ", "assert", "context", "is", "not", "None", ",", "'Specify exactly one of start_token and context!'", "\n", "if", "input_type", "==", "'ids'", ":", "\n", "            ", "context", "=", "torch", ".", "tensor", "(", "context", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "else", ":", "\n", "            ", "context", "=", "torch", ".", "tensor", "(", "context", ",", "device", "=", "device", ")", "\n", "", "", "else", ":", "\n", "        ", "assert", "context", "is", "None", ",", "'Specify exactly one of start_token and context!'", "\n", "context", "=", "torch", ".", "full", "(", "(", "batch_size", ",", "1", ")", ",", "start_token", ",", "device", "=", "device", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "", "prev", "=", "context", "\n", "output_id", "=", "None", "\n", "output", "=", "context", "\n", "past", "=", "None", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "i", "in", "trange", "(", "length", ")", ":", "\n", "            ", "if", "input_type", "==", "'ids'", ":", "\n", "                ", "gen_output", "=", "model", "(", "input_ids", "=", "output", ",", "attention_mask", "=", "attention_mask", ",", "past_key_values", "=", "None", ",", "mode", "=", "'test'", ")", "\n", "logits", "=", "gen_output", "[", "'logits'", "]", "\n", "", "else", ":", "\n", "                ", "logits", ",", "past", ",", "hiddens", "=", "model", "(", "inputs_embeds", "=", "output", ",", "attention_mask", "=", "attention_mask", ",", "past_key_values", "=", "None", ",", "output_hidden_states", "=", "True", ")", "\n", "", "logits", "=", "logits", "[", ":", ",", "-", "1", ",", ":", "]", "/", "temperature", "\n", "logits", "=", "top_k_logits", "(", "logits", ",", "k", "=", "top_k", ")", "\n", "log_probs", "=", "F", ".", "softmax", "(", "logits", ",", "dim", "=", "-", "1", ")", "\n", "if", "sample", ":", "\n", "                ", "prev", "=", "torch", ".", "multinomial", "(", "log_probs", ",", "num_samples", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "_", ",", "prev", "=", "torch", ".", "topk", "(", "log_probs", ",", "k", "=", "1", ",", "dim", "=", "-", "1", ")", "\n", "", "if", "input_type", "==", "'ids'", ":", "\n", "                ", "output", "=", "torch", ".", "cat", "(", "(", "output", ",", "prev", ")", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "                ", "output", "=", "torch", ".", "cat", "(", "(", "output", ",", "hiddens", "[", "-", "1", "]", "[", ":", ",", "-", "1", ",", ":", "]", ".", "unsqueeze", "(", "1", ")", ")", ",", "1", ")", "\n", "output_id", "=", "prev", "if", "output_id", "is", "None", "else", "torch", ".", "cat", "(", "(", "output_id", ",", "prev", ")", ",", "1", ")", "\n", "\n", "", "attention_mask", "=", "torch", ".", "cat", "(", "(", "attention_mask", ",", "torch", ".", "ones", "(", "prev", ".", "shape", ")", ".", "long", "(", ")", ".", "cuda", "(", ")", ")", ",", "-", "1", ")", "\n", "", "", "return", "output", "if", "input_type", "==", "'ids'", "else", "output_id", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.gpt2_evaluate": [[657, 731], ["transformers.GPT2Tokenizer.from_pretrained", "rouge.Rouge", "datetime.datetime.now().strftime", "open", "csv.writer", "csv.writer.writerows", "model.generate", "range", "datetime.datetime.now", "tuple", "GPT2Tokenizer.from_pretrained.decode", "GPT2Tokenizer.from_pretrained.decode", "GPT2Tokenizer.from_pretrained.decode", "nltk.bleu", "nltk.bleu", "nltk.bleu", "nltk.bleu", "model.generate.cpu().tolist", "gen_ids.cpu().tolist", "range", "rouge.Rouge.get_scores", "len", "term.cuda", "len", "model.generate.cpu", "gen_ids.cpu", "generated_text[].split", "generated_text[].split", "generated_text[].split", "generated_text[].split", "generated_text[].split"], "function", ["None"], ["", "def", "gpt2_evaluate", "(", "model", ",", "length", ",", "data_loader", ",", "hps", ")", ":", "\n", "    ", "tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "\n", "bleu1", ",", "bleu2", ",", "bleu3", ",", "bleu4", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "rouge1p", ",", "rouge1r", ",", "rouge1f", ",", "rouge2p", ",", "rouge2r", ",", "rouge2f", ",", "rougelp", ",", "rougelr", ",", "rougelf", "=", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "\n", "rouge", "=", "Rouge", "(", ")", "\n", "output_text", "=", "[", "]", "\n", "nowtime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "\n", "for", "batch", "in", "data_loader", ":", "\n", "        ", "if", "hps", ".", "cuda", ":", "\n", "            ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "", "gen_ids", ",", "gen_mask", ",", "_", ",", "premise_ids", ",", "premise_mask", ",", "premise_token_type_ids", "=", "batch", "\n", "\n", "# output = sample_sequence(model, length, device='cuda', context=premise_ids, batch_size=hps.batch_size, attention_mask=premise_mask, input_type='ids')", "\n", "generated", "=", "model", ".", "generate", "(", "input_ids", "=", "premise_ids", ",", "\n", "attention_mask", "=", "premise_mask", ",", "\n", "max_length", "=", "length", "+", "premise_ids", ".", "shape", "[", "1", "]", ",", "\n", "num_beams", "=", "5", ",", "\n", "early_stopping", "=", "True", ",", "\n", "do_sample", "=", "True", ",", "\n", "no_repeat_ngram_size", "=", "3", ",", "\n", "repetition_penalty", "=", "1.5", "\n", ")", "\n", "\n", "# generated = output[:, premise_ids.shape[1]:]", "\n", "# pdb.set_trace()", "\n", "generated", "=", "generated", "[", ":", ",", "premise_ids", ".", "shape", "[", "1", "]", ":", "]", "\n", "\n", "generated_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "generated", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "]", "\n", "gold_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "gen_ids", ".", "cpu", "(", ")", ".", "tolist", "(", ")", "]", "\n", "input_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "premise_ids", "]", "\n", "output_text", "+=", "[", "[", "input_text", "[", "i", "]", ",", "gold_text", "[", "i", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", "]", "for", "i", "in", "range", "(", "len", "(", "input_text", ")", ")", "]", "\n", "\n", "\n", "for", "i", "in", "range", "(", "generated", ".", "shape", "[", "0", "]", ")", ":", "\n", "# predict_tokens = tokenizer.convert_ids_to_tokens(generated[i])", "\n", "# generated_text = remove_special_tokens(tokenizer.convert_tokens_to_string(predict_tokens))", "\n", "\n", "\n", "# gold_tokens = tokenizer.convert_ids_to_tokens(gen_ids[i])", "\n", "# gold_text = remove_special_tokens(tokenizer.convert_tokens_to_string(gold_tokens))", "\n", "\n", "            ", "bleu1", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "1", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "bleu2", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "0", ",", "1", ",", "0", ",", "0", "]", ")", "\n", "bleu3", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "0", ",", "0", ",", "1", ",", "0", "]", ")", "\n", "bleu4", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generated_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "0", ",", "0", ",", "0", ",", "1", "]", ")", "\n", "\n", "try", ":", "\n", "                ", "scores", "=", "rouge", ".", "get_scores", "(", "generated_text", "[", "i", "]", ",", "gold_text", "[", "i", "]", ")", "\n", "rouge1", "=", "scores", "[", "0", "]", "[", "'rouge-1'", "]", "\n", "rouge1f", "+=", "rouge1", "[", "'f'", "]", "\n", "rougelp", "+=", "rouge1", "[", "'p'", "]", "\n", "rouge1r", "+=", "rouge1", "[", "'r'", "]", "\n", "\n", "rouge2", "=", "scores", "[", "0", "]", "[", "'rouge-2'", "]", "\n", "rouge2f", "+=", "rouge2", "[", "'f'", "]", "\n", "rouge1p", "+=", "rouge2", "[", "'p'", "]", "\n", "rouge2r", "+=", "rouge2", "[", "'r'", "]", "\n", "\n", "rougel", "=", "scores", "[", "0", "]", "[", "'rouge-l'", "]", "\n", "rougelf", "+=", "rougel", "[", "'f'", "]", "\n", "rougelp", "+=", "rougel", "[", "'p'", "]", "\n", "rougelr", "+=", "rougel", "[", "'r'", "]", "\n", "", "except", ":", "\n", "                ", "continue", "\n", "\n", "", "", "", "num_instances", "=", "(", "len", "(", "data_loader", ")", "-", "1", ")", "*", "hps", ".", "batch_size", "+", "gen_ids", ".", "shape", "[", "0", "]", "\n", "\n", "fo", "=", "open", "(", "hps", ".", "output_dir", "+", "'/gpt2_predict_'", "+", "nowtime", "+", "'.csv'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "writer", "=", "csv", ".", "writer", "(", "fo", ")", "\n", "writer", ".", "writerows", "(", "output_text", ")", "\n", "\n", "return", "bleu1", "/", "num_instances", ",", "bleu2", "/", "num_instances", ",", "bleu3", "/", "num_instances", ",", "bleu4", "/", "num_instances", ",", "rouge1r", "/", "num_instances", ",", "rouge2r", "/", "num_instances", ",", "rougelr", "/", "num_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.utils.utils.bart_evaluate": [[734, 819], ["transformers.BartTokenizer.from_pretrained", "rouge.Rouge", "datetime.datetime.now().strftime", "open", "csv.writer", "csv.writer.writerows", "model.generate", "range", "datetime.datetime.now", "tuple", "BartTokenizer.from_pretrained.decode", "BartTokenizer.from_pretrained.decode", "BartTokenizer.from_pretrained.decode", "len", "nltk.bleu", "nltk.bleu", "nltk.bleu", "nltk.bleu", "range", "rouge.Rouge.get_scores", "len", "term.cuda", "len", "generate_text[].split", "generate_text[].split", "generate_text[].split", "generate_text[].split", "generate_text[].split"], "function", ["None"], ["", "def", "bart_evaluate", "(", "model", ",", "data_loader", ",", "hps", ")", ":", "\n", "    ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "\n", "bleu1", ",", "bleu2", ",", "bleu3", ",", "bleu4", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "rouge1p", ",", "rouge1r", ",", "rouge1f", ",", "rouge2p", ",", "rouge2r", ",", "rouge2f", ",", "rougelp", ",", "rougelr", ",", "rougelf", "=", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", ",", "0", "\n", "# rouge1p, rouge1r, rouge1f, rouge2p, rouge2r, rouge2f, rougelp, rougelr, rougelf = 0, 0, 0, 0, 0, 0, 0, 0, 0", "\n", "rouge", "=", "Rouge", "(", ")", "\n", "nowtime", "=", "datetime", ".", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%Y%m%d_%H%M%S'", ")", "\n", "output_text", "=", "[", "]", "\n", "\n", "for", "batch", "in", "data_loader", ":", "\n", "        ", "if", "hps", ".", "cuda", ":", "\n", "            ", "batch", "=", "tuple", "(", "term", ".", "cuda", "(", ")", "for", "term", "in", "batch", ")", "\n", "\n", "", "input_ids", ",", "input_mask", ",", "labels", ",", "label_mask", "=", "batch", "\n", "generate_ids", "=", "model", ".", "generate", "(", "input_ids", ",", "\n", "attention_mask", "=", "input_mask", ",", "\n", "num_beams", "=", "hps", ".", "beam_size", ",", "\n", "max_length", "=", "hps", ".", "length", ",", "\n", "early_stopping", "=", "True", ",", "\n", "no_repeat_ngram_size", "=", "3", ",", "\n", "repetition_penalty", "=", "1.5", ",", "\n", "# temperature=0.7,", "\n", "# length_penalty=0.6", "\n", ")", "\n", "# generate_ids = generate_ids[:, input_ids.shape[1]:]", "\n", "\n", "generate_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "generate_ids", "]", "\n", "gold_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "labels", "]", "\n", "input_text", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "input_ids", "]", "\n", "\n", "output_text", "+=", "[", "[", "input_text", "[", "i", "]", ",", "gold_text", "[", "i", "]", ",", "generate_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", "]", "for", "i", "in", "range", "(", "len", "(", "input_text", ")", ")", "]", "\n", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "gold_text", ")", ")", ":", "\n", "\n", "            ", "bleu1", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generate_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "1", ",", "0", ",", "0", ",", "0", "]", ")", "\n", "bleu2", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generate_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "0", ",", "1", ",", "0", ",", "0", "]", ")", "\n", "bleu3", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generate_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "0", ",", "0", ",", "1", ",", "0", "]", ")", "\n", "bleu4", "+=", "bleu", "(", "[", "gold_text", "[", "i", "]", "]", ",", "generate_text", "[", "i", "]", ".", "split", "(", "'.'", ")", "[", "0", "]", "+", "'.'", ",", "[", "0", ",", "0", ",", "0", ",", "1", "]", ")", "\n", "\n", "try", ":", "\n", "                ", "scores", "=", "rouge", ".", "get_scores", "(", "generate_text", "[", "i", "]", ",", "gold_text", "[", "i", "]", ")", "\n", "", "except", ":", "\n", "                ", "scores", "=", "[", "\n", "{", "\n", "\"rouge-1\"", ":", "{", "\n", "\"f\"", ":", "0.0", ",", "\n", "\"p\"", ":", "0.0", ",", "\n", "\"r\"", ":", "0.0", "\n", "}", ",", "\n", "\"rouge-2\"", ":", "{", "\n", "\"f\"", ":", "0.0", ",", "\n", "\"p\"", ":", "0.0", ",", "\n", "\"r\"", ":", "0.0", "\n", "}", ",", "\n", "\"rouge-l\"", ":", "{", "\n", "\"f\"", ":", "0.0", ",", "\n", "\"p\"", ":", "0.0", ",", "\n", "\"r\"", ":", "0.0", "\n", "}", "\n", "}", "\n", "]", "\n", "", "rouge1", "=", "scores", "[", "0", "]", "[", "'rouge-1'", "]", "\n", "rouge1f", "+=", "rouge1", "[", "'f'", "]", "\n", "rougelp", "+=", "rouge1", "[", "'p'", "]", "\n", "rouge1r", "+=", "rouge1", "[", "'r'", "]", "\n", "\n", "rouge2", "=", "scores", "[", "0", "]", "[", "'rouge-2'", "]", "\n", "rouge2f", "+=", "rouge2", "[", "'f'", "]", "\n", "rouge1p", "+=", "rouge2", "[", "'p'", "]", "\n", "rouge2r", "+=", "rouge2", "[", "'r'", "]", "\n", "\n", "rougel", "=", "scores", "[", "0", "]", "[", "'rouge-l'", "]", "\n", "rougelf", "+=", "rougel", "[", "'f'", "]", "\n", "rougelp", "+=", "rougel", "[", "'p'", "]", "\n", "rougelr", "+=", "rougel", "[", "'r'", "]", "\n", "\n", "", "", "num_instances", "=", "(", "len", "(", "data_loader", ")", "-", "1", ")", "*", "hps", ".", "batch_size", "+", "input_ids", ".", "shape", "[", "0", "]", "\n", "\n", "fo", "=", "open", "(", "hps", ".", "output_dir", "+", "'/bart_predict_'", "+", "nowtime", "+", "'.csv'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ")", "\n", "writer", "=", "csv", ".", "writer", "(", "fo", ")", "\n", "writer", ".", "writerows", "(", "output_text", ")", "\n", "\n", "return", "bleu1", "/", "num_instances", ",", "bleu2", "/", "num_instances", ",", "bleu3", "/", "num_instances", ",", "bleu4", "/", "num_instances", ",", "rouge1r", "/", "num_instances", ",", "rouge2r", "/", "num_instances", ",", "rougelr", "/", "num_instances", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.model.generatively_model.gpt2_generate.__init__": [[9, 17], ["torch.Module.__init__", "transformers.GPT2Config.from_pretrained", "transformers.GPT2LMHeadModel.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.waste-wood_e-care.model.discriminate_model.pretrained_model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hps", ")", ":", "\n", "        ", "super", "(", "gpt2_generate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hps", "=", "hps", "\n", "self", ".", "model_dir", "=", "hps", ".", "model_dir", "\n", "\n", "self", ".", "config", "=", "GPT2Config", ".", "from_pretrained", "(", "self", ".", "model_dir", ")", "\n", "# self.model = GPT2Model.from_pretrained(self.model_dir)", "\n", "self", ".", "model", "=", "GPT2LMHeadModel", ".", "from_pretrained", "(", "self", ".", "model_dir", ")", "\n", "# self.output_vocab = nn.Linear(self.config.hidden_size, self.config.vocab_size)", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.model.generatively_model.gpt2_generate.forward": [[19, 25], ["generatively_model.gpt2_generate.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", "=", "None", ",", "past_key_values", "=", "None", ",", "mode", "=", "'train'", ",", "true_labels", "=", "None", ")", ":", "\n", "        ", "outputs", "=", "self", ".", "model", "(", "input_ids", "=", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "token_type_ids", "=", "token_type_ids", ",", "labels", "=", "true_labels", ",", "past_key_values", "=", "past_key_values", ")", "\n", "if", "mode", "==", "'train'", ":", "\n", "            ", "return", "outputs", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.model.multi_task_model.discriminate_generate.__init__": [[35, 43], ["torch.Module.__init__", "transformers.BartForConditionalGeneration.from_pretrained", "transformers.BartConfig.from_pretrained", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.waste-wood_e-care.model.discriminate_model.pretrained_model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hps", ")", ":", "\n", "        ", "super", "(", "discriminate_generate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hps", "=", "hps", "\n", "self", ".", "model", "=", "BartForConditionalGeneration", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "config", "=", "BartConfig", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "# self.linear = nn.Linear()", "\n", "self", ".", "linear1", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", ",", "self", ".", "config", ".", "hidden_size", ")", "\n", "self", ".", "linear2", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.model.multi_task_model.discriminate_generate.forward": [[44, 68], ["multi_task_model.discriminate_generate.model", "input_ids.eq", "multi_task_model.discriminate_generate.linear2().squeeze", "multi_task_model.discriminate_generate.model.generate", "[].view", "output[].size", "output[].size", "multi_task_model.discriminate_generate.linear2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "decoder_input_ids", ",", "decoder_mask", ",", "labels", ",", "mode", "=", "'train'", ")", ":", "\n", "        ", "if", "mode", "==", "'train'", ":", "\n", "            ", "output", "=", "self", ".", "model", "(", "input_ids", ",", "\n", "attention_mask", ",", "\n", "decoder_input_ids", "=", "decoder_input_ids", ",", "\n", "decoder_attention_mask", "=", "decoder_mask", ",", "\n", "labels", "=", "decoder_input_ids", ")", "\n", "eos_mask", "=", "input_ids", ".", "eq", "(", "self", ".", "config", ".", "eos_token_id", ")", "\n", "sentence_representation", "=", "output", "[", "-", "1", "]", "[", "eos_mask", ",", ":", "]", ".", "view", "(", "output", "[", "-", "1", "]", ".", "size", "(", "0", ")", ",", "-", "1", ",", "output", "[", "-", "1", "]", ".", "size", "(", "-", "1", ")", ")", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "# token_for_classification = output[2][:, 0, :]", "\n", "# dense = self.linear1(sentence_representation)", "\n", "score", "=", "self", ".", "linear2", "(", "sentence_representation", ")", ".", "squeeze", "(", "1", ")", "\n", "gen_logits", "=", "output", "[", "1", "]", "\n", "return", "score", ",", "gen_logits", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "model", ".", "generate", "(", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "max_length", "=", "self", ".", "hps", ".", "length", ",", "\n", "# min_length=self.hps.length,", "\n", "# no_repeat_ngram_size=3,", "\n", "early_stopping", "=", "False", ",", "\n", "repetition_penalty", "=", "3", ",", "\n", "num_beams", "=", "30", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.model.multi_task_model.generate_discriminate.__init__": [[71, 79], ["torch.Module.__init__", "transformers.BartForConditionalGeneration.from_pretrained", "transformers.BartConfig.from_pretrained", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.waste-wood_e-care.model.discriminate_model.pretrained_model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hps", ")", ":", "\n", "        ", "super", "(", "generate_discriminate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hps", "=", "hps", "\n", "self", ".", "model", "=", "BartForConditionalGeneration", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "config", "=", "BartConfig", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "# self.linear = nn.Linear()", "\n", "self", ".", "linear1", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", ",", "self", ".", "config", ".", "hidden_size", ")", "\n", "self", ".", "linear2", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.model.multi_task_model.generate_discriminate.forward": [[80, 105], ["multi_task_model.generate_discriminate.model", "decoder_input_ids.eq", "multi_task_model.generate_discriminate.linear2().squeeze", "multi_task_model.generate_discriminate.model.generate", "[].view", "[].size", "[].size", "multi_task_model.generate_discriminate.linear2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "decoder_input_ids", ",", "decoder_mask", ",", "labels", ",", "mode", "=", "'train'", ")", ":", "\n", "        ", "if", "mode", "==", "'train'", ":", "\n", "            ", "output", "=", "self", ".", "model", "(", "input_ids", ",", "\n", "attention_mask", ",", "\n", "decoder_input_ids", "=", "decoder_input_ids", ",", "\n", "decoder_attention_mask", "=", "decoder_mask", ",", "\n", "labels", "=", "decoder_input_ids", ",", "\n", "output_hidden_states", "=", "True", ")", "\n", "eos_mask", "=", "decoder_input_ids", ".", "eq", "(", "self", ".", "config", ".", "eos_token_id", ")", "\n", "sentence_representation", "=", "output", "[", "2", "]", "[", "-", "1", "]", "[", "eos_mask", ",", ":", "]", ".", "view", "(", "output", "[", "2", "]", "[", "-", "1", "]", ".", "size", "(", "0", ")", ",", "-", "1", ",", "output", "[", "2", "]", "[", "-", "1", "]", ".", "size", "(", "-", "1", ")", ")", "[", ":", ",", "-", "1", ",", ":", "]", "\n", "# token_for_classification = output[2][:, 0, :]", "\n", "# dense = self.linear1(sentence_representation)", "\n", "score", "=", "self", ".", "linear2", "(", "sentence_representation", ")", ".", "squeeze", "(", "1", ")", "\n", "gen_logits", "=", "output", "[", "1", "]", "\n", "return", "score", ",", "gen_logits", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "model", ".", "generate", "(", "input_ids", "=", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "max_length", "=", "self", ".", "hps", ".", "length", ",", "\n", "# min_length=self.hps.length,", "\n", "# no_repeat_ngram_size=3,", "\n", "early_stopping", "=", "False", ",", "\n", "repetition_penalty", "=", "3", ",", "\n", "num_beams", "=", "self", ".", "hps", ".", "beam_size", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.model.multi_task_model.tokenize_data": [[9, 32], ["transformers.BartTokenizer.from_pretrained", "BartTokenizer.from_pretrained.", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "BartTokenizer.from_pretrained.", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.LongTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor"], "function", ["None"], ["def", "tokenize_data", "(", "hps", ",", "data", ")", ":", "\n", "    ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "# tokenizer.pad_token = '<|endoftext|>'", "\n", "input_text", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "truths", "=", "[", "]", "\n", "for", "example", "in", "data", ":", "\n", "        ", "if", "example", "[", "'ask-for'", "]", "==", "'cause'", ":", "\n", "            ", "input_text", "+=", "[", "[", "example", "[", "'alternative1'", "]", ",", "example", "[", "'premise'", "]", "]", ",", "[", "example", "[", "'alternative2'", "]", ",", "example", "[", "'premise'", "]", "]", "]", "\n", "", "else", ":", "\n", "            ", "input_text", "+=", "[", "[", "example", "[", "'premise'", "]", ",", "example", "[", "'alternative1'", "]", "]", ",", "[", "example", "[", "'premise'", "]", ",", "example", "[", "'alternative2'", "]", "]", "]", "\n", "", "labels", "+=", "[", "1", ",", "0", "]", "if", "example", "[", "'label'", "]", "==", "0", "else", "[", "0", ",", "1", "]", "\n", "truths", "+=", "[", "example", "[", "'general_truth'", "]", "]", "*", "2", "\n", "", "input_tokenized", "=", "tokenizer", "(", "input_text", ",", "padding", "=", "True", ")", "\n", "input_ids", "=", "torch", ".", "LongTensor", "(", "input_tokenized", "[", "'input_ids'", "]", ")", "\n", "attention_mask", "=", "torch", ".", "LongTensor", "(", "input_tokenized", "[", "'attention_mask'", "]", ")", "\n", "\n", "output_tokenized", "=", "tokenizer", "(", "truths", ",", "padding", "=", "True", ")", "\n", "decoder_input_ids", "=", "torch", ".", "LongTensor", "(", "output_tokenized", "[", "'input_ids'", "]", ")", "\n", "decoder_attention_mask", "=", "torch", ".", "LongTensor", "(", "output_tokenized", "[", "'attention_mask'", "]", ")", "\n", "\n", "labels", "=", "torch", ".", "FloatTensor", "(", "labels", ")", "\n", "return", "input_ids", ",", "attention_mask", ",", "decoder_input_ids", ",", "decoder_attention_mask", ",", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.model.discriminate_model.pretrained_model.__init__": [[12, 41], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "transformers.BertModel.from_pretrained", "transformers.BertConfig", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "transformers.RobertaModel.from_pretrained", "transformers.RobertaConfig", "transformers.AlbertModel.from_pretrained", "transformers.AlbertConfig.from_pretrained", "transformers.OpenAIGPTConfig.from_pretrained", "transformers.OpenAIGPTModel.from_pretrained", "transformers.BartConfig.from_pretrained", "transformers.BartForSequenceClassification.from_pretrained", "transformers.XLNetConfig.from_pretrained", "transformers.XLNetModel.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.waste-wood_e-care.model.discriminate_model.pretrained_model.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hps", ")", ":", "\n", "        ", "super", "(", "pretrained_model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "model_name", "=", "hps", ".", "model_name", "\n", "self", ".", "hps", "=", "hps", "\n", "if", "hps", ".", "model_name", "==", "'bert'", ":", "\n", "            ", "self", ".", "model", "=", "BertModel", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "config", "=", "BertConfig", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'roberta'", ":", "\n", "            ", "self", ".", "model", "=", "RobertaModel", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "config", "=", "RobertaConfig", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'albert'", ":", "\n", "            ", "self", ".", "model", "=", "AlbertModel", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "config", "=", "AlbertConfig", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'gpt'", ":", "\n", "            ", "self", ".", "config", "=", "OpenAIGPTConfig", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "model", "=", "OpenAIGPTModel", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "elif", "hps", ".", "model_name", "==", "'bart'", ":", "\n", "            ", "self", ".", "config", "=", "BartConfig", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "model", "=", "BartForSequenceClassification", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "config", "=", "XLNetConfig", ".", "from_pretrained", "(", "hps", ".", "model_dir", ")", "\n", "self", ".", "model", "=", "XLNetModel", ".", "from_pretrained", "(", "hps", ".", "model_dir", ",", "mem_len", "=", "1024", ")", "\n", "\n", "", "if", "hps", ".", "loss_func", "==", "'CrossEntropy'", ":", "\n", "            ", "self", ".", "classification", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", ",", "2", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "classification", "=", "nn", ".", "Linear", "(", "self", ".", "config", ".", "hidden_size", ",", "1", ")", "\n", "\n", "", "self", ".", "linear", "=", "nn", ".", "Linear", "(", "3", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.waste-wood_e-care.model.discriminate_model.pretrained_model.forward": [[42, 66], ["discriminate_model.pretrained_model.model", "discriminate_model.pretrained_model.model", "discriminate_model.pretrained_model.linear", "discriminate_model.pretrained_model.classification", "range", "length.cpu().tolist", "length.cpu"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "seg_ids", "=", "None", ",", "length", "=", "None", ")", ":", "\n", "\n", "# model list: Bert, ALBERT, GPT", "\n", "        ", "if", "self", ".", "model_name", "in", "[", "'bert'", ",", "'albert'", ",", "'gpt'", "]", ":", "\n", "            ", "output", "=", "self", ".", "model", "(", "input_ids", "=", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "token_type_ids", "=", "seg_ids", ")", "\n", "\n", "# model list: Roberta, XLNet", "\n", "", "else", ":", "\n", "            ", "output", "=", "self", ".", "model", "(", "input_ids", "=", "input_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "\n", "# get the cls token for classification", "\n", "", "if", "self", ".", "model_name", "in", "[", "'bert'", ",", "'roberta'", ",", "'albert'", "]", ":", "\n", "            ", "cls_token", "=", "output", "[", "1", "]", "# Bert, Roberta, ALBERT", "\n", "", "elif", "self", ".", "model_name", "==", "'gpt'", ":", "\n", "            ", "cls_token", "=", "output", "[", "0", "]", "[", "range", "(", "output", "[", "0", "]", ".", "shape", "[", "0", "]", ")", ",", "length", ".", "cpu", "(", ")", ".", "tolist", "(", ")", ",", ":", "]", "# GPT", "\n", "", "elif", "self", ".", "model_name", "==", "'xlnet'", ":", "\n", "            ", "cls_token", "=", "output", "[", "0", "]", "[", ":", ",", "-", "1", ",", ":", "]", "# XLNet", "\n", "\n", "", "if", "self", ".", "model_name", "==", "'bart'", ":", "\n", "            ", "scores", "=", "self", ".", "linear", "(", "output", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "scores", "=", "self", ".", "classification", "(", "cls_token", ")", "\n", "\n", "", "return", "scores", "\n", "\n"]]}