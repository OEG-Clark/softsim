{"home.repos.pwc.inspect_result.SingerGao_ECAC.None.main.run": [[18, 127], ["mpi_comm.Get_size", "mpi_comm.Get_rank", "env.make_pytorch_env", "env.make_pytorch_env", "env.make_pytorch_env.seed", "env.make_pytorch_env.action_space.seed", "env.make_pytorch_env.seed", "torch.manual_seed", "torch.cuda.manual_seed", "numpy.random.seed", "random.seed", "datetime.datetime.now().strftime", "os.path.join", "ECAC.ECACAgent", "itertools.count", "env.make_pytorch_env.close", "env.make_pytorch_env.close", "torch.cuda.set_device", "print", "torch.utils.tensorboard.SummaryWriter", "env.make_pytorch_env.reset", "torch.utils.tensorboard.SummaryWriter.close", "datetime.datetime.now", "env.make_pytorch_env.step", "ECAC.ECACAgent.memory.push", "env.make_pytorch_env.action_space.sample", "ECAC.ECACAgent.explore", "float", "ECAC.ECACAgent.update_parameters", "ECAC.ECACAgent.evaluate", "mpi_comm.gather", "len", "numpy.mean", "numpy.min", "numpy.max", "torch.utils.tensorboard.SummaryWriter.add_scalar", "print", "print"], "function", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.make_pytorch_env", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.make_pytorch_env", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.replay_memory.ReplayMemory.push", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.replay_memory.ReplayMemory.sample", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.explore", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.update_parameters", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.evaluate"], ["def", "run", "(", "args", ")", ":", "\n", "\n", "# Set MPI.", "\n", "    ", "mpi_comm", "=", "MPI", ".", "COMM_WORLD", "\n", "mpi_size", "=", "mpi_comm", ".", "Get_size", "(", ")", "\n", "mpi_rank", "=", "mpi_comm", ".", "Get_rank", "(", ")", "\n", "\n", "# Set GPU device.", "\n", "if", "args", ".", "cuda", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "GPU_id", ")", "\n", "print", "(", "'Program is running on GPU {}'", ".", "format", "(", "args", ".", "GPU_id", ")", ")", "\n", "\n", "# Create environments.", "\n", "", "env", "=", "make_pytorch_env", "(", "args", ".", "env_name", ",", "clip_rewards", "=", "False", ")", "\n", "test_env", "=", "make_pytorch_env", "(", "\n", "args", ".", "env_name", ",", "episode_life", "=", "False", ",", "clip_rewards", "=", "False", ")", "\n", "\n", "# Set random seed.", "\n", "SEED", "=", "(", "args", ".", "seed", "+", "mpi_rank", ")", "*", "31", "\n", "env", ".", "seed", "(", "SEED", ")", "\n", "env", ".", "action_space", ".", "seed", "(", "SEED", ")", "\n", "test_env", ".", "seed", "(", "SEED", ")", "\n", "torch", ".", "manual_seed", "(", "SEED", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "SEED", ")", "\n", "np", ".", "random", ".", "seed", "(", "SEED", ")", "\n", "random", ".", "seed", "(", "SEED", ")", "\n", "\n", "# Set the number of max episode steps.", "\n", "try", ":", "\n", "        ", "max_episode_steps", "=", "env", ".", "_max_episode_steps", "\n", "", "except", ":", "\n", "        ", "max_episode_steps", "=", "args", ".", "max_episode_steps", "\n", "\n", "# Specify directory for logging.", "\n", "", "name", "=", "args", ".", "env_name", "\n", "time", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d_%H-%M-%S\"", ")", "\n", "log_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "'logs'", ",", "args", ".", "env_name", ",", "f'{time}_lr-{args.lr}_kl-{args.kl_target}_rs-{args.reward_scale}_limit-kl-{args.limit_kl}'", ")", "\n", "\n", "# Create a ECAC agent.", "\n", "agent", "=", "ECACAgent", "(", "env", ",", "test_env", ",", "log_dir", "+", "f'_{mpi_rank}'", ",", "args", ".", "batch_size", ",", "args", ".", "lr", ",", "\n", "memory_size", "=", "args", ".", "memory_size", ",", "\n", "hidden_dim", "=", "args", ".", "hidden_dim", ",", "gamma", "=", "args", ".", "gamma", ",", "tau", "=", "args", ".", "tau", ",", "\n", "limit_kl", "=", "args", ".", "limit_kl", ",", "kl_target", "=", "args", ".", "kl_target", ",", "\n", "update_interval", "=", "args", ".", "update_interval", ",", "\n", "Q_updates_per_step", "=", "args", ".", "Q_updates_per_step", ",", "\n", "num_steps", "=", "args", ".", "num_steps", ",", "max_episode_steps", "=", "max_episode_steps", ",", "\n", "log_interval", "=", "args", ".", "log_interval", ",", "cuda", "=", "args", ".", "cuda", ")", "\n", "\n", "# Set directory for results and create tensorboard writers.", "\n", "if", "mpi_rank", "==", "0", ":", "\n", "        ", "tb_writer", "=", "SummaryWriter", "(", "log_dir", "+", "'-Main'", ")", "\n", "\n", "# Training Loop", "\n", "", "total_numsteps", "=", "0", "# Counting total actions steps.", "\n", "for", "i_episode", "in", "itertools", ".", "count", "(", "1", ")", ":", "\n", "        ", "episode_reward", "=", "0", "# Total reward in one episode.", "\n", "episode_steps", "=", "0", "# Total steps in one episode.", "\n", "done", "=", "False", "# Episode end flag.", "\n", "state", "=", "env", ".", "reset", "(", ")", "# Reset the env.", "\n", "\n", "while", "not", "done", ":", "# Run until episode ends.", "\n", "            ", "if", "args", ".", "start_steps", ">", "total_numsteps", ":", "\n", "                ", "action", "=", "env", ".", "action_space", ".", "sample", "(", ")", "# Sample action from uniform distibution.", "\n", "", "else", ":", "\n", "                ", "action", "=", "agent", ".", "explore", "(", "state", ")", "# Sample action from policy.", "\n", "\n", "", "next_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "# Interact with env.", "\n", "episode_steps", "+=", "1", "\n", "total_numsteps", "+=", "1", "\n", "episode_reward", "+=", "reward", "\n", "\n", "# Ignore the \"done\" signal if it comes from hitting the time horizon.", "\n", "mask", "=", "1", "if", "episode_steps", "==", "max_episode_steps", "else", "float", "(", "not", "done", ")", "\n", "# Append the transition to memory", "\n", "scaled_reward", "=", "reward", "*", "args", ".", "reward_scale", "\n", "agent", ".", "memory", ".", "push", "(", "state", ",", "action", ",", "scaled_reward", ",", "next_state", ",", "mask", ")", "\n", "state", "=", "next_state", "\n", "\n", "# Update all parameters of the networks.", "\n", "if", "len", "(", "agent", ".", "memory", ")", ">", "args", ".", "batch_size", "and", "total_numsteps", "%", "args", ".", "update_interval", "==", "0", "and", "total_numsteps", ">=", "args", ".", "start_steps", ":", "\n", "                ", "agent", ".", "update_parameters", "(", ")", "\n", "\n", "# Testing.", "\n", "", "if", "total_numsteps", "%", "args", ".", "eval_interval", "==", "0", ":", "\n", "                ", "test_return", "=", "agent", ".", "evaluate", "(", ")", "\n", "test_returns", "=", "mpi_comm", ".", "gather", "(", "test_return", ",", "root", "=", "0", ")", "\n", "if", "mpi_rank", "==", "0", ":", "\n", "# calculate mean, min, and max for plotting.", "\n", "                    ", "test_mean_return", "=", "np", ".", "mean", "(", "test_returns", ")", "\n", "test_min_return", "=", "np", ".", "min", "(", "test_returns", ")", "\n", "test_max_return", "=", "np", ".", "max", "(", "test_returns", ")", "\n", "tb_writer", ".", "add_scalar", "(", "'reward/mean_return'", ",", "test_mean_return", ",", "total_numsteps", ")", "\n", "print", "(", "f'Num steps: {total_numsteps:<5}  '", "\n", "f'Min return: {test_min_return:<5.1f}  '", "\n", "f'Mean return: {test_mean_return:<5.1f}  '", "\n", "f'Max return: {test_max_return:<5.1f}  '", ")", "\n", "print", "(", "'-'", "*", "60", ")", "\n", "\n", "", "", "", "if", "total_numsteps", ">", "args", ".", "num_steps", ":", "# Break training loop.", "\n", "            ", "break", "\n", "\n", "", "", "env", ".", "close", "(", ")", "\n", "test_env", ".", "close", "(", ")", "\n", "\n", "if", "mpi_rank", "==", "0", ":", "\n", "        ", "tb_writer", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.arguments.get_args": [[4, 122], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.cuda.is_available"], "function", ["None"], ["def", "get_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'PyTorch Error controlled Actor-Critic Args'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--env-name'", ",", "\n", "default", "=", "\"Hopper-v2\"", ",", "\n", "help", "=", "'Gym environment (default: Hopper-v2)'", ")", "# environment name", "\n", "parser", ".", "add_argument", "(", "\n", "'--gamma'", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "0.99", ",", "\n", "metavar", "=", "'G'", ",", "\n", "help", "=", "'discount factor for reward (default: 0.99)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--tau'", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "0.005", ",", "\n", "metavar", "=", "'G'", ",", "\n", "help", "=", "'target smoothing coefficient(\u03c4) (default: 0.005)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--lr'", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "1e-3", ",", "\n", "metavar", "=", "'G'", ",", "\n", "help", "=", "'The learning rate (default: 1e-3)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--kl_target'", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "0.5", ",", "\n", "metavar", "=", "'G'", ",", "\n", "help", "=", "'Target KL (default: 0.5)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--reward_scale'", ",", "\n", "type", "=", "float", ",", "\n", "default", "=", "1", ",", "\n", "metavar", "=", "'G'", ",", "\n", "help", "=", "'Reward scale (default: 1)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--batch_size'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "128", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'batch size (default: 128)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--num_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1000001", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'maximum number of steps (default: 1000000)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--hidden_dim'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "256", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'hidden dim of Multi-layer perceptron (default: 256)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--Q_updates_per_step'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Q function updates per step (default: 1)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--max_episode_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1000", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Steps sampling random actions (default: 1000)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--start_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "5000", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'Steps sampling random actions (default: 5000)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--memory_size'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "500000", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'size of replay buffer (default: 500000)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--update_interval'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "2", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'update interval (default: 2)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--eval_interval'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1000", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'evaluation interval (default: 1000)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--log_interval'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "10", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'log interval (default: 10)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--limit_kl'", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'enable kl limitation (default: False)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--cuda'", ",", "\n", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "'run on CUDA (default: False)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--GPU-id'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "0", ",", "\n", "metavar", "=", "'N'", ",", "\n", "help", "=", "'run on GPU x (default: 0)'", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'--seed'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'random seed (default: 1)'", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "args", ".", "cuda", "=", "args", ".", "cuda", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "\n", "return", "args", "\n", "", ""]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.replay_memory.ReplayMemory.__init__": [[6, 11], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "capacity", ",", "device", ")", ":", "\n", "        ", "self", ".", "capacity", "=", "capacity", "# \u6700\u5927\u5bb9\u91cf", "\n", "self", ".", "buffer", "=", "[", "]", "# \u7528list\u5b58", "\n", "self", ".", "position", "=", "0", "\n", "self", ".", "device", "=", "device", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.replay_memory.ReplayMemory.push": [[12, 17], ["len", "replay_memory.ReplayMemory.buffer.append"], "methods", ["None"], ["", "def", "push", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "mask", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "buffer", ")", "<", "self", ".", "capacity", ":", "\n", "            ", "self", ".", "buffer", ".", "append", "(", "None", ")", "\n", "", "self", ".", "buffer", "[", "self", ".", "position", "]", "=", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "mask", ")", "\n", "self", ".", "position", "=", "(", "self", ".", "position", "+", "1", ")", "%", "self", ".", "capacity", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.replay_memory.ReplayMemory.sample": [[18, 29], ["random.sample", "map", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "zip", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor", "torch.FloatTensor"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.replay_memory.ReplayMemory.sample"], ["", "def", "sample", "(", "self", ",", "batch_size", ")", ":", "\n", "        ", "batch", "=", "random", ".", "sample", "(", "self", ".", "buffer", ",", "batch_size", ")", "# \u968f\u673a\u91c7\u96c6\u6837\u672c", "\n", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "masks", "=", "map", "(", "np", ".", "stack", ",", "zip", "(", "*", "batch", ")", ")", "\n", "\n", "states", "=", "torch", ".", "FloatTensor", "(", "states", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "next_states", "=", "torch", ".", "FloatTensor", "(", "next_states", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "actions", "=", "torch", ".", "FloatTensor", "(", "actions", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "rewards", "=", "torch", ".", "FloatTensor", "(", "rewards", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "1", ")", "\n", "masks", "=", "torch", ".", "FloatTensor", "(", "masks", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "1", ")", "\n", "\n", "return", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "masks", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.replay_memory.ReplayMemory.__len__": [[30, 32], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "buffer", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.__init__": [[11, 13], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "BaseNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save": [[14, 16], ["torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "model.BaseNetwork.state_dict"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save"], ["", "def", "save", "(", "self", ",", "path", ")", ":", "\n", "        ", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load": [[17, 19], ["model.BaseNetwork.load_state_dict", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.load"], ["", "def", "load", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "load_state_dict", "(", "torch", ".", "load", "(", "path", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MLPBase.__init__": [[21, 25], ["model.BaseNetwork.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "num_ouputs", ",", "hidden_dim", "=", "256", ")", ":", "\n", "        ", "super", "(", "MLPBase", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "linear1", "=", "nn", ".", "Linear", "(", "num_inputs", ",", "hidden_dim", ")", "\n", "self", ".", "linear2", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "num_ouputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MLPBase.forward": [[26, 30], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "model.MLPBase.linear1", "model.MLPBase.linear2"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "linear1", "(", "x", ")", ",", "inplace", "=", "False", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "linear2", "(", "x", ")", ",", "inplace", "=", "False", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoDoubleQNetwork.__init__": [[32, 42], ["model.BaseNetwork.__init__", "model.MLPBase", "torch.Linear", "torch.Linear", "torch.Linear", "model.MLPBase", "torch.Linear", "torch.Linear", "torch.Linear", "model.MujocoDoubleQNetwork.apply"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_obs", ",", "num_actions", ",", "hidden_dim", "=", "256", ")", ":", "\n", "        ", "super", "(", "MujocoDoubleQNetwork", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# Q1 architecture", "\n", "self", ".", "q1_base", "=", "MLPBase", "(", "num_obs", "+", "num_actions", ",", "hidden_dim", ")", "\n", "self", ".", "q1_out_layer", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "1", ")", "\n", "# Q2 architecture", "\n", "self", ".", "q2_base", "=", "MLPBase", "(", "num_obs", "+", "num_actions", ",", "hidden_dim", ")", "\n", "self", ".", "q2_out_layer", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "1", ")", "\n", "\n", "self", ".", "apply", "(", "weights_init_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoDoubleQNetwork.forward": [[43, 52], ["torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.relu", "torch.relu", "torch.relu", "model.MujocoDoubleQNetwork.q1_out_layer", "torch.relu", "torch.relu", "torch.relu", "model.MujocoDoubleQNetwork.q2_out_layer", "model.MujocoDoubleQNetwork.q1_base", "model.MujocoDoubleQNetwork.q2_base"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "\n", "        ", "xu", "=", "torch", ".", "cat", "(", "[", "state", ",", "action", "]", ",", "1", ")", "\n", "x1", "=", "F", ".", "relu", "(", "self", ".", "q1_base", "(", "xu", ")", ",", "inplace", "=", "False", ")", "\n", "x1", "=", "self", ".", "q1_out_layer", "(", "x1", ")", "\n", "x2", "=", "F", ".", "relu", "(", "self", ".", "q2_base", "(", "xu", ")", ",", "inplace", "=", "False", ")", "\n", "x2", "=", "self", ".", "q2_out_layer", "(", "x2", ")", "\n", "\n", "return", "x1", ",", "x2", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.__init__": [[58, 68], ["model.BaseNetwork.__init__", "model.MLPBase", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "model.MujocoPolicy.apply"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_obs", ",", "num_actions", ",", "action_scale", ",", "action_bias", ",", "hidden_dim", "=", "256", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "action_scale", "=", "action_scale", "\n", "self", ".", "action_bias", "=", "action_bias", "\n", "self", ".", "base", "=", "MLPBase", "(", "num_obs", ",", "hidden_dim", ")", "\n", "self", ".", "mean_layer", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "num_actions", ")", "\n", "self", ".", "log_std_layer", "=", "nn", ".", "Linear", "(", "hidden_dim", ",", "num_actions", ")", "\n", "\n", "self", ".", "apply", "(", "weights_init_", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.forward": [[69, 75], ["model.MujocoPolicy.base", "model.MujocoPolicy.mean_layer", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal", "model.MujocoPolicy.log_std_layer", "torch.clamp.exp", "torch.clamp.exp", "torch.clamp.exp"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "state", ")", ":", "\n", "        ", "x", "=", "self", ".", "base", "(", "state", ")", "\n", "mean", "=", "self", ".", "mean_layer", "(", "x", ")", "\n", "logstd", "=", "torch", ".", "clamp", "(", "self", ".", "log_std_layer", "(", "x", ")", ",", "min", "=", "LOG_SIG_MIN", ",", "max", "=", "LOG_SIG_MAX", ")", "\n", "action_dist", "=", "Normal", "(", "mean", ",", "logstd", ".", "exp", "(", ")", ")", "\n", "return", "action_dist", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.greedy_action": [[76, 81], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "model.MujocoPolicy.forward", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.forward"], ["", "def", "greedy_action", "(", "self", ",", "states", ")", ":", "# sample actions mainly for trianing Q function.", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "pi_action_dist", "=", "self", ".", "forward", "(", "states", ")", "\n", "", "greedy_actions", "=", "torch", ".", "tanh", "(", "pi_action_dist", ".", "mean", ")", "*", "self", ".", "action_scale", "+", "self", ".", "action_bias", "\n", "return", "greedy_actions", "# mean actions", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.sample_random_action": [[82, 97], ["model.MujocoPolicy.forward", "model.MujocoPolicy.rsample", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "policy2", "model.MujocoPolicy.log_prob().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "policy2.log_prob().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "torch.log().sum", "pi1_log_prob.mean", "pi2_log_prob.mean", "model.MujocoPolicy.log_prob", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "policy2.log_prob", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.log", "torch.tanh.pow", "torch.tanh.pow", "torch.tanh.pow", "torch.tanh.pow", "torch.tanh.pow", "torch.tanh.pow"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.forward"], ["", "def", "sample_random_action", "(", "self", ",", "states", ",", "policy2", "=", "None", ")", ":", "# sample actions for trianing policy.", "\n", "        ", "pi1_action_dist", "=", "self", ".", "forward", "(", "states", ")", "\n", "x_t", "=", "pi1_action_dist", ".", "rsample", "(", ")", "\n", "y_t", "=", "torch", ".", "tanh", "(", "x_t", ")", "# \u5c06\u8f93\u51fa\u538b\u7f29\u5728(-1,1)", "\n", "actions", "=", "y_t", "*", "self", ".", "action_scale", "+", "self", ".", "action_bias", "\n", "if", "policy2", "!=", "None", ":", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "pi2_action_dist", "=", "policy2", "(", "states", ")", "\n", "", "pi1_log_prob", "=", "pi1_action_dist", ".", "log_prob", "(", "x_t", ")", ".", "sum", "(", "-", "1", ",", "keepdim", "=", "True", ")", "-", "torch", ".", "log", "(", "self", ".", "action_scale", "*", "(", "1", "-", "y_t", ".", "pow", "(", "2", ")", ")", "+", "epsilon", ")", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "pi2_log_prob", "=", "pi2_action_dist", ".", "log_prob", "(", "x_t", ")", ".", "sum", "(", "-", "1", ",", "keepdim", "=", "True", ")", "-", "torch", ".", "log", "(", "self", ".", "action_scale", "*", "(", "1", "-", "y_t", ".", "pow", "(", "2", ")", ")", "+", "epsilon", ")", ".", "sum", "(", "1", ",", "keepdim", "=", "True", ")", "\n", "entropies", "=", "-", "pi1_log_prob", ".", "mean", "(", ")", "\n", "cross_entropies", "=", "-", "pi2_log_prob", ".", "mean", "(", ")", "\n", "return", "actions", ",", "entropies", ",", "cross_entropies", "\n", "", "else", ":", "\n", "            ", "return", "actions", "# random actions", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.__init__": [[16, 83], ["torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "torch.device", "print", "os.path.join", "os.path.join", "torch.utils.tensorboard.SummaryWriter", "torch.utils.tensorboard.SummaryWriter", "torch.utils.tensorboard.SummaryWriter", "replay_memory.ReplayMemory", "model.MujocoDoubleQNetwork().to", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "model.MujocoDoubleQNetwork().to().eval", "ECAC.ECACAgent.update_target", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "model.MujocoPolicy().to", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "model.MujocoPolicy().to().eval", "ECAC.ECACAgent.backup_policy", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.optim.Adam", "torch.optim.Adam", "torch.optim.Adam", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "ECAC.ECACAgent.critic_online.parameters", "ECAC.ECACAgent.policy.parameters", "torch.prod().item", "torch.prod().item", "torch.prod().item", "torch.prod().item", "torch.prod().item", "torch.prod().item", "torch.prod().item", "torch.prod().item", "torch.prod().item", "model.MujocoDoubleQNetwork", "model.MujocoDoubleQNetwork().to", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "model.MujocoPolicy", "model.MujocoPolicy().to", "torch.prod", "torch.prod", "torch.prod", "torch.prod", "torch.prod", "torch.prod", "torch.prod", "torch.prod", "torch.prod", "model.MujocoDoubleQNetwork", "model.MujocoPolicy", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor().to", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.update_target", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.backup_policy"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "test_env", ",", "log_dir", ",", "batch_size", "=", "64", ",", "lr", "=", "0.0003", ",", "\n", "memory_size", "=", "1000000", ",", "hidden_dim", "=", "256", ",", "\n", "gamma", "=", "0.99", ",", "tau", "=", "0.005", ",", "limit_kl", "=", "False", ",", "kl_target", "=", "0.002", ",", "\n", "update_interval", "=", "4", ",", "Q_updates_per_step", "=", "2", ",", "num_steps", "=", "1000001", ",", "\n", "max_episode_steps", "=", "1000", ",", "log_interval", "=", "10", ",", "cuda", "=", "True", ")", ":", "\n", "\n", "        ", "self", ".", "env", "=", "env", "\n", "self", ".", "test_env", "=", "test_env", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "tau", "=", "tau", "\n", "self", ".", "Q_updates_per_step", "=", "Q_updates_per_step", "\n", "self", ".", "num_steps", "=", "num_steps", "\n", "self", ".", "log_interval", "=", "log_interval", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "cuda", "else", "\"cpu\"", ")", "\n", "self", ".", "max_episode_steps", "=", "max_episode_steps", "\n", "self", ".", "max_grad_norm", "=", "0.5", "\n", "self", ".", "target_kl", "=", "kl_target", "\n", "self", ".", "target_entropy", "=", "-", "torch", ".", "prod", "(", "torch", ".", "Tensor", "(", "self", ".", "env", ".", "action_space", ".", "shape", ")", ".", "to", "(", "self", ".", "device", ")", ")", ".", "item", "(", ")", "/", "2.0", "\n", "print", "(", "f\"target_kl: {self.target_kl}\"", ")", "\n", "self", ".", "limit_kl", "=", "limit_kl", "\n", "if", "self", ".", "limit_kl", ":", "\n", "            ", "self", ".", "log_alpha", "=", "torch", ".", "ones", "(", "1", ",", "requires_grad", "=", "True", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "alpha_optim", "=", "Adam", "(", "[", "self", ".", "log_alpha", "]", ",", "lr", "=", "lr", ")", "\n", "self", ".", "log_rho", "=", "torch", ".", "ones", "(", "1", ",", "requires_grad", "=", "True", ",", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "rho_optim", "=", "Adam", "(", "[", "self", ".", "log_rho", "]", ",", "lr", "=", "lr", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "alpha", "=", "0", "\n", "self", ".", "rho", "=", "0", "\n", "# Specify the directory for logging.", "\n", "", "self", ".", "model_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "'model'", ")", "\n", "self", ".", "summary_dir", "=", "os", ".", "path", ".", "join", "(", "log_dir", ",", "'summary'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "model_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "model_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "summary_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "summary_dir", ")", "\n", "\n", "# Create tensorboard.", "\n", "", "self", ".", "writer", "=", "SummaryWriter", "(", "log_dir", "=", "self", ".", "summary_dir", ")", "\n", "\n", "# Create replay buffer.", "\n", "self", ".", "memory", "=", "ReplayMemory", "(", "memory_size", ",", "self", ".", "device", ")", "\n", "\n", "# Create critic networks.", "\n", "self", ".", "critic_online", "=", "MujocoDoubleQNetwork", "(", "env", ".", "observation_space", ".", "shape", "[", "0", "]", ",", "\n", "env", ".", "action_space", ".", "shape", "[", "0", "]", ",", "hidden_dim", ")", ".", "to", "(", "device", "=", "self", ".", "device", ")", "\n", "self", ".", "critic_optim", "=", "Adam", "(", "self", ".", "critic_online", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "self", ".", "critic_target", "=", "MujocoDoubleQNetwork", "(", "env", ".", "observation_space", ".", "shape", "[", "0", "]", ",", "\n", "env", ".", "action_space", ".", "shape", "[", "0", "]", ",", "hidden_dim", ")", ".", "to", "(", "device", "=", "self", ".", "device", ")", ".", "eval", "(", ")", "\n", "self", ".", "update_target", "(", ")", "\n", "\n", "# Record the scale and bias of action space.", "\n", "self", ".", "action_scale", "=", "torch", ".", "FloatTensor", "(", "\n", "(", "env", ".", "action_space", ".", "high", "-", "env", ".", "action_space", ".", "low", ")", "/", "2.", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "action_bias", "=", "torch", ".", "FloatTensor", "(", "\n", "(", "env", ".", "action_space", ".", "high", "+", "env", ".", "action_space", ".", "low", ")", "/", "2.", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Create policy networks.", "\n", "self", ".", "policy", "=", "MujocoPolicy", "(", "env", ".", "observation_space", ".", "shape", "[", "0", "]", ",", "env", ".", "action_space", ".", "shape", "[", "0", "]", ",", "\n", "self", ".", "action_scale", ",", "self", ".", "action_bias", ",", "hidden_dim", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "policy_optim", "=", "Adam", "(", "self", ".", "policy", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "self", ".", "old_policy", "=", "MujocoPolicy", "(", "env", ".", "observation_space", ".", "shape", "[", "0", "]", ",", "env", ".", "action_space", ".", "shape", "[", "0", "]", ",", "\n", "self", ".", "action_scale", ",", "self", ".", "action_bias", ",", "hidden_dim", ")", ".", "to", "(", "self", ".", "device", ")", ".", "eval", "(", ")", "\n", "self", ".", "backup_policy", "(", ")", "\n", "\n", "self", ".", "learning_steps", "=", "0", "\n", "self", ".", "best_eval_score", "=", "-", "np", ".", "inf", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.backup_policy": [[84, 86], ["ECAC.ECACAgent.old_policy.load_state_dict", "ECAC.ECACAgent.policy.state_dict"], "methods", ["None"], ["", "def", "backup_policy", "(", "self", ")", ":", "\n", "        ", "self", ".", "old_policy", ".", "load_state_dict", "(", "self", ".", "policy", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.update_target": [[87, 89], ["ECAC.ECACAgent.critic_target.load_state_dict", "ECAC.ECACAgent.critic_online.state_dict"], "methods", ["None"], ["", "def", "update_target", "(", "self", ")", ":", "\n", "        ", "self", ".", "critic_target", ".", "load_state_dict", "(", "self", ".", "critic_online", ".", "state_dict", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.explore": [[90, 95], ["torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ECAC.ECACAgent.policy.sample_random_action", "ECAC.ECACAgent.detach().cpu().numpy", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "ECAC.ECACAgent.detach().cpu", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "ECAC.ECACAgent.detach"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.sample_random_action"], ["", "def", "explore", "(", "self", ",", "state", ")", ":", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "action", "=", "self", ".", "policy", ".", "sample_random_action", "(", "state", ")", "\n", "", "return", "action", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.exploit": [[96, 101], ["torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.FloatTensor().to().unsqueeze", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ECAC.ECACAgent.policy.greedy_action", "ECAC.ECACAgent.detach().cpu().numpy", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "torch.FloatTensor().to", "ECAC.ECACAgent.detach().cpu", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "ECAC.ECACAgent.detach"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.greedy_action"], ["", "def", "exploit", "(", "self", ",", "state", ")", ":", "\n", "        ", "state", "=", "torch", ".", "FloatTensor", "(", "state", ")", ".", "to", "(", "self", ".", "device", ")", ".", "unsqueeze", "(", "0", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "action", "=", "self", ".", "policy", ".", "greedy_action", "(", "state", ")", "\n", "", "return", "action", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "[", "0", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.calc_current_q": [[102, 105], ["ECAC.ECACAgent.critic_online"], "methods", ["None"], ["", "def", "calc_current_q", "(", "self", ",", "states", ",", "actions", ")", ":", "\n", "        ", "curr_q1", ",", "curr_q2", "=", "self", ".", "critic_online", "(", "states", ",", "actions", ")", "\n", "return", "curr_q1", ",", "curr_q2", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.calc_target_q": [[106, 112], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ECAC.ECACAgent.policy.sample_random_action", "ECAC.ECACAgent.critic_target", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.sample_random_action"], ["", "def", "calc_target_q", "(", "self", ",", "rewards", ",", "next_states", ",", "masks", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "next_action_mean", "=", "self", ".", "policy", ".", "sample_random_action", "(", "next_states", ")", "\n", "next_q1", ",", "next_q2", "=", "self", ".", "critic_target", "(", "next_states", ",", "next_action_mean", ")", "\n", "next_q", "=", "torch", ".", "min", "(", "next_q1", ",", "next_q2", ")", "\n", "", "return", "rewards", "+", "masks", "*", "self", ".", "gamma", "*", "next_q", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.calc_critic_loss": [[113, 122], ["ECAC.ECACAgent.calc_current_q", "ECAC.ECACAgent.calc_target_q", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss", "torch.mse_loss"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.calc_current_q", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.calc_target_q"], ["", "def", "calc_critic_loss", "(", "self", ",", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "masks", ")", ":", "\n", "        ", "curr_q1", ",", "curr_q2", "=", "self", ".", "calc_current_q", "(", "states", ",", "actions", ")", "\n", "target_q", "=", "self", ".", "calc_target_q", "(", "rewards", ",", "next_states", ",", "masks", ")", "\n", "\n", "# Critic loss is mean squared TD errors.", "\n", "q1_loss", "=", "F", ".", "mse_loss", "(", "curr_q1", ",", "target_q", ")", "\n", "q2_loss", "=", "F", ".", "mse_loss", "(", "curr_q2", ",", "target_q", ")", "\n", "\n", "return", "q1_loss", ",", "q2_loss", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.calc_policy_loss": [[123, 136], ["ECAC.ECACAgent.policy.sample_random_action", "ECAC.ECACAgent.critic_online", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "torch.exp().detach", "kl.detach", "entropies.detach", "cross_entropies.detach", "policy_grad_term.detach", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.MujocoPolicy.sample_random_action"], ["", "def", "calc_policy_loss", "(", "self", ",", "states", ",", "updates", ")", ":", "\n", "        ", "actions", ",", "entropies", ",", "cross_entropies", "=", "self", ".", "policy", ".", "sample_random_action", "(", "states", ",", "self", ".", "old_policy", ")", "# Sample actions for training policy.", "\n", "qf1", ",", "qf2", "=", "self", ".", "critic_online", "(", "states", ",", "actions", ")", "\n", "policy_grad_term", "=", "(", "-", "qf1", ")", ".", "mean", "(", ")", "\n", "kl", "=", "cross_entropies", "-", "entropies", "\n", "if", "self", ".", "limit_kl", ":", "\n", "#self.alpha = 0", "\n", "            ", "self", ".", "alpha", "=", "torch", ".", "exp", "(", "self", ".", "log_alpha", ")", ".", "detach", "(", ")", "\n", "self", ".", "rho", "=", "torch", ".", "exp", "(", "self", ".", "log_rho", ")", ".", "detach", "(", ")", "\n", "policy_loss", "=", "policy_grad_term", "+", "self", ".", "alpha", "*", "cross_entropies", "-", "self", ".", "rho", "*", "entropies", "\n", "", "else", ":", "\n", "            ", "policy_loss", "=", "policy_grad_term", "\n", "", "return", "policy_loss", ",", "kl", ".", "detach", "(", ")", ",", "entropies", ".", "detach", "(", ")", ",", "cross_entropies", ".", "detach", "(", ")", ",", "policy_grad_term", ".", "detach", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.update_parameters": [[137, 187], ["range", "ECAC.ECACAgent.calc_policy_loss", "ECAC.ECACAgent.backup_policy", "ECAC.ECACAgent.policy_optim.zero_grad", "policy_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "ECAC.ECACAgent.policy_optim.step", "utils.soft_update", "ECAC.ECACAgent.memory.sample", "ECAC.ECACAgent.critic_optim.zero_grad", "ECAC.ECACAgent.calc_critic_loss", "qf1_loss.backward", "qf2_loss.backward", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "torch.nn.utils.clip_grad_norm_", "ECAC.ECACAgent.critic_optim.step", "ECAC.ECACAgent.alpha_optim.zero_grad", "alpha_loss.backward", "ECAC.ECACAgent.alpha_optim.step", "ECAC.ECACAgent.alpha_optim.zero_grad", "rho_loss.backward", "ECAC.ECACAgent.rho_optim.step", "ECAC.ECACAgent.rho_optim.zero_grad", "ECAC.ECACAgent.policy.parameters", "ECAC.ECACAgent.writer.add_scalar", "ECAC.ECACAgent.writer.add_scalar", "ECAC.ECACAgent.writer.add_scalar", "ECAC.ECACAgent.writer.add_scalar", "ECAC.ECACAgent.writer.add_scalar", "ECAC.ECACAgent.critic_online.parameters", "qf1_loss.detach().item", "policy_loss.detach().item", "qf1_loss.detach", "policy_loss.detach"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.calc_policy_loss", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.backup_policy", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.utils.soft_update", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.replay_memory.ReplayMemory.sample", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.calc_critic_loss", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step"], ["", "def", "update_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "learning_steps", "+=", "1", "\n", "# Update Q networks.", "\n", "for", "i", "in", "range", "(", "self", ".", "Q_updates_per_step", ")", ":", "\n", "# Sample a batch from memory.", "\n", "            ", "states", ",", "actions", ",", "rewards", ",", "next_states", ",", "masks", "=", "self", ".", "memory", ".", "sample", "(", "\n", "batch_size", "=", "self", ".", "batch_size", ")", "\n", "self", ".", "critic_optim", ".", "zero_grad", "(", ")", "\n", "qf1_loss", ",", "qf2_loss", "=", "self", ".", "calc_critic_loss", "(", "states", ",", "actions", ",", "rewards", ",", "\n", "next_states", ",", "masks", ")", "\n", "qf1_loss", ".", "backward", "(", ")", "\n", "qf2_loss", ".", "backward", "(", ")", "\n", "clip_grad_norm_", "(", "self", ".", "critic_online", ".", "parameters", "(", ")", ",", "max_norm", "=", "self", ".", "max_grad_norm", ")", "\n", "self", ".", "critic_optim", ".", "step", "(", ")", "\n", "\n", "", "policy_loss", ",", "kl", ",", "entropies", ",", "cross_entropies", ",", "policy_term", "=", "self", ".", "calc_policy_loss", "(", "states", ",", "self", ".", "learning_steps", ")", "\n", "# Update coefficents", "\n", "if", "self", ".", "limit_kl", ":", "\n", "            ", "self", ".", "alpha_optim", ".", "zero_grad", "(", ")", "\n", "alpha_loss", "=", "self", ".", "log_alpha", "*", "(", "(", "self", ".", "target_kl", "-", "self", ".", "target_entropy", ")", "-", "cross_entropies", ")", "\n", "alpha_loss", ".", "backward", "(", ")", "\n", "self", ".", "alpha_optim", ".", "step", "(", ")", "\n", "self", ".", "alpha_optim", ".", "zero_grad", "(", ")", "\n", "rho_loss", "=", "self", ".", "log_rho", "*", "(", "entropies", "-", "self", ".", "target_entropy", ")", "\n", "rho_loss", ".", "backward", "(", ")", "\n", "self", ".", "rho_optim", ".", "step", "(", ")", "\n", "self", ".", "rho_optim", ".", "zero_grad", "(", ")", "\n", "# Update policy networks.", "\n", "", "self", ".", "backup_policy", "(", ")", "\n", "self", ".", "policy_optim", ".", "zero_grad", "(", ")", "\n", "policy_loss", ".", "backward", "(", ")", "\n", "clip_grad_norm_", "(", "self", ".", "policy", ".", "parameters", "(", ")", ",", "max_norm", "=", "self", ".", "max_grad_norm", ")", "\n", "self", ".", "policy_optim", ".", "step", "(", ")", "\n", "\n", "# Soft update target critic network.", "\n", "soft_update", "(", "self", ".", "critic_target", ",", "self", ".", "critic_online", ",", "self", ".", "tau", ")", "\n", "# Log training information.", "\n", "if", "self", ".", "learning_steps", "%", "self", ".", "log_interval", "==", "0", "and", "self", ".", "learning_steps", ">", "100", ":", "\n", "            ", "self", ".", "writer", ".", "add_scalar", "(", "\n", "'loss/Q1'", ",", "qf1_loss", ".", "detach", "(", ")", ".", "item", "(", ")", ",", "\n", "self", ".", "learning_steps", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\n", "'loss/policy'", ",", "policy_loss", ".", "detach", "(", ")", ".", "item", "(", ")", ",", "\n", "self", ".", "learning_steps", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\n", "'stats/kl'", ",", "kl", ",", "self", ".", "learning_steps", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\n", "'stats/entropies'", ",", "entropies", ",", "self", ".", "learning_steps", ")", "\n", "self", ".", "writer", ".", "add_scalar", "(", "\n", "'stats/cross_entropies'", ",", "cross_entropies", ",", "self", ".", "learning_steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.evaluate": [[188, 206], ["range", "ECAC.ECACAgent.test_env.reset", "ECAC.ECACAgent.exploit", "ECAC.ECACAgent.test_env.step"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.exploit", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step"], ["", "", "def", "evaluate", "(", "self", ",", "num_episodes", "=", "1", ")", ":", "\n", "        ", "num_steps", "=", "0", "\n", "total_return", "=", "0.0", "\n", "for", "i_episode", "in", "range", "(", "num_episodes", ")", ":", "\n", "            ", "state", "=", "self", ".", "test_env", ".", "reset", "(", ")", "\n", "episode_steps", "=", "0", "\n", "episode_return", "=", "0.0", "\n", "done", "=", "False", "\n", "while", "(", "not", "done", ")", "and", "episode_steps", "<=", "self", ".", "max_episode_steps", ":", "\n", "                ", "action", "=", "self", ".", "exploit", "(", "state", ")", "\n", "next_state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "test_env", ".", "step", "(", "action", ")", "\n", "num_steps", "+=", "1", "\n", "episode_steps", "+=", "1", "\n", "episode_return", "+=", "reward", "\n", "state", "=", "next_state", "\n", "", "total_return", "+=", "episode_return", "\n", "", "mean_return", "=", "total_return", "/", "num_episodes", "\n", "return", "mean_return", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.ECAC.ECACAgent.save_models": [[207, 211], ["ECAC.ECACAgent.policy.save", "ECAC.ECACAgent.critic_online.save", "ECAC.ECACAgent.critic_target.save", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.model.BaseNetwork.save"], ["", "def", "save_models", "(", "self", ",", "save_dir", ")", ":", "\n", "        ", "self", ".", "policy", ".", "save", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'policy.pth'", ")", ")", "\n", "self", ".", "critic_online", ".", "save", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'critic_online.pth'", ")", ")", "\n", "self", ".", "critic_target", ".", "save", "(", "os", ".", "path", ".", "join", "(", "save_dir", ",", "'critic_target.pth'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.utils.create_log_gaussian": [[4, 11], ["math.log", "quadratic.sum", "log_z.sum", "log_std.exp"], "function", ["None"], ["def", "create_log_gaussian", "(", "mean", ",", "log_std", ",", "t", ")", ":", "\n", "    ", "quadratic", "=", "-", "(", "(", "0.5", "*", "(", "t", "-", "mean", ")", "/", "(", "log_std", ".", "exp", "(", ")", ")", ")", ".", "pow", "(", "2", ")", ")", "\n", "l", "=", "mean", ".", "shape", "\n", "log_z", "=", "log_std", "\n", "z", "=", "l", "[", "-", "1", "]", "*", "math", ".", "log", "(", "2", "*", "math", ".", "pi", ")", "\n", "log_p", "=", "quadratic", ".", "sum", "(", "dim", "=", "-", "1", ")", "-", "log_z", ".", "sum", "(", "dim", "=", "-", "1", ")", "-", "0.5", "*", "z", "\n", "return", "log_p", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.utils.logsumexp": [[12, 21], ["torch.max", "inputs.view.view", "outputs.squeeze.squeeze"], "function", ["None"], ["", "def", "logsumexp", "(", "inputs", ",", "dim", "=", "None", ",", "keepdim", "=", "False", ")", ":", "\n", "    ", "if", "dim", "is", "None", ":", "\n", "        ", "inputs", "=", "inputs", ".", "view", "(", "-", "1", ")", "\n", "dim", "=", "0", "\n", "", "s", ",", "_", "=", "torch", ".", "max", "(", "inputs", ",", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", "\n", "outputs", "=", "s", "+", "(", "inputs", "-", "s", ")", ".", "exp", "(", ")", ".", "sum", "(", "dim", "=", "dim", ",", "keepdim", "=", "True", ")", ".", "log", "(", ")", "\n", "if", "not", "keepdim", ":", "\n", "        ", "outputs", "=", "outputs", ".", "squeeze", "(", "dim", ")", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.utils.soft_update": [[22, 25], ["zip", "target.parameters", "source.parameters", "target_param.data.copy_"], "function", ["None"], ["", "def", "soft_update", "(", "target", ",", "source", ",", "tau", ")", ":", "\n", "    ", "for", "target_param", ",", "param", "in", "zip", "(", "target", ".", "parameters", "(", ")", ",", "source", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "target_param", ".", "data", ".", "copy_", "(", "target_param", ".", "data", "*", "(", "1.0", "-", "tau", ")", "+", "param", ".", "data", "*", "tau", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.utils.hard_update": [[26, 29], ["zip", "target.parameters", "source.parameters", "target_param.data.copy_"], "function", ["None"], ["", "", "def", "hard_update", "(", "target", ",", "source", ")", ":", "\n", "    ", "for", "target_param", ",", "param", "in", "zip", "(", "target", ".", "parameters", "(", ")", ",", "source", ".", "parameters", "(", ")", ")", ":", "\n", "        ", "target_param", ".", "data", ".", "copy_", "(", "param", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.utils.weights_init_": [[31, 41], ["classname.find", "m.weight.data.normal_", "classname.find", "m.weight.data.normal_", "m.bias.data.fill_", "classname.find", "torch.nn.init.xavier_uniform_", "torch.nn.init.constant_"], "function", ["None"], ["", "", "def", "weights_init_", "(", "m", ")", ":", "\n", "    ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "classname", ".", "find", "(", "'Conv'", ")", "!=", "-", "1", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0.0", ",", "0.02", ")", "\n", "", "elif", "classname", ".", "find", "(", "'BatchNorm'", ")", "!=", "-", "1", ":", "\n", "        ", "m", ".", "weight", ".", "data", ".", "normal_", "(", "1.0", ",", "0.02", ")", "\n", "m", ".", "bias", ".", "data", ".", "fill_", "(", "0", ")", "\n", "", "elif", "classname", ".", "find", "(", "'Linear'", ")", "!=", "-", "1", ":", "\n", "        ", "torch", ".", "nn", ".", "init", ".", "xavier_uniform_", "(", "m", ".", "weight", ",", "gain", "=", "1", ")", "\n", "torch", ".", "nn", ".", "init", ".", "constant_", "(", "m", ".", "bias", ",", "0", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.NoopResetEnv.__init__": [[13, 25], ["gym.Wrapper.__init__", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "noop_max", "=", "30", ")", ":", "\n", "        ", "\"\"\"\n        Sample initial states by taking random number of no-ops on reset.\n        No-op is assumed to be action 0.\n        :param env: (Gym Environment) the environment to wrap\n        :param noop_max: (int) the maximum value of no-ops to run\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "noop_max", "=", "noop_max", "\n", "self", ".", "override_num_noops", "=", "None", "\n", "self", ".", "noop_action", "=", "0", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "0", "]", "==", "'NOOP'", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.NoopResetEnv.reset": [[26, 39], ["env.NoopResetEnv.env.reset", "range", "env.NoopResetEnv.unwrapped.np_random.randint", "env.NoopResetEnv.env.step", "env.NoopResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "if", "self", ".", "override_num_noops", "is", "not", "None", ":", "\n", "            ", "noops", "=", "self", ".", "override_num_noops", "\n", "", "else", ":", "\n", "            ", "noops", "=", "self", ".", "unwrapped", ".", "np_random", ".", "randint", "(", "1", ",", "self", ".", "noop_max", "+", "1", ")", "\n", "", "assert", "noops", ">", "0", "\n", "obs", "=", "None", "\n", "for", "_", "in", "range", "(", "noops", ")", ":", "\n", "            ", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "self", ".", "noop_action", ")", "\n", "if", "done", ":", "\n", "                ", "obs", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.NoopResetEnv.step": [[40, 42], ["env.NoopResetEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FireResetEnv.__init__": [[45, 53], ["gym.Wrapper.__init__", "len", "env.unwrapped.get_action_meanings", "env.unwrapped.get_action_meanings"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        Take action on reset for environments that are fixed until firing.\n        :param env: (Gym Environment) the environment to wrap\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "assert", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", "[", "1", "]", "==", "'FIRE'", "\n", "assert", "len", "(", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ")", ">=", "3", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FireResetEnv.reset": [[54, 63], ["env.FireResetEnv.env.reset", "env.FireResetEnv.env.step", "env.FireResetEnv.env.step", "env.FireResetEnv.env.reset", "env.FireResetEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "1", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "obs", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "2", ")", "\n", "if", "done", ":", "\n", "            ", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FireResetEnv.step": [[64, 66], ["env.FireResetEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.EpisodicLifeEnv.__init__": [[69, 78], ["gym.Wrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        :param env: (Gym Environment) the environment to wrap\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "lives", "=", "0", "\n", "self", ".", "was_real_done", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.EpisodicLifeEnv.step": [[79, 92], ["env.EpisodicLifeEnv.env.step", "env.EpisodicLifeEnv.env.unwrapped.ale.lives"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "was_real_done", "=", "done", "\n", "# check current lives, make loss of life terminal,", "\n", "# then update lives to handle bonus lives", "\n", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "if", "0", "<", "lives", "<", "self", ".", "lives", ":", "\n", "# for Qbert sometimes we stay in lives == 0 condtion for a few", "\n", "# frames so its important to keep lives > 0, so that we only reset", "\n", "# once the environment advertises done.", "\n", "            ", "done", "=", "True", "\n", "", "self", ".", "lives", "=", "lives", "\n", "return", "obs", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.EpisodicLifeEnv.reset": [[93, 108], ["env.EpisodicLifeEnv.env.unwrapped.ale.lives", "env.EpisodicLifeEnv.env.reset", "env.EpisodicLifeEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Calls the Gym environment reset, only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        :param kwargs: Extra keywords passed to env.reset() call\n        :return: ([int] or [float]) the first observation of the environment\n        \"\"\"", "\n", "if", "self", ".", "was_real_done", ":", "\n", "            ", "obs", "=", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "", "else", ":", "\n", "# no-op step to advance from terminal/lost life state", "\n", "            ", "obs", ",", "_", ",", "_", ",", "_", "=", "self", ".", "env", ".", "step", "(", "0", ")", "\n", "", "self", ".", "lives", "=", "self", ".", "env", ".", "unwrapped", ".", "ale", ".", "lives", "(", ")", "\n", "return", "obs", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.MaxAndSkipEnv.__init__": [[111, 123], ["gym.Wrapper.__init__", "numpy.zeros"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "skip", "=", "4", ")", ":", "\n", "        ", "\"\"\"\n        Return only every `skip`-th frame (frameskipping)\n        :param env: (Gym Environment) the environment\n        :param skip: (int) number of `skip`-th frame\n        \"\"\"", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "# most recent raw observations (for max pooling across time steps)", "\n", "self", ".", "_obs_buffer", "=", "np", ".", "zeros", "(", "\n", "(", "2", ",", ")", "+", "env", ".", "observation_space", ".", "shape", ",", "\n", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "self", ".", "_skip", "=", "skip", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.MaxAndSkipEnv.step": [[124, 148], ["range", "env.MaxAndSkipEnv._obs_buffer.max", "env.MaxAndSkipEnv.env.step"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "\"\"\"\n        Step the environment with the given action\n        Repeat action, sum reward, and max over last observations.\n        :param action: ([int] or [float]) the action\n        :return: ([int] or [float], [float], [bool], dict) observation, reward,\n                 done, information\n        \"\"\"", "\n", "total_reward", "=", "0.0", "\n", "done", "=", "None", "\n", "for", "i", "in", "range", "(", "self", ".", "_skip", ")", ":", "\n", "            ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "if", "i", "==", "self", ".", "_skip", "-", "2", ":", "\n", "                ", "self", ".", "_obs_buffer", "[", "0", "]", "=", "obs", "\n", "", "if", "i", "==", "self", ".", "_skip", "-", "1", ":", "\n", "                ", "self", ".", "_obs_buffer", "[", "1", "]", "=", "obs", "\n", "", "total_reward", "+=", "reward", "\n", "if", "done", ":", "\n", "                ", "break", "\n", "# Note that the observation on the done=True frame", "\n", "# doesn't matter", "\n", "", "", "max_frame", "=", "self", ".", "_obs_buffer", ".", "max", "(", "axis", "=", "0", ")", "\n", "\n", "return", "max_frame", ",", "total_reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.MaxAndSkipEnv.reset": [[149, 151], ["env.MaxAndSkipEnv.env.reset"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "reset", "(", "**", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.ClipRewardEnv.__init__": [[154, 160], ["gym.RewardWrapper.__init__"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        clips the reward to {+1, 0, -1} by its sign.\n        :param env: (Gym Environment) the environment\n        \"\"\"", "\n", "gym", ".", "RewardWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.ClipRewardEnv.reward": [[161, 167], ["numpy.sign"], "methods", ["None"], ["", "def", "reward", "(", "self", ",", "reward", ")", ":", "\n", "        ", "\"\"\"\n        Bin reward to {+1, 0, -1} by its sign.\n        :param reward: (float)\n        \"\"\"", "\n", "return", "np", ".", "sign", "(", "reward", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.WarpFramePyTorch.__init__": [[170, 181], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "\"\"\"\n        Warp frames to 84x84 as done in the Nature paper and later work.\n        :param env: (Gym Environment) the environment\n        \"\"\"", "\n", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "width", "=", "84", "\n", "self", ".", "height", "=", "84", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "0", ",", "high", "=", "255", ",", "shape", "=", "(", "1", ",", "self", ".", "height", ",", "self", ".", "width", ")", ",", "\n", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.WarpFramePyTorch.observation": [[182, 192], ["cv2.cvtColor", "cv2.resize"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "frame", ")", ":", "\n", "        ", "\"\"\"\n        returns the current observation from a frame\n        :param frame: ([int] or [float]) environment frame\n        :return: ([int] or [float]) the observation\n        \"\"\"", "\n", "frame", "=", "cv2", ".", "cvtColor", "(", "frame", ",", "cv2", ".", "COLOR_RGB2GRAY", ")", "\n", "frame", "=", "cv2", ".", "resize", "(", "\n", "frame", ",", "(", "self", ".", "width", ",", "self", ".", "height", ")", ",", "interpolation", "=", "cv2", ".", "INTER_AREA", ")", "\n", "return", "frame", "[", "None", ",", ":", ",", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.__init__": [[195, 216], ["gym.Wrapper.__init__", "collections.deque", "gym.spaces.Box", "numpy.min", "numpy.max"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "n_frames", ")", ":", "\n", "        ", "\"\"\"Stack n_frames last frames.\n        Returns lazy array, which is much more memory efficient.\n        See Also\n        --------\n        stable_baselines.common.atari_wrappers.LazyFrames\n        :param env: (Gym Environment) the environment\n        :param n_frames: (int) the number of frames to stack\n        \"\"\"", "\n", "assert", "env", ".", "observation_space", ".", "dtype", "==", "np", ".", "uint8", "\n", "\n", "gym", ".", "Wrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "n_frames", "=", "n_frames", "\n", "self", ".", "frames", "=", "deque", "(", "[", "]", ",", "maxlen", "=", "n_frames", ")", "\n", "shp", "=", "env", ".", "observation_space", ".", "shape", "\n", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "np", ".", "min", "(", "env", ".", "observation_space", ".", "low", ")", ",", "\n", "high", "=", "np", ".", "max", "(", "env", ".", "observation_space", ".", "high", ")", ",", "\n", "shape", "=", "(", "shp", "[", "0", "]", "*", "n_frames", ",", "shp", "[", "1", "]", ",", "shp", "[", "2", "]", ")", ",", "\n", "dtype", "=", "env", ".", "observation_space", ".", "dtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset": [[217, 222], ["env.FrameStackPyTorch.env.reset", "range", "env.FrameStackPyTorch._get_ob", "env.FrameStackPyTorch.frames.append"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.reset", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch._get_ob"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "obs", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "n_frames", ")", ":", "\n", "            ", "self", ".", "frames", ".", "append", "(", "obs", ")", "\n", "", "return", "self", ".", "_get_ob", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step": [[223, 227], ["env.FrameStackPyTorch.env.step", "env.FrameStackPyTorch.frames.append", "env.FrameStackPyTorch._get_ob"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch.step", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch._get_ob"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "obs", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "frames", ".", "append", "(", "obs", ")", "\n", "return", "self", ".", "_get_ob", "(", ")", ",", "reward", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FrameStackPyTorch._get_ob": [[228, 231], ["env.LazyFrames", "len", "list"], "methods", ["None"], ["", "def", "_get_ob", "(", "self", ")", ":", "\n", "        ", "assert", "len", "(", "self", ".", "frames", ")", "==", "self", ".", "n_frames", "\n", "return", "LazyFrames", "(", "list", "(", "self", ".", "frames", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.ScaledFloatFrame.__init__": [[234, 239], ["gym.ObservationWrapper.__init__", "gym.spaces.Box"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["    ", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "        ", "gym", ".", "ObservationWrapper", ".", "__init__", "(", "self", ",", "env", ")", "\n", "self", ".", "observation_space", "=", "spaces", ".", "Box", "(", "\n", "low", "=", "0", ",", "high", "=", "1.0", ",", "shape", "=", "env", ".", "observation_space", ".", "shape", ",", "\n", "dtype", "=", "np", ".", "float32", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.ScaledFloatFrame.observation": [[240, 244], ["numpy.array().astype", "numpy.array"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "observation", ")", ":", "\n", "# careful! This undoes the memory optimization, use", "\n", "# with smaller replay buffers only.", "\n", "        ", "return", "np", ".", "array", "(", "observation", ")", ".", "astype", "(", "np", ".", "float32", ")", "/", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.LazyFrames.__init__": [[247, 250], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "frames", ")", ":", "\n", "        ", "self", ".", "_frames", "=", "frames", "\n", "self", ".", "dtype", "=", "frames", "[", "0", "]", ".", "dtype", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.LazyFrames._force": [[251, 254], ["numpy.concatenate", "numpy.array"], "methods", ["None"], ["", "def", "_force", "(", "self", ")", ":", "\n", "        ", "return", "np", ".", "concatenate", "(", "\n", "np", ".", "array", "(", "self", ".", "_frames", ",", "dtype", "=", "self", ".", "dtype", ")", ",", "axis", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.LazyFrames.__array__": [[255, 260], ["env.LazyFrames._force", "out.astype.astype.astype"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.LazyFrames._force"], ["", "def", "__array__", "(", "self", ",", "dtype", "=", "None", ")", ":", "\n", "        ", "out", "=", "self", ".", "_force", "(", ")", "\n", "if", "dtype", "is", "not", "None", ":", "\n", "            ", "out", "=", "out", ".", "astype", "(", "dtype", ")", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.LazyFrames.__len__": [[261, 263], ["len", "env.LazyFrames._force"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.LazyFrames._force"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_force", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.LazyFrames.__getitem__": [[264, 266], ["env.LazyFrames._force"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.LazyFrames._force"], ["", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "return", "self", ".", "_force", "(", ")", "[", "i", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__": [[309, 319], ["gym.ObservationWrapper.__init__", "gym.spaces.Box", "numpy.prod"], "methods", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.__init__"], ["def", "__init__", "(", "self", ",", "env", ",", "dict_keys", ")", ":", "\n", "        ", "super", "(", "FlattenDictWrapper", ",", "self", ")", ".", "__init__", "(", "env", ")", "\n", "self", ".", "dict_keys", "=", "dict_keys", "\n", "\n", "# Figure out observation_space dimension.", "\n", "size", "=", "0", "\n", "for", "key", "in", "dict_keys", ":", "\n", "            ", "shape", "=", "self", ".", "env", ".", "observation_space", ".", "spaces", "[", "key", "]", ".", "shape", "\n", "size", "+=", "np", ".", "prod", "(", "shape", ")", "\n", "", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "-", "np", ".", "inf", ",", "np", ".", "inf", ",", "shape", "=", "(", "size", ",", ")", ",", "dtype", "=", "'float32'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.FlattenDictWrapper.observation": [[320, 326], ["isinstance", "numpy.concatenate", "obs.append", "observation[].ravel"], "methods", ["None"], ["", "def", "observation", "(", "self", ",", "observation", ")", ":", "\n", "        ", "assert", "isinstance", "(", "observation", ",", "dict", ")", "\n", "obs", "=", "[", "]", "\n", "for", "key", "in", "self", ".", "dict_keys", ":", "\n", "            ", "obs", ".", "append", "(", "observation", "[", "key", "]", ".", "ravel", "(", ")", ")", "\n", "", "return", "np", ".", "concatenate", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.make_atari": [[268, 279], ["gym.make", "env.NoopResetEnv", "env.MaxAndSkipEnv"], "function", ["None"], ["", "", "def", "make_atari", "(", "env_id", ")", ":", "\n", "    ", "\"\"\"\n    Create a wrapped atari envrionment\n    :param env_id: (str) the environment ID\n    :return: (Gym Environment) the wrapped atari environment\n    \"\"\"", "\n", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "assert", "'NoFrameskip'", "in", "env", ".", "spec", ".", "id", "\n", "env", "=", "NoopResetEnv", "(", "env", ",", "noop_max", "=", "30", ")", "\n", "env", "=", "MaxAndSkipEnv", "(", "env", ",", "skip", "=", "4", ")", "\n", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.wrap_deepmind_pytorch": [[281, 304], ["env.WarpFramePyTorch", "env.EpisodicLifeEnv", "FrameStackPyTorch.unwrapped.get_action_meanings", "env.FireResetEnv", "env.ClipRewardEnv", "env.ScaledFloatFrame", "env.FrameStackPyTorch"], "function", ["None"], ["", "def", "wrap_deepmind_pytorch", "(", "env", ",", "episode_life", "=", "True", ",", "clip_rewards", "=", "True", ",", "\n", "frame_stack", "=", "True", ",", "scale", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Configure environment for DeepMind-style Atari.\n    :param env: (Gym Environment) the atari environment\n    :param episode_life: (bool) wrap the episode life wrapper\n    :param clip_rewards: (bool) wrap the reward clipping wrapper\n    :param frame_stack: (bool) wrap the frame stacking wrapper\n    :param scale: (bool) wrap the scaling observation wrapper\n    :return: (Gym Environment) the wrapped atari environment\n    \"\"\"", "\n", "if", "episode_life", ":", "\n", "        ", "env", "=", "EpisodicLifeEnv", "(", "env", ")", "\n", "", "if", "'FIRE'", "in", "env", ".", "unwrapped", ".", "get_action_meanings", "(", ")", ":", "\n", "        ", "env", "=", "FireResetEnv", "(", "env", ")", "\n", "", "env", "=", "WarpFramePyTorch", "(", "env", ")", "\n", "if", "clip_rewards", ":", "\n", "        ", "env", "=", "ClipRewardEnv", "(", "env", ")", "\n", "", "if", "scale", ":", "\n", "        ", "env", "=", "ScaledFloatFrame", "(", "env", ")", "\n", "", "if", "frame_stack", ":", "\n", "        ", "env", "=", "FrameStackPyTorch", "(", "env", ",", "4", ")", "\n", "", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.make_pytorch_env": [[327, 342], ["gym.make", "hasattr", "isinstance", "hasattr", "isinstance", "env.make_atari", "env.wrap_deepmind_pytorch", "env.FlattenDictWrapper"], "function", ["home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.make_atari", "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.wrap_deepmind_pytorch"], ["", "", "def", "make_pytorch_env", "(", "env_id", ",", "episode_life", "=", "True", ",", "clip_rewards", "=", "True", ",", "\n", "frame_stack", "=", "True", ",", "scale", "=", "False", ")", ":", "\n", "    ", "env", "=", "gym", ".", "make", "(", "env_id", ")", "\n", "\n", "is_atari", "=", "hasattr", "(", "gym", ".", "envs", ",", "'atari'", ")", "and", "isinstance", "(", "\n", "env", ".", "unwrapped", ",", "gym", ".", "envs", ".", "atari", ".", "atari_env", ".", "AtariEnv", ")", "\n", "is_robotics", "=", "hasattr", "(", "gym", ".", "envs", ",", "'robotics'", ")", "and", "isinstance", "(", "\n", "env", ".", "unwrapped", ",", "gym", ".", "envs", ".", "robotics", ".", "robot_env", ".", "RobotEnv", ")", "\n", "if", "is_atari", ":", "\n", "        ", "env", "=", "make_atari", "(", "env_id", ")", "\n", "env", "=", "wrap_deepmind_pytorch", "(", "\n", "env", ",", "episode_life", ",", "clip_rewards", ",", "frame_stack", ",", "scale", ")", "\n", "", "elif", "is_robotics", ":", "\n", "        ", "env", "=", "FlattenDictWrapper", "(", "env", ",", "dict_keys", "=", "[", "'observation'", ",", "'achieved_goal'", ",", "'desired_goal'", "]", ")", "\n", "", "return", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.SingerGao_ECAC.None.env.wrap_monitor": [[345, 349], ["gym.wrappers.Monitor"], "function", ["None"], ["", "def", "wrap_monitor", "(", "env", ",", "log_dir", ")", ":", "\n", "    ", "env", "=", "wrappers", ".", "Monitor", "(", "\n", "env", ",", "log_dir", ",", "video_callable", "=", "lambda", "x", ":", "True", ")", "\n", "return", "env", "\n", "", ""]]}