{"home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.train_both_auxiliary.set_seed": [[17, 24], ["torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed"], "function", ["None"], ["def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.train_both_auxiliary.main": [[26, 187], ["transformers.BertTokenizer.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained", "transformers.BertTokenizer.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained", "utils.load_csv", "train_both_auxiliary.set_seed", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "transformers.BartTokenizer.from_pretrained", "model.MultiTaskBart.from_pretrained", "MultiTaskBart.from_pretrained.to", "MultiTaskBart.from_pretrained.train", "transformers.AdamW", "range", "MultiTaskBart.from_pretrained.save_pretrained", "BartTokenizer.from_pretrained.save_pretrained", "MultiTaskBart.from_pretrained.eval", "torch.cuda.is_available", "torch.cuda.is_available", "torch.device", "torch.device", "torch.device", "torch.device", "MultiTaskBart.from_pretrained.parameters", "batch[].to", "MultiTaskBart.from_pretrained.generate", "BartTokenizer.from_pretrained.decode", "generated_results.append", "generated_results_labels.append", "open", "open", "batch[].to", "MultiTaskBart.from_pretrained.generate", "BartTokenizer.from_pretrained.decode", "generated_results.append", "generated_results_labels.append", "open", "open", "transformers.AdamW.zero_grad", "batch[].to", "batch[].to", "batch[].to", "MultiTaskBart.from_pretrained.", "BartTokenizer.from_pretrained.decode", "avg_loss1.append", "avg_loss2.append", "loss.backward", "transformers.AdamW.step", "open", "file.write", "file.write", "file.write", "file.write", "file.writelines", "file.write", "file.write", "file.write", "file.write", "batch[].to.eq", "BartTokenizer.from_pretrained.encode_plus", "batch_query_encoding.to.to", "MultiTaskBart.from_pretrained.", "torch.cat", "torch.cat", "MultiTaskBart.from_pretrained.classification_head", "MultiTaskBart.from_pretrained.classification_head", "MultiTaskBart.from_pretrained.generate", "BertTokenizer.from_pretrained.encode_plus", "torch.sigmoid().to", "torch.sigmoid().to", "torch.nn.MSELoss", "torch.nn.MSELoss.", "BertTokenizer.from_pretrained.encode_plus", "torch.sigmoid().to", "torch.sigmoid().to", "torch.nn.MSELoss", "torch.nn.MSELoss.", "float", "float", "print", "print", "print", "print", "print", "print", "print", "print", "str", "BartTokenizer.from_pretrained.decode", "tokenizer.decode.find", "BartTokenizer.from_pretrained.decode", "tokenizer.decode.find", "tokenizer.decode.find", "BertForSequenceClassification.from_pretrained.", "BertForSequenceClassification.from_pretrained.", "BartTokenizer.from_pretrained.decode", "BartTokenizer.from_pretrained.decode", "BartTokenizer.from_pretrained.decode", "tokenizer.decode.find", "len", "tokenizer.decode.find", "len", "tokenizer.decode.find", "len", "summa_last_hidden_state[].view", "BartTokenizer.from_pretrained.decode", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "str", "str", "str", "summa_last_hidden_state.size", "summa_last_hidden_state.size", "model.classification_head.view", "model.classification_head.view", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.utils.load_csv", "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.train.set_seed", "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.eval.eval"], ["", "def", "main", "(", ")", ":", "\n", "    ", "tokenizer_rel", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"/shared/siyiliu/transformers/examples/seq2seq/Bert_Xander_finetuned\"", ")", "\n", "model_rel", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "\"/shared/siyiliu/transformers/examples/seq2seq/Bert_Xander_finetuned\"", ")", "\n", "\n", "tokenizer_stance", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"/shared/siyiliu/transformers/examples/seq2seq/xander_stance_finetuned\"", ")", "\n", "model_stance", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "\"/shared/siyiliu/transformers/examples/seq2seq/xander_stance_finetuned\"", ")", "\n", "\n", "train_dataset", ",", "dev_dataset", ",", "test_dataset", ",", "label_to_query", "=", "load_csv", "(", "'dataset.csv'", ")", "\n", "for", "seed", "in", "(", "1", ",", "6", ",", "9", ")", ":", "\n", "        ", "set_seed", "(", "seed", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "shuffle", "=", "True", ")", "\n", "dev_loader", "=", "DataLoader", "(", "dev_dataset", ",", "shuffle", "=", "False", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "shuffle", "=", "False", ")", "\n", "\n", "results_path_prefix", "=", "\"results/seed_%d_stance=1+both\"", "%", "seed", "\n", "models_path_prefix", "=", "\"trained_models/seed_%d_stance=1+both\"", "%", "seed", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "results_path_prefix", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "results_path_prefix", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "models_path_prefix", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "models_path_prefix", ")", "\n", "", "for", "alpha", "in", "(", "1", ",", "5", ",", "50", ")", ":", "\n", "            ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "'facebook/bart-base'", ",", "cache_dir", "=", "\"/shared/siyiliu/transformers/examples/seq2seq/cached_models\"", ")", "\n", "model", "=", "MultiTaskBart", ".", "from_pretrained", "(", "'facebook/bart-base'", ",", "cache_dir", "=", "\"/shared/siyiliu/transformers/examples/seq2seq/cached_models\"", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "torch", ".", "device", "(", "'cpu'", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "train", "(", ")", "\n", "optim", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "3e-5", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "6", ")", ":", "\n", "                ", "i", "=", "0", "\n", "avg_loss1", "=", "[", "]", "\n", "avg_loss2", "=", "[", "]", "\n", "for", "batch", "in", "train_loader", ":", "\n", "                    ", "optim", ".", "zero_grad", "(", ")", "\n", "input_ids", "=", "batch", "[", "'input_ids'", "]", ".", "to", "(", "device", ")", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ".", "to", "(", "device", ")", "\n", "labels", "=", "batch", "[", "'labels'", "]", ".", "to", "(", "device", ")", "\n", "outputs", "=", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "labels", "=", "labels", ",", "return_dict", "=", "True", ")", "\n", "loss2", "=", "0", "\n", "batch_label", "=", "tokenizer", ".", "decode", "(", "batch", "[", "'labels'", "]", "[", "0", "]", ")", "\n", "batch_label", "=", "batch_label", "[", "batch_label", ".", "find", "(", "'<s>'", ")", "+", "len", "(", "'<s>'", ")", ":", "batch_label", ".", "find", "(", "'</s>'", ")", "]", "\n", "batch_query", "=", "label_to_query", "[", "batch_label", "]", "\n", "\n", "if", "alpha", ">", "0.0", ":", "\n", "                        ", "summa_last_hidden_state", "=", "outputs", ".", "encoder_last_hidden_state", "# this is actually decoder's last hidden state, look at model.py for details", "\n", "eos_mask", "=", "labels", ".", "eq", "(", "model", ".", "config", ".", "eos_token_id", ")", "\n", "sum_sentence_embedding", "=", "summa_last_hidden_state", "[", "eos_mask", ",", ":", "]", ".", "view", "(", "summa_last_hidden_state", ".", "size", "(", "0", ")", ",", "-", "1", ",", "summa_last_hidden_state", ".", "size", "(", "-", "1", ")", ")", "[", ":", ",", "-", "1", ",", ":", "]", "[", "0", "]", "\n", "\n", "batch_query_encoding", "=", "tokenizer", ".", "encode_plus", "(", "batch_query", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "batch_query_encoding", "=", "batch_query_encoding", ".", "to", "(", "device", ")", "\n", "outputs_query", "=", "model", "(", "**", "batch_query_encoding", ",", "return_dict", "=", "True", ")", "\n", "query_last_hidden_state", "=", "outputs_query", ".", "encoder_last_hidden_state", "#decoder's last hidden state", "\n", "query_sentence_embedding", "=", "query_last_hidden_state", "[", "0", "]", "[", "-", "1", "]", "\n", "\n", "concat_embedding", "=", "torch", ".", "cat", "(", "(", "sum_sentence_embedding", ",", "query_sentence_embedding", ")", ",", "0", ")", "\n", "\n", "logits_classification_rel", "=", "model", ".", "classification_head", "(", "concat_embedding", ")", "\n", "logits_classification_stance", "=", "model", ".", "classification_head", "(", "concat_embedding", ")", "\n", "\n", "summary_ids", "=", "model", ".", "generate", "(", "input_ids", ")", "\n", "generated_txt", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "summary_ids", "]", "[", "0", "]", "\n", "\n", "\n", "token_rel", "=", "tokenizer_rel", ".", "encode_plus", "(", "batch_query", ",", "generated_txt", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "rel_logits", "=", "model_rel", "(", "**", "token_rel", ")", "[", "0", "]", "\n", "loss_rel_prob", "=", "torch", ".", "sigmoid", "(", "rel_logits", "[", "0", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss2", "=", "loss_fct", "(", "torch", ".", "sigmoid", "(", "logits_classification_rel", ".", "view", "(", "-", "1", ")", ")", "[", "0", "]", ",", "loss_rel_prob", "[", "0", "]", ")", "\n", "\n", "token_stance", "=", "tokenizer_stance", ".", "encode_plus", "(", "batch_query", ",", "generated_txt", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "stance_logits", "=", "model_stance", "(", "**", "token_stance", ")", "[", "0", "]", "\n", "loss_stance_prob", "=", "torch", ".", "sigmoid", "(", "stance_logits", "[", "0", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss3", "=", "loss_fct", "(", "torch", ".", "sigmoid", "(", "logits_classification_stance", ".", "view", "(", "-", "1", ")", ")", "[", "0", "]", ",", "loss_stance_prob", "[", "0", "]", ")", "\n", "\n", "\n", "\n", "\n", "\n", "\n", "", "loss", "=", "outputs", ".", "loss", "+", "alpha", "*", "loss2", "+", "1", "*", "loss3", "\n", "avg_loss1", ".", "append", "(", "float", "(", "outputs", ".", "loss", ")", ")", "\n", "avg_loss2", ".", "append", "(", "float", "(", "alpha", "*", "loss2", ")", ")", "\n", "\n", "loss", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "optim", ".", "step", "(", ")", "\n", "\n", "if", "i", "<", "3", "and", "epoch", "==", "0", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "\"--------Start--------\"", ")", "\n", "print", "(", "\"Alpha = \"", ",", "alpha", ")", "\n", "print", "(", "'Source:'", ",", "tokenizer", ".", "decode", "(", "batch", "[", "'input_ids'", "]", "[", "0", "]", ")", ")", "\n", "print", "(", "'Target:'", ",", "batch_label", ")", "\n", "print", "(", "'Query:'", ",", "batch_query", ")", "\n", "print", "(", "'LM Loss:'", ",", "outputs", "[", "0", "]", ",", "'Auxiliary Loss:'", ",", "loss2", ")", "\n", "print", "(", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_log.txt'", "%", "alpha", ",", "'a'", ")", "as", "file", ":", "\n", "                    ", "file", ".", "write", "(", "'\\n'", ")", "\n", "file", ".", "write", "(", "\"Epoch \"", ")", "\n", "file", ".", "write", "(", "str", "(", "epoch", ")", ")", "\n", "file", ".", "write", "(", "'\\n'", ")", "\n", "file", ".", "writelines", "(", "[", "'Source: '", ",", "tokenizer", ".", "decode", "(", "batch", "[", "'input_ids'", "]", "[", "0", "]", ")", ",", "'\\n'", ",", "'Target: '", ",", "tokenizer", ".", "decode", "(", "batch", "[", "'labels'", "]", "[", "0", "]", ")", ",", "'\\n'", ",", "'Query: '", ",", "batch_query", ",", "'\\n'", ",", "\"Avg loss1 = \"", "+", "str", "(", "np", ".", "mean", "(", "avg_loss1", ")", ")", ",", "\"Avg loss2 = \"", "+", "str", "(", "np", ".", "mean", "(", "avg_loss2", ")", ")", ",", "\"Avg total loss= \"", "+", "str", "(", "np", ".", "mean", "(", "avg_loss1", ")", "+", "np", ".", "mean", "(", "avg_loss2", ")", ")", "]", ")", "\n", "\n", "", "", "path_model", "=", "models_path_prefix", "+", "\"/alpha=%f_models\"", "%", "alpha", "\n", "model", ".", "save_pretrained", "(", "path_model", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "path_model", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "generated_results", "=", "[", "]", "\n", "generated_results_labels", "=", "[", "]", "\n", "for", "batch", "in", "test_loader", ":", "\n", "                ", "input_ids", "=", "batch", "[", "'input_ids'", "]", ".", "to", "(", "device", ")", "\n", "summary_ids", "=", "model", ".", "generate", "(", "input_ids", ")", "\n", "generated_txt", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "summary_ids", "]", "[", "0", "]", "\n", "\n", "batch_label", "=", "tokenizer", ".", "decode", "(", "batch", "[", "'labels'", "]", "[", "0", "]", ")", "\n", "batch_label", "=", "batch_label", "[", "batch_label", ".", "find", "(", "'<s>'", ")", "+", "len", "(", "'<s>'", ")", ":", "batch_label", ".", "find", "(", "'</s>'", ")", "]", "\n", "\n", "generated_results", ".", "append", "(", "generated_txt", ")", "\n", "generated_results_labels", ".", "append", "(", "batch_label", ")", "\n", "\n", "\n", "\n", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_test_generated.txt'", "%", "alpha", ",", "'w'", ")", "as", "file", ":", "\n", "                ", "for", "txt", "in", "generated_results", ":", "\n", "                    ", "file", ".", "write", "(", "txt", "+", "'\\n'", ")", "\n", "\n", "", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_test_labels.txt'", "%", "alpha", ",", "'w'", ")", "as", "file", ":", "\n", "                ", "for", "txt", "in", "generated_results_labels", ":", "\n", "                    ", "file", ".", "write", "(", "txt", "+", "'\\n'", ")", "\n", "\n", "\n", "", "", "generated_results", "=", "[", "]", "\n", "generated_results_labels", "=", "[", "]", "\n", "for", "batch", "in", "dev_loader", ":", "\n", "                ", "input_ids", "=", "batch", "[", "'input_ids'", "]", ".", "to", "(", "device", ")", "\n", "summary_ids", "=", "model", ".", "generate", "(", "input_ids", ")", "\n", "generated_txt", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "summary_ids", "]", "[", "0", "]", "\n", "\n", "batch_label", "=", "tokenizer", ".", "decode", "(", "batch", "[", "'labels'", "]", "[", "0", "]", ")", "\n", "batch_label", "=", "batch_label", "[", "batch_label", ".", "find", "(", "'<s>'", ")", "+", "len", "(", "'<s>'", ")", ":", "batch_label", ".", "find", "(", "'</s>'", ")", "]", "\n", "\n", "generated_results", ".", "append", "(", "generated_txt", ")", "\n", "generated_results_labels", ".", "append", "(", "batch_label", ")", "\n", "\n", "\n", "\n", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_dev_generated.txt'", "%", "alpha", ",", "'w'", ")", "as", "file", ":", "\n", "                ", "for", "txt", "in", "generated_results", ":", "\n", "                    ", "file", ".", "write", "(", "txt", "+", "'\\n'", ")", "\n", "\n", "", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_dev_labels.txt'", "%", "alpha", ",", "'w'", ")", "as", "file", ":", "\n", "                ", "for", "txt", "in", "generated_results_labels", ":", "\n", "                    ", "file", ".", "write", "(", "txt", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.eval.get_query_lst": [[11, 19], ["utils.load_csv", "enumerate", "query_lst.append"], "function", ["home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.utils.load_csv"], ["def", "get_query_lst", "(", "dataset_path", ",", "target_txt", ")", ":", "\n", "    ", "train_dataset", ",", "dev_dataset", ",", "test_dataset", ",", "label_to_query", "=", "load_csv", "(", "dataset_path", ")", "\n", "query_lst", "=", "[", "]", "\n", "for", "i", ",", "txt", "in", "enumerate", "(", "target_txt", ")", ":", "\n", "        ", "query_lst", ".", "append", "(", "label_to_query", "[", "txt", "]", ")", "\n", "\n", "\n", "", "return", "query_lst", ",", "target_txt", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.eval.get_stance_label": [[20, 37], ["pandas.read_csv", "all", "stance_label.append", "int"], "function", ["None"], ["", "def", "get_stance_label", "(", "dataset_path", ",", "target_txt", ")", ":", "\n", "    ", "stance_label", "=", "[", "]", "\n", "df", "=", "pd", ".", "read_csv", "(", "dataset_path", ")", "\n", "for", "txt", "in", "target_txt", ":", "\n", "#print(df.loc[df['replaced_text'] == txt+'\\n'])", "\n", "#print(txt)", "\n", "        ", "row", "=", "df", ".", "loc", "[", "df", "[", "'replaced_text'", "]", "==", "txt", "+", "'\\n'", "]", "[", "'Support'", "]", "\n", "#print(row['Support'])", "\n", "if", "row", ".", "empty", ":", "\n", "            ", "row", "=", "df", ".", "loc", "[", "df", "[", "'replaced_text'", "]", "==", "txt", "]", "[", "'Support'", "]", "\n", "\n", "\n", "", "stance_label", ".", "append", "(", "int", "(", "row", ")", ")", "\n", "\n", "", "assert", "all", "(", "v", "==", "0", "or", "v", "==", "1", "for", "v", "in", "stance_label", ")", "\n", "#print(len(stance_label), len(target_txt))", "\n", "return", "stance_label", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.eval.eval": [[38, 83], ["rouge_score.rouge_scorer.RougeScorer", "range", "transformers.BertForSequenceClassification.from_pretrained", "transformers.BertTokenizer.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained", "transformers.BertTokenizer.from_pretrained", "len", "rouge_scorer.RougeScorer.score", "rouge1_sum.append", "rouge2_sum.append", "rougeL_sum.append", "BertTokenizer.from_pretrained.encode_plus", "BertTokenizer.from_pretrained.encode_plus", "bert_score.score", "F1.mean", "round", "round", "round", "round", "round", "round", "BertForSequenceClassification.from_pretrained.", "torch.softmax().tolist", "loss.index", "rel_lst.append", "rel_lst.append", "BertForSequenceClassification.from_pretrained.", "torch.softmax().tolist", "loss.index", "int", "stance_lst.append", "stance_lst.append", "max", "max", "numpy.mean", "numpy.mean", "numpy.mean", "float", "numpy.mean", "numpy.mean", "torch.softmax", "torch.softmax"], "function", ["None"], ["", "def", "eval", "(", "generated_txt", ",", "target_txt", ",", "query_lst", ",", "stance_label", ",", "stance_path", "=", "None", ",", "rel_path", "=", "None", ",", "bertscore", "=", "False", ")", ":", "\n", "    ", "if", "stance_path", "!=", "None", ":", "\n", "        ", "stance_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "stance_path", ")", "\n", "stance_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "stance_path", ")", "\n", "", "if", "rel_path", "!=", "None", ":", "\n", "        ", "rel_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "rel_path", ")", "\n", "rel_tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "rel_path", ")", "\n", "\n", "", "scorer", "=", "rouge_scorer", ".", "RougeScorer", "(", "[", "'rouge1'", ",", "'rouge2'", ",", "'rougeL'", "]", ",", "use_stemmer", "=", "True", ")", "\n", "rouge1_sum", "=", "[", "]", "\n", "rouge2_sum", "=", "[", "]", "\n", "rougeL_sum", "=", "[", "]", "\n", "rel_lst", "=", "[", "]", "\n", "stance_lst", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "generated_txt", ")", ")", ":", "\n", "        ", "rouge_scores", "=", "scorer", ".", "score", "(", "generated_txt", "[", "i", "]", ",", "target_txt", "[", "i", "]", ")", "\n", "rouge1_sum", ".", "append", "(", "rouge_scores", "[", "'rouge1'", "]", "[", "2", "]", ")", "\n", "rouge2_sum", ".", "append", "(", "rouge_scores", "[", "'rouge2'", "]", "[", "2", "]", ")", "\n", "rougeL_sum", ".", "append", "(", "rouge_scores", "[", "'rougeL'", "]", "[", "2", "]", ")", "\n", "\n", "\n", "rel", "=", "rel_tokenizer", ".", "encode_plus", "(", "query_lst", "[", "i", "]", ",", "generated_txt", "[", "i", "]", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "rel_logits", "=", "rel_model", "(", "**", "rel", ")", "[", "0", "]", "\n", "loss", "=", "torch", ".", "softmax", "(", "rel_logits", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "if", "loss", ".", "index", "(", "max", "(", "loss", ")", ")", "==", "0", ":", "\n", "            ", "rel_lst", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "rel_lst", ".", "append", "(", "0", ")", "\n", "\n", "", "stance", "=", "stance_tokenizer", ".", "encode_plus", "(", "query_lst", "[", "i", "]", ",", "generated_txt", "[", "i", "]", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "stance_logits", "=", "stance_model", "(", "**", "stance", ")", "[", "0", "]", "\n", "loss", "=", "torch", ".", "softmax", "(", "stance_logits", ",", "dim", "=", "1", ")", ".", "tolist", "(", ")", "[", "0", "]", "\n", "if", "loss", ".", "index", "(", "max", "(", "loss", ")", ")", "!=", "int", "(", "stance_label", "[", "i", "]", ")", ":", "\n", "            ", "stance_lst", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "            ", "stance_lst", ".", "append", "(", "0", ")", "\n", "\n", "\n", "", "", "if", "bertscore", "==", "True", ":", "\n", "        ", "P", ",", "R", ",", "F1", "=", "score", "(", "generated_txt", ",", "target_txt", ",", "lang", "=", "'en'", ")", "\n", "score_bert", "=", "F1", ".", "mean", "(", ")", "\n", "\n", "\n", "\n", "", "return", "round", "(", "np", ".", "mean", "(", "rouge1_sum", ")", "*", "100", ",", "2", ")", ",", "round", "(", "np", ".", "mean", "(", "rouge2_sum", ")", "*", "100", ",", "2", ")", ",", "round", "(", "np", ".", "mean", "(", "rougeL_sum", ")", "*", "100", ",", "2", ")", ",", "round", "(", "float", "(", "score_bert", ")", "*", "100", ",", "2", ")", ",", "round", "(", "np", ".", "mean", "(", "rel_lst", ")", "*", "100", ",", "2", ")", ",", "round", "(", "np", ".", "mean", "(", "stance_lst", ")", "*", "100", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.train.set_seed": [[17, 24], ["torch.manual_seed", "torch.manual_seed", "torch.cuda.manual_seed_all", "torch.cuda.manual_seed_all", "numpy.random.seed", "random.seed"], "function", ["None"], ["def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "torch", ".", "backends", ".", "cudnn", ".", "deterministic", "=", "True", "\n", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "False", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed_all", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.train.main": [[26, 171], ["transformers.BertTokenizer.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained", "utils.load_csv", "train.set_seed", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "os.path.exists", "os.mkdir", "os.path.exists", "os.mkdir", "transformers.BartTokenizer.from_pretrained", "model.MultiTaskBart.from_pretrained", "MultiTaskBart.from_pretrained.to", "MultiTaskBart.from_pretrained.train", "transformers.AdamW", "range", "MultiTaskBart.from_pretrained.save_pretrained", "BartTokenizer.from_pretrained.save_pretrained", "MultiTaskBart.from_pretrained.eval", "torch.cuda.is_available", "torch.cuda.is_available", "torch.device", "torch.device", "torch.device", "torch.device", "MultiTaskBart.from_pretrained.parameters", "batch[].to", "MultiTaskBart.from_pretrained.generate", "BartTokenizer.from_pretrained.decode", "generated_results.append", "generated_results_labels.append", "open", "open", "batch[].to", "MultiTaskBart.from_pretrained.generate", "BartTokenizer.from_pretrained.decode", "generated_results.append", "generated_results_labels.append", "open", "open", "transformers.AdamW.zero_grad", "batch[].to", "batch[].to", "batch[].to", "MultiTaskBart.from_pretrained.", "BartTokenizer.from_pretrained.decode", "avg_loss1.append", "avg_loss2.append", "loss.backward", "transformers.AdamW.step", "open", "file.write", "file.write", "file.write", "file.write", "file.writelines", "file.write", "file.write", "file.write", "file.write", "batch[].to.eq", "BartTokenizer.from_pretrained.encode_plus", "batch_query_encoding.to.to", "MultiTaskBart.from_pretrained.", "torch.cat", "torch.cat", "MultiTaskBart.from_pretrained.classification_head", "MultiTaskBart.from_pretrained.generate", "BertTokenizer.from_pretrained.encode_plus", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid().to", "torch.sigmoid().to", "torch.nn.MSELoss", "torch.nn.MSELoss.", "float", "float", "print", "print", "print", "print", "print", "print", "print", "print", "str", "BartTokenizer.from_pretrained.decode", "tokenizer.decode.find", "BartTokenizer.from_pretrained.decode", "tokenizer.decode.find", "tokenizer.decode.find", "BertForSequenceClassification.from_pretrained.", "BartTokenizer.from_pretrained.decode", "BartTokenizer.from_pretrained.decode", "BartTokenizer.from_pretrained.decode", "tokenizer.decode.find", "len", "tokenizer.decode.find", "len", "tokenizer.decode.find", "len", "summa_last_hidden_state[].view", "BartTokenizer.from_pretrained.decode", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "str", "str", "str", "summa_last_hidden_state.size", "summa_last_hidden_state.size", "model.classification_head.view", "numpy.mean", "numpy.mean", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.utils.load_csv", "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.train.set_seed", "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.eval.eval"], ["", "def", "main", "(", ")", ":", "\n", "    ", "tokenizer_rel", "=", "BertTokenizer", ".", "from_pretrained", "(", "\"/shared/siyiliu/transformers/examples/seq2seq/Bert_Xander_finetuned\"", ")", "\n", "model_rel", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "\"/shared/siyiliu/transformers/examples/seq2seq/Bert_Xander_finetuned\"", ")", "\n", "train_dataset", ",", "dev_dataset", ",", "test_dataset", ",", "label_to_query", "=", "load_csv", "(", "'dataset.csv'", ")", "\n", "for", "seed", "in", "(", "1", ",", "6", ",", "9", ")", ":", "\n", "        ", "set_seed", "(", "seed", ")", "\n", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "shuffle", "=", "True", ")", "\n", "dev_loader", "=", "DataLoader", "(", "dev_dataset", ",", "shuffle", "=", "False", ")", "\n", "test_loader", "=", "DataLoader", "(", "test_dataset", ",", "shuffle", "=", "False", ")", "\n", "\n", "results_path_prefix", "=", "\"results/seed_%d_+rel\"", "%", "seed", "\n", "models_path_prefix", "=", "\"trained_models/seed_%d_+rel\"", "%", "seed", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "results_path_prefix", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "results_path_prefix", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "models_path_prefix", ")", ":", "\n", "            ", "os", ".", "mkdir", "(", "models_path_prefix", ")", "\n", "", "for", "alpha", "in", "[", "0", ",", "0.1", ",", "1", ",", "5", ",", "15", ",", "30", ",", "50", "]", ":", "\n", "\n", "            ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "'facebook/bart-base'", ",", "cache_dir", "=", "\"/shared/siyiliu/transformers/examples/seq2seq/cached_models\"", ")", "\n", "model", "=", "MultiTaskBart", ".", "from_pretrained", "(", "'facebook/bart-base'", ",", "cache_dir", "=", "\"/shared/siyiliu/transformers/examples/seq2seq/cached_models\"", ")", "\n", "\n", "device", "=", "torch", ".", "device", "(", "'cuda'", ")", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "torch", ".", "device", "(", "'cpu'", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "model", ".", "train", "(", ")", "\n", "optim", "=", "AdamW", "(", "model", ".", "parameters", "(", ")", ",", "lr", "=", "3e-5", ")", "\n", "\n", "for", "epoch", "in", "range", "(", "6", ")", ":", "\n", "                ", "i", "=", "0", "\n", "avg_loss1", "=", "[", "]", "\n", "avg_loss2", "=", "[", "]", "\n", "for", "batch", "in", "train_loader", ":", "\n", "                    ", "optim", ".", "zero_grad", "(", ")", "\n", "input_ids", "=", "batch", "[", "'input_ids'", "]", ".", "to", "(", "device", ")", "\n", "attention_mask", "=", "batch", "[", "'attention_mask'", "]", ".", "to", "(", "device", ")", "\n", "labels", "=", "batch", "[", "'labels'", "]", ".", "to", "(", "device", ")", "\n", "outputs", "=", "model", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ",", "labels", "=", "labels", ",", "return_dict", "=", "True", ")", "\n", "loss2", "=", "0", "\n", "batch_label", "=", "tokenizer", ".", "decode", "(", "batch", "[", "'labels'", "]", "[", "0", "]", ")", "\n", "batch_label", "=", "batch_label", "[", "batch_label", ".", "find", "(", "'<s>'", ")", "+", "len", "(", "'<s>'", ")", ":", "batch_label", ".", "find", "(", "'</s>'", ")", "]", "\n", "batch_query", "=", "label_to_query", "[", "batch_label", "]", "\n", "\n", "if", "alpha", ">", "0.0", ":", "\n", "                        ", "summa_last_hidden_state", "=", "outputs", ".", "encoder_last_hidden_state", "# this is actually decoder's last hidden state, look at model.py for details", "\n", "eos_mask", "=", "labels", ".", "eq", "(", "model", ".", "config", ".", "eos_token_id", ")", "\n", "sum_sentence_embedding", "=", "summa_last_hidden_state", "[", "eos_mask", ",", ":", "]", ".", "view", "(", "summa_last_hidden_state", ".", "size", "(", "0", ")", ",", "-", "1", ",", "summa_last_hidden_state", ".", "size", "(", "-", "1", ")", ")", "[", ":", ",", "-", "1", ",", ":", "]", "[", "0", "]", "\n", "\n", "batch_query_encoding", "=", "tokenizer", ".", "encode_plus", "(", "batch_query", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "batch_query_encoding", "=", "batch_query_encoding", ".", "to", "(", "device", ")", "\n", "outputs_query", "=", "model", "(", "**", "batch_query_encoding", ",", "return_dict", "=", "True", ")", "\n", "query_last_hidden_state", "=", "outputs_query", ".", "encoder_last_hidden_state", "#decoder's last hidden state", "\n", "query_sentence_embedding", "=", "query_last_hidden_state", "[", "0", "]", "[", "-", "1", "]", "\n", "\n", "concat_embedding", "=", "torch", ".", "cat", "(", "(", "sum_sentence_embedding", ",", "query_sentence_embedding", ")", ",", "0", ")", "\n", "\n", "logits_classification", "=", "model", ".", "classification_head", "(", "concat_embedding", ")", "\n", "\n", "\n", "summary_ids", "=", "model", ".", "generate", "(", "input_ids", ")", "\n", "generated_txt", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "summary_ids", "]", "[", "0", "]", "\n", "token", "=", "tokenizer_rel", ".", "encode_plus", "(", "batch_query", ",", "generated_txt", ",", "return_tensors", "=", "\"pt\"", ")", "\n", "rel_logits", "=", "model_rel", "(", "**", "token", ")", "[", "0", "]", "\n", "loss_prob_xander", "=", "torch", ".", "sigmoid", "(", "rel_logits", "[", "0", "]", ")", "\n", "loss_prob_xander", "=", "torch", ".", "sigmoid", "(", "rel_logits", "[", "0", "]", ")", ".", "to", "(", "device", ")", "\n", "\n", "loss_fct", "=", "MSELoss", "(", ")", "\n", "loss2", "=", "loss_fct", "(", "torch", ".", "sigmoid", "(", "logits_classification", ".", "view", "(", "-", "1", ")", ")", "[", "0", "]", ",", "loss_prob_xander", "[", "0", "]", ")", "\n", "\n", "", "loss", "=", "outputs", ".", "loss", "+", "alpha", "*", "loss2", "\n", "avg_loss1", ".", "append", "(", "float", "(", "outputs", ".", "loss", ")", ")", "\n", "avg_loss2", ".", "append", "(", "float", "(", "alpha", "*", "loss2", ")", ")", "\n", "\n", "loss", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "optim", ".", "step", "(", ")", "\n", "\n", "if", "i", "<", "3", "and", "epoch", "==", "0", ":", "\n", "                        ", "print", "(", ")", "\n", "print", "(", "\"--------Start--------\"", ")", "\n", "print", "(", "\"Alpha = \"", ",", "alpha", ")", "\n", "print", "(", "'Source:'", ",", "tokenizer", ".", "decode", "(", "batch", "[", "'input_ids'", "]", "[", "0", "]", ")", ")", "\n", "print", "(", "'Target:'", ",", "batch_label", ")", "\n", "print", "(", "'Query:'", ",", "batch_query", ")", "\n", "print", "(", "'LM Loss:'", ",", "outputs", "[", "0", "]", ",", "'Auxiliary Loss:'", ",", "loss2", ")", "\n", "print", "(", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_log.txt'", "%", "alpha", ",", "'a'", ")", "as", "file", ":", "\n", "                    ", "file", ".", "write", "(", "'\\n'", ")", "\n", "file", ".", "write", "(", "\"Epoch \"", ")", "\n", "file", ".", "write", "(", "str", "(", "epoch", ")", ")", "\n", "file", ".", "write", "(", "'\\n'", ")", "\n", "file", ".", "writelines", "(", "[", "'Source: '", ",", "tokenizer", ".", "decode", "(", "batch", "[", "'input_ids'", "]", "[", "0", "]", ")", ",", "'\\n'", ",", "'Target: '", ",", "tokenizer", ".", "decode", "(", "batch", "[", "'labels'", "]", "[", "0", "]", ")", ",", "'\\n'", ",", "'Query: '", ",", "batch_query", ",", "'\\n'", ",", "\"Avg loss1 = \"", "+", "str", "(", "np", ".", "mean", "(", "avg_loss1", ")", ")", ",", "\"Avg loss2 = \"", "+", "str", "(", "np", ".", "mean", "(", "avg_loss2", ")", ")", ",", "\"Avg total loss= \"", "+", "str", "(", "np", ".", "mean", "(", "avg_loss1", ")", "+", "np", ".", "mean", "(", "avg_loss2", ")", ")", "]", ")", "\n", "\n", "", "", "path_model", "=", "models_path_prefix", "+", "\"/alpha=%f_models\"", "%", "alpha", "\n", "model", ".", "save_pretrained", "(", "path_model", ")", "\n", "tokenizer", ".", "save_pretrained", "(", "path_model", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "generated_results", "=", "[", "]", "\n", "generated_results_labels", "=", "[", "]", "\n", "for", "batch", "in", "test_loader", ":", "\n", "                ", "input_ids", "=", "batch", "[", "'input_ids'", "]", ".", "to", "(", "device", ")", "\n", "summary_ids", "=", "model", ".", "generate", "(", "input_ids", ")", "\n", "generated_txt", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "summary_ids", "]", "[", "0", "]", "\n", "\n", "batch_label", "=", "tokenizer", ".", "decode", "(", "batch", "[", "'labels'", "]", "[", "0", "]", ")", "\n", "batch_label", "=", "batch_label", "[", "batch_label", ".", "find", "(", "'<s>'", ")", "+", "len", "(", "'<s>'", ")", ":", "batch_label", ".", "find", "(", "'</s>'", ")", "]", "\n", "\n", "generated_results", ".", "append", "(", "generated_txt", ")", "\n", "generated_results_labels", ".", "append", "(", "batch_label", ")", "\n", "\n", "\n", "\n", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_test_generated.txt'", "%", "alpha", ",", "'w'", ")", "as", "file", ":", "\n", "                ", "for", "txt", "in", "generated_results", ":", "\n", "                    ", "file", ".", "write", "(", "txt", "+", "'\\n'", ")", "\n", "\n", "", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_test_labels.txt'", "%", "alpha", ",", "'w'", ")", "as", "file", ":", "\n", "                ", "for", "txt", "in", "generated_results_labels", ":", "\n", "                    ", "file", ".", "write", "(", "txt", "+", "'\\n'", ")", "\n", "\n", "\n", "", "", "generated_results", "=", "[", "]", "\n", "generated_results_labels", "=", "[", "]", "\n", "for", "batch", "in", "dev_loader", ":", "\n", "                ", "input_ids", "=", "batch", "[", "'input_ids'", "]", ".", "to", "(", "device", ")", "\n", "summary_ids", "=", "model", ".", "generate", "(", "input_ids", ")", "\n", "generated_txt", "=", "[", "tokenizer", ".", "decode", "(", "g", ",", "skip_special_tokens", "=", "True", ",", "clean_up_tokenization_spaces", "=", "False", ")", "for", "g", "in", "summary_ids", "]", "[", "0", "]", "\n", "\n", "batch_label", "=", "tokenizer", ".", "decode", "(", "batch", "[", "'labels'", "]", "[", "0", "]", ")", "\n", "batch_label", "=", "batch_label", "[", "batch_label", ".", "find", "(", "'<s>'", ")", "+", "len", "(", "'<s>'", ")", ":", "batch_label", ".", "find", "(", "'</s>'", ")", "]", "\n", "\n", "generated_results", ".", "append", "(", "generated_txt", ")", "\n", "generated_results_labels", ".", "append", "(", "batch_label", ")", "\n", "\n", "\n", "\n", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_dev_generated.txt'", "%", "alpha", ",", "'w'", ")", "as", "file", ":", "\n", "                ", "for", "txt", "in", "generated_results", ":", "\n", "                    ", "file", ".", "write", "(", "txt", "+", "'\\n'", ")", "\n", "\n", "", "", "with", "open", "(", "results_path_prefix", "+", "'/alpha=%f_dev_labels.txt'", "%", "alpha", ",", "'w'", ")", "as", "file", ":", "\n", "                ", "for", "txt", "in", "generated_results_labels", ":", "\n", "                    ", "file", ".", "write", "(", "txt", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.model.BartClassificationHead.__init__": [[25, 36], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Dropout", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.utils.Dataset.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "input_dim", ":", "int", ",", "\n", "inner_dim", ":", "int", ",", "\n", "num_classes", ":", "int", ",", "\n", "pooler_dropout", ":", "float", ",", "\n", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "input_dim", ",", "inner_dim", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "pooler_dropout", ")", "\n", "self", ".", "out_proj", "=", "nn", ".", "Linear", "(", "inner_dim", ",", "num_classes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.model.BartClassificationHead.forward": [[37, 44], ["model.BartClassificationHead.dropout", "model.BartClassificationHead.dense", "torch.tanh", "model.BartClassificationHead.dropout", "model.BartClassificationHead.out_proj"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ":", "torch", ".", "Tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "torch", ".", "tanh", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "out_proj", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.model.MultiTaskBart.__init__": [[47, 67], ["transformers.BartForConditionalGeneration.__init__", "model.BartClassificationHead", "model.MultiTaskBart.model._init_weights", "model.MultiTaskBart.model._init_weights", "model.BartClassificationHead", "model.MultiTaskBart.model._init_weights", "model.MultiTaskBart.model._init_weights"], "methods", ["home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.utils.Dataset.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "classification_head", "=", "BartClassificationHead", "(", "\n", "config", ".", "d_model", "*", "2", ",", "\n", "config", ".", "d_model", "*", "2", ",", "\n", "2", ",", "#num_labels", "\n", "config", ".", "classifier_dropout", ",", "\n", ")", "\n", "self", ".", "model", ".", "_init_weights", "(", "self", ".", "classification_head", ".", "dense", ")", "\n", "self", ".", "model", ".", "_init_weights", "(", "self", ".", "classification_head", ".", "out_proj", ")", "\n", "\n", "self", ".", "classification_head2", "=", "BartClassificationHead", "(", "\n", "config", ".", "d_model", "*", "2", ",", "\n", "config", ".", "d_model", "*", "2", ",", "\n", "2", ",", "\n", "config", ".", "classifier_dropout", ",", "\n", ")", "\n", "self", ".", "model", ".", "_init_weights", "(", "self", ".", "classification_head2", ".", "dense", ")", "\n", "self", ".", "model", ".", "_init_weights", "(", "self", ".", "classification_head2", ".", "out_proj", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.model.MultiTaskBart.forward": [[68, 141], ["model.MultiTaskBart.model", "transformers.modeling_outputs.Seq2SeqLMOutput", "model.MultiTaskBart.lm_head", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "model.shift_tokens_right", "lm_logits.view", "labels.view"], "methods", ["home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.model.shift_tokens_right"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "input_ids", "=", "None", ",", "\n", "attention_mask", "=", "None", ",", "\n", "decoder_input_ids", "=", "None", ",", "\n", "decoder_attention_mask", "=", "None", ",", "\n", "head_mask", "=", "None", ",", "\n", "decoder_head_mask", "=", "None", ",", "\n", "encoder_outputs", "=", "None", ",", "\n", "past_key_values", "=", "None", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", "decoder_inputs_embeds", "=", "None", ",", "\n", "labels", "=", "None", ",", "\n", "use_cache", "=", "None", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", "return_dict", "=", "None", ",", "\n", ")", ":", "\n", "        ", "r\"\"\"\n        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n            Labels for computing the masked language modeling loss. Indices should either be in ``[0, ...,\n            config.vocab_size]`` or -100 (see ``input_ids`` docstring). Tokens with indices set to ``-100`` are ignored\n            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``.\n\n        Returns:\n        \"\"\"", "\n", "return_dict", "=", "return_dict", "if", "return_dict", "is", "not", "None", "else", "self", ".", "config", ".", "use_return_dict", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "if", "decoder_input_ids", "is", "None", ":", "\n", "                ", "decoder_input_ids", "=", "shift_tokens_right", "(", "\n", "labels", ",", "self", ".", "config", ".", "pad_token_id", ",", "self", ".", "config", ".", "decoder_start_token_id", "\n", ")", "\n", "\n", "", "", "outputs", "=", "self", ".", "model", "(", "\n", "input_ids", ",", "\n", "attention_mask", "=", "attention_mask", ",", "\n", "decoder_input_ids", "=", "decoder_input_ids", ",", "\n", "encoder_outputs", "=", "encoder_outputs", ",", "\n", "decoder_attention_mask", "=", "decoder_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "decoder_head_mask", "=", "decoder_head_mask", ",", "\n", "past_key_values", "=", "past_key_values", ",", "\n", "inputs_embeds", "=", "inputs_embeds", ",", "\n", "decoder_inputs_embeds", "=", "decoder_inputs_embeds", ",", "\n", "use_cache", "=", "use_cache", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "lm_logits", "=", "self", ".", "lm_head", "(", "outputs", "[", "0", "]", ")", "+", "self", ".", "final_logits_bias", "\n", "#print(outputs[0])", "\n", "\n", "masked_lm_loss", "=", "None", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "lm_logits", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "\n", "", "if", "not", "return_dict", ":", "\n", "            ", "output", "=", "(", "lm_logits", ",", ")", "+", "outputs", "[", "1", ":", "]", "\n", "return", "(", "(", "masked_lm_loss", ",", ")", "+", "output", ")", "if", "masked_lm_loss", "is", "not", "None", "else", "output", "\n", "\n", "# modified: here we change the output of encoder_last_hidden_state to decoder's last hidden state", "\n", "", "return", "Seq2SeqLMOutput", "(", "\n", "loss", "=", "masked_lm_loss", ",", "\n", "logits", "=", "lm_logits", ",", "\n", "past_key_values", "=", "outputs", ".", "past_key_values", ",", "\n", "decoder_hidden_states", "=", "outputs", ".", "decoder_hidden_states", ",", "\n", "decoder_attentions", "=", "outputs", ".", "decoder_attentions", ",", "\n", "cross_attentions", "=", "outputs", ".", "cross_attentions", ",", "\n", "encoder_last_hidden_state", "=", "outputs", "[", "0", "]", ",", "\n", "encoder_hidden_states", "=", "outputs", ".", "encoder_hidden_states", ",", "\n", "encoder_attentions", "=", "outputs", ".", "encoder_attentions", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.model.shift_tokens_right": [[7, 20], ["input_ids.new_zeros", "input_ids[].clone", "input_ids.new_zeros.masked_fill_"], "function", ["None"], ["def", "shift_tokens_right", "(", "input_ids", ":", "torch", ".", "Tensor", ",", "pad_token_id", ":", "int", ",", "decoder_start_token_id", ":", "int", ")", ":", "\n", "    ", "\"\"\"\n    Shift input ids one token to the right.\n    \"\"\"", "\n", "shifted_input_ids", "=", "input_ids", ".", "new_zeros", "(", "input_ids", ".", "shape", ")", "\n", "shifted_input_ids", "[", ":", ",", "1", ":", "]", "=", "input_ids", "[", ":", ",", ":", "-", "1", "]", ".", "clone", "(", ")", "\n", "shifted_input_ids", "[", ":", ",", "0", "]", "=", "decoder_start_token_id", "\n", "\n", "assert", "pad_token_id", "is", "not", "None", ",", "\"self.model.config.pad_token_id has to be defined.\"", "\n", "# replace possible -100 values in labels by `pad_token_id`", "\n", "shifted_input_ids", ".", "masked_fill_", "(", "shifted_input_ids", "==", "-", "100", ",", "pad_token_id", ")", "\n", "\n", "return", "shifted_input_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.utils.Dataset.__init__": [[17, 20], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "encodings", ",", "labels", ")", ":", "\n", "        ", "self", ".", "encodings", "=", "encodings", "\n", "self", ".", "labels", "=", "labels", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.utils.Dataset.__getitem__": [[21, 27], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "utils.Dataset.encodings.items"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "#print(self.encodings.items()[0])", "\n", "        ", "item", "=", "{", "key", ":", "torch", ".", "tensor", "(", "val", "[", "idx", "]", ")", "for", "key", ",", "val", "in", "self", ".", "encodings", ".", "items", "(", ")", "}", "\n", "#print(self.labels[idx])", "\n", "item", "[", "'labels'", "]", "=", "torch", ".", "tensor", "(", "self", ".", "labels", "[", "idx", "]", ")", "\n", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.utils.Dataset.__len__": [[28, 30], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.CogComp_MultiOpEd.None.utils.load_csv": [[32, 156], ["range", "random.Random().shuffle", "random.Random().shuffle", "range", "range", "range", "range", "range", "range", "T5Tokenizer.from_pretrained.", "utils.Dataset", "T5Tokenizer.from_pretrained.", "utils.Dataset", "T5Tokenizer.from_pretrained.", "utils.Dataset", "open", "csv.reader", "len", "source[].replace", "target[].replace", "query[].replace", "train_texts.append", "train_texts.append", "train_labels.append", "train_labels.append", "train_query.append", "train_query.append", "dev_texts.append", "dev_texts.append", "dev_labels.append", "dev_labels.append", "dev_query.append", "dev_query.append", "len", "test_texts.append", "test_texts.append", "test_labels.append", "test_labels.append", "test_query.append", "test_query.append", "len", "len", "len", "transformers.BartTokenizer.from_pretrained", "transformers.T5Tokenizer.from_pretrained", "T5Tokenizer.from_pretrained.", "T5Tokenizer.from_pretrained.", "T5Tokenizer.from_pretrained.", "query.append", "source.append", "target.append", "range", "range", "random.Random", "random.Random", "len", "len", "len", "len"], "function", ["None"], ["", "", "def", "load_csv", "(", "path", ",", "tok_type", "=", "\"bart\"", ")", ":", "\n", "    ", "\"\"\"loading the csv data file\"\"\"", "\n", "\n", "source", "=", "[", "]", "\n", "target", "=", "[", "]", "\n", "query", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "'r'", ")", "as", "csvfile", ":", "\n", "        ", "csvreader", "=", "csv", ".", "reader", "(", "csvfile", ")", "\n", "for", "row", "in", "csvreader", ":", "\n", "#print(row)", "\n", "            ", "query", ".", "append", "(", "row", "[", "0", "]", ")", "\n", "source", ".", "append", "(", "row", "[", "4", "]", ")", "\n", "target", ".", "append", "(", "row", "[", "3", "]", ")", "\n", "\n", "\n", "", "", "source", "=", "source", "[", "1", ":", "]", "\n", "target", "=", "target", "[", "1", ":", "]", "\n", "query", "=", "query", "[", "1", ":", "]", "\n", "\n", "#print(len(source))", "\n", "#print(len(target))", "\n", "#print(len(query))", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "source", ")", ")", ":", "\n", "        ", "source", "[", "i", "]", "=", "source", "[", "i", "]", ".", "replace", "(", "'\\n'", ",", "''", ")", "\n", "target", "[", "i", "]", "=", "target", "[", "i", "]", ".", "replace", "(", "'\\n'", ",", "''", ")", "\n", "query", "[", "i", "]", "=", "query", "[", "i", "]", ".", "replace", "(", "'\\n'", ",", "''", ")", "\n", "\n", "#print(len(total_texts))", "\n", "# randomize the train/dev/test/ split", "\n", "", "total_texts", "=", "[", "(", "source", "[", "i", "]", ",", "source", "[", "i", "+", "1", "]", ",", "query", "[", "i", "]", ")", "for", "i", "in", "range", "(", "0", ",", "len", "(", "source", ")", "-", "1", ",", "2", ")", "]", "\n", "total_labels", "=", "[", "(", "target", "[", "i", "]", ",", "target", "[", "i", "+", "1", "]", ",", "query", "[", "i", "]", ")", "for", "i", "in", "range", "(", "0", ",", "len", "(", "target", ")", "-", "1", ",", "2", ")", "]", "\n", "#print(total_texts[:3])", "\n", "#print(total_labels[:3])", "\n", "\n", "#print(total_query[:3])", "\n", "random", ".", "Random", "(", "4", ")", ".", "shuffle", "(", "total_texts", ")", "\n", "random", ".", "Random", "(", "4", ")", ".", "shuffle", "(", "total_labels", ")", "\n", "#random.Random(4).shuffle(total_query)", "\n", "#print(total_texts[:3])", "\n", "#print(total_labels[:3])", "\n", "#print(len(total_texts))", "\n", "#print(total_query[:3])", "\n", "\n", "train_len", "=", "len", "(", "total_texts", ")", "*", "7", "//", "10", "\n", "dev_len", "=", "len", "(", "total_texts", ")", "*", "8", "//", "10", "\n", "#print(train_len)", "\n", "\n", "train_texts", "=", "[", "]", "\n", "train_labels", "=", "[", "]", "\n", "train_query", "=", "[", "]", "\n", "\n", "dev_texts", "=", "[", "]", "\n", "dev_labels", "=", "[", "]", "\n", "dev_query", "=", "[", "]", "\n", "\n", "test_texts", "=", "[", "]", "\n", "test_labels", "=", "[", "]", "\n", "test_query", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "train_len", ")", ":", "\n", "        ", "train_texts", ".", "append", "(", "total_texts", "[", "i", "]", "[", "0", "]", ")", "\n", "train_texts", ".", "append", "(", "total_texts", "[", "i", "]", "[", "1", "]", ")", "\n", "train_labels", ".", "append", "(", "total_labels", "[", "i", "]", "[", "0", "]", ")", "\n", "train_labels", ".", "append", "(", "total_labels", "[", "i", "]", "[", "1", "]", ")", "\n", "train_query", ".", "append", "(", "total_texts", "[", "i", "]", "[", "2", "]", ")", "\n", "train_query", ".", "append", "(", "total_labels", "[", "i", "]", "[", "2", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "train_len", ",", "dev_len", ")", ":", "\n", "        ", "dev_texts", ".", "append", "(", "total_texts", "[", "i", "]", "[", "0", "]", ")", "\n", "dev_texts", ".", "append", "(", "total_texts", "[", "i", "]", "[", "1", "]", ")", "\n", "dev_labels", ".", "append", "(", "total_labels", "[", "i", "]", "[", "0", "]", ")", "\n", "dev_labels", ".", "append", "(", "total_labels", "[", "i", "]", "[", "1", "]", ")", "\n", "dev_query", ".", "append", "(", "total_texts", "[", "i", "]", "[", "2", "]", ")", "\n", "dev_query", ".", "append", "(", "total_labels", "[", "i", "]", "[", "2", "]", ")", "\n", "\n", "", "for", "i", "in", "range", "(", "dev_len", ",", "len", "(", "total_texts", ")", ")", ":", "\n", "        ", "test_texts", ".", "append", "(", "total_texts", "[", "i", "]", "[", "0", "]", ")", "\n", "test_texts", ".", "append", "(", "total_texts", "[", "i", "]", "[", "1", "]", ")", "\n", "test_labels", ".", "append", "(", "total_labels", "[", "i", "]", "[", "0", "]", ")", "\n", "test_labels", ".", "append", "(", "total_labels", "[", "i", "]", "[", "1", "]", ")", "\n", "test_query", ".", "append", "(", "total_texts", "[", "i", "]", "[", "2", "]", ")", "\n", "test_query", ".", "append", "(", "total_labels", "[", "i", "]", "[", "2", "]", ")", "\n", "\n", "\n", "\n", "", "dic", "=", "{", "}", "\n", "for", "i", "in", "range", "(", "len", "(", "train_labels", ")", ")", ":", "\n", "        ", "dic", "[", "train_labels", "[", "i", "]", "]", "=", "train_query", "[", "i", "]", "\n", "#if train_query[i]== \"was trump right to kill soleimani?\":", "\n", "#    print(\"here\", train_labels[i])", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "dev_labels", ")", ")", ":", "\n", "        ", "dic", "[", "dev_labels", "[", "i", "]", "]", "=", "dev_query", "[", "i", "]", "\n", "\n", "", "for", "i", "in", "range", "(", "len", "(", "test_labels", ")", ")", ":", "\n", "        ", "dic", "[", "test_labels", "[", "i", "]", "]", "=", "test_query", "[", "i", "]", "\n", "\n", "", "if", "tok_type", "==", "\"bart\"", ":", "\n", "        ", "tokenizer", "=", "BartTokenizer", ".", "from_pretrained", "(", "'facebook/bart-base'", ",", "cache_dir", "=", "\"/shared/siyiliu/transformers/examples/seq2seq/cached_models\"", ")", "\n", "", "else", ":", "\n", "        ", "tokenizer", "=", "T5Tokenizer", ".", "from_pretrained", "(", "'t5-base'", ",", "cache_dir", "=", "\"/shared/siyiliu/transformers/examples/seq2seq/cached_models\"", ")", "\n", "\n", "\n", "\n", "", "train_encodings", "=", "tokenizer", "(", "train_query", ",", "text_pair", "=", "train_texts", ")", "\n", "train_label_encodings", "=", "tokenizer", "(", "train_labels", ")", "[", "'input_ids'", "]", "\n", "train_dataset", "=", "Dataset", "(", "train_encodings", ",", "train_label_encodings", ")", "\n", "\n", "dev_encodings", "=", "tokenizer", "(", "dev_query", ",", "text_pair", "=", "dev_texts", ")", "\n", "dev_label_encodings", "=", "tokenizer", "(", "dev_labels", ")", "[", "'input_ids'", "]", "\n", "dev_dataset", "=", "Dataset", "(", "dev_encodings", ",", "dev_label_encodings", ")", "\n", "\n", "test_encodings", "=", "tokenizer", "(", "test_query", ",", "text_pair", "=", "test_texts", ")", "\n", "test_label_encodings", "=", "tokenizer", "(", "test_labels", ")", "[", "'input_ids'", "]", "\n", "test_dataset", "=", "Dataset", "(", "test_encodings", ",", "test_label_encodings", ")", "\n", "\n", "#print(train_dataset[0])", "\n", "#print(train_dataset[0]['input_ids'])", "\n", "#print(tokenizer.decode(train_dataset[0]['input_ids']))", "\n", "#print(tokenizer.decode(train_dataset[0]['labels']))", "\n", "\n", "\n", "return", "train_dataset", ",", "dev_dataset", ",", "test_dataset", ",", "dic", "\n", "\n"]]}