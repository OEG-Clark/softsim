{"home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_TDGAN_2DperTask_nuclei.save_data": [[45, 84], ["model.get_image_paths", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "file.create_dataset", "file.create_dataset", "file.create_dataset", "file.create_dataset", "file.create_dataset", "numpy.zeros", "skimage.io.imsave", "np.moveaxis.cpu", "np.moveaxis.cpu", "np.moveaxis.cpu", "weight_map.cpu().numpy", "weight_map.cpu"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_image_paths", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset"], ["def", "save_data", "(", "gt", ",", "visuals", ",", "index", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "dataset_num", ",", "append_idx", ",", "web_dir", ")", ":", "\n", "\n", "    ", "real_img", "=", "visuals", "[", "'real_B'", "]", "[", "index", "]", "\n", "syn_img", "=", "visuals", "[", "'fake_B'", "]", "[", "index", "]", "\n", "label", "=", "visuals", "[", "'real_A'", "]", "[", "index", "]", "\n", "label_ternary", "=", "label_ternary", "[", "index", "]", "\n", "weight_map", "=", "weight_map", "[", "index", "]", "\n", "\n", "img_path", "=", "model", ".", "get_image_paths", "(", ")", "# get image paths", "\n", "\n", "syn_img", "=", "syn_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "syn_img", "=", "(", "syn_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "syn_img", "=", "syn_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "syn_img", "=", "np", ".", "moveaxis", "(", "syn_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "label", "=", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label", "=", "(", "label", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "label", "=", "label", ".", "astype", "(", "\"uint8\"", ")", "\n", "label", "=", "np", ".", "moveaxis", "(", "label", ",", "0", ",", "-", "1", ")", "\n", "\n", "real_img", "=", "real_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "real_img", "=", "(", "real_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "real_img", "=", "real_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "real_img", "=", "np", ".", "moveaxis", "(", "real_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "# write to the h5 file", "\n", "file", ".", "create_dataset", "(", "f\"images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "syn_img", ")", "\n", "file", ".", "create_dataset", "(", "f\"labels/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "label", ")", "\n", "file", ".", "create_dataset", "(", "f\"real_images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "real_img", ")", "\n", "file", ".", "create_dataset", "(", "f\"label_ternary/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "label_ternary", ")", "\n", "file", ".", "create_dataset", "(", "f\"weight_map/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "weight_map", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# save as png image file for better visualization: [label, synthetic image, real image]", "\n", "h", ",", "w", ",", "c", "=", "syn_img", ".", "shape", "\n", "new_img", "=", "np", ".", "zeros", "(", "(", "h", ",", "3", "*", "w", ",", "c", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "new_img", "[", ":", ",", ":", "w", ",", ":", "]", "=", "label", "\n", "new_img", "[", ":", ",", "w", ":", "2", "*", "w", ",", ":", "]", "=", "syn_img", "\n", "new_img", "[", ":", ",", "2", "*", "w", ":", ",", ":", "]", "=", "real_img", "\n", "io", ".", "imsave", "(", "'{:s}/images/img_D{:d}_{:s}_{:d}_{:d}.png'", ".", "format", "(", "web_dir", ",", "dataset_num", ",", "img_path", "[", "0", "]", ",", "index", ",", "append_idx", ")", ",", "new_img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_TDGAN_2DperTask_nuclei.launch_test_once": [[86, 96], ["model.test", "model.get_current_visuals", "range", "sys.stdout.flush", "test_TDGAN_2DperTask_nuclei.save_data"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.test", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_current_visuals", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_TDGAN_nuclei.save_data"], ["", "def", "launch_test_once", "(", "dataset_num", ",", "idx", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "web_dir", ")", ":", "\n", "# test again.", "\n", "    ", "model", ".", "test", "(", ")", "# run inference", "\n", "visuals", "=", "model", ".", "get_current_visuals", "(", ")", "# get image results", "\n", "#", "\n", "gt", "=", "visuals", "[", "'real_A'", "]", "\n", "\n", "for", "j", "in", "range", "(", "gt", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "save_data", "(", "gt", ",", "visuals", ",", "j", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "dataset_num", ",", "idx", ",", "web_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_brain.save_brain_data": [[45, 99], ["os.makedirs", "model.get_image_paths", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "file.create_dataset", "file.create_dataset", "file.create_dataset", "file.create_dataset", "numpy.zeros", "numpy.array", "numpy.array", "numpy.array", "numpy.array", "PIL.Image.fromarray", "Image.fromarray.save", "numpy.array", "np.moveaxis.cpu", "np.moveaxis.cpu", "np.moveaxis.cpu", "np.moveaxis.cpu"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_image_paths", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.save"], ["def", "save_brain_data", "(", "gt", ",", "visuals", ",", "index", ",", "model", ",", "file", ",", "dataset_num", ",", "seg_label", ",", "append_idx", ",", "save_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "real_img", "=", "visuals", "[", "'real_B'", "]", "[", "index", "]", "\n", "syn_img", "=", "visuals", "[", "'fake_B'", "]", "[", "index", "]", "\n", "label", "=", "visuals", "[", "'real_A'", "]", "[", "index", "]", "\n", "seg_label", "=", "seg_label", "[", "index", "]", "\n", "\n", "img_path", "=", "model", ".", "get_image_paths", "(", ")", "# get image paths", "\n", "\n", "syn_img", "=", "syn_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "syn_img", "=", "(", "syn_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "syn_img", "=", "syn_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "syn_img", "=", "np", ".", "moveaxis", "(", "syn_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "label", "=", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label", "=", "(", "label", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "label", "=", "label", ".", "astype", "(", "\"uint8\"", ")", "\n", "label", "=", "np", ".", "moveaxis", "(", "label", ",", "0", ",", "-", "1", ")", "\n", "\n", "seg_label", "=", "seg_label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "seg_label", "=", "(", "seg_label", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "seg_label", "=", "seg_label", ".", "astype", "(", "\"uint8\"", ")", "\n", "seg_label", "=", "np", ".", "moveaxis", "(", "seg_label", ",", "0", ",", "-", "1", ")", "\n", "\n", "real_img", "=", "real_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "real_img", "=", "(", "real_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "real_img", "=", "real_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "real_img", "=", "np", ".", "moveaxis", "(", "real_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "# # # for brats, resize back to 240x240", "\n", "# if dataset_num > 0: # brats", "\n", "#     size = (240, 240)", "\n", "#     syn_img = Image.fromarray(syn_img).resize(size)", "\n", "#     label = Image.fromarray(label).resize(size, Image.NEAREST)", "\n", "#     seg_label = Image.fromarray(seg_label).resize(size, Image.NEAREST)", "\n", "#     real_img = Image.fromarray(real_img).resize(size)", "\n", "\n", "# write to the h5 file", "\n", "file", ".", "create_dataset", "(", "f\"images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "syn_img", ")", "\n", "file", ".", "create_dataset", "(", "f\"labels/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "label", ")", "\n", "file", ".", "create_dataset", "(", "f\"seg_labels/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "seg_label", ")", "\n", "file", ".", "create_dataset", "(", "f\"real_images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "real_img", ")", "\n", "\n", "# save as png image file for better visualization: [label (with skull), seg_label, synthetic image, real image]", "\n", "h", ",", "w", ",", "c", "=", "np", ".", "array", "(", "syn_img", ")", ".", "shape", "\n", "new_img", "=", "np", ".", "zeros", "(", "(", "h", ",", "4", "*", "w", ",", "c", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "new_img", "[", ":", ",", ":", "w", ",", ":", "]", "=", "np", ".", "array", "(", "label", ")", "\n", "new_img", "[", ":", ",", "w", ":", "2", "*", "w", ",", ":", "]", "=", "np", ".", "array", "(", "seg_label", ")", "\n", "new_img", "[", ":", ",", "2", "*", "w", ":", "3", "*", "w", ",", ":", "]", "=", "np", ".", "array", "(", "syn_img", ")", "\n", "new_img", "[", ":", ",", "3", "*", "w", ":", ",", ":", "]", "=", "np", ".", "array", "(", "real_img", ")", "\n", "new_img", "=", "Image", ".", "fromarray", "(", "new_img", ")", "\n", "\n", "new_img", ".", "save", "(", "'{:s}/img_D{:d}_{:s}_{:d}_{:d}.png'", ".", "format", "(", "save_dir", ",", "dataset_num", ",", "img_path", "[", "0", "]", ",", "index", ",", "append_idx", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_brain.save_nuclei_data": [[101, 141], ["os.makedirs", "model.get_image_paths", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "file.create_dataset", "file.create_dataset", "file.create_dataset", "file.create_dataset", "file.create_dataset", "numpy.zeros", "skimage.io.imsave", "np.moveaxis.cpu", "np.moveaxis.cpu", "np.moveaxis.cpu", "weight_map.cpu().numpy", "weight_map.cpu"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_image_paths", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset"], ["", "def", "save_nuclei_data", "(", "gt", ",", "visuals", ",", "index", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "dataset_num", ",", "append_idx", ",", "save_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "real_img", "=", "visuals", "[", "'real_B'", "]", "[", "index", "]", "\n", "syn_img", "=", "visuals", "[", "'fake_B'", "]", "[", "index", "]", "\n", "label", "=", "visuals", "[", "'real_A'", "]", "[", "index", "]", "\n", "label_ternary", "=", "label_ternary", "[", "index", "]", "\n", "weight_map", "=", "weight_map", "[", "index", "]", "\n", "\n", "img_path", "=", "model", ".", "get_image_paths", "(", ")", "# get image paths", "\n", "\n", "syn_img", "=", "syn_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "syn_img", "=", "(", "syn_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "syn_img", "=", "syn_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "syn_img", "=", "np", ".", "moveaxis", "(", "syn_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "label", "=", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label", "=", "(", "label", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "label", "=", "label", ".", "astype", "(", "\"uint8\"", ")", "\n", "label", "=", "np", ".", "moveaxis", "(", "label", ",", "0", ",", "-", "1", ")", "\n", "\n", "real_img", "=", "real_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "real_img", "=", "(", "real_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "real_img", "=", "real_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "real_img", "=", "np", ".", "moveaxis", "(", "real_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "# write to the h5 file", "\n", "file", ".", "create_dataset", "(", "f\"images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "syn_img", ")", "\n", "file", ".", "create_dataset", "(", "f\"labels/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "label", ")", "\n", "file", ".", "create_dataset", "(", "f\"real_images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "real_img", ")", "\n", "file", ".", "create_dataset", "(", "f\"label_ternary/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "label_ternary", ")", "\n", "file", ".", "create_dataset", "(", "f\"weight_map/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "weight_map", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# save as png image file for better visualization: [label, synthetic image, real image]", "\n", "h", ",", "w", ",", "c", "=", "syn_img", ".", "shape", "\n", "new_img", "=", "np", ".", "zeros", "(", "(", "h", ",", "3", "*", "w", ",", "c", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "new_img", "[", ":", ",", ":", "w", ",", ":", "]", "=", "label", "\n", "new_img", "[", ":", ",", "w", ":", "2", "*", "w", ",", ":", "]", "=", "syn_img", "\n", "new_img", "[", ":", ",", "2", "*", "w", ":", ",", ":", "]", "=", "real_img", "\n", "io", ".", "imsave", "(", "'{:s}/img_D{:d}_{:s}_{:d}_{:d}.png'", ".", "format", "(", "save_dir", ",", "dataset_num", ",", "img_path", "[", "0", "]", ",", "index", ",", "append_idx", ")", ",", "new_img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_brain.launch_brain_test_once": [[143, 153], ["model.test", "model.get_current_visuals", "range", "sys.stdout.flush", "test_brain.save_brain_data"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.test", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_current_visuals", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_brain.save_brain_data"], ["", "def", "launch_brain_test_once", "(", "dataset_num", ",", "idx", ",", "model", ",", "file", ",", "seg_label", ",", "save_dir", ")", ":", "\n", "# test again.", "\n", "    ", "model", ".", "test", "(", ")", "# run inference", "\n", "visuals", "=", "model", ".", "get_current_visuals", "(", ")", "# get image results", "\n", "#", "\n", "gt", "=", "visuals", "[", "'real_A'", "]", "\n", "\n", "for", "j", "in", "range", "(", "gt", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "save_brain_data", "(", "gt", ",", "visuals", ",", "j", ",", "model", ",", "file", ",", "dataset_num", ",", "seg_label", ",", "idx", ",", "save_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_brain.launch_nuclei_test_once": [[155, 165], ["model.test", "model.get_current_visuals", "range", "sys.stdout.flush", "test_brain.save_nuclei_data"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.test", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_current_visuals", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_brain.save_nuclei_data"], ["", "", "def", "launch_nuclei_test_once", "(", "dataset_num", ",", "idx", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "save_dir", ")", ":", "\n", "# test again.", "\n", "    ", "model", ".", "test", "(", ")", "# run inference", "\n", "visuals", "=", "model", ".", "get_current_visuals", "(", ")", "# get image results", "\n", "#", "\n", "gt", "=", "visuals", "[", "'real_A'", "]", "\n", "\n", "for", "j", "in", "range", "(", "gt", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "save_nuclei_data", "(", "gt", ",", "visuals", ",", "j", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "dataset_num", ",", "idx", ",", "save_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_pix2pix_nuclei.save_data": [[45, 84], ["model.get_image_paths", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "file.create_dataset", "file.create_dataset", "file.create_dataset", "file.create_dataset", "file.create_dataset", "numpy.zeros", "skimage.io.imsave", "np.moveaxis.cpu", "np.moveaxis.cpu", "np.moveaxis.cpu", "weight_map.cpu().numpy", "weight_map.cpu"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_image_paths", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset"], ["def", "save_data", "(", "gt", ",", "visuals", ",", "index", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "dataset_num", ",", "append_idx", ",", "web_dir", ")", ":", "\n", "\n", "    ", "real_img", "=", "visuals", "[", "'real_B'", "]", "[", "index", "]", "\n", "syn_img", "=", "visuals", "[", "'fake_B'", "]", "[", "index", "]", "\n", "label", "=", "visuals", "[", "'real_A'", "]", "[", "index", "]", "\n", "label_ternary", "=", "label_ternary", "[", "index", "]", "\n", "weight_map", "=", "weight_map", "[", "index", "]", "\n", "\n", "img_path", "=", "model", ".", "get_image_paths", "(", ")", "# get image paths", "\n", "\n", "syn_img", "=", "syn_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "syn_img", "=", "(", "syn_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "syn_img", "=", "syn_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "syn_img", "=", "np", ".", "moveaxis", "(", "syn_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "label", "=", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label", "=", "(", "label", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "label", "=", "label", ".", "astype", "(", "\"uint8\"", ")", "\n", "label", "=", "np", ".", "moveaxis", "(", "label", ",", "0", ",", "-", "1", ")", "\n", "\n", "real_img", "=", "real_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "real_img", "=", "(", "real_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "real_img", "=", "real_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "real_img", "=", "np", ".", "moveaxis", "(", "real_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "# write to the h5 file", "\n", "file", ".", "create_dataset", "(", "f\"images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "syn_img", ")", "\n", "file", ".", "create_dataset", "(", "f\"labels/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "label", ")", "\n", "file", ".", "create_dataset", "(", "f\"real_images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "real_img", ")", "\n", "file", ".", "create_dataset", "(", "f\"label_ternary/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "label_ternary", ")", "\n", "file", ".", "create_dataset", "(", "f\"weight_map/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "weight_map", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# save as png image file for better visualization: [label, synthetic image, real image]", "\n", "h", ",", "w", ",", "c", "=", "syn_img", ".", "shape", "\n", "new_img", "=", "np", ".", "zeros", "(", "(", "h", ",", "3", "*", "w", ",", "c", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "new_img", "[", ":", ",", ":", "w", ",", ":", "]", "=", "label", "\n", "new_img", "[", ":", ",", "w", ":", "2", "*", "w", ",", ":", "]", "=", "syn_img", "\n", "new_img", "[", ":", ",", "2", "*", "w", ":", ",", ":", "]", "=", "real_img", "\n", "io", ".", "imsave", "(", "'{:s}/images/img_D{:d}_{:s}_{:d}_{:d}.png'", ".", "format", "(", "web_dir", ",", "dataset_num", ",", "img_path", "[", "0", "]", ",", "index", ",", "append_idx", ")", ",", "new_img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_pix2pix_nuclei.launch_test_once": [[86, 96], ["model.test", "model.get_current_visuals", "range", "sys.stdout.flush", "test_pix2pix_nuclei.save_data"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.test", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_current_visuals", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_TDGAN_nuclei.save_data"], ["", "def", "launch_test_once", "(", "dataset_num", ",", "idx", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "web_dir", ")", ":", "\n", "# test again.", "\n", "    ", "model", ".", "test", "(", ")", "# run inference", "\n", "visuals", "=", "model", ".", "get_current_visuals", "(", ")", "# get image results", "\n", "#", "\n", "gt", "=", "visuals", "[", "'real_A'", "]", "\n", "\n", "for", "j", "in", "range", "(", "gt", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "save_data", "(", "gt", ",", "visuals", ",", "j", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "dataset_num", ",", "idx", ",", "web_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_TDGAN_nuclei.save_data": [[45, 84], ["model.get_image_paths", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "np.moveaxis.cpu().numpy", "np.moveaxis.astype", "numpy.moveaxis", "file.create_dataset", "file.create_dataset", "file.create_dataset", "file.create_dataset", "file.create_dataset", "numpy.zeros", "skimage.io.imsave", "np.moveaxis.cpu", "np.moveaxis.cpu", "np.moveaxis.cpu", "weight_map.cpu().numpy", "weight_map.cpu"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_image_paths", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset"], ["def", "save_data", "(", "gt", ",", "visuals", ",", "index", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "dataset_num", ",", "append_idx", ",", "web_dir", ")", ":", "\n", "\n", "    ", "real_img", "=", "visuals", "[", "'real_B'", "]", "[", "index", "]", "\n", "syn_img", "=", "visuals", "[", "'fake_B'", "]", "[", "index", "]", "\n", "label", "=", "visuals", "[", "'real_A'", "]", "[", "index", "]", "\n", "label_ternary", "=", "label_ternary", "[", "index", "]", "\n", "weight_map", "=", "weight_map", "[", "index", "]", "\n", "\n", "img_path", "=", "model", ".", "get_image_paths", "(", ")", "# get image paths", "\n", "\n", "syn_img", "=", "syn_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "syn_img", "=", "(", "syn_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "syn_img", "=", "syn_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "syn_img", "=", "np", ".", "moveaxis", "(", "syn_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "label", "=", "label", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label", "=", "(", "label", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "label", "=", "label", ".", "astype", "(", "\"uint8\"", ")", "\n", "label", "=", "np", ".", "moveaxis", "(", "label", ",", "0", ",", "-", "1", ")", "\n", "\n", "real_img", "=", "real_img", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "real_img", "=", "(", "real_img", "+", "1", ")", "*", "(", "255", "/", "2", ")", "\n", "real_img", "=", "real_img", ".", "astype", "(", "\"uint8\"", ")", "\n", "real_img", "=", "np", ".", "moveaxis", "(", "real_img", ",", "0", ",", "-", "1", ")", "\n", "\n", "# write to the h5 file", "\n", "file", ".", "create_dataset", "(", "f\"images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "syn_img", ")", "\n", "file", ".", "create_dataset", "(", "f\"labels/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "label", ")", "\n", "file", ".", "create_dataset", "(", "f\"real_images/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "real_img", ")", "\n", "file", ".", "create_dataset", "(", "f\"label_ternary/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "label_ternary", ")", "\n", "file", ".", "create_dataset", "(", "f\"weight_map/D{dataset_num}_{img_path[0]}_{index}_{append_idx}\"", ",", "data", "=", "weight_map", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "# save as png image file for better visualization: [label, synthetic image, real image]", "\n", "h", ",", "w", ",", "c", "=", "syn_img", ".", "shape", "\n", "new_img", "=", "np", ".", "zeros", "(", "(", "h", ",", "3", "*", "w", ",", "c", ")", ",", "dtype", "=", "np", ".", "uint8", ")", "\n", "new_img", "[", ":", ",", ":", "w", ",", ":", "]", "=", "label", "\n", "new_img", "[", ":", ",", "w", ":", "2", "*", "w", ",", ":", "]", "=", "syn_img", "\n", "new_img", "[", ":", ",", "2", "*", "w", ":", ",", ":", "]", "=", "real_img", "\n", "io", ".", "imsave", "(", "'{:s}/images/img_D{:d}_{:s}_{:d}_{:d}.png'", ".", "format", "(", "web_dir", ",", "dataset_num", ",", "img_path", "[", "0", "]", ",", "index", ",", "append_idx", ")", ",", "new_img", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_TDGAN_nuclei.launch_test_once": [[86, 96], ["model.test", "model.get_current_visuals", "range", "sys.stdout.flush", "test_TDGAN_nuclei.save_data"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.test", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_current_visuals", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.test_TDGAN_nuclei.save_data"], ["", "def", "launch_test_once", "(", "dataset_num", ",", "idx", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "web_dir", ")", ":", "\n", "# test again.", "\n", "    ", "model", ".", "test", "(", ")", "# run inference", "\n", "visuals", "=", "model", ".", "get_current_visuals", "(", ")", "# get image results", "\n", "#", "\n", "gt", "=", "visuals", "[", "'real_A'", "]", "\n", "\n", "for", "j", "in", "range", "(", "gt", ".", "shape", "[", "0", "]", ")", ":", "\n", "        ", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "save_data", "(", "gt", ",", "visuals", ",", "j", ",", "model", ",", "file", ",", "label_ternary", ",", "weight_map", ",", "dataset_num", ",", "idx", ",", "web_dir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config.ConfigParser.__init__": [[12, 48], ["parse_config._update_config", "pathlib.Path", "parse_config.ConfigParser.save_dir.mkdir", "parse_config.ConfigParser.log_dir.mkdir", "tmp.utils.write_json", "tmp.logger.setup_logging", "datetime.datetime.datetime.now().strftime", "datetime.datetime.datetime.now"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config._update_config", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.mkdir", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.mkdir", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.write_json", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.logger.logger.setup_logging"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "resume", "=", "None", ",", "modification", "=", "None", ",", "run_id", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n        class to parse configuration json file. Handles hyperparameters for training, initializations of modules, checkpoint saving\n        and logging module.\n        :param config: Dict containing configurations, hyperparameters for training. contents of `config.json` file for example.\n        :param resume: String, path to the checkpoint being loaded.\n        :param modification: Dict keychain:value, specifying position values to be replaced from config dict.\n        :param run_id: Unique Identifier for training processes. Used to save checkpoints and training log. Timestamp is being used as default\n        \"\"\"", "\n", "# load config file and apply modification", "\n", "self", ".", "_config", "=", "_update_config", "(", "config", ",", "modification", ")", "\n", "self", ".", "resume", "=", "resume", "\n", "\n", "# set save_dir where trained model and log will be saved.", "\n", "save_dir", "=", "Path", "(", "self", ".", "config", "[", "'trainer'", "]", "[", "'save_dir'", "]", ")", "\n", "\n", "exper_name", "=", "self", ".", "config", "[", "'name'", "]", "\n", "if", "run_id", "is", "None", ":", "# use timestamp as default run-id", "\n", "            ", "run_id", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "r'%m%d_%H%M%S'", ")", "\n", "", "self", ".", "_save_dir", "=", "save_dir", "/", "'models'", "/", "exper_name", "/", "run_id", "\n", "self", ".", "_log_dir", "=", "save_dir", "/", "'log'", "/", "exper_name", "/", "run_id", "\n", "\n", "# make directory for saving checkpoints and log.", "\n", "exist_ok", "=", "run_id", "==", "''", "\n", "self", ".", "save_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "exist_ok", ")", "\n", "self", ".", "log_dir", ".", "mkdir", "(", "parents", "=", "True", ",", "exist_ok", "=", "exist_ok", ")", "\n", "\n", "# save updated config file to the checkpoint dir", "\n", "write_json", "(", "self", ".", "config", ",", "self", ".", "save_dir", "/", "'config.json'", ")", "\n", "\n", "# configure logging module", "\n", "setup_logging", "(", "self", ".", "log_dir", ")", "\n", "self", ".", "log_levels", "=", "{", "\n", "0", ":", "logging", ".", "WARNING", ",", "\n", "1", ":", "logging", ".", "INFO", ",", "\n", "2", ":", "logging", ".", "DEBUG", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config.ConfigParser.from_args": [[50, 79], ["tmp.utils.read_json", "cls", "args.parse_args.parse_args.add_argument", "isinstance", "args.parse_args.parse_args.parse_args", "pathlib.Path", "pathlib.Path", "tmp.utils.read_json.update", "getattr", "tmp.utils.read_json", "parse_config._get_opt_name"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.read_json", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.MetricTracker.update", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.read_json", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config._get_opt_name"], ["", "@", "classmethod", "\n", "def", "from_args", "(", "cls", ",", "args", ",", "options", "=", "''", ")", ":", "\n", "        ", "\"\"\"\n        Initialize this class from some cli arguments. Used in train, test.\n        \"\"\"", "\n", "for", "opt", "in", "options", ":", "\n", "            ", "args", ".", "add_argument", "(", "*", "opt", ".", "flags", ",", "default", "=", "None", ",", "type", "=", "opt", ".", "type", ")", "\n", "", "if", "not", "isinstance", "(", "args", ",", "tuple", ")", ":", "\n", "            ", "args", "=", "args", ".", "parse_args", "(", ")", "\n", "\n", "", "if", "args", ".", "device", "is", "not", "None", ":", "\n", "            ", "os", ".", "environ", "[", "\"CUDA_VISIBLE_DEVICES\"", "]", "=", "args", ".", "device", "\n", "", "if", "args", ".", "resume", "is", "not", "None", ":", "\n", "            ", "resume", "=", "Path", "(", "args", ".", "resume", ")", "\n", "cfg_fname", "=", "resume", ".", "parent", "/", "'config.json'", "\n", "", "else", ":", "\n", "            ", "msg_no_cfg", "=", "\"Configuration file need to be specified. Add '-c config.json', for example.\"", "\n", "assert", "args", ".", "config", "is", "not", "None", ",", "msg_no_cfg", "\n", "resume", "=", "None", "\n", "cfg_fname", "=", "Path", "(", "args", ".", "config", ")", "\n", "\n", "", "config", "=", "read_json", "(", "cfg_fname", ")", "\n", "if", "args", ".", "config", "and", "resume", ":", "\n", "# update new config for fine-tuning", "\n", "            ", "config", ".", "update", "(", "read_json", "(", "args", ".", "config", ")", ")", "\n", "\n", "# parse custom cli options into dictionary", "\n", "", "modification", "=", "{", "opt", ".", "target", ":", "getattr", "(", "args", ",", "_get_opt_name", "(", "opt", ".", "flags", ")", ")", "for", "opt", "in", "options", "}", "\n", "return", "cls", "(", "config", ",", "resume", ",", "modification", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config.ConfigParser.init_obj": [[80, 94], ["dict", "all", "dict.update", "getattr"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.MetricTracker.update"], ["", "def", "init_obj", "(", "self", ",", "name", ",", "module", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Finds a function handle with the name given as 'type' in config, and returns the\n        instance initialized with corresponding arguments given.\n\n        `object = config.init_obj('name', module, a, b=1)`\n        is equivalent to\n        `object = module.name(a, b=1)`\n        \"\"\"", "\n", "module_name", "=", "self", "[", "name", "]", "[", "'type'", "]", "\n", "module_args", "=", "dict", "(", "self", "[", "name", "]", "[", "'args'", "]", ")", "\n", "assert", "all", "(", "[", "k", "not", "in", "module_args", "for", "k", "in", "kwargs", "]", ")", ",", "'Overwriting kwargs given in config file is not allowed'", "\n", "module_args", ".", "update", "(", "kwargs", ")", "\n", "return", "getattr", "(", "module", ",", "module_name", ")", "(", "*", "args", ",", "**", "module_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config.ConfigParser.init_ftn": [[95, 109], ["dict", "all", "dict.update", "functools.partial", "getattr"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.MetricTracker.update"], ["", "def", "init_ftn", "(", "self", ",", "name", ",", "module", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Finds a function handle with the name given as 'type' in config, and returns the\n        function with given arguments fixed with functools.partial.\n\n        `function = config.init_ftn('name', module, a, b=1)`\n        is equivalent to\n        `function = lambda *args, **kwargs: module.name(a, *args, b=1, **kwargs)`.\n        \"\"\"", "\n", "module_name", "=", "self", "[", "name", "]", "[", "'type'", "]", "\n", "module_args", "=", "dict", "(", "self", "[", "name", "]", "[", "'args'", "]", ")", "\n", "assert", "all", "(", "[", "k", "not", "in", "module_args", "for", "k", "in", "kwargs", "]", ")", ",", "'Overwriting kwargs given in config file is not allowed'", "\n", "module_args", ".", "update", "(", "kwargs", ")", "\n", "return", "partial", "(", "getattr", "(", "module", ",", "module_name", ")", ",", "*", "args", ",", "**", "module_args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config.ConfigParser.__getitem__": [[110, 113], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"Access items like ordinary dict.\"\"\"", "\n", "return", "self", ".", "config", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config.ConfigParser.get_logger": [[114, 120], ["logging.getLogger", "logging.getLogger.setLevel", "parse_config.ConfigParser.log_levels.keys"], "methods", ["None"], ["", "def", "get_logger", "(", "self", ",", "name", ",", "verbosity", "=", "2", ")", ":", "\n", "        ", "msg_verbosity", "=", "'verbosity option {} is invalid. Valid options are {}.'", ".", "format", "(", "verbosity", ",", "self", ".", "log_levels", ".", "keys", "(", ")", ")", "\n", "assert", "verbosity", "in", "self", ".", "log_levels", ",", "msg_verbosity", "\n", "logger", "=", "logging", ".", "getLogger", "(", "name", ")", "\n", "logger", ".", "setLevel", "(", "self", ".", "log_levels", "[", "verbosity", "]", ")", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config.ConfigParser.config": [[122, 125], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "config", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_config", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config.ConfigParser.save_dir": [[126, 129], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "save_dir", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_save_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config.ConfigParser.log_dir": [[130, 133], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "log_dir", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_log_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config._update_config": [[135, 143], ["modification.items", "parse_config._set_by_path"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config._set_by_path"], ["", "", "def", "_update_config", "(", "config", ",", "modification", ")", ":", "\n", "    ", "if", "modification", "is", "None", ":", "\n", "        ", "return", "config", "\n", "\n", "", "for", "k", ",", "v", "in", "modification", ".", "items", "(", ")", ":", "\n", "        ", "if", "v", "is", "not", "None", ":", "\n", "            ", "_set_by_path", "(", "config", ",", "k", ",", "v", ")", "\n", "", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config._get_opt_name": [[144, 149], ["flags[].replace", "flg.startswith", "flg.replace"], "function", ["None"], ["", "def", "_get_opt_name", "(", "flags", ")", ":", "\n", "    ", "for", "flg", "in", "flags", ":", "\n", "        ", "if", "flg", ".", "startswith", "(", "'--'", ")", ":", "\n", "            ", "return", "flg", ".", "replace", "(", "'--'", ",", "''", ")", "\n", "", "", "return", "flags", "[", "0", "]", ".", "replace", "(", "'--'", ",", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config._set_by_path": [[150, 154], ["keys.split.split", "parse_config._get_by_path"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config._get_by_path"], ["", "def", "_set_by_path", "(", "tree", ",", "keys", ",", "value", ")", ":", "\n", "    ", "\"\"\"Set a value in a nested object in tree by sequence of keys.\"\"\"", "\n", "keys", "=", "keys", ".", "split", "(", "';'", ")", "\n", "_get_by_path", "(", "tree", ",", "keys", "[", ":", "-", "1", "]", ")", "[", "keys", "[", "-", "1", "]", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.None.parse_config._get_by_path": [[155, 158], ["functools.reduce"], "function", ["None"], ["", "def", "_get_by_path", "(", "tree", ",", "keys", ")", ":", "\n", "    ", "\"\"\"Access a nested object in tree by sequence of keys.\"\"\"", "\n", "return", "reduce", "(", "getitem", ",", "keys", ",", "tree", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.__init__": [[14, 34], ["os.path.join", "dominate.document", "os.path.exists", "os.makedirs", "os.path.exists", "os.makedirs", "dominate.tags.meta", "str"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "web_dir", ",", "title", ",", "refresh", "=", "0", ")", ":", "\n", "        ", "\"\"\"Initialize the HTML classes\n\n        Parameters:\n            web_dir (str) -- a directory that stores the webpage. HTML file will be created at <web_dir>/index.html; images will be saved at <web_dir/images/\n            title (str)   -- the webpage name\n            refresh (int) -- how often the website refresh itself; if 0; no refreshing\n        \"\"\"", "\n", "self", ".", "title", "=", "title", "\n", "self", ".", "web_dir", "=", "web_dir", "\n", "self", ".", "img_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "web_dir", ",", "'images'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "web_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "web_dir", ")", "\n", "", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "img_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "img_dir", ")", "\n", "\n", "", "self", ".", "doc", "=", "dominate", ".", "document", "(", "title", "=", "title", ")", "\n", "if", "refresh", ">", "0", ":", "\n", "            ", "with", "self", ".", "doc", ".", "head", ":", "\n", "                ", "meta", "(", "http_equiv", "=", "\"refresh\"", ",", "content", "=", "str", "(", "refresh", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.get_image_dir": [[35, 38], ["None"], "methods", ["None"], ["", "", "", "def", "get_image_dir", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the directory that stores images\"\"\"", "\n", "return", "self", ".", "img_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.add_header": [[39, 47], ["dominate.tags.h3"], "methods", ["None"], ["", "def", "add_header", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Insert a header to the HTML file\n\n        Parameters:\n            text (str) -- the header text\n        \"\"\"", "\n", "with", "self", ".", "doc", ":", "\n", "            ", "h3", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.add_images": [[48, 67], ["dominate.tags.table", "html.HTML.doc.add", "dominate.tags.tr", "zip", "dominate.tags.td", "dominate.tags.p", "dominate.tags.br", "dominate.tags.p", "dominate.tags.a", "dominate.tags.img", "os.path.join", "os.path.join"], "methods", ["None"], ["", "", "def", "add_images", "(", "self", ",", "ims", ",", "txts", ",", "links", ",", "width", "=", "400", ")", ":", "\n", "        ", "\"\"\"add images to the HTML file\n\n        Parameters:\n            ims (str list)   -- a list of image paths\n            txts (str list)  -- a list of image names shown on the website\n            links (str list) --  a list of hyperref links; when you click an image, it will redirect you to a new page\n        \"\"\"", "\n", "self", ".", "t", "=", "table", "(", "border", "=", "1", ",", "style", "=", "\"table-layout: fixed;\"", ")", "# Insert a table", "\n", "self", ".", "doc", ".", "add", "(", "self", ".", "t", ")", "\n", "with", "self", ".", "t", ":", "\n", "            ", "with", "tr", "(", ")", ":", "\n", "                ", "for", "im", ",", "txt", ",", "link", "in", "zip", "(", "ims", ",", "txts", ",", "links", ")", ":", "\n", "                    ", "with", "td", "(", "style", "=", "\"word-wrap: break-word;\"", ",", "halign", "=", "\"center\"", ",", "valign", "=", "\"top\"", ")", ":", "\n", "                        ", "with", "p", "(", ")", ":", "\n", "                            ", "with", "a", "(", "href", "=", "os", ".", "path", ".", "join", "(", "'images'", ",", "link", ")", ")", ":", "\n", "                                ", "img", "(", "style", "=", "\"width:%dpx\"", "%", "width", ",", "src", "=", "os", ".", "path", ".", "join", "(", "'images'", ",", "im", ")", ")", "\n", "", "br", "(", ")", "\n", "p", "(", "txt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.save": [[68, 74], ["open", "open.write", "open.close", "html.HTML.doc.render"], "methods", ["None"], ["", "", "", "", "", "", "def", "save", "(", "self", ")", ":", "\n", "        ", "\"\"\"save the current content to the HMTL file\"\"\"", "\n", "html_file", "=", "'%s/index.html'", "%", "self", ".", "web_dir", "\n", "f", "=", "open", "(", "html_file", ",", "'wt'", ")", "\n", "f", ".", "write", "(", "self", ".", "doc", ".", "render", "(", ")", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.image_pool.ImagePool.__init__": [[12, 22], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "pool_size", ")", ":", "\n", "        ", "\"\"\"Initialize the ImagePool class\n\n        Parameters:\n            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created\n        \"\"\"", "\n", "self", ".", "pool_size", "=", "pool_size", "\n", "if", "self", ".", "pool_size", ">", "0", ":", "# create an empty pool", "\n", "            ", "self", ".", "num_imgs", "=", "0", "\n", "self", ".", "images", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.image_pool.ImagePool.query": [[23, 55], ["torch.cat", "torch.unsqueeze", "image_pool.ImagePool.images.append", "torch.cat.append", "random.uniform", "random.randint", "image_pool.ImagePool.images[].clone", "torch.cat.append", "torch.cat.append"], "methods", ["None"], ["", "", "def", "query", "(", "self", ",", "images", ")", ":", "\n", "        ", "\"\"\"Return an image from the pool.\n\n        Parameters:\n            images: the latest generated images from the generator\n\n        Returns images from the buffer.\n\n        By 50/100, the buffer will return input images.\n        By 50/100, the buffer will return images previously stored in the buffer,\n        and insert the current images to the buffer.\n        \"\"\"", "\n", "if", "self", ".", "pool_size", "==", "0", ":", "# if the buffer size is 0, do nothing", "\n", "            ", "return", "images", "\n", "", "return_images", "=", "[", "]", "\n", "for", "image", "in", "images", ":", "\n", "            ", "image", "=", "torch", ".", "unsqueeze", "(", "image", ".", "data", ",", "0", ")", "\n", "if", "self", ".", "num_imgs", "<", "self", ".", "pool_size", ":", "# if the buffer is not full; keep inserting current images to the buffer", "\n", "                ", "self", ".", "num_imgs", "=", "self", ".", "num_imgs", "+", "1", "\n", "self", ".", "images", ".", "append", "(", "image", ")", "\n", "return_images", ".", "append", "(", "image", ")", "\n", "", "else", ":", "\n", "                ", "p", "=", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "p", ">", "0.5", ":", "# by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer", "\n", "                    ", "random_id", "=", "random", ".", "randint", "(", "0", ",", "self", ".", "pool_size", "-", "1", ")", "# randint is inclusive", "\n", "tmp", "=", "self", ".", "images", "[", "random_id", "]", ".", "clone", "(", ")", "\n", "self", ".", "images", "[", "random_id", "]", "=", "image", "\n", "return_images", ".", "append", "(", "tmp", ")", "\n", "", "else", ":", "# by another 50% chance, the buffer will return the current image", "\n", "                    ", "return_images", ".", "append", "(", "image", ")", "\n", "", "", "", "return_images", "=", "torch", ".", "cat", "(", "return_images", ",", "0", ")", "# collect all the images and return", "\n", "return", "return_images", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.__init__": [[52, 86], ["os.path.join", "visdom.Visdom", "os.path.join", "os.path.join", "print", "util.mkdirs", "open", "time.strftime", "log_file.write", "visualizer.Visualizer.vis.check_connection", "visualizer.Visualizer.create_visdom_connections"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.mkdirs", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.create_visdom_connections"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize the Visualizer class\n\n        Parameters:\n            opt -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        Step 1: Cache the training/test options\n        Step 2: connect to a visdom server\n        Step 3: create an HTML object for saveing HTML filters\n        Step 4: create a logging file to store training losses\n        \"\"\"", "\n", "self", ".", "opt", "=", "opt", "# cache the option", "\n", "self", ".", "display_id", "=", "opt", ".", "display_id", "\n", "self", ".", "use_html", "=", "opt", ".", "isTrain", "and", "not", "opt", ".", "no_html", "\n", "self", ".", "win_size", "=", "opt", ".", "display_winsize", "\n", "self", ".", "name", "=", "opt", ".", "name", "\n", "self", ".", "port", "=", "opt", ".", "display_port", "\n", "self", ".", "saved", "=", "False", "\n", "if", "self", ".", "display_id", ">", "0", ":", "# connect to a visdom server given <display_port> and <display_server>", "\n", "            ", "import", "visdom", "\n", "self", ".", "ncols", "=", "opt", ".", "display_ncols", "\n", "self", ".", "vis", "=", "visdom", ".", "Visdom", "(", "server", "=", "opt", ".", "display_server", ",", "port", "=", "opt", ".", "display_port", ",", "env", "=", "opt", ".", "display_env", ")", "\n", "if", "not", "self", ".", "vis", ".", "check_connection", "(", ")", ":", "\n", "                ", "self", ".", "create_visdom_connections", "(", ")", "\n", "\n", "", "", "if", "self", ".", "use_html", ":", "# create an HTML object at <checkpoints_dir>/web/; images will be saved under <checkpoints_dir>/web/images/", "\n", "            ", "self", ".", "web_dir", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "checkpoints_dir", ",", "opt", ".", "name", ",", "'web'", ")", "\n", "self", ".", "img_dir", "=", "os", ".", "path", ".", "join", "(", "self", ".", "web_dir", ",", "'images'", ")", "\n", "print", "(", "'create web directory %s...'", "%", "self", ".", "web_dir", ")", "\n", "util", ".", "mkdirs", "(", "[", "self", ".", "web_dir", ",", "self", ".", "img_dir", "]", ")", "\n", "# create a logging file to store training losses", "\n", "", "self", ".", "log_name", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "checkpoints_dir", ",", "opt", ".", "name", ",", "'loss_log.txt'", ")", "\n", "with", "open", "(", "self", ".", "log_name", ",", "\"a\"", ")", "as", "log_file", ":", "\n", "            ", "now", "=", "time", ".", "strftime", "(", "\"%c\"", ")", "\n", "log_file", ".", "write", "(", "'================ Training Loss (%s) ================\\n'", "%", "now", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.reset": [[87, 90], ["None"], "methods", ["None"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset the self.saved status\"\"\"", "\n", "self", ".", "saved", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.create_visdom_connections": [[91, 97], ["print", "print", "subprocess.Popen"], "methods", ["None"], ["", "def", "create_visdom_connections", "(", "self", ")", ":", "\n", "        ", "\"\"\"If the program could not connect to Visdom server, this function will start a new server at port < self.port > \"\"\"", "\n", "cmd", "=", "sys", ".", "executable", "+", "' -m visdom.server -p %d &>/dev/null &'", "%", "self", ".", "port", "\n", "print", "(", "'\\n\\nCould not connect to Visdom server. \\n Trying to start a server....'", ")", "\n", "print", "(", "'Command: %s'", "%", "cmd", ")", "\n", "Popen", "(", "cmd", ",", "shell", "=", "True", ",", "stdout", "=", "PIPE", ",", "stderr", "=", "PIPE", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.display_current_results": [[98, 178], ["visuals.items", "html.HTML", "range", "html.HTML.save", "min", "visuals.items", "util.tensor2im", "os.path.join", "util.save_image", "html.HTML.add_header", "visuals.items", "html.HTML.add_images", "len", "util.tensor2im", "images.append", "numpy.ones_like", "images.append", "visualizer.Visualizer.vis.images", "visualizer.Visualizer.vis.text", "visuals.items", "util.tensor2im", "ims.append", "txts.append", "links.append", "next", "util.tensor2im.transpose", "util.tensor2im.transpose", "visualizer.Visualizer.create_visdom_connections", "util.tensor2im", "visualizer.Visualizer.vis.image", "visualizer.Visualizer.create_visdom_connections", "iter", "dict", "dict", "util.tensor2im.transpose", "visuals.values", "dict", "str", "str"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.save", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.tensor2im", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.save_image", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.add_header", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.add_images", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.tensor2im", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.tensor2im", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.create_visdom_connections", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.tensor2im", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.create_visdom_connections"], ["", "def", "display_current_results", "(", "self", ",", "visuals", ",", "epoch", ",", "save_result", ")", ":", "\n", "        ", "\"\"\"Display current results on visdom; save current results to an HTML file.\n\n        Parameters:\n            visuals (OrderedDict) - - dictionary of images to display or save\n            epoch (int) - - the current epoch\n            save_result (bool) - - if save the current results to an HTML file\n        \"\"\"", "\n", "if", "self", ".", "display_id", ">", "0", ":", "# show images in the browser using visdom", "\n", "            ", "ncols", "=", "self", ".", "ncols", "\n", "if", "ncols", ">", "0", ":", "# show all the images in one visdom panel", "\n", "                ", "ncols", "=", "min", "(", "ncols", ",", "len", "(", "visuals", ")", ")", "\n", "h", ",", "w", "=", "next", "(", "iter", "(", "visuals", ".", "values", "(", ")", ")", ")", ".", "shape", "[", ":", "2", "]", "\n", "table_css", "=", "\"\"\"<style>\n                        table {border-collapse: separate; border-spacing: 4px; white-space: nowrap; text-align: center}\n                        table td {width: % dpx; height: % dpx; padding: 4px; outline: 4px solid black}\n                        </style>\"\"\"", "%", "(", "w", ",", "h", ")", "# create a table css", "\n", "# create a table of images.", "\n", "title", "=", "self", ".", "name", "\n", "label_html", "=", "''", "\n", "label_html_row", "=", "''", "\n", "images", "=", "[", "]", "\n", "idx", "=", "0", "\n", "for", "label", ",", "image", "in", "visuals", ".", "items", "(", ")", ":", "\n", "                    ", "image_numpy", "=", "util", ".", "tensor2im", "(", "image", ")", "\n", "label_html_row", "+=", "'<td>%s</td>'", "%", "label", "\n", "images", ".", "append", "(", "image_numpy", ".", "transpose", "(", "[", "2", ",", "0", ",", "1", "]", ")", ")", "\n", "idx", "+=", "1", "\n", "if", "idx", "%", "ncols", "==", "0", ":", "\n", "                        ", "label_html", "+=", "'<tr>%s</tr>'", "%", "label_html_row", "\n", "label_html_row", "=", "''", "\n", "", "", "white_image", "=", "np", ".", "ones_like", "(", "image_numpy", ".", "transpose", "(", "[", "2", ",", "0", ",", "1", "]", ")", ")", "*", "255", "\n", "while", "idx", "%", "ncols", "!=", "0", ":", "\n", "                    ", "images", ".", "append", "(", "white_image", ")", "\n", "label_html_row", "+=", "'<td></td>'", "\n", "idx", "+=", "1", "\n", "", "if", "label_html_row", "!=", "''", ":", "\n", "                    ", "label_html", "+=", "'<tr>%s</tr>'", "%", "label_html_row", "\n", "", "try", ":", "\n", "                    ", "self", ".", "vis", ".", "images", "(", "images", ",", "nrow", "=", "ncols", ",", "win", "=", "str", "(", "self", ".", "display_id", ")", "+", "str", "(", "idx", ")", "+", "self", ".", "name", ",", "\n", "padding", "=", "2", ",", "opts", "=", "dict", "(", "title", "=", "title", "+", "' images'", ")", ")", "\n", "label_html", "=", "'<table>%s</table>'", "%", "label_html", "\n", "self", ".", "vis", ".", "text", "(", "table_css", "+", "label_html", ",", "win", "=", "self", ".", "display_id", "+", "2", ",", "\n", "opts", "=", "dict", "(", "title", "=", "title", "+", "' labels'", ")", ")", "\n", "", "except", "VisdomExceptionBase", ":", "\n", "                    ", "self", ".", "create_visdom_connections", "(", ")", "\n", "\n", "", "", "else", ":", "# show each image in a separate visdom panel;", "\n", "                ", "idx", "=", "1", "\n", "try", ":", "\n", "                    ", "for", "label", ",", "image", "in", "visuals", ".", "items", "(", ")", ":", "\n", "                        ", "image_numpy", "=", "util", ".", "tensor2im", "(", "image", ")", "\n", "self", ".", "vis", ".", "image", "(", "image_numpy", ".", "transpose", "(", "[", "2", ",", "0", ",", "1", "]", ")", ",", "opts", "=", "dict", "(", "title", "=", "label", ")", ",", "\n", "win", "=", "self", ".", "display_id", "+", "idx", ")", "\n", "idx", "+=", "1", "\n", "", "", "except", "VisdomExceptionBase", ":", "\n", "                    ", "self", ".", "create_visdom_connections", "(", ")", "\n", "\n", "", "", "", "if", "self", ".", "use_html", "and", "(", "save_result", "or", "not", "self", ".", "saved", ")", ":", "# save images to an HTML file if they haven't been saved.", "\n", "            ", "self", ".", "saved", "=", "True", "\n", "# save images to the disk", "\n", "for", "label", ",", "image", "in", "visuals", ".", "items", "(", ")", ":", "\n", "                ", "image_numpy", "=", "util", ".", "tensor2im", "(", "image", ")", "\n", "img_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "img_dir", ",", "'epoch%.3d_%s.png'", "%", "(", "epoch", ",", "label", ")", ")", "\n", "util", ".", "save_image", "(", "image_numpy", ",", "img_path", ")", "\n", "\n", "# update website", "\n", "", "webpage", "=", "html", ".", "HTML", "(", "self", ".", "web_dir", ",", "'Experiment name = %s'", "%", "self", ".", "name", ",", "refresh", "=", "1", ")", "\n", "for", "n", "in", "range", "(", "epoch", ",", "0", ",", "-", "1", ")", ":", "\n", "                ", "webpage", ".", "add_header", "(", "'epoch [%d]'", "%", "n", ")", "\n", "ims", ",", "txts", ",", "links", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "for", "label", ",", "image_numpy", "in", "visuals", ".", "items", "(", ")", ":", "\n", "                    ", "image_numpy", "=", "util", ".", "tensor2im", "(", "image", ")", "\n", "img_path", "=", "'epoch%.3d_%s.png'", "%", "(", "n", ",", "label", ")", "\n", "ims", ".", "append", "(", "img_path", ")", "\n", "txts", ".", "append", "(", "label", ")", "\n", "links", ".", "append", "(", "img_path", ")", "\n", "", "webpage", ".", "add_images", "(", "ims", ",", "txts", ",", "links", ",", "width", "=", "self", ".", "win_size", ")", "\n", "", "webpage", ".", "save", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.plot_current_losses": [[179, 203], ["visualizer.Visualizer.plot_data[].append", "visualizer.Visualizer.plot_data[].append", "hasattr", "visualizer.Visualizer.vis.line", "list", "visualizer.Visualizer.create_visdom_connections", "losses.keys", "numpy.stack", "numpy.array", "str", "len", "str", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.create_visdom_connections"], ["", "", "def", "plot_current_losses", "(", "self", ",", "epoch", ",", "counter_ratio", ",", "losses", ")", ":", "\n", "        ", "\"\"\"display the current losses on visdom display: dictionary of error labels and values\n\n        Parameters:\n            epoch (int)           -- current epoch\n            counter_ratio (float) -- progress (percentage) in the current epoch, between 0 to 1\n            losses (OrderedDict)  -- training losses stored in the format of (name, float) pairs\n        \"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'plot_data'", ")", ":", "\n", "            ", "self", ".", "plot_data", "=", "{", "'X'", ":", "[", "]", ",", "'Y'", ":", "[", "]", ",", "'legend'", ":", "list", "(", "losses", ".", "keys", "(", ")", ")", "}", "\n", "", "self", ".", "plot_data", "[", "'X'", "]", ".", "append", "(", "epoch", "+", "counter_ratio", ")", "\n", "self", ".", "plot_data", "[", "'Y'", "]", ".", "append", "(", "[", "losses", "[", "k", "]", "for", "k", "in", "self", ".", "plot_data", "[", "'legend'", "]", "]", ")", "\n", "try", ":", "\n", "            ", "self", ".", "vis", ".", "line", "(", "\n", "X", "=", "np", ".", "stack", "(", "[", "np", ".", "array", "(", "self", ".", "plot_data", "[", "'X'", "]", ")", "]", "*", "len", "(", "self", ".", "plot_data", "[", "'legend'", "]", ")", ",", "1", ")", ",", "\n", "Y", "=", "np", ".", "array", "(", "self", ".", "plot_data", "[", "'Y'", "]", ")", ",", "\n", "opts", "=", "{", "\n", "'title'", ":", "self", ".", "name", "+", "' loss over time'", ",", "\n", "'legend'", ":", "self", ".", "plot_data", "[", "'legend'", "]", ",", "\n", "'xlabel'", ":", "'epoch'", ",", "\n", "'ylabel'", ":", "'loss'", "}", ",", "\n", "win", "=", "str", "(", "self", ".", "display_id", ")", "+", "\"|\"", "+", "str", "(", "self", ".", "name", ")", ")", "\n", "", "except", "VisdomExceptionBase", ":", "\n", "            ", "self", ".", "create_visdom_connections", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.plot_Ds_prediction": [[204, 228], ["visualizer.Visualizer.plot_data_pred[].append", "visualizer.Visualizer.plot_data_pred[].append", "hasattr", "visualizer.Visualizer.vis.line", "list", "visualizer.Visualizer.create_visdom_connections", "preds.keys", "numpy.stack", "numpy.array", "str", "len", "str", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.create_visdom_connections"], ["", "", "def", "plot_Ds_prediction", "(", "self", ",", "epoch", ",", "counter_ratio", ",", "preds", ")", ":", "\n", "        ", "\"\"\"display the current losses on visdom display: dictionary of error labels and values\n\n        Parameters:\n            epoch (int)           -- current epoch\n            counter_ratio (float) -- progress (percentage) in the current epoch, between 0 to 1\n            losses (OrderedDict)  -- training losses stored in the format of (name, float) pairs\n        \"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'plot_data_pred'", ")", ":", "\n", "            ", "self", ".", "plot_data_pred", "=", "{", "'X'", ":", "[", "]", ",", "'Y'", ":", "[", "]", ",", "'legend'", ":", "list", "(", "preds", ".", "keys", "(", ")", ")", "}", "\n", "", "self", ".", "plot_data_pred", "[", "'X'", "]", ".", "append", "(", "epoch", "+", "counter_ratio", ")", "\n", "self", ".", "plot_data_pred", "[", "'Y'", "]", ".", "append", "(", "[", "preds", "[", "k", "]", "for", "k", "in", "self", ".", "plot_data_pred", "[", "'legend'", "]", "]", ")", "\n", "try", ":", "\n", "            ", "self", ".", "vis", ".", "line", "(", "\n", "X", "=", "np", ".", "stack", "(", "[", "np", ".", "array", "(", "self", ".", "plot_data_pred", "[", "'X'", "]", ")", "]", "*", "len", "(", "self", ".", "plot_data_pred", "[", "'legend'", "]", ")", ",", "1", ")", ",", "\n", "Y", "=", "np", ".", "array", "(", "self", ".", "plot_data_pred", "[", "'Y'", "]", ")", ",", "\n", "opts", "=", "{", "\n", "'title'", ":", "self", ".", "name", "+", "' Ds prediction over time'", ",", "\n", "'legend'", ":", "self", ".", "plot_data_pred", "[", "'legend'", "]", ",", "\n", "'xlabel'", ":", "'epoch'", ",", "\n", "'ylabel'", ":", "'pred'", "}", ",", "\n", "win", "=", "str", "(", "self", ".", "display_id", "+", "1", ")", "+", "\"|\"", "+", "str", "(", "'prediction of Ds'", ")", ")", "\n", "", "except", "VisdomExceptionBase", ":", "\n", "            ", "self", ".", "create_visdom_connections", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.plot_label_prediction": [[229, 253], ["visualizer.Visualizer.plot_label_pred[].append", "visualizer.Visualizer.plot_label_pred[].append", "hasattr", "visualizer.Visualizer.vis.line", "list", "visualizer.Visualizer.create_visdom_connections", "preds.keys", "numpy.stack", "numpy.array", "str", "len", "str", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.create_visdom_connections"], ["", "", "def", "plot_label_prediction", "(", "self", ",", "epoch", ",", "counter_ratio", ",", "preds", ")", ":", "\n", "        ", "\"\"\"display the current losses on visdom display: dictionary of error labels and values\n\n        Parameters:\n            epoch (int)           -- current epoch\n            counter_ratio (float) -- progress (percentage) in the current epoch, between 0 to 1\n            losses (OrderedDict)  -- training losses stored in the format of (name, float) pairs\n        \"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'plot_label_pred'", ")", ":", "\n", "            ", "self", ".", "plot_label_pred", "=", "{", "'X'", ":", "[", "]", ",", "'Y'", ":", "[", "]", ",", "'legend'", ":", "list", "(", "preds", ".", "keys", "(", ")", ")", "}", "\n", "", "self", ".", "plot_label_pred", "[", "'X'", "]", ".", "append", "(", "epoch", "+", "counter_ratio", ")", "\n", "self", ".", "plot_label_pred", "[", "'Y'", "]", ".", "append", "(", "[", "preds", "[", "k", "]", "for", "k", "in", "self", ".", "plot_label_pred", "[", "'legend'", "]", "]", ")", "\n", "try", ":", "\n", "            ", "self", ".", "vis", ".", "line", "(", "\n", "X", "=", "np", ".", "stack", "(", "[", "np", ".", "array", "(", "self", ".", "plot_label_pred", "[", "'X'", "]", ")", "]", "*", "len", "(", "self", ".", "plot_label_pred", "[", "'legend'", "]", ")", ",", "1", ")", ",", "\n", "Y", "=", "np", ".", "array", "(", "self", ".", "plot_label_pred", "[", "'Y'", "]", ")", ",", "\n", "opts", "=", "{", "\n", "'title'", ":", "self", ".", "name", "+", "' predicted label over time'", ",", "\n", "'legend'", ":", "self", ".", "plot_label_pred", "[", "'legend'", "]", ",", "\n", "'xlabel'", ":", "'epoch'", ",", "\n", "'ylabel'", ":", "'pred'", "}", ",", "\n", "win", "=", "str", "(", "self", ".", "display_id", "+", "1", ")", "+", "\"|\"", "+", "str", "(", "'predicted labels'", ")", ")", "\n", "", "except", "VisdomExceptionBase", ":", "\n", "            ", "self", ".", "create_visdom_connections", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.plot_fc_prediction": [[254, 280], ["visualizer.Visualizer.plot_data_fc_pred[].append", "range", "hasattr", "visualizer.Visualizer.plot_data_fc_pred[].append", "list", "visualizer.Visualizer.vis.line", "fc_preds.keys", "[].item", "visualizer.Visualizer.create_visdom_connections", "numpy.stack", "numpy.array", "str", "len", "str", "str", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.create_visdom_connections"], ["", "", "def", "plot_fc_prediction", "(", "self", ",", "epoch", ",", "counter_ratio", ",", "fc_preds", ")", ":", "\n", "        ", "\"\"\"display the current losses on visdom display: dictionary of error labels and values\n\n        Parameters:\n            epoch (int)           -- current epoch\n            counter_ratio (float) -- progress (percentage) in the current epoch, between 0 to 1\n            losses (OrderedDict)  -- training losses stored in the format of (name, float) pairs\n        \"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'plot_data_fc_pred'", ")", ":", "\n", "            ", "self", ".", "plot_data_fc_pred", "=", "{", "'X'", ":", "[", "]", ",", "'Y1'", ":", "[", "]", ",", "'Y3'", ":", "[", "]", ",", "'Y5'", ":", "[", "]", ",", "'Y7'", ":", "[", "]", ",", "'Y9'", ":", "[", "]", ",", "\n", "'legend'", ":", "list", "(", "fc_preds", ".", "keys", "(", ")", ")", "}", "\n", "", "self", ".", "plot_data_fc_pred", "[", "'X'", "]", ".", "append", "(", "epoch", "+", "counter_ratio", ")", "\n", "for", "i", "in", "range", "(", "1", ",", "10", ",", "2", ")", ":", "\n", "            ", "self", ".", "plot_data_fc_pred", "[", "'Y{:d}'", ".", "format", "(", "i", ")", "]", ".", "append", "(", "[", "fc_preds", "[", "k", "]", "[", "i", "]", ".", "item", "(", ")", "for", "k", "in", "self", ".", "plot_data_fc_pred", "[", "'legend'", "]", "]", ")", "\n", "try", ":", "\n", "                ", "self", ".", "vis", ".", "line", "(", "\n", "X", "=", "np", ".", "stack", "(", "[", "np", ".", "array", "(", "self", ".", "plot_data_fc_pred", "[", "'X'", "]", ")", "]", "*", "len", "(", "self", ".", "plot_data_fc_pred", "[", "'legend'", "]", ")", ",", "1", ")", ",", "\n", "Y", "=", "np", ".", "array", "(", "self", ".", "plot_data_fc_pred", "[", "'Y{:d}'", ".", "format", "(", "i", ")", "]", ")", ",", "\n", "opts", "=", "{", "\n", "'title'", ":", "self", ".", "name", "+", "' fc prediction_label='", "+", "str", "(", "i", ")", ",", "\n", "'legend'", ":", "self", ".", "plot_data_fc_pred", "[", "'legend'", "]", ",", "\n", "'xlabel'", ":", "'epoch'", ",", "\n", "'ylabel'", ":", "'pred'", "}", ",", "\n", "win", "=", "str", "(", "self", ".", "display_id", "+", "10", "+", "i", ")", "+", "\"|\"", "+", "str", "(", "'prediction of fc'", ")", ")", "\n", "", "except", "VisdomExceptionBase", ":", "\n", "                ", "self", ".", "create_visdom_connections", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.plot_fc_weights": [[281, 301], ["hasattr", "visualizer.Visualizer.vis.bar", "list", "visualizer.Visualizer.create_visdom_connections", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.create_visdom_connections"], ["", "", "", "def", "plot_fc_weights", "(", "self", ",", "epoch", ",", "weights", ")", ":", "\n", "        ", "\"\"\"display the current fc layer weights\n\n        Parameters:\n            epoch (int)           -- current epoch\n            weights (Dict)  -- training losses stored in the format of (name, float) pairs\n        \"\"\"", "\n", "if", "not", "hasattr", "(", "self", ",", "'plot_data_fc_weights'", ")", ":", "\n", "            ", "self", ".", "plot_data_fc_weights", "=", "{", "'X'", ":", "[", "]", ",", "'legend'", ":", "list", "(", "weights", "[", "'names'", "]", ")", "}", "\n", "", "try", ":", "\n", "            ", "self", ".", "vis", ".", "bar", "(", "\n", "X", "=", "weights", "[", "'values'", "]", ",", "\n", "opts", "=", "{", "\n", "'stacked'", ":", "False", ",", "\n", "'title'", ":", "self", ".", "name", "+", "' fc abs weights_epoch '", "+", "str", "(", "epoch", ")", ",", "\n", "'legend'", ":", "self", ".", "plot_data_fc_weights", "[", "'legend'", "]", "\n", "}", ",", "\n", "win", "=", "str", "(", "self", ".", "display_id", "+", "epoch", "+", "2", ")", "+", "\"|\"", "+", "str", "(", "'fc weights'", ")", ")", "\n", "", "except", "VisdomExceptionBase", ":", "\n", "            ", "self", ".", "create_visdom_connections", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.Visualizer.print_current_losses": [[303, 320], ["losses.items", "print", "open", "log_file.write"], "methods", ["None"], ["", "", "def", "print_current_losses", "(", "self", ",", "epoch", ",", "iters", ",", "losses", ",", "t_comp", ",", "t_data", ")", ":", "\n", "        ", "\"\"\"print current losses on console; also save the losses to the disk\n\n        Parameters:\n            epoch (int) -- current epoch\n            iters (int) -- current training iteration during this epoch (reset to 0 at the end of every epoch)\n            losses (OrderedDict) -- training losses stored in the format of (name, float) pairs\n            t_comp (float) -- computational time per data point (normalized by batch_size)\n            t_data (float) -- data loading time per data point (normalized by batch_size)\n        \"\"\"", "\n", "message", "=", "'(epoch: %d, iters: %d, time: %.3f, data: %.3f) '", "%", "(", "epoch", ",", "iters", ",", "t_comp", ",", "t_data", ")", "\n", "for", "k", ",", "v", "in", "losses", ".", "items", "(", ")", ":", "\n", "            ", "message", "+=", "'%s: %.3f '", "%", "(", "k", ",", "v", ")", "\n", "\n", "", "print", "(", "message", ")", "# print the message", "\n", "with", "open", "(", "self", ".", "log_name", ",", "\"a\"", ")", "as", "log_file", ":", "\n", "            ", "log_file", ".", "write", "(", "'%s\\n'", "%", "message", ")", "# save the message", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.visualizer.save_images": [[16, 44], ["webpage.get_image_dir", "ntpath.basename", "webpage.add_header", "visuals.items", "webpage.add_images", "os.path.splitext", "util.tensor2im", "os.path.join", "util.save_image", "ims.append", "txts.append", "links.append"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.get_image_dir", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.add_header", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.add_images", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.tensor2im", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.save_image"], ["", "def", "save_images", "(", "webpage", ",", "visuals", ",", "image_path", ",", "aspect_ratio", "=", "1.0", ",", "width", "=", "256", ")", ":", "\n", "    ", "\"\"\"Save images to the disk.\n\n    Parameters:\n        webpage (the HTML class) -- the HTML webpage class that stores these imaegs (see html.py for more details)\n        visuals (OrderedDict)    -- an ordered dictionary that stores (name, images (either tensor or numpy) ) pairs\n        image_path (str)         -- the string is used to create image paths\n        aspect_ratio (float)     -- the aspect ratio of saved images\n        width (int)              -- the images will be resized to width x width\n\n    This function will save images stored in 'visuals' to the HTML file specified by 'webpage'.\n    \"\"\"", "\n", "image_dir", "=", "webpage", ".", "get_image_dir", "(", ")", "\n", "short_path", "=", "ntpath", ".", "basename", "(", "image_path", "[", "0", "]", ")", "\n", "name", "=", "os", ".", "path", ".", "splitext", "(", "short_path", ")", "[", "0", "]", "\n", "\n", "webpage", ".", "add_header", "(", "name", ")", "\n", "ims", ",", "txts", ",", "links", "=", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "\n", "for", "label", ",", "im_data", "in", "visuals", ".", "items", "(", ")", ":", "\n", "        ", "im", "=", "util", ".", "tensor2im", "(", "im_data", ")", "\n", "image_name", "=", "'%s_%s.png'", "%", "(", "name", ",", "label", ")", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "image_dir", ",", "image_name", ")", "\n", "util", ".", "save_image", "(", "im", ",", "save_path", ",", "aspect_ratio", "=", "aspect_ratio", ")", "\n", "ims", ".", "append", "(", "image_name", ")", "\n", "txts", ".", "append", "(", "label", ")", "\n", "links", ".", "append", "(", "image_name", ")", "\n", "", "webpage", ".", "add_images", "(", "ims", ",", "txts", ",", "links", ",", "width", "=", "width", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData.__init__": [[27, 34], ["url_dict.get", "technique.lower"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData.get"], ["def", "__init__", "(", "self", ",", "technique", "=", "'cyclegan'", ",", "verbose", "=", "True", ")", ":", "\n", "        ", "url_dict", "=", "{", "\n", "'pix2pix'", ":", "'http://efrosgans.eecs.berkeley.edu/pix2pix/datasets/'", ",", "\n", "'cyclegan'", ":", "'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets'", "\n", "}", "\n", "self", ".", "url", "=", "url_dict", ".", "get", "(", "technique", ".", "lower", "(", ")", ")", "\n", "self", ".", "_verbose", "=", "verbose", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData._print": [[35, 38], ["print"], "methods", ["None"], ["", "def", "_print", "(", "self", ",", "text", ")", ":", "\n", "        ", "if", "self", ".", "_verbose", ":", "\n", "            ", "print", "(", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData._get_options": [[39, 45], ["bs4.BeautifulSoup", "bs4.BeautifulSoup.find_all", "h.text.endswith"], "methods", ["None"], ["", "", "@", "staticmethod", "\n", "def", "_get_options", "(", "r", ")", ":", "\n", "        ", "soup", "=", "BeautifulSoup", "(", "r", ".", "text", ",", "'lxml'", ")", "\n", "options", "=", "[", "h", ".", "text", "for", "h", "in", "soup", ".", "find_all", "(", "'a'", ",", "href", "=", "True", ")", "\n", "if", "h", ".", "text", ".", "endswith", "(", "(", "'.zip'", ",", "'tar.gz'", ")", ")", "]", "\n", "return", "options", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData._present_options": [[46, 55], ["requests.get", "get_data.GetData._get_options", "print", "enumerate", "input", "print", "int"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData.get", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData._get_options"], ["", "def", "_present_options", "(", "self", ")", ":", "\n", "        ", "r", "=", "requests", ".", "get", "(", "self", ".", "url", ")", "\n", "options", "=", "self", ".", "_get_options", "(", "r", ")", "\n", "print", "(", "'Options:\\n'", ")", "\n", "for", "i", ",", "o", "in", "enumerate", "(", "options", ")", ":", "\n", "            ", "print", "(", "\"{0}: {1}\"", ".", "format", "(", "i", ",", "o", ")", ")", "\n", "", "choice", "=", "input", "(", "\"\\nPlease enter the number of the \"", "\n", "\"dataset above you wish to download:\"", ")", "\n", "return", "options", "[", "int", "(", "choice", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData._download_data": [[56, 78], ["os.path.basename", "os.path.join", "os.path.basename.endswith", "get_data.GetData._print", "zipfile.ZipFile.extractall", "zipfile.ZipFile.close", "os.remove", "os.path.isdir", "os.makedirs", "open", "requests.get", "f.write", "tarfile.open", "os.path.basename.endswith", "zipfile.ZipFile", "ValueError"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData._print", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData.get"], ["", "def", "_download_data", "(", "self", ",", "dataset_url", ",", "save_path", ")", ":", "\n", "        ", "if", "not", "isdir", "(", "save_path", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "save_path", ")", "\n", "\n", "", "base", "=", "basename", "(", "dataset_url", ")", "\n", "temp_save_path", "=", "join", "(", "save_path", ",", "base", ")", "\n", "\n", "with", "open", "(", "temp_save_path", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "r", "=", "requests", ".", "get", "(", "dataset_url", ")", "\n", "f", ".", "write", "(", "r", ".", "content", ")", "\n", "\n", "", "if", "base", ".", "endswith", "(", "'.tar.gz'", ")", ":", "\n", "            ", "obj", "=", "tarfile", ".", "open", "(", "temp_save_path", ")", "\n", "", "elif", "base", ".", "endswith", "(", "'.zip'", ")", ":", "\n", "            ", "obj", "=", "ZipFile", "(", "temp_save_path", ",", "'r'", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"Unknown File Type: {0}.\"", ".", "format", "(", "base", ")", ")", "\n", "\n", "", "self", ".", "_print", "(", "\"Unpacking Data...\"", ")", "\n", "obj", ".", "extractall", "(", "save_path", ")", "\n", "obj", ".", "close", "(", ")", "\n", "os", ".", "remove", "(", "temp_save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData.get": [[79, 111], ["os.path.join", "os.path.isdir", "os.path.abspath", "get_data.GetData._present_options", "warnings.warn", "get_data.GetData._print", "get_data.GetData._download_data", "get_data.GetData.split"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData._present_options", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData._print", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.get_data.GetData._download_data"], ["", "def", "get", "(", "self", ",", "save_path", ",", "dataset", "=", "None", ")", ":", "\n", "        ", "\"\"\"\n\n        Download a dataset.\n\n        Parameters:\n            save_path (str) -- A directory to save the data to.\n            dataset (str)   -- (optional). A specific dataset to download.\n                            Note: this must include the file extension.\n                            If None, options will be presented for you\n                            to choose from.\n\n        Returns:\n            save_path_full (str) -- the absolute path to the downloaded data.\n\n        \"\"\"", "\n", "if", "dataset", "is", "None", ":", "\n", "            ", "selected_dataset", "=", "self", ".", "_present_options", "(", ")", "\n", "", "else", ":", "\n", "            ", "selected_dataset", "=", "dataset", "\n", "\n", "", "save_path_full", "=", "join", "(", "save_path", ",", "selected_dataset", ".", "split", "(", "'.'", ")", "[", "0", "]", ")", "\n", "\n", "if", "isdir", "(", "save_path_full", ")", ":", "\n", "            ", "warn", "(", "\"\\n'{0}' already exists. Voiding Download.\"", ".", "format", "(", "\n", "save_path_full", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "_print", "(", "'Downloading Data...'", ")", "\n", "url", "=", "\"{0}/{1}\"", ".", "format", "(", "self", ".", "url", ",", "selected_dataset", ")", "\n", "self", ".", "_download_data", "(", "url", ",", "save_path", "=", "save_path", ")", "\n", "\n", "", "return", "abspath", "(", "save_path_full", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.tensor2im": [[9, 28], ["np.tile.astype", "isinstance", "isinstance", "image_tensor[].cpu().float().numpy", "numpy.tile", "image_tensor[].cpu().float", "numpy.transpose", "image_tensor[].cpu"], "function", ["None"], ["def", "tensor2im", "(", "input_image", ",", "imtype", "=", "np", ".", "uint8", ")", ":", "\n", "    ", "\"\"\"\"Converts a Tensor array into a numpy image array.\n\n    Parameters:\n        input_image (tensor) --  the input image tensor array\n        imtype (type)        --  the desired type of the converted numpy array\n    \"\"\"", "\n", "if", "not", "isinstance", "(", "input_image", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "if", "isinstance", "(", "input_image", ",", "torch", ".", "Tensor", ")", ":", "# get the data from a variable", "\n", "            ", "image_tensor", "=", "input_image", ".", "data", "\n", "", "else", ":", "\n", "            ", "return", "input_image", "\n", "", "image_numpy", "=", "image_tensor", "[", "0", "]", ".", "cpu", "(", ")", ".", "float", "(", ")", ".", "numpy", "(", ")", "# convert it into a numpy array", "\n", "if", "image_numpy", ".", "shape", "[", "0", "]", "==", "1", ":", "# grayscale to RGB", "\n", "            ", "image_numpy", "=", "np", ".", "tile", "(", "image_numpy", ",", "(", "3", ",", "1", ",", "1", ")", ")", "\n", "", "image_numpy", "=", "(", "np", ".", "transpose", "(", "image_numpy", ",", "(", "1", ",", "2", ",", "0", ")", ")", "+", "1", ")", "/", "2.0", "*", "255.0", "# post-processing: tranpose and scaling", "\n", "", "else", ":", "# if it is a numpy array, do nothing", "\n", "        ", "image_numpy", "=", "input_image", "\n", "", "return", "image_numpy", ".", "astype", "(", "imtype", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.diagnose_network": [[30, 47], ["net.parameters", "print", "print", "torch.mean", "torch.abs"], "function", ["None"], ["", "def", "diagnose_network", "(", "net", ",", "name", "=", "'network'", ")", ":", "\n", "    ", "\"\"\"Calculate and print the mean of average absolute(gradients)\n\n    Parameters:\n        net (torch network) -- Torch network\n        name (str) -- the name of the network\n    \"\"\"", "\n", "mean", "=", "0.0", "\n", "count", "=", "0", "\n", "for", "param", "in", "net", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "            ", "mean", "+=", "torch", ".", "mean", "(", "torch", ".", "abs", "(", "param", ".", "grad", ".", "data", ")", ")", "\n", "count", "+=", "1", "\n", "", "", "if", "count", ">", "0", ":", "\n", "        ", "mean", "=", "mean", "/", "count", "\n", "", "print", "(", "name", ")", "\n", "print", "(", "mean", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.save_image": [[49, 65], ["PIL.Image.fromarray", "image_pil.resize.save", "image_pil.resize.resize", "image_pil.resize.resize", "int", "int"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.save"], ["", "def", "save_image", "(", "image_numpy", ",", "image_path", ",", "aspect_ratio", "=", "1.0", ")", ":", "\n", "    ", "\"\"\"Save a numpy image to the disk\n\n    Parameters:\n        image_numpy (numpy array) -- input numpy array\n        image_path (str)          -- the path of the image\n    \"\"\"", "\n", "\n", "image_pil", "=", "Image", ".", "fromarray", "(", "image_numpy", ")", "\n", "h", ",", "w", ",", "_", "=", "image_numpy", ".", "shape", "\n", "\n", "if", "aspect_ratio", ">", "1.0", ":", "\n", "        ", "image_pil", "=", "image_pil", ".", "resize", "(", "(", "h", ",", "int", "(", "w", "*", "aspect_ratio", ")", ")", ",", "Image", ".", "BICUBIC", ")", "\n", "", "if", "aspect_ratio", "<", "1.0", ":", "\n", "        ", "image_pil", "=", "image_pil", ".", "resize", "(", "(", "int", "(", "h", "/", "aspect_ratio", ")", ",", "w", ")", ",", "Image", ".", "BICUBIC", ")", "\n", "", "image_pil", ".", "save", "(", "image_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.print_numpy": [[67, 81], ["x.flatten.astype", "print", "x.flatten.flatten", "print", "numpy.mean", "numpy.min", "numpy.max", "numpy.median", "numpy.std"], "function", ["None"], ["", "def", "print_numpy", "(", "x", ",", "val", "=", "True", ",", "shp", "=", "False", ")", ":", "\n", "    ", "\"\"\"Print the mean, min, max, median, std, and size of a numpy array\n\n    Parameters:\n        val (bool) -- if print the values of the numpy array\n        shp (bool) -- if print the shape of the numpy array\n    \"\"\"", "\n", "x", "=", "x", ".", "astype", "(", "np", ".", "float64", ")", "\n", "if", "shp", ":", "\n", "        ", "print", "(", "'shape,'", ",", "x", ".", "shape", ")", "\n", "", "if", "val", ":", "\n", "        ", "x", "=", "x", ".", "flatten", "(", ")", "\n", "print", "(", "'mean = %3.3f, min = %3.3f, max = %3.3f, median = %3.3f, std=%3.3f'", "%", "(", "\n", "np", ".", "mean", "(", "x", ")", ",", "np", ".", "min", "(", "x", ")", ",", "np", ".", "max", "(", "x", ")", ",", "np", ".", "median", "(", "x", ")", ",", "np", ".", "std", "(", "x", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.mkdirs": [[83, 94], ["isinstance", "util.mkdir", "isinstance", "util.mkdir"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.mkdir", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.mkdir"], ["", "", "def", "mkdirs", "(", "paths", ")", ":", "\n", "    ", "\"\"\"create empty directories if they don't exist\n\n    Parameters:\n        paths (str list) -- a list of directory paths\n    \"\"\"", "\n", "if", "isinstance", "(", "paths", ",", "list", ")", "and", "not", "isinstance", "(", "paths", ",", "str", ")", ":", "\n", "        ", "for", "path", "in", "paths", ":", "\n", "            ", "mkdir", "(", "path", ")", "\n", "", "", "else", ":", "\n", "        ", "mkdir", "(", "paths", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.mkdir": [[96, 104], ["os.path.exists", "os.makedirs"], "function", ["None"], ["", "", "def", "mkdir", "(", "path", ")", ":", "\n", "    ", "\"\"\"create a single empty directory if it didn't exist\n\n    Parameters:\n        path (str) -- a single directory path\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "path", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.MetricTracker.__init__": [[29, 33], ["pandas.DataFrame", "util.MetricTracker.reset"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.MetricTracker.reset"], ["\n", "", "def", "diagnose_network", "(", "net", ",", "name", "=", "'network'", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.MetricTracker.reset": [[34, 37], ["None"], "methods", ["None"], ["\n", "mean", "=", "0.0", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.MetricTracker.update": [[38, 44], ["util.MetricTracker.writer.add_scalar"], "methods", ["None"], ["count", "=", "0", "\n", "for", "param", "in", "net", ".", "parameters", "(", ")", ":", "\n", "        ", "if", "param", ".", "grad", "is", "not", "None", ":", "\n", "            ", "mean", "+=", "torch", ".", "mean", "(", "torch", ".", "abs", "(", "param", ".", "grad", ".", "data", ")", ")", "\n", "count", "+=", "1", "\n", "", "", "if", "count", ">", "0", ":", "\n", "        ", "mean", "=", "mean", "/", "count", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.MetricTracker.avg": [[45, 47], ["None"], "methods", ["None"], ["", "print", "(", "name", ")", "\n", "print", "(", "mean", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.MetricTracker.result": [[48, 50], ["dict"], "methods", ["None"], ["\n", "", "def", "save_image", "(", "image_numpy", ",", "image_path", ",", "aspect_ratio", "=", "1.0", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.ensure_dir": [[8, 12], ["pathlib.Path", "pathlib.Path.is_dir", "pathlib.Path.mkdir"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.mkdir"], ["\n", "def", "tensor2im", "(", "input_image", ",", "imtype", "=", "np", ".", "uint8", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.read_json": [[13, 17], ["pathlib.Path", "pathlib.Path.open", "json.load"], "function", ["None"], ["\n", "if", "not", "isinstance", "(", "input_image", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "if", "isinstance", "(", "input_image", ",", "torch", ".", "Tensor", ")", ":", "# get the data from a variable", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.write_json": [[18, 22], ["pathlib.Path", "pathlib.Path.open", "json.dump"], "function", ["None"], ["            ", "image_tensor", "=", "input_image", ".", "data", "\n", "", "else", ":", "\n", "            ", "return", "input_image", "\n", "", "image_numpy", "=", "image_tensor", "[", "0", "]", ".", "cpu", "(", ")", ".", "float", "(", ")", ".", "numpy", "(", ")", "# convert it into a numpy array", "\n", "if", "image_numpy", ".", "shape", "[", "0", "]", "==", "1", ":", "# grayscale to RGB", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.inf_loop": [[23, 27], ["itertools.repeat"], "function", ["None"], ["            ", "image_numpy", "=", "np", ".", "tile", "(", "image_numpy", ",", "(", "3", ",", "1", ",", "1", ")", ")", "\n", "", "image_numpy", "=", "(", "np", ".", "transpose", "(", "image_numpy", ",", "(", "1", ",", "2", ",", "0", ")", ")", "+", "1", ")", "/", "2.0", "*", "255.0", "# post-processing: tranpose and scaling", "\n", "", "else", ":", "# if it is a numpy array, do nothing", "\n", "        ", "image_numpy", "=", "input_image", "\n", "", "return", "image_numpy", ".", "astype", "(", "imtype", ")", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.show_figures": [[52, 64], ["plt.show", "range", "range", "len", "plt.figure", "plt.imshow", "len", "plt.figure", "plt.imshow"], "function", ["None"], ["\n", "\n", "image_pil", "=", "Image", ".", "fromarray", "(", "image_numpy", ")", "\n", "h", ",", "w", ",", "_", "=", "image_numpy", ".", "shape", "\n", "\n", "if", "aspect_ratio", ">", "1.0", ":", "\n", "        ", "image_pil", "=", "image_pil", ".", "resize", "(", "(", "h", ",", "int", "(", "w", "*", "aspect_ratio", ")", ")", ",", "Image", ".", "BICUBIC", ")", "\n", "", "if", "aspect_ratio", "<", "1.0", ":", "\n", "        ", "image_pil", "=", "image_pil", ".", "resize", "(", "(", "int", "(", "h", "/", "aspect_ratio", ")", ",", "w", ")", ",", "Image", ".", "BICUBIC", ")", "\n", "", "image_pil", ".", "save", "(", "image_path", ")", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.logger.visualization.TensorboardWriter.__init__": [[6, 39], ["datetime.datetime.datetime.now", "str", "logger.warning", "importlib.import_module().SummaryWriter", "importlib.import_module"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "log_dir", ",", "logger", ",", "enabled", ")", ":", "\n", "        ", "self", ".", "writer", "=", "None", "\n", "self", ".", "selected_module", "=", "\"\"", "\n", "\n", "if", "enabled", ":", "\n", "            ", "log_dir", "=", "str", "(", "log_dir", ")", "\n", "\n", "# Retrieve vizualization writer.", "\n", "succeeded", "=", "False", "\n", "for", "module", "in", "[", "\"torch.utils.tensorboard\"", ",", "\"tensorboardX\"", "]", ":", "\n", "                ", "try", ":", "\n", "                    ", "self", ".", "writer", "=", "importlib", ".", "import_module", "(", "module", ")", ".", "SummaryWriter", "(", "log_dir", ")", "\n", "succeeded", "=", "True", "\n", "break", "\n", "", "except", "ImportError", ":", "\n", "                    ", "succeeded", "=", "False", "\n", "", "self", ".", "selected_module", "=", "module", "\n", "\n", "", "if", "not", "succeeded", ":", "\n", "                ", "message", "=", "\"Warning: visualization (Tensorboard) is configured to use, but currently not installed on \"", "\"this machine. Please install TensorboardX with 'pip install tensorboardx', upgrade PyTorch to \"", "\"version >= 1.1 to use 'torch.utils.tensorboard' or turn off the option in the 'config.json' file.\"", "\n", "logger", ".", "warning", "(", "message", ")", "\n", "\n", "", "", "self", ".", "step", "=", "0", "\n", "self", ".", "mode", "=", "''", "\n", "\n", "self", ".", "tb_writer_ftns", "=", "{", "\n", "'add_scalar'", ",", "'add_scalars'", ",", "'add_image'", ",", "'add_images'", ",", "'add_audio'", ",", "\n", "'add_text'", ",", "'add_histogram'", ",", "'add_pr_curve'", ",", "'add_embedding'", "\n", "}", "\n", "self", ".", "tag_mode_exceptions", "=", "{", "'add_histogram'", ",", "'add_embedding'", "}", "\n", "self", ".", "timer", "=", "datetime", ".", "now", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.logger.visualization.TensorboardWriter.set_step": [[40, 49], ["datetime.datetime.datetime.now", "visualization.TensorboardWriter.add_scalar", "datetime.datetime.datetime.now", "datetime.datetime.datetime.now", "duration.total_seconds"], "methods", ["None"], ["", "def", "set_step", "(", "self", ",", "step", ",", "mode", "=", "'train'", ")", ":", "\n", "        ", "self", ".", "mode", "=", "mode", "\n", "self", ".", "step", "=", "step", "\n", "if", "step", "==", "0", ":", "\n", "            ", "self", ".", "timer", "=", "datetime", ".", "now", "(", ")", "\n", "", "else", ":", "\n", "            ", "duration", "=", "datetime", ".", "now", "(", ")", "-", "self", ".", "timer", "\n", "self", ".", "add_scalar", "(", "'steps_per_sec'", ",", "1", "/", "duration", ".", "total_seconds", "(", ")", ")", "\n", "self", ".", "timer", "=", "datetime", ".", "now", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.logger.visualization.TensorboardWriter.__getattr__": [[50, 74], ["getattr", "object.__getattr__", "getattr.", "AttributeError"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.logger.visualization.TensorboardWriter.__getattr__"], ["", "", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "        ", "\"\"\"\n        If visualization is configured to use:\n            return add_data() methods of tensorboard with additional information (step, tag) added.\n        Otherwise:\n            return a blank function handle that does nothing\n        \"\"\"", "\n", "if", "name", "in", "self", ".", "tb_writer_ftns", ":", "\n", "            ", "add_data", "=", "getattr", "(", "self", ".", "writer", ",", "name", ",", "None", ")", "\n", "\n", "def", "wrapper", "(", "tag", ",", "data", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "                ", "if", "add_data", "is", "not", "None", ":", "\n", "# add mode(train/valid) tag", "\n", "                    ", "if", "name", "not", "in", "self", ".", "tag_mode_exceptions", ":", "\n", "                        ", "tag", "=", "'{}/{}'", ".", "format", "(", "tag", ",", "self", ".", "mode", ")", "\n", "", "add_data", "(", "tag", ",", "data", ",", "self", ".", "step", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "", "return", "wrapper", "\n", "", "else", ":", "\n", "# default action for returning methods defined in this class, set_step() for instance.", "\n", "            ", "try", ":", "\n", "                ", "attr", "=", "object", ".", "__getattr__", "(", "name", ")", "\n", "", "except", "AttributeError", ":", "\n", "                ", "raise", "AttributeError", "(", "\"type object '{}' has no attribute '{}'\"", ".", "format", "(", "self", ".", "selected_module", ",", "name", ")", ")", "\n", "", "return", "attr", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.logger.logger.setup_logging": [[7, 23], ["pathlib.Path", "pathlib.Path.is_file", "tmp.utils.read_json", "config[].items", "logging.config.dictConfig", "logging.config.dictConfig", "print", "logging.basicConfig", "logging.basicConfig", "str"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.utils.util.read_json"], ["def", "setup_logging", "(", "save_dir", ",", "log_config", "=", "'logger/logger_config.json'", ",", "default_level", "=", "logging", ".", "INFO", ")", ":", "\n", "    ", "\"\"\"\n    Setup logging configuration\n    \"\"\"", "\n", "log_config", "=", "Path", "(", "log_config", ")", "\n", "if", "log_config", ".", "is_file", "(", ")", ":", "\n", "        ", "config", "=", "read_json", "(", "log_config", ")", "\n", "# modify logging paths based on run config", "\n", "for", "_", ",", "handler", "in", "config", "[", "'handlers'", "]", ".", "items", "(", ")", ":", "\n", "            ", "if", "'filename'", "in", "handler", ":", "\n", "                ", "handler", "[", "'filename'", "]", "=", "str", "(", "save_dir", "/", "handler", "[", "'filename'", "]", ")", "\n", "\n", "", "", "logging", ".", "config", ".", "dictConfig", "(", "config", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Warning: logging configuration file is not found in {}.\"", ".", "format", "(", "log_config", ")", ")", "\n", "logging", ".", "basicConfig", "(", "level", "=", "default_level", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.Identity.forward": [[17, 19], ["None"], "methods", ["None"], ["    ", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.GANLoss.__init__": [[233, 258], ["torch.Module.__init__", "networks.GANLoss.register_buffer", "networks.GANLoss.register_buffer", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.MSELoss", "torch.MSELoss", "torch.MSELoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCEWithLogitsLoss", "torch.BCELoss", "torch.BCELoss", "torch.BCELoss", "NotImplementedError"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["def", "__init__", "(", "self", ",", "gan_mode", ",", "target_real_label", "=", "1.0", ",", "target_fake_label", "=", "0.0", ")", ":", "\n", "        ", "\"\"\" Initialize the GANLoss class.\n\n        Parameters:\n            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n            target_real_label (bool) - - label for a real image\n            target_fake_label (bool) - - label of a fake image\n\n        Note: Do not use sigmoid as the last layer of Discriminator.\n        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n        \"\"\"", "\n", "super", "(", "GANLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "register_buffer", "(", "'real_label'", ",", "torch", ".", "tensor", "(", "target_real_label", ")", ")", "\n", "self", ".", "register_buffer", "(", "'fake_label'", ",", "torch", ".", "tensor", "(", "target_fake_label", ")", ")", "\n", "self", ".", "gan_mode", "=", "gan_mode", "\n", "if", "gan_mode", "==", "'lsgan'", ":", "\n", "            ", "self", ".", "loss", "=", "nn", ".", "MSELoss", "(", ")", "\n", "", "elif", "gan_mode", "==", "'vanilla'", ":", "\n", "            ", "self", ".", "loss", "=", "nn", ".", "BCEWithLogitsLoss", "(", ")", "\n", "", "elif", "gan_mode", "==", "'bce'", ":", "\n", "            ", "self", ".", "loss", "=", "nn", ".", "BCELoss", "(", ")", "\n", "", "elif", "gan_mode", "in", "[", "'wgangp'", "]", ":", "\n", "            ", "self", ".", "loss", "=", "None", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'gan mode %s not implemented'", "%", "gan_mode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.GANLoss.get_target_tensor": [[259, 275], ["target_tensor.expand_as"], "methods", ["None"], ["", "", "def", "get_target_tensor", "(", "self", ",", "prediction", ",", "target_is_real", ")", ":", "\n", "        ", "\"\"\"Create label tensors with the same size as the input.\n\n        Parameters:\n            prediction (tensor) - - tpyically the prediction from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n\n        Returns:\n            A label tensor filled with ground truth label, and with the size of the input\n        \"\"\"", "\n", "\n", "if", "target_is_real", ":", "\n", "            ", "target_tensor", "=", "self", ".", "real_label", "\n", "", "else", ":", "\n", "            ", "target_tensor", "=", "self", ".", "fake_label", "\n", "", "return", "target_tensor", ".", "expand_as", "(", "prediction", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.GANLoss.__call__": [[276, 297], ["networks.GANLoss.get_target_tensor", "networks.GANLoss.loss", "target_tensor.double.double.double", "prediction.mean", "prediction.mean"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.GANLoss.get_target_tensor"], ["", "def", "__call__", "(", "self", ",", "prediction", ",", "target_is_real", ")", ":", "\n", "        ", "\"\"\"Calculate loss given Discriminator's output and grount truth labels.\n\n        Parameters:\n            prediction (tensor) - - tpyically the prediction output from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n\n        Returns:\n            the calculated loss.\n        \"\"\"", "\n", "if", "self", ".", "gan_mode", "in", "[", "'lsgan'", ",", "'vanilla'", ",", "'bce'", "]", ":", "\n", "            ", "target_tensor", "=", "self", ".", "get_target_tensor", "(", "prediction", ",", "target_is_real", ")", "\n", "if", "prediction", ".", "dtype", "==", "torch", ".", "float64", ":", "\n", "                ", "target_tensor", "=", "target_tensor", ".", "double", "(", ")", "\n", "", "loss", "=", "self", ".", "loss", "(", "prediction", ",", "target_tensor", ")", "\n", "", "elif", "self", ".", "gan_mode", "==", "'wgangp'", ":", "\n", "            ", "if", "target_is_real", ":", "\n", "                ", "loss", "=", "-", "prediction", ".", "mean", "(", ")", "\n", "", "else", ":", "\n", "                ", "loss", "=", "prediction", ".", "mean", "(", ")", "\n", "", "", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.SoftBCELoss.__init__": [[300, 303], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", "SoftBCELoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "return", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.SoftBCELoss.forward": [[304, 314], ["loss.mean.mean.mean", "inputs.log"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "inputs", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        :param inputs: predictions\n        :param target: target labels\n        :return: loss\n        \"\"\"", "\n", "loss", "=", "-", "target", "*", "inputs", ".", "log", "(", ")", "-", "(", "1", "-", "target", ")", "*", "(", "1", "-", "inputs", ")", ".", "log", "(", ")", "\n", "loss", "=", "loss", ".", "mean", "(", ")", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.ResnetGenerator.__init__": [[359, 408], ["torch.Module.__init__", "range", "range", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "type", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "networks.ResnetBlock", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "int", "int"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["def", "__init__", "(", "self", ",", "input_nc", ",", "output_nc", ",", "ngf", "=", "64", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "use_dropout", "=", "False", ",", "n_blocks", "=", "6", ",", "padding_type", "=", "'reflect'", ")", ":", "\n", "        ", "\"\"\"Construct a Resnet-based generator\n\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        \"\"\"", "\n", "assert", "(", "n_blocks", ">=", "0", ")", "\n", "super", "(", "ResnetGenerator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "type", "(", "norm_layer", ")", "==", "functools", ".", "partial", ":", "\n", "            ", "use_bias", "=", "norm_layer", ".", "func", "==", "nn", ".", "InstanceNorm2d", "\n", "", "else", ":", "\n", "            ", "use_bias", "=", "norm_layer", "==", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "model", "=", "[", "nn", ".", "ReflectionPad2d", "(", "3", ")", ",", "\n", "nn", ".", "Conv2d", "(", "input_nc", ",", "ngf", ",", "kernel_size", "=", "7", ",", "padding", "=", "0", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ngf", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "\n", "n_downsampling", "=", "2", "\n", "for", "i", "in", "range", "(", "n_downsampling", ")", ":", "# add downsampling layers", "\n", "            ", "mult", "=", "2", "**", "i", "\n", "model", "+=", "[", "nn", ".", "Conv2d", "(", "ngf", "*", "mult", ",", "ngf", "*", "mult", "*", "2", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ngf", "*", "mult", "*", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "\n", "", "mult", "=", "2", "**", "n_downsampling", "\n", "for", "i", "in", "range", "(", "n_blocks", ")", ":", "# add ResNet blocks", "\n", "\n", "            ", "model", "+=", "[", "ResnetBlock", "(", "ngf", "*", "mult", ",", "padding_type", "=", "padding_type", ",", "norm_layer", "=", "norm_layer", ",", "use_dropout", "=", "use_dropout", ",", "use_bias", "=", "use_bias", ")", "]", "\n", "\n", "", "for", "i", "in", "range", "(", "n_downsampling", ")", ":", "# add upsampling layers", "\n", "            ", "mult", "=", "2", "**", "(", "n_downsampling", "-", "i", ")", "\n", "model", "+=", "[", "nn", ".", "ConvTranspose2d", "(", "ngf", "*", "mult", ",", "int", "(", "ngf", "*", "mult", "/", "2", ")", ",", "\n", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "\n", "padding", "=", "1", ",", "output_padding", "=", "1", ",", "\n", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "int", "(", "ngf", "*", "mult", "/", "2", ")", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "", "model", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "3", ")", "]", "\n", "model", "+=", "[", "nn", ".", "Conv2d", "(", "ngf", ",", "output_nc", ",", "kernel_size", "=", "7", ",", "padding", "=", "0", ")", "]", "\n", "model", "+=", "[", "nn", ".", "Tanh", "(", ")", "]", "\n", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "*", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.ResnetGenerator.forward": [[409, 412], ["networks.ResnetGenerator.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Standard forward\"\"\"", "\n", "return", "self", ".", "model", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.ResnetGeneratorV2.__init__": [[420, 469], ["torch.Module.__init__", "range", "range", "range", "torch.Sequential", "torch.Sequential", "torch.Sequential", "type", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "networks.ResnetBlock", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "int", "int"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["def", "__init__", "(", "self", ",", "input_nc", ",", "output_nc", ",", "ngf", "=", "64", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "use_dropout", "=", "False", ",", "n_blocks", "=", "6", ",", "padding_type", "=", "'reflect'", ")", ":", "\n", "        ", "\"\"\"Construct a Resnet-based generator\n\n        Parameters:\n            input_nc (int)      -- the number of channels in input images\n            output_nc (int)     -- the number of channels in output images\n            ngf (int)           -- the number of filters in the last conv layer\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers\n            n_blocks (int)      -- the number of ResNet blocks\n            padding_type (str)  -- the name of padding layer in conv layers: reflect | replicate | zero\n        \"\"\"", "\n", "assert", "(", "n_blocks", ">=", "0", ")", "\n", "super", "(", "ResnetGeneratorV2", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "type", "(", "norm_layer", ")", "==", "functools", ".", "partial", ":", "\n", "            ", "use_bias", "=", "norm_layer", ".", "func", "==", "nn", ".", "InstanceNorm2d", "\n", "", "else", ":", "\n", "            ", "use_bias", "=", "norm_layer", "==", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "model", "=", "[", "nn", ".", "ReflectionPad2d", "(", "3", ")", ",", "\n", "nn", ".", "Conv2d", "(", "input_nc", ",", "ngf", ",", "kernel_size", "=", "7", ",", "padding", "=", "0", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ngf", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "\n", "n_downsampling", "=", "2", "\n", "for", "i", "in", "range", "(", "n_downsampling", ")", ":", "# add downsampling layers", "\n", "            ", "mult", "=", "2", "**", "i", "\n", "model", "+=", "[", "nn", ".", "Conv2d", "(", "ngf", "*", "mult", ",", "ngf", "*", "mult", "*", "2", ",", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ngf", "*", "mult", "*", "2", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "\n", "", "mult", "=", "2", "**", "n_downsampling", "\n", "for", "i", "in", "range", "(", "n_blocks", ")", ":", "# add ResNet blocks", "\n", "\n", "            ", "model", "+=", "[", "ResnetBlock", "(", "ngf", "*", "mult", ",", "padding_type", "=", "padding_type", ",", "norm_layer", "=", "norm_layer", ",", "use_dropout", "=", "use_dropout", ",", "use_bias", "=", "use_bias", ")", "]", "\n", "\n", "", "for", "i", "in", "range", "(", "n_downsampling", ")", ":", "# add upsampling layers", "\n", "            ", "mult", "=", "2", "**", "(", "n_downsampling", "-", "i", ")", "\n", "model", "+=", "[", "nn", ".", "ConvTranspose2d", "(", "ngf", "*", "mult", ",", "int", "(", "ngf", "*", "mult", "/", "2", ")", ",", "\n", "kernel_size", "=", "3", ",", "stride", "=", "2", ",", "\n", "padding", "=", "1", ",", "output_padding", "=", "1", ",", "\n", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "int", "(", "ngf", "*", "mult", "/", "2", ")", ")", ",", "\n", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "", "model", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "3", ")", "]", "\n", "model", "+=", "[", "nn", ".", "Conv2d", "(", "ngf", ",", "output_nc", ",", "kernel_size", "=", "7", ",", "padding", "=", "0", ")", "]", "\n", "model", "+=", "[", "nn", ".", "Tanh", "(", ")", "]", "\n", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "*", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.ResnetGeneratorV2.forward": [[470, 476], ["range", "networks.ResnetGeneratorV2.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Standard forward\"\"\"", "\n", "x", "=", "input", "\n", "for", "i", "in", "range", "(", "19", ")", ":", "\n", "            ", "x", "=", "self", ".", "model", "[", "i", "]", "(", "x", ")", "\n", "", "return", "self", ".", "model", "(", "input", ")", ",", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.ResnetBlock.__init__": [[481, 491], ["torch.Module.__init__", "networks.ResnetBlock.build_conv_block"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.ResnetBlock.build_conv_block"], ["def", "__init__", "(", "self", ",", "dim", ",", "padding_type", ",", "norm_layer", ",", "use_dropout", ",", "use_bias", ")", ":", "\n", "        ", "\"\"\"Initialize the Resnet block\n\n        A resnet block is a conv block with skip connections\n        We construct a conv block with build_conv_block function,\n        and implement skip connections in <forward> function.\n        Original Resnet paper: https://arxiv.org/pdf/1512.03385.pdf\n        \"\"\"", "\n", "super", "(", "ResnetBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "conv_block", "=", "self", ".", "build_conv_block", "(", "dim", ",", "padding_type", ",", "norm_layer", ",", "use_dropout", ",", "use_bias", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.ResnetBlock.build_conv_block": [[492, 531], ["torch.Sequential", "torch.Sequential", "torch.Sequential", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReflectionPad2d", "torch.ReplicationPad2d", "torch.ReplicationPad2d", "torch.ReplicationPad2d", "NotImplementedError", "torch.ReplicationPad2d", "torch.ReplicationPad2d", "torch.ReplicationPad2d", "NotImplementedError"], "methods", ["None"], ["", "def", "build_conv_block", "(", "self", ",", "dim", ",", "padding_type", ",", "norm_layer", ",", "use_dropout", ",", "use_bias", ")", ":", "\n", "        ", "\"\"\"Construct a convolutional block.\n\n        Parameters:\n            dim (int)           -- the number of channels in the conv layer.\n            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers.\n            use_bias (bool)     -- if the conv layer uses bias or not\n\n        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n        \"\"\"", "\n", "conv_block", "=", "[", "]", "\n", "p", "=", "0", "\n", "if", "padding_type", "==", "'reflect'", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", "]", "\n", "", "elif", "padding_type", "==", "'replicate'", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "ReplicationPad2d", "(", "1", ")", "]", "\n", "", "elif", "padding_type", "==", "'zero'", ":", "\n", "            ", "p", "=", "1", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'padding [%s] is not implemented'", "%", "padding_type", ")", "\n", "\n", "", "conv_block", "+=", "[", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ",", "padding", "=", "p", ",", "bias", "=", "use_bias", ")", ",", "norm_layer", "(", "dim", ")", ",", "nn", ".", "ReLU", "(", "True", ")", "]", "\n", "if", "use_dropout", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "Dropout", "(", "0.5", ")", "]", "\n", "\n", "", "p", "=", "0", "\n", "if", "padding_type", "==", "'reflect'", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "ReflectionPad2d", "(", "1", ")", "]", "\n", "", "elif", "padding_type", "==", "'replicate'", ":", "\n", "            ", "conv_block", "+=", "[", "nn", ".", "ReplicationPad2d", "(", "1", ")", "]", "\n", "", "elif", "padding_type", "==", "'zero'", ":", "\n", "            ", "p", "=", "1", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'padding [%s] is not implemented'", "%", "padding_type", ")", "\n", "", "conv_block", "+=", "[", "nn", ".", "Conv2d", "(", "dim", ",", "dim", ",", "kernel_size", "=", "3", ",", "padding", "=", "p", ",", "bias", "=", "use_bias", ")", ",", "norm_layer", "(", "dim", ")", "]", "\n", "\n", "return", "nn", ".", "Sequential", "(", "*", "conv_block", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.ResnetBlock.forward": [[532, 536], ["networks.ResnetBlock.conv_block"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"Forward function (with skip connections)\"\"\"", "\n", "out", "=", "x", "+", "self", ".", "conv_block", "(", "x", ")", "# add skip connections", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.UnetGenerator.__init__": [[541, 564], ["torch.Module.__init__", "networks.UnetSkipConnectionBlock", "range", "networks.UnetSkipConnectionBlock", "networks.UnetSkipConnectionBlock", "networks.UnetSkipConnectionBlock", "networks.UnetSkipConnectionBlock", "networks.UnetSkipConnectionBlock"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["def", "__init__", "(", "self", ",", "input_nc", ",", "output_nc", ",", "num_downs", ",", "ngf", "=", "64", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "use_dropout", "=", "False", ")", ":", "\n", "        ", "\"\"\"Construct a Unet generator\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            output_nc (int) -- the number of channels in output images\n            num_downs (int) -- the number of downsamplings in UNet. For example, # if |num_downs| == 7,\n                                image of size 128x128 will become of size 1x1 # at the bottleneck\n            ngf (int)       -- the number of filters in the last conv layer\n            norm_layer      -- normalization layer\n\n        We construct the U-Net from the innermost layer to the outermost layer.\n        It is a recursive process.\n        \"\"\"", "\n", "super", "(", "UnetGenerator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# construct unet structure", "\n", "unet_block", "=", "UnetSkipConnectionBlock", "(", "ngf", "*", "8", ",", "ngf", "*", "8", ",", "input_nc", "=", "None", ",", "submodule", "=", "None", ",", "norm_layer", "=", "norm_layer", ",", "innermost", "=", "True", ")", "# add the innermost layer", "\n", "for", "i", "in", "range", "(", "num_downs", "-", "5", ")", ":", "# add intermediate layers with ngf * 8 filters", "\n", "            ", "unet_block", "=", "UnetSkipConnectionBlock", "(", "ngf", "*", "8", ",", "ngf", "*", "8", ",", "input_nc", "=", "None", ",", "submodule", "=", "unet_block", ",", "norm_layer", "=", "norm_layer", ",", "use_dropout", "=", "use_dropout", ")", "\n", "# gradually reduce the number of filters from ngf * 8 to ngf", "\n", "", "unet_block", "=", "UnetSkipConnectionBlock", "(", "ngf", "*", "4", ",", "ngf", "*", "8", ",", "input_nc", "=", "None", ",", "submodule", "=", "unet_block", ",", "norm_layer", "=", "norm_layer", ")", "\n", "unet_block", "=", "UnetSkipConnectionBlock", "(", "ngf", "*", "2", ",", "ngf", "*", "4", ",", "input_nc", "=", "None", ",", "submodule", "=", "unet_block", ",", "norm_layer", "=", "norm_layer", ")", "\n", "unet_block", "=", "UnetSkipConnectionBlock", "(", "ngf", ",", "ngf", "*", "2", ",", "input_nc", "=", "None", ",", "submodule", "=", "unet_block", ",", "norm_layer", "=", "norm_layer", ")", "\n", "self", ".", "model", "=", "UnetSkipConnectionBlock", "(", "output_nc", ",", "ngf", ",", "input_nc", "=", "input_nc", ",", "submodule", "=", "unet_block", ",", "outermost", "=", "True", ",", "norm_layer", "=", "norm_layer", ")", "# add the outermost layer", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.UnetGenerator.forward": [[565, 568], ["networks.UnetGenerator.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Standard forward\"\"\"", "\n", "return", "self", ".", "model", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.cDCGANGenerator.__init__": [[573, 595], ["torch.Module.__init__", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "norm_layer", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "norm_layer", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "norm_layer", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "norm_layer", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["def", "__init__", "(", "self", ",", "input_nz", ",", "output_nc", ",", "ngf", "=", "64", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "num_classes", "=", "10", ")", ":", "\n", "        ", "\"\"\"Construct a Unet generator\n        Parameters:\n            input_nz (int)  -- the length of input noise vector\n            output_nc (int) -- the number of channels in output images\n            num_classes (int) -- the number of classes in the dataset\n            ngf (int)       -- the number of filters in the last conv layer\n            norm_layer      -- normalization layer\n\n        \"\"\"", "\n", "super", "(", "cDCGANGenerator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "deconv1_1", "=", "nn", ".", "ConvTranspose2d", "(", "input_nz", ",", "ngf", "*", "2", ",", "kernel_size", "=", "4", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "deconv1_1_bn", "=", "norm_layer", "(", "ngf", "*", "2", ")", "\n", "# class one-hot vector input", "\n", "self", ".", "deconv1_2", "=", "nn", ".", "ConvTranspose2d", "(", "num_classes", ",", "ngf", "*", "2", ",", "kernel_size", "=", "4", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", "\n", "self", ".", "deconv1_2_bn", "=", "norm_layer", "(", "ngf", "*", "2", ")", "\n", "self", ".", "deconv2", "=", "nn", ".", "ConvTranspose2d", "(", "ngf", "*", "4", ",", "ngf", "*", "2", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "deconv2_bn", "=", "norm_layer", "(", "ngf", "*", "2", ")", "\n", "self", ".", "deconv3", "=", "nn", ".", "ConvTranspose2d", "(", "ngf", "*", "2", ",", "ngf", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "self", ".", "deconv3_bn", "=", "norm_layer", "(", "ngf", ")", "\n", "self", ".", "deconv4", "=", "nn", ".", "ConvTranspose2d", "(", "ngf", ",", "output_nc", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.cDCGANGenerator.forward": [[596, 605], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "networks.cDCGANGenerator.deconv1_1_bn", "networks.cDCGANGenerator.deconv1_2_bn", "networks.cDCGANGenerator.deconv2_bn", "networks.cDCGANGenerator.deconv3_bn", "networks.cDCGANGenerator.deconv4", "networks.cDCGANGenerator.deconv1_1", "networks.cDCGANGenerator.deconv1_2", "networks.cDCGANGenerator.deconv2", "networks.cDCGANGenerator.deconv3"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "label", ")", ":", "\n", "        ", "\"\"\"Standard forward\"\"\"", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "deconv1_1_bn", "(", "self", ".", "deconv1_1", "(", "input", ")", ")", ")", "\n", "y", "=", "F", ".", "relu", "(", "self", ".", "deconv1_2_bn", "(", "self", ".", "deconv1_2", "(", "label", ")", ")", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "y", "]", ",", "dim", "=", "1", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "deconv2_bn", "(", "self", ".", "deconv2", "(", "x", ")", ")", ")", "\n", "x", "=", "F", ".", "relu", "(", "self", ".", "deconv3_bn", "(", "self", ".", "deconv3", "(", "x", ")", ")", ")", "\n", "x", "=", "torch", ".", "tanh", "(", "self", ".", "deconv4", "(", "x", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.UnetSkipConnectionBlock.__init__": [[613, 669], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "norm_layer", "torch.ReLU", "torch.ReLU", "torch.ReLU", "norm_layer", "torch.Sequential", "torch.Sequential", "torch.Sequential", "type", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.Tanh", "torch.Tanh", "torch.Tanh", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.ConvTranspose2d", "torch.Dropout", "torch.Dropout", "torch.Dropout"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["def", "__init__", "(", "self", ",", "outer_nc", ",", "inner_nc", ",", "input_nc", "=", "None", ",", "\n", "submodule", "=", "None", ",", "outermost", "=", "False", ",", "innermost", "=", "False", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "use_dropout", "=", "False", ")", ":", "\n", "        ", "\"\"\"Construct a Unet submodule with skip connections.\n\n        Parameters:\n            outer_nc (int) -- the number of filters in the outer conv layer\n            inner_nc (int) -- the number of filters in the inner conv layer\n            input_nc (int) -- the number of channels in input images/features\n            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n            outermost (bool)    -- if this module is the outermost module\n            innermost (bool)    -- if this module is the innermost module\n            norm_layer          -- normalization layer\n            user_dropout (bool) -- if use dropout layers.\n        \"\"\"", "\n", "super", "(", "UnetSkipConnectionBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "outermost", "=", "outermost", "\n", "if", "type", "(", "norm_layer", ")", "==", "functools", ".", "partial", ":", "\n", "            ", "use_bias", "=", "norm_layer", ".", "func", "==", "nn", ".", "InstanceNorm2d", "\n", "", "else", ":", "\n", "            ", "use_bias", "=", "norm_layer", "==", "nn", ".", "InstanceNorm2d", "\n", "", "if", "input_nc", "is", "None", ":", "\n", "            ", "input_nc", "=", "outer_nc", "\n", "", "downconv", "=", "nn", ".", "Conv2d", "(", "input_nc", ",", "inner_nc", ",", "kernel_size", "=", "4", ",", "\n", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "use_bias", ")", "\n", "downrelu", "=", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", "\n", "downnorm", "=", "norm_layer", "(", "inner_nc", ")", "\n", "uprelu", "=", "nn", ".", "ReLU", "(", "True", ")", "\n", "upnorm", "=", "norm_layer", "(", "outer_nc", ")", "\n", "\n", "if", "outermost", ":", "\n", "            ", "upconv", "=", "nn", ".", "ConvTranspose2d", "(", "inner_nc", "*", "2", ",", "outer_nc", ",", "\n", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "\n", "padding", "=", "1", ")", "\n", "down", "=", "[", "downconv", "]", "\n", "up", "=", "[", "uprelu", ",", "upconv", ",", "nn", ".", "Tanh", "(", ")", "]", "\n", "model", "=", "down", "+", "[", "submodule", "]", "+", "up", "\n", "", "elif", "innermost", ":", "\n", "            ", "upconv", "=", "nn", ".", "ConvTranspose2d", "(", "inner_nc", ",", "outer_nc", ",", "\n", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "\n", "padding", "=", "1", ",", "bias", "=", "use_bias", ")", "\n", "down", "=", "[", "downrelu", ",", "downconv", "]", "\n", "up", "=", "[", "uprelu", ",", "upconv", ",", "upnorm", "]", "\n", "model", "=", "down", "+", "up", "\n", "", "else", ":", "\n", "            ", "upconv", "=", "nn", ".", "ConvTranspose2d", "(", "inner_nc", "*", "2", ",", "outer_nc", ",", "\n", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "\n", "padding", "=", "1", ",", "bias", "=", "use_bias", ")", "\n", "down", "=", "[", "downrelu", ",", "downconv", ",", "downnorm", "]", "\n", "up", "=", "[", "uprelu", ",", "upconv", ",", "upnorm", "]", "\n", "\n", "if", "use_dropout", ":", "\n", "                ", "model", "=", "down", "+", "[", "submodule", "]", "+", "up", "+", "[", "nn", ".", "Dropout", "(", "0.5", ")", "]", "\n", "", "else", ":", "\n", "                ", "model", "=", "down", "+", "[", "submodule", "]", "+", "up", "\n", "\n", "", "", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "*", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.UnetSkipConnectionBlock.forward": [[670, 675], ["networks.UnetSkipConnectionBlock.model", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "networks.UnetSkipConnectionBlock.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "outermost", ":", "\n", "            ", "return", "self", ".", "model", "(", "x", ")", "\n", "", "else", ":", "# add skip connections", "\n", "            ", "return", "torch", ".", "cat", "(", "[", "x", ",", "self", ".", "model", "(", "x", ")", "]", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.NLayerDiscriminator.__init__": [[680, 719], ["torch.Module.__init__", "range", "min", "torch.Sequential", "torch.Sequential", "torch.Sequential", "type", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "min", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["def", "__init__", "(", "self", ",", "input_nc", ",", "ndf", "=", "64", ",", "n_layers", "=", "3", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "\"\"\"Construct a PatchGAN discriminator\n\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator\n            norm_layer      -- normalization layer\n        \"\"\"", "\n", "super", "(", "NLayerDiscriminator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "type", "(", "norm_layer", ")", "==", "functools", ".", "partial", ":", "# no need to use bias as BatchNorm2d has affine parameters", "\n", "            ", "use_bias", "=", "norm_layer", ".", "func", "==", "nn", ".", "InstanceNorm2d", "\n", "", "else", ":", "\n", "            ", "use_bias", "=", "norm_layer", "==", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "kw", "=", "4", "\n", "padw", "=", "1", "\n", "sequence", "=", "[", "nn", ".", "Conv2d", "(", "input_nc", ",", "ndf", ",", "kernel_size", "=", "kw", ",", "stride", "=", "2", ",", "padding", "=", "padw", ")", ",", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", "]", "\n", "nf_mult", "=", "1", "\n", "nf_mult_prev", "=", "1", "\n", "for", "n", "in", "range", "(", "1", ",", "n_layers", ")", ":", "# gradually increase the number of filters", "\n", "            ", "nf_mult_prev", "=", "nf_mult", "\n", "nf_mult", "=", "min", "(", "2", "**", "n", ",", "8", ")", "\n", "sequence", "+=", "[", "\n", "nn", ".", "Conv2d", "(", "ndf", "*", "nf_mult_prev", ",", "ndf", "*", "nf_mult", ",", "kernel_size", "=", "kw", ",", "stride", "=", "2", ",", "padding", "=", "padw", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ndf", "*", "nf_mult", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", "\n", "]", "\n", "\n", "", "nf_mult_prev", "=", "nf_mult", "\n", "nf_mult", "=", "min", "(", "2", "**", "n_layers", ",", "8", ")", "\n", "sequence", "+=", "[", "\n", "nn", ".", "Conv2d", "(", "ndf", "*", "nf_mult_prev", ",", "ndf", "*", "nf_mult", ",", "kernel_size", "=", "kw", ",", "stride", "=", "1", ",", "padding", "=", "padw", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ndf", "*", "nf_mult", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", "\n", "]", "\n", "\n", "sequence", "+=", "[", "nn", ".", "Conv2d", "(", "ndf", "*", "nf_mult", ",", "1", ",", "kernel_size", "=", "kw", ",", "stride", "=", "1", ",", "padding", "=", "padw", ")", "]", "# output 1 channel prediction map", "\n", "self", ".", "model", "=", "nn", ".", "Sequential", "(", "*", "sequence", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.NLayerDiscriminator.forward": [[720, 723], ["networks.NLayerDiscriminator.model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Standard forward.\"\"\"", "\n", "return", "self", ".", "model", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.PixelDiscriminator.__init__": [[728, 751], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Sequential", "type", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "norm_layer", "torch.LeakyReLU", "torch.LeakyReLU", "torch.LeakyReLU", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["def", "__init__", "(", "self", ",", "input_nc", ",", "ndf", "=", "64", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ")", ":", "\n", "        ", "\"\"\"Construct a 1x1 PatchGAN discriminator\n\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            norm_layer      -- normalization layer\n        \"\"\"", "\n", "super", "(", "PixelDiscriminator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "type", "(", "norm_layer", ")", "==", "functools", ".", "partial", ":", "# no need to use bias as BatchNorm2d has affine parameters", "\n", "            ", "use_bias", "=", "norm_layer", ".", "func", "==", "nn", ".", "InstanceNorm2d", "\n", "", "else", ":", "\n", "            ", "use_bias", "=", "norm_layer", "==", "nn", ".", "InstanceNorm2d", "\n", "\n", "", "self", ".", "net", "=", "[", "\n", "nn", ".", "Conv2d", "(", "input_nc", ",", "ndf", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "ndf", ",", "ndf", "*", "2", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "use_bias", ")", ",", "\n", "norm_layer", "(", "ndf", "*", "2", ")", ",", "\n", "nn", ".", "LeakyReLU", "(", "0.2", ",", "True", ")", ",", "\n", "nn", ".", "Conv2d", "(", "ndf", "*", "2", ",", "1", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "use_bias", ")", "]", "\n", "\n", "self", ".", "net", "=", "nn", ".", "Sequential", "(", "*", "self", ".", "net", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.PixelDiscriminator.forward": [[752, 755], ["networks.PixelDiscriminator.net"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Standard forward.\"\"\"", "\n", "return", "self", ".", "net", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.cDCGANDiscriminator.__init__": [[760, 777], ["torch.Module.__init__", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.nn.utils.spectral_norm", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["def", "__init__", "(", "self", ",", "input_nc", ",", "ndf", "=", "64", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "num_classes", "=", "10", ")", ":", "\n", "        ", "\"\"\"Construct a 1x1 PatchGAN discriminator\n\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            norm_layer      -- normalization layer\n        \"\"\"", "\n", "super", "(", "cDCGANDiscriminator", ",", "self", ")", ".", "__init__", "(", ")", "\n", "ndf", "=", "ndf", "+", "1", "if", "ndf", "%", "2", "==", "1", "else", "ndf", "\n", "self", ".", "conv1_1", "=", "spectral_norm", "(", "nn", ".", "Conv2d", "(", "input_nc", ",", "ndf", "//", "2", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "self", ".", "conv1_2", "=", "spectral_norm", "(", "nn", ".", "Conv2d", "(", "num_classes", ",", "ndf", "//", "2", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "self", ".", "conv2", "=", "spectral_norm", "(", "nn", ".", "Conv2d", "(", "ndf", ",", "ndf", "*", "2", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "# self.conv2_bn = norm_layer(ndf * 2)", "\n", "self", ".", "conv3", "=", "spectral_norm", "(", "nn", ".", "Conv2d", "(", "ndf", "*", "2", ",", "ndf", "*", "4", ",", "kernel_size", "=", "4", ",", "stride", "=", "2", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", ")", "\n", "# self.conv3_bn = norm_layer(ndf * 4)", "\n", "self", ".", "conv4", "=", "spectral_norm", "(", "nn", ".", "Conv2d", "(", "ndf", "*", "4", ",", "1", ",", "kernel_size", "=", "4", ",", "stride", "=", "1", ",", "padding", "=", "0", ",", "bias", "=", "False", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.cDCGANDiscriminator.forward": [[778, 787], ["torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "networks.cDCGANDiscriminator.conv1_1", "networks.cDCGANDiscriminator.conv1_2", "networks.cDCGANDiscriminator.conv2", "networks.cDCGANDiscriminator.conv3", "networks.cDCGANDiscriminator.conv4"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "label", ")", ":", "\n", "        ", "\"\"\"Standard forward.\"\"\"", "\n", "x", "=", "F", ".", "leaky_relu", "(", "self", ".", "conv1_1", "(", "input", ")", ",", "0.2", ")", "\n", "y", "=", "F", ".", "leaky_relu", "(", "self", ".", "conv1_2", "(", "label", ")", ",", "0.2", ")", "\n", "x", "=", "torch", ".", "cat", "(", "[", "x", ",", "y", "]", ",", "1", ")", "\n", "x", "=", "F", ".", "leaky_relu", "(", "self", ".", "conv2", "(", "x", ")", ",", "0.2", ")", "\n", "x", "=", "F", ".", "leaky_relu", "(", "self", ".", "conv3", "(", "x", ")", ",", "0.2", ")", "\n", "x", "=", "torch", ".", "sigmoid", "(", "self", ".", "conv4", "(", "x", ")", ")", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.get_norm_layer": [[21, 39], ["functools.partial", "functools.partial", "NotImplementedError", "networks.Identity"], "function", ["None"], ["", "", "def", "get_norm_layer", "(", "norm_type", "=", "'instance'", ")", ":", "\n", "    ", "\"\"\"Return a normalization layer\n\n    Parameters:\n        norm_type (str) -- the name of the normalization layer: batch | instance | none\n\n    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n    \"\"\"", "\n", "if", "norm_type", "==", "'batch'", ":", "\n", "        ", "norm_layer", "=", "functools", ".", "partial", "(", "nn", ".", "BatchNorm2d", ",", "affine", "=", "True", ",", "track_running_stats", "=", "True", ")", "\n", "", "elif", "norm_type", "==", "'instance'", ":", "\n", "        ", "norm_layer", "=", "functools", ".", "partial", "(", "nn", ".", "InstanceNorm2d", ",", "affine", "=", "False", ",", "track_running_stats", "=", "False", ")", "\n", "", "elif", "norm_type", "==", "'none'", ":", "\n", "        ", "norm_layer", "=", "lambda", "x", ":", "Identity", "(", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'normalization layer [%s] is not found'", "%", "norm_type", ")", "\n", "", "return", "norm_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.get_scheduler": [[41, 76], ["torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.LambdaLR", "torch.optim.lr_scheduler.StepLR", "max", "float", "torch.optim.lr_scheduler.ReduceLROnPlateau", "float", "torch.optim.lr_scheduler.CosineAnnealingLR", "NotImplementedError", "max", "float"], "function", ["None"], ["", "def", "get_scheduler", "(", "optimizer", ",", "opt", ")", ":", "\n", "    ", "\"\"\"Return a learning rate scheduler\n\n    Parameters:\n        optimizer          -- the optimizer of the network\n        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\uff0e\u3000\n                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n\n    For 'linear', we keep the same learning rate for the first <opt.niter> epochs\n    and linearly decay the rate to zero over the next <opt.niter_decay> epochs.\n    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n    See https://pytorch.org/docs/stable/optim.html for more details.\n    \"\"\"", "\n", "if", "opt", ".", "lr_policy", "==", "'linear'", ":", "\n", "        ", "def", "lambda_rule", "(", "epoch", ")", ":", "\n", "            ", "lr_l", "=", "1.0", "-", "max", "(", "0", ",", "epoch", "+", "opt", ".", "epoch_count", "-", "opt", ".", "niter", ")", "/", "float", "(", "opt", ".", "niter_decay", "+", "1", ")", "\n", "return", "lr_l", "\n", "", "scheduler", "=", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", "=", "lambda_rule", ")", "\n", "", "elif", "opt", ".", "lr_policy", "==", "'triangle'", ":", "\n", "        ", "def", "lambda_rule", "(", "epoch", ")", ":", "\n", "            ", "if", "epoch", "<", "opt", ".", "niter", ":", "\n", "                ", "lr_l", "=", "(", "epoch", "+", "opt", ".", "epoch_count", ")", "/", "float", "(", "opt", ".", "niter", "+", "1", ")", "\n", "", "else", ":", "\n", "                ", "lr_l", "=", "1.0", "-", "max", "(", "0", ",", "epoch", "+", "opt", ".", "epoch_count", "-", "opt", ".", "niter", ")", "/", "float", "(", "opt", ".", "niter_decay", "+", "1", ")", "\n", "", "return", "lr_l", "\n", "", "scheduler", "=", "lr_scheduler", ".", "LambdaLR", "(", "optimizer", ",", "lr_lambda", "=", "lambda_rule", ")", "\n", "", "elif", "opt", ".", "lr_policy", "==", "'step'", ":", "\n", "        ", "scheduler", "=", "lr_scheduler", ".", "StepLR", "(", "optimizer", ",", "step_size", "=", "opt", ".", "lr_decay_iters", ",", "gamma", "=", "0.1", ")", "\n", "", "elif", "opt", ".", "lr_policy", "==", "'plateau'", ":", "\n", "        ", "scheduler", "=", "lr_scheduler", ".", "ReduceLROnPlateau", "(", "optimizer", ",", "mode", "=", "'min'", ",", "factor", "=", "0.2", ",", "threshold", "=", "0.01", ",", "patience", "=", "5", ")", "\n", "", "elif", "opt", ".", "lr_policy", "==", "'cosine'", ":", "\n", "        ", "scheduler", "=", "lr_scheduler", ".", "CosineAnnealingLR", "(", "optimizer", ",", "T_max", "=", "opt", ".", "niter", ",", "eta_min", "=", "0", ")", "\n", "", "else", ":", "\n", "        ", "return", "NotImplementedError", "(", "'learning rate policy [%s] is not implemented'", ",", "opt", ".", "lr_policy", ")", "\n", "", "return", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.init_weights": [[78, 110], ["print", "net.apply", "hasattr", "torch.nn.init.normal_", "hasattr", "torch.nn.init.constant_", "classname.find", "torch.nn.init.normal_", "torch.nn.init.constant_", "classname.find", "classname.find", "torch.nn.init.xavier_normal_", "torch.nn.init.kaiming_normal_", "torch.nn.init.orthogonal_", "NotImplementedError"], "function", ["None"], ["", "def", "init_weights", "(", "net", ",", "init_type", "=", "'normal'", ",", "init_gain", "=", "0.02", ")", ":", "\n", "    ", "\"\"\"Initialize network weights.\n\n    Parameters:\n        net (network)   -- network to be initialized\n        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n\n    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n    work better for some applications. Feel free to try yourself.\n    \"\"\"", "\n", "def", "init_func", "(", "m", ")", ":", "# define the initialization function", "\n", "        ", "classname", "=", "m", ".", "__class__", ".", "__name__", "\n", "if", "hasattr", "(", "m", ",", "'weight'", ")", "and", "(", "classname", ".", "find", "(", "'Conv'", ")", "!=", "-", "1", "or", "classname", ".", "find", "(", "'Linear'", ")", "!=", "-", "1", ")", ":", "\n", "            ", "if", "init_type", "==", "'normal'", ":", "\n", "                ", "init", ".", "normal_", "(", "m", ".", "weight", ".", "data", ",", "0.0", ",", "init_gain", ")", "\n", "", "elif", "init_type", "==", "'xavier'", ":", "\n", "                ", "init", ".", "xavier_normal_", "(", "m", ".", "weight", ".", "data", ",", "gain", "=", "init_gain", ")", "\n", "", "elif", "init_type", "==", "'kaiming'", ":", "\n", "                ", "init", ".", "kaiming_normal_", "(", "m", ".", "weight", ".", "data", ",", "a", "=", "0", ",", "mode", "=", "'fan_in'", ")", "\n", "", "elif", "init_type", "==", "'orthogonal'", ":", "\n", "                ", "init", ".", "orthogonal_", "(", "m", ".", "weight", ".", "data", ",", "gain", "=", "init_gain", ")", "\n", "", "else", ":", "\n", "                ", "raise", "NotImplementedError", "(", "'initialization method [%s] is not implemented'", "%", "init_type", ")", "\n", "", "if", "hasattr", "(", "m", ",", "'bias'", ")", "and", "m", ".", "bias", "is", "not", "None", ":", "\n", "                ", "init", ".", "constant_", "(", "m", ".", "bias", ".", "data", ",", "0.0", ")", "\n", "", "", "elif", "classname", ".", "find", "(", "'BatchNorm2d'", ")", "!=", "-", "1", ":", "# BatchNorm Layer's weight is not a matrix; only normal distribution applies.", "\n", "            ", "init", ".", "normal_", "(", "m", ".", "weight", ".", "data", ",", "1.0", ",", "init_gain", ")", "\n", "init", ".", "constant_", "(", "m", ".", "bias", ".", "data", ",", "0.0", ")", "\n", "\n", "", "", "print", "(", "'initialize network with %s'", "%", "init_type", ")", "\n", "net", ".", "apply", "(", "init_func", ")", "# apply the initialization function <init_func>", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.init_net": [[112, 128], ["networks.init_weights", "len", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.nn.DataParallel.to", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.init_weights"], ["", "def", "init_net", "(", "net", ",", "init_type", "=", "'normal'", ",", "init_gain", "=", "0.02", ",", "gpu_ids", "=", "[", "]", ")", ":", "\n", "    ", "\"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n    Parameters:\n        net (network)      -- the network to be initialized\n        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Return an initialized network.\n    \"\"\"", "\n", "if", "len", "(", "gpu_ids", ")", ">", "0", ":", "\n", "        ", "assert", "(", "torch", ".", "cuda", ".", "is_available", "(", ")", ")", "\n", "net", ".", "to", "(", "gpu_ids", "[", "0", "]", ")", "\n", "net", "=", "torch", ".", "nn", ".", "DataParallel", "(", "net", ",", "gpu_ids", ")", "# multi-GPUs", "\n", "", "init_weights", "(", "net", ",", "init_type", ",", "init_gain", "=", "init_gain", ")", "\n", "return", "net", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_G": [[130, 175], ["networks.get_norm_layer", "networks.init_net", "networks.ResnetGenerator", "networks.ResnetGeneratorV2", "networks.ResnetGenerator", "networks.UnetGenerator", "networks.UnetGenerator", "networks.cDCGANGenerator", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.get_norm_layer", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.init_net"], ["", "def", "define_G", "(", "input_nc", ",", "output_nc", ",", "ngf", ",", "netG", ",", "norm", "=", "'batch'", ",", "use_dropout", "=", "False", ",", "init_type", "=", "'normal'", ",", "init_gain", "=", "0.02", ",", "gpu_ids", "=", "[", "]", ")", ":", "\n", "    ", "\"\"\"Create a generator\n\n    Parameters:\n        input_nc (int) -- the number of channels in input images\n        output_nc (int) -- the number of channels in output images\n        ngf (int) -- the number of filters in the last conv layer\n        netG (str) -- the architecture's name: resnet_9blocks | resnet_6blocks | unet_256 | unet_128\n        norm (str) -- the name of normalization layers used in the network: batch | instance | none\n        use_dropout (bool) -- if use dropout layers.\n        init_type (str)    -- the name of our initialization method.\n        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Returns a generator\n\n    Our current implementation provides two types of generators:\n        U-Net: [unet_128] (for 128x128 input images) and [unet_256] (for 256x256 input images)\n        The original U-Net paper: https://arxiv.org/abs/1505.04597\n\n        Resnet-based generator: [resnet_6blocks] (with 6 Resnet blocks) and [resnet_9blocks] (with 9 Resnet blocks)\n        Resnet-based generator consists of several Resnet blocks between a few downsampling/upsampling operations.\n        We adapt Torch code from Justin Johnson's neural style transfer project (https://github.com/jcjohnson/fast-neural-style).\n\n\n    The generator has been initialized by <init_net>. It uses RELU for non-linearity.\n    \"\"\"", "\n", "net", "=", "None", "\n", "norm_layer", "=", "get_norm_layer", "(", "norm_type", "=", "norm", ")", "\n", "\n", "if", "netG", "==", "'resnet_9blocks'", ":", "\n", "        ", "net", "=", "ResnetGenerator", "(", "input_nc", ",", "output_nc", ",", "ngf", ",", "norm_layer", "=", "norm_layer", ",", "use_dropout", "=", "use_dropout", ",", "n_blocks", "=", "9", ")", "\n", "", "elif", "netG", "==", "'resnet_9blocks_v2'", ":", "\n", "        ", "net", "=", "ResnetGeneratorV2", "(", "input_nc", ",", "output_nc", ",", "ngf", ",", "norm_layer", "=", "norm_layer", ",", "use_dropout", "=", "use_dropout", ",", "n_blocks", "=", "9", ")", "\n", "", "elif", "netG", "==", "'resnet_6blocks'", ":", "\n", "        ", "net", "=", "ResnetGenerator", "(", "input_nc", ",", "output_nc", ",", "ngf", ",", "norm_layer", "=", "norm_layer", ",", "use_dropout", "=", "use_dropout", ",", "n_blocks", "=", "6", ")", "\n", "", "elif", "netG", "==", "'unet_128'", ":", "\n", "        ", "net", "=", "UnetGenerator", "(", "input_nc", ",", "output_nc", ",", "7", ",", "ngf", ",", "norm_layer", "=", "norm_layer", ",", "use_dropout", "=", "use_dropout", ")", "\n", "", "elif", "netG", "==", "'unet_256'", ":", "\n", "        ", "net", "=", "UnetGenerator", "(", "input_nc", ",", "output_nc", ",", "8", ",", "ngf", ",", "norm_layer", "=", "norm_layer", ",", "use_dropout", "=", "use_dropout", ")", "\n", "", "elif", "netG", "==", "'cDCGAN'", ":", "\n", "        ", "net", "=", "cDCGANGenerator", "(", "100", ",", "output_nc", ",", "ngf", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "num_classes", "=", "10", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'Generator model name [%s] is not recognized'", "%", "netG", ")", "\n", "", "return", "init_net", "(", "net", ",", "init_type", ",", "init_gain", ",", "gpu_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_D": [[177, 221], ["networks.get_norm_layer", "networks.init_net", "networks.NLayerDiscriminator", "networks.NLayerDiscriminator", "networks.PixelDiscriminator", "networks.cDCGANDiscriminator", "NotImplementedError"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.get_norm_layer", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.init_net"], ["", "def", "define_D", "(", "input_nc", ",", "ndf", ",", "netD", ",", "n_layers_D", "=", "3", ",", "norm", "=", "'batch'", ",", "init_type", "=", "'normal'", ",", "init_gain", "=", "0.02", ",", "gpu_ids", "=", "[", "]", ")", ":", "\n", "    ", "\"\"\"Create a discriminator\n\n    Parameters:\n        input_nc (int)     -- the number of channels in input images\n        ndf (int)          -- the number of filters in the first conv layer\n        netD (str)         -- the architecture's name: basic | n_layers | pixel\n        n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD=='n_layers'\n        norm (str)         -- the type of normalization layers used in the network.\n        init_type (str)    -- the name of the initialization method.\n        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Returns a discriminator\n\n    Our current implementation provides three types of discriminators:\n        [basic]: 'PatchGAN' classifier described in the original pix2pix paper.\n        It can classify whether 70\u00d770 overlapping patches are real or fake.\n        Such a patch-level discriminator architecture has fewer parameters\n        than a full-image discriminator and can work on arbitrarily-sized images\n        in a fully convolutional fashion.\n\n        [n_layers]: With this mode, you cna specify the number of conv layers in the discriminator\n        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)\n\n        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\n        It encourages greater color diversity but has no effect on spatial statistics.\n\n    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.\n    \"\"\"", "\n", "net", "=", "None", "\n", "norm_layer", "=", "get_norm_layer", "(", "norm_type", "=", "norm", ")", "\n", "\n", "if", "netD", "==", "'basic'", ":", "# default PatchGAN classifier", "\n", "        ", "net", "=", "NLayerDiscriminator", "(", "input_nc", ",", "ndf", ",", "n_layers", "=", "3", ",", "norm_layer", "=", "norm_layer", ")", "\n", "", "elif", "netD", "==", "'n_layers'", ":", "# more options", "\n", "        ", "net", "=", "NLayerDiscriminator", "(", "input_nc", ",", "ndf", ",", "n_layers_D", ",", "norm_layer", "=", "norm_layer", ")", "\n", "", "elif", "netD", "==", "'pixel'", ":", "# classify if each pixel is real or fake", "\n", "        ", "net", "=", "PixelDiscriminator", "(", "input_nc", ",", "ndf", ",", "norm_layer", "=", "norm_layer", ")", "\n", "", "elif", "netD", "==", "'cDCGAN'", ":", "\n", "        ", "net", "=", "cDCGANDiscriminator", "(", "input_nc", ",", "ndf", ",", "norm_layer", "=", "nn", ".", "BatchNorm2d", ",", "num_classes", "=", "10", ")", "\n", "", "else", ":", "\n", "        ", "raise", "NotImplementedError", "(", "'Discriminator model name [%s] is not recognized'", "%", "netD", ")", "\n", "", "return", "init_net", "(", "net", ",", "init_type", ",", "init_gain", ",", "gpu_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.cal_gradient_penalty": [[316, 351], ["interpolatesv.requires_grad_", "netD", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "gradients[].view", "real_data.size", "torch.ones().to", "torch.ones().to", "torch.ones().to", "torch.rand", "torch.rand", "torch.rand", "alpha.expand().contiguous().view.expand().contiguous().view", "NotImplementedError", "torch.ones", "torch.ones", "torch.ones", "alpha.expand().contiguous().view.expand().contiguous", "netD.size", "alpha.expand().contiguous().view.expand", "real_data.nelement"], "function", ["None"], ["", "", "def", "cal_gradient_penalty", "(", "netD", ",", "real_data", ",", "fake_data", ",", "device", ",", "type", "=", "'mixed'", ",", "constant", "=", "1.0", ",", "lambda_gp", "=", "10.0", ")", ":", "\n", "    ", "\"\"\"Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n\n    Arguments:\n        netD (network)              -- discriminator network\n        real_data (tensor array)    -- real images\n        fake_data (tensor array)    -- generated images from the generator\n        device (str)                -- GPU / CPU: from torch.device('cuda:{}'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device('cpu')\n        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n        lambda_gp (float)           -- weight for this loss\n\n    Returns the gradient penalty loss\n    \"\"\"", "\n", "if", "lambda_gp", ">", "0.0", ":", "\n", "        ", "if", "type", "==", "'real'", ":", "# either use real images, fake images, or a linear interpolation of two.", "\n", "            ", "interpolatesv", "=", "real_data", "\n", "", "elif", "type", "==", "'fake'", ":", "\n", "            ", "interpolatesv", "=", "fake_data", "\n", "", "elif", "type", "==", "'mixed'", ":", "\n", "            ", "alpha", "=", "torch", ".", "rand", "(", "real_data", ".", "shape", "[", "0", "]", ",", "1", ",", "device", "=", "device", ")", "\n", "alpha", "=", "alpha", ".", "expand", "(", "real_data", ".", "shape", "[", "0", "]", ",", "real_data", ".", "nelement", "(", ")", "//", "real_data", ".", "shape", "[", "0", "]", ")", ".", "contiguous", "(", ")", ".", "view", "(", "*", "real_data", ".", "shape", ")", "\n", "interpolatesv", "=", "alpha", "*", "real_data", "+", "(", "(", "1", "-", "alpha", ")", "*", "fake_data", ")", "\n", "", "else", ":", "\n", "            ", "raise", "NotImplementedError", "(", "'{} not implemented'", ".", "format", "(", "type", ")", ")", "\n", "", "interpolatesv", ".", "requires_grad_", "(", "True", ")", "\n", "disc_interpolates", "=", "netD", "(", "interpolatesv", ")", "\n", "gradients", "=", "torch", ".", "autograd", ".", "grad", "(", "outputs", "=", "disc_interpolates", ",", "inputs", "=", "interpolatesv", ",", "\n", "grad_outputs", "=", "torch", ".", "ones", "(", "disc_interpolates", ".", "size", "(", ")", ")", ".", "to", "(", "device", ")", ",", "\n", "create_graph", "=", "True", ",", "retain_graph", "=", "True", ",", "only_inputs", "=", "True", ")", "\n", "gradients", "=", "gradients", "[", "0", "]", ".", "view", "(", "real_data", ".", "size", "(", "0", ")", ",", "-", "1", ")", "# flat the data", "\n", "gradient_penalty", "=", "(", "(", "(", "gradients", "+", "1e-16", ")", ".", "norm", "(", "2", ",", "dim", "=", "1", ")", "-", "constant", ")", "**", "2", ")", ".", "mean", "(", ")", "*", "lambda_gp", "# added eps", "\n", "return", "gradient_penalty", ",", "gradients", "\n", "", "else", ":", "\n", "        ", "return", "0.0", ",", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_model.TDGANModel.modify_commandline_options": [[21, 50], ["parser.set_defaults", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "modify_commandline_options", "(", "parser", ",", "is_train", "=", "True", ")", ":", "\n", "        ", "\"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n\n        For pix2pix, we do not use image buffer\n        \"\"\"", "\n", "# changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)", "\n", "# parser.set_defaults(norm='batch', netG='unet_256', dataset_mode='aligned')", "\n", "if", "is_train", ":", "\n", "            ", "parser", ".", "set_defaults", "(", "pool_size", "=", "0", ",", "gan_mode", "=", "'vanilla'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_digesting_L1'", ",", "type", "=", "float", ",", "default", "=", "100.0", ",", "help", "=", "'weight for digesting L1 loss'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_digesting_perceptual'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'weight for digesting perceptual loss'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--prev_model_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'the path of trained model using previous tasks'", ")", "\n", "parser", ".", "add_argument", "(", "'--prev_model_epoch'", ",", "type", "=", "int", ",", "default", "=", "400", ",", "help", "=", "'the epoch of trained model using previous tasks'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_reminding_L1'", ",", "type", "=", "float", ",", "default", "=", "10.0", ",", "help", "=", "'weight for reminding L1 loss'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_reminding_perceptual'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'weight for reminding perceptual loss'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--lambda_G'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'weight for dadgan G '", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_D'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'weight for dadgan D'", ")", "\n", "\n", "", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_model.TDGANModel.__init__": [[51, 95], ["base_model.BaseModel.__init__", "networks.define_G", "networks.define_G", "networks.define_D", "TDGAN_model.TDGANModel.load_prev_model", "networks.GANLoss().to", "torch.nn.L1Loss", "torch.optim.Adam", "torch.optim.Adam", "TDGAN_model.TDGANModel.optimizers.append", "TDGAN_model.TDGANModel.optimizers.append", "models.perception_loss.vgg16_feat().cuda", "models.perception_loss.perceptual_loss", "TDGAN_model.TDGANModel.netG.parameters", "TDGAN_model.TDGANModel.netD.parameters", "networks.GANLoss", "models.perception_loss.vgg16_feat"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_G", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_G", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_D", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.load_prev_model"], ["", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize the pix2pix class.\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"", "\n", "BaseModel", ".", "__init__", "(", "self", ",", "opt", ")", "\n", "if", "self", ".", "opt", ".", "task_num", "==", "1", ":", "\n", "            ", "self", ".", "loss_names", "=", "[", "'G_GAN'", ",", "'G_L1'", ",", "'G_perceptual'", ",", "'D_real'", ",", "'D_fake'", "]", "\n", "self", ".", "visual_names", "=", "[", "'real_A_cur'", ",", "'fake_B_cur'", ",", "'real_B_cur'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_names", "=", "[", "'G_GAN'", ",", "'G_L1'", ",", "'G_perceptual'", ",", "'D_real'", ",", "'D_fake'", ",", "'reminding_L1_all'", ",", "'reminding_perceptual_all'", "]", "\n", "self", ".", "visual_names", "=", "[", "'real_A_prev'", ",", "'real_B_prev'", ",", "'fake_B_cur_prev'", ",", "'fake_B_prev'", ",", "'real_A_cur'", ",", "'fake_B_cur'", ",", "'real_B_cur'", "]", "\n", "# specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>", "\n", "", "if", "self", ".", "isTrain", ":", "\n", "            ", "self", ".", "model_names", "=", "[", "'G'", ",", "'D'", "]", "\n", "", "else", ":", "# during test time, only load G", "\n", "            ", "self", ".", "model_names", "=", "[", "'G'", "]", "\n", "# define networks (both generator and discriminator)", "\n", "", "self", ".", "netG", "=", "networks", ".", "define_G", "(", "opt", ".", "input_nc", ",", "opt", ".", "output_nc", ",", "opt", ".", "ngf", ",", "opt", ".", "netG", ",", "opt", ".", "norm", ",", "\n", "not", "opt", ".", "no_dropout", ",", "opt", ".", "init_type", ",", "opt", ".", "init_gain", ",", "self", ".", "gpu_ids", ")", "\n", "\n", "self", ".", "netG_prev", "=", "networks", ".", "define_G", "(", "opt", ".", "input_nc", ",", "opt", ".", "output_nc", ",", "opt", ".", "ngf", ",", "opt", ".", "netG", ",", "opt", ".", "norm", ",", "\n", "not", "opt", ".", "no_dropout", ",", "opt", ".", "init_type", ",", "opt", ".", "init_gain", ",", "self", ".", "gpu_ids", ")", "\n", "\n", "if", "self", ".", "isTrain", ":", "# define a discriminator; conditional GANs need to take both input and output images", "\n", "            ", "self", ".", "netD", "=", "networks", ".", "define_D", "(", "opt", ".", "input_nc", "+", "opt", ".", "output_nc", ",", "opt", ".", "ndf", ",", "opt", ".", "netD", ",", "\n", "opt", ".", "n_layers_D", ",", "opt", ".", "norm", ",", "opt", ".", "init_type", ",", "opt", ".", "init_gain", ",", "self", ".", "gpu_ids", ")", "\n", "\n", "", "if", "self", ".", "isTrain", "and", "self", ".", "opt", ".", "prev_model_path", ":", "\n", "            ", "self", ".", "load_prev_model", "(", ")", "\n", "\n", "", "if", "self", ".", "isTrain", ":", "\n", "# define loss functions", "\n", "            ", "self", ".", "criterionGAN", "=", "networks", ".", "GANLoss", "(", "opt", ".", "gan_mode", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "criterionL1", "=", "torch", ".", "nn", ".", "L1Loss", "(", ")", "\n", "# initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.", "\n", "self", ".", "optimizer_G", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "netG", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ",", "betas", "=", "(", "opt", ".", "beta1", ",", "0.999", ")", ")", "\n", "self", ".", "optimizer_D", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "netD", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ",", "betas", "=", "(", "opt", ".", "beta1", ",", "0.999", ")", ")", "\n", "self", ".", "optimizers", ".", "append", "(", "self", ".", "optimizer_G", ")", "\n", "self", ".", "optimizers", ".", "append", "(", "self", ".", "optimizer_D", ")", "\n", "\n", "self", ".", "vgg_model", "=", "vgg16_feat", "(", ")", ".", "cuda", "(", ")", "\n", "self", ".", "criterion_perceptual", "=", "perceptual_loss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_model.TDGANModel.load_prev_model": [[96, 112], ["range", "len", "isinstance", "print", "torch.load", "hasattr", "net.load_state_dict"], "methods", ["None"], ["", "", "def", "load_prev_model", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load trained Generator using previous tasks\n        \"\"\"", "\n", "nets", "=", "[", "self", ".", "netG_prev", ",", "self", ".", "netG", "]", "\n", "model_paths", "=", "[", "'{:s}/{:d}_net_G.pth'", ".", "format", "(", "self", ".", "opt", ".", "prev_model_path", ",", "self", ".", "opt", ".", "prev_model_epoch", ")", ",", "\n", "'{:s}/{:d}_net_G.pth'", ".", "format", "(", "self", ".", "opt", ".", "prev_model_path", ",", "self", ".", "opt", ".", "prev_model_epoch", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "nets", ")", ")", ":", "\n", "            ", "net", "=", "nets", "[", "i", "]", "\n", "path", "=", "model_paths", "[", "i", "]", "\n", "if", "isinstance", "(", "net", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "net", "=", "net", ".", "module", "\n", "", "print", "(", "'loading the model from %s'", "%", "path", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "self", ".", "device", ")", "\n", "if", "hasattr", "(", "state_dict", ",", "'_metadata'", ")", ":", "\n", "                ", "del", "state_dict", ".", "_metadata", "\n", "", "net", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_model.TDGANModel.set_input": [[113, 134], ["range", "TDGAN_model.TDGANModel.real_A.append", "TDGAN_model.TDGANModel.real_B.append", "TDGAN_model.TDGANModel.image_paths.append", "input[].to", "input[].to", "str", "str", "str"], "methods", ["None"], ["", "", "def", "set_input", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n\n        The option 'direction' can be used to swap images in domain A and domain B.\n        \"\"\"", "\n", "self", ".", "real_A", "=", "[", "]", "\n", "self", ".", "real_B", "=", "[", "]", "\n", "self", ".", "image_paths", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "opt", ".", "task_num", ")", ":", "\n", "            ", "self", ".", "real_A", ".", "append", "(", "input", "[", "'A_'", "+", "str", "(", "i", ")", "]", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "self", ".", "real_B", ".", "append", "(", "input", "[", "'B_'", "+", "str", "(", "i", ")", "]", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "self", ".", "image_paths", ".", "append", "(", "input", "[", "'A_paths_'", "+", "str", "(", "i", ")", "]", ")", "\n", "\n", "", "self", ".", "real_A_cur", "=", "self", ".", "real_A", "[", "self", ".", "opt", ".", "task_num", "-", "1", "]", "# real label for current task", "\n", "self", ".", "real_B_cur", "=", "self", ".", "real_B", "[", "self", ".", "opt", ".", "task_num", "-", "1", "]", "# real image for current task", "\n", "if", "self", ".", "opt", ".", "task_num", ">", "1", ":", "\n", "            ", "self", ".", "real_A_prev", "=", "self", ".", "real_A", "[", "0", "]", "# real label for the first task", "\n", "self", ".", "real_B_prev", "=", "self", ".", "real_B", "[", "0", "]", "# real image for the first task", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_model.TDGANModel.forward": [[135, 150], ["TDGAN_model.TDGANModel.netG", "range", "TDGAN_model.TDGANModel.fake_B_curG.append", "TDGAN_model.TDGANModel.fake_B_prevG.append", "TDGAN_model.TDGANModel.netG", "TDGAN_model.TDGANModel.netG_prev"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ")", ":", "\n", "        ", "\"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"", "\n", "self", ".", "fake_B", "=", "self", ".", "netG", "(", "self", ".", "real_A", "[", "self", ".", "opt", ".", "task_num", "-", "1", "]", ")", "# for current task", "\n", "\n", "# for previous tasks", "\n", "self", ".", "fake_B_curG", "=", "[", "]", "\n", "self", ".", "fake_B_prevG", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "opt", ".", "task_num", "-", "1", ")", ":", "# fake images from labels of previous tasks using both previous G and current G", "\n", "            ", "self", ".", "fake_B_curG", ".", "append", "(", "self", ".", "netG", "(", "self", ".", "real_A", "[", "i", "]", ")", ")", "\n", "self", ".", "fake_B_prevG", ".", "append", "(", "self", ".", "netG_prev", "(", "self", ".", "real_A", "[", "i", "]", ")", ")", "\n", "\n", "", "self", ".", "fake_B_cur", "=", "self", ".", "fake_B", "\n", "if", "self", ".", "opt", ".", "task_num", ">", "1", ":", "\n", "            ", "self", ".", "fake_B_cur_prev", "=", "self", ".", "fake_B_curG", "[", "0", "]", "# fake image from label of the first task using current G", "\n", "self", ".", "fake_B_prev", "=", "self", ".", "fake_B_prevG", "[", "0", "]", "# fake image from label of the first task using previous G", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_model.TDGANModel.backward_D": [[151, 164], ["torch.cat", "TDGAN_model.TDGANModel.netD", "TDGAN_model.TDGANModel.criterionGAN", "torch.cat", "TDGAN_model.TDGANModel.netD", "TDGAN_model.TDGANModel.criterionGAN", "TDGAN_model.TDGANModel.loss_D.backward", "torch.cat.detach"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.backward"], ["", "", "def", "backward_D", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculate GAN loss for the discriminator\"\"\"", "\n", "# Fake; stop backprop to the generator by detaching fake_B", "\n", "fake_AB", "=", "torch", ".", "cat", "(", "(", "self", ".", "real_A", "[", "self", ".", "opt", ".", "task_num", "-", "1", "]", ",", "self", ".", "fake_B", ")", ",", "1", ")", "# we use conditional GANs; we need to feed both input and output to the discriminator", "\n", "pred_fake", "=", "self", ".", "netD", "(", "fake_AB", ".", "detach", "(", ")", ")", "\n", "self", ".", "loss_D_fake", "=", "self", ".", "criterionGAN", "(", "pred_fake", ",", "False", ")", "\n", "# Real", "\n", "real_AB", "=", "torch", ".", "cat", "(", "(", "self", ".", "real_A", "[", "self", ".", "opt", ".", "task_num", "-", "1", "]", ",", "self", ".", "real_B", "[", "self", ".", "opt", ".", "task_num", "-", "1", "]", ")", ",", "1", ")", "\n", "pred_real", "=", "self", ".", "netD", "(", "real_AB", ")", "\n", "self", ".", "loss_D_real", "=", "self", ".", "criterionGAN", "(", "pred_real", ",", "True", ")", "\n", "# combine loss and calculate gradients", "\n", "self", ".", "loss_D", "=", "(", "self", ".", "loss_D_fake", "+", "self", ".", "loss_D_real", ")", "*", "self", ".", "opt", ".", "lambda_D", "\n", "self", ".", "loss_D", ".", "backward", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_model.TDGANModel.backward_G": [[165, 202], ["torch.cat", "TDGAN_model.TDGANModel.netD", "TDGAN_model.TDGANModel.criterionGAN", "TDGAN_model.TDGANModel.criterionL1", "TDGAN_model.TDGANModel.vgg_model", "TDGAN_model.TDGANModel.vgg_model", "TDGAN_model.TDGANModel.criterion_perceptual", "range", "range", "TDGAN_model.TDGANModel.loss_G.backward", "TDGAN_model.TDGANModel.loss_reminding_L1.append", "TDGAN_model.TDGANModel.vgg_model", "TDGAN_model.TDGANModel.vgg_model", "TDGAN_model.TDGANModel.loss_reminding_perceptual.append", "len", "TDGAN_model.TDGANModel.criterionL1", "TDGAN_model.TDGANModel.criterion_perceptual"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.backward"], ["", "def", "backward_G", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculate GAN and L1 loss for the generator\"\"\"", "\n", "# ----- previous Ds are not available ----- #", "\n", "# for current task", "\n", "fake_AB", "=", "torch", ".", "cat", "(", "(", "self", ".", "real_A", "[", "self", ".", "opt", ".", "task_num", "-", "1", "]", ",", "self", ".", "fake_B", ")", ",", "1", ")", "\n", "pred_fake", "=", "self", ".", "netD", "(", "fake_AB", ")", "\n", "self", ".", "loss_G_GAN", "=", "self", ".", "criterionGAN", "(", "pred_fake", ",", "True", ")", "\n", "self", ".", "loss_G_L1", "=", "self", ".", "criterionL1", "(", "self", ".", "fake_B", ",", "self", ".", "real_B", "[", "self", ".", "opt", ".", "task_num", "-", "1", "]", ")", "\n", "pred_feat", "=", "self", ".", "vgg_model", "(", "self", ".", "fake_B", ")", "\n", "target_feat", "=", "self", ".", "vgg_model", "(", "self", ".", "real_B", "[", "self", ".", "opt", ".", "task_num", "-", "1", "]", ")", "\n", "self", ".", "loss_G_perceptual", "=", "self", ".", "criterion_perceptual", "(", "pred_feat", ",", "target_feat", ")", "\n", "\n", "# for previous tasks", "\n", "self", ".", "loss_reminding_L1", "=", "[", "]", "\n", "self", ".", "loss_reminding_perceptual", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "opt", ".", "task_num", "-", "1", ")", ":", "\n", "            ", "self", ".", "loss_reminding_L1", ".", "append", "(", "self", ".", "criterionL1", "(", "self", ".", "fake_B_curG", "[", "i", "]", ",", "self", ".", "fake_B_prevG", "[", "i", "]", ")", ")", "\n", "pred_feat_G", "=", "self", ".", "vgg_model", "(", "self", ".", "fake_B_curG", "[", "i", "]", ")", "\n", "pred_feat_G_prev", "=", "self", ".", "vgg_model", "(", "self", ".", "fake_B_prevG", "[", "i", "]", ")", "\n", "self", ".", "loss_reminding_perceptual", ".", "append", "(", "self", ".", "criterion_perceptual", "(", "pred_feat_G", ",", "pred_feat_G_prev", ")", ")", "\n", "\n", "", "self", ".", "loss_reminding_L1_all", "=", "None", "\n", "self", ".", "loss_reminding_perceptual_all", "=", "None", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "loss_reminding_L1", ")", ")", ":", "\n", "            ", "if", "self", ".", "loss_reminding_L1_all", "is", "None", ":", "\n", "                ", "self", ".", "loss_reminding_L1_all", "=", "self", ".", "loss_reminding_L1", "[", "i", "]", "\n", "self", ".", "loss_reminding_perceptual_all", "=", "self", ".", "loss_reminding_perceptual", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "loss_reminding_L1_all", "+=", "self", ".", "loss_reminding_L1", "[", "i", "]", "\n", "self", ".", "loss_reminding_perceptual_all", "+=", "self", ".", "loss_reminding_perceptual", "[", "i", "]", "\n", "\n", "", "", "if", "self", ".", "opt", ".", "task_num", "==", "1", ":", "\n", "            ", "self", ".", "loss_G", "=", "(", "self", ".", "loss_G_GAN", "+", "self", ".", "loss_G_L1", "*", "self", ".", "opt", ".", "lambda_digesting_L1", "+", "self", ".", "loss_G_perceptual", "*", "self", ".", "opt", ".", "lambda_digesting_perceptual", ")", "*", "self", ".", "opt", ".", "lambda_G", "\n", "", "else", ":", "# digesting loss = loss_G_GAN + loss_G_L1 + loss_G_perceptual,  reminding loss = loss_reminding_L1 + loss_reminding_perceptual", "\n", "            ", "self", ".", "loss_G", "=", "(", "self", ".", "loss_G_GAN", "+", "self", ".", "loss_G_L1", "*", "self", ".", "opt", ".", "lambda_digesting_L1", "+", "self", ".", "loss_G_perceptual", "*", "self", ".", "opt", ".", "lambda_digesting_perceptual", "\n", "+", "self", ".", "loss_reminding_L1_all", "*", "self", ".", "opt", ".", "lambda_reminding_L1", "+", "self", ".", "loss_reminding_perceptual_all", "*", "self", ".", "opt", ".", "lambda_reminding_perceptual", ")", "*", "self", ".", "opt", ".", "lambda_G", "\n", "", "self", ".", "loss_G", ".", "backward", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_model.TDGANModel.optimize_parameters": [[203, 217], ["TDGAN_model.TDGANModel.forward", "TDGAN_model.TDGANModel.set_requires_grad", "TDGAN_model.TDGANModel.optimizer_D.zero_grad", "TDGAN_model.TDGANModel.backward_D", "TDGAN_model.TDGANModel.optimizer_D.step", "TDGAN_model.TDGANModel.set_requires_grad", "TDGAN_model.TDGANModel.optimizer_G.zero_grad", "TDGAN_model.TDGANModel.backward_G", "TDGAN_model.TDGANModel.optimizer_G.step"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.forward", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.set_requires_grad", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.backward_D", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.set_requires_grad", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.backward_G"], ["", "def", "optimize_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "forward", "(", ")", "# compute fake images: G(A)", "\n", "\n", "# update D", "\n", "self", ".", "set_requires_grad", "(", "self", ".", "netD", ",", "True", ")", "\n", "self", ".", "optimizer_D", ".", "zero_grad", "(", ")", "\n", "self", ".", "backward_D", "(", ")", "\n", "self", ".", "optimizer_D", ".", "step", "(", ")", "\n", "\n", "# update G", "\n", "self", ".", "set_requires_grad", "(", "self", ".", "netD", ",", "False", ")", "# D requires no gradients when optimizing G", "\n", "self", ".", "optimizer_G", ".", "zero_grad", "(", ")", "# set G's gradients to zero", "\n", "self", ".", "backward_G", "(", ")", "# calculate graidents for G", "\n", "self", ".", "optimizer_G", ".", "step", "(", ")", "# udpate G's weights", "", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_multiD_model.TDGANMultiDModel.modify_commandline_options": [[22, 52], ["parser.set_defaults", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "modify_commandline_options", "(", "parser", ",", "is_train", "=", "True", ")", ":", "\n", "        ", "\"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n\n        For pix2pix, we do not use image buffer\n        \"\"\"", "\n", "# changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)", "\n", "# parser.set_defaults(norm='batch', netG='unet_256', dataset_mode='aligned')", "\n", "if", "is_train", ":", "\n", "            ", "parser", ".", "set_defaults", "(", "pool_size", "=", "0", ",", "gan_mode", "=", "'vanilla'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_digesting_L1'", ",", "type", "=", "float", ",", "default", "=", "100.0", ",", "help", "=", "'weight for digesting L1 loss'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_digesting_perceptual'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'weight for digesting perceptual loss'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--prev_model_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'the path of trained model from previous tasks'", ")", "\n", "parser", ".", "add_argument", "(", "'--prev_model_epoch'", ",", "type", "=", "int", ",", "default", "=", "400", ",", "help", "=", "'the epoch of trained model from previous tasks'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_reminding_L1'", ",", "type", "=", "float", ",", "default", "=", "10.0", ",", "help", "=", "'weight for reminding L1 loss'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_reminding_perceptual'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'weight for reminding perceptual loss'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_lifelong_training'", ",", "action", "=", "'store_true'", ",", "help", "=", "'flag to train on current task without computing the remiding loss of previous tasks,'", "\n", "'in sequential finetuning for example'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_G'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'weight for dadgan G '", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_D'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'weight for dadgan D'", ")", "\n", "\n", "", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_multiD_model.TDGANMultiDModel.__init__": [[53, 103], ["base_model.BaseModel.__init__", "networks.define_G", "networks.define_G", "range", "TDGAN_multiD_model.TDGANMultiDModel.load_prev_model", "networks.GANLoss().to", "torch.nn.L1Loss", "torch.optim.Adam", "TDGAN_multiD_model.TDGANMultiDModel.optimizers.append", "models.perception_loss.vgg16_feat().cuda", "models.perception_loss.perceptual_loss", "TDGAN_multiD_model.TDGANMultiDModel.netD.append", "TDGAN_multiD_model.TDGANMultiDModel.netG.parameters", "torch.optim.Adam", "TDGAN_multiD_model.TDGANMultiDModel.optimizer_D.append", "TDGAN_multiD_model.TDGANMultiDModel.optimizers.append", "networks.define_D", "networks.GANLoss", "i.parameters", "models.perception_loss.vgg16_feat"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_G", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_G", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.load_prev_model", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_D"], ["", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize the pix2pix class.\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"", "\n", "BaseModel", ".", "__init__", "(", "self", ",", "opt", ")", "\n", "if", "self", ".", "opt", ".", "task_num", "==", "1", "or", "self", ".", "opt", ".", "no_lifelong_training", ":", "\n", "            ", "self", ".", "loss_names", "=", "[", "'G_GAN_all'", ",", "'G_L1_all'", ",", "'G_perceptual_all'", ",", "'D_real_all'", ",", "'D_fake_all'", "]", "\n", "self", ".", "visual_names", "=", "[", "'real_A_cur1'", ",", "'fake_B_cur1'", ",", "'real_B_cur1'", ",", "'real_A_cur2'", ",", "'fake_B_cur2'", ",", "'real_B_cur2'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_names", "=", "[", "'G_GAN_all'", ",", "'G_L1_all'", ",", "'G_perceptual_all'", ",", "'D_real_all'", ",", "'D_fake_all'", ",", "'reminding_L1_all'", ",", "'reminding_perceptual_all'", "]", "\n", "self", ".", "visual_names", "=", "[", "'real_A_prev1'", ",", "'real_B_prev1'", ",", "'fake_B_cur_prev1'", ",", "'fake_B_prev1'", ",", "'real_A_cur1'", ",", "'fake_B_cur1'", ",", "'real_B_cur1'", ",", "\n", "'real_A_prev2'", ",", "'real_B_prev2'", ",", "'fake_B_cur_prev2'", ",", "'fake_B_prev2'", ",", "'real_A_cur2'", ",", "'fake_B_cur2'", ",", "'real_B_cur2'", "]", "\n", "# specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>", "\n", "", "if", "self", ".", "isTrain", ":", "\n", "            ", "self", ".", "model_names", "=", "[", "'G'", ",", "'D'", "]", "\n", "", "else", ":", "# during test time, only load G", "\n", "            ", "self", ".", "model_names", "=", "[", "'G'", "]", "\n", "# define networks (both generator and discriminator)", "\n", "", "self", ".", "netG", "=", "networks", ".", "define_G", "(", "opt", ".", "input_nc", ",", "opt", ".", "output_nc", ",", "opt", ".", "ngf", ",", "opt", ".", "netG", ",", "opt", ".", "norm", ",", "\n", "not", "opt", ".", "no_dropout", ",", "opt", ".", "init_type", ",", "opt", ".", "init_gain", ",", "self", ".", "gpu_ids", ")", "\n", "\n", "self", ".", "netG_prev", "=", "networks", ".", "define_G", "(", "opt", ".", "input_nc", ",", "opt", ".", "output_nc", ",", "opt", ".", "ngf", ",", "opt", ".", "netG", ",", "opt", ".", "norm", ",", "\n", "not", "opt", ".", "no_dropout", ",", "opt", ".", "init_type", ",", "opt", ".", "init_gain", ",", "self", ".", "gpu_ids", ")", "\n", "\n", "if", "self", ".", "isTrain", ":", "# define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc", "\n", "            ", "self", ".", "netD", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "                ", "self", ".", "netD", ".", "append", "(", "networks", ".", "define_D", "(", "opt", ".", "input_nc", "+", "opt", ".", "output_nc", ",", "opt", ".", "ndf", ",", "opt", ".", "netD", ",", "\n", "opt", ".", "n_layers_D", ",", "opt", ".", "norm", ",", "opt", ".", "init_type", ",", "opt", ".", "init_gain", ",", "self", ".", "gpu_ids", ")", ")", "\n", "\n", "", "", "if", "self", ".", "isTrain", "and", "self", ".", "opt", ".", "prev_model_path", ":", "\n", "            ", "self", ".", "load_prev_model", "(", ")", "\n", "\n", "", "if", "self", ".", "isTrain", ":", "\n", "# define loss functions", "\n", "            ", "self", ".", "criterionGAN", "=", "networks", ".", "GANLoss", "(", "opt", ".", "gan_mode", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "criterionL1", "=", "torch", ".", "nn", ".", "L1Loss", "(", ")", "\n", "# initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.", "\n", "self", ".", "optimizer_G", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "netG", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ",", "betas", "=", "(", "opt", ".", "beta1", ",", "0.999", ")", ")", "\n", "self", ".", "optimizer_D", "=", "[", "]", "\n", "for", "i", "in", "self", ".", "netD", ":", "\n", "                ", "opt_D", "=", "torch", ".", "optim", ".", "Adam", "(", "i", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ",", "betas", "=", "(", "opt", ".", "beta1", ",", "0.999", ")", ")", "\n", "self", ".", "optimizer_D", ".", "append", "(", "opt_D", ")", "\n", "self", ".", "optimizers", ".", "append", "(", "opt_D", ")", "\n", "", "self", ".", "optimizers", ".", "append", "(", "self", ".", "optimizer_G", ")", "\n", "\n", "self", ".", "vgg_model", "=", "vgg16_feat", "(", ")", ".", "cuda", "(", ")", "\n", "self", ".", "criterion_perceptual", "=", "perceptual_loss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_multiD_model.TDGANMultiDModel.load_prev_model": [[104, 120], ["range", "len", "isinstance", "print", "torch.load", "hasattr", "net.load_state_dict"], "methods", ["None"], ["", "", "def", "load_prev_model", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load trained Generator using previous tasks\n        \"\"\"", "\n", "nets", "=", "[", "self", ".", "netG_prev", ",", "self", ".", "netG", "]", "\n", "model_paths", "=", "[", "'{:s}/{:d}_net_G.pth'", ".", "format", "(", "self", ".", "opt", ".", "prev_model_path", ",", "self", ".", "opt", ".", "prev_model_epoch", ")", ",", "\n", "'{:s}/{:d}_net_G.pth'", ".", "format", "(", "self", ".", "opt", ".", "prev_model_path", ",", "self", ".", "opt", ".", "prev_model_epoch", ")", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "nets", ")", ")", ":", "\n", "            ", "net", "=", "nets", "[", "i", "]", "\n", "path", "=", "model_paths", "[", "i", "]", "\n", "if", "isinstance", "(", "net", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "net", "=", "net", ".", "module", "\n", "", "print", "(", "'loading the model from %s'", "%", "path", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "self", ".", "device", ")", "\n", "if", "hasattr", "(", "state_dict", ",", "'_metadata'", ")", ":", "\n", "                ", "del", "state_dict", ".", "_metadata", "\n", "", "net", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_multiD_model.TDGANMultiDModel.toggle_dropout": [[121, 129], ["isinstance", "net.modules", "isinstance"], "methods", ["None"], ["", "", "def", "toggle_dropout", "(", "self", ",", "activate", "=", "True", ")", ":", "\n", "        ", "nets", "=", "[", "self", ".", "netG_prev", ",", "self", ".", "netG", "]", "\n", "for", "net", "in", "nets", ":", "\n", "            ", "if", "isinstance", "(", "net", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "net", "=", "net", ".", "module", "\n", "", "for", "module", "in", "net", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "module", ",", "nn", ".", "Dropout", ")", ":", "\n", "                    ", "module", ".", "training", "=", "activate", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_multiD_model.TDGANMultiDModel.set_input": [[130, 156], ["range", "TDGAN_multiD_model.TDGANMultiDModel.real_A.append", "TDGAN_multiD_model.TDGANMultiDModel.real_B.append", "TDGAN_multiD_model.TDGANMultiDModel.image_paths.append", "input[].to", "input[].to", "str", "str", "str"], "methods", ["None"], ["", "", "", "", "def", "set_input", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n\n        The option 'direction' can be used to swap images in domain A and domain B.\n        \"\"\"", "\n", "# AtoB = self.opt.direction == 'AtoB'", "\n", "self", ".", "real_A", "=", "[", "]", "\n", "self", ".", "real_B", "=", "[", "]", "\n", "self", ".", "image_paths", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "opt", ".", "task_num", "*", "2", ")", ":", "\n", "            ", "self", ".", "real_A", ".", "append", "(", "input", "[", "'A_'", "+", "str", "(", "i", ")", "]", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "self", ".", "real_B", ".", "append", "(", "input", "[", "'B_'", "+", "str", "(", "i", ")", "]", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "self", ".", "image_paths", ".", "append", "(", "input", "[", "'A_paths_'", "+", "str", "(", "i", ")", "]", ")", "\n", "\n", "", "self", ".", "real_A_cur1", "=", "self", ".", "real_A", "[", "2", "*", "self", ".", "opt", ".", "task_num", "-", "2", "]", "\n", "self", ".", "real_A_cur2", "=", "self", ".", "real_A", "[", "2", "*", "self", ".", "opt", ".", "task_num", "-", "1", "]", "\n", "self", ".", "real_B_cur1", "=", "self", ".", "real_B", "[", "2", "*", "self", ".", "opt", ".", "task_num", "-", "2", "]", "\n", "self", ".", "real_B_cur2", "=", "self", ".", "real_B", "[", "2", "*", "self", ".", "opt", ".", "task_num", "-", "1", "]", "\n", "if", "self", ".", "opt", ".", "task_num", ">", "1", "and", "(", "not", "self", ".", "opt", ".", "no_lifelong_training", ")", ":", "\n", "            ", "self", ".", "real_A_prev1", "=", "self", ".", "real_A", "[", "0", "]", "\n", "self", ".", "real_A_prev2", "=", "self", ".", "real_A", "[", "1", "]", "\n", "self", ".", "real_B_prev1", "=", "self", ".", "real_B", "[", "0", "]", "\n", "self", ".", "real_B_prev2", "=", "self", ".", "real_B", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_multiD_model.TDGANMultiDModel.forward": [[157, 179], ["range", "TDGAN_multiD_model.TDGANMultiDModel.fake_B.append", "range", "TDGAN_multiD_model.TDGANMultiDModel.netG", "TDGAN_multiD_model.TDGANMultiDModel.fake_B_curG.append", "TDGAN_multiD_model.TDGANMultiDModel.fake_B_prevG.append", "TDGAN_multiD_model.TDGANMultiDModel.netG", "TDGAN_multiD_model.TDGANMultiDModel.netG_prev"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ")", ":", "\n", "        ", "\"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"", "\n", "# self.toggle_dropout(True)", "\n", "self", ".", "fake_B", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "2", "*", "self", ".", "opt", ".", "task_num", "-", "2", ",", "2", "*", "self", ".", "opt", ".", "task_num", ")", ":", "\n", "            ", "self", ".", "fake_B", ".", "append", "(", "self", ".", "netG", "(", "self", ".", "real_A", "[", "i", "]", ")", ")", "# for current task", "\n", "", "self", ".", "fake_B_cur1", "=", "self", ".", "fake_B", "[", "0", "]", "\n", "self", ".", "fake_B_cur2", "=", "self", ".", "fake_B", "[", "1", "]", "\n", "\n", "# for previous task", "\n", "if", "self", ".", "opt", ".", "task_num", ">", "1", "and", "(", "not", "self", ".", "opt", ".", "no_lifelong_training", ")", ":", "\n", "# self.toggle_dropout(False)", "\n", "            ", "self", ".", "fake_B_curG", "=", "[", "]", "\n", "self", ".", "fake_B_prevG", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "2", "*", "self", ".", "opt", ".", "task_num", "-", "2", ")", ":", "\n", "                ", "self", ".", "fake_B_curG", ".", "append", "(", "self", ".", "netG", "(", "self", ".", "real_A", "[", "i", "]", ")", ")", "\n", "self", ".", "fake_B_prevG", ".", "append", "(", "self", ".", "netG_prev", "(", "self", ".", "real_A", "[", "i", "]", ")", ")", "\n", "\n", "", "self", ".", "fake_B_cur_prev1", "=", "self", ".", "fake_B_curG", "[", "0", "]", "\n", "self", ".", "fake_B_cur_prev2", "=", "self", ".", "fake_B_curG", "[", "1", "]", "\n", "self", ".", "fake_B_prev1", "=", "self", ".", "fake_B_prevG", "[", "0", "]", "\n", "self", ".", "fake_B_prev2", "=", "self", ".", "fake_B_prevG", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_multiD_model.TDGANMultiDModel.backward_D": [[180, 208], ["range", "range", "TDGAN_multiD_model.TDGANMultiDModel.loss_D.backward", "torch.cat", "TDGAN_multiD_model.TDGANMultiDModel.loss_D_fake.append", "torch.cat", "TDGAN_multiD_model.TDGANMultiDModel.loss_D_real.append", "len", "torch.cat.detach", "TDGAN_multiD_model.TDGANMultiDModel.criterionGAN", "TDGAN_multiD_model.TDGANMultiDModel.criterionGAN"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.backward"], ["", "", "def", "backward_D", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculate GAN loss for the discriminator\"\"\"", "\n", "\n", "self", ".", "loss_D_fake", "=", "[", "]", "\n", "self", ".", "loss_D_real", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "2", "*", "self", ".", "opt", ".", "task_num", "-", "2", ",", "2", "*", "self", ".", "opt", ".", "task_num", ")", ":", "\n", "# Fake; stop backprop to the generator by detaching fake_B", "\n", "            ", "fake_AB", "=", "torch", ".", "cat", "(", "(", "self", ".", "real_A", "[", "i", "]", ",", "self", ".", "fake_B", "[", "i", "%", "2", "]", ")", ",", "1", ")", "# we use conditional GANs; we need to feed both input and output to the discriminator", "\n", "pred_fake", "=", "self", ".", "netD", "[", "i", "%", "2", "]", "(", "fake_AB", ".", "detach", "(", ")", ")", "\n", "self", ".", "loss_D_fake", ".", "append", "(", "self", ".", "criterionGAN", "(", "pred_fake", ",", "False", ")", ")", "\n", "# Real", "\n", "real_AB", "=", "torch", ".", "cat", "(", "(", "self", ".", "real_A", "[", "i", "]", ",", "self", ".", "real_B", "[", "i", "]", ")", ",", "1", ")", "\n", "pred_real", "=", "self", ".", "netD", "[", "i", "%", "2", "]", "(", "real_AB", ")", "\n", "self", ".", "loss_D_real", ".", "append", "(", "self", ".", "criterionGAN", "(", "pred_real", ",", "True", ")", ")", "\n", "\n", "", "self", ".", "loss_D_fake_all", "=", "None", "\n", "self", ".", "loss_D_real_all", "=", "None", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "loss_D_fake", ")", ")", ":", "\n", "            ", "if", "self", ".", "loss_D_fake_all", "is", "None", ":", "\n", "                ", "self", ".", "loss_D_fake_all", "=", "self", ".", "loss_D_fake", "[", "i", "]", "\n", "self", ".", "loss_D_real_all", "=", "self", ".", "loss_D_real", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "loss_D_fake_all", "+=", "self", ".", "loss_D_fake", "[", "i", "]", "\n", "self", ".", "loss_D_real_all", "+=", "self", ".", "loss_D_real", "[", "i", "]", "\n", "\n", "# combine loss and calculate gradients", "\n", "", "", "self", ".", "loss_D", "=", "(", "self", ".", "loss_D_fake_all", "+", "self", ".", "loss_D_real_all", ")", "*", "self", ".", "opt", ".", "lambda_D", "#0.05", "\n", "self", ".", "loss_D", ".", "backward", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_multiD_model.TDGANMultiDModel.backward_G": [[209, 264], ["range", "range", "TDGAN_multiD_model.TDGANMultiDModel.loss_G.backward", "torch.cat", "TDGAN_multiD_model.TDGANMultiDModel.loss_G_GAN.append", "TDGAN_multiD_model.TDGANMultiDModel.loss_G_L1.append", "TDGAN_multiD_model.TDGANMultiDModel.vgg_model", "TDGAN_multiD_model.TDGANMultiDModel.vgg_model", "TDGAN_multiD_model.TDGANMultiDModel.loss_G_perceptual.append", "len", "range", "range", "TDGAN_multiD_model.TDGANMultiDModel.criterionGAN", "TDGAN_multiD_model.TDGANMultiDModel.criterionL1", "TDGAN_multiD_model.TDGANMultiDModel.criterion_perceptual", "TDGAN_multiD_model.TDGANMultiDModel.loss_reminding_L1.append", "TDGAN_multiD_model.TDGANMultiDModel.vgg_model", "TDGAN_multiD_model.TDGANMultiDModel.vgg_model", "TDGAN_multiD_model.TDGANMultiDModel.loss_reminding_perceptual.append", "len", "TDGAN_multiD_model.TDGANMultiDModel.criterionL1", "TDGAN_multiD_model.TDGANMultiDModel.criterion_perceptual"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.backward"], ["", "def", "backward_G", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculate GAN and L1 loss for the generator\"\"\"", "\n", "# ----- previous Ds are not available ----- #", "\n", "# for current task", "\n", "self", ".", "loss_G_GAN", "=", "[", "]", "\n", "self", ".", "loss_G_L1", "=", "[", "]", "\n", "self", ".", "loss_G_perceptual", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "2", "*", "self", ".", "opt", ".", "task_num", "-", "2", ",", "2", "*", "self", ".", "opt", ".", "task_num", ")", ":", "\n", "            ", "fake_AB", "=", "torch", ".", "cat", "(", "(", "self", ".", "real_A", "[", "i", "]", ",", "self", ".", "fake_B", "[", "i", "%", "2", "]", ")", ",", "1", ")", "\n", "pred_fake", "=", "self", ".", "netD", "[", "i", "%", "2", "]", "(", "fake_AB", ")", "\n", "self", ".", "loss_G_GAN", ".", "append", "(", "self", ".", "criterionGAN", "(", "pred_fake", ",", "True", ")", ")", "\n", "self", ".", "loss_G_L1", ".", "append", "(", "self", ".", "criterionL1", "(", "self", ".", "fake_B", "[", "i", "%", "2", "]", ",", "self", ".", "real_B", "[", "i", "]", ")", ")", "\n", "pred_feat", "=", "self", ".", "vgg_model", "(", "self", ".", "fake_B", "[", "i", "%", "2", "]", ")", "\n", "target_feat", "=", "self", ".", "vgg_model", "(", "self", ".", "real_B", "[", "i", "]", ")", "\n", "self", ".", "loss_G_perceptual", ".", "append", "(", "self", ".", "criterion_perceptual", "(", "pred_feat", ",", "target_feat", ")", ")", "\n", "\n", "", "self", ".", "loss_G_GAN_all", "=", "None", "\n", "self", ".", "loss_G_L1_all", "=", "None", "\n", "self", ".", "loss_G_perceptual_all", "=", "None", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "loss_G_GAN", ")", ")", ":", "\n", "            ", "if", "self", ".", "loss_G_GAN_all", "is", "None", ":", "\n", "                ", "self", ".", "loss_G_GAN_all", "=", "self", ".", "loss_G_GAN", "[", "i", "]", "\n", "self", ".", "loss_G_L1_all", "=", "self", ".", "loss_G_L1", "[", "i", "]", "\n", "self", ".", "loss_G_perceptual_all", "=", "self", ".", "loss_G_perceptual", "[", "i", "]", "\n", "", "else", ":", "\n", "                ", "self", ".", "loss_G_GAN_all", "+=", "self", ".", "loss_G_GAN", "[", "i", "]", "\n", "self", ".", "loss_G_L1_all", "+=", "self", ".", "loss_G_L1", "[", "i", "]", "\n", "self", ".", "loss_G_perceptual_all", "+=", "self", ".", "loss_G_perceptual", "[", "i", "]", "\n", "\n", "# for previous tasks", "\n", "", "", "if", "self", ".", "opt", ".", "task_num", ">", "1", "and", "(", "not", "self", ".", "opt", ".", "no_lifelong_training", ")", ":", "\n", "            ", "self", ".", "loss_reminding_L1", "=", "[", "]", "\n", "self", ".", "loss_reminding_perceptual", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "0", ",", "2", "*", "self", ".", "opt", ".", "task_num", "-", "2", ")", ":", "\n", "                ", "self", ".", "loss_reminding_L1", ".", "append", "(", "self", ".", "criterionL1", "(", "self", ".", "fake_B_curG", "[", "i", "]", ",", "self", ".", "fake_B_prevG", "[", "i", "]", ")", ")", "\n", "pred_feat_G", "=", "self", ".", "vgg_model", "(", "self", ".", "fake_B_curG", "[", "i", "]", ")", "\n", "pred_feat_G_prev", "=", "self", ".", "vgg_model", "(", "self", ".", "fake_B_prevG", "[", "i", "]", ")", "\n", "self", ".", "loss_reminding_perceptual", ".", "append", "(", "self", ".", "criterion_perceptual", "(", "pred_feat_G", ",", "pred_feat_G_prev", ")", ")", "\n", "\n", "", "self", ".", "loss_reminding_L1_all", "=", "None", "\n", "self", ".", "loss_reminding_perceptual_all", "=", "None", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "loss_reminding_L1", ")", ")", ":", "\n", "                ", "if", "self", ".", "loss_reminding_L1_all", "is", "None", ":", "\n", "                    ", "self", ".", "loss_reminding_L1_all", "=", "self", ".", "loss_reminding_L1", "[", "i", "]", "\n", "self", ".", "loss_reminding_perceptual_all", "=", "self", ".", "loss_reminding_perceptual", "[", "i", "]", "\n", "", "else", ":", "\n", "                    ", "self", ".", "loss_reminding_L1_all", "+=", "self", ".", "loss_reminding_L1", "[", "i", "]", "\n", "self", ".", "loss_reminding_perceptual_all", "+=", "self", ".", "loss_reminding_perceptual", "[", "i", "]", "\n", "\n", "", "", "", "if", "self", ".", "opt", ".", "task_num", "==", "1", "or", "self", ".", "opt", ".", "no_lifelong_training", ":", "\n", "            ", "self", ".", "loss_G", "=", "(", "self", ".", "loss_G_GAN_all", "+", "self", ".", "loss_G_L1_all", "*", "self", ".", "opt", ".", "lambda_digesting_L1", "+", "self", ".", "loss_G_perceptual_all", "*", "self", ".", "opt", ".", "lambda_digesting_perceptual", ")", "*", "self", ".", "opt", ".", "lambda_G", "\n", "", "else", ":", "\n", "            ", "self", ".", "loss_G", "=", "(", "self", ".", "loss_G_GAN_all", "+", "self", ".", "loss_G_L1_all", "*", "self", ".", "opt", ".", "lambda_digesting_L1", "+", "self", ".", "loss_G_perceptual_all", "*", "self", ".", "opt", ".", "lambda_digesting_perceptual", "\n", "+", "self", ".", "loss_reminding_L1_all", "*", "self", ".", "opt", ".", "lambda_reminding_L1", "+", "self", ".", "loss_reminding_perceptual_all", "*", "self", ".", "opt", ".", "lambda_reminding_perceptual", ")", "*", "self", ".", "opt", ".", "lambda_G", "\n", "", "self", ".", "loss_G", ".", "backward", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.TDGAN_multiD_model.TDGANMultiDModel.optimize_parameters": [[265, 279], ["TDGAN_multiD_model.TDGANMultiDModel.forward", "TDGAN_multiD_model.TDGANMultiDModel.set_requires_grad", "TDGAN_multiD_model.TDGANMultiDModel.backward_D", "TDGAN_multiD_model.TDGANMultiDModel.set_requires_grad", "TDGAN_multiD_model.TDGANMultiDModel.optimizer_G.zero_grad", "TDGAN_multiD_model.TDGANMultiDModel.backward_G", "TDGAN_multiD_model.TDGANMultiDModel.optimizer_G.step", "opt.zero_grad", "opt.step"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.forward", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.set_requires_grad", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.backward_D", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.set_requires_grad", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.backward_G"], ["", "def", "optimize_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "forward", "(", ")", "# compute fake images: G(A)", "\n", "# update D", "\n", "self", ".", "set_requires_grad", "(", "self", ".", "netD", ",", "True", ")", "# enable backprop for D", "\n", "for", "opt", "in", "self", ".", "optimizer_D", ":", "\n", "            ", "opt", ".", "zero_grad", "(", ")", "\n", "", "self", ".", "backward_D", "(", ")", "\n", "for", "opt", "in", "self", ".", "optimizer_D", ":", "\n", "            ", "opt", ".", "step", "(", ")", "\n", "\n", "", "self", ".", "set_requires_grad", "(", "self", ".", "netD", ",", "False", ")", "# D requires no gradients when optimizing G", "\n", "self", ".", "optimizer_G", ".", "zero_grad", "(", ")", "# set G's gradients to zero", "\n", "self", ".", "backward_G", "(", ")", "# calculate graidents for G", "\n", "self", ".", "optimizer_G", ".", "step", "(", ")", "# udpate G's weights", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.__init__.find_model_using_name": [[25, 46], ["importlib.import_module", "importlib.import_module.__dict__.items", "model_name.replace", "print", "exit", "issubclass", "name.lower", "target_model_name.lower"], "function", ["None"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.__init__.get_option_setter": [[48, 52], ["__init__.find_model_using_name"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.__init__.find_model_using_name"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.__init__.create_model": [[54, 68], ["__init__.find_model_using_name", "find_model_using_name.", "print", "type"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.__init__.find_model_using_name"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.perception_loss.vgg16_feat.__init__": [[7, 15], ["torch.nn.Module.__init__", "torchvision.models.vgg16"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "vgg_layers", "=", "models", ".", "vgg16", "(", "pretrained", "=", "True", ")", ".", "features", "\n", "self", ".", "layer_name_mapping", "=", "{", "\n", "'3'", ":", "\"relu1_2\"", ",", "\n", "'8'", ":", "\"relu2_2\"", ",", "\n", "'15'", ":", "\"relu3_3\"", ",", "\n", "'22'", ":", "\"relu4_3\"", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.perception_loss.vgg16_feat.forward": [[17, 24], ["perception_loss.vgg16_feat.vgg_layers._modules.items", "module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "output", "=", "{", "}", "\n", "for", "name", ",", "module", "in", "self", ".", "vgg_layers", ".", "_modules", ".", "items", "(", ")", ":", "\n", "            ", "x", "=", "module", "(", "x", ")", "\n", "if", "name", "in", "self", ".", "layer_name_mapping", ":", "\n", "                ", "output", "[", "self", ".", "layer_name_mapping", "[", "name", "]", "]", "=", "x", "\n", "", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.perception_loss.perceptual_loss.__init__": [[27, 30], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "names", "=", "[", "'relu1_2'", ",", "'relu2_2'", ",", "'relu3_3'", ",", "'relu4_3'", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.perception_loss.perceptual_loss.forward": [[31, 45], ["x1_feat[].size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "x1_feat", "=", "args", "[", "0", "]", "\n", "x2_feat", "=", "args", "[", "1", "]", "\n", "\n", "loss", "=", "0", "\n", "for", "key", "in", "self", ".", "names", ":", "\n", "            ", "size", "=", "x1_feat", "[", "key", "]", ".", "size", "(", ")", "\n", "# L1 loss", "\n", "# loss += (x1_feat[key] - x2_feat[key]).abs().sum() / (size[0] * size[1] * size[2] * size[3])", "\n", "# MSE loss", "\n", "loss", "+=", "(", "(", "x1_feat", "[", "key", "]", "-", "x2_feat", "[", "key", "]", ")", "**", "2", ")", ".", "sum", "(", ")", "/", "(", "size", "[", "0", "]", "*", "size", "[", "1", "]", "*", "size", "[", "2", "]", "*", "size", "[", "3", "]", ")", "\n", "\n", "", "loss", "/=", "4", "\n", "return", "loss", "", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.__init__": [[18, 45], ["os.path.join", "torch.device", "torch.device"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize the BaseModel class.\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n\n        When creating your custom class, you need to implement your own initialization.\n        In this fucntion, you should first call <BaseModel.__init__(self, opt)>\n        Then, you need to define four lists:\n            -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n            -- self.model_names (str list):         specify the images that you want to display and save.\n            -- self.visual_names (str list):        define networks used in our training.\n            -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them.\n        \"\"\"", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "gpu_ids", "=", "opt", ".", "gpu_ids", "\n", "self", ".", "isTrain", "=", "opt", ".", "isTrain", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "'cuda:{}'", ".", "format", "(", "self", ".", "gpu_ids", "[", "0", "]", ")", ")", "if", "self", ".", "gpu_ids", "else", "torch", ".", "device", "(", "'cpu'", ")", "# get device name: CPU or GPU", "\n", "self", ".", "save_dir", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "checkpoints_dir", ",", "opt", ".", "name", ")", "# save all the checkpoints to save_dir", "\n", "if", "opt", ".", "preprocess", "!=", "'scale_width'", ":", "# with [scale_width], input images might have different sizes, which hurts the performance of cudnn.benchmark.", "\n", "            ", "torch", ".", "backends", ".", "cudnn", ".", "benchmark", "=", "True", "\n", "", "self", ".", "loss_names", "=", "[", "]", "\n", "self", ".", "model_names", "=", "[", "]", "\n", "self", ".", "visual_names", "=", "[", "]", "\n", "self", ".", "optimizers", "=", "[", "]", "\n", "self", ".", "image_paths", "=", "[", "]", "\n", "self", ".", "metric", "=", "0", "# used for learning rate policy 'plateau'", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.modify_commandline_options": [[46, 58], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "modify_commandline_options", "(", "parser", ",", "is_train", ")", ":", "\n", "        ", "\"\"\"Add new model-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        \"\"\"", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.set_input": [[59, 67], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "set_input", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input (dict): includes the data itself and its metadata information.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.forward": [[68, 72], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "forward", "(", "self", ")", ":", "\n", "        ", "\"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.optimize_parameters": [[73, 77], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "optimize_parameters", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.setup": [[78, 90], ["base_model.BaseModel.print_networks", "base_model.BaseModel.load_networks", "networks.get_scheduler"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.print_networks", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.load_networks", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.get_scheduler"], ["", "def", "setup", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Load and print networks; create schedulers\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"", "\n", "if", "self", ".", "isTrain", ":", "\n", "            ", "self", ".", "schedulers", "=", "[", "networks", ".", "get_scheduler", "(", "optimizer", ",", "opt", ")", "for", "optimizer", "in", "self", ".", "optimizers", "]", "\n", "", "if", "not", "self", ".", "isTrain", "or", "opt", ".", "continue_train", ":", "\n", "            ", "load_suffix", "=", "'iter_%d'", "%", "opt", ".", "load_iter", "if", "opt", ".", "load_iter", ">", "0", "else", "opt", ".", "epoch", "\n", "self", ".", "load_networks", "(", "load_suffix", ")", "\n", "", "self", ".", "print_networks", "(", "opt", ".", "verbose", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.eval": [[91, 97], ["isinstance", "getattr", "getattr.eval"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.eval"], ["", "def", "eval", "(", "self", ")", ":", "\n", "        ", "\"\"\"Make models eval mode during test time\"\"\"", "\n", "for", "name", "in", "self", ".", "model_names", ":", "\n", "            ", "if", "isinstance", "(", "name", ",", "str", ")", ":", "\n", "                ", "net", "=", "getattr", "(", "self", ",", "'net'", "+", "name", ")", "\n", "net", ".", "eval", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.test": [[98, 107], ["torch.no_grad", "base_model.BaseModel.forward", "base_model.BaseModel.compute_visuals"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.forward", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.compute_visuals"], ["", "", "", "def", "test", "(", "self", ")", ":", "\n", "        ", "\"\"\"Forward function used in test time.\n\n        This function wraps <forward> function in no_grad() so we don't save intermediate steps for backprop\n        It also calls <compute_visuals> to produce additional visualization results\n        \"\"\"", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "self", ".", "forward", "(", ")", "\n", "self", ".", "compute_visuals", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.compute_visuals": [[108, 111], ["None"], "methods", ["None"], ["", "", "def", "compute_visuals", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculate additional output images for visdom and HTML visualization\"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_image_paths": [[112, 115], ["None"], "methods", ["None"], ["", "def", "get_image_paths", "(", "self", ")", ":", "\n", "        ", "\"\"\" Return image paths that are used to load current data\"\"\"", "\n", "return", "self", ".", "image_paths", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.update_learning_rate": [[127, 137], ["range", "len", "scheduler.step", "scheduler.step"], "methods", ["None"], ["", "def", "update_learning_rate", "(", "self", ")", ":", "\n", "        ", "\"\"\"Update learning rates for all the networks; called at the end of every epoch\"\"\"", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "schedulers", ")", ")", ":", "\n", "            ", "scheduler", "=", "self", ".", "schedulers", "[", "i", "]", "\n", "if", "self", ".", "opt", ".", "lr_policy", "==", "'plateau'", ":", "\n", "                ", "scheduler", ".", "step", "(", "self", ".", "metric", ")", "\n", "", "else", ":", "\n", "                ", "scheduler", ".", "step", "(", ")", "\n", "\n", "", "lr", "=", "self", ".", "optimizers", "[", "i", "]", ".", "param_groups", "[", "0", "]", "[", "'lr'", "]", "\n", "# print('learning rate = %.7f' % lr)", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_current_visuals": [[139, 146], ["collections.OrderedDict", "isinstance", "getattr"], "methods", ["None"], ["", "", "def", "get_current_visuals", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return visualization images. train.py will display these images with visdom, and save the images to a HTML\"\"\"", "\n", "visual_ret", "=", "OrderedDict", "(", ")", "\n", "for", "name", "in", "self", ".", "visual_names", ":", "\n", "            ", "if", "isinstance", "(", "name", ",", "str", ")", ":", "\n", "                ", "visual_ret", "[", "name", "]", "=", "getattr", "(", "self", ",", "name", ")", "\n", "", "", "return", "visual_ret", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.get_current_losses": [[147, 154], ["collections.OrderedDict", "isinstance", "float", "getattr"], "methods", ["None"], ["", "def", "get_current_losses", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return traning losses / errors. train.py will print out these errors on console, and save them to a file\"\"\"", "\n", "errors_ret", "=", "OrderedDict", "(", ")", "\n", "for", "name", "in", "self", ".", "loss_names", ":", "\n", "            ", "if", "isinstance", "(", "name", ",", "str", ")", ":", "\n", "                ", "errors_ret", "[", "name", "]", "=", "float", "(", "getattr", "(", "self", ",", "'loss_'", "+", "name", ")", ")", "# float(...) works for both scalar tensor and float number", "\n", "", "", "return", "errors_ret", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.save_networks": [[155, 182], ["isinstance", "getattr", "getattr", "os.path.join", "range", "torch.cuda.is_available", "torch.save", "getattr.cuda", "torch.save", "len", "os.path.join", "len", "getattr.module.cpu().state_dict", "getattr.cpu().state_dict", "torch.cuda.is_available", "torch.save", "net[].cuda", "torch.save", "len", "net[].module.cpu().state_dict", "net[].cpu().state_dict", "getattr.module.cpu", "getattr.cpu", "net[].module.cpu", "net[].cpu"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.save", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.save", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.save", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.html.HTML.save"], ["", "def", "save_networks", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"Save all the networks to the disk.\n\n        Parameters:\n            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n        \"\"\"", "\n", "for", "name", "in", "self", ".", "model_names", ":", "\n", "            ", "if", "isinstance", "(", "name", ",", "str", ")", ":", "\n", "                ", "net", "=", "getattr", "(", "self", ",", "'net'", "+", "name", ")", "\n", "module", "=", "getattr", "(", "net", ",", "\"module\"", ",", "None", ")", "\n", "if", "module", "is", "not", "None", ":", "\n", "                    ", "save_filename", "=", "'%s_net_%s.pth'", "%", "(", "epoch", ",", "name", ")", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "save_filename", ")", "\n", "if", "len", "(", "self", ".", "gpu_ids", ")", ">", "0", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                        ", "torch", ".", "save", "(", "net", ".", "module", ".", "cpu", "(", ")", ".", "state_dict", "(", ")", ",", "save_path", ")", "\n", "net", ".", "cuda", "(", "self", ".", "gpu_ids", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                        ", "torch", ".", "save", "(", "net", ".", "cpu", "(", ")", ".", "state_dict", "(", ")", ",", "save_path", ")", "\n", "", "", "else", ":", "\n", "                    ", "for", "i", "in", "range", "(", "len", "(", "net", ")", ")", ":", "\n", "                        ", "save_filename", "=", "'%s_net_%s_%s.pth'", "%", "(", "epoch", ",", "name", ",", "i", ")", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "save_filename", ")", "\n", "if", "len", "(", "self", ".", "gpu_ids", ")", ">", "0", "and", "torch", ".", "cuda", ".", "is_available", "(", ")", ":", "\n", "                            ", "torch", ".", "save", "(", "net", "[", "i", "]", ".", "module", ".", "cpu", "(", ")", ".", "state_dict", "(", ")", ",", "save_path", ")", "\n", "net", "[", "i", "]", ".", "cuda", "(", "self", ".", "gpu_ids", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "                            ", "torch", ".", "save", "(", "net", "[", "i", "]", ".", "cpu", "(", ")", ".", "state_dict", "(", ")", ",", "save_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.__patch_instance_norm_state_dict": [[190, 203], ["len", "base_model.BaseModel.__patch_instance_norm_state_dict", "module.__class__.__name__.startswith", "module.__class__.__name__.startswith", "state_dict.pop", "getattr", "getattr", "state_dict.pop"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.__patch_instance_norm_state_dict"], ["", "", "", "", "", "", "def", "__patch_instance_norm_state_dict", "(", "self", ",", "state_dict", ",", "module", ",", "keys", ",", "i", "=", "0", ")", ":", "\n", "        ", "\"\"\"Fix InstanceNorm checkpoints incompatibility (prior to 0.4)\"\"\"", "\n", "key", "=", "keys", "[", "i", "]", "\n", "if", "i", "+", "1", "==", "len", "(", "keys", ")", ":", "# at the end, pointing to a parameter/buffer", "\n", "            ", "if", "module", ".", "__class__", ".", "__name__", ".", "startswith", "(", "'InstanceNorm'", ")", "and", "(", "key", "==", "'running_mean'", "or", "key", "==", "'running_var'", ")", ":", "\n", "                ", "if", "getattr", "(", "module", ",", "key", ")", "is", "None", ":", "\n", "                    ", "state_dict", ".", "pop", "(", "'.'", ".", "join", "(", "keys", ")", ")", "\n", "", "", "if", "module", ".", "__class__", ".", "__name__", ".", "startswith", "(", "'InstanceNorm'", ")", "and", "(", "key", "==", "'num_batches_tracked'", ")", ":", "\n", "                ", "state_dict", ".", "pop", "(", "'.'", ".", "join", "(", "keys", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "__patch_instance_norm_state_dict", "(", "state_dict", ",", "getattr", "(", "module", ",", "key", ")", ",", "keys", ",", "i", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.load_networks": [[204, 228], ["isinstance", "os.path.join", "getattr", "isinstance", "print", "torch.load", "hasattr", "list", "getattr.load_state_dict", "torch.load.keys", "base_model.BaseModel.__patch_instance_norm_state_dict", "str", "key.split"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.__patch_instance_norm_state_dict"], ["", "", "def", "load_networks", "(", "self", ",", "epoch", ")", ":", "\n", "        ", "\"\"\"Load all the networks from the disk.\n\n        Parameters:\n            epoch (int) -- current epoch; used in the file name '%s_net_%s.pth' % (epoch, name)\n        \"\"\"", "\n", "for", "name", "in", "self", ".", "model_names", ":", "\n", "            ", "if", "isinstance", "(", "name", ",", "str", ")", ":", "\n", "                ", "load_filename", "=", "'%s_net_%s.pth'", "%", "(", "epoch", ",", "name", ")", "\n", "load_path", "=", "os", ".", "path", ".", "join", "(", "self", ".", "save_dir", ",", "load_filename", ")", "\n", "net", "=", "getattr", "(", "self", ",", "'net'", "+", "name", ")", "\n", "if", "isinstance", "(", "net", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                    ", "net", "=", "net", ".", "module", "\n", "", "print", "(", "'loading the model from %s'", "%", "load_path", ")", "\n", "# if you are using PyTorch newer than 0.4 (e.g., built from", "\n", "# GitHub source), you can remove str() on self.device", "\n", "state_dict", "=", "torch", ".", "load", "(", "load_path", ",", "map_location", "=", "str", "(", "self", ".", "device", ")", ")", "\n", "if", "hasattr", "(", "state_dict", ",", "'_metadata'", ")", ":", "\n", "                    ", "del", "state_dict", ".", "_metadata", "\n", "\n", "# patch InstanceNorm checkpoints prior to 0.4", "\n", "", "for", "key", "in", "list", "(", "state_dict", ".", "keys", "(", ")", ")", ":", "# need to copy keys here because we mutate in loop", "\n", "                    ", "self", ".", "__patch_instance_norm_state_dict", "(", "state_dict", ",", "net", ",", "key", ".", "split", "(", "'.'", ")", ")", "\n", "", "net", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.print_networks": [[229, 252], ["print", "print", "isinstance", "getattr", "getattr", "print", "getattr.parameters", "print", "param.numel", "i.parameters", "param.numel"], "methods", ["None"], ["", "", "", "def", "print_networks", "(", "self", ",", "verbose", ")", ":", "\n", "        ", "\"\"\"Print the total number of parameters in the network and (if verbose) network architecture\n\n        Parameters:\n            verbose (bool) -- if verbose: print the network architecture\n        \"\"\"", "\n", "print", "(", "'---------- Networks initialized -------------'", ")", "\n", "for", "name", "in", "self", ".", "model_names", ":", "\n", "            ", "if", "isinstance", "(", "name", ",", "str", ")", ":", "\n", "                ", "net", "=", "getattr", "(", "self", ",", "'net'", "+", "name", ")", "\n", "num_params", "=", "0", "\n", "parameters", "=", "getattr", "(", "net", ",", "\"parameters\"", ",", "None", ")", "\n", "if", "parameters", "is", "not", "None", ":", "\n", "                    ", "for", "param", "in", "net", ".", "parameters", "(", ")", ":", "\n", "                        ", "num_params", "+=", "param", ".", "numel", "(", ")", "\n", "", "", "else", ":", "\n", "                    ", "for", "i", "in", "net", ":", "\n", "                        ", "for", "param", "in", "i", ".", "parameters", "(", ")", ":", "\n", "                            ", "num_params", "+=", "param", ".", "numel", "(", ")", "\n", "", "", "", "if", "verbose", ":", "\n", "                    ", "print", "(", "net", ")", "\n", "", "print", "(", "'[Network %s] Total number of parameters : %.3f M'", "%", "(", "name", ",", "num_params", "/", "1e6", ")", ")", "\n", "", "", "print", "(", "'-----------------------------------------------'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.set_requires_grad": [[253, 265], ["isinstance", "net.parameters"], "methods", ["None"], ["", "def", "set_requires_grad", "(", "self", ",", "nets", ",", "requires_grad", "=", "False", ")", ":", "\n", "        ", "\"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n        Parameters:\n            nets (network list)   -- a list of networks\n            requires_grad (bool)  -- whether the networks require gradients or not\n        \"\"\"", "\n", "if", "not", "isinstance", "(", "nets", ",", "list", ")", ":", "\n", "            ", "nets", "=", "[", "nets", "]", "\n", "", "for", "net", "in", "nets", ":", "\n", "            ", "if", "net", "is", "not", "None", ":", "\n", "                ", "for", "param", "in", "net", ".", "parameters", "(", ")", ":", "\n", "                    ", "param", ".", "requires_grad", "=", "requires_grad", "\n", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.modify_commandline_options": [[24, 40], ["parser.set_defaults", "parser.add_argument"], "methods", ["None"], ["    ", "@", "staticmethod", "\n", "def", "modify_commandline_options", "(", "parser", ",", "is_train", "=", "True", ")", ":", "\n", "        ", "\"\"\"Add new model-specific options and rewrite default values for existing options.\n\n        Parameters:\n            parser -- the option parser\n            is_train -- if it is training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        \"\"\"", "\n", "parser", ".", "set_defaults", "(", "dataset_mode", "=", "'aligned'", ")", "# You can rewrite default values for this model. For example, this model usually uses aligned dataset as its dataset.", "\n", "if", "is_train", ":", "\n", "            ", "parser", ".", "add_argument", "(", "'--lambda_regression'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'weight for the regression loss'", ")", "# You can define new arguments for this model.", "\n", "\n", "", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.__init__": [[41, 69], ["base_model.BaseModel.__init__", "networks.define_G", "torch.nn.L1Loss", "torch.optim.Adam", "template_model.TemplateModel.netG.parameters"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_G"], ["", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize this model class.\n\n        Parameters:\n            opt -- training/test options\n\n        A few things can be done here.\n        - (required) call the initialization function of BaseModel\n        - define loss function, visualization images, model names, and optimizers\n        \"\"\"", "\n", "BaseModel", ".", "__init__", "(", "self", ",", "opt", ")", "# call the initialization method of BaseModel", "\n", "# specify the training losses you want to print out. The program will call base_model.get_current_losses to plot the losses to the console and save them to the disk.", "\n", "self", ".", "loss_names", "=", "[", "'loss_G'", "]", "\n", "# specify the images you want to save and display. The program will call base_model.get_current_visuals to save and display these images.", "\n", "self", ".", "visual_names", "=", "[", "'data_A'", ",", "'data_B'", ",", "'output'", "]", "\n", "# specify the models you want to save to the disk. The program will call base_model.save_networks and base_model.load_networks to save and load networks.", "\n", "# you can use opt.isTrain to specify different behaviors for training and test. For example, some networks will not be used during test, and you don't need to load them.", "\n", "self", ".", "model_names", "=", "[", "'G'", "]", "\n", "# define networks; you can use opt.isTrain to specify different behaviors for training and test.", "\n", "self", ".", "netG", "=", "networks", ".", "define_G", "(", "opt", ".", "input_nc", ",", "opt", ".", "output_nc", ",", "opt", ".", "ngf", ",", "opt", ".", "netG", ",", "gpu_ids", "=", "self", ".", "gpu_ids", ")", "\n", "if", "self", ".", "isTrain", ":", "# only defined during training time", "\n", "# define your loss functions. You can use losses provided by torch.nn such as torch.nn.L1Loss.", "\n", "# We also provide a GANLoss class \"networks.GANLoss\". self.criterionGAN = networks.GANLoss().to(self.device)", "\n", "            ", "self", ".", "criterionLoss", "=", "torch", ".", "nn", ".", "L1Loss", "(", ")", "\n", "# define and initialize optimizers. You can define one optimizer for each network.", "\n", "# If two networks are updated at the same time, you can use itertools.chain to group them.", "\n", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "netG", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ",", "betas", "=", "(", "opt", ".", "beta1", ",", "0.999", ")", ")", "\n", "self", ".", "optimizers", "=", "[", "self", ".", "optimizer", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.set_input": [[72, 82], ["input[].to", "input[].to"], "methods", ["None"], ["", "", "def", "set_input", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input: a dictionary that contains the data itself and its metadata information.\n        \"\"\"", "\n", "AtoB", "=", "self", ".", "opt", ".", "direction", "==", "'AtoB'", "# use <direction> to swap data_A and data_B", "\n", "self", ".", "data_A", "=", "input", "[", "'A'", "if", "AtoB", "else", "'B'", "]", ".", "to", "(", "self", ".", "device", ")", "# get image data A", "\n", "self", ".", "data_B", "=", "input", "[", "'B'", "if", "AtoB", "else", "'A'", "]", ".", "to", "(", "self", ".", "device", ")", "# get image data B", "\n", "self", ".", "image_paths", "=", "input", "[", "'A_paths'", "if", "AtoB", "else", "'B_paths'", "]", "# get image paths", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.forward": [[83, 86], ["template_model.TemplateModel.netG"], "methods", ["None"], ["", "def", "forward", "(", "self", ")", ":", "\n", "        ", "\"\"\"Run forward pass. This will be called by both functions <optimize_parameters> and <test>.\"\"\"", "\n", "self", ".", "output", "=", "self", ".", "netG", "(", "self", ".", "data_A", ")", "# generate output image given the input data_A", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.backward": [[87, 93], ["template_model.TemplateModel.loss_G.backward", "template_model.TemplateModel.criterionLoss"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.backward"], ["", "def", "backward", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculate losses, gradients, and update network weights; called in every training iteration\"\"\"", "\n", "# caculate the intermediate results if necessary; here self.output has been computed during function <forward>", "\n", "# calculate loss given the input and intermediate results", "\n", "self", ".", "loss_G", "=", "self", ".", "criterionLoss", "(", "self", ".", "output", ",", "self", ".", "data_B", ")", "*", "self", ".", "opt", ".", "lambda_regression", "\n", "self", ".", "loss_G", ".", "backward", "(", ")", "# calculate gradients of network G w.r.t. loss_G", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.optimize_parameters": [[94, 100], ["template_model.TemplateModel.forward", "template_model.TemplateModel.optimizer.zero_grad", "template_model.TemplateModel.backward", "template_model.TemplateModel.optimizer.step"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.forward", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.backward"], ["", "def", "optimize_parameters", "(", "self", ")", ":", "\n", "        ", "\"\"\"Update network weights; it will be called in every training iteration.\"\"\"", "\n", "self", ".", "forward", "(", ")", "# first call forward to calculate intermediate results", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "# clear network G's existing gradients", "\n", "self", ".", "backward", "(", ")", "# calculate gradients for network G", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "# update gradients for network G", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.modify_commandline_options": [[20, 45], ["parser.set_defaults", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "modify_commandline_options", "(", "parser", ",", "is_train", "=", "True", ")", ":", "\n", "        ", "\"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        \"\"\"", "\n", "# changing the default values to match the pix2pix paper (https://phillipi.github.io/pix2pix/)", "\n", "# parser.set_defaults(norm='batch', netG='unet_256', dataset_mode='aligned')", "\n", "if", "is_train", ":", "\n", "            ", "parser", ".", "set_defaults", "(", "pool_size", "=", "0", ",", "gan_mode", "=", "'vanilla'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_digesting_L1'", ",", "type", "=", "float", ",", "default", "=", "100.0", ",", "help", "=", "'weight for L1 loss'", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_digesting_perceptual'", ",", "type", "=", "float", ",", "default", "=", "10.0", ",", "help", "=", "'weight for perceptual loss'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--prev_model_path'", ",", "type", "=", "str", ",", "default", "=", "''", ",", "help", "=", "'the path of trained model using previous tasks'", ")", "\n", "parser", ".", "add_argument", "(", "'--prev_model_epoch'", ",", "type", "=", "int", ",", "default", "=", "400", ",", "help", "=", "'the epoch of trained model using previous tasks'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--lambda_G'", ",", "type", "=", "float", ",", "default", "=", "0.1", ",", "help", "=", "'weight for dadgan G '", ")", "\n", "parser", ".", "add_argument", "(", "'--lambda_D'", ",", "type", "=", "float", ",", "default", "=", "0.05", ",", "help", "=", "'weight for dadgan D'", ")", "\n", "\n", "", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.__init__": [[46, 83], ["base_model.BaseModel.__init__", "networks.define_G", "networks.define_D", "pix2pix_model.Pix2pixModel.load_prev_model", "networks.GANLoss().to", "torch.nn.L1Loss", "torch.optim.Adam", "torch.optim.Adam", "pix2pix_model.Pix2pixModel.optimizers.append", "pix2pix_model.Pix2pixModel.optimizers.append", "models.perception_loss.vgg16_feat().cuda", "models.perception_loss.perceptual_loss", "pix2pix_model.Pix2pixModel.netG.parameters", "pix2pix_model.Pix2pixModel.netD.parameters", "networks.GANLoss", "models.perception_loss.vgg16_feat"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_G", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.networks.define_D", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.load_prev_model"], ["", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize the pix2pix class.\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"", "\n", "BaseModel", ".", "__init__", "(", "self", ",", "opt", ")", "\n", "self", ".", "loss_names", "=", "[", "'G_GAN'", ",", "'G_L1'", ",", "'G_perceptual'", ",", "'D_real'", ",", "'D_fake'", "]", "\n", "self", ".", "visual_names", "=", "[", "'real_A'", ",", "'fake_B'", ",", "'real_B'", "]", "\n", "# specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>", "\n", "if", "self", ".", "isTrain", ":", "\n", "            ", "self", ".", "model_names", "=", "[", "'G'", ",", "'D'", "]", "\n", "", "else", ":", "# during test time, only load G", "\n", "            ", "self", ".", "model_names", "=", "[", "'G'", "]", "\n", "# define networks (both generator and discriminator)", "\n", "", "self", ".", "netG", "=", "networks", ".", "define_G", "(", "opt", ".", "input_nc", ",", "opt", ".", "output_nc", ",", "opt", ".", "ngf", ",", "opt", ".", "netG", ",", "opt", ".", "norm", ",", "\n", "not", "opt", ".", "no_dropout", ",", "opt", ".", "init_type", ",", "opt", ".", "init_gain", ",", "self", ".", "gpu_ids", ")", "\n", "\n", "if", "self", ".", "isTrain", ":", "# define a discriminator; conditional GANs need to take both input and output images; Therefore, #channels for D is input_nc + output_nc", "\n", "            ", "self", ".", "netD", "=", "networks", ".", "define_D", "(", "opt", ".", "input_nc", "+", "opt", ".", "output_nc", ",", "opt", ".", "ndf", ",", "opt", ".", "netD", ",", "\n", "opt", ".", "n_layers_D", ",", "opt", ".", "norm", ",", "opt", ".", "init_type", ",", "opt", ".", "init_gain", ",", "self", ".", "gpu_ids", ")", "\n", "\n", "", "if", "self", ".", "isTrain", "and", "self", ".", "opt", ".", "prev_model_path", ":", "\n", "            ", "self", ".", "load_prev_model", "(", ")", "\n", "\n", "", "if", "self", ".", "isTrain", ":", "\n", "# define loss functions", "\n", "            ", "self", ".", "criterionGAN", "=", "networks", ".", "GANLoss", "(", "opt", ".", "gan_mode", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "criterionL1", "=", "torch", ".", "nn", ".", "L1Loss", "(", ")", "\n", "# initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.", "\n", "self", ".", "optimizer_G", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "netG", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ",", "betas", "=", "(", "opt", ".", "beta1", ",", "0.999", ")", ")", "\n", "self", ".", "optimizer_D", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "netD", ".", "parameters", "(", ")", ",", "lr", "=", "opt", ".", "lr", ",", "betas", "=", "(", "opt", ".", "beta1", ",", "0.999", ")", ")", "\n", "self", ".", "optimizers", ".", "append", "(", "self", ".", "optimizer_G", ")", "\n", "self", ".", "optimizers", ".", "append", "(", "self", ".", "optimizer_D", ")", "\n", "\n", "self", ".", "vgg_model", "=", "vgg16_feat", "(", ")", ".", "cuda", "(", ")", "\n", "self", ".", "criterion_perceptual", "=", "perceptual_loss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.load_prev_model": [[84, 100], ["range", "len", "isinstance", "print", "torch.load", "hasattr", "net.load_state_dict"], "methods", ["None"], ["", "", "def", "load_prev_model", "(", "self", ")", ":", "\n", "        ", "\"\"\"Load trained Generator using previous tasks\n        \"\"\"", "\n", "nets", "=", "[", "self", ".", "netG", "]", "\n", "model_paths", "=", "[", "'{:s}/{:d}_net_G.pth'", ".", "format", "(", "self", ".", "opt", ".", "prev_model_path", ",", "self", ".", "opt", ".", "prev_model_epoch", ")", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "nets", ")", ")", ":", "\n", "            ", "net", "=", "nets", "[", "i", "]", "\n", "path", "=", "model_paths", "[", "i", "]", "\n", "if", "isinstance", "(", "net", ",", "torch", ".", "nn", ".", "DataParallel", ")", ":", "\n", "                ", "net", "=", "net", ".", "module", "\n", "", "print", "(", "'loading the model from %s'", "%", "path", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "path", ",", "map_location", "=", "self", ".", "device", ")", "\n", "if", "hasattr", "(", "state_dict", ",", "'_metadata'", ")", ":", "\n", "                ", "del", "state_dict", ".", "_metadata", "\n", "", "net", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.set_input": [[101, 117], ["input[].to", "input[].to", "input[].to", "input[].to", "pix2pix_model.Pix2pixModel.opt.dataset_mode.split", "str", "str", "str"], "methods", ["None"], ["", "", "def", "set_input", "(", "self", ",", "input", ")", ":", "\n", "        ", "\"\"\"Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n\n        The option 'direction' can be used to swap images in domain A and domain B.\n        \"\"\"", "\n", "if", "self", ".", "opt", ".", "dataset_mode", ".", "split", "(", "'_'", ")", "[", "-", "1", "]", "!=", "'split'", "or", "(", "not", "self", ".", "opt", ".", "isTrain", ")", ":", "\n", "            ", "self", ".", "real_A", "=", "input", "[", "'A'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "real_B", "=", "input", "[", "'B'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "image_paths", "=", "input", "[", "'A_paths'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "real_A", "=", "input", "[", "'A_'", "+", "str", "(", "self", ".", "opt", ".", "task_num", "-", "1", ")", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "real_B", "=", "input", "[", "'B_'", "+", "str", "(", "self", ".", "opt", ".", "task_num", "-", "1", ")", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "self", ".", "image_paths", "=", "input", "[", "'A_paths_'", "+", "str", "(", "self", ".", "opt", ".", "task_num", "-", "1", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.forward": [[118, 121], ["pix2pix_model.Pix2pixModel.netG"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ")", ":", "\n", "        ", "\"\"\"Run forward pass; called by both functions <optimize_parameters> and <test>.\"\"\"", "\n", "self", ".", "fake_B", "=", "self", ".", "netG", "(", "self", ".", "real_A", ")", "# for current task", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.backward_D": [[122, 135], ["torch.cat", "pix2pix_model.Pix2pixModel.netD", "pix2pix_model.Pix2pixModel.criterionGAN", "torch.cat", "pix2pix_model.Pix2pixModel.netD", "pix2pix_model.Pix2pixModel.criterionGAN", "pix2pix_model.Pix2pixModel.loss_D.backward", "torch.cat.detach"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.backward"], ["", "def", "backward_D", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculate GAN loss for the discriminator\"\"\"", "\n", "# Fake; stop backprop to the generator by detaching fake_B", "\n", "fake_AB", "=", "torch", ".", "cat", "(", "(", "self", ".", "real_A", ",", "self", ".", "fake_B", ")", ",", "1", ")", "# we use conditional GANs; we need to feed both input and output to the discriminator", "\n", "pred_fake", "=", "self", ".", "netD", "(", "fake_AB", ".", "detach", "(", ")", ")", "\n", "self", ".", "loss_D_fake", "=", "self", ".", "criterionGAN", "(", "pred_fake", ",", "False", ")", "\n", "# Real", "\n", "real_AB", "=", "torch", ".", "cat", "(", "(", "self", ".", "real_A", ",", "self", ".", "real_B", ")", ",", "1", ")", "\n", "pred_real", "=", "self", ".", "netD", "(", "real_AB", ")", "\n", "self", ".", "loss_D_real", "=", "self", ".", "criterionGAN", "(", "pred_real", ",", "True", ")", "\n", "# combine loss and calculate gradients", "\n", "self", ".", "loss_D", "=", "(", "self", ".", "loss_D_fake", "+", "self", ".", "loss_D_real", ")", "*", "self", ".", "opt", ".", "lambda_D", "\n", "self", ".", "loss_D", ".", "backward", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.backward_G": [[136, 148], ["torch.cat", "pix2pix_model.Pix2pixModel.netD", "pix2pix_model.Pix2pixModel.criterionGAN", "pix2pix_model.Pix2pixModel.criterionL1", "pix2pix_model.Pix2pixModel.vgg_model", "pix2pix_model.Pix2pixModel.vgg_model", "pix2pix_model.Pix2pixModel.criterion_perceptual", "pix2pix_model.Pix2pixModel.loss_G.backward"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.template_model.TemplateModel.backward"], ["", "def", "backward_G", "(", "self", ")", ":", "\n", "        ", "\"\"\"Calculate GAN and L1 loss for the generator\"\"\"", "\n", "fake_AB", "=", "torch", ".", "cat", "(", "(", "self", ".", "real_A", ",", "self", ".", "fake_B", ")", ",", "1", ")", "\n", "pred_fake", "=", "self", ".", "netD", "(", "fake_AB", ")", "\n", "self", ".", "loss_G_GAN", "=", "self", ".", "criterionGAN", "(", "pred_fake", ",", "True", ")", "\n", "self", ".", "loss_G_L1", "=", "self", ".", "criterionL1", "(", "self", ".", "fake_B", ",", "self", ".", "real_B", ")", "\n", "pred_feat", "=", "self", ".", "vgg_model", "(", "self", ".", "fake_B", ")", "\n", "target_feat", "=", "self", ".", "vgg_model", "(", "self", ".", "real_B", ")", "\n", "self", ".", "loss_G_perceptual", "=", "self", ".", "criterion_perceptual", "(", "pred_feat", ",", "target_feat", ")", "\n", "\n", "self", ".", "loss_G", "=", "(", "self", ".", "loss_G_GAN", "+", "self", ".", "loss_G_L1", "*", "self", ".", "opt", ".", "lambda_digesting_L1", "+", "self", ".", "loss_G_perceptual", "*", "self", ".", "opt", ".", "lambda_digesting_perceptual", ")", "*", "self", ".", "opt", ".", "lambda_G", "\n", "self", ".", "loss_G", ".", "backward", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.optimize_parameters": [[149, 162], ["pix2pix_model.Pix2pixModel.forward", "pix2pix_model.Pix2pixModel.set_requires_grad", "pix2pix_model.Pix2pixModel.optimizer_D.zero_grad", "pix2pix_model.Pix2pixModel.backward_D", "pix2pix_model.Pix2pixModel.optimizer_D.step", "pix2pix_model.Pix2pixModel.set_requires_grad", "pix2pix_model.Pix2pixModel.optimizer_G.zero_grad", "pix2pix_model.Pix2pixModel.backward_G", "pix2pix_model.Pix2pixModel.optimizer_G.step"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.forward", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.set_requires_grad", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.backward_D", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.base_model.BaseModel.set_requires_grad", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.models.pix2pix_model.Pix2pixModel.backward_G"], ["", "def", "optimize_parameters", "(", "self", ")", ":", "\n", "        ", "self", ".", "forward", "(", ")", "# compute fake images: G(A)", "\n", "# update D", "\n", "self", ".", "set_requires_grad", "(", "self", ".", "netD", ",", "True", ")", "\n", "self", ".", "optimizer_D", ".", "zero_grad", "(", ")", "\n", "self", ".", "backward_D", "(", ")", "\n", "self", ".", "optimizer_D", ".", "step", "(", ")", "\n", "\n", "# update G", "\n", "self", ".", "set_requires_grad", "(", "self", ".", "netD", ",", "False", ")", "# D requires no gradients when optimizing G", "\n", "self", ".", "optimizer_G", ".", "zero_grad", "(", ")", "# set G's gradients to zero", "\n", "self", ".", "backward_G", "(", ")", "# calculate graidents for G", "\n", "self", ".", "optimizer_G", ".", "step", "(", ")", "# udpate G's weights", "", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_dataset.NucleiDataset.__init__": [[16, 53], ["print", "data.base_dataset.BaseDataset.__init__", "h5py.File", "nuclei_dataset.NucleiDataset.build_pairs", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.build_pairs"], ["def", "__init__", "(", "self", ",", "opt", ",", "idx", "=", "None", ")", ":", "\n", "        ", "\"\"\"Initialize this dataset class.\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"", "\n", "self", ".", "idx", "=", "idx", "\n", "if", "idx", "is", "None", "or", "idx", "==", "4", ":", "# 4 is the flag using the whole dataset", "\n", "            ", "h5_name", "=", "\"train_nuclei.h5\"", "\n", "", "else", ":", "\n", "            ", "if", "idx", "==", "0", ":", "\n", "                ", "h5_name", "=", "\"train_liver.h5\"", "\n", "", "elif", "idx", "==", "1", ":", "\n", "                ", "h5_name", "=", "\"train_breast.h5\"", "\n", "", "elif", "idx", "==", "2", ":", "\n", "                ", "h5_name", "=", "\"train_kidney.h5\"", "\n", "", "elif", "idx", "==", "3", ":", "\n", "                ", "h5_name", "=", "\"train_prostate.h5\"", "\n", "\n", "", "", "print", "(", "f\"Load: {h5_name}\"", ")", "\n", "self", ".", "is_test", "=", "True", "\n", "self", ".", "real_tumor", "=", "False", "\n", "self", ".", "extend_len", "=", "0", "\n", "BaseDataset", ".", "__init__", "(", "self", ",", "opt", ")", "\n", "self", ".", "nuclei_file", "=", "h5py", ".", "File", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "dataroot", ",", "h5_name", ")", ",", "'r'", ")", "\n", "\n", "if", "'train'", "in", "self", ".", "nuclei_file", ":", "\n", "            ", "train_db", "=", "self", ".", "nuclei_file", "[", "'train'", "]", "\n", "", "else", ":", "\n", "            ", "train_db", "=", "self", ".", "nuclei_file", "\n", "", "self", ".", "image", ",", "self", ".", "label", ",", "self", ".", "labels_ternary", ",", "self", ".", "weight_maps", "=", "self", ".", "build_pairs", "(", "train_db", ")", "\n", "\n", "# self.dir_AB = os.path.join(opt.dataroot, opt.phase)  # get the image directory", "\n", "# self.AB_paths = sorted(make_dataset(self.dir_AB, opt.max_dataset_size))  # get image paths", "\n", "assert", "(", "self", ".", "opt", ".", "load_size", ">=", "self", ".", "opt", ".", "crop_size", ")", "# crop_size should be smaller than the size of loaded image", "\n", "self", ".", "input_nc", "=", "self", ".", "opt", ".", "output_nc", "if", "self", ".", "opt", ".", "direction", "==", "'BtoA'", "else", "self", ".", "opt", ".", "input_nc", "\n", "self", ".", "output_nc", "=", "self", ".", "opt", ".", "input_nc", "if", "self", ".", "opt", ".", "direction", "==", "'BtoA'", "else", "self", ".", "opt", ".", "output_nc", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_dataset.NucleiDataset.build_pairs": [[54, 79], ["images.keys", "image_arr.append", "label_arr.append", "labels_ternary_arr.append", "weight_maps_arr.append"], "methods", ["None"], ["", "def", "build_pairs", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "image_arr", "=", "[", "]", "\n", "label_arr", "=", "[", "]", "\n", "labels_ternary_arr", "=", "[", "]", "\n", "weight_maps_arr", "=", "[", "]", "\n", "\n", "images", "=", "dataset", "[", "'images'", "]", "\n", "labels", "=", "dataset", "[", "'labels'", "]", "\n", "labels_ternary", "=", "dataset", "[", "'labels_ternary'", "]", "\n", "weight_maps", "=", "dataset", "[", "'weight_maps'", "]", "\n", "\n", "keys", "=", "images", ".", "keys", "(", ")", "\n", "# keys = list(keys)[:2]", "\n", "for", "key", "in", "keys", ":", "\n", "            ", "img", "=", "images", "[", "key", "]", "[", "(", ")", "]", "\n", "label", "=", "labels", "[", "key", "]", "[", "(", ")", "]", "\n", "label_t", "=", "labels_ternary", "[", "key", "]", "[", "(", ")", "]", "\n", "weight_m", "=", "weight_maps", "[", "key", "]", "[", "(", ")", "]", "\n", "\n", "image_arr", ".", "append", "(", "img", ")", "\n", "label_arr", ".", "append", "(", "label", ")", "\n", "labels_ternary_arr", ".", "append", "(", "label_t", ")", "\n", "weight_maps_arr", ".", "append", "(", "weight_m", ")", "\n", "\n", "", "return", "image_arr", ",", "label_arr", ",", "labels_ternary_arr", ",", "weight_maps_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_dataset.NucleiDataset.__getitem__": [[80, 130], ["PIL.Image.fromarray().convert", "PIL.Image.fromarray().convert", "data.base_dataset.get_params", "data.base_dataset.get_transform", "data.base_dataset.get_transform", "data.base_dataset.get_transform.", "data.base_dataset.get_transform.", "nuclei_dataset.NucleiDataset.opt.phase.lower", "str", "str", "PIL.Image.fromarray", "PIL.Image.fromarray"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_params", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return a data point and its metadata information.\n\n        Parameters:\n            index - - a random integer for data indexing\n\n        Returns a dictionary that contains A, B, A_paths and B_paths\n            A (tensor) - - an image in the input domain\n            B (tensor) - - its corresponding image in the target domain\n            A_paths (str) - - image paths\n            B_paths (str) - - image paths (same as A_paths)\n        \"\"\"", "\n", "import", "numpy", "as", "np", "\n", "A", "=", "self", ".", "label", "[", "index", "]", "\n", "# set the condition for different tasks", "\n", "if", "self", ".", "idx", "==", "0", ":", "\n", "            ", "A", "=", "A", "//", "4", "\n", "", "elif", "self", ".", "idx", "==", "1", ":", "\n", "            ", "A", "=", "A", "//", "4", "*", "2", "\n", "", "elif", "self", ".", "idx", "==", "2", ":", "\n", "            ", "A", "=", "A", "//", "4", "*", "3", "\n", "", "elif", "self", ".", "idx", "==", "4", ":", "# training using the whole dataset, set condition", "\n", "            ", "A", "=", "A", "//", "3", "\n", "\n", "", "B", "=", "self", ".", "image", "[", "index", "]", "\n", "A", "=", "Image", ".", "fromarray", "(", "A", ")", ".", "convert", "(", "'RGB'", ")", "\n", "B", "=", "Image", ".", "fromarray", "(", "B", ")", ".", "convert", "(", "'RGB'", ")", "\n", "labels_ternary", "=", "self", ".", "labels_ternary", "[", "index", "]", "# for subsequent segmentation task, ignore if only training GAN", "\n", "weight_map", "=", "self", ".", "weight_maps", "[", "index", "]", "# for subsequent segmentation task, ignore if only training GAN", "\n", "labels_ternary", "=", "labels_ternary", "[", ":", "256", ",", ":", "256", ",", ":", "]", "\n", "weight_map", "=", "weight_map", "[", ":", "256", ",", ":", "256", "]", "\n", "\n", "# apply the same transform to both A and B", "\n", "transform_params", "=", "get_params", "(", "self", ".", "opt", ",", "A", ".", "size", ")", "\n", "if", "self", ".", "opt", ".", "phase", ".", "lower", "(", ")", "!=", "'train'", ":", "\n", "            ", "transform_params", "[", "'crop_pos'", "]", "=", "(", "0", ",", "0", ")", "\n", "transform_params", "[", "'vflip'", "]", "=", "False", "\n", "transform_params", "[", "'hflip'", "]", "=", "False", "\n", "\n", "# self.opt.load_size = 286", "\n", "", "A_transform", "=", "get_transform", "(", "self", ".", "opt", ",", "transform_params", ",", "grayscale", "=", "(", "self", ".", "input_nc", "==", "1", ")", ",", "method", "=", "Image", ".", "NEAREST", ")", "\n", "B_transform", "=", "get_transform", "(", "self", ".", "opt", ",", "transform_params", ",", "grayscale", "=", "(", "self", ".", "output_nc", "==", "1", ")", ")", "\n", "\n", "A", "=", "A_transform", "(", "A", ")", "\n", "B", "=", "B_transform", "(", "B", ")", "\n", "\n", "return", "{", "'A'", ":", "A", ",", "'B'", ":", "B", ",", "'A_paths'", ":", "str", "(", "index", ")", ",", "'B_paths'", ":", "str", "(", "index", ")", ",", "\n", "\"label_ternary\"", ":", "labels_ternary", ",", "\n", "\"weight_map\"", ":", "weight_map", ",", "\n", "\"Seg_label\"", ":", "A", "}", "# seg_label not used", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_dataset.NucleiDataset.__len__": [[131, 134], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the total number of images in the dataset.\"\"\"", "\n", "return", "len", "(", "self", ".", "image", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brats_dataset.BratsDataset.__init__": [[21, 53], ["print", "data.base_dataset.BaseDataset.__init__", "h5py.File", "brats_dataset.BratsDataset.build_pairs", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.build_pairs"], ["def", "__init__", "(", "self", ",", "opt", ",", "idx", "=", "1", ")", ":", "\n", "        ", "\"\"\"Initialize this dataset class.\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"", "\n", "\n", "self", ".", "idx", "=", "idx", "\n", "\n", "if", "idx", "==", "1", ":", "\n", "            ", "h5_name", "=", "\"train_brats_0.1.h5\"", "\n", "", "elif", "idx", "==", "2", ":", "\n", "            ", "h5_name", "=", "\"train_brats_t1_0.1.h5\"", "\n", "\n", "", "print", "(", "f\"Load: {h5_name}\"", ")", "\n", "self", ".", "is_test", "=", "False", "\n", "self", ".", "real_tumor", "=", "False", "\n", "self", ".", "extend_len", "=", "0", "\n", "self", ".", "multi_label", "=", "True", "\n", "BaseDataset", ".", "__init__", "(", "self", ",", "opt", ")", "\n", "self", ".", "brats_file", "=", "h5py", ".", "File", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "dataroot", ",", "h5_name", ")", ",", "'r'", ")", "\n", "if", "'train'", "in", "self", ".", "brats_file", ":", "\n", "            ", "train_db", "=", "self", ".", "brats_file", "[", "'train'", "]", "\n", "", "else", ":", "\n", "            ", "train_db", "=", "self", ".", "brats_file", "\n", "", "self", ".", "dcm", ",", "self", ".", "label", ",", "self", ".", "seg_label", "=", "self", ".", "build_pairs", "(", "train_db", ")", "\n", "\n", "# self.dir_AB = os.path.join(opt.dataroot, opt.phase)  # get the image directory", "\n", "# self.AB_paths = sorted(make_dataset(self.dir_AB, opt.max_dataset_size))  # get image paths", "\n", "assert", "(", "self", ".", "opt", ".", "load_size", ">=", "self", ".", "opt", ".", "crop_size", ")", "# crop_size should be smaller than the size of loaded image", "\n", "self", ".", "input_nc", "=", "self", ".", "opt", ".", "output_nc", "if", "self", ".", "opt", ".", "direction", "==", "'BtoA'", "else", "self", ".", "opt", ".", "input_nc", "\n", "self", ".", "output_nc", "=", "self", ".", "opt", ".", "input_nc", "if", "self", ".", "opt", ".", "direction", "==", "'BtoA'", "else", "self", ".", "opt", ".", "output_nc", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brats_dataset.BratsDataset.build_pairs": [[54, 78], ["images.keys", "brats_dataset.BratsDataset.merge_skull", "dcm_arr.append", "label_arr.append", "seg_label_arr.append"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.merge_skull"], ["", "def", "build_pairs", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "dcm_arr", "=", "[", "]", "\n", "label_arr", "=", "[", "]", "\n", "seg_label_arr", "=", "[", "]", "\n", "images", "=", "dataset", "[", "'images'", "]", "\n", "labels", "=", "dataset", "[", "'labels'", "]", "\n", "\n", "keys", "=", "images", ".", "keys", "(", ")", "\n", "for", "key", "in", "keys", ":", "\n", "            ", "img", "=", "images", "[", "key", "]", "[", "(", ")", "]", "\n", "label", "=", "labels", "[", "key", "]", "[", "(", ")", "]", "\n", "\n", "seg_label", "=", "(", "label", ">", "0", ")", ".", "astype", "(", "np", ".", "uint8", ")", "*", "255", "\n", "\n", "# label = label * (250 / (label.max() + 1e-8))  # multi-label", "\n", "label", "=", "(", "label", ">", "0", ")", ".", "astype", "(", "np", ".", "uint8", ")", "*", "255", "# single label", "\n", "label", "=", "self", ".", "merge_skull", "(", "img", "[", ":", ",", ":", ",", "0", "]", ",", "label", ",", "default_skull_value", "=", "255", ")", "\n", "# label = label[:, :, None].repeat(3, axis=2)", "\n", "\n", "dcm_arr", ".", "append", "(", "img", ")", "\n", "label_arr", ".", "append", "(", "label", ")", "\n", "seg_label_arr", ".", "append", "(", "seg_label", ")", "\n", "\n", "", "return", "dcm_arr", ",", "label_arr", ",", "seg_label_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brats_dataset.BratsDataset.seg_in_skull": [[79, 83], ["None"], "methods", ["None"], ["", "def", "seg_in_skull", "(", "self", ",", "seg", ",", "mask", ")", ":", "\n", "# ndimage.binary_fill_holes(skull_mask)", "\n", "        ", "seg", "=", "mask", "*", "seg", "\n", "return", "seg", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brats_dataset.BratsDataset.merge_skull": [[84, 97], ["scipy.binary_fill_holes", "cv2.Laplacian", "cv2.Laplacian.astype"], "methods", ["None"], ["", "def", "merge_skull", "(", "self", ",", "skull_mask", ",", "slice_label", ",", "default_skull_value", "=", "5", ")", ":", "\n", "# Add skull structure into label", "\n", "        ", "skull_mask", "=", "ndimage", ".", "binary_fill_holes", "(", "skull_mask", ")", "\n", "skull_mask", "=", "cv2", ".", "Laplacian", "(", "skull_mask", ".", "astype", "(", "\"uint8\"", ")", ",", "cv2", ".", "CV_8U", ")", "\n", "skull_mask", "[", "skull_mask", ">", "0", "]", "=", "default_skull_value", "\n", "slice_label", "=", "slice_label", "+", "skull_mask", "\n", "\n", "# slice_label = slice_label * (255 / (slice_label.max() + 1e-8))", "\n", "# slice_label = slice_label.astype(\"uint8\")", "\n", "\n", "# slice_label[slice_label > 0] = 255", "\n", "\n", "return", "slice_label", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brats_dataset.BratsDataset.__getitem__": [[98, 136], ["PIL.Image.fromarray().convert", "PIL.Image.fromarray().convert", "PIL.Image.fromarray().convert", "data.base_dataset.get_params", "data.base_dataset.get_transform", "data.base_dataset.get_transform", "data.base_dataset.get_transform", "data.base_dataset.get_transform.", "data.base_dataset.get_transform.", "data.base_dataset.get_transform.", "brats_dataset.BratsDataset.opt.phase.lower", "str", "str", "PIL.Image.fromarray", "PIL.Image.fromarray", "PIL.Image.fromarray"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_params", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return a data point and its metadata information.\n\n        Parameters:\n            index - - a random integer for data indexing\n\n        Returns a dictionary that contains A, B, A_paths and B_paths\n            A (tensor) - - an image in the input domain\n            B (tensor) - - its corresponding image in the target domain\n            A_paths (str) - - image paths\n            B_paths (str) - - image paths (same as A_paths)\n        \"\"\"", "\n", "A", "=", "self", ".", "label", "[", "index", "]", "\n", "if", "self", ".", "idx", "==", "1", ":", "\n", "            ", "A", "=", "A", "//", "3", "*", "2", "\n", "\n", "", "B", "=", "self", ".", "dcm", "[", "index", "]", "\n", "seg", "=", "self", ".", "seg_label", "[", "index", "]", "\n", "A", "=", "Image", ".", "fromarray", "(", "A", ")", ".", "convert", "(", "'RGB'", ")", "\n", "B", "=", "Image", ".", "fromarray", "(", "B", ")", ".", "convert", "(", "'RGB'", ")", "\n", "seg", "=", "Image", ".", "fromarray", "(", "seg", ")", ".", "convert", "(", "'RGB'", ")", "\n", "\n", "# apply the same transform to both A and B", "\n", "transform_params", "=", "get_params", "(", "self", ".", "opt", ",", "A", ".", "size", ")", "\n", "if", "self", ".", "opt", ".", "phase", ".", "lower", "(", ")", "!=", "'train'", ":", "\n", "            ", "transform_params", "[", "'crop_pos'", "]", "=", "(", "0", ",", "0", ")", "\n", "transform_params", "[", "'vflip'", "]", "=", "False", "\n", "transform_params", "[", "'hflip'", "]", "=", "False", "\n", "\n", "", "A_transform", "=", "get_transform", "(", "self", ".", "opt", ",", "transform_params", ",", "grayscale", "=", "(", "self", ".", "input_nc", "==", "1", ")", ",", "method", "=", "Image", ".", "NEAREST", ")", "\n", "B_transform", "=", "get_transform", "(", "self", ".", "opt", ",", "transform_params", ",", "grayscale", "=", "(", "self", ".", "output_nc", "==", "1", ")", ")", "\n", "seg_transform", "=", "get_transform", "(", "self", ".", "opt", ",", "transform_params", ",", "grayscale", "=", "(", "self", ".", "input_nc", "==", "1", ")", ",", "method", "=", "Image", ".", "NEAREST", ")", "\n", "\n", "A", "=", "A_transform", "(", "A", ")", "\n", "B", "=", "B_transform", "(", "B", ")", "\n", "seg", "=", "seg_transform", "(", "seg", ")", "\n", "\n", "return", "{", "'A'", ":", "A", ",", "'B'", ":", "B", ",", "'A_paths'", ":", "str", "(", "index", ")", ",", "'B_paths'", ":", "str", "(", "index", ")", ",", "'Seg_label'", ":", "seg", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brats_dataset.BratsDataset.__len__": [[137, 140], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the total number of images in the dataset.\"\"\"", "\n", "return", "len", "(", "self", ".", "dcm", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brain_joint_dataset.BrainJointDataset.__init__": [[17, 25], ["data.nuclei_dataset.NucleiDataset", "data.brats_dataset.BratsDataset", "data.nuclei_dataset.NucleiDataset", "data.brats_dataset.BratsDataset", "data.brats_dataset.BratsDataset"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "if", "opt", ".", "task_num", "==", "2", ":", "\n", "            ", "self", ".", "split_db", "=", "[", "NucleiDataset", "(", "opt", ",", "4", ")", ",", "\n", "BratsDataset", "(", "opt", ",", "1", ")", "]", "\n", "", "elif", "opt", ".", "task_num", "==", "3", ":", "\n", "            ", "self", ".", "split_db", "=", "[", "NucleiDataset", "(", "opt", ",", "4", ")", ",", "\n", "BratsDataset", "(", "opt", ",", "1", ")", ",", "\n", "BratsDataset", "(", "opt", ",", "2", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brain_joint_dataset.BrainJointDataset.__getitem__": [[26, 41], ["enumerate", "len", "len"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "result", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "enumerate", "(", "self", ".", "split_db", ")", ":", "\n", "            ", "database", "=", "v", "\n", "if", "index", ">=", "len", "(", "database", ")", ":", "\n", "                ", "index", "=", "index", "-", "len", "(", "database", ")", "\n", "", "else", ":", "\n", "                ", "index_value", "=", "database", "[", "index", "]", "\n", "result", "[", "'A'", "]", "=", "index_value", "[", "'A'", "]", "\n", "result", "[", "'B'", "]", "=", "index_value", "[", "'B'", "]", "\n", "result", "[", "'A_paths'", "]", "=", "index_value", "[", "'A_paths'", "]", "\n", "result", "[", "'B_paths'", "]", "=", "index_value", "[", "'B_paths'", "]", "\n", "result", "[", "'Seg_label'", "]", "=", "index_value", "[", "'Seg_label'", "]", "\n", "break", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brain_joint_dataset.BrainJointDataset.__len__": [[42, 49], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the total number of images in the dataset.\"\"\"", "\n", "length", "=", "0", "\n", "for", "i", "in", "self", ".", "split_db", ":", "\n", "            ", "length", "+=", "len", "(", "i", ")", "\n", "\n", "", "return", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.__init__": [[21, 48], ["print", "data.base_dataset.BaseDataset.__init__", "h5py.File", "bratsT1_dataset.BratsT1Dataset.build_pairs", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.build_pairs"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize this dataset class.\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"", "\n", "\n", "h5_name", "=", "\"train_brats_t1_0.1.h5\"", "\n", "\n", "print", "(", "f\"Load: {h5_name}\"", ")", "\n", "self", ".", "is_test", "=", "False", "\n", "self", ".", "real_tumor", "=", "False", "\n", "self", ".", "extend_len", "=", "0", "\n", "self", ".", "multi_label", "=", "True", "\n", "BaseDataset", ".", "__init__", "(", "self", ",", "opt", ")", "\n", "self", ".", "brats_file", "=", "h5py", ".", "File", "(", "os", ".", "path", ".", "join", "(", "opt", ".", "dataroot", ",", "h5_name", ")", ",", "'r'", ")", "\n", "if", "'train'", "in", "self", ".", "brats_file", ":", "\n", "            ", "train_db", "=", "self", ".", "brats_file", "[", "'train'", "]", "\n", "", "else", ":", "\n", "            ", "train_db", "=", "self", ".", "brats_file", "\n", "", "self", ".", "dcm", ",", "self", ".", "label", ",", "self", ".", "seg_label", "=", "self", ".", "build_pairs", "(", "train_db", ")", "\n", "\n", "# self.dir_AB = os.path.join(opt.dataroot, opt.phase)  # get the image directory", "\n", "# self.AB_paths = sorted(make_dataset(self.dir_AB, opt.max_dataset_size))  # get image paths", "\n", "assert", "(", "self", ".", "opt", ".", "load_size", ">=", "self", ".", "opt", ".", "crop_size", ")", "# crop_size should be smaller than the size of loaded image", "\n", "self", ".", "input_nc", "=", "self", ".", "opt", ".", "output_nc", "if", "self", ".", "opt", ".", "direction", "==", "'BtoA'", "else", "self", ".", "opt", ".", "input_nc", "\n", "self", ".", "output_nc", "=", "self", ".", "opt", ".", "input_nc", "if", "self", ".", "opt", ".", "direction", "==", "'BtoA'", "else", "self", ".", "opt", ".", "output_nc", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.build_pairs": [[49, 73], ["images.keys", "bratsT1_dataset.BratsT1Dataset.merge_skull", "dcm_arr.append", "label_arr.append", "seg_label_arr.append"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.merge_skull"], ["", "def", "build_pairs", "(", "self", ",", "dataset", ")", ":", "\n", "        ", "dcm_arr", "=", "[", "]", "\n", "label_arr", "=", "[", "]", "\n", "seg_label_arr", "=", "[", "]", "\n", "images", "=", "dataset", "[", "'images'", "]", "\n", "labels", "=", "dataset", "[", "'labels'", "]", "\n", "\n", "keys", "=", "images", ".", "keys", "(", ")", "\n", "for", "key", "in", "keys", ":", "\n", "            ", "img", "=", "images", "[", "key", "]", "[", "(", ")", "]", "\n", "label", "=", "labels", "[", "key", "]", "[", "(", ")", "]", "\n", "\n", "seg_label", "=", "(", "label", ">", "0", ")", ".", "astype", "(", "np", ".", "uint8", ")", "*", "255", "\n", "\n", "# label = label * (250 / (label.max() + 1e-8))  # multi-label", "\n", "label", "=", "(", "label", ">", "0", ")", ".", "astype", "(", "np", ".", "uint8", ")", "*", "255", "# single label", "\n", "label", "=", "self", ".", "merge_skull", "(", "img", "[", ":", ",", ":", ",", "0", "]", ",", "label", ",", "default_skull_value", "=", "255", ")", "\n", "# label = label[:, :, None].repeat(3, axis=2)", "\n", "\n", "dcm_arr", ".", "append", "(", "img", ")", "\n", "label_arr", ".", "append", "(", "label", ")", "\n", "seg_label_arr", ".", "append", "(", "seg_label", ")", "\n", "\n", "", "return", "dcm_arr", ",", "label_arr", ",", "seg_label_arr", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.seg_in_skull": [[75, 79], ["None"], "methods", ["None"], ["", "def", "seg_in_skull", "(", "self", ",", "seg", ",", "mask", ")", ":", "\n", "# ndimage.binary_fill_holes(skull_mask)", "\n", "        ", "seg", "=", "mask", "*", "seg", "\n", "return", "seg", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.merge_skull": [[80, 93], ["scipy.binary_fill_holes", "cv2.Laplacian", "cv2.Laplacian.astype"], "methods", ["None"], ["", "def", "merge_skull", "(", "self", ",", "skull_mask", ",", "slice_label", ",", "default_skull_value", "=", "5", ")", ":", "\n", "# Add skull structure into label", "\n", "        ", "skull_mask", "=", "ndimage", ".", "binary_fill_holes", "(", "skull_mask", ")", "\n", "skull_mask", "=", "cv2", ".", "Laplacian", "(", "skull_mask", ".", "astype", "(", "\"uint8\"", ")", ",", "cv2", ".", "CV_8U", ")", "\n", "skull_mask", "[", "skull_mask", ">", "0", "]", "=", "default_skull_value", "\n", "slice_label", "=", "slice_label", "+", "skull_mask", "\n", "\n", "# slice_label = slice_label * (255 / (slice_label.max() + 1e-8))", "\n", "# slice_label = slice_label.astype(\"uint8\")", "\n", "\n", "# slice_label[slice_label > 0] = 255", "\n", "\n", "return", "slice_label", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.__getitem__": [[94, 129], ["PIL.Image.fromarray().convert", "PIL.Image.fromarray().convert", "PIL.Image.fromarray().convert", "data.base_dataset.get_params", "data.base_dataset.get_transform", "data.base_dataset.get_transform", "data.base_dataset.get_transform", "data.base_dataset.get_transform.", "data.base_dataset.get_transform.", "data.base_dataset.get_transform.", "bratsT1_dataset.BratsT1Dataset.opt.phase.lower", "str", "str", "PIL.Image.fromarray", "PIL.Image.fromarray", "PIL.Image.fromarray"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_params", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return a data point and its metadata information.\n\n        Parameters:\n            index - - a random integer for data indexing\n\n        Returns a dictionary that contains A, B, A_paths and B_paths\n            A (tensor) - - an image in the input domain\n            B (tensor) - - its corresponding image in the target domain\n            A_paths (str) - - image paths\n            B_paths (str) - - image paths (same as A_paths)\n        \"\"\"", "\n", "A", "=", "self", ".", "label", "[", "index", "]", "\n", "B", "=", "self", ".", "dcm", "[", "index", "]", "\n", "seg", "=", "self", ".", "seg_label", "[", "index", "]", "\n", "A", "=", "Image", ".", "fromarray", "(", "A", ")", ".", "convert", "(", "'RGB'", ")", "\n", "B", "=", "Image", ".", "fromarray", "(", "B", ")", ".", "convert", "(", "'RGB'", ")", "\n", "seg", "=", "Image", ".", "fromarray", "(", "seg", ")", ".", "convert", "(", "'RGB'", ")", "\n", "\n", "# apply the same transform to both A and B", "\n", "transform_params", "=", "get_params", "(", "self", ".", "opt", ",", "A", ".", "size", ")", "\n", "if", "self", ".", "opt", ".", "phase", ".", "lower", "(", ")", "!=", "'train'", ":", "\n", "            ", "transform_params", "[", "'crop_pos'", "]", "=", "(", "0", ",", "0", ")", "\n", "transform_params", "[", "'vflip'", "]", "=", "False", "\n", "transform_params", "[", "'hflip'", "]", "=", "False", "\n", "\n", "", "A_transform", "=", "get_transform", "(", "self", ".", "opt", ",", "transform_params", ",", "grayscale", "=", "(", "self", ".", "input_nc", "==", "1", ")", ",", "method", "=", "Image", ".", "NEAREST", ")", "\n", "B_transform", "=", "get_transform", "(", "self", ".", "opt", ",", "transform_params", ",", "grayscale", "=", "(", "self", ".", "output_nc", "==", "1", ")", ")", "\n", "seg_transform", "=", "get_transform", "(", "self", ".", "opt", ",", "transform_params", ",", "grayscale", "=", "(", "self", ".", "input_nc", "==", "1", ")", ",", "method", "=", "Image", ".", "NEAREST", ")", "\n", "\n", "A", "=", "A_transform", "(", "A", ")", "\n", "B", "=", "B_transform", "(", "B", ")", "\n", "seg", "=", "seg_transform", "(", "seg", ")", "\n", "\n", "return", "{", "'A'", ":", "A", ",", "'B'", ":", "B", ",", "'A_paths'", ":", "str", "(", "index", ")", ",", "'B_paths'", ":", "str", "(", "index", ")", ",", "'Seg_label'", ":", "seg", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.bratsT1_dataset.BratsT1Dataset.__len__": [[130, 133], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the total number of images in the dataset.\"\"\"", "\n", "return", "len", "(", "self", ".", "dcm", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.BaseDataset.__init__": [[23, 31], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize the class; save the options in the class\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        \"\"\"", "\n", "self", ".", "opt", "=", "opt", "\n", "self", ".", "root", "=", "opt", ".", "dataroot", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.BaseDataset.modify_commandline_options": [[32, 44], ["None"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "modify_commandline_options", "(", "parser", ",", "is_train", ")", ":", "\n", "        ", "\"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        \"\"\"", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.BaseDataset.__len__": [[45, 49], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the total number of images in the dataset.\"\"\"", "\n", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.BaseDataset.__getitem__": [[50, 61], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return a data point and its metadata information.\n\n        Parameters:\n            index - - a random integer for data indexing\n\n        Returns:\n            a dictionary of data with their names. It ususally contains the data itself and its metadata information.\n        \"\"\"", "\n", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_params": [[63, 80], ["random.randint", "random.randint", "numpy.maximum", "numpy.maximum", "random.random", "random.random"], "function", ["None"], ["", "", "def", "get_params", "(", "opt", ",", "size", ")", ":", "\n", "    ", "w", ",", "h", "=", "size", "\n", "new_h", "=", "h", "\n", "new_w", "=", "w", "\n", "if", "opt", ".", "preprocess", "==", "'resize_and_crop'", ":", "\n", "        ", "new_h", "=", "new_w", "=", "opt", ".", "load_size", "\n", "", "elif", "opt", ".", "preprocess", "==", "'scale_width_and_crop'", ":", "\n", "        ", "new_w", "=", "opt", ".", "load_size", "\n", "new_h", "=", "opt", ".", "load_size", "*", "h", "//", "w", "\n", "\n", "", "x", "=", "random", ".", "randint", "(", "0", ",", "np", ".", "maximum", "(", "0", ",", "new_w", "-", "opt", ".", "crop_size", ")", ")", "\n", "y", "=", "random", ".", "randint", "(", "0", ",", "np", ".", "maximum", "(", "0", ",", "new_h", "-", "opt", ".", "crop_size", ")", ")", "\n", "\n", "vflip", "=", "random", ".", "random", "(", ")", ">", "0.5", "\n", "hflip", "=", "random", ".", "random", "(", ")", ">", "0.5", "\n", "\n", "return", "{", "'crop_pos'", ":", "(", "x", ",", "y", ")", ",", "'vflip'", ":", "vflip", ",", "'hflip'", ":", "hflip", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform": [[82, 117], ["torchvision.Compose", "transform_list.append", "transform_list.append", "transform_list.append", "torchvision.Grayscale", "torchvision.Resize", "transform_list.append", "transform_list.append", "transform_list.append", "torchvision.Lambda", "transform_list.append", "torchvision.ToTensor", "torchvision.Lambda", "torchvision.RandomCrop", "torchvision.Lambda", "torchvision.RandomHorizontalFlip", "transform_list.append", "transform_list.append", "torchvision.Normalize", "torchvision.Normalize", "base_dataset.__make_power_2", "torchvision.Lambda", "torchvision.Lambda", "base_dataset.__scale_width", "base_dataset.__crop", "base_dataset.__hflip", "base_dataset.__vflip"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__make_power_2", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__scale_width", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__crop", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__hflip", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__vflip"], ["", "def", "get_transform", "(", "opt", ",", "params", "=", "None", ",", "grayscale", "=", "False", ",", "method", "=", "Image", ".", "BICUBIC", ",", "convert", "=", "True", ")", ":", "\n", "    ", "transform_list", "=", "[", "]", "\n", "if", "grayscale", ":", "\n", "        ", "transform_list", ".", "append", "(", "transforms", ".", "Grayscale", "(", "1", ")", ")", "\n", "", "if", "'resize'", "in", "opt", ".", "preprocess", ":", "\n", "        ", "osize", "=", "[", "opt", ".", "load_size", ",", "opt", ".", "load_size", "]", "\n", "transform_list", ".", "append", "(", "transforms", ".", "Resize", "(", "osize", ",", "method", ")", ")", "\n", "", "elif", "'scale_width'", "in", "opt", ".", "preprocess", ":", "\n", "        ", "transform_list", ".", "append", "(", "transforms", ".", "Lambda", "(", "lambda", "img", ":", "__scale_width", "(", "img", ",", "opt", ".", "load_size", ",", "method", ")", ")", ")", "\n", "\n", "", "if", "'crop'", "in", "opt", ".", "preprocess", ":", "\n", "        ", "if", "params", "is", "None", ":", "\n", "            ", "transform_list", ".", "append", "(", "transforms", ".", "RandomCrop", "(", "opt", ".", "crop_size", ")", ")", "\n", "", "else", ":", "\n", "            ", "transform_list", ".", "append", "(", "transforms", ".", "Lambda", "(", "lambda", "img", ":", "__crop", "(", "img", ",", "params", "[", "'crop_pos'", "]", ",", "opt", ".", "crop_size", ")", ")", ")", "\n", "\n", "", "", "if", "opt", ".", "preprocess", "==", "'none'", ":", "\n", "        ", "transform_list", ".", "append", "(", "transforms", ".", "Lambda", "(", "lambda", "img", ":", "__make_power_2", "(", "img", ",", "base", "=", "4", ",", "method", "=", "method", ")", ")", ")", "\n", "\n", "", "if", "not", "opt", ".", "no_flip", ":", "\n", "        ", "if", "params", "is", "None", ":", "\n", "            ", "transform_list", ".", "append", "(", "transforms", ".", "RandomHorizontalFlip", "(", ")", ")", "\n", "", "else", ":", "\n", "            ", "if", "params", "[", "'hflip'", "]", ":", "\n", "                ", "transform_list", ".", "append", "(", "transforms", ".", "Lambda", "(", "lambda", "img", ":", "__hflip", "(", "img", ",", "params", "[", "'hflip'", "]", ")", ")", ")", "\n", "", "if", "params", "[", "'vflip'", "]", ":", "\n", "                ", "transform_list", ".", "append", "(", "transforms", ".", "Lambda", "(", "lambda", "img", ":", "__vflip", "(", "img", ",", "params", "[", "'vflip'", "]", ")", ")", ")", "\n", "\n", "", "", "", "if", "convert", ":", "\n", "        ", "transform_list", "+=", "[", "transforms", ".", "ToTensor", "(", ")", "]", "\n", "if", "grayscale", ":", "\n", "            ", "transform_list", "+=", "[", "transforms", ".", "Normalize", "(", "(", "0.5", ",", ")", ",", "(", "0.5", ",", ")", ")", "]", "\n", "", "else", ":", "\n", "            ", "transform_list", "+=", "[", "transforms", ".", "Normalize", "(", "(", "0.5", ",", "0.5", ",", "0.5", ")", ",", "(", "0.5", ",", "0.5", ",", "0.5", ")", ")", "]", "\n", "", "", "return", "transforms", ".", "Compose", "(", "transform_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__make_power_2": [[119, 128], ["int", "int", "base_dataset.__print_size_warning", "img.resize", "round", "round"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__print_size_warning"], ["", "def", "__make_power_2", "(", "img", ",", "base", ",", "method", "=", "Image", ".", "BICUBIC", ")", ":", "\n", "    ", "ow", ",", "oh", "=", "img", ".", "size", "\n", "h", "=", "int", "(", "round", "(", "oh", "/", "base", ")", "*", "base", ")", "\n", "w", "=", "int", "(", "round", "(", "ow", "/", "base", ")", "*", "base", ")", "\n", "if", "(", "h", "==", "oh", ")", "and", "(", "w", "==", "ow", ")", ":", "\n", "        ", "return", "img", "\n", "\n", "", "__print_size_warning", "(", "ow", ",", "oh", ",", "w", ",", "h", ")", "\n", "return", "img", ".", "resize", "(", "(", "w", ",", "h", ")", ",", "method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__scale_width": [[130, 137], ["int", "img.resize"], "function", ["None"], ["", "def", "__scale_width", "(", "img", ",", "target_width", ",", "method", "=", "Image", ".", "BICUBIC", ")", ":", "\n", "    ", "ow", ",", "oh", "=", "img", ".", "size", "\n", "if", "(", "ow", "==", "target_width", ")", ":", "\n", "        ", "return", "img", "\n", "", "w", "=", "target_width", "\n", "h", "=", "int", "(", "target_width", "*", "oh", "/", "ow", ")", "\n", "return", "img", ".", "resize", "(", "(", "w", ",", "h", ")", ",", "method", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__crop": [[139, 146], ["img.crop"], "function", ["None"], ["", "def", "__crop", "(", "img", ",", "pos", ",", "size", ")", ":", "\n", "    ", "ow", ",", "oh", "=", "img", ".", "size", "\n", "x1", ",", "y1", "=", "pos", "\n", "tw", "=", "th", "=", "size", "\n", "if", "(", "ow", ">", "tw", "or", "oh", ">", "th", ")", ":", "\n", "        ", "return", "img", ".", "crop", "(", "(", "x1", ",", "y1", ",", "x1", "+", "tw", ",", "y1", "+", "th", ")", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__hflip": [[148, 152], ["img.transpose"], "function", ["None"], ["", "def", "__hflip", "(", "img", ",", "flip", ")", ":", "\n", "    ", "if", "flip", ":", "\n", "        ", "return", "img", ".", "transpose", "(", "Image", ".", "FLIP_LEFT_RIGHT", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__vflip": [[153, 157], ["img.transpose"], "function", ["None"], ["", "def", "__vflip", "(", "img", ",", "flip", ")", ":", "\n", "    ", "if", "flip", ":", "\n", "        ", "return", "img", ".", "transpose", "(", "Image", ".", "FLIP_TOP_BOTTOM", ")", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.__print_size_warning": [[159, 167], ["hasattr", "print"], "function", ["None"], ["", "def", "__print_size_warning", "(", "ow", ",", "oh", ",", "w", ",", "h", ")", ":", "\n", "    ", "\"\"\"Print warning information about image size(only print once)\"\"\"", "\n", "if", "not", "hasattr", "(", "__print_size_warning", ",", "'has_printed'", ")", ":", "\n", "        ", "print", "(", "\"The image size needs to be a multiple of 4. \"", "\n", "\"The loaded image size was (%d, %d), so it was adjusted to \"", "\n", "\"(%d, %d). This adjustment will be done to all images \"", "\n", "\"whose sizes are not multiples of 4\"", "%", "(", "ow", ",", "oh", ",", "w", ",", "h", ")", ")", "\n", "__print_size_warning", ".", "has_printed", "=", "True", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brain_split_dataset.BrainSplitDataset.__init__": [[17, 21], ["data.nuclei_dataset.NucleiDataset", "data.brats_dataset.BratsDataset", "data.brats_dataset.BratsDataset"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "split_db", "=", "[", "NucleiDataset", "(", "opt", ",", "4", ")", ",", "\n", "BratsDataset", "(", "opt", ",", "1", ")", ",", "\n", "BratsDataset", "(", "opt", ",", "2", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brain_split_dataset.BrainSplitDataset.__getitem__": [[22, 37], ["enumerate", "len", "len", "str", "str", "str", "str", "str"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "\n", "        ", "result", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "enumerate", "(", "self", ".", "split_db", ")", ":", "\n", "            ", "database", "=", "v", "\n", "if", "index", ">=", "len", "(", "database", ")", ":", "\n", "                ", "index", "=", "index", "%", "len", "(", "database", ")", "\n", "\n", "", "index_value", "=", "database", "[", "index", "]", "\n", "result", "[", "'A_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'A'", "]", "\n", "result", "[", "'B_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'B'", "]", "\n", "result", "[", "'A_paths_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'A_paths'", "]", "\n", "result", "[", "'B_paths_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'B_paths'", "]", "\n", "result", "[", "'Seg_label_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'Seg_label'", "]", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.brain_split_dataset.BrainSplitDataset.__len__": [[38, 46], ["len", "len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the total number of images in the dataset.\"\"\"", "\n", "length", "=", "0", "\n", "for", "i", "in", "self", ".", "split_db", ":", "\n", "            ", "if", "len", "(", "i", ")", ">", "length", ":", "\n", "                ", "length", "=", "len", "(", "i", ")", "\n", "\n", "", "", "return", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.template_dataset.TemplateDataset.modify_commandline_options": [[21, 35], ["parser.add_argument", "parser.set_defaults"], "methods", ["None"], ["@", "staticmethod", "\n", "def", "modify_commandline_options", "(", "parser", ",", "is_train", ")", ":", "\n", "        ", "\"\"\"Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        \"\"\"", "\n", "parser", ".", "add_argument", "(", "'--new_dataset_option'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'new dataset option'", ")", "\n", "parser", ".", "set_defaults", "(", "max_dataset_size", "=", "10", ",", "new_dataset_option", "=", "2.0", ")", "# specify dataset-specific default values", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.template_dataset.TemplateDataset.__init__": [[36, 53], ["data.base_dataset.BaseDataset.__init__", "data.base_dataset.get_transform"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.base_dataset.get_transform"], ["", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Initialize this dataset class.\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n\n        A few things can be done here.\n        - save the options (have been done in BaseDataset)\n        - get image paths and meta information of the dataset.\n        - define the image transformation.\n        \"\"\"", "\n", "# save the option and dataset root", "\n", "BaseDataset", ".", "__init__", "(", "self", ",", "opt", ")", "\n", "# get the image paths of your dataset;", "\n", "self", ".", "image_paths", "=", "[", "]", "# You can call sorted(make_dataset(self.root, opt.max_dataset_size)) to get all the image paths under the directory self.root", "\n", "# define the default transform function. You can use <base_dataset.get_transform>; You can also define your custom transform function", "\n", "self", ".", "transform", "=", "get_transform", "(", "opt", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.template_dataset.TemplateDataset.__getitem__": [[54, 72], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"Return a data point and its metadata information.\n\n        Parameters:\n            index -- a random integer for data indexing\n\n        Returns:\n            a dictionary of data with their names. It usually contains the data itself and its metadata information.\n\n        Step 1: get a random image path: e.g., path = self.image_paths[index]\n        Step 2: load your data from the disk: e.g., image = Image.open(path).convert('RGB').\n        Step 3: convert your data to a PyTorch tensor. You can use helpder functions such as self.transform. e.g., data = self.transform(image)\n        Step 4: return a data point as a dictionary.\n        \"\"\"", "\n", "path", "=", "'temp'", "# needs to be a string", "\n", "data_A", "=", "None", "# needs to be a tensor", "\n", "data_B", "=", "None", "# needs to be a tensor", "\n", "return", "{", "'data_A'", ":", "data_A", ",", "'data_B'", ":", "data_B", ",", "'path'", ":", "path", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.template_dataset.TemplateDataset.__len__": [[73, 76], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the total number of images.\"\"\"", "\n", "return", "len", "(", "self", ".", "image_paths", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_joint_dataset.NucleiJointDataset.__init__": [[8, 12], ["range", "nuclei_joint_dataset.NucleiJointDataset.split_db.append", "data.nuclei_dataset.NucleiDataset"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "split_db", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "opt", ".", "task_num", ")", ":", "\n", "            ", "self", ".", "split_db", ".", "append", "(", "NucleiDataset", "(", "opt", ",", "i", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_joint_dataset.NucleiJointDataset.__getitem__": [[13, 29], ["enumerate", "len", "len"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "result", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "enumerate", "(", "self", ".", "split_db", ")", ":", "\n", "            ", "database", "=", "v", "\n", "if", "index", ">=", "len", "(", "database", ")", ":", "# go to next dataset", "\n", "                ", "index", "=", "index", "-", "len", "(", "database", ")", "\n", "", "else", ":", "\n", "                ", "index_value", "=", "database", "[", "index", "]", "\n", "result", "[", "'A'", "]", "=", "index_value", "[", "'A'", "]", "\n", "result", "[", "'B'", "]", "=", "index_value", "[", "'B'", "]", "\n", "result", "[", "'A_paths'", "]", "=", "index_value", "[", "'A_paths'", "]", "\n", "result", "[", "'B_paths'", "]", "=", "index_value", "[", "'B_paths'", "]", "\n", "result", "[", "'label_ternary'", "]", "=", "index_value", "[", "'label_ternary'", "]", "\n", "result", "[", "'weight_map'", "]", "=", "index_value", "[", "'weight_map'", "]", "\n", "break", "\n", "", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_joint_dataset.NucleiJointDataset.__len__": [[30, 37], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the total number of images in the dataset.\"\"\"", "\n", "length", "=", "0", "\n", "for", "i", "in", "self", ".", "split_db", ":", "\n", "            ", "length", "+=", "len", "(", "i", ")", "\n", "\n", "", "return", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.CustomDatasetDataLoader.__init__": [[65, 80], ["__init__.find_dataset_using_name", "find_dataset_using_name.", "print", "torch.utils.data.DataLoader", "int", "type"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.find_dataset_using_name"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.CustomDatasetDataLoader.load_data": [[81, 83], ["None"], "methods", ["None"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.CustomDatasetDataLoader.__len__": [[84, 87], ["min", "len"], "methods", ["None"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.CustomDatasetDataLoader.__iter__": [[88, 94], ["enumerate"], "methods", ["None"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.find_dataset_using_name": [[18, 39], ["importlib.import_module", "importlib.import_module.__dict__.items", "dataset_name.replace", "NotImplementedError", "issubclass", "name.lower", "target_dataset_name.lower"], "function", ["None"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.get_option_setter": [[41, 45], ["__init__.find_dataset_using_name"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.find_dataset_using_name"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset": [[47, 60], ["__init__.CustomDatasetDataLoader", "__init__.CustomDatasetDataLoader.load_data"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.CustomDatasetDataLoader.load_data"], []], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_split_dataset.NucleiSplitDataset.__init__": [[8, 12], ["range", "nuclei_split_dataset.NucleiSplitDataset.split_db.append", "data.nuclei_dataset.NucleiDataset"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "opt", ")", ":", "\n", "        ", "self", ".", "split_db", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "4", ")", ":", "\n", "            ", "self", ".", "split_db", ".", "append", "(", "NucleiDataset", "(", "opt", ",", "i", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_split_dataset.NucleiSplitDataset.__getitem__": [[13, 29], ["enumerate", "len", "len", "str", "str", "str", "str", "str", "str"], "methods", ["None"], ["", "", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "result", "=", "{", "}", "\n", "for", "k", ",", "v", "in", "enumerate", "(", "self", ".", "split_db", ")", ":", "\n", "            ", "database", "=", "v", "\n", "if", "index", ">=", "len", "(", "database", ")", ":", "\n", "                ", "index", "=", "index", "%", "len", "(", "database", ")", "\n", "\n", "", "index_value", "=", "database", "[", "index", "]", "\n", "result", "[", "'A_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'A'", "]", "\n", "result", "[", "'B_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'B'", "]", "\n", "result", "[", "'A_paths_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'A_paths'", "]", "\n", "result", "[", "'B_paths_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'B_paths'", "]", "\n", "result", "[", "'label_ternary_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'label_ternary'", "]", "\n", "result", "[", "'weight_map_'", "+", "str", "(", "k", ")", "]", "=", "index_value", "[", "'weight_map'", "]", "\n", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.nuclei_split_dataset.NucleiSplitDataset.__len__": [[30, 38], ["len", "len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Return the total number of images in the dataset.\"\"\"", "\n", "length", "=", "0", "\n", "for", "i", "in", "self", ".", "split_db", ":", "\n", "            ", "if", "len", "(", "i", ")", ">", "length", ":", "\n", "                ", "length", "=", "len", "(", "i", ")", "\n", "\n", "", "", "return", "length", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build.build_h5": [[11, 44], ["h5py.File", "print", "nuclei_dataset_build._dump_training_files", "h5py.File.close", "open", "json.load"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build._dump_training_files"], ["def", "build_h5", "(", "data_dir", ",", "save_dir", ")", ":", "\n", "    ", "with", "open", "(", "'{:s}/train_test.json'", ".", "format", "(", "data_dir", ")", ",", "'r'", ")", "as", "file", ":", "\n", "        ", "data_list", "=", "json", ".", "load", "(", "file", ")", "\n", "train_list", ",", "test_list", ",", "test2_list", "=", "data_list", "[", "'train'", "]", ",", "data_list", "[", "'testA'", "]", ",", "data_list", "[", "'testB'", "]", "\n", "\n", "", "train_file_all", "=", "h5py", ".", "File", "(", "'{:s}/train_nulei.h5'", ".", "format", "(", "save_dir", ")", ",", "\"w\"", ")", "\n", "# test_file = h5py.File('{:s}/test_nuclei.h5'.format(save_dir), \"w\")", "\n", "\n", "# train imgs are 250 x 250 resolution extracted from original images", "\n", "print", "(", "'Processing all training files'", ")", "\n", "_dump_training_files", "(", "train_file_all", ",", "data_dir", ",", "train_list", ")", "\n", "\n", "# # test imgs are original 1000 x 1000 resolution", "\n", "# print('Processing test files')", "\n", "# for img_name in tqdm(test_list):", "\n", "#     name = img_name.split('.')[0]", "\n", "#     # images", "\n", "#     img = io.imread('{:s}/original_images/{:s}.png'.format(data_dir, name))", "\n", "#     test_file.create_dataset('images/{:s}'.format(name), data=img)", "\n", "#     # labels", "\n", "#     label = io.imread('{:s}/labels/{:s}.png'.format(data_dir, name))", "\n", "#     test_file.create_dataset('labels/{:s}'.format(name), data=label)", "\n", "#     # ternary labels (for segmentation)", "\n", "#     label_ternary = io.imread('{:s}/labels_ternary/{:s}_label.png'.format(data_dir, name))", "\n", "#     test_file.create_dataset('labels_ternary/{:s}'.format(name), data=label_ternary)", "\n", "#     # weight maps (for segmentation)", "\n", "#     weight_map = io.imread('{:s}/weight_maps/{:s}_weight.png'.format(data_dir, name))", "\n", "#     test_file.create_dataset('weight_maps/{:s}'.format(name), data=weight_map)", "\n", "#     # instance labels (for segmentation)", "\n", "#     label_instance = io.imread('{:s}/labels_instance/{:s}.png'.format(data_dir, name))", "\n", "#     test_file.create_dataset('labels_instance/{:s}'.format(name), data=label_instance)", "\n", "\n", "train_file_all", ".", "close", "(", ")", "\n", "# test_file.close()", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build.build_h5_diff_organ": [[47, 73], ["os.makedirs", "h5py.File", "h5py.File", "h5py.File", "h5py.File", "print", "nuclei_dataset_build._dump_training_files", "nuclei_dataset_build._dump_training_files", "nuclei_dataset_build._dump_training_files", "nuclei_dataset_build._dump_training_files", "h5py.File.close", "h5py.File.close", "h5py.File.close", "h5py.File.close", "open", "json.load"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build._dump_training_files", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build._dump_training_files", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build._dump_training_files", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build._dump_training_files"], ["", "def", "build_h5_diff_organ", "(", "data_dir", ",", "save_dir", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "with", "open", "(", "'{:s}/train_test.json'", ".", "format", "(", "data_dir", ")", ",", "'r'", ")", "as", "file", ":", "\n", "        ", "data_list", "=", "json", ".", "load", "(", "file", ")", "\n", "train_list", ",", "test_list", ",", "test2_list", "=", "data_list", "[", "'train'", "]", ",", "data_list", "[", "'testA'", "]", ",", "data_list", "[", "'testB'", "]", "\n", "\n", "", "train_file_D1", "=", "h5py", ".", "File", "(", "'{:s}/train_liver.h5'", ".", "format", "(", "save_dir", ")", ",", "\"w\"", ")", "\n", "train_file_D2", "=", "h5py", ".", "File", "(", "'{:s}/train_breast.h5'", ".", "format", "(", "save_dir", ")", ",", "\"w\"", ")", "\n", "train_file_D3", "=", "h5py", ".", "File", "(", "'{:s}/train_kidney.h5'", ".", "format", "(", "save_dir", ")", ",", "\"w\"", ")", "\n", "train_file_D4", "=", "h5py", ".", "File", "(", "'{:s}/train_prostate.h5'", ".", "format", "(", "save_dir", ")", ",", "\"w\"", ")", "\n", "\n", "liver_file_list", "=", "[", "filename", "for", "filename", "in", "train_list", "if", "'Liver'", "in", "filename", "]", "\n", "breast_file_list", "=", "[", "filename", "for", "filename", "in", "train_list", "if", "'Breast'", "in", "filename", "]", "\n", "kidney_file_list", "=", "[", "filename", "for", "filename", "in", "train_list", "if", "'Kidney'", "in", "filename", "]", "\n", "prostate_file_list", "=", "[", "filename", "for", "filename", "in", "train_list", "if", "'Prostate'", "in", "filename", "]", "\n", "\n", "print", "(", "'Processing subset training files'", ")", "\n", "_dump_training_files", "(", "train_file_D1", ",", "data_dir", ",", "liver_file_list", ")", "\n", "_dump_training_files", "(", "train_file_D2", ",", "data_dir", ",", "breast_file_list", ")", "\n", "_dump_training_files", "(", "train_file_D3", ",", "data_dir", ",", "kidney_file_list", ")", "\n", "_dump_training_files", "(", "train_file_D4", ",", "data_dir", ",", "prostate_file_list", ")", "\n", "\n", "train_file_D1", ".", "close", "(", ")", "\n", "train_file_D2", ".", "close", "(", ")", "\n", "train_file_D3", ".", "close", "(", ")", "\n", "train_file_D4", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build._dump_training_files": [[75, 91], ["tqdm.tqdm", "range", "img_name.split", "skimage.io.imread", "h5_file.create_dataset", "skimage.io.imread", "h5_file.create_dataset", "skimage.io.imread", "h5_file.create_dataset", "skimage.io.imread", "h5_file.create_dataset"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset"], ["", "def", "_dump_training_files", "(", "h5_file", ",", "data_dir", ",", "img_names", ")", ":", "\n", "    ", "for", "img_name", "in", "tqdm", "(", "img_names", ")", ":", "\n", "        ", "name", "=", "img_name", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "for", "i", "in", "range", "(", "16", ")", ":", "# 16 patches for each large image", "\n", "# images", "\n", "            ", "img", "=", "io", ".", "imread", "(", "'{:s}/patches_256/original_images/{:s}_{:d}.png'", ".", "format", "(", "data_dir", ",", "name", ",", "i", ")", ")", "\n", "h5_file", ".", "create_dataset", "(", "'images/{:s}_{:d}'", ".", "format", "(", "name", ",", "i", ")", ",", "data", "=", "img", ")", "\n", "# labels", "\n", "label", "=", "io", ".", "imread", "(", "'{:s}/patches_256/labels/{:s}_{:d}.png'", ".", "format", "(", "data_dir", ",", "name", ",", "i", ")", ")", "\n", "h5_file", ".", "create_dataset", "(", "'labels/{:s}_{:d}'", ".", "format", "(", "name", ",", "i", ")", ",", "data", "=", "label", ")", "\n", "# ternary labels (for segmentation)", "\n", "label_ternary", "=", "io", ".", "imread", "(", "'{:s}/patches_256/labels_ternary/{:s}_{:d}_label.png'", ".", "format", "(", "data_dir", ",", "name", ",", "i", ")", ")", "\n", "h5_file", ".", "create_dataset", "(", "'labels_ternary/{:s}_{:d}'", ".", "format", "(", "name", ",", "i", ")", ",", "data", "=", "label_ternary", ")", "\n", "# weight maps (for segmentation)", "\n", "weight_map", "=", "io", ".", "imread", "(", "'{:s}/patches_256/weight_maps/{:s}_{:d}_weight.png'", ".", "format", "(", "data_dir", ",", "name", ",", "i", ")", ")", "\n", "h5_file", ".", "create_dataset", "(", "'weight_maps/{:s}_{:d}'", ".", "format", "(", "name", ",", "i", ")", ",", "data", "=", "weight_map", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build.split_patches": [[93, 125], ["os.makedirs", "os.listdir", "os.path.join", "skimage.io.imread", "math.ceil", "math.ceil", "range", "range", "image_name.split", "range", "len", "seg_imgs.append", "skimage.io.imsave", "skimage.io.imsave", "len", "len", "len"], "function", ["None"], ["", "", "", "def", "split_patches", "(", "data_dir", ",", "save_dir", ",", "post_fix", "=", "None", ")", ":", "\n", "    ", "import", "math", "\n", "\"\"\" split large image into small patches \"\"\"", "\n", "os", ".", "makedirs", "(", "save_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "image_list", "=", "os", ".", "listdir", "(", "data_dir", ")", "\n", "for", "image_name", "in", "image_list", ":", "\n", "        ", "name", "=", "image_name", ".", "split", "(", "'.'", ")", "[", "0", "]", "\n", "if", "post_fix", "and", "name", "[", "-", "len", "(", "post_fix", ")", ":", "]", "!=", "post_fix", ":", "\n", "            ", "continue", "\n", "", "image_path", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "image_name", ")", "\n", "image", "=", "io", ".", "imread", "(", "image_path", ")", "\n", "seg_imgs", "=", "[", "]", "\n", "\n", "# split into 16 patches of size 250x250", "\n", "h", ",", "w", "=", "image", ".", "shape", "[", "0", "]", ",", "image", ".", "shape", "[", "1", "]", "\n", "patch_size", "=", "256", "\n", "h_overlap", "=", "math", ".", "ceil", "(", "(", "4", "*", "patch_size", "-", "h", ")", "/", "3", ")", "\n", "w_overlap", "=", "math", ".", "ceil", "(", "(", "4", "*", "patch_size", "-", "w", ")", "/", "3", ")", "\n", "for", "i", "in", "range", "(", "0", ",", "h", "-", "patch_size", "+", "1", ",", "patch_size", "-", "h_overlap", ")", ":", "\n", "            ", "for", "j", "in", "range", "(", "0", ",", "w", "-", "patch_size", "+", "1", ",", "patch_size", "-", "w_overlap", ")", ":", "\n", "                ", "if", "len", "(", "image", ".", "shape", ")", "==", "3", ":", "\n", "                    ", "patch", "=", "image", "[", "i", ":", "i", "+", "patch_size", ",", "j", ":", "j", "+", "patch_size", ",", ":", "]", "\n", "", "else", ":", "\n", "                    ", "patch", "=", "image", "[", "i", ":", "i", "+", "patch_size", ",", "j", ":", "j", "+", "patch_size", "]", "\n", "", "seg_imgs", ".", "append", "(", "patch", ")", "\n", "\n", "", "", "for", "k", "in", "range", "(", "len", "(", "seg_imgs", ")", ")", ":", "\n", "            ", "if", "post_fix", ":", "\n", "                ", "io", ".", "imsave", "(", "'{:s}/{:s}_{:d}_{:s}.png'", ".", "format", "(", "save_dir", ",", "name", "[", ":", "-", "len", "(", "post_fix", ")", "-", "1", "]", ",", "k", ",", "post_fix", ")", ",", "seg_imgs", "[", "k", "]", ")", "\n", "", "else", ":", "\n", "                ", "io", ".", "imsave", "(", "'{:s}/{:s}_{:d}.png'", ".", "format", "(", "save_dir", ",", "name", ",", "k", ")", ",", "seg_imgs", "[", "k", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.nuclei_dataset_build.extract_imgs": [[127, 145], ["h5py.File", "h5py.File.keys", "os.makedirs", "print", "list", "skimage.io.imsave", "h5_file[].keys", "numpy.count_nonzero"], "function", ["None"], ["", "", "", "", "def", "extract_imgs", "(", "h5_file_path", ",", "save_path", ")", ":", "\n", "    ", "h5_file", "=", "h5py", ".", "File", "(", "h5_file_path", ",", "'r'", ")", "\n", "for", "key", "in", "h5_file", ".", "keys", "(", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "'{:s}/{:s}'", ".", "format", "(", "save_path", ",", "key", ")", ",", "exist_ok", "=", "True", ")", "\n", "count", "=", "0", "\n", "for", "file_key", "in", "list", "(", "h5_file", "[", "key", "]", ".", "keys", "(", ")", ")", "[", ":", "1000", "]", ":", "\n", "            ", "img", "=", "h5_file", "[", "'{:s}/{:s}'", ".", "format", "(", "key", ",", "file_key", ")", "]", "[", "(", ")", "]", "\n", "# if key == 'images':", "\n", "#     img = (img + 1) / 2.0 * 255", "\n", "#     img = img.astype(np.uint8)", "\n", "# if len(img.shape) > 2:", "\n", "#     img = np.swapaxes(np.swapaxes(img, 0, 1), 1, 2)", "\n", "if", "key", "==", "'labels'", ":", "\n", "                ", "img", "=", "(", "img", ">", "0", ")", ".", "astype", "(", "np", ".", "uint8", ")", "*", "255", "\n", "if", "np", ".", "count_nonzero", "(", "img", ")", "<", "10", ":", "\n", "                    ", "count", "+=", "1", "\n", "", "", "io", ".", "imsave", "(", "'{:s}/{:s}/{:s}.png'", ".", "format", "(", "save_path", ",", "key", ",", "file_key", ")", ",", "img", ")", "\n", "", "print", "(", "count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.brats_dataset_build.read_raw_data": [[12, 47], ["os.makedirs", "sorted", "h5py.File", "h5py.File.close", "os.listdir", "os.path.join", "print", "os.path.isdir", "os.path.join", "sys.stdout.flush", "nibabel.load().get_fdata", "nibabel.load().get_fdata", "nibabel.load().get_fdata", "nibabel.load().get_fdata", "nibabel.load().get_fdata", "h5py.File.create_dataset", "h5py.File.create_dataset", "h5py.File.create_dataset", "h5py.File.create_dataset", "h5py.File.create_dataset", "len", "nibabel.load", "nibabel.load", "nibabel.load", "nibabel.load", "nibabel.load", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset"], ["def", "read_raw_data", "(", "dcm_path", ",", "save_path", ",", "save_name", ",", "test_case_number", ")", ":", "\n", "    ", "\"\"\"\n    read images and labels from raw data of HGG cases, split the 210 cases into train (170) and test (40),\n    and save them in h5 file.\n\n    \"\"\"", "\n", "os", ".", "makedirs", "(", "save_path", ",", "exist_ok", "=", "True", ")", "\n", "dcm_folders", "=", "sorted", "(", "os", ".", "listdir", "(", "dcm_path", ")", ")", "\n", "result_file", "=", "h5py", ".", "File", "(", "os", ".", "path", ".", "join", "(", "save_path", ",", "save_name", ")", ",", "\"w\"", ")", "\n", "\n", "idx", "=", "0", "\n", "for", "folder", "in", "dcm_folders", ":", "\n", "        ", "print", "(", "'[{:d}/{:d}] Processing {:s}'", ".", "format", "(", "idx", "+", "1", ",", "len", "(", "dcm_folders", ")", ",", "folder", ")", ")", "\n", "if", "idx", "<", "test_case_number", ":", "\n", "            ", "type", "=", "\"test\"", "\n", "", "else", ":", "\n", "            ", "type", "=", "\"train\"", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "os", ".", "path", ".", "join", "(", "dcm_path", ",", "folder", ")", ")", ":", "\n", "            ", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "flair", "=", "nib", ".", "load", "(", "os", ".", "path", ".", "join", "(", "dcm_path", ",", "folder", ",", "f\"{folder}_flair.nii.gz\"", ")", ")", ".", "get_fdata", "(", ")", "\n", "seg", "=", "nib", ".", "load", "(", "os", ".", "path", ".", "join", "(", "dcm_path", ",", "folder", ",", "f\"{folder}_seg.nii.gz\"", ")", ")", ".", "get_fdata", "(", ")", "\n", "t1", "=", "nib", ".", "load", "(", "os", ".", "path", ".", "join", "(", "dcm_path", ",", "folder", ",", "f\"{folder}_t1.nii.gz\"", ")", ")", ".", "get_fdata", "(", ")", "\n", "t1ce", "=", "nib", ".", "load", "(", "os", ".", "path", ".", "join", "(", "dcm_path", ",", "folder", ",", "f\"{folder}_t1ce.nii.gz\"", ")", ")", ".", "get_fdata", "(", ")", "\n", "t2", "=", "nib", ".", "load", "(", "os", ".", "path", ".", "join", "(", "dcm_path", ",", "folder", ",", "f\"{folder}_t2.nii.gz\"", ")", ")", ".", "get_fdata", "(", ")", "\n", "\n", "result_file", ".", "create_dataset", "(", "f\"{type}/{idx}/flair\"", ",", "data", "=", "flair", ")", "\n", "db", "=", "result_file", ".", "create_dataset", "(", "f\"{type}/{idx}/seg\"", ",", "data", "=", "seg", ")", "\n", "result_file", ".", "create_dataset", "(", "f\"{type}/{idx}/t1\"", ",", "data", "=", "t1", ")", "\n", "result_file", ".", "create_dataset", "(", "f\"{type}/{idx}/t1ce\"", ",", "data", "=", "t1ce", ")", "\n", "result_file", ".", "create_dataset", "(", "f\"{type}/{idx}/t2\"", ",", "data", "=", "t2", ")", "\n", "\n", "db", ".", "attrs", "[", "'id'", "]", "=", "folder", "\n", "idx", "+=", "1", "\n", "\n", "", "", "result_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.brats_dataset_build.get_training_data": [[49, 82], ["h5py.File", "h5py.File", "tqdm.tqdm", "print", "h5py.File.close", "h5py.File.close", "range", "max", "min", "dataset.keys", "brats_dataset_build.get_training_data._range_idx"], "function", ["None"], ["", "def", "get_training_data", "(", "h5_filepath", ",", "save_filepath", ",", "modality", ")", ":", "\n", "    ", "def", "_range_idx", "(", "label", ",", "margin", ")", ":", "\n", "        ", "uniq", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "label", ".", "shape", "[", "2", "]", ")", ":", "\n", "            ", "if", "len", "(", "np", ".", "unique", "(", "label", "[", ":", ",", ":", ",", "i", "]", ")", ")", ">", "1", ":", "\n", "                ", "uniq", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "uniq", ".", "append", "(", "0", ")", "\n", "", "", "min_idx", "=", "max", "(", "0", ",", "uniq", ".", "index", "(", "1", ")", "-", "margin", ")", "\n", "upper_idx", "=", "len", "(", "uniq", ")", "-", "uniq", "[", ":", ":", "-", "1", "]", ".", "index", "(", "1", ")", "-", "1", "\n", "max_idx", "=", "min", "(", "len", "(", "uniq", ")", ",", "upper_idx", "+", "margin", ")", "\n", "return", "min_idx", ",", "max_idx", "\n", "\n", "", "original_file", "=", "h5py", ".", "File", "(", "h5_filepath", ",", "'r'", ")", "\n", "dataset", "=", "original_file", "[", "'train'", "]", "\n", "save_file", "=", "h5py", ".", "File", "(", "save_filepath", ",", "'w'", ")", "\n", "for", "key", "in", "tqdm", "(", "dataset", ".", "keys", "(", ")", ")", ":", "\n", "        ", "dcm", "=", "dataset", "[", "f\"{key}/{modality}\"", "]", "[", "(", ")", "]", "\n", "label", "=", "dataset", "[", "f\"{key}/seg\"", "]", "[", "(", ")", "]", "\n", "\n", "start", ",", "end", "=", "_range_idx", "(", "label", ",", "20", ")", "\n", "for", "i", "in", "range", "(", "start", ",", "end", ")", ":", "\n", "            ", "slice_dcm", "=", "dcm", "[", ":", ",", ":", ",", "i", "]", "\n", "slice_dcm", "=", "slice_dcm", "*", "(", "(", "pow", "(", "2", ",", "8", ")", "-", "1", ")", "/", "slice_dcm", ".", "max", "(", ")", ")", "\n", "slice_dcm", "=", "np", ".", "repeat", "(", "slice_dcm", "[", "...", ",", "np", ".", "newaxis", "]", ",", "3", ",", "axis", "=", "2", ")", "\n", "slice_dcm", "=", "slice_dcm", ".", "astype", "(", "'uint8'", ")", "\n", "slice_label", "=", "label", "[", ":", ",", ":", ",", "i", "]", ".", "astype", "(", "'uint8'", ")", "\n", "\n", "save_file", ".", "create_dataset", "(", "'images/{:s}_{:d}'", ".", "format", "(", "key", ",", "i", ")", ",", "data", "=", "slice_dcm", ")", "\n", "save_file", ".", "create_dataset", "(", "'labels/{:s}_{:d}'", ".", "format", "(", "key", ",", "i", ")", ",", "data", "=", "slice_label", ")", "\n", "", "", "print", "(", "'number of images:'", ",", "len", "(", "save_file", "[", "'images'", "]", ")", ")", "\n", "original_file", ".", "close", "(", ")", "\n", "save_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.brats_dataset_build.remove_empty_slices": [[84, 100], ["h5py.File", "h5py.File", "tqdm.tqdm", "print", "print", "h5py.File.close", "h5py.File.close", "ori_h5[].keys", "h5py.File.create_dataset", "h5py.File.create_dataset", "len", "len", "numpy.count_nonzero", "ori_h5[].keys", "new_h5[].keys"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset"], ["", "def", "remove_empty_slices", "(", "h5_filepath", ",", "save_filepath", ")", ":", "\n", "    ", "ori_h5", "=", "h5py", ".", "File", "(", "h5_filepath", ",", "'r'", ")", "\n", "new_h5", "=", "h5py", ".", "File", "(", "save_filepath", ",", "'w'", ")", "\n", "for", "key", "in", "tqdm", "(", "ori_h5", "[", "'images'", "]", ".", "keys", "(", ")", ")", ":", "\n", "        ", "image", "=", "ori_h5", "[", "'images/{:s}'", ".", "format", "(", "key", ")", "]", "[", "(", ")", "]", "\n", "label", "=", "ori_h5", "[", "'labels/{:s}'", ".", "format", "(", "key", ")", "]", "[", "(", ")", "]", "\n", "\n", "if", "np", ".", "count_nonzero", "(", "label", ")", "<", "10", ":", "\n", "            ", "continue", "\n", "\n", "", "new_h5", ".", "create_dataset", "(", "'images/{:s}'", ".", "format", "(", "key", ")", ",", "data", "=", "image", ")", "\n", "new_h5", ".", "create_dataset", "(", "'labels/{:s}'", ".", "format", "(", "key", ")", ",", "data", "=", "label", ")", "\n", "", "print", "(", "len", "(", "ori_h5", "[", "'images'", "]", ".", "keys", "(", ")", ")", ")", "\n", "print", "(", "len", "(", "new_h5", "[", "'images'", "]", ".", "keys", "(", ")", ")", ")", "\n", "ori_h5", ".", "close", "(", ")", "\n", "new_h5", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.preprocess.brats_dataset_build.select_10_percent_data": [[102, 127], ["h5py.File", "h5py.File", "list", "numpy.unique", "random.seed", "random.shuffle", "print", "print", "h5py.File.close", "h5py.File.close", "ori_h5[].keys", "x.split", "ValueError", "h5py.File.create_dataset", "h5py.File.create_dataset", "len", "len", "int", "key.split", "new_h5[].keys", "int", "int", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.create_dataset"], ["", "def", "select_10_percent_data", "(", "h5_filepath", ",", "save_filepath", ",", "modality", ")", ":", "\n", "    ", "ori_h5", "=", "h5py", ".", "File", "(", "h5_filepath", ",", "'r'", ")", "\n", "new_h5", "=", "h5py", ".", "File", "(", "save_filepath", ",", "'w'", ")", "\n", "\n", "keys", "=", "list", "(", "ori_h5", "[", "'images'", "]", ".", "keys", "(", ")", ")", "\n", "patients", "=", "[", "x", ".", "split", "(", "'_'", ")", "[", "0", "]", "for", "x", "in", "keys", "]", "\n", "patients", "=", "np", ".", "unique", "(", "patients", ")", "\n", "random", ".", "seed", "(", "1", ")", "\n", "random", ".", "shuffle", "(", "patients", ")", "\n", "if", "modality", "==", "'t2'", ":", "\n", "        ", "patients_part", "=", "patients", "[", ":", "int", "(", "0.1", "*", "len", "(", "patients", ")", ")", "]", "\n", "", "elif", "modality", "==", "'t1'", ":", "\n", "        ", "patients_part", "=", "patients", "[", "int", "(", "0.2", "*", "len", "(", "patients", ")", ")", ":", "int", "(", "0.3", "*", "len", "(", "patients", ")", ")", "]", "\n", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Modality should be either t2 or t1.'", ")", "\n", "\n", "", "for", "key", "in", "keys", ":", "\n", "        ", "if", "key", ".", "split", "(", "'_'", ")", "[", "0", "]", "in", "patients_part", ":", "\n", "            ", "new_h5", ".", "create_dataset", "(", "'images/{:s}'", ".", "format", "(", "key", ")", ",", "data", "=", "ori_h5", "[", "'images/{:s}'", ".", "format", "(", "key", ")", "]", "[", "(", ")", "]", ")", "\n", "new_h5", ".", "create_dataset", "(", "'labels/{:s}'", ".", "format", "(", "key", ")", ",", "data", "=", "ori_h5", "[", "'labels/{:s}'", ".", "format", "(", "key", ")", "]", "[", "(", ")", "]", ")", "\n", "\n", "", "", "print", "(", "'number of files in original dataset: {:d}'", ".", "format", "(", "len", "(", "keys", ")", ")", ")", "\n", "print", "(", "'number of files in new dataset: {:d}'", ".", "format", "(", "len", "(", "new_h5", "[", "'images'", "]", ".", "keys", "(", ")", ")", ")", ")", "\n", "ori_h5", ".", "close", "(", ")", "\n", "new_h5", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.test_options.TestOptions.initialize": [[10, 27], ["base_options.BaseOptions.initialize", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.set_defaults", "base_options.BaseOptions.initialize.set_defaults", "float", "float", "base_options.BaseOptions.initialize.get_default"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.initialize"], ["def", "initialize", "(", "self", ",", "parser", ")", ":", "\n", "        ", "parser", "=", "BaseOptions", ".", "initialize", "(", "self", ",", "parser", ")", "# define shared options", "\n", "parser", ".", "add_argument", "(", "'--ntest'", ",", "type", "=", "int", ",", "default", "=", "float", "(", "\"inf\"", ")", ",", "help", "=", "'# of test examples.'", ")", "\n", "parser", ".", "add_argument", "(", "'--results_dir'", ",", "type", "=", "str", ",", "default", "=", "'./results/'", ",", "help", "=", "'saves results here.'", ")", "\n", "parser", ".", "add_argument", "(", "'--aspect_ratio'", ",", "type", "=", "float", ",", "default", "=", "1.0", ",", "help", "=", "'aspect ratio of result images'", ")", "\n", "parser", ".", "add_argument", "(", "'--phase'", ",", "type", "=", "str", ",", "default", "=", "'test'", ",", "help", "=", "'train, val, test, etc'", ")", "\n", "# Dropout and Batchnorm has different behavioir during training and test.", "\n", "parser", ".", "add_argument", "(", "'--eval'", ",", "action", "=", "'store_true'", ",", "help", "=", "'use eval mode during test time.'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_test'", ",", "type", "=", "int", ",", "default", "=", "float", "(", "\"inf\"", ")", ",", "help", "=", "'how many test images to run'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--task_num'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'current task number'", ")", "\n", "# rewrite devalue values", "\n", "parser", ".", "set_defaults", "(", "model", "=", "'test'", ")", "\n", "# To avoid cropping, the load_size should be the same as crop_size", "\n", "parser", ".", "set_defaults", "(", "load_size", "=", "parser", ".", "get_default", "(", "'crop_size'", ")", ")", "\n", "self", ".", "isTrain", "=", "False", "\n", "return", "parser", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.train_options.TrainOptions.initialize": [[10, 43], ["base_options.BaseOptions.initialize", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument", "base_options.BaseOptions.initialize.add_argument"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.initialize"], ["def", "initialize", "(", "self", ",", "parser", ")", ":", "\n", "        ", "parser", "=", "BaseOptions", ".", "initialize", "(", "self", ",", "parser", ")", "\n", "# visdom and HTML visualization parameters", "\n", "parser", ".", "add_argument", "(", "'--display_freq'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "'frequency of showing training results on screen'", ")", "\n", "parser", ".", "add_argument", "(", "'--display_ncols'", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "'if positive, display all images in a single visdom web panel with certain number of images per row.'", ")", "\n", "parser", ".", "add_argument", "(", "'--display_id'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'window id of the web display'", ")", "\n", "parser", ".", "add_argument", "(", "'--display_server'", ",", "type", "=", "str", ",", "default", "=", "\"http://localhost\"", ",", "help", "=", "'visdom server of the web display'", ")", "\n", "parser", ".", "add_argument", "(", "'--display_env'", ",", "type", "=", "str", ",", "default", "=", "'main'", ",", "help", "=", "'visdom display environment name (default is \"main\")'", ")", "\n", "parser", ".", "add_argument", "(", "'--display_port'", ",", "type", "=", "int", ",", "default", "=", "8000", ",", "help", "=", "'visdom port of the web display'", ")", "\n", "parser", ".", "add_argument", "(", "'--update_html_freq'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "'frequency of saving training results to html'", ")", "\n", "parser", ".", "add_argument", "(", "'--print_freq'", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "'frequency of showing training results on console'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_html'", ",", "action", "=", "'store_true'", ",", "help", "=", "'do not save intermediate training results to [opt.checkpoints_dir]/[opt.name]/web/'", ")", "\n", "# network saving and loading parameters", "\n", "parser", ".", "add_argument", "(", "'--save_latest_freq'", ",", "type", "=", "int", ",", "default", "=", "5000", ",", "help", "=", "'frequency of saving the latest results'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_epoch_freq'", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "'frequency of saving checkpoints at the end of epochs'", ")", "\n", "parser", ".", "add_argument", "(", "'--save_by_iter'", ",", "action", "=", "'store_true'", ",", "help", "=", "'whether saves model by iteration'", ")", "\n", "parser", ".", "add_argument", "(", "'--continue_train'", ",", "action", "=", "'store_true'", ",", "help", "=", "'continue training: load the latest model'", ")", "\n", "parser", ".", "add_argument", "(", "'--epoch_count'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'the starting epoch count, we save the model by <epoch_count>, <epoch_count>+<save_latest_freq>, ...'", ")", "\n", "parser", ".", "add_argument", "(", "'--phase'", ",", "type", "=", "str", ",", "default", "=", "'train'", ",", "help", "=", "'train, val, test, etc'", ")", "\n", "# training parameters", "\n", "parser", ".", "add_argument", "(", "'--niter'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'# of iter at starting learning rate'", ")", "\n", "parser", ".", "add_argument", "(", "'--niter_decay'", ",", "type", "=", "int", ",", "default", "=", "100", ",", "help", "=", "'# of iter to linearly decay learning rate to zero'", ")", "\n", "parser", ".", "add_argument", "(", "'--beta1'", ",", "type", "=", "float", ",", "default", "=", "0.5", ",", "help", "=", "'momentum term of adam'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr'", ",", "type", "=", "float", ",", "default", "=", "0.0002", ",", "help", "=", "'initial learning rate for adam'", ")", "\n", "parser", ".", "add_argument", "(", "'--gan_mode'", ",", "type", "=", "str", ",", "default", "=", "'lsgan'", ",", "help", "=", "'the type of GAN objective. [vanilla| lsgan | wgangp]. vanilla GAN loss is the cross-entropy objective used in the original GAN paper.'", ")", "\n", "parser", ".", "add_argument", "(", "'--pool_size'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'the size of image buffer that stores previously generated images'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr_policy'", ",", "type", "=", "str", ",", "default", "=", "'linear'", ",", "help", "=", "'learning rate policy. [linear | step | plateau | cosine]'", ")", "\n", "parser", ".", "add_argument", "(", "'--lr_decay_iters'", ",", "type", "=", "int", ",", "default", "=", "50", ",", "help", "=", "'multiply by a gamma every lr_decay_iters iterations'", ")", "\n", "\n", "parser", ".", "add_argument", "(", "'--task_num'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'current task number'", ")", "\n", "\n", "self", ".", "isTrain", "=", "True", "\n", "return", "parser", "\n", "", "", ""]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.__init__": [[16, 19], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Reset the class; indicates the class hasn't been initailized\"\"\"", "\n", "self", ".", "initialized", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.initialize": [[20, 59], ["parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "parser.add_argument", "float"], "methods", ["None"], ["", "def", "initialize", "(", "self", ",", "parser", ")", ":", "\n", "        ", "\"\"\"Define the common options that are used in both training and test.\"\"\"", "\n", "# basic parameters", "\n", "parser", ".", "add_argument", "(", "'--dataroot'", ",", "required", "=", "True", ",", "help", "=", "'path to images (should have subfolders trainA, trainB, valA, valB, etc)'", ")", "\n", "parser", ".", "add_argument", "(", "'--name'", ",", "type", "=", "str", ",", "default", "=", "'experiment_name'", ",", "help", "=", "'name of the experiment. It decides where to store samples and models'", ")", "\n", "parser", ".", "add_argument", "(", "'--gpu_ids'", ",", "type", "=", "str", ",", "default", "=", "'0'", ",", "help", "=", "'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU'", ")", "\n", "parser", ".", "add_argument", "(", "'--checkpoints_dir'", ",", "type", "=", "str", ",", "default", "=", "'./checkpoints'", ",", "help", "=", "'models are saved here'", ")", "\n", "# model parameters", "\n", "parser", ".", "add_argument", "(", "'--model'", ",", "type", "=", "str", ",", "default", "=", "'cycle_gan'", ",", "help", "=", "'chooses which model to use. [cycle_gan | pix2pix | test | colorization]'", ")", "\n", "parser", ".", "add_argument", "(", "'--input_nc'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'# of input image channels: 3 for RGB and 1 for grayscale'", ")", "\n", "parser", ".", "add_argument", "(", "'--output_nc'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'# of output image channels: 3 for RGB and 1 for grayscale'", ")", "\n", "parser", ".", "add_argument", "(", "'--ngf'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "'# of gen filters in the last conv layer'", ")", "\n", "parser", ".", "add_argument", "(", "'--ndf'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "'# of discrim filters in the first conv layer'", ")", "\n", "parser", ".", "add_argument", "(", "'--netD'", ",", "type", "=", "str", ",", "default", "=", "'basic'", ",", "help", "=", "'specify discriminator architecture [basic | n_layers | pixel]. The basic model is a 70x70 PatchGAN. n_layers allows you to specify the layers in the discriminator'", ")", "\n", "parser", ".", "add_argument", "(", "'--netG'", ",", "type", "=", "str", ",", "default", "=", "'resnet_9blocks'", ",", "help", "=", "'specify generator architecture [resnet_9blocks | resnet_6blocks | unet_256 | unet_128]'", ")", "\n", "parser", ".", "add_argument", "(", "'--n_layers_D'", ",", "type", "=", "int", ",", "default", "=", "3", ",", "help", "=", "'only used if netD==n_layers'", ")", "\n", "parser", ".", "add_argument", "(", "'--norm'", ",", "type", "=", "str", ",", "default", "=", "'instance'", ",", "help", "=", "'instance normalization or batch normalization [instance | batch | none]'", ")", "\n", "parser", ".", "add_argument", "(", "'--init_type'", ",", "type", "=", "str", ",", "default", "=", "'normal'", ",", "help", "=", "'network initialization [normal | xavier | kaiming | orthogonal]'", ")", "\n", "parser", ".", "add_argument", "(", "'--init_gain'", ",", "type", "=", "float", ",", "default", "=", "0.02", ",", "help", "=", "'scaling factor for normal, xavier and orthogonal.'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_dropout'", ",", "action", "=", "'store_true'", ",", "help", "=", "'no dropout for the generator'", ")", "\n", "# dataset parameters", "\n", "parser", ".", "add_argument", "(", "'--dataset_mode'", ",", "type", "=", "str", ",", "default", "=", "'unaligned'", ",", "help", "=", "'chooses how datasets are loaded. [unaligned | aligned | single | colorization]'", ")", "\n", "parser", ".", "add_argument", "(", "'--direction'", ",", "type", "=", "str", ",", "default", "=", "'AtoB'", ",", "help", "=", "'AtoB or BtoA'", ")", "\n", "parser", ".", "add_argument", "(", "'--serial_batches'", ",", "action", "=", "'store_true'", ",", "help", "=", "'if true, takes images in order to make batches, otherwise takes them randomly'", ")", "\n", "parser", ".", "add_argument", "(", "'--num_threads'", ",", "default", "=", "4", ",", "type", "=", "int", ",", "help", "=", "'# threads for loading data'", ")", "\n", "parser", ".", "add_argument", "(", "'--batch_size'", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "'input batch size'", ")", "\n", "parser", ".", "add_argument", "(", "'--load_size'", ",", "type", "=", "int", ",", "default", "=", "286", ",", "help", "=", "'scale images to this size'", ")", "\n", "parser", ".", "add_argument", "(", "'--crop_size'", ",", "type", "=", "int", ",", "default", "=", "256", ",", "help", "=", "'then crop to this size'", ")", "\n", "parser", ".", "add_argument", "(", "'--max_dataset_size'", ",", "type", "=", "int", ",", "default", "=", "float", "(", "\"inf\"", ")", ",", "help", "=", "'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.'", ")", "\n", "parser", ".", "add_argument", "(", "'--preprocess'", ",", "type", "=", "str", ",", "default", "=", "'resize_and_crop'", ",", "help", "=", "'scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]'", ")", "\n", "parser", ".", "add_argument", "(", "'--no_flip'", ",", "action", "=", "'store_true'", ",", "help", "=", "'if specified, do not flip the images for data augmentation'", ")", "\n", "parser", ".", "add_argument", "(", "'--display_winsize'", ",", "type", "=", "int", ",", "default", "=", "64", ",", "help", "=", "'display window size for both visdom and HTML'", ")", "\n", "# additional parameters", "\n", "parser", ".", "add_argument", "(", "'--epoch'", ",", "type", "=", "str", ",", "default", "=", "'latest'", ",", "help", "=", "'which epoch to load? set to latest to use latest cached model'", ")", "\n", "parser", ".", "add_argument", "(", "'--load_iter'", ",", "type", "=", "int", ",", "default", "=", "'0'", ",", "help", "=", "'which iteration to load? if load_iter > 0, the code will load models by iter_[load_iter]; otherwise, the code will load models by [epoch]'", ")", "\n", "parser", ".", "add_argument", "(", "'--verbose'", ",", "action", "=", "'store_true'", ",", "help", "=", "'if specified, print more debugging information'", ")", "\n", "parser", ".", "add_argument", "(", "'--suffix'", ",", "default", "=", "''", ",", "type", "=", "str", ",", "help", "=", "'customized suffix: opt.name = opt.name + suffix: e.g., {model}_{netG}_size{load_size}'", ")", "\n", "self", ".", "initialized", "=", "True", "\n", "return", "parser", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.gather_options": [[60, 87], ["base_options.BaseOptions.parse_known_args", "models.get_option_setter", "models.get_option_setter.", "base_options.BaseOptions.parse_known_args", "data.get_option_setter", "data.get_option_setter.", "base_options.BaseOptions.parse_args", "argparse.ArgumentParser", "base_options.BaseOptions.initialize"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.get_option_setter", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.data.__init__.get_option_setter", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.initialize"], ["", "def", "gather_options", "(", "self", ")", ":", "\n", "        ", "\"\"\"Initialize our parser with basic options(only once).\n        Add additional model-specific and dataset-specific options.\n        These options are defined in the <modify_commandline_options> function\n        in model and dataset classes.\n        \"\"\"", "\n", "if", "not", "self", ".", "initialized", ":", "# check if it has been initialized", "\n", "            ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "formatter_class", "=", "argparse", ".", "ArgumentDefaultsHelpFormatter", ")", "\n", "parser", "=", "self", ".", "initialize", "(", "parser", ")", "\n", "\n", "# get the basic options", "\n", "", "opt", ",", "_", "=", "parser", ".", "parse_known_args", "(", ")", "\n", "\n", "# modify model-related parser options", "\n", "model_name", "=", "opt", ".", "model", "\n", "model_option_setter", "=", "models", ".", "get_option_setter", "(", "model_name", ")", "\n", "parser", "=", "model_option_setter", "(", "parser", ",", "self", ".", "isTrain", ")", "\n", "opt", ",", "_", "=", "parser", ".", "parse_known_args", "(", ")", "# parse again with new defaults", "\n", "\n", "# modify dataset-related parser options", "\n", "dataset_name", "=", "opt", ".", "dataset_mode", "\n", "dataset_option_setter", "=", "data", ".", "get_option_setter", "(", "dataset_name", ")", "\n", "parser", "=", "dataset_option_setter", "(", "parser", ",", "self", ".", "isTrain", ")", "\n", "\n", "# save and return the parser", "\n", "self", ".", "parser", "=", "parser", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.print_options": [[88, 112], ["sorted", "print", "os.path.join", "util.util.util.mkdirs", "os.path.join", "vars().items", "base_options.BaseOptions.parser.get_default", "open", "opt_file.write", "opt_file.write", "str", "str", "vars", "str"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.util.util.mkdirs"], ["", "def", "print_options", "(", "self", ",", "opt", ")", ":", "\n", "        ", "\"\"\"Print and save options\n\n        It will print both current options and default values(if different).\n        It will save options into a text file / [checkpoints_dir] / opt.txt\n        \"\"\"", "\n", "message", "=", "''", "\n", "message", "+=", "'----------------- Options ---------------\\n'", "\n", "for", "k", ",", "v", "in", "sorted", "(", "vars", "(", "opt", ")", ".", "items", "(", ")", ")", ":", "\n", "            ", "comment", "=", "''", "\n", "default", "=", "self", ".", "parser", ".", "get_default", "(", "k", ")", "\n", "if", "v", "!=", "default", ":", "\n", "                ", "comment", "=", "'\\t[default: %s]'", "%", "str", "(", "default", ")", "\n", "", "message", "+=", "'{:>25}: {:<30}{}\\n'", ".", "format", "(", "str", "(", "k", ")", ",", "str", "(", "v", ")", ",", "comment", ")", "\n", "", "message", "+=", "'----------------- End -------------------'", "\n", "print", "(", "message", ")", "\n", "\n", "# save to the disk", "\n", "expr_dir", "=", "os", ".", "path", ".", "join", "(", "opt", ".", "checkpoints_dir", ",", "opt", ".", "name", ")", "\n", "util", ".", "mkdirs", "(", "expr_dir", ")", "\n", "file_name", "=", "os", ".", "path", ".", "join", "(", "expr_dir", ",", "'{}_opt.txt'", ".", "format", "(", "opt", ".", "phase", ")", ")", "\n", "with", "open", "(", "file_name", ",", "'wt'", ")", "as", "opt_file", ":", "\n", "            ", "opt_file", ".", "write", "(", "message", ")", "\n", "opt_file", ".", "write", "(", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.parse": [[113, 137], ["base_options.BaseOptions.gather_options", "base_options.BaseOptions.print_options", "base_options.BaseOptions.gpu_ids.split", "int", "len", "torch.cuda.set_device", "base_options.BaseOptions.gpu_ids.append", "base_options.BaseOptions.suffix.format", "vars"], "methods", ["home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.gather_options", "home.repos.pwc.inspect_result.huiqu18_TDGAN-PyTorch.options.base_options.BaseOptions.print_options"], ["", "", "def", "parse", "(", "self", ")", ":", "\n", "        ", "\"\"\"Parse our options, create checkpoints directory suffix, and set up gpu device.\"\"\"", "\n", "opt", "=", "self", ".", "gather_options", "(", ")", "\n", "opt", ".", "isTrain", "=", "self", ".", "isTrain", "# train or test", "\n", "\n", "# process opt.suffix", "\n", "if", "opt", ".", "suffix", ":", "\n", "            ", "suffix", "=", "(", "'_'", "+", "opt", ".", "suffix", ".", "format", "(", "**", "vars", "(", "opt", ")", ")", ")", "if", "opt", ".", "suffix", "!=", "''", "else", "''", "\n", "opt", ".", "name", "=", "opt", ".", "name", "+", "suffix", "\n", "\n", "", "self", ".", "print_options", "(", "opt", ")", "\n", "\n", "# set gpu ids", "\n", "str_ids", "=", "opt", ".", "gpu_ids", ".", "split", "(", "','", ")", "\n", "opt", ".", "gpu_ids", "=", "[", "]", "\n", "for", "str_id", "in", "str_ids", ":", "\n", "            ", "id", "=", "int", "(", "str_id", ")", "\n", "if", "id", ">=", "0", ":", "\n", "                ", "opt", ".", "gpu_ids", ".", "append", "(", "id", ")", "\n", "", "", "if", "len", "(", "opt", ".", "gpu_ids", ")", ">", "0", ":", "\n", "            ", "torch", ".", "cuda", ".", "set_device", "(", "opt", ".", "gpu_ids", "[", "0", "]", ")", "\n", "\n", "", "self", ".", "opt", "=", "opt", "\n", "return", "self", ".", "opt", "\n", "", "", ""]]}