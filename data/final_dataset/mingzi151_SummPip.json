{"home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.__init__": [[34, 52], ["len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "sentences_list", ",", "w2v", ",", "use_lm", ",", "lm_model", ",", "lm_tokenizer", ",", "ita", "=", "0.9", ",", "threshold", "=", "0.65", ")", ":", "\n", "        ", "self", ".", "sentences_list", "=", "sentences_list", "\n", "\n", "self", ".", "length", "=", "len", "(", "sentences_list", ")", "\n", "\n", "self", ".", "w2v", "=", "w2v", "\n", "\n", "self", ".", "use_lm", "=", "use_lm", "\n", "\n", "self", ".", "lm_model", "=", "lm_model", "\n", "\n", "self", ".", "tokenizer", "=", "lm_tokenizer", "\n", "\n", "# threshold for step1", "\n", "self", ".", "threshold", "=", "threshold", "\n", "\n", "# threshold for step4", "\n", "self", ".", "ita", "=", "ita", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.get_nouns_for_verbs": [[53, 64], ["spacynlp", "len", "sentence_graph.SentenceGraph._nounify", "nouns_list.extend"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph._nounify"], ["", "def", "get_nouns_for_verbs", "(", "self", ",", "string", ")", ":", "\n", "        ", "doc", "=", "spacynlp", "(", "string", ")", "\n", "nouns_list", "=", "[", "]", "\n", "if", "len", "(", "doc", ")", ">", "0", ":", "\n", "            ", "for", "token", "in", "doc", ":", "\n", "# find noun reference for verbs, escaping verbs that are too ambiguous", "\n", "                ", "if", "token", ".", "pos_", "==", "\"VERB\"", "and", "token", ".", "text", "not", "in", "verbs_to_escape", ":", "\n", "# print(\"token.text \", token.text)", "\n", "                    ", "noun_forms", "=", "self", ".", "_nounify", "(", "token", ".", "text", ")", "\n", "nouns_list", ".", "extend", "(", "noun_forms", ")", "\n", "", "", "", "return", "nouns_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph._nounify": [[65, 80], ["nltk.corpus.wordnet.morphy", "nltk.corpus.wordnet.lemmas", "orderedset.OrderedSet", "noun_forms.extend", "forms.name", "lemma.derivationally_related_forms"], "methods", ["None"], ["", "def", "_nounify", "(", "self", ",", "verb", ")", ":", "\n", "# get the lemmas of base verbs;", "\n", "        ", "base", "=", "wn", ".", "morphy", "(", "verb", ",", "wn", ".", "VERB", ")", "\n", "if", "base", ":", "\n", "            ", "lemmas", "=", "wn", ".", "lemmas", "(", "base", ",", "pos", "=", "\"v\"", ")", "\n", "noun_forms", "=", "[", "]", "\n", "# derive noun forms for each lemma", "\n", "for", "lemma", "in", "lemmas", ":", "\n", "                ", "nouns", "=", "[", "forms", ".", "name", "(", ")", "for", "forms", "in", "lemma", ".", "derivationally_related_forms", "(", ")", "]", "\n", "noun_forms", ".", "extend", "(", "nouns", ")", "\n", "# remove repetition", "\n", "", "nouns_set", "=", "OrderedSet", "(", "noun_forms", ")", "\n", "return", "nouns_set", "\n", "", "else", ":", "\n", "            ", "return", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.find_most_similar_words": [[83, 97], ["list", "similar_nouns_list.extend", "list", "set", "set", "word_vectors.most_similar", "similar_nouns_list.extend"], "methods", ["None"], ["", "", "def", "find_most_similar_words", "(", "self", ",", "word_vectors", ",", "nouns_list", ",", "threshold", "=", "0.65", ")", ":", "\n", "        ", "similar_nouns_list", "=", "[", "]", "\n", "nouns_list", "=", "list", "(", "set", "(", "nouns_list", ")", ")", "\n", "for", "noun", "in", "nouns_list", ":", "\n", "            ", "try", ":", "\n", "                ", "nn", "=", "word_vectors", ".", "most_similar", "(", "positive", "=", "[", "noun", "]", ")", "\n", "# keep nn whose have high similary score", "\n", "nn", "=", "[", "tuple_", "[", "0", "]", "for", "tuple_", "in", "nn", "if", "tuple_", "[", "1", "]", ">", "threshold", "]", "\n", "similar_nouns_list", ".", "extend", "(", "nn", ")", "\n", "# pass on uncommon words", "\n", "", "except", "KeyError", ":", "\n", "                ", "pass", "\n", "", "", "similar_nouns_list", ".", "extend", "(", "nouns_list", ")", "\n", "return", "list", "(", "set", "(", "similar_nouns_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.check_noun_reference": [[99, 109], ["spacynlp", "len"], "methods", ["None"], ["", "def", "check_noun_reference", "(", "self", ",", "similar_nouns_list", ",", "subsequent_sen", ")", ":", "\n", "        ", "flag", "=", "False", "\n", "doc", "=", "spacynlp", "(", "subsequent_sen", ")", "\n", "if", "len", "(", "doc", ")", ">", "0", ":", "\n", "            ", "for", "token", "in", "doc", ":", "\n", "                ", "if", "token", ".", "pos_", "==", "\"NOUN\"", ":", "\n", "                    ", "if", "token", ".", "text", "in", "similar_nouns_list", ":", "\n", "                        ", "flag", "=", "True", "\n", "break", "\n", "", "", "", "", "return", "flag", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.compare_name_entity": [[112, 124], ["spacynlp", "spacynlp", "len", "len"], "methods", ["None"], ["", "def", "compare_name_entity", "(", "self", ",", "str1", ",", "str2", ")", ":", "\n", "        ", "flag", "=", "False", "\n", "doc1", "=", "spacynlp", "(", "str1", ")", "\n", "doc2", "=", "spacynlp", "(", "str2", ")", "\n", "if", "len", "(", "doc1", ")", ">", "0", "and", "len", "(", "doc2", ")", ">", "0", ":", "\n", "            ", "ent_list1", "=", "[", "(", "ent", ".", "text", ",", "ent", ".", "label_", ")", "for", "ent", "in", "doc1", ".", "ents", "]", "\n", "ent_list2", "=", "[", "(", "ent", ".", "text", ",", "ent", ".", "label_", ")", "for", "ent", "in", "doc2", ".", "ents", "]", "\n", "for", "(", "text", ",", "label", ")", "in", "ent_list1", ":", "\n", "                ", "if", "(", "text", ",", "label", ")", "in", "ent_list2", ":", "\n", "                    ", "flag", "=", "True", "\n", "break", "\n", "", "", "", "return", "flag", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.check_discourse_markers": [[125, 133], ["spacynlp", "len", "first_token.lower"], "methods", ["None"], ["", "def", "check_discourse_markers", "(", "self", ",", "str1", ",", "str2", ")", ":", "\n", "        ", "flag", "=", "False", "\n", "doc2", "=", "spacynlp", "(", "str2", ")", "\n", "if", "len", "(", "doc2", ")", ">", "0", ":", "\n", "            ", "first_token", "=", "doc2", "[", "0", "]", ".", "text", "\n", "if", "first_token", ".", "lower", "(", ")", "in", "markers", ":", "\n", "                ", "flag", "=", "True", "\n", "", "", "return", "flag", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.cos_sim": [[135, 137], ["scipy.spatial.distance.cosine"], "methods", ["None"], ["", "def", "cos_sim", "(", "self", ",", "a", ",", "b", ")", ":", "\n", "        ", "return", "1", "-", "scipy", ".", "spatial", ".", "distance", ".", "cosine", "(", "a", ",", "b", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.make_graph_undirected": [[139, 154], ["source.extend", "target.extend", "weight.extend", "sorted", "sorted_source.append", "sorted_target.append", "sorted_weight.append", "range", "len"], "methods", ["None"], ["", "def", "make_graph_undirected", "(", "self", ",", "source", ",", "target", ",", "weight", ")", ":", "\n", "        ", "source", ".", "extend", "(", "target", ")", "\n", "target", ".", "extend", "(", "source", ")", "\n", "weight", ".", "extend", "(", "weight", ")", "\n", "triplet_list", "=", "[", "(", "source", "[", "i", "]", ",", "target", "[", "i", "]", ",", "weight", "[", "i", "]", ")", "for", "i", "in", "range", "(", "len", "(", "source", ")", ")", "]", "\n", "sorted_by_src", "=", "sorted", "(", "triplet_list", ",", "key", "=", "lambda", "x", ":", "(", "x", "[", "0", "]", ",", "x", "[", "1", "]", ")", ")", "\n", "\n", "sorted_source", "=", "[", "]", "\n", "sorted_target", "=", "[", "]", "\n", "sorted_weight", "=", "[", "]", "\n", "for", "triplet", "in", "sorted_by_src", ":", "\n", "            ", "sorted_source", ".", "append", "(", "triplet", "[", "0", "]", ")", "\n", "sorted_target", ".", "append", "(", "triplet", "[", "1", "]", ")", "\n", "sorted_weight", ".", "append", "(", "triplet", "[", "2", "]", ")", "\n", "", "return", "sorted_source", ",", "sorted_target", ",", "sorted_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.get_sentence_embeddings": [[156, 162], ["sentence_graph.SentenceGraph.get_wv_embedding", "sentence_graph.SentenceGraph.get_lm_embedding"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.get_wv_embedding", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.get_lm_embedding"], ["", "def", "get_sentence_embeddings", "(", "self", ",", "string", ")", ":", "\n", "        ", "if", "not", "self", ".", "use_lm", ":", "\n", "            ", "v", "=", "self", ".", "get_wv_embedding", "(", "string", ")", "\n", "", "else", ":", "\n", "            ", "v", "=", "self", ".", "get_lm_embedding", "(", "string", ")", "\n", "", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.get_wv_embedding": [[164, 175], ["string.lower", "len", "numpy.mean", "numpy.zeros", "word_embeddings.get", "numpy.zeros", "string.lower.split"], "methods", ["None"], ["", "def", "get_wv_embedding", "(", "self", ",", "string", ")", ":", "\n", "        ", "word_embeddings", "=", "self", ".", "w2v", "\n", "sent", "=", "string", ".", "lower", "(", ")", "\n", "eps", "=", "1e-10", "\n", "if", "len", "(", "sent", ")", "!=", "0", ":", "\n", "            ", "vectors", "=", "[", "word_embeddings", ".", "get", "(", "w", ",", "np", ".", "zeros", "(", "(", "100", ",", ")", ")", ")", "for", "w", "in", "sent", ".", "split", "(", ")", "]", "\n", "v", "=", "np", ".", "mean", "(", "vectors", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "            ", "v", "=", "np", ".", "zeros", "(", "(", "100", ",", ")", ")", "\n", "", "v", "=", "v", "+", "eps", "\n", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.get_lm_embedding": [[177, 189], ["string.lower", "len", "torch.tensor", "last_hidden_state.tolist", "numpy.mean", "numpy.zeros", "sentence_graph.SentenceGraph.lm_model", "sentence_graph.SentenceGraph.tokenizer.encode"], "methods", ["None"], ["", "def", "get_lm_embedding", "(", "self", ",", "string", ")", ":", "\n", "        ", "sent", "=", "string", ".", "lower", "(", ")", "\n", "eps", "=", "1e-10", "\n", "if", "len", "(", "sent", ")", "!=", "0", ":", "\n", "            ", "input_ids", "=", "torch", ".", "tensor", "(", "[", "self", ".", "tokenizer", ".", "encode", "(", "sent", ")", "]", ")", "\n", "last_hidden_state", "=", "self", ".", "lm_model", "(", "input_ids", ")", "[", "0", "]", "\n", "hidden_state", "=", "last_hidden_state", ".", "tolist", "(", ")", "\n", "v", "=", "np", ".", "mean", "(", "hidden_state", ",", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "v", "=", "np", ".", "zeros", "(", "(", "768", ",", ")", ")", "\n", "", "v", "=", "v", "+", "eps", "\n", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.check_if_similar_sentences": [[191, 197], ["sentence_graph.SentenceGraph.cos_sim"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.cos_sim"], ["", "def", "check_if_similar_sentences", "(", "self", ",", "sentence_emb1", ",", "sentence_emb2", ")", ":", "\n", "        ", "flag", "=", "False", "\n", "similarity", "=", "self", ".", "cos_sim", "(", "sentence_emb1", ",", "sentence_emb2", ")", "\n", "if", "similarity", ">", "self", ".", "ita", ":", "\n", "            ", "flag", "=", "True", "\n", "", "return", "flag", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.build_sentence_graph": [[199, 245], ["numpy.zeros", "len", "numpy.zeros", "range", "range", "sentence_graph.SentenceGraph.get_sentence_embeddings", "sentence_graph.SentenceGraph.get_sentence_embeddings", "range", "sentence_graph.SentenceGraph.get_nouns_for_verbs", "sentence_graph.SentenceGraph.find_most_similar_words", "sentence_graph.SentenceGraph.check_noun_reference", "sentence_graph.SentenceGraph.compare_name_entity", "sentence_graph.SentenceGraph.check_if_similar_sentences", "sentence_graph.SentenceGraph.check_discourse_markers"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.get_sentence_embeddings", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.get_sentence_embeddings", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.get_nouns_for_verbs", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.find_most_similar_words", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.check_noun_reference", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.compare_name_entity", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.check_if_similar_sentences", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.check_discourse_markers"], ["", "def", "build_sentence_graph", "(", "self", ",", ")", ":", "\n", "# spectral clustering  ", "\n", "        ", "X", "=", "np", ".", "zeros", "(", "[", "self", ".", "length", ",", "self", ".", "length", "]", ")", "\n", "\n", "# get the vector size", "\n", "self", ".", "size", "=", "len", "(", "self", ".", "get_sentence_embeddings", "(", "self", ".", "sentences_list", "[", "0", "]", ")", ")", "\n", "\n", "# get sentence vector holder", "\n", "emb_sentence_vectors", "=", "np", ".", "zeros", "(", "[", "self", ".", "length", ",", "self", ".", "size", "]", ")", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "length", ")", ":", "\n", "             ", "emb_sen", "=", "self", ".", "get_sentence_embeddings", "(", "self", ".", "sentences_list", "[", "i", "]", ")", "\n", "emb_sentence_vectors", "[", "i", ",", "]", "=", "emb_sen", "\n", "\n", "# iterate all sentence nodes to check if they should be connected", "\n", "", "for", "i", "in", "range", "(", "self", ".", "length", ")", ":", "\n", "            ", "flag", "=", "False", "\n", "sen_i", "=", "self", ".", "sentences_list", "[", "i", "]", "\n", "# check above steps", "\n", "for", "j", "in", "range", "(", "i", "+", "1", ",", "self", ".", "length", ")", ":", "\n", "                ", "sen_j", "=", "self", ".", "sentences_list", "[", "j", "]", "\n", "if", "(", "j", "-", "i", ")", "==", "1", ":", "\n", "# perform step1 and step3,which are only for adjacent sentences", "\n", "                    ", "nouns_list", "=", "self", ".", "get_nouns_for_verbs", "(", "sen_i", ")", "\n", "# get most similar words for above nouns, including itself", "\n", "similar_nouns_list", "=", "self", ".", "find_most_similar_words", "(", "glove_word_vectors", ",", "nouns_list", ",", "self", ".", "threshold", ")", "\n", "# check for devebal noun ", "\n", "flag", "=", "self", ".", "check_noun_reference", "(", "similar_nouns_list", ",", "sen_j", ")", "\n", "if", "not", "flag", ":", "\n", "# check for disourse markers", "\n", "                        ", "flag", "=", "self", ".", "check_discourse_markers", "(", "sen_i", ",", "sen_j", ")", "\n", "", "", "else", ":", "\n", "# check for name entities", "\n", "                    ", "flag", "=", "self", ".", "compare_name_entity", "(", "sen_i", ",", "sen_j", ")", "\n", "\n", "# => step4 check for similar sentences", "\n", "", "if", "not", "flag", ":", "\n", "# continue", "\n", "                    ", "i_sen_emb", "=", "emb_sentence_vectors", "[", "i", ",", "]", "\n", "j_sen_emb", "=", "emb_sentence_vectors", "[", "j", ",", "]", "\n", "flag", "=", "self", ".", "check_if_similar_sentences", "(", "i_sen_emb", ",", "j_sen_emb", ")", "\n", "\n", "", "if", "flag", ":", "\n", "                    ", "X", "[", "i", ",", "j", "]", "=", "1", "\n", "X", "[", "j", ",", "i", "]", "=", "1", "\n", "", "", "", "return", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.run_multinews.read_arguments": [[8, 23], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "read_arguments", "(", ")", ":", "\n", "# read arguments", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--input_path\"", ",", "type", "=", "str", ",", "default", "=", "\"dataset/multi_news/\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--source_file\"", ",", "type", "=", "str", ",", "default", "=", "\"test.truncate.fix.pun.src.txt\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_path\"", ",", "type", "=", "str", ",", "default", "=", "\"\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--w2v_file\"", ",", "type", "=", "str", ",", "default", "=", "\"word_vec/multi_news/news_w2v.txt\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--cluster\"", ",", "type", "=", "str", ",", "default", "=", "\"spc\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seed\"", ",", "type", "=", "int", ",", "default", "=", "88", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_file\"", ",", "type", "=", "str", ",", "default", "=", "\"testSummary.txt\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--nb_words\"", ",", "type", "=", "int", ",", "default", "=", "5", ")", "\n", "parser", ".", "add_argument", "(", "\"--nb_clusters\"", ",", "type", "=", "int", ",", "default", "=", "9", ",", "help", "=", "\"for spectral clustering\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ita\"", ",", "type", "=", "float", ",", "default", "=", "0.98", ")", "\n", "\n", "return", "parser", ".", "parse_args", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.run_multinews.main": [[24, 55], ["run_multinews.read_arguments", "print", "print", "timeit.default_timer", "summarizer.SummPip", "summarizer.SummPip.summarize", "timeit.default_timer", "open", "print", "open.writelines", "open.close", "print", "utils.read_file", "len", "os.path.join", "len", "line.replace"], "function", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.run_multinews.read_arguments", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.summarize", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.read_file"], ["", "def", "main", "(", ")", ":", "\n", "# read arguments", "\n", "    ", "args", "=", "read_arguments", "(", ")", "\n", "print", "(", "\"arguments\"", ",", "args", ")", "\n", "nb_words", "=", "args", ".", "nb_words", "\n", "nb_clusters", "=", "args", ".", "nb_clusters", "\n", "seed", "=", "args", ".", "seed", "\n", "path", "=", "args", ".", "input_path", "\n", "clus_alg", "=", "args", ".", "cluster", "\n", "w2v_file", "=", "args", ".", "w2v_file", "\n", "ita", "=", "args", ".", "ita", "\n", "\n", "#read source file", "\n", "src_file", "=", "args", ".", "source_file", "\n", "src_list", "=", "read_file", "(", "path", ",", "src_file", ")", "[", ":", "2", "]", "\n", "print", "(", "\"Number of instances: \"", ",", "len", "(", "src_list", ")", ")", "\n", "start", "=", "timeit", ".", "default_timer", "(", ")", "\n", "\n", "pipe", "=", "SummPip", "(", "nb_clusters", "=", "nb_clusters", ",", "nb_words", "=", "nb_words", ",", "ita", "=", "ita", ",", "seed", "=", "seed", ",", "w2v_file", "=", "w2v_file", ")", "\n", "summary_list", "=", "pipe", ".", "summarize", "(", "src_list", ")", "\n", "\n", "# write output to file", "\n", "stop", "=", "timeit", ".", "default_timer", "(", ")", "\n", "out_path", "=", "args", ".", "output_path", "\n", "outfile", "=", "args", ".", "output_file", "\n", "f", "=", "open", "(", "os", ".", "path", ".", "join", "(", "out_path", ",", "outfile", ")", ",", "\"w\"", ")", "\n", "print", "(", "\"summary list length\"", ",", "len", "(", "summary_list", ")", ")", "\n", "summary_list", "=", "[", "line", ".", "replace", "(", "\"\\n\"", ",", "\"\"", ")", "+", "\"\\n\"", "for", "line", "in", "summary_list", "]", "\n", "f", ".", "writelines", "(", "summary_list", ")", "\n", "f", ".", "close", "(", ")", "\n", "print", "(", "'Done'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.__init__": [[10, 55], ["nltk.data.load", "torch.manual_seed", "torch.cuda.manual_seed", "summarizer.SummPip._get_w2v_embeddings", "GPT2Tokenizer.from_pretrained", "GPT2Model.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip._get_w2v_embeddings"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "nb_clusters", ":", "int", "=", "14", ",", "\n", "nb_words", ":", "int", "=", "5", ",", "\n", "ita", ":", "float", "=", "0.98", ",", "\n", "seed", ":", "int", "=", "88", ",", "\n", "w2v_file", ":", "str", "=", "\"word_vec/multi_news/news_w2v.txt\"", ",", "\n", "lm_path", ":", "str", "=", "\"gpt2/mutli_news\"", ",", "\n", "use_lm", ":", "bool", "=", "False", "\n", ")", ":", "\n", "        ", "\"\"\"\n        This is the SummPip class\n\n        :param nb_clusters: this determines the number of sentences in the output summary\n        :param nb_words: this controls the length of each sentence in the output summary\n        :param ita: threshold for determining whether two sentences are similar by vector similarity\n        :param seed: the random state to reproduce summarization\n        :param w2v_file: file for storing w2v matrix\n        :param lm_path: path for langauge model\n        :param use_lm: use language model or not \n        \"\"\"", "\n", "\n", "self", ".", "nb_clusters", "=", "nb_clusters", "\n", "self", ".", "nb_words", "=", "nb_words", "\n", "self", ".", "ita", "=", "ita", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "use_lm", "=", "use_lm", "\n", "\n", "if", "not", "self", ".", "use_lm", ":", "\n", "            ", "self", ".", "w2v", "=", "self", ".", "_get_w2v_embeddings", "(", "w2v_file", ")", "\n", "self", ".", "lm_tokenizer", "=", "\"\"", "\n", "self", ".", "lm_model", "=", "\"\"", "\n", "", "else", ":", "\n", "            ", "from", "transformers", "import", "GPT2Tokenizer", ",", "GPT2Model", "\n", "self", ".", "lm_tokenizer", "=", "GPT2Tokenizer", ".", "from_pretrained", "(", "lm_path", ")", "\n", "self", ".", "lm_model", "=", "GPT2Model", ".", "from_pretrained", "(", "lm_path", ",", "\n", "output_hidden_states", "=", "True", ",", "\n", "output_attentions", "=", "False", ")", "\n", "self", ".", "w2v", "=", "\"\"", "\n", "\n", "", "self", ".", "sent_detector", "=", "nltk", ".", "data", ".", "load", "(", "'tokenizers/punkt/english.pickle'", ")", "\n", "\n", "# set seed", "\n", "torch", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "torch", ".", "cuda", ".", "manual_seed", "(", "self", ".", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip._get_w2v_embeddings": [[56, 71], ["open", "open.close", "line.split", "np.asarray"], "methods", ["None"], ["", "def", "_get_w2v_embeddings", "(", "self", ",", "w2v_file", ")", ":", "\n", "        ", "\"\"\"\n\t\tGet w2v word embedding matrix\n\t\t\n\t\t:return: w2v matrix\n        \"\"\"", "\n", "word_embeddings", "=", "{", "}", "\n", "f", "=", "open", "(", "w2v_file", ",", "encoding", "=", "'utf-8'", ")", "\n", "for", "line", "in", "f", ":", "\n", "            ", "values", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "values", "[", "0", "]", "\n", "coefs", "=", "np", ".", "asarray", "(", "values", "[", "1", ":", "]", ",", "dtype", "=", "'float32'", ")", "\n", "word_embeddings", "[", "word", "]", "=", "coefs", "\n", "", "f", ".", "close", "(", ")", "\n", "return", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.construct_sentence_graph": [[72, 82], ["sentence_graph.SentenceGraph", "sentence_graph.SentenceGraph.build_sentence_graph"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.sentence_graph.SentenceGraph.build_sentence_graph"], ["", "def", "construct_sentence_graph", "(", "self", ",", "sentences_list", ")", ":", "\n", "        ", "\"\"\"\n\t\tConstruct a sentence graph\n\n\t\t:return: adjacency matrix \n        \"\"\"", "\n", "\n", "graph", "=", "SentenceGraph", "(", "sentences_list", ",", "self", ".", "w2v", ",", "self", ".", "use_lm", ",", "self", ".", "lm_model", ",", "self", ".", "lm_tokenizer", ",", "self", ".", "ita", ")", "\n", "X", "=", "graph", ".", "build_sentence_graph", "(", ")", "\n", "return", "X", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.cluster_graph": [[83, 99], ["sklearn.cluster.SpectralClustering().fit", "enumerate", "max", "cluster_dict[].append", "sklearn.cluster.SpectralClustering", "range"], "methods", ["None"], ["", "def", "cluster_graph", "(", "self", ",", "X", ",", "sentences_list", ")", ":", "\n", "        ", "\"\"\"\n\t\tPerform graph clustering\n\n\t\t:return: a dictionary with key, value pairs of cluster Id and sentences\n        \"\"\"", "\n", "# ???? n", "\n", "clustering", "=", "SpectralClustering", "(", "n_clusters", "=", "self", ".", "nb_clusters", ",", "random_state", "=", "self", ".", "seed", ")", ".", "fit", "(", "X", ")", "\n", "clusterIDs", "=", "clustering", ".", "labels_", "\n", "\n", "num_clusters", "=", "max", "(", "clusterIDs", ")", "+", "1", "\n", "cluster_dict", "=", "{", "new_list", ":", "[", "]", "for", "new_list", "in", "range", "(", "num_clusters", ")", "}", "\n", "# group sentences by cluster ID", "\n", "for", "i", ",", "clusterID", "in", "enumerate", "(", "clusterIDs", ")", ":", "\n", "            ", "cluster_dict", "[", "clusterID", "]", ".", "append", "(", "sentences_list", "[", "i", "]", ")", "\n", "", "return", "cluster_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.convert_sents_to_tagged_sents": [[100, 111], ["len", "tagged_list.append", "s.replace.replace.replace", "utils.tag_pos", "tagged_list.append", "utils.tag_pos"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.tag_pos", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.tag_pos"], ["", "def", "convert_sents_to_tagged_sents", "(", "self", ",", "sent_list", ")", ":", "\n", "        ", "tagged_list", "=", "[", "]", "\n", "if", "(", "len", "(", "sent_list", ")", ">", "0", ")", ":", "\n", "            ", "for", "s", "in", "sent_list", ":", "\n", "                ", "s", "=", "s", ".", "replace", "(", "\"/\"", ",", "\"\"", ")", "\n", "# print(\"original sent -------- \\n\",s)", "\n", "temp_tagged", "=", "tag_pos", "(", "s", ")", "\n", "tagged_list", ".", "append", "(", "temp_tagged", ")", "\n", "", "", "else", ":", "\n", "            ", "tagged_list", ".", "append", "(", "tag_pos", "(", "\".\"", ")", ")", "\n", "", "return", "tagged_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.get_compressed_sen": [[112, 125], ["takahe.word_graph", "takahe.word_graph.get_compression", "takahe.keyphrase_reranker", "takahe.keyphrase_reranker.rerank_nbest_compressions", "len"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_compression", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.rerank_nbest_compressions"], ["", "def", "get_compressed_sen", "(", "self", ",", "sentences", ")", ":", "\n", "        ", "compresser", "=", "takahe", ".", "word_graph", "(", "sentence_list", "=", "sentences", ",", "nb_words", "=", "self", ".", "nb_words", ",", "lang", "=", "'en'", ",", "punct_tag", "=", "\".\"", ")", "\n", "candidates", "=", "compresser", ".", "get_compression", "(", "3", ")", "\n", "reranker", "=", "takahe", ".", "keyphrase_reranker", "(", "sentences", ",", "candidates", ",", "lang", "=", "'en'", ")", "\n", "\n", "reranked_candidates", "=", "reranker", ".", "rerank_nbest_compressions", "(", ")", "\n", "# print(reranked_candidates)", "\n", "if", "len", "(", "reranked_candidates", ")", ">", "0", ":", "\n", "            ", "score", ",", "path", "=", "reranked_candidates", "[", "0", "]", "\n", "result", "=", "' '", ".", "join", "(", "[", "u", "[", "0", "]", "for", "u", "in", "path", "]", ")", "\n", "", "else", ":", "\n", "            ", "result", "=", "' '", "\n", "", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.compress_cluster": [[126, 139], ["cluster_dict.items", "summarizer.SummPip.convert_sents_to_tagged_sents", "summarizer.SummPip.get_compressed_sen", "summary.append"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.convert_sents_to_tagged_sents", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.get_compressed_sen"], ["", "def", "compress_cluster", "(", "self", ",", "cluster_dict", ")", ":", "\n", "        ", "\"\"\"\n\t\tPerform cluster compression\n\n\t\t:return: a string of concatenated sentences from all clusters\n        \"\"\"", "\n", "\n", "summary", "=", "[", "]", "\n", "for", "k", ",", "v", "in", "cluster_dict", ".", "items", "(", ")", ":", "\n", "            ", "tagged_sens", "=", "self", ".", "convert_sents_to_tagged_sents", "(", "v", ")", "\n", "compressed_sent", "=", "self", ".", "get_compressed_sen", "(", "tagged_sens", ")", "\n", "summary", ".", "append", "(", "compressed_sent", ")", "\n", "", "return", "\" \"", ".", "join", "(", "summary", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.split_sentences": [[140, 148], ["doc.replace.replace.replace", "summarizer.SummPip.sent_detector.tokenize", "src_list.append", "doc.replace.replace.strip"], "methods", ["None"], ["", "def", "split_sentences", "(", "self", ",", "docs", ")", ":", "\n", "        ", "tag", "=", "\"story_separator_special_tag\"", "\n", "src_list", "=", "[", "]", "\n", "for", "doc", "in", "docs", ":", "\n", "            ", "doc", "=", "doc", ".", "replace", "(", "tag", ",", "\"\"", ")", "\n", "sent_list", "=", "self", ".", "sent_detector", ".", "tokenize", "(", "doc", ".", "strip", "(", ")", ")", "\n", "src_list", ".", "append", "(", "sent_list", ")", "\n", "", "return", "src_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.summarize": [[151, 174], ["enumerate", "len", "summarizer.SummPip.construct_sentence_graph", "summarizer.SummPip.cluster_graph", "summarizer.SummPip.compress_cluster", "summary_list.append", "summary_list.append", "print"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.construct_sentence_graph", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.cluster_graph", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.summarizer.SummPip.compress_cluster"], ["", "def", "summarize", "(", "self", ",", "src_list", ")", ":", "\n", "        ", "\"\"\"\n\t\tConstruct a graph, run graph clustering, compress each cluster, then concatenate sentences\n\n\t\t:param src_list: a list of input documents each of whose elements is a list of multiple documents\n\t\t:return: a list of summaries\n        \"\"\"", "\n", "#TODO: split sentences", "\n", "summary_list", "=", "[", "]", "\n", "# iterate over all docs", "\n", "for", "idx", ",", "sentences_list", "in", "enumerate", "(", "src_list", ")", ":", "\n", "            ", "num_sents", "=", "len", "(", "sentences_list", ")", "\n", "# handle short doc", "\n", "if", "num_sents", "<=", "self", ".", "nb_clusters", ":", "\n", "                ", "summary_list", ".", "append", "(", "\" \"", ".", "join", "(", "sentences_list", ")", ")", "\n", "print", "(", "\"continue----\"", ")", "\n", "continue", "\n", "\n", "", "X", "=", "self", ".", "construct_sentence_graph", "(", "sentences_list", ")", "\n", "cluster_dict", "=", "self", ".", "cluster_graph", "(", "X", ",", "sentences_list", ")", "\n", "summary", "=", "self", ".", "compress_cluster", "(", "cluster_dict", ")", "\n", "summary_list", ".", "append", "(", "summary", ")", "\n", "", "return", "summary_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.__init__": [[139, 199], ["list", "len", "takahe.word_graph.load_stopwords", "networkx.DiGraph", "set", "takahe.word_graph.pre_process_sentences", "takahe.word_graph.compute_statistics", "takahe.word_graph.build_graph", "os.path.dirname", "set"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.load_stopwords", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.pre_process_sentences", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.compute_statistics", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.build_graph"], ["def", "__init__", "(", "self", ",", "sentence_list", ",", "nb_words", "=", "8", ",", "lang", "=", "\"en\"", ",", "punct_tag", "=", "\"PUNCT\"", ",", "pos_separator", "=", "'/'", ")", ":", "\n", "\n", "        ", "self", ".", "sentence", "=", "list", "(", "sentence_list", ")", "\n", "\"\"\" A list of sentences provided by the user. \"\"\"", "\n", "\n", "self", ".", "length", "=", "len", "(", "sentence_list", ")", "\n", "\"\"\" The number of sentences given for fusion. \"\"\"", "\n", "\n", "self", ".", "nb_words", "=", "nb_words", "\n", "\"\"\" The minimal number of words in the compression. \"\"\"", "\n", "\n", "self", ".", "resources", "=", "os", ".", "path", ".", "dirname", "(", "__file__", ")", "+", "'/resources/'", "\n", "\"\"\" The path of the resources folder. \"\"\"", "\n", "\n", "self", ".", "stopword_path", "=", "self", ".", "resources", "+", "'stopwords.'", "+", "lang", "+", "'.dat'", "\n", "\"\"\" The path of the stopword list, e.g. stopwords.[lang].dat. \"\"\"", "\n", "\n", "self", ".", "stopwords", "=", "self", ".", "load_stopwords", "(", "self", ".", "stopword_path", ")", "\n", "\"\"\" The set of stopwords loaded from stopwords.[lang].dat. \"\"\"", "\n", "\n", "self", ".", "punct_tag", "=", "punct_tag", "\n", "\"\"\" The stopword tag used in the graph. \"\"\"", "\n", "\n", "self", ".", "pos_separator", "=", "pos_separator", "\n", "\"\"\" The character (or string) used to separate a word and its Part of Speech tag \"\"\"", "\n", "\n", "self", ".", "graph", "=", "nx", ".", "DiGraph", "(", ")", "\n", "\"\"\" The directed graph used for fusion. \"\"\"", "\n", "\n", "self", ".", "start", "=", "'-start-'", "\n", "\"\"\" The start token in the graph. \"\"\"", "\n", "\n", "self", ".", "stop", "=", "'-end-'", "\n", "\"\"\" The end token in the graph. \"\"\"", "\n", "\n", "self", ".", "sep", "=", "'/-/'", "\n", "\"\"\" The separator used between a word and its POS in the graph. \"\"\"", "\n", "\n", "self", ".", "term_freq", "=", "{", "}", "\n", "\"\"\" The frequency of a given term. \"\"\"", "\n", "\n", "self", ".", "verbs", "=", "set", "(", "[", "'VB'", ",", "'VBD'", ",", "'VBP'", ",", "'VBZ'", ",", "'VH'", ",", "'VHD'", ",", "'VHP'", ",", "'VBZ'", ",", "\n", "'VV'", ",", "'VVD'", ",", "'VVP'", ",", "'VVZ'", "]", ")", "\n", "\"\"\"\n        The list of verb POS tags required in the compression. At least *one* \n        verb must occur in the candidate compressions.\n        \"\"\"", "\n", "\n", "# Replacing default values for French", "\n", "if", "lang", "==", "\"fr\"", ":", "\n", "            ", "self", ".", "verbs", "=", "set", "(", "[", "'V'", ",", "'VPP'", ",", "'VINF'", "]", ")", "\n", "\n", "# 1. Pre-process the sentences", "\n", "", "self", ".", "pre_process_sentences", "(", ")", "\n", "\n", "# 2. Compute term statistics", "\n", "self", ".", "compute_statistics", "(", ")", "\n", "\n", "# 3. Build the word graph", "\n", "self", ".", "build_graph", "(", ")", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.pre_process_sentences": [[203, 241], ["range", "re.sub", "takahe.word_graph.sentence[].strip", "takahe.word_graph.sentence[].split", "container.append", "re.escape", "re.match", "container.append", "re.match.group", "re.match.group", "token.lower"], "methods", ["None"], ["", "def", "pre_process_sentences", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Pre-process the list of sentences given as input. Split sentences using \n        whitespaces and convert each sentence to a list of (word, POS) tuples.\n        \"\"\"", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "length", ")", ":", "\n", "\n", "# Normalise extra white spaces", "\n", "            ", "self", ".", "sentence", "[", "i", "]", "=", "re", ".", "sub", "(", "' +'", ",", "' '", ",", "self", ".", "sentence", "[", "i", "]", ")", "\n", "self", ".", "sentence", "[", "i", "]", "=", "self", ".", "sentence", "[", "i", "]", ".", "strip", "(", ")", "\n", "\n", "# Tokenize the current sentence in word/POS", "\n", "sentence", "=", "self", ".", "sentence", "[", "i", "]", ".", "split", "(", "' '", ")", "\n", "\n", "# Creating an empty container for the cleaned up sentence", "\n", "container", "=", "[", "(", "self", ".", "start", ",", "self", ".", "start", ")", "]", "\n", "\n", "# Looping over the words", "\n", "for", "w", "in", "sentence", ":", "\n", "\n", "# Splitting word, POS", "\n", "                ", "pos_separator_re", "=", "re", ".", "escape", "(", "self", ".", "pos_separator", ")", "\n", "m", "=", "re", ".", "match", "(", "\"^(.+)\"", "+", "pos_separator_re", "+", "\"(.+)$\"", ",", "w", ")", "\n", "\n", "if", "m", "is", "not", "None", ":", "\n", "\n", "# Extract the word information", "\n", "                    ", "token", ",", "POS", "=", "m", ".", "group", "(", "1", ")", ",", "m", ".", "group", "(", "2", ")", "\n", "\n", "# Add the token/POS to the sentence container", "\n", "container", ".", "append", "(", "(", "token", ".", "lower", "(", ")", ",", "POS", ")", ")", "\n", "\n", "# Add the stop token at the end of the container", "\n", "", "", "container", ".", "append", "(", "(", "self", ".", "stop", ",", "self", ".", "stop", ")", ")", "\n", "\n", "# Recopy the container into the current sentence", "\n", "self", ".", "sentence", "[", "i", "]", "=", "container", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.build_graph": [[245, 607], ["range", "takahe.word_graph.graph.edges", "len", "range", "range", "range", "range", "range", "takahe.word_graph.get_edge_weight", "takahe.word_graph.graph.add_edge", "takahe.word_graph.ambiguous_nodes", "takahe.word_graph.ambiguous_nodes", "takahe.word_graph.ambiguous_nodes", "len", "takahe.word_graph.graph.add_edge", "re.search", "takahe.word_graph.graph.add_node", "re.search", "takahe.word_graph.ambiguous_nodes", "range", "takahe.word_graph.graph.add_node", "range", "takahe.word_graph.max_index", "re.search", "takahe.word_graph.graph.add_node", "range", "takahe.word_graph.max_index", "token.lower", "takahe.word_graph.get_directed_context", "takahe.word_graph.get_directed_context", "takahe.word_graph.count", "takahe.word_graph.count", "ambinode_overlap.append", "ambinode_frequency.append", "takahe.word_graph.max_index", "[].append", "takahe.word_graph.graph.add_node", "token.lower", "takahe.word_graph.get_directed_context", "takahe.word_graph.get_directed_context", "takahe.word_graph.count", "takahe.word_graph.count", "ambinode_overlap.append", "ids.append", "[].append", "takahe.word_graph.graph.add_node", "token.lower", "takahe.word_graph.get_directed_context", "takahe.word_graph.get_directed_context", "takahe.word_graph.count", "takahe.word_graph.count", "ambinode_overlap.append", "ids.append", "[].append", "takahe.word_graph.graph.add_node", "token.lower", "ids.append", "[].append", "takahe.word_graph.graph.add_node", "token.lower", "prev_token.lower", "next_token.lower", "len", "takahe.word_graph.max_index", "ids.append", "len", "token.lower", "prev_token.lower", "next_token.lower", "token.lower", "prev_token.lower", "next_token.lower", "token.lower", "token.lower", "token.lower", "token.lower"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_edge_weight", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.ambiguous_nodes", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.ambiguous_nodes", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.ambiguous_nodes", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.ambiguous_nodes", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.max_index", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.max_index", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_directed_context", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_directed_context", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.max_index", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_directed_context", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_directed_context", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_directed_context", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_directed_context", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.max_index"], ["", "", "def", "build_graph", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Constructs a directed word graph from the list of input sentences. Each\n        sentence is iteratively added to the directed graph according to the \n        following algorithm:\n\n        - Word mapping/creation is done in four steps:\n\n            1. non-stopwords for which no candidate exists in the graph or for \n               which an unambiguous mapping is possible or which occur more than\n               once in the sentence\n\n            2. non-stopwords for which there are either several possible\n               candidates in the graph\n\n            3. stopwords\n\n            4. punctuation marks\n\n        For the last three groups of words where mapping is ambiguous we check \n        the immediate context (the preceding and following words in the sentence \n        and the neighboring nodes in the graph) and select the candidate which \n        has larger overlap in the context, or the one with a greater frequency \n        (i.e. the one which has more words mapped onto it). Stopwords are mapped \n        only if there is some overlap in non-stopwords neighbors, otherwise a \n        new node is created. Punctuation marks are mapped only if the preceding \n        and following words in the sentence and the neighboring nodes are the\n        same.\n\n        - Edges are then computed and added between mapped words.\n        \n        Each node in the graph is represented as a tuple ('word/POS', id) and \n        possesses an info list containing (sentence_id, position_in_sentence)\n        tuples.\n        \"\"\"", "\n", "\n", "# Iteratively add each sentence in the graph ---------------------------", "\n", "for", "i", "in", "range", "(", "self", ".", "length", ")", ":", "\n", "\n", "# Compute the sentence length", "\n", "            ", "sentence_len", "=", "len", "(", "self", ".", "sentence", "[", "i", "]", ")", "\n", "\n", "# Create the mapping container", "\n", "mapping", "=", "[", "0", "]", "*", "sentence_len", "\n", "\n", "#-------------------------------------------------------------------", "\n", "# 1. non-stopwords for which no candidate exists in the graph or for ", "\n", "#    which an unambiguous mapping is possible or which occur more ", "\n", "#    than once in the sentence.", "\n", "#-------------------------------------------------------------------", "\n", "for", "j", "in", "range", "(", "sentence_len", ")", ":", "\n", "\n", "# Get the word and tag", "\n", "                ", "token", ",", "POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "]", "\n", "\n", "# If stopword or punctuation mark, continues", "\n", "if", "token", "in", "self", ".", "stopwords", "or", "re", ".", "search", "(", "'(?u)^\\W$'", ",", "token", ")", ":", "\n", "                    ", "continue", "\n", "\n", "# Create the node identifier", "\n", "", "node", "=", "token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "POS", "\n", "\n", "# Find the number of ambiguous nodes in the graph", "\n", "k", "=", "self", ".", "ambiguous_nodes", "(", "node", ")", "\n", "\n", "# If there is no node in the graph, create one with id = 0", "\n", "if", "k", "==", "0", ":", "\n", "\n", "# Add the node in the graph", "\n", "                    ", "self", ".", "graph", ".", "add_node", "(", "(", "node", ",", "0", ")", ",", "info", "=", "[", "(", "i", ",", "j", ")", "]", ",", "\n", "label", "=", "token", ".", "lower", "(", ")", ")", "\n", "\n", "# Mark the word as mapped to k", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "0", ")", "\n", "\n", "# If there is only one matching node in the graph (id is 0)", "\n", "", "elif", "k", "==", "1", ":", "\n", "\n", "# Get the sentences id of this node", "\n", "                    ", "ids", "=", "[", "]", "\n", "for", "sid", ",", "pos_s", "in", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "0", ")", "]", "[", "'info'", "]", ":", "\n", "                        ", "ids", ".", "append", "(", "sid", ")", "\n", "\n", "# Update the node in the graph if not same sentence", "\n", "", "if", "not", "i", "in", "ids", ":", "\n", "                        ", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "0", ")", "]", "[", "'info'", "]", ".", "append", "(", "(", "i", ",", "j", ")", ")", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "0", ")", "\n", "\n", "# Else Create new node for redundant word", "\n", "", "else", ":", "\n", "                        ", "self", ".", "graph", ".", "add_node", "(", "(", "node", ",", "1", ")", ",", "info", "=", "[", "(", "i", ",", "j", ")", "]", ",", "\n", "label", "=", "token", ".", "lower", "(", ")", ")", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "1", ")", "\n", "\n", "#-------------------------------------------------------------------", "\n", "# 2. non-stopwords for which there are either several possible", "\n", "#    candidates in the graph.", "\n", "#-------------------------------------------------------------------", "\n", "", "", "", "for", "j", "in", "range", "(", "sentence_len", ")", ":", "\n", "\n", "# Get the word and tag", "\n", "                ", "token", ",", "POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "]", "\n", "\n", "# If stopword or punctuation mark, continues", "\n", "if", "token", "in", "self", ".", "stopwords", "or", "re", ".", "search", "(", "'(?u)^\\W$'", ",", "token", ")", ":", "\n", "                    ", "continue", "\n", "\n", "# If word is not already mapped to a node", "\n", "", "if", "mapping", "[", "j", "]", "==", "0", ":", "\n", "\n", "# Create the node identifier", "\n", "                    ", "node", "=", "token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "POS", "\n", "\n", "# Create the neighboring nodes identifiers", "\n", "prev_token", ",", "prev_POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "-", "1", "]", "\n", "next_token", ",", "next_POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "+", "1", "]", "\n", "prev_node", "=", "prev_token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "prev_POS", "\n", "next_node", "=", "next_token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "next_POS", "\n", "\n", "# Find the number of ambiguous nodes in the graph", "\n", "k", "=", "self", ".", "ambiguous_nodes", "(", "node", ")", "\n", "\n", "# Search for the ambiguous node with the larger overlap in", "\n", "# context or the greater frequency.", "\n", "ambinode_overlap", "=", "[", "]", "\n", "ambinode_frequency", "=", "[", "]", "\n", "\n", "# For each ambiguous node", "\n", "for", "l", "in", "range", "(", "k", ")", ":", "\n", "\n", "# Get the immediate context words of the nodes", "\n", "                        ", "l_context", "=", "self", ".", "get_directed_context", "(", "node", ",", "l", ",", "'left'", ")", "\n", "r_context", "=", "self", ".", "get_directed_context", "(", "node", ",", "l", ",", "'right'", ")", "\n", "\n", "# Compute the (directed) context sum", "\n", "val", "=", "l_context", ".", "count", "(", "prev_node", ")", "\n", "val", "+=", "r_context", ".", "count", "(", "next_node", ")", "\n", "\n", "# Add the count of the overlapping words", "\n", "ambinode_overlap", ".", "append", "(", "val", ")", "\n", "\n", "# Add the frequency of the ambiguous node", "\n", "ambinode_frequency", ".", "append", "(", "\n", "len", "(", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "l", ")", "]", "[", "'info'", "]", ")", "\n", ")", "\n", "\n", "# Search for the best candidate while avoiding a loop", "\n", "", "found", "=", "False", "\n", "selected", "=", "0", "\n", "while", "not", "found", ":", "\n", "\n", "# Select the ambiguous node", "\n", "                        ", "selected", "=", "self", ".", "max_index", "(", "ambinode_overlap", ")", "\n", "if", "ambinode_overlap", "[", "selected", "]", "==", "0", ":", "\n", "                            ", "selected", "=", "self", ".", "max_index", "(", "ambinode_frequency", ")", "\n", "\n", "# Get the sentences id of this node", "\n", "", "ids", "=", "[", "]", "\n", "for", "sid", ",", "p", "in", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "selected", ")", "]", "[", "'info'", "]", ":", "\n", "                            ", "ids", ".", "append", "(", "sid", ")", "\n", "\n", "# Test if there is no loop", "\n", "", "if", "i", "not", "in", "ids", ":", "\n", "                            ", "found", "=", "True", "\n", "break", "\n", "\n", "# Remove the candidate from the lists", "\n", "", "else", ":", "\n", "                            ", "del", "ambinode_overlap", "[", "selected", "]", "\n", "del", "ambinode_frequency", "[", "selected", "]", "\n", "\n", "# Avoid endless loops", "\n", "", "if", "len", "(", "ambinode_overlap", ")", "==", "0", ":", "\n", "                            ", "break", "\n", "\n", "# Update the node in the graph if not same sentence", "\n", "", "", "if", "found", ":", "\n", "                        ", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "selected", ")", "]", "[", "'info'", "]", ".", "append", "(", "(", "i", ",", "j", ")", ")", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "selected", ")", "\n", "\n", "# Else create new node for redundant word", "\n", "", "else", ":", "\n", "                        ", "self", ".", "graph", ".", "add_node", "(", "(", "node", ",", "k", ")", ",", "info", "=", "[", "(", "i", ",", "j", ")", "]", ",", "\n", "label", "=", "token", ".", "lower", "(", ")", ")", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "k", ")", "\n", "\n", "#-------------------------------------------------------------------", "\n", "# 3. map the stopwords to the nodes", "\n", "#-------------------------------------------------------------------", "\n", "", "", "", "for", "j", "in", "range", "(", "sentence_len", ")", ":", "\n", "\n", "# Get the word and tag", "\n", "                ", "token", ",", "POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "]", "\n", "\n", "# If *NOT* stopword, continues", "\n", "if", "not", "token", "in", "self", ".", "stopwords", ":", "\n", "                    ", "continue", "\n", "\n", "# Create the node identifier", "\n", "", "node", "=", "token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "POS", "\n", "\n", "# Find the number of ambiguous nodes in the graph", "\n", "k", "=", "self", ".", "ambiguous_nodes", "(", "node", ")", "\n", "\n", "# If there is no node in the graph, create one with id = 0", "\n", "if", "k", "==", "0", ":", "\n", "\n", "# Add the node in the graph", "\n", "                    ", "self", ".", "graph", ".", "add_node", "(", "(", "node", ",", "0", ")", ",", "info", "=", "[", "(", "i", ",", "j", ")", "]", ",", "\n", "label", "=", "token", ".", "lower", "(", ")", ")", "\n", "\n", "# Mark the word as mapped to k", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "0", ")", "\n", "\n", "# Else find the node with overlap in context or create one", "\n", "", "else", ":", "\n", "\n", "# Create the neighboring nodes identifiers", "\n", "                    ", "prev_token", ",", "prev_POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "-", "1", "]", "\n", "next_token", ",", "next_POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "+", "1", "]", "\n", "prev_node", "=", "prev_token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "prev_POS", "\n", "next_node", "=", "next_token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "next_POS", "\n", "\n", "ambinode_overlap", "=", "[", "]", "\n", "\n", "# For each ambiguous node", "\n", "for", "l", "in", "range", "(", "k", ")", ":", "\n", "\n", "# Get the immediate context words of the nodes, the", "\n", "# boolean indicates to consider only non stopwords", "\n", "                        ", "l_context", "=", "self", ".", "get_directed_context", "(", "node", ",", "l", ",", "'left'", ",", "True", ")", "\n", "r_context", "=", "self", ".", "get_directed_context", "(", "node", ",", "l", ",", "'right'", ",", "True", ")", "\n", "\n", "# Compute the (directed) context sum", "\n", "val", "=", "l_context", ".", "count", "(", "prev_node", ")", "\n", "val", "+=", "r_context", ".", "count", "(", "next_node", ")", "\n", "\n", "# Add the count of the overlapping words", "\n", "ambinode_overlap", ".", "append", "(", "val", ")", "\n", "\n", "# Get best overlap candidate", "\n", "", "selected", "=", "self", ".", "max_index", "(", "ambinode_overlap", ")", "\n", "\n", "# Get the sentences id of the best candidate node", "\n", "ids", "=", "[", "]", "\n", "for", "sid", ",", "pos_s", "in", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "selected", ")", "]", "[", "'info'", "]", ":", "\n", "                        ", "ids", ".", "append", "(", "sid", ")", "\n", "\n", "# Update the node in the graph if not same sentence and ", "\n", "# there is at least one overlap in context", "\n", "", "if", "i", "not", "in", "ids", "and", "ambinode_overlap", "[", "selected", "]", ">", "0", ":", "\n", "# if i not in ids and \\", "\n", "# (ambinode_overlap[selected] > 1 and POS==self.punct_tag) or\\", "\n", "# (ambinode_overlap[selected] > 0 and POS!=self.punct_tag) :", "\n", "\n", "# Update the node in the graph", "\n", "                        ", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "selected", ")", "]", "[", "'info'", "]", ".", "append", "(", "(", "i", ",", "j", ")", ")", "\n", "\n", "# Mark the word as mapped to k", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "selected", ")", "\n", "\n", "# Else create a new node", "\n", "", "else", ":", "\n", "# Add the node in the graph", "\n", "                        ", "self", ".", "graph", ".", "add_node", "(", "(", "node", ",", "k", ")", ",", "info", "=", "[", "(", "i", ",", "j", ")", "]", ",", "\n", "label", "=", "token", ".", "lower", "(", ")", ")", "\n", "\n", "# Mark the word as mapped to k", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "k", ")", "\n", "\n", "#-------------------------------------------------------------------", "\n", "# 4. lasty map the punctuation marks to the nodes", "\n", "#-------------------------------------------------------------------", "\n", "", "", "", "for", "j", "in", "range", "(", "sentence_len", ")", ":", "\n", "\n", "# Get the word and tag", "\n", "                ", "token", ",", "POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "]", "\n", "\n", "# If *NOT* punctuation mark, continues", "\n", "if", "not", "re", ".", "search", "(", "'(?u)^\\W$'", ",", "token", ")", ":", "\n", "                    ", "continue", "\n", "\n", "# Create the node identifier", "\n", "", "node", "=", "token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "POS", "\n", "\n", "# Find the number of ambiguous nodes in the graph", "\n", "k", "=", "self", ".", "ambiguous_nodes", "(", "node", ")", "\n", "\n", "# If there is no node in the graph, create one with id = 0", "\n", "if", "k", "==", "0", ":", "\n", "\n", "# Add the node in the graph", "\n", "                    ", "self", ".", "graph", ".", "add_node", "(", "(", "node", ",", "0", ")", ",", "info", "=", "[", "(", "i", ",", "j", ")", "]", ",", "\n", "label", "=", "token", ".", "lower", "(", ")", ")", "\n", "\n", "# Mark the word as mapped to k", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "0", ")", "\n", "\n", "# Else find the node with overlap in context or create one", "\n", "", "else", ":", "\n", "\n", "# Create the neighboring nodes identifiers", "\n", "                    ", "prev_token", ",", "prev_POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "-", "1", "]", "\n", "next_token", ",", "next_POS", "=", "self", ".", "sentence", "[", "i", "]", "[", "j", "+", "1", "]", "\n", "prev_node", "=", "prev_token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "prev_POS", "\n", "next_node", "=", "next_token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "next_POS", "\n", "\n", "ambinode_overlap", "=", "[", "]", "\n", "\n", "# For each ambiguous node", "\n", "for", "l", "in", "range", "(", "k", ")", ":", "\n", "\n", "# Get the immediate context words of the nodes", "\n", "                        ", "l_context", "=", "self", ".", "get_directed_context", "(", "node", ",", "l", ",", "'left'", ")", "\n", "r_context", "=", "self", ".", "get_directed_context", "(", "node", ",", "l", ",", "'right'", ")", "\n", "\n", "# Compute the (directed) context sum", "\n", "val", "=", "l_context", ".", "count", "(", "prev_node", ")", "\n", "val", "+=", "r_context", ".", "count", "(", "next_node", ")", "\n", "\n", "# Add the count of the overlapping words", "\n", "ambinode_overlap", ".", "append", "(", "val", ")", "\n", "\n", "# Get best overlap candidate", "\n", "", "selected", "=", "self", ".", "max_index", "(", "ambinode_overlap", ")", "\n", "\n", "# Get the sentences id of the best candidate node", "\n", "ids", "=", "[", "]", "\n", "for", "sid", ",", "pos_s", "in", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "selected", ")", "]", "[", "'info'", "]", ":", "\n", "                        ", "ids", ".", "append", "(", "sid", ")", "\n", "\n", "# Update the node in the graph if not same sentence and ", "\n", "# there is at least one overlap in context", "\n", "", "if", "i", "not", "in", "ids", "and", "ambinode_overlap", "[", "selected", "]", ">", "1", ":", "\n", "\n", "# Update the node in the graph", "\n", "                        ", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "selected", ")", "]", "[", "'info'", "]", ".", "append", "(", "(", "i", ",", "j", ")", ")", "\n", "\n", "# Mark the word as mapped to k", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "selected", ")", "\n", "\n", "# Else create a new node", "\n", "", "else", ":", "\n", "# Add the node in the graph", "\n", "                        ", "self", ".", "graph", ".", "add_node", "(", "(", "node", ",", "k", ")", ",", "info", "=", "[", "(", "i", ",", "j", ")", "]", ",", "\n", "label", "=", "token", ".", "lower", "(", ")", ")", "\n", "\n", "# Mark the word as mapped to k", "\n", "mapping", "[", "j", "]", "=", "(", "node", ",", "k", ")", "\n", "\n", "#-------------------------------------------------------------------", "\n", "# 4. Connects the mapped words with directed edges", "\n", "#-------------------------------------------------------------------", "\n", "", "", "", "for", "j", "in", "range", "(", "1", ",", "len", "(", "mapping", ")", ")", ":", "\n", "                ", "self", ".", "graph", ".", "add_edge", "(", "mapping", "[", "j", "-", "1", "]", ",", "mapping", "[", "j", "]", ")", "\n", "\n", "# Assigns a weight to each node in the graph ---------------------------", "\n", "", "", "for", "node1", ",", "node2", "in", "self", ".", "graph", ".", "edges", "(", ")", ":", "\n", "            ", "edge_weight", "=", "self", ".", "get_edge_weight", "(", "node1", ",", "node2", ")", "\n", "self", ".", "graph", ".", "add_edge", "(", "node1", ",", "node2", ",", "weight", "=", "edge_weight", ")", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.ambiguous_nodes": [[611, 620], ["takahe.word_graph.graph.has_node"], "methods", ["None"], ["", "", "def", "ambiguous_nodes", "(", "self", ",", "node", ")", ":", "\n", "        ", "\"\"\"\n        Takes a node in parameter and returns the number of possible candidate \n        (ambiguous) nodes in the graph.\n        \"\"\"", "\n", "k", "=", "0", "\n", "while", "(", "self", ".", "graph", ".", "has_node", "(", "(", "node", ",", "k", ")", ")", ")", ":", "\n", "            ", "k", "+=", "1", "\n", "", "return", "k", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_directed_context": [[624, 671], ["l_context.append", "r_context.append", "l_context.extend", "[].lower", "[].lower", "l_context.append", "r_context.append"], "methods", ["None"], ["", "def", "get_directed_context", "(", "self", ",", "node", ",", "k", ",", "dir", "=", "'all'", ",", "non_pos", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        Returns the directed context of a given node, i.e. a list of word/POS of\n        the left or right neighboring nodes in the graph. The function takes \n        four parameters :\n\n        - node is the word/POS tuple\n        - k is the node identifier used when multiple nodes refer to the same \n          word/POS (e.g. k=0 for (the/DET, 0), k=1 for (the/DET, 1), etc.)\n        - dir is the parameter that controls the directed context calculation, \n          it can be set to left, right or all (default)\n        - non_pos is a boolean allowing to remove stopwords from the context \n          (default is false)\n        \"\"\"", "\n", "\n", "# Define the context containers", "\n", "l_context", "=", "[", "]", "\n", "r_context", "=", "[", "]", "\n", "\n", "# For all the sentence/position tuples", "\n", "for", "sid", ",", "off", "in", "self", ".", "graph", ".", "nodes", "[", "(", "node", ",", "k", ")", "]", "[", "'info'", "]", ":", "\n", "\n", "            ", "prev", "=", "self", ".", "sentence", "[", "sid", "]", "[", "off", "-", "1", "]", "[", "0", "]", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "self", ".", "sentence", "[", "sid", "]", "[", "off", "-", "1", "]", "[", "1", "]", "\n", "\n", "next", "=", "self", ".", "sentence", "[", "sid", "]", "[", "off", "+", "1", "]", "[", "0", "]", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "self", ".", "sentence", "[", "sid", "]", "[", "off", "+", "1", "]", "[", "1", "]", "\n", "\n", "if", "non_pos", ":", "\n", "                ", "if", "self", ".", "sentence", "[", "sid", "]", "[", "off", "-", "1", "]", "[", "0", "]", "not", "in", "self", ".", "stopwords", ":", "\n", "                    ", "l_context", ".", "append", "(", "prev", ")", "\n", "", "if", "self", ".", "sentence", "[", "sid", "]", "[", "off", "+", "1", "]", "[", "0", "]", "not", "in", "self", ".", "stopwords", ":", "\n", "                    ", "r_context", ".", "append", "(", "next", ")", "\n", "", "", "else", ":", "\n", "                ", "l_context", ".", "append", "(", "prev", ")", "\n", "r_context", ".", "append", "(", "next", ")", "\n", "\n", "# Returns the left (previous) context", "\n", "", "", "if", "dir", "==", "'left'", ":", "\n", "            ", "return", "l_context", "\n", "# Returns the right (next) context", "\n", "", "elif", "dir", "==", "'right'", ":", "\n", "            ", "return", "r_context", "\n", "# Returns the whole context", "\n", "", "else", ":", "\n", "            ", "l_context", ".", "extend", "(", "r_context", ")", "\n", "return", "l_context", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_edge_weight": [[675, 753], ["len", "len", "range", "range", "len", "range", "len", "diff.append", "diff.append", "sum", "pos_i_in_s.append", "pos_j_in_s.append", "len", "all_diff_pos_i_j.append", "min"], "methods", ["None"], ["", "", "def", "get_edge_weight", "(", "self", ",", "node1", ",", "node2", ")", ":", "\n", "        ", "\"\"\"\n        Compute the weight of an edge *e* between nodes *node1* and *node2*. It \n        is computed as e_ij = (A / B) / C with:\n        \n        - A = freq(i) + freq(j), \n        - B = Sum (s in S) 1 / diff(s, i, j)\n        - C = freq(i) * freq(j)\n        \n        A node is a tuple of ('word/POS', unique_id).\n        \"\"\"", "\n", "\n", "# Get the list of (sentence_id, pos_in_sentence) for node1", "\n", "info1", "=", "self", ".", "graph", ".", "nodes", "[", "node1", "]", "[", "'info'", "]", "\n", "\n", "# Get the list of (sentence_id, pos_in_sentence) for node2", "\n", "info2", "=", "self", ".", "graph", ".", "nodes", "[", "node2", "]", "[", "'info'", "]", "\n", "\n", "# Get the frequency of node1 in the graph", "\n", "# freq1 = self.graph.degree(node1)", "\n", "freq1", "=", "len", "(", "info1", ")", "\n", "\n", "# Get the frequency of node2 in cluster", "\n", "# freq2 = self.graph.degree(node2)", "\n", "freq2", "=", "len", "(", "info2", ")", "\n", "\n", "# Initializing the diff function list container", "\n", "diff", "=", "[", "]", "\n", "\n", "# For each sentence of the cluster (for s in S)", "\n", "for", "s", "in", "range", "(", "self", ".", "length", ")", ":", "\n", "\n", "# Compute diff(s, i, j) which is calculated as", "\n", "# pos(s, i) - pos(s, j) if pos(s, i) < pos(s, j)", "\n", "# O otherwise", "\n", "\n", "# Get the positions of i and j in s, named pos(s, i) and pos(s, j)", "\n", "# As a word can appear at multiple positions in a sentence, a list", "\n", "# of positions is used", "\n", "            ", "pos_i_in_s", "=", "[", "]", "\n", "pos_j_in_s", "=", "[", "]", "\n", "\n", "# For each (sentence_id, pos_in_sentence) of node1", "\n", "for", "sentence_id", ",", "pos_in_sentence", "in", "info1", ":", "\n", "# If the sentence_id is s", "\n", "                ", "if", "sentence_id", "==", "s", ":", "\n", "# Add the position in s", "\n", "                    ", "pos_i_in_s", ".", "append", "(", "pos_in_sentence", ")", "\n", "\n", "# For each (sentence_id, pos_in_sentence) of node2", "\n", "", "", "for", "sentence_id", ",", "pos_in_sentence", "in", "info2", ":", "\n", "# If the sentence_id is s", "\n", "                ", "if", "sentence_id", "==", "s", ":", "\n", "# Add the position in s", "\n", "                    ", "pos_j_in_s", ".", "append", "(", "pos_in_sentence", ")", "\n", "\n", "# Container for all the diff(s, i, j) for i and j", "\n", "", "", "all_diff_pos_i_j", "=", "[", "]", "\n", "\n", "# Loop over all the i, j couples", "\n", "for", "x", "in", "range", "(", "len", "(", "pos_i_in_s", ")", ")", ":", "\n", "                ", "for", "y", "in", "range", "(", "len", "(", "pos_j_in_s", ")", ")", ":", "\n", "                    ", "diff_i_j", "=", "pos_i_in_s", "[", "x", "]", "-", "pos_j_in_s", "[", "y", "]", "\n", "# Test if word i appears *BEFORE* word j in s", "\n", "if", "diff_i_j", "<", "0", ":", "\n", "                        ", "all_diff_pos_i_j", ".", "append", "(", "-", "1.0", "*", "diff_i_j", ")", "\n", "\n", "# Add the mininum distance to diff (i.e. in case of multiple ", "\n", "# occurrencies of i or/and j in sentence s), 0 otherwise.", "\n", "", "", "", "if", "len", "(", "all_diff_pos_i_j", ")", ">", "0", ":", "\n", "                ", "diff", ".", "append", "(", "1.0", "/", "min", "(", "all_diff_pos_i_j", ")", ")", "\n", "", "else", ":", "\n", "                ", "diff", ".", "append", "(", "0.0", ")", "\n", "\n", "", "", "weight1", "=", "freq1", "\n", "weight2", "=", "freq2", "\n", "\n", "return", "(", "(", "freq1", "+", "freq2", ")", "/", "sum", "(", "diff", ")", ")", "/", "(", "weight1", "*", "weight2", ")", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.k_shortest_paths": [[757, 876], ["orderedX.append", "orderedX.pop", "takahe.word_graph.graph.neighbors", "len", "len", "range", "raw_sentence.strip.strip.strip", "visited.__contains__", "bisect.insort", "paths[].extend", "[].split", "path.extend", "path.reverse", "float", "kshortestpaths.append", "len", "re.search", "sentence_container.__contains__"], "methods", ["None"], ["", "def", "k_shortest_paths", "(", "self", ",", "start", ",", "end", ",", "k", "=", "10", ")", ":", "\n", "        ", "\"\"\"\n        Simple implementation of a k-shortest paths algorithms. Takes three\n        parameters: the starting node, the ending node and the number of \n        shortest paths desired. Returns a list of k tuples (path, weight).\n        \"\"\"", "\n", "\n", "# Initialize the list of shortest paths", "\n", "kshortestpaths", "=", "[", "]", "\n", "\n", "# Initializing the label container ", "\n", "orderedX", "=", "[", "]", "\n", "orderedX", ".", "append", "(", "(", "0", ",", "start", ",", "0", ")", ")", "\n", "\n", "# Initializing the path container", "\n", "paths", "=", "{", "}", "\n", "paths", "[", "(", "0", ",", "start", ",", "0", ")", "]", "=", "[", "start", "]", "\n", "\n", "# Initialize the visited container", "\n", "visited", "=", "{", "}", "\n", "visited", "[", "start", "]", "=", "0", "\n", "\n", "# Initialize the sentence container that will be used to remove ", "\n", "# duplicate sentences passing throught different nodes", "\n", "sentence_container", "=", "{", "}", "\n", "\n", "# While the number of shortest paths isn't reached or all paths explored", "\n", "while", "len", "(", "kshortestpaths", ")", "<", "k", "and", "len", "(", "orderedX", ")", ">", "0", ":", "\n", "\n", "# Searching for the shortest distance in orderedX", "\n", "            ", "shortest", "=", "orderedX", ".", "pop", "(", "0", ")", "\n", "shortestpath", "=", "paths", "[", "shortest", "]", "\n", "\n", "# Removing the shortest node from X and paths", "\n", "del", "paths", "[", "shortest", "]", "\n", "\n", "# Iterating over the accessible nodes", "\n", "for", "node", "in", "self", ".", "graph", ".", "neighbors", "(", "shortest", "[", "1", "]", ")", ":", "\n", "\n", "# To avoid loops", "\n", "                ", "if", "node", "in", "shortestpath", ":", "\n", "                    ", "continue", "\n", "\n", "# Compute the weight to node", "\n", "", "w", "=", "shortest", "[", "0", "]", "+", "self", ".", "graph", "[", "shortest", "[", "1", "]", "]", "[", "node", "]", "[", "'weight'", "]", "\n", "\n", "# If found the end, adds to k-shortest paths ", "\n", "if", "node", "==", "end", ":", "\n", "\n", "#-T-------------------------------------------------------T-", "\n", "# --- Constraints on the shortest paths", "\n", "\n", "# 1. Check if path contains at least one werb", "\n", "# 2. Check the length of the shortest path, without ", "\n", "#    considering punctuation marks and starting node (-1 in", "\n", "#    the range loop, because nodes are reversed)", "\n", "# 3. Check the paired parentheses and quotation marks", "\n", "# 4. Check if sentence is not redundant", "\n", "\n", "                    ", "nb_verbs", "=", "0", "\n", "length", "=", "0", "\n", "paired_parentheses", "=", "0", "\n", "quotation_mark_number", "=", "0", "\n", "raw_sentence", "=", "''", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "shortestpath", ")", "-", "1", ")", ":", "\n", "                        ", "word", ",", "tag", "=", "shortestpath", "[", "i", "]", "[", "0", "]", ".", "split", "(", "self", ".", "sep", ")", "\n", "# 1.", "\n", "if", "tag", "in", "self", ".", "verbs", ":", "\n", "                            ", "nb_verbs", "+=", "1", "\n", "# 2.", "\n", "", "if", "not", "re", ".", "search", "(", "'(?u)^\\W$'", ",", "word", ")", ":", "\n", "                            ", "length", "+=", "1", "\n", "# 3.", "\n", "", "else", ":", "\n", "                            ", "if", "word", "==", "'('", ":", "\n", "                                ", "paired_parentheses", "-=", "1", "\n", "", "elif", "word", "==", "')'", ":", "\n", "                                ", "paired_parentheses", "+=", "1", "\n", "", "elif", "word", "==", "'\"'", ":", "\n", "                                ", "quotation_mark_number", "+=", "1", "\n", "# 4.", "\n", "", "", "raw_sentence", "+=", "word", "+", "' '", "\n", "\n", "# Remove extra space from sentence", "\n", "", "raw_sentence", "=", "raw_sentence", ".", "strip", "(", ")", "\n", "\n", "if", "nb_verbs", ">", "0", "and", "length", ">=", "self", ".", "nb_words", "and", "paired_parentheses", "==", "0", "and", "(", "quotation_mark_number", "%", "2", ")", "==", "0", "and", "not", "sentence_container", ".", "__contains__", "(", "raw_sentence", ")", ":", "\n", "                        ", "path", "=", "[", "node", "]", "\n", "path", ".", "extend", "(", "shortestpath", ")", "\n", "path", ".", "reverse", "(", ")", "\n", "weight", "=", "float", "(", "w", ")", "#/ float(length)", "\n", "kshortestpaths", ".", "append", "(", "(", "path", ",", "weight", ")", ")", "\n", "sentence_container", "[", "raw_sentence", "]", "=", "1", "\n", "\n", "#-B-------------------------------------------------------B-", "\n", "\n", "", "", "else", ":", "\n", "\n", "# test if node has already been visited", "\n", "                    ", "if", "visited", ".", "__contains__", "(", "node", ")", ":", "\n", "                        ", "visited", "[", "node", "]", "+=", "1", "\n", "", "else", ":", "\n", "                        ", "visited", "[", "node", "]", "=", "0", "\n", "", "id", "=", "visited", "[", "node", "]", "\n", "\n", "# Add the node to orderedX", "\n", "bisect", ".", "insort", "(", "orderedX", ",", "(", "w", ",", "node", ",", "id", ")", ")", "\n", "\n", "# Add the node to paths", "\n", "paths", "[", "(", "w", ",", "node", ",", "id", ")", "]", "=", "[", "node", "]", "\n", "paths", "[", "(", "w", ",", "node", ",", "id", ")", "]", ".", "extend", "(", "shortestpath", ")", "\n", "\n", "# Returns the list of shortest paths", "\n", "", "", "", "return", "kshortestpaths", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_compression": [[879, 911], ["takahe.word_graph.k_shortest_paths", "len", "range", "min", "range", "bisect.insort", "len", "[].split", "sentence.append", "len"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.k_shortest_paths"], ["", "def", "get_compression", "(", "self", ",", "nb_candidates", "=", "50", ")", ":", "\n", "        ", "\"\"\"\n        Searches all possible paths from **start** to **end** in the word graph,\n        removes paths containing no verb or shorter than *n* words. Returns an\n        ordered list (smaller first) of nb (default value is 50) (cummulative \n        score, path) tuples. The score is not normalized with the sentence \n        length.\n        \"\"\"", "\n", "\n", "# Search for the k-shortest paths in the graph", "\n", "self", ".", "paths", "=", "self", ".", "k_shortest_paths", "(", "(", "self", ".", "start", "+", "self", ".", "sep", "+", "self", ".", "start", ",", "0", ")", ",", "\n", "(", "self", ".", "stop", "+", "self", ".", "sep", "+", "self", ".", "stop", ",", "0", ")", ",", "\n", "nb_candidates", ")", "\n", "\n", "# Initialize the fusion container", "\n", "fusions", "=", "[", "]", "\n", "\n", "# Test if there are some paths", "\n", "if", "len", "(", "self", ".", "paths", ")", ">", "0", ":", "\n", "\n", "# For nb candidates ", "\n", "            ", "for", "i", "in", "range", "(", "min", "(", "nb_candidates", ",", "len", "(", "self", ".", "paths", ")", ")", ")", ":", "\n", "                ", "nodes", "=", "self", ".", "paths", "[", "i", "]", "[", "0", "]", "\n", "sentence", "=", "[", "]", "\n", "\n", "for", "j", "in", "range", "(", "1", ",", "len", "(", "nodes", ")", "-", "1", ")", ":", "\n", "                    ", "word", ",", "tag", "=", "nodes", "[", "j", "]", "[", "0", "]", ".", "split", "(", "self", ".", "sep", ")", "\n", "sentence", ".", "append", "(", "(", "word", ",", "tag", ")", ")", "\n", "\n", "", "bisect", ".", "insort", "(", "fusions", ",", "(", "self", ".", "paths", "[", "i", "]", "[", "1", "]", ",", "sentence", ")", ")", "\n", "\n", "", "", "return", "fusions", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.max_index": [[914, 929], ["len", "range"], "methods", ["None"], ["", "def", "max_index", "(", "self", ",", "l", ")", ":", "\n", "        ", "\"\"\" Returns the index of the maximum value of a given list. \"\"\"", "\n", "\n", "ll", "=", "len", "(", "l", ")", "\n", "if", "ll", "<", "0", ":", "\n", "            ", "return", "None", "\n", "", "elif", "ll", "==", "1", ":", "\n", "            ", "return", "0", "\n", "", "max_val", "=", "l", "[", "0", "]", "\n", "max_ind", "=", "0", "\n", "for", "z", "in", "range", "(", "1", ",", "ll", ")", ":", "\n", "            ", "if", "l", "[", "z", "]", ">", "max_val", ":", "\n", "                ", "max_val", "=", "l", "[", "z", "]", "\n", "max_ind", "=", "z", "\n", "", "", "return", "max_ind", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.compute_statistics": [[933, 965], ["range", "len", "terms.__contains__", "terms[].append", "token.lower"], "methods", ["None"], ["", "def", "compute_statistics", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        This function iterates over the cluster's sentences and computes the\n        following statistics about each word:\n        \n        - term frequency (self.term_freq)\n        \"\"\"", "\n", "\n", "# Structure for containing the list of sentences in which a term occurs", "\n", "terms", "=", "{", "}", "\n", "\n", "# Loop over the sentences", "\n", "for", "i", "in", "range", "(", "self", ".", "length", ")", ":", "\n", "\n", "# For each tuple (token, POS) of sentence i", "\n", "            ", "for", "token", ",", "POS", "in", "self", ".", "sentence", "[", "i", "]", ":", "\n", "\n", "# generate the word/POS token", "\n", "                ", "node", "=", "token", ".", "lower", "(", ")", "+", "self", ".", "sep", "+", "POS", "\n", "\n", "# Add the token to the terms list , ", "\n", "#in python3, has_key(key) is replaced by __contains__(key)", "\n", "if", "not", "terms", ".", "__contains__", "(", "node", ")", ":", "\n", "                    ", "terms", "[", "node", "]", "=", "[", "i", "]", "\n", "", "else", ":", "\n", "                    ", "terms", "[", "node", "]", ".", "append", "(", "i", ")", "\n", "\n", "# Loop over the terms", "\n", "", "", "", "for", "w", "in", "terms", ":", "\n", "\n", "# Compute the term frequency", "\n", "            ", "self", ".", "term_freq", "[", "w", "]", "=", "len", "(", "terms", "[", "w", "]", ")", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.load_stopwords": [[969, 985], ["set", "codecs.open", "set.add", "re.search", "len", "line.strip().lower", "line.strip", "line.strip"], "methods", ["None"], ["", "", "def", "load_stopwords", "(", "self", ",", "path", ")", ":", "\n", "        ", "\"\"\"\n        This function loads a stopword list from the *path* file and returns a \n        set of words. Lines begining by '#' are ignored.\n        \"\"\"", "\n", "\n", "# Set of stopwords", "\n", "stopwords", "=", "set", "(", "[", "]", ")", "\n", "\n", "# For each line in the file", "\n", "for", "line", "in", "codecs", ".", "open", "(", "path", ",", "'r'", ",", "'utf-8'", ")", ":", "\n", "            ", "if", "not", "re", ".", "search", "(", "'^#'", ",", "line", ")", "and", "len", "(", "line", ".", "strip", "(", ")", ")", ">", "0", ":", "\n", "                ", "stopwords", ".", "add", "(", "line", ".", "strip", "(", ")", ".", "lower", "(", ")", ")", "\n", "\n", "# Return the set of stopwords", "\n", "", "", "return", "stopwords", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.write_into_dot": [[989, 992], ["networkx.drawing.nx_pydot.write_dot"], "methods", ["None"], ["", "def", "write_into_dot", "(", "self", ",", "dotfile", ")", ":", "\n", "        ", "\"\"\" Outputs the word graph in dot format in the specified file. \"\"\"", "\n", "write_dot", "(", "self", ".", "graph", ",", "dotfile", ")", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.__init__": [[1031, 1090], ["list", "networkx.Graph", "set", "takahe.keyphrase_reranker.syntactic_patterns.extend", "takahe.keyphrase_reranker.build_graph", "takahe.keyphrase_reranker.generate_candidates", "takahe.keyphrase_reranker.undirected_TextRank", "takahe.keyphrase_reranker.score_keyphrase_candidates", "takahe.keyphrase_reranker.cluster_keyphrase_candidates"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.build_graph", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.generate_candidates", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.undirected_TextRank", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.score_keyphrase_candidates", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.cluster_keyphrase_candidates"], ["def", "__init__", "(", "self", ",", "sentence_list", ",", "nbest_compressions", ",", "lang", "=", "\"en\"", ",", "\n", "patterns", "=", "[", "]", ",", "stopwords", "=", "[", "]", ",", "pos_separator", "=", "'/'", ")", ":", "\n", "\n", "        ", "self", ".", "sentences", "=", "list", "(", "sentence_list", ")", "\n", "\"\"\" The list of related sentences provided by the user. \"\"\"", "\n", "\n", "self", ".", "nbest_compressions", "=", "nbest_compressions", "\n", "\"\"\" The nbest compression candidates provided by the user. \"\"\"", "\n", "\n", "self", ".", "graph", "=", "nx", ".", "Graph", "(", ")", "\n", "\"\"\" The graph used for keyphrase extraction. \"\"\"", "\n", "\n", "self", ".", "lang", "=", "lang", "\n", "\"\"\" The language of the input sentences, default is English (en).\"\"\"", "\n", "\n", "self", ".", "stopwords", "=", "set", "(", "stopwords", ")", "\n", "\"\"\" The set of words to be excluded from keyphrase extraction. \"\"\"", "\n", "\n", "self", ".", "pos_separator", "=", "pos_separator", "\n", "\"\"\" The character (or string) used to separate a word and its\n        Part Of Speech tag. \"\"\"", "\n", "\n", "self", ".", "syntactic_filter", "=", "[", "'JJ'", ",", "'NNP'", ",", "'NNS'", ",", "'NN'", ",", "'NNPS'", "]", "\n", "\"\"\" The POS tags used for generating keyphrase candidates. \"\"\"", "\n", "\n", "self", ".", "keyphrase_candidates", "=", "{", "}", "\n", "\"\"\" Keyphrase candidates generated from the set of sentences. \"\"\"", "\n", "\n", "self", ".", "word_scores", "=", "{", "}", "\n", "\"\"\" Scores for each word computed with TextRank. \"\"\"", "\n", "\n", "self", ".", "keyphrase_scores", "=", "{", "}", "\n", "\"\"\" Scores for each keyphrase candidate. \"\"\"", "\n", "\n", "self", ".", "syntactic_patterns", "=", "[", "'^(JJ)*(NNP|NNS|NN)+$'", "]", "\n", "\"\"\" Syntactic patterns for filtering keyphrase candidates. \"\"\"", "\n", "\n", "# Specific rules for French", "\n", "if", "self", ".", "lang", "==", "\"fr\"", ":", "\n", "            ", "self", ".", "syntactic_filter", "=", "[", "'NPP'", ",", "'NC'", ",", "'ADJ'", "]", "\n", "self", ".", "syntactic_patterns", "=", "[", "'^(ADJ)*(NC|NPP)+(ADJ)*$'", "]", "\n", "\n", "# Add extra patterns", "\n", "", "self", ".", "syntactic_patterns", ".", "extend", "(", "patterns", ")", "\n", "\n", "# 1. Build the word graph from the sentences", "\n", "self", ".", "build_graph", "(", ")", "\n", "\n", "# 2. Generate the keyphrase candidates", "\n", "self", ".", "generate_candidates", "(", ")", "\n", "\n", "# 3. Compute the TextRank scores for each word in the graph", "\n", "self", ".", "undirected_TextRank", "(", ")", "\n", "\n", "# 4. Compute the score of each keyphrase candidate", "\n", "self", ".", "score_keyphrase_candidates", "(", ")", "\n", "\n", "# 5. Cluster keyphrases to remove redundancy", "\n", "self", ".", "cluster_keyphrase_candidates", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.build_graph": [[1095, 1163], ["range", "len", "re.sub", "takahe.keyphrase_reranker.sentences[].split", "range", "range", "len", "takahe.keyphrase_reranker.wordpos_to_tuple", "len", "range", "word.lower", "len", "min", "takahe.keyphrase_reranker.graph.has_node", "takahe.keyphrase_reranker.graph.add_node", "len", "takahe.keyphrase_reranker.graph.has_node", "takahe.keyphrase_reranker.graph.has_node", "takahe.keyphrase_reranker.graph.has_edge", "takahe.keyphrase_reranker.graph.add_edge"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.wordpos_to_tuple"], ["", "def", "build_graph", "(", "self", ",", "window", "=", "0", ")", ":", "\n", "        ", "\"\"\"\n        Build a word graph from the list of sentences. Each node in the graph \n        represents a word. An edge is created between two nodes if they co-occur\n        in a given window (default is 0, indicating the whole sentence).\n        \"\"\"", "\n", "\n", "# For each sentence ", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "sentences", ")", ")", ":", "\n", "\n", "# Normalise extra white spaces", "\n", "            ", "self", ".", "sentences", "[", "i", "]", "=", "re", ".", "sub", "(", "' +'", ",", "' '", ",", "self", ".", "sentences", "[", "i", "]", ")", "\n", "\n", "# Tokenize the current sentence in word/POS", "\n", "sentence", "=", "self", ".", "sentences", "[", "i", "]", ".", "split", "(", "' '", ")", "\n", "\n", "# 1. Looping over the words and creating the nodes. Sentences are", "\n", "#    also converted to a list of tuples", "\n", "for", "j", "in", "range", "(", "len", "(", "sentence", ")", ")", ":", "\n", "\n", "# Convert word/POS to (word, POS) tuple", "\n", "                ", "word", ",", "pos", "=", "self", ".", "wordpos_to_tuple", "(", "sentence", "[", "j", "]", ")", "\n", "\n", "# Replace word/POS by (word, POS) tuple in the sentence", "\n", "sentence", "[", "j", "]", "=", "(", "word", ".", "lower", "(", ")", ",", "pos", ")", "\n", "\n", "# Modify the POS tags of stopwords to exclude them", "\n", "if", "sentence", "[", "j", "]", "[", "0", "]", "in", "self", ".", "stopwords", ":", "\n", "                    ", "sentence", "[", "j", "]", "=", "(", "sentence", "[", "j", "]", "[", "0", "]", ",", "\"STOPWORD\"", ")", "\n", "\n", "# Add the word only if it belongs to one of the syntactic ", "\n", "# categories", "\n", "", "if", "sentence", "[", "j", "]", "[", "1", "]", "in", "self", ".", "syntactic_filter", ":", "\n", "\n", "# Add node to the graph if not exists", "\n", "                    ", "if", "not", "self", ".", "graph", ".", "has_node", "(", "sentence", "[", "j", "]", ")", ":", "\n", "                        ", "self", ".", "graph", ".", "add_node", "(", "sentence", "[", "j", "]", ")", "\n", "\n", "# 2. Create the edges between the nodes using co-occurencies", "\n", "", "", "", "for", "j", "in", "range", "(", "len", "(", "sentence", ")", ")", ":", "\n", "\n", "# Get the first node", "\n", "                ", "first_node", "=", "sentence", "[", "j", "]", "\n", "\n", "# Switch to set the window for the whole sentence", "\n", "max_window", "=", "window", "\n", "if", "window", "<", "1", ":", "\n", "                    ", "max_window", "=", "len", "(", "sentence", ")", "\n", "\n", "# For the other words in the window", "\n", "", "for", "k", "in", "range", "(", "j", "+", "1", ",", "min", "(", "len", "(", "sentence", ")", ",", "j", "+", "max_window", ")", ")", ":", "\n", "\n", "# Get the second node", "\n", "                    ", "second_node", "=", "sentence", "[", "k", "]", "\n", "\n", "# Check if nodes exists ", "\n", "if", "self", ".", "graph", ".", "has_node", "(", "first_node", ")", "and", "self", ".", "graph", ".", "has_node", "(", "second_node", ")", ":", "\n", "\n", "# Add edge if not exists", "\n", "                        ", "if", "not", "self", ".", "graph", ".", "has_edge", "(", "first_node", ",", "second_node", ")", ":", "\n", "                            ", "self", ".", "graph", ".", "add_edge", "(", "first_node", ",", "second_node", ",", "weight", "=", "1", ")", "\n", "# Else modify weight", "\n", "", "else", ":", "\n", "                            ", "self", ".", "graph", "[", "first_node", "]", "[", "second_node", "]", "[", "'weight'", "]", "+=", "1", "\n", "\n", "# Replace sentence by the list of tuples", "\n", "", "", "", "", "self", ".", "sentences", "[", "i", "]", "=", "sentence", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.generate_candidates": [[1167, 1214], ["range", "len", "range", "len", "takahe.keyphrase_reranker.is_a_candidate", "candidate.append", "len", "takahe.keyphrase_reranker.is_a_candidate", "len"], "methods", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.is_a_candidate", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.is_a_candidate"], ["", "", "def", "generate_candidates", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Function to generate the keyphrase candidates from the set of related \n        sentences. Keyphrases candidates are the largest n-grams containing only\n        words from the defined syntactic categories.\n        \"\"\"", "\n", "\n", "# For each sentence ", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "sentences", ")", ")", ":", "\n", "\n", "            ", "sentence", "=", "self", ".", "sentences", "[", "i", "]", "\n", "\n", "# List for iteratively constructing a keyphrase candidate", "\n", "candidate", "=", "[", "]", "\n", "\n", "# For each (word, pos) tuple in the sentence", "\n", "for", "j", "in", "range", "(", "len", "(", "sentence", ")", ")", ":", "\n", "\n", "                ", "word", ",", "pos", "=", "sentence", "[", "j", "]", "\n", "\n", "# If word is to be included in a candidate", "\n", "if", "pos", "in", "self", ".", "syntactic_filter", ":", "\n", "\n", "# Adds word to candidate", "\n", "                    ", "candidate", ".", "append", "(", "sentence", "[", "j", "]", ")", "\n", "\n", "# If a candidate keyphrase is in the buffer", "\n", "", "elif", "len", "(", "candidate", ")", ">", "0", "and", "self", ".", "is_a_candidate", "(", "candidate", ")", ":", "\n", "\n", "# Add candidate", "\n", "                    ", "keyphrase", "=", "' '", ".", "join", "(", "u", "[", "0", "]", "for", "u", "in", "candidate", ")", "\n", "self", ".", "keyphrase_candidates", "[", "keyphrase", "]", "=", "candidate", "\n", "\n", "# Flush the buffer", "\n", "candidate", "=", "[", "]", "\n", "\n", "", "else", ":", "\n", "\n", "# Flush the buffer", "\n", "                    ", "candidate", "=", "[", "]", "\n", "\n", "# Handle the last possible candidate", "\n", "", "", "if", "len", "(", "candidate", ")", ">", "0", "and", "self", ".", "is_a_candidate", "(", "candidate", ")", ":", "\n", "\n", "# Add candidate", "\n", "                ", "keyphrase", "=", "' '", ".", "join", "(", "u", "[", "0", "]", "for", "u", "in", "candidate", ")", "\n", "self", ".", "keyphrase_candidates", "[", "keyphrase", "]", "=", "candidate", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.is_a_candidate": [[1218, 1231], ["re.search"], "methods", ["None"], ["", "", "", "def", "is_a_candidate", "(", "self", ",", "keyphrase_candidate", ")", ":", "\n", "        ", "\"\"\"\n        Function to check if a keyphrase candidate is a valid one according to \n        the syntactic patterns.\n        \"\"\"", "\n", "\n", "candidate_pattern", "=", "''", ".", "join", "(", "u", "[", "1", "]", "for", "u", "in", "keyphrase_candidate", ")", "\n", "\n", "for", "pattern", "in", "self", ".", "syntactic_patterns", ":", "\n", "            ", "if", "not", "re", ".", "search", "(", "pattern", ",", "candidate_pattern", ")", ":", "\n", "                ", "return", "False", "\n", "\n", "", "", "return", "True", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.undirected_TextRank": [[1235, 1292], ["takahe.keyphrase_reranker.graph.nodes", "takahe.keyphrase_reranker.word_scores.copy", "takahe.keyphrase_reranker.graph.nodes", "list", "takahe.keyphrase_reranker.graph.neighbors", "math.fabs", "max", "takahe.keyphrase_reranker.graph.nodes", "takahe.keyphrase_reranker.graph.neighbors"], "methods", ["None"], ["", "def", "undirected_TextRank", "(", "self", ",", "d", "=", "0.85", ",", "f_conv", "=", "0.0001", ")", ":", "\n", "        ", "\"\"\"\n        Implementation of the TextRank algorithm as described in \n        [mihalcea-tarau:2004:EMNLP]_. Node scores are computed iteratively until\n        convergence (a threshold is used, default is 0.0001). The dampling \n        factor is by default set to 0.85 as recommended in the article.\n        \"\"\"", "\n", "\n", "# Initialise the maximum node difference for checking stability", "\n", "max_node_difference", "=", "f_conv", "\n", "\n", "# Initialise node scores to 1", "\n", "self", ".", "word_scores", "=", "{", "}", "\n", "for", "node", "in", "self", ".", "graph", ".", "nodes", "(", ")", ":", "\n", "            ", "self", ".", "word_scores", "[", "node", "]", "=", "1.0", "\n", "\n", "# While the node scores are not stabilized", "\n", "", "while", "(", "max_node_difference", ">=", "f_conv", ")", ":", "\n", "\n", "# Create a copy of the current node scores", "\n", "            ", "current_node_scores", "=", "self", ".", "word_scores", ".", "copy", "(", ")", "\n", "\n", "# For each node I in the graph", "\n", "# print(\"self.graph.nodes() type\", type(self.graph.nodes()))", "\n", "# print(\"self.graph.nodes(): \", self.graph.nodes())", "\n", "\n", "# if no nodes are found in graph, break", "\n", "if", "list", "(", "self", ".", "graph", ".", "nodes", "(", ")", ")", "==", "[", "]", ":", "\n", "# print(\"break --\"*50)", "\n", "                ", "break", "\n", "\n", "\n", "", "for", "node_i", "in", "self", ".", "graph", ".", "nodes", "(", ")", ":", "\n", "\n", "                ", "sum_Vj", "=", "0", "\n", "\n", "# For each node J connected to I", "\n", "for", "node_j", "in", "self", ".", "graph", ".", "neighbors", "(", "node_i", ")", ":", "\n", "\n", "                    ", "wji", "=", "self", ".", "graph", "[", "node_j", "]", "[", "node_i", "]", "[", "'weight'", "]", "\n", "WSVj", "=", "current_node_scores", "[", "node_j", "]", "\n", "sum_wjk", "=", "0.0", "\n", "\n", "# For each node K connected to J", "\n", "for", "node_k", "in", "self", ".", "graph", ".", "neighbors", "(", "node_j", ")", ":", "\n", "                        ", "sum_wjk", "+=", "self", ".", "graph", "[", "node_j", "]", "[", "node_k", "]", "[", "'weight'", "]", "\n", "\n", "", "sum_Vj", "+=", "(", "(", "wji", "*", "WSVj", ")", "/", "sum_wjk", ")", "\n", "\n", "# Modify node score", "\n", "", "self", ".", "word_scores", "[", "node_i", "]", "=", "(", "1", "-", "d", ")", "+", "(", "d", "*", "sum_Vj", ")", "\n", "\n", "# Compute the difference between old and new score", "\n", "score_difference", "=", "math", ".", "fabs", "(", "self", ".", "word_scores", "[", "node_i", "]", "-", "current_node_scores", "[", "node_i", "]", ")", "\n", "\n", "max_node_difference", "=", "max", "(", "score_difference", ",", "score_difference", ")", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.score_keyphrase_candidates": [[1296, 1316], ["len"], "methods", ["None"], ["", "", "", "def", "score_keyphrase_candidates", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Function to compute the score of each keyphrase candidate according to \n        the words it contains. The score of each keyphrase is calculated as the \n        sum of its word scores normalized by its length + 1.\n        \"\"\"", "\n", "\n", "# Compute the score of each candidate according to its words", "\n", "for", "keyphrase", "in", "self", ".", "keyphrase_candidates", ":", "\n", "\n", "# Compute the sum of word scores for each candidate", "\n", "            ", "keyphrase_score", "=", "0.0", "\n", "for", "word_pos_tuple", "in", "self", ".", "keyphrase_candidates", "[", "keyphrase", "]", ":", "\n", "                ", "keyphrase_score", "+=", "self", ".", "word_scores", "[", "word_pos_tuple", "]", "\n", "\n", "# Normalise score by length", "\n", "", "keyphrase_score", "/=", "(", "len", "(", "self", ".", "keyphrase_candidates", "[", "keyphrase", "]", ")", "+", "1.0", ")", "\n", "\n", "# Add score to the keyphrase candidates", "\n", "self", ".", "keyphrase_scores", "[", "keyphrase", "]", "=", "keyphrase_score", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.cluster_keyphrase_candidates": [[1320, 1415], ["sorted", "sorted", "list", "set", "sorted", "best_candidate_keyphrases.append", "keyphrase.split", "set", "non_redundant_keyphrases.append", "len", "cluster.split", "len", "clusters[].append", "set.difference"], "methods", ["None"], ["", "", "def", "cluster_keyphrase_candidates", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Function to cluster keyphrase candidates and remove redundancy. A large \n        number of the generated keyphrase candidates are redundant. Some \n        keyphrases may be contained within larger ones, e.g. *giant tortoise*\n        and *Pinta Island giant tortoise*. To solve this problem, generated \n        keyphrases are clustered using word overlap. For each cluster, the \n        keyphrase with the highest score is selected.\n        \"\"\"", "\n", "\n", "# Sort keyphrase candidates by length", "\n", "descending", "=", "sorted", "(", "self", ".", "keyphrase_candidates", ",", "\n", "key", "=", "lambda", "x", ":", "len", "(", "self", ".", "keyphrase_candidates", "[", "x", "]", ")", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "# Initialize the cluster container", "\n", "clusters", "=", "{", "}", "\n", "\n", "# Loop over keyphrases by decreasing length", "\n", "for", "keyphrase", "in", "descending", ":", "\n", "\n", "            ", "found_cluster", "=", "False", "\n", "\n", "# Create a set of words from the keyphrase", "\n", "keyphrase_words", "=", "set", "(", "keyphrase", ".", "split", "(", "' '", ")", ")", "\n", "\n", "# Loop over existing clusters", "\n", "for", "cluster", "in", "clusters", ":", "\n", "\n", "# Create a set of words from the cluster representative", "\n", "                ", "cluster_words", "=", "set", "(", "cluster", ".", "split", "(", "' '", ")", ")", "\n", "\n", "# Check if keyphrase words are all contained in the cluster", "\n", "# representative words", "\n", "if", "len", "(", "keyphrase_words", ".", "difference", "(", "cluster_words", ")", ")", "==", "0", ":", "\n", "\n", "# Add keyphrase to cluster", "\n", "                    ", "clusters", "[", "cluster", "]", ".", "append", "(", "keyphrase", ")", "\n", "\n", "# Mark cluster as found", "\n", "found_cluster", "=", "True", "\n", "\n", "# If keyphrase does not fit into any existing cluster", "\n", "", "", "if", "not", "found_cluster", ":", "\n", "                ", "clusters", "[", "keyphrase", "]", "=", "[", "keyphrase", "]", "\n", "\n", "# Initialize the best candidate cluster container", "\n", "", "", "best_candidate_keyphrases", "=", "[", "]", "\n", "\n", "# Loop over the clusters to find the best keyphrases", "\n", "for", "cluster", "in", "clusters", ":", "\n", "\n", "# Find the best scored keyphrase candidate in the cluster", "\n", "            ", "sorted_cluster", "=", "sorted", "(", "clusters", "[", "cluster", "]", ",", "\n", "key", "=", "lambda", "cluster", ":", "self", ".", "keyphrase_scores", "[", "cluster", "]", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "best_candidate_keyphrases", ".", "append", "(", "sorted_cluster", "[", "0", "]", ")", "\n", "\n", "# Initialize the non redundant clustered keyphrases", "\n", "", "non_redundant_keyphrases", "=", "[", "]", "\n", "\n", "# Sort best candidate by score", "\n", "sorted_keyphrases", "=", "sorted", "(", "best_candidate_keyphrases", ",", "\n", "key", "=", "lambda", "keyphrase", ":", "self", ".", "keyphrase_scores", "[", "keyphrase", "]", ",", "\n", "reverse", "=", "True", ")", "\n", "\n", "# Last loop to remove redundancy in cluster best candidates", "\n", "for", "keyphrase", "in", "sorted_keyphrases", ":", "\n", "            ", "is_redundant", "=", "False", "\n", "for", "prev_keyphrase", "in", "non_redundant_keyphrases", ":", "\n", "                ", "if", "keyphrase", "in", "prev_keyphrase", ":", "\n", "                    ", "is_redundant", "=", "True", "\n", "break", "\n", "", "", "if", "not", "is_redundant", ":", "\n", "                ", "non_redundant_keyphrases", ".", "append", "(", "keyphrase", ")", "\n", "\n", "# Modify the keyphrase candidate dictionnaries according to the clusters", "\n", "", "", "'''\n\n    #In Python 2.x calling keys makes a copy of the key that you can iterate over while modifying the dict:\n\n    #for i in d.keys():\n    #Note that this doesn't work in Python 3.x because keys returns an iterator instead of a list.\n\n    #Another way is to use list to force a copy of the keys to be made. This one also works in Python 3.x:\n\n    #for i in list(d):\n        '''", "\n", "for", "keyphrase", "in", "list", "(", "self", ".", "keyphrase_candidates", ")", ":", "\n", "\n", "# Remove candidate if not in cluster", "\n", "            ", "if", "not", "keyphrase", "in", "non_redundant_keyphrases", ":", "\n", "                ", "del", "self", ".", "keyphrase_candidates", "[", "keyphrase", "]", "\n", "del", "self", ".", "keyphrase_scores", "[", "keyphrase", "]", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.rerank_nbest_compressions": [[1419, 1448], ["bisect.insort", "len"], "methods", ["None"], ["", "", "", "def", "rerank_nbest_compressions", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Function that reranks the nbest compressions according to the keyphrases\n        they contain. The cummulative score (original score) is normalized by \n        (compression length * Sum of keyphrase scores).\n        \"\"\"", "\n", "\n", "reranked_compressions", "=", "[", "]", "\n", "\n", "# Loop over the compression candidates", "\n", "for", "cummulative_score", ",", "path", "in", "self", ".", "nbest_compressions", ":", "\n", "\n", "# Generate the sentence form the path", "\n", "            ", "compression", "=", "' '", ".", "join", "(", "[", "u", "[", "0", "]", "for", "u", "in", "path", "]", ")", "\n", "\n", "# Initialize total keyphrase score", "\n", "total_keyphrase_score", "=", "1.0", "\n", "\n", "# Loop over the keyphrases and sum the scores", "\n", "for", "keyphrase", "in", "self", ".", "keyphrase_candidates", ":", "\n", "                ", "if", "keyphrase", "in", "compression", ":", "\n", "                    ", "total_keyphrase_score", "+=", "self", ".", "keyphrase_scores", "[", "keyphrase", "]", "\n", "\n", "", "", "score", "=", "(", "cummulative_score", "/", "(", "len", "(", "path", ")", "*", "total_keyphrase_score", ")", ")", "\n", "\n", "bisect", ".", "insort", "(", "reranked_compressions", ",", "\n", "(", "score", ",", "path", ")", ")", "\n", "\n", "", "return", "reranked_compressions", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.wordpos_to_tuple": [[1451, 1470], ["re.escape", "re.match", "token.lower", "re.match.group", "re.match.group"], "methods", ["None"], ["", "def", "wordpos_to_tuple", "(", "self", ",", "word", ")", ":", "\n", "        ", "\"\"\"\n        This function converts a word/POS to a (word, POS) tuple. The character\n        used for separating word and POS can be specified (default is /).\n        \"\"\"", "\n", "\n", "# Splitting word, POS using regex", "\n", "pos_separator_re", "=", "re", ".", "escape", "(", "self", ".", "pos_separator", ")", "\n", "m", "=", "re", ".", "match", "(", "\"^(.+)\"", "+", "pos_separator_re", "+", "\"(.+)$\"", ",", "word", ")", "\n", "\n", "if", "m", "is", "not", "None", ":", "\n", "# Extract the word information", "\n", "            ", "token", ",", "POS", "=", "m", ".", "group", "(", "1", ")", ",", "m", ".", "group", "(", "2", ")", "\n", "", "else", ":", "\n", "            ", "token", "=", "'the'", "\n", "POS", "=", "'DT'", "\n", "\n", "# Return the tuple ", "\n", "", "return", "(", "token", ".", "lower", "(", ")", ",", "POS", ")", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.tuple_to_wordpos": [[1474, 1482], ["None"], "methods", ["None"], ["", "def", "tuple_to_wordpos", "(", "self", ",", "wordpos_tuple", ")", ":", "\n", "        ", "\"\"\"\n        This function converts a (word, POS) tuple to word/POS. The character \n        used for separating word and POS can be specified (default is /).\n        \"\"\"", "\n", "\n", "# Return the word +delim+ POS", "\n", "return", "wordpos_tuple", "[", "0", "]", "+", "self", ".", "pos_separator", "+", "wordpos_tuple", "[", "1", "]", "\n", "#-B-----------------------------------------------------------------------B-", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.rank_sent_Lexrank": [[17, 24], ["list", "lxr.rank_sentences", "enumerate"], "function", ["None"], ["def", "rank_sent_Lexrank", "(", "lxr", ",", "sentences_list", ",", "cutoff", "=", "1.5", ")", ":", "\n", "    ", "scores", "=", "list", "(", "lxr", ".", "rank_sentences", "(", "sentences_list", ",", "threshold", "=", "None", ",", "fast_power_method", "=", "True", ")", ")", "\n", "# idx_to_slice=[sscores.index(s) for s in scores if s> cutoff]", "\n", "idx_to_slice", "=", "[", "i", "for", "i", ",", "score", "in", "enumerate", "(", "scores", ")", "if", "score", ">", "cutoff", "]", "\n", "sentences_list", "=", "[", "sentences_list", "[", "idx", "]", "for", "idx", "in", "idx_to_slice", "]", "\n", "# print(\"ranked sentence:\", sentences_list)", "\n", "return", "sentences_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.rank_sent_centroid": [[25, 28], ["centroid_rank.centroidRank"], "function", ["None"], ["", "def", "rank_sent_centroid", "(", "centroid_rank", ",", "sentences_list", ")", ":", "\n", "    ", "sentences_list", "=", "centroid_rank", ".", "centroidRank", "(", "sentences_list", ")", "\n", "return", "sentences_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.get_w2v_embeddings": [[30, 40], ["open", "open.close", "line.split", "numpy.asarray"], "function", ["None"], ["", "def", "get_w2v_embeddings", "(", "filename", ")", ":", "\n", "    ", "word_embeddings", "=", "{", "}", "\n", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "for", "line", "in", "f", ":", "\n", "        ", "values", "=", "line", ".", "split", "(", ")", "\n", "word", "=", "values", "[", "0", "]", "\n", "coefs", "=", "np", ".", "asarray", "(", "values", "[", "1", ":", "]", ",", "dtype", "=", "'float32'", ")", "\n", "word_embeddings", "[", "word", "]", "=", "coefs", "\n", "", "f", ".", "close", "(", ")", "\n", "return", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.get_sentence_embedding": [[41, 52], ["sent.lower.lower", "len", "numpy.mean", "numpy.zeros", "word_embeddings.get", "numpy.zeros", "sent.lower.split"], "function", ["None"], ["", "def", "get_sentence_embedding", "(", "sent", ",", "word_embeddings", ")", ":", "\n", "    ", "sent", "=", "sent", ".", "lower", "(", ")", "\n", "eps", "=", "1e-10", "\n", "#print(sent)", "\n", "if", "len", "(", "sent", ")", "!=", "0", ":", "\n", "        ", "vectors", "=", "[", "word_embeddings", ".", "get", "(", "w", ",", "np", ".", "zeros", "(", "(", "100", ",", ")", ")", ")", "for", "w", "in", "sent", ".", "split", "(", ")", "]", "\n", "v", "=", "np", ".", "mean", "(", "vectors", ",", "axis", "=", "0", ")", "\n", "", "else", ":", "\n", "        ", "v", "=", "np", ".", "zeros", "(", "(", "100", ",", ")", ")", "\n", "", "v", "=", "v", "+", "eps", "\n", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.get_SentNode_embedding": [[54, 60], ["numpy.zeros", "enumerate", "utils.get_sentence_embedding", "len"], "function", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.get_sentence_embedding"], ["", "def", "get_SentNode_embedding", "(", "sentences_list", ",", "word_embeddings", ")", ":", "\n", "    ", "emb_sentence_vectors", "=", "np", ".", "zeros", "(", "[", "len", "(", "sentences_list", ")", ",", "100", "]", ")", "\n", "for", "count", ",", "sent", "in", "enumerate", "(", "sentences_list", ")", ":", "\n", "        ", "emb_sen", "=", "get_sentence_embedding", "(", "sent", ",", "word_embeddings", ")", "\n", "emb_sentence_vectors", "[", "count", ",", "]", "=", "emb_sen", "\n", "", "return", "emb_sentence_vectors", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.build_similarity_matrix": [[61, 69], ["numpy.zeros", "range", "len", "range", "len", "len", "len", "cosine_similarity", "emb_sentence_vectors[].reshape", "emb_sentence_vectors[].reshape"], "function", ["None"], ["", "def", "build_similarity_matrix", "(", "emb_sentence_vectors", ")", ":", "\n", "    ", "from", "sklearn", ".", "metrics", ".", "pairwise", "import", "cosine_similarity", "\n", "sim_mat", "=", "np", ".", "zeros", "(", "[", "len", "(", "emb_sentence_vectors", ")", ",", "len", "(", "emb_sentence_vectors", ")", "]", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "emb_sentence_vectors", ")", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "len", "(", "emb_sentence_vectors", ")", ")", ":", "\n", "            ", "if", "i", "!=", "j", ":", "\n", "                ", "sim_mat", "[", "i", "]", "[", "j", "]", "=", "cosine_similarity", "(", "emb_sentence_vectors", "[", "i", "]", ".", "reshape", "(", "1", ",", "100", ")", ",", "emb_sentence_vectors", "[", "j", "]", ".", "reshape", "(", "1", ",", "100", ")", ")", "[", "0", ",", "0", "]", "\n", "", "", "", "return", "sim_mat", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.rank_sent_Pagerank": [[70, 77], ["nx.from_numpy_array", "nx.pagerank", "sorted", "enumerate", "enumerate"], "function", ["None"], ["", "def", "rank_sent_Pagerank", "(", "sim_mat", ",", "sentences_list", ",", "n", "=", "3", ",", "alpha", "=", "0.85", ",", "tol", "=", "1.0e-6", ")", ":", "\n", "    ", "import", "networkx", "as", "nx", "\n", "nx_graph", "=", "nx", ".", "from_numpy_array", "(", "sim_mat", ")", "\n", "scores", "=", "nx", ".", "pagerank", "(", "nx_graph", ",", "alpha", "=", "alpha", ",", "tol", "=", "tol", ")", "\n", "ranked_sent", "=", "sorted", "(", "(", "(", "scores", "[", "i", "]", ",", "s", ")", "for", "i", ",", "s", "in", "enumerate", "(", "sentences_list", ")", ")", ",", "reverse", "=", "True", ")", "\n", "ranked_sentences_list", "=", "[", "tuple_", "[", "1", "]", "for", "i", ",", "tuple_", "in", "enumerate", "(", "ranked_sent", ")", "]", "\n", "return", "ranked_sentences_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.get_first_doc": [[78, 81], ["line.split"], "function", ["None"], ["", "def", "get_first_doc", "(", "line", ",", "tag", "=", "\"|||||\"", ")", ":", "\n", "    ", "first_doc", "=", "line", ".", "split", "(", "tag", ")", "[", "0", "]", "\n", "return", "first_doc", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.truncate_doc": [[82, 89], ["sent_detector.tokenize", "doc.strip", "len"], "function", ["None"], ["", "def", "truncate_doc", "(", "doc", ")", ":", "\n", "    ", "sent_list", "=", "sent_detector", ".", "tokenize", "(", "doc", ".", "strip", "(", ")", ")", "\n", "seg", "=", "6", "\n", "if", "len", "(", "sent_list", ")", ">", "seg", ":", "\n", "        ", "return", "sent_list", "[", ":", "seg", "]", "\n", "", "else", ":", "\n", "        ", "return", "sent_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.read_lead_sentences": [[90, 114], ["doc.split", "print", "print", "print", "len", "enumerate", "sent_detector.tokenize", "len", "enumerate", "doc.strip", "sent_detector.tokenize", "len", "len", "doc.strip"], "function", ["None"], ["", "", "def", "read_lead_sentences", "(", "doc", ",", "tag", "=", "\"story_separator_special_tag\"", ")", ":", "\n", "    ", "doc_list", "=", "doc", ".", "split", "(", "tag", ")", "\n", "# print(doc_list)", "\n", "sent_list", "=", "[", "]", "\n", "# only keep the first three sources", "\n", "if", "len", "(", "doc_list", ")", "==", "2", ":", "\n", "        ", "for", "count", ",", "doc", "in", "enumerate", "(", "doc_list", ")", ":", "\n", "            ", "doc_sent_list", "=", "sent_detector", ".", "tokenize", "(", "doc", ".", "strip", "(", ")", ")", "\n", "try", ":", "\n", "                ", "sent_list", "+=", "doc_sent_list", "[", ":", "4", "]", "\n", "", "except", ":", "\n", "                ", "sent_list", "+=", "doc_sent_list", "\n", "", "", "", "elif", "len", "(", "doc_list", ")", ">", "2", ":", "\n", "        ", "doc_list", "=", "doc_list", "[", ":", "3", "]", "\n", "for", "count", ",", "doc", "in", "enumerate", "(", "doc_list", ")", ":", "\n", "            ", "doc_sent_list", "=", "sent_detector", ".", "tokenize", "(", "doc", ".", "strip", "(", ")", ")", "\n", "try", ":", "\n", "                ", "sent_list", "+=", "doc_sent_list", "[", ":", "3", "]", "\n", "", "except", ":", "\n", "                ", "sent_list", "+=", "doc_sent_list", "\n", "", "", "", "print", "(", "f\"number of sources in this instance: {len(doc_list)}\"", ")", "\n", "print", "(", "f\"number of lead sentences in this instance: {len(sent_list)}\"", ")", "\n", "print", "(", "\"*\"", "*", "80", ")", "\n", "return", "sent_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.read_file": [[118, 136], ["open", "open.readlines", "os.path.join", "src_list.append", "utils.get_first_doc", "utils.truncate_doc", "utils.read_lead_sentences", "line.replace.replace", "sent_detector.tokenize", "line.replace.strip"], "function", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.get_first_doc", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.truncate_doc", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.read_lead_sentences"], ["", "def", "read_file", "(", "path", ",", "file_name", ",", "read_lead_only", "=", "False", ",", "read_first_doc", "=", "False", ")", ":", "\n", "    ", "f", "=", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "file_name", ")", ",", "\"r\"", ")", "\n", "lines", "=", "f", ".", "readlines", "(", ")", "\n", "src_list", "=", "[", "]", "\n", "tag", "=", "\"story_separator_special_tag\"", "\n", "for", "line", "in", "lines", ":", "\n", "        ", "if", "read_first_doc", ":", "\n", "            ", "line", "=", "get_first_doc", "(", "line", ")", "\n", "sent_list", "=", "truncate_doc", "(", "line", ")", "\n", "", "elif", "read_lead_only", ":", "\n", "            ", "sent_list", "=", "read_lead_sentences", "(", "line", ",", "tag", "=", "tag", ")", "\n", "", "else", ":", "\n", "# remove tag; uncomment below for baseline", "\n", "            ", "line", "=", "line", ".", "replace", "(", "tag", ",", "\"\"", ")", "\n", "# tokenzie line to sentences", "\n", "sent_list", "=", "sent_detector", ".", "tokenize", "(", "line", ".", "strip", "(", ")", ")", "\n", "", "src_list", ".", "append", "(", "sent_list", ")", "\n", "", "return", "src_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.tag_pos": [[137, 146], ["spacynlp", "textlist.append"], "function", ["None"], ["", "def", "tag_pos", "(", "str_text", ")", ":", "\n", "    ", "doc", "=", "spacynlp", "(", "str_text", ")", "\n", "textlist", "=", "[", "]", "\n", "# compare the words between two strings", "\n", "for", "item", "in", "doc", ":", "\n", "        ", "source_token", "=", "item", ".", "text", "\n", "source_pos", "=", "item", ".", "tag_", "\n", "textlist", ".", "append", "(", "source_token", "+", "'/'", "+", "source_pos", ")", "\n", "", "return", "' '", ".", "join", "(", "textlist", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.convert_sents_to_tagged_sents": [[148, 159], ["len", "tagged_list.append", "s.replace.replace", "utils.tag_pos", "tagged_list.append", "utils.tag_pos"], "function", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.tag_pos", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.tag_pos"], ["", "def", "convert_sents_to_tagged_sents", "(", "sent_list", ")", ":", "\n", "    ", "tagged_list", "=", "[", "]", "\n", "if", "(", "len", "(", "sent_list", ")", ">", "0", ")", ":", "\n", "        ", "for", "s", "in", "sent_list", ":", "\n", "            ", "s", "=", "s", ".", "replace", "(", "\"/\"", ",", "\"\"", ")", "\n", "# print(\"original sent -------- \\n\",s)", "\n", "temp_tagged", "=", "tag_pos", "(", "s", ")", "\n", "tagged_list", ".", "append", "(", "temp_tagged", ")", "\n", "", "", "else", ":", "\n", "        ", "tagged_list", ".", "append", "(", "tag_pos", "(", "\".\"", ")", ")", "\n", "", "return", "tagged_list", "\n", "\n"]], "home.repos.pwc.inspect_result.mingzi151_SummPip.None.utils.get_compressed_sen": [[160, 179], ["takahe.word_graph", "takahe.word_graph.get_compression", "takahe.keyphrase_reranker", "takahe.keyphrase_reranker.rerank_nbest_compressions", "len"], "function", ["home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.word_graph.get_compression", "home.repos.pwc.inspect_result.mingzi151_SummPip.None.takahe.keyphrase_reranker.rerank_nbest_compressions"], ["", "def", "get_compressed_sen", "(", "sentences", ",", "nb_words", ")", ":", "\n", "    ", "compresser", "=", "takahe", ".", "word_graph", "(", "sentences", ",", "nb_words", "=", "nb_words", ",", "lang", "=", "'en'", ",", "punct_tag", "=", "\".\"", ")", "\n", "candidates", "=", "compresser", ".", "get_compression", "(", "3", ")", "\n", "# print(\"--------------------Top 3 candicate---------------\", candidates)", "\n", "reranker", "=", "takahe", ".", "keyphrase_reranker", "(", "sentences", ",", "\n", "candidates", ",", "\n", "lang", "=", "'en'", ")", "\n", "# print(\"reranker: \", reranker)", "\n", "# print(\"finish initialising reranker------------\")", "\n", "\n", "reranked_candidates", "=", "reranker", ".", "rerank_nbest_compressions", "(", ")", "\n", "# print(reranked_candidates)", "\n", "if", "len", "(", "reranked_candidates", ")", ">", "0", ":", "\n", "        ", "score", ",", "path", "=", "reranked_candidates", "[", "0", "]", "\n", "result", "=", "' '", ".", "join", "(", "[", "u", "[", "0", "]", "for", "u", "in", "path", "]", ")", "\n", "", "else", ":", "\n", "        ", "result", "=", "' '", "\n", "# print(\"----------------selected candicate as final output-------------- \", result)", "\n", "", "return", "result", "\n", "", ""]]}