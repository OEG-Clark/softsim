{"home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.Model.__init__": [[25, 40], ["super().__init__", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.InputLayer", "agent.Model.hidden_layers.append", "tensorflow.keras.layers.InputLayer", "tensorflow.keras.layers.InputLayer", "tensorflow.keras.layers.Dense"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.__init__"], ["def", "__init__", "(", "self", ",", "num_states", ",", "hidden_units", ",", "num_actions", ",", "alg", ",", "use_stochastic_delay", ",", "max_dimension", ")", ":", "\n", "        ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "alg", "==", "'IS'", ":", "\n", "            ", "if", "use_stochastic_delay", ":", "\n", "                ", "self", ".", "input_layer", "=", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "(", "num_states", "+", "1", "+", "max_dimension", ",", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "input_layer", "=", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "(", "num_states", "+", "max_dimension", ",", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "input_layer", "=", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "(", "num_states", ",", ")", ")", "\n", "", "self", ".", "hidden_layers", "=", "[", "]", "\n", "for", "i", "in", "hidden_units", ":", "\n", "            ", "self", ".", "hidden_layers", ".", "append", "(", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "i", ",", "activation", "=", "'tanh'", ",", "kernel_initializer", "=", "'RandomNormal'", ")", ")", "\n", "", "self", ".", "output_layer", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "num_actions", ",", "activation", "=", "'linear'", ",", "kernel_initializer", "=", "'RandomNormal'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.Model.call": [[41, 48], ["agent.Model.input_layer", "agent.Model.output_layer", "layer"], "methods", ["None"], ["", "@", "tf", ".", "function", "\n", "def", "call", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "z", "=", "self", ".", "input_layer", "(", "inputs", ")", "\n", "for", "layer", "in", "self", ".", "hidden_layers", ":", "\n", "            ", "z", "=", "layer", "(", "z", ")", "\n", "", "output", "=", "self", ".", "output_layer", "(", "z", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.DQN.__init__": [[51, 72], ["numpy.random.seed", "tensorflow.random.set_seed", "random.seed", "tensorflow.optimizers.Adam", "agent.Model", "collections.deque", "collections.deque"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "num_states", ",", "num_actions", ",", "model_params", ",", "alg_params", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "alg_params", "[", "'seed'", "]", ")", "\n", "tf", ".", "random", ".", "set_seed", "(", "alg_params", "[", "'seed'", "]", ")", "\n", "random", ".", "seed", "(", "alg_params", "[", "'seed'", "]", ")", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "self", ".", "alg", "=", "alg_params", "[", "'algorithm'", "]", "\n", "self", ".", "batch_size", "=", "alg_params", "[", "'batch_size'", "]", "\n", "self", ".", "optimizer", "=", "tf", ".", "optimizers", ".", "Adam", "(", "alg_params", "[", "'learning_rate'", "]", ")", "\n", "self", ".", "use_stochastic_delay", "=", "alg_params", "[", "'use_stochastic_delay'", "]", "\n", "self", ".", "max_dimension", "=", "model_params", "[", "'max_dimension'", "]", "\n", "hidden_units", "=", "model_params", "[", "'hidden_units'", "]", "\n", "self", ".", "delay", "=", "alg_params", "[", "'delay'", "]", "\n", "self", ".", "gamma", "=", "alg_params", "[", "'gamma'", "]", "\n", "self", ".", "model", "=", "Model", "(", "num_states", ",", "hidden_units", ",", "num_actions", ",", "self", ".", "use_stochastic_delay", ",", "self", ".", "max_dimension", ",", "\n", "self", ".", "alg", ")", "\n", "self", ".", "experience", "=", "{", "'s'", ":", "[", "]", ",", "'a'", ":", "[", "]", ",", "'r'", ":", "[", "]", ",", "'s2'", ":", "[", "]", ",", "'done'", ":", "[", "]", "}", "\n", "self", ".", "max_experiences", "=", "model_params", "[", "'max_buffer_size'", "]", "\n", "self", ".", "min_experiences", "=", "model_params", "[", "'min_buffer_size'", "]", "\n", "if", "self", ".", "alg", "!=", "'normal'", ":", "\n", "            ", "self", ".", "action_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "max_dimension", "+", "1", ")", "\n", "self", ".", "action_buffer_padded", "=", "deque", "(", "maxlen", "=", "self", ".", "max_dimension", "+", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.DQN.predict": [[73, 75], ["agent.DQN.model", "numpy.atleast_2d", "inputs.astype"], "methods", ["None"], ["", "", "def", "predict", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "return", "self", ".", "model", "(", "np", ".", "atleast_2d", "(", "inputs", ".", "astype", "(", "'float32'", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.DQN.fill_up_buffer": [[76, 80], ["agent.DQN.action_buffer_padded.clear", "range", "agent.DQN.action_buffer_padded.append"], "methods", ["None"], ["", "def", "fill_up_buffer", "(", "self", ")", ":", "\n", "        ", "self", ".", "action_buffer_padded", ".", "clear", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "max_dimension", ")", ":", "\n", "            ", "self", ".", "action_buffer_padded", ".", "append", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.DQN.buffer_padding": [[81, 86], ["len", "copy.deepcopy", "range", "agent.DQN.action_buffer_padded.append"], "methods", ["None"], ["", "", "def", "buffer_padding", "(", "self", ")", ":", "\n", "        ", "current_length", "=", "len", "(", "self", ".", "action_buffer", ")", "\n", "self", ".", "action_buffer_padded", "=", "copy", ".", "deepcopy", "(", "self", ".", "action_buffer", ")", "\n", "for", "_", "in", "range", "(", "0", ",", "self", ".", "max_dimension", "-", "current_length", ")", ":", "\n", "            ", "self", ".", "action_buffer_padded", ".", "append", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.DQN.train": [[87, 107], ["numpy.random.randint", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.max", "numpy.where", "tape.gradient", "agent.DQN.optimizer.apply_gradients", "len", "TargetNet.predict", "tensorflow.GradientTape", "tensorflow.math.reduce_sum", "tensorflow.math.reduce_mean", "zip", "len", "tensorflow.square", "agent.DQN.predict", "tensorflow.one_hot"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict"], ["", "", "def", "train", "(", "self", ",", "TargetNet", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "experience", "[", "'s'", "]", ")", "<", "self", ".", "min_experiences", ":", "\n", "            ", "return", "0", "\n", "", "ids", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "len", "(", "self", ".", "experience", "[", "'s'", "]", ")", ",", "size", "=", "self", ".", "batch_size", ")", "\n", "states", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'s'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "actions", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'a'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'r'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "states_next", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'s2'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "dones", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'done'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "value_next", "=", "np", ".", "max", "(", "TargetNet", ".", "predict", "(", "states_next", ")", ",", "axis", "=", "1", ")", "\n", "actual_values", "=", "np", ".", "where", "(", "dones", ",", "rewards", ",", "rewards", "+", "self", ".", "gamma", "*", "value_next", ")", "\n", "\n", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "\n", "            ", "selected_action_values", "=", "tf", ".", "math", ".", "reduce_sum", "(", "\n", "self", ".", "predict", "(", "states", ")", "*", "tf", ".", "one_hot", "(", "actions", ",", "self", ".", "num_actions", ")", ",", "axis", "=", "1", ")", "\n", "loss", "=", "tf", ".", "math", ".", "reduce_mean", "(", "tf", ".", "square", "(", "actual_values", "-", "selected_action_values", ")", ")", "\n", "", "variables", "=", "self", ".", "model", ".", "trainable_variables", "\n", "gradients", "=", "tape", ".", "gradient", "(", "loss", ",", "variables", ")", "\n", "self", ".", "optimizer", ".", "apply_gradients", "(", "zip", "(", "gradients", ",", "variables", ")", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.DQN.get_action": [[108, 113], ["numpy.random.random", "numpy.random.choice", "numpy.argmax", "agent.DQN.predict", "numpy.atleast_2d"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict"], ["", "def", "get_action", "(", "self", ",", "states", ",", "epsilon", ")", ":", "\n", "        ", "if", "np", ".", "random", ".", "random", "(", ")", "<", "epsilon", ":", "\n", "            ", "return", "np", ".", "random", ".", "choice", "(", "self", ".", "num_actions", ")", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "argmax", "(", "self", ".", "predict", "(", "np", ".", "atleast_2d", "(", "states", ")", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.DQN.add_experience": [[114, 120], ["exp.items", "len", "agent.DQN.experience.keys", "agent.DQN.experience[].append", "agent.DQN.experience[].pop"], "methods", ["None"], ["", "", "def", "add_experience", "(", "self", ",", "exp", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "experience", "[", "'s'", "]", ")", ">=", "self", ".", "max_experiences", ":", "\n", "            ", "for", "key", "in", "self", ".", "experience", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "experience", "[", "key", "]", ".", "pop", "(", "0", ")", "\n", "", "", "for", "key", ",", "value", "in", "exp", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "experience", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.DQN.copy_weights": [[121, 126], ["zip", "v1.assign", "v2.numpy"], "methods", ["None"], ["", "", "def", "copy_weights", "(", "self", ",", "TrainNet", ")", ":", "\n", "        ", "variables1", "=", "self", ".", "model", ".", "trainable_variables", "\n", "variables2", "=", "TrainNet", ".", "model", ".", "trainable_variables", "\n", "for", "v1", ",", "v2", "in", "zip", "(", "variables1", ",", "variables2", ")", ":", "\n", "            ", "v1", ".", "assign", "(", "v2", ".", "numpy", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.to_onehot": [[18, 21], ["numpy.eye"], "function", ["None"], ["", "", "def", "to_onehot", "(", "size", ",", "value", ")", ":", "\n", "    ", "\"\"\"1 hot encoding for observed state\"\"\"", "\n", "return", "np", ".", "eye", "(", "size", ")", "[", "value", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.play_game": [[128, 234], ["env.reset", "env.game_name.startswith", "list", "agent.to_onehot", "TrainNet.fill_up_buffer", "len", "TrainNet.add_experience", "TrainNet.train", "isinstance", "statistics.mean", "TrainNet.get_action", "env.step", "env.game_name.startswith", "env.step", "env.game_name.startswith", "env.reset", "list.append", "list.append", "TargetNet.copy_weights", "agent.to_onehot", "TrainNet.get_action", "TrainNet.get_action", "agent.to_onehot", "TrainNet.action_buffer.append", "range", "TrainNet.buffer_padding", "TrainNet.action_buffer.append", "TrainNet.buffer_padding", "len", "TrainNet.action_buffer.clear", "TrainNet.buffer_padding", "env.state_buffer.pop", "env.state_buffer.clear", "numpy.sum", "env.done_buffer.pop", "env.done_buffer.clear", "env.reward_buffer.clear", "numpy.append", "numpy.append", "numpy.append", "TrainNet.action_buffer.clear", "TrainNet.buffer_padding", "TrainNet.train.numpy", "numpy.append", "numpy.append", "numpy.append", "random.randint", "TrainNet.action_buffer.popleft"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.to_onehot", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.fill_up_buffer", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.add_experience", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.train", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.get_action", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.copy_weights", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.to_onehot", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.get_action", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.get_action", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.to_onehot", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.buffer_padding", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.buffer_padding", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.buffer_padding", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.buffer_padding"], ["", "", "", "def", "play_game", "(", "global_step", ",", "env", ",", "TrainNet", ",", "TargetNet", ",", "epsilon", ",", "copy_step", ")", ":", "\n", "    ", "rewards", "=", "0", "\n", "episode_step", "=", "0", "\n", "last_state_observed", "=", "0", "\n", "done", "=", "False", "\n", "observations", "=", "env", ".", "reset", "(", ")", "\n", "observations_original", "=", "observations", "\n", "if", "env", ".", "game_name", ".", "startswith", "(", "'Frozen'", ")", ":", "\n", "        ", "observations", "=", "to_onehot", "(", "env", ".", "state_space", ".", "n", ",", "observations", ")", "\n", "", "if", "TrainNet", ".", "alg", "!=", "'normal'", ":", "\n", "        ", "TrainNet", ".", "fill_up_buffer", "(", ")", "\n", "", "losses", "=", "list", "(", ")", "\n", "clear", "=", "False", "\n", "while", "not", "done", ":", "\n", "        ", "delay", "=", "env", ".", "delay", "\n", "len_buffer", "=", "len", "(", "env", ".", "state_buffer", ")", "\n", "if", "TrainNet", ".", "alg", "==", "'normal'", ":", "\n", "            ", "action", "=", "TrainNet", ".", "get_action", "(", "observations", ",", "epsilon", ")", "\n", "prev_observations", "=", "observations", "\n", "observations", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "observations_original", ",", "action", ")", "\n", "observations_original", "=", "observations", "\n", "if", "env", ".", "game_name", ".", "startswith", "(", "'Frozen'", ")", ":", "\n", "                ", "observations", "=", "to_onehot", "(", "env", ".", "state_space", ".", "n", ",", "observations", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "episode_step", "==", "0", ":", "\n", "                ", "if", "env", ".", "use_stochastic_delay", ":", "\n", "                    ", "last_state_observed", "=", "(", "episode_step", "-", "env", ".", "turn_limit", "/", "2", ")", "/", "env", ".", "turn_limit", "\n", "action_state", "=", "np", ".", "append", "(", "last_state_observed", ",", "TrainNet", ".", "action_buffer_padded", ")", "\n", "information_state", "=", "np", ".", "append", "(", "observations", ",", "action_state", ")", "\n", "# information_state = np.append(observations, TrainNet.action_buffer_padded)", "\n", "", "else", ":", "\n", "                    ", "information_state", "=", "np", ".", "append", "(", "observations", ",", "TrainNet", ".", "action_buffer_padded", ")", "\n", "", "", "if", "TrainNet", ".", "alg", "==", "'IS'", ":", "\n", "                ", "action", "=", "TrainNet", ".", "get_action", "(", "information_state", ",", "epsilon", ")", "\n", "", "else", ":", "\n", "                ", "action", "=", "TrainNet", ".", "get_action", "(", "observations", ",", "epsilon", ")", "\n", "", "prev_observations", "=", "observations", "\n", "prev_information_state", "=", "information_state", "\n", "observations", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "observations_original", ",", "action", ")", "\n", "observations_original", "=", "observations", "\n", "if", "env", ".", "game_name", ".", "startswith", "(", "'Frozen'", ")", ":", "\n", "                ", "observations", "=", "to_onehot", "(", "env", ".", "state_space", ".", "n", ",", "observations", ")", "\n", "", "episode_step", "+=", "1", "\n", "\n", "if", "env", ".", "train", ":", "\n", "                ", "last_state_observed", "=", "(", "episode_step", "-", "1", "-", "env", ".", "turn_limit", "/", "2", ")", "/", "env", ".", "turn_limit", "\n", "TrainNet", ".", "action_buffer", ".", "append", "(", "action", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len_buffer", "+", "1", "-", "delay", ")", ":", "\n", "                    ", "TrainNet", ".", "action_buffer", ".", "popleft", "(", ")", "-", "1", "\n", "", "TrainNet", ".", "buffer_padding", "(", ")", "\n", "", "else", ":", "\n", "# delayed_action = random.randint(0, TrainNet.num_actions)", "\n", "                ", "TrainNet", ".", "action_buffer", ".", "append", "(", "action", "+", "1", ")", "\n", "TrainNet", ".", "buffer_padding", "(", ")", "\n", "\n", "", "if", "env", ".", "delay", "==", "0", ":", "\n", "                ", "delayed_action", "=", "action", "\n", "", "else", ":", "\n", "                ", "if", "not", "TrainNet", ".", "action_buffer", ":", "\n", "                    ", "delayed_action", "=", "random", ".", "randint", "(", "0", ",", "TrainNet", ".", "num_actions", ")", "\n", "", "else", ":", "\n", "                    ", "delayed_action", "=", "TrainNet", ".", "action_buffer", "[", "0", "]", "\n", "\n", "", "", "if", "delay", "==", "0", ":", "\n", "                ", "delayed_action", "=", "action", "\n", "\n", "", "if", "len", "(", "TrainNet", ".", "action_buffer", ")", "==", "TrainNet", ".", "max_dimension", "+", "1", ":", "\n", "                ", "TrainNet", ".", "action_buffer", ".", "clear", "(", ")", "\n", "TrainNet", ".", "buffer_padding", "(", ")", "\n", "observations", "=", "env", ".", "state_buffer", ".", "pop", "(", ")", "\n", "env", ".", "state_buffer", ".", "clear", "(", ")", "\n", "reward", "=", "np", ".", "sum", "(", "env", ".", "reward_buffer", ")", "\n", "done", "=", "env", ".", "done_buffer", ".", "pop", "(", ")", "\n", "env", ".", "done_buffer", ".", "clear", "(", ")", "\n", "env", ".", "reward_buffer", ".", "clear", "(", ")", "\n", "clear", "=", "True", "\n", "\n", "", "if", "env", ".", "use_stochastic_delay", ":", "\n", "                ", "action_state", "=", "np", ".", "append", "(", "last_state_observed", ",", "TrainNet", ".", "action_buffer_padded", ")", "\n", "information_state", "=", "np", ".", "append", "(", "observations", ",", "action_state", ")", "\n", "# information_state = np.append(observations, TrainNet.action_buffer_padded)", "\n", "", "else", ":", "\n", "                ", "information_state", "=", "np", ".", "append", "(", "observations", ",", "TrainNet", ".", "action_buffer_padded", ")", "\n", "", "", "rewards", "+=", "reward", "\n", "if", "done", ":", "\n", "            ", "episode_step", "=", "0", "\n", "env", ".", "reset", "(", ")", "\n", "if", "TrainNet", ".", "alg", "!=", "'normal'", ":", "\n", "                ", "TrainNet", ".", "action_buffer", ".", "clear", "(", ")", "\n", "TrainNet", ".", "buffer_padding", "(", ")", "\n", "", "", "global_step", "+=", "1", "\n", "if", "TrainNet", ".", "alg", "==", "'normal'", ":", "\n", "            ", "exp", "=", "{", "'s'", ":", "prev_observations", ",", "'a'", ":", "action", ",", "'r'", ":", "reward", ",", "'s2'", ":", "observations", ",", "'done'", ":", "done", "}", "\n", "", "if", "TrainNet", ".", "alg", "==", "'delay'", ":", "\n", "            ", "exp", "=", "{", "'s'", ":", "prev_observations", ",", "'a'", ":", "delayed_action", ",", "'r'", ":", "reward", ",", "'s2'", ":", "observations", ",", "'done'", ":", "done", "}", "\n", "", "if", "TrainNet", ".", "alg", "==", "'IS'", ":", "\n", "            ", "exp", "=", "{", "'s'", ":", "prev_information_state", ",", "'a'", ":", "action", ",", "'r'", ":", "reward", ",", "'s2'", ":", "information_state", ",", "'done'", ":", "done", "}", "\n", "", "TrainNet", ".", "add_experience", "(", "exp", ")", "\n", "loss", "=", "TrainNet", ".", "train", "(", "TargetNet", ")", "\n", "if", "isinstance", "(", "loss", ",", "int", ")", ":", "\n", "            ", "losses", ".", "append", "(", "loss", ")", "\n", "", "else", ":", "\n", "            ", "losses", ".", "append", "(", "loss", ".", "numpy", "(", ")", ")", "\n", "", "if", "global_step", "%", "copy_step", "==", "0", ":", "\n", "            ", "TargetNet", ".", "copy_weights", "(", "TrainNet", ")", "\n", "", "", "return", "global_step", ",", "rewards", ",", "mean", "(", "losses", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.test": [[236, 250], ["range", "env.reset", "print", "TrainNet.get_action", "env.step", "open", "print"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.get_action", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step"], ["", "def", "test", "(", "env", ",", "TrainNet", ",", "logs", ",", "num_episodes", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "num_episodes", ")", ":", "\n", "        ", "observation", "=", "env", ".", "reset", "(", ")", "\n", "rewards", "=", "0", "\n", "steps", "=", "0", "\n", "done", "=", "False", "\n", "while", "not", "done", ":", "\n", "            ", "action", "=", "TrainNet", ".", "get_action", "(", "observation", ",", "0", ")", "\n", "observation", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "steps", "+=", "1", "\n", "rewards", "+=", "reward", "\n", "", "with", "open", "(", "logs", "[", "'log_file_name'", "]", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "print", "(", "\"Testing steps: {} rewards :{} \"", ".", "format", "(", "steps", ",", "rewards", ")", ",", "file", "=", "f", ")", "\n", "", "print", "(", "\"Testing steps: {} rewards :{} \"", ".", "format", "(", "steps", ",", "rewards", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).agent.train_agent": [[252, 293], ["agent.DQN", "agent.DQN", "env.close", "len", "agent.play_game", "total_rewards_list.append", "total_losses_list.append", "numpy.array", "numpy.array", "total_rewards[].mean", "total_losses[].mean", "env.state_space.sample", "numpy.exp", "print", "open", "print", "max", "max"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.close", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.play_game"], ["", "", "def", "train_agent", "(", "env", ",", "num_frames", ",", "model_params", ",", "algorithm_params", ",", "logs", ",", "verbose", ")", ":", "\n", "    ", "num_actions", "=", "env", ".", "number_of_actions", "\n", "try", ":", "\n", "        ", "state_space", "=", "len", "(", "env", ".", "state_space", ".", "sample", "(", ")", ")", "\n", "", "except", "TypeError", ":", "\n", "        ", "state_space", "=", "env", ".", "state_space", ".", "n", "\n", "\n", "", "copy_step", "=", "model_params", "[", "'copy_step'", "]", "\n", "TrainNet", "=", "DQN", "(", "state_space", ",", "num_actions", ",", "model_params", ",", "algorithm_params", ")", "\n", "TargetNet", "=", "DQN", "(", "state_space", ",", "num_actions", ",", "model_params", ",", "algorithm_params", ")", "\n", "# N = num_episodes", "\n", "total_rewards_list", "=", "[", "]", "\n", "total_losses_list", "=", "[", "]", "\n", "epsilon_start", "=", "algorithm_params", "[", "'start_epsilon'", "]", "\n", "decay", "=", "algorithm_params", "[", "'epsilon_decay'", "]", "\n", "min_epsilon", "=", "algorithm_params", "[", "'stop_epsilon'", "]", "\n", "global_step", "=", "1", "\n", "n", "=", "0", "\n", "while", "True", ":", "\n", "        ", "epsilon", "=", "min_epsilon", "+", "(", "epsilon_start", "-", "min_epsilon", ")", "*", "np", ".", "exp", "(", "-", "decay", "*", "global_step", ")", "\n", "global_step", ",", "total_reward", ",", "losses", "=", "play_game", "(", "global_step", ",", "env", ",", "TrainNet", ",", "TargetNet", ",", "epsilon", ",", "copy_step", ")", "\n", "total_rewards_list", ".", "append", "(", "total_reward", ")", "\n", "total_losses_list", ".", "append", "(", "losses", ")", "\n", "total_rewards", "=", "np", ".", "array", "(", "total_rewards_list", ")", "\n", "total_losses", "=", "np", ".", "array", "(", "total_losses_list", ")", "\n", "avg_rewards", "=", "total_rewards", "[", "max", "(", "0", ",", "n", "-", "100", ")", ":", "(", "n", "+", "1", ")", "]", ".", "mean", "(", ")", "\n", "avg_losses", "=", "total_losses", "[", "max", "(", "0", ",", "n", "-", "100", ")", ":", "(", "n", "+", "1", ")", "]", ".", "mean", "(", ")", "\n", "if", "n", "%", "logs", "[", "'log_interval'", "]", "==", "0", ":", "\n", "            ", "if", "verbose", ":", "\n", "                ", "with", "open", "(", "logs", "[", "'log_file_name'", "]", ",", "\"a\"", ")", "as", "f", ":", "\n", "                    ", "print", "(", "\"episode:{}, eps:{:.3f}, avg reward (last 100):{:.2f}, avg loss:{:.2f}\"", "\n", ".", "format", "(", "n", ",", "epsilon", ",", "avg_rewards", ",", "avg_losses", ")", ",", "file", "=", "f", ")", "\n", "", "", "if", "not", "verbose", ":", "\n", "                ", "print", "(", "\"episode:{}, eps:{:.3f}, avg reward (last 100):{:.2f}\"", "\n", ".", "format", "(", "n", ",", "epsilon", ",", "avg_rewards", ")", ")", "\n", "# test(env, TrainNet, logs, 100)", "\n", "", "", "n", "+=", "1", "\n", "if", "global_step", ">", "num_frames", ":", "\n", "            ", "break", "\n", "", "", "env", ".", "close", "(", ")", "\n", "return", "total_rewards", ",", "total_losses", "\n", "", ""]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env_stochasticdelay.Environment.__init__": [[10, 45], ["gym.make", "env_stochasticdelay.Environment.env.seed", "numpy.random.seed", "random.seed", "collections.deque", "collections.deque", "collections.deque", "env_stochasticdelay.Environment.reset", "env_stochasticdelay.Environment.update_delay"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.update_delay"], ["    ", "def", "__init__", "(", "self", ",", "seed", ",", "game_name", ",", "gamma", ",", "use_stochastic_delay", ",", "delay", ",", "min_delay", ")", ":", "\n", "        ", "\"\"\"Initialize Environment\"\"\"", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "env", "=", "gym", ".", "make", "(", "self", ".", "game_name", ")", "\n", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "number_of_actions", "=", "self", ".", "env", ".", "action_space", ".", "n", "\n", "self", ".", "delay", "=", "delay", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "[", "'image'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "\n", "", "self", ".", "use_stochastic_delay", "=", "use_stochastic_delay", "\n", "self", ".", "no_action", "=", "0", "\n", "self", ".", "index", "=", "0", "\n", "self", ".", "use_stochastic_delay", "=", "use_stochastic_delay", "\n", "if", "self", ".", "use_stochastic_delay", ":", "\n", "            ", "self", ".", "min_delay", "=", "min_delay", "\n", "self", ".", "delay", "=", "self", ".", "min_delay", "\n", "self", ".", "max_delay", "=", "delay", "\n", "", "else", ":", "\n", "            ", "self", ".", "min_delay", "=", "delay", "\n", "self", ".", "delay", "=", "delay", "\n", "self", ".", "max_delay", "=", "delay", "\n", "", "self", ".", "state_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "max_delay", "+", "2", ")", "\n", "self", ".", "reward_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "max_delay", "+", "2", ")", "\n", "self", ".", "done_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "max_delay", "+", "2", ")", "\n", "self", ".", "turn_limit", "=", "200", "\n", "self", ".", "state", "=", "self", ".", "reset", "(", ")", "\n", "self", ".", "update_delay", "(", ")", "\n", "self", ".", "train", "=", "True", "\n", "self", ".", "step_count", "=", "0", "\n", "self", ".", "delayed_action", "=", "0", "\n", "self", ".", "gamma", "=", "gamma", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env_stochasticdelay.Environment.process_state": [[46, 52], ["numpy.array"], "methods", ["None"], ["", "def", "process_state", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Pre-process state if required\"\"\"", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "return", "np", ".", "array", "(", "observation", "[", "'image'", "]", ",", "dtype", "=", "'float32'", ")", "# Using only image as state (7x7x3)", "\n", "", "else", ":", "\n", "            ", "return", "observation", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env_stochasticdelay.Environment.reset": [[53, 59], ["env_stochasticdelay.Environment.env.reset", "env_stochasticdelay.Environment.process_state"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.process_state"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "return", "self", ".", "process_state", "(", "state", ")", "\n", "", "else", ":", "\n", "            ", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env_stochasticdelay.Environment.update_delay": [[60, 65], ["random.randint"], "methods", ["None"], ["", "", "def", "update_delay", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "use_stochastic_delay", ":", "\n", "            ", "self", ".", "delay", "=", "random", ".", "randint", "(", "self", ".", "min_delay", ",", "self", ".", "max_delay", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "delay", "=", "self", ".", "max_delay", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env_stochasticdelay.Environment.step": [[66, 114], ["env_stochasticdelay.Environment.env.step", "env_stochasticdelay.Environment.env.step", "len", "env_stochasticdelay.Environment.state_buffer.append", "env_stochasticdelay.Environment.reward_buffer.append", "env_stochasticdelay.Environment.done_buffer.append", "len", "env_stochasticdelay.Environment.state_buffer.append", "env_stochasticdelay.Environment.reward_buffer.append", "env_stochasticdelay.Environment.done_buffer.append", "range", "env_stochasticdelay.Environment.update_delay", "env_stochasticdelay.Environment.state_buffer.append", "env_stochasticdelay.Environment.reward_buffer.append", "env_stochasticdelay.Environment.done_buffer.append", "env_stochasticdelay.Environment.state_buffer.popleft", "env_stochasticdelay.Environment.reward_buffer.popleft", "env_stochasticdelay.Environment.done_buffer.popleft", "env_stochasticdelay.Environment.update_delay", "len", "env_stochasticdelay.Environment.state_buffer.popleft", "numpy.power", "env_stochasticdelay.Environment.done_buffer.popleft", "env_stochasticdelay.Environment.state_buffer.clear", "env_stochasticdelay.Environment.reward_buffer.clear", "env_stochasticdelay.Environment.done_buffer.clear", "env_stochasticdelay.Environment.state_buffer.clear", "env_stochasticdelay.Environment.reward_buffer.clear", "env_stochasticdelay.Environment.done_buffer.clear", "env_stochasticdelay.Environment.reward_buffer.popleft"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.update_delay", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.update_delay"], ["", "", "def", "step", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "if", "self", ".", "max_delay", "!=", "0", ":", "\n", "            ", "self", ".", "train", "=", "True", "\n", "if", "True", "not", "in", "self", ".", "done_buffer", ":", "\n", "                ", "next_state", ",", "rewards", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "", "else", ":", "\n", "                ", "next_state", "=", "state", "\n", "rewards", "=", "0", "\n", "done", "=", "True", "\n", "", "if", "len", "(", "self", ".", "state_buffer", ")", "<", "self", ".", "delay", ":", "# delay is greater than the number of unobserved states", "\n", "                ", "self", ".", "state_buffer", ".", "append", "(", "next_state", ")", "\n", "self", ".", "reward_buffer", ".", "append", "(", "rewards", ")", "\n", "self", ".", "done_buffer", ".", "append", "(", "done", ")", "\n", "self", ".", "train", "=", "False", "\n", "return", "state", ",", "0", ",", "False", "\n", "", "elif", "len", "(", "self", ".", "state_buffer", ")", ">", "self", ".", "delay", ":", "# delay is less than the number of unobserved states", "\n", "                ", "self", ".", "state_buffer", ".", "append", "(", "next_state", ")", "\n", "self", ".", "reward_buffer", ".", "append", "(", "rewards", ")", "\n", "self", ".", "done_buffer", ".", "append", "(", "done", ")", "\n", "rewards", "=", "0", "\n", "no_observed_states", "=", "len", "(", "self", ".", "state_buffer", ")", "-", "self", ".", "delay", "\n", "for", "i", "in", "range", "(", "no_observed_states", ")", ":", "\n", "                    ", "next_state", "=", "self", ".", "state_buffer", ".", "popleft", "(", ")", "\n", "gamma", "=", "np", ".", "power", "(", "self", ".", "gamma", ",", "no_observed_states", "-", "(", "i", "+", "1", ")", ")", "\n", "rewards", "+=", "gamma", "*", "self", ".", "reward_buffer", ".", "popleft", "(", ")", "# add all unobserved rewards", "\n", "done", "=", "self", ".", "done_buffer", ".", "popleft", "(", ")", "\n", "", "self", ".", "update_delay", "(", ")", "\n", "if", "done", ":", "\n", "                    ", "self", ".", "state_buffer", ".", "clear", "(", ")", "\n", "self", ".", "reward_buffer", ".", "clear", "(", ")", "\n", "self", ".", "done_buffer", ".", "clear", "(", ")", "\n", "", "return", "next_state", ",", "rewards", ",", "done", "\n", "", "else", ":", "\n", "                ", "self", ".", "state_buffer", ".", "append", "(", "next_state", ")", "\n", "self", ".", "reward_buffer", ".", "append", "(", "rewards", ")", "\n", "self", ".", "done_buffer", ".", "append", "(", "done", ")", "\n", "delayed_next_state", "=", "self", ".", "state_buffer", ".", "popleft", "(", ")", "\n", "delayed_rewards", "=", "self", ".", "reward_buffer", ".", "popleft", "(", ")", "\n", "delayed_done", "=", "self", ".", "done_buffer", ".", "popleft", "(", ")", "\n", "self", ".", "update_delay", "(", ")", "\n", "if", "delayed_done", ":", "\n", "                    ", "self", ".", "state_buffer", ".", "clear", "(", ")", "\n", "self", ".", "reward_buffer", ".", "clear", "(", ")", "\n", "self", ".", "done_buffer", ".", "clear", "(", ")", "\n", "", "return", "delayed_next_state", ",", "delayed_rewards", ",", "delayed_done", "\n", "", "", "else", ":", "\n", "            ", "next_state", ",", "rewards", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "return", "next_state", ",", "rewards", ",", "done", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env_stochasticdelay.Environment.render": [[115, 117], ["env_stochasticdelay.Environment.env.render"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.render"], ["", "", "def", "render", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env_stochasticdelay.Environment.close": [[118, 120], ["env_stochasticdelay.Environment.env.close"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).plot.running_mean": [[14, 17], ["numpy.cumsum", "numpy.insert", "float"], "function", ["None"], ["def", "running_mean", "(", "x", ",", "n", ")", ":", "\n", "    ", "cumulative_sum", "=", "np", ".", "cumsum", "(", "np", ".", "insert", "(", "x", ",", "0", ",", "0", ")", ")", "\n", "return", "(", "cumulative_sum", "[", "n", ":", "]", "-", "cumulative_sum", "[", ":", "-", "n", "]", ")", "/", "float", "(", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).plot.get_file": [[19, 30], ["os.listdir", "os.getcwd", "open", "open", "[].split", "[].split", "f.readline().split", "f.readline().split", "f.readline", "f.readline"], "function", ["None"], ["", "def", "get_file", "(", "index", ",", "ver", ")", ":", "\n", "    ", "save_dir", "=", "os", ".", "getcwd", "(", ")", "+", "'/Results/Results-{}/'", ".", "format", "(", "ver", ")", "# Save Directory", "\n", "files_list", "=", "os", ".", "listdir", "(", "save_dir", ")", "\n", "if", "ver", "==", "'6.12'", ":", "\n", "        ", "with", "open", "(", "save_dir", "+", "files_list", "[", "index", "]", "+", "'/log.txt'", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "env_name", "=", "f", ".", "readline", "(", ")", ".", "split", "(", "','", ")", "[", "0", "]", ".", "split", "(", "':'", ")", "[", "1", "]", "# Gets the environment name", "\n", "", "", "else", ":", "\n", "        ", "with", "open", "(", "save_dir", "+", "files_list", "[", "index", "]", "+", "'/log_sd.txt'", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "env_name", "=", "f", ".", "readline", "(", ")", ".", "split", "(", "','", ")", "[", "0", "]", ".", "split", "(", "':'", ")", "[", "1", "]", "# Gets the environment name", "\n", "", "", "file_name", "=", "save_dir", "+", "files_list", "[", "index", "]", "# Final files directory", "\n", "return", "env_name", ",", "file_name", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).plot.plot_reward": [[32, 68], ["plot.get_file", "matplotlib.figure", "matplotlib.legend", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.tight_layout", "matplotlib.show", "matplotlib.title", "matplotlib.title", "matplotlib.title", "numpy.arange", "numpy.zeros", "range", "numpy.mean", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.plot", "os.getcwd", "os.getcwd", "os.getcwd", "numpy.std", "numpy.sqrt", "plot.running_mean", "numpy.load", "numpy.load"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.running_mean", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load"], ["", "def", "plot_reward", "(", "index", ",", "runs", ",", "delays", ",", "n", ")", ":", "\n", "    ", "env_name", ",", "file_name", "=", "get_file", "(", "index", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "if", "index", "==", "-", "1", ":", "\n", "        ", "plt", ".", "title", "(", "'DQN'", ")", "\n", "save_file", "=", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}/rewards_normal.pdf'", ".", "format", "(", "ver", ")", "\n", "", "if", "index", "==", "-", "2", ":", "\n", "        ", "plt", ".", "title", "(", "'DQN+IS'", ")", "\n", "save_file", "=", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}/rewards_IS.pdf'", ".", "format", "(", "ver", ")", "\n", "", "if", "index", "==", "-", "3", ":", "\n", "        ", "plt", ".", "title", "(", "'delay-DQN'", ")", "\n", "save_file", "=", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}/rewards_delay.pdf'", ".", "format", "(", "ver", ")", "\n", "", "for", "delay", "in", "delays", ":", "\n", "        ", "episodes", "=", "10000", "\n", "X_axis", "=", "np", ".", "arange", "(", "episodes", ")", "\n", "rewards_plot", "=", "np", ".", "zeros", "(", "[", "runs", ",", "episodes", "]", ")", "\n", "for", "run", "in", "range", "(", "runs", ")", ":", "\n", "            ", "if", "delay", "==", "'stochastic'", ":", "\n", "                ", "rewards", "=", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/rewards_delay_20_sd_run_{}.npy'", ".", "format", "(", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "\n", "(", ")", "]", "\n", "", "else", ":", "\n", "                ", "rewards", "=", "np", ".", "load", "(", "file_name", "+", "'/rewards_delay_{}_run_{}.npy'", ".", "format", "(", "delay", ",", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "# plt.plot(running_mean(rewards, n), alpha=0.25, linestyle='-.', color='blue')", "\n", "", "rewards_plot", "[", "run", "]", "=", "rewards", "[", "0", ":", "episodes", "]", "\n", "", "rewards_mean", "=", "np", ".", "mean", "(", "rewards_plot", ",", "axis", "=", "0", ")", "\n", "rewards_deviation", "=", "np", ".", "std", "(", "rewards_plot", ",", "axis", "=", "0", ")", "/", "np", ".", "sqrt", "(", "runs", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Rewards'", ")", "\n", "# plt.ylim(0, 210)", "\n", "plt", ".", "plot", "(", "running_mean", "(", "rewards_mean", ",", "n", ")", ",", "label", "=", "'delay={}'", ".", "format", "(", "delay", ")", ")", "\n", "# plt.fill_between(X_axis, rewards_mean+rewards_deviation, rewards_mean-rewards_deviation, alpha=1.5)", "\n", "", "plt", ".", "legend", "(", "title", "=", "'Delays'", ",", "bbox_to_anchor", "=", "(", "1.05", ",", "1", ")", ",", "loc", "=", "'upper left'", ")", "\n", "plt", ".", "savefig", "(", "save_file", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "plt", ".", "savefig", "(", "file_name", "+", "'/rewards.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).plot.compare_learning_curves": [[70, 109], ["matplotlib.figure", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "range", "matplotlib.legend", "matplotlib.tight_layout", "matplotlib.show", "plot.get_file", "env_name.append", "file_name.append", "len", "range", "os.getcwd", "matplotlib.savefig", "matplotlib.plot", "os.makedirs", "matplotlib.savefig", "plot.running_mean", "numpy.load", "numpy.load", "os.getcwd"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.running_mean", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load"], ["", "def", "compare_learning_curves", "(", "indices", ",", "label", ",", "ver", ",", "runs", ",", "delay", ",", "n", "=", "1000", ")", ":", "\n", "    ", "env_name", "=", "[", "]", "\n", "file_name", "=", "[", "]", "\n", "for", "index", "in", "indices", ":", "\n", "        ", "env", ",", "file", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "env_name", ".", "append", "(", "env", ")", "\n", "file_name", ".", "append", "(", "file", ")", "\n", "# if not env_name.count(env_name[0]) == len(env_name):  # Check if all the environments are same", "\n", "#     raise Exception('Environments are different')", "\n", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "title", "(", "env_name", "[", "0", "]", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Rewards'", ")", "\n", "colors", "=", "[", "'blue'", ",", "'tab:orange'", ",", "'green'", "]", "\n", "for", "index", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "# episodes = 10000", "\n", "# X_axis = np.arange(episodes)", "\n", "# rewards_plot = np.zeros([runs, episodes])", "\n", "        ", "for", "run", "in", "range", "(", "runs", ")", ":", "\n", "            ", "if", "delay", "==", "'stochastic'", ":", "\n", "                ", "rewards", "=", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/rewards_delay_10_sd_run_{}.npy'", ".", "format", "(", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "# rewards_plot[run] = np.mean(rewards)", "\n", "# print('Algorithm: {} Delay: Stochastic Run: {} Reward: {}'.format(index, run, rewards_plot[run]))", "\n", "", "else", ":", "\n", "                ", "rewards", "=", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/rewards_delay_{}_run_{}.npy'", ".", "format", "(", "delay", ",", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "# rewards_plot[run] = np.mean(rewards)", "\n", "# print('Algorithm: {} Delay: {} Run: {} Reward: {}'.format(index, delay, run, rewards_plot[run]))", "\n", "", "plt", ".", "plot", "(", "running_mean", "(", "rewards", ",", "n", ")", ",", "label", "=", "label", "[", "index", "]", "if", "run", "==", "0", "else", "''", ",", "color", "=", "colors", "[", "index", "]", ",", "alpha", "=", "0.5", ")", "\n", "", "", "plt", ".", "legend", "(", "title", "=", "'Algorithms'", ",", "bbox_to_anchor", "=", "(", "1.05", ",", "1", ")", ",", "loc", "=", "'upper left'", ")", "\n", "save_dir", "=", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}'", ".", "format", "(", "ver", ")", "\n", "try", ":", "\n", "        ", "plt", ".", "savefig", "(", "save_dir", "+", "'/rewards_curves.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}'", ".", "format", "(", "ver", ")", ")", "\n", "plt", ".", "savefig", "(", "save_dir", "+", "'/rewards_curves.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).plot.plot_losses": [[111, 128], ["plot.get_file", "matplotlib.figure", "matplotlib.title", "len", "numpy.arange", "numpy.zeros", "range", "numpy.mean", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.plot", "matplotlib.savefig", "matplotlib.show", "numpy.load", "numpy.std", "numpy.sqrt", "plot.running_mean"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.running_mean"], ["", "def", "plot_losses", "(", "index", ",", "runs", ",", "n", ")", ":", "\n", "    ", "env_name", ",", "file_name", "=", "get_file", "(", "index", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "title", "(", "env_name", ")", "\n", "losses", "=", "np", ".", "load", "(", "file_name", "+", "'/loss.npy'", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "episodes", "=", "len", "(", "losses", "[", "0", "]", ")", "\n", "X_axis", "=", "np", ".", "arange", "(", "episodes", ")", "\n", "losses_plot", "=", "np", ".", "zeros", "(", "[", "runs", ",", "episodes", "]", ")", "\n", "for", "run", "in", "range", "(", "runs", ")", ":", "\n", "        ", "losses_plot", "[", "run", "]", "=", "losses", "[", "run", "]", "\n", "", "losses_mean", "=", "np", ".", "mean", "(", "losses_plot", ",", "axis", "=", "0", ")", "\n", "losses_deviation", "=", "np", ".", "std", "(", "losses_plot", ",", "axis", "=", "0", ")", "/", "np", ".", "sqrt", "(", "runs", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Losses'", ")", "\n", "plt", ".", "plot", "(", "running_mean", "(", "losses_mean", ",", "n", ")", ")", "\n", "plt", ".", "savefig", "(", "file_name", "+", "'/losses.pdf'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).plot.compare_algorithms": [[130, 191], ["matplotlib.figure", "matplotlib.title", "range", "matplotlib.legend", "matplotlib.xlabel", "matplotlib.xticks", "matplotlib.ylabel", "matplotlib.yticks", "matplotlib.tight_layout", "matplotlib.show", "plot.get_file", "env_name.append", "file_name.append", "plot.get_file", "file_name_sd.append", "len", "list", "numpy.zeros", "numpy.zeros", "numpy.zeros", "matplotlib.errorbar", "os.getcwd", "matplotlib.savefig", "map", "len", "len", "range", "numpy.mean", "numpy.mean", "os.makedirs", "matplotlib.savefig", "numpy.std", "numpy.sqrt", "numpy.mean", "print", "numpy.mean", "print", "os.getcwd", "numpy.load", "numpy.load"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load"], ["", "def", "compare_algorithms", "(", "indices", ",", "label", ",", "runs", ",", "delays", ",", "ver", ",", "colors", ")", ":", "\n", "    ", "env_name", "=", "[", "]", "\n", "file_name", "=", "[", "]", "\n", "file_name_sd", "=", "[", "]", "\n", "for", "index", "in", "indices", ":", "\n", "        ", "env", ",", "file", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "env_name", ".", "append", "(", "env", ")", "\n", "file_name", ".", "append", "(", "file", ")", "\n", "", "for", "index", "in", "indices", ":", "\n", "        ", "_", ",", "file_sd", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "file_name_sd", ".", "append", "(", "file_sd", ")", "\n", "# if not env_name.count(env_name[0]) == len(env_name):  # Check if all the environments are same", "\n", "#     raise Exception('Environments are different')", "\n", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "title", "(", "env_name", "[", "0", "]", ",", "fontsize", "=", "20", ")", "\n", "for", "index", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "        ", "count", "=", "0", "\n", "X_axis", "=", "list", "(", "map", "(", "str", ",", "delays", ")", ")", "\n", "r_mean", "=", "np", ".", "zeros", "(", "len", "(", "delays", ")", ")", "\n", "r_std", "=", "np", ".", "zeros", "(", "len", "(", "delays", ")", ")", "\n", "episodes", "=", "10000", "\n", "# rewards_plot = np.zeros([runs, episodes])", "\n", "rewards_plot", "=", "np", ".", "zeros", "(", "runs", ")", "\n", "for", "delay", "in", "delays", ":", "\n", "            ", "for", "run", "in", "range", "(", "runs", ")", ":", "\n", "                ", "if", "delay", "==", "'stochastic'", ":", "\n", "                    ", "rewards", "=", "np", ".", "load", "(", "file_name_sd", "[", "index", "]", "+", "'/rewards_delay_10_sd_run_{}.npy'", ".", "format", "(", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "\n", "(", ")", "]", "\n", "rewards_plot", "[", "run", "]", "=", "np", ".", "mean", "(", "rewards", ")", "\n", "print", "(", "'Algorithm: {} Delay: Stochastic Run: {} Reward: {}'", ".", "format", "(", "index", ",", "run", ",", "rewards_plot", "[", "run", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "rewards", "=", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/rewards_delay_{}_run_{}.npy'", ".", "format", "(", "delay", ",", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "rewards_plot", "[", "run", "]", "=", "np", ".", "mean", "(", "rewards", ")", "\n", "print", "(", "'Algorithm: {} Delay: {} Run: {} Reward: {}'", ".", "format", "(", "index", ",", "delay", ",", "run", ",", "rewards_plot", "[", "run", "]", ")", ")", "\n", "", "", "rewards_mean", "=", "rewards_plot", "\n", "# rewards_mean = np.mean(rewards_plot, axis=0)", "\n", "rewards_deviation", "=", "np", ".", "std", "(", "rewards_plot", ",", "axis", "=", "0", ")", "/", "np", ".", "sqrt", "(", "runs", ")", "\n", "r_mean", "[", "count", "]", "=", "np", ".", "mean", "(", "rewards_mean", ")", "\n", "r_std", "[", "count", "]", "=", "np", ".", "mean", "(", "rewards_deviation", ")", "\n", "count", "+=", "1", "\n", "", "if", "label", "[", "index", "]", "==", "'DQN+IS'", ":", "\n", "            ", "alg", "=", "'DRDQN'", "\n", "", "else", ":", "\n", "            ", "alg", "=", "label", "[", "index", "]", "\n", "# plt.plot(X_axis, r_mean, marker='o', label=alg, color=colors[label[index]])", "\n", "", "plt", ".", "errorbar", "(", "X_axis", ",", "r_mean", ",", "marker", "=", "'o'", ",", "yerr", "=", "r_std", ",", "label", "=", "alg", ",", "color", "=", "colors", "[", "label", "[", "index", "]", "]", ",", "uplims", "=", "True", ",", "lolims", "=", "True", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "xlabel", "(", "'Delays'", ",", "fontsize", "=", "16", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "16", ")", "\n", "plt", ".", "ylabel", "(", "'Rewards'", ",", "fontsize", "=", "16", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "16", ")", "\n", "save_dir", "=", "os", ".", "getcwd", "(", ")", "+", "'/CartPole/Plots/v{}'", ".", "format", "(", "ver", ")", "\n", "try", ":", "\n", "        ", "plt", ".", "savefig", "(", "save_dir", "+", "'/rewards_comparison.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "getcwd", "(", ")", "+", "'/CartPole/Plots/v{}'", ".", "format", "(", "ver", ")", ")", "\n", "plt", ".", "savefig", "(", "save_dir", "+", "'/rewards_comparison.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).plot.plot_time": [[193, 226], ["matplotlib.figure", "range", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.legend", "matplotlib.tight_layout", "matplotlib.show", "plot.get_file", "env_name.append", "file_name.append", "plot.get_file", "file_name_sd.append", "len", "numpy.zeros", "list", "range", "matplotlib.plot", "os.getcwd", "matplotlib.savefig", "map", "len", "os.makedirs", "matplotlib.savefig", "len", "numpy.mean", "numpy.mean", "numpy.load", "numpy.load", "os.getcwd"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load"], ["", "def", "plot_time", "(", "indices", ",", "labels", ",", "delays", ",", "ver", ")", ":", "\n", "    ", "plt", ".", "figure", "(", ")", "\n", "env_name", "=", "[", "]", "\n", "file_name", "=", "[", "]", "\n", "file_name_sd", "=", "[", "]", "\n", "for", "index", "in", "indices", ":", "\n", "        ", "env", ",", "file", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "env_name", ".", "append", "(", "env", ")", "\n", "file_name", ".", "append", "(", "file", ")", "\n", "", "for", "index", "in", "indices", ":", "\n", "        ", "_", ",", "file_sd", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "file_name_sd", ".", "append", "(", "file_sd", ")", "\n", "", "for", "index", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "        ", "time", "=", "np", ".", "zeros", "(", "[", "len", "(", "delays", ")", "]", ")", "\n", "X_axis", "=", "list", "(", "map", "(", "str", ",", "delays", ")", ")", "\n", "for", "delay", "in", "range", "(", "len", "(", "delays", ")", ")", ":", "\n", "            ", "if", "delays", "[", "delay", "]", "==", "'stochastic'", ":", "\n", "                ", "time", "[", "delay", "]", "=", "np", ".", "mean", "(", "np", ".", "load", "(", "file_name_sd", "[", "index", "]", "+", "'/time_delay_10_sd.npy'", ")", ")", "\n", "", "else", ":", "\n", "                ", "time", "[", "delay", "]", "=", "np", ".", "mean", "(", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/time_delay_{}.npy'", ".", "format", "(", "delays", "[", "delay", "]", ")", ")", ")", "\n", "", "", "plt", ".", "plot", "(", "X_axis", ",", "time", "/", "3600", ",", "label", "=", "labels", "[", "index", "]", ",", "marker", "=", "'o'", ")", "\n", "", "plt", ".", "title", "(", "env_name", "[", "0", "]", ")", "\n", "plt", ".", "xlabel", "(", "'Delays'", ")", "\n", "plt", ".", "ylabel", "(", "'Average Hours per run'", ")", "\n", "plt", ".", "legend", "(", ")", "\n", "save_dir", "=", "os", ".", "getcwd", "(", ")", "+", "'/CartPole/Plots/v{}/'", ".", "format", "(", "ver", ")", "\n", "try", ":", "\n", "        ", "plt", ".", "savefig", "(", "save_dir", "+", "'/time_comparison.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "getcwd", "(", ")", "+", "'/CartPole/Plots/v{}'", ".", "format", "(", "ver", ")", ")", "\n", "plt", ".", "savefig", "(", "save_dir", "+", "'/time_comparison.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.__init__": [[8, 23], ["gym.make", "env.Environment.env.seed", "numpy.random.seed", "collections.deque", "env.Environment.fill_up_buffer"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.fill_up_buffer"], ["    ", "def", "__init__", "(", "self", ",", "game_name", ",", "delay", ",", "seed", ")", ":", "\n", "        ", "\"\"\"Initialize Environment\"\"\"", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "env", "=", "gym", ".", "make", "(", "self", ".", "game_name", ")", "\n", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "number_of_actions", "=", "self", ".", "env", ".", "action_space", ".", "n", "\n", "self", ".", "delay", "=", "delay", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "[", "'image'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "\n", "", "self", ".", "actions_in_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "delay", ")", "\n", "self", ".", "fill_up_buffer", "(", ")", "\n", "self", ".", "delayed_action", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.process_state": [[24, 30], ["numpy.array"], "methods", ["None"], ["", "def", "process_state", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Pre-process state if required\"\"\"", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "return", "np", ".", "array", "(", "observation", "[", "'image'", "]", ",", "dtype", "=", "'float32'", ")", "# Using only image as state (7x7x3)", "\n", "", "else", ":", "\n", "            ", "return", "observation", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.fill_up_buffer": [[31, 35], ["range", "numpy.random.choice", "env.Environment.actions_in_buffer.append"], "methods", ["None"], ["", "", "def", "fill_up_buffer", "(", "self", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "self", ".", "delay", ")", ":", "\n", "            ", "action", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "number_of_actions", ")", "\n", "self", ".", "actions_in_buffer", ".", "append", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.reset": [[36, 43], ["env.Environment.env.reset", "env.Environment.fill_up_buffer", "env.Environment.process_state"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.fill_up_buffer", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.process_state"], ["", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "self", ".", "fill_up_buffer", "(", ")", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "return", "self", ".", "process_state", "(", "state", ")", "\n", "", "else", ":", "\n", "            ", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.step": [[44, 56], ["env.Environment.actions_in_buffer.popleft", "env.Environment.actions_in_buffer.append", "env.Environment.env.step", "env.Environment.env.step", "env.Environment.process_state"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.process_state"], ["", "", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "self", ".", "delay", "!=", "0", ":", "\n", "            ", "chosen_action", "=", "action", "\n", "self", ".", "delayed_action", "=", "self", ".", "actions_in_buffer", ".", "popleft", "(", ")", "\n", "self", ".", "actions_in_buffer", ".", "append", "(", "chosen_action", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "delayed_action", "=", "action", "\n", "", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "self", ".", "delayed_action", ")", "\n", "return", "self", ".", "process_state", "(", "next_state", ")", ",", "reward", ",", "done", ",", "info", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "env", ".", "step", "(", "self", ".", "delayed_action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.render": [[57, 59], ["env.Environment.env.render"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.render"], ["", "", "def", "render", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.close": [[60, 62], ["env.Environment.env.close"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Stochastic).env.Environment.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).init_main.init_main": [[66, 106], ["dict", "wandb.init", "delayed_env.DelayedEnv", "gym.make", "gym.make.seed", "gym.make", "gym.make.seed"], "function", ["None"], ["def", "init_main", "(", "algorithm", ",", "delay", ",", "seed", ")", ":", "\n", "    ", "hyperparameter_defaults", "=", "dict", "(", "\n", "is_delayed_agent", "=", "False", ",", "\n", "double_q", "=", "True", ",", "\n", "delay_value", "=", "delay", ",", "\n", "epsilon_decay", "=", "1e-4", ",", "\n", "epsilon_min", "=", "0.001", ",", "#0.001", "\n", "learning_rate", "=", "0.001", ",", "#0.005, #mountainCar: 0.0001", "\n", "seed", "=", "seed", ",", "\n", "epsilon", "=", "1.0", ",", "\n", "use_m_step_reward", "=", "False", ",", "\n", "use_latest_reward", "=", "False", ",", "\n", "use_reward_shaping", "=", "False", ",", "\n", "physical_noise_std_ratio", "=", "0.0", ",", "#0.1", "\n", "env_name", "=", "'CartPole-v0'", ",", "#'CartPole-v1', 'Acrobot-v1', 'MountainCar-v0'", "\n", "train_freq", "=", "1", ",", "\n", "target_network_update_freq", "=", "300", ",", "\n", "use_learned_forward_model", "=", "True", ",", "\n", "agent_type", "=", "algorithm", ",", "# 'augmented' (DRDQN), 'delayed' (https://openreview.net/forum?id=j1RMMKeP2gR), 'oblivious' (DQN), 'delay' (https://ieeexplore.ieee.org/document/5650345)", "\n", "total_steps", "=", "3000", ",", "\n", ")", "\n", "# Pass your defaults to wandb.init", "\n", "wandb", ".", "init", "(", "project", "=", "'delay-rl'", ",", "name", "=", "'DQN_{}_delay_{}_seed_{}'", ".", "format", "(", "algorithm", ",", "delay", ",", "seed", ")", ",", "\n", "config", "=", "hyperparameter_defaults", ")", "\n", "config", "=", "wandb", ".", "config", "\n", "if", "'CartPole'", "in", "config", ".", "env_name", "or", "'Acrobot'", "in", "config", ".", "env_name", ":", "\n", "        ", "orig_env", "=", "gym", ".", "make", "(", "config", ".", "env_name", ")", "\n", "orig_env", ".", "seed", "(", "seed", ")", "\n", "", "else", ":", "\n", "        ", "orig_env", "=", "gym", ".", "make", "(", "config", ".", "env_name", ")", "\n", "orig_env", ".", "seed", "(", "seed", ")", "\n", "# orig_env = DiscretizeActions(orig_env) # for mujoco envs", "\n", "", "delayed_env", "=", "DelayedEnv", "(", "orig_env", ",", "config", ".", "delay_value", ")", "\n", "state_size", "=", "orig_env", ".", "observation_space", ".", "shape", "\n", "if", "not", "delayed_env", ".", "is_atari_env", ":", "\n", "        ", "state_size", "=", "state_size", "[", "0", "]", "\n", "", "action_size", "=", "orig_env", ".", "action_space", ".", "n", "\n", "done", "=", "False", "\n", "batch_size", "=", "32", "\n", "return", "config", ",", "delayed_env", ",", "state_size", ",", "action_size", ",", "done", ",", "batch_size", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).ddqn_main.init_episode": [[35, 45], ["delayed_env.reset", "ddqn_main.massage_state", "agent.clear_action_buffer"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).ddqn_main.massage_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.clear_action_buffer"], ["def", "init_episode", "(", "delayed_env", ",", "agent", ",", "augment_state", ",", "state_size", ")", ":", "\n", "    ", "ep_reward", "=", "0", "\n", "ep_reshaped_reward", "=", "0", "\n", "state", "=", "delayed_env", ".", "reset", "(", ")", "\n", "state", "=", "massage_state", "(", "state", ",", "augment_state", ",", "delayed_env", ",", "state_size", ")", "\n", "agent", ".", "clear_action_buffer", "(", ")", "\n", "loss_dict", "=", "{", "}", "\n", "loss_count", "=", "0", "\n", "ep_step", "=", "0", "\n", "return", "ep_reward", ",", "ep_reshaped_reward", ",", "state", ",", "loss_dict", ",", "loss_count", ",", "ep_step", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).ddqn_main.routinely_save_agent": [[46, 48], ["None"], "function", ["None"], ["", "def", "routinely_save_agent", "(", "e", ",", "env_name", ")", ":", "\n", "    ", "pass", "\n", "# agent_name = env_name + '_ddqn_delay.h5'", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).ddqn_main.agent_act": [[58, 64], ["agent.act", "agent.act", "delayed_env.get_pending_actions"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_pending_actions"], ["", "def", "agent_act", "(", "config", ",", "agent", ",", "state", ",", "delayed_env", ",", "eval", "=", "False", ")", ":", "\n", "    ", "if", "config", ".", "agent_type", "==", "'delayed'", ":", "\n", "        ", "action", "=", "agent", ".", "act", "(", "state", ",", "pending_actions", "=", "delayed_env", ".", "get_pending_actions", "(", ")", ",", "eval", "=", "eval", ")", "\n", "", "else", ":", "\n", "        ", "action", "=", "agent", ".", "act", "(", "state", ",", "eval", ")", "\n", "", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).ddqn_main.massage_state": [[65, 70], ["dqn_agents.reshape_state", "numpy.concatenate", "delayed_env.get_pending_actions"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.reshape_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_pending_actions"], ["", "def", "massage_state", "(", "state", ",", "augment_state", ",", "delayed_env", ",", "state_size", ")", ":", "\n", "    ", "if", "augment_state", ":", "\n", "        ", "state", "=", "np", ".", "concatenate", "(", "(", "state", ",", "delayed_env", ".", "get_pending_actions", "(", ")", ")", ")", "\n", "", "state", "=", "reshape_state", "(", "state", ",", "delayed_env", ".", "is_atari_env", ",", "state_size", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.__init__": [[34, 56], ["numpy.random.seed", "tensorflow.random.set_seed", "random.seed", "collections.deque", "collections.deque", "dqn_agents.DQNAgent._build_model"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent._build_model"], ["    ", "def", "__init__", "(", "self", ",", "seed", ",", "state_size", ",", "action_size", ",", "is_atari_env", ",", "is_delayed_agent", "=", "False", ",", "delay_value", "=", "0", ",", "epsilon_min", "=", "0.001", ",", "\n", "epsilon_decay", "=", "0.999", ",", "learning_rate", "=", "0.001", ",", "epsilon", "=", "1.0", ",", "use_m_step_reward", "=", "False", ",", "use_latest_reward", "=", "True", ",", "\n", "loss", "=", "'mse'", ",", "**", "kwargs", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "tf", ".", "random", ".", "set_seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "state_size", "=", "state_size", "\n", "self", ".", "action_size", "=", "action_size", "\n", "self", ".", "is_atari_env", "=", "is_atari_env", "\n", "mem_len", "=", "50000", "if", "self", ".", "is_atari_env", "else", "1000", "\n", "self", ".", "memory", "=", "deque", "(", "maxlen", "=", "mem_len", ")", "\n", "self", ".", "gamma", "=", "0.99", "# discount rate", "\n", "self", ".", "epsilon", "=", "epsilon", "# exploration rate", "\n", "self", ".", "epsilon_min", "=", "epsilon_min", "\n", "self", ".", "epsilon_decay", "=", "epsilon_decay", "\n", "self", ".", "learning_rate", "=", "learning_rate", "\n", "self", ".", "sample_buffer", "=", "deque", "(", ")", "\n", "self", ".", "is_delayed_agent", "=", "is_delayed_agent", "\n", "self", ".", "delay_value", "=", "delay_value", "\n", "self", ".", "model", "=", "self", ".", "_build_model", "(", "loss", "=", "loss", ")", "\n", "self", ".", "use_m_step_reward", "=", "use_m_step_reward", "\n", "self", ".", "use_latest_reward", "=", "use_latest_reward", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent._huber_loss": [[58, 70], ["keras.backend.mean", "keras.backend.abs", "keras.backend.square", "tensorflow.where", "keras.backend.square", "keras.backend.abs"], "methods", ["None"], ["", "def", "_huber_loss", "(", "self", ",", "y_true", ",", "y_pred", ",", "clip_delta", "=", "1.0", ")", ":", "\n", "        ", "\"\"\"Huber loss for Q Learning\n        References: https://en.wikipedia.org/wiki/Huber_loss\n                    https://www.tensorflow.org/api_docs/python/tf/losses/huber_loss\n        \"\"\"", "\n", "error", "=", "y_true", "-", "y_pred", "\n", "cond", "=", "K", ".", "abs", "(", "error", ")", "<=", "clip_delta", "\n", "\n", "squared_loss", "=", "0.5", "*", "K", ".", "square", "(", "error", ")", "\n", "quadratic_loss", "=", "0.5", "*", "K", ".", "square", "(", "clip_delta", ")", "+", "clip_delta", "*", "(", "K", ".", "abs", "(", "error", ")", "-", "clip_delta", ")", "\n", "\n", "return", "K", ".", "mean", "(", "tf", ".", "where", "(", "cond", ",", "squared_loss", ",", "quadratic_loss", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent._build_forward_model": [[71, 82], ["keras.models.Sequential", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.compile", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.Adam"], "methods", ["None"], ["", "def", "_build_forward_model", "(", "self", ",", "loss", "=", "'mse'", ",", "input_size", "=", "None", ",", "output_size", "=", "None", ")", ":", "\n", "        ", "input_size", "=", "self", ".", "state_size", "if", "input_size", "is", "None", "else", "input_size", "\n", "output_size", "=", "self", ".", "action_size", "if", "output_size", "is", "None", "else", "output_size", "\n", "model", "=", "Sequential", "(", ")", "\n", "model", ".", "add", "(", "Dense", "(", "200", ",", "input_dim", "=", "input_size", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "Dense", "(", "200", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "Dense", "(", "output_size", ",", "activation", "=", "'linear'", ")", ")", "\n", "\n", "model", ".", "compile", "(", "loss", "=", "loss", ",", "\n", "optimizer", "=", "Adam", "(", "lr", "=", "self", ".", "learning_rate", ")", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent._build_model": [[83, 113], ["keras.models.Sequential", "keras.models.Sequential.compile", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.models.Sequential.add", "keras.layers.Conv2D", "keras.layers.MaxPool2D", "keras.layers.Conv2D", "keras.layers.MaxPool2D", "keras.layers.Conv2D", "keras.layers.MaxPool2D", "keras.layers.Flatten", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.layers.Dense", "keras.optimizers.Adam"], "methods", ["None"], ["", "def", "_build_model", "(", "self", ",", "loss", "=", "None", ",", "input_size", "=", "None", ",", "output_size", "=", "None", ")", ":", "\n", "        ", "loss", "=", "self", ".", "_huber_loss", "if", "loss", "is", "'huber'", "else", "loss", "\n", "input_size", "=", "self", ".", "state_size", "if", "input_size", "is", "None", "else", "input_size", "\n", "output_size", "=", "self", ".", "action_size", "if", "output_size", "is", "None", "else", "output_size", "\n", "\n", "# Neural Net for Deep-Q learning Model", "\n", "model", "=", "Sequential", "(", ")", "\n", "if", "self", ".", "is_atari_env", ":", "\n", "            ", "model", ".", "add", "(", "Conv2D", "(", "32", ",", "8", ",", "strides", "=", "(", "4", ",", "4", ")", ",", "input_shape", "=", "input_size", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "MaxPool2D", "(", ")", ")", "\n", "model", ".", "add", "(", "Conv2D", "(", "64", ",", "4", ",", "strides", "=", "(", "2", ",", "2", ")", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "MaxPool2D", "(", ")", ")", "\n", "model", ".", "add", "(", "Conv2D", "(", "64", ",", "3", ",", "strides", "=", "(", "1", ",", "1", ")", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "MaxPool2D", "(", ")", ")", "\n", "model", ".", "add", "(", "Flatten", "(", ")", ")", "\n", "model", ".", "add", "(", "Dense", "(", "64", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "Dense", "(", "64", ",", "activation", "=", "'relu'", ")", ")", "\n", "model", ".", "add", "(", "Dense", "(", "output_size", ",", "activation", "=", "'linear'", ")", ")", "\n", "", "else", ":", "\n", "# model.add(Dense(24, input_dim=input_size, activation='relu'))", "\n", "# model.add(Dense(24, activation='relu'))", "\n", "# model.add(Dense(output_size, activation='linear'))", "\n", "\n", "            ", "model", ".", "add", "(", "Dense", "(", "200", ",", "input_dim", "=", "input_size", ",", "activation", "=", "'tanh'", ",", "kernel_initializer", "=", "'RandomNormal'", ")", ")", "\n", "# model.add(Dense(200, activation='tanh'))", "\n", "model", ".", "add", "(", "Dense", "(", "output_size", ",", "activation", "=", "'linear'", ",", "kernel_initializer", "=", "'RandomNormal'", ")", ")", "\n", "\n", "", "model", ".", "compile", "(", "loss", "=", "loss", ",", "\n", "optimizer", "=", "Adam", "(", "lr", "=", "self", ".", "learning_rate", ")", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.memorize": [[114, 131], ["dqn_agents.DQNAgent.sample_buffer.append", "dqn_agents.DQNAgent.memory.append", "dqn_agents.DQNAgent.sample_buffer.popleft", "list", "dqn_agents.DQNAgent.m_step_reward", "tuple", "dqn_agents.DQNAgent.memory.append", "len", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.m_step_reward"], ["", "def", "memorize", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", ":", "\n", "        ", "if", "self", ".", "is_delayed_agent", ":", "\n", "# for earlier time than delay_value, the data is problematic (non-delayed response)", "\n", "# Construct modified tuple by keeping old s_t with new a_{t+m}, r_{t+m} s_{t+m+1}", "\n", "            ", "new_tuple", "=", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", "\n", "self", ".", "sample_buffer", ".", "append", "(", "new_tuple", ")", "\n", "if", "len", "(", "self", ".", "sample_buffer", ")", "-", "1", ">=", "self", ".", "delay_value", ":", "\n", "                ", "old_tuple", "=", "self", ".", "sample_buffer", ".", "popleft", "(", ")", "\n", "modified_tuple", "=", "list", "(", "deepcopy", "(", "old_tuple", ")", ")", "\n", "modified_tuple", "[", "1", "]", "=", "action", "\n", "modified_tuple", "[", "2", "]", "=", "self", ".", "m_step_reward", "(", "first_reward", "=", "old_tuple", "[", "2", "]", ")", "\n", "# trying to use s_{t+1} instead of s_{t+m} as in the original ICML2020 submission", "\n", "# modified_tuple[3] = next_state", "\n", "modified_tuple", "=", "tuple", "(", "modified_tuple", ")", "\n", "self", ".", "memory", ".", "append", "(", "modified_tuple", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "memory", ".", "append", "(", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.act": [[132, 137], ["dqn_agents.DQNAgent.model.predict", "numpy.argmax", "random.randrange", "numpy.random.rand"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict"], ["", "", "def", "act", "(", "self", ",", "state", ",", "eval", "=", "False", ")", ":", "\n", "        ", "if", "not", "eval", "and", "np", ".", "random", ".", "rand", "(", ")", "<=", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "action_size", ")", "\n", "", "act_values", "=", "self", ".", "model", ".", "predict", "(", "state", ")", "\n", "return", "np", ".", "argmax", "(", "act_values", "[", "0", "]", ")", "# returns action", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.m_step_reward": [[138, 149], ["range"], "methods", ["None"], ["", "def", "m_step_reward", "(", "self", ",", "first_reward", ")", ":", "\n", "        ", "if", "not", "self", ".", "use_m_step_reward", ":", "\n", "            ", "if", "self", ".", "use_latest_reward", ":", "\n", "                ", "return", "self", ".", "sample_buffer", "[", "-", "1", "]", "[", "2", "]", "\n", "", "else", ":", "\n", "                ", "return", "first_reward", "\n", "", "", "else", ":", "\n", "            ", "discounted_rew", "=", "first_reward", "\n", "for", "i", "in", "range", "(", "self", ".", "delay_value", ")", ":", "\n", "                ", "discounted_rew", "+=", "self", ".", "gamma", "**", "(", "i", "+", "1", ")", "*", "self", ".", "sample_buffer", "[", "i", "]", "[", "2", "]", "\n", "", "return", "discounted_rew", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.effective_gamma": [[150, 152], ["None"], "methods", ["None"], ["", "", "def", "effective_gamma", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "gamma", "if", "not", "self", ".", "use_m_step_reward", "else", "(", "self", ".", "gamma", "**", "(", "self", ".", "delay_value", "+", "1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.replay": [[153, 166], ["random.sample", "dqn_agents.DQNAgent.model.predict", "dqn_agents.DQNAgent.model.fit", "numpy.exp", "dqn_agents.DQNAgent.effective_gamma", "numpy.amax", "dqn_agents.DQNAgent.model.predict"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.effective_gamma", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict"], ["", "def", "replay", "(", "self", ",", "batch_size", ",", "global_step", ")", ":", "\n", "        ", "minibatch", "=", "random", ".", "sample", "(", "self", ".", "memory", ",", "batch_size", ")", "\n", "for", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", "in", "minibatch", ":", "\n", "            ", "target", "=", "reward", "\n", "if", "not", "done", ":", "\n", "                ", "target", "=", "(", "reward", "+", "self", ".", "effective_gamma", "(", ")", "*", "\n", "np", ".", "amax", "(", "self", ".", "model", ".", "predict", "(", "next_state", ")", "[", "0", "]", ")", ")", "\n", "", "target_f", "=", "self", ".", "model", ".", "predict", "(", "state", ")", "\n", "target_f", "[", "0", "]", "[", "action", "]", "=", "target", "\n", "# self.model.fit(state, target_f, epochs=1, verbose=0,", "\n", "#                callbacks=[WandbCallback()])", "\n", "self", ".", "model", ".", "fit", "(", "state", ",", "target_f", ",", "epochs", "=", "1", ",", "verbose", "=", "0", ")", "\n", "", "self", ".", "epsilon", "=", "self", ".", "epsilon_min", "+", "(", "1.0", "-", "self", ".", "epsilon_min", ")", "*", "np", ".", "exp", "(", "-", "self", ".", "epsilon_decay", "*", "global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load": [[167, 169], ["dqn_agents.DQNAgent.model.load_weights"], "methods", ["None"], ["", "def", "load", "(", "self", ",", "name", ")", ":", "\n", "        ", "self", ".", "model", ".", "load_weights", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.save": [[170, 172], ["dqn_agents.DQNAgent.model.save_weights"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "name", ")", ":", "\n", "        ", "self", ".", "model", ".", "save_weights", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.clear_action_buffer": [[173, 175], ["dqn_agents.DQNAgent.sample_buffer.clear"], "methods", ["None"], ["", "def", "clear_action_buffer", "(", "self", ")", ":", "\n", "        ", "self", ".", "sample_buffer", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNAgent.__init__": [[178, 187], ["dqn_agents.DQNAgent.__init__", "dqn_agents.DDQNAgent._build_model", "dqn_agents.DDQNAgent.update_target_model"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.__init__", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent._build_model", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNAgent.update_target_model"], ["    ", "def", "__init__", "(", "self", ",", "seed", ",", "state_size", ",", "action_size", ",", "is_atari_env", ",", "is_delayed_agent", "=", "False", ",", "delay_value", "=", "0", ",", "epsilon_min", "=", "0.001", ",", "\n", "epsilon_decay", "=", "0.999", ",", "learning_rate", "=", "0.001", ",", "epsilon", "=", "1.0", ",", "use_m_step_reward", "=", "False", ",", "use_latest_reward", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "seed", ",", "state_size", ",", "action_size", ",", "is_atari_env", "=", "is_atari_env", ",", "is_delayed_agent", "=", "is_delayed_agent", ",", "delay_value", "=", "delay_value", ",", "\n", "epsilon_min", "=", "epsilon_min", ",", "epsilon_decay", "=", "epsilon_decay", ",", "learning_rate", "=", "learning_rate", ",", "\n", "epsilon", "=", "epsilon", ",", "use_m_step_reward", "=", "use_m_step_reward", ",", "use_latest_reward", "=", "use_latest_reward", ",", "\n", "loss", "=", "'huber'", ")", "\n", "# self.model = self._build_model()", "\n", "self", ".", "target_model", "=", "self", ".", "_build_model", "(", "loss", "=", "'mse'", ")", "\n", "self", ".", "update_target_model", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNAgent.update_target_model": [[188, 191], ["dqn_agents.DDQNAgent.target_model.set_weights", "dqn_agents.DDQNAgent.model.get_weights"], "methods", ["None"], ["", "def", "update_target_model", "(", "self", ")", ":", "\n", "# copy weights from model to target_model", "\n", "        ", "self", ".", "target_model", ".", "set_weights", "(", "self", ".", "model", ".", "get_weights", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNAgent.train_model": [[192, 204], ["dqn_agents.DDQNAgent.model.predict", "dqn_agents.DDQNAgent.target_model.predict", "numpy.invert", "range", "dqn_agents.DDQNAgent.model.fit", "numpy.asarray", "len", "numpy.amax", "dqn_agents.DDQNAgent.effective_gamma"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.effective_gamma"], ["", "def", "train_model", "(", "self", ",", "batch", ")", ":", "\n", "        ", "state_vec", ",", "action_vec", ",", "reward_vec", ",", "next_state_vec", ",", "done_vec", "=", "batch", "\n", "target", "=", "self", ".", "model", ".", "predict", "(", "state_vec", ")", "\n", "t", "=", "self", ".", "target_model", ".", "predict", "(", "next_state_vec", ")", "\n", "not_done_arr", "=", "np", ".", "invert", "(", "np", ".", "asarray", "(", "done_vec", ")", ")", "\n", "new_targets", "=", "reward_vec", "+", "not_done_arr", "*", "self", ".", "effective_gamma", "(", ")", "*", "np", ".", "amax", "(", "t", ",", "axis", "=", "1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "batch", "[", "0", "]", ")", ")", ":", "\n", "            ", "target", "[", "i", "]", "[", "action_vec", "[", "i", "]", "]", "=", "new_targets", "[", "i", "]", "\n", "", "train_history", "=", "self", ".", "model", ".", "fit", "(", "state_vec", ",", "target", ",", "epochs", "=", "1", ",", "verbose", "=", "0", ")", "\n", "q_loss", "=", "train_history", ".", "history", "[", "'loss'", "]", "[", "0", "]", "\n", "loss_dict", "=", "{", "'q_loss'", ":", "q_loss", "}", "\n", "return", "loss_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNAgent._create_batch": [[205, 216], ["state_vec.append", "action_vec.append", "reward_vec.append", "next_state_vec.append", "done_vec.append", "numpy.concatenate", "numpy.concatenate", "numpy.array", "numpy.array"], "methods", ["None"], ["", "def", "_create_batch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "state_vec", ",", "action_vec", ",", "reward_vec", ",", "next_state_vec", ",", "done_vec", "=", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", ",", "[", "]", "\n", "for", "i", "in", "indices", ":", "\n", "            ", "data", "=", "self", ".", "memory", "[", "i", "]", "\n", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", "=", "data", "\n", "state_vec", ".", "append", "(", "np", ".", "array", "(", "state", ",", "copy", "=", "False", ")", ")", "\n", "action_vec", ".", "append", "(", "action", ")", "\n", "reward_vec", ".", "append", "(", "reward", ")", "\n", "next_state_vec", ".", "append", "(", "np", ".", "array", "(", "next_state", ",", "copy", "=", "False", ")", ")", "\n", "done_vec", ".", "append", "(", "done", ")", "\n", "", "return", "np", ".", "concatenate", "(", "state_vec", ",", "axis", "=", "0", ")", ",", "action_vec", ",", "reward_vec", ",", "np", ".", "concatenate", "(", "next_state_vec", ",", "axis", "=", "0", ")", ",", "done_vec", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNAgent.replay": [[217, 225], ["numpy.random.choice", "dqn_agents.DDQNAgent._create_batch", "dqn_agents.DDQNAgent.train_model", "dqn_agents.update_loss", "len", "numpy.exp"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNAgent._create_batch", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNPlanningAgent.train_model", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.update_loss"], ["", "def", "replay", "(", "self", ",", "batch_size", ",", "global_step", ")", ":", "\n", "        ", "loss", "=", "{", "}", "\n", "indices", "=", "np", ".", "random", ".", "choice", "(", "len", "(", "self", ".", "memory", ")", ",", "batch_size", ")", "\n", "batch", "=", "self", ".", "_create_batch", "(", "indices", ")", "\n", "sample_loss", "=", "self", ".", "train_model", "(", "batch", ")", "\n", "update_loss", "(", "loss", ",", "sample_loss", ")", "\n", "self", ".", "epsilon", "=", "self", ".", "epsilon_min", "+", "(", "1.0", "-", "self", ".", "epsilon_min", ")", "*", "np", ".", "exp", "(", "-", "self", ".", "epsilon_decay", "*", "global_step", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNPlanningAgent.__init__": [[227, 239], ["dqn_agents.DDQNAgent.__init__", "dqn_agents.DDQNPlanningAgent._build_forward_model", "dqn_agents.ForwardModel"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.__init__", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent._build_forward_model"], ["    ", "def", "__init__", "(", "self", ",", "seed", ",", "state_size", ",", "action_size", ",", "is_atari_env", ",", "is_delayed_agent", "=", "False", ",", "delay_value", "=", "0", ",", "epsilon_min", "=", "0.001", ",", "\n", "epsilon_decay", "=", "0.999", ",", "learning_rate", "=", "0.001", ",", "epsilon", "=", "1.0", ",", "use_m_step_reward", "=", "False", ",", "\n", "use_latest_reward", "=", "True", ",", "env", "=", "None", ",", "use_learned_forward_model", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "seed", ",", "state_size", ",", "action_size", ",", "is_atari_env", "=", "is_atari_env", ",", "is_delayed_agent", "=", "is_delayed_agent", ",", "delay_value", "=", "delay_value", ",", "\n", "epsilon_min", "=", "epsilon_min", ",", "epsilon_decay", "=", "epsilon_decay", ",", "learning_rate", "=", "learning_rate", ",", "\n", "epsilon", "=", "epsilon", ",", "use_m_step_reward", "=", "use_m_step_reward", ",", "use_latest_reward", "=", "use_latest_reward", ")", "\n", "self", ".", "use_learned_forward_model", "=", "use_learned_forward_model", "\n", "if", "self", ".", "use_learned_forward_model", ":", "\n", "            ", "keras_forward_model", "=", "self", ".", "_build_forward_model", "(", "loss", "=", "'mse'", ",", "input_size", "=", "self", ".", "state_size", "+", "1", ",", "output_size", "=", "self", ".", "state_size", ")", "\n", "self", ".", "forward_model", "=", "ForwardModel", "(", "keras_forward_model", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "forward_model", "=", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNPlanningAgent.train_model": [[240, 250], ["dqn_agents.DDQNAgent.train_model", "numpy.asarray().transpose", "numpy.concatenate", "dqn_agents.DDQNPlanningAgent.forward_model.keras_model.fit", "numpy.asarray"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNPlanningAgent.train_model"], ["", "", "def", "train_model", "(", "self", ",", "batch", ")", ":", "\n", "        ", "loss_dict", "=", "super", "(", ")", ".", "train_model", "(", "batch", ")", "\n", "if", "self", ".", "use_learned_forward_model", "and", "self", ".", "delay_value", ">", "0", ":", "\n", "            ", "state_vec", ",", "action_vec", ",", "_", ",", "next_state_vec", ",", "_", "=", "batch", "\n", "act_t", "=", "np", ".", "asarray", "(", "[", "action_vec", "]", ")", ".", "transpose", "(", ")", "\n", "concat_vec", "=", "np", ".", "concatenate", "(", "(", "state_vec", ",", "act_t", ")", ",", "axis", "=", "1", ")", "\n", "train_history", "=", "self", ".", "forward_model", ".", "keras_model", ".", "fit", "(", "concat_vec", ",", "next_state_vec", ",", "epochs", "=", "1", ",", "verbose", "=", "0", ")", "\n", "f_model_loss", "=", "train_history", ".", "history", "[", "'loss'", "]", "[", "0", "]", "\n", "loss_dict", "[", "'f_model_loss'", "]", "=", "f_model_loss", "\n", "", "return", "loss_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNPlanningAgent.act": [[251, 266], ["dqn_agents.reshape_state", "dqn_agents.DDQNPlanningAgent.model.predict", "numpy.argmax", "random.randrange", "numpy.random.rand", "dqn_agents.DDQNPlanningAgent.forward_model.store_initial_state", "dqn_agents.DDQNPlanningAgent.forward_model.get_next_state", "dqn_agents.DDQNPlanningAgent.forward_model.restore_initial_state"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.reshape_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.store_initial_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_next_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.restore_initial_state"], ["", "def", "act", "(", "self", ",", "state", ",", "pending_actions", ",", "eval", ")", ":", "\n", "        ", "if", "not", "eval", "and", "np", ".", "random", ".", "rand", "(", ")", "<=", "self", ".", "epsilon", ":", "\n", "            ", "return", "random", ".", "randrange", "(", "self", ".", "action_size", ")", "\n", "", "last_state", "=", "state", "\n", "if", "self", ".", "delay_value", ">", "0", ":", "\n", "            ", "if", "not", "self", ".", "use_learned_forward_model", ":", "\n", "                ", "self", ".", "forward_model", ".", "store_initial_state", "(", ")", "\n", "# initial_state = deepcopy(state)", "\n", "", "for", "curr_action", "in", "pending_actions", ":", "\n", "                ", "last_state", "=", "self", ".", "forward_model", ".", "get_next_state", "(", "state", "=", "last_state", ",", "action", "=", "curr_action", ")", "\n", "", "if", "not", "self", ".", "use_learned_forward_model", ":", "\n", "                ", "self", ".", "forward_model", ".", "restore_initial_state", "(", ")", "\n", "", "", "last_state_r", "=", "reshape_state", "(", "last_state", ",", "self", ".", "is_atari_env", ",", "self", ".", "state_size", ")", "\n", "act_values", "=", "self", ".", "model", ".", "predict", "(", "last_state_r", ")", "\n", "return", "np", ".", "argmax", "(", "act_values", "[", "0", "]", ")", "# returns best action for last state", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DDQNPlanningAgent.memorize": [[267, 282], ["dqn_agents.DDQNPlanningAgent.sample_buffer.append", "dqn_agents.DDQNPlanningAgent.sample_buffer.popleft", "list", "tuple", "dqn_agents.DDQNPlanningAgent.memory.append", "len", "copy.deepcopy"], "methods", ["None"], ["", "def", "memorize", "(", "self", ",", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", ":", "\n", "# for earlier time than delay_value, the data is problematic (non-delayed response)", "\n", "# Construct modified tuple by keeping old s_t with new a_{t+m}, r_{t+m} s_{t+m+1}", "\n", "        ", "new_tuple", "=", "(", "state", ",", "action", ",", "reward", ",", "next_state", ",", "done", ")", "\n", "self", ".", "sample_buffer", ".", "append", "(", "new_tuple", ")", "\n", "if", "len", "(", "self", ".", "sample_buffer", ")", "-", "1", ">=", "self", ".", "delay_value", ":", "\n", "            ", "old_tuple", "=", "self", ".", "sample_buffer", ".", "popleft", "(", ")", "\n", "modified_tuple", "=", "list", "(", "deepcopy", "(", "old_tuple", ")", ")", "\n", "# build time-coherent tuple from new tuple and old action", "\n", "modified_tuple", "[", "0", "]", "=", "state", "\n", "# modified_tuple[1] = action", "\n", "modified_tuple", "[", "2", "]", "=", "reward", "# self.m_step_reward(first_reward=old_tuple[2])", "\n", "modified_tuple", "[", "3", "]", "=", "next_state", "\n", "modified_tuple", "=", "tuple", "(", "modified_tuple", ")", "\n", "self", ".", "memory", ".", "append", "(", "modified_tuple", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.ForwardModel.__init__": [[284, 286], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "keras_model", ")", ":", "\n", "        ", "self", ".", "keras_model", "=", "keras_model", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.ForwardModel.get_next_state": [[287, 290], ["dqn_agents.concatenate_state_action", "dqn_agents.ForwardModel.keras_model.predict"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.concatenate_state_action", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict"], ["", "def", "get_next_state", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "input", "=", "concatenate_state_action", "(", "state", ",", "action", ")", "\n", "return", "self", ".", "keras_model", ".", "predict", "(", "input", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.ForwardModel.reset_to_state": [[291, 294], ["None"], "methods", ["None"], ["", "def", "reset_to_state", "(", "self", ",", "state", ")", ":", "\n", "# not necessary here. Only used if the forward_model is the actual env instance", "\n", "        ", "pass", "", "", "", ""]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.reshape_state": [[11, 19], ["numpy.reshape", "len", "numpy.expand_dims"], "function", ["None"], ["def", "reshape_state", "(", "state", ",", "is_atari_env", ",", "state_size", ")", ":", "\n", "    ", "reshaped", "=", "state", "\n", "if", "not", "is_atari_env", ":", "\n", "        ", "reshaped", "=", "np", ".", "reshape", "(", "state", ",", "[", "1", ",", "state_size", "]", ")", "\n", "", "else", ":", "\n", "        ", "if", "len", "(", "state", ".", "shape", ")", "<", "4", ":", "\n", "            ", "reshaped", "=", "np", ".", "expand_dims", "(", "state", ",", "axis", "=", "0", ")", "\n", "", "", "return", "reshaped", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.update_loss": [[20, 27], ["sample_loss.items"], "function", ["None"], ["", "def", "update_loss", "(", "loss", ",", "sample_loss", ")", ":", "\n", "    ", "if", "loss", "is", "not", "None", "and", "sample_loss", "is", "not", "None", ":", "\n", "        ", "for", "key", ",", "val", "in", "sample_loss", ".", "items", "(", ")", ":", "\n", "            ", "if", "key", "in", "loss", ":", "\n", "                ", "loss", "[", "key", "]", "+=", "val", "\n", "", "else", ":", "\n", "                ", "loss", "[", "key", "]", "=", "val", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.concatenate_state_action": [[28, 32], ["numpy.concatenate", "numpy.reshape", "len"], "function", ["None"], ["", "", "", "", "def", "concatenate_state_action", "(", "state", ",", "action", ")", ":", "\n", "    ", "out", "=", "np", ".", "concatenate", "(", "(", "state", "[", "0", "]", ",", "[", "action", "]", ")", ")", "\n", "out", "=", "np", ".", "reshape", "(", "out", ",", "[", "1", ",", "len", "(", "out", ")", "]", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.__init__": [[9, 24], ["str", "collections.deque", "dqn_agents.DQNAgent"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "orig_env", ",", "delay_value", ")", ":", "\n", "        ", "self", ".", "orig_env", "=", "orig_env", "\n", "self", ".", "env_name", "=", "str", "(", "self", ".", "orig_env", ")", "\n", "self", ".", "is_atari_env", "=", "'AtariEnv'", "in", "self", ".", "env_name", "\n", "self", ".", "pending_actions", "=", "deque", "(", ")", "\n", "self", ".", "delay_value", "=", "delay_value", "\n", "self", ".", "state_size", "=", "orig_env", ".", "observation_space", ".", "shape", "\n", "if", "not", "self", ".", "is_atari_env", ":", "\n", "            ", "self", ".", "state_size", "=", "self", ".", "state_size", "[", "0", "]", "\n", "", "self", ".", "action_size", "=", "orig_env", ".", "action_space", ".", "n", "\n", "self", ".", "stored_init_state", "=", "None", "\n", "self", ".", "trained_non_delayed_agent", "=", "DQNAgent", "(", "seed", "=", "0", ",", "state_size", "=", "self", ".", "state_size", ",", "\n", "action_size", "=", "self", ".", "action_size", ",", "is_delayed_agent", "=", "False", ",", "\n", "delay_value", "=", "0", ",", "epsilon", "=", "0", ",", "is_atari_env", "=", "self", ".", "is_atari_env", ")", "\n", "self", ".", "pretrained_agent_loaded", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.step": [[26, 37], ["delayed_env.DelayedEnv.orig_env.step", "delayed_env.DelayedEnv.pending_actions.append", "delayed_env.DelayedEnv.pending_actions.popleft", "dqn_agents.reshape_state", "delayed_env.DelayedEnv.trained_non_delayed_agent.act", "len", "delayed_env.DelayedEnv.get_curr_state"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.reshape_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_curr_state"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "self", ".", "delay_value", ">", "0", ":", "\n", "            ", "self", ".", "pending_actions", ".", "append", "(", "action", ")", "\n", "if", "len", "(", "self", ".", "pending_actions", ")", "-", "1", ">=", "self", ".", "delay_value", ":", "\n", "                ", "executed_action", "=", "self", ".", "pending_actions", ".", "popleft", "(", ")", "\n", "", "else", ":", "\n", "                ", "curr_state", "=", "reshape_state", "(", "self", ".", "get_curr_state", "(", ")", ",", "self", ".", "is_atari_env", ",", "self", ".", "state_size", ")", "\n", "executed_action", "=", "self", ".", "trained_non_delayed_agent", ".", "act", "(", "curr_state", ")", "\n", "", "", "else", ":", "\n", "            ", "executed_action", "=", "action", "\n", "", "return", "self", ".", "orig_env", ".", "step", "(", "executed_action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.reset": [[38, 41], ["delayed_env.DelayedEnv.pending_actions.clear", "delayed_env.DelayedEnv.orig_env.reset"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "pending_actions", ".", "clear", "(", ")", "\n", "return", "self", ".", "orig_env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_shaped_reward": [[42, 60], ["abs", "abs"], "methods", ["None"], ["", "def", "get_shaped_reward", "(", "self", ",", "state", ",", "orig_reward", ")", ":", "\n", "        ", "reward", "=", "orig_reward", "\n", "if", "'CartPole'", "in", "self", ".", "env_name", ":", "\n", "            ", "x", ",", "x_dot", ",", "theta", ",", "theta_dot", "=", "state", "\n", "r1", "=", "(", "self", ".", "orig_env", ".", "x_threshold", "-", "abs", "(", "x", ")", ")", "/", "self", ".", "orig_env", ".", "x_threshold", "-", "0.8", "\n", "r2", "=", "(", "self", ".", "orig_env", ".", "theta_threshold_radians", "-", "abs", "(", "\n", "theta", ")", ")", "/", "self", ".", "orig_env", ".", "theta_threshold_radians", "-", "0.5", "\n", "reward", "=", "r1", "+", "r2", "\n", "", "if", "'MountainCar'", "in", "self", ".", "env_name", ":", "\n", "            ", "position", "=", "state", "[", "0", "]", "\n", "reward", "=", "(", "position", "-", "self", ".", "orig_env", ".", "goal_position", ")", "/", "(", "(", "self", ".", "orig_env", ".", "max_position", "-", "self", ".", "orig_env", ".", "min_position", ")", "*", "10", ")", "\n", "if", "position", ">=", "0.1", ":", "\n", "                ", "reward", "+=", "10", "\n", "", "elif", "position", ">=", "0.25", ":", "\n", "                ", "reward", "+=", "50", "\n", "", "elif", "position", ">=", "0.5", ":", "\n", "                ", "reward", "+=", "100", "\n", "", "", "return", "reward", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_pending_actions": [[61, 74], ["delayed_env.DelayedEnv.store_initial_state", "delayed_env.DelayedEnv.get_curr_state", "range", "delayed_env.DelayedEnv.restore_initial_state", "len", "dqn_agents.reshape_state", "delayed_env.DelayedEnv.trained_non_delayed_agent.act", "delayed_env.DelayedEnv.pending_actions.append", "delayed_env.DelayedEnv.get_next_state"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.store_initial_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_curr_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.restore_initial_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.reshape_state", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_next_state"], ["", "def", "get_pending_actions", "(", "self", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "pending_actions", ")", "==", "0", "and", "self", ".", "delay_value", ">", "0", ":", "\n", "# reconstruct anticipated trajectory using the oracle", "\n", "            ", "self", ".", "store_initial_state", "(", ")", "\n", "curr_state", "=", "self", ".", "get_curr_state", "(", ")", "\n", "for", "i", "in", "range", "(", "self", ".", "delay_value", ")", ":", "\n", "                ", "curr_state", "=", "reshape_state", "(", "curr_state", ",", "self", ".", "is_atari_env", ",", "self", ".", "state_size", ")", "\n", "estimated_action", "=", "self", ".", "trained_non_delayed_agent", ".", "act", "(", "curr_state", ")", "\n", "self", ".", "pending_actions", ".", "append", "(", "estimated_action", ")", "\n", "curr_state", "=", "self", ".", "get_next_state", "(", "state", "=", "None", ",", "action", "=", "estimated_action", ")", "\n", "", "self", ".", "restore_initial_state", "(", ")", "\n", "\n", "", "return", "self", ".", "pending_actions", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.store_initial_state": [[75, 80], ["delayed_env.DelayedEnv.orig_env.clone_state"], "methods", ["None"], ["", "def", "store_initial_state", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "is_atari_env", ":", "\n", "            ", "self", ".", "stored_init_state", "=", "self", ".", "orig_env", ".", "clone_state", "(", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "stored_init_state", "=", "self", ".", "orig_env", ".", "unwrapped", ".", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.restore_initial_state": [[81, 86], ["delayed_env.DelayedEnv.orig_env.restore_state"], "methods", ["None"], ["", "", "def", "restore_initial_state", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "is_atari_env", ":", "\n", "            ", "self", ".", "orig_env", ".", "restore_state", "(", "self", ".", "stored_init_state", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "orig_env", ".", "unwrapped", ".", "state", "=", "self", ".", "stored_init_state", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_curr_state": [[87, 96], ["delayed_env.DelayedEnv.orig_env.ale.getScreenRGB2", "numpy.array", "numpy.cos", "numpy.sin", "numpy.cos", "numpy.sin"], "methods", ["None"], ["", "", "def", "get_curr_state", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "is_atari_env", ":", "\n", "            ", "curr_state", "=", "self", ".", "orig_env", ".", "ale", ".", "getScreenRGB2", "(", ")", "\n", "", "else", ":", "\n", "            ", "curr_state", "=", "self", ".", "orig_env", ".", "unwrapped", ".", "state", "\n", "", "if", "'Acrobot'", "in", "self", ".", "env_name", ":", "\n", "            ", "curr_state", "=", "np", ".", "array", "(", "[", "cos", "(", "curr_state", "[", "0", "]", ")", ",", "sin", "(", "curr_state", "[", "0", "]", ")", ",", "cos", "(", "curr_state", "[", "1", "]", ")", ",", "sin", "(", "curr_state", "[", "1", "]", ")", ",", "\n", "curr_state", "[", "2", "]", ",", "curr_state", "[", "3", "]", "]", ")", "\n", "", "return", "curr_state", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.get_next_state": [[97, 101], ["delayed_env.DelayedEnv.orig_env.step"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step"], ["", "def", "get_next_state", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "next_state", ",", "_", ",", "_", ",", "_", "=", "self", ".", "orig_env", ".", "step", "(", "action", ")", "\n", "self", ".", "orig_env", ".", "_elapsed_steps", "-=", "1", "\n", "return", "next_state", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).delayed_env.DelayedEnv.reset_to_state": [[102, 104], ["None"], "methods", ["None"], ["", "def", "reset_to_state", "(", "self", ",", "state", ")", ":", "\n", "        ", "self", ".", "orig_env", ".", "unwrapped", ".", "state", "=", "state", "\n", "#", ""]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.agent.Agent.__init__": [[7, 13], ["collections.deque", "numpy.zeros"], "methods", ["None"], ["\n", "GPUs", "=", "tf", ".", "config", ".", "experimental", ".", "list_physical_devices", "(", "'GPU'", ")", "\n", "\n", "if", "GPUs", ":", "\n", "    ", "try", ":", "\n", "        ", "for", "gpu", "in", "GPUs", ":", "\n", "            ", "tf", ".", "config", ".", "experimental", ".", "set_memory_growth", "(", "gpu", ",", "True", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.agent.Agent.randargmax": [[15, 19], ["numpy.argmax", "numpy.random.random", "b.max"], "methods", ["None"], ["        ", "print", "(", "e", ")", "\n", "\n", "\n", "", "", "def", "to_onehot", "(", "size", ",", "value", ")", ":", "\n", "    ", "\"\"\"1 hot encoding for observed state\"\"\"", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.agent.Agent.update_epsilon": [[20, 22], ["None"], "methods", ["None"], ["return", "np", ".", "eye", "(", "size", ")", "[", "value", "]", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.agent.Agent.fill_up_buffer": [[25, 29], ["range", "agent.Agent.act", "agent.Agent.actions_in_buffer.append"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act"], ["def", "__init__", "(", "self", ",", "num_states", ",", "hidden_units", ",", "num_actions", ",", "alg", ",", "use_stochastic_delay", ",", "max_dimension", ")", ":", "\n", "        ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "alg", "==", "'IS'", ":", "\n", "            ", "if", "use_stochastic_delay", ":", "\n", "                ", "self", ".", "input_layer", "=", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "(", "num_states", "+", "1", "+", "max_dimension", ",", ")", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.agent.Agent.choose_action": [[30, 37], ["agent.Agent.actions_in_buffer.popleft", "agent.Agent.act", "agent.Agent.actions_in_buffer.append", "agent.Agent.act", "agent.Agent.act"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act"], ["", "else", ":", "\n", "                ", "self", ".", "input_layer", "=", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "(", "num_states", "+", "max_dimension", ",", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "input_layer", "=", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "(", "num_states", ",", ")", ")", "\n", "", "self", ".", "hidden_layers", "=", "[", "]", "\n", "for", "i", "in", "hidden_units", ":", "\n", "            ", "self", ".", "hidden_layers", ".", "append", "(", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "i", ",", "activation", "=", "'tanh'", ",", "kernel_initializer", "=", "'RandomNormal'", ")", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.agent.Agent.act": [[38, 44], ["numpy.random.random", "agent.Agent.randargmax", "numpy.random.randint"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.randargmax"], ["", "self", ".", "output_layer", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "num_actions", ",", "activation", "=", "'linear'", ",", "kernel_initializer", "=", "'RandomNormal'", ")", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "call", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "z", "=", "self", ".", "input_layer", "(", "inputs", ")", "\n", "for", "layer", "in", "self", ".", "hidden_layers", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.__init__": [[6, 15], ["collections.deque", "collections.deque", "numpy.zeros", "range"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "state_space", ",", "num_actions", ",", "delay", ")", ":", "\n", "        ", "self", ".", "epsilon", "=", "1.0", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "self", ".", "delay", "=", "delay", "\n", "self", ".", "actions_in_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "delay", ")", "\n", "self", ".", "actions_in_buffer_prev", "=", "deque", "(", "maxlen", "=", "self", ".", "delay", ")", "\n", "tabular_value_shape", "=", "[", "state_space", ".", "shape", "[", "0", "]", "]", "+", "[", "state_space", ".", "shape", "[", "1", "]", "]", "+", "[", "num_actions", "for", "_", "in", "range", "(", "self", ".", "delay", "+", "1", ")", "]", "\n", "self", ".", "Q_values", "=", "np", ".", "zeros", "(", "tabular_value_shape", ")", "\n", "# self.E = np.zeros(tabular_value_shape)", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.randargmax": [[17, 21], ["numpy.argmax", "numpy.random.random", "b.max"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "randargmax", "(", "b", ",", "**", "kw", ")", ":", "\n", "        ", "\"\"\" a random tie-breaking argmax\"\"\"", "\n", "return", "np", ".", "argmax", "(", "np", ".", "random", ".", "random", "(", "b", ".", "shape", ")", "*", "(", "b", "==", "b", ".", "max", "(", ")", ")", ",", "**", "kw", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.update_epsilon": [[22, 24], ["None"], "methods", ["None"], ["", "def", "update_epsilon", "(", "self", ",", "epsilon", ")", ":", "\n", "        ", "self", ".", "epsilon", "=", "epsilon", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.fill_up_buffer": [[27, 31], ["range", "dr_agent.Agent.act", "dr_agent.Agent.actions_in_buffer.append"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act"], ["def", "fill_up_buffer", "(", "self", ",", "state", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "self", ".", "delay", ")", ":", "\n", "            ", "action", "=", "self", ".", "act", "(", "state", ")", "\n", "self", ".", "actions_in_buffer", ".", "append", "(", "action", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.choose_action": [[32, 40], ["dr_agent.Agent.act", "numpy.copy", "dr_agent.Agent.actions_in_buffer.popleft", "dr_agent.Agent.actions_in_buffer.append", "dr_agent.Agent.act", "dr_agent.Agent.act"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act"], ["", "", "def", "choose_action", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "self", ".", "delay", "==", "0", ":", "\n", "            ", "return", "self", ".", "act", "(", "state", ")", ",", "self", ".", "act", "(", "state", ")", "# return undelayed action", "\n", "", "next_action", "=", "self", ".", "act", "(", "state", ")", "\n", "self", ".", "actions_in_buffer_prev", "=", "np", ".", "copy", "(", "self", ".", "actions_in_buffer", ")", "\n", "action", "=", "self", ".", "actions_in_buffer", ".", "popleft", "(", ")", "# get delayed action", "\n", "self", ".", "actions_in_buffer", ".", "append", "(", "next_action", ")", "# put undelayed action into the buffer", "\n", "return", "action", ",", "next_action", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.act": [[41, 47], ["numpy.random.random", "dr_agent.Agent.randargmax", "numpy.random.randint", "tuple"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.dr_agent.Agent.randargmax"], ["", "def", "act", "(", "self", ",", "state", ")", ":", "\n", "        ", "if", "self", ".", "epsilon", "<", "np", ".", "random", ".", "random", "(", ")", ":", "# exploration", "\n", "            ", "action", "=", "self", ".", "randargmax", "(", "self", ".", "Q_values", "[", "(", "state", "[", "0", "]", ",", "state", "[", "1", "]", ")", "+", "tuple", "(", "self", ".", "actions_in_buffer", ")", "]", ")", "\n", "", "else", ":", "\n", "            ", "action", "=", "np", ".", "random", ".", "randint", "(", "self", ".", "num_actions", ")", "# greedy", "\n", "", "return", "action", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.env.Environment.__init__": [[7, 28], ["numpy.random.seed", "numpy.empty", "len", "collections.deque", "env.Environment.fill_up_buffer", "env.Environment.reset"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.fill_up_buffer", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset"], ["class", "Environment", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "game_name", ",", "delay", ",", "seed", ")", ":", "\n", "        ", "\"\"\"Initialize Environment\"\"\"", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "env", "=", "gym", ".", "make", "(", "self", ".", "game_name", ")", "\n", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "number_of_actions", "=", "self", ".", "env", ".", "action_space", ".", "n", "\n", "self", ".", "delay", "=", "delay", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "[", "'image'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "\n", "", "self", ".", "actions_in_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "delay", ")", "\n", "self", ".", "fill_up_buffer", "(", ")", "\n", "self", ".", "delayed_action", "=", "0", "\n", "\n", "", "def", "process_state", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Pre-process state if required\"\"\"", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "return", "np", ".", "array", "(", "observation", "[", "'image'", "]", ",", "dtype", "=", "'float32'", ")", "# Using only image as state (7x7x3)", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.env.Environment.reset": [[29, 36], ["numpy.random.randint", "env.Environment.fill_up_buffer"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.fill_up_buffer"], ["            ", "return", "observation", "\n", "\n", "", "", "def", "fill_up_buffer", "(", "self", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "self", ".", "delay", ")", ":", "\n", "            ", "action", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "number_of_actions", ")", "\n", "self", ".", "actions_in_buffer", ".", "append", "(", "action", ")", "\n", "\n", "", "", "def", "reset", "(", "self", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.env.Environment.fill_up_buffer": [[37, 41], ["range", "numpy.random.choice", "env.Environment.actions_in_buffer.append"], "methods", ["None"], ["        ", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "self", ".", "fill_up_buffer", "(", ")", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "return", "self", ".", "process_state", "(", "state", ")", "\n", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Tabular-Q.env.Environment.step": [[42, 93], ["env.Environment.reset", "env.Environment.reset", "env.Environment.reset", "env.Environment.reset"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset"], ["            ", "return", "state", "\n", "\n", "", "", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "if", "self", ".", "delay", "!=", "0", ":", "\n", "            ", "chosen_action", "=", "action", "\n", "self", ".", "delayed_action", "=", "self", ".", "actions_in_buffer", ".", "popleft", "(", ")", "\n", "self", ".", "actions_in_buffer", ".", "append", "(", "chosen_action", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "delayed_action", "=", "action", "\n", "", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "self", ".", "delayed_action", ")", "\n", "return", "self", ".", "process_state", "(", "next_state", ")", ",", "reward", ",", "done", ",", "info", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "env", ".", "step", "(", "self", ".", "delayed_action", ")", "\n", "\n", "", "", "def", "render", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "render", "(", ")", "\n", "\n", "", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.Model.__init__": [[20, 36], ["super().__init__", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.InputLayer", "agent.Model.hidden_layers.append", "tensorflow.keras.layers.InputLayer", "tensorflow.keras.layers.InputLayer", "tensorflow.keras.layers.Dense"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.__init__"], ["return", "np", ".", "eye", "(", "size", ")", "[", "value", "]", "\n", "\n", "\n", "", "class", "Model", "(", "tf", ".", "keras", ".", "Model", ")", ":", "\n", "    ", "\"\"\"DQN Model\"\"\"", "\n", "def", "__init__", "(", "self", ",", "num_states", ",", "hidden_units", ",", "num_actions", ",", "alg", ",", "use_stochastic_delay", ",", "max_dimension", ")", ":", "\n", "        ", "super", "(", "Model", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "alg", "==", "'IS'", ":", "\n", "            ", "if", "use_stochastic_delay", ":", "\n", "                ", "self", ".", "input_layer", "=", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "(", "num_states", "+", "1", "+", "max_dimension", ",", ")", ")", "\n", "", "else", ":", "\n", "                ", "self", ".", "input_layer", "=", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "(", "num_states", "+", "max_dimension", ",", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "self", ".", "input_layer", "=", "tf", ".", "keras", ".", "layers", ".", "InputLayer", "(", "input_shape", "=", "(", "num_states", ",", ")", ")", "\n", "", "self", ".", "hidden_layers", "=", "[", "]", "\n", "for", "i", "in", "hidden_units", ":", "\n", "            ", "self", ".", "hidden_layers", ".", "append", "(", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.Model.call": [[37, 44], ["agent.Model.input_layer", "agent.Model.output_layer", "layer"], "methods", ["None"], ["i", ",", "activation", "=", "'tanh'", ",", "kernel_initializer", "=", "'RandomNormal'", ")", ")", "\n", "", "self", ".", "output_layer", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "num_actions", ",", "activation", "=", "'linear'", ",", "kernel_initializer", "=", "'RandomNormal'", ")", "\n", "\n", "", "@", "tf", ".", "function", "\n", "def", "call", "(", "self", ",", "inputs", ")", ":", "\n", "        ", "z", "=", "self", ".", "input_layer", "(", "inputs", ")", "\n", "for", "layer", "in", "self", ".", "hidden_layers", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.__init__": [[47, 65], ["numpy.linspace", "tensorflow.optimizers.Adam", "agent.Model", "collections.deque", "collections.deque"], "methods", ["None"], ["return", "output", "\n", "\n", "\n", "", "", "class", "DQN", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "num_states", ",", "num_actions", ",", "model_params", ",", "alg_params", ")", ":", "\n", "        ", "np", ".", "random", ".", "seed", "(", "alg_params", "[", "'seed'", "]", ")", "\n", "tf", ".", "random", ".", "set_seed", "(", "alg_params", "[", "'seed'", "]", ")", "\n", "random", ".", "seed", "(", "alg_params", "[", "'seed'", "]", ")", "\n", "self", ".", "num_actions", "=", "num_actions", "\n", "self", ".", "alg", "=", "alg_params", "[", "'algorithm'", "]", "\n", "self", ".", "batch_size", "=", "alg_params", "[", "'batch_size'", "]", "\n", "self", ".", "optimizer", "=", "tf", ".", "optimizers", ".", "Adam", "(", "alg_params", "[", "'learning_rate'", "]", ")", "\n", "self", ".", "use_stochastic_delay", "=", "alg_params", "[", "'use_stochastic_delay'", "]", "\n", "self", ".", "max_dimension", "=", "model_params", "[", "'max_dimension'", "]", "\n", "hidden_units", "=", "model_params", "[", "'hidden_units'", "]", "\n", "self", ".", "delay", "=", "alg_params", "[", "'delay'", "]", "\n", "self", ".", "gamma", "=", "alg_params", "[", "'gamma'", "]", "\n", "self", ".", "model", "=", "Model", "(", "num_states", ",", "hidden_units", ",", "num_actions", ",", "self", ".", "use_stochastic_delay", ",", "self", ".", "max_dimension", ",", "\n", "self", ".", "alg", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict": [[66, 68], ["agent.DQN.model", "numpy.atleast_2d", "inputs.astype"], "methods", ["None"], ["self", ".", "experience", "=", "{", "'s'", ":", "[", "]", ",", "'a'", ":", "[", "]", ",", "'r'", ":", "[", "]", ",", "'s2'", ":", "[", "]", ",", "'done'", ":", "[", "]", "}", "\n", "self", ".", "max_experiences", "=", "model_params", "[", "'max_buffer_size'", "]", "\n", "self", ".", "min_experiences", "=", "model_params", "[", "'min_buffer_size'", "]", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.fill_up_buffer": [[69, 73], ["agent.DQN.action_buffer_padded.clear", "range", "agent.DQN.action_buffer_padded.append"], "methods", ["None"], ["if", "self", ".", "alg", "!=", "'normal'", ":", "\n", "            ", "self", ".", "action_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "max_dimension", "+", "1", ")", "\n", "self", ".", "action_buffer_padded", "=", "deque", "(", "maxlen", "=", "self", ".", "max_dimension", "+", "1", ")", "\n", "\n", "", "", "def", "predict", "(", "self", ",", "inputs", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.buffer_padding": [[74, 79], ["len", "copy.deepcopy", "range", "agent.DQN.action_buffer_padded.append"], "methods", ["None"], ["        ", "return", "self", ".", "model", "(", "np", ".", "atleast_2d", "(", "inputs", ".", "astype", "(", "'float32'", ")", ")", ")", "\n", "\n", "", "def", "fill_up_buffer", "(", "self", ")", ":", "\n", "        ", "self", ".", "action_buffer_padded", ".", "clear", "(", ")", "\n", "for", "_", "in", "range", "(", "self", ".", "max_dimension", ")", ":", "\n", "            ", "self", ".", "action_buffer_padded", ".", "append", "(", "0", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.train": [[80, 100], ["numpy.random.randint", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.max", "numpy.where", "tape.gradient", "agent.DQN.optimizer.apply_gradients", "len", "TargetNet.predict", "tensorflow.GradientTape", "tensorflow.math.reduce_sum", "tensorflow.math.reduce_mean", "zip", "len", "tensorflow.square", "agent.DQN.predict", "tensorflow.one_hot"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict"], ["\n", "", "", "def", "buffer_padding", "(", "self", ")", ":", "\n", "        ", "current_length", "=", "len", "(", "self", ".", "action_buffer", ")", "\n", "self", ".", "action_buffer_padded", "=", "copy", ".", "deepcopy", "(", "self", ".", "action_buffer", ")", "\n", "for", "_", "in", "range", "(", "0", ",", "self", ".", "max_dimension", "-", "current_length", ")", ":", "\n", "            ", "self", ".", "action_buffer_padded", ".", "append", "(", "0", ")", "\n", "\n", "", "", "def", "train", "(", "self", ",", "TargetNet", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "experience", "[", "'s'", "]", ")", "<", "self", ".", "min_experiences", ":", "\n", "            ", "return", "0", "\n", "", "ids", "=", "np", ".", "random", ".", "randint", "(", "low", "=", "0", ",", "high", "=", "len", "(", "self", ".", "experience", "[", "'s'", "]", ")", ",", "size", "=", "self", ".", "batch_size", ")", "\n", "states", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'s'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "actions", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'a'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "rewards", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'r'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "states_next", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'s2'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "dones", "=", "np", ".", "asarray", "(", "[", "self", ".", "experience", "[", "'done'", "]", "[", "i", "]", "for", "i", "in", "ids", "]", ")", "\n", "value_next", "=", "np", ".", "max", "(", "TargetNet", ".", "predict", "(", "states_next", ")", ",", "axis", "=", "1", ")", "\n", "actual_values", "=", "np", ".", "where", "(", "dones", ",", "rewards", ",", "rewards", "+", "self", ".", "gamma", "*", "value_next", ")", "\n", "\n", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "\n", "            ", "selected_action_values", "=", "tf", ".", "math", ".", "reduce_sum", "(", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.get_action": [[101, 106], ["numpy.random.random", "numpy.random.choice", "numpy.argmax", "agent.DQN.predict", "numpy.atleast_2d"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.predict"], ["self", ".", "predict", "(", "states", ")", "*", "tf", ".", "one_hot", "(", "actions", ",", "self", ".", "num_actions", ")", ",", "axis", "=", "1", ")", "\n", "loss", "=", "tf", ".", "math", ".", "reduce_mean", "(", "tf", ".", "square", "(", "actual_values", "-", "selected_action_values", ")", ")", "\n", "", "variables", "=", "self", ".", "model", ".", "trainable_variables", "\n", "gradients", "=", "tape", ".", "gradient", "(", "loss", ",", "variables", ")", "\n", "self", ".", "optimizer", ".", "apply_gradients", "(", "zip", "(", "gradients", ",", "variables", ")", ")", "\n", "return", "loss", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.add_experience": [[107, 113], ["exp.items", "len", "agent.DQN.experience.keys", "agent.DQN.experience[].append", "agent.DQN.experience[].pop"], "methods", ["None"], ["\n", "", "def", "get_action", "(", "self", ",", "states", ",", "epsilon", ")", ":", "\n", "        ", "if", "np", ".", "random", ".", "random", "(", ")", "<", "epsilon", ":", "\n", "            ", "return", "np", ".", "random", ".", "choice", "(", "self", ".", "num_actions", ")", "\n", "", "else", ":", "\n", "            ", "return", "np", ".", "argmax", "(", "self", ".", "predict", "(", "np", ".", "atleast_2d", "(", "states", ")", ")", "[", "0", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.copy_weights": [[114, 119], ["zip", "v1.assign", "v2.numpy"], "methods", ["None"], ["", "", "def", "add_experience", "(", "self", ",", "exp", ")", ":", "\n", "        ", "if", "len", "(", "self", ".", "experience", "[", "'s'", "]", ")", ">=", "self", ".", "max_experiences", ":", "\n", "            ", "for", "key", "in", "self", ".", "experience", ".", "keys", "(", ")", ":", "\n", "                ", "self", ".", "experience", "[", "key", "]", ".", "pop", "(", "0", ")", "\n", "", "", "for", "key", ",", "value", "in", "exp", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "experience", "[", "key", "]", ".", "append", "(", "value", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.play_game": [[121, 212], ["env.reset", "list", "TrainNet.fill_up_buffer", "len", "TrainNet.add_experience", "TrainNet.train", "isinstance", "statistics.mean", "TrainNet.get_action", "env.step", "env.step", "env.reset", "list.append", "list.append", "TargetNet.copy_weights", "TrainNet.get_action", "TrainNet.get_action", "TrainNet.action_buffer.append", "range", "TrainNet.buffer_padding", "TrainNet.action_buffer.append", "TrainNet.buffer_padding", "len", "TrainNet.action_buffer.clear", "TrainNet.buffer_padding", "env.state_buffer.pop", "env.state_buffer.clear", "numpy.sum", "env.done_buffer.pop", "env.done_buffer.clear", "env.reward_buffer.clear", "numpy.append", "numpy.append", "numpy.append", "TrainNet.action_buffer.clear", "TrainNet.buffer_padding", "TrainNet.train.numpy", "numpy.append", "numpy.append", "numpy.append", "random.randint", "TrainNet.action_buffer.popleft"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.fill_up_buffer", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.add_experience", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.train", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.get_action", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.copy_weights", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.get_action", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.get_action", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.buffer_padding", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.buffer_padding", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.buffer_padding", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.buffer_padding"], ["", "", "def", "copy_weights", "(", "self", ",", "TrainNet", ")", ":", "\n", "        ", "variables1", "=", "self", ".", "model", ".", "trainable_variables", "\n", "variables2", "=", "TrainNet", ".", "model", ".", "trainable_variables", "\n", "for", "v1", ",", "v2", "in", "zip", "(", "variables1", ",", "variables2", ")", ":", "\n", "            ", "v1", ".", "assign", "(", "v2", ".", "numpy", "(", ")", ")", "\n", "\n", "\n", "", "", "", "def", "play_game", "(", "global_step", ",", "env", ",", "TrainNet", ",", "TargetNet", ",", "epsilon", ",", "copy_step", ")", ":", "\n", "    ", "rewards", "=", "0", "\n", "episode_step", "=", "0", "\n", "last_state_observed", "=", "0", "\n", "done", "=", "False", "\n", "observations", "=", "env", ".", "reset", "(", ")", "\n", "observations_original", "=", "observations", "\n", "if", "env", ".", "game_name", ".", "startswith", "(", "'Frozen'", ")", ":", "\n", "        ", "observations", "=", "to_onehot", "(", "env", ".", "state_space", ".", "n", ",", "observations", ")", "\n", "", "if", "TrainNet", ".", "alg", "!=", "'normal'", ":", "\n", "        ", "TrainNet", ".", "fill_up_buffer", "(", ")", "\n", "", "losses", "=", "list", "(", ")", "\n", "clear", "=", "False", "\n", "while", "not", "done", ":", "\n", "        ", "delay", "=", "env", ".", "delay", "\n", "len_buffer", "=", "len", "(", "env", ".", "state_buffer", ")", "\n", "if", "TrainNet", ".", "alg", "==", "'normal'", ":", "\n", "            ", "action", "=", "TrainNet", ".", "get_action", "(", "observations", ",", "epsilon", ")", "\n", "prev_observations", "=", "observations", "\n", "observations", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "observations_original", ",", "action", ")", "\n", "observations_original", "=", "observations", "\n", "if", "env", ".", "game_name", ".", "startswith", "(", "'Frozen'", ")", ":", "\n", "                ", "observations", "=", "to_onehot", "(", "env", ".", "state_space", ".", "n", ",", "observations", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "episode_step", "==", "0", ":", "\n", "                ", "if", "env", ".", "use_stochastic_delay", ":", "\n", "                    ", "last_state_observed", "=", "(", "episode_step", "-", "env", ".", "turn_limit", "/", "2", ")", "/", "env", ".", "turn_limit", "\n", "action_state", "=", "np", ".", "append", "(", "last_state_observed", ",", "TrainNet", ".", "action_buffer_padded", ")", "\n", "information_state", "=", "np", ".", "append", "(", "observations", ",", "action_state", ")", "\n", "# information_state = np.append(observations, TrainNet.action_buffer_padded)", "\n", "", "else", ":", "\n", "                    ", "information_state", "=", "np", ".", "append", "(", "observations", ",", "TrainNet", ".", "action_buffer_padded", ")", "\n", "", "", "if", "TrainNet", ".", "alg", "==", "'IS'", ":", "\n", "                ", "action", "=", "TrainNet", ".", "get_action", "(", "information_state", ",", "epsilon", ")", "\n", "", "else", ":", "\n", "                ", "action", "=", "TrainNet", ".", "get_action", "(", "observations", ",", "epsilon", ")", "\n", "", "prev_observations", "=", "observations", "\n", "prev_information_state", "=", "information_state", "\n", "observations", ",", "reward", ",", "done", "=", "env", ".", "step", "(", "observations_original", ",", "action", ")", "\n", "observations_original", "=", "observations", "\n", "if", "env", ".", "game_name", ".", "startswith", "(", "'Frozen'", ")", ":", "\n", "                ", "observations", "=", "to_onehot", "(", "env", ".", "state_space", ".", "n", ",", "observations", ")", "\n", "", "episode_step", "+=", "1", "\n", "\n", "if", "env", ".", "train", ":", "\n", "                ", "last_state_observed", "=", "(", "episode_step", "-", "1", "-", "env", ".", "turn_limit", "/", "2", ")", "/", "env", ".", "turn_limit", "\n", "TrainNet", ".", "action_buffer", ".", "append", "(", "action", "+", "1", ")", "\n", "for", "i", "in", "range", "(", "len_buffer", "+", "1", "-", "delay", ")", ":", "\n", "                    ", "TrainNet", ".", "action_buffer", ".", "popleft", "(", ")", "-", "1", "\n", "", "TrainNet", ".", "buffer_padding", "(", ")", "\n", "", "else", ":", "\n", "# delayed_action = random.randint(0, TrainNet.num_actions)", "\n", "                ", "TrainNet", ".", "action_buffer", ".", "append", "(", "action", "+", "1", ")", "\n", "TrainNet", ".", "buffer_padding", "(", ")", "\n", "\n", "", "if", "env", ".", "delay", "==", "0", ":", "\n", "                ", "delayed_action", "=", "action", "\n", "", "else", ":", "\n", "                ", "if", "not", "TrainNet", ".", "action_buffer", ":", "\n", "                    ", "delayed_action", "=", "random", ".", "randint", "(", "0", ",", "TrainNet", ".", "num_actions", ")", "\n", "", "else", ":", "\n", "                    ", "delayed_action", "=", "TrainNet", ".", "action_buffer", "[", "0", "]", "\n", "\n", "", "", "if", "delay", "==", "0", ":", "\n", "                ", "delayed_action", "=", "action", "\n", "\n", "", "if", "len", "(", "TrainNet", ".", "action_buffer", ")", "==", "TrainNet", ".", "max_dimension", "+", "1", ":", "\n", "                ", "TrainNet", ".", "action_buffer", ".", "clear", "(", ")", "\n", "TrainNet", ".", "buffer_padding", "(", ")", "\n", "observations", "=", "env", ".", "state_buffer", ".", "pop", "(", ")", "\n", "env", ".", "state_buffer", ".", "clear", "(", ")", "\n", "reward", "=", "np", ".", "sum", "(", "env", ".", "reward_buffer", ")", "\n", "done", "=", "env", ".", "done_buffer", ".", "pop", "(", ")", "\n", "env", ".", "done_buffer", ".", "clear", "(", ")", "\n", "env", ".", "reward_buffer", ".", "clear", "(", ")", "\n", "clear", "=", "True", "\n", "\n", "", "if", "env", ".", "use_stochastic_delay", ":", "\n", "                ", "action_state", "=", "np", ".", "append", "(", "last_state_observed", ",", "TrainNet", ".", "action_buffer_padded", ")", "\n", "information_state", "=", "np", ".", "append", "(", "observations", ",", "action_state", ")", "\n", "# information_state = np.append(observations, TrainNet.action_buffer_padded)", "\n", "", "else", ":", "\n", "                ", "information_state", "=", "np", ".", "append", "(", "observations", ",", "TrainNet", ".", "action_buffer_padded", ")", "\n", "", "", "rewards", "+=", "reward", "\n", "if", "done", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.test": [[214, 228], ["range", "env.reset", "print", "TrainNet.get_action", "env.step", "open", "print"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.DQN.get_action", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step"], ["env", ".", "reset", "(", ")", "\n", "if", "TrainNet", ".", "alg", "!=", "'normal'", ":", "\n", "                ", "TrainNet", ".", "action_buffer", ".", "clear", "(", ")", "\n", "TrainNet", ".", "buffer_padding", "(", ")", "\n", "", "", "global_step", "+=", "1", "\n", "if", "TrainNet", ".", "alg", "==", "'normal'", ":", "\n", "            ", "exp", "=", "{", "'s'", ":", "prev_observations", ",", "'a'", ":", "action", ",", "'r'", ":", "reward", ",", "'s2'", ":", "observations", ",", "'done'", ":", "done", "}", "\n", "", "if", "TrainNet", ".", "alg", "==", "'delay'", ":", "\n", "            ", "exp", "=", "{", "'s'", ":", "prev_observations", ",", "'a'", ":", "delayed_action", ",", "'r'", ":", "reward", ",", "'s2'", ":", "observations", ",", "'done'", ":", "done", "}", "\n", "", "if", "TrainNet", ".", "alg", "==", "'IS'", ":", "\n", "            ", "exp", "=", "{", "'s'", ":", "prev_information_state", ",", "'a'", ":", "action", ",", "'r'", ":", "reward", ",", "'s2'", ":", "information_state", ",", "'done'", ":", "done", "}", "\n", "", "TrainNet", ".", "add_experience", "(", "exp", ")", "\n", "loss", "=", "TrainNet", ".", "train", "(", "TargetNet", ")", "\n", "if", "isinstance", "(", "loss", ",", "int", ")", ":", "\n", "            ", "losses", ".", "append", "(", "loss", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.train_agent": [[230, 270], ["numpy.random.seed", "tensorflow.random.set_seed", "random.seed", "agent.DQN", "agent.DQN", "agent.play_game", "total_rewards_list.append", "total_losses_list.append", "numpy.array", "numpy.array", "total_rewards[].mean", "total_losses[].mean", "numpy.exp", "print", "open", "print", "max", "max"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.agent.play_game"], ["            ", "losses", ".", "append", "(", "loss", ".", "numpy", "(", ")", ")", "\n", "", "if", "global_step", "%", "copy_step", "==", "0", ":", "\n", "            ", "TargetNet", ".", "copy_weights", "(", "TrainNet", ")", "\n", "", "", "return", "global_step", ",", "rewards", ",", "mean", "(", "losses", ")", "\n", "\n", "\n", "", "def", "test", "(", "env", ",", "TrainNet", ",", "logs", ",", "num_episodes", ")", ":", "\n", "    ", "for", "_", "in", "range", "(", "num_episodes", ")", ":", "\n", "        ", "observation", "=", "env", ".", "reset", "(", ")", "\n", "rewards", "=", "0", "\n", "steps", "=", "0", "\n", "done", "=", "False", "\n", "while", "not", "done", ":", "\n", "            ", "action", "=", "TrainNet", ".", "get_action", "(", "observation", ",", "0", ")", "\n", "observation", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "steps", "+=", "1", "\n", "rewards", "+=", "reward", "\n", "", "with", "open", "(", "logs", "[", "'log_file_name'", "]", ",", "\"a\"", ")", "as", "f", ":", "\n", "            ", "print", "(", "\"Testing steps: {} rewards :{} \"", ".", "format", "(", "steps", ",", "rewards", ")", ",", "file", "=", "f", ")", "\n", "", "print", "(", "\"Testing steps: {} rewards :{} \"", ".", "format", "(", "steps", ",", "rewards", ")", ")", "\n", "\n", "\n", "", "", "def", "train_agent", "(", "env", ",", "num_frames", ",", "model_params", ",", "algorithm_params", ",", "logs", ",", "verbose", ")", ":", "\n", "    ", "num_actions", "=", "env", ".", "number_of_actions", "\n", "try", ":", "\n", "        ", "state_space", "=", "len", "(", "env", ".", "state_space", ".", "sample", "(", ")", ")", "\n", "", "except", "TypeError", ":", "\n", "        ", "state_space", "=", "env", ".", "state_space", ".", "n", "\n", "\n", "", "copy_step", "=", "model_params", "[", "'copy_step'", "]", "\n", "TrainNet", "=", "DQN", "(", "state_space", ",", "num_actions", ",", "model_params", ",", "algorithm_params", ")", "\n", "TargetNet", "=", "DQN", "(", "state_space", ",", "num_actions", ",", "model_params", ",", "algorithm_params", ")", "\n", "# N = num_episodes", "\n", "total_rewards_list", "=", "[", "]", "\n", "total_losses_list", "=", "[", "]", "\n", "epsilon_start", "=", "algorithm_params", "[", "'start_epsilon'", "]", "\n", "decay", "=", "algorithm_params", "[", "'epsilon_decay'", "]", "\n", "min_epsilon", "=", "algorithm_params", "[", "'stop_epsilon'", "]", "\n", "global_step", "=", "1", "\n", "n", "=", "0", "\n", "while", "True", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.__init__": [[9, 40], ["numpy.random.seed", "random.seed", "numpy.empty", "len", "collections.deque", "collections.deque", "collections.deque", "env_stochasticdelay.Environment.reset", "env_stochasticdelay.Environment.update_delay"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.update_delay"], ["class", "Environment", ":", "\n", "    ", "def", "__init__", "(", "self", ",", "seed", ",", "game_name", ",", "gamma", ",", "use_stochastic_delay", ",", "delay", ",", "min_delay", ")", ":", "\n", "        ", "\"\"\"Initialize Environment\"\"\"", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "env", "=", "gym", ".", "make", "(", "self", ".", "game_name", ")", "\n", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "number_of_actions", "=", "self", ".", "env", ".", "action_space", ".", "n", "\n", "self", ".", "delay", "=", "delay", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "[", "'image'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "\n", "", "self", ".", "use_stochastic_delay", "=", "use_stochastic_delay", "\n", "self", ".", "no_action", "=", "0", "\n", "self", ".", "index", "=", "0", "\n", "self", ".", "use_stochastic_delay", "=", "use_stochastic_delay", "\n", "if", "self", ".", "use_stochastic_delay", ":", "\n", "            ", "self", ".", "min_delay", "=", "min_delay", "\n", "self", ".", "delay", "=", "self", ".", "min_delay", "\n", "self", ".", "max_delay", "=", "delay", "\n", "", "else", ":", "\n", "            ", "self", ".", "min_delay", "=", "delay", "\n", "self", ".", "delay", "=", "delay", "\n", "self", ".", "max_delay", "=", "delay", "\n", "", "self", ".", "state_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "max_delay", "+", "2", ")", "\n", "self", ".", "reward_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "max_delay", "+", "2", ")", "\n", "self", ".", "done_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "max_delay", "+", "2", ")", "\n", "self", ".", "turn_limit", "=", "200", "\n", "self", ".", "state", "=", "self", ".", "reset", "(", ")", "\n", "self", ".", "update_delay", "(", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.reset": [[41, 48], ["random.randint"], "methods", ["None"], ["self", ".", "train", "=", "True", "\n", "self", ".", "step_count", "=", "0", "\n", "self", ".", "delayed_action", "=", "0", "\n", "self", ".", "gamma", "=", "gamma", "\n", "\n", "", "def", "process_state", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Pre-process state if required\"\"\"", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.update_delay": [[49, 54], ["random.randint"], "methods", ["None"], ["            ", "return", "np", ".", "array", "(", "observation", "[", "'image'", "]", ",", "dtype", "=", "'float32'", ")", "# Using only image as state (7x7x3)", "\n", "", "else", ":", "\n", "            ", "return", "observation", "\n", "\n", "", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.step": [[55, 102], ["env_stochasticdelay.Environment.env_step", "env_stochasticdelay.Environment.env_step", "len", "env_stochasticdelay.Environment.state_buffer.append", "env_stochasticdelay.Environment.reward_buffer.append", "env_stochasticdelay.Environment.done_buffer.append", "len", "env_stochasticdelay.Environment.state_buffer.append", "env_stochasticdelay.Environment.reward_buffer.append", "env_stochasticdelay.Environment.done_buffer.append", "range", "env_stochasticdelay.Environment.update_delay", "env_stochasticdelay.Environment.state_buffer.append", "env_stochasticdelay.Environment.reward_buffer.append", "env_stochasticdelay.Environment.done_buffer.append", "env_stochasticdelay.Environment.state_buffer.popleft", "env_stochasticdelay.Environment.reward_buffer.popleft", "env_stochasticdelay.Environment.done_buffer.popleft", "env_stochasticdelay.Environment.update_delay", "len", "env_stochasticdelay.Environment.state_buffer.popleft", "numpy.power", "env_stochasticdelay.Environment.done_buffer.popleft", "env_stochasticdelay.Environment.state_buffer.clear", "env_stochasticdelay.Environment.reward_buffer.clear", "env_stochasticdelay.Environment.done_buffer.clear", "env_stochasticdelay.Environment.state_buffer.clear", "env_stochasticdelay.Environment.reward_buffer.clear", "env_stochasticdelay.Environment.done_buffer.clear", "env_stochasticdelay.Environment.reward_buffer.popleft"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.env_step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.env_step", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.update_delay", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.update_delay"], ["if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "return", "self", ".", "process_state", "(", "state", ")", "\n", "", "else", ":", "\n", "            ", "return", "state", "\n", "\n", "", "", "def", "update_delay", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "use_stochastic_delay", ":", "\n", "            ", "self", ".", "delay", "=", "random", ".", "randint", "(", "self", ".", "min_delay", ",", "self", ".", "max_delay", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "delay", "=", "self", ".", "max_delay", "\n", "\n", "", "", "def", "step", "(", "self", ",", "state", ",", "action", ")", ":", "\n", "        ", "if", "self", ".", "max_delay", "!=", "0", ":", "\n", "            ", "self", ".", "train", "=", "True", "\n", "if", "True", "not", "in", "self", ".", "done_buffer", ":", "\n", "                ", "next_state", ",", "rewards", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "", "else", ":", "\n", "                ", "next_state", "=", "state", "\n", "rewards", "=", "0", "\n", "done", "=", "True", "\n", "", "if", "len", "(", "self", ".", "state_buffer", ")", "<", "self", ".", "delay", ":", "# delay is greater than the number of unobserved states", "\n", "                ", "self", ".", "state_buffer", ".", "append", "(", "next_state", ")", "\n", "self", ".", "reward_buffer", ".", "append", "(", "rewards", ")", "\n", "self", ".", "done_buffer", ".", "append", "(", "done", ")", "\n", "self", ".", "train", "=", "False", "\n", "return", "state", ",", "0", ",", "False", "\n", "", "elif", "len", "(", "self", ".", "state_buffer", ")", ">", "self", ".", "delay", ":", "# delay is less than the number of unobserved states", "\n", "                ", "self", ".", "state_buffer", ".", "append", "(", "next_state", ")", "\n", "self", ".", "reward_buffer", ".", "append", "(", "rewards", ")", "\n", "self", ".", "done_buffer", ".", "append", "(", "done", ")", "\n", "rewards", "=", "0", "\n", "no_observed_states", "=", "len", "(", "self", ".", "state_buffer", ")", "-", "self", ".", "delay", "\n", "for", "i", "in", "range", "(", "no_observed_states", ")", ":", "\n", "                    ", "next_state", "=", "self", ".", "state_buffer", ".", "popleft", "(", ")", "\n", "gamma", "=", "np", ".", "power", "(", "self", ".", "gamma", ",", "no_observed_states", "-", "(", "i", "+", "1", ")", ")", "\n", "rewards", "+=", "gamma", "*", "self", ".", "reward_buffer", ".", "popleft", "(", ")", "# add all unobserved rewards", "\n", "done", "=", "self", ".", "done_buffer", ".", "popleft", "(", ")", "\n", "", "self", ".", "update_delay", "(", ")", "\n", "if", "done", ":", "\n", "                    ", "self", ".", "state_buffer", ".", "clear", "(", ")", "\n", "self", ".", "reward_buffer", ".", "clear", "(", ")", "\n", "self", ".", "done_buffer", ".", "clear", "(", ")", "\n", "", "return", "next_state", ",", "rewards", ",", "done", "\n", "", "else", ":", "\n", "                ", "self", ".", "state_buffer", ".", "append", "(", "next_state", ")", "\n", "self", ".", "reward_buffer", ".", "append", "(", "rewards", ")", "\n", "self", ".", "done_buffer", ".", "append", "(", "done", ")", "\n", "delayed_next_state", "=", "self", ".", "state_buffer", ".", "popleft", "(", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env_stochasticdelay.Environment.env_step": [[103, 158], ["env_stochasticdelay.Environment.reset", "env_stochasticdelay.Environment.reset", "env_stochasticdelay.Environment.reset", "env_stochasticdelay.Environment.reset"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset"], ["delayed_rewards", "=", "self", ".", "reward_buffer", ".", "popleft", "(", ")", "\n", "delayed_done", "=", "self", ".", "done_buffer", ".", "popleft", "(", ")", "\n", "self", ".", "update_delay", "(", ")", "\n", "if", "delayed_done", ":", "\n", "                    ", "self", ".", "state_buffer", ".", "clear", "(", ")", "\n", "self", ".", "reward_buffer", ".", "clear", "(", ")", "\n", "self", ".", "done_buffer", ".", "clear", "(", ")", "\n", "", "return", "delayed_next_state", ",", "delayed_rewards", ",", "delayed_done", "\n", "", "", "else", ":", "\n", "            ", "next_state", ",", "rewards", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "return", "next_state", ",", "rewards", ",", "done", "\n", "\n", "", "", "def", "render", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "render", "(", ")", "\n", "\n", "", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.running_mean": [[14, 17], ["numpy.cumsum", "numpy.insert", "float"], "function", ["None"], ["def", "running_mean", "(", "x", ",", "n", ")", ":", "\n", "    ", "cumulative_sum", "=", "np", ".", "cumsum", "(", "np", ".", "insert", "(", "x", ",", "0", ",", "0", ")", ")", "\n", "return", "(", "cumulative_sum", "[", "n", ":", "]", "-", "cumulative_sum", "[", ":", "-", "n", "]", ")", "/", "float", "(", "n", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file": [[19, 30], ["os.listdir", "os.getcwd", "open", "open", "[].split", "[].split", "f.readline().split", "f.readline().split", "f.readline", "f.readline"], "function", ["None"], ["", "def", "get_file", "(", "index", ",", "ver", ")", ":", "\n", "    ", "save_dir", "=", "os", ".", "getcwd", "(", ")", "+", "'/Results/Results-{}/'", ".", "format", "(", "ver", ")", "# Save Directory", "\n", "files_list", "=", "os", ".", "listdir", "(", "save_dir", ")", "\n", "if", "ver", "==", "'6.12'", ":", "\n", "        ", "with", "open", "(", "save_dir", "+", "files_list", "[", "index", "]", "+", "'/log.txt'", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "env_name", "=", "f", ".", "readline", "(", ")", ".", "split", "(", "','", ")", "[", "0", "]", ".", "split", "(", "':'", ")", "[", "1", "]", "# Gets the environment name", "\n", "", "", "else", ":", "\n", "        ", "with", "open", "(", "save_dir", "+", "files_list", "[", "index", "]", "+", "'/log_sd.txt'", ",", "'r'", ")", "as", "f", ":", "\n", "            ", "env_name", "=", "f", ".", "readline", "(", ")", ".", "split", "(", "','", ")", "[", "0", "]", ".", "split", "(", "':'", ")", "[", "1", "]", "# Gets the environment name", "\n", "", "", "file_name", "=", "save_dir", "+", "files_list", "[", "index", "]", "# Final files directory", "\n", "return", "env_name", ",", "file_name", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.plot_reward": [[32, 68], ["plot.get_file", "matplotlib.figure", "matplotlib.legend", "matplotlib.savefig", "matplotlib.savefig", "matplotlib.tight_layout", "matplotlib.show", "matplotlib.title", "matplotlib.title", "matplotlib.title", "numpy.arange", "numpy.zeros", "range", "numpy.mean", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.plot", "os.getcwd", "os.getcwd", "os.getcwd", "numpy.std", "numpy.sqrt", "plot.running_mean", "numpy.load", "numpy.load"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.running_mean", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load"], ["", "def", "plot_reward", "(", "index", ",", "runs", ",", "delays", ",", "n", ")", ":", "\n", "    ", "env_name", ",", "file_name", "=", "get_file", "(", "index", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "if", "index", "==", "-", "1", ":", "\n", "        ", "plt", ".", "title", "(", "'DQN'", ")", "\n", "save_file", "=", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}/rewards_normal.pdf'", ".", "format", "(", "ver", ")", "\n", "", "if", "index", "==", "-", "2", ":", "\n", "        ", "plt", ".", "title", "(", "'DQN+IS'", ")", "\n", "save_file", "=", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}/rewards_IS.pdf'", ".", "format", "(", "ver", ")", "\n", "", "if", "index", "==", "-", "3", ":", "\n", "        ", "plt", ".", "title", "(", "'delay-DQN'", ")", "\n", "save_file", "=", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}/rewards_delay.pdf'", ".", "format", "(", "ver", ")", "\n", "", "for", "delay", "in", "delays", ":", "\n", "        ", "episodes", "=", "10000", "\n", "X_axis", "=", "np", ".", "arange", "(", "episodes", ")", "\n", "rewards_plot", "=", "np", ".", "zeros", "(", "[", "runs", ",", "episodes", "]", ")", "\n", "for", "run", "in", "range", "(", "runs", ")", ":", "\n", "            ", "if", "delay", "==", "'stochastic'", ":", "\n", "                ", "rewards", "=", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/rewards_delay_20_sd_run_{}.npy'", ".", "format", "(", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "\n", "(", ")", "]", "\n", "", "else", ":", "\n", "                ", "rewards", "=", "np", ".", "load", "(", "file_name", "+", "'/rewards_delay_{}_run_{}.npy'", ".", "format", "(", "delay", ",", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "# plt.plot(running_mean(rewards, n), alpha=0.25, linestyle='-.', color='blue')", "\n", "", "rewards_plot", "[", "run", "]", "=", "rewards", "[", "0", ":", "episodes", "]", "\n", "", "rewards_mean", "=", "np", ".", "mean", "(", "rewards_plot", ",", "axis", "=", "0", ")", "\n", "rewards_deviation", "=", "np", ".", "std", "(", "rewards_plot", ",", "axis", "=", "0", ")", "/", "np", ".", "sqrt", "(", "runs", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Rewards'", ")", "\n", "# plt.ylim(0, 210)", "\n", "plt", ".", "plot", "(", "running_mean", "(", "rewards_mean", ",", "n", ")", ",", "label", "=", "'delay={}'", ".", "format", "(", "delay", ")", ")", "\n", "# plt.fill_between(X_axis, rewards_mean+rewards_deviation, rewards_mean-rewards_deviation, alpha=1.5)", "\n", "", "plt", ".", "legend", "(", "title", "=", "'Delays'", ",", "bbox_to_anchor", "=", "(", "1.05", ",", "1", ")", ",", "loc", "=", "'upper left'", ")", "\n", "plt", ".", "savefig", "(", "save_file", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "plt", ".", "savefig", "(", "file_name", "+", "'/rewards.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.compare_learning_curves": [[70, 102], ["matplotlib.figure", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "range", "matplotlib.legend", "matplotlib.tight_layout", "matplotlib.show", "plot.get_file", "env_name.append", "file_name.append", "len", "range", "os.getcwd", "matplotlib.savefig", "matplotlib.plot", "os.makedirs", "matplotlib.savefig", "plot.running_mean", "numpy.load", "numpy.load", "os.getcwd"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.running_mean", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load"], ["", "def", "compare_learning_curves", "(", "indices", ",", "label", ",", "ver", ",", "runs", ",", "delay", ",", "n", "=", "1000", ")", ":", "\n", "    ", "env_name", "=", "[", "]", "\n", "file_name", "=", "[", "]", "\n", "for", "index", "in", "indices", ":", "\n", "        ", "env", ",", "file", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "env_name", ".", "append", "(", "env", ")", "\n", "file_name", ".", "append", "(", "file", ")", "\n", "# if not env_name.count(env_name[0]) == len(env_name):  # Check if all the environments are same", "\n", "#     raise Exception('Environments are different')", "\n", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "title", "(", "env_name", "[", "0", "]", ")", "\n", "plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Rewards'", ")", "\n", "colors", "=", "[", "'blue'", ",", "'tab:orange'", ",", "'green'", "]", "\n", "for", "index", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "# episodes = 10000", "\n", "# X_axis = np.arange(episodes)", "\n", "# rewards_plot = np.zeros([runs, episodes])", "\n", "        ", "for", "run", "in", "range", "(", "runs", ")", ":", "\n", "            ", "if", "delay", "==", "'stochastic'", ":", "\n", "                ", "rewards", "=", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/rewards_delay_10_sd_run_{}.npy'", ".", "format", "(", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "# rewards_plot[run] = np.mean(rewards)", "\n", "# print('Algorithm: {} Delay: Stochastic Run: {} Reward: {}'.format(index, run, rewards_plot[run]))", "\n", "", "else", ":", "\n", "                ", "rewards", "=", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/rewards_delay_{}_run_{}.npy'", ".", "format", "(", "delay", ",", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "# rewards_plot[run] = np.mean(rewards)", "\n", "# print('Algorithm: {} Delay: {} Run: {} Reward: {}'.format(index, delay, run, rewards_plot[run]))", "\n", "", "plt", ".", "plot", "(", "running_mean", "(", "rewards", ",", "n", ")", ",", "label", "=", "label", "[", "index", "]", "if", "run", "==", "0", "else", "''", ",", "color", "=", "colors", "[", "index", "]", ",", "alpha", "=", "0.5", ")", "\n", "", "", "plt", ".", "legend", "(", "title", "=", "'Algorithms'", ",", "bbox_to_anchor", "=", "(", "1.05", ",", "1", ")", ",", "loc", "=", "'upper left'", ")", "\n", "save_dir", "=", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}'", ".", "format", "(", "ver", ")", "\n", "try", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.plot_losses": [[104, 121], ["plot.get_file", "matplotlib.figure", "matplotlib.title", "len", "numpy.arange", "numpy.zeros", "range", "numpy.mean", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.plot", "matplotlib.savefig", "matplotlib.show", "numpy.load", "numpy.std", "numpy.sqrt", "plot.running_mean"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.running_mean"], ["", "except", "FileNotFoundError", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "getcwd", "(", ")", "+", "'/Plots/v{}'", ".", "format", "(", "ver", ")", ")", "\n", "plt", ".", "savefig", "(", "save_dir", "+", "'/rewards_curves.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "plt", ".", "tight_layout", "(", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "\n", "", "def", "plot_losses", "(", "index", ",", "runs", ",", "n", ")", ":", "\n", "    ", "env_name", ",", "file_name", "=", "get_file", "(", "index", ")", "\n", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "title", "(", "env_name", ")", "\n", "losses", "=", "np", ".", "load", "(", "file_name", "+", "'/loss.npy'", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "episodes", "=", "len", "(", "losses", "[", "0", "]", ")", "\n", "X_axis", "=", "np", ".", "arange", "(", "episodes", ")", "\n", "losses_plot", "=", "np", ".", "zeros", "(", "[", "runs", ",", "episodes", "]", ")", "\n", "for", "run", "in", "range", "(", "runs", ")", ":", "\n", "        ", "losses_plot", "[", "run", "]", "=", "losses", "[", "run", "]", "\n", "", "losses_mean", "=", "np", ".", "mean", "(", "losses_plot", ",", "axis", "=", "0", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.compare_algorithms": [[123, 189], ["matplotlib.figure", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "range", "matplotlib.legend", "matplotlib.xlabel", "matplotlib.xticks", "matplotlib.ylabel", "matplotlib.yticks", "matplotlib.tight_layout", "matplotlib.show", "plot.get_file", "env_name.append", "file_name.append", "plot.get_file", "file_name_sd.append", "len", "list", "numpy.zeros", "numpy.zeros", "numpy.zeros", "matplotlib.errorbar", "os.getcwd", "matplotlib.savefig", "map", "len", "len", "range", "numpy.mean", "numpy.mean", "numpy.mean", "os.makedirs", "matplotlib.savefig", "numpy.std", "numpy.sqrt", "numpy.mean", "print", "numpy.mean", "print", "os.getcwd", "numpy.load", "numpy.load"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load"], ["plt", ".", "xlabel", "(", "'Episodes'", ")", "\n", "plt", ".", "ylabel", "(", "'Losses'", ")", "\n", "plt", ".", "plot", "(", "running_mean", "(", "losses_mean", ",", "n", ")", ")", "\n", "plt", ".", "savefig", "(", "file_name", "+", "'/losses.pdf'", ")", "\n", "plt", ".", "show", "(", ")", "\n", "\n", "\n", "", "def", "compare_algorithms", "(", "indices", ",", "label", ",", "runs", ",", "delays", ",", "ver", ",", "colors", ")", ":", "\n", "    ", "env_name", "=", "[", "]", "\n", "file_name", "=", "[", "]", "\n", "file_name_sd", "=", "[", "]", "\n", "for", "index", "in", "indices", ":", "\n", "        ", "env", ",", "file", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "env_name", ".", "append", "(", "env", ")", "\n", "file_name", ".", "append", "(", "file", ")", "\n", "", "for", "index", "in", "indices", ":", "\n", "        ", "_", ",", "file_sd", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "file_name_sd", ".", "append", "(", "file_sd", ")", "\n", "# if not env_name.count(env_name[0]) == len(env_name):  # Check if all the environments are same", "\n", "#     raise Exception('Environments are different')", "\n", "", "plt", ".", "figure", "(", ")", "\n", "plt", ".", "title", "(", "env_name", "[", "0", "]", ",", "fontsize", "=", "20", ")", "\n", "for", "index", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "        ", "count", "=", "0", "\n", "X_axis", "=", "list", "(", "map", "(", "str", ",", "delays", ")", ")", "\n", "r_mean", "=", "np", ".", "zeros", "(", "len", "(", "delays", ")", ")", "\n", "r_std", "=", "np", ".", "zeros", "(", "len", "(", "delays", ")", ")", "\n", "episodes", "=", "10000", "\n", "# rewards_plot = np.zeros([runs, episodes])", "\n", "rewards_plot", "=", "np", ".", "zeros", "(", "runs", ")", "\n", "for", "delay", "in", "delays", ":", "\n", "            ", "for", "run", "in", "range", "(", "runs", ")", ":", "\n", "                ", "if", "delay", "==", "'stochastic'", ":", "\n", "                    ", "rewards", "=", "np", ".", "load", "(", "file_name_sd", "[", "index", "]", "+", "'/rewards_delay_10_sd_run_{}.npy'", ".", "format", "(", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "\n", "(", ")", "]", "\n", "rewards_plot", "[", "run", "]", "=", "np", ".", "mean", "(", "rewards", ")", "\n", "print", "(", "'Algorithm: {} Delay: Stochastic Run: {} Reward: {}'", ".", "format", "(", "index", ",", "run", ",", "rewards_plot", "[", "run", "]", ")", ")", "\n", "", "else", ":", "\n", "                    ", "rewards", "=", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/rewards_delay_{}_run_{}.npy'", ".", "format", "(", "delay", ",", "run", ")", ",", "allow_pickle", "=", "True", ")", "[", "(", ")", "]", "\n", "rewards_plot", "[", "run", "]", "=", "np", ".", "mean", "(", "rewards", ")", "\n", "print", "(", "'Algorithm: {} Delay: {} Run: {} Reward: {}'", ".", "format", "(", "index", ",", "delay", ",", "run", ",", "rewards_plot", "[", "run", "]", ")", ")", "\n", "", "", "rewards_mean", "=", "rewards_plot", "\n", "# rewards_mean = np.mean(rewards_plot, axis=0)", "\n", "rewards_deviation", "=", "np", ".", "std", "(", "rewards_plot", ",", "axis", "=", "0", ")", "/", "np", ".", "sqrt", "(", "runs", ")", "\n", "r_mean", "[", "count", "]", "=", "np", ".", "mean", "(", "rewards_mean", ")", "\n", "r_std", "[", "count", "]", "=", "np", ".", "mean", "(", "rewards_deviation", ")", "\n", "count", "+=", "1", "\n", "", "if", "label", "[", "index", "]", "==", "'DQN+IS'", ":", "\n", "            ", "alg", "=", "'DRDQN'", "\n", "", "else", ":", "\n", "            ", "alg", "=", "label", "[", "index", "]", "\n", "# plt.plot(X_axis, r_mean, marker='o', label=alg, color=colors[label[index]])", "\n", "", "plt", ".", "errorbar", "(", "X_axis", ",", "r_mean", ",", "marker", "=", "'o'", ",", "yerr", "=", "r_std", ",", "label", "=", "alg", ",", "color", "=", "colors", "[", "label", "[", "index", "]", "]", ",", "uplims", "=", "True", ",", "lolims", "=", "True", ")", "\n", "", "plt", ".", "legend", "(", ")", "\n", "plt", ".", "xlabel", "(", "'Delays'", ",", "fontsize", "=", "16", ")", "\n", "plt", ".", "xticks", "(", "fontsize", "=", "16", ")", "\n", "plt", ".", "ylabel", "(", "'Rewards'", ",", "fontsize", "=", "16", ")", "\n", "plt", ".", "yticks", "(", "fontsize", "=", "16", ")", "\n", "save_dir", "=", "os", ".", "getcwd", "(", ")", "+", "'/CartPole/Plots/v{}'", ".", "format", "(", "ver", ")", "\n", "try", ":", "\n", "        ", "plt", ".", "savefig", "(", "save_dir", "+", "'/rewards_comparison.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "getcwd", "(", ")", "+", "'/CartPole/Plots/v{}'", ".", "format", "(", "ver", ")", ")", "\n", "plt", ".", "savefig", "(", "save_dir", "+", "'/rewards_comparison.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "plt", ".", "tight_layout", "(", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.plot_time": [[191, 224], ["matplotlib.figure", "range", "matplotlib.title", "matplotlib.xlabel", "matplotlib.ylabel", "matplotlib.legend", "matplotlib.tight_layout", "matplotlib.show", "plot.get_file", "env_name.append", "file_name.append", "plot.get_file", "file_name_sd.append", "len", "numpy.zeros", "list", "range", "matplotlib.plot", "os.getcwd", "matplotlib.savefig", "map", "len", "os.makedirs", "matplotlib.savefig", "len", "numpy.mean", "numpy.mean", "numpy.load", "numpy.load", "os.getcwd"], "function", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.plot.get_file", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.Gym(Constant).dqn_agents.DQNAgent.load"], ["\n", "\n", "", "def", "plot_time", "(", "indices", ",", "labels", ",", "delays", ",", "ver", ")", ":", "\n", "    ", "plt", ".", "figure", "(", ")", "\n", "env_name", "=", "[", "]", "\n", "file_name", "=", "[", "]", "\n", "file_name_sd", "=", "[", "]", "\n", "for", "index", "in", "indices", ":", "\n", "        ", "env", ",", "file", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "env_name", ".", "append", "(", "env", ")", "\n", "file_name", ".", "append", "(", "file", ")", "\n", "", "for", "index", "in", "indices", ":", "\n", "        ", "_", ",", "file_sd", "=", "get_file", "(", "index", ",", "ver", ")", "\n", "file_name_sd", ".", "append", "(", "file_sd", ")", "\n", "", "for", "index", "in", "range", "(", "len", "(", "indices", ")", ")", ":", "\n", "        ", "time", "=", "np", ".", "zeros", "(", "[", "len", "(", "delays", ")", "]", ")", "\n", "X_axis", "=", "list", "(", "map", "(", "str", ",", "delays", ")", ")", "\n", "for", "delay", "in", "range", "(", "len", "(", "delays", ")", ")", ":", "\n", "            ", "if", "delays", "[", "delay", "]", "==", "'stochastic'", ":", "\n", "                ", "time", "[", "delay", "]", "=", "np", ".", "mean", "(", "np", ".", "load", "(", "file_name_sd", "[", "index", "]", "+", "'/time_delay_10_sd.npy'", ")", ")", "\n", "", "else", ":", "\n", "                ", "time", "[", "delay", "]", "=", "np", ".", "mean", "(", "np", ".", "load", "(", "file_name", "[", "index", "]", "+", "'/time_delay_{}.npy'", ".", "format", "(", "delays", "[", "delay", "]", ")", ")", ")", "\n", "", "", "plt", ".", "plot", "(", "X_axis", ",", "time", "/", "3600", ",", "label", "=", "labels", "[", "index", "]", ",", "marker", "=", "'o'", ")", "\n", "", "plt", ".", "title", "(", "env_name", "[", "0", "]", ")", "\n", "plt", ".", "xlabel", "(", "'Delays'", ")", "\n", "plt", ".", "ylabel", "(", "'Average Hours per run'", ")", "\n", "plt", ".", "legend", "(", ")", "\n", "save_dir", "=", "os", ".", "getcwd", "(", ")", "+", "'/CartPole/Plots/v{}/'", ".", "format", "(", "ver", ")", "\n", "try", ":", "\n", "        ", "plt", ".", "savefig", "(", "save_dir", "+", "'/time_comparison.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "        ", "os", ".", "makedirs", "(", "os", ".", "getcwd", "(", ")", "+", "'/CartPole/Plots/v{}'", ".", "format", "(", "ver", ")", ")", "\n", "plt", ".", "savefig", "(", "save_dir", "+", "'/time_comparison.pdf'", ",", "bbox_inches", "=", "\"tight\"", ")", "\n", "", "plt", ".", "tight_layout", "(", ")", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.__init__": [[8, 30], ["numpy.random.seed", "random.seed", "numpy.empty", "len", "collections.deque", "env.Environment.fill_up_buffer", "env.Environment.reset"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.fill_up_buffer", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset"], ["    ", "def", "__init__", "(", "self", ",", "game_name", ",", "delay", ",", "seed", ")", ":", "\n", "        ", "\"\"\"Initialize Environment\"\"\"", "\n", "self", ".", "game_name", "=", "game_name", "\n", "self", ".", "env", "=", "gym", ".", "make", "(", "self", ".", "game_name", ")", "\n", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "self", ".", "number_of_actions", "=", "self", ".", "env", ".", "action_space", ".", "n", "\n", "self", ".", "delay", "=", "delay", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "[", "'image'", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "state_space", "=", "self", ".", "env", ".", "observation_space", "\n", "", "self", ".", "actions_in_buffer", "=", "deque", "(", "maxlen", "=", "self", ".", "delay", ")", "\n", "self", ".", "fill_up_buffer", "(", ")", "\n", "self", ".", "delayed_action", "=", "0", "\n", "\n", "", "def", "process_state", "(", "self", ",", "observation", ")", ":", "\n", "        ", "\"\"\"Pre-process state if required\"\"\"", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "return", "np", ".", "array", "(", "observation", "[", "'image'", "]", ",", "dtype", "=", "'float32'", ")", "# Using only image as state (7x7x3)", "\n", "", "else", ":", "\n", "            ", "return", "observation", "\n", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset": [[31, 39], ["random.randint", "env.Environment.fill_up_buffer"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.fill_up_buffer"], ["", "", "def", "fill_up_buffer", "(", "self", ")", ":", "\n", "        ", "for", "_", "in", "range", "(", "self", ".", "delay", ")", ":", "\n", "            ", "action", "=", "np", ".", "random", ".", "choice", "(", "self", ".", "number_of_actions", ")", "\n", "self", ".", "actions_in_buffer", ".", "append", "(", "action", ")", "\n", "\n", "", "", "def", "reset", "(", "self", ")", ":", "\n", "        ", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "self", ".", "fill_up_buffer", "(", ")", "\n", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.fill_up_buffer": [[40, 44], ["range", "env.Environment.actions_in_buffer.append"], "methods", ["None"], ["            ", "return", "self", ".", "process_state", "(", "state", ")", "\n", "", "else", ":", "\n", "            ", "return", "state", "\n", "\n", "", "", "def", "step", "(", "self", ",", "action", ")", ":", "\n"]], "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.step": [[45, 107], ["env.Environment.actions_in_buffer.popleft", "env.Environment.actions_in_buffer.append", "env.Environment.reset", "env.Environment.reset", "env.Environment.reset", "env.Environment.reset"], "methods", ["home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset", "home.repos.pwc.inspect_result.baranwa2_delayresolvedrl.DQN.env.Environment.reset"], ["        ", "if", "self", ".", "delay", "!=", "0", ":", "\n", "            ", "chosen_action", "=", "action", "\n", "self", ".", "delayed_action", "=", "self", ".", "actions_in_buffer", ".", "popleft", "(", ")", "\n", "self", ".", "actions_in_buffer", ".", "append", "(", "chosen_action", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "delayed_action", "=", "action", "\n", "", "if", "'MiniGrid'", "in", "self", ".", "game_name", ":", "\n", "            ", "next_state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "self", ".", "delayed_action", ")", "\n", "return", "self", ".", "process_state", "(", "next_state", ")", ",", "reward", ",", "done", ",", "info", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "env", ".", "step", "(", "self", ".", "delayed_action", ")", "\n", "\n", "", "", "def", "render", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "render", "(", ")", "\n", "\n", "", "def", "close", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "env", ".", "close", "(", ")", "\n", "", "", ""]]}