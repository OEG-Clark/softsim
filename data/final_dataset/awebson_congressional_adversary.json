{"home.repos.pwc.inspect_result.awebson_congressional_adversary.src.data.GroundedWord.__str__": [[37, 46], ["None"], "methods", ["None"], ["def", "__str__", "(", "self", ")", "->", "str", ":", "\n", "        ", "if", "self", ".", "deno", "is", "not", "None", ":", "\n", "            ", "return", "(", "\n", "f'{self.text}\\t'", "\n", "f'{self.deno}\\t'", "\n", "f'{self.cono}\\t'", ")", "\n", "", "else", ":", "\n", "            ", "return", "(", "\n", "f'{self.text}\\t'", "\n", "f'{self.cono}\\t'", ")", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing.Scalar.check_rank": [[16, 18], ["improvised_typing._check_rank"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing._check_rank"], ["    ", "def", "check_rank", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "if", "_check_rank", "(", "0", ",", "self", ".", "shape", ")", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing.Vector.check_rank": [[21, 23], ["improvised_typing._check_rank"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing._check_rank"], ["    ", "def", "check_rank", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "if", "_check_rank", "(", "1", ",", "self", ".", "shape", ")", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing.Matrix.check_rank": [[26, 28], ["improvised_typing._check_rank"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing._check_rank"], ["    ", "def", "check_rank", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "if", "_check_rank", "(", "2", ",", "self", ".", "shape", ")", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing.R3Tensor.check_rank": [[31, 33], ["improvised_typing._check_rank"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing._check_rank"], ["    ", "def", "check_rank", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "if", "_check_rank", "(", "3", ",", "self", ".", "shape", ")", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing.R4Tensor.check_rank": [[36, 38], ["improvised_typing._check_rank"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing._check_rank"], ["    ", "def", "check_rank", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "if", "_check_rank", "(", "4", ",", "self", ".", "shape", ")", "else", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing.R5Tensor.check_rank": [[41, 43], ["improvised_typing._check_rank"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing._check_rank"], ["    ", "def", "check_rank", "(", "self", ")", "->", "bool", ":", "\n", "        ", "return", "True", "if", "_check_rank", "(", "5", ",", "self", ".", "shape", ")", "else", "False", "\n", "", "", ""]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.improvised_typing._check_rank": [[5, 13], ["len", "RuntimeError"], "function", ["None"], ["def", "_check_rank", "(", "declared_rank", ":", "int", ",", "actual_shape", ":", "Tuple", "[", "int", ",", "...", "]", ")", "->", "bool", ":", "\n", "    ", "if", "declared_rank", "==", "len", "(", "actual_shape", ")", ":", "\n", "        ", "return", "True", "\n", "", "else", ":", "\n", "        ", "raise", "RuntimeError", "(", "\n", "f'Tensor is declared to be Rank-{declared_rank}, '", "\n", "f'but it actually has shape {actual_shape}'", ")", "\n", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.__init__": [[29, 39], ["install", "print"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "ExperimentConfig", ")", ":", "\n", "        ", "self", ".", "tb_global_step", "=", "0", "\n", "self", ".", "custom_stats_format", "=", "None", "\n", "self", ".", "config", "=", "config", "\n", "self", ".", "device", "=", "config", ".", "device", "\n", "try", ":", "\n", "            ", "from", "rich", ".", "traceback", "import", "install", "\n", "install", "(", ")", "\n", "", "except", "ImportError", ":", "\n", "            ", "print", "(", "'Note: Rich text traceback avaliable by pip install rich'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.__enter__": [[40, 81], ["datetime.datetime.datetime.now().strftime", "torch.utils.tensorboard.SummaryWriter", "dataclasses.asdict", "hasattr", "isinstance", "TypeError", "pathlib.Path.exists", "pathlib.Path.mkdir", "print", "print", "str", "open", "dataclasses.asdict.items", "shutil.rmtree", "pathlib.Path.mkdir", "print", "datetime.datetime.datetime.now", "preview_file.write", "config.output_dir.iterdir", "file.name.startswith", "shutil.rmtree", "print"], "methods", ["None"], ["", "", "def", "__enter__", "(", "self", ")", "->", "'Experiment'", ":", "\n", "        ", "config", "=", "self", ".", "config", "\n", "if", "not", "isinstance", "(", "config", ".", "output_dir", ",", "Path", ")", ":", "\n", "            ", "raise", "TypeError", "(", "'config.output_dir must be a pathlib.Path'", ")", "\n", "", "if", "not", "Path", ".", "exists", "(", "config", ".", "output_dir", ")", ":", "\n", "            ", "Path", ".", "mkdir", "(", "config", ".", "output_dir", ",", "parents", "=", "True", ")", "\n", "print", "(", "f'Created new directory {config.output_dir} as output_dir.'", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "f'output_dir = {config.output_dir}'", ")", "\n", "if", "config", ".", "delete_all_exisiting_files_in_output_dir", ":", "\n", "                ", "import", "shutil", "\n", "shutil", ".", "rmtree", "(", "config", ".", "output_dir", ")", "\n", "Path", ".", "mkdir", "(", "config", ".", "output_dir", ",", "parents", "=", "True", ")", "\n", "print", "(", "'config.delete_all_exisiting_files_in_output_dir = True'", ")", "\n", "", "elif", "config", ".", "clear_tensorboard_log_in_output_dir", ":", "\n", "                ", "import", "shutil", "\n", "for", "file", "in", "config", ".", "output_dir", ".", "iterdir", "(", ")", ":", "\n", "                    ", "if", "file", ".", "name", ".", "startswith", "(", "'TB '", ")", ":", "\n", "                        ", "tb_log_dir", "=", "config", ".", "output_dir", "/", "file", ".", "name", "\n", "shutil", ".", "rmtree", "(", "tb_log_dir", ")", "\n", "print", "(", "f'Deleted {file.name} in output_dir'", ")", "\n", "\n", "", "", "", "", "timestamp", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "\"%m-%d %H-%M-%S\"", ")", "\n", "log_dir", "=", "config", ".", "output_dir", "/", "f'TB {timestamp}'", "\n", "self", ".", "tensorboard", "=", "SummaryWriter", "(", "log_dir", "=", "log_dir", ")", "\n", "\n", "config_dict", "=", "asdict", "(", "self", ".", "config", ")", "\n", "config_dict", "[", "'pytorch_version'", "]", "=", "torch", ".", "__version__", "\n", "config_dict", "[", "'cuda_version'", "]", "=", "torch", ".", "version", ".", "cuda", "\n", "config_dict", "[", "'timestamp'", "]", "=", "timestamp", "\n", "if", "hasattr", "(", "self", ",", "'model'", ")", ":", "\n", "            ", "config_dict", "[", "'_model'", "]", "=", "str", "(", "self", ".", "model", ")", "\n", "", "preview_path", "=", "config", ".", "output_dir", "/", "'config.txt'", "\n", "with", "open", "(", "preview_path", ",", "'w'", ")", "as", "preview_file", ":", "\n", "# pprint.pprint(config_dict, preview_file)", "\n", "            ", "for", "key", ",", "val", "in", "config_dict", ".", "items", "(", ")", ":", "\n", "                ", "preview_file", ".", "write", "(", "f'{key} = {val}\\n'", ")", "\n", "# if not isinstance(val, (int, float, str, bool, torch.Tensor)):", "\n", "#     config_dict[key] = str(val)  # for TensorBoard HParams", "\n", "# self.tensorboard.add_hparams(hparam_dict=config_dict, metric_dict={})", "\n", "", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.__exit__": [[82, 91], ["experiment.Experiment.tensorboard.close", "print", "print", "experiment.Experiment.save_everything"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.save_everything"], ["", "@", "no_type_check", "\n", "def", "__exit__", "(", "self", ",", "exception_type", ",", "exception_value", ",", "traceback", ")", "->", "None", ":", "\n", "        ", "if", "exception_type", "is", "not", "None", ":", "\n", "            ", "print", "(", "f'Experiment interrupted.'", ")", "\n", "if", "self", ".", "config", ".", "auto_save_if_interrupted", ":", "\n", "                ", "self", ".", "save_everything", "(", "self", ".", "config", ".", "output_dir", "/", "f'interrupted.pt'", ")", "\n", "", "", "else", ":", "\n", "            ", "print", "(", "'\\n\u2705 Training Complete'", ")", "\n", "", "self", ".", "tensorboard", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.auto_save": [[92, 102], ["experiment.Experiment.save_everything"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.save_everything"], ["", "def", "auto_save", "(", "self", ",", "epoch_index", ":", "int", ")", "->", "None", ":", "\n", "        ", "if", "self", ".", "config", ".", "auto_save_per_epoch", ":", "\n", "            ", "interim_save", "=", "epoch_index", "%", "self", ".", "config", ".", "auto_save_per_epoch", "==", "0", "\n", "", "else", ":", "\n", "            ", "interim_save", "=", "False", "\n", "", "initial_save", "=", "epoch_index", "==", "1", "\n", "final_save", "=", "epoch_index", "==", "self", ".", "config", ".", "num_epochs", "\n", "if", "initial_save", "or", "interim_save", "or", "final_save", ":", "\n", "            ", "self", ".", "save_everything", "(", "\n", "self", ".", "config", ".", "output_dir", "/", "f'epoch{epoch_index}.pt'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.save_state_dict": [[103, 120], ["torch.save", "tqdm.tqdm.tqdm.write", "experiment.Experiment.model.state_dict"], "methods", ["None"], ["", "", "@", "no_type_check", "\n", "def", "save_state_dict", "(", "self", ",", "save_path", ":", "str", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        PyTorch's recommended method of saving a model,\n        but this requires re-instantiate the model object first,\n        along with all of its required arguments, which I find to be finicky;\n        thus save_everything is used by default.\n        \"\"\"", "\n", "cucumbers", "=", "{", "\n", "'model'", ":", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "'config'", ":", "self", ".", "config", ",", "\n", "# TODO needs data to init model too", "\n", "# 'optimizer': self.optimizer.state_dict(),", "\n", "# 'lr_scheduler': self.lr_scheduler.state_dict()", "\n", "}", "\n", "torch", ".", "save", "(", "cucumbers", ",", "save_path", ",", "pickle_protocol", "=", "-", "1", ")", "\n", "tqdm", ".", "write", "(", "f'\ud83d\udcbe state_dict saved to {save_path}\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.reload_state_dict": [[121, 123], ["None"], "methods", ["None"], ["", "def", "reload_state_dict", "(", "self", ")", "->", "Any", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.save_everything": [[124, 139], ["torch.save", "tqdm.tqdm.tqdm.write"], "methods", ["None"], ["", "@", "no_type_check", "\n", "def", "save_everything", "(", "\n", "self", ",", "\n", "save_path", ":", "str", ",", "\n", "# include_data: bool = False", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        PyTorch's non-recommended method of saving an entire model\n        (as opposed to only the state_dict).\n\n        Unfortunately, the Experiment class itself is not picklable,\n        even with the dill module.\n        \"\"\"", "\n", "torch", ".", "save", "(", "self", ".", "model", ",", "save_path", ",", "pickle_protocol", "=", "-", "1", ")", "\n", "tqdm", ".", "write", "(", "f'\ud83d\udcbe Experiment saved to {save_path}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.print_stats": [[140, 155], ["tqdm.tqdm.tqdm.write", "tqdm.tqdm.tqdm.write", "stats.items", "tqdm.tqdm.tqdm.write", "experiment.Experiment.custom_stats_format.format_map", "tqdm.tqdm.tqdm.write"], "methods", ["None"], ["", "def", "print_stats", "(", "\n", "self", ",", "\n", "epoch_index", ":", "int", ",", "\n", "batch_index", ":", "int", ",", "\n", "stats", ":", "Dict", "\n", ")", "->", "None", ":", "\n", "        ", "tqdm", ".", "write", "(", "\n", "f'Epoch {epoch_index} Batch {batch_index:,}:'", ",", "\n", "end", "=", "'\\t'", ")", "\n", "if", "self", ".", "custom_stats_format", ":", "\n", "            ", "tqdm", ".", "write", "(", "self", ".", "custom_stats_format", ".", "format_map", "(", "stats", ")", ")", "\n", "", "else", ":", "\n", "            ", "for", "key", ",", "val", "in", "stats", ".", "items", "(", ")", ":", "\n", "                ", "tqdm", ".", "write", "(", "f'{key} = {val:.3f}'", ",", "end", "=", "'\\t'", ")", "\n", "", "tqdm", ".", "write", "(", "''", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard": [[156, 171], ["stats.items", "stats.items", "experiment.Experiment.tensorboard.add_scalar", "experiment.Experiment.tensorboard.add_scalar"], "methods", ["None"], ["", "", "def", "update_tensorboard", "(", "\n", "self", ",", "\n", "stats", ":", "Dict", "[", "str", ",", "Any", "]", ",", "\n", "manual_step", ":", "Optional", "[", "int", "]", "=", "None", "\n", ")", "->", "None", ":", "\n", "        ", "\"\"\"\n        Cannot simply use **kwargs because TensorBoard uses slashes to\n        organize scope, and slashes are not allowed as Python variable names.\n        \"\"\"", "\n", "if", "manual_step", ":", "\n", "            ", "for", "key", ",", "val", "in", "stats", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "tensorboard", ".", "add_scalar", "(", "key", ",", "val", ",", "manual_step", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "key", ",", "val", "in", "stats", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "tensorboard", ".", "add_scalar", "(", "key", ",", "val", ",", "self", ".", "tb_global_step", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.print_timestamp": [[172, 177], ["datetime.datetime.datetime.now().strftime", "tqdm.tqdm.tqdm.write", "datetime.datetime.datetime.now"], "methods", ["None"], ["", "", "", "def", "print_timestamp", "(", "self", ",", "epoch_index", ":", "int", ")", "->", "None", ":", "\n", "# timestamp = datetime.now().strftime('%-I:%M %p')", "\n", "        ", "timestamp", "=", "datetime", ".", "now", "(", ")", ".", "strftime", "(", "'%X'", ")", "\n", "tqdm", ".", "write", "(", "\n", "f'{timestamp}, '", "\n", "f'Epoch {epoch_index}, '", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.train": [[180, 183], ["None"], "methods", ["None"], ["", "@", "abstractmethod", "\n", "def", "train", "(", "self", ")", "->", "Any", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.uniform_init_embedding": [[198, 207], ["torch.nn.Embedding", "torch.nn.init.uniform_"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "uniform_init_embedding", "(", "\n", "vocab_size", ":", "int", ",", "\n", "embed_size", ":", "int", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "embedding", "=", "nn", ".", "Embedding", "(", "vocab_size", ",", "embed_size", ")", "\n", "init_range", "=", "1.0", "/", "embed_size", "\n", "nn", ".", "init", ".", "uniform_", "(", "embedding", ".", "weight", ".", "data", ",", "-", "init_range", ",", "init_range", ")", "\n", "return", "embedding", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.load_txt_embedding": [[208, 254], ["len", "print", "set", "torch.zeros", "torch.nn.init.uniform_", "word_to_id.items", "torch.nn.Embedding.from_pretrained", "open", "file.readline().split", "int", "print", "print", "print", "line.split.split.split", "list", "torch.tensor", "file.readline", "map", "out_of_vocabulary.add", "int"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "load_txt_embedding", "(", "\n", "in_path", ":", "str", ",", "\n", "word_to_id", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "special_tokens", ":", "Optional", "[", "List", "[", "str", "]", "]", "=", "None", ",", "\n", "verbose", ":", "bool", "=", "True", "\n", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "\"\"\"\n        Load pretrained emebdding from a plain text file, where the first line\n        specifies the vocabulary size and embedding dimensions.\n\n        The word_to_id argument should reflect the tokens in the training corpus,\n        the word ids of the pretrained matrix will be converted to match that\n        of the corpus.\n        \"\"\"", "\n", "real_vocab_size", "=", "len", "(", "word_to_id", ")", "\n", "print", "(", "f'Loading pretrained embedding from {in_path}'", ",", "flush", "=", "True", ")", "\n", "pretrained", ":", "Dict", "[", "str", ",", "List", "[", "float", "]", "]", "=", "{", "}", "\n", "with", "open", "(", "in_path", ")", "as", "file", ":", "\n", "            ", "PE_vocab_size", ",", "embed_size", "=", "file", ".", "readline", "(", ")", ".", "split", "(", ")", "\n", "embed_size", "=", "int", "(", "embed_size", ")", "\n", "print", "(", "f'Pretrained vocab size = {int(PE_vocab_size):,}, '", "\n", "f'embed dim = {embed_size}'", ")", "\n", "for", "line", "in", "file", ":", "\n", "                ", "line", "=", "line", ".", "split", "(", ")", "# type: ignore", "\n", "word", "=", "line", "[", "0", "]", "\n", "pretrained", "[", "word", "]", "=", "list", "(", "map", "(", "float", ",", "line", "[", "-", "embed_size", ":", "]", ")", ")", "\n", "# if len(pretrained) < real_vocab_size:  # len(pretrained) != PE_vocab_size", "\n", "#     print(f'Note: pretrained vocab size = {len(pretrained):,}'", "\n", "#           f' < {real_vocab_size:,} = training corpus vocab size')", "\n", "\n", "", "", "out_of_vocabulary", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "embedding", "=", "torch", ".", "zeros", "(", "real_vocab_size", ",", "embed_size", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "init_range", "=", "1.0", "/", "embed_size", "# initialize matrix for OOV rows", "\n", "nn", ".", "init", ".", "uniform_", "(", "embedding", ",", "-", "init_range", ",", "init_range", ")", "\n", "for", "word", ",", "word_id", "in", "word_to_id", ".", "items", "(", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "vector", "=", "pretrained", "[", "word", "]", "\n", "embedding", "[", "word_id", "]", "=", "torch", ".", "tensor", "(", "vector", ")", "\n", "", "except", "KeyError", ":", "\n", "                ", "out_of_vocabulary", ".", "add", "(", "word", ")", "\n", "", "", "if", "out_of_vocabulary", ":", "\n", "            ", "print", "(", "'\\nThe following words in the training corpus are out of '", "\n", "'the vocabulary of the given pretrained embedding: '", ")", "\n", "print", "(", "out_of_vocabulary", ",", "end", "=", "'\\n\\n'", ")", "\n", "", "return", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embedding", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.trace_memory_allocation.display_top": [[5, 31], ["snapshot.filter_traces.filter_traces", "snapshot.filter_traces.statistics", "print", "enumerate", "print", "os.sep.join", "print", "linecache.getline().strip", "print", "sum", "tracemalloc.Filter", "tracemalloc.Filter", "print", "sum", "frame.filename.split", "linecache.getline", "len"], "function", ["None"], ["def", "display_top", "(", "snapshot", ",", "key_type", "=", "'lineno'", ",", "limit", "=", "10", ")", ":", "\n", "    ", "\"\"\"https://docs.python.org/3/library/tracemalloc.html\"\"\"", "\n", "\n", "# TODO write this as a context manager, then import as library", "\n", "snapshot", "=", "snapshot", ".", "filter_traces", "(", "(", "\n", "tracemalloc", ".", "Filter", "(", "False", ",", "\"<frozen importlib._bootstrap>\"", ")", ",", "\n", "tracemalloc", ".", "Filter", "(", "False", ",", "\"<unknown>\"", ")", ",", ")", ")", "\n", "top_stats", "=", "snapshot", ".", "statistics", "(", "key_type", ")", "\n", "\n", "print", "(", "f'Top {limit} lines'", ")", "\n", "for", "index", ",", "stat", "in", "enumerate", "(", "top_stats", "[", ":", "limit", "]", ",", "1", ")", ":", "\n", "        ", "frame", "=", "stat", ".", "traceback", "[", "0", "]", "\n", "# replace \"/path/to/module/file.py\" with \"module/file.py\"", "\n", "filename", "=", "os", ".", "sep", ".", "join", "(", "frame", ".", "filename", ".", "split", "(", "os", ".", "sep", ")", "[", "-", "2", ":", "]", ")", "\n", "print", "(", "f'#{index}: {filename}:{frame.lineno}: '", "\n", "f'{stat.size / 1048576:.1f} MB'", ")", "\n", "line", "=", "linecache", ".", "getline", "(", "frame", ".", "filename", ",", "frame", ".", "lineno", ")", ".", "strip", "(", ")", "\n", "if", "line", ":", "\n", "            ", "print", "(", "f'    {line}'", ")", "\n", "\n", "", "", "other", "=", "top_stats", "[", "limit", ":", "]", "\n", "if", "other", ":", "\n", "        ", "size", "=", "sum", "(", "stat", ".", "size", "for", "stat", "in", "other", ")", "/", "1048576", "\n", "print", "(", "f\"{len(other)} other: {size:.1f} MB\"", ")", "\n", "", "total", "=", "sum", "(", "stat", ".", "size", "for", "stat", "in", "top_stats", ")", "/", "1048576", "\n", "print", "(", "f'Total allocated size: {total:.1f} MB'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S1_qsub_CoreNLP.parse": [[47, 65], ["subprocess.run", "parse_command.split"], "function", ["None"], ["def", "parse", "(", "path", ")", ":", "\n", "    ", "parse_command", "=", "(", "\n", "'java '", "\n", "f'-Xmx{java_heap_memory} '", "\n", "f'-cp {CoreNLP_installation} '", "\n", "'edu.stanford.nlp.pipeline.StanfordCoreNLP '", "\n", "f'-file {path} '", "\n", "f'-outputDirectory {output_dir} '", "\n", "# '-annotators tokenize,ssplit,pos,lemma,ner,parse '", "\n", "'-annotators tokenize,ssplit,truecase,pos,lemma,ner,parse '", "\n", "'-truecase.overwriteText true '", "\n", "'-ner.applyNumericClassifiers false '", "\n", "'-ner.useSUTime false '", "\n", "# '-ner.combinationMode HIGH_RECALL '", "\n", "f'-parse.maxlen {max_sentence_length} '", "\n", "'-outputFormat json '", "\n", "'-replaceExtension'", ")", "\n", "subprocess", ".", "run", "(", "parse_command", ".", "split", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.search_bill_mentions.export_bill_mentions": [[57, 136], ["tqdm.tqdm.write", "map", "re.compile", "typing.Counter", "tqdm.tqdm.write", "os.path.join", "open", "csv.DictReader", "open", "csv.DictReader", "open", "corpus_file.readline", "re.compile.search", "conglomerate_pattern.search.group", "open", "pickle.dump", "speeches.append", "title_to_bill.values", "search_bill_mentions.Speaker", "line.split", "search_bill_mentions.Speech", "int", "len", "sum", "num_mentions.values"], "function", ["None"], ["def", "export_bill_mentions", "(", "session", ":", "int", ")", "->", "None", ":", "\n", "# load SpeakerMap files for speaker metadata", "\n", "    ", "speakers", ":", "Dict", "[", "str", ",", "Speaker", "]", "=", "{", "}", "\n", "speech_to_speaker", ":", "Dict", "[", "str", ",", "Speaker", "]", "=", "{", "}", "\n", "metadata_path", "=", "f'../../data/raw/daily/{session:0>3d}_SpeakerMap.txt'", "\n", "with", "open", "(", "metadata_path", ")", "as", "metadata_file", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "metadata_file", ",", "delimiter", "=", "'|'", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "if", "row", "[", "'nonvoting'", "]", "==", "'nonvoting'", ":", "\n", "                ", "continue", "\n", "", "if", "row", "[", "'speakerid'", "]", "not", "in", "speakers", ":", "\n", "                ", "speakers", "[", "row", "[", "'speakerid'", "]", "]", "=", "Speaker", "(", "\n", "row", "[", "'speakerid'", "]", ",", "\n", "row", "[", "'lastname'", "]", ",", "\n", "row", "[", "'firstname'", "]", ",", "\n", "row", "[", "'chamber'", "]", ",", "\n", "row", "[", "'state'", "]", ",", "\n", "row", "[", "'gender'", "]", ",", "\n", "row", "[", "'party'", "]", ",", "\n", "row", "[", "'district'", "]", ")", "\n", "", "speech_to_speaker", "[", "row", "[", "'speech_id'", "]", "]", "=", "speakers", "[", "row", "[", "'speakerid'", "]", "]", "\n", "\n", "# load descr_ files for speech date information", "\n", "", "", "speech_to_date", ":", "Dict", "[", "str", ",", "str", "]", "=", "{", "}", "\n", "metadata_path", "=", "f'../../data/raw/daily/descr_{session:0>3d}.txt'", "\n", "with", "open", "(", "metadata_path", ")", "as", "metadata_file", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "metadata_file", ",", "delimiter", "=", "'|'", ")", "\n", "for", "row", "in", "reader", ":", "\n", "            ", "speech_to_date", "[", "row", "[", "'speech_id'", "]", "]", "=", "row", "[", "'date'", "]", "\n", "\n", "# load speech files", "\n", "", "", "speeches", ":", "List", "[", "Speech", "]", "=", "[", "]", "\n", "corpus_path", "=", "f'../../data/raw/daily/speeches_{session:0>3d}.txt'", "\n", "with", "open", "(", "corpus_path", ",", "encoding", "=", "'mac_roman'", ")", "as", "corpus_file", ":", "\n", "        ", "corpus_file", ".", "readline", "(", ")", "# discard header line", "\n", "for", "line", "in", "corpus_file", ":", "\n", "            ", "try", ":", "\n", "                ", "speech_id", ",", "speech_text", "=", "line", ".", "split", "(", "'|'", ")", "\n", "", "except", "ValueError", ":", "# from spliting line with '|'", "\n", "                ", "continue", "\n", "# speech_count += 1", "\n", "", "if", "speech_id", "not", "in", "speech_to_speaker", ":", "\n", "# missing_metadata_count += 1", "\n", "                ", "continue", "\n", "# num_words = len(speech.split())", "\n", "# if num_words < min_speech_length:", "\n", "#     short_speech_count += 1", "\n", "#     continue", "\n", "# word_count += num_words", "\n", "", "speaker", "=", "speech_to_speaker", "[", "speech_id", "]", "\n", "date", "=", "speech_to_date", "[", "speech_id", "]", "\n", "speeches", ".", "append", "(", "\n", "Speech", "(", "speech_id", ",", "date", ",", "speech_text", ",", "speaker", ")", ")", "\n", "\n", "", "", "queries", "=", "[", "\n", "b", ".", "title", "\n", "for", "b", "in", "title_to_bill", ".", "values", "(", ")", "\n", "if", "int", "(", "b", ".", "session", ")", "==", "session", "]", "\n", "tqdm", ".", "write", "(", "f'{session}th Congress: {len(queries)} bills'", ",", "end", "=", "', '", ")", "\n", "queries", "=", "map", "(", "re", ".", "escape", ",", "queries", ")", "# type: ignore", "\n", "conglomerate_pattern", "=", "re", ".", "compile", "(", "f'({\"|\".join(queries)})'", ")", "# type: ignore", "\n", "\n", "num_mentions", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "speech", "in", "speeches", ":", "\n", "# line = line.lower()", "\n", "        ", "match", "=", "conglomerate_pattern", ".", "search", "(", "speech", ".", "text", ")", "\n", "if", "not", "match", ":", "\n", "            ", "continue", "\n", "", "bill_title", "=", "match", ".", "group", "(", "0", ")", "\n", "num_mentions", "[", "bill_title", "]", "+=", "1", "\n", "speech", ".", "mentions_bill", "=", "True", "\n", "speech", ".", "bill", "=", "title_to_bill", "[", "bill_title", "]", "\n", "", "tqdm", ".", "write", "(", "f'{sum(num_mentions.values())} matches'", ")", "\n", "\n", "# IPython.embed()", "\n", "# sys.exit()", "\n", "out_path", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "f'cache_{session}.pickle'", ")", "\n", "with", "open", "(", "out_path", ",", "'wb'", ")", "as", "pickle_file", ":", "\n", "        ", "pickle", ".", "dump", "(", "(", "speeches", ",", "num_mentions", ")", ",", "pickle_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.search_bill_mentions.main": [[138, 144], ["list", "range", "multiprocessing.Pool", "tuple", "tqdm.tqdm", "team.imap_unordered", "len"], "function", ["None"], ["", "", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "scraped_sessions", "=", "list", "(", "range", "(", "97", ",", "115", ")", ")", "# bound 93-115", "\n", "with", "mp", ".", "Pool", "(", "12", ")", "as", "team", ":", "\n", "        ", "_", "=", "tuple", "(", "\n", "tqdm", "(", "team", ".", "imap_unordered", "(", "export_bill_mentions", ",", "scraped_sessions", ")", ",", "\n", "total", "=", "len", "(", "scraped_sessions", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S3_underscore_phrases.underscored_token": [[32, 34], ["regex_match.group().split", "regex_match.group"], "function", ["None"], ["def", "underscored_token", "(", "regex_match", ")", ":", "\n", "    ", "return", "'_'", ".", "join", "(", "regex_match", ".", "group", "(", "0", ")", ".", "split", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S3_underscore_phrases.underscore_phrases": [[36, 67], ["os.path.join", "map", "re.compile", "copy.copy", "copy.copy.append", "os.path.join", "os.path.join", "open", "open", "pickle.load", "speech.text.lower().translate", "pattern.sub.translate", "open", "pickle.dump", "line.split", "map.append", "pattern.sub", "phrase.strip", "speech.text.lower"], "function", ["None"], ["", "def", "underscore_phrases", "(", "session", ":", "int", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    Combine phrases from both parties to underscore.\n    \"\"\"", "\n", "phrases", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "phrases_path", "=", "os", ".", "path", ".", "join", "(", "phrases_dir", ",", "f'{session}.txt'", ")", "\n", "with", "open", "(", "phrases_path", ")", "as", "in_file", ":", "\n", "        ", "for", "line", "in", "in_file", ":", "\n", "            ", "_", ",", "_", ",", "phrase", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "phrases", ".", "append", "(", "phrase", ".", "strip", "(", ")", ")", "\n", "", "", "phrases", "=", "map", "(", "re", ".", "escape", ",", "phrases", ")", "# type: ignore", "\n", "conglomerate_pattern", "=", "re", ".", "compile", "(", "f'({\"|\".join(phrases)})'", ")", "# type: ignore", "\n", "patterns", "=", "copy", ".", "copy", "(", "manual_regex_patterns", ")", "\n", "patterns", ".", "append", "(", "conglomerate_pattern", ")", "\n", "\n", "# bill mentions RegEx results", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "bill_mentions_dir", ",", "f'cache_{session}.pickle'", ")", "\n", "with", "open", "(", "cache_path", ",", "'rb'", ")", "as", "cache_file", ":", "\n", "        ", "speeches", ",", "num_mentions", "=", "pickle", ".", "load", "(", "cache_file", ")", "\n", "\n", "", "for", "speech", "in", "speeches", ":", "\n", "        ", "text", "=", "speech", ".", "text", ".", "lower", "(", ")", ".", "translate", "(", "remove_numbers", ")", "\n", "# text = stopwords_pattern.sub('', text)", "\n", "for", "pattern", "in", "patterns", ":", "\n", "            ", "text", "=", "pattern", ".", "sub", "(", "underscored_token", ",", "text", ")", "\n", "", "text", "=", "text", ".", "translate", "(", "remove_punctutations", ")", "\n", "speech", ".", "text", "=", "text", "\n", "\n", "", "output_path", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "f'underscored_{session}.pickle'", ")", "\n", "with", "open", "(", "output_path", ",", "'wb'", ")", "as", "pickle_file", ":", "\n", "        ", "pickle", ".", "dump", "(", "(", "speeches", ",", "num_mentions", ")", ",", "pickle_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S3_underscore_phrases.main": [[69, 83], ["range", "multiprocessing.Pool", "tuple", "tqdm.tqdm", "team.imap_unordered", "len"], "function", ["None"], ["", "", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "sessions", "=", "range", "(", "97", ",", "115", ")", "# 93", "\n", "# num_threads = 12", "\n", "# phrases_dir = '../../data/interim/aggregated_phrases'", "\n", "# bill_mentions_dir = '../../data/interim/bill_mentions'", "\n", "# output_dir = '../../data/interim/bill_mentions_underscored'", "\n", "# manual_regex_patterns = [", "\n", "#     re.compile(r'the (\\w+ \\b){1,2}(bill|act|amendment|reform)')", "\n", "# ]", "\n", "# print(f'Underscoring {sessions}. Writing to {output_dir}')", "\n", "\n", "with", "mp", ".", "Pool", "(", "12", ")", "as", "team", ":", "\n", "        ", "_", "=", "tuple", "(", "tqdm", "(", "team", ".", "imap_unordered", "(", "underscore_phrases", ",", "sessions", ")", ",", "\n", "total", "=", "len", "(", "sessions", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded.subsampling": [[26, 61], ["sum", "dict", "typing.DefaultDict", "frequency.items", "frequency.items", "ValueError", "frequency.values", "math.sqrt", "math.sqrt"], "function", ["None"], ["def", "subsampling", "(", "\n", "frequency", ":", "Counter", "[", "str", "]", ",", "\n", "heuristic", ":", "Optional", "[", "str", "]", ",", "\n", "threshold", ":", "float", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "    ", "\"\"\"\n    Downsample frequent words.\n\n    Subsampling implementation from annotated C code of Mikolov et al. 2013:\n    http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling\n    This blog post is linked from TensorFlow's website, so authoratative?\n\n    NOTE the default threshold is 1e-3, not 1e-5 as in the paper version\n    \"\"\"", "\n", "cumulative_freq", "=", "sum", "(", "abs_freq", "for", "abs_freq", "in", "frequency", ".", "values", "(", ")", ")", "\n", "keep_prob", ":", "Dict", "[", "str", ",", "float", "]", "=", "dict", "(", ")", "\n", "\n", "if", "heuristic", "is", "None", ":", "\n", "        ", "keep_prob", "=", "DefaultDict", "(", "lambda", ":", "1", ")", "\n", "return", "keep_prob", "\n", "\n", "", "if", "heuristic", "==", "'code'", ":", "\n", "        ", "for", "word_id", ",", "abs_freq", "in", "frequency", ".", "items", "(", ")", ":", "\n", "            ", "rel_freq", "=", "abs_freq", "/", "cumulative_freq", "\n", "keep_prob", "[", "word_id", "]", "=", "(", "\n", "(", "math", ".", "sqrt", "(", "rel_freq", "/", "threshold", ")", "+", "1", ")", "\n", "*", "(", "threshold", "/", "rel_freq", ")", "\n", ")", "\n", "", "", "elif", "heuristic", "==", "'paper'", ":", "\n", "        ", "for", "word_id", ",", "abs_freq", "in", "frequency", ".", "items", "(", ")", ":", "\n", "            ", "rel_freq", "=", "abs_freq", "/", "cumulative_freq", "\n", "keep_prob", "[", "word_id", "]", "=", "math", ".", "sqrt", "(", "threshold", "/", "rel_freq", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown heuristic of subsampling.'", ")", "\n", "", "return", "keep_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded.partition": [[63, 72], ["len"], "function", ["None"], ["", "def", "partition", "(", "speeches", ":", "List", ",", "num_chunks", ":", "int", ")", "->", "Iterable", "[", "List", "]", ":", "\n", "    ", "chunk_size", "=", "len", "(", "speeches", ")", "//", "num_chunks", "\n", "speech_index", "=", "0", "\n", "chunk_index", "=", "0", "\n", "while", "chunk_index", "<=", "num_chunks", "-", "2", ":", "\n", "        ", "yield", "speeches", "[", "speech_index", ":", "speech_index", "+", "chunk_size", "]", "\n", "speech_index", "+=", "chunk_size", "\n", "chunk_index", "+=", "1", "\n", "", "yield", "speeches", "[", "speech_index", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded.export_sorted_frequency": [[74, 92], ["raw_frequency.items", "output_iterable.sort", "open", "output_iterable.append", "out_file.write"], "function", ["None"], ["", "def", "export_sorted_frequency", "(", "\n", "raw_frequency", ":", "Counter", "[", "str", "]", ",", "\n", "subsampled_frequency", ":", "Counter", "[", "str", "]", ",", "\n", "min_freq", ":", "int", ",", "\n", "out_path", ":", "str", "\n", ")", "->", "None", ":", "\n", "    ", "output_iterable", ":", "List", "[", "Tuple", "[", "int", ",", "int", ",", "str", "]", "]", "=", "[", "]", "\n", "for", "word", ",", "raw_freq", "in", "raw_frequency", ".", "items", "(", ")", ":", "\n", "        ", "if", "raw_freq", ">", "min_freq", ":", "\n", "            ", "try", ":", "\n", "                ", "final_freq", "=", "subsampled_frequency", "[", "word", "]", "\n", "", "except", "KeyError", ":", "\n", "                ", "final_freq", "=", "0", "\n", "", "output_iterable", ".", "append", "(", "(", "raw_freq", ",", "final_freq", ",", "word", ")", ")", "\n", "", "", "output_iterable", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "tup", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "out_file", ":", "\n", "        ", "for", "raw_freq", ",", "final_freq", ",", "phrase", "in", "output_iterable", ":", "\n", "            ", "out_file", ".", "write", "(", "f'{raw_freq:,}\\t{final_freq:,}\\t{phrase}\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded.export_sampled_frequency_by_party": [[94, 134], ["output.sort", "output.append", "open", "out_file.write", "open", "out_file.write", "out_file.write", "out_file.write", "float"], "function", ["None"], ["", "", "", "def", "export_sampled_frequency_by_party", "(", "\n", "D_raw", ":", "Counter", "[", "str", "]", ",", "\n", "R_raw", ":", "Counter", "[", "str", "]", ",", "\n", "D_final", ":", "Counter", "[", "str", "]", ",", "\n", "R_final", ":", "Counter", "[", "str", "]", ",", "\n", "word_to_id", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "out_path", ":", "str", "\n", ")", "->", "None", ":", "\n", "\n", "    ", "def", "sort_creteria", "(", "tup", ":", "Tuple", ")", "->", "Tuple", "[", "bool", ",", "float", "]", ":", "\n", "        ", "df", ",", "rf", "=", "tup", "[", "1", "]", ",", "tup", "[", "2", "]", "# initial frequnecy", "\n", "if", "df", "!=", "0", ":", "\n", "            ", "ratio", "=", "rf", "/", "df", "\n", "", "else", ":", "\n", "            ", "ratio", "=", "rf", "/", "1e-8", "\n", "", "nontrivial", "=", "df", "+", "rf", ">", "100", "\n", "return", "nontrivial", ",", "ratio", "\n", "\n", "", "output", "=", "[", "]", "\n", "for", "word", "in", "word_to_id", ":", "\n", "        ", "output", ".", "append", "(", "\n", "(", "word", ",", "D_raw", "[", "word", "]", ",", "R_raw", "[", "word", "]", ",", "D_final", "[", "word", "]", ",", "R_final", "[", "word", "]", ")", ")", "\n", "", "output", ".", "sort", "(", "key", "=", "sort_creteria", ",", "reverse", "=", "True", ")", "\n", "with", "open", "(", "out_path", "+", "'_pretty.txt'", ",", "'w'", ")", "as", "out_file", ":", "\n", "        ", "out_file", ".", "write", "(", "'Sorted by GOP/Dem Ratio    '", "\n", "'Original (Dem, GOP)    '", "\n", "'Sampled & Balanced [Dem, GOP]\\n'", ")", "\n", "for", "word", ",", "dr", ",", "rr", ",", "df", ",", "rf", "in", "output", ":", "\n", "            ", "raw_freq", "=", "f'({dr:,}, {rr:,})'", "\n", "final_freq", "=", "f'[{df:,}, {rf:,}]'", "\n", "out_file", ".", "write", "(", "f'{word:<30}{raw_freq:<20}{final_freq}\\n'", ")", "\n", "\n", "", "", "with", "open", "(", "out_path", "+", "'.tsv'", ",", "'w'", ")", "as", "out_file", ":", "\n", "        ", "out_file", ".", "write", "(", "'D/R_Ratio\\tTotal_Freq\\tD_Freq\\tR_Freq\\tPhrase\\n'", ")", "\n", "for", "word", ",", "dr", ",", "rr", ",", "df", ",", "rf", "in", "output", ":", "\n", "            ", "try", ":", "\n", "                ", "ratio", "=", "dr", "/", "rr", "\n", "", "except", "ZeroDivisionError", ":", "\n", "                ", "ratio", "=", "float", "(", "'inf'", ")", "\n", "", "out_file", ".", "write", "(", "f'{ratio:.5}\\t{dr + rr}\\t{dr}\\t{rr}\\t{word}\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded.build_vocabulary": [[176, 198], ["len", "frequency.items", "print", "word_to_id.items", "len"], "function", ["None"], ["", "", "", "def", "build_vocabulary", "(", "\n", "frequency", ":", "Counter", ",", "\n", "min_frequency", ":", "int", "=", "0", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", "\n", ")", "->", "Tuple", "[", "\n", "Dict", "[", "str", ",", "int", "]", ",", "\n", "Dict", "[", "int", ",", "str", "]", "]", ":", "\n", "    ", "word_to_id", ":", "Dict", "[", "str", ",", "int", "]", "=", "{", "}", "\n", "if", "add_special_tokens", ":", "\n", "        ", "word_to_id", "[", "'[PAD]'", "]", "=", "0", "\n", "word_to_id", "[", "'[UNK]'", "]", "=", "1", "\n", "word_to_id", "[", "'[CLS]'", "]", "=", "2", "\n", "word_to_id", "[", "'[SEP]'", "]", "=", "3", "\n", "", "id_to_word", "=", "{", "val", ":", "key", "for", "key", ",", "val", "in", "word_to_id", ".", "items", "(", ")", "}", "\n", "next_vocab_id", "=", "len", "(", "word_to_id", ")", "\n", "for", "word", ",", "freq", "in", "frequency", ".", "items", "(", ")", ":", "\n", "        ", "if", "word", "not", "in", "word_to_id", "and", "freq", ">=", "min_frequency", ":", "\n", "            ", "word_to_id", "[", "word", "]", "=", "next_vocab_id", "\n", "id_to_word", "[", "next_vocab_id", "]", "=", "word", "\n", "next_vocab_id", "+=", "1", "\n", "", "", "print", "(", "f'Vocabulary size = {len(word_to_id):,}'", ")", "\n", "return", "word_to_id", ",", "id_to_word", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded._export_sorted_frequency_by_party": [[200, 223], ["output.sort", "output.append", "open", "out_file.write", "out_file.write"], "function", ["None"], ["", "def", "_export_sorted_frequency_by_party", "(", "\n", "D_freq", ":", "Counter", "[", "str", "]", ",", "\n", "R_freq", ":", "Counter", "[", "str", "]", ",", "\n", "word_to_id", ":", "Dict", "[", "str", ",", "int", "]", ",", "\n", "out_path", ":", "str", ",", "\n", "min_freq", ":", "int", "\n", ")", "->", "None", ":", "\n", "    ", "output", "=", "[", "]", "\n", "for", "word", "in", "word_to_id", ":", "\n", "        ", "df", "=", "D_freq", "[", "word", "]", "\n", "rf", "=", "R_freq", "[", "word", "]", "\n", "total", "=", "df", "+", "rf", "\n", "above_min_freq", "=", "total", ">", "min_freq", "\n", "dr", "=", "df", "/", "total", "\n", "rr", "=", "rf", "/", "total", "\n", "output", ".", "append", "(", "(", "above_min_freq", ",", "dr", ",", "rr", ",", "df", ",", "rf", ",", "total", ",", "word", ")", ")", "\n", "", "output", ".", "sort", "(", "key", "=", "lambda", "tup", ":", "(", "tup", "[", "0", "]", ",", "tup", "[", "2", "]", ",", "tup", "[", "4", "]", ")", ",", "reverse", "=", "True", ")", "\n", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "out_file", ":", "\n", "        ", "out_file", ".", "write", "(", "\n", "f'd_ratio\\tr_ratio\\td_freq\\tr_freq\\tphrase\\n'", ")", "\n", "for", "above_min_freq", ",", "dr", ",", "rr", ",", "df", ",", "rf", ",", "total", ",", "phrase", "in", "output", ":", "\n", "            ", "out_file", ".", "write", "(", "f'{dr:.5}\\t{rr:.5}\\t{df}\\t{rf}\\t{phrase}\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded.export_sorted_frequency_by_party": [[225, 248], ["typing.Counter", "typing.Counter", "tqdm.tqdm", "S5_proxy_grounded.build_vocabulary", "S5_proxy_grounded._export_sorted_frequency_by_party", "open", "line.split", "D_freq.update", "R_freq.update"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S4_export_train_data.build_vocabulary", "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded._export_sorted_frequency_by_party"], ["", "", "", "def", "export_sorted_frequency_by_party", "(", "\n", "sessions", ":", "Iterable", "[", "int", "]", ",", "\n", "output_dir", ":", "str", ",", "\n", "in_dir", ":", "Path", ",", "\n", "training_min_freq", ":", "int", ",", "\n", "sort_min_freq", ":", "int", "\n", ")", "->", "None", ":", "\n", "    ", "D_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "R_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "session", "in", "tqdm", "(", "sessions", ",", "desc", "=", "'Loading underscored corpora'", ")", ":", "\n", "        ", "for", "party", "in", "(", "'D'", ",", "'R'", ")", ":", "\n", "            ", "underscored_path", "=", "in_dir", "/", "f'underscored_{party}{session}.txt'", "\n", "with", "open", "(", "underscored_path", ",", "'r'", ")", "as", "underscored_corpus", ":", "\n", "                ", "for", "line", "in", "underscored_corpus", ":", "\n", "                    ", "words", "=", "line", ".", "split", "(", ")", "\n", "if", "party", "==", "'D'", ":", "\n", "                        ", "D_freq", ".", "update", "(", "words", ")", "\n", "", "else", ":", "\n", "                        ", "R_freq", ".", "update", "(", "words", ")", "\n", "", "", "", "", "", "word_to_id", ",", "_", "=", "build_vocabulary", "(", "D_freq", "+", "R_freq", ",", "training_min_freq", ")", "\n", "output_path", "=", "output_dir", "/", "'vocab_partisan_frequency.tsv'", "\n", "_export_sorted_frequency_by_party", "(", "\n", "D_freq", ",", "R_freq", ",", "word_to_id", ",", "output_path", ",", "sort_min_freq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded.balance_classes": [[250, 265], ["print", "print", "min", "len", "len", "len", "len", "len", "len", "len", "len", "random.sample", "print", "random.sample", "print", "len", "len"], "function", ["None"], ["", "def", "balance_classes", "(", "socialism", ":", "List", ",", "capitalism", ":", "List", ")", "->", "List", ":", "\n", "    ", "total", "=", "len", "(", "socialism", ")", "+", "len", "(", "capitalism", ")", "\n", "S_ratio", "=", "len", "(", "socialism", ")", "/", "total", "\n", "C_ratio", "=", "len", "(", "capitalism", ")", "/", "total", "\n", "print", "(", "f'Pre-balanced Dem = {len(socialism):,}\\t{S_ratio:.2%}'", ")", "\n", "print", "(", "f'Pre-balanced GOP = {len(capitalism):,}\\t{C_ratio:.2%}'", ")", "\n", "minority", "=", "min", "(", "len", "(", "capitalism", ")", ",", "len", "(", "socialism", ")", ")", "\n", "if", "len", "(", "capitalism", ")", ">", "len", "(", "socialism", ")", ":", "\n", "        ", "capitalism", "=", "random", ".", "sample", "(", "capitalism", ",", "k", "=", "minority", ")", "\n", "print", "(", "f'Balancing training data by sampling GOP to {minority:,}.'", ")", "\n", "", "else", ":", "\n", "        ", "socialism", "=", "random", ".", "sample", "(", "socialism", ",", "k", "=", "minority", ")", "\n", "print", "(", "f'Balancing training data by sampling Dem to {minority:,}.'", ")", "\n", "", "gridlock", "=", "socialism", "+", "capitalism", "\n", "return", "gridlock", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded.faux_sent_tokenize": [[267, 281], ["len", "len"], "function", ["None"], ["", "def", "faux_sent_tokenize", "(", "\n", "tokens", ":", "List", ",", "\n", "fixed_sent_len", ":", "int", ",", "\n", "min_sent_len", ":", "int", "\n", ")", "->", "Iterable", "[", "List", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"partion a document into fixed-length faux sentences\"\"\"", "\n", "start_index", "=", "0", "\n", "while", "(", "start_index", "+", "fixed_sent_len", ")", "<", "(", "len", "(", "tokens", ")", "-", "1", ")", ":", "\n", "        ", "yield", "tokens", "[", "start_index", ":", "start_index", "+", "fixed_sent_len", "]", "\n", "start_index", "+=", "fixed_sent_len", "\n", "\n", "", "trailing_words", "=", "tokens", "[", "start_index", ":", "-", "1", "]", "\n", "if", "len", "(", "trailing_words", ")", ">=", "min_sent_len", ":", "\n", "        ", "yield", "trailing_words", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S5_proxy_grounded.main": [[283, 468], ["pathlib.Path.mkdir", "open", "print", "print", "print", "print", "print", "print", "print", "typing.Counter", "tqdm.tqdm", "sum", "print", "print", "typing.Counter", "norm_freq.items", "print", "S5_proxy_grounded.subsampling", "typing.Counter", "tqdm.tqdm", "print", "print", "S5_proxy_grounded.build_vocabulary", "tqdm.tqdm", "set", "ground.values", "random.sample", "sum", "len", "random.shuffle", "print", "random.sample", "open.write", "open.write", "final_freq.most_common", "open.close", "print", "sum", "S5_proxy_grounded.faux_sent_tokenize", "sum", "open", "file.write", "ground.values", "print", "negative_sampling_probs.get", "open", "pickle.dump", "print", "final_freq.update", "doc.sentences.append", "gw.cono.values", "random.sample.add", "open", "print", "len", "random.sample", "random.shuffle", "len", "open", "open", "final_freq.items", "range", "print", "print", "print", "open", "norm_freq.values", "len", "len", "random.random", "subsampled_words.append", "data.Sentence", "len", "sum", "len", "gw.cono.most_common", "print", "print", "print", "final_freq.values", "line.split", "norm_freq.update", "corpus.append", "norm_freq.values", "data.GroundedWord", "random.sample.append", "len", "data.LabeledDoc", "typing.Counter", "final_freq.values"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S4_export_train_data.subsampling", "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S4_export_train_data.build_vocabulary", "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S4_ideal_grounded.faux_sent_tokenize"], ["", "", "def", "main", "(", "\n", "sessions", ":", "Iterable", "[", "int", "]", ",", "\n", "in_dir", ":", "Path", ",", "\n", "out_dir", ":", "Path", ",", "\n", "subsampling_implementation", ":", "Optional", "[", "str", "]", ",", "\n", "subsampling_threshold", ":", "float", ",", "\n", "min_word_freq", ":", "int", ",", "\n", "min_sent_len", ":", "int", ",", "\n", "fixed_sent_len", ":", "int", ",", "\n", "eval_min_freq", ":", "int", ",", "\n", "eval_R_thresholds", ":", "Iterable", "[", "float", "]", ",", "\n", "eval_num_random_samples", ":", "int", ",", "\n", "conserve_RAM", ":", "bool", "\n", ")", "->", "None", ":", "\n", "    ", "Path", ".", "mkdir", "(", "out_dir", ",", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "preview", "=", "open", "(", "out_dir", "/", "f'preview.txt'", ",", "'w'", ")", "\n", "print", "(", "f'Reading sessions {sessions}. Writing to {out_dir}'", ")", "\n", "print", "(", "f'Reading sessions {sessions}. Writing to {out_dir}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Min word frequency = {min_word_freq}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Min sentence length = {min_sent_len}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Faux sentence fixed length = {fixed_sent_len}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'SGNS subsample implementation= {subsampling_implementation}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'SGNS subsample threshold = {subsampling_threshold}'", ",", "file", "=", "preview", ")", "\n", "\n", "corpus", ":", "List", "[", "LabeledDoc", "]", "=", "[", "]", "\n", "norm_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "session", "in", "tqdm", "(", "\n", "sessions", ",", "\n", "desc", "=", "'Loading multi-word expression underscored pickles...'", ")", ":", "\n", "        ", "for", "party", "in", "(", "'D'", ",", "'R'", ")", ":", "\n", "            ", "in_path", "=", "in_dir", "/", "f'underscored_{party}{session}.txt'", "\n", "with", "open", "(", "in_path", ")", "as", "underscored_corpus", ":", "\n", "                ", "for", "line", "in", "underscored_corpus", ":", "\n", "                    ", "underscored_tokens", "=", "line", ".", "split", "(", ")", "\n", "norm_freq", ".", "update", "(", "underscored_tokens", ")", "\n", "corpus", ".", "append", "(", "LabeledDoc", "(", "\n", "uid", "=", "None", ",", "\n", "title", "=", "None", ",", "\n", "url", "=", "None", ",", "\n", "party", "=", "party", ",", "\n", "referent", "=", "None", ",", "\n", "text", "=", "underscored_tokens", ",", "\n", "date", "=", "None", ",", "\n", "sentences", "=", "[", "]", ")", ")", "\n", "", "", "", "", "cumulative_freq", "=", "sum", "(", "freq", "for", "freq", "in", "norm_freq", ".", "values", "(", ")", ")", "\n", "print", "(", "f'Noramlized vocabulary size = {len(norm_freq):,}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Number of words = {cumulative_freq:,}'", ",", "file", "=", "preview", ")", "\n", "\n", "# Filter counter with MIN_FREQ and count UNK", "\n", "UNK_filtered_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "key", ",", "val", "in", "norm_freq", ".", "items", "(", ")", ":", "\n", "        ", "if", "val", ">=", "min_word_freq", ":", "\n", "            ", "UNK_filtered_freq", "[", "key", "]", "=", "val", "\n", "", "else", ":", "\n", "            ", "UNK_filtered_freq", "[", "'[UNK]'", "]", "+=", "val", "\n", "", "", "print", "(", "f'Filtered vocabulary size = {len(UNK_filtered_freq):,}'", ",", "file", "=", "preview", ")", "\n", "assert", "sum", "(", "freq", "for", "freq", "in", "norm_freq", ".", "values", "(", ")", ")", "==", "cumulative_freq", "\n", "\n", "# Subsampling & filter by min/max sentence length", "\n", "keep_prob", "=", "subsampling", "(", "\n", "UNK_filtered_freq", ",", "subsampling_implementation", ",", "subsampling_threshold", ")", "\n", "ground", ":", "Dict", "[", "str", ",", "GroundedWord", "]", "=", "{", "}", "\n", "final_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "'Subsampling frequent words'", ")", ":", "\n", "        ", "subsampled_words", "=", "[", "]", "\n", "for", "token", "in", "doc", ".", "text", ":", "\n", "            ", "if", "token", "in", "discard", ":", "\n", "                ", "continue", "\n", "", "if", "token", "not", "in", "UNK_filtered_freq", ":", "\n", "                ", "token", "=", "'[UNK]'", "\n", "", "if", "random", ".", "random", "(", ")", "<", "keep_prob", "[", "token", "]", ":", "\n", "                ", "subsampled_words", ".", "append", "(", "token", ")", "\n", "\n", "", "", "for", "faux_sent", "in", "faux_sent_tokenize", "(", "\n", "subsampled_words", ",", "fixed_sent_len", ",", "min_sent_len", ")", ":", "\n", "            ", "final_freq", ".", "update", "(", "faux_sent", ")", "\n", "doc", ".", "sentences", ".", "append", "(", "Sentence", "(", "subsampled_tokens", "=", "faux_sent", ")", ")", "\n", "for", "word", "in", "faux_sent", ":", "\n", "                ", "if", "word", "not", "in", "ground", ":", "\n", "                    ", "ground", "[", "word", "]", "=", "GroundedWord", "(", "\n", "text", "=", "word", ",", "deno", "=", "None", ",", "cono", "=", "Counter", "(", "{", "doc", ".", "party", ":", "1", "}", ")", ")", "\n", "", "else", ":", "\n", "                    ", "ground", "[", "word", "]", ".", "cono", "[", "doc", ".", "party", "]", "+=", "1", "\n", "\n", "", "", "", "if", "conserve_RAM", ":", "\n", "            ", "doc", ".", "text", "=", "None", "\n", "# End looping documents", "\n", "", "", "print", "(", "f'Final vocabulary size = {len(final_freq):,}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Subsampled number of words = '", "\n", "f'{sum(freq for freq in final_freq.values()):,}'", ",", "file", "=", "preview", ")", "\n", "\n", "# Filter out empty documents", "\n", "corpus", "=", "[", "doc", "for", "doc", "in", "corpus", "if", "len", "(", "doc", ".", "sentences", ")", ">", "0", "]", "\n", "\n", "# Numericalize corpus by word_ids", "\n", "word_to_id", ",", "id_to_word", "=", "build_vocabulary", "(", "final_freq", ")", "\n", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "'Converting to word ids'", ")", ":", "\n", "        ", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "            ", "sent", ".", "numerical_tokens", "=", "[", "\n", "word_to_id", "[", "token", "]", "for", "token", "in", "sent", ".", "subsampled_tokens", "]", "\n", "if", "conserve_RAM", ":", "\n", "                ", "sent", ".", "subsampled_tokens", "=", "None", "\n", "\n", "# Prepare grounding for intrinsic evaluation", "\n", "", "", "", "random_eval_words", "=", "set", "(", ")", "\n", "for", "gw", "in", "ground", ".", "values", "(", ")", ":", "\n", "        ", "gw", ".", "majority_cono", "=", "gw", ".", "cono", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "\n", "gw", ".", "freq", "=", "sum", "(", "gw", ".", "cono", ".", "values", "(", ")", ")", "\n", "gw", ".", "R_ratio", "=", "gw", ".", "cono", "[", "'R'", "]", "/", "gw", ".", "freq", "\n", "if", "gw", ".", "freq", ">=", "eval_min_freq", ":", "\n", "            ", "random_eval_words", ".", "add", "(", "gw", ".", "text", ")", "\n", "", "", "random_eval_words", "=", "random", ".", "sample", "(", "random_eval_words", ",", "eval_num_random_samples", ")", "\n", "with", "open", "(", "out_dir", "/", "f'eval_words_random.txt'", ",", "'w'", ")", "as", "file", ":", "\n", "        ", "file", ".", "write", "(", "'\\n'", ".", "join", "(", "random_eval_words", ")", ")", "\n", "\n", "", "for", "R_threshold", "in", "eval_R_thresholds", ":", "\n", "        ", "D_threshold", "=", "1", "-", "R_threshold", "\n", "partisan_eval_words", "=", "[", "]", "\n", "for", "gw", "in", "ground", ".", "values", "(", ")", ":", "\n", "            ", "if", "gw", ".", "freq", ">=", "eval_min_freq", ":", "\n", "                ", "if", "gw", ".", "R_ratio", ">=", "R_threshold", "or", "gw", ".", "R_ratio", "<=", "D_threshold", ":", "\n", "                    ", "partisan_eval_words", ".", "append", "(", "gw", ")", "\n", "", "", "", "print", "(", "f'{len(partisan_eval_words)} partisan eval words '", "\n", "f'with R_threshold = {R_threshold}'", ",", "file", "=", "preview", ")", "\n", "\n", "out_path", "=", "out_dir", "/", "f'inspect_{R_threshold}_partisan.tsv'", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "file", ":", "\n", "            ", "print", "(", "'word\\tfreq\\tR_ratio'", ",", "file", "=", "file", ")", "\n", "for", "gw", "in", "partisan_eval_words", ":", "\n", "                ", "print", "(", "gw", ".", "text", ",", "gw", ".", "freq", ",", "gw", ".", "R_ratio", ",", "sep", "=", "'\\t'", ",", "file", "=", "file", ")", "\n", "\n", "", "", "if", "len", "(", "partisan_eval_words", ")", ">", "2", "*", "eval_num_random_samples", ":", "\n", "            ", "partisan_eval_words", "=", "random", ".", "sample", "(", "\n", "partisan_eval_words", ",", "2", "*", "eval_num_random_samples", ")", "\n", "", "else", ":", "\n", "            ", "random", ".", "shuffle", "(", "partisan_eval_words", ")", "\n", "\n", "", "mid", "=", "len", "(", "partisan_eval_words", ")", "//", "2", "\n", "with", "open", "(", "out_dir", "/", "f'{R_threshold}partisan_dev_words.txt'", ",", "'w'", ")", "as", "file", ":", "\n", "            ", "for", "gw", "in", "partisan_eval_words", "[", ":", "mid", "]", ":", "\n", "                ", "print", "(", "gw", ".", "text", ",", "file", "=", "file", ")", "\n", "", "", "with", "open", "(", "out_dir", "/", "f'{R_threshold}partisan_test_words.txt'", ",", "'w'", ")", "as", "file", ":", "\n", "            ", "for", "gw", "in", "partisan_eval_words", "[", "mid", ":", "]", ":", "\n", "                ", "print", "(", "gw", ".", "text", ",", "file", "=", "file", ")", "\n", "\n", "# Helper for negative sampling", "\n", "", "", "", "cumulative_freq", "=", "sum", "(", "freq", "**", "0.75", "for", "freq", "in", "final_freq", ".", "values", "(", ")", ")", "\n", "negative_sampling_probs", ":", "Dict", "[", "int", ",", "float", "]", "=", "{", "\n", "word_to_id", "[", "word", "]", ":", "(", "freq", "**", "0.75", ")", "/", "cumulative_freq", "\n", "for", "word", ",", "freq", "in", "final_freq", ".", "items", "(", ")", "}", "\n", "vocab_size", "=", "len", "(", "word_to_id", ")", "\n", "negative_sampling_probs", ":", "List", "[", "float", "]", "=", "[", "\n", "# negative_sampling_probs[word_id]  # strict", "\n", "negative_sampling_probs", ".", "get", "(", "word_id", ",", "0", ")", "# prob = 0 if missing vocab", "\n", "for", "word_id", "in", "range", "(", "vocab_size", ")", "]", "\n", "\n", "random", ".", "shuffle", "(", "corpus", ")", "\n", "cucumbers", "=", "{", "\n", "'word_to_id'", ":", "word_to_id", ",", "\n", "'id_to_word'", ":", "id_to_word", ",", "\n", "'ground'", ":", "ground", ",", "\n", "'negative_sampling_probs'", ":", "negative_sampling_probs", ",", "\n", "'documents'", ":", "corpus", "}", "\n", "print", "(", "f'Writing to {out_dir}'", ")", "\n", "with", "open", "(", "out_dir", "/", "'train.pickle'", ",", "'wb'", ")", "as", "out_file", ":", "\n", "        ", "pickle", ".", "dump", "(", "cucumbers", ",", "out_file", ",", "protocol", "=", "-", "1", ")", "\n", "\n", "# Print out vocabulary & some random sentences for sanity check", "\n", "", "docs", "=", "random", ".", "sample", "(", "corpus", ",", "100", ")", "\n", "preview", ".", "write", "(", "'\\n'", ")", "\n", "for", "doc", "in", "docs", ":", "\n", "        ", "sent", "=", "doc", ".", "sentences", "[", "0", "]", "\n", "if", "not", "conserve_RAM", ":", "\n", "# print(sent.tokens, file=preview)", "\n", "# print(sent.normalized_tokens, file=preview)", "\n", "            ", "print", "(", "sent", ".", "subsampled_tokens", ",", "file", "=", "preview", ")", "\n", "print", "(", "sent", ".", "numerical_tokens", ",", "file", "=", "preview", ",", "end", "=", "'\\n\\n'", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "sent", ".", "numerical_tokens", ",", "file", "=", "preview", ")", "\n", "# print(vars(doc), end='\\n\\n', file=preview)", "\n", "", "", "preview", ".", "write", "(", "'\\n\\nfinal_freq\\tword\\n'", ")", "\n", "for", "key", ",", "val", "in", "final_freq", ".", "most_common", "(", ")", ":", "\n", "        ", "print", "(", "f'{val:,}\\t{ground[key]}'", ",", "file", "=", "preview", ")", "\n", "", "preview", ".", "close", "(", ")", "\n", "print", "(", "'All set!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S0_partition_corpus.partition_corpus": [[13, 97], ["dict", "enumerate", "enumerate", "open", "csv.DictReader", "open", "corpus_file.readline", "S0_partition_corpus.partition_corpus.decimate"], "function", ["None"], ["def", "partition_corpus", "(", "\n", "session", ":", "int", ",", "\n", "corpus_path", ":", "str", ",", "\n", "metadata_path", ":", "str", ",", "\n", "output_dir", ":", "str", ",", "\n", "min_speech_length", ":", "int", ",", "\n", "num_chunks", ":", "int", ",", "\n", "input_encoding", ":", "str", "\n", ")", "->", "Tuple", "[", "int", ",", "int", ",", "int", ",", "int", "]", ":", "\n", "    ", "\"\"\"\n    Partition a large corpus into multiple smaller chunks.\n\n    Albert Webson [21:27]\n    This [grid] job since noon is the full 200 MB. I just submitted a quartered\n    50 MB job a couple hours ago. If that still doesn\u2019t work, I will octo-...\n    How is there no verb for \u201cdivide into 8 parts\u201d? There is \u201cdecimate\u201d,\n    although it doesn\u2019t mean \u201cdivide into 10 parts\u201d because its etymology is\n    the Roman punishment for mutiny by executing every one in ten soldiers.\n\n    Ellie Pavlick [21:30]\n    i officially declare \u201cdecimate\u201d means \u201csplit the job into 10 parallel jobs.\u201d\n    \u201cyour parsing job is slow? did you try decimating it?\u201d\n    \"\"\"", "\n", "\n", "def", "decimate", "(", "speeches", ":", "List", "[", "str", "]", ")", "->", "Iterable", "[", "List", "[", "str", "]", "]", ":", "\n", "        ", "chunk_size", "=", "len", "(", "speeches", ")", "//", "num_chunks", "\n", "speech_index", "=", "0", "\n", "chunk_index", "=", "0", "\n", "while", "chunk_index", "<=", "num_chunks", "-", "2", ":", "\n", "            ", "yield", "speeches", "[", "speech_index", ":", "speech_index", "+", "chunk_size", "]", "\n", "speech_index", "+=", "chunk_size", "\n", "chunk_index", "+=", "1", "\n", "", "yield", "speeches", "[", "speech_index", ":", "-", "1", "]", "\n", "\n", "", "speech_id_to_party", ":", "Dict", "[", "str", ",", "str", "]", "=", "dict", "(", ")", "\n", "with", "open", "(", "metadata_path", ")", "as", "metadata_file", ":", "\n", "        ", "reader", "=", "csv", ".", "DictReader", "(", "metadata_file", ",", "delimiter", "=", "'|'", ")", "\n", "for", "speaker_data", "in", "reader", ":", "\n", "            ", "if", "speaker_data", "[", "'nonvoting'", "]", "==", "'nonvoting'", ":", "\n", "                ", "continue", "\n", "", "speech_id_to_party", "[", "speaker_data", "[", "'speech_id'", "]", "]", "=", "speaker_data", "[", "'party'", "]", "\n", "\n", "", "", "dem_corpus", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "gop_corpus", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "speech_count", "=", "0", "\n", "word_count", "=", "0", "\n", "short_speech_count", "=", "0", "\n", "missing_metadata_count", "=", "0", "\n", "\n", "with", "open", "(", "corpus_path", ",", "encoding", "=", "input_encoding", ")", "as", "corpus_file", ":", "\n", "        ", "corpus_file", ".", "readline", "(", ")", "# discard header line", "\n", "for", "line", "in", "corpus_file", ":", "\n", "            ", "try", ":", "\n", "                ", "speech_id", ",", "speech", "=", "line", ".", "split", "(", "'|'", ")", "\n", "", "except", "ValueError", ":", "# from spliting line with '|'", "\n", "                ", "continue", "\n", "", "speech_count", "+=", "1", "\n", "if", "speech_id", "not", "in", "speech_id_to_party", ":", "\n", "                ", "missing_metadata_count", "+=", "1", "\n", "continue", "\n", "", "num_words", "=", "len", "(", "speech", ".", "split", "(", ")", ")", "\n", "if", "num_words", "<", "min_speech_length", ":", "\n", "                ", "short_speech_count", "+=", "1", "\n", "continue", "\n", "", "word_count", "+=", "num_words", "\n", "party", "=", "speech_id_to_party", "[", "speech_id", "]", "\n", "if", "party", "==", "'D'", ":", "\n", "                ", "dem_corpus", ".", "append", "(", "speech", ")", "\n", "", "elif", "party", "==", "'R'", ":", "\n", "                ", "gop_corpus", ".", "append", "(", "speech", ")", "\n", "\n", "", "", "", "for", "chunk_index", ",", "corpus_chunk", "in", "enumerate", "(", "decimate", "(", "dem_corpus", ")", ")", ":", "\n", "        ", "out_path", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "f'{session}_D{chunk_index}.txt'", ")", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "out_file", ":", "\n", "            ", "for", "speech", "in", "corpus_chunk", ":", "\n", "                ", "out_file", ".", "write", "(", "speech", ")", "\n", "\n", "", "", "", "for", "chunk_index", ",", "corpus_chunk", "in", "enumerate", "(", "decimate", "(", "gop_corpus", ")", ")", ":", "\n", "        ", "out_path", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "f'{session}_R{chunk_index}.txt'", ")", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "out_file", ":", "\n", "            ", "for", "speech", "in", "corpus_chunk", ":", "\n", "                ", "out_file", ".", "write", "(", "speech", ")", "\n", "\n", "", "", "", "return", "speech_count", ",", "word_count", ",", "short_speech_count", ",", "missing_metadata_count", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S0_partition_corpus.partition_jsonl_corpus": [[99, 173], ["collections.Counter", "print", "enumerate", "enumerate", "open", "S0_partition_corpus.partition_corpus.decimate"], "function", ["None"], ["", "def", "partition_jsonl_corpus", "(", "\n", "corpus_path", ":", "str", ",", "\n", "output_dir", ":", "str", ",", "\n", "num_chunks", ":", "int", ",", "\n", "input_encoding", ":", "str", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    For the corpus scraped from the Presidency Project at UC Santa Barbara.\n    Partition a single JSONL corpus into multiple smaller chunks.\n\n    No checking minimum speech length, since they're all reasonably long.\n    \"\"\"", "\n", "\n", "def", "decimate", "(", "speeches", ":", "List", "[", "str", "]", ")", "->", "Iterable", "[", "List", "[", "str", "]", "]", ":", "\n", "        ", "chunk_size", "=", "len", "(", "speeches", ")", "//", "num_chunks", "\n", "speech_index", "=", "0", "\n", "chunk_index", "=", "0", "\n", "while", "chunk_index", "<=", "num_chunks", "-", "2", ":", "\n", "            ", "yield", "speeches", "[", "speech_index", ":", "speech_index", "+", "chunk_size", "]", "\n", "speech_index", "+=", "chunk_size", "\n", "chunk_index", "+=", "1", "\n", "", "yield", "speeches", "[", "speech_index", ":", "-", "1", "]", "\n", "\n", "", "polisci101", "=", "{", "\n", "'Harry S. Truman'", ":", "0", ",", "\n", "'Dwight D. Eisenhower'", ":", "1", ",", "\n", "'John F. Kennedy'", ":", "0", ",", "\n", "'Lyndon B. Johnson'", ":", "0", ",", "\n", "'Richard Nixon'", ":", "1", ",", "\n", "'Gerald R. Ford'", ":", "1", ",", "\n", "'Jimmy Carter'", ":", "0", ",", "\n", "'Ronald Reagan'", ":", "1", ",", "\n", "'George Bush'", ":", "1", ",", "\n", "'William J. Clinton'", ":", "0", ",", "\n", "'George W. Bush'", ":", "1", ",", "\n", "'Barack Obama'", ":", "0", ",", "\n", "'Donald J. Trump'", ":", "1", "\n", "}", "\n", "\n", "dem_corpus", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "gop_corpus", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "speech_count", "=", "0", "\n", "word_count", "=", "0", "\n", "\n", "skipped", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "with", "open", "(", "corpus_path", ",", "'r'", ")", "as", "jsonl_file", ":", "\n", "        ", "for", "line", "in", "jsonl_file", ":", "\n", "            ", "json_obj", "=", "json", ".", "loads", "(", "line", ")", "\n", "name", "=", "json_obj", "[", "'person'", "]", "\n", "if", "name", "not", "in", "polisci101", ":", "\n", "                ", "skipped", "[", "name", "]", "+=", "1", "\n", "continue", "\n", "", "try", ":", "\n", "                ", "speech", "=", "'\\n'", ".", "join", "(", "json_obj", "[", "'speech'", "]", ")", "\n", "", "except", "KeyError", ":", "\n", "                ", "continue", "\n", "", "party", "=", "polisci101", "[", "name", "]", "\n", "if", "party", "==", "0", ":", "\n", "                ", "dem_corpus", ".", "append", "(", "speech", ")", "\n", "", "else", ":", "\n", "                ", "gop_corpus", ".", "append", "(", "speech", ")", "\n", "", "", "", "print", "(", "'Skipped the following undefined people: '", ",", "skipped", ")", "\n", "\n", "for", "chunk_index", ",", "corpus_chunk", "in", "enumerate", "(", "decimate", "(", "dem_corpus", ")", ")", ":", "\n", "        ", "out_path", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "f'D{chunk_index}.txt'", ")", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "out_file", ":", "\n", "            ", "for", "speech", "in", "corpus_chunk", ":", "\n", "                ", "out_file", ".", "write", "(", "speech", ")", "\n", "\n", "", "", "", "for", "chunk_index", ",", "corpus_chunk", "in", "enumerate", "(", "decimate", "(", "gop_corpus", ")", ")", ":", "\n", "        ", "out_path", "=", "os", ".", "path", ".", "join", "(", "output_dir", ",", "f'R{chunk_index}.txt'", ")", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "out_file", ":", "\n", "            ", "for", "speech", "in", "corpus_chunk", ":", "\n", "                ", "out_file", ".", "write", "(", "speech", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S0_partition_corpus.speech_length_histogram": [[175, 255], ["defaultdict", "tqdm.tqdm", "dict", "tqdm.tqdm.write", "matplotlib.subplots", "seaborn.distplot", "sns.distplot.legend", "fig.savefig", "open", "csv.DictReader", "open", "corpus_file.readline", "len", "ValueError", "line.split", "len", "speeches_length[].append", "speech.split", "speeches_length[].append", "speeches_length[].append", "speeches_length[].append", "speeches_length[].append", "speeches_length[].append", "print", "speeches_length[].append", "print"], "function", ["None"], ["", "", "", "", "def", "speech_length_histogram", "(", "\n", "sessions", ":", "Iterable", "[", "int", "]", ",", "\n", "histogram_upper_bound", ":", "int", "=", "50", ",", "\n", "metadata_of_interest", ":", "Set", "[", "str", "]", "=", "{", "'party'", ",", "'chamber'", ",", "'gender'", ",", "'state'", "}", ",", "\n", "identities", ":", "Set", "[", "str", "]", "=", "{", "'Dem'", ",", "'GOP'", ",", "'Senate'", ",", "'House'", ",", "'Male'", ",", "'Female'", "}", "\n", ")", "->", "None", ":", "\n", "\n", "    ", "speeches_length", ":", "defaultdict", "[", "str", ",", "List", "[", "int", "]", "]", "=", "defaultdict", "(", "list", ")", "\n", "for", "session_index", "in", "tqdm", "(", "sessions", ")", ":", "\n", "        ", "metadata", ":", "Dict", "[", "str", ",", "Dict", "[", "str", ",", "str", "]", "]", "=", "dict", "(", ")", "\n", "metadata_path", "=", "f'corpora/bound/{session_index:0>3d}_SpeakerMap.txt'", "\n", "with", "open", "(", "metadata_path", ")", "as", "metadata_file", ":", "\n", "            ", "reader", "=", "csv", ".", "DictReader", "(", "metadata_file", ",", "delimiter", "=", "'|'", ")", "\n", "for", "speaker_data", "in", "reader", ":", "\n", "                ", "if", "speaker_data", "[", "'nonvoting'", "]", "==", "'nonvoting'", ":", "\n", "                    ", "continue", "\n", "", "speaker", ":", "Dict", "[", "str", ",", "str", "]", "=", "{", "\n", "attribute", ":", "speaker_data", "[", "attribute", "]", "\n", "for", "attribute", "in", "metadata_of_interest", "}", "\n", "metadata", "[", "speaker_data", "[", "'speech_id'", "]", "]", "=", "speaker", "\n", "\n", "", "", "speech_count", "=", "0", "\n", "missing_metadata_count", "=", "0", "\n", "corpus_path", "=", "f'corpora/bound/speeches_{session_index:0>3d}.txt'", "\n", "with", "open", "(", "corpus_path", ",", "encoding", "=", "input_encoding", ")", "as", "corpus_file", ":", "\n", "            ", "corpus_file", ".", "readline", "(", ")", "# discard header line", "\n", "for", "line", "in", "corpus_file", ":", "\n", "                ", "try", ":", "\n", "                    ", "speech_id", ",", "speech", "=", "line", ".", "split", "(", "'|'", ")", "\n", "speech_count", "+=", "1", "\n", "if", "speech_id", "not", "in", "metadata", ":", "\n", "                        ", "missing_metadata_count", "+=", "1", "\n", "continue", "\n", "\n", "", "speaker", "=", "metadata", "[", "speech_id", "]", "\n", "party", "=", "speaker", "[", "'party'", "]", "\n", "chamber", "=", "speaker", "[", "'chamber'", "]", "\n", "gender", "=", "speaker", "[", "'gender'", "]", "\n", "state", "=", "speaker", "[", "'state'", "]", "\n", "\n", "speech_length", "=", "len", "(", "speech", ".", "split", "(", ")", ")", "\n", "\n", "speeches_length", "[", "state", "]", ".", "append", "(", "speech_length", ")", "\n", "if", "party", "==", "'D'", ":", "\n", "                        ", "speeches_length", "[", "'Dem'", "]", ".", "append", "(", "speech_length", ")", "\n", "", "elif", "party", "==", "'R'", ":", "\n", "                        ", "speeches_length", "[", "'GOP'", "]", ".", "append", "(", "speech_length", ")", "\n", "# else:", "\n", "#     print('Spoiler effect:', party)", "\n", "\n", "", "if", "chamber", "==", "'S'", ":", "\n", "                        ", "speeches_length", "[", "'Senate'", "]", ".", "append", "(", "speech_length", ")", "\n", "", "elif", "chamber", "==", "'H'", ":", "\n", "                        ", "speeches_length", "[", "'House'", "]", ".", "append", "(", "speech_length", ")", "\n", "", "else", ":", "\n", "                        ", "print", "(", "'Bicameralism is bad enough:'", ",", "chamber", ")", "\n", "\n", "", "if", "gender", "==", "'M'", ":", "\n", "                        ", "speeches_length", "[", "'Male'", "]", ".", "append", "(", "speech_length", ")", "\n", "", "elif", "gender", "==", "'F'", ":", "\n", "                        ", "speeches_length", "[", "'Female'", "]", ".", "append", "(", "speech_length", ")", "\n", "", "else", ":", "\n", "                        ", "print", "(", "'Nonbinary:'", ")", "\n", "", "", "except", "ValueError", ":", "# from spliting line with '|'", "\n", "                    ", "continue", "\n", "", "", "", "missing_metadata_ratio", "=", "missing_metadata_count", "/", "speech_count", "\n", "tqdm", ".", "write", "(", "f'{missing_metadata_ratio:.2%} speeches in {corpus_path} '", "\n", "'are missing metadata and excluded from the output corpus.'", ")", "\n", "\n", "", "for", "metadata_name", "in", "identities", ":", "\n", "        ", "bounded_lengths", "=", "[", "length", "for", "length", "in", "speeches_length", "[", "metadata_name", "]", "\n", "if", "length", "<", "histogram_upper_bound", "]", "\n", "\n", "if", "len", "(", "bounded_lengths", ")", "==", "0", ":", "\n", "            ", "raise", "ValueError", "(", "f'{metadata_name} is empty?'", ")", "\n", "\n", "", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", ")", "\n", "ax", "=", "sns", ".", "distplot", "(", "bounded_lengths", ",", "label", "=", "metadata_name", ")", "\n", "ax", ".", "legend", "(", ")", "\n", "fig", ".", "savefig", "(", "f'graphs/speech_length/{metadata_name}.pdf'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S0_partition_corpus.main": [[257, 278], ["range", "os.makedirs", "print", "S0_partition_corpus.partition_corpus", "print"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S0_partition_corpus.partition_corpus"], ["", "", "def", "main", "(", ")", "->", "None", ":", "\n", "# Congressional Record", "\n", "    ", "post_WWII_sessions", "=", "range", "(", "79", ",", "112", ")", "\n", "output_dir", "=", "'../../data/debug_partitioned_corpora'", "\n", "min_speech_length", "=", "20", "\n", "num_chunks", "=", "10", "\n", "input_encoding", ":", "str", "=", "'mac_roman'", "# NOTE", "\n", "\n", "os", ".", "makedirs", "(", "output_dir", ")", "\n", "print", "(", "'Session\\t\\tSpeech Count\\tWord Count\\tShort Speech\\tMissing Metadata'", ")", "\n", "for", "session", "in", "post_WWII_sessions", ":", "\n", "        ", "original_corpus_path", "=", "f'../../data/raw/bound/speeches_{session:0>3d}.txt'", "\n", "metadata_path", "=", "f'../../data/raw/bound/{session:0>3d}_SpeakerMap.txt'", "\n", "stats", "=", "partition_corpus", "(", "\n", "session", ",", "original_corpus_path", ",", "metadata_path", ",", "output_dir", ",", "\n", "min_speech_length", ",", "num_chunks", ",", "input_encoding", ")", "\n", "speech_count", ",", "word_count", ",", "short_speech_count", ",", "missing_metadata_count", "=", "stats", "\n", "short_speech_ratio", "=", "short_speech_count", "/", "speech_count", "\n", "missing_metadata_ratio", "=", "missing_metadata_count", "/", "speech_count", "\n", "remaining_speech_count", "=", "speech_count", "-", "short_speech_count", "-", "missing_metadata_count", "\n", "print", "(", "f'{session}\\t\\t'", "\n", "f'{remaining_speech_count:,}\\t\\t'", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S4_ideal_grounded.faux_sent_tokenize": [[56, 67], ["len", "line.split", "len"], "function", ["None"], ["", "def", "faux_sent_tokenize", "(", "line", ":", "str", ")", "->", "Iterable", "[", "List", "[", "str", "]", "]", ":", "\n", "    ", "\"\"\"discard procedural words and punctuations\"\"\"", "\n", "words", "=", "[", "w", "for", "w", "in", "line", ".", "split", "(", ")", "if", "w", "not", "in", "discard", "]", "\n", "start_index", "=", "0", "\n", "while", "(", "start_index", "+", "FIXED_SENT_LEN", ")", "<", "(", "len", "(", "words", ")", "-", "1", ")", ":", "\n", "        ", "yield", "words", "[", "start_index", ":", "start_index", "+", "FIXED_SENT_LEN", "]", "\n", "start_index", "+=", "FIXED_SENT_LEN", "\n", "\n", "", "trailing_words", "=", "words", "[", "start_index", ":", "-", "1", "]", "\n", "if", "len", "(", "trailing_words", ")", ">=", "MIN_SENT_LEN", ":", "\n", "        ", "yield", "trailing_words", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S4_ideal_grounded.process_sentences": [[69, 120], ["enumerate", "random.shuffle", "sum", "print", "print", "open", "pickle.load", "range", "getattr", "S4_ideal_grounded.Sentence", "len", "len", "S4_ideal_grounded.faux_sent_tokenize", "num_mentions.values"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S4_ideal_grounded.faux_sent_tokenize"], ["", "", "def", "process_sentences", "(", "session", ":", "int", ")", "->", "Tuple", "[", "List", "[", "Sentence", "]", ",", "List", "[", "Sentence", "]", "]", ":", "\n", "    ", "in_path", "=", "in_dir", "/", "f'underscored_{session}.pickle'", "\n", "with", "open", "(", "in_path", ",", "'rb'", ")", "as", "in_file", ":", "\n", "        ", "speeches", ",", "num_mentions", "=", "pickle", ".", "load", "(", "in_file", ")", "\n", "\n", "# Mark context speeches with bill denotation", "\n", "", "per_session_mention", "=", "0", "\n", "for", "speech_index", ",", "speech", "in", "enumerate", "(", "speeches", ")", ":", "\n", "        ", "if", "speech", ".", "mentions_bill", "is", "False", ":", "\n", "            ", "continue", "\n", "", "if", "num_mentions", "[", "speech", ".", "bill", ".", "title", "]", "<", "MIN_NUM_MENTIONS", ":", "\n", "            ", "continue", "\n", "\n", "", "per_session_mention", "+=", "1", "\n", "for", "i", "in", "range", "(", "speech_index", "+", "1", ",", "\n", "speech_index", "+", "1", "+", "NUM_CONTEXT_SPEECHES", ")", ":", "\n", "            ", "try", ":", "\n", "                ", "speeches", "[", "i", "]", ".", "bill", "=", "speech", ".", "bill", "\n", "", "except", "IndexError", ":", "\n", "                ", "continue", "\n", "\n", "# Chop up sentences", "\n", "", "", "", "sentences", ":", "List", "[", "Sentence", "]", "=", "[", "]", "\n", "for", "speech", "in", "speeches", ":", "\n", "        ", "if", "speech", ".", "bill", "is", "None", ":", "\n", "            ", "continue", "\n", "", "if", "num_mentions", "[", "speech", ".", "bill", ".", "title", "]", "<", "MIN_NUM_MENTIONS", ":", "\n", "            ", "continue", "\n", "\n", "# deno = speech.bill.title", "\n", "", "deno", "=", "getattr", "(", "speech", ".", "bill", ",", "DENO_LABEL", ")", "# either 'topic' or 'title'", "\n", "cono", "=", "speech", ".", "speaker", ".", "party", "\n", "if", "cono", "!=", "'D'", "and", "cono", "!=", "'R'", ":", "# skip independent members for now", "\n", "            ", "continue", "\n", "\n", "", "sentences", "+=", "[", "\n", "Sentence", "(", "faux_sent", ",", "deno", ",", "cono", ")", "\n", "for", "faux_sent", "in", "faux_sent_tokenize", "(", "speech", ".", "text", ")", "]", "\n", "\n", "", "random", ".", "shuffle", "(", "sentences", ")", "\n", "dev", "=", "sentences", "[", ":", "NUM_DEV_HOLDOUT", "]", "\n", "train", "=", "sentences", "[", "NUM_DEV_HOLDOUT", ":", "]", "\n", "\n", "check", "=", "sum", "(", "[", "m", "for", "m", "in", "num_mentions", ".", "values", "(", ")", "if", "m", ">", "2", "]", ")", "\n", "progress", "=", "(", "\n", "f'Session {session}: '", "\n", "f'{per_session_mention} bill mentions above min, '", "\n", "f'{len(train):,} sentences, {len(dev)} dev holdout'", ")", "\n", "print", "(", "progress", ")", "\n", "print", "(", "progress", ",", "file", "=", "log", ")", "\n", "return", "train", ",", "dev", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S4_ideal_grounded.build_vocabulary": [[122, 144], ["len", "frequency.items", "print", "word_to_id.items", "len"], "function", ["None"], ["", "def", "build_vocabulary", "(", "\n", "frequency", ":", "Counter", ",", "\n", "min_frequency", ":", "int", ",", "\n", "add_special_tokens", ":", "bool", "\n", ")", "->", "Tuple", "[", "\n", "Dict", "[", "str", ",", "int", "]", ",", "\n", "Dict", "[", "int", ",", "str", "]", "]", ":", "\n", "    ", "word_to_id", ":", "Dict", "[", "str", ",", "int", "]", "=", "{", "}", "\n", "if", "add_special_tokens", ":", "\n", "        ", "word_to_id", "[", "'[PAD]'", "]", "=", "0", "\n", "word_to_id", "[", "'[UNK]'", "]", "=", "1", "\n", "word_to_id", "[", "'[CLS]'", "]", "=", "2", "\n", "word_to_id", "[", "'[SEP]'", "]", "=", "3", "\n", "", "id_to_word", "=", "{", "val", ":", "key", "for", "key", ",", "val", "in", "word_to_id", ".", "items", "(", ")", "}", "\n", "next_vocab_id", "=", "len", "(", "word_to_id", ")", "\n", "for", "word", ",", "freq", "in", "frequency", ".", "items", "(", ")", ":", "\n", "        ", "if", "word", "not", "in", "word_to_id", "and", "freq", ">=", "min_frequency", ":", "\n", "            ", "word_to_id", "[", "word", "]", "=", "next_vocab_id", "\n", "id_to_word", "[", "next_vocab_id", "]", "=", "word", "\n", "next_vocab_id", "+=", "1", "\n", "", "", "print", "(", "f'Vocabulary size = {len(word_to_id):,}'", ",", "file", "=", "log", ")", "\n", "return", "word_to_id", ",", "id_to_word", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S4_ideal_grounded.wrap": [[221, 231], ["sent_word_ids.append", "deno_labels.append", "cono_labels.append"], "function", ["None"], ["def", "wrap", "(", "sentences", ")", ":", "\n", "    ", "sent_word_ids", ":", "List", "[", "List", "[", "int", "]", "]", "=", "[", "]", "\n", "deno_labels", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "cono_labels", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "for", "sent", "in", "sentences", ":", "\n", "        ", "sent_word_ids", ".", "append", "(", "\n", "[", "word_to_id", "[", "w", "]", "for", "w", "in", "sent", ".", "words", "if", "w", "in", "word_to_id", "]", ")", "\n", "deno_labels", ".", "append", "(", "deno_to_id", "[", "sent", ".", "deno", "]", ")", "\n", "cono_labels", ".", "append", "(", "cono_to_id", "[", "sent", ".", "cono", "]", ")", "\n", "", "return", "sent_word_ids", ",", "deno_labels", ",", "cono_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.load_parsing_result": [[15, 57], ["os.path.join", "os.path.exists", "tqdm.tqdm", "tqdm.tqdm.write", "range", "os.path.join", "tqdm.tqdm.write", "open", "pickle.load", "open", "json.load", "append_to_filtered_sentences", "open", "pickle.dump"], "function", ["None"], ["def", "load_parsing_result", "(", "\n", "parsing_result_dir", ":", "str", ",", "\n", "session", ":", "int", ",", "\n", "party", ":", "str", ",", "\n", "num_chunks", ":", "int", ",", "\n", "auto_caching", ":", "Optional", "[", "Set", "[", "str", "]", "]", ",", "\n", ")", "->", "List", "[", "Dict", "]", ":", "\n", "    ", "\"\"\"\n    If auto_caching is True, it pickles the decoded JSON, so re-loading\n    the same JSON will automatically load the pickled binary.\n\n    Note that there is no mechainsm to check and update the cache file\n    if the original file has been updated. In that case, you should manually\n    delete the outdated cache files.\n    \"\"\"", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "parsing_result_dir", ",", "f'cache_{session}_{party}.pickle'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "# haven't cached yet", "\n", "        ", "filtered_sentences", ":", "List", "[", "Dict", "]", "=", "[", "]", "\n", "append_to_filtered_sentences", "=", "filtered_sentences", ".", "append", "\n", "for", "chunk", "in", "tqdm", "(", "range", "(", "num_chunks", ")", ",", "\n", "desc", "=", "f'Decoding parsing result of {session}_{party}'", ")", ":", "\n", "            ", "in_path", "=", "os", ".", "path", ".", "join", "(", "\n", "parsing_result_dir", ",", "f'{session}_{party}{chunk}.json'", ")", "\n", "with", "open", "(", "in_path", ")", "as", "parsing_result_file", ":", "\n", "                ", "parsing_result", "=", "json", ".", "load", "(", "parsing_result_file", ")", "\n", "", "for", "original_sentence", "in", "parsing_result", "[", "'sentences'", "]", ":", "\n", "                ", "append_to_filtered_sentences", "(", "\n", "{", "key", ":", "original_sentence", "[", "key", "]", "\n", "for", "key", "in", "original_sentence", "\n", "if", "key", "in", "auto_caching", "}", ")", "# type: ignore", "\n", "\n", "", "", "if", "auto_caching", ":", "\n", "            ", "tqdm", ".", "write", "(", "f'Caching decoded json to {cache_path}... '", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "pickle", ".", "dump", "(", "filtered_sentences", ",", "cache_file", ",", "protocol", "=", "-", "1", ")", "\n", "", "", "return", "filtered_sentences", "\n", "\n", "", "else", ":", "# already cached", "\n", "        ", "tqdm", ".", "write", "(", "f'Loading {cache_path}...'", ")", "\n", "with", "open", "(", "cache_path", ",", "'rb'", ")", "as", "cache_file", ":", "\n", "            ", "filtered_sentences", "=", "pickle", ".", "load", "(", "cache_file", ")", "\n", "", "return", "filtered_sentences", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.sort_frequency_and_write": [[59, 76], ["isinstance", "output_iterable.sort", "collections.Counter", "open", "out_file.write"], "function", ["None"], ["", "", "def", "sort_frequency_and_write", "(", "\n", "list_of_phrases", ":", "Iterable", ",", "\n", "out_path", ":", "str", ",", "\n", "min_frequency", ":", "int", "\n", ")", "->", "None", ":", "\n", "    ", "if", "isinstance", "(", "list_of_phrases", ",", "Counter", ")", ":", "\n", "        ", "frequency", "=", "list_of_phrases", "\n", "", "else", ":", "\n", "        ", "frequency", "=", "Counter", "(", "list_of_phrases", ")", "\n", "", "output_iterable", "=", "[", "\n", "(", "frequency", "[", "phrase", "]", ",", "phrase", ")", "\n", "for", "phrase", "in", "frequency", "\n", "if", "frequency", "[", "phrase", "]", ">", "min_frequency", "]", "\n", "output_iterable", ".", "sort", "(", "key", "=", "lambda", "t", ":", "t", "[", "0", "]", ",", "reverse", "=", "True", ")", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "out_file", ":", "\n", "        ", "for", "freq", ",", "phrase", "in", "output_iterable", ":", "\n", "            ", "out_file", ".", "write", "(", "f'{freq}\\t{phrase}\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.extract_named_entities": [[78, 93], ["re.compile", "S2_extract_phrases.sort_frequency_and_write", "entities.append", "len", "re.compile.search", "entity_text.lower", "entity_text.split"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.sort_frequency_and_write"], ["", "", "", "def", "extract_named_entities", "(", "\n", "sentences", ":", "List", "[", "Dict", "]", ",", "\n", "out_path", ":", "str", ",", "\n", "min_frequency", ":", "int", "\n", ")", "->", "None", ":", "\n", "    ", "contains_number", "=", "re", ".", "compile", "(", "r'\\d'", ")", "# HACK check if still necessary with new NER settings", "\n", "entities", "=", "[", "]", "\n", "for", "sent", "in", "sentences", ":", "\n", "        ", "if", "'entitymentions'", "in", "sent", ":", "\n", "            ", "for", "entity", "in", "sent", "[", "'entitymentions'", "]", ":", "\n", "                ", "entity_text", "=", "entity", "[", "'text'", "]", "\n", "if", "(", "len", "(", "entity_text", ".", "split", "(", ")", ")", ">", "1", "# Excluding Unigrams", "\n", "and", "not", "contains_number", ".", "search", "(", "entity_text", ")", ")", ":", "\n", "                    ", "entities", ".", "append", "(", "entity_text", ".", "lower", "(", ")", ")", "# TODO remove punctuaion here?", "\n", "", "", "", "", "sort_frequency_and_write", "(", "entities", ",", "out_path", ",", "min_frequency", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.extract_noun_and_verb_phrases": [[95, 141], ["re.compile", "S2_extract_phrases.sort_frequency_and_write", "tuple", "string_to_tree", "set", "tree.subtrees", "phrases_in_doc.extend", "re.compile.search", "len", "S2_extract_phrases.extract_noun_and_verb_phrases.normalize"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.sort_frequency_and_write"], ["", "def", "extract_noun_and_verb_phrases", "(", "\n", "sentences", ":", "List", "[", "Dict", "]", ",", "\n", "out_path", ":", "str", ",", "\n", "discard_tokens", ":", "Set", "[", "str", "]", ",", "\n", "stop_words", ":", "Set", "[", "str", "]", ",", "\n", "min_frequency", ":", "int", "\n", ")", "->", "None", ":", "\n", "    ", "contains_number", "=", "re", ".", "compile", "(", "r'\\d'", ")", "# HACK check if still necessary with new NER settings", "\n", "\n", "def", "normalize", "(", "iterable_of_tokens", ":", "Tuple", "[", "str", ",", "...", "]", ")", "->", "Tuple", "[", "str", ",", "...", "]", ":", "\n", "        ", "\"\"\" remove stop_words, punctuation; all lowercase, \"\"\"", "\n", "return", "tuple", "(", "word", ".", "lower", "(", ")", "\n", "for", "word", "in", "iterable_of_tokens", "\n", "if", "word", ".", "lower", "(", ")", "not", "in", "stop_words", ")", "\n", "\n", "", "def", "heuristic", "(", "tokens", ":", "Tuple", "[", "str", ",", "...", "]", ")", "->", "Tuple", "[", "str", ",", "...", "]", ":", "\n", "        ", "\"\"\" permits the second token to be a stop word \"\"\"", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "if", "contains_number", ".", "search", "(", "token", ")", ":", "\n", "                ", "return", "(", "''", ",", ")", "\n", "", "", "if", "len", "(", "tokens", ")", "==", "2", ":", "\n", "            ", "return", "normalize", "(", "tokens", ")", "\n", "", "elif", "len", "(", "tokens", ")", "==", "3", ":", "\n", "            ", "tokens", "=", "tuple", "(", "t", ".", "lower", "(", ")", "for", "t", "in", "tokens", ")", "\n", "if", "(", "tokens", "[", "0", "]", "not", "in", "stop_words", "\n", "and", "tokens", "[", "1", "]", "not", "in", "discard_tokens", "\n", "and", "tokens", "[", "2", "]", "not", "in", "stop_words", ")", ":", "\n", "                ", "return", "tokens", "\n", "", "else", ":", "\n", "                ", "return", "normalize", "(", "tokens", ")", "\n", "", "", "else", ":", "\n", "            ", "return", "(", "''", ",", ")", "\n", "\n", "", "", "string_to_tree", "=", "nltk", ".", "tree", ".", "Tree", ".", "fromstring", "\n", "parse_trees", "=", "(", "string_to_tree", "(", "s", "[", "'parse'", "]", ")", "for", "s", "in", "sentences", ")", "\n", "phrases_in_doc", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "for", "tree", "in", "parse_trees", ":", "\n", "        ", "phrases_in_sentence", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "for", "subtree", "in", "tree", ".", "subtrees", "(", ")", ":", "\n", "            ", "if", "subtree", ".", "label", "(", ")", "!=", "'NP'", "and", "subtree", ".", "label", "(", ")", "!=", "'VP'", ":", "\n", "                ", "continue", "\n", "", "phrase", "=", "heuristic", "(", "subtree", ".", "leaves", "(", ")", ")", "\n", "if", "1", "<", "len", "(", "phrase", ")", "<", "4", ":", "# bigrams & trigrams only", "\n", "                ", "phrases_in_sentence", ".", "add", "(", "' '", ".", "join", "(", "phrase", ")", ")", "\n", "", "", "phrases_in_doc", ".", "extend", "(", "list", "(", "phrases_in_sentence", ")", ")", "\n", "", "sort_frequency_and_write", "(", "phrases_in_doc", ",", "out_path", ",", "min_frequency", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.compute_collocation": [[143, 196], ["range", "nltk.collocations.BigramCollocationFinder.from_words", "BigramCollocationFinder.from_words.apply_freq_filter", "BigramCollocationFinder.from_words.apply_word_filter", "BigramCollocationFinder.from_words.score_ngrams", "nltk.collocations.TrigramCollocationFinder.from_words", "TrigramCollocationFinder.from_words.apply_freq_filter", "TrigramCollocationFinder.from_words.apply_ngram_filter", "TrigramCollocationFinder.from_words.score_ngrams", "len", "os.path.join", "nltk.tokenize.word_tokenize", "tokenized_corpus.extend", "open", "open", "open", "corpus_file.read", "t.lower", "nltk.collocations.BigramAssocMeasures", "nltk.collocations.TrigramAssocMeasures", "bigram_file.write", "trigram_file.write", "t.isdigit"], "function", ["None"], ["", "def", "compute_collocation", "(", "\n", "corpora_dir", ":", "str", ",", "\n", "session", ":", "int", ",", "\n", "party", ":", "str", ",", "\n", "num_chunks", ":", "int", ",", "\n", "bigram_out_path", ":", "str", ",", "\n", "trigram_out_path", ":", "str", ",", "\n", "discard_tokens", ":", "Set", "[", "str", "]", ",", "\n", "stop_words", ":", "Set", "[", "str", "]", ",", "\n", "min_frequency", ":", "int", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"\n    discard_tokens should be a subset of stop_words. This is used for\n    a heuristic to filter trigrams, where the second word is permitted\n    to be a stop word (e.g. \"freedom of speech\") but not a discarded token\n    (e.g. \"I yield to\"). The first and third words can never be a stop word.\n    \"\"\"", "\n", "tokenized_corpus", ":", "List", "[", "str", "]", "=", "[", "]", "\n", "for", "chunk_index", "in", "range", "(", "num_chunks", ")", ":", "\n", "        ", "corpus_path", "=", "os", ".", "path", ".", "join", "(", "corpora_dir", ",", "f'{session}_{party}{chunk_index}.txt'", ")", "\n", "with", "open", "(", "corpus_path", ")", "as", "corpus_file", ":", "\n", "            ", "raw_text", "=", "corpus_file", ".", "read", "(", ")", "\n", "", "tokens", ":", "List", "[", "str", "]", "=", "nltk", ".", "tokenize", ".", "word_tokenize", "(", "raw_text", ")", "\n", "tokens", "=", "[", "t", ".", "lower", "(", ")", "for", "t", "in", "tokens", "\n", "if", "t", "not", "in", "discard_tokens", "\n", "and", "not", "t", ".", "isdigit", "(", ")", "]", "\n", "tokenized_corpus", ".", "extend", "(", "tokens", ")", "\n", "", "del", "tokens", "\n", "\n", "bigram_finder", "=", "BigramCollocationFinder", ".", "from_words", "(", "tokenized_corpus", ")", "\n", "bigram_finder", ".", "apply_freq_filter", "(", "min_frequency", ")", "\n", "bigram_finder", ".", "apply_word_filter", "(", "\n", "lambda", "word", ":", "word", "in", "stop_words", ")", "\n", "bigrams", "=", "bigram_finder", ".", "score_ngrams", "(", "BigramAssocMeasures", "(", ")", ".", "raw_freq", ")", "\n", "\n", "trigram_finder", "=", "TrigramCollocationFinder", ".", "from_words", "(", "tokenized_corpus", ")", "\n", "trigram_finder", ".", "apply_freq_filter", "(", "min_frequency", ")", "\n", "trigram_finder", ".", "apply_ngram_filter", "(", "\n", "lambda", "w1", ",", "w2", ",", "w3", ":", "\n", "(", "w1", "in", "stop_words", ")", "or", "(", "w3", "in", "stop_words", ")", "or", "(", "w2", "in", "discard_tokens", ")", ")", "\n", "trigrams", "=", "trigram_finder", ".", "score_ngrams", "(", "TrigramAssocMeasures", "(", ")", ".", "raw_freq", ")", "\n", "\n", "num_tokens", "=", "len", "(", "tokenized_corpus", ")", "\n", "with", "open", "(", "bigram_out_path", ",", "'w'", ")", "as", "bigram_file", ":", "\n", "        ", "for", "bigram", ",", "relative_freq", "in", "bigrams", ":", "\n", "            ", "absolute_freq", "=", "relative_freq", "*", "num_tokens", "\n", "bigram_str", "=", "' '", ".", "join", "(", "bigram", ")", "\n", "bigram_file", ".", "write", "(", "f'{absolute_freq:.0f}\\t{bigram_str}\\n'", ")", "\n", "", "", "with", "open", "(", "trigram_out_path", ",", "'w'", ")", "as", "trigram_file", ":", "\n", "        ", "for", "trigram", ",", "relative_freq", "in", "trigrams", ":", "\n", "            ", "absolute_freq", "=", "relative_freq", "*", "num_tokens", "\n", "trigram_str", "=", "' '", ".", "join", "(", "trigram", ")", "\n", "trigram_file", ".", "write", "(", "f'{absolute_freq:.0f}\\t{trigram_str}\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.aggregate_phrases": [[198, 238], ["S2_extract_phrases.aggregate_phrases.load_counters"], "function", ["None"], ["", "", "", "def", "aggregate_phrases", "(", "\n", "Dem_phrase_sources", ":", "List", "[", "str", "]", ",", "\n", "GOP_phrase_sources", ":", "List", "[", "str", "]", ",", "\n", "out_path", ":", "str", ",", "\n", "top_k_phrases_per_source", ":", "int", ",", "\n", "min_frequency", ":", "int", "\n", ")", "->", "None", ":", "\n", "    ", "\"\"\"load named_entities, NP_VP, collocations, write statistics\"\"\"", "\n", "\n", "def", "load_counters", "(", "counter_paths", ":", "List", "[", "str", "]", ")", "->", "Counter", ":", "\n", "        ", "union_counter", ":", "Counter", "=", "Counter", "(", ")", "\n", "for", "source_path", "in", "counter_paths", ":", "\n", "            ", "with", "open", "(", "source_path", ")", "as", "in_file", ":", "\n", "                ", "temp_counter", ":", "Counter", "=", "Counter", "(", ")", "\n", "for", "line_num", ",", "line", "in", "enumerate", "(", "in_file", ")", ":", "\n", "                    ", "if", "line_num", ">", "top_k_phrases_per_source", ":", "\n", "                        ", "break", "\n", "", "frequency", ",", "phrase", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "frequency", "=", "int", "(", "frequency", ")", "\n", "if", "frequency", "<", "min_frequency", ":", "\n", "                        ", "break", "\n", "", "phrase", "=", "phrase", ".", "strip", "(", ")", "\n", "temp_counter", "[", "phrase", "]", "=", "frequency", "\n", "", "", "union_counter", "=", "union_counter", "|", "temp_counter", "\n", "", "return", "union_counter", "\n", "\n", "", "Dem_counter", "=", "load_counters", "(", "Dem_phrase_sources", ")", "\n", "GOP_counter", "=", "load_counters", "(", "GOP_phrase_sources", ")", "\n", "phrase_counter", "=", "Dem_counter", "+", "GOP_counter", "\n", "\n", "output_iterable", "=", "[", "\n", "(", "len", "(", "phrase", ".", "split", "(", ")", ")", ",", "Dem_counter", "[", "phrase", "]", ",", "GOP_counter", "[", "phrase", "]", ",", "phrase", ")", "\n", "for", "phrase", "in", "phrase_counter", "]", "\n", "\n", "# When replacing words with underscored phrases,", "\n", "# replace long phrases first, and then frequent phrases.", "\n", "output_iterable", ".", "sort", "(", "key", "=", "lambda", "t", ":", "(", "t", "[", "0", "]", ",", "t", "[", "1", "]", ")", ",", "reverse", "=", "True", ")", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "out_file", ":", "\n", "        ", "for", "_", ",", "D_freq", ",", "R_freq", ",", "phrase", "in", "output_iterable", ":", "\n", "            ", "out_file", ".", "write", "(", "f'{D_freq}\\t{R_freq}\\t{phrase}\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.main": [[240, 323], ["range", "set", "set.union", "punctuations.union.union", "print", "tqdm.tqdm", "set", "os.path.join", "S2_extract_phrases.aggregate_phrases", "nltk.corpus.stopwords.words", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S2_extract_phrases.aggregate_phrases"], ["", "", "", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "sessions", "=", "range", "(", "79", ",", "112", ")", "\n", "parsing_result_dir", "=", "'../../data/interim/CoreNLP_parsed'", "\n", "num_parsing_chunks", "=", "10", "\n", "out_dir", "=", "'../../data/interim/'", "\n", "\n", "# Pickle these from the parsing results", "\n", "auto_caching", ":", "Optional", "[", "Set", "[", "str", "]", "]", "=", "{", "'parse'", ",", "'entitymentions'", "}", "\n", "\n", "# For compute_collocation", "\n", "# Note partitioned corpora removed speeches without known speaker metadata", "\n", "corpora_dir", "=", "'partitioned_corpora'", "\n", "\n", "# Noun phrases/ verb phrases including these words will be excluded", "\n", "procedural_words", "=", "{", "\n", "'yield'", ",", "'motion'", ",", "'order'", ",", "'ordered'", ",", "'quorum'", ",", "'roll'", ",", "'unanimous'", ",", "\n", "'mr.'", ",", "'madam'", ",", "'speaker'", ",", "'chairman'", ",", "'president'", ",", "'senator'", ",", "\n", "'gentleman'", ",", "'colleague'", ",", "'colleagues'", ",", "'...'", ",", "'``'", ",", "\"''\"", ",", "'--'", "}", "\n", "punctuations", "=", "set", "(", "char", "for", "char", "in", "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'", ")", "\n", "discard_tokens", "=", "punctuations", ".", "union", "(", "procedural_words", ")", "\n", "stop_words", "=", "discard_tokens", ".", "union", "(", "\n", "set", "(", "nltk", ".", "corpus", ".", "stopwords", ".", "words", "(", "'english'", ")", ")", ")", "\n", "\n", "# For aggregate_phrases", "\n", "min_frequency_per_source", "=", "15", "\n", "top_k_phrases_per_source", "=", "1000", "\n", "final_min_frequency", "=", "30", "\n", "\n", "# Sanity check that parsing result files exist", "\n", "# sane = True", "\n", "# for session in sessions:", "\n", "#     for party in ('D', 'R'):", "\n", "#         for chunk in range(num_parsing_chunks):", "\n", "#             in_path = f'{parsing_result_dir}/{session}_{party}{chunk}.json'", "\n", "#             if not os.path.isfile(in_path):", "\n", "#                 print(f'{in_path} does not exist!')", "\n", "#                 sane = False", "\n", "# if not sane:", "\n", "#     raise FileNotFoundError()", "\n", "\n", "print", "(", "f'Processing sessions from {sessions}'", ")", "\n", "for", "session", "in", "tqdm", "(", "sessions", ",", "desc", "=", "'Sessions'", ")", ":", "\n", "        ", "for", "party", "in", "(", "'D'", ",", "'R'", ")", ":", "\n", "            ", "name_entities_path", "=", "os", ".", "path", ".", "join", "(", "\n", "out_dir", ",", "'named_entities'", ",", "f'{session}_{party}.txt'", ")", "\n", "noun_and_verb_phrases_path", "=", "os", ".", "path", ".", "join", "(", "\n", "out_dir", ",", "'noun_and_verb_phrases'", ",", "f'{session}_{party}.txt'", ")", "\n", "collocation_bigram_path", "=", "os", ".", "path", ".", "join", "(", "\n", "out_dir", ",", "'collocation_bigram'", ",", "f'{session}_{party}.txt'", ")", "\n", "collocation_trigram_path", "=", "os", ".", "path", ".", "join", "(", "\n", "out_dir", ",", "'collocation_trigram'", ",", "f'{session}_{party}.txt'", ")", "\n", "\n", "# sentences = load_parsing_result(", "\n", "#     parsing_result_dir, session, party, num_parsing_chunks, auto_caching)", "\n", "# extract_named_entities(", "\n", "#     sentences, name_entities_path, min_frequency_per_source)", "\n", "# extract_noun_and_verb_phrases(", "\n", "#     sentences, noun_and_verb_phrases_path,", "\n", "#     discard_tokens, stop_words, min_frequency_per_source)", "\n", "# del sentences", "\n", "\n", "# compute_collocation(", "\n", "#     corpora_dir, session, party, num_parsing_chunks,", "\n", "#     collocation_bigram_path, collocation_trigram_path,", "\n", "#     discard_tokens, stop_words, min_frequency_per_source)", "\n", "\n", "# Combine phrases from both parties", "\n", "", "Dem_phrase_sources", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'named_entities'", ",", "f'{session}_D.txt'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'noun_and_verb_phrases'", ",", "f'{session}_D.txt'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'collocation_bigram'", ",", "f'{session}_D.txt'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'collocation_trigram'", ",", "f'{session}_D.txt'", ")", ",", "\n", "]", "\n", "GOP_phrase_sources", "=", "[", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'named_entities'", ",", "f'{session}_R.txt'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'noun_and_verb_phrases'", ",", "f'{session}_R.txt'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'collocation_bigram'", ",", "f'{session}_R.txt'", ")", ",", "\n", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'collocation_trigram'", ",", "f'{session}_R.txt'", ")", "\n", "]", "\n", "final_output_path", "=", "os", ".", "path", ".", "join", "(", "out_dir", ",", "'aggregated_phrases'", ",", "f'{session}.txt'", ")", "\n", "aggregate_phrases", "(", "\n", "Dem_phrase_sources", ",", "GOP_phrase_sources", ",", "final_output_path", ",", "\n", "top_k_phrases_per_source", ",", "final_min_frequency", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S4_export_train_data.build_vocabulary": [[15, 37], ["len", "frequency.items", "print", "word_to_id.items", "len"], "function", ["None"], ["def", "build_vocabulary", "(", "\n", "frequency", ":", "Counter", ",", "\n", "min_frequency", ":", "int", "=", "0", ",", "\n", "add_special_tokens", ":", "bool", "=", "True", "\n", ")", "->", "Tuple", "[", "\n", "Dict", "[", "str", ",", "int", "]", ",", "\n", "Dict", "[", "int", ",", "str", "]", "]", ":", "\n", "    ", "word_to_id", ":", "Dict", "[", "str", ",", "int", "]", "=", "{", "}", "\n", "if", "add_special_tokens", ":", "\n", "        ", "word_to_id", "[", "'[PAD]'", "]", "=", "0", "\n", "word_to_id", "[", "'[UNK]'", "]", "=", "1", "\n", "word_to_id", "[", "'[CLS]'", "]", "=", "2", "\n", "word_to_id", "[", "'[SEP]'", "]", "=", "3", "\n", "", "id_to_word", "=", "{", "val", ":", "key", "for", "key", ",", "val", "in", "word_to_id", ".", "items", "(", ")", "}", "\n", "next_vocab_id", "=", "len", "(", "word_to_id", ")", "\n", "for", "word", ",", "freq", "in", "frequency", ".", "items", "(", ")", ":", "\n", "        ", "if", "word", "not", "in", "word_to_id", "and", "freq", ">=", "min_frequency", ":", "\n", "            ", "word_to_id", "[", "word", "]", "=", "next_vocab_id", "\n", "id_to_word", "[", "next_vocab_id", "]", "=", "word", "\n", "next_vocab_id", "+=", "1", "\n", "", "", "print", "(", "f'Vocabulary size = {len(word_to_id):,}'", ")", "\n", "return", "word_to_id", ",", "id_to_word", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S4_export_train_data.subsampling": [[39, 75], ["sum", "dict", "DefaultDict", "frequency.items", "frequency.items", "ValueError", "frequency.values", "math.sqrt", "math.sqrt"], "function", ["None"], ["", "def", "subsampling", "(", "\n", "frequency", ":", "Counter", "[", "str", "]", ",", "\n", "heuristic", ":", "Optional", "[", "str", "]", ",", "\n", "threshold", ":", "float", ",", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "    ", "\"\"\"\n    Downsample frequent words.\n\n    Subsampling implementation from annotated C code of Mikolov et al. 2013:\n    http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling\n    This blog post is linked from TensorFlow's website, so authoratative?\n\n    NOTE the default threshold is 1e-3, not 1e-5 as in the paper version\n    \"\"\"", "\n", "cumulative_freq", "=", "sum", "(", "abs_freq", "for", "abs_freq", "in", "frequency", ".", "values", "(", ")", ")", "\n", "keep_prob", ":", "Dict", "[", "str", ",", "float", "]", "=", "dict", "(", ")", "\n", "\n", "if", "heuristic", "is", "None", ":", "\n", "        ", "from", "typing", "import", "DefaultDict", "\n", "keep_prob", "=", "DefaultDict", "(", "lambda", ":", "1", ")", "\n", "return", "keep_prob", "\n", "\n", "", "if", "heuristic", "==", "'code'", ":", "\n", "        ", "for", "word", ",", "abs_freq", "in", "frequency", ".", "items", "(", ")", ":", "\n", "            ", "rel_freq", "=", "abs_freq", "/", "cumulative_freq", "\n", "keep_prob", "[", "word", "]", "=", "(", "\n", "(", "math", ".", "sqrt", "(", "rel_freq", "/", "threshold", ")", "+", "1", ")", "\n", "*", "(", "threshold", "/", "rel_freq", ")", "\n", ")", "\n", "", "", "elif", "heuristic", "==", "'paper'", ":", "\n", "        ", "for", "word", ",", "abs_freq", "in", "frequency", ".", "items", "(", ")", ":", "\n", "            ", "rel_freq", "=", "abs_freq", "/", "cumulative_freq", "\n", "keep_prob", "[", "word", "]", "=", "math", ".", "sqrt", "(", "threshold", "/", "rel_freq", ")", "\n", "", "", "else", ":", "\n", "        ", "raise", "ValueError", "(", "'Unknown heuristic of subsampling.'", ")", "\n", "", "return", "keep_prob", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S4_export_train_data.main": [[77, 272], ["pathlib.Path.mkdir", "open", "print", "print", "print", "print", "print", "print", "typing.Counter", "tqdm.tqdm", "sum", "print", "print", "typing.Counter", "norm_freq.items", "print", "S4_export_train_data.subsampling", "typing.Counter", "tqdm.tqdm", "print", "print", "S4_export_train_data.build_vocabulary", "tqdm.tqdm", "set", "ground.values", "random.sample", "sum", "random.shuffle", "print", "random.sample", "open.write", "open.write", "final_freq.most_common", "open.close", "print", "open", "pickle.load", "sum", "sum", "open", "file.write", "ground.values", "print", "negative_sampling_probs.get", "open", "pickle.dump", "print", "norm_freq.update", "gw.cono.values", "random.sample.add", "open", "print", "len", "random.sample", "random.shuffle", "len", "open", "open", "final_freq.items", "range", "print", "print", "print", "print", "print", "norm_freq.values", "len", "len", "len", "final_freq.update", "len", "sum", "len", "gw.cono.most_common", "print", "print", "print", "final_freq.values", "len", "norm_freq.values", "random.random", "sent.subsampled_tokens.append", "len", "random.sample.append", "len", "data.GroundedWord", "final_freq.values", "typing.Counter"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S4_export_train_data.subsampling", "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S4_export_train_data.build_vocabulary"], ["", "def", "main", "(", "\n", "in_dir", ":", "Path", ",", "\n", "out_dir", ":", "Path", ",", "\n", "min_frequency", ":", "int", ",", "\n", "min_sent_len", ":", "int", ",", "\n", "max_sent_len", ":", "int", ",", "\n", "subsample_heuristic", ":", "Optional", "[", "str", "]", ",", "\n", "subsample_threshold", ":", "float", ",", "\n", "eval_min_freq", ":", "int", ",", "\n", "eval_R_thresholds", ":", "Iterable", "[", "float", "]", ",", "\n", "eval_num_random_samples", ":", "int", ",", "\n", "conserve_RAM", ":", "bool", "=", "True", "# turn off to inspect intermediate results", "\n", ")", "->", "None", ":", "\n", "    ", "Path", ".", "mkdir", "(", "out_dir", ",", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "preview", "=", "open", "(", "out_dir", "/", "f'preview.txt'", ",", "'w'", ")", "\n", "print", "(", "f'Min word frequency = {min_frequency}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Min sentence length = {min_sent_len}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Max sentence length = {max_sent_len}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'SGNS subsample heuristic= {subsample_heuristic}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'SGNS subsample threshold = {subsample_threshold}'", ",", "file", "=", "preview", ")", "\n", "\n", "corpus", ":", "List", "[", "LabeledDoc", "]", "=", "[", "]", "\n", "print", "(", "'Loading multi-word expression underscored pickle...'", ")", "\n", "with", "open", "(", "in_dir", "/", "f'MWE_underscored.pickle'", ",", "'rb'", ")", "as", "in_file", ":", "\n", "        ", "corpus", "+=", "pickle", ".", "load", "(", "in_file", ")", "\n", "\n", "", "norm_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "'Counting UNKs'", ")", ":", "\n", "        ", "if", "conserve_RAM", ":", "\n", "            ", "doc", ".", "text", "=", "None", "\n", "", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "            ", "norm_freq", ".", "update", "(", "sent", ".", "underscored_tokens", ")", "\n", "", "", "cumulative_freq", "=", "sum", "(", "freq", "for", "freq", "in", "norm_freq", ".", "values", "(", ")", ")", "\n", "print", "(", "f'Noramlized vocabulary size = {len(norm_freq):,}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Number of words = {cumulative_freq:,}'", ",", "file", "=", "preview", ")", "\n", "\n", "# Filter counter with MIN_FREQ and count UNK", "\n", "UNK_filtered_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "key", ",", "val", "in", "norm_freq", ".", "items", "(", ")", ":", "\n", "        ", "if", "val", ">=", "min_frequency", ":", "\n", "            ", "UNK_filtered_freq", "[", "key", "]", "=", "val", "\n", "", "else", ":", "\n", "            ", "UNK_filtered_freq", "[", "'[UNK]'", "]", "+=", "val", "\n", "", "", "print", "(", "f'Filtered vocabulary size = {len(UNK_filtered_freq):,}'", ",", "file", "=", "preview", ")", "\n", "assert", "sum", "(", "freq", "for", "freq", "in", "norm_freq", ".", "values", "(", ")", ")", "==", "cumulative_freq", "\n", "\n", "\n", "# Subsampling & filter by min/max sentence length", "\n", "ground", ":", "Dict", "[", "str", ",", "GroundedWord", "]", "=", "{", "}", "\n", "keep_prob", "=", "subsampling", "(", "\n", "UNK_filtered_freq", ",", "subsample_heuristic", ",", "subsample_threshold", ")", "\n", "final_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "'Subsampling frequent words'", ")", ":", "\n", "        ", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "            ", "for", "token", "in", "sent", ".", "underscored_tokens", ":", "\n", "                ", "if", "token", "not", "in", "UNK_filtered_freq", ":", "\n", "                    ", "token", "=", "'[UNK]'", "\n", "", "if", "random", ".", "random", "(", ")", "<", "keep_prob", "[", "token", "]", ":", "\n", "                    ", "sent", ".", "subsampled_tokens", ".", "append", "(", "token", ")", "\n", "# End looping tokens", "\n", "\n", "", "", "if", "len", "(", "sent", ".", "subsampled_tokens", ")", ">=", "min_sent_len", ":", "\n", "                ", "if", "len", "(", "sent", ".", "subsampled_tokens", ")", ">", "max_sent_len", ":", "\n", "                    ", "sent", ".", "subsampled_tokens", "=", "sent", ".", "subsampled_tokens", "[", ":", "max_sent_len", "]", "\n", "", "final_freq", ".", "update", "(", "sent", ".", "subsampled_tokens", ")", "\n", "for", "word", "in", "sent", ".", "subsampled_tokens", ":", "\n", "                    ", "if", "word", "not", "in", "ground", ":", "\n", "                        ", "ground", "[", "word", "]", "=", "GroundedWord", "(", "\n", "text", "=", "word", ",", "deno", "=", "None", ",", "cono", "=", "Counter", "(", "{", "doc", ".", "party", ":", "1", "}", ")", ")", "\n", "", "else", ":", "\n", "                        ", "ground", "[", "word", "]", ".", "cono", "[", "doc", ".", "party", "]", "+=", "1", "\n", "", "", "", "else", ":", "# discard short sentences", "\n", "                ", "sent", ".", "subsampled_tokens", "=", "None", "\n", "", "if", "conserve_RAM", ":", "\n", "                ", "sent", ".", "tokens", "=", "None", "\n", "sent", ".", "normalized_tokens", "=", "None", "\n", "sent", ".", "underscored_tokens", "=", "None", "\n", "# End looping sentences", "\n", "\n", "", "", "doc", ".", "sentences", "=", "[", "# Filter out empty sentences", "\n", "sent", "for", "sent", "in", "doc", ".", "sentences", "\n", "if", "sent", ".", "subsampled_tokens", "is", "not", "None", "]", "\n", "# End looping documents", "\n", "", "print", "(", "f'Final vocabulary size = {len(final_freq):,}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Subsampled number of words = '", "\n", "f'{sum(freq for freq in final_freq.values()):,}'", ",", "file", "=", "preview", ")", "\n", "\n", "# Filter out empty documents", "\n", "corpus", "=", "[", "doc", "for", "doc", "in", "corpus", "if", "len", "(", "doc", ".", "sentences", ")", ">", "0", "]", "\n", "\n", "# Numericalize corpus by word_ids", "\n", "word_to_id", ",", "id_to_word", "=", "build_vocabulary", "(", "final_freq", ")", "\n", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "'Converting to word ids'", ")", ":", "\n", "        ", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "            ", "sent", ".", "numerical_tokens", "=", "[", "\n", "word_to_id", "[", "token", "]", "for", "token", "in", "sent", ".", "subsampled_tokens", "]", "\n", "if", "conserve_RAM", ":", "\n", "                ", "sent", ".", "subsampled_tokens", "=", "None", "\n", "\n", "# # Compute PMI", "\n", "# def prob(count: int) -> float:", "\n", "#     return count / cumulative_freq  # presampled frequency", "\n", "\n", "# Prepare grounding for intrinsic evaluation", "\n", "", "", "", "random_eval_words", "=", "set", "(", ")", "\n", "for", "gw", "in", "ground", ".", "values", "(", ")", ":", "\n", "        ", "gw", ".", "majority_cono", "=", "gw", ".", "cono", ".", "most_common", "(", "1", ")", "[", "0", "]", "[", "0", "]", "\n", "gw", ".", "freq", "=", "sum", "(", "gw", ".", "cono", ".", "values", "(", ")", ")", "\n", "gw", ".", "R_ratio", "=", "(", "gw", ".", "cono", "[", "'right'", "]", "+", "gw", ".", "cono", "[", "'right-center'", "]", ")", "/", "gw", ".", "freq", "\n", "if", "gw", ".", "freq", ">=", "eval_min_freq", ":", "\n", "            ", "random_eval_words", ".", "add", "(", "gw", ".", "text", ")", "\n", "", "", "random_eval_words", "=", "random", ".", "sample", "(", "random_eval_words", ",", "eval_num_random_samples", ")", "\n", "with", "open", "(", "out_dir", "/", "f'eval_words_random.txt'", ",", "'w'", ")", "as", "file", ":", "\n", "        ", "file", ".", "write", "(", "'\\n'", ".", "join", "(", "random_eval_words", ")", ")", "\n", "\n", "", "for", "R_threshold", "in", "eval_R_thresholds", ":", "\n", "        ", "D_threshold", "=", "1", "-", "R_threshold", "\n", "partisan_eval_words", "=", "[", "]", "\n", "for", "gw", "in", "ground", ".", "values", "(", ")", ":", "\n", "            ", "if", "gw", ".", "freq", ">=", "eval_min_freq", ":", "\n", "                ", "if", "gw", ".", "R_ratio", ">=", "R_threshold", "or", "gw", ".", "R_ratio", "<=", "D_threshold", ":", "\n", "                    ", "partisan_eval_words", ".", "append", "(", "gw", ")", "\n", "", "", "", "print", "(", "f'{len(partisan_eval_words)} partisan eval words '", "\n", "f'with R_threshold = {R_threshold}'", ",", "file", "=", "preview", ")", "\n", "\n", "out_path", "=", "out_dir", "/", "f'inspect_{R_threshold}_partisan.tsv'", "\n", "with", "open", "(", "out_path", ",", "'w'", ")", "as", "file", ":", "\n", "            ", "print", "(", "'word\\tfreq\\tR_ratio'", ",", "file", "=", "file", ")", "\n", "for", "gw", "in", "partisan_eval_words", ":", "\n", "                ", "print", "(", "gw", ".", "text", ",", "gw", ".", "freq", ",", "gw", ".", "R_ratio", ",", "sep", "=", "'\\t'", ",", "file", "=", "file", ")", "\n", "\n", "", "", "if", "len", "(", "partisan_eval_words", ")", ">", "2", "*", "eval_num_random_samples", ":", "\n", "            ", "partisan_eval_words", "=", "random", ".", "sample", "(", "\n", "partisan_eval_words", ",", "2", "*", "eval_num_random_samples", ")", "\n", "", "else", ":", "\n", "            ", "random", ".", "shuffle", "(", "partisan_eval_words", ")", "\n", "\n", "", "mid", "=", "len", "(", "partisan_eval_words", ")", "//", "2", "\n", "with", "open", "(", "out_dir", "/", "f'{R_threshold}partisan_dev_words.txt'", ",", "'w'", ")", "as", "file", ":", "\n", "            ", "for", "gw", "in", "partisan_eval_words", "[", ":", "mid", "]", ":", "\n", "                ", "print", "(", "gw", ".", "text", ",", "file", "=", "file", ")", "\n", "", "", "with", "open", "(", "out_dir", "/", "f'{R_threshold}partisan_test_words.txt'", ",", "'w'", ")", "as", "file", ":", "\n", "            ", "for", "gw", "in", "partisan_eval_words", "[", "mid", ":", "]", ":", "\n", "                ", "print", "(", "gw", ".", "text", ",", "file", "=", "file", ")", "\n", "\n", "# ground: Dict[str, GroundedWord] = {}", "\n", "# # cono_labels = set(numericalize_cono.values())", "\n", "# for word in tqdm(word_to_id.keys(), desc='Computing PMIs (-\u221e are okay)'):", "\n", "#     cono = np.array(cono_freq[word])", "\n", "#     # cono_ratio = cono / np.sum(cono)", "\n", "#     # PMI = np.log2([  # can be -inf if freq = 0", "\n", "#     #     prob(cono[party_id])", "\n", "#     #     / (prob(norm_freq[word]) * prob(party_cumulative[party_id]))", "\n", "#     #     for party_id in cono_labels])", "\n", "#     ground[word] = GroundedWord(text=word, deno=None, cono=cono)", "\n", "\n", "# Helper for negative sampling", "\n", "", "", "", "cumulative_freq", "=", "sum", "(", "freq", "**", "0.75", "for", "freq", "in", "final_freq", ".", "values", "(", ")", ")", "\n", "negative_sampling_probs", ":", "Dict", "[", "int", ",", "float", "]", "=", "{", "\n", "word_to_id", "[", "word", "]", ":", "(", "freq", "**", "0.75", ")", "/", "cumulative_freq", "\n", "for", "word", ",", "freq", "in", "final_freq", ".", "items", "(", ")", "}", "\n", "negative_sampling_probs", ":", "List", "[", "float", "]", "=", "[", "\n", "# negative_sampling_probs[word_id]  # strict", "\n", "negative_sampling_probs", ".", "get", "(", "word_id", ",", "0", ")", "# prob = 0 if missing vocab", "\n", "for", "word_id", "in", "range", "(", "len", "(", "word_to_id", ")", ")", "]", "\n", "\n", "random", ".", "shuffle", "(", "corpus", ")", "\n", "cucumbers", "=", "{", "\n", "'word_to_id'", ":", "word_to_id", ",", "\n", "'id_to_word'", ":", "id_to_word", ",", "\n", "'ground'", ":", "ground", ",", "\n", "'negative_sampling_probs'", ":", "negative_sampling_probs", ",", "\n", "'documents'", ":", "corpus", "}", "\n", "print", "(", "f'Writing to {out_dir}'", ")", "\n", "with", "open", "(", "out_dir", "/", "'train.pickle'", ",", "'wb'", ")", "as", "out_file", ":", "\n", "        ", "pickle", ".", "dump", "(", "cucumbers", ",", "out_file", ",", "protocol", "=", "-", "1", ")", "\n", "\n", "# Print out vocabulary & some random sentences for sanity check", "\n", "", "docs", "=", "random", ".", "sample", "(", "corpus", ",", "100", ")", "\n", "preview", ".", "write", "(", "'\\n'", ")", "\n", "for", "doc", "in", "docs", ":", "\n", "        ", "sent", "=", "doc", ".", "sentences", "[", "0", "]", "\n", "if", "not", "conserve_RAM", ":", "\n", "            ", "print", "(", "sent", ".", "tokens", ",", "file", "=", "preview", ")", "\n", "print", "(", "sent", ".", "normalized_tokens", ",", "file", "=", "preview", ")", "\n", "print", "(", "sent", ".", "subsampled_tokens", ",", "file", "=", "preview", ")", "\n", "print", "(", "sent", ".", "numerical_tokens", ",", "file", "=", "preview", ",", "end", "=", "'\\n\\n'", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "sent", ".", "numerical_tokens", ",", "file", "=", "preview", ")", "\n", "# print(vars(doc), end='\\n\\n', file=preview)", "\n", "", "", "preview", ".", "write", "(", "'\\n\\nfinal_freq\\tword\\n'", ")", "\n", "for", "key", ",", "val", "in", "final_freq", ".", "most_common", "(", ")", ":", "\n", "        ", "print", "(", "f'{val:,}\\t{ground[key]}'", ",", "file", "=", "preview", ")", "\n", "", "preview", ".", "close", "(", ")", "\n", "print", "(", "'All set!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S3_export_plain_text.main": [[12, 108], ["pathlib.Path.mkdir", "open", "print", "print", "print", "print", "typing.Counter", "tqdm.tqdm", "print", "print", "typing.Counter", "norm_freq.items", "print", "typing.Counter", "tqdm.tqdm", "print", "print", "random.sample", "open.write", "open.write", "final_freq.most_common", "open.close", "print", "open", "pickle.load", "open", "tqdm.tqdm", "print", "norm_freq.update", "train_file.write", "print", "print", "print", "print", "len", "sum", "len", "len", "len", "len", "sum", "print", "len", "final_freq.update", "final_freq.update", "norm_freq.values", "final_freq.values"], "function", ["None"], ["def", "main", "(", "\n", "in_dir", ":", "Path", ",", "\n", "out_dir", ":", "Path", ",", "\n", "min_frequency", ":", "int", ",", "\n", "min_sent_len", ":", "int", ",", "\n", "max_sent_len", ":", "int", ",", "\n", "conserve_RAM", ":", "bool", "=", "False", "\n", ")", "->", "None", ":", "\n", "    ", "Path", ".", "mkdir", "(", "out_dir", ",", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "preview", "=", "open", "(", "out_dir", "/", "f'preview.txt'", ",", "'w'", ")", "\n", "print", "(", "f'Min word frequency = {min_frequency}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Min sentence length = {min_sent_len}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Max sentence length = {max_sent_len}'", ",", "file", "=", "preview", ")", "\n", "\n", "corpus", ":", "List", "[", "LabeledDoc", "]", "=", "[", "]", "\n", "print", "(", "'Loading multi-word expression underscored pickle...'", ")", "\n", "with", "open", "(", "in_dir", "/", "f'MWE_underscored.pickle'", ",", "'rb'", ")", "as", "in_file", ":", "\n", "        ", "corpus", "+=", "pickle", ".", "load", "(", "in_file", ")", "\n", "\n", "", "norm_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "'Counting UNKs'", ")", ":", "\n", "        ", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "            ", "norm_freq", ".", "update", "(", "sent", ".", "underscored_tokens", ")", "\n", "", "", "print", "(", "f'Noramlized vocabulary size = {len(norm_freq):,}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Number of words = {sum(freq for freq in norm_freq.values()):,}'", ",", "\n", "file", "=", "preview", ")", "\n", "\n", "# Filter counter with MIN_FREQ and count UNK", "\n", "UNK_filtered_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "key", ",", "val", "in", "norm_freq", ".", "items", "(", ")", ":", "\n", "        ", "if", "val", ">=", "min_frequency", ":", "\n", "            ", "UNK_filtered_freq", "[", "key", "]", "=", "val", "\n", "", "else", ":", "\n", "            ", "UNK_filtered_freq", "[", "'<UNK>'", "]", "+=", "val", "\n", "", "", "print", "(", "f'UNK-filtered vocabulary size = {len(UNK_filtered_freq):,}'", ",", "file", "=", "preview", ")", "\n", "\n", "final_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "'Filtering by sentence length'", ")", ":", "\n", "        ", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "            ", "sent", ".", "subsampled_tokens", "=", "[", "\n", "token", "if", "token", "in", "UNK_filtered_freq", "else", "'<UNK>'", "\n", "for", "token", "in", "sent", ".", "underscored_tokens", "]", "\n", "if", "len", "(", "sent", ".", "subsampled_tokens", ")", ">=", "min_sent_len", ":", "\n", "                ", "if", "len", "(", "sent", ".", "subsampled_tokens", ")", "<=", "max_sent_len", ":", "\n", "                    ", "final_freq", ".", "update", "(", "sent", ".", "subsampled_tokens", ")", "\n", "", "else", ":", "# NOTE truncate long sentences", "\n", "                    ", "sent", ".", "subsampled_tokens", "=", "sent", ".", "subsampled_tokens", "[", ":", "max_sent_len", "]", "\n", "final_freq", ".", "update", "(", "sent", ".", "subsampled_tokens", ")", "\n", "", "", "else", ":", "# discard short sentences", "\n", "                ", "sent", ".", "subsampled_tokens", "=", "None", "\n", "", "if", "conserve_RAM", ":", "\n", "                ", "del", "sent", ".", "normalized_tokens", "\n", "del", "sent", ".", "underscored_tokens", "\n", "# End looping sentences", "\n", "", "", "doc", ".", "sentences", "=", "[", "\n", "sent", "for", "sent", "in", "doc", ".", "sentences", "\n", "if", "sent", ".", "subsampled_tokens", "is", "not", "None", "]", "\n", "# End looping documents", "\n", "", "corpus", "=", "[", "doc", "for", "doc", "in", "corpus", "if", "len", "(", "doc", ".", "sentences", ")", ">", "0", "]", "\n", "\n", "print", "(", "f'Final vocabulary size = {len(final_freq):,}'", ",", "file", "=", "preview", ")", "\n", "print", "(", "f'Sentence-length filtered number of words = '", "\n", "f'{sum(freq for freq in final_freq.values()):,}'", ",", "\n", "file", "=", "preview", ")", "\n", "\n", "# random.shuffle(corpus)", "\n", "# # Print one sentence per line", "\n", "# with open(out_dir / 'train.txt', 'w') as train_file:", "\n", "#     for doc in tqdm(corpus, desc=f'Writing to {out_dir}'):", "\n", "#         for sent in doc.sentences:", "\n", "#             print(' '.join(sent.subsampled_tokens), file=train_file)", "\n", "# Print one document per line", "\n", "with", "open", "(", "out_dir", "/", "'train.txt'", ",", "'w'", ")", "as", "train_file", ":", "\n", "        ", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "f'Writing to {out_dir}'", ")", ":", "\n", "            ", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "                ", "print", "(", "' '", ".", "join", "(", "sent", ".", "subsampled_tokens", ")", ",", "end", "=", "' '", ",", "file", "=", "train_file", ")", "\n", "", "train_file", ".", "write", "(", "'\\n'", ")", "\n", "\n", "\n", "# Print out vocabulary & some random sentences for sanity check", "\n", "", "", "docs", "=", "random", ".", "sample", "(", "corpus", ",", "100", ")", "\n", "preview", ".", "write", "(", "'\\n'", ")", "\n", "for", "doc", "in", "docs", ":", "\n", "        ", "sent", "=", "doc", ".", "sentences", "[", "0", "]", "\n", "if", "not", "conserve_RAM", ":", "\n", "            ", "print", "(", "sent", ".", "tokens", ",", "file", "=", "preview", ")", "\n", "print", "(", "sent", ".", "normalized_tokens", ",", "file", "=", "preview", ")", "\n", "print", "(", "sent", ".", "subsampled_tokens", ",", "file", "=", "preview", ",", "end", "=", "'\\n\\n'", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "sent", ".", "numerical_tokens", ",", "file", "=", "preview", ")", "\n", "# print(vars(doc), end='\\n\\n', file=preview)", "\n", "", "", "preview", ".", "write", "(", "'\\n\\nword\\tsentence_length_filtered_freq\\n'", ")", "\n", "for", "key", ",", "val", "in", "final_freq", ".", "most_common", "(", ")", ":", "\n", "        ", "print", "(", "f'{val:,}:\\t{key}'", ",", "file", "=", "preview", ")", "\n", "", "preview", ".", "close", "(", ")", "\n", "print", "(", "'All set!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1.main": [[15, 78], ["pathlib.Path.mkdir", "open", "tqdm.tqdm", "set", "typing.Counter", "re.compile", "re.compile", "re.compile", "typing.Counter", "tqdm.tqdm", "print", "typing.Counter.most_common", "range", "len", "print", "open", "pickle.load", "tuple", "re.compile.search", "re.compile.search", "sent.normalized_tokens.append", "existed.add", "re.compile.search", "token.lower"], "function", ["None"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize_with_NER.parse_xml": [[36, 83], ["print", "xml.parse", "xml.parse", "print", "print", "set", "ET.parse.getroot", "print", "print", "article.itertext", "ET.parse.getroot", "para.strip.strip", "existed.add", "data.append", "print", "len", "text.append", "S1_tokenize_with_NER.LabeledDoc", "len", "bool", "attr.get"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S1_qsub_CoreNLP.parse", "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S1_qsub_CoreNLP.parse"], ["", "def", "parse_xml", "(", "\n", "corpus_path", ":", "Path", ",", "\n", "metadata_path", ":", "Path", ",", "\n", ")", "->", "List", "[", "LabeledDoc", "]", ":", "\n", "    ", "print", "(", "'Loading XML...'", ")", "\n", "metadata", "=", "ET", ".", "parse", "(", "metadata_path", ")", "# type: ignore", "\n", "corpus", "=", "ET", ".", "parse", "(", "corpus_path", ")", "# type: ignore", "\n", "\n", "labels", "=", "{", "\n", "stuff", ".", "attrib", "[", "'id'", "]", ":", "stuff", ".", "attrib", "\n", "for", "stuff", "in", "metadata", ".", "getroot", "(", ")", "}", "\n", "print", "(", "f'Number of metadata entries = {len(labels):,}'", ")", "\n", "\n", "print", "(", "'Parsing XML...'", ")", "\n", "data", ":", "List", "[", "LabeledDoc", "]", "=", "[", "]", "\n", "existed", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "duplicate_count", "=", "0", "\n", "for", "article", "in", "corpus", ".", "getroot", "(", ")", ":", "\n", "        ", "attr", "=", "article", ".", "attrib", "\n", "text", "=", "[", "]", "\n", "# Joining the body text of the document.", "\n", "for", "para", "in", "article", ".", "itertext", "(", ")", ":", "# pseudo-paragraph", "\n", "            ", "para", "=", "para", ".", "strip", "(", ")", "\n", "if", "para", ":", "# not whitespace", "\n", "                ", "text", ".", "append", "(", "para", ")", "\n", "", "", "text", "=", "' '", ".", "join", "(", "text", ")", "\n", "if", "text", "in", "existed", ":", "# NOTE better deduplication to come later. Sorry!", "\n", "            ", "duplicate_count", "+=", "1", "\n", "continue", "\n", "", "else", ":", "\n", "            ", "existed", ".", "add", "(", "text", ")", "\n", "\n", "", "if", "text", ":", "# nonempty", "\n", "            ", "label", "=", "labels", "[", "attr", "[", "'id'", "]", "]", "\n", "data", ".", "append", "(", "LabeledDoc", "(", "\n", "uid", "=", "attr", "[", "'id'", "]", ",", "\n", "party", "=", "label", "[", "'bias'", "]", ",", "\n", "partisan", "=", "bool", "(", "label", "[", "'hyperpartisan'", "]", ")", ",", "\n", "url", "=", "label", "[", "'url'", "]", ",", "\n", "title", "=", "attr", "[", "'title'", "]", ",", "\n", "date", "=", "attr", ".", "get", "(", "'published-at'", ",", "None", ")", ",", "\n", "text", "=", "text", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Missing:'", ",", "article", ".", "attrib", ")", "\n", "", "", "print", "(", "f'Number of duplicated articles = {duplicate_count:,}'", ")", "\n", "print", "(", "f'Number of nonduplicate articles = {len(data):,}'", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize_with_NER.underscore_NER": [[85, 119], ["enumerate", "token.ner.split", "new_seq.append", "new_seq.append", "new_seq.append", "new_seq.append", "token_ahead.ner.split", "joint_token.append", "joint_token.append", "print", "print", "print", "RuntimeError"], "function", ["None"], ["", "def", "underscore_NER", "(", "tokens", ")", ":", "\n", "    ", "new_seq", "=", "[", "]", "\n", "for", "index", ",", "token", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "if", "token", ".", "ner", "==", "'O'", ":", "# No NER", "\n", "            ", "new_seq", ".", "append", "(", "token", ".", "text", ")", "\n", "continue", "\n", "", "prefix", ",", "ent_type", "=", "token", ".", "ner", ".", "split", "(", "'-'", ")", "\n", "if", "ent_type", "in", "EXCLUDE_NER_TYPES", ":", "\n", "            ", "new_seq", ".", "append", "(", "'<NUM>'", ")", "\n", "", "elif", "prefix", "==", "'S'", ":", "# single-token length", "\n", "            ", "new_seq", ".", "append", "(", "token", ".", "text", ")", "\n", "\n", "", "elif", "prefix", "==", "'B'", ":", "# multi-token phrase", "\n", "            ", "joint_token", "=", "[", "token", ".", "text", ",", "]", "\n", "for", "token_ahead", "in", "tokens", "[", "index", "+", "1", ":", "]", ":", "\n", "                ", "if", "token_ahead", ".", "ner", "==", "'O'", ":", "\n", "                    ", "break", "\n", "", "prefix_ahead", ",", "_", "=", "token_ahead", ".", "ner", ".", "split", "(", "'-'", ")", "\n", "if", "prefix_ahead", "==", "'I'", ":", "\n", "                    ", "joint_token", ".", "append", "(", "token_ahead", ".", "text", ")", "\n", "", "elif", "prefix_ahead", "==", "'E'", ":", "\n", "                    ", "joint_token", ".", "append", "(", "token_ahead", ".", "text", ")", "\n", "break", "\n", "", "elif", "prefix_ahead", "==", "'B'", "or", "prefix_ahead", "==", "'S'", ":", "\n", "                    ", "break", "\n", "", "else", ":", "\n", "                    ", "print", "(", "token", ",", "token", ".", "ner", ")", "\n", "print", "(", "token_ahead", ",", "token", ".", "ner", ")", "\n", "print", "(", "[", "(", "t", ".", "text", ",", "t", ".", "ner", ")", "for", "t", "in", "tokens", "]", ")", "\n", "raise", "RuntimeError", "(", "'Malformed NER IOB tag'", ")", "\n", "", "", "new_seq", ".", "append", "(", "'_'", ".", "join", "(", "joint_token", ")", ")", "\n", "", "else", ":", "\n", "            ", "continue", "\n", "", "", "return", "new_seq", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize_with_NER.partition": [[121, 130], ["len"], "function", ["None"], ["", "def", "partition", "(", "corpus", ":", "List", ",", "num_chunks", ":", "int", ")", "->", "Iterable", "[", "List", "]", ":", "\n", "    ", "chunk_size", "=", "len", "(", "corpus", ")", "//", "num_chunks", "\n", "corpus_index", "=", "0", "\n", "chunk_index", "=", "0", "\n", "while", "chunk_index", "<", "num_chunks", "-", "1", ":", "\n", "        ", "yield", "corpus", "[", "corpus_index", ":", "corpus_index", "+", "chunk_size", "]", "\n", "corpus_index", "+=", "chunk_size", "\n", "chunk_index", "+=", "1", "\n", "", "yield", "corpus", "[", "corpus_index", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize_with_NER.main": [[132, 177], ["pathlib.Path", "pathlib.Path.mkdir", "S1_tokenize_with_NER.parse_xml", "stanza.Pipeline", "pathlib.Path", "pathlib.Path.mkdir", "tqdm.tqdm", "pathlib.Path.home", "enumerate", "tqdm.tqdm", "S1_tokenize_with_NER.partition", "stanza.Pipeline.", "open", "pickle.dump", "S1_tokenize_with_NER.Sentence", "S1_tokenize_with_NER.underscore_NER"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize.parse_xml", "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize.partition", "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize_with_NER.underscore_NER"], ["", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "in_dir", "=", "Path", ".", "home", "(", ")", "/", "'Research/hyperpartisan_news'", "\n", "out_dir", "=", "Path", "(", "'../../data/interim/news/validation'", ")", "\n", "Path", ".", "mkdir", "(", "out_dir", ",", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "# metadata = in_dir / 'ground-truth-training-bypublisher-20181122.xml'", "\n", "# corpus = in_dir / 'articles-training-bypublisher-20181122.xml'", "\n", "dev_corpus", "=", "in_dir", "/", "'articles-validation-bypublisher-20181122.xml'", "\n", "dev_metadata", "=", "in_dir", "/", "'ground-truth-validation-bypublisher-20181122.xml'", "\n", "data", "=", "parse_xml", "(", "dev_corpus", ",", "dev_metadata", ")", "\n", "\n", "stanza_processor", "=", "stanza", ".", "Pipeline", "(", "\n", "lang", "=", "'en'", ",", "processors", "=", "'tokenize,ner'", ",", "\n", "tokenize_batch_size", "=", "32", ",", "ner_batch_size", "=", "32", ")", "\n", "# stanza_processor = stanza.Pipeline(", "\n", "#   lang='en', processors={'tokenize': 'spacy'})", "\n", "\n", "\n", "# NOTE further dividing data for distributing among GPUs", "\n", "# data = data[:len(data) // 2]  # first half", "\n", "# data = data[len(data) // 2:]  # second half", "\n", "\n", "out_dir", "=", "Path", "(", "'../../data/interim/news/validation'", ")", "\n", "Path", ".", "mkdir", "(", "out_dir", ",", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "\n", "num_chunks", "=", "100", "\n", "for", "part_index", ",", "some_docs", "in", "tqdm", "(", "\n", "enumerate", "(", "partition", "(", "data", ",", "num_chunks", ")", ")", ",", "\n", "total", "=", "num_chunks", ",", "\n", "desc", "=", "'Total'", ")", ":", "\n", "\n", "# for doc in corpus:", "\n", "#     doc.compressed = [", "\n", "#         Sentence(underscore_NER(stanza_sent.tokens))", "\n", "#         for stanza_sent in doc.sentences]", "\n", "        ", "for", "doc", "in", "tqdm", "(", "some_docs", ",", "desc", "=", "'Chunk'", ")", ":", "\n", "            ", "processed", "=", "stanza_processor", "(", "doc", ".", "text", ")", "\n", "doc", ".", "sentences", "=", "[", "# throw away extra info to conserve disk space", "\n", "Sentence", "(", "\n", "tokens", "=", "[", "token", ".", "text", "for", "token", "in", "stanza_sent", ".", "tokens", "]", ",", "\n", "underscored_tokens", "=", "underscore_NER", "(", "stanza_sent", ".", "tokens", ")", ",", "\n", "entities", "=", "[", "(", "ent", ".", "type", ",", "ent", ".", "text", ")", "for", "ent", "in", "stanza_sent", ".", "ents", "]", ")", "\n", "for", "stanza_sent", "in", "processed", ".", "sentences", "\n", "]", "\n", "", "with", "open", "(", "out_dir", "/", "f'tokenized_{part_index}.pickle'", ",", "'wb'", ")", "as", "file", ":", "\n", "            ", "pickle", ".", "dump", "(", "some_docs", ",", "file", ",", "protocol", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S2_underscore_phrases.main": [[15, 134], ["pathlib.Path.mkdir", "open", "tqdm.tqdm", "re.compile", "re.compile", "re.compile", "typing.Counter", "set", "tqdm.tqdm", "print", "typing.Counter", "norm_freq.items", "print", "print", "print", "nltk.collocations.BigramCollocationFinder.from_words", "len", "BigramCollocationFinder.from_words.apply_freq_filter", "set().union", "BigramCollocationFinder.from_words.apply_word_filter", "BigramCollocationFinder.from_words.score_ngrams", "print", "print", "nltk.tokenize.MWETokenizer", "typing.Counter", "tqdm.tqdm", "print", "vocab.most_common", "open.close", "range", "open", "open", "pickle.dump", "open", "pickle.load", "tuple", "set", "nltk.collocations.BigramAssocMeasures", "bigram_file.write", "nltk.tokenize.MWETokenizer.tokenize", "vocab.update", "print", "re.compile.search", "re.compile.search", "sent.normalized_tokens.append", "existed.add", "len", "len", "nltk.corpus.stopwords.words", "len", "len", "re.compile.search", "token.lower", "tuple"], "function", ["None"], ["def", "main", "(", "\n", "in_dir", ":", "Path", ",", "\n", "out_dir", ":", "Path", ",", "\n", "num_corpus_chunks", ":", "int", ",", "\n", "min_frequency", ":", "int", ",", "\n", "conserve_RAM", ":", "bool", "=", "False", "\n", ")", "->", "None", ":", "\n", "    ", "Path", ".", "mkdir", "(", "out_dir", ",", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "preview", "=", "open", "(", "out_dir", "/", "f'vocab.txt'", ",", "'w'", ")", "\n", "\n", "corpus", ":", "List", "[", "LabeledDoc", "]", "=", "[", "]", "\n", "for", "part_index", "in", "tqdm", "(", "range", "(", "num_corpus_chunks", ")", ",", "desc", "=", "'Loading cache'", ")", ":", "\n", "        ", "with", "open", "(", "in_dir", "/", "f'tokenized_{part_index}.pickle'", ",", "'rb'", ")", "as", "in_file", ":", "\n", "            ", "corpus", "+=", "pickle", ".", "load", "(", "in_file", ")", "\n", "\n", "# Lowercase, discard punctuations, replace numbers, deduplicate", "\n", "", "", "number", "=", "re", ".", "compile", "(", "r'\\d'", ")", "\n", "starts_with_letter", "=", "re", ".", "compile", "(", "r\"^\\w\"", ")", "\n", "select_punctuations", "=", "re", ".", "compile", "(", "r\"[@#&:]|.com\"", ")", "\n", "norm_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "existed", ":", "Set", "[", "Tuple", "[", "str", ",", "...", "]", "]", "=", "set", "(", ")", "\n", "duplicates", "=", "0", "\n", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "'Normalizing tokens'", ")", ":", "\n", "        ", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "            ", "for", "token", "in", "sent", ".", "tokens", ":", "\n", "                ", "if", "not", "starts_with_letter", ".", "search", "(", "token", ")", ":", "\n", "                    ", "continue", "\n", "", "if", "select_punctuations", ".", "search", "(", "token", ")", ":", "\n", "                    ", "continue", "\n", "", "if", "number", ".", "search", "(", "token", ")", ":", "\n", "                    ", "norm_token", "=", "'<NUM>'", "\n", "", "else", ":", "\n", "                    ", "norm_token", "=", "token", ".", "lower", "(", ")", "\n", "", "sent", ".", "normalized_tokens", ".", "append", "(", "norm_token", ")", "\n", "norm_freq", "[", "norm_token", "]", "+=", "1", "\n", "", "if", "conserve_RAM", ":", "\n", "                ", "del", "sent", ".", "tokens", "\n", "# all_norm_tokens += sent.normalized_tokens", "\n", "", "hashable", "=", "tuple", "(", "sent", ".", "normalized_tokens", ")", "\n", "if", "hashable", "not", "in", "existed", ":", "\n", "                ", "existed", ".", "add", "(", "hashable", ")", "\n", "", "else", ":", "\n", "                ", "duplicates", "+=", "1", "\n", "\n", "", "", "doc", ".", "sentences", "=", "[", "# Filter out duplicate sentences", "\n", "sent", "for", "sent", "in", "doc", ".", "sentences", "\n", "if", "tuple", "(", "sent", ".", "tokens", ")", "not", "in", "existed", "]", "\n", "", "print", "(", "f'Number of duplicate sentences = {duplicates:,}'", ")", "\n", "\n", "\n", "UNK_filtered_freq", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "key", ",", "val", "in", "norm_freq", ".", "items", "(", ")", ":", "\n", "        ", "if", "val", ">=", "min_frequency", ":", "\n", "            ", "UNK_filtered_freq", "[", "key", "]", "=", "val", "\n", "", "else", ":", "\n", "            ", "UNK_filtered_freq", "[", "'<UNK>'", "]", "+=", "val", "\n", "", "", "print", "(", "f'Number of filtered unigrams = {len(UNK_filtered_freq):,}'", ")", "\n", "print", "(", "f'Number of filtered unigrams = {len(UNK_filtered_freq):,}'", ",", "file", "=", "preview", ")", "\n", "\n", "\n", "all_norm_tokens", ":", "List", "[", "str", "]", "=", "[", "\n", "nt", "\n", "for", "doc", "in", "corpus", "\n", "for", "sent", "in", "doc", ".", "sentences", "\n", "for", "nt", "in", "sent", ".", "normalized_tokens", "]", "\n", "\n", "special_tokens", "=", "{", "'<UNK>'", ",", "'<NUM>'", ",", "\"n't\"", ",", "\"n\u2019t\"", "}", "\n", "print", "(", "'Finding bigrams...'", ")", "\n", "bigram_finder", "=", "BigramCollocationFinder", ".", "from_words", "(", "all_norm_tokens", ")", "\n", "num_tokens", "=", "len", "(", "all_norm_tokens", ")", "\n", "bigram_finder", ".", "apply_freq_filter", "(", "min_frequency", ")", "\n", "stop_words", "=", "set", "(", "stopwords", ".", "words", "(", "'english'", ")", ")", ".", "union", "(", "special_tokens", ")", "\n", "bigram_finder", ".", "apply_word_filter", "(", "lambda", "word", ":", "word", "in", "stop_words", ")", "\n", "bigrams", "=", "bigram_finder", ".", "score_ngrams", "(", "BigramAssocMeasures", "(", ")", ".", "raw_freq", ")", "\n", "# bigrams = bigram_finder.score_ngrams(BigramAssocMeasures().pmi)", "\n", "print", "(", "f'Number of filtered bigrams = {len(bigrams):,}'", ")", "\n", "print", "(", "f'Number of filtered bigrams = {len(bigrams):,}'", ",", "file", "=", "preview", ")", "\n", "with", "open", "(", "out_dir", "/", "'bigrams.txt'", ",", "'w'", ")", "as", "bigram_file", ":", "\n", "        ", "for", "bigram", ",", "relative_freq", "in", "bigrams", ":", "\n", "            ", "absolute_freq", "=", "relative_freq", "*", "num_tokens", "\n", "bigram_str", "=", "' '", ".", "join", "(", "bigram", ")", "\n", "# bigram_file.write(f'{relative_freq:.4f}\\t{bigram_str}\\n')  # for PMI", "\n", "bigram_file", ".", "write", "(", "f'{absolute_freq:.0f}\\t{bigram_str}\\n'", ")", "\n", "\n", "# print('Finding trigrams...')", "\n", "# trigram_finder = TrigramCollocationFinder.from_words(all_norm_tokens)", "\n", "# trigram_finder.apply_freq_filter(min_frequency)", "\n", "# trigram_finder.apply_word_filter(lambda word: word in stop_words)", "\n", "# # trigram_finder.apply_ngram_filter(", "\n", "# #     lambda w1, w2, w3: (w1 in stop_words) or (w3 in stop_words) or (w2 in special_tokens))", "\n", "# trigrams = trigram_finder.score_ngrams(TrigramAssocMeasures().raw_freq)", "\n", "# print(f'Number of filtered trigrams = {len(trigrams):,}')", "\n", "# print(f'Number of filtered trigrams = {len(trigrams):,}', file=preview)", "\n", "# with open(out_dir / 'trigrams.txt', 'w') as trigram_file:", "\n", "#     for trigram, relative_freq in trigrams:", "\n", "#         absolute_freq = relative_freq * num_tokens", "\n", "#         trigram_str = ' '.join(trigram)", "\n", "#         trigram_file.write(f'{absolute_freq:.0f}\\t{trigram_str}\\n')", "\n", "", "", "del", "all_norm_tokens", "\n", "\n", "# Multi-Word Expression tokenize to underscored", "\n", "underscorer", "=", "MWETokenizer", "(", "[", "bi", "for", "bi", ",", "_", "in", "bigrams", "]", ")", "# maybe add affordable care act", "\n", "# underscorer = MWETokenizer(", "\n", "#     [tri for tri, _ in trigrams] + [bi for bi, _ in bigrams])", "\n", "vocab", ":", "Counter", "[", "str", "]", "=", "Counter", "(", ")", "\n", "for", "doc", "in", "tqdm", "(", "corpus", ",", "desc", "=", "'Underscoring multi-phrase expressions'", ")", ":", "\n", "        ", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "            ", "sent", ".", "underscored_tokens", "=", "underscorer", ".", "tokenize", "(", "sent", ".", "normalized_tokens", ")", "\n", "vocab", ".", "update", "(", "sent", ".", "underscored_tokens", ")", "\n", "if", "conserve_RAM", ":", "\n", "                ", "del", "sent", ".", "normalized_tokens", "\n", "", "", "", "print", "(", "'Pickling...'", ")", "\n", "with", "open", "(", "out_dir", "/", "'MWE_underscored.pickle'", ",", "'wb'", ")", "as", "out_file", ":", "\n", "        ", "pickle", ".", "dump", "(", "corpus", ",", "out_file", ")", "\n", "\n", "", "for", "key", ",", "val", "in", "vocab", ".", "most_common", "(", ")", ":", "\n", "        ", "if", "val", ">=", "min_frequency", ":", "\n", "            ", "print", "(", "f'{val:,}:\\t{key}'", ",", "file", "=", "preview", ")", "\n", "", "", "preview", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize.parse_xml": [[12, 59], ["print", "xml.parse", "xml.parse", "print", "print", "set", "ET.parse.getroot", "print", "print", "article.itertext", "ET.parse.getroot", "para.strip.strip", "existed.add", "data.append", "print", "len", "text.append", "data.LabeledDoc", "len", "bool", "attr.get"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S1_qsub_CoreNLP.parse", "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_CR.S1_qsub_CoreNLP.parse"], ["def", "parse_xml", "(", "\n", "corpus_path", ":", "Path", ",", "\n", "metadata_path", ":", "Path", ",", "\n", ")", "->", "List", "[", "LabeledDoc", "]", ":", "\n", "    ", "print", "(", "'Loading XML...'", ")", "\n", "metadata", "=", "ET", ".", "parse", "(", "metadata_path", ")", "# type: ignore", "\n", "corpus", "=", "ET", ".", "parse", "(", "corpus_path", ")", "# type: ignore", "\n", "\n", "labels", "=", "{", "\n", "stuff", ".", "attrib", "[", "'id'", "]", ":", "stuff", ".", "attrib", "\n", "for", "stuff", "in", "metadata", ".", "getroot", "(", ")", "}", "\n", "print", "(", "f'Number of metadata entries = {len(labels):,}'", ")", "\n", "\n", "print", "(", "'Parsing XML...'", ")", "\n", "data", ":", "List", "[", "LabeledDoc", "]", "=", "[", "]", "\n", "existed", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "duplicate_count", "=", "0", "\n", "for", "article", "in", "corpus", ".", "getroot", "(", ")", ":", "\n", "        ", "attr", "=", "article", ".", "attrib", "\n", "text", "=", "[", "]", "\n", "# Joining the body text of the document.", "\n", "for", "para", "in", "article", ".", "itertext", "(", ")", ":", "# pseudo-paragraph", "\n", "            ", "para", "=", "para", ".", "strip", "(", ")", "\n", "if", "para", ":", "# not whitespace", "\n", "                ", "text", ".", "append", "(", "para", ")", "\n", "", "", "text", "=", "' '", ".", "join", "(", "text", ")", "\n", "if", "text", "in", "existed", ":", "# NOTE better deduplication to come later. Sorry!", "\n", "            ", "duplicate_count", "+=", "1", "\n", "continue", "\n", "", "else", ":", "\n", "            ", "existed", ".", "add", "(", "text", ")", "\n", "\n", "", "if", "text", ":", "# nonempty", "\n", "            ", "label", "=", "labels", "[", "attr", "[", "'id'", "]", "]", "\n", "data", ".", "append", "(", "LabeledDoc", "(", "\n", "uid", "=", "attr", "[", "'id'", "]", ",", "\n", "party", "=", "label", "[", "'bias'", "]", ",", "\n", "partisan", "=", "bool", "(", "label", "[", "'hyperpartisan'", "]", ")", ",", "\n", "url", "=", "label", "[", "'url'", "]", ",", "\n", "title", "=", "attr", "[", "'title'", "]", ",", "\n", "date", "=", "attr", ".", "get", "(", "'published-at'", ",", "None", ")", ",", "\n", "text", "=", "text", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "'Missing:'", ",", "article", ".", "attrib", ")", "\n", "", "", "print", "(", "f'Number of duplicated articles = {duplicate_count:,}'", ")", "\n", "print", "(", "f'Number of nonduplicate articles = {len(data):,}'", ")", "\n", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize.partition": [[61, 70], ["len"], "function", ["None"], ["", "def", "partition", "(", "corpus", ":", "List", ",", "num_chunks", ":", "int", ")", "->", "Iterable", "[", "List", "]", ":", "\n", "    ", "chunk_size", "=", "len", "(", "corpus", ")", "//", "num_chunks", "\n", "corpus_index", "=", "0", "\n", "chunk_index", "=", "0", "\n", "while", "chunk_index", "<", "num_chunks", "-", "1", ":", "\n", "        ", "yield", "corpus", "[", "corpus_index", ":", "corpus_index", "+", "chunk_size", "]", "\n", "corpus_index", "+=", "chunk_size", "\n", "chunk_index", "+=", "1", "\n", "", "yield", "corpus", "[", "corpus_index", ":", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize.main": [[72, 95], ["pathlib.Path", "pathlib.Path.mkdir", "S1_tokenize.parse_xml", "stanza.Pipeline", "tqdm.tqdm", "pathlib.Path.home", "enumerate", "tqdm.tqdm", "S1_tokenize.partition", "stanza.Pipeline.", "open", "pickle.dump", "data.Sentence"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize.parse_xml", "home.repos.pwc.inspect_result.awebson_congressional_adversary.preprocess_PN.S1_tokenize.partition"], ["", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "in_dir", "=", "Path", ".", "home", "(", ")", "/", "'Research/hyperpartisan_news'", "\n", "out_dir", "=", "Path", "(", "'../../data/interim/news/train'", ")", "\n", "Path", ".", "mkdir", "(", "out_dir", ",", "parents", "=", "True", ",", "exist_ok", "=", "True", ")", "\n", "corpus", "=", "in_dir", "/", "'articles-training-bypublisher-20181122.xml'", "\n", "metadata", "=", "in_dir", "/", "'ground-truth-training-bypublisher-20181122.xml'", "\n", "# dev_corpus = in_dir / 'articles-validation-bypublisher-20181122.xml'", "\n", "# dev_metadata = in_dir / 'ground-truth-validation-bypublisher-20181122.xml'", "\n", "data", "=", "parse_xml", "(", "corpus", ",", "metadata", ")", "\n", "\n", "processor", "=", "stanza", ".", "Pipeline", "(", "\n", "lang", "=", "'en'", ",", "processors", "=", "'tokenize'", ",", "tokenize_batch_size", "=", "4096", ")", "\n", "# processor = stanza.Pipeline(lang='en', processors={'tokenize': 'spacy'})", "\n", "\n", "for", "part_index", ",", "some_docs", "in", "tqdm", "(", "\n", "enumerate", "(", "partition", "(", "data", ",", "100", ")", ")", ",", "total", "=", "100", ",", "desc", "=", "'Total'", ")", ":", "\n", "        ", "for", "doc", "in", "tqdm", "(", "some_docs", ",", "desc", "=", "'Chunk'", ")", ":", "\n", "            ", "processed", "=", "processor", "(", "doc", ".", "text", ")", "\n", "doc", ".", "sentences", "=", "[", "\n", "Sentence", "(", "[", "token", ".", "text", "for", "token", "in", "stanza_sent", ".", "tokens", "]", ")", "\n", "for", "stanza_sent", "in", "processed", ".", "sentences", "]", "\n", "", "with", "open", "(", "out_dir", "/", "f'tokenized_{part_index}.pickle'", ",", "'wb'", ")", "as", "file", ":", "\n", "            ", "pickle", ".", "dump", "(", "some_docs", ",", "file", ",", "protocol", "=", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Decomposer.__init__": [[22, 54], ["torch.nn.Module.__init__", "torch.nn.Embedding.from_pretrained", "ideal_grounded.Decomposer.to"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "preserve", ":", "str", ",", "# either 'deno' or 'cono'", "\n", "initial_space", ":", "Matrix", ",", "\n", "deno_probe", ":", "nn", ".", "Module", ",", "\n", "cono_probe", ":", "nn", ".", "Module", ",", "\n", "id_to_word", ":", "Dict", "[", "int", ",", "str", "]", ",", "\n", "ground", ":", "Dict", "[", "str", ",", "GroundedWord", "]", ",", "\n", "device", ":", "torch", ".", "device", ")", ":", "\n", "        ", "\"\"\"\n        Denotation Loss: bill title or policy topic classifier\n        Connotation Loss: party classifier\n\n        If preserve = 'deno', decomposer will preserve deno and remove cono\n        information from the decomposed space, and vice versa.\n        \"\"\"", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "decomposed", "=", "nn", ".", "Embedding", ".", "from_pretrained", "(", "initial_space", ")", "\n", "self", ".", "decomposed", ".", "weight", ".", "requires_grad", "=", "True", "\n", "self", ".", "deno_probe", "=", "deno_probe", "\n", "self", ".", "cono_probe", "=", "cono_probe", "\n", "self", ".", "num_deno_classes", "=", "deno_probe", "[", "-", "1", "]", ".", "out_features", "\n", "self", ".", "num_cono_classes", "=", "cono_probe", "[", "-", "1", "]", ".", "out_features", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "preserve", "=", "preserve", "\n", "# self.deno_to_id = data.deno_to_id  # for homogeneity evaluation", "\n", "# self.id_to_deno = data.id_to_deno  # for error analysis", "\n", "# self.word_to_id = data.word_to_id", "\n", "self", ".", "id_to_word", "=", "id_to_word", "\n", "self", ".", "ground", "=", "ground", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Decomposer.forward": [[55, 80], ["ideal_grounded.Decomposer.decomposed", "torch.mean", "ideal_grounded.Decomposer.deno_probe", "torch.nn.functional.log_softmax", "torch.nn.functional.nll_loss", "ideal_grounded.Decomposer.cono_probe", "torch.nn.functional.log_softmax", "torch.nn.functional.nll_loss", "torch.full_like", "torch.nn.functional.kl_div", "torch.full_like", "torch.nn.functional.kl_div"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "seq_word_ids", ":", "Matrix", ",", "\n", "deno_labels", ":", "Vector", ",", "\n", "cono_labels", ":", "Vector", "\n", ")", "->", "Tuple", "[", "Scalar", ",", "...", "]", ":", "\n", "        ", "seq_word_vecs", ":", "R3Tensor", "=", "self", ".", "decomposed", "(", "seq_word_ids", ")", "\n", "seq_repr", ":", "Matrix", "=", "torch", ".", "mean", "(", "seq_word_vecs", ",", "dim", "=", "1", ")", "\n", "\n", "deno_logits", "=", "self", ".", "deno_probe", "(", "seq_repr", ")", "\n", "deno_log_prob", "=", "F", ".", "log_softmax", "(", "deno_logits", ",", "dim", "=", "1", ")", "\n", "deno_probe_loss", "=", "F", ".", "nll_loss", "(", "deno_log_prob", ",", "deno_labels", ")", "\n", "\n", "cono_logits", "=", "self", ".", "cono_probe", "(", "seq_repr", ")", "\n", "cono_log_prob", "=", "F", ".", "log_softmax", "(", "cono_logits", ",", "dim", "=", "1", ")", "\n", "cono_probe_loss", "=", "F", ".", "nll_loss", "(", "cono_log_prob", ",", "cono_labels", ")", "\n", "\n", "if", "self", ".", "preserve", "==", "'deno'", ":", "# DS removing connotation (gamma < 0)", "\n", "            ", "uniform_dist", "=", "torch", ".", "full_like", "(", "cono_log_prob", ",", "1", "/", "self", ".", "num_cono_classes", ")", "\n", "cono_adversary_loss", "=", "F", ".", "kl_div", "(", "cono_log_prob", ",", "uniform_dist", ",", "reduction", "=", "'batchmean'", ")", "\n", "return", "deno_probe_loss", ",", "cono_probe_loss", ",", "cono_adversary_loss", ",", "seq_word_vecs", "\n", "", "else", ":", "# CS removing denotation", "\n", "            ", "uniform_dist", "=", "torch", ".", "full_like", "(", "deno_log_prob", ",", "1", "/", "self", ".", "num_deno_classes", ")", "\n", "deno_adversary_loss", "=", "F", ".", "kl_div", "(", "deno_log_prob", ",", "uniform_dist", ",", "reduction", "=", "'batchmean'", ")", "\n", "return", "deno_probe_loss", ",", "deno_adversary_loss", ",", "cono_probe_loss", ",", "seq_word_vecs", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Decomposer.predict": [[81, 92], ["ideal_grounded.Decomposer.eval", "ideal_grounded.Decomposer.train", "torch.no_grad", "ideal_grounded.Decomposer.decomposed", "torch.mean", "ideal_grounded.Decomposer.deno_probe", "ideal_grounded.Decomposer.cono_probe", "torch.nn.functional.softmax", "torch.nn.functional.softmax"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.train"], ["", "", "def", "predict", "(", "self", ",", "seq_word_ids", ":", "Vector", ")", "->", "Vector", ":", "\n", "        ", "self", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "word_vecs", ":", "R3Tensor", "=", "self", ".", "decomposed", "(", "seq_word_ids", ")", "\n", "seq_repr", ":", "Matrix", "=", "torch", ".", "mean", "(", "word_vecs", ",", "dim", "=", "1", ")", "\n", "deno", "=", "self", ".", "deno_probe", "(", "seq_repr", ")", "\n", "cono", "=", "self", ".", "cono_probe", "(", "seq_repr", ")", "\n", "deno_conf", "=", "F", ".", "softmax", "(", "deno", ",", "dim", "=", "1", ")", "\n", "cono_conf", "=", "F", ".", "softmax", "(", "cono", ",", "dim", "=", "1", ")", "\n", "", "self", ".", "train", "(", ")", "\n", "return", "deno_conf", ",", "cono_conf", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Decomposer.accuracy": [[93, 141], ["ideal_grounded.Decomposer.predict", "deno_conf.argmax", "cono_conf.argmax", "deno_conf.argmax.eq", "cono_conf.argmax.eq", "deno_conf.argmax.eq.float().mean().item", "cono_conf.argmax.eq.float().mean().item", "open", "open.write", "zip", "deno_conf.argmax.eq.float().mean", "cono_conf.argmax.eq.float().mean", "output.append", "open.write", "deno_conf.argmax.eq.float", "cono_conf.argmax.eq.float", "pred_confs[].item", "pred_confs[].item", "pred_id.item", "label_id.item", "i.item"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.predict"], ["", "def", "accuracy", "(", "\n", "self", ",", "\n", "seq_word_ids", ":", "Matrix", ",", "\n", "deno_labels", ":", "Vector", ",", "\n", "cono_labels", ":", "Vector", ",", "\n", "error_analysis_path", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", "->", "Tuple", "[", "float", ",", "float", "]", ":", "\n", "        ", "deno_conf", ",", "cono_conf", "=", "self", ".", "predict", "(", "seq_word_ids", ")", "\n", "deno_predictions", "=", "deno_conf", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "cono_predictions", "=", "cono_conf", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "# # Random Guess Baseline", "\n", "# deno_predictions = torch.randint_like(deno_labels, high=len(self.deno_to_id))", "\n", "# # Majority Class Baseline", "\n", "# majority_label = self.deno_to_id['Health']", "\n", "# deno_predictions = torch.full_like(deno_labels, majority_label)", "\n", "\n", "deno_correct_indicies", "=", "deno_predictions", ".", "eq", "(", "deno_labels", ")", "\n", "cono_correct_indicies", "=", "cono_predictions", ".", "eq", "(", "cono_labels", ")", "\n", "deno_accuracy", "=", "deno_correct_indicies", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "cono_accuracy", "=", "cono_correct_indicies", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "if", "error_analysis_path", ":", "\n", "            ", "analysis_file", "=", "open", "(", "error_analysis_path", ",", "'w'", ")", "\n", "analysis_file", ".", "write", "(", "'pred_conf\\tpred\\tlabel_conf\\tlabel\\tseq\\n'", ")", "\n", "output", "=", "[", "]", "\n", "for", "pred_confs", ",", "pred_id", ",", "label_id", ",", "seq_ids", "in", "zip", "(", "\n", "deno_conf", ",", "deno_predictions", ",", "deno_labels", ",", "seq_word_ids", ")", ":", "\n", "                ", "pred_conf", "=", "f'{pred_confs[pred_id].item():.4f}'", "\n", "label_conf", "=", "f'{pred_confs[label_id].item():.4f}'", "\n", "pred", "=", "self", ".", "id_to_deno", "[", "pred_id", ".", "item", "(", ")", "]", "\n", "label", "=", "self", ".", "id_to_deno", "[", "label_id", ".", "item", "(", ")", "]", "\n", "seq", "=", "' '", ".", "join", "(", "[", "self", ".", "id_to_word", "[", "i", ".", "item", "(", ")", "]", "for", "i", "in", "seq_ids", "]", ")", "\n", "output", ".", "append", "(", "(", "pred_conf", ",", "pred", ",", "label_conf", ",", "label", ",", "seq", ")", ")", "\n", "# output.sort(key=lambda t: t[1], reverse=True)", "\n", "", "for", "stuff", "in", "output", ":", "\n", "                ", "analysis_file", ".", "write", "(", "'\\t'", ".", "join", "(", "stuff", ")", "+", "'\\n'", ")", "\n", "# if error_analysis_path:  # confusion  matrix", "\n", "#     cf_mtx = confusion_matrix(deno_labels.cpu(), deno_predictions.cpu())", "\n", "#     fig, ax = plt.subplots(figsize=(20, 20))", "\n", "#     sns.heatmap(", "\n", "#         cf_mtx, annot=True, robust=True, ax=ax, cbar=False, fmt='d', linewidths=.5,", "\n", "#         mask=np.equal(cf_mtx, 0),", "\n", "#         xticklabels=self.graph_labels, yticklabels=self.graph_labels)", "\n", "#     ax.set_xlabel('Predicted Label')", "\n", "#     ax.set_ylabel('True Label')", "\n", "#     with open(error_analysis_path, 'wb') as file:", "\n", "#         fig.savefig(file, dpi=300, bbox_inches='tight')", "\n", "", "", "return", "deno_accuracy", ",", "cono_accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Decomposer.nearest_neighbors": [[142, 164], ["torch.no_grad", "ideal_grounded.Decomposer.decomposed", "torch.stack.topk", "torch.nn.functional.cosine_similarity", "ideal_grounded.Decomposer.unsqueeze", "ideal_grounded.Decomposer.decomposed.weight.unsqueeze", "torch.stack", "torch.nn.functional.cosine_similarity", "qv.unsqueeze"], "methods", ["None"], ["", "def", "nearest_neighbors", "(", "\n", "self", ",", "\n", "query_ids", ":", "Vector", ",", "\n", "top_k", ":", "int", "=", "10", ",", "\n", "verbose", ":", "bool", "=", "False", ",", "\n", ")", "->", "Matrix", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "query_vectors", "=", "self", ".", "decomposed", "(", "query_ids", ")", "\n", "try", ":", "\n", "                ", "cos_sim", "=", "F", ".", "cosine_similarity", "(", "\n", "query_vectors", ".", "unsqueeze", "(", "1", ")", ",", "\n", "self", ".", "decomposed", ".", "weight", ".", "unsqueeze", "(", "0", ")", ",", "\n", "dim", "=", "2", ")", "\n", "", "except", "RuntimeError", ":", "# insufficient GPU memory", "\n", "                ", "cos_sim", "=", "torch", ".", "stack", "(", "[", "\n", "F", ".", "cosine_similarity", "(", "qv", ".", "unsqueeze", "(", "0", ")", ",", "self", ".", "decomposed", ".", "weight", ")", "\n", "for", "qv", "in", "query_vectors", "]", ")", "\n", "", "cos_sim", ",", "neighbor_ids", "=", "cos_sim", ".", "topk", "(", "k", "=", "top_k", ",", "dim", "=", "-", "1", ")", "\n", "if", "verbose", ":", "\n", "                ", "return", "cos_sim", "[", ":", ",", "1", ":", "]", ",", "neighbor_ids", "[", ":", ",", "1", ":", "]", "\n", "", "else", ":", "# excludes the first neighbor, which is always the query itself", "\n", "                ", "return", "neighbor_ids", "[", ":", ",", "1", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Decomposer.homogeneity": [[165, 206], ["ideal_grounded.Decomposer.nearest_neighbors", "enumerate", "query_ids[].item", "deno_homogeneity.append", "cono_homogeneity.append", "statistics.mean", "statistics.mean", "len", "neighbor_ids.tolist", "len", "len", "editdistance.eval"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Decomposer.nearest_neighbors"], ["", "", "", "def", "homogeneity", "(", "\n", "self", ",", "\n", "query_ids", ":", "Vector", ",", "\n", "top_k", ":", "int", "=", "10", "\n", ")", "->", "Tuple", "[", "float", ",", "float", "]", ":", "\n", "# extra 5 top-k for excluding edit distance neighbors", "\n", "        ", "top_neighbor_ids", "=", "self", ".", "nearest_neighbors", "(", "query_ids", ",", "top_k", "+", "5", ")", "\n", "deno_homogeneity", "=", "[", "]", "\n", "cono_homogeneity", "=", "[", "]", "\n", "for", "query_index", ",", "neighbor_ids", "in", "enumerate", "(", "top_neighbor_ids", ")", ":", "\n", "            ", "query_id", "=", "query_ids", "[", "query_index", "]", ".", "item", "(", ")", "\n", "query_word", "=", "self", ".", "id_to_word", "[", "query_id", "]", "\n", "\n", "neighbor_ids", "=", "[", "\n", "nid", "for", "nid", "in", "neighbor_ids", ".", "tolist", "(", ")", "\n", "if", "editdistance", ".", "eval", "(", "query_word", ",", "self", ".", "id_to_word", "[", "nid", "]", ")", ">", "3", "]", "\n", "neighbor_ids", "=", "neighbor_ids", "[", ":", "top_k", "]", "\n", "if", "len", "(", "neighbor_ids", ")", "==", "0", ":", "\n", "# print(query_word, [self.id_to_word[i.item()]", "\n", "#                    for i in top_neighbor_ids[query_index]])", "\n", "# raise RuntimeWarning", "\n", "                ", "continue", "\n", "\n", "", "query_deno", "=", "self", ".", "ground", "[", "query_word", "]", ".", "majority_deno", "\n", "query_cono", "=", "self", ".", "ground", "[", "query_word", "]", ".", "majority_cono", "\n", "same_deno", "=", "0", "\n", "same_cono", "=", "0", "\n", "for", "nid", "in", "neighbor_ids", ":", "\n", "                ", "try", ":", "\n", "                    ", "neighbor_word", "=", "self", ".", "id_to_word", "[", "nid", "]", "\n", "neighbor_deno", "=", "self", ".", "ground", "[", "neighbor_word", "]", ".", "majority_deno", "\n", "neighbor_cono", "=", "self", ".", "ground", "[", "neighbor_word", "]", ".", "majority_cono", "\n", "if", "neighbor_deno", "==", "query_deno", ":", "\n", "                        ", "same_deno", "+=", "1", "\n", "", "if", "neighbor_cono", "==", "query_cono", ":", "\n", "                        ", "same_cono", "+=", "1", "\n", "", "", "except", "KeyError", ":", "# special tokens like [PAD] are ungrounded", "\n", "                    ", "continue", "\n", "", "", "deno_homogeneity", ".", "append", "(", "same_deno", "/", "len", "(", "neighbor_ids", ")", ")", "\n", "cono_homogeneity", ".", "append", "(", "same_cono", "/", "len", "(", "neighbor_ids", ")", ")", "\n", "", "return", "mean", "(", "deno_homogeneity", ")", ",", "mean", "(", "cono_homogeneity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Recomposer.__init__": [[210, 247], ["torch.nn.Module.__init__", "utils.experiment.Experiment.load_txt_embedding", "ideal_grounded.Decomposer", "ideal_grounded.Decomposer", "torch.nn.Linear", "ideal_grounded.Recomposer.to"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.__init__", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.load_txt_embedding"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "config", ":", "'IdealGroundedConfig'", ",", "\n", "data", ":", "'LabeledSentences'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "config", ".", "device", "\n", "\n", "self", ".", "pretrained_embed", "=", "Experiment", ".", "load_txt_embedding", "(", "\n", "config", ".", "pretrained_embed_path", ",", "data", ".", "word_to_id", ")", "\n", "self", ".", "pretrained_embed", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n", "self", ".", "deno_space", "=", "Decomposer", "(", "\n", "preserve", "=", "'deno'", ",", "\n", "initial_space", "=", "self", ".", "pretrained_embed", ".", "weight", ",", "\n", "deno_probe", "=", "config", ".", "deno_probe", ",", "\n", "cono_probe", "=", "config", ".", "cono_probe", ",", "\n", "id_to_word", "=", "data", ".", "id_to_word", ",", "\n", "ground", "=", "data", ".", "ground", ",", "\n", "device", "=", "self", ".", "device", ")", "\n", "\n", "self", ".", "cono_space", "=", "Decomposer", "(", "\n", "preserve", "=", "'cono'", ",", "\n", "initial_space", "=", "self", ".", "pretrained_embed", ".", "weight", ",", "\n", "deno_probe", "=", "config", ".", "deno_probe", ",", "\n", "cono_probe", "=", "config", ".", "cono_probe", ",", "\n", "id_to_word", "=", "data", ".", "id_to_word", ",", "\n", "ground", "=", "data", ".", "ground", ",", "\n", "device", "=", "self", ".", "device", ")", "\n", "\n", "# Recomposer", "\n", "self", ".", "recomposer", "=", "nn", ".", "Linear", "(", "600", ",", "300", ")", "\n", "self", ".", "rho", "=", "config", ".", "recomposer_rho", "\n", "self", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "word_to_id", "=", "data", ".", "word_to_id", "\n", "self", ".", "id_to_word", "=", "data", ".", "id_to_word", "\n", "self", ".", "ground", "=", "data", ".", "ground", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Recomposer.forward": [[248, 274], ["ideal_grounded.Recomposer.deno_space", "ideal_grounded.Recomposer.cono_space", "ideal_grounded.Recomposer.recomposer", "ideal_grounded.Recomposer.pretrained_embed", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.nn.functional.cosine_similarity().mean", "torch.nn.functional.cosine_similarity"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "seq_word_ids", ":", "Matrix", ",", "\n", "deno_labels", ":", "Vector", ",", "\n", "cono_labels", ":", "Vector", "\n", ")", "->", "Tuple", "[", "Scalar", ",", "...", "]", ":", "\n", "# Denotation Space", "\n", "        ", "DS_deno_probe", ",", "DS_cono_probe", ",", "DS_cono_adver", ",", "deno_vecs", "=", "self", ".", "deno_space", "(", "\n", "seq_word_ids", ",", "deno_labels", ",", "cono_labels", ")", "\n", "DS_decomp", "=", "torch", ".", "sigmoid", "(", "DS_deno_probe", ")", "+", "torch", ".", "sigmoid", "(", "DS_cono_adver", ")", "\n", "\n", "# Connotation Space", "\n", "CS_deno_probe", ",", "CS_deno_adver", ",", "CS_cono_probe", ",", "cono_vecs", "=", "self", ".", "cono_space", "(", "\n", "seq_word_ids", ",", "deno_labels", ",", "cono_labels", ")", "\n", "CS_decomp", "=", "torch", ".", "sigmoid", "(", "CS_deno_adver", ")", "+", "torch", ".", "sigmoid", "(", "CS_cono_probe", ")", "\n", "\n", "# Recomposed Space", "\n", "recomposed", "=", "self", ".", "recomposer", "(", "torch", ".", "cat", "(", "(", "deno_vecs", ",", "cono_vecs", ")", ",", "dim", "=", "-", "1", ")", ")", "\n", "# recomposed = deno_vecs + cono_vecs  # cosine similarity ignores magnitude", "\n", "pretrained", "=", "self", ".", "pretrained_embed", "(", "seq_word_ids", ")", "\n", "L_R", "=", "1", "-", "F", ".", "cosine_similarity", "(", "recomposed", ",", "pretrained", ",", "dim", "=", "-", "1", ")", ".", "mean", "(", ")", "\n", "\n", "L_joint", "=", "DS_decomp", "+", "CS_decomp", "+", "self", ".", "rho", "*", "L_R", "\n", "return", "(", "L_joint", ",", "L_R", ",", "\n", "DS_decomp", ",", "DS_deno_probe", ",", "DS_cono_probe", ",", "DS_cono_adver", ",", "\n", "CS_decomp", ",", "CS_deno_probe", ",", "CS_deno_adver", ",", "CS_cono_probe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Recomposer.predict": [[275, 279], ["ideal_grounded.Recomposer.deno_space.predict", "ideal_grounded.Recomposer.cono_space.predict"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.predict", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.predict"], ["", "def", "predict", "(", "self", ",", "seq_word_ids", ":", "Vector", ")", "->", "Tuple", "[", "Vector", ",", "...", "]", ":", "\n", "        ", "DS_deno_conf", ",", "DS_cono_conf", "=", "self", ".", "deno_space", ".", "predict", "(", "seq_word_ids", ")", "\n", "CS_deno_conf", ",", "CS_cono_conf", "=", "self", ".", "cono_space", ".", "predict", "(", "seq_word_ids", ")", "\n", "return", "DS_deno_conf", ",", "DS_cono_conf", ",", "CS_deno_conf", ",", "CS_cono_conf", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Recomposer.accuracy": [[280, 292], ["ideal_grounded.Recomposer.deno_space.accuracy", "ideal_grounded.Recomposer.cono_space.accuracy"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.accuracy", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.accuracy"], ["", "def", "accuracy", "(", "\n", "self", ",", "\n", "seq_word_ids", ":", "Matrix", ",", "\n", "deno_labels", ":", "Vector", ",", "\n", "cono_labels", ":", "Vector", ",", "\n", "error_analysis_path", ":", "Optional", "[", "str", "]", "=", "None", "\n", ")", "->", "Tuple", "[", "float", ",", "...", "]", ":", "\n", "        ", "DS_deno_acc", ",", "DS_cono_acc", "=", "self", ".", "deno_space", ".", "accuracy", "(", "\n", "seq_word_ids", ",", "deno_labels", ",", "cono_labels", ")", "\n", "CS_deno_acc", ",", "CS_cono_acc", "=", "self", ".", "cono_space", ".", "accuracy", "(", "\n", "seq_word_ids", ",", "deno_labels", ",", "cono_labels", ")", "\n", "return", "DS_deno_acc", ",", "DS_cono_acc", ",", "CS_deno_acc", ",", "CS_cono_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Recomposer.homogeneity": [[293, 301], ["ideal_grounded.Recomposer.deno_space.homogeneity", "ideal_grounded.Recomposer.cono_space.homogeneity"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity"], ["", "def", "homogeneity", "(", "\n", "self", ",", "\n", "query_ids", ":", "Vector", ",", "\n", "top_k", ":", "int", "=", "10", "\n", ")", "->", "Tuple", "[", "float", ",", "...", "]", ":", "\n", "        ", "DS_Hdeno", ",", "DS_Hcono", "=", "self", ".", "deno_space", ".", "homogeneity", "(", "query_ids", ",", "top_k", "=", "top_k", ")", "\n", "CS_Hdeno", ",", "CS_Hcono", "=", "self", ".", "cono_space", ".", "homogeneity", "(", "query_ids", ",", "top_k", "=", "top_k", ")", "\n", "return", "DS_Hdeno", ",", "DS_Hcono", ",", "CS_Hdeno", ",", "CS_Hcono", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Recomposer.tabulate": [[302, 346], ["ideal_grounded.Recomposer.homogeneity", "row.update", "ideal_grounded.Recomposer.homogeneity", "row.update", "ideal_grounded.Recomposer.homogeneity", "row.update", "round", "row.items"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity"], ["", "def", "tabulate", "(", "\n", "self", ",", "\n", "# dev_ids: Vector,", "\n", "# test_ids: Vector,", "\n", "# rand_ids: Vector,", "\n", "rounding", ":", "int", "=", "4", ",", "\n", "top_k", ":", "int", "=", "10", "\n", ")", "->", "Dict", "[", "str", ",", "float", "]", ":", "\n", "        ", "row", "=", "{", "}", "\n", "PE", "=", "self", ".", "PE_homogeneity", "\n", "DS_Hd", ",", "DS_Hc", ",", "CS_Hd", ",", "CS_Hc", "=", "self", ".", "homogeneity", "(", "self", ".", "dev_ids", ")", "\n", "row", ".", "update", "(", "{", "\n", "'Dev DS Hdeno'", ":", "DS_Hd", ",", "\n", "'Dev DS Hcono'", ":", "DS_Hc", ",", "\n", "'Dev CS Hdeno'", ":", "CS_Hd", ",", "\n", "'Dev CS Hcono'", ":", "CS_Hc", ",", "\n", "'Dev DS Hdeno delta'", ":", "DS_Hd", "-", "PE", "[", "'dev Hd'", "]", ",", "\n", "'Dev DS Hcono delta'", ":", "DS_Hc", "-", "PE", "[", "'dev Hc'", "]", ",", "\n", "'Dev CS Hdeno delta'", ":", "CS_Hd", "-", "PE", "[", "'dev Hd'", "]", ",", "\n", "'Dev CS Hcono delta'", ":", "CS_Hc", "-", "PE", "[", "'dev Hc'", "]", ",", "\n", "}", ")", "\n", "DS_Hd", ",", "DS_Hc", ",", "CS_Hd", ",", "CS_Hc", "=", "self", ".", "homogeneity", "(", "self", ".", "test_ids", ")", "\n", "row", ".", "update", "(", "{", "\n", "'Test DS Hdeno'", ":", "DS_Hd", ",", "\n", "'Test DS Hcono'", ":", "DS_Hc", ",", "\n", "'Test CS Hdeno'", ":", "CS_Hd", ",", "\n", "'Test CS Hcono'", ":", "CS_Hc", ",", "\n", "'Test DS Hdeno delta'", ":", "DS_Hd", "-", "PE", "[", "'test Hd'", "]", ",", "\n", "'Test DS Hcono delta'", ":", "DS_Hc", "-", "PE", "[", "'test Hc'", "]", ",", "\n", "'Test CS Hdeno delta'", ":", "CS_Hd", "-", "PE", "[", "'test Hd'", "]", ",", "\n", "'Test CS Hcono delta'", ":", "CS_Hc", "-", "PE", "[", "'test Hc'", "]", ",", "\n", "}", ")", "\n", "DS_Hd", ",", "DS_Hc", ",", "CS_Hd", ",", "CS_Hc", "=", "self", ".", "homogeneity", "(", "self", ".", "rand_ids", ")", "\n", "row", ".", "update", "(", "{", "\n", "'Random DS Hdeno'", ":", "DS_Hd", ",", "\n", "'Random DS Hcono'", ":", "DS_Hc", ",", "\n", "'Random CS Hdeno'", ":", "CS_Hd", ",", "\n", "'Random CS Hcono'", ":", "CS_Hc", ",", "\n", "'Random DS Hdeno delta'", ":", "DS_Hd", "-", "PE", "[", "'rand Hd'", "]", ",", "\n", "'Random DS Hcono delta'", ":", "DS_Hc", "-", "PE", "[", "'rand Hc'", "]", ",", "\n", "'Random CS Hdeno delta'", ":", "CS_Hd", "-", "PE", "[", "'rand Hd'", "]", ",", "\n", "'Random CS Hcono delta'", ":", "CS_Hc", "-", "PE", "[", "'rand Hc'", "]", ",", "\n", "}", ")", "\n", "return", "{", "key", ":", "round", "(", "val", ",", "rounding", ")", "for", "key", ",", "val", "in", "row", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Recomposer.cf_cos_sim": [[347, 372], ["ideal_grounded.Recomposer.pretrained_embed", "ideal_grounded.Recomposer.pretrained_embed", "torch.nn.functional.cosine_similarity().item", "ideal_grounded.Recomposer.deno_space.decomposed", "ideal_grounded.Recomposer.deno_space.decomposed", "torch.nn.functional.cosine_similarity().item", "ideal_grounded.Recomposer.cono_space.decomposed", "ideal_grounded.Recomposer.cono_space.decomposed", "torch.nn.functional.cosine_similarity().item", "torch.tensor", "torch.tensor", "print", "print", "torch.nn.functional.cosine_similarity", "torch.nn.functional.cosine_similarity", "torch.nn.functional.cosine_similarity"], "methods", ["None"], ["", "def", "cf_cos_sim", "(", "self", ",", "query1", ":", "str", ",", "query2", ":", "str", ")", "->", "Tuple", "[", "float", ",", "...", "]", ":", "\n", "        ", "try", ":", "\n", "            ", "query1_id", "=", "torch", ".", "tensor", "(", "self", ".", "word_to_id", "[", "query1", "]", ",", "device", "=", "self", ".", "device", ")", "\n", "", "except", "KeyError", ":", "\n", "            ", "print", "(", "f'Out of vocabulary: {query1}'", ")", "\n", "return", "-", "1", ",", "-", "1", ",", "-", "1", "\n", "", "try", ":", "\n", "            ", "query2_id", "=", "torch", ".", "tensor", "(", "self", ".", "word_to_id", "[", "query2", "]", ",", "device", "=", "self", ".", "device", ")", "\n", "", "except", "KeyError", ":", "\n", "            ", "print", "(", "f'Out of vocabulary: {query2}'", ")", "\n", "return", "-", "1", ",", "-", "1", ",", "-", "1", "\n", "\n", "", "v1", "=", "self", ".", "pretrained_embed", "(", "query1_id", ")", "\n", "v2", "=", "self", ".", "pretrained_embed", "(", "query2_id", ")", "\n", "pre_sim", "=", "F", ".", "cosine_similarity", "(", "v1", ",", "v2", ",", "dim", "=", "0", ")", ".", "item", "(", ")", "\n", "\n", "v1", "=", "self", ".", "deno_space", ".", "decomposed", "(", "query1_id", ")", "\n", "v2", "=", "self", ".", "deno_space", ".", "decomposed", "(", "query2_id", ")", "\n", "deno_sim", "=", "F", ".", "cosine_similarity", "(", "v1", ",", "v2", ",", "dim", "=", "0", ")", ".", "item", "(", ")", "\n", "\n", "v1", "=", "self", ".", "cono_space", ".", "decomposed", "(", "query1_id", ")", "\n", "v2", "=", "self", ".", "cono_space", ".", "decomposed", "(", "query2_id", ")", "\n", "cono_sim", "=", "F", ".", "cosine_similarity", "(", "v1", ",", "v2", ",", "dim", "=", "0", ")", ".", "item", "(", ")", "\n", "\n", "return", "pre_sim", ",", "deno_sim", ",", "cono_sim", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Recomposer.export_embeddings": [[373, 375], ["None"], "methods", ["None"], ["", "def", "export_embeddings", "(", "self", ",", "out_path", ":", "Path", ")", "->", "Tuple", "[", "Matrix", ",", "Matrix", "]", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.LabeledSentences.__init__": [[379, 412], ["super().__init__", "torch.nn.utils.rnn.pad_sequence", "torch.tensor", "torch.tensor", "open", "pickle.load", "open", "torch.tensor", "open", "torch.tensor", "open", "torch.tensor", "torch.tensor", "word.strip", "word.strip", "word.strip", "word.strip"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "'IdealGroundedConfig'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "with", "open", "(", "config", ".", "corpus_path", ",", "'rb'", ")", "as", "corpus_file", ":", "\n", "            ", "preprocessed", "=", "pickle", ".", "load", "(", "corpus_file", ")", "\n", "", "self", ".", "word_to_id", "=", "preprocessed", "[", "'word_to_id'", "]", "\n", "self", ".", "id_to_word", "=", "preprocessed", "[", "'id_to_word'", "]", "\n", "self", ".", "deno_to_id", "=", "preprocessed", "[", "'deno_to_id'", "]", "\n", "self", ".", "id_to_deno", "=", "preprocessed", "[", "'id_to_deno'", "]", "\n", "self", ".", "ground", ":", "Dict", "[", "str", ",", "GroundedWord", "]", "=", "preprocessed", "[", "'ground'", "]", "\n", "\n", "self", ".", "train_seq", ":", "List", "[", "List", "[", "int", "]", "]", "=", "preprocessed", "[", "'train_sent_word_ids'", "]", "\n", "self", ".", "train_deno_labels", ":", "List", "[", "int", "]", "=", "preprocessed", "[", "'train_deno_labels'", "]", "\n", "self", ".", "train_cono_labels", ":", "List", "[", "int", "]", "=", "preprocessed", "[", "'train_cono_labels'", "]", "\n", "\n", "self", ".", "dev_seq", "=", "rnn", ".", "pad_sequence", "(", "\n", "[", "torch", ".", "tensor", "(", "seq", ")", "for", "seq", "in", "preprocessed", "[", "'dev_sent_word_ids'", "]", "]", ",", "\n", "batch_first", "=", "True", ")", "\n", "self", ".", "dev_deno_labels", "=", "torch", ".", "tensor", "(", "preprocessed", "[", "'dev_deno_labels'", "]", ")", "\n", "self", ".", "dev_cono_labels", "=", "torch", ".", "tensor", "(", "preprocessed", "[", "'dev_cono_labels'", "]", ")", "\n", "\n", "with", "open", "(", "config", ".", "dev_path", ")", "as", "file", ":", "\n", "            ", "self", ".", "dev_ids", "=", "torch", ".", "tensor", "(", "\n", "[", "self", ".", "word_to_id", "[", "word", ".", "strip", "(", ")", "]", "for", "word", "in", "file", "]", ",", "\n", "device", "=", "config", ".", "device", ")", "\n", "", "with", "open", "(", "config", ".", "test_path", ")", "as", "file", ":", "\n", "            ", "self", ".", "test_ids", "=", "torch", ".", "tensor", "(", "\n", "[", "self", ".", "word_to_id", "[", "word", ".", "strip", "(", ")", "]", "for", "word", "in", "file", "]", ",", "\n", "device", "=", "config", ".", "device", ")", "\n", "", "with", "open", "(", "config", ".", "rand_path", ")", "as", "file", ":", "\n", "            ", "self", ".", "rand_ids", "=", "torch", ".", "tensor", "(", "\n", "[", "self", ".", "word_to_id", "[", "word", ".", "strip", "(", ")", "]", "for", "word", "in", "file", "\n", "if", "word", ".", "strip", "(", ")", "in", "self", ".", "word_to_id", "]", ",", "\n", "device", "=", "config", ".", "device", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.LabeledSentences.__len__": [[413, 415], ["len"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", "->", "int", ":", "\n", "        ", "return", "len", "(", "self", ".", "train_seq", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.LabeledSentences.__getitem__": [[416, 421], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ":", "int", ")", "->", "Tuple", "[", "List", "[", "int", "]", ",", "int", ",", "int", "]", ":", "\n", "        ", "return", "(", "\n", "self", ".", "train_seq", "[", "index", "]", ",", "\n", "self", ".", "train_deno_labels", "[", "index", "]", ",", "\n", "self", ".", "train_cono_labels", "[", "index", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.LabeledSentences.collate": [[422, 434], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.nn.utils.rnn.pad_sequence"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "collate", "(", "\n", "batch", ":", "List", "[", "Tuple", "[", "List", "[", "int", "]", ",", "int", ",", "int", "]", "]", "\n", ")", "->", "Tuple", "[", "Matrix", ",", "Vector", ",", "Vector", "]", ":", "\n", "# seq_word_ids = torch.cat([torch.tensor(w) for w, _, _ in batch])", "\n", "        ", "seq_word_ids", "=", "[", "torch", ".", "tensor", "(", "w", ")", "for", "w", ",", "_", ",", "_", "in", "batch", "]", "\n", "deno_labels", "=", "torch", ".", "tensor", "(", "[", "d", "for", "_", ",", "d", ",", "_", "in", "batch", "]", ")", "\n", "cono_labels", "=", "torch", ".", "tensor", "(", "[", "c", "for", "_", ",", "_", ",", "c", "in", "batch", "]", ")", "\n", "return", "(", "\n", "rnn", ".", "pad_sequence", "(", "seq_word_ids", ",", "batch_first", "=", "True", ")", ",", "\n", "deno_labels", ",", "\n", "cono_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.IdealGroundedExperiment.__init__": [[438, 487], ["utils.experiment.Experiment.__init__", "ideal_grounded.LabeledSentences", "torch.utils.data.DataLoader", "ideal_grounded.Recomposer", "config.optimizer", "config.optimizer", "config.optimizer", "config.optimizer", "config.optimizer", "config.optimizer", "model.deno_space.homogeneity", "model.deno_space.homogeneity", "model.deno_space.homogeneity", "print", "model.deno_space.deno_probe.parameters", "model.deno_space.cono_probe.parameters", "model.cono_space.deno_probe.parameters", "model.cono_space.cono_probe.parameters", "model.recomposer.parameters", "list", "list", "model.deno_space.decomposed.parameters", "model.cono_space.decomposed.parameters"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.__init__", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "'IdealGroundedConfig'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "data", "=", "LabeledSentences", "(", "config", ")", "\n", "self", ".", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "self", ".", "data", ",", "\n", "batch_size", "=", "config", ".", "batch_size", ",", "\n", "shuffle", "=", "True", ",", "\n", "collate_fn", "=", "self", ".", "data", ".", "collate", ",", "\n", "num_workers", "=", "config", ".", "num_dataloader_threads", ",", "\n", "pin_memory", "=", "config", ".", "pin_memory", ")", "\n", "self", ".", "model", "=", "Recomposer", "(", "config", ",", "self", ".", "data", ")", "\n", "model", "=", "self", ".", "model", "\n", "\n", "# for name, param in self.model.named_parameters():", "\n", "#     if param.requires_grad:", "\n", "#         print(name)  # param.data)", "\n", "\n", "self", ".", "DS_deno_optimizer", "=", "config", ".", "optimizer", "(", "\n", "model", ".", "deno_space", ".", "deno_probe", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ")", "\n", "self", ".", "DS_cono_optimizer", "=", "config", ".", "optimizer", "(", "\n", "model", ".", "deno_space", ".", "cono_probe", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ")", "\n", "\n", "self", ".", "CS_deno_optimizer", "=", "config", ".", "optimizer", "(", "\n", "model", ".", "cono_space", ".", "deno_probe", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ")", "\n", "self", ".", "CS_cono_optimizer", "=", "config", ".", "optimizer", "(", "\n", "model", ".", "cono_space", ".", "cono_probe", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ")", "\n", "\n", "self", ".", "joint_optimizer", "=", "config", ".", "optimizer", "(", "\n", "list", "(", "model", ".", "deno_space", ".", "decomposed", ".", "parameters", "(", ")", ")", "+", "\n", "list", "(", "model", ".", "cono_space", ".", "decomposed", ".", "parameters", "(", ")", ")", ",", "\n", "lr", "=", "config", ".", "learning_rate", ")", "\n", "self", ".", "R_optimizer", "=", "config", ".", "optimizer", "(", "\n", "model", ".", "recomposer", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ")", "\n", "\n", "model", ".", "dev_ids", "=", "self", ".", "data", ".", "dev_ids", "\n", "model", ".", "test_ids", "=", "self", ".", "data", ".", "test_ids", "\n", "model", ".", "rand_ids", "=", "self", ".", "data", ".", "rand_ids", "\n", "dev_Hd", ",", "dev_Hc", "=", "model", ".", "deno_space", ".", "homogeneity", "(", "self", ".", "data", ".", "dev_ids", ")", "\n", "test_Hd", ",", "test_Hc", "=", "model", ".", "deno_space", ".", "homogeneity", "(", "self", ".", "data", ".", "test_ids", ")", "\n", "rand_Hd", ",", "rand_Hc", "=", "model", ".", "deno_space", ".", "homogeneity", "(", "self", ".", "data", ".", "rand_ids", ")", "\n", "model", ".", "PE_homogeneity", "=", "{", "\n", "'dev Hd'", ":", "dev_Hd", ",", "\n", "'dev Hc'", ":", "dev_Hc", ",", "\n", "'test Hd'", ":", "test_Hd", ",", "\n", "'test Hc'", ":", "test_Hc", ",", "\n", "'rand Hd'", ":", "rand_Hd", ",", "\n", "'rand Hc'", ":", "rand_Hc", ",", "\n", "}", "\n", "print", "(", "model", ".", "PE_homogeneity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.IdealGroundedExperiment.train_step": [[488, 552], ["batch[].to", "batch[].to", "batch[].to", "model.zero_grad", "model.deno_space", "DS_deno_probe.backward", "DS_cono_probe.backward", "ideal_grounded.IdealGroundedExperiment.DS_deno_optimizer.step", "ideal_grounded.IdealGroundedExperiment.DS_cono_optimizer.step", "model.zero_grad", "model.cono_space", "CS_deno_probe.backward", "CS_cono_probe.backward", "ideal_grounded.IdealGroundedExperiment.CS_deno_optimizer.step", "ideal_grounded.IdealGroundedExperiment.CS_cono_optimizer.step", "model.zero_grad", "model", "L_joint.backward", "ideal_grounded.IdealGroundedExperiment.joint_optimizer.step", "ideal_grounded.IdealGroundedExperiment.R_optimizer.step", "model.accuracy", "ideal_grounded.IdealGroundedExperiment.update_tensorboard", "model.accuracy", "ideal_grounded.IdealGroundedExperiment.update_tensorboard", "ideal_grounded.IdealGroundedExperiment.data.dev_seq.to", "ideal_grounded.IdealGroundedExperiment.data.dev_deno_labels.to", "ideal_grounded.IdealGroundedExperiment.data.dev_cono_labels.to"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.accuracy", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.accuracy", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard"], ["", "def", "train_step", "(", "self", ",", "batch_index", ":", "int", ",", "batch", ":", "Tuple", ")", "->", "None", ":", "\n", "        ", "model", "=", "self", ".", "model", "\n", "seq_word_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "deno_labels", "=", "batch", "[", "1", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "cono_labels", "=", "batch", "[", "2", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Update probes with proper (non-adversarial) losses", "\n", "model", ".", "zero_grad", "(", ")", "\n", "DS_deno_probe", ",", "DS_cono_probe", ",", "DS_cono_adver", ",", "_", "=", "model", ".", "deno_space", "(", "\n", "seq_word_ids", ",", "deno_labels", ",", "cono_labels", ")", "\n", "DS_deno_probe", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "DS_cono_probe", ".", "backward", "(", ")", "\n", "self", ".", "DS_deno_optimizer", ".", "step", "(", ")", "\n", "self", ".", "DS_cono_optimizer", ".", "step", "(", ")", "\n", "\n", "model", ".", "zero_grad", "(", ")", "\n", "CS_deno_probe", ",", "CS_deno_adver", ",", "CS_cono_probe", ",", "_", "=", "model", ".", "cono_space", "(", "\n", "seq_word_ids", ",", "deno_labels", ",", "cono_labels", ")", "\n", "CS_deno_probe", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "CS_cono_probe", ".", "backward", "(", ")", "\n", "self", ".", "CS_deno_optimizer", ".", "step", "(", ")", "\n", "self", ".", "CS_cono_optimizer", ".", "step", "(", ")", "\n", "\n", "model", ".", "zero_grad", "(", ")", "\n", "(", "L_joint", ",", "L_R", ",", "\n", "DS_decomp", ",", "DS_deno_probe", ",", "DS_cono_probe", ",", "DS_cono_adver", ",", "\n", "CS_decomp", ",", "CS_deno_probe", ",", "CS_deno_adver", ",", "CS_cono_probe", ")", "=", "model", "(", "\n", "seq_word_ids", ",", "deno_labels", ",", "cono_labels", ")", "\n", "L_joint", ".", "backward", "(", ")", "\n", "self", ".", "joint_optimizer", ".", "step", "(", ")", "\n", "self", ".", "R_optimizer", ".", "step", "(", ")", "\n", "\n", "if", "batch_index", "%", "self", ".", "config", ".", "update_tensorboard", "==", "0", ":", "\n", "            ", "D_deno_acc", ",", "D_cono_acc", ",", "C_deno_acc", ",", "C_cono_acc", "=", "model", ".", "accuracy", "(", "\n", "seq_word_ids", ",", "deno_labels", ",", "cono_labels", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Denotation Decomposer/deno_loss'", ":", "DS_deno_probe", ",", "\n", "'Denotation Decomposer/cono_loss_proper'", ":", "DS_cono_probe", ",", "\n", "'Denotation Decomposer/cono_loss_adversary'", ":", "DS_cono_adver", ",", "\n", "'Denotation Decomposer/combined loss'", ":", "DS_decomp", ",", "\n", "'Denotation Decomposer/accuracy_train_deno'", ":", "D_deno_acc", ",", "\n", "'Denotation Decomposer/accuracy_train_cono'", ":", "D_cono_acc", ",", "\n", "\n", "'Connotation Decomposer/cono_loss'", ":", "CS_cono_probe", ",", "\n", "'Connotation Decomposer/deno_loss_proper'", ":", "CS_deno_probe", ",", "\n", "'Connotation Decomposer/deno_loss_adversary'", ":", "CS_deno_adver", ",", "\n", "'Connotation Decomposer/combined_loss'", ":", "CS_decomp", ",", "\n", "'Connotation Decomposer/accuracy_train_deno'", ":", "C_deno_acc", ",", "\n", "'Connotation Decomposer/accuracy_train_cono'", ":", "C_cono_acc", ",", "\n", "\n", "'Joint/Loss'", ":", "L_joint", ",", "\n", "'Joint/Recomposer'", ":", "L_R", "\n", "}", ")", "\n", "\n", "", "if", "batch_index", "%", "self", ".", "config", ".", "eval_dev_set", "==", "0", ":", "\n", "            ", "D_deno_acc", ",", "D_cono_acc", ",", "C_deno_acc", ",", "C_cono_acc", "=", "model", ".", "accuracy", "(", "\n", "self", ".", "data", ".", "dev_seq", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "self", ".", "data", ".", "dev_deno_labels", ".", "to", "(", "self", ".", "device", ")", ",", "\n", "self", ".", "data", ".", "dev_cono_labels", ".", "to", "(", "self", ".", "device", ")", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Denotation Decomposer/accuracy_dev_deno'", ":", "D_deno_acc", ",", "\n", "'Denotation Decomposer/accuracy_dev_cono'", ":", "D_cono_acc", ",", "\n", "'Connotation Decomposer/accuracy_dev_deno'", ":", "C_deno_acc", ",", "\n", "'Connotation Decomposer/accuracy_dev_cono'", ":", "C_cono_acc", "}", ")", "\n", "# self.update_tensorboard(", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.IdealGroundedExperiment.eval_step": [[559, 582], ["ideal_grounded.IdealGroundedExperiment.model.homogeneity", "ideal_grounded.IdealGroundedExperiment.update_tensorboard", "ideal_grounded.IdealGroundedExperiment.model.homogeneity", "ideal_grounded.IdealGroundedExperiment.update_tensorboard", "ideal_grounded.IdealGroundedExperiment.model.homogeneity", "ideal_grounded.IdealGroundedExperiment.update_tensorboard"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard"], ["", "", "def", "eval_step", "(", "self", ",", "epoch_index", ":", "int", ")", "->", "None", ":", "\n", "        ", "PE", "=", "self", ".", "model", ".", "PE_homogeneity", "\n", "DS_Hd", ",", "DS_Hc", ",", "CS_Hd", ",", "CS_Hc", "=", "self", ".", "model", ".", "homogeneity", "(", "self", ".", "data", ".", "dev_ids", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Homogeneity Diff Dev/DS Hdeno'", ":", "DS_Hd", "-", "PE", "[", "'dev Hd'", "]", ",", "\n", "'Homogeneity Diff Dev/DS Hcono'", ":", "DS_Hc", "-", "PE", "[", "'dev Hc'", "]", ",", "\n", "'Homogeneity Diff Dev/CS Hdeno'", ":", "CS_Hd", "-", "PE", "[", "'dev Hd'", "]", ",", "\n", "'Homogeneity Diff Dev/CS Hcono'", ":", "CS_Hc", "-", "PE", "[", "'dev Hc'", "]", ",", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "DS_Hd", ",", "DS_Hc", ",", "CS_Hd", ",", "CS_Hc", "=", "self", ".", "model", ".", "homogeneity", "(", "self", ".", "data", ".", "test_ids", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Homogeneity Diff Test/DS Hdeno'", ":", "DS_Hd", "-", "PE", "[", "'test Hd'", "]", ",", "\n", "'Homogeneity Diff Test/DS Hcono'", ":", "DS_Hc", "-", "PE", "[", "'test Hc'", "]", ",", "\n", "'Homogeneity Diff Test/CS Hdeno'", ":", "CS_Hd", "-", "PE", "[", "'test Hd'", "]", ",", "\n", "'Homogeneity Diff Test/CS Hcono'", ":", "CS_Hc", "-", "PE", "[", "'test Hc'", "]", ",", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "DS_Hd", ",", "DS_Hc", ",", "CS_Hd", ",", "CS_Hc", "=", "self", ".", "model", ".", "homogeneity", "(", "self", ".", "data", ".", "rand_ids", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Homogeneity Diff Random/DS Hdeno'", ":", "DS_Hd", "-", "PE", "[", "'rand Hd'", "]", ",", "\n", "'Homogeneity Diff Random/DS Hcono'", ":", "DS_Hc", "-", "PE", "[", "'rand Hc'", "]", ",", "\n", "'Homogeneity Diff Random/CS Hdeno'", ":", "CS_Hd", "-", "PE", "[", "'rand Hd'", "]", ",", "\n", "'Homogeneity Diff Random/CS Hcono'", ":", "CS_Hc", "-", "PE", "[", "'rand Hc'", "]", ",", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.IdealGroundedExperiment.train": [[583, 617], ["tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm", "ideal_grounded.IdealGroundedExperiment.auto_save", "ideal_grounded.IdealGroundedExperiment.eval_step", "range", "range", "tqdm.tqdm.tqdm", "enumerate", "ideal_grounded.IdealGroundedExperiment.train_step", "ideal_grounded.IdealGroundedExperiment.print_timestamp", "enumerate", "len"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.auto_save", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.eval_step", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.train_step", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.print_timestamp"], ["", "def", "train", "(", "self", ")", "->", "None", ":", "\n", "        ", "config", "=", "self", ".", "config", "\n", "# # For debugging only", "\n", "# self.save_everything(self.config.output_dir / 'init_recomposer.pt')", "\n", "# raise SystemExit", "\n", "\n", "if", "config", ".", "print_stats", ":", "\n", "            ", "epoch_pbar", "=", "tqdm", "(", "\n", "range", "(", "1", ",", "config", ".", "num_epochs", "+", "1", ")", ",", "\n", "desc", "=", "'Epochs'", ")", "\n", "", "else", ":", "\n", "            ", "epoch_pbar", "=", "tqdm", "(", "\n", "range", "(", "1", ",", "config", ".", "num_epochs", "+", "1", ")", ",", "\n", "desc", "=", "config", ".", "output_dir", ".", "name", ")", "\n", "\n", "", "for", "epoch_index", "in", "epoch_pbar", ":", "\n", "            ", "if", "config", ".", "print_stats", ":", "\n", "                ", "batches", "=", "tqdm", "(", "\n", "enumerate", "(", "self", ".", "dataloader", ")", ",", "\n", "total", "=", "len", "(", "self", ".", "dataloader", ")", ",", "\n", "mininterval", "=", "config", ".", "progress_bar_refresh_rate", ",", "\n", "desc", "=", "'Batches'", ")", "\n", "", "else", ":", "\n", "                ", "batches", "=", "enumerate", "(", "self", ".", "dataloader", ")", "\n", "\n", "", "for", "batch_index", ",", "batch", "in", "batches", ":", "\n", "                ", "self", ".", "train_step", "(", "batch_index", ",", "batch", ")", "\n", "self", ".", "tb_global_step", "+=", "1", "\n", "", "self", ".", "auto_save", "(", "epoch_index", ")", "\n", "\n", "self", ".", "eval_step", "(", "epoch_index", ")", "\n", "\n", "if", "config", ".", "print_stats", ":", "\n", "                ", "self", ".", "print_timestamp", "(", "epoch_index", ")", "\n", "# if config.export_error_analysis:", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.IdealGroundedConfig.__post_init__": [[691, 839], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Sequential", "torch.nn.Sequential", "ValueError", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-i'", ",", "'--input-dir'", ",", "action", "=", "'store'", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-o'", ",", "'--output-dir'", ",", "action", "=", "'store'", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-d'", ",", "'--device'", ",", "action", "=", "'store'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-a'", ",", "'--architecture'", ",", "action", "=", "'store'", ",", "type", "=", "str", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-lr'", ",", "'--learning-rate'", ",", "action", "=", "'store'", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-bs'", ",", "'--batch-size'", ",", "action", "=", "'store'", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-ep'", ",", "'--num-epochs'", ",", "action", "=", "'store'", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-pe'", ",", "'--pretrained-embed-path'", ",", "action", "=", "'store'", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-sv'", ",", "'--auto-save-per-epoch'", ",", "action", "=", "'store'", ",", "type", "=", "int", ")", "\n", "parser", ".", "parse_args", "(", "namespace", "=", "self", ")", "\n", "\n", "if", "self", ".", "architecture", "==", "'linear'", ":", "\n", "            ", "self", ".", "deno_probe", "=", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_deno_classes", ")", "\n", "self", ".", "cono_probe", "=", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", "\n", "", "if", "self", ".", "architecture", "==", "'MLP1'", ":", "\n", "            ", "self", ".", "deno_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_deno_classes", ")", ")", "\n", "self", ".", "cono_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", ")", "\n", "", "elif", "self", ".", "architecture", "==", "'MLP2'", ":", "\n", "            ", "self", ".", "deno_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_deno_classes", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "cono_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "elif", "self", ".", "architecture", "==", "'MLP2_large'", ":", "\n", "            ", "self", ".", "deno_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "1024", ",", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "1024", ",", "self", ".", "num_deno_classes", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "self", ".", "cono_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "1024", ",", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "1024", ",", "self", ".", "num_cono_classes", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "elif", "self", ".", "architecture", "==", "'MLP3'", ":", "\n", "            ", "self", ".", "deno_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_deno_classes", ")", ")", "\n", "self", ".", "cono_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", ")", "\n", "", "elif", "self", ".", "architecture", "==", "'MLP4'", ":", "\n", "            ", "self", ".", "deno_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_deno_classes", ")", ")", "\n", "self", ".", "cono_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", ")", "\n", "", "elif", "self", ".", "architecture", "==", "'MLP4_large'", ":", "\n", "            ", "self", ".", "deno_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "1024", ",", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "1024", ",", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "1024", ",", "1024", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "1024", ",", "self", ".", "num_deno_classes", ")", ")", "\n", "self", ".", "cono_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown architecture argument.'", ")", "\n", "\n", "", "assert", "self", ".", "cono_probe", "[", "-", "1", "]", ".", "out_features", "==", "self", ".", "num_cono_classes", "\n", "assert", "self", ".", "deno_probe", "[", "-", "1", "]", ".", "out_features", "==", "self", ".", "num_deno_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.main": [[841, 846], ["ideal_grounded.IdealGroundedConfig", "ideal_grounded.IdealGroundedExperiment", "auto_save_wrapped.train"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.train"], ["", "", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "config", "=", "IdealGroundedConfig", "(", ")", "\n", "black_box", "=", "IdealGroundedExperiment", "(", "config", ")", "\n", "with", "black_box", "as", "auto_save_wrapped", ":", "\n", "        ", "auto_save_wrapped", ".", "train", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.__init__": [[24, 51], ["models.ideal_grounded.Decomposer.__init__", "torch.nn.Embedding.from_pretrained", "proxy_grounded.ProxyGroundedDecomposer.to"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "preserve", ":", "str", ",", "# either 'deno' or 'cono'", "\n", "initial_space", ":", "Matrix", ",", "\n", "cono_probe", ":", "nn", ".", "Module", ",", "\n", "id_to_word", ":", "Dict", "[", "int", ",", "str", "]", ",", "\n", "ground", ":", "Dict", "[", "str", ",", "GroundedWord", "]", ",", "\n", "num_negative_samples", ":", "int", ",", "\n", "negative_sampling_probs", ":", "Vector", ",", "\n", "device", ":", "torch", ".", "device", ")", ":", "\n", "        ", "super", "(", "Decomposer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "decomposed", "=", "nn", ".", "Embedding", ".", "from_pretrained", "(", "initial_space", ")", "\n", "self", ".", "decomposed", ".", "weight", ".", "requires_grad", "=", "True", "\n", "# self.SGNS_context = nn.Embedding.from_pretrained(initial_space)", "\n", "# self.SGNS_context.weight.requires_grad = True", "\n", "self", ".", "cono_probe", "=", "cono_probe", "\n", "self", ".", "num_cono_classes", "=", "cono_probe", "[", "-", "1", "]", ".", "out_features", "\n", "self", ".", "device", "=", "device", "\n", "self", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# for skip-gram negative sampling loss", "\n", "self", ".", "negative_sampling_probs", "=", "negative_sampling_probs", "\n", "self", ".", "num_negative_samples", "=", "num_negative_samples", "\n", "\n", "self", ".", "preserve", "=", "preserve", "\n", "self", ".", "id_to_word", "=", "id_to_word", "\n", "self", ".", "ground", "=", "ground", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.forward": [[52, 74], ["proxy_grounded.ProxyGroundedDecomposer.decomposed", "torch.mean", "proxy_grounded.ProxyGroundedDecomposer.cono_probe", "torch.nn.functional.log_softmax", "torch.nn.functional.nll_loss", "proxy_grounded.ProxyGroundedDecomposer.skip_gram_loss", "torch.full_like", "torch.nn.functional.kl_div"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.skip_gram_loss"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "center_word_ids", ":", "Vector", ",", "\n", "true_context_ids", ":", "Vector", ",", "\n", "seq_word_ids", ":", "Matrix", ",", "\n", "cono_labels", ":", "Vector", ",", "\n", ")", "->", "Tuple", "[", "Scalar", ",", "...", "]", ":", "\n", "        ", "seq_word_vecs", ":", "R3Tensor", "=", "self", ".", "decomposed", "(", "seq_word_ids", ")", "\n", "seq_repr", ":", "Matrix", "=", "torch", ".", "mean", "(", "seq_word_vecs", ",", "dim", "=", "1", ")", "\n", "\n", "cono_logits", "=", "self", ".", "cono_probe", "(", "seq_repr", ")", "\n", "cono_log_prob", "=", "F", ".", "log_softmax", "(", "cono_logits", ",", "dim", "=", "1", ")", "\n", "cono_probe_loss", "=", "F", ".", "nll_loss", "(", "cono_log_prob", ",", "cono_labels", ")", "\n", "\n", "proxy_deno_loss", "=", "self", ".", "skip_gram_loss", "(", "center_word_ids", ",", "true_context_ids", ")", "\n", "\n", "if", "self", ".", "preserve", "==", "'deno'", ":", "# DS removing connotation (gamma < 0)", "\n", "            ", "uniform_dist", "=", "torch", ".", "full_like", "(", "cono_log_prob", ",", "1", "/", "self", ".", "num_cono_classes", ")", "\n", "cono_adversary_loss", "=", "F", ".", "kl_div", "(", "cono_log_prob", ",", "uniform_dist", ",", "reduction", "=", "'batchmean'", ")", "\n", "return", "proxy_deno_loss", ",", "cono_probe_loss", ",", "cono_adversary_loss", ",", "seq_word_vecs", "\n", "", "else", ":", "# CS removing denotation", "\n", "            ", "return", "proxy_deno_loss", ",", "cono_probe_loss", ",", "seq_word_vecs", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.skip_gram_loss": [[75, 123], ["torch.multinomial().view().to", "proxy_grounded.ProxyGroundedDecomposer.decomposed", "proxy_grounded.ProxyGroundedDecomposer.decomposed", "proxy_grounded.ProxyGroundedDecomposer.decomposed", "torch.sum", "torch.nn.functional.logsigmoid", "torch.bmm().squeeze", "torch.nn.functional.logsigmoid", "torch.sum", "torch.mul", "torch.mean", "torch.multinomial().view", "torch.bmm", "len", "proxy_grounded.ProxyGroundedDecomposer.unsqueeze", "torch.multinomial", "len"], "methods", ["None"], ["", "", "def", "skip_gram_loss", "(", "\n", "self", ",", "\n", "center_word_ids", ":", "Vector", ",", "\n", "true_context_ids", ":", "Vector", "\n", ")", "->", "Scalar", ":", "\n", "# negative_context_ids = torch.multinomial(", "\n", "#     self.negative_sampling_probs,", "\n", "#     len(true_context_ids),", "\n", "#     replacement=True", "\n", "#     ).to(self.device)", "\n", "# center = self.decomposed(center_word_ids)", "\n", "# true_context = self.decomposed(true_context_ids)", "\n", "# negative_context = self.decomposed(negative_context_ids)", "\n", "\n", "# true_context_loss = F.cosine_embedding_loss(", "\n", "#     center, true_context, torch.ones_like(center_word_ids))", "\n", "# negative_context_loss = F.cosine_embedding_loss(", "\n", "#     center, negative_context, torch.zeros_like(center_word_ids))", "\n", "# deno_loss = true_context_loss - negative_context_loss", "\n", "\n", "# Faster but less readable", "\n", "        ", "negative_context_ids", "=", "torch", ".", "multinomial", "(", "\n", "self", ".", "negative_sampling_probs", ",", "\n", "len", "(", "true_context_ids", ")", "*", "self", ".", "num_negative_samples", ",", "\n", "replacement", "=", "True", "\n", ")", ".", "view", "(", "len", "(", "true_context_ids", ")", ",", "self", ".", "num_negative_samples", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "center", "=", "self", ".", "decomposed", "(", "center_word_ids", ")", "\n", "true_context", "=", "self", ".", "decomposed", "(", "true_context_ids", ")", "\n", "negative_context", "=", "self", ".", "decomposed", "(", "negative_context_ids", ")", "\n", "# true_context = self.SGNS_context(true_context_ids)", "\n", "# negative_context = self.SGNS_context(negative_context_ids)", "\n", "\n", "# batch_size * embed_size", "\n", "objective", "=", "torch", ".", "sum", "(", "# dot product", "\n", "torch", ".", "mul", "(", "center", ",", "true_context", ")", ",", "# Hadamard product", "\n", "dim", "=", "1", ")", "# be -> b", "\n", "objective", "=", "F", ".", "logsigmoid", "(", "objective", ")", "\n", "\n", "# batch_size * num_negative_samples * embed_size", "\n", "# negative_context: bne", "\n", "# center: be -> be1", "\n", "negative_objective", "=", "torch", ".", "bmm", "(", "# bne, be1 -> bn1", "\n", "negative_context", ",", "center", ".", "unsqueeze", "(", "2", ")", "\n", ")", ".", "squeeze", "(", ")", "# bn1 -> bn", "\n", "negative_objective", "=", "F", ".", "logsigmoid", "(", "-", "negative_objective", ")", "\n", "negative_objective", "=", "torch", ".", "sum", "(", "negative_objective", ",", "dim", "=", "1", ")", "# bn -> b", "\n", "return", "-", "torch", ".", "mean", "(", "objective", "+", "negative_objective", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.predict": [[124, 133], ["proxy_grounded.ProxyGroundedDecomposer.eval", "proxy_grounded.ProxyGroundedDecomposer.train", "torch.no_grad", "proxy_grounded.ProxyGroundedDecomposer.decomposed", "torch.mean", "proxy_grounded.ProxyGroundedDecomposer.cono_probe", "torch.nn.functional.softmax"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.train"], ["", "def", "predict", "(", "self", ",", "seq_word_ids", ":", "Vector", ")", "->", "Vector", ":", "\n", "        ", "self", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "word_vecs", ":", "R3Tensor", "=", "self", ".", "decomposed", "(", "seq_word_ids", ")", "\n", "seq_repr", ":", "Matrix", "=", "torch", ".", "mean", "(", "word_vecs", ",", "dim", "=", "1", ")", "\n", "cono", "=", "self", ".", "cono_probe", "(", "seq_repr", ")", "\n", "cono_conf", "=", "F", ".", "softmax", "(", "cono", ",", "dim", "=", "1", ")", "\n", "", "self", ".", "train", "(", ")", "\n", "return", "cono_conf", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.accuracy": [[134, 144], ["proxy_grounded.ProxyGroundedDecomposer.predict", "proxy_grounded.ProxyGroundedDecomposer.argmax", "proxy_grounded.ProxyGroundedDecomposer.argmax.eq", "cono_conf.argmax.eq.float().mean().item", "cono_conf.argmax.eq.float().mean", "cono_conf.argmax.eq.float"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.predict"], ["", "def", "accuracy", "(", "\n", "self", ",", "\n", "seq_word_ids", ":", "Matrix", ",", "\n", "cono_labels", ":", "Vector", ",", "\n", ")", "->", "float", ":", "\n", "        ", "cono_conf", "=", "self", ".", "predict", "(", "seq_word_ids", ")", "\n", "cono_predictions", "=", "cono_conf", ".", "argmax", "(", "dim", "=", "1", ")", "\n", "cono_correct_indicies", "=", "cono_predictions", ".", "eq", "(", "cono_labels", ")", "\n", "cono_accuracy", "=", "cono_correct_indicies", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "return", "cono_accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.homogeneity": [[145, 190], ["proxy_grounded.ProxyGroundedDecomposer.nearest_neighbors", "enumerate", "statistics.mean", "query_ids[].item", "cono_homogeneity.append", "len", "neighbor_ids.tolist", "len", "editdistance.eval"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Decomposer.nearest_neighbors"], ["", "def", "homogeneity", "(", "\n", "self", ",", "\n", "query_ids", ":", "Vector", ",", "\n", "top_k", ":", "int", "=", "10", "\n", ")", "->", "float", ":", "\n", "# extra top_k buffer for excluding edit distance neighbors", "\n", "        ", "top_neighbor_ids", "=", "self", ".", "nearest_neighbors", "(", "query_ids", ",", "top_k", "+", "5", ")", "\n", "cono_homogeneity", "=", "[", "]", "\n", "for", "query_index", ",", "neighbor_ids", "in", "enumerate", "(", "top_neighbor_ids", ")", ":", "\n", "            ", "query_id", "=", "query_ids", "[", "query_index", "]", ".", "item", "(", ")", "\n", "query_word", "=", "self", ".", "id_to_word", "[", "query_id", "]", "\n", "\n", "neighbor_ids", "=", "[", "\n", "nid", "for", "nid", "in", "neighbor_ids", ".", "tolist", "(", ")", "\n", "if", "editdistance", ".", "eval", "(", "query_word", ",", "self", ".", "id_to_word", "[", "nid", "]", ")", ">", "3", "]", "\n", "neighbor_ids", "=", "neighbor_ids", "[", ":", "top_k", "]", "\n", "\n", "if", "len", "(", "neighbor_ids", ")", "==", "0", ":", "\n", "# print(query_word, [self.id_to_word[i.item()]", "\n", "#                    for i in top_neighbor_ids[query_index]])", "\n", "# raise RuntimeWarning", "\n", "                ", "continue", "\n", "\n", "", "query_cono", "=", "self", ".", "ground", "[", "query_word", "]", ".", "majority_cono", "\n", "same_cono", "=", "0", "\n", "for", "nid", "in", "neighbor_ids", ":", "\n", "                ", "neighbor_word", "=", "self", ".", "id_to_word", "[", "nid", "]", "\n", "try", ":", "\n", "                    ", "neighbor_cono", "=", "self", ".", "ground", "[", "neighbor_word", "]", ".", "majority_cono", "\n", "if", "neighbor_cono", "==", "query_cono", ":", "\n", "                        ", "same_cono", "+=", "1", "\n", "", "", "except", "KeyError", ":", "# special tokens like [PAD] are ungrounded", "\n", "                    ", "continue", "\n", "", "", "cono_homogeneity", ".", "append", "(", "same_cono", "/", "len", "(", "neighbor_ids", ")", ")", "\n", "\n", "# Continuous Connotation Version", "\n", "# neighbor_cono = torch.stack([", "\n", "#     self.cono_grounding[nid] for nid in neighbor_ids])", "\n", "# diveregence = F.kl_div(", "\n", "#     query_cono.unsqueeze(0),", "\n", "#     neighbor_cono,", "\n", "#     reduction='batchmean').item()", "\n", "# if np.isfinite(diveregence):", "\n", "#     cono_homogeneity.append(-diveregence)", "\n", "", "return", "mean", "(", "cono_homogeneity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.extra_homogeneity": [[191, 233], ["proxy_grounded.ProxyGroundedDecomposer.nearest_neighbors", "enumerate", "query_ids[].item", "deno_homogeneity.append", "cono_homogeneity.append", "statistics.mean", "statistics.mean", "len", "neighbor_ids.tolist", "len", "len", "editdistance.eval"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.ideal_grounded.Decomposer.nearest_neighbors"], ["", "def", "extra_homogeneity", "(", "\n", "self", ",", "\n", "query_ids", ":", "Vector", ",", "\n", "top_k", ":", "int", "=", "10", "\n", ")", "->", "Tuple", "[", "float", ",", "float", "]", ":", "\n", "# extra 5 top-k for excluding edit distance neighbors", "\n", "        ", "top_neighbor_ids", "=", "self", ".", "nearest_neighbors", "(", "query_ids", ",", "top_k", "+", "5", ")", "\n", "deno_homogeneity", "=", "[", "]", "\n", "cono_homogeneity", "=", "[", "]", "\n", "for", "query_index", ",", "neighbor_ids", "in", "enumerate", "(", "top_neighbor_ids", ")", ":", "\n", "            ", "query_id", "=", "query_ids", "[", "query_index", "]", ".", "item", "(", ")", "\n", "query_word", "=", "self", ".", "id_to_word", "[", "query_id", "]", "\n", "\n", "neighbor_ids", "=", "[", "\n", "nid", "for", "nid", "in", "neighbor_ids", ".", "tolist", "(", ")", "\n", "if", "editdistance", ".", "eval", "(", "query_word", ",", "self", ".", "id_to_word", "[", "nid", "]", ")", ">", "3", "]", "\n", "neighbor_ids", "=", "neighbor_ids", "[", ":", "top_k", "]", "\n", "if", "len", "(", "neighbor_ids", ")", "==", "0", ":", "\n", "# print(query_word, [self.id_to_word[i.item()]", "\n", "#                    for i in top_neighbor_ids[query_index]])", "\n", "# raise RuntimeWarning", "\n", "                ", "continue", "\n", "\n", "", "query_deno", "=", "self", ".", "ground", "[", "query_word", "]", ".", "majority_deno", "\n", "query_cono", "=", "self", ".", "ground", "[", "query_word", "]", ".", "majority_cono", "\n", "same_deno", "=", "0", "\n", "same_cono", "=", "0", "\n", "for", "nid", "in", "neighbor_ids", ":", "\n", "                ", "try", ":", "\n", "                    ", "neighbor_word", "=", "self", ".", "id_to_word", "[", "nid", "]", "\n", "neighbor_deno", "=", "self", ".", "ground", "[", "neighbor_word", "]", ".", "majority_deno", "\n", "neighbor_cono", "=", "self", ".", "ground", "[", "neighbor_word", "]", ".", "majority_cono", "\n", "if", "neighbor_deno", "==", "query_deno", ":", "\n", "                        ", "same_deno", "+=", "1", "\n", "", "if", "neighbor_cono", "==", "query_cono", ":", "\n", "                        ", "same_cono", "+=", "1", "\n", "", "", "except", "(", "KeyError", ",", "AttributeError", ")", ":", "# special tokens like [PAD] are ungrounded", "\n", "# print(neighbor_word)", "\n", "                    ", "continue", "\n", "", "", "deno_homogeneity", ".", "append", "(", "same_deno", "/", "len", "(", "neighbor_ids", ")", ")", "\n", "cono_homogeneity", ".", "append", "(", "same_cono", "/", "len", "(", "neighbor_ids", ")", ")", "\n", "", "return", "mean", "(", "deno_homogeneity", ")", ",", "mean", "(", "cono_homogeneity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.__init__": [[237, 277], ["models.ideal_grounded.Recomposer.__init__", "utils.experiment.Experiment.load_txt_embedding", "proxy_grounded.ProxyGroundedDecomposer", "proxy_grounded.ProxyGroundedDecomposer", "torch.nn.Linear", "proxy_grounded.ProxyGroundedRecomposer.to", "hasattr"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.__init__", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.load_txt_embedding"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "config", ":", "'ProxyGroundedConfig'", ",", "\n", "data", ":", "'LabeledDocuments'", ")", ":", "\n", "        ", "super", "(", "Recomposer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "device", "=", "config", ".", "device", "\n", "\n", "self", ".", "pretrained_embed", "=", "Experiment", ".", "load_txt_embedding", "(", "\n", "config", ".", "pretrained_embed_path", ",", "data", ".", "word_to_id", ")", "\n", "self", ".", "pretrained_embed", ".", "weight", ".", "requires_grad", "=", "False", "\n", "\n", "self", ".", "deno_space", "=", "ProxyGroundedDecomposer", "(", "\n", "preserve", "=", "'deno'", ",", "\n", "initial_space", "=", "self", ".", "pretrained_embed", ".", "weight", ",", "\n", "cono_probe", "=", "config", ".", "cono_probe", ",", "\n", "id_to_word", "=", "data", ".", "id_to_word", ",", "\n", "ground", "=", "data", ".", "ground", ",", "\n", "num_negative_samples", "=", "config", ".", "num_negative_samples", ",", "\n", "negative_sampling_probs", "=", "data", ".", "negative_sampling_probs", ",", "\n", "device", "=", "self", ".", "device", ")", "\n", "\n", "self", ".", "cono_space", "=", "ProxyGroundedDecomposer", "(", "\n", "preserve", "=", "'cono'", ",", "\n", "initial_space", "=", "self", ".", "pretrained_embed", ".", "weight", ",", "\n", "cono_probe", "=", "config", ".", "cono_probe", ",", "\n", "id_to_word", "=", "data", ".", "id_to_word", ",", "\n", "ground", "=", "data", ".", "ground", ",", "\n", "num_negative_samples", "=", "config", ".", "num_negative_samples", ",", "\n", "negative_sampling_probs", "=", "data", ".", "negative_sampling_probs", ",", "\n", "device", "=", "self", ".", "device", ")", "\n", "\n", "# Recomposer", "\n", "self", ".", "recomposer", "=", "nn", ".", "Linear", "(", "600", ",", "300", ")", "\n", "self", ".", "rho", "=", "config", ".", "recomposer_rho", "\n", "self", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "self", ".", "word_to_id", "=", "data", ".", "word_to_id", "\n", "self", ".", "id_to_word", "=", "data", ".", "id_to_word", "\n", "self", ".", "ground", "=", "data", ".", "ground", "\n", "self", ".", "eval_deno", "=", "hasattr", "(", "config", ",", "\"extra_grounding\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.forward": [[278, 305], ["proxy_grounded.ProxyGroundedRecomposer.deno_space", "proxy_grounded.ProxyGroundedRecomposer.cono_space", "proxy_grounded.ProxyGroundedRecomposer.recomposer", "proxy_grounded.ProxyGroundedRecomposer.pretrained_embed", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.cat", "torch.nn.functional.cosine_similarity().mean", "torch.sigmoid", "torch.nn.functional.cosine_similarity"], "methods", ["None"], ["", "def", "forward", "(", "\n", "self", ",", "\n", "center_word_ids", ":", "Vector", ",", "\n", "context_word_ids", ":", "Vector", ",", "\n", "seq_word_ids", ":", "Matrix", ",", "\n", "cono_labels", ":", "Vector", ",", "\n", ")", "->", "Tuple", "[", "Scalar", ",", "...", "]", ":", "\n", "# Denotation Space", "\n", "        ", "DS_deno_proxy", ",", "DS_cono_probe", ",", "DS_cono_adver", ",", "deno_vecs", "=", "self", ".", "deno_space", "(", "\n", "center_word_ids", ",", "context_word_ids", ",", "seq_word_ids", ",", "cono_labels", ")", "\n", "DS_decomp", "=", "torch", ".", "sigmoid", "(", "DS_deno_proxy", ")", "+", "torch", ".", "sigmoid", "(", "DS_cono_adver", ")", "\n", "\n", "# Connotation Space", "\n", "CS_deno_proxy", ",", "CS_cono_probe", ",", "cono_vecs", "=", "self", ".", "cono_space", "(", "\n", "center_word_ids", ",", "context_word_ids", ",", "seq_word_ids", ",", "cono_labels", ")", "\n", "CS_decomp", "=", "1", "-", "torch", ".", "sigmoid", "(", "CS_deno_proxy", ")", "+", "torch", ".", "sigmoid", "(", "CS_cono_probe", ")", "\n", "\n", "# Recomposer", "\n", "recomposed", "=", "self", ".", "recomposer", "(", "torch", ".", "cat", "(", "(", "deno_vecs", ",", "cono_vecs", ")", ",", "dim", "=", "-", "1", ")", ")", "\n", "# recomposed = deno_vecs + cono_vecs  # cosine similarity ignores magnitude", "\n", "pretrained", "=", "self", ".", "pretrained_embed", "(", "seq_word_ids", ")", "\n", "L_R", "=", "1", "-", "F", ".", "cosine_similarity", "(", "recomposed", ",", "pretrained", ",", "dim", "=", "-", "1", ")", ".", "mean", "(", ")", "\n", "\n", "L_joint", "=", "DS_decomp", "+", "CS_decomp", "+", "self", ".", "rho", "*", "L_R", "\n", "return", "(", "L_joint", ",", "L_R", ",", "\n", "DS_decomp", ",", "DS_deno_proxy", ",", "DS_cono_probe", ",", "DS_cono_adver", ",", "\n", "CS_decomp", ",", "CS_deno_proxy", ",", "CS_cono_probe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.predict": [[306, 310], ["proxy_grounded.ProxyGroundedRecomposer.deno_space.predict", "proxy_grounded.ProxyGroundedRecomposer.cono_space.predict"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.predict", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.predict"], ["", "def", "predict", "(", "self", ",", "seq_word_ids", ":", "Vector", ")", "->", "Tuple", "[", "Vector", ",", "Vector", "]", ":", "\n", "        ", "DS_cono_conf", "=", "self", ".", "deno_space", ".", "predict", "(", "seq_word_ids", ")", "\n", "CS_cono_conf", "=", "self", ".", "cono_space", ".", "predict", "(", "seq_word_ids", ")", "\n", "return", "DS_cono_conf", ",", "CS_cono_conf", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.accuracy": [[311, 319], ["proxy_grounded.ProxyGroundedRecomposer.deno_space.accuracy", "proxy_grounded.ProxyGroundedRecomposer.cono_space.accuracy"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.accuracy", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.accuracy"], ["", "def", "accuracy", "(", "\n", "self", ",", "\n", "seq_word_ids", ":", "Matrix", ",", "\n", "cono_labels", ":", "Vector", ",", "\n", ")", "->", "Tuple", "[", "float", ",", "float", "]", ":", "\n", "        ", "DS_cono_acc", "=", "self", ".", "deno_space", ".", "accuracy", "(", "seq_word_ids", ",", "cono_labels", ")", "\n", "CS_cono_acc", "=", "self", ".", "cono_space", ".", "accuracy", "(", "seq_word_ids", ",", "cono_labels", ")", "\n", "return", "DS_cono_acc", ",", "CS_cono_acc", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity": [[320, 342], ["proxy_grounded.ProxyGroundedRecomposer.deno_space.homogeneity", "proxy_grounded.ProxyGroundedRecomposer.cono_space.homogeneity", "proxy_grounded.ProxyGroundedRecomposer.deno_space.extra_homogeneity", "proxy_grounded.ProxyGroundedRecomposer.cono_space.extra_homogeneity"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.extra_homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.extra_homogeneity"], ["", "def", "homogeneity", "(", "\n", "self", ",", "\n", "query_ids", ":", "Vector", ",", "\n", "top_k", ":", "int", "=", "10", "\n", ")", "->", "Tuple", "[", "float", ",", "...", "]", ":", "\n", "        ", "\"\"\"\n        Only compute connotation homogeneity; but if denotation grounding\n        is added retroactively, call the parent class's homogeneity() method,\n        which returns DS_Hdeno, DS_Hcono, CS_Hdeno, CS_Hcono\n        \"\"\"", "\n", "if", "self", ".", "eval_deno", ":", "\n", "# DS_Hdeno, DS_Hcono = Decomposer.homogeneity(", "\n", "#     self.deno_space, query_ids, top_k=top_k)", "\n", "# CS_Hdeno, CS_Hcono = Decomposer.homogeneity(", "\n", "#     self.cono_space, query_ids, top_k=top_k)", "\n", "            ", "DS_Hdeno", ",", "DS_Hcono", "=", "self", ".", "deno_space", ".", "extra_homogeneity", "(", "query_ids", ",", "top_k", "=", "top_k", ")", "\n", "CS_Hdeno", ",", "CS_Hcono", "=", "self", ".", "cono_space", ".", "extra_homogeneity", "(", "query_ids", ",", "top_k", "=", "top_k", ")", "\n", "return", "DS_Hdeno", ",", "DS_Hcono", ",", "CS_Hdeno", ",", "CS_Hcono", "\n", "\n", "", "DS_Hcono", "=", "self", ".", "deno_space", ".", "homogeneity", "(", "query_ids", ",", "top_k", "=", "top_k", ")", "\n", "CS_Hcono", "=", "self", ".", "cono_space", ".", "homogeneity", "(", "query_ids", ",", "top_k", "=", "top_k", ")", "\n", "return", "DS_Hcono", ",", "CS_Hcono", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.LabeledDocuments.__init__": [[367, 420], ["super().__init__", "print", "torch.tensor", "hasattr", "random.shuffle", "proxy_grounded.LabeledDocuments.__init__.load_eval_words"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "'ProxyGroundedConfig'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "batch_size", "=", "config", ".", "batch_size", "\n", "self", ".", "window_radius", "=", "config", ".", "skip_gram_window_radius", "\n", "self", ".", "numericalize_cono", "=", "config", ".", "numericalize_cono", "\n", "\n", "print", "(", "f'Loading {config.corpus_path}'", ",", "flush", "=", "True", ")", "\n", "with", "open", "(", "config", ".", "corpus_path", ",", "'rb'", ")", "as", "corpus_file", ":", "\n", "            ", "preprocessed", "=", "pickle", ".", "load", "(", "corpus_file", ")", "\n", "", "self", ".", "word_to_id", ":", "Dict", "[", "str", ",", "int", "]", "=", "preprocessed", "[", "'word_to_id'", "]", "\n", "self", ".", "id_to_word", ":", "Dict", "[", "int", ",", "str", "]", "=", "preprocessed", "[", "'id_to_word'", "]", "\n", "self", ".", "ground", ":", "Dict", "[", "str", ",", "GroundedWord", "]", "=", "preprocessed", "[", "'ground'", "]", "\n", "self", ".", "documents", ":", "List", "[", "LabeledDoc", "]", "=", "preprocessed", "[", "'documents'", "]", "\n", "self", ".", "negative_sampling_probs", ":", "Vector", "=", "torch", ".", "tensor", "(", "\n", "preprocessed", "[", "'negative_sampling_probs'", "]", ")", "\n", "\n", "if", "hasattr", "(", "config", ",", "\"extra_grounding\"", ")", ":", "\n", "            ", "with", "open", "(", "config", ".", "extra_grounding", ",", "'rb'", ")", "as", "extra_file", ":", "\n", "                ", "preprocessed", "=", "pickle", ".", "load", "(", "extra_file", ")", "\n", "", "deno_ground", "=", "preprocessed", "[", "'ground'", "]", "\n", "for", "word", "in", "deno_ground", ".", "values", "(", ")", ":", "\n", "                ", "if", "word", ".", "text", "in", "self", ".", "ground", ":", "\n", "                    ", "self", ".", "ground", "[", "word", ".", "text", "]", ".", "deno", "=", "word", ".", "deno", "\n", "self", ".", "ground", "[", "word", ".", "text", "]", ".", "majority_deno", "=", "word", ".", "majority_deno", "\n", "\n", "", "", "", "random", ".", "shuffle", "(", "self", ".", "documents", ")", "\n", "self", ".", "estimated_len", "=", "(", "\n", "sum", "(", "[", "len", "(", "sent", ".", "numerical_tokens", ")", "\n", "for", "doc", "in", "self", ".", "documents", "\n", "for", "sent", "in", "doc", ".", "sentences", "]", ")", "\n", "*", "self", ".", "window_radius", "//", "self", ".", "batch_size", ")", "\n", "\n", "def", "load_eval_words", "(", "path", ":", "Path", ")", "->", "Vector", ":", "\n", "            ", "in_vocab", "=", "[", "]", "\n", "with", "open", "(", "path", ")", "as", "file", ":", "\n", "                ", "for", "line", "in", "file", ":", "\n", "                    ", "word", "=", "line", ".", "strip", "(", ")", "\n", "if", "word", "in", "self", ".", "word_to_id", ":", "\n", "                        ", "in_vocab", ".", "append", "(", "word", ")", "\n", "", "", "", "if", "hasattr", "(", "config", ",", "\"extra_grounding\"", ")", ":", "\n", "                ", "in_vocab", "=", "[", "w", "for", "w", "in", "in_vocab", "if", "w", "in", "deno_ground", "]", "\n", "", "print", "(", "f'Loaded {len(in_vocab)} in-vocab eval words from {path}'", ")", "\n", "return", "torch", ".", "tensor", "(", "\n", "[", "self", ".", "word_to_id", "[", "w", "]", "for", "w", "in", "in_vocab", "]", ",", "device", "=", "config", ".", "device", ")", "\n", "\n", "", "self", ".", "dev_ids", "=", "load_eval_words", "(", "config", ".", "dev_path", ")", "\n", "self", ".", "test_ids", "=", "load_eval_words", "(", "config", ".", "test_path", ")", "\n", "self", ".", "rand_ids", "=", "load_eval_words", "(", "config", ".", "rand_path", ")", "\n", "\n", "# Set up multiprocessing", "\n", "self", ".", "total_workload", "=", "len", "(", "self", ".", "documents", ")", "\n", "self", ".", "worker_start", ":", "Optional", "[", "int", "]", "=", "None", "\n", "self", ".", "worker_end", ":", "Optional", "[", "int", "]", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.LabeledDocuments.__iter__": [[424, 464], ["enumerate", "batch_seq.append", "batch_cono.append", "len", "max", "min", "torch.tensor", "len", "torch.nn.utils.rnn.pad_sequence", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", "->", "Iterable", "[", "Tuple", "]", ":", "\n", "        ", "\"\"\"\n        Denotation: Parsing (center, context) word_id pairs\n        Connotation: (a sentence of word_ids, cono_label)\n        \"\"\"", "\n", "documents", "=", "self", ".", "documents", "[", "self", ".", "worker_start", ":", "self", ".", "worker_end", "]", "\n", "batch_seq", ":", "List", "[", "Vector", "]", "=", "[", "]", "\n", "batch_cono", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "batch_center", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "batch_context", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "for", "doc", "in", "documents", ":", "\n", "            ", "cono_label", "=", "self", ".", "numericalize_cono", "[", "doc", ".", "party", "]", "\n", "for", "sent", "in", "doc", ".", "sentences", ":", "\n", "                ", "if", "len", "(", "batch_center", ")", ">", "self", ".", "batch_size", ":", "\n", "                    ", "yield", "(", "\n", "nn", ".", "utils", ".", "rnn", ".", "pad_sequence", "(", "batch_seq", ",", "batch_first", "=", "True", ")", ",", "\n", "torch", ".", "tensor", "(", "batch_center", ")", ",", "\n", "torch", ".", "tensor", "(", "batch_context", ")", ",", "\n", "torch", ".", "tensor", "(", "batch_cono", ")", ")", "\n", "batch_seq", "=", "[", "]", "\n", "batch_cono", "=", "[", "]", "\n", "batch_center", "=", "[", "]", "\n", "batch_context", "=", "[", "]", "\n", "\n", "", "center_word_ids", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "context_word_ids", ":", "List", "[", "int", "]", "=", "[", "]", "\n", "seq", "=", "sent", ".", "numerical_tokens", "\n", "for", "center_index", ",", "center_word_id", "in", "enumerate", "(", "seq", ")", ":", "\n", "                    ", "left_index", "=", "max", "(", "center_index", "-", "self", ".", "window_radius", ",", "0", ")", "\n", "right_index", "=", "min", "(", "center_index", "+", "self", ".", "window_radius", ",", "\n", "len", "(", "seq", ")", "-", "1", ")", "\n", "context_word_id", ":", "List", "[", "int", "]", "=", "(", "\n", "seq", "[", "left_index", ":", "center_index", "]", "+", "\n", "seq", "[", "center_index", "+", "1", ":", "right_index", "+", "1", "]", ")", "\n", "context_word_ids", "+=", "context_word_id", "\n", "center_word_ids", "+=", "[", "center_word_id", "]", "*", "len", "(", "context_word_id", ")", "\n", "", "batch_seq", ".", "append", "(", "torch", ".", "tensor", "(", "seq", ")", ")", "\n", "batch_cono", ".", "append", "(", "cono_label", ")", "\n", "batch_center", "+=", "center_word_ids", "\n", "batch_context", "+=", "context_word_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.LabeledDocuments.worker_init_fn": [[465, 474], ["torch.utils.data.get_worker_info", "min"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "worker_init_fn", "(", "worker_id", ":", "int", ")", "->", "None", ":", "\n", "        ", "worker", "=", "torch", ".", "utils", ".", "data", ".", "get_worker_info", "(", ")", "\n", "assert", "worker", ".", "id", "==", "worker_id", "\n", "dataset", "=", "worker", ".", "dataset", "\n", "per_worker", "=", "dataset", ".", "total_workload", "//", "worker", ".", "num_workers", "\n", "dataset", ".", "worker_start", "=", "worker", ".", "id", "*", "per_worker", "\n", "dataset", ".", "worker_end", "=", "min", "(", "dataset", ".", "worker_start", "+", "per_worker", ",", "\n", "dataset", ".", "total_workload", ")", "\n", "# print(f'Worker {worker_id + 1}/{worker.num_workers} loading data '", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.__init__": [[480, 533], ["utils.experiment.Experiment.__init__", "proxy_grounded.LabeledDocuments", "torch.utils.data.DataLoader", "proxy_grounded.ProxyGroundedRecomposer", "config.optimizer", "config.optimizer", "config.optimizer", "config.optimizer", "print", "model.deno_space.cono_probe.parameters", "model.cono_space.cono_probe.parameters", "model.recomposer.parameters", "model.deno_space.homogeneity", "model.deno_space.homogeneity", "model.deno_space.homogeneity", "model.deno_space.extra_homogeneity", "model.deno_space.extra_homogeneity", "model.deno_space.extra_homogeneity", "list", "list", "model.deno_space.decomposed.parameters", "model.cono_space.decomposed.parameters"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.__init__", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.extra_homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.extra_homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedDecomposer.extra_homogeneity"], ["    ", "def", "__init__", "(", "self", ",", "config", ":", "'ProxyGroundedConfig'", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "data", "=", "LabeledDocuments", "(", "config", ")", "\n", "self", ".", "dataloader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "self", ".", "data", ",", "\n", "batch_size", "=", "None", ",", "# disable auto batching, see __iter__", "\n", "num_workers", "=", "config", ".", "num_dataloader_threads", ",", "\n", "worker_init_fn", "=", "self", ".", "data", ".", "worker_init_fn", ",", "\n", "pin_memory", "=", "True", ")", "\n", "self", ".", "model", "=", "ProxyGroundedRecomposer", "(", "config", ",", "self", ".", "data", ")", "\n", "model", "=", "self", ".", "model", "\n", "\n", "# for name, param in model.named_parameters():", "\n", "#     if param.requires_grad:", "\n", "#         print(name)  # param.data)", "\n", "\n", "self", ".", "DS_cono_optimizer", "=", "config", ".", "optimizer", "(", "\n", "model", ".", "deno_space", ".", "cono_probe", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ")", "\n", "self", ".", "CS_cono_optimizer", "=", "config", ".", "optimizer", "(", "\n", "model", ".", "cono_space", ".", "cono_probe", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ")", "\n", "\n", "self", ".", "joint_optimizer", "=", "config", ".", "optimizer", "(", "\n", "list", "(", "model", ".", "deno_space", ".", "decomposed", ".", "parameters", "(", ")", ")", "+", "\n", "list", "(", "model", ".", "cono_space", ".", "decomposed", ".", "parameters", "(", ")", ")", ",", "\n", "lr", "=", "config", ".", "learning_rate", ")", "\n", "self", ".", "R_optimizer", "=", "config", ".", "optimizer", "(", "\n", "model", ".", "recomposer", ".", "parameters", "(", ")", ",", "lr", "=", "config", ".", "learning_rate", ")", "\n", "\n", "model", ".", "dev_ids", "=", "self", ".", "data", ".", "dev_ids", "\n", "model", ".", "test_ids", "=", "self", ".", "data", ".", "test_ids", "\n", "model", ".", "rand_ids", "=", "self", ".", "data", ".", "rand_ids", "\n", "if", "not", "model", ".", "eval_deno", ":", "\n", "            ", "dev_Hc", "=", "model", ".", "deno_space", ".", "homogeneity", "(", "model", ".", "dev_ids", ")", "\n", "test_Hc", "=", "model", ".", "deno_space", ".", "homogeneity", "(", "model", ".", "test_ids", ")", "\n", "rand_Hc", "=", "model", ".", "deno_space", ".", "homogeneity", "(", "model", ".", "rand_ids", ")", "\n", "model", ".", "PE_homogeneity", "=", "{", "\n", "'dev Hc'", ":", "dev_Hc", ",", "\n", "'test Hc'", ":", "test_Hc", ",", "\n", "'rand Hc'", ":", "rand_Hc", ",", "\n", "}", "\n", "", "else", ":", "\n", "            ", "dev_Hd", ",", "dev_Hc", "=", "model", ".", "deno_space", ".", "extra_homogeneity", "(", "model", ".", "dev_ids", ")", "\n", "test_Hd", ",", "test_Hc", "=", "model", ".", "deno_space", ".", "extra_homogeneity", "(", "model", ".", "test_ids", ")", "\n", "rand_Hd", ",", "rand_Hc", "=", "model", ".", "deno_space", ".", "extra_homogeneity", "(", "model", ".", "rand_ids", ")", "\n", "model", ".", "PE_homogeneity", "=", "{", "\n", "'dev Hd'", ":", "dev_Hd", ",", "\n", "'dev Hc'", ":", "dev_Hc", ",", "\n", "'test Hd'", ":", "test_Hd", ",", "\n", "'test Hc'", ":", "test_Hc", ",", "\n", "'rand Hd'", ":", "rand_Hd", ",", "\n", "'rand Hc'", ":", "rand_Hc", ",", "\n", "}", "\n", "", "print", "(", "model", ".", "PE_homogeneity", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.train_step": [[534, 579], ["batch[].to", "batch[].to", "batch[].to", "batch[].to", "model.zero_grad", "model.deno_space", "DS_cono_probe.backward", "proxy_grounded.ProxyGroundedExperiment.DS_cono_optimizer.step", "model.zero_grad", "model.cono_space", "CS_cono_probe.backward", "proxy_grounded.ProxyGroundedExperiment.CS_cono_optimizer.step", "model.zero_grad", "model", "L_joint.backward", "proxy_grounded.ProxyGroundedExperiment.joint_optimizer.step", "proxy_grounded.ProxyGroundedExperiment.R_optimizer.step", "model.accuracy", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.accuracy", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard"], ["", "def", "train_step", "(", "self", ",", "batch_index", ":", "int", ",", "batch", ":", "Tuple", ")", "->", "None", ":", "\n", "        ", "model", "=", "self", ".", "model", "\n", "seq_word_ids", "=", "batch", "[", "0", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "center_word_ids", "=", "batch", "[", "1", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "context_word_ids", "=", "batch", "[", "2", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "cono_labels", "=", "batch", "[", "3", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Update probes with proper (non-adversarial) losses", "\n", "model", ".", "zero_grad", "(", ")", "\n", "DS_deno_proxy", ",", "DS_cono_probe", ",", "DS_cono_adver", ",", "_", "=", "model", ".", "deno_space", "(", "\n", "center_word_ids", ",", "context_word_ids", ",", "seq_word_ids", ",", "cono_labels", ")", "\n", "DS_cono_probe", ".", "backward", "(", ")", "\n", "self", ".", "DS_cono_optimizer", ".", "step", "(", ")", "\n", "\n", "model", ".", "zero_grad", "(", ")", "\n", "CS_deno_proxy", ",", "CS_cono_probe", ",", "_", "=", "model", ".", "cono_space", "(", "\n", "center_word_ids", ",", "context_word_ids", ",", "seq_word_ids", ",", "cono_labels", ")", "\n", "CS_cono_probe", ".", "backward", "(", ")", "\n", "self", ".", "CS_cono_optimizer", ".", "step", "(", ")", "\n", "\n", "model", ".", "zero_grad", "(", ")", "\n", "(", "L_joint", ",", "L_R", ",", "\n", "DS_decomp", ",", "DS_deno_proxy", ",", "DS_cono_probe", ",", "DS_cono_adver", ",", "\n", "CS_decomp", ",", "CS_deno_proxy", ",", "CS_cono_probe", ")", "=", "model", "(", "\n", "center_word_ids", ",", "context_word_ids", ",", "seq_word_ids", ",", "cono_labels", ")", "\n", "L_joint", ".", "backward", "(", ")", "\n", "self", ".", "joint_optimizer", ".", "step", "(", ")", "\n", "self", ".", "R_optimizer", ".", "step", "(", ")", "\n", "\n", "if", "batch_index", "%", "self", ".", "config", ".", "update_tensorboard", "==", "0", ":", "\n", "            ", "DS_cono_acc", ",", "CS_cono_acc", "=", "model", ".", "accuracy", "(", "seq_word_ids", ",", "cono_labels", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Denotation Decomposer/deno_loss'", ":", "DS_deno_proxy", ",", "\n", "'Denotation Decomposer/cono_loss_proper'", ":", "DS_cono_probe", ",", "\n", "'Denotation Decomposer/cono_loss_adversary'", ":", "DS_cono_adver", ",", "\n", "'Denotation Decomposer/combined_loss'", ":", "DS_decomp", ",", "\n", "'Denotation Decomposer/accuracy_train_cono'", ":", "DS_cono_acc", ",", "\n", "\n", "'Connotation Decomposer/deno_loss'", ":", "CS_cono_probe", ",", "\n", "'Connotation Decomposer/cono_loss_proper'", ":", "CS_cono_probe", ",", "\n", "'Connotation Decomposer/combined_loss'", ":", "CS_decomp", ",", "\n", "'Connotation Decomposer/accuracy_train_cono'", ":", "CS_cono_acc", ",", "\n", "\n", "'Recomposer/joint loss'", ":", "L_joint", ",", "\n", "'Recomposer/recomposition loss'", ":", "L_R", "\n", "}", ")", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.eval_step": [[583, 664], ["evaluations.word_similarity.all_wordsim.mean_delta", "torch.nn.functional.cosine_similarity().mean", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard", "evaluations.word_similarity.all_wordsim.mean_delta", "torch.nn.functional.cosine_similarity().mean", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard", "evaluations.word_similarity.all_wordsim.mean_delta", "torch.nn.functional.cosine_similarity().mean", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard", "model.homogeneity", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard", "model.homogeneity", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard", "model.homogeneity", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard", "model.homogeneity", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard", "model.homogeneity", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard", "model.homogeneity", "proxy_grounded.ProxyGroundedExperiment.update_tensorboard", "torch.no_grad", "torch.arange", "model.deno_space.decomposed", "model.cono_space.decomposed", "model.recomposer", "torch.nn.functional.cosine_similarity", "torch.nn.functional.cosine_similarity", "torch.cat", "torch.nn.functional.cosine_similarity"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedRecomposer.homogeneity", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.update_tensorboard"], ["", "", "def", "eval_step", "(", "self", ",", "epoch_index", ":", "int", ")", "->", "None", ":", "\n", "        ", "model", "=", "self", ".", "model", "\n", "PE", "=", "model", ".", "PE_homogeneity", "\n", "if", "not", "model", ".", "eval_deno", ":", "\n", "            ", "DS_Hc", ",", "CS_Hc", "=", "model", ".", "homogeneity", "(", "self", ".", "data", ".", "dev_ids", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Homogeneity Diff Dev/DS Hcono'", ":", "DS_Hc", "-", "PE", "[", "'dev Hc'", "]", ",", "\n", "'Homogeneity Diff Dev/CS Hcono'", ":", "CS_Hc", "-", "PE", "[", "'dev Hc'", "]", ",", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "DS_Hc", ",", "CS_Hc", "=", "model", ".", "homogeneity", "(", "self", ".", "data", ".", "test_ids", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Homogeneity Diff Test/DS Hcono'", ":", "DS_Hc", "-", "PE", "[", "'test Hc'", "]", ",", "\n", "'Homogeneity Diff Test/CS Hcono'", ":", "CS_Hc", "-", "PE", "[", "'test Hc'", "]", ",", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "DS_Hc", ",", "CS_Hc", "=", "model", ".", "homogeneity", "(", "self", ".", "data", ".", "rand_ids", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Homogeneity Diff Random/DS Hcono'", ":", "DS_Hc", "-", "PE", "[", "'rand Hc'", "]", ",", "\n", "'Homogeneity Diff Random/CS Hcono'", ":", "CS_Hc", "-", "PE", "[", "'rand Hc'", "]", ",", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "", "else", ":", "\n", "            ", "DS_Hd", ",", "DS_Hc", ",", "CS_Hd", ",", "CS_Hc", "=", "model", ".", "homogeneity", "(", "self", ".", "data", ".", "dev_ids", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Homogeneity Diff Dev/DS Hdeno'", ":", "DS_Hd", "-", "PE", "[", "'dev Hd'", "]", ",", "\n", "'Homogeneity Diff Dev/DS Hcono'", ":", "DS_Hc", "-", "PE", "[", "'dev Hc'", "]", ",", "\n", "'Homogeneity Diff Dev/CS Hdeno'", ":", "CS_Hd", "-", "PE", "[", "'dev Hd'", "]", ",", "\n", "'Homogeneity Diff Dev/CS Hcono'", ":", "CS_Hc", "-", "PE", "[", "'dev Hc'", "]", ",", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "DS_Hd", ",", "DS_Hc", ",", "CS_Hd", ",", "CS_Hc", "=", "model", ".", "homogeneity", "(", "self", ".", "data", ".", "test_ids", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Homogeneity Diff Test/DS Hdeno'", ":", "DS_Hd", "-", "PE", "[", "'test Hd'", "]", ",", "\n", "'Homogeneity Diff Test/DS Hcono'", ":", "DS_Hc", "-", "PE", "[", "'test Hc'", "]", ",", "\n", "'Homogeneity Diff Test/CS Hdeno'", ":", "CS_Hd", "-", "PE", "[", "'test Hd'", "]", ",", "\n", "'Homogeneity Diff Test/CS Hcono'", ":", "CS_Hc", "-", "PE", "[", "'test Hc'", "]", ",", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "DS_Hd", ",", "DS_Hc", ",", "CS_Hd", ",", "CS_Hc", "=", "model", ".", "homogeneity", "(", "self", ".", "data", ".", "rand_ids", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Homogeneity Diff Random/DS Hdeno'", ":", "DS_Hd", "-", "PE", "[", "'rand Hd'", "]", ",", "\n", "'Homogeneity Diff Random/DS Hcono'", ":", "DS_Hc", "-", "PE", "[", "'rand Hc'", "]", ",", "\n", "'Homogeneity Diff Random/CS Hdeno'", ":", "CS_Hd", "-", "PE", "[", "'rand Hd'", "]", ",", "\n", "'Homogeneity Diff Random/CS Hcono'", ":", "CS_Hc", "-", "PE", "[", "'rand Hc'", "]", ",", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "\n", "", "mean_delta", ",", "abs_rhos", "=", "word_sim", ".", "mean_delta", "(", "\n", "model", ".", "deno_space", ".", "decomposed", ".", "weight", ",", "model", ".", "pretrained_embed", ".", "weight", ",", "\n", "model", ".", "id_to_word", ",", "reduce", "=", "False", ")", "\n", "cos_sim", "=", "F", ".", "cosine_similarity", "(", "\n", "model", ".", "deno_space", ".", "decomposed", ".", "weight", ",", "model", ".", "pretrained_embed", ".", "weight", ")", ".", "mean", "(", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "# 'Denotation Decomposer/rho difference cf pretrained': mean_delta,", "\n", "'Denotation Decomposer/MTurk-771'", ":", "abs_rhos", "[", "0", "]", ",", "\n", "'Denotation Decomposer/cosine similarity'", ":", "cos_sim", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "\n", "mean_delta", ",", "abs_rhos", "=", "word_sim", ".", "mean_delta", "(", "\n", "model", ".", "cono_space", ".", "decomposed", ".", "weight", ",", "model", ".", "pretrained_embed", ".", "weight", ",", "\n", "model", ".", "id_to_word", ",", "reduce", "=", "False", ")", "\n", "cos_sim", "=", "F", ".", "cosine_similarity", "(", "\n", "model", ".", "cono_space", ".", "decomposed", ".", "weight", ",", "model", ".", "pretrained_embed", ".", "weight", ")", ".", "mean", "(", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "'Connotation Decomposer/MTurk-771'", ":", "abs_rhos", "[", "0", "]", ",", "\n", "'Connotation Decomposer/cosine similarity'", ":", "cos_sim", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# sample = torch.randint(", "\n", "#     D_model.decomposed.num_embeddings, size=(25_000,), device=self.device)", "\n", "            ", "sample", "=", "torch", ".", "arange", "(", "model", ".", "pretrained_embed", ".", "num_embeddings", ",", "device", "=", "self", ".", "device", ")", "\n", "# recomposed = model.deno_space.decomposed(sample) + model.cono_space.decomposed(sample)", "\n", "deno_vecs", "=", "model", ".", "deno_space", ".", "decomposed", "(", "sample", ")", "\n", "cono_vecs", "=", "model", ".", "cono_space", ".", "decomposed", "(", "sample", ")", "\n", "recomposed", "=", "model", ".", "recomposer", "(", "torch", ".", "cat", "(", "(", "deno_vecs", ",", "cono_vecs", ")", ",", "dim", "=", "-", "1", ")", ")", "\n", "\n", "", "mean_delta", ",", "abs_rhos", "=", "word_sim", ".", "mean_delta", "(", "\n", "recomposed", ",", "model", ".", "pretrained_embed", ".", "weight", ",", "\n", "model", ".", "id_to_word", ",", "reduce", "=", "False", ")", "\n", "cos_sim", "=", "F", ".", "cosine_similarity", "(", "recomposed", ",", "model", ".", "pretrained_embed", ".", "weight", ")", ".", "mean", "(", ")", "\n", "self", ".", "update_tensorboard", "(", "{", "\n", "# 'Recomposer/rho difference cf pretrained': mean_delta,", "\n", "'Recomposer/MTurk-771'", ":", "abs_rhos", "[", "0", "]", ",", "\n", "'Recomposer/cosine similarity'", ":", "cos_sim", "\n", "}", ",", "manual_step", "=", "epoch_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.train": [[665, 701], ["tqdm.tqdm.tqdm", "tqdm.tqdm.tqdm", "proxy_grounded.ProxyGroundedExperiment.auto_save", "proxy_grounded.ProxyGroundedExperiment.eval_step", "range", "range", "enumerate", "tqdm.tqdm.tqdm", "proxy_grounded.ProxyGroundedExperiment.train_step", "proxy_grounded.ProxyGroundedExperiment.print_timestamp", "enumerate"], "methods", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.auto_save", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.eval_step", "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.train_step", "home.repos.pwc.inspect_result.awebson_congressional_adversary.utils.experiment.Experiment.print_timestamp"], ["", "def", "train", "(", "self", ")", "->", "None", ":", "\n", "        ", "config", "=", "self", ".", "config", "\n", "# # For debugging", "\n", "# self.save_everything(self.config.output_dir / 'init_recomposer.pt')", "\n", "# raise SystemExit", "\n", "\n", "if", "not", "config", ".", "print_stats", ":", "\n", "            ", "epoch_pbar", "=", "tqdm", "(", "range", "(", "1", ",", "config", ".", "num_epochs", "+", "1", ")", ",", "desc", "=", "config", ".", "output_dir", ".", "name", ")", "\n", "", "else", ":", "\n", "            ", "epoch_pbar", "=", "tqdm", "(", "range", "(", "1", ",", "config", ".", "num_epochs", "+", "1", ")", ",", "desc", "=", "'Epochs'", ")", "\n", "\n", "# estimated_save = self.data.estimated_len // 10", "\n", "\n", "", "for", "epoch_index", "in", "epoch_pbar", ":", "\n", "            ", "if", "not", "config", ".", "print_stats", ":", "\n", "                ", "batches", "=", "enumerate", "(", "self", ".", "dataloader", ")", "\n", "", "else", ":", "\n", "                ", "batches", "=", "tqdm", "(", "\n", "enumerate", "(", "self", ".", "dataloader", ")", ",", "\n", "total", "=", "self", ".", "data", ".", "estimated_len", ",", "\n", "mininterval", "=", "config", ".", "progress_bar_refresh_rate", ",", "\n", "desc", "=", "'Batches'", ")", "\n", "\n", "", "for", "batch_index", ",", "batch", "in", "batches", ":", "\n", "                ", "self", ".", "train_step", "(", "batch_index", ",", "batch", ")", "\n", "self", ".", "tb_global_step", "+=", "1", "\n", "# # For very long epochs", "\n", "# if batch_index % estimated_save == 0:", "\n", "#     point = batch_index // estimated_save", "\n", "#     self.save_everything(", "\n", "#         self.config.output_dir / f'epoch{epoch_index}_{point}.pt')", "\n", "", "self", ".", "auto_save", "(", "epoch_index", ")", "\n", "self", ".", "eval_step", "(", "epoch_index", ")", "\n", "self", ".", "data", ".", "estimated_len", "=", "batch_index", "\n", "if", "config", ".", "print_stats", ":", "\n", "                ", "self", ".", "print_timestamp", "(", "epoch_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedConfig.__post_init__": [[777, 845], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "len", "torch.nn.Linear", "set", "torch.nn.Sequential", "proxy_grounded.ProxyGroundedConfig.numericalize_cono.values", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Sequential", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Sequential", "ValueError", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.Linear"], "methods", ["None"], ["def", "__post_init__", "(", "self", ")", "->", "None", ":", "\n", "        ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-i'", ",", "'--input-dir'", ",", "action", "=", "'store'", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-o'", ",", "'--output-dir'", ",", "action", "=", "'store'", ",", "type", "=", "Path", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-d'", ",", "'--device'", ",", "action", "=", "'store'", ",", "type", "=", "str", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'-a'", ",", "'--architecture'", ",", "action", "=", "'store'", ",", "type", "=", "str", ")", "\n", "# parser.add_argument(", "\n", "#     '-dd', '--deno-delta', action='store', type=float)", "\n", "# parser.add_argument(", "\n", "#     '-dg', '--deno-gamma', action='store', type=float)", "\n", "# parser.add_argument(", "\n", "#     '-cd', '--cono-delta', action='store', type=float)", "\n", "# parser.add_argument(", "\n", "#     '-cg', '--cono-gamma', action='store', type=float)", "\n", "parser", ".", "add_argument", "(", "\n", "'-ns'", ",", "'--num-negative-samples'", ",", "action", "=", "'store'", ",", "type", "=", "int", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\n", "'-lr'", ",", "'--learning-rate'", ",", "action", "=", "'store'", ",", "type", "=", "float", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-bs'", ",", "'--batch-size'", ",", "action", "=", "'store'", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-ep'", ",", "'--num-epochs'", ",", "action", "=", "'store'", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\n", "'-sv'", ",", "'--auto-save-per-epoch'", ",", "action", "=", "'store'", ",", "type", "=", "int", ")", "\n", "parser", ".", "parse_args", "(", "namespace", "=", "self", ")", "\n", "\n", "assert", "self", ".", "num_cono_classes", "==", "len", "(", "set", "(", "self", ".", "numericalize_cono", ".", "values", "(", ")", ")", ")", "\n", "\n", "if", "self", ".", "architecture", "==", "'linear'", ":", "\n", "            ", "self", ".", "cono_probe", "=", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", "\n", "", "elif", "self", ".", "architecture", "==", "'MLP1'", ":", "\n", "            ", "self", ".", "cono_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", ")", "\n", "", "elif", "self", ".", "architecture", "==", "'MLP2'", ":", "\n", "            ", "self", ".", "cono_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ")", "\n", "", "elif", "self", ".", "architecture", "==", "'MLP4'", ":", "\n", "            ", "self", ".", "cono_probe", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Dropout", "(", "p", "=", "self", ".", "dropout_p", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "300", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "300", ",", "self", ".", "num_cono_classes", ")", ")", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "'Unknown architecture argument.'", ")", "\n", "\n", "", "assert", "self", ".", "cono_probe", "[", "-", "1", "]", ".", "out_features", "==", "self", ".", "num_cono_classes", "\n", "\n"]], "home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.main": [[847, 852], ["proxy_grounded.ProxyGroundedConfig", "proxy_grounded.ProxyGroundedExperiment", "auto_save_wrapped.train"], "function", ["home.repos.pwc.inspect_result.awebson_congressional_adversary.models.proxy_grounded.ProxyGroundedExperiment.train"], ["", "", "def", "main", "(", ")", "->", "None", ":", "\n", "    ", "config", "=", "ProxyGroundedConfig", "(", ")", "\n", "black_box", "=", "ProxyGroundedExperiment", "(", "config", ")", "\n", "with", "black_box", "as", "auto_save_wrapped", ":", "\n", "        ", "auto_save_wrapped", ".", "train", "(", ")", "\n", "\n"]]}