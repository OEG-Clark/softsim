{"home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.main.calc_class_weight": [[15, 24], ["max", "len", "enumerate"], "function", ["None"], ["def", "calc_class_weight", "(", "examples", ",", "label_list", ")", ":", "\n", "    ", "class_weight", "=", "[", "0", "]", "*", "len", "(", "label_list", ")", "\n", "label2id", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "for", "example", "in", "examples", ":", "\n", "        ", "label_id", "=", "label2id", "[", "example", ".", "label", "]", "\n", "class_weight", "[", "label_id", "]", "+=", "1", "\n", "", "max_w", "=", "max", "(", "class_weight", ")", "\n", "class_weight", "=", "[", "max_w", "/", "w", "for", "w", "in", "class_weight", "]", "\n", "return", "class_weight", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.main.load_model": [[26, 42], ["model_utils.BertClassifier", "torch.load", "model_utils.SentenceBertClassifier.load_state_dict", "model_utils.SentenceBertClassifier.bert.parameters", "model_utils.SentenceBertClassifier"], "function", ["None"], ["", "def", "load_model", "(", "args", ")", ":", "\n", "    ", "if", "args", ".", "model_type", "==", "'bert'", ":", "\n", "        ", "model", "=", "BertClassifier", "(", "\n", "bert_name_or_dir", "=", "args", ".", "bert_name_or_dir", ",", "num_labels", "=", "3", ",", "max_seq_len", "=", "args", ".", "max_seq_len", ")", "\n", "", "elif", "args", ".", "model_type", "==", "'sbert'", ":", "\n", "        ", "model", "=", "SentenceBertClassifier", "(", "\n", "bert_name_or_dir", "=", "args", ".", "bert_name_or_dir", ",", "num_labels", "=", "3", ",", "max_seq_len", "=", "args", ".", "max_sent_len", ")", "\n", "\n", "", "if", "args", ".", "weight_file", "is", "not", "None", ":", "\n", "        ", "state_dict", "=", "torch", ".", "load", "(", "args", ".", "weight_file", ",", "map_location", "=", "args", ".", "device", ")", "\n", "model", ".", "load_state_dict", "(", "state_dict", ",", "strict", "=", "False", ")", "\n", "\n", "", "if", "args", ".", "freeze_bert", ":", "\n", "        ", "for", "param", "in", "model", ".", "bert", ".", "parameters", "(", ")", ":", "\n", "            ", "param", ".", "requires_grad", "=", "False", "\n", "", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.main.load_label_lst": [[43, 52], ["open", "line.strip.strip", "label_lst.append"], "function", ["None"], ["", "def", "load_label_lst", "(", "file", ")", ":", "\n", "    ", "label_lst", "=", "[", "]", "\n", "with", "open", "(", "file", ",", "mode", "=", "'r'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "line", "==", "''", ":", "\n", "                ", "continue", "\n", "", "label_lst", ".", "append", "(", "line", ")", "\n", "", "", "return", "label_lst", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.main.set_seed": [[54, 58], ["random.seed", "numpy.random.seed", "torch.manual_seed"], "function", ["None"], ["", "def", "set_seed", "(", "seed", ")", ":", "\n", "    ", "random", ".", "seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.model_utils.SentenceBertClassifier.__init__": [[12, 20], ["torch.nn.Module.__init__", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModel.from_pretrained", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.STDEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert_name_or_dir", ",", "num_labels", ",", "max_seq_len", ")", ":", "\n", "        ", "super", "(", "SentenceBertClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "max_seq_len", "=", "max_seq_len", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "bert_name_or_dir", ")", "\n", "self", ".", "bert", "=", "AutoModel", ".", "from_pretrained", "(", "bert_name_or_dir", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "3", "*", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.model_utils.SentenceBertClassifier.encode": [[21, 39], ["model_utils.SentenceBertClassifier.tokenizer.batch_encode_plus", "model_utils.SentenceBertClassifier.bert", "next", "v.to", "attention_mask.unsqueeze", "masked_output.sum", "attention_mask.sum().unsqueeze", "model_utils.SentenceBertClassifier.parameters", "model_utils.SentenceBertClassifier.items", "attention_mask.sum"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "sentences", ")", ":", "\n", "        ", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", "\n", "model_inputs", "=", "self", ".", "tokenizer", ".", "batch_encode_plus", "(", "\n", "sentences", ",", "\n", "max_length", "=", "self", ".", "max_seq_len", ",", "\n", "padding", "=", "'max_length'", ",", "\n", "truncation", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_tensors", "=", "'pt'", "\n", ")", "\n", "model_inputs", "=", "{", "k", ":", "v", ".", "to", "(", "device", ")", "for", "k", ",", "v", "in", "model_inputs", ".", "items", "(", ")", "}", "\n", "input_ids", "=", "model_inputs", "[", "'input_ids'", "]", "\n", "attention_mask", "=", "model_inputs", "[", "'attention_mask'", "]", "\n", "outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "masked_output", "=", "outputs", "[", "0", "]", "*", "attention_mask", ".", "unsqueeze", "(", "-", "1", ")", "\n", "pooled_output", "=", "masked_output", ".", "sum", "(", "1", ")", "/", "attention_mask", ".", "sum", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# pooled_output = outputs[1]", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.model_utils.SentenceBertClassifier.forward": [[40, 48], ["model_utils.SentenceBertClassifier.encode", "model_utils.SentenceBertClassifier.encode", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "model_utils.SentenceBertClassifier.classifier"], "methods", ["home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.model_utils.BertClassifier.encode", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.model_utils.BertClassifier.encode"], ["", "def", "forward", "(", "self", ",", "sentences_1", ",", "sentences_2", ")", ":", "\n", "        ", "u", "=", "self", ".", "encode", "(", "sentences_1", ")", "\n", "v", "=", "self", ".", "encode", "(", "sentences_2", ")", "\n", "abs_diff", "=", "(", "u", "-", "v", ")", ".", "abs", "(", ")", "\n", "features", "=", "torch", ".", "cat", "(", "(", "u", ",", "v", ",", "abs_diff", ")", ",", "dim", "=", "-", "1", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "features", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.model_utils.BertClassifier.__init__": [[52, 61], ["torch.nn.Module.__init__", "transformers.AutoTokenizer.from_pretrained", "transformers.AutoModel.from_pretrained", "torch.nn.Dropout", "torch.nn.Dropout", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.STDEvaluator.__init__"], ["    ", "def", "__init__", "(", "self", ",", "bert_name_or_dir", ",", "num_labels", ",", "max_seq_len", ")", ":", "\n", "        ", "super", "(", "BertClassifier", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "max_seq_len", "=", "max_seq_len", "\n", "self", ".", "tokenizer", "=", "AutoTokenizer", ".", "from_pretrained", "(", "bert_name_or_dir", ")", "\n", "self", ".", "bert", "=", "AutoModel", ".", "from_pretrained", "(", "bert_name_or_dir", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "self", ".", "bert", ".", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "self", ".", "bert", ".", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.model_utils.BertClassifier.encode": [[62, 82], ["model_utils.BertClassifier.tokenizer.batch_encode_plus", "model_utils.BertClassifier.bert", "next", "v.to", "model_utils.BertClassifier.parameters", "model_utils.BertClassifier.items"], "methods", ["None"], ["", "def", "encode", "(", "self", ",", "sentence_pairs", ")", ":", "\n", "        ", "device", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "device", "\n", "model_inputs", "=", "self", ".", "tokenizer", ".", "batch_encode_plus", "(", "\n", "sentence_pairs", ",", "\n", "max_length", "=", "self", ".", "max_seq_len", ",", "\n", "padding", "=", "'max_length'", ",", "\n", "truncation", "=", "True", ",", "\n", "return_token_type_ids", "=", "True", ",", "\n", "return_attention_mask", "=", "True", ",", "\n", "return_tensors", "=", "'pt'", "\n", ")", "\n", "model_inputs", "=", "{", "k", ":", "v", ".", "to", "(", "device", ")", "for", "k", ",", "v", "in", "model_inputs", ".", "items", "(", ")", "}", "\n", "input_ids", "=", "model_inputs", "[", "'input_ids'", "]", "\n", "token_type_ids", "=", "model_inputs", "[", "'token_type_ids'", "]", "\n", "attention_mask", "=", "model_inputs", "[", "'attention_mask'", "]", "\n", "\n", "outputs", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", "=", "token_type_ids", ",", "attention_mask", "=", "attention_mask", ")", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.model_utils.BertClassifier.forward": [[83, 89], ["list", "model_utils.BertClassifier.encode", "model_utils.BertClassifier.dropout", "model_utils.BertClassifier.classifier", "zip"], "methods", ["home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.model_utils.BertClassifier.encode"], ["", "def", "forward", "(", "self", ",", "sentences_1", ",", "sentences_2", ")", ":", "\n", "        ", "sentence_pairs", "=", "list", "(", "zip", "(", "sentences_1", ",", "sentences_2", ")", ")", "\n", "pooled_output", "=", "self", ".", "encode", "(", "sentence_pairs", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.data_utils.STDDataset.__init__": [[24, 32], ["len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "examples", ")", ":", "\n", "        ", "self", ".", "num", "=", "len", "(", "examples", ")", "\n", "self", ".", "sentences_1", "=", "[", "example", ".", "sent1_text", "for", "example", "in", "examples", "]", "\n", "self", ".", "sentences_2", "=", "[", "example", ".", "sent2_text", "for", "example", "in", "examples", "]", "\n", "\n", "self", ".", "labels", "=", "None", "\n", "if", "examples", "[", "0", "]", ".", "label", "is", "not", "None", ":", "\n", "            ", "self", ".", "labels", "=", "[", "example", ".", "label", "for", "example", "in", "examples", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.data_utils.STDDataset.__len__": [[33, 35], ["None"], "methods", ["None"], ["", "", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "num", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.data_utils.STDDataset.__getitem__": [[36, 45], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "\n", "        ", "item", "=", "{", "\n", "'sentences_1'", ":", "self", ".", "sentences_1", "[", "idx", "]", ",", "\n", "'sentences_2'", ":", "self", ".", "sentences_2", "[", "idx", "]", ",", "\n", "}", "\n", "if", "self", ".", "labels", "is", "not", "None", ":", "\n", "            ", "item", "[", "'labels'", "]", "=", "self", ".", "labels", "[", "idx", "]", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.data_utils.read_examples_from_file": [[47, 87], ["logger.info", "open", "json.loads", "str", "text_preprocess.preprocess_bert", "str", "text_preprocess.preprocess_bert", "examples.append", "print", "data_utils.STDExample", "len"], "function", ["home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.preprocess_bert", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.preprocess_bert"], ["", "", "def", "read_examples_from_file", "(", "file", ",", "swap_order", "=", "False", ")", ":", "\n", "\n", "    ", "examples", "=", "[", "]", "\n", "\n", "with", "open", "(", "file", ",", "mode", "=", "'r'", ")", "as", "f", ":", "\n", "        ", "for", "line", "in", "f", ":", "\n", "            ", "line", "=", "json", ".", "loads", "(", "line", ")", "\n", "sent1_id", "=", "str", "(", "line", "[", "'sent1_id'", "]", ")", "\n", "sent1_orig", "=", "line", "[", "'sentence1'", "]", "\n", "sent1_text", "=", "preprocess_bert", "(", "sent1_orig", ",", "text_preprocess_args", ")", "\n", "# sent1_text = sent1_orig", "\n", "sent2_id", "=", "str", "(", "line", "[", "'sent2_id'", "]", ")", "\n", "sent2_orig", "=", "line", "[", "'sentence2'", "]", "\n", "sent2_text", "=", "preprocess_bert", "(", "sent2_orig", ",", "text_preprocess_args", ")", "\n", "# sent2_text = sent2_orig", "\n", "\n", "label", "=", "line", "[", "'label'", "]", "if", "'label'", "in", "line", "else", "None", "\n", "if", "label", "is", "None", ":", "\n", "                ", "print", "(", "1", ")", "\n", "", "source", "=", "line", "[", "'source'", "]", "if", "'source'", "in", "line", "else", "None", "\n", "is_reply", "=", "line", "[", "'is_reply'", "]", "if", "'is_reply'", "in", "line", "else", "None", "\n", "\n", "if", "swap_order", ":", "\n", "                ", "sent1_id", ",", "sent2_id", "=", "sent2_id", ",", "sent1_id", "\n", "sent1_orig", ",", "sent2_orig", "=", "sent2_orig", ",", "sent1_orig", "\n", "sent1_text", ",", "sent2_text", "=", "sent2_text", ",", "sent1_text", "\n", "\n", "", "examples", ".", "append", "(", "STDExample", "(", "\n", "sent1_id", "=", "sent1_id", ",", "\n", "sent1_orig", "=", "sent1_orig", ",", "\n", "sent1_text", "=", "sent1_text", ",", "\n", "sent2_id", "=", "sent2_id", ",", "\n", "sent2_orig", "=", "sent2_orig", ",", "\n", "sent2_text", "=", "sent2_text", ",", "\n", "label", "=", "label", ",", "\n", "source", "=", "source", ",", "\n", "is_reply", "=", "is_reply", ",", "\n", ")", ")", "\n", "", "", "logger", ".", "info", "(", "f\"{len(examples)} examples\"", ")", "\n", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.trainer.FocalLoss.__init__": [[17, 23], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.STDEvaluator.__init__"], ["def", "__init__", "(", "self", ",", "gamma", "=", "2", ",", "weight", "=", "None", ",", "reduction", "=", "'mean'", ",", "ignore_index", "=", "-", "100", ")", ":", "\n", "        ", "super", "(", "FocalLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "weight", "=", "weight", "\n", "self", ".", "ignore_index", "=", "ignore_index", "\n", "self", ".", "reduction", "=", "reduction", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.trainer.FocalLoss.forward": [[24, 34], ["torch.log_softmax", "torch.exp", "torch.nn.functional.nll_loss"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ",", "target", ")", ":", "\n", "        ", "\"\"\"\n        input: [N, C]\n        target: [N, ]\n        \"\"\"", "\n", "log_pt", "=", "torch", ".", "log_softmax", "(", "input", ",", "dim", "=", "1", ")", "\n", "pt", "=", "torch", ".", "exp", "(", "log_pt", ")", "\n", "log_pt", "=", "(", "1", "-", "pt", ")", "**", "self", ".", "gamma", "*", "log_pt", "\n", "loss", "=", "nn", ".", "functional", ".", "nll_loss", "(", "log_pt", ",", "target", ",", "self", ".", "weight", ",", "reduction", "=", "self", ".", "reduction", ",", "ignore_index", "=", "self", ".", "ignore_index", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.trainer.build_loss_fct": [[36, 44], ["torch.tensor().to", "torch.nn.CrossEntropyLoss", "trainer.FocalLoss", "torch.tensor"], "function", ["None"], ["", "", "def", "build_loss_fct", "(", "loss_type", ",", "class_weight", ",", "device", ")", ":", "\n", "    ", "if", "class_weight", "is", "not", "None", ":", "\n", "        ", "class_weight", "=", "torch", ".", "tensor", "(", "class_weight", ")", ".", "to", "(", "device", ")", "\n", "", "if", "loss_type", "==", "'ce'", ":", "\n", "        ", "loss_fct", "=", "nn", ".", "CrossEntropyLoss", "(", "weight", "=", "class_weight", ")", "\n", "", "elif", "loss_type", "==", "'focal'", ":", "\n", "        ", "loss_fct", "=", "FocalLoss", "(", "weight", "=", "class_weight", ")", "\n", "", "return", "loss_fct", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.trainer.build_optimizer_and_scheduler": [[46, 60], ["transformers.AdamW", "transformers.get_linear_schedule_with_warmup", "model.named_parameters", "model.named_parameters", "any", "any"], "function", ["None"], ["", "def", "build_optimizer_and_scheduler", "(", "args", ",", "model", ",", "t_total", ")", ":", "\n", "\n", "# Prepare optimizer and schedule (linear warmup and decay)", "\n", "    ", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay\"", ":", "args", ".", "weight_decay", "}", ",", "\n", "{", "\"params\"", ":", "[", "p", "for", "n", ",", "p", "in", "model", ".", "named_parameters", "(", ")", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "\"weight_decay\"", ":", "0.0", "}", ",", "\n", "]", "\n", "optimizer", "=", "AdamW", "(", "optimizer_grouped_parameters", ",", "lr", "=", "args", ".", "learning_rate", ",", "eps", "=", "args", ".", "adam_epsilon", ")", "\n", "scheduler", "=", "get_linear_schedule_with_warmup", "(", "\n", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "warmup_steps", ",", "num_training_steps", "=", "t_total", "\n", ")", "\n", "\n", "return", "optimizer", ",", "scheduler", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.trainer.train": [[61, 133], ["torch.utils.data.RandomSampler", "torch.utils.data.DataLoader", "dict", "trainer.build_loss_fct", "trainer.build_optimizer_and_scheduler", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "range", "math.ceil", "len", "logger.info", "model.train", "tqdm", "enumerate", "os.path.join", "torch.save", "logger.info", "torch.tensor().to", "model", "build_loss_fct.", "loss_fct.backward", "loss_fct.item", "tqdm.set_description", "str", "os.path.exists", "os.makedirs", "model.state_dict", "evaluator", "enumerate", "len", "int", "torch.nn.utils.clip_grad_norm_", "optimizer.step", "scheduler.step", "model.zero_grad", "len", "torch.tensor", "model.parameters", "loss_fct.item"], "function", ["home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.trainer.build_loss_fct", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.trainer.build_optimizer_and_scheduler", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.trainer.train"], ["", "def", "train", "(", "args", ",", "model", ",", "train_dataset", ",", "label_list", ",", "evaluator", "=", "None", ",", "class_weight", "=", "None", ")", ":", "\n", "\n", "    ", "train_sampler", "=", "RandomSampler", "(", "train_dataset", ")", "\n", "train_loader", "=", "DataLoader", "(", "train_dataset", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "label2idx", "=", "dict", "(", "(", "label", ",", "i", ")", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", ")", "\n", "\n", "if", "args", ".", "warmup_ratio", "is", "not", "None", ":", "\n", "        ", "args", ".", "warmup_steps", "=", "math", ".", "ceil", "(", "len", "(", "train_loader", ")", "*", "args", ".", "num_train_epochs", "*", "args", ".", "warmup_ratio", ")", "\n", "\n", "", "t_total", "=", "len", "(", "train_loader", ")", "*", "args", ".", "num_train_epochs", "\n", "loss_fct", "=", "build_loss_fct", "(", "args", ".", "loss_type", ",", "class_weight", ",", "device", "=", "args", ".", "device", ")", "\n", "optimizer", ",", "scheduler", "=", "build_optimizer_and_scheduler", "(", "args", ",", "model", ",", "t_total", ")", "\n", "\n", "# Train!", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "f\"  Num Examples = {len(train_dataset)}\"", ")", "\n", "logger", ".", "info", "(", "f\"  Num Epochs = {args.num_train_epochs}\"", ")", "\n", "logger", ".", "info", "(", "f\"  Training batch size = {args.train_batch_size}\"", ")", "\n", "logger", ".", "info", "(", "f\"  Loss type = {args.loss_type}, weight = {class_weight}\"", ")", "\n", "logger", ".", "info", "(", "f\"  Total optimization steps = {t_total}\"", ")", "\n", "logger", ".", "info", "(", "f\"  seed = {args.seed}\"", ")", "\n", "logger", ".", "info", "(", "f\"  lr = {int(args.learning_rate/1e-5)}\"", ")", "\n", "\n", "global_steps", "=", "0", "\n", "total_loss", "=", "0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "num_train_epochs", ")", ":", "\n", "        ", "logger", ".", "info", "(", "f'Epoch: {epoch}'", ")", "\n", "model", ".", "train", "(", ")", "\n", "iterable", "=", "tqdm", "(", "train_loader", ")", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "iterable", ")", ":", "\n", "\n", "            ", "b_labels", "=", "batch", "[", "'labels'", "]", "\n", "b_label_ids", "=", "torch", ".", "tensor", "(", "[", "label2idx", "[", "label", "]", "for", "label", "in", "b_labels", "]", ")", ".", "to", "(", "args", ".", "device", ")", "\n", "del", "batch", "[", "'labels'", "]", "\n", "\n", "outputs", "=", "model", "(", "**", "batch", ")", "\n", "\n", "loss", "=", "loss_fct", "(", "outputs", ",", "b_label_ids", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "# Perform a backward pass to calculate gradients", "\n", "", "loss", ".", "backward", "(", ")", "\n", "\n", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "\n", "if", "(", "i", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "# Clip the norm of the gradients to 1.0 to prevent exploding gradients", "\n", "                ", "torch", ".", "nn", ".", "utils", ".", "clip_grad_norm_", "(", "model", ".", "parameters", "(", ")", ",", "1.0", ")", "\n", "\n", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "model", ".", "zero_grad", "(", ")", "\n", "\n", "", "global_steps", "+=", "1", "\n", "\n", "# if global_steps%args.checkpoint_save_steps==0:", "\n", "#     torch.save(model.state_dict(), f'{args.ckpt_dir}/{global_steps}.pt')", "\n", "\n", "iterable", ".", "set_description", "(", "f'Loss: {loss.item() : 0.4f}'", ")", "\n", "\n", "", "ckpt_path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "ckpt_dir", ",", "str", "(", "args", ".", "seed", ")", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "ckpt_path", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "ckpt_path", ")", "\n", "", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "f'{ckpt_path}/checkpoint_{epoch}.pt'", ")", "\n", "\n", "avg_train_loss", "=", "total_loss", "/", "global_steps", "\n", "logger", ".", "info", "(", "\"Average train loss: %.4f\"", "%", "(", "avg_train_loss", ")", ")", "\n", "\n", "if", "args", ".", "eval_during_training", ":", "\n", "            ", "evaluator", "(", "args", ",", "model", ",", "on", "=", "[", "'overall'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.build_spacy_model": [[15, 26], ["spacy.lang.en.English", "spacy.lang.en.English.create_pipe", "spacy.lang.en.English.add_pipe", "spacy.lang.en.English.add_pipe"], "function", ["None"], ["def", "build_spacy_model", "(", ")", ":", "\n", "    ", "def", "_avoid_sentence_boundary_on_hashtag", "(", "doc", ")", ":", "\n", "        ", "for", "token", "in", "doc", "[", ":", "-", "1", "]", ":", "\n", "            ", "if", "token", ".", "text", "==", "'#'", ":", "\n", "                ", "doc", "[", "token", ".", "i", "+", "1", "]", ".", "is_sent_start", "=", "False", "\n", "", "", "return", "doc", "\n", "", "nlp", "=", "English", "(", ")", "\n", "sentencizer", "=", "nlp", ".", "create_pipe", "(", "\"sentencizer\"", ")", "\n", "nlp", ".", "add_pipe", "(", "sentencizer", ")", "\n", "nlp", ".", "add_pipe", "(", "_avoid_sentence_boundary_on_hashtag", ")", "\n", "return", "nlp", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.preprocess_bert": [[37, 61], ["text_preprocess.standardize_text", "text_preprocess.replace_usernames", "text_preprocess.replace_urls", "text_preprocess.asciify_emojis", "text_preprocess.standardize_punctuation", "remove_accented_characters.lower", "text_preprocess.replace_multi_occurrences", "text_preprocess.replace_multi_occurrences", "text_preprocess.remove_unicode_symbols", "text_preprocess.remove_accented_characters"], "function", ["home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.standardize_text", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.replace_usernames", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.replace_urls", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.asciify_emojis", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.standardize_punctuation", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.replace_multi_occurrences", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.replace_multi_occurrences", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.remove_unicode_symbols", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.remove_accented_characters"], ["def", "preprocess_bert", "(", "text", ",", "args", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Preprocesses tweet for BERT\"\"\"", "\n", "# standardize", "\n", "text", "=", "standardize_text", "(", "text", ")", "\n", "# replace usernames/urls", "\n", "if", "args", ".", "replace_usernames", ":", "\n", "        ", "text", "=", "replace_usernames", "(", "text", ",", "filler", "=", "args", ".", "username_filler", ")", "\n", "", "if", "args", ".", "replace_urls", ":", "\n", "        ", "text", "=", "replace_urls", "(", "text", ",", "filler", "=", "args", ".", "url_filler", ")", "\n", "", "if", "args", ".", "asciify_emojis", ":", "\n", "        ", "text", "=", "asciify_emojis", "(", "text", ")", "\n", "", "if", "args", ".", "standardize_punctuation", ":", "\n", "        ", "text", "=", "standardize_punctuation", "(", "text", ")", "\n", "", "if", "do_lower_case", ":", "\n", "        ", "text", "=", "text", ".", "lower", "(", ")", "\n", "", "if", "args", ".", "replace_multiple_usernames", ":", "\n", "        ", "text", "=", "replace_multi_occurrences", "(", "text", ",", "args", ".", "username_filler", ")", "\n", "", "if", "args", ".", "replace_multiple_urls", ":", "\n", "        ", "text", "=", "replace_multi_occurrences", "(", "text", ",", "args", ".", "url_filler", ")", "\n", "", "if", "args", ".", "remove_unicode_symbols", ":", "\n", "        ", "text", "=", "remove_unicode_symbols", "(", "text", ")", "\n", "", "if", "args", ".", "remove_accented_characters", ":", "\n", "        ", "text", "=", "remove_accented_characters", "(", "text", ")", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.remove_accented_characters": [[62, 65], ["unidecode.unidecode"], "function", ["None"], ["", "def", "remove_accented_characters", "(", "text", ")", ":", "\n", "    ", "text", "=", "unidecode", ".", "unidecode", "(", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.remove_unicode_symbols": [[66, 69], ["unicodedata.category"], "function", ["None"], ["", "def", "remove_unicode_symbols", "(", "text", ")", ":", "\n", "    ", "text", "=", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "unicodedata", ".", "category", "(", "ch", ")", "[", "0", "]", "!=", "'So'", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.replace_multi_occurrences": [[70, 108], ["text.replace.replace", "re.finditer", "enumerate", "text.replace.count", "text.replace.split", "m.start", "indices.append", "len", "merge_list.append", "len", "len", "len"], "function", ["None"], ["", "def", "replace_multi_occurrences", "(", "text", ",", "filler", ")", ":", "\n", "    ", "\"\"\"Replaces multiple occurrences of filler with n filler\"\"\"", "\n", "# only run if we have multiple occurrences of filler", "\n", "if", "text", ".", "count", "(", "filler", ")", "<=", "1", ":", "\n", "        ", "return", "text", "\n", "# pad fillers with whitespace", "\n", "", "text", "=", "text", ".", "replace", "(", "f'{filler}'", ",", "f' {filler} '", ")", "\n", "# remove introduced duplicate whitespaces", "\n", "text", "=", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "# find indices of occurrences", "\n", "indices", "=", "[", "]", "\n", "for", "m", "in", "re", ".", "finditer", "(", "r'{}'", ".", "format", "(", "filler", ")", ",", "text", ")", ":", "\n", "        ", "index", "=", "m", ".", "start", "(", ")", "\n", "indices", ".", "append", "(", "index", ")", "\n", "# collect merge list", "\n", "", "merge_list", "=", "[", "]", "\n", "for", "i", ",", "index", "in", "enumerate", "(", "indices", ")", ":", "\n", "        ", "if", "i", ">", "0", "and", "index", "-", "old_index", "==", "len", "(", "filler", ")", "+", "1", ":", "\n", "# found two consecutive fillers", "\n", "            ", "if", "len", "(", "merge_list", ")", ">", "0", "and", "merge_list", "[", "-", "1", "]", "[", "1", "]", "==", "old_index", ":", "\n", "# extend previous item", "\n", "                ", "merge_list", "[", "-", "1", "]", "[", "1", "]", "=", "index", "\n", "merge_list", "[", "-", "1", "]", "[", "2", "]", "+=", "1", "\n", "", "else", ":", "\n", "# create new item", "\n", "                ", "merge_list", ".", "append", "(", "[", "old_index", ",", "index", ",", "2", "]", ")", "\n", "", "", "old_index", "=", "index", "\n", "# merge occurrences", "\n", "", "if", "len", "(", "merge_list", ")", ">", "0", ":", "\n", "        ", "new_text", "=", "''", "\n", "pos", "=", "0", "\n", "for", "(", "start", ",", "end", ",", "count", ")", "in", "merge_list", ":", "\n", "            ", "new_text", "+=", "text", "[", "pos", ":", "start", "]", "\n", "new_text", "+=", "f'{count} {filler}'", "\n", "pos", "=", "end", "+", "len", "(", "filler", ")", "\n", "", "new_text", "+=", "text", "[", "pos", ":", "]", "\n", "text", "=", "new_text", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.segment_sentences": [[109, 126], ["nlp", "len", "enumerate", "s.string.strip", "re.match", "len"], "function", ["None"], ["", "def", "segment_sentences", "(", "text", ",", "args", ")", ":", "\n", "    ", "\"\"\"Uses spacy to segment text into sentences. Sentences which only consist of a filler will be merged with previous or following sentences\"\"\"", "\n", "doc", "=", "nlp", "(", "text", ")", "\n", "regex_fillers", "=", "r'(^\\d {username}$)|^{username}$|(^\\d {url}$)|^{url}$'", ".", "format", "(", "username", "=", "args", ".", "username_filler", ",", "url", "=", "args", ".", "url_filler", ")", "\n", "num_tokens", "=", "len", "(", "doc", ")", "\n", "sentences", "=", "[", "s", ".", "string", ".", "strip", "(", ")", "for", "s", "in", "doc", ".", "sents", "]", "\n", "for", "i", ",", "sentence", "in", "enumerate", "(", "sentences", ")", ":", "\n", "        ", "if", "re", ".", "match", "(", "regex_fillers", ",", "sentence", ")", ":", "\n", "            ", "if", "i", "==", "0", "and", "len", "(", "sentences", ")", ">", "1", ":", "\n", "# prepend to next sentence", "\n", "                ", "sentences", "[", "i", "+", "1", "]", "=", "f'{sentence} {sentences[i+1]}'", "\n", "", "elif", "i", ">", "0", ":", "\n", "# add sentence to previous", "\n", "                ", "sentences", "[", "i", "-", "1", "]", "+=", "f' {sentence}'", "\n", "# remove current", "\n", "", "del", "sentences", "[", "i", "]", "\n", "", "", "return", "sentences", ",", "num_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.asciify_emojis": [[127, 134], ["emoji.demojize"], "function", ["None"], ["", "def", "asciify_emojis", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    Converts emojis into text aliases. E.g. \ud83d\udc4d becomes :thumbs_up:\n    For a full list of text aliases see: https://www.webfx.com/tools/emoji-cheat-sheet/\n    \"\"\"", "\n", "text", "=", "emoji", ".", "demojize", "(", "text", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.standardize_text": [[135, 155], ["html_parser.unescape", "re.sub.translate", "re.sub.replace", "re.sub", "re.sub.strip", "re.sub.split", "unicodedata.category"], "function", ["None"], ["", "def", "standardize_text", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    1) Escape HTML\n    2) Replaces some non-standard punctuation with standard versions. \n    3) Replace \\r, \\n and \\t with white spaces\n    4) Removes all other control characters and the NULL byte\n    5) Removes duplicate white spaces\n    \"\"\"", "\n", "# escape HTML symbols", "\n", "text", "=", "html_parser", ".", "unescape", "(", "text", ")", "\n", "# standardize punctuation", "\n", "text", "=", "text", ".", "translate", "(", "transl_table", ")", "\n", "text", "=", "text", ".", "replace", "(", "'\u2026'", ",", "'...'", ")", "\n", "# replace \\t, \\n and \\r characters by a whitespace", "\n", "text", "=", "re", ".", "sub", "(", "control_char_regex", ",", "' '", ",", "text", ")", "\n", "# remove all remaining control characters", "\n", "text", "=", "''", ".", "join", "(", "ch", "for", "ch", "in", "text", "if", "unicodedata", ".", "category", "(", "ch", ")", "[", "0", "]", "!=", "'C'", ")", "\n", "# replace multiple spaces with single space", "\n", "text", "=", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "return", "text", ".", "strip", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.standardize_punctuation": [[156, 158], ["unidecode.unidecode", "unicodedata.category"], "function", ["None"], ["", "def", "standardize_punctuation", "(", "text", ")", ":", "\n", "    ", "return", "''", ".", "join", "(", "[", "unidecode", ".", "unidecode", "(", "t", ")", "if", "unicodedata", ".", "category", "(", "t", ")", "[", "0", "]", "==", "'P'", "else", "t", "for", "t", "in", "text", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.replace_usernames": [[159, 168], ["text.replace.replace", "re.sub", "text.replace.replace", "text.replace.split"], "function", ["None"], ["", "def", "replace_usernames", "(", "text", ",", "filler", "=", "'user'", ")", ":", "\n", "# @<user> is a marker used internally. use filler instead", "\n", "    ", "text", "=", "text", ".", "replace", "(", "'@<user>'", ",", "f'{filler}'", ")", "\n", "# replace other user handles by filler", "\n", "text", "=", "re", ".", "sub", "(", "username_regex", ",", "filler", ",", "text", ")", "\n", "# add spaces between, and remove double spaces again", "\n", "text", "=", "text", ".", "replace", "(", "filler", ",", "f' {filler} '", ")", "\n", "text", "=", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess.replace_urls": [[169, 178], ["text.replace.replace", "re.sub", "text.replace.replace", "text.replace.split"], "function", ["None"], ["", "def", "replace_urls", "(", "text", ",", "filler", "=", "'url'", ")", ":", "\n", "# <url> is a marker used internally. use filler instead", "\n", "    ", "text", "=", "text", ".", "replace", "(", "'<url>'", ",", "filler", ")", "\n", "# replace other urls by filler", "\n", "text", "=", "re", ".", "sub", "(", "url_regex", ",", "filler", ",", "text", ")", "\n", "# add spaces between, and remove double spaces again", "\n", "text", "=", "text", ".", "replace", "(", "filler", ",", "f' {filler} '", ")", "\n", "text", "=", "' '", ".", "join", "(", "text", ".", "split", "(", ")", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.text_preprocess_args.dotdict.__getattr__": [[2, 4], ["None"], "methods", ["None"], ["  ", "def", "__getattr__", "(", "self", ",", "name", ")", ":", "\n", "      ", "return", "self", "[", "name", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.STDEvaluator.__init__": [[34, 40], ["enumerate", "enumerate"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "examples", ",", "batch_size", ",", "label_lst", ")", ":", "\n", "\n", "        ", "self", ".", "examples", "=", "examples", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "id2label", "=", "{", "i", ":", "label", "for", "i", ",", "label", "in", "enumerate", "(", "label_lst", ")", "}", "\n", "self", ".", "label2id", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_lst", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.STDEvaluator.compute_metrics": [[41, 65], ["numpy.array", "numpy.array", "sklearn.metrics.confusion_matrix", "print", "numpy.sum", "numpy.diag", "numpy.sum", "class_precision.tolist.tolist.tolist", "class_recall.tolist.tolist.tolist", "class_f1.tolist.tolist.tolist", "sum", "len"], "methods", ["None"], ["", "def", "compute_metrics", "(", "self", ",", "y_true", ",", "y_pred", ")", ":", "\n", "        ", "y_true", "=", "np", ".", "array", "(", "y_true", ")", "\n", "y_pred", "=", "np", ".", "array", "(", "y_pred", ")", "\n", "\n", "acc", "=", "sum", "(", "y_pred", "==", "y_true", ")", "/", "len", "(", "y_pred", ")", "\n", "\n", "confusion_matrix", "=", "metrics", ".", "confusion_matrix", "(", "y_true", ",", "y_pred", ")", "\n", "print", "(", "confusion_matrix", ")", "\n", "\n", "class_pred", "=", "np", ".", "sum", "(", "confusion_matrix", ",", "axis", "=", "0", ")", "\n", "true_pred", "=", "np", ".", "diag", "(", "confusion_matrix", ")", "\n", "class_true", "=", "np", ".", "sum", "(", "confusion_matrix", ",", "axis", "=", "1", ")", "\n", "\n", "class_precision", "=", "true_pred", "/", "class_pred", "\n", "class_recall", "=", "true_pred", "/", "class_true", "\n", "class_f1", "=", "2", "*", "(", "class_precision", "*", "class_recall", ")", "/", "(", "class_precision", "+", "class_recall", ")", "\n", "\n", "class_precision", "=", "class_precision", ".", "tolist", "(", ")", "\n", "class_recall", "=", "class_recall", ".", "tolist", "(", ")", "\n", "class_f1", "=", "class_f1", ".", "tolist", "(", ")", "\n", "\n", "scores", "=", "{", "'accuracy'", ":", "acc", ",", "'precision'", ":", "class_precision", ",", "'recall'", ":", "class_recall", ",", "'f1'", ":", "class_f1", "}", "\n", "\n", "return", "scores", "\n", "\n"]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.STDEvaluator.__call__": [[66, 143], ["data_utils.STDDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "tqdm.tqdm.tqdm", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.softmax().cpu().numpy", "torch.softmax().cpu().numpy", "numpy.argmax", "numpy.array", "collections.defaultdict", "collections.defaultdict.items", "model.eval", "torch.cat.append", "torch.cat.append", "print", "print", "print", "pandas.DataFrame", "pandas.DataFrame.to_csv", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.softmax().cpu", "torch.softmax().cpu", "evaluator.STDEvaluator.compute_metrics", "evaluator.custom_group", "custom_group.items", "print", "range", "print", "model", "model", "numpy.array", "numpy.array", "evaluator.STDEvaluator.compute_metrics", "score_dict[].append", "range", "len", "sum", "len", "sum", "len", "sum", "len", "torch.softmax", "torch.softmax", "len"], "methods", ["home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.STDEvaluator.compute_metrics", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.custom_group", "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.STDEvaluator.compute_metrics"], ["", "def", "__call__", "(", "self", ",", "args", ",", "model", ",", "output_file", "=", "None", ",", "on", "=", "[", "'overall'", ",", "'source'", "]", ")", ":", "\n", "\n", "        ", "dataset", "=", "STDDataset", "(", "self", ".", "examples", ")", "\n", "\n", "dataloader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "self", ".", "batch_size", ")", "\n", "\n", "all_logits", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "dataloader", ")", ":", "\n", "            ", "model", ".", "eval", "(", ")", "\n", "\n", "del", "batch", "[", "'labels'", "]", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "if", "args", ".", "only_tweet", ":", "\n", "                    ", "logits", "=", "model", "(", "**", "batch", ",", "only_tweet", "=", "True", ")", "\n", "", "else", ":", "\n", "                    ", "logits", "=", "model", "(", "**", "batch", ")", "\n", "", "", "all_logits", ".", "append", "(", "logits", ")", "\n", "\n", "", "all_logits", "=", "torch", ".", "cat", "(", "all_logits", ",", "dim", "=", "0", ")", "\n", "\n", "probs", "=", "F", ".", "softmax", "(", "all_logits", ",", "dim", "=", "-", "1", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "y_pred", "=", "np", ".", "argmax", "(", "probs", ",", "axis", "=", "-", "1", ")", "\n", "y_true", "=", "np", ".", "array", "(", "[", "self", ".", "label2id", "[", "example", ".", "label", "]", "for", "example", "in", "self", ".", "examples", "]", ")", "\n", "\n", "score_dict", "=", "defaultdict", "(", "list", ")", "\n", "for", "type_", "in", "on", ":", "\n", "            ", "if", "type_", "==", "'overall'", ":", "\n", "                ", "scores", "=", "self", ".", "compute_metrics", "(", "y_true", ",", "y_pred", ")", "\n", "scores", "[", "'type'", "]", "=", "'overall'", "\n", "score_dict", "[", "type_", "]", "=", "[", "scores", "]", "\n", "", "else", ":", "\n", "                ", "group2idx", "=", "custom_group", "(", "self", ".", "examples", ",", "by", "=", "type_", ")", "\n", "for", "g", ",", "gids", "in", "group2idx", ".", "items", "(", ")", ":", "\n", "                    ", "y_true_g", "=", "np", ".", "array", "(", "y_true", "[", "gids", "]", ")", "\n", "y_pred_g", "=", "np", ".", "array", "(", "y_pred", "[", "gids", "]", ")", "\n", "scores", "=", "self", ".", "compute_metrics", "(", "y_true_g", ",", "y_pred_g", ")", "\n", "scores", "[", "'type'", "]", "=", "g", "\n", "score_dict", "[", "type_", "]", ".", "append", "(", "scores", ")", "\n", "\n", "", "", "", "for", "key", ",", "scores_list", "in", "score_dict", ".", "items", "(", ")", ":", "\n", "            ", "metric_strings", "=", "[", "self", ".", "id2label", "[", "i", "]", "for", "i", "in", "range", "(", "len", "(", "self", ".", "id2label", ")", ")", "]", "\n", "metric_strings", "=", "[", "'acc'", "]", "+", "metric_strings", "+", "[", "'macro'", "]", "\n", "print", "(", "' & '", ".", "join", "(", "metric_strings", ")", ")", "\n", "\n", "print", "(", "f\"{key} results\"", ")", "\n", "for", "scores", "in", "scores_list", ":", "\n", "                ", "print", "(", "f\"Type: {scores['type']}\"", ")", "\n", "\n", "score_strings", "=", "[", "f\"{scores['accuracy']*100:.2f}\"", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "id2label", ")", ")", ":", "\n", "                    ", "score_strings", "+=", "[", "f\"{scores['precision'][i]*100:.2f}\"", ",", "f\"{scores['recall'][i]*100:.2f}\"", ",", "f\"{scores['f1'][i]*100:.2f}\"", "]", "\n", "\n", "", "macro_precision", "=", "sum", "(", "scores", "[", "'precision'", "]", ")", "/", "len", "(", "scores", "[", "'precision'", "]", ")", "\n", "macro_recall", "=", "sum", "(", "scores", "[", "'recall'", "]", ")", "/", "len", "(", "scores", "[", "'recall'", "]", ")", "\n", "macro_f1", "=", "sum", "(", "scores", "[", "'f1'", "]", ")", "/", "len", "(", "scores", "[", "'f1'", "]", ")", "\n", "score_strings", "+=", "[", "f\"{macro_precision*100:.2f}\"", ",", "f\"{macro_recall*100:.2f}\"", ",", "f\"{macro_f1*100:.2f}\"", "]", "\n", "print", "(", "' & '", ".", "join", "(", "score_strings", ")", ")", "\n", "", "print", "(", "'\\n'", ")", "\n", "\n", "\n", "", "if", "output_file", "is", "not", "None", ":", "\n", "\n", "            ", "pred_labels", "=", "[", "self", ".", "id2label", "[", "i", "]", "for", "i", "in", "y_pred", "]", "\n", "true_labels", "=", "[", "self", ".", "id2label", "[", "i", "]", "for", "i", "in", "y_true", "]", "\n", "\n", "output_df", "=", "pd", ".", "DataFrame", "(", "{", "\n", "'source'", ":", "[", "example", ".", "source", "for", "example", "in", "self", ".", "examples", "]", ",", "\n", "'is_reply'", ":", "[", "example", ".", "is_reply", "for", "example", "in", "self", ".", "examples", "]", ",", "\n", "'sent2_id'", ":", "[", "example", ".", "sent2_id", "for", "example", "in", "self", ".", "examples", "]", ",", "\n", "'sent2'", ":", "[", "example", ".", "sent2_orig", "for", "example", "in", "self", ".", "examples", "]", ",", "\n", "'sent1_id'", ":", "[", "example", ".", "sent1_id", "for", "example", "in", "self", ".", "examples", "]", ",", "\n", "'sent1'", ":", "[", "example", ".", "sent1_orig", "for", "example", "in", "self", ".", "examples", "]", ",", "\n", "'true'", ":", "true_labels", ",", "\n", "'pred'", ":", "pred_labels", ",", "\n", "}", ")", "\n", "output_df", ".", "to_csv", "(", "args", ".", "output_file", ",", "index", "=", "False", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.yanfangh_covid-rumor-stance.code.evaluator.custom_group": [[17, 30], ["collections.defaultdict", "enumerate", "group2idx[].append", "enumerate", "group2idx[].append", "group2idx[].append"], "function", ["None"], ["def", "custom_group", "(", "examples", ",", "by", ")", ":", "\n", "    ", "group2idx", "=", "defaultdict", "(", "list", ")", "\n", "if", "by", "==", "'source'", ":", "\n", "        ", "for", "i", ",", "example", "in", "enumerate", "(", "examples", ")", ":", "\n", "            ", "new_source", "=", "source_map", "[", "example", ".", "source", "]", "\n", "group2idx", "[", "new_source", "]", ".", "append", "(", "i", ")", "\n", "", "", "elif", "by", "==", "'reply'", ":", "\n", "        ", "for", "i", ",", "example", "in", "enumerate", "(", "examples", ")", ":", "\n", "            ", "if", "example", ".", "is_reply", ":", "\n", "                ", "group2idx", "[", "'reply'", "]", ".", "append", "(", "i", ")", "\n", "", "else", ":", "\n", "                ", "group2idx", "[", "'non_reply'", "]", ".", "append", "(", "i", ")", "\n", "", "", "", "return", "group2idx", "\n", "\n"]]}