{"home.repos.pwc.inspect_result.suamin_meddistant19.None.word2vec.CorpusIterator.__init__": [[24, 26], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "fname", ")", ":", "\n", "        ", "self", ".", "fname", "=", "fname", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.word2vec.CorpusIterator.__iter__": [[27, 36], ["open", "line.strip.strip.strip", "json.loads", "jsonl[].split"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "with", "open", "(", "self", ".", "fname", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "rf", ":", "\n", "            ", "for", "line", "in", "rf", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                    ", "continue", "\n", "", "jsonl", "=", "json", ".", "loads", "(", "line", ")", "\n", "sentence_tokens", "=", "jsonl", "[", "'text'", "]", ".", "split", "(", ")", "\n", "yield", "sentence_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.word2vec.train_and_dump_word2vec": [[38, 80], ["gensim.Word2Vec", "word2vec.CorpusIterator", "logger.info", "w2v.Word2Vec.build_vocab", "logger.info", "w2v.Word2Vec.train", "os.makedirs", "logger.info", "w2v.Word2Vec.save", "numpy.zeros", "numpy.random.randn", "sorted", "logger.info", "numpy.save", "os.path.join", "wv.vocab.keys", "len", "pathlib.Path", "pathlib.Path", "open", "json.dump", "len", "len", "wv.vocab.keys"], "function", ["None"], ["", "", "", "", "def", "train_and_dump_word2vec", "(", "\n", "medline_entities_linked_fname", ",", "\n", "output_dir", ",", "\n", "n_workers", "=", "4", ",", "\n", "n_iter", "=", "3", "\n", ")", ":", "\n", "# fix embed dim = 50 and max vocab size to 50k", "\n", "    ", "model", "=", "w2v", ".", "Word2Vec", "(", "size", "=", "50", ",", "workers", "=", "n_workers", ",", "iter", "=", "n_iter", ",", "max_final_vocab", "=", "50000", ")", "\n", "sentences", "=", "CorpusIterator", "(", "medline_entities_linked_fname", ")", "\n", "\n", "logger", ".", "info", "(", "f'Building word2vec vocab on {medline_entities_linked_fname}...'", ")", "\n", "model", ".", "build_vocab", "(", "sentences", ")", "\n", "\n", "logger", ".", "info", "(", "'Training ...'", ")", "\n", "model", ".", "train", "(", "sentences", ",", "total_examples", "=", "model", ".", "corpus_count", ",", "epochs", "=", "model", ".", "iter", ")", "\n", "\n", "os", ".", "makedirs", "(", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "logger", ".", "info", "(", "'Saving word2vec model ...'", ")", "\n", "model", ".", "save", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "'word2vec.pubmed2019.50d.gz'", ")", ")", "\n", "\n", "wv", "=", "model", ".", "wv", "\n", "del", "model", "# free up memory", "\n", "\n", "word2id", "=", "{", "\"<PAD>\"", ":", "0", ",", "\"<UNK>\"", ":", "1", "}", "\n", "mat", "=", "np", ".", "zeros", "(", "(", "len", "(", "wv", ".", "vocab", ".", "keys", "(", ")", ")", "+", "2", ",", "50", ")", ")", "\n", "# initialize UNK embedding with random normal", "\n", "mat", "[", "1", "]", "=", "np", ".", "random", ".", "randn", "(", "50", ")", "\n", "\n", "for", "word", "in", "sorted", "(", "wv", ".", "vocab", ".", "keys", "(", ")", ")", ":", "\n", "        ", "vocab_item", "=", "wv", ".", "vocab", "[", "word", "]", "\n", "vector", "=", "wv", ".", "vectors", "[", "vocab_item", ".", "index", "]", "\n", "mat", "[", "len", "(", "word2id", ")", "]", "=", "vector", "\n", "word2id", "[", "word", "]", "=", "len", "(", "word2id", ")", "\n", "\n", "", "mat_fname", "=", "Path", "(", "output_dir", ")", "/", "f'word2vec.pubmed2019.50d_mat.npy'", "\n", "map_fname", "=", "Path", "(", "output_dir", ")", "/", "f'word2vec.pubmed2019.50d_word2id.json'", "\n", "\n", "logger", ".", "info", "(", "f'Saving word2id at {map_fname} and numpy matrix at {mat_fname} ...'", ")", "\n", "\n", "np", ".", "save", "(", "mat_fname", ",", "mat", ")", "\n", "with", "open", "(", "map_fname", ",", "'w'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "wf", ":", "\n", "        ", "json", ".", "dump", "(", "word2id", ",", "wf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.__init__": [[32, 45], ["create_kb_aligned_text_corpora.Triples.read_triples", "create_kb_aligned_text_corpora.Triples.set_rel_to_pairs", "create_kb_aligned_text_corpora.Triples.set_pair_to_rels", "create_kb_aligned_text_corpora.Triples.counts", "create_kb_aligned_text_corpora.Triples.set_pair_to_type"], "methods", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.read_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.set_rel_to_pairs", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.set_pair_to_rels", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.counts", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.set_pair_to_type"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "fname", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "cui2sty_fname", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", "\n", ")", ":", "\n", "        ", "self", ".", "read_triples", "(", "fname", ")", "\n", "self", ".", "set_rel_to_pairs", "(", ")", "\n", "self", ".", "set_pair_to_rels", "(", ")", "\n", "\n", "if", "cui2sty_fname", ":", "\n", "            ", "self", ".", "set_pair_to_type", "(", "cui2sty_fname", ")", "\n", "\n", "", "self", ".", "counts", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.read_triples": [[46, 60], ["list", "set", "set", "open", "tqdm.tqdm.tqdm", "line.strip.strip.strip", "line.strip.strip.split", "create_kb_aligned_text_corpora.Triples.triples.append", "create_kb_aligned_text_corpora.Triples.entities.update", "create_kb_aligned_text_corpora.Triples.relations.add"], "methods", ["None"], ["", "def", "read_triples", "(", "self", ",", "fname", ")", ":", "\n", "        ", "self", ".", "triples", ":", "List", "[", "str", ",", "str", ",", "str", "]", "=", "list", "(", ")", "\n", "self", ".", "entities", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "self", ".", "relations", ":", "Set", "[", "str", "]", "=", "set", "(", ")", "\n", "\n", "with", "open", "(", "fname", ")", "as", "rf", ":", "\n", "            ", "for", "line", "in", "tqdm", "(", "rf", ",", "desc", "=", "'Reading raw triples ...'", ")", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                    ", "continue", "\n", "", "h", ",", "r", ",", "t", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "self", ".", "triples", ".", "append", "(", "(", "h", ",", "r", ",", "t", ")", ")", "\n", "self", ".", "entities", ".", "update", "(", "[", "h", ",", "t", "]", ")", "\n", "self", ".", "relations", ".", "add", "(", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.set_rel_to_pairs": [[61, 65], ["collections.defaultdict", "tqdm.tqdm.tqdm", "create_kb_aligned_text_corpora.Triples.rel2pairs[].append"], "methods", ["None"], ["", "", "", "def", "set_rel_to_pairs", "(", "self", ")", ":", "\n", "        ", "self", ".", "rel2pairs", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "h", ",", "r", ",", "t", "in", "tqdm", "(", "self", ".", "triples", ",", "desc", "=", "' --- Intializing relation -> pairs map ...'", ")", ":", "\n", "            ", "self", ".", "rel2pairs", "[", "r", "]", ".", "append", "(", "(", "h", ",", "t", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.set_pair_to_rels": [[66, 70], ["collections.defaultdict", "tqdm.tqdm.tqdm", "create_kb_aligned_text_corpora.Triples.pair2rels[].append"], "methods", ["None"], ["", "", "def", "set_pair_to_rels", "(", "self", ")", ":", "\n", "        ", "self", ".", "pair2rels", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "h", ",", "r", ",", "t", "in", "tqdm", "(", "self", ".", "triples", ",", "desc", "=", "' --- Intializing pair -> relations map ...'", ")", ":", "\n", "            ", "self", ".", "pair2rels", "[", "(", "h", ",", "t", ")", "]", ".", "append", "(", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.set_pair_to_type": [[71, 77], ["dict", "tqdm.tqdm.tqdm", "open", "json.load"], "methods", ["None"], ["", "", "def", "set_pair_to_type", "(", "self", ",", "cui2sty_fname", ")", ":", "\n", "        ", "with", "open", "(", "cui2sty_fname", ")", "as", "rf", ":", "\n", "            ", "cui2sty", "=", "json", ".", "load", "(", "rf", ")", "\n", "", "self", ".", "pair2type", "=", "dict", "(", ")", "\n", "for", "h", ",", "r", ",", "t", "in", "tqdm", "(", "self", ".", "triples", ",", "desc", "=", "' --- Intializing pair -> type map ...'", ")", ":", "\n", "            ", "self", ".", "pair2type", "[", "(", "h", ",", "t", ")", "]", "=", "(", "cui2sty", "[", "h", "]", ",", "cui2sty", "[", "t", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.counts": [[78, 89], ["collections.Counter", "collections.Counter", "collections.Counter", "tqdm.tqdm.tqdm", "logger.info", "logger.info", "logger.info", "create_kb_aligned_text_corpora.Triples.ent_counts.update", "create_kb_aligned_text_corpora.Triples.pair_counts.update", "create_kb_aligned_text_corpora.Triples.rel_counts.update", "create_kb_aligned_text_corpora.Triples.ent_counts.most_common", "create_kb_aligned_text_corpora.Triples.pair_counts.most_common", "create_kb_aligned_text_corpora.Triples.rel_counts.most_common"], "methods", ["None"], ["", "", "def", "counts", "(", "self", ")", ":", "\n", "        ", "self", ".", "ent_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "self", ".", "rel_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "self", ".", "pair_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "for", "h", ",", "r", ",", "t", "in", "tqdm", "(", "self", ".", "triples", ",", "desc", "=", "' --- Counting stats ...'", ")", ":", "\n", "            ", "self", ".", "ent_counts", ".", "update", "(", "[", "h", ",", "t", "]", ")", "\n", "self", ".", "pair_counts", ".", "update", "(", "[", "(", "h", ",", "t", ")", "]", ")", "\n", "self", ".", "rel_counts", ".", "update", "(", "[", "r", "]", ")", "\n", "", "logger", ".", "info", "(", "f'Top-10 frequent entities: {self.ent_counts.most_common(10)}'", ")", "\n", "logger", ".", "info", "(", "f'Top-10 frequent arg pairs: {self.pair_counts.most_common(10)}'", ")", "\n", "logger", ".", "info", "(", "f'Top-10 frequent relations: {self.rel_counts.most_common(10)}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.__getitem__": [[90, 92], ["None"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "triples", "[", "idx", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.Triples.__len__": [[93, 95], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "triples", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.__init__": [[99, 135], ["dict", "set", "pathlib.Path", "open", "json.load", "create_kb_aligned_text_corpora.Triples", "create_kb_aligned_text_corpora.BioDSRECorpus.entities.update", "os.path.split", "os.path.join", "os.path.join", "pathlib.Path"], "methods", ["None"], ["    ", "def", "__init__", "(", "\n", "self", ",", "\n", "medline_entities_linked_fname", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "snomed_triples_dir", ":", "Union", "[", "str", ",", "Path", "]", ",", "\n", "split", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "has_def", ":", "Optional", "[", "bool", "]", "=", "False", "\n", ")", ":", "\n", "        ", "self", ".", "medline_entities_linked_fname", "=", "medline_entities_linked_fname", "\n", "self", ".", "snomed_triples_dir", "=", "snomed_triples_dir", "\n", "\n", "# map from split to its triples", "\n", "self", ".", "triples", ":", "Dict", "[", "str", ",", "List", "[", "str", ",", "str", ",", "str", "]", "]", "=", "dict", "(", ")", "\n", "# entities across all splits", "\n", "self", ".", "entities", "=", "set", "(", ")", "\n", "\n", "# identify inductive or transductive split", "\n", "self", ".", "split", "=", "(", "split", "+", "(", "'_def'", "if", "has_def", "else", "''", ")", "+", "'-'", ")", "if", "split", "==", "'ind'", "else", "''", "\n", "\n", "# type information is required to prune the negative pairs", "\n", "cui2sty_fname", "=", "Path", "(", "self", ".", "snomed_triples_dir", ")", "/", "'cui2sty.json'", "\n", "with", "open", "(", "cui2sty_fname", ")", "as", "rf", ":", "\n", "            ", "self", ".", "cui2sty", "=", "json", ".", "load", "(", "rf", ")", "\n", "\n", "", "for", "data_split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "            ", "fname", "=", "Path", "(", "self", ".", "snomed_triples_dir", ")", "/", "f'{self.split}{data_split}.tsv'", "\n", "self", ".", "triples", "[", "data_split", "]", "=", "Triples", "(", "fname", ",", "cui2sty_fname", ")", "\n", "self", ".", "entities", ".", "update", "(", "self", ".", "triples", "[", "data_split", "]", ".", "entities", ")", "\n", "\n", "", "base_dir", "=", "os", ".", "path", ".", "split", "(", "medline_entities_linked_fname", ")", "[", "0", "]", "\n", "self", ".", "pos_fname", "=", "lambda", "split", ":", "os", ".", "path", ".", "join", "(", "\n", "base_dir", ",", "f'{self.split}{split}_pos_idxs_linked.tsv'", "\n", ")", "\n", "self", ".", "neg_fname", "=", "lambda", "split", ":", "os", ".", "path", ".", "join", "(", "\n", "base_dir", ",", "f'{self.split}{split}_neg_idxs_linked.tsv'", "\n", ")", "\n", "self", ".", "base_dir", "=", "base_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.iter_entities_linked_file": [[136, 160], ["open", "tqdm.tqdm.tqdm", "json.loads.strip", "json.loads", "len"], "methods", ["None"], ["", "def", "iter_entities_linked_file", "(", "self", ")", ":", "\n", "        ", "with", "open", "(", "self", ".", "medline_entities_linked_fname", ")", "as", "rf", ":", "\n", "            ", "idx", "=", "0", "\n", "for", "jsonl", "in", "tqdm", "(", "rf", ",", "'Reading entities linked file ...'", ")", ":", "\n", "                ", "jsonl", "=", "jsonl", ".", "strip", "(", ")", "\n", "if", "not", "jsonl", ":", "\n", "                    ", "continue", "\n", "", "jsonl", "=", "json", ".", "loads", "(", "jsonl", ")", "\n", "text", "=", "jsonl", "[", "\"text\"", "]", "\n", "# transform mention into OpenNRE mention format", "\n", "# NB. m[1] is the score of entity linking, we keep", "\n", "# it for entity linking error analysis otherwise it", "\n", "# can be ignored", "\n", "jsonl", "[", "\"mentions\"", "]", "=", "[", "\n", "{", "\n", "\"id\"", ":", "m", "[", "0", "]", ",", "\"pos\"", ":", "m", "[", "2", "]", ",", "\"name\"", ":", "text", "[", "m", "[", "2", "]", "[", "0", "]", ":", "m", "[", "2", "]", "[", "1", "]", "]", "\n", "}", "for", "m", "in", "jsonl", "[", "\"mentions\"", "]", "\n", "]", "\n", "# keep mentions if at least 3 character length only", "\n", "jsonl", "[", "\"mentions\"", "]", "=", "[", "m", "for", "m", "in", "jsonl", "[", "\"mentions\"", "]", "if", "len", "(", "m", "[", "\"name\"", "]", ")", ">=", "3", "]", "\n", "if", "not", "jsonl", "[", "\"mentions\"", "]", ":", "\n", "                    ", "continue", "\n", "", "yield", "idx", ",", "jsonl", "\n", "idx", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.search_pos_and_neg_instances": [[161, 513], ["dict", "dict", "set", "set", "set", "set", "set", "set", "zip", "sorted", "dict", "create_kb_aligned_text_corpora.BioDSRECorpus.iter_entities_linked_file", "create_kb_aligned_text_corpora.BioDSRECorpus.triples[].pair2rels.keys", "create_kb_aligned_text_corpora.BioDSRECorpus.triples[].pair2type.values", "create_kb_aligned_text_corpora.BioDSRECorpus.triples[].pair2rels.keys", "create_kb_aligned_text_corpora.BioDSRECorpus.triples[].pair2type.values", "create_kb_aligned_text_corpora.BioDSRECorpus.triples[].pair2rels.keys", "create_kb_aligned_text_corpora.BioDSRECorpus.triples[].pair2type.values", "set", "set", "sorted", "sorted", "list", "tqdm.tqdm.tqdm", "logger.info", "collections.Counter", "set", "set.intersection", "set", "set.intersection", "set.intersection", "set.intersection", "list", "list", "list", "list", "list", "list", "list", "list", "set", "set", "set", "sorted", "random.choice", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "collections.Counter.keys", "itertools.permutations", "set.intersection", "set.intersection", "set.intersection", "list", "neg_pairs[].update", "neg_pairs[].update", "list", "list", "sum", "len", "set", "set", "random.choice", "neg_idxs[].append", "random.choice", "pos_idxs[].append", "random.choices", "random.choices", "random.choices", "random.choices", "len", "open", "create_kb_aligned_text_corpora.save_items", "open", "create_kb_aligned_text_corpora.save_items", "collections.Counter.values", "set.add", "set.add", "sorted", "random.choice", "neg_idxs[].append", "sorted", "random.choice", "pos_idxs[].append", "create_kb_aligned_text_corpora.BioDSRECorpus.pos_fname", "create_kb_aligned_text_corpora.BioDSRECorpus.neg_fname", "list", "sorted", "random.choice", "neg_idxs[].append", "list", "sorted", "random.choice", "pos_idxs[].append", "list", "sorted", "list", "sorted", "list", "list"], "methods", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.iter_entities_linked_file", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.save_items", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.save_items"], ["", "", "", "def", "search_pos_and_neg_instances", "(", "\n", "self", ",", "\n", "raw_neg_sample_size", ":", "int", "=", "500", ",", "\n", "corrupt_arg", ":", "bool", "=", "True", ",", "\n", "remove_multimentions_sents", ":", "bool", "=", "True", ",", "\n", "use_type_constraint", ":", "bool", "=", "True", ",", "\n", "use_arg_constraint", ":", "bool", "=", "True", "\n", ")", ":", "\n", "        ", "\"\"\"Read positive pairs from triples and search for them in size-2\n        permutations of entities linked file, with mentions field. These\n        pairs are created using the CUIs and matched against SNOMED-CT\n        pairs' CUIs.\n        \n        \"\"\"", "\n", "# list of positive instances containing the fact pair with CUIs", "\n", "# e.g. pos_idxs['train'] = [(128, ('C0205253', 'C0014653')), (.., ..), ...]", "\n", "pos_idxs", ":", "Dict", "[", "str", ",", "List", "[", "Tuple", "[", "int", ",", "Tuple", "[", "str", ",", "str", "]", "]", "]", "]", "=", "dict", "(", "\n", "train", "=", "list", "(", ")", ",", "dev", "=", "list", "(", ")", ",", "test", "=", "list", "(", ")", "\n", ")", "\n", "# similarly, list of all negatives", "\n", "neg_idxs", ":", "Dict", "[", "str", ",", "List", "[", "Tuple", "[", "int", ",", "Tuple", "[", "str", ",", "str", "]", "]", "]", "]", "=", "dict", "(", "\n", "train", "=", "list", "(", ")", ",", "dev", "=", "list", "(", ")", ",", "test", "=", "list", "(", ")", "\n", ")", "\n", "\n", "# ----------------------------------------", "\n", "# **TRAIN** pairs, inverse pairs and types ", "\n", "# ----------------------------------------", "\n", "train_pairs", "=", "set", "(", "self", ".", "triples", "[", "'train'", "]", ".", "pair2rels", ".", "keys", "(", ")", ")", "\n", "train_pairs_inv", "=", "{", "(", "t", ",", "h", ")", "for", "h", ",", "t", "in", "train_pairs", "}", "\n", "train_pairs_types", "=", "set", "(", "self", ".", "triples", "[", "'train'", "]", ".", "pair2type", ".", "values", "(", ")", ")", "\n", "\n", "# ----------------------------------------", "\n", "# **DEV** pairs, inverse pairs and types ", "\n", "# ----------------------------------------", "\n", "dev_pairs", "=", "set", "(", "self", ".", "triples", "[", "'dev'", "]", ".", "pair2rels", ".", "keys", "(", ")", ")", "\n", "dev_pairs_inv", "=", "{", "(", "t", ",", "h", ")", "for", "h", ",", "t", "in", "dev_pairs", "}", "\n", "dev_pairs_types", "=", "set", "(", "self", ".", "triples", "[", "'dev'", "]", ".", "pair2type", ".", "values", "(", ")", ")", "\n", "\n", "# ----------------------------------------", "\n", "# **TEST** pairs, inverse pairs and types ", "\n", "# ----------------------------------------", "\n", "test_pairs", "=", "set", "(", "self", ".", "triples", "[", "'test'", "]", ".", "pair2rels", ".", "keys", "(", ")", ")", "\n", "test_pairs_inv", "=", "{", "(", "t", ",", "h", ")", "for", "h", ",", "t", "in", "test_pairs", "}", "\n", "test_pairs_types", "=", "set", "(", "self", ".", "triples", "[", "'test'", "]", ".", "pair2type", ".", "values", "(", ")", ")", "\n", "\n", "#-----------------------------------------------------------------", "\n", "\n", "pairs", "=", "train_pairs", "|", "dev_pairs", "|", "test_pairs", "\n", "heads", ",", "tails", "=", "zip", "(", "*", "pairs", ")", "\n", "heads", ",", "tails", "=", "set", "(", "heads", ")", ",", "set", "(", "tails", ")", "\n", "heads_list", ",", "tails_list", "=", "sorted", "(", "list", "(", "heads", ")", ")", ",", "sorted", "(", "list", "(", "tails", ")", ")", "\n", "\n", "# combine all inverse pairs and types pairs", "\n", "inv", "=", "train_pairs_inv", "|", "dev_pairs_inv", "|", "test_pairs_inv", "\n", "types", "=", "train_pairs_types", "|", "dev_pairs_types", "|", "test_pairs_types", "\n", "\n", "entities", "=", "sorted", "(", "list", "(", "self", ".", "entities", ")", ")", "\n", "\n", "################################################################", "\n", "#", "\n", "# 1. COLLECT RAW NEGATIVE SAMPLES FROM +ve GROUPS", "\n", "#", "\n", "################################################################", "\n", "# similar to usual corruption mechanism in KGC, we remove head", "\n", "# or tail entity with probability 0.5 and sample from all entities;", "\n", "# depending on the number of entities we sample for generating", "\n", "# negatives, the positive to negative ratio will change.", "\n", "# Too less negatives will result in lesser (more constrained)", "\n", "# negative examples, i.e., for the NA relation type. These are", "\n", "# raw candidate negative pairs but not the final set, as we", "\n", "# further prune based on the positive semantic type pairs and", "\n", "# check if the resulting heads appeared in any of the heads", "\n", "# across all the splits and same for the tails.", "\n", "\n", "neg_pairs", "=", "dict", "(", "train", "=", "set", "(", ")", ",", "dev", "=", "set", "(", ")", ",", "test", "=", "set", "(", ")", ")", "\n", "n_samples", "=", "raw_neg_sample_size", "\n", "\n", "for", "split", ",", "split_pairs", "in", "(", "\n", "(", "'train'", ",", "train_pairs", ")", ",", "\n", "(", "'dev'", ",", "dev_pairs", ")", ",", "\n", "(", "'test'", ",", "test_pairs", ")", "\n", ")", ":", "\n", "            ", "for", "h", ",", "t", "in", "tqdm", "(", "\n", "sorted", "(", "list", "(", "split_pairs", ")", ")", ",", "\n", "desc", "=", "f'Creating candidate negative triples from positives for {split} ...'", "\n", ")", ":", "\n", "# choose left or right side to corrupt", "\n", "                ", "h_or_t", "=", "random", ".", "choice", "(", "[", "0", ",", "1", "]", ")", "\n", "\n", "# corrupt the **TAIL**", "\n", "if", "h_or_t", ":", "\n", "                    ", "if", "corrupt_arg", ":", "\n", "                        ", "neg_tails", "=", "random", ".", "choices", "(", "tails_list", ",", "k", "=", "n_samples", ")", "\n", "", "else", ":", "\n", "                        ", "neg_tails", "=", "random", ".", "choices", "(", "entities", ",", "k", "=", "n_samples", ")", "\n", "", "neg_pairs", "[", "split", "]", ".", "update", "(", "{", "(", "h", ",", "t_neg", ")", "for", "t_neg", "in", "neg_tails", "}", ")", "\n", "# corrupt the **HEAD**", "\n", "", "else", ":", "\n", "                    ", "if", "corrupt_arg", ":", "\n", "                        ", "neg_heads", "=", "random", ".", "choices", "(", "heads_list", ",", "k", "=", "n_samples", ")", "\n", "", "else", ":", "\n", "                        ", "neg_heads", "=", "random", ".", "choices", "(", "entities", ",", "k", "=", "n_samples", ")", "\n", "", "neg_pairs", "[", "split", "]", ".", "update", "(", "{", "(", "h_neg", ",", "t", ")", "for", "h_neg", "in", "neg_heads", "}", ")", "\n", "\n", "# remove positive groups if created during negative sampling and inverses as well", "\n", "", "", "neg_pairs", "[", "split", "]", "=", "(", "neg_pairs", "[", "split", "]", "-", "pairs", ")", "-", "inv", "\n", "logger", ".", "info", "(", "f'collected {len(neg_pairs[split])} number of negative samples for {split}'", ")", "\n", "\n", "", "ntr", ",", "nval", ",", "nte", "=", "0", ",", "0", ",", "0", "\n", "nntr", ",", "nnval", ",", "nnte", "=", "0", ",", "0", ",", "0", "\n", "\n", "for", "idx", ",", "jsonl", "in", "self", ".", "iter_entities_linked_file", "(", ")", ":", "\n", "\n", "            ", "if", "idx", "%", "1000000", "==", "0", "and", "idx", ">", "0", ":", "\n", "\n", "                ", "ntr", "+=", "len", "(", "pos_idxs", "[", "'train'", "]", ")", "\n", "nval", "+=", "len", "(", "pos_idxs", "[", "'dev'", "]", ")", "\n", "nte", "+=", "len", "(", "pos_idxs", "[", "'test'", "]", ")", "\n", "\n", "nntr", "+=", "len", "(", "neg_idxs", "[", "'train'", "]", ")", "\n", "nnval", "+=", "len", "(", "neg_idxs", "[", "'dev'", "]", ")", "\n", "nnte", "+=", "len", "(", "neg_idxs", "[", "'test'", "]", ")", "\n", "\n", "ptr", "=", "(", "nntr", "/", "(", "nntr", "+", "ntr", ")", ")", "*", "100", "\n", "pval", "=", "(", "nnval", "/", "(", "nnval", "+", "nval", ")", ")", "*", "100", "\n", "pte", "=", "(", "nnte", "/", "(", "nnte", "+", "nte", ")", ")", "*", "100", "\n", "logger", ".", "info", "(", "\n", "f'[Progress @ {idx}] -- POSITIVE # train {ntr} / # dev {nval} / # test {nte}'", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "f'[Progress @ {idx}] -- NA # train {nntr} / # dev {nnval} / # test {nnte}'", "\n", ")", "\n", "logger", ".", "info", "(", "\n", "f'[Progress @ {idx}] -- NA (%) # train {ptr:.2f} / # dev {pval:.2f} / # test {pte:.2f}'", "\n", ")", "\n", "\n", "# periodically dump the collected positive and negative samples", "\n", "# for each split to clear the memory; it grows fast :)", "\n", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "\n", "                    ", "with", "open", "(", "self", ".", "pos_fname", "(", "split", ")", ",", "'a'", ")", "as", "wf", ":", "\n", "                        ", "save_items", "(", "pos_idxs", "[", "split", "]", ",", "wf", ")", "\n", "\n", "", "with", "open", "(", "self", ".", "neg_fname", "(", "split", ")", ",", "'a'", ")", "as", "wf", ":", "\n", "                        ", "save_items", "(", "neg_idxs", "[", "split", "]", ",", "wf", ")", "\n", "\n", "# reset the split indices", "\n", "", "pos_idxs", "[", "split", "]", "=", "list", "(", ")", "\n", "neg_idxs", "[", "split", "]", "=", "list", "(", ")", "\n", "\n", "################################################################", "\n", "#", "\n", "# 2. REMOVE ENTITIES APPEARING MORE THAN ONCE (IF SET)", "\n", "#", "\n", "################################################################", "\n", "", "", "cui2count", "=", "collections", ".", "Counter", "(", "[", "item", "[", "'id'", "]", "for", "item", "in", "jsonl", "[", "'mentions'", "]", "]", ")", "\n", "# check if any entity is present more than once, drop this sentence", "\n", "# akin to: https://github.com/suamin/umls-medline-distant-re/blob/master/data_utils/link_entities.py#L45", "\n", "if", "remove_multimentions_sents", ":", "\n", "                ", "if", "sum", "(", "cui2count", ".", "values", "(", ")", ")", ">", "len", "(", "cui2count", ")", ":", "\n", "                    ", "continue", "\n", "", "", "cuis", "=", "set", "(", "cui2count", ".", "keys", "(", ")", ")", "\n", "\n", "################################################################", "\n", "#", "\n", "# 3. FIND OVERLAPPING ENTITES WITH THOSE FROM SNOMED TRIPLES", "\n", "#", "\n", "################################################################", "\n", "# NB. this intersection prunes out entities which are not covered in any of the splits", "\n", "snomed_cuis", "=", "cuis", ".", "intersection", "(", "self", ".", "entities", ")", "\n", "if", "not", "snomed_cuis", ":", "\n", "                ", "continue", "\n", "\n", "################################################################", "\n", "#", "\n", "# 4. CREATE SIZE 2 (PAIRS) PERMUTATIONS OF INTERSECTING ENTITIES", "\n", "#", "\n", "################################################################", "\n", "# permutations of size for concepts in a sentence that has SNOMED CUIs", "\n", "", "matching_snomed_permutations", "=", "set", "(", "itertools", ".", "permutations", "(", "snomed_cuis", ",", "2", ")", ")", "\n", "\n", "################################################################", "\n", "#", "\n", "# 5. LEAKAGE CHECK: REMOVE INVERSE GROUPS FROM THESE PAIRS", "\n", "#", "\n", "################################################################", "\n", "# first remove inverses, we are not modeling them", "\n", "matching_snomed_permutations", "=", "matching_snomed_permutations", "-", "inv", "\n", "\n", "if", "not", "matching_snomed_permutations", ":", "\n", "                ", "continue", "\n", "\n", "################################################################", "\n", "#", "\n", "# 6. CHECK INTERSECTING PAIRS in TRAIN, DEV, OR TEST", "\n", "#", "\n", "################################################################", "\n", "# check if we have the matching pairs in any of the splits", "\n", "", "pairs_in_train", "=", "matching_snomed_permutations", ".", "intersection", "(", "train_pairs", ")", "\n", "pairs_in_dev", "=", "matching_snomed_permutations", ".", "intersection", "(", "dev_pairs", ")", "\n", "pairs_in_test", "=", "matching_snomed_permutations", ".", "intersection", "(", "test_pairs", ")", "\n", "\n", "################################################################", "\n", "#", "\n", "# 7. LEAKAGE CHECK - NO MATCHING PAIRS ACROSS THE SPLITS", "\n", "#", "\n", "################################################################", "\n", "# unlikely, but we make sure that no pairs are seen across the split; even though", "\n", "# the triples creation process has taken care of it, this is to be on the safe-side", "\n", "try", ":", "\n", "                ", "assert", "not", "(", "pairs_in_train", "&", "pairs_in_dev", ")", "\n", "assert", "not", "(", "pairs_in_train", "&", "pairs_in_test", ")", "\n", "assert", "not", "(", "pairs_in_dev", "&", "pairs_in_test", ")", "\n", "", "except", "AssertionError", ":", "\n", "                ", "continue", "\n", "\n", "# gather the pairs that have been matched", "\n", "", "pruned_snomed_permutations", "=", "pairs_in_train", "|", "pairs_in_dev", "|", "pairs_in_test", "\n", "\n", "################################################################", "\n", "#", "\n", "# 8. NO MATCHING PAIRS; CANDIDATE FOR NEGATIVE (NA) CLASS", "\n", "#", "\n", "################################################################", "\n", "# we find no positive pairs across all the splits, this sentence", "\n", "# can be considered for not applicable (NA) relations", "\n", "if", "not", "pruned_snomed_permutations", ":", "\n", "\n", "################################################################", "\n", "#", "\n", "# 8a. APPLY PAIRS TYPE CONSTRAINT ON NEGATIVE GROUPS", "\n", "#", "\n", "################################################################", "\n", "                ", "if", "use_type_constraint", ":", "\n", "# we prune out pairs which do not respect the type constraint, i.e.,", "\n", "# the negative pair's type (TYPE_HEAD, TYPE_TAIL) must appear in", "\n", "# pairs' types from fact triples. Next, we check if such entities", "\n", "# appeared as head / tail **somewhere** across all the splits of facts.", "\n", "# this is regarded as argument-role constraint. This filters out", "\n", "# many of easy NA samples, which model can learn by simple heuristics.", "\n", "                    ", "temp", "=", "set", "(", ")", "\n", "for", "ent1", ",", "ent2", "in", "matching_snomed_permutations", ":", "\n", "                        ", "ent1t", "=", "self", ".", "cui2sty", "[", "ent1", "]", "\n", "ent2t", "=", "self", ".", "cui2sty", "[", "ent2", "]", "\n", "if", "(", "ent1t", ",", "ent2t", ")", "in", "types", ":", "\n", "                            ", "pair", "=", "(", "ent1", ",", "ent2", ")", "\n", "", "elif", "(", "ent2t", ",", "ent1t", ")", "in", "types", ":", "\n", "                            ", "pair", "=", "(", "ent2", ",", "ent1", ")", "\n", "", "else", ":", "\n", "                            ", "continue", "\n", "", "temp", ".", "add", "(", "pair", ")", "\n", "\n", "", "matching_snomed_permutations", "=", "temp", "\n", "\n", "if", "not", "matching_snomed_permutations", ":", "\n", "                        ", "continue", "\n", "\n", "################################################################", "\n", "#", "\n", "# 8b. APPLY PAIRS ARG ROLE CONSTRAINT ON NEGATIVE GROUPS", "\n", "#", "\n", "################################################################", "\n", "", "", "if", "use_arg_constraint", ":", "\n", "                    ", "temp", "=", "set", "(", ")", "\n", "for", "ent1", ",", "ent2", "in", "matching_snomed_permutations", ":", "\n", "                        ", "if", "ent1", "in", "heads", "and", "ent2", "in", "tails", ":", "\n", "                            ", "pair", "=", "(", "ent1", ",", "ent2", ")", "\n", "", "elif", "ent1", "in", "tails", "and", "ent2", "in", "heads", ":", "\n", "                            ", "pair", "=", "(", "ent2", ",", "ent1", ")", "\n", "", "else", ":", "\n", "                            ", "continue", "\n", "", "temp", ".", "add", "(", "pair", ")", "\n", "\n", "", "matching_snomed_permutations", "=", "temp", "\n", "\n", "if", "not", "matching_snomed_permutations", ":", "\n", "                        ", "continue", "\n", "\n", "################################################################", "\n", "#", "\n", "# 8c. FROM COLLECTED GROUPS MATCH WITH THE RAW ONES", "\n", "#", "\n", "################################################################", "\n", "# here, we consider the resultant pairs and see if they overlap", "\n", "# with our candidates generated by simple head / tail entity replacements", "\n", "", "", "candid_neg_pairs_in_test", "=", "matching_snomed_permutations", ".", "intersection", "(", "neg_pairs", "[", "'test'", "]", ")", "\n", "candid_neg_pairs_in_dev", "=", "matching_snomed_permutations", ".", "intersection", "(", "neg_pairs", "[", "'dev'", "]", ")", "\n", "candid_neg_pairs_in_train", "=", "matching_snomed_permutations", ".", "intersection", "(", "neg_pairs", "[", "'train'", "]", ")", "\n", "\n", "candid_neg_pairs", "=", "candid_neg_pairs_in_train", "|", "candid_neg_pairs_in_dev", "|", "candid_neg_pairs_in_test", "\n", "\n", "# after all the filtration steps we didn't find anything then we skip this sentence", "\n", "if", "not", "candid_neg_pairs", ":", "\n", "                    ", "continue", "\n", "\n", "################################################################", "\n", "#", "\n", "# 8d. LEAKAGE CHECK - NO MATCHING PAIRS IN NEGATIVE SPLITS", "\n", "#", "\n", "################################################################", "\n", "", "try", ":", "\n", "                    ", "assert", "not", "(", "candid_neg_pairs_in_train", "&", "candid_neg_pairs_in_dev", ")", "\n", "assert", "not", "(", "candid_neg_pairs_in_train", "&", "candid_neg_pairs_in_test", ")", "\n", "assert", "not", "(", "candid_neg_pairs_in_dev", "&", "candid_neg_pairs_in_test", ")", "\n", "", "except", "AssertionError", ":", "\n", "                    ", "continue", "\n", "\n", "# we might have multiple matches, so we only consider one pair to", "\n", "# associate with this sentence as negative to avoid duplicate sents in NA", "\n", "\n", "################################################################", "\n", "#", "\n", "# FINAL STEP WHICH COLLECTS THE **NEGATIVE** GROUPS", "\n", "#", "\n", "################################################################", "\n", "", "if", "candid_neg_pairs_in_test", ":", "\n", "                    ", "pair", "=", "random", ".", "choice", "(", "sorted", "(", "list", "(", "candid_neg_pairs_in_test", ")", ")", ")", "\n", "neg_idxs", "[", "'test'", "]", ".", "append", "(", "(", "idx", ",", "pair", ")", ")", "\n", "\n", "", "elif", "candid_neg_pairs_in_dev", ":", "\n", "                    ", "pair", "=", "random", ".", "choice", "(", "sorted", "(", "list", "(", "candid_neg_pairs_in_dev", ")", ")", ")", "\n", "neg_idxs", "[", "'dev'", "]", ".", "append", "(", "(", "idx", ",", "pair", ")", ")", "\n", "\n", "", "elif", "candid_neg_pairs_in_train", ":", "\n", "                    ", "pair", "=", "random", ".", "choice", "(", "sorted", "(", "list", "(", "candid_neg_pairs_in_train", ")", ")", ")", "\n", "neg_idxs", "[", "'train'", "]", ".", "append", "(", "(", "idx", ",", "pair", ")", ")", "\n", "\n", "", "", "else", ":", "\n", "\n", "################################################################", "\n", "#", "\n", "# FINAL STEP WHICH COLLECTS THE **POSITIVE** GROUPS", "\n", "#", "\n", "################################################################", "\n", "                ", "if", "pairs_in_test", ":", "\n", "                    ", "pair", "=", "random", ".", "choice", "(", "sorted", "(", "list", "(", "pairs_in_test", ")", ")", ")", "\n", "pos_idxs", "[", "'test'", "]", ".", "append", "(", "(", "idx", ",", "pair", ")", ")", "\n", "\n", "", "elif", "pairs_in_dev", ":", "\n", "                    ", "pair", "=", "random", ".", "choice", "(", "sorted", "(", "list", "(", "pairs_in_dev", ")", ")", ")", "\n", "pos_idxs", "[", "'dev'", "]", ".", "append", "(", "(", "idx", ",", "pair", ")", ")", "\n", "\n", "", "elif", "pairs_in_train", ":", "\n", "                    ", "pair", "=", "random", ".", "choice", "(", "sorted", "(", "list", "(", "pairs_in_train", ")", ")", ")", "\n", "pos_idxs", "[", "'train'", "]", ".", "append", "(", "(", "idx", ",", "pair", ")", ")", "\n", "\n", "", "", "h", ",", "t", "=", "pair", "\n", "# safety check", "\n", "assert", "h", "in", "cui2count", "\n", "assert", "t", "in", "cui2count", "\n", "\n", "", "return", "ntr", ",", "nval", ",", "nte", ",", "nntr", ",", "nnval", ",", "nnte", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.process_jsonl_with_pair": [[514, 563], ["collections.Counter", "list", "set", "list", "item[].lower", "list.append", "ent.canonical_name.lower", "map"], "methods", ["None"], ["", "def", "process_jsonl_with_pair", "(", "\n", "self", ",", "\n", "jsonl", ":", "str", ",", "\n", "pair", ":", "Tuple", "[", "str", ",", "str", "]", ",", "\n", "canonical_only", ":", "bool", "=", "False", ",", "\n", "kb", ":", "KnowledgeBase", "=", "None", "\n", ")", ":", "\n", "        ", "h", ",", "t", "=", "pair", "\n", "\n", "# look for multiple appearence of head / tail entities, discard such ambigious examples", "\n", "mention_ids", "=", "[", "item", "[", "'id'", "]", "for", "item", "in", "jsonl", "[", "'mentions'", "]", "]", "\n", "counts", "=", "collections", ".", "Counter", "(", "mention_ids", ")", "\n", "\n", "# make sure head and tail are present", "\n", "assert", "h", "in", "counts", "\n", "assert", "t", "in", "counts", "\n", "\n", "head_mention", ",", "tail_mention", "=", "None", ",", "None", "\n", "\n", "################################################################", "\n", "#", "\n", "# KEEP ONLY SENTENCES WITH SINGLE MENTION OF HEAD / TAIL", "\n", "#", "\n", "################################################################", "\n", "# since multiple mentions can cause ambiguity, we only pick instances of single h/t mention", "\n", "if", "counts", "[", "h", "]", "==", "1", "and", "counts", "[", "t", "]", "==", "1", ":", "\n", "            ", "other_mentions", "=", "list", "(", ")", "\n", "\n", "for", "item", "in", "jsonl", "[", "'mentions'", "]", ":", "\n", "                ", "if", "canonical_only", ":", "\n", "                    ", "ent", "=", "kb", ".", "cui_to_entity", "[", "item", "[", "'id'", "]", "]", "\n", "names", "=", "[", "ent", ".", "canonical_name", ".", "lower", "(", ")", ",", "]", "+", "list", "(", "map", "(", "str", ".", "lower", ",", "ent", ".", "aliases", ")", ")", "\n", "names", "=", "set", "(", "names", ")", "\n", "if", "item", "[", "'name'", "]", ".", "lower", "(", ")", "not", "in", "names", ":", "\n", "                        ", "continue", "\n", "", "", "if", "item", "[", "'id'", "]", "==", "h", ":", "\n", "                    ", "head_mention", "=", "item", "\n", "", "elif", "item", "[", "'id'", "]", "==", "t", ":", "\n", "                    ", "tail_mention", "=", "item", "\n", "", "else", ":", "\n", "                    ", "other_mentions", ".", "append", "(", "item", ")", "\n", "\n", "", "", "if", "head_mention", "==", "None", "or", "tail_mention", "==", "None", ":", "\n", "                ", "return", "None", "\n", "\n", "", "return", "head_mention", ",", "tail_mention", ",", "other_mentions", "\n", "\n", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.create_corpus": [[564, 864], ["dict", "dict", "set", "test_relations.intersection", "logger.info", "create_kb_aligned_text_corpora.read_idx_file", "collections.defaultdict", "read_idx_file.items", "logger.info", "logger.info", "logger.info", "list", "create_kb_aligned_text_corpora.read_idx_file", "collections.defaultdict", "read_idx_file.items", "logger.info", "logger.info", "logger.info", "list", "random.shuffle", "random.shuffle", "set", "set", "logger.info", "dict", "logger.info", "list", "create_kb_aligned_text_corpora.BioDSRECorpus.iter_entities_linked_file", "logger.info", "os.path.join", "logger.info", "random.shuffle", "collections.Counter", "os.path.join", "set.update", "dev_relations.intersection", "os.path.join", "create_kb_aligned_text_corpora.prune_by_rels", "logger.info", "logger.info", "scispacy.umls_linking.UmlsEntityLinker", "create_kb_aligned_text_corpora.BioDSRECorpus.pos_fname", "pair2pos_idxs[].append", "list", "read_idx_file.keys", "create_kb_aligned_text_corpora.BioDSRECorpus.neg_fname", "pair2neg_idxs[].append", "list", "read_idx_file.keys", "logger.info", "list", "list", "len", "len", "list.intersection", "open", "os.path.join", "os.path.join", "create_kb_aligned_text_corpora.remove_overlapping_pairs", "os.path.join", "create_kb_aligned_text_corpora.prune_by_type_mentions", "open", "list", "split2rels[].items", "split2rels[].items", "all", "split2rels[].items", "all", "collections.defaultdict.keys", "collections.defaultdict.keys", "random.sample", "len", "len", "int", "int", "len", "random.choice", "logger.info", "create_kb_aligned_text_corpora.BioDSRECorpus.process_jsonl_with_pair", "list.append", "wf.write", "json.loads.strip", "json.loads", "collections.Counter.update", "collections.Counter.keys", "len", "len", "len", "len", "len", "len", "int", "len", "random.choice", "enumerate", "create_kb_aligned_text_corpora.BioDSRECorpus.process_jsonl_with_pair", "list.append", "len", "list.append", "json.dumps", "len", "len"], "methods", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.read_idx_file", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.read_idx_file", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.iter_entities_linked_file", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.prune_by_rels", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.remove_overlapping_pairs", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.prune_by_type_mentions", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.process_jsonl_with_pair", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.process_jsonl_with_pair"], ["", "", "def", "create_corpus", "(", "\n", "self", ",", "\n", "train_size", ":", "float", "=", "0.7", ",", "\n", "dev_size", ":", "float", "=", "0.1", ",", "\n", "sample", ":", "float", "=", "1.0", ",", "\n", "use_sent_level_noise", ":", "bool", "=", "False", ",", "\n", "neg_prop", ":", "float", "=", "None", ",", "\n", "remove_mention_overlaps", ":", "bool", "=", "True", ",", "\n", "canonical_or_aliases_only", ":", "bool", "=", "True", ",", "\n", "prune_frequent_bags", ":", "bool", "=", "True", ",", "\n", "max_bag_size", ":", "int", "=", "500", ",", "\n", "prune_frequent_mentions", ":", "bool", "=", "True", ",", "\n", "max_mention_freq", ":", "int", "=", "1000", ",", "\n", "min_rel_freq", ":", "int", "=", "1", ",", "\n", "include_other_mentions", ":", "bool", "=", "True", ",", "\n", "dataset", ":", "str", "=", "'med_distant'", "\n", ")", ":", "\n", "        ", "\"\"\"Once positive and negative pairs have been read, we now create the corpora in\n        OpenNRE format with different sizes and proportion of negative samples.\n        \n        \"\"\"", "\n", "assert", "0", "<", "sample", "<=", "1.0", "\n", "if", "neg_prop", "is", "not", "None", ":", "\n", "            ", "assert", "0", "<", "neg_prop", "<=", "1.0", "\n", "\n", "", "counts", "=", "dict", "(", ")", "\n", "\n", "if", "canonical_or_aliases_only", ":", "\n", "            ", "kb", "=", "UmlsEntityLinker", "(", "name", "=", "'scispacy_linker'", ")", ".", "kb", "\n", "", "else", ":", "\n", "            ", "kb", "=", "None", "\n", "\n", "", "for", "split", "in", "[", "'test'", ",", "'dev'", ",", "'train'", "]", ":", "\n", "\n", "            ", "logger", ".", "info", "(", "f'Creating corpus for split {split} ...'", ")", "\n", "\n", "pos_idx2pair", "=", "read_idx_file", "(", "self", ".", "pos_fname", "(", "split", ")", ")", "\n", "\n", "pair2pos_idxs", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "pos_idx", ",", "pair", "in", "pos_idx2pair", ".", "items", "(", ")", ":", "\n", "                ", "pair2pos_idxs", "[", "pair", "]", ".", "append", "(", "pos_idx", ")", "\n", "\n", "", "logger", ".", "info", "(", "f'Found {len(pair2pos_idxs)} positive pairs in `{split}`'", ")", "\n", "logger", ".", "info", "(", "'Pruning noisy (high-frequency) positive pairs ...'", ")", "\n", "\n", "################################################################", "\n", "#", "\n", "# PRUNE HIGH-FREQUENCY POSITIVE (+ve) BAGS", "\n", "#", "\n", "################################################################", "\n", "if", "prune_frequent_bags", ":", "\n", "# remove highly-frequent (non-informative) pairs", "\n", "                ", "for", "pair", "in", "list", "(", "pair2pos_idxs", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "if", "len", "(", "pair2pos_idxs", "[", "pair", "]", ")", ">", "max_bag_size", ":", "\n", "                        ", "del", "pair2pos_idxs", "[", "pair", "]", "\n", "\n", "", "", "", "logger", ".", "info", "(", "f'Number of positive pairs after pruning = {len(pair2pos_idxs)}'", ")", "\n", "pos_idxs", "=", "list", "(", "pos_idx2pair", ".", "keys", "(", ")", ")", "\n", "\n", "neg_idx2pair", "=", "read_idx_file", "(", "self", ".", "neg_fname", "(", "split", ")", ")", "\n", "\n", "pair2neg_idxs", "=", "collections", ".", "defaultdict", "(", "list", ")", "\n", "for", "neg_idx", ",", "pair", "in", "neg_idx2pair", ".", "items", "(", ")", ":", "\n", "                ", "pair2neg_idxs", "[", "pair", "]", ".", "append", "(", "neg_idx", ")", "\n", "\n", "", "logger", ".", "info", "(", "f'Found {len(pair2neg_idxs)} negative pairs in `{split}`'", ")", "\n", "logger", ".", "info", "(", "'Pruning noisy (high-frequency) negative pairs ...'", ")", "\n", "\n", "################################################################", "\n", "#", "\n", "# PRUNE HIGH-FREQUENCY NEGATIVE (-ve) BAGS", "\n", "#", "\n", "################################################################", "\n", "if", "prune_frequent_bags", ":", "\n", "# remove highly-frequent (non-informative) pairs", "\n", "                ", "for", "pair", "in", "list", "(", "pair2neg_idxs", ".", "keys", "(", ")", ")", ":", "\n", "                    ", "if", "len", "(", "pair2neg_idxs", "[", "pair", "]", ")", ">", "max_bag_size", ":", "\n", "                        ", "del", "pair2neg_idxs", "[", "pair", "]", "\n", "\n", "", "", "", "logger", ".", "info", "(", "f'Number of negative pairs after pruning = {len(pair2neg_idxs)}'", ")", "\n", "neg_idxs", "=", "list", "(", "neg_idx2pair", ".", "keys", "(", ")", ")", "\n", "\n", "################################################################", "\n", "#", "\n", "# SUBSAMPLING POSITIVE SAMPLES", "\n", "#", "\n", "################################################################", "\n", "# subsample the positive proportion to len(pos) * sample", "\n", "if", "sample", "<", "1.0", ":", "\n", "                ", "logger", ".", "info", "(", "f'Subsampling positive idxs ...'", ")", "\n", "pos_idxs", "=", "list", "(", "random", ".", "sample", "(", "pos_idxs", ",", "int", "(", "len", "(", "pos_idxs", ")", "*", "sample", ")", ")", ")", "\n", "", "else", ":", "\n", "                ", "pos_idxs", "=", "list", "(", "pos_idxs", ")", "\n", "\n", "", "random", ".", "shuffle", "(", "pos_idxs", ")", "\n", "random", ".", "shuffle", "(", "neg_idxs", ")", "\n", "################################################################", "\n", "#", "\n", "# ADJUST +ve TO -ve's RATIO IF NEG PROP SPECIFIED", "\n", "#", "\n", "################################################################", "\n", "if", "neg_prop", "is", "not", "None", ":", "\n", "                ", "n_pos", ",", "n_neg", "=", "len", "(", "pos_idxs", ")", ",", "len", "(", "neg_idxs", ")", "\n", "# if positives sample size bigger than negatives", "\n", "if", "n_pos", ">", "n_neg", ":", "\n", "                    ", "m", "=", "int", "(", "n_neg", "/", "neg_prop", ")", "\n", "k_pos", "=", "m", "-", "n_neg", "\n", "pos_idxs", "=", "pos_idxs", "[", ":", "k_pos", "]", "\n", "# else we have more negatives than positives", "\n", "", "else", ":", "\n", "                    ", "m", "=", "int", "(", "n_pos", "/", "(", "1", "-", "neg_prop", ")", ")", "\n", "k_neg", "=", "m", "-", "n_pos", "\n", "neg_idxs", "=", "neg_idxs", "[", ":", "k_neg", "]", "\n", "\n", "", "", "n_pos", ",", "n_neg", "=", "len", "(", "pos_idxs", ")", ",", "len", "(", "neg_idxs", ")", "\n", "# for fast lookup", "\n", "pos_idxs", "=", "set", "(", "pos_idxs", ")", "\n", "neg_idxs", "=", "set", "(", "neg_idxs", ")", "\n", "\n", "################################################################", "\n", "#", "\n", "# CHECK THERE FOR OVERLAP BETWEEN POSITIVE AND NEGATIVE SAMPLES", "\n", "#", "\n", "################################################################", "\n", "assert", "not", "pos_idxs", ".", "intersection", "(", "neg_idxs", ")", "\n", "\n", "logger", ".", "info", "(", "f'number of positive {n_pos} and negative {n_neg} idxs'", ")", "\n", "triples", "=", "self", ".", "triples", "[", "split", "]", "\n", "\n", "# go through positive examples", "\n", "pair2rel", "=", "dict", "(", ")", "\n", "for", "pos_idx", "in", "pos_idxs", ":", "\n", "                ", "pair", "=", "pos_idx2pair", "[", "pos_idx", "]", "\n", "rels", "=", "triples", ".", "pair2rels", "[", "pair", "]", "\n", "# when labeled with multiple relations, consider a random one", "\n", "if", "len", "(", "rels", ")", ">", "1", ":", "\n", "                    ", "rel", "=", "random", ".", "choice", "(", "rels", ")", "\n", "", "else", ":", "\n", "                    ", "rel", "=", "rels", "[", "0", "]", "\n", "", "pair2rel", "[", "pair", "]", "=", "rel", "\n", "\n", "", "logger", ".", "info", "(", "f'number of facts = {len(pair2rel)}'", ")", "\n", "\n", "corpus", "=", "list", "(", ")", "\n", "n_pos", ",", "n_neg", "=", "0", ",", "0", "# fact instances count and NA instances count", "\n", "\n", "for", "idx", ",", "jsonl", "in", "self", ".", "iter_entities_linked_file", "(", ")", ":", "\n", "\n", "                ", "if", "idx", "%", "1000000", "==", "0", "and", "idx", ">", "0", ":", "\n", "                    ", "logger", ".", "info", "(", "\n", "f'[Progress @ {idx}] Collected {len(corpus)} lines with pos: {n_pos} and neg: {n_neg}'", "\n", ")", "\n", "\n", "", "if", "idx", "in", "pos_idxs", ":", "\n", "                    ", "pair", "=", "pos_idx2pair", "[", "idx", "]", "\n", "ret", "=", "self", ".", "process_jsonl_with_pair", "(", "jsonl", ",", "pair", ",", "canonical_or_aliases_only", ",", "kb", ")", "\n", "if", "ret", "is", "None", ":", "\n", "                        ", "continue", "\n", "", "head_mention", ",", "tail_mention", ",", "other_mentions", "=", "ret", "\n", "\n", "################################################################", "\n", "#", "\n", "# APPLY SENTENCE LEVEL NOISE (cf. Amin et al., 2020)", "\n", "#", "\n", "################################################################", "\n", "if", "use_sent_level_noise", ":", "\n", "# choose left or right side to corrupt", "\n", "                        ", "h_or_t", "=", "random", ".", "choice", "(", "[", "0", ",", "1", "]", ")", "\n", "has_neg", "=", "False", "\n", "\n", "for", "idx", ",", "m", "in", "enumerate", "(", "other_mentions", ")", ":", "\n", "# corrupt the tail", "\n", "                            ", "if", "h_or_t", ":", "\n", "                                ", "candid_neg_pair", "=", "head_mention", "[", "'id'", "]", ",", "m", "[", "'id'", "]", "\n", "tail_mention_noise", "=", "m", "\n", "head_mention_noise", "=", "None", "\n", "# corrupt the head", "\n", "", "else", ":", "\n", "                                ", "candid_neg_pair", "=", "m", "[", "'id'", "]", ",", "tail_mention", "[", "'id'", "]", "\n", "head_mention_noise", "=", "m", "\n", "tail_mention_noise", "=", "None", "\n", "\n", "", "if", "candid_neg_pair", "not", "in", "pair2neg_idxs", ":", "\n", "                                ", "continue", "\n", "", "else", ":", "\n", "                                ", "has_neg", "=", "True", "\n", "\n", "", "example", "=", "{", "\n", "'text'", ":", "jsonl", "[", "'text'", "]", ",", "\n", "'h'", ":", "head_mention", "if", "h_or_t", "else", "head_mention_noise", ",", "\n", "'t'", ":", "tail_mention", "if", "not", "h_or_t", "else", "tail_mention_noise", ",", "\n", "'relation'", ":", "'NA'", "\n", "}", "\n", "\n", "if", "include_other_mentions", ":", "\n", "                                ", "example", "[", "'o'", "]", "=", "other_mentions", "\n", "\n", "", "corpus", ".", "append", "(", "example", ")", "\n", "n_neg", "+=", "1", "\n", "\n", "", "if", "not", "has_neg", ":", "\n", "                            ", "continue", "\n", "\n", "", "", "rel", "=", "pair2rel", "[", "pair", "]", "\n", "example", "=", "{", "\n", "'text'", ":", "jsonl", "[", "'text'", "]", ",", "\n", "'h'", ":", "head_mention", ",", "\n", "'t'", ":", "tail_mention", ",", "\n", "'relation'", ":", "rel", "\n", "}", "\n", "\n", "if", "include_other_mentions", ":", "\n", "                        ", "example", "[", "'o'", "]", "=", "other_mentions", "\n", "\n", "", "corpus", ".", "append", "(", "example", ")", "\n", "n_pos", "+=", "1", "\n", "\n", "", "elif", "idx", "in", "neg_idxs", "and", "not", "use_sent_level_noise", ":", "\n", "                    ", "pair", "=", "neg_idx2pair", "[", "idx", "]", "\n", "ret", "=", "self", ".", "process_jsonl_with_pair", "(", "jsonl", ",", "pair", ",", "canonical_or_aliases_only", ",", "kb", ")", "\n", "if", "ret", "is", "None", ":", "\n", "                        ", "continue", "\n", "", "head_mention", ",", "tail_mention", ",", "other_mentions", "=", "ret", "\n", "example", "=", "{", "\n", "'text'", ":", "jsonl", "[", "'text'", "]", ",", "\n", "'h'", ":", "head_mention", ",", "\n", "'t'", ":", "tail_mention", ",", "\n", "'relation'", ":", "'NA'", "\n", "}", "\n", "\n", "if", "include_other_mentions", ":", "\n", "                        ", "example", "[", "'o'", "]", "=", "other_mentions", "\n", "\n", "", "corpus", ".", "append", "(", "example", ")", "\n", "n_neg", "+=", "1", "\n", "\n", "", "", "logger", ".", "info", "(", "f'Final = Collected {len(corpus)} lines with pos: {n_pos} and neg: {n_neg}'", ")", "\n", "output_fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "f'{self.split}{dataset}_{split}.txt'", ")", "\n", "logger", ".", "info", "(", "f'Saving the collected corpus to output file {output_fname} ...'", ")", "\n", "\n", "random", ".", "shuffle", "(", "corpus", ")", "\n", "\n", "with", "open", "(", "output_fname", ",", "'w'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "wf", ":", "\n", "                ", "for", "line", "in", "corpus", ":", "\n", "                    ", "wf", ".", "write", "(", "json", ".", "dumps", "(", "line", ")", "+", "'\\n'", ")", "\n", "\n", "", "", "counts", "[", "split", "]", "=", "{", "'npos'", ":", "n_pos", ",", "'nneg'", ":", "n_neg", "}", "\n", "\n", "", "if", "remove_mention_overlaps", ":", "\n", "            ", "for", "src", ",", "tgt", "in", "[", "(", "'train'", ",", "'dev'", ")", ",", "(", "'train'", ",", "'test'", ")", ",", "(", "'dev'", ",", "'test'", ")", "]", ":", "\n", "                ", "src_jsonl_fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "f'{self.split}{dataset}_{src}.txt'", ")", "\n", "tgt_jsonl_fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "f'{self.split}{dataset}_{tgt}.txt'", ")", "\n", "################################################################", "\n", "#", "\n", "# REMOVE MENTION LEVEL OVERLAPS (IF ANY)", "\n", "#", "\n", "################################################################", "\n", "remove_overlapping_pairs", "(", "src_jsonl_fname", ",", "tgt_jsonl_fname", ",", "tgt", ")", "\n", "\n", "", "", "if", "prune_frequent_mentions", ":", "\n", "            ", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "                ", "jsonl_fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "f'{self.split}{dataset}_{split}.txt'", ")", "\n", "prune_by_type_mentions", "(", "\n", "jsonl_fname", ",", "self", ".", "cui2sty", ",", "max_freq", "=", "max_mention_freq", "\n", ")", "\n", "\n", "# remove relations which have no sample in dev / test from train as well, use min rel freq as well", "\n", "", "", "split2rels", "=", "dict", "(", ")", "\n", "all_rels", "=", "set", "(", ")", "\n", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "            ", "rel2count", "=", "collections", ".", "Counter", "(", ")", "\n", "jsonl_fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "f'{self.split}{dataset}_{split}.txt'", ")", "\n", "with", "open", "(", "jsonl_fname", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "rf", ":", "\n", "                ", "for", "line", "in", "rf", ":", "\n", "                    ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                        ", "continue", "\n", "", "line", "=", "json", ".", "loads", "(", "line", ")", "\n", "rel2count", ".", "update", "(", "[", "line", "[", "'relation'", "]", ",", "]", ")", "\n", "", "", "split2rels", "[", "split", "]", "=", "rel2count", "\n", "all_rels", ".", "update", "(", "list", "(", "rel2count", ".", "keys", "(", ")", ")", ")", "\n", "\n", "", "train_relations", "=", "{", "rel", "for", "rel", ",", "rel_count", "in", "split2rels", "[", "'train'", "]", ".", "items", "(", ")", "if", "rel_count", ">=", "min_rel_freq", "}", "\n", "dev_relations", "=", "{", "\n", "rel", "for", "rel", ",", "rel_count", "in", "split2rels", "[", "'dev'", "]", ".", "items", "(", ")", "\n", "if", "all", "(", "[", "rel_count", ">=", "min_rel_freq", ",", "rel", "in", "train_relations", "]", ")", "\n", "}", "\n", "test_relations", "=", "{", "\n", "rel", "for", "rel", ",", "rel_count", "in", "split2rels", "[", "'test'", "]", ".", "items", "(", ")", "\n", "if", "all", "(", "[", "rel_count", ">=", "min_rel_freq", ",", "rel", "in", "train_relations", "]", ")", "\n", "}", "\n", "keep", "=", "test_relations", ".", "intersection", "(", "dev_relations", ".", "intersection", "(", "train_relations", ")", ")", "\n", "rels_to_discard", "=", "all_rels", "-", "keep", "\n", "\n", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", ":", "\n", "            ", "jsonl_fname", "=", "os", ".", "path", ".", "join", "(", "self", ".", "base_dir", ",", "f'{self.split}{dataset}_{split}.txt'", ")", "\n", "n_pos", ",", "n_neg", "=", "prune_by_rels", "(", "jsonl_fname", ",", "rels_to_discard", ")", "\n", "\n", "logger", ".", "info", "(", "f'Final corpus statistics ...'", ")", "\n", "logger", ".", "info", "(", "f'{split}: {n_pos} +ve and {n_neg} NA instances'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.save_items": [[866, 871], ["len", "range", "items.pop", "wf.write", "str"], "function", ["None"], ["", "", "", "def", "save_items", "(", "items", ",", "wf", ")", ":", "\n", "    ", "n", "=", "len", "(", "items", ")", "\n", "for", "_", "in", "range", "(", "n", ")", ":", "\n", "        ", "idx", ",", "pair", "=", "items", ".", "pop", "(", ")", "\n", "wf", ".", "write", "(", "'\\t'", ".", "join", "(", "[", "str", "(", "idx", ")", ",", "','", ".", "join", "(", "pair", ")", "]", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.read_idx_file": [[873, 884], ["dict", "open", "tqdm.tqdm", "line.strip.strip", "line.strip.split", "tuple", "tuple.split", "int"], "function", ["None"], ["", "", "def", "read_idx_file", "(", "fname", ")", ":", "\n", "    ", "idx2pair", "=", "dict", "(", ")", "\n", "with", "open", "(", "fname", ")", "as", "rf", ":", "\n", "        ", "for", "line", "in", "tqdm", "(", "rf", ",", "desc", "=", "f'Reading idxs file {fname}'", ")", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "continue", "\n", "", "idx", ",", "pair", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "pair", "=", "tuple", "(", "pair", ".", "split", "(", "','", ")", ")", "\n", "idx2pair", "[", "int", "(", "idx", ")", "]", "=", "pair", "\n", "", "", "return", "idx2pair", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.read_triples": [[886, 896], ["set", "open", "line.strip.strip", "line.strip.split", "set.add"], "function", ["None"], ["", "def", "read_triples", "(", "fname", ")", ":", "\n", "    ", "triples", "=", "set", "(", ")", "\n", "with", "open", "(", "fname", ")", "as", "rf", ":", "\n", "        ", "for", "line", "in", "rf", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "continue", "\n", "", "h", ",", "r", ",", "t", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "triples", ".", "add", "(", "(", "h", ",", "r", ",", "t", ")", ")", "\n", "", "", "return", "triples", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.read_triples_from_jsonl": [[898, 911], ["set", "open", "json.loads.strip", "json.loads", "set.add"], "function", ["None"], ["", "def", "read_triples_from_jsonl", "(", "fname", ")", ":", "\n", "    ", "triples", "=", "set", "(", ")", "\n", "with", "open", "(", "fname", ")", "as", "rf", ":", "\n", "        ", "for", "line", "in", "rf", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "continue", "\n", "", "line", "=", "json", ".", "loads", "(", "line", ")", "\n", "h", "=", "line", "[", "'h'", "]", "[", "'name'", "]", "\n", "r", "=", "line", "[", "'relation'", "]", "\n", "t", "=", "line", "[", "'t'", "]", "[", "'name'", "]", "\n", "triples", ".", "add", "(", "(", "h", ",", "r", ",", "t", ")", ")", "\n", "", "", "return", "triples", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.remove_overlapping_pairs": [[913, 949], ["create_kb_aligned_text_corpora.check_overlap", "list", "logger.info", "create_kb_aligned_text_corpora.read_triples_from_jsonl", "create_kb_aligned_text_corpora.read_triples_from_jsonl", "open", "open", "jsonl.strip.strip", "json.loads", "list.append", "wf.write"], "function", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.check_overlap", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.read_triples_from_jsonl", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.read_triples_from_jsonl"], ["", "def", "remove_overlapping_pairs", "(", "src_jsonl_fname", ",", "tgt_jsonl_fname", ",", "name", ")", ":", "\n", "    ", "triples_to_remove", ",", "pairs_to_remove", "=", "check_overlap", "(", "\n", "read_triples_from_jsonl", "(", "src_jsonl_fname", ")", ",", "\n", "read_triples_from_jsonl", "(", "tgt_jsonl_fname", ")", ",", "\n", "name", "\n", ")", "\n", "\n", "orig", "=", "0", "\n", "updated", "=", "0", "\n", "updated_jsonls", "=", "list", "(", ")", "\n", "with", "open", "(", "src_jsonl_fname", ")", "as", "rf", ":", "\n", "        ", "for", "jsonl", "in", "rf", ":", "\n", "            ", "jsonl", "=", "jsonl", ".", "strip", "(", ")", "\n", "if", "not", "jsonl", ":", "\n", "                ", "continue", "\n", "", "line", "=", "json", ".", "loads", "(", "jsonl", ")", "\n", "orig", "+=", "1", "\n", "h", "=", "line", "[", "'h'", "]", "[", "'name'", "]", "\n", "r", "=", "line", "[", "'relation'", "]", "\n", "t", "=", "line", "[", "'t'", "]", "[", "'name'", "]", "\n", "if", "(", "h", ",", "r", ",", "t", ")", "in", "triples_to_remove", ":", "\n", "                ", "continue", "\n", "", "elif", "(", "t", ",", "r", ",", "h", ")", "in", "triples_to_remove", ":", "\n", "                ", "continue", "\n", "", "elif", "(", "h", ",", "t", ")", "in", "pairs_to_remove", ":", "\n", "                ", "continue", "\n", "", "elif", "(", "t", ",", "h", ")", "in", "pairs_to_remove", ":", "\n", "                ", "continue", "\n", "", "updated_jsonls", ".", "append", "(", "jsonl", ")", "\n", "updated", "+=", "1", "\n", "\n", "", "", "logger", ".", "info", "(", "f'Removed {orig - updated} instances from {orig}'", ")", "\n", "\n", "with", "open", "(", "src_jsonl_fname", ",", "'w'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "wf", ":", "\n", "        ", "for", "jsonl", "in", "updated_jsonls", ":", "\n", "            ", "wf", ".", "write", "(", "jsonl", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.check_overlap": [[951, 992], ["logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "function", ["None"], ["", "", "", "def", "check_overlap", "(", "train_triples", ",", "test_triples", ",", "name", ")", ":", "\n", "    ", "train_triples_inv", "=", "{", "(", "t", ",", "r", ",", "h", ")", "for", "h", ",", "r", ",", "t", "in", "train_triples", "}", "\n", "train_pairs", "=", "{", "(", "h", ",", "t", ")", "for", "h", ",", "_", ",", "t", "in", "train_triples", "}", "\n", "train_pairs_inv", "=", "{", "(", "t", ",", "h", ")", "for", "h", ",", "t", "in", "train_pairs", "}", "\n", "\n", "test_triples_inv", "=", "{", "(", "t", ",", "r", ",", "h", ")", "for", "h", ",", "r", ",", "t", "in", "test_triples", "}", "\n", "test_pairs", "=", "{", "(", "h", ",", "t", ")", "for", "h", ",", "_", ",", "t", "in", "test_triples", "}", "\n", "test_pairs_inv", "=", "{", "(", "t", ",", "h", ")", "for", "h", ",", "t", "in", "test_pairs", "}", "\n", "\n", "inter_triples", "=", "train_triples", "&", "test_triples", "\n", "union_triples", "=", "train_triples", "|", "test_triples", "\n", "\n", "inter_triples_inv", "=", "train_triples_inv", "&", "test_triples", "\n", "union_triples_inv", "=", "train_triples_inv", "|", "test_triples", "\n", "\n", "inter_pairs", "=", "train_pairs", "&", "test_pairs", "\n", "inter_pairs_inv", "=", "train_pairs_inv", "&", "test_pairs", "\n", "\n", "triples_to_remove", "=", "inter_triples", "|", "inter_triples_inv", "\n", "pairs_to_remove", "=", "inter_pairs", "|", "inter_pairs_inv", "\n", "\n", "logger", ".", "info", "(", "f'Training/{name} intersection size: {len(inter_triples)}'", ")", "\n", "\n", "logger", ".", "info", "(", "f'Number of {name} triples in Training: '", "\n", "f'{(len(inter_triples) / len(test_triples)) * 100:.2f}%'", ")", "\n", "\n", "logger", ".", "info", "(", "f'Inverse Training/{name} intersection size: {len(inter_triples_inv)}'", ")", "\n", "\n", "logger", ".", "info", "(", "f'Number of {name} triples in Inverse Training: '", "\n", "f'{(len(inter_triples_inv) / len(test_triples)) * 100:.2f}%'", ")", "\n", "\n", "logger", ".", "info", "(", "f'Number of {name} triples in Training or Inverse Training: '", "\n", "f'{(len(inter_triples | inter_triples_inv) / len(test_triples)) * 100:.2f}%'", ")", "\n", "\n", "logger", ".", "info", "(", "f'Number of {name} pairs in Training: '", "\n", "f'{(len(inter_pairs) / len(test_pairs)) * 100:.2f}%'", ")", "\n", "\n", "logger", ".", "info", "(", "f'Number of {name} pairs in Inverse Training: '", "\n", "f'{(len(inter_pairs_inv) / len(test_pairs)) * 100:.2f}%'", ")", "\n", "\n", "return", "triples_to_remove", ",", "pairs_to_remove", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.prune_by_type_mentions": [[994, 1045], ["collections.defaultdict", "logging.info", "set", "collections.defaultdict.items", "list", "logger.info", "open", "mentions.most_common", "open", "open", "json.loads.strip", "json.loads", "sty2mention[].update", "sty2mention[].update", "json.loads.strip", "json.loads", "list.append", "wf.write", "collections.Counter", "collections.Counter", "set.add", "h_n.lower", "t_n.lower", "h_n.lower", "t_n.lower", "json.dumps"], "function", ["None"], ["", "def", "prune_by_type_mentions", "(", "benchmark_split_file", ",", "cui2sty", ",", "max_freq", "=", "1000", ")", ":", "\n", "    ", "sty2mention", "=", "collections", ".", "defaultdict", "(", "set", ")", "\n", "\n", "logging", ".", "info", "(", "f'Reading file {benchmark_split_file} for type mentions-based pruning ...'", ")", "\n", "\n", "with", "open", "(", "benchmark_split_file", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "as", "rf", ":", "\n", "        ", "for", "line", "in", "rf", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "continue", "\n", "", "line", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "h", ",", "t", "=", "line", "[", "\"h\"", "]", "[", "\"id\"", "]", ",", "line", "[", "\"t\"", "]", "[", "\"id\"", "]", "\n", "h_n", ",", "t_n", "=", "line", "[", "\"h\"", "]", "[", "\"name\"", "]", ",", "line", "[", "\"t\"", "]", "[", "\"name\"", "]", "\n", "r", "=", "line", "[", "\"relation\"", "]", "\n", "ht", "=", "cui2sty", "[", "h", "]", "\n", "tt", "=", "cui2sty", "[", "t", "]", "\n", "if", "ht", "not", "in", "sty2mention", ":", "\n", "                ", "sty2mention", "[", "ht", "]", "=", "collections", ".", "Counter", "(", ")", "\n", "", "if", "tt", "not", "in", "sty2mention", ":", "\n", "                ", "sty2mention", "[", "tt", "]", "=", "collections", ".", "Counter", "(", ")", "\n", "", "sty2mention", "[", "ht", "]", ".", "update", "(", "[", "h_n", ".", "lower", "(", ")", "]", ")", "\n", "sty2mention", "[", "tt", "]", ".", "update", "(", "[", "t_n", ".", "lower", "(", ")", "]", ")", "\n", "\n", "", "", "mentions_to_discard", "=", "set", "(", ")", "\n", "for", "sty", ",", "mentions", "in", "sty2mention", ".", "items", "(", ")", ":", "\n", "        ", "for", "mention", ",", "freq", "in", "mentions", ".", "most_common", "(", ")", ":", "\n", "            ", "if", "freq", ">", "max_freq", ":", "# 1000", "\n", "                ", "mentions_to_discard", ".", "add", "(", "mention", ")", "\n", "\n", "", "", "", "new_jsonls", "=", "list", "(", ")", "\n", "count", "=", "0", "\n", "old", "=", "0", "\n", "with", "open", "(", "benchmark_split_file", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "as", "rf", ":", "\n", "        ", "for", "line", "in", "rf", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "continue", "\n", "", "line", "=", "json", ".", "loads", "(", "line", ")", "\n", "old", "+=", "1", "\n", "h_n", ",", "t_n", "=", "line", "[", "\"h\"", "]", "[", "\"name\"", "]", ",", "line", "[", "\"t\"", "]", "[", "\"name\"", "]", "\n", "if", "h_n", ".", "lower", "(", ")", "in", "mentions_to_discard", "or", "t_n", ".", "lower", "(", ")", "in", "mentions_to_discard", ":", "\n", "                ", "count", "+=", "1", "\n", "continue", "\n", "", "new_jsonls", ".", "append", "(", "line", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "f'removed {count} lines from total of {old} !'", ")", "\n", "\n", "with", "open", "(", "benchmark_split_file", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "as", "wf", ":", "\n", "        ", "for", "line", "in", "new_jsonls", ":", "\n", "            ", "wf", ".", "write", "(", "json", ".", "dumps", "(", "line", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.prune_by_rels": [[1047, 1071], ["logging.info", "list", "open", "open", "json.loads.strip", "json.loads", "list.append", "wf.write", "json.dumps"], "function", ["None"], ["", "", "", "def", "prune_by_rels", "(", "benchmark_split_file", ",", "rels_to_discard", ")", ":", "\n", "    ", "logging", ".", "info", "(", "f'Reading file {benchmark_split_file} for relations pruning ...'", ")", "\n", "\n", "new_jsonls", "=", "list", "(", ")", "\n", "n_pos", ",", "n_neg", "=", "0", ",", "0", "\n", "with", "open", "(", "benchmark_split_file", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "as", "rf", ":", "\n", "        ", "for", "line", "in", "rf", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "continue", "\n", "", "line", "=", "json", ".", "loads", "(", "line", ")", "\n", "if", "line", "[", "\"relation\"", "]", "in", "rels_to_discard", ":", "\n", "                ", "continue", "\n", "", "if", "line", "[", "'relation'", "]", "==", "'NA'", ":", "\n", "                ", "n_neg", "+=", "1", "\n", "", "else", ":", "\n", "                ", "n_pos", "+=", "1", "\n", "", "new_jsonls", ".", "append", "(", "line", ")", "\n", "\n", "", "", "with", "open", "(", "benchmark_split_file", ",", "'w'", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "as", "wf", ":", "\n", "        ", "for", "line", "in", "new_jsonls", ":", "\n", "            ", "wf", ".", "write", "(", "json", ".", "dumps", "(", "line", ")", "+", "'\\n'", ")", "\n", "\n", "", "", "return", "n_pos", ",", "n_neg", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.add_logging_handlers": [[1073, 1080], ["os.path.join", "logging.FileHandler", "logging.FileHandler.setFormatter", "logger.addHandler", "logging.Formatter"], "function", ["None"], ["", "def", "add_logging_handlers", "(", "logger", ",", "dir_name", ",", "fname", ")", ":", "\n", "    ", "log_file", "=", "os", ".", "path", ".", "join", "(", "dir_name", ",", "fname", ")", "\n", "fh", "=", "logging", ".", "FileHandler", "(", "log_file", ")", "\n", "fh", ".", "setFormatter", "(", "logging", ".", "Formatter", "(", "\n", "'%(asctime)s - %(levelname)s - %(name)s -   %(message)s'", ",", "'%m/%d/%Y %H:%M:%S'", "\n", ")", ")", "\n", "logger", ".", "addHandler", "(", "fh", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.main": [[1082, 1145], ["create_kb_aligned_text_corpora.BioDSRECorpus", "all", "create_kb_aligned_text_corpora.add_logging_handlers", "logger.info", "random.seed", "logger.info", "create_kb_aligned_text_corpora.BioDSRECorpus.create_corpus", "list", "vars", "create_kb_aligned_text_corpora.BioDSRECorpus.search_pos_and_neg_instances", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "map", "BioDSRECorpus.pos_fname", "BioDSRECorpus.neg_fname"], "function", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.add_logging_handlers", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.create_corpus", "home.repos.pwc.inspect_result.suamin_meddistant19.None.create_kb_aligned_text_corpora.BioDSRECorpus.search_pos_and_neg_instances"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "corpus", "=", "BioDSRECorpus", "(", "\n", "args", ".", "medline_entities_linked_fname", ",", "\n", "args", ".", "triples_dir", ",", "\n", "'ind'", "if", "args", ".", "split", "==", "'ind'", "else", "None", ",", "\n", "args", ".", "has_def", "\n", ")", "\n", "\n", "# see if pos and neg linked files have been created before, simply check for train", "\n", "check", "=", "all", "(", "list", "(", "\n", "map", "(", "\n", "os", ".", "path", ".", "exists", ",", "\n", "[", "corpus", ".", "pos_fname", "(", "split", ")", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "]", "+", "\n", "[", "corpus", ".", "neg_fname", "(", "split", ")", "for", "split", "in", "[", "'train'", ",", "'dev'", ",", "'test'", "]", "]", "\n", ")", "\n", ")", ")", "\n", "\n", "log_file", "=", "f'corpus_{args.split}'", "+", "(", "'_def'", "if", "args", ".", "has_def", "else", "''", ")", "+", "'.log'", "\n", "add_logging_handlers", "(", "logger", ",", "corpus", ".", "base_dir", ",", "log_file", ")", "\n", "logger", ".", "info", "(", "vars", "(", "args", ")", ")", "\n", "\n", "# --------------------------------------------------", "\n", "# WARNING: please don't change this to another seed, ", "\n", "# required for reproducing the corpus when we have ", "\n", "# multiple pairs matching for a given sentence", "\n", "# --------------------------------------------------", "\n", "random", ".", "seed", "(", "0", ")", "\n", "\n", "if", "not", "check", ":", "\n", "# this will take time, go grab 2 cups of coffee :)", "\n", "        ", "ntr", ",", "nval", ",", "nte", ",", "nntr", ",", "nnval", ",", "nnte", "=", "corpus", ".", "search_pos_and_neg_instances", "(", "\n", "raw_neg_sample_size", "=", "args", ".", "raw_neg_sample_size", ",", "\n", "corrupt_arg", "=", "args", ".", "corrupt_arg", ",", "\n", "remove_multimentions_sents", "=", "args", ".", "remove_multimentions_sents", ",", "\n", "use_type_constraint", "=", "args", ".", "use_type_constraint", ",", "\n", "use_arg_constraint", "=", "args", ".", "use_arg_constraint", "\n", ")", "\n", "\n", "logger", ".", "info", "(", "f'Positive and negative instances statistics ...'", ")", "\n", "logger", ".", "info", "(", "f'--- train instances (+ve) = {ntr}'", ")", "\n", "logger", ".", "info", "(", "f'--- dev instances (+ve) = {nval}'", ")", "\n", "logger", ".", "info", "(", "f'--- test instances (+ve) = {nte}'", ")", "\n", "logger", ".", "info", "(", "f'--- train NA instances = {nntr}'", ")", "\n", "logger", ".", "info", "(", "f'--- dev NA instances = {nnval}'", ")", "\n", "logger", ".", "info", "(", "f'--- test NA instances = {nnte}'", ")", "\n", "\n", "", "logger", ".", "info", "(", "f'Creating corpus ...'", ")", "\n", "\n", "corpus", ".", "create_corpus", "(", "\n", "train_size", "=", "args", ".", "train_size", ",", "\n", "dev_size", "=", "args", ".", "dev_size", ",", "\n", "sample", "=", "args", ".", "sample", ",", "\n", "use_sent_level_noise", "=", "args", ".", "use_sent_level_noise", ",", "\n", "neg_prop", "=", "args", ".", "neg_prop", ",", "\n", "remove_mention_overlaps", "=", "args", ".", "remove_mention_overlaps", ",", "\n", "canonical_or_aliases_only", "=", "args", ".", "canonical_or_aliases_only", ",", "\n", "prune_frequent_bags", "=", "args", ".", "prune_frequent_bags", ",", "\n", "max_bag_size", "=", "args", ".", "max_bag_size", ",", "\n", "prune_frequent_mentions", "=", "args", ".", "prune_frequent_mentions", ",", "\n", "max_mention_freq", "=", "args", ".", "max_mention_freq", ",", "\n", "min_rel_freq", "=", "args", ".", "min_rel_freq", ",", "\n", "include_other_mentions", "=", "args", ".", "include_other_mentions", ",", "\n", "dataset", "=", "'med_distant'", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_abstracts.main": [[11, 38], ["tqdm.tqdm", "open", "gzip.open", "xml.fromstring", "ET.fromstring.findall", "rf.read", "wf.write", "abstract.text.strip.text.strip"], "function", ["None"], ["def", "main", "(", "fname", ")", ":", "\n", "    ", "fnames", ":", "List", "[", "str", "]", "=", "[", "fname", ",", "]", "\n", "\n", "for", "fname", "in", "tqdm", "(", "fnames", ",", "desc", "=", "'Reading ``AbstractText`` from PubMed MEDLINE 2019 abstracts ...'", ")", ":", "\n", "        ", "with", "open", "(", "fname", "+", "'.txt'", ",", "'w'", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "wf", ":", "\n", "            ", "with", "gzip", ".", "open", "(", "fname", ",", "'rb'", ")", "as", "rf", ":", "\n", "\n", "                ", "tree", "=", "ET", ".", "fromstring", "(", "rf", ".", "read", "(", ")", ")", "\n", "abstracts", "=", "tree", ".", "findall", "(", "'.//AbstractText'", ")", "\n", "\n", "for", "abstract", "in", "abstracts", ":", "\n", "                    ", "try", ":", "\n", "                        ", "abstract", "=", "abstract", ".", "text", ".", "strip", "(", ")", "\n", "", "except", ":", "\n", "                        ", "continue", "\n", "\n", "", "if", "not", "abstract", ":", "\n", "                        ", "continue", "\n", "\n", "# Strip starting b' or b\" and ending ' or \"", "\n", "", "if", "(", "abstract", "[", ":", "2", "]", "==", "\"b'\"", "and", "abstract", "[", "-", "1", "]", "==", "\"'\"", ")", "or", "(", "abstract", "[", ":", "2", "]", "==", "'b\"'", "and", "abstract", "[", "-", "1", "]", "==", "'\"'", ")", ":", "\n", "                        ", "abstract", "=", "abstract", "[", "2", ":", "-", "1", "]", "\n", "\n", "", "if", "not", "abstract", ":", "\n", "                        ", "continue", "\n", "\n", "", "wf", ".", "write", "(", "abstract", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.read_entity_rel_fact_na_maps": [[24, 92], ["dict", "dict", "dict", "dict", "collections.defaultdict", "logging.info", "logging.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "open", "json.loads.strip", "json.loads", "[].add", "[].add", "bags[].add", "bags[].add", "len", "len", "len", "set", "set", "len", "len"], "function", ["None"], ["def", "read_entity_rel_fact_na_maps", "(", "benchmark_split_file", ")", ":", "\n", "    ", "entity_dict", "=", "dict", "(", ")", "# map ent -> {'h' -> counts as head, 't' -> counts as tails, 'mention' -> mentions set}", "\n", "facts_dict", "=", "dict", "(", ")", "# map (h, r, t) -> count", "\n", "na_dict", "=", "dict", "(", ")", "# map (h, NA, t) -> count ", "\n", "rel_dict", "=", "dict", "(", ")", "# map r -> count", "\n", "bags", "=", "collections", ".", "defaultdict", "(", "set", ")", "\n", "\n", "instances", "=", "0", "\n", "na_instances", "=", "0", "\n", "\n", "logging", ".", "info", "(", "f'Reading file {benchmark_split_file} ...'", ")", "\n", "\n", "with", "open", "(", "benchmark_split_file", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "as", "rf", ":", "\n", "        ", "for", "line", "in", "rf", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "continue", "\n", "", "line", "=", "json", ".", "loads", "(", "line", ")", "\n", "\n", "h", ",", "t", "=", "line", "[", "\"h\"", "]", "[", "\"id\"", "]", ",", "line", "[", "\"t\"", "]", "[", "\"id\"", "]", "\n", "h_n", ",", "t_n", "=", "line", "[", "\"h\"", "]", "[", "\"name\"", "]", ",", "line", "[", "\"t\"", "]", "[", "\"name\"", "]", "\n", "r", "=", "line", "[", "\"relation\"", "]", "\n", "triple", "=", "(", "h", ",", "r", ",", "t", ")", "\n", "\n", "if", "h", "not", "in", "entity_dict", ":", "\n", "                ", "entity_dict", "[", "h", "]", "=", "{", "\"h\"", ":", "0", ",", "\"t\"", ":", "0", ",", "\"mention\"", ":", "set", "(", ")", "}", "\n", "\n", "", "if", "t", "not", "in", "entity_dict", ":", "\n", "                ", "entity_dict", "[", "t", "]", "=", "{", "\"h\"", ":", "0", ",", "\"t\"", ":", "0", ",", "\"mention\"", ":", "set", "(", ")", "}", "\n", "\n", "", "entity_dict", "[", "h", "]", "[", "\"h\"", "]", "+=", "1", "\n", "entity_dict", "[", "h", "]", "[", "\"mention\"", "]", ".", "add", "(", "h_n", ")", "\n", "entity_dict", "[", "t", "]", "[", "\"t\"", "]", "+=", "1", "\n", "entity_dict", "[", "t", "]", "[", "\"mention\"", "]", ".", "add", "(", "t_n", ")", "\n", "\n", "if", "r", "==", "\"NA\"", ":", "\n", "                ", "if", "triple", "not", "in", "na_dict", ":", "\n", "                    ", "na_dict", "[", "triple", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "na_dict", "[", "triple", "]", "+=", "1", "\n", "", "na_instances", "+=", "1", "\n", "", "else", ":", "\n", "                ", "if", "triple", "not", "in", "facts_dict", ":", "\n", "                    ", "facts_dict", "[", "triple", "]", "=", "1", "\n", "", "else", ":", "\n", "                    ", "facts_dict", "[", "triple", "]", "+=", "1", "\n", "\n", "", "", "if", "r", "not", "in", "rel_dict", ":", "\n", "                ", "rel_dict", "[", "r", "]", "=", "1", "\n", "", "else", ":", "\n", "                ", "rel_dict", "[", "r", "]", "+=", "1", "\n", "\n", "", "if", "r", "==", "'NA'", ":", "\n", "                ", "bags", "[", "'neg'", "]", ".", "add", "(", "(", "h", ",", "t", ")", ")", "\n", "", "else", ":", "\n", "                ", "bags", "[", "'pos'", "]", ".", "add", "(", "(", "h", ",", "t", ")", ")", "\n", "\n", "", "instances", "+=", "1", "\n", "\n", "", "", "na_percent", "=", "(", "na_instances", "/", "instances", ")", "*", "100", "\n", "logger", ".", "info", "(", "f'# of instances = {instances}'", ")", "\n", "logger", ".", "info", "(", "f'# of facts = {len(facts_dict)}'", ")", "\n", "logger", ".", "info", "(", "f'# NA (%) = {na_percent:4.1f}%'", ")", "\n", "logger", ".", "info", "(", "f'# of +ve bags = {len(bags[\"pos\"])}'", ")", "\n", "logger", ".", "info", "(", "f'# of -ve bags = {len(bags[\"neg\"])}'", ")", "\n", "logger", ".", "info", "(", "f'# of bags = {len(bags[\"pos\"]) + len(bags[\"neg\"])}'", ")", "\n", "\n", "return", "entity_dict", ",", "rel_dict", ",", "facts_dict", ",", "na_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_ents": [[104, 108], ["open", "wf.write"], "function", ["None"], ["def", "save_ents", "(", "fname", ",", "ents", ")", ":", "\n", "    ", "with", "open", "(", "fname", ",", "\"w\"", ")", "as", "wf", ":", "\n", "        ", "for", "ent", "in", "ents", ":", "\n", "            ", "wf", ".", "write", "(", "ent", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_triples": [[110, 114], ["open", "wf.write"], "function", ["None"], ["", "", "", "def", "save_triples", "(", "fname", ",", "triples", ")", ":", "\n", "    ", "with", "open", "(", "fname", ",", "\"w\"", ")", "as", "wf", ":", "\n", "        ", "for", "triple", "in", "triples", ":", "\n", "            ", "wf", ".", "write", "(", "\"\\t\"", ".", "join", "(", "triple", ")", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.iter_split_lines": [[116, 123], ["open", "line.strip.strip", "json.loads"], "function", ["None"], ["", "", "", "def", "iter_split_lines", "(", "split_file", ")", ":", "\n", "    ", "with", "open", "(", "split_file", ",", "encoding", "=", "\"utf-8\"", ",", "errors", "=", "\"ignore\"", ")", "as", "rf", ":", "\n", "        ", "for", "line", "in", "rf", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "continue", "\n", "", "yield", "json", ".", "loads", "(", "line", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.create_rel2id": [[125, 138], ["dict", "set", "sorted", "enumerate", "extract_benchmark_metadata.iter_split_lines", "list", "sorted.add"], "function", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.iter_split_lines"], ["", "", "", "def", "create_rel2id", "(", "base_dir", ",", "splits", ")", ":", "\n", "    ", "rel2id", "=", "dict", "(", ")", "\n", "rels", "=", "set", "(", ")", "\n", "rel2id", "[", "'NA'", "]", "=", "0", "\n", "for", "split", "in", "splits", ":", "\n", "        ", "path", "=", "base_dir", "/", "f'{args.dataset}_{split}.txt'", "\n", "for", "line", "in", "iter_split_lines", "(", "path", ")", ":", "\n", "            ", "if", "line", "[", "\"relation\"", "]", "!=", "'NA'", ":", "\n", "                ", "rels", ".", "add", "(", "line", "[", "\"relation\"", "]", ")", "\n", "", "", "", "rels", "=", "sorted", "(", "list", "(", "rels", ")", ")", "\n", "for", "idx", ",", "rel", "in", "enumerate", "(", "rels", ")", ":", "\n", "        ", "rel2id", "[", "rel", "]", "=", "idx", "+", "1", "\n", "", "return", "rel2id", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.read_json_map": [[140, 144], ["open", "json.load"], "function", ["None"], ["", "def", "read_json_map", "(", "fname", ")", ":", "\n", "    ", "with", "open", "(", "fname", ")", "as", "rf", ":", "\n", "        ", "json_map", "=", "json", ".", "load", "(", "rf", ")", "\n", "", "return", "json_map", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.main": [[146, 255], ["dict", "set", "set", "set", "extract_benchmark_metadata.create_rel2id", "extract_benchmark_metadata.read_json_map", "extract_benchmark_metadata.read_json_map", "extract_benchmark_metadata.read_json_map", "extract_benchmark_metadata.read_json_map", "extract_benchmark_metadata.read_json_map", "args.splits.split", "extract_benchmark_metadata.save_triples", "extract_benchmark_metadata.save_triples", "set.update", "extract_benchmark_metadata.save_triples", "pathlib.Path", "args.splits.split", "open", "json.dump", "dict", "Split", "extract_benchmark_metadata.save_ents", "extract_benchmark_metadata.save_triples", "extract_benchmark_metadata.save_triples", "set.update", "set.update", "set.update", "open", "wf.write", "sorted", "sorted", "sorted", "open", "wf.write", "open", "wf.write", "open", "wf.write", "open", "wf.write", "open", "wf.write", "open", "pickle.dump", "pathlib.Path", "pathlib.Path", "open", "pathlib.Path", "pathlib.Path", "pathlib.Path", "sorted", "sorted", "sorted", "set", "extract_benchmark_metadata.save_ents", "set", "extract_benchmark_metadata.save_triples", "set", "extract_benchmark_metadata.save_triples", "Split.facts_dict.keys", "Split.na_dict.keys", "Split.entity_dict.keys", "enumerate", "json.dumps", "json.dumps", "json.dumps", "open", "wf.write", "json.dumps", "json.dumps", "json.dumps", "line.strip.strip", "line.strip.split", "extract_benchmark_metadata.read_entity_rel_fact_na_maps", "Split.entity_dict.keys", "Split.facts_dict.keys", "Split.na_dict.keys", "data[].entity_dict.keys", "set", "sorted", "data[].facts_dict.keys", "set", "sorted", "data[].na_dict.keys", "set", "sorted", "sorted", "read_json_map.items", "read_json_map.items", "json.dumps", "read_json_map.items", "read_json_map.items", "read_json_map.items", "pathlib.Path", "Split.entity_dict.keys", "Split.facts_dict.keys", "Split.na_dict.keys", "dict.items"], "function", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.create_rel2id", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.read_json_map", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.read_json_map", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.read_json_map", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.read_json_map", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.read_json_map", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_ents", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_ents", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.save_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_benchmark_metadata.read_entity_rel_fact_na_maps"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "data", "=", "dict", "(", ")", "\n", "ents", "=", "set", "(", ")", "\n", "facts", "=", "set", "(", ")", "\n", "na_facts", "=", "set", "(", ")", "\n", "base_dir", "=", "Path", "(", "args", ".", "benchmark_dir", ")", "/", "args", ".", "dataset", "\n", "\n", "rel2id", "=", "create_rel2id", "(", "base_dir", ",", "args", ".", "splits", ".", "split", "(", "','", ")", ")", "\n", "with", "open", "(", "base_dir", "/", "f'{args.dataset}_rel2id.json'", ",", "'w'", ")", "as", "wf", ":", "\n", "        ", "json", ".", "dump", "(", "rel2id", ",", "wf", ")", "\n", "\n", "# relevant paths", "\n", "", "ent2type", "=", "read_json_map", "(", "Path", "(", "args", ".", "umls_dir", ")", "/", "'cui2sty.json'", ")", "\n", "ent2group", "=", "read_json_map", "(", "Path", "(", "args", ".", "umls_dir", ")", "/", "'cui2sg.json'", ")", "\n", "if", "'def'", "in", "args", ".", "dataset", ":", "\n", "        ", "ent2def", "=", "dict", "(", ")", "\n", "with", "open", "(", "Path", "(", "args", ".", "umls_dir", ")", "/", "'cui2def.txt'", ")", "as", "rf", ":", "\n", "            ", "for", "line", "in", "rf", ":", "\n", "                ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                    ", "continue", "\n", "", "cui", ",", "defi", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "ent2def", "[", "cui", "]", "=", "defi", "\n", "", "", "", "else", ":", "\n", "        ", "ent2def", "=", "None", "\n", "", "rel2type", "=", "read_json_map", "(", "Path", "(", "args", ".", "umls_dir", ")", "/", "'relation2broad.json'", ")", "\n", "rel2cat", "=", "read_json_map", "(", "Path", "(", "args", ".", "umls_dir", ")", "/", "'relation2oneormany.json'", ")", "\n", "rel2cat_sg", "=", "read_json_map", "(", "Path", "(", "args", ".", "umls_dir", ")", "/", "'relation2sg_oneormany.json'", ")", "\n", "\n", "for", "split", "in", "args", ".", "splits", ".", "split", "(", "','", ")", ":", "\n", "        ", "path", "=", "base_dir", "/", "f'{args.dataset}_{split}.txt'", "\n", "split_data", "=", "Split", "(", "*", "read_entity_rel_fact_na_maps", "(", "path", ")", ")", "\n", "\n", "ent_file", "=", "base_dir", "/", "f'{split}-ents.txt'", "\n", "pos_triples_file", "=", "base_dir", "/", "f'{split}-triples.tsv'", "\n", "na_triples_file", "=", "base_dir", "/", "f'{split}-na-triples.tsv'", "\n", "\n", "save_ents", "(", "ent_file", ",", "sorted", "(", "split_data", ".", "entity_dict", ".", "keys", "(", ")", ")", ")", "\n", "save_triples", "(", "pos_triples_file", ",", "sorted", "(", "split_data", ".", "facts_dict", ".", "keys", "(", ")", ")", ")", "\n", "save_triples", "(", "na_triples_file", ",", "sorted", "(", "split_data", ".", "na_dict", ".", "keys", "(", ")", ")", ")", "\n", "\n", "if", "split", "!=", "\"train\"", "and", "\"train\"", "in", "data", ":", "\n", "            ", "train_ents", "=", "set", "(", "data", "[", "\"train\"", "]", ".", "entity_dict", ".", "keys", "(", ")", ")", "\n", "unseen_split_ents", "=", "set", "(", "split_data", ".", "entity_dict", ".", "keys", "(", ")", ")", "-", "train_ents", "\n", "unseen_ent_file", "=", "base_dir", "/", "f'{split}-unseen-ents.txt'", "\n", "save_ents", "(", "unseen_ent_file", ",", "sorted", "(", "unseen_split_ents", ")", ")", "\n", "\n", "train_facts", "=", "set", "(", "data", "[", "\"train\"", "]", ".", "facts_dict", ".", "keys", "(", ")", ")", "\n", "unseen_split_facts", "=", "set", "(", "split_data", ".", "facts_dict", ".", "keys", "(", ")", ")", "-", "train_facts", "\n", "unseen_facts_file", "=", "base_dir", "/", "f\"{split}-unseen-triples.tsv\"", "\n", "save_triples", "(", "unseen_facts_file", ",", "sorted", "(", "unseen_split_facts", ")", ")", "\n", "\n", "train_na_facts", "=", "set", "(", "data", "[", "\"train\"", "]", ".", "na_dict", ".", "keys", "(", ")", ")", "\n", "unseen_split_na_facts", "=", "set", "(", "split_data", ".", "na_dict", ".", "keys", "(", ")", ")", "-", "train_na_facts", "\n", "unseen_na_fact_file", "=", "base_dir", "/", "f\"{split}-unseen-na-triples.tsv\"", "\n", "save_triples", "(", "unseen_na_fact_file", ",", "sorted", "(", "unseen_split_na_facts", ")", ")", "\n", "\n", "", "facts", ".", "update", "(", "split_data", ".", "facts_dict", ".", "keys", "(", ")", ")", "\n", "na_facts", ".", "update", "(", "split_data", ".", "na_dict", ".", "keys", "(", ")", ")", "\n", "\n", "ents", ".", "update", "(", "split_data", ".", "entity_dict", ".", "keys", "(", ")", ")", "\n", "data", "[", "split", "]", "=", "split_data", "\n", "\n", "", "ent2id", "=", "{", "ent", ":", "idx", "for", "idx", ",", "ent", "in", "enumerate", "(", "sorted", "(", "ents", ")", ")", "}", "\n", "\n", "with", "open", "(", "base_dir", "/", "f'{args.dataset}_ent2id.json'", ",", "\"w\"", ")", "as", "wf", ":", "\n", "        ", "wf", ".", "write", "(", "json", ".", "dumps", "(", "ent2id", ")", ")", "\n", "\n", "", "save_triples", "(", "base_dir", "/", "'triples.tsv'", ",", "sorted", "(", "facts", ")", ")", "\n", "\n", "save_triples", "(", "base_dir", "/", "'na-triples.tsv'", ",", "sorted", "(", "na_facts", ")", ")", "\n", "\n", "facts", ".", "update", "(", "na_facts", ")", "\n", "save_triples", "(", "base_dir", "/", "'all-triples.tsv'", ",", "sorted", "(", "facts", ")", ")", "\n", "\n", "# subset relevant maps based on final entities and relations", "\n", "with", "open", "(", "base_dir", "/", "f'{args.dataset}_ent2type.json'", ",", "\"w\"", ")", "as", "wf", ":", "\n", "        ", "ent2type", "=", "{", "ent", ":", "t", "for", "ent", ",", "t", "in", "ent2type", ".", "items", "(", ")", "if", "ent", "in", "ent2id", "}", "\n", "wf", ".", "write", "(", "json", ".", "dumps", "(", "ent2type", ")", ")", "\n", "\n", "", "with", "open", "(", "base_dir", "/", "f'{args.dataset}_ent2group.json'", ",", "\"w\"", ")", "as", "wf", ":", "\n", "        ", "ent2group", "=", "{", "ent", ":", "g", "for", "ent", ",", "g", "in", "ent2group", ".", "items", "(", ")", "if", "ent", "in", "ent2id", "}", "\n", "wf", ".", "write", "(", "json", ".", "dumps", "(", "ent2group", ")", ")", "\n", "\n", "", "if", "ent2def", ":", "\n", "        ", "with", "open", "(", "base_dir", "/", "f'{args.dataset}_ent2def.json'", ",", "\"w\"", ")", "as", "wf", ":", "\n", "            ", "ent2def", "=", "{", "ent", ":", "d", "for", "ent", ",", "d", "in", "ent2def", ".", "items", "(", ")", "if", "ent", "in", "ent2id", "}", "\n", "wf", ".", "write", "(", "json", ".", "dumps", "(", "ent2def", ")", ")", "\n", "\n", "", "", "with", "open", "(", "base_dir", "/", "f'{args.dataset}_rel2type.json'", ",", "\"w\"", ")", "as", "wf", ":", "\n", "        ", "rel2type", "=", "{", "rel", ":", "t", "for", "rel", ",", "t", "in", "rel2type", ".", "items", "(", ")", "if", "rel", "in", "rel2id", "and", "rel", "!=", "'NA'", "}", "\n", "rel2type", "[", "'NA'", "]", "=", "'None'", "\n", "wf", ".", "write", "(", "json", ".", "dumps", "(", "rel2type", ")", ")", "\n", "\n", "", "with", "open", "(", "base_dir", "/", "f'{args.dataset}_rel2cat.json'", ",", "\"w\"", ")", "as", "wf", ":", "\n", "        ", "rel2cat", "=", "{", "rel", ":", "c", "for", "rel", ",", "c", "in", "rel2cat", ".", "items", "(", ")", "if", "rel", "in", "rel2id", "and", "rel", "!=", "'NA'", "}", "\n", "rel2cat", "[", "'NA'", "]", "=", "'None'", "\n", "wf", ".", "write", "(", "json", ".", "dumps", "(", "rel2cat", ")", ")", "\n", "\n", "", "with", "open", "(", "base_dir", "/", "f'{args.dataset}_rel2cat_sg.json'", ",", "\"w\"", ")", "as", "wf", ":", "\n", "        ", "rel2cat_sg", "=", "{", "rel", ":", "g", "for", "rel", ",", "g", "in", "rel2cat_sg", ".", "items", "(", ")", "if", "rel", "in", "rel2id", "and", "rel", "!=", "'NA'", "}", "\n", "rel2cat", "[", "'NA'", "]", "=", "'None'", "\n", "wf", ".", "write", "(", "json", ".", "dumps", "(", "rel2cat_sg", ")", ")", "\n", "\n", "", "data", "[", "\"rel2id\"", "]", "=", "rel2id", "\n", "data", "[", "\"ent2id\"", "]", "=", "ent2id", "\n", "\n", "with", "open", "(", "base_dir", "/", "f'metadata.pkl'", ",", "'wb'", ")", "as", "wf", ":", "\n", "        ", "pickle", ".", "dump", "(", "data", ",", "wf", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.edge_split": [[93, 112], ["numpy.random.seed", "len", "len", "numpy.cumsum", "numpy.sum", "open", "open", "file.close", "numpy.searchsorted", "files[].write", "numpy.random.rand"], "function", ["None"], ["", "def", "edge_split", "(", "graph_file", ",", "files", ",", "portions", ")", ":", "\n", "    ", "\"\"\"\n    Divide a graph into several splits.\n    Parameters:\n        graph_file (str): graph file\n        files (list of str): file names\n        portions (list of float): split portions\n    \"\"\"", "\n", "assert", "len", "(", "files", ")", "==", "len", "(", "portions", ")", "\n", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "\n", "portions", "=", "np", ".", "cumsum", "(", "portions", ",", "dtype", "=", "np", ".", "float32", ")", "/", "np", ".", "sum", "(", "portions", ")", "\n", "files", "=", "[", "open", "(", "file", ",", "\"w\"", ")", "for", "file", "in", "files", "]", "\n", "with", "open", "(", "graph_file", ",", "\"r\"", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "i", "=", "np", ".", "searchsorted", "(", "portions", ",", "np", ".", "random", ".", "rand", "(", ")", ")", "\n", "files", "[", "i", "]", ".", "write", "(", "line", ")", "\n", "", "", "for", "file", "in", "files", ":", "\n", "        ", "file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.filter_triplets_by_cuis": [[114, 117], ["triplets[].isin", "triplets[].isin"], "function", ["None"], ["", "", "def", "filter_triplets_by_cuis", "(", "triplets", ",", "cui_iterable", ")", ":", "\n", "    ", "filtered", "=", "triplets", "[", "(", "triplets", "[", "'CUI1'", "]", ".", "isin", "(", "cui_iterable", ")", ")", "&", "(", "triplets", "[", "'CUI2'", "]", ".", "isin", "(", "cui_iterable", ")", ")", "]", "\n", "return", "filtered", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.create_datasets": [[119, 187], ["case2.sample.sample", "case2.sample.to_csv", "os.path.join", "extract_transductive_triples_split.edge_split", "pandas.read_csv", "pandas.read_csv", "pandas.read_csv", "extract_transductive_triples_split.move_unseen_to_train", "extract_transductive_triples_split.remove_overlapping_pairs", "extract_transductive_triples_split.remove_overlapping_pairs", "extract_transductive_triples_split.remove_overlapping_pairs", "extract_transductive_triples_split.check_overlap", "extract_transductive_triples_split.check_overlap", "extract_transductive_triples_split.check_overlap", "remove_overlapping_pairs.to_csv", "remove_overlapping_pairs.to_csv", "pd.read_csv.to_csv", "pandas.DataFrame().to_csv", "pandas.DataFrame().to_csv", "pandas.DataFrame().to_csv", "pandas.DataFrame().to_csv", "pandas.DataFrame().to_csv", "pandas.DataFrame().to_csv", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "triplets[].isin", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "pandas.DataFrame", "reciprocal_relations_dict.keys", "broad_rel_types.items", "case2[].isin"], "function", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.edge_split", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.move_unseen_to_train", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.remove_overlapping_pairs", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.remove_overlapping_pairs", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.remove_overlapping_pairs", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.check_overlap", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.check_overlap", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.check_overlap"], ["", "def", "create_datasets", "(", "triplets", ",", "data_dir", ",", "use_ro_only", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    2. no reciprocal relations at all\n    \"\"\"", "\n", "# Case 2: no reprical relations at all, so no leakage", "\n", "case2", "=", "triplets", "[", "triplets", "[", "'RELA'", "]", ".", "isin", "(", "reciprocal_relations_dict", ".", "keys", "(", ")", ")", "]", "\n", "if", "use_ro_only", ":", "\n", "        ", "rels", "=", "{", "k", "for", "k", ",", "v", "in", "broad_rel_types", ".", "items", "(", ")", "if", "v", "==", "'RO'", "}", "\n", "case2", "=", "case2", "[", "case2", "[", "'RELA'", "]", ".", "isin", "(", "rels", ")", "]", "\n", "", "case2", "=", "case2", ".", "sample", "(", "frac", "=", "1", ",", "random_state", "=", "0", ")", "\n", "case2", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'all-triples.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", ")", "\n", "\n", "# ds = Dataset(name='case2')", "\n", "graph_file", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'all-triples.tsv'", ")", "\n", "files", "=", "[", "'train.tsv'", ",", "'dev.tsv'", ",", "'test.tsv'", "]", "\n", "files", "=", "[", "os", ".", "path", ".", "join", "(", "data_dir", ",", "f", ")", "for", "f", "in", "files", "]", "\n", "portions", "=", "[", "70", ",", "10", ",", "20", "]", "\n", "edge_split", "(", "graph_file", ",", "files", ",", "portions", ")", "\n", "\n", "case2_train", "=", "pd", ".", "read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'train.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ")", "\n", "case2_train", ".", "columns", "=", "[", "'CUI1'", ",", "'RELA'", ",", "'CUI2'", "]", "\n", "case2_valid", "=", "pd", ".", "read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'dev.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ")", "\n", "case2_valid", ".", "columns", "=", "[", "'CUI1'", ",", "'RELA'", ",", "'CUI2'", "]", "\n", "case2_test", "=", "pd", ".", "read_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'test.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ")", "\n", "case2_test", ".", "columns", "=", "[", "'CUI1'", ",", "'RELA'", ",", "'CUI2'", "]", "\n", "\n", "case2_train", ",", "case2_valid", ",", "case2_test", "=", "move_unseen_to_train", "(", "case2_train", ",", "case2_valid", ",", "case2_test", ")", "\n", "\n", "case2_train", "=", "remove_overlapping_pairs", "(", "case2_train", ",", "case2_test", ")", "\n", "case2_train", "=", "remove_overlapping_pairs", "(", "case2_train", ",", "case2_valid", ")", "\n", "case2_valid", "=", "remove_overlapping_pairs", "(", "case2_valid", ",", "case2_test", ")", "\n", "\n", "check_overlap", "(", "case2_train", ",", "case2_valid", ",", "'valid'", ")", "\n", "check_overlap", "(", "case2_train", ",", "case2_test", ",", "'test'", ")", "\n", "check_overlap", "(", "case2_valid", ",", "case2_test", ",", "'test'", ")", "\n", "\n", "case2_train", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'train.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", ")", "\n", "case2_valid", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'dev.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", ")", "\n", "case2_test", ".", "to_csv", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'test.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", ")", "\n", "\n", "case2_train_triples", "=", "case2_train", ".", "values", "\n", "case2_train_triples_sty", "=", "[", "(", "cui2sty", "[", "h", "]", ",", "r", ",", "cui2sty", "[", "t", "]", ")", "for", "h", ",", "r", ",", "t", "in", "case2_train_triples", "]", "\n", "pd", ".", "DataFrame", "(", "case2_train_triples_sty", ",", "columns", "=", "None", ")", ".", "to_csv", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'train_types.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", "\n", ")", "\n", "case2_train_triples_sg", "=", "[", "(", "cui2sg", "[", "h", "]", ",", "r", ",", "cui2sg", "[", "t", "]", ")", "for", "h", ",", "r", ",", "t", "in", "case2_train_triples", "]", "\n", "pd", ".", "DataFrame", "(", "case2_train_triples_sg", ",", "columns", "=", "None", ")", ".", "to_csv", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'train_groups.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", "\n", ")", "\n", "\n", "case2_valid_triples", "=", "case2_valid", ".", "values", "\n", "case2_valid_triples_sty", "=", "[", "(", "cui2sty", "[", "h", "]", ",", "r", ",", "cui2sty", "[", "t", "]", ")", "for", "h", ",", "r", ",", "t", "in", "case2_valid_triples", "]", "\n", "pd", ".", "DataFrame", "(", "case2_valid_triples_sty", ",", "columns", "=", "None", ")", ".", "to_csv", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'valid_types.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", "\n", ")", "\n", "case2_valid_triples_sg", "=", "[", "(", "cui2sg", "[", "h", "]", ",", "r", ",", "cui2sg", "[", "t", "]", ")", "for", "h", ",", "r", ",", "t", "in", "case2_valid_triples", "]", "\n", "pd", ".", "DataFrame", "(", "case2_valid_triples_sg", ",", "columns", "=", "None", ")", ".", "to_csv", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'valid_groups.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", "\n", ")", "\n", "\n", "case2_test_triples", "=", "case2_test", ".", "values", "\n", "case2_test_triples_sty", "=", "[", "(", "cui2sty", "[", "h", "]", ",", "r", ",", "cui2sty", "[", "t", "]", ")", "for", "h", ",", "r", ",", "t", "in", "case2_test_triples", "]", "\n", "pd", ".", "DataFrame", "(", "case2_test_triples_sty", ",", "columns", "=", "None", ")", ".", "to_csv", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'test_types.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", "\n", ")", "\n", "case2_test_triples_sg", "=", "[", "(", "cui2sg", "[", "h", "]", ",", "r", ",", "cui2sg", "[", "t", "]", ")", "for", "h", ",", "r", ",", "t", "in", "case2_test_triples", "]", "\n", "pd", ".", "DataFrame", "(", "case2_test_triples_sg", ",", "columns", "=", "None", ")", ".", "to_csv", "(", "\n", "os", ".", "path", ".", "join", "(", "data_dir", ",", "'test_groups.tsv'", ")", ",", "sep", "=", "'\\t'", ",", "header", "=", "None", ",", "index", "=", "None", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.move_unseen_to_train": [[190, 199], ["pandas.concat", "pandas.concat", "set", "set", "valid[].isin", "valid[].isin", "test[].isin", "test[].isin"], "function", ["None"], ["", "def", "move_unseen_to_train", "(", "train", ",", "valid", ",", "test", ")", ":", "\n", "    ", "train_cuis", "=", "set", "(", "train", "[", "'CUI1'", "]", ")", "|", "set", "(", "train", "[", "'CUI2'", "]", ")", "\n", "valid_unseen_idx", "=", "-", "(", "(", "valid", "[", "'CUI1'", "]", ".", "isin", "(", "train_cuis", ")", ")", "&", "(", "valid", "[", "'CUI2'", "]", ".", "isin", "(", "train_cuis", ")", ")", ")", "\n", "train", "=", "pd", ".", "concat", "(", "[", "train", ",", "valid", "[", "valid_unseen_idx", "]", "]", ",", "axis", "=", "0", ")", "\n", "test_unseen_idx", "=", "-", "(", "(", "test", "[", "'CUI1'", "]", ".", "isin", "(", "train_cuis", ")", ")", "&", "(", "test", "[", "'CUI2'", "]", ".", "isin", "(", "train_cuis", ")", ")", ")", "\n", "train", "=", "pd", ".", "concat", "(", "[", "train", ",", "test", "[", "test_unseen_idx", "]", "]", ",", "axis", "=", "0", ")", "\n", "valid", "=", "valid", "[", "-", "valid_unseen_idx", "]", "\n", "test", "=", "test", "[", "-", "test_unseen_idx", "]", "\n", "return", "train", ",", "valid", ",", "test", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.remove_overlapping_pairs": [[201, 223], ["zip", "list", "enumerate", "enumerate", "train.drop", "set", "set", "train[].values.tolist", "train[].values.tolist", "train[].values.tolist", "test[].values.tolist", "list.append", "list.append"], "function", ["None"], ["", "def", "remove_overlapping_pairs", "(", "train", ",", "test", ")", ":", "\n", "    ", "train_pairs", "=", "{", "(", "h", ",", "t", ")", "for", "h", ",", "t", "in", "train", "[", "[", "'CUI1'", ",", "'CUI2'", "]", "]", ".", "values", ".", "tolist", "(", ")", "}", "\n", "train_pairs_inv", "=", "{", "(", "t", ",", "h", ")", "for", "h", ",", "t", "in", "train_pairs", "}", "\n", "\n", "test_pairs", "=", "{", "(", "h", ",", "t", ")", "for", "h", ",", "t", "in", "test", "[", "[", "'CUI1'", ",", "'CUI2'", "]", "]", ".", "values", ".", "tolist", "(", ")", "}", "\n", "test_pairs_inv", "=", "{", "(", "t", ",", "h", ")", "for", "h", ",", "t", "in", "test_pairs", "}", "\n", "\n", "inter_pairs", "=", "train_pairs", "&", "test_pairs", "\n", "inter_pairs_inv", "=", "train_pairs_inv", "&", "test_pairs", "\n", "\n", "pairs_to_remove", "=", "inter_pairs", "|", "inter_pairs_inv", "\n", "heads", ",", "tails", "=", "zip", "(", "*", "pairs_to_remove", ")", "\n", "heads", ",", "tails", "=", "set", "(", "heads", ")", ",", "set", "(", "tails", ")", "\n", "locs", "=", "list", "(", ")", "\n", "for", "idx", ",", "(", "cui1", ",", "cui2", ")", "in", "enumerate", "(", "train", "[", "[", "'CUI1'", ",", "'CUI2'", "]", "]", ".", "values", ".", "tolist", "(", ")", ")", ":", "\n", "        ", "if", "(", "cui1", ",", "cui2", ")", "in", "pairs_to_remove", ":", "\n", "            ", "locs", ".", "append", "(", "idx", ")", "\n", "", "", "for", "idx", ",", "(", "cui2", ",", "cui1", ")", "in", "enumerate", "(", "train", "[", "[", "'CUI2'", ",", "'CUI1'", "]", "]", ".", "values", ".", "tolist", "(", ")", ")", ":", "\n", "        ", "if", "(", "cui2", ",", "cui1", ")", "in", "pairs_to_remove", ":", "\n", "            ", "locs", ".", "append", "(", "idx", ")", "\n", "", "", "train", ".", "drop", "(", "train", ".", "index", "[", "locs", "]", ",", "inplace", "=", "True", ")", "\n", "return", "train", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_transductive_triples_split.check_overlap": [[225, 268], ["print", "print", "print", "print", "print", "print", "print", "train[].values.tolist", "test[].values.tolist", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len", "len"], "function", ["None"], ["", "def", "check_overlap", "(", "train", ",", "test", ",", "name", ")", ":", "\n", "    ", "train_triples", "=", "{", "(", "h", ",", "r", ",", "t", ")", "for", "h", ",", "r", ",", "t", "in", "train", "[", "[", "'CUI1'", ",", "'RELA'", ",", "'CUI2'", "]", "]", ".", "values", ".", "tolist", "(", ")", "}", "\n", "train_triples_inv", "=", "{", "(", "t", ",", "r", ",", "h", ")", "for", "h", ",", "r", ",", "t", "in", "train_triples", "}", "\n", "train_pairs", "=", "{", "(", "h", ",", "t", ")", "for", "h", ",", "_", ",", "t", "in", "train_triples", "}", "\n", "train_pairs_inv", "=", "{", "(", "t", ",", "h", ")", "for", "h", ",", "t", "in", "train_pairs", "}", "\n", "\n", "test_triples", "=", "{", "(", "h", ",", "r", ",", "t", ")", "for", "h", ",", "r", ",", "t", "in", "test", "[", "[", "'CUI1'", ",", "'RELA'", ",", "'CUI2'", "]", "]", ".", "values", ".", "tolist", "(", ")", "}", "\n", "test_triples_inv", "=", "{", "(", "t", ",", "r", ",", "h", ")", "for", "h", ",", "r", ",", "t", "in", "test_triples", "}", "\n", "test_pairs", "=", "{", "(", "h", ",", "t", ")", "for", "h", ",", "_", ",", "t", "in", "test_triples", "}", "\n", "test_pairs_inv", "=", "{", "(", "t", ",", "h", ")", "for", "h", ",", "t", "in", "test_pairs", "}", "\n", "\n", "inter_triples", "=", "train_triples", "&", "test_triples", "\n", "union_triples", "=", "train_triples", "|", "test_triples", "\n", "\n", "inter_triples_inv", "=", "train_triples_inv", "&", "test_triples", "\n", "union_triples_inv", "=", "train_triples_inv", "|", "test_triples", "\n", "\n", "inter_pairs", "=", "train_pairs", "&", "test_pairs", "\n", "inter_pairs_inv", "=", "train_pairs_inv", "&", "test_pairs", "\n", "\n", "triples_to_remove", "=", "inter_triples", "|", "inter_triples_inv", "\n", "pairs_to_remove", "=", "inter_pairs", "|", "inter_pairs_inv", "\n", "\n", "print", "(", "f'Training/{name} intersection size: {len(inter_triples)}'", ")", "\n", "\n", "print", "(", "f'Number of {name} triples in Training: '", "\n", "f'{(len(inter_triples) / len(test_triples)) * 100:.2f}%'", ")", "\n", "\n", "print", "(", "f'Inverse Training/{name} intersection size: {len(inter_triples_inv)}'", ")", "\n", "\n", "print", "(", "f'Number of {name} triples in Inverse Training: '", "\n", "f'{(len(inter_triples_inv) / len(test_triples)) * 100:.2f}%'", ")", "\n", "\n", "print", "(", "f'Number of {name} triples in Training or Inverse Training: '", "\n", "f'{(len(inter_triples | inter_triples_inv) / len(test_triples)) * 100:.2f}%'", ")", "\n", "\n", "print", "(", "f'Number of {name} pairs in Training: '", "\n", "f'{(len(inter_pairs) / len(test_pairs)) * 100:.2f}%'", ")", "\n", "\n", "print", "(", "f'Number of {name} pairs in Inverse Training: '", "\n", "f'{(len(inter_pairs_inv) / len(test_pairs)) * 100:.2f}%'", ")", "\n", "\n", "return", "triples_to_remove", ",", "pairs_to_remove", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_entity_linking.load_scispacy_model": [[34, 64], ["spacy.load", "logger.info", "spacy.load.add_pipe"], "function", ["None"], ["def", "load_scispacy_model", "(", "\n", "scispacy_model_name", ":", "str", "=", "\"en_core_sci_lg\"", ",", "\n", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", "\n", ")", "->", "Language", ":", "\n", "    ", "\"\"\"\n    Parameters\n    ----------\n    \n    scispacy_model_name: `str`, optional, (default = 'en_core_sci_lg')\n        Name of the scispacy model to use for entity linking.\n    cache_dir: `str` or `Path`, optional, (default = None)\n        Path to set up for the scispacy cache directory. \n    \n    Returns\n    -------\n    \n        A spacy language pipeline``Language``.\n    \n    \"\"\"", "\n", "if", "cache_dir", "is", "not", "None", ":", "\n", "        ", "os", ".", "environ", "[", "'SCISPACY_CACHE'", "]", "=", "cache_dir", "\n", "\n", "", "nlp", "=", "spacy", ".", "load", "(", "scispacy_model_name", ")", "\n", "\n", "# We use the defaults set in scispacy see ", "\n", "# https://github.com/allenai/scispacy/blob/main/scispacy/linking.py#L67", "\n", "logger", ".", "info", "(", "'Loading and adding ``UmlsEntityLinker`` to ``nlp.pipe`` ...'", ")", "\n", "nlp", ".", "add_pipe", "(", "'scispacy_linker'", ")", "\n", "\n", "return", "nlp", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_entity_linking.process_linked_entities": [[66, 73], ["None"], "function", ["None"], ["", "def", "process_linked_entities", "(", "doc", ":", "Doc", ")", "->", "List", "[", "Dict", "[", "str", ",", "Union", "[", "str", ",", "Tuple", "[", "int", ",", "int", "]", "]", "]", "]", ":", "\n", "    ", "return", "[", "\n", "[", "\n", "ent", ".", "_", ".", "umls_ents", "[", "0", "]", "[", "0", "]", ",", "# id", "\n", "f'{ent._.umls_ents[0][1]:.2f}'", ",", "# score ", "\n", "[", "ent", ".", "start_char", ",", "ent", ".", "end_char", "]", ",", "# (start, end) pos", "\n", "]", "for", "ent", "in", "doc", ".", "ents", "if", "ent", ".", "_", ".", "umls_ents", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_entity_linking.process_syntactic_features": [[76, 86], ["next", "iter", "next.as_doc().to_json", "next.as_doc"], "function", ["None"], ["", "def", "process_syntactic_features", "(", "doc", ":", "Doc", ")", ":", "\n", "    ", "sent", "=", "next", "(", "iter", "(", "doc", ".", "sents", ")", ")", "# since we are already processing a single sentence", "\n", "return", "[", "\n", "[", "\n", "token", "[", "'tag'", "]", ",", "\n", "token", "[", "'pos'", "]", ",", "\n", "token", "[", "'morph'", "]", ",", "\n", "token", "[", "'dep'", "]", ",", "\n", "token", "[", "'head'", "]", "\n", "]", "for", "token", "in", "sent", ".", "as_doc", "(", ")", ".", "to_json", "(", ")", "[", "'tokens'", "]", "# spacy returns order token", "\n", "]", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_entity_linking.iter_sentences_from_txt": [[89, 102], ["open", "tqdm.tqdm", "sent.strip.strip", "sent.strip.split", "len", "len"], "function", ["None"], ["", "def", "iter_sentences_from_txt", "(", "args", ")", "->", "Generator", "[", "str", ",", "None", ",", "None", "]", ":", "\n", "    ", "with", "open", "(", "args", ".", "medline_unique_sents_fname", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "rf", ":", "\n", "        ", "for", "sent", "in", "tqdm", "(", "rf", ",", "desc", "=", "'Reading sentences ...'", ")", ":", "\n", "            ", "sent", "=", "sent", ".", "strip", "(", ")", "\n", "if", "not", "sent", ":", "\n", "                ", "continue", "\n", "\n", "", "tokens", "=", "sent", ".", "split", "(", ")", "\n", "# Remove too short or too long sentences", "\n", "if", "len", "(", "tokens", ")", "<", "args", ".", "min_sent_tokens", "or", "len", "(", "tokens", ")", ">", "args", ".", "max_sent_tokens", ":", "\n", "                ", "continue", "\n", "\n", "", "yield", "sent", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_entity_linking.main": [[104, 141], ["scispacy_entity_linking.load_scispacy_model", "list", "list", "time.time", "tqdm.tqdm", "logger.info", "scispacy_entity_linking.iter_sentences_from_txt", "load_scispacy_model.pipe", "scispacy_entity_linking.process_linked_entities", "scispacy_entity_linking.process_syntactic_features", "logger.info", "logger.info", "len", "list.append", "time.time", "open", "len", "range", "wf.write", "time.time", "json.dumps", "list.pop"], "function", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_entity_linking.load_scispacy_model", "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_entity_linking.iter_sentences_from_txt", "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_entity_linking.process_linked_entities", "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_entity_linking.process_syntactic_features"], ["", "", "", "def", "main", "(", "args", ")", ":", "\n", "    ", "nlp", "=", "load_scispacy_model", "(", "args", ".", "scispacy_model_name", ",", "args", ".", "cache_dir", ")", "\n", "sents", "=", "list", "(", "iter_sentences_from_txt", "(", "args", ")", ")", "\n", "output_file", "=", "args", ".", "output_file", "\n", "\n", "idx", "=", "0", "\n", "jsonls", "=", "list", "(", ")", "\n", "\n", "t", "=", "time", ".", "time", "(", ")", "\n", "\n", "for", "sent", "in", "tqdm", "(", "nlp", ".", "pipe", "(", "sents", ",", "n_process", "=", "args", ".", "n_process", ",", "batch_size", "=", "args", ".", "batch_size", ")", ")", ":", "\n", "\n", "        ", "if", "idx", "%", "500000", "==", "0", "and", "idx", ">", "0", ":", "\n", "            ", "speed", "=", "idx", "//", "(", "(", "time", ".", "time", "(", ")", "-", "t", ")", "/", "60", ")", "\n", "\n", "logger", ".", "info", "(", "f'Processed {idx} sents @ {speed} sents/min ...'", ")", "\n", "logger", ".", "info", "(", "f'Dumping batch of sentences!'", ")", "\n", "\n", "with", "open", "(", "output_file", ",", "'a'", ")", "as", "wf", ":", "\n", "                ", "count", "=", "len", "(", "jsonls", ")", "\n", "for", "_", "in", "range", "(", "count", ")", ":", "\n", "                    ", "wf", ".", "write", "(", "json", ".", "dumps", "(", "jsonls", ".", "pop", "(", ")", ")", "+", "'\\n'", ")", "# clear out sents list", "\n", "\n", "", "", "", "mentions", "=", "process_linked_entities", "(", "sent", ")", "\n", "features", "=", "process_syntactic_features", "(", "sent", ")", "\n", "\n", "# Consider only mentions with 2 or more (need at least two ents)", "\n", "if", "len", "(", "mentions", ")", ">=", "2", ":", "\n", "            ", "jsonls", ".", "append", "(", "{", "'text'", ":", "sent", ".", "text", ",", "'mentions'", ":", "mentions", ",", "'features'", ":", "features", "}", ")", "\n", "", "else", ":", "\n", "            ", "continue", "\n", "\n", "", "idx", "+=", "1", "\n", "\n", "", "t", "=", "(", "time", ".", "time", "(", ")", "-", "t", ")", "//", "60", "\n", "\n", "logger", ".", "info", "(", "f'Took {t} minutes !'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_inductive_triples_split.parse_triples": [[31, 43], ["collections.Counter", "open", "line.split", "triples.append"], "function", ["None"], ["def", "parse_triples", "(", "triples_file", ")", ":", "\n", "    ", "\"\"\"Read a file containing triples, with head, relation, and tail\n    separated by space. Returns list of lists.\"\"\"", "\n", "triples", "=", "[", "]", "\n", "rel_counts", "=", "Counter", "(", ")", "\n", "file", "=", "open", "(", "triples_file", ")", "\n", "for", "line", "in", "file", ":", "\n", "        ", "head", ",", "rel", ",", "tail", "=", "line", ".", "split", "(", ")", "\n", "triples", ".", "append", "(", "[", "head", ",", "tail", ",", "rel", "]", ")", "\n", "rel_counts", "[", "rel", "]", "+=", "1", "\n", "\n", "", "return", "triples", ",", "rel_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_inductive_triples_split.read_entity_types": [[45, 56], ["set", "collections.defaultdict", "dict", "open", "json.load", "json.load.items", "type2entities[].add"], "function", ["None"], ["", "def", "read_entity_types", "(", "entity2type_file", ",", "keep_ents", "=", "set", "(", ")", ")", ":", "\n", "    ", "type2entities", "=", "defaultdict", "(", "set", ")", "\n", "with", "open", "(", "entity2type_file", ")", "as", "f", ":", "\n", "        ", "entity2type", "=", "json", ".", "load", "(", "f", ")", "\n", "for", "entity", ",", "label", "in", "entity2type", ".", "items", "(", ")", ":", "\n", "            ", "if", "keep_ents", ":", "\n", "                ", "if", "entity", "not", "in", "keep_ents", ":", "\n", "                    ", "continue", "\n", "", "", "type2entities", "[", "label", "]", ".", "add", "(", "entity", ")", "\n", "\n", "", "", "return", "dict", "(", "type2entities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_inductive_triples_split.get_safely_removed_edges": [[58, 100], ["set", "collections.Counter", "networkx.all_neighbors", "set", "networkx.all_neighbors", "len", "range", "graph.get_edge_data", "list", "graph.get_edge_data.values", "reversed", "removed_edges.append"], "function", ["None"], ["", "def", "get_safely_removed_edges", "(", "graph", ",", "node", ",", "rel_counts", ",", "min_edges_left", "=", "100", ")", ":", "\n", "    ", "\"\"\"Get counts of edge removed by type, after safely removing a given node.\n    Safely removing a node entails checking that no nodes are left\n    disconnected, and not removing edge types with count less than\n    a given amount.\n    \"\"\"", "\n", "neighbors", "=", "set", "(", "nx", ".", "all_neighbors", "(", "graph", ",", "node", ")", ")", "\n", "removed_rel_counts", "=", "Counter", "(", ")", "\n", "removed_edges", "=", "[", "]", "\n", "\n", "for", "m", "in", "neighbors", ":", "\n", "# Check if m has more than 2 neighbors (node, and potentially itself)", "\n", "# before continuing", "\n", "        ", "m_neighborhood", "=", "set", "(", "nx", ".", "all_neighbors", "(", "graph", ",", "m", ")", ")", "\n", "if", "len", "(", "m_neighborhood", ")", ">", "2", ":", "\n", "# Check edges in both directions between node and m", "\n", "            ", "pair", "=", "[", "node", ",", "m", "]", "\n", "for", "i", "in", "range", "(", "2", ")", ":", "\n", "                ", "edge_dict", "=", "graph", ".", "get_edge_data", "(", "*", "pair", ")", "\n", "if", "edge_dict", "is", "not", "None", ":", "\n", "# Check that removing the edges between node and m", "\n", "# does not leave less than min_edges_left", "\n", "                    ", "edges", "=", "edge_dict", ".", "values", "(", ")", "\n", "for", "edge", "in", "edges", ":", "\n", "                        ", "rel", "=", "edge", "[", "'weight'", "]", "\n", "edges_left", "=", "rel_counts", "[", "rel", "]", "-", "removed_rel_counts", "[", "rel", "]", "\n", "if", "edges_left", ">=", "min_edges_left", ":", "\n", "                            ", "removed_rel_counts", "[", "rel", "]", "+=", "1", "\n", "head", ",", "tail", "=", "pair", "\n", "removed_edges", ".", "append", "(", "(", "head", ",", "tail", ",", "rel", ")", ")", "\n", "", "else", ":", "\n", "                            ", "return", "None", "\n", "\n", "# Don't count self-loops twice", "\n", "", "", "", "if", "node", "==", "m", ":", "\n", "                    ", "break", "\n", "\n", "", "pair", "=", "list", "(", "reversed", "(", "pair", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "return", "None", "\n", "\n", "", "", "return", "removed_edges", ",", "removed_rel_counts", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_inductive_triples_split.drop_entities": [[102, 221], ["random.seed", "networkx.MultiDiGraph", "extract_inductive_triples_split.parse_triples", "nx.MultiDiGraph.add_weighted_edges_from", "nx.MultiDiGraph.number_of_edges", "nx.MultiDiGraph.number_of_nodes", "logger.info", "dict", "int", "int", "int", "logger.info", "tqdm.tqdm", "tqdm.tqdm.close", "sum", "set", "set", "set", "networkx.MultiDiGraph", "nx.MultiDiGraph.add_weighted_edges_from", "os.dirname", "zip", "logger.info", "logger.info", "logger.info", "ValueError", "extract_inductive_triples_split.read_entity_types", "list", "len", "extract_inductive_triples_split.get_safely_removed_edges", "len", "map", "nx.MultiDiGraph.nodes", "len", "len", "len", "len", "open", "nx.MultiDiGraph.edges", "set", "read_entity_types.keys", "random.choice", "random.choice", "nx.MultiDiGraph.remove_node", "dropped_entities.append", "rel_counts.subtract", "tqdm.tqdm.update", "list", "dict.values", "nx.MultiDiGraph.number_of_edges", "set.intersection", "set.intersection", "set.intersection", "set().intersection", "open", "os.join", "train_file.write", "nx.MultiDiGraph.number_of_nodes", "nx.MultiDiGraph.number_of_edges", "random.choices", "list", "list", "type2entities[].remove", "networkx.isolates", "os.join", "len", "len", "nx.MultiDiGraph.number_of_nodes", "len", "set", "file.write", "nx.MultiDiGraph.nodes"], "function", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_inductive_triples_split.parse_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_inductive_triples_split.read_entity_types", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_inductive_triples_split.get_safely_removed_edges"], ["", "def", "drop_entities", "(", "triples_file", ",", "train_size", "=", "0.7", ",", "valid_size", "=", "0.1", ",", "test_size", "=", "0.2", ",", "\n", "seed", "=", "0", ",", "types_file", "=", "None", ",", "has_def", "=", "False", ")", ":", "\n", "    ", "\"\"\"Drop entities from a graph, to create training, validation and test\n    splits.\n    Entities are dropped so that no disconnected nodes are left in the training\n    graph. Dropped entities are distributed between disjoint validation\n    and test sets.\n    \"\"\"", "\n", "splits_sum", "=", "train_size", "+", "valid_size", "+", "test_size", "\n", "if", "splits_sum", "<", "0", "or", "splits_sum", ">", "1", ":", "\n", "        ", "raise", "ValueError", "(", "'Sum of split sizes must be between greater than 0'", "\n", "' and less than or equal to 1.'", ")", "\n", "\n", "", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "graph", "=", "nx", ".", "MultiDiGraph", "(", ")", "\n", "triples", ",", "rel_counts", "=", "parse_triples", "(", "triples_file", ")", "\n", "graph", ".", "add_weighted_edges_from", "(", "triples", ")", "\n", "original_num_edges", "=", "graph", ".", "number_of_edges", "(", ")", "\n", "original_num_nodes", "=", "graph", ".", "number_of_nodes", "(", ")", "\n", "\n", "use_types", "=", "types_file", "is", "not", "None", "\n", "if", "use_types", ":", "\n", "        ", "type2entities", "=", "read_entity_types", "(", "types_file", ",", "set", "(", "graph", ".", "nodes", ")", ")", "\n", "types", "=", "list", "(", "type2entities", ".", "keys", "(", ")", ")", "\n", "\n", "", "logger", ".", "info", "(", "f'Loaded graph with {graph.number_of_nodes():,} entities '", "\n", "f'and {graph.number_of_edges():,} edges'", ")", "\n", "\n", "dropped_entities", "=", "[", "]", "\n", "dropped_edges", "=", "dict", "(", ")", "\n", "num_to_drop", "=", "int", "(", "original_num_nodes", "*", "(", "1", "-", "train_size", ")", ")", "\n", "num_val", "=", "int", "(", "original_num_nodes", "*", "valid_size", ")", "\n", "num_test", "=", "int", "(", "original_num_nodes", "*", "test_size", ")", "\n", "\n", "logger", ".", "info", "(", "f'Removing {num_to_drop:,} entities...'", ")", "\n", "progress", "=", "tqdm", "(", "total", "=", "num_to_drop", ",", "file", "=", "sys", ".", "stdout", ")", "\n", "while", "len", "(", "dropped_entities", ")", "<", "num_to_drop", ":", "\n", "        ", "if", "use_types", ":", "\n", "# Sample an entity with probability proportional to its type count", "\n", "# (minus 1 to keep at least one entity of any type)", "\n", "            ", "weights", "=", "[", "len", "(", "type2entities", "[", "t", "]", ")", "-", "1", "for", "t", "in", "types", "]", "\n", "rand_type", "=", "random", ".", "choices", "(", "types", ",", "weights", ",", "k", "=", "1", ")", "[", "0", "]", "\n", "rand_ent", "=", "random", ".", "choice", "(", "list", "(", "type2entities", "[", "rand_type", "]", ")", ")", "\n", "", "else", ":", "\n", "# Sample an entity uniformly at random", "\n", "            ", "rand_ent", "=", "random", ".", "choice", "(", "list", "(", "graph", ".", "nodes", ")", ")", "\n", "\n", "", "removed_tuple", "=", "get_safely_removed_edges", "(", "graph", ",", "rand_ent", ",", "rel_counts", ")", "\n", "\n", "if", "removed_tuple", "is", "not", "None", ":", "\n", "            ", "removed_edges", ",", "removed_counts", "=", "removed_tuple", "\n", "dropped_edges", "[", "rand_ent", "]", "=", "removed_edges", "\n", "graph", ".", "remove_node", "(", "rand_ent", ")", "\n", "dropped_entities", ".", "append", "(", "rand_ent", ")", "\n", "rel_counts", ".", "subtract", "(", "removed_counts", ")", "\n", "\n", "if", "use_types", ":", "\n", "                ", "type2entities", "[", "rand_type", "]", ".", "remove", "(", "rand_ent", ")", "\n", "\n", "", "progress", ".", "update", "(", "1", ")", "\n", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n", "# Are there indeed no disconnected nodes?", "\n", "assert", "len", "(", "list", "(", "nx", ".", "isolates", "(", "graph", ")", ")", ")", "==", "0", "\n", "\n", "# Did we keep track of removed edges correctly?", "\n", "num_removed_edges", "=", "sum", "(", "map", "(", "len", ",", "dropped_edges", ".", "values", "(", ")", ")", ")", "\n", "assert", "num_removed_edges", "+", "graph", ".", "number_of_edges", "(", ")", "==", "original_num_edges", "\n", "\n", "# Test entities MUST come from first slice! This guarantees that", "\n", "# validation entities don't have edges with them (because nodes were", "\n", "# removed in sequence)", "\n", "test_ents", "=", "set", "(", "dropped_entities", "[", ":", "num_test", "]", ")", "\n", "val_ents", "=", "set", "(", "dropped_entities", "[", "num_test", ":", "num_test", "+", "num_val", "]", ")", "\n", "train_ents", "=", "set", "(", "graph", ".", "nodes", "(", ")", ")", "\n", "\n", "# Check that entity sets are disjoint", "\n", "assert", "len", "(", "train_ents", ".", "intersection", "(", "val_ents", ")", ")", "==", "0", "\n", "assert", "len", "(", "train_ents", ".", "intersection", "(", "test_ents", ")", ")", "==", "0", "\n", "assert", "len", "(", "val_ents", ".", "intersection", "(", "test_ents", ")", ")", "==", "0", "\n", "\n", "# Check that validation graph does not contain test entities", "\n", "val_graph", "=", "nx", ".", "MultiDiGraph", "(", ")", "\n", "val_edges", "=", "[", "]", "\n", "for", "entity", "in", "val_ents", ":", "\n", "        ", "val_edges", "+=", "dropped_edges", "[", "entity", "]", "\n", "", "val_graph", ".", "add_weighted_edges_from", "(", "val_edges", ")", "\n", "assert", "len", "(", "set", "(", "val_graph", ".", "nodes", "(", ")", ")", ".", "intersection", "(", "test_ents", ")", ")", "==", "0", "\n", "\n", "names", "=", "(", "'train'", ",", "'dev'", ",", "'test'", ")", "\n", "\n", "dirname", "=", "osp", ".", "dirname", "(", "triples_file", ")", "\n", "prefix_type", "=", "'_type'", "if", "use_types", "else", "''", "\n", "prefix_def", "=", "'_def'", "if", "has_def", "else", "''", "\n", "prefix", "=", "f'ind{prefix_type}{prefix_def}-'", "\n", "\n", "for", "entity_set", ",", "set_name", "in", "zip", "(", "(", "train_ents", ",", "val_ents", ",", "test_ents", ")", ",", "names", ")", ":", "\n", "\n", "        ", "if", "set_name", "==", "'train'", ":", "\n", "# Triples for train split are saved later", "\n", "            ", "continue", "\n", "\n", "# Save file with triples for entities in set", "\n", "", "with", "open", "(", "osp", ".", "join", "(", "dirname", ",", "f'{prefix}{set_name}.tsv'", ")", ",", "'w'", ")", "as", "file", ":", "\n", "            ", "for", "entity", "in", "entity_set", ":", "\n", "                ", "triples", "=", "dropped_edges", "[", "entity", "]", "\n", "for", "head", ",", "tail", ",", "rel", "in", "triples", ":", "\n", "                    ", "file", ".", "write", "(", "f'{head}\\t{rel}\\t{tail}\\n'", ")", "\n", "\n", "", "", "", "", "with", "open", "(", "osp", ".", "join", "(", "dirname", ",", "f'{prefix}train.tsv'", ")", ",", "'w'", ")", "as", "train_file", ":", "\n", "        ", "for", "head", ",", "tail", ",", "rel", "in", "graph", ".", "edges", "(", "data", "=", "True", ")", ":", "\n", "            ", "train_file", ".", "write", "(", "f'{head}\\t{rel[\"weight\"]}\\t{tail}\\n'", ")", "\n", "\n", "", "", "logger", ".", "info", "(", "f'Dropped {len(val_ents):,} entities for validation'", "\n", "f' and {len(test_ents):,} for test.'", ")", "\n", "logger", ".", "info", "(", "f'{graph.number_of_nodes():,} entities are left for training.'", ")", "\n", "logger", ".", "info", "(", "f'Saved output files to {dirname}/'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_cui_definitions.load_umls_linker": [[22, 28], ["logger.info", "scispacy.umls_linking.UmlsEntityLinker"], "function", ["None"], ["def", "load_umls_linker", "(", "cache_dir", "=", "None", ")", ":", "\n", "    ", "if", "cache_dir", "is", "not", "None", ":", "\n", "        ", "os", ".", "environ", "[", "'SCISPACY_CACHE'", "]", "=", "cache_dir", "\n", "", "logger", ".", "info", "(", "'Loading ``UmlsEntityLinker`` ...'", ")", "\n", "linker", "=", "UmlsEntityLinker", "(", "name", "=", "'scispacy_linker'", ")", "\n", "return", "linker", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_cui_definitions.load_cuis": [[30, 34], ["sorted", "open", "json.load", "json.load.keys", "pathlib.Path"], "function", ["None"], ["", "def", "load_cuis", "(", "umls_dir", ")", ":", "\n", "    ", "with", "open", "(", "Path", "(", "umls_dir", ")", "/", "'cui2string.json'", ")", "as", "rf", ":", "\n", "        ", "cui2string", "=", "json", ".", "load", "(", "rf", ")", "\n", "", "return", "sorted", "(", "cui2string", ".", "keys", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_cui_definitions.search_definitions": [[36, 58], ["logger.info", "dict", "logger.info", "logger.info", "logger.info", "len", "re.sub", "len", "len"], "function", ["None"], ["", "def", "search_definitions", "(", "linker", ",", "cuis", ")", ":", "\n", "    ", "logger", ".", "info", "(", "f'Searching for definitions of {len(cuis)} CUIs ...'", ")", "\n", "missing_cuis_in_db", "=", "0", "\n", "cuis_in_db_missing_definitions", "=", "0", "\n", "cui2def", "=", "dict", "(", ")", "\n", "for", "cui", "in", "cuis", ":", "\n", "        ", "if", "cui", "in", "linker", ".", "kb", ".", "cui_to_entity", ":", "\n", "            ", "items", "=", "linker", ".", "kb", ".", "cui_to_entity", "[", "cui", "]", "\n", "definition", "=", "items", "[", "4", "]", "\n", "if", "not", "definition", ":", "\n", "                ", "cuis_in_db_missing_definitions", "+=", "1", "\n", "continue", "\n", "", "else", ":", "\n", "                ", "definition", "=", "re", ".", "sub", "(", "r'\\s+'", ",", "' '", ",", "definition", ")", "\n", "cui2def", "[", "cui", "]", "=", "definition", "\n", "", "", "else", ":", "\n", "            ", "missing_cuis_in_db", "+=", "1", "\n", "continue", "\n", "", "", "logger", ".", "info", "(", "f'Found definitions for {len(cui2def)} / {len(cuis)} CUIs'", ")", "\n", "logger", ".", "info", "(", "f'CUIs missing in UMLS2020AA db = {missing_cuis_in_db}'", ")", "\n", "logger", ".", "info", "(", "f'CUIs found in UMLS2020AA db but missing definition = {cuis_in_db_missing_definitions}'", ")", "\n", "return", "cui2def", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_cui_definitions.create_triples": [[60, 76], ["list", "logger.info", "logger.info", "open", "open", "line.strip.strip", "line.strip.split", "pathlib.Path", "pathlib.Path", "wf.write"], "function", ["None"], ["", "def", "create_triples", "(", "umls_dir", ",", "cui2def", ")", ":", "\n", "    ", "all_triples_with_def", "=", "list", "(", ")", "\n", "total", "=", "0", "\n", "with_def", "=", "0", "\n", "logger", ".", "info", "(", "f'Subsetting the triples that have definitions ...'", ")", "\n", "with", "open", "(", "Path", "(", "umls_dir", ")", "/", "'all-triples.tsv'", ")", "as", "rf", ",", "open", "(", "Path", "(", "umls_dir", ")", "/", "'all-triples_with-def.tsv'", ",", "'w'", ")", "as", "wf", ":", "\n", "        ", "for", "line", "in", "rf", ":", "\n", "            ", "line", "=", "line", ".", "strip", "(", ")", "\n", "if", "not", "line", ":", "\n", "                ", "continue", "\n", "", "h", ",", "r", ",", "t", "=", "line", ".", "split", "(", "'\\t'", ")", "\n", "total", "+=", "1", "\n", "if", "h", "in", "cui2def", "and", "t", "in", "cui2def", ":", "\n", "                ", "wf", ".", "write", "(", "line", "+", "'\\n'", ")", "\n", "with_def", "+=", "1", "\n", "", "", "", "logger", ".", "info", "(", "f'Found triples with definitions {with_def} out of {total}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_cui_definitions.main": [[78, 84], ["extract_cui_definitions.search_definitions", "extract_cui_definitions.create_triples", "extract_cui_definitions.load_umls_linker", "extract_cui_definitions.load_cuis", "open", "sorted", "search_definitions.items", "wf.write", "pathlib.Path"], "function", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_cui_definitions.search_definitions", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_cui_definitions.create_triples", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_cui_definitions.load_umls_linker", "home.repos.pwc.inspect_result.suamin_meddistant19.None.extract_cui_definitions.load_cuis"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "cui2def", "=", "search_definitions", "(", "load_umls_linker", "(", "args", ".", "cache_dir", ")", ",", "load_cuis", "(", "args", ".", "umls_dir", ")", ")", "\n", "with", "open", "(", "Path", "(", "args", ".", "umls_dir", ")", "/", "'cui2def.txt'", ",", "'w'", ")", "as", "wf", ":", "\n", "        ", "for", "cui", ",", "defi", "in", "sorted", "(", "cui2def", ".", "items", "(", ")", ")", ":", "\n", "            ", "wf", ".", "write", "(", "f'{cui}\\t{defi}\\n'", ")", "\n", "", "", "create_triples", "(", "args", ".", "umls_dir", ",", "cui2def", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.init_config_prop.main": [[7, 28], ["open", "os.path.join", "os.path.join", "os.path.join", "open", "line.strip", "wf.write", "rf.readlines", "config_prop_lines[].split", "config_prop_lines[].split", "config_prop_lines[].split", "config_prop_lines[].split", "config_prop_lines[].split", "config_prop_lines[].split"], "function", ["None"], ["def", "main", "(", "args", ")", ":", "\n", "\n", "    ", "with", "open", "(", "args", ".", "config_prop_file", ")", "as", "rf", ":", "\n", "        ", "config_prop_lines", "=", "[", "line", ".", "strip", "(", ")", "for", "line", "in", "rf", ".", "readlines", "(", ")", "]", "\n", "\n", "# Line 9", "\n", "", "config_prop_lines", "[", "8", "]", "=", "config_prop_lines", "[", "8", "]", ".", "split", "(", "'='", ")", "[", "0", "]", "+", "'='", "+", "os", ".", "path", ".", "join", "(", "args", ".", "dst", ",", "'2019AB'", ",", "'META'", ")", "\n", "# Line 17", "\n", "config_prop_lines", "[", "16", "]", "=", "config_prop_lines", "[", "16", "]", ".", "split", "(", "'='", ")", "[", "0", "]", "+", "'='", "+", "os", ".", "path", ".", "join", "(", "args", ".", "dst", ",", "'2019AB'", ",", "'META'", ")", "\n", "# Line 19", "\n", "config_prop_lines", "[", "18", "]", "=", "config_prop_lines", "[", "18", "]", ".", "split", "(", "'='", ")", "[", "0", "]", "+", "'='", "+", "os", ".", "path", ".", "join", "(", "args", ".", "src", ",", "'config'", ",", "'2019AB'", ",", "'user.c.prop'", ")", "\n", "# Line 25", "\n", "config_prop_lines", "[", "24", "]", "=", "config_prop_lines", "[", "24", "]", ".", "split", "(", "'='", ")", "[", "0", "]", "+", "'='", "+", "args", ".", "src", "\n", "# Line 32", "\n", "config_prop_lines", "[", "31", "]", "=", "config_prop_lines", "[", "31", "]", ".", "split", "(", "'='", ")", "[", "0", "]", "+", "'='", "+", "args", ".", "dst", "\n", "# Line 38", "\n", "config_prop_lines", "[", "37", "]", "=", "config_prop_lines", "[", "37", "]", ".", "split", "(", "'='", ")", "[", "0", "]", "+", "'='", "+", "args", ".", "src", "\n", "\n", "with", "open", "(", "args", ".", "config_prop_file", ",", "'w'", ")", "as", "wf", ":", "\n", "        ", "for", "line", "in", "config_prop_lines", ":", "\n", "            ", "wf", ".", "write", "(", "line", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_tokenization.AbstractsCorpus.__init__": [[26, 28], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data_dir", ":", "str", ")", ":", "\n", "        ", "self", ".", "data_dir", "=", "data_dir", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_tokenization.AbstractsCorpus.__iter__": [[29, 34], ["tqdm.tqdm.tqdm", "os.listdir", "fname.endswith", "pathlib.Path", "scispacy_tokenization.AbstractsCorpus.iter_abstracts_from_xml_gz_txt"], "methods", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_tokenization.AbstractsCorpus.iter_abstracts_from_xml_gz_txt"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "fname", "in", "tqdm", "(", "os", ".", "listdir", "(", "self", ".", "data_dir", ")", ",", "desc", "=", "'Reading abstract texts from *.xml.gz.txt files ...'", ")", ":", "\n", "            ", "if", "fname", ".", "endswith", "(", "'.xml.gz.txt'", ")", ":", "\n", "                ", "fname", "=", "Path", "(", "self", ".", "data_dir", ")", "/", "fname", "\n", "yield", "from", "self", ".", "iter_abstracts_from_xml_gz_txt", "(", "fname", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_tokenization.AbstractsCorpus.iter_abstracts_from_xml_gz_txt": [[35, 44], ["open", "abstract.strip.strip.strip"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "iter_abstracts_from_xml_gz_txt", "(", "fname", ":", "str", ")", "->", "Generator", "[", "str", ",", "None", ",", "None", "]", ":", "\n", "        ", "\"\"\"Read an *.xml.gz.txt file and return abstract texts stored per line.\"\"\"", "\n", "with", "open", "(", "fname", ",", "encoding", "=", "'utf-8'", ",", "errors", "=", "'ignore'", ")", "as", "rf", ":", "\n", "            ", "for", "abstract", "in", "rf", ":", "\n", "                ", "abstract", "=", "abstract", ".", "strip", "(", ")", "\n", "if", "not", "abstract", ":", "\n", "                    ", "continue", "\n", "", "yield", "abstract", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_tokenization.process_doc": [[46, 48], ["None"], "function", ["None"], ["", "", "", "", "def", "process_doc", "(", "doc", ":", "Doc", ")", "->", "List", "[", "str", "]", ":", "\n", "    ", "return", "[", "' '", ".", "join", "(", "[", "tok", ".", "text", "for", "tok", "in", "sent", "]", ")", "for", "sent", "in", "doc", ".", "sents", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_tokenization.main": [[50, 93], ["spacy.load", "iter", "list", "os.listdir", "time.time", "tqdm.tqdm", "logger.info", "scispacy_tokenization.AbstractsCorpus", "pathlib.Path", "fname.endswith", "spacy.load.pipe", "scispacy_tokenization.process_doc", "logger.info", "logger.info", "list.append", "time.time", "open", "len", "range", "wf.write", "time.time", "list.pop"], "function", ["home.repos.pwc.inspect_result.suamin_meddistant19.None.scispacy_tokenization.process_doc"], ["", "def", "main", "(", "args", ")", ":", "\n", "    ", "nlp", "=", "spacy", ".", "load", "(", "args", ".", "scispacy_model_name", ")", "\n", "abstracts", "=", "iter", "(", "AbstractsCorpus", "(", "args", ".", "data_dir", ")", ")", "\n", "output_file", "=", "Path", "(", "args", ".", "data_dir", ")", "/", "'medline_pubmed_2019_sents.txt'", "\n", "idx", "=", "0", "\n", "sents", "=", "list", "(", ")", "\n", "num_sents", "=", "0", "\n", "\n", "total", "=", "0", "\n", "for", "fname", "in", "os", ".", "listdir", "(", "args", ".", "data_dir", ")", ":", "\n", "        ", "if", "fname", ".", "endswith", "(", "'.xml.gz.txt'", ")", ":", "total", "+=", "1", "\n", "\n", "", "t", "=", "time", ".", "time", "(", ")", "\n", "\n", "# ---------------------------------------------------------------------------------------", "\n", "# WARNING: depending on ``n_process`` and ``batch_size`` selection, multi-processing", "\n", "#          can be worse than sequential processing. One has to play around a bit with the", "\n", "#          system before it finds the right combination. There is no one size fits all!", "\n", "#", "\n", "# more here: https://spacy.io/usage/processing-pipelines#multiprocessing", "\n", "# ---------------------------------------------------------------------------------------", "\n", "for", "doc", "in", "tqdm", "(", "nlp", ".", "pipe", "(", "abstracts", ",", "n_process", "=", "args", ".", "n_process", ",", "batch_size", "=", "args", ".", "batch_size", ")", ")", ":", "\n", "\n", "        ", "if", "idx", "%", "100000", "==", "0", "and", "idx", ">", "0", ":", "\n", "\n", "            ", "speed", "=", "idx", "//", "(", "(", "time", ".", "time", "(", ")", "-", "t", ")", "/", "60", ")", "\n", "\n", "logger", ".", "info", "(", "f'Processed {idx} abstracts from {total} pooled abstract files @ {speed} abstracts/min ...'", ")", "\n", "logger", ".", "info", "(", "f'Dumping batch of sentences!'", ")", "\n", "\n", "with", "open", "(", "output_file", ",", "'a'", ")", "as", "wf", ":", "\n", "                ", "count", "=", "len", "(", "sents", ")", "\n", "num_sents", "+=", "count", "\n", "for", "_", "in", "range", "(", "count", ")", ":", "\n", "                    ", "wf", ".", "write", "(", "sents", ".", "pop", "(", ")", "+", "'\\n'", ")", "# clear out sents list", "\n", "\n", "", "", "", "for", "sent", "in", "process_doc", "(", "doc", ")", ":", "\n", "            ", "sents", ".", "append", "(", "sent", ")", "\n", "", "idx", "+=", "1", "\n", "\n", "", "t", "=", "(", "time", ".", "time", "(", ")", "-", "t", ")", "//", "60", "\n", "\n", "logger", ".", "info", "(", "f'Took {t} minutes and collected {num_sents} sentences !'", ")", "\n", "\n"]]}