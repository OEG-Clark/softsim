{"home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.make_env.make_env": [[6, 9], ["Touchdown.TouchdownEnv"], "function", ["None"], ["def", "make_env", "(", ")", ":", "\n", "\t", "return", "TouchdownEnv", "(", "3", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "\n", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.Touchdown.TouchdownEnv.__init__": [[20, 78], ["range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "len", "len"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "team_size", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", ":", "\n", "\n", "\t\t", "self", ".", "team_size", "=", "team_size", "\n", "self", ".", "blue_obs", "=", "blue_obs", "\n", "self", ".", "blue_actions", "=", "blue_actions", "\n", "self", ".", "red_obs", "=", "red_obs", "\n", "self", ".", "red_actions", "=", "red_actions", "\n", "\n", "self", ".", "start_pos", "=", "0.9", "\n", "self", ".", "capture_radius", "=", "0.1", "\n", "self", ".", "player_movement", "=", "0.1", "\n", "\n", "#track player movement in a normalized space", "\n", "self", ".", "blue_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "#obs spaces", "\n", "self", ".", "observation_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "blue_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "#action spaces", "\n", "", "", "self", ".", "action_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.Touchdown.TouchdownEnv.dist": [[80, 84], ["math.sqrt"], "methods", ["None"], ["", "", "", "def", "dist", "(", "self", ",", "p1", ",", "p2", ")", ":", "\n", "\t\t", "dx", "=", "p1", "[", "0", "]", "-", "p2", "[", "0", "]", "\n", "dy", "=", "p1", "[", "1", "]", "-", "p2", "[", "1", "]", "\n", "return", "math", ".", "sqrt", "(", "dx", "*", "dx", "+", "dy", "*", "dy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.Touchdown.TouchdownEnv.reset": [[86, 106], ["range", "range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "len", "states.append", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\n", "\t\t", "self", ".", "blue_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "self", ".", "epstep", "=", "0", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n", "\n", "", "return", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.Touchdown.TouchdownEnv.step": [[108, 147], ["range", "range", "len", "Touchdown.TouchdownEnv.step_player", "len", "states.append", "Touchdown.TouchdownEnv.dist", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.step_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.dist", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "\n", "\t\t", "self", ".", "epstep", "+=", "1", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "actions", ")", ")", ":", "\n", "\t\t\t", "self", ".", "step_player", "(", "actions", "[", "i", "]", ",", "i", ")", "\n", "\n", "#check for collisions", "\n", "", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t\t", "d", "=", "self", ".", "dist", "(", "bp", ",", "rp", ")", "\n", "if", "d", "<", "self", ".", "capture_radius", ":", "\n", "\t\t\t\t\t", "bp", "[", "0", "]", "=", "0.0", "\n", "bp", "[", "1", "]", "=", "self", ".", "start_pos", "\n", "rp", "[", "0", "]", "=", "0.0", "\n", "rp", "[", "1", "]", "=", "-", "self", ".", "start_pos", "\n", "\n", "#check for end-of-game", "\n", "", "", "", "done", "=", "False", "\n", "blue_reward", "=", "0.0", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "bp", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "+=", "1.0", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "rp", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "-=", "1.0", "\n", "\n", "", "", "done", "=", "done", "or", "self", ".", "epstep", ">=", "1500", "\n", "rewards", "=", "[", "blue_reward", "for", "p", "in", "self", ".", "blue_team", "]", "+", "[", "-", "blue_reward", "for", "p", "in", "self", ".", "red_team", "]", "\n", "infos", "=", "[", "{", "}", "for", "p", "in", "self", ".", "all_players", "]", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n", "\n", "", "return", "states", ",", "rewards", ",", "done", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.Touchdown.TouchdownEnv.step_player": [[149, 207], ["None"], "methods", ["None"], ["", "def", "step_player", "(", "self", ",", "action", ",", "idx", ")", ":", "\n", "\n", "\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "#blue player", "\n", "\t\t\t", "if", "self", ".", "blue_actions", "==", "\"discrete\"", ":", "\n", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "self", ".", "player_movement", "\n", "\n", "", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "self", ".", "player_movement", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "self", ".", "player_movement", "\n", "\n", "#blue player cannot exceed y=1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "1.0", "\n", "\n", "", "", "else", ":", "\n", "#red player - actions are all reversed", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "#red player cannot preceed y=-1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "-", "1.0", "\n", "\n", "\n", "#check for left-right walls", "\n", "", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", ">", "1.0", ":", "\n", "\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "<", "-", "1.0", ":", "\n", "\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "-", "1.0", "\n", "\n", "\n", "", "self", ".", "blue_team", "=", "self", ".", "all_players", "[", ":", "self", ".", "team_size", "]", "\n", "self", ".", "red_team", "=", "self", ".", "all_players", "[", "self", ".", "team_size", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.Touchdown.TouchdownEnv.render": [[209, 233], ["numpy.flip", "cv2.namedWindow", "cv2.imshow", "cv2.waitKey", "numpy.ones", "int", "int", "cv2.circle", "int", "int", "cv2.circle"], "methods", ["None"], ["", "def", "render", "(", "self", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "ones", "(", "(", "300", ",", "300", ",", "3", ")", ",", "np", ".", "uint8", ")", "*", "255.0", "\n", "\n", "#draw all blue", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "(", "0", ",", "0", ",", "255.0", ")", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "(", "255.0", ",", "0", ",", "0", ")", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n", "#we actually are drawing in BGR, so flip last axis", "\n", "", "img", "=", "np", ".", "flip", "(", "img", ",", "axis", "=", "-", "1", ")", "\n", "\n", "cv2", ".", "namedWindow", "(", "\"img\"", ",", "cv2", ".", "WINDOW_NORMAL", ")", "\n", "cv2", ".", "imshow", "(", "\"img\"", ",", "img", ")", "\n", "cv2", ".", "waitKey", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.Touchdown.TouchdownEnv.get_state_for_player": [[235, 250], ["Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player"], ["", "def", "get_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n", "", "elif", "self", ".", "blue_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.Touchdown.TouchdownEnv.get_vector_state_for_player": [[252, 276], ["range", "numpy.asarray", "len"], "methods", ["None"], ["", "", "", "def", "get_vector_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "self_state", "=", "[", "]", "\n", "ally_states", "=", "[", "]", "\n", "enemy_states", "=", "[", "]", "\n", "\n", "for", "idx", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\n", "\t\t\t", "if", "idx", "==", "entity_idx", ":", "\n", "\t\t\t\t", "self_state", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "else", ":", "\n", "\t\t\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "", "", "", "state", "=", "self_state", "+", "ally_states", "+", "enemy_states", "\n", "return", "np", ".", "asarray", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_multi_agent.Touchdown.TouchdownEnv.get_image_state_for_player": [[278, 309], ["numpy.zeros", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "numpy.flipud", "numpy.fliplr"], "methods", ["None"], ["", "def", "get_image_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "zeros", "(", "(", "84", ",", "84", ",", "1", ")", ",", "np", ".", "int8", ")", "\n", "\n", "#draw all blue", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", "<", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", ">=", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#now re-draw this entity", "\n", "", "color", "=", "2.0", "\n", "p", "=", "self", ".", "all_players", "[", "entity_idx", "]", "\n", "pix_x", "=", "int", "(", "(", "p", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "p", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#flip", "\n", "if", "entity_idx", ">=", "self", ".", "team_size", ":", "\n", "\t\t\t", "img", "=", "np", ".", "flipud", "(", "img", ")", "\n", "img", "=", "np", ".", "fliplr", "(", "img", ")", "\n", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.atari.make_env.make_env": [[7, 15], ["stable_baselines.common.atari_wrappers.wrap_deepmind", "single_agent_wrapper.seed", "single_agent_wrapper", "stable_baselines.common.atari_wrappers.make_atari", "mpi4py.MPI.COMM_WORLD.Get_rank"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed"], ["\t", "return", "TouchdownEnv", "(", "3", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "\n", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.make_env.make_env": [[6, 9], ["Touchdown.TouchdownEnv"], "function", ["None"], ["def", "make_env", "(", ")", ":", "\n", "\t", "return", "TouchdownEnv", "(", "3", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "\n", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.Touchdown.TouchdownEnv.__init__": [[20, 82], ["range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "len", "len"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "team_size", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", ":", "\n", "\n", "\t\t", "self", ".", "team_size", "=", "team_size", "\n", "self", ".", "blue_obs", "=", "blue_obs", "\n", "self", ".", "blue_actions", "=", "blue_actions", "\n", "self", ".", "red_obs", "=", "red_obs", "\n", "self", ".", "red_actions", "=", "red_actions", "\n", "\n", "self", ".", "start_pos", "=", "0.9", "\n", "self", ".", "capture_radius", "=", "0.1", "\n", "self", ".", "player_movement", "=", "0.1", "\n", "\n", "#track player movement in a normalized space", "\n", "self", ".", "blue_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "#obs spaces", "\n", "self", ".", "observation_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "blue_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "#action spaces", "\n", "", "", "self", ".", "action_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "\n", "", "", "", "def", "dist", "(", "self", ",", "p1", ",", "p2", ")", ":", "\n", "\t\t", "dx", "=", "p1", "[", "0", "]", "-", "p2", "[", "0", "]", "\n", "dy", "=", "p1", "[", "1", "]", "-", "p2", "[", "1", "]", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.Touchdown.TouchdownEnv.dist": [[84, 88], ["math.sqrt"], "methods", ["None"], ["\n", "\n", "", "def", "reset", "(", "self", ")", ":", "\n", "\n", "\t\t", "self", ".", "blue_team", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.Touchdown.TouchdownEnv.reset": [[90, 110], ["range", "range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "len", "states.append", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "self", ".", "epstep", "=", "0", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n", "\n", "", "return", "states", "\n", "\n", "\n", "", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "\n", "\t\t", "self", ".", "epstep", "+=", "1", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.Touchdown.TouchdownEnv.step": [[112, 152], ["range", "range", "len", "Touchdown.TouchdownEnv.step_player", "len", "states.append", "Touchdown.TouchdownEnv.dist", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.step_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.dist", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["for", "i", "in", "range", "(", "len", "(", "actions", ")", ")", ":", "\n", "\t\t\t", "self", ".", "step_player", "(", "actions", "[", "i", "]", ",", "i", ")", "\n", "\n", "#check for collisions", "\n", "", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t\t", "d", "=", "self", ".", "dist", "(", "bp", ",", "rp", ")", "\n", "if", "d", "<", "self", ".", "capture_radius", ":", "\n", "\t\t\t\t\t", "bp", "[", "0", "]", "=", "0.0", "\n", "bp", "[", "1", "]", "=", "self", ".", "start_pos", "\n", "rp", "[", "0", "]", "=", "0.0", "\n", "rp", "[", "1", "]", "=", "-", "self", ".", "start_pos", "\n", "\n", "#check for end-of-game", "\n", "", "", "", "done", "=", "False", "\n", "blue_reward", "=", "0.0", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "bp", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "+=", "1.0", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "rp", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "-=", "1.0", "\n", "\n", "", "", "done", "=", "done", "or", "self", ".", "epstep", ">=", "1500", "\n", "rewards", "=", "[", "blue_reward", "for", "p", "in", "self", ".", "blue_team", "]", "+", "[", "-", "blue_reward", "for", "p", "in", "self", ".", "red_team", "]", "\n", "infos", "=", "[", "{", "}", "for", "p", "in", "self", ".", "all_players", "]", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n", "\n", "", "return", "states", ",", "rewards", ",", "done", ",", "infos", "\n", "\n", "\n", "", "def", "step_player", "(", "self", ",", "action", ",", "idx", ")", ":", "\n", "\n", "\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "#blue player", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.Touchdown.TouchdownEnv.step_player": [[154, 212], ["None"], "methods", ["None"], ["\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "self", ".", "player_movement", "\n", "\n", "", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "self", ".", "player_movement", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "self", ".", "player_movement", "\n", "\n", "#blue player cannot exceed y=1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "1.0", "\n", "\n", "", "", "else", ":", "\n", "#red player - actions are all reversed", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "#red player cannot preceed y=-1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "-", "1.0", "\n", "\n", "\n", "#check for left-right walls", "\n", "", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", ">", "1.0", ":", "\n", "\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "<", "-", "1.0", ":", "\n", "\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "-", "1.0", "\n", "\n", "\n", "", "self", ".", "blue_team", "=", "self", ".", "all_players", "[", ":", "self", ".", "team_size", "]", "\n", "self", ".", "red_team", "=", "self", ".", "all_players", "[", "self", ".", "team_size", ":", "]", "\n", "\n", "\n", "", "def", "render", "(", "self", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "ones", "(", "(", "300", ",", "300", ",", "3", ")", ",", "np", ".", "uint8", ")", "*", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.Touchdown.TouchdownEnv.render": [[214, 238], ["numpy.flip", "cv2.namedWindow", "cv2.imshow", "cv2.waitKey", "numpy.ones", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "int", "int"], "methods", ["None"], ["for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "(", "0", ",", "0", ",", "255.0", ")", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "(", "255.0", ",", "0", ",", "0", ")", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n", "#we actually are drawing in BGR, so flip last axis", "\n", "", "img", "=", "np", ".", "flip", "(", "img", ",", "axis", "=", "-", "1", ")", "\n", "\n", "cv2", ".", "namedWindow", "(", "\"img\"", ",", "cv2", ".", "WINDOW_NORMAL", ")", "\n", "cv2", ".", "imshow", "(", "\"img\"", ",", "img", ")", "\n", "cv2", ".", "waitKey", "(", "1", ")", "\n", "\n", "\n", "", "def", "get_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.Touchdown.TouchdownEnv.get_state_for_player": [[240, 255], ["Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player"], ["\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "# TODO: support vector states as well", "\n", "", "", "", "def", "get_vector_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "self_state", "=", "[", "]", "\n", "ally_states", "=", "[", "]", "\n", "enemy_states", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.Touchdown.TouchdownEnv.get_vector_state_for_player": [[257, 281], ["range", "numpy.asarray", "len"], "methods", ["None"], ["for", "idx", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\n", "\t\t\t", "if", "idx", "==", "entity_idx", ":", "\n", "\t\t\t\t", "self_state", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "else", ":", "\n", "\t\t\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "", "", "", "state", "=", "self_state", "+", "ally_states", "+", "enemy_states", "\n", "return", "np", ".", "asarray", "(", "state", ")", "\n", "\n", "\n", "", "def", "get_image_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "zeros", "(", "(", "84", ",", "84", ",", "1", ")", ",", "np", ".", "int8", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.round_robin_with_parameters.Touchdown.TouchdownEnv.get_image_state_for_player": [[283, 314], ["numpy.zeros", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "numpy.flipud", "numpy.fliplr"], "methods", ["None"], ["for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", "<", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", ">=", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#now re-draw this entity", "\n", "", "color", "=", "2.0", "\n", "p", "=", "self", ".", "all_players", "[", "entity_idx", "]", "\n", "pix_x", "=", "int", "(", "(", "p", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "p", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#flip", "\n", "if", "entity_idx", ">=", "self", ".", "team_size", ":", "\n", "\t\t\t", "img", "=", "np", ".", "flipud", "(", "img", ")", "\n", "img", "=", "np", ".", "fliplr", "(", "img", ")", "\n", "\n", "", "return", "img", "\n", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.make_env.make_env": [[4, 7], ["Touchdown.TouchdownEnv"], "function", ["None"], ["# 2 v 2 touchdown environment", "\n", "# this will therefore expose 6 entities: blue 1-3 and red 1-3 (indexed 0-5)", "\n", "def", "make_env", "(", ")", ":", "\n", "\t", "return", "TouchdownEnv", "(", "3", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.Touchdown.TouchdownEnv.__init__": [[20, 78], ["range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "len", "len"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "team_size", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", ":", "\n", "\n", "\t\t", "self", ".", "team_size", "=", "team_size", "\n", "self", ".", "blue_obs", "=", "blue_obs", "\n", "self", ".", "blue_actions", "=", "blue_actions", "\n", "self", ".", "red_obs", "=", "red_obs", "\n", "self", ".", "red_actions", "=", "red_actions", "\n", "\n", "self", ".", "start_pos", "=", "0.9", "\n", "self", ".", "capture_radius", "=", "0.1", "\n", "self", ".", "player_movement", "=", "0.1", "\n", "\n", "#track player movement in a normalized space", "\n", "self", ".", "blue_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "#obs spaces", "\n", "self", ".", "observation_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "blue_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "#action spaces", "\n", "", "", "self", ".", "action_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.Touchdown.TouchdownEnv.dist": [[80, 84], ["math.sqrt"], "methods", ["None"], ["", "", "", "def", "dist", "(", "self", ",", "p1", ",", "p2", ")", ":", "\n", "\t\t", "dx", "=", "p1", "[", "0", "]", "-", "p2", "[", "0", "]", "\n", "dy", "=", "p1", "[", "1", "]", "-", "p2", "[", "1", "]", "\n", "return", "math", ".", "sqrt", "(", "dx", "*", "dx", "+", "dy", "*", "dy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.Touchdown.TouchdownEnv.reset": [[86, 106], ["range", "range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "len", "states.append", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\n", "\t\t", "self", ".", "blue_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "self", ".", "epstep", "=", "0", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n", "\n", "", "return", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.Touchdown.TouchdownEnv.step": [[108, 147], ["range", "range", "len", "Touchdown.TouchdownEnv.step_player", "len", "states.append", "Touchdown.TouchdownEnv.dist", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.step_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.dist", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "\n", "\t\t", "self", ".", "epstep", "+=", "1", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "actions", ")", ")", ":", "\n", "\t\t\t", "self", ".", "step_player", "(", "actions", "[", "i", "]", ",", "i", ")", "\n", "\n", "#check for collisions", "\n", "", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t\t", "d", "=", "self", ".", "dist", "(", "bp", ",", "rp", ")", "\n", "if", "d", "<", "self", ".", "capture_radius", ":", "\n", "\t\t\t\t\t", "bp", "[", "0", "]", "=", "0.0", "\n", "bp", "[", "1", "]", "=", "self", ".", "start_pos", "\n", "rp", "[", "0", "]", "=", "0.0", "\n", "rp", "[", "1", "]", "=", "-", "self", ".", "start_pos", "\n", "\n", "#check for end-of-game", "\n", "", "", "", "done", "=", "False", "\n", "blue_reward", "=", "0.0", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "bp", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "+=", "1.0", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "rp", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "-=", "1.0", "\n", "\n", "", "", "done", "=", "done", "or", "self", ".", "epstep", ">=", "1500", "\n", "rewards", "=", "[", "blue_reward", "for", "p", "in", "self", ".", "blue_team", "]", "+", "[", "-", "blue_reward", "for", "p", "in", "self", ".", "red_team", "]", "\n", "infos", "=", "[", "{", "}", "for", "p", "in", "self", ".", "all_players", "]", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n", "\n", "", "return", "states", ",", "rewards", ",", "done", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.Touchdown.TouchdownEnv.step_player": [[149, 207], ["None"], "methods", ["None"], ["", "def", "step_player", "(", "self", ",", "action", ",", "idx", ")", ":", "\n", "\n", "\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "#blue player", "\n", "\t\t\t", "if", "self", ".", "blue_actions", "==", "\"discrete\"", ":", "\n", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "self", ".", "player_movement", "\n", "\n", "", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "self", ".", "player_movement", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "self", ".", "player_movement", "\n", "\n", "#blue player cannot exceed y=1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "1.0", "\n", "\n", "", "", "else", ":", "\n", "#red player - actions are all reversed", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "#red player cannot preceed y=-1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "-", "1.0", "\n", "\n", "\n", "#check for left-right walls", "\n", "", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", ">", "1.0", ":", "\n", "\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "<", "-", "1.0", ":", "\n", "\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "-", "1.0", "\n", "\n", "\n", "", "self", ".", "blue_team", "=", "self", ".", "all_players", "[", ":", "self", ".", "team_size", "]", "\n", "self", ".", "red_team", "=", "self", ".", "all_players", "[", "self", ".", "team_size", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.Touchdown.TouchdownEnv.render": [[209, 233], ["numpy.flip", "cv2.namedWindow", "cv2.imshow", "cv2.waitKey", "numpy.ones", "int", "int", "cv2.circle", "int", "int", "cv2.circle"], "methods", ["None"], ["", "def", "render", "(", "self", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "ones", "(", "(", "300", ",", "300", ",", "3", ")", ",", "np", ".", "uint8", ")", "*", "255.0", "\n", "\n", "#draw all blue", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "(", "0", ",", "0", ",", "255.0", ")", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "(", "255.0", ",", "0", ",", "0", ")", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n", "#we actually are drawing in BGR, so flip last axis", "\n", "", "img", "=", "np", ".", "flip", "(", "img", ",", "axis", "=", "-", "1", ")", "\n", "\n", "cv2", ".", "namedWindow", "(", "\"img\"", ",", "cv2", ".", "WINDOW_NORMAL", ")", "\n", "cv2", ".", "imshow", "(", "\"img\"", ",", "img", ")", "\n", "cv2", ".", "waitKey", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.Touchdown.TouchdownEnv.get_state_for_player": [[235, 250], ["Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player"], ["", "def", "get_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n", "", "elif", "self", ".", "blue_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.Touchdown.TouchdownEnv.get_vector_state_for_player": [[252, 276], ["range", "numpy.asarray", "len"], "methods", ["None"], ["", "", "", "def", "get_vector_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "self_state", "=", "[", "]", "\n", "ally_states", "=", "[", "]", "\n", "enemy_states", "=", "[", "]", "\n", "\n", "for", "idx", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\n", "\t\t\t", "if", "idx", "==", "entity_idx", ":", "\n", "\t\t\t\t", "self_state", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "else", ":", "\n", "\t\t\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "", "", "", "state", "=", "self_state", "+", "ally_states", "+", "enemy_states", "\n", "return", "np", ".", "asarray", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown.Touchdown.TouchdownEnv.get_image_state_for_player": [[278, 309], ["numpy.zeros", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "numpy.flipud", "numpy.fliplr"], "methods", ["None"], ["", "def", "get_image_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "zeros", "(", "(", "84", ",", "84", ",", "1", ")", ",", "np", ".", "int8", ")", "\n", "\n", "#draw all blue", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", "<", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", ">=", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#now re-draw this entity", "\n", "", "color", "=", "2.0", "\n", "p", "=", "self", ".", "all_players", "[", "entity_idx", "]", "\n", "pix_x", "=", "int", "(", "(", "p", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "p", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#flip", "\n", "if", "entity_idx", ">=", "self", ".", "team_size", ":", "\n", "\t\t\t", "img", "=", "np", ".", "flipud", "(", "img", ")", "\n", "img", "=", "np", ".", "fliplr", "(", "img", ")", "\n", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.custom_policy_random_agent.my_policy.MyCustomPolicyRandom.__init__": [[30, 33], ["None"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ")", ":", "\n", "\t\t", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.custom_policy_random_agent.my_policy.MyCustomPolicyRandom.run": [[35, 50], ["my_policy.MyCustomPolicyRandom.env.reset", "int", "range", "my_policy.MyCustomPolicyRandom.env.action_space.sample", "my_policy.MyCustomPolicyRandom.env.step", "my_policy.MyCustomPolicyRandom.comm.Get_size", "my_policy.MyCustomPolicyRandom.env.reset"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "\n", "#do an initial reset of the environment", "\n", "\t\t", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "#since we do not synchronize step counts, get steps needed for individual worker:", "\n", "local_steps", "=", "int", "(", "num_steps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "\n", "# run!", "\n", "for", "stp", "in", "range", "(", "local_steps", ")", ":", "\n", "\t\t\t", "a", "=", "self", ".", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "_", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "a", ")", "\n", "\n", "if", "done", ":", "\n", "\t\t\t\t", "self", ".", "env", ".", "reset", "(", ")", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.custom_policy_random_agent.my_main_script.make_my_policy": [[14, 17], ["my_policy.MyCustomPolicyRandom"], "function", ["None"], ["\n", "match_list", "=", "[", "[", "1", "]", "]", "*", "8", "\n", "policy_types", "=", "{", "1", ":", "\"ppo\"", "}", "\n", "# arena.kickoff(match_list, policy_types, 5000000, render=False, scale=False)", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.custom_policy_random_agent.make_env.make_env": [[9, 19], ["stable_baselines.common.atari_wrappers.wrap_deepmind", "single_agent_wrapper.seed", "single_agent_wrapper", "stable_baselines.common.atari_wrappers.make_atari", "mpi4py.MPI.COMM_WORLD.Get_rank"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed"], ["", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.make_env.make_env": [[7, 10], ["Touchdown.TouchdownEnv"], "function", ["None"], ["\t", "return", "TouchdownEnv", "(", "3", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "\n", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.Touchdown.TouchdownEnv.__init__": [[20, 78], ["range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "len", "len"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "team_size", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", ":", "\n", "\n", "\t\t", "self", ".", "team_size", "=", "team_size", "\n", "self", ".", "blue_obs", "=", "blue_obs", "\n", "self", ".", "blue_actions", "=", "blue_actions", "\n", "self", ".", "red_obs", "=", "red_obs", "\n", "self", ".", "red_actions", "=", "red_actions", "\n", "\n", "self", ".", "start_pos", "=", "0.9", "\n", "self", ".", "capture_radius", "=", "0.1", "\n", "self", ".", "player_movement", "=", "0.1", "\n", "\n", "#track player movement in a normalized space", "\n", "self", ".", "blue_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "#obs spaces", "\n", "self", ".", "observation_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "blue_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "#action spaces", "\n", "", "", "self", ".", "action_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.Touchdown.TouchdownEnv.dist": [[80, 84], ["math.sqrt"], "methods", ["None"], ["", "", "", "def", "dist", "(", "self", ",", "p1", ",", "p2", ")", ":", "\n", "\t\t", "dx", "=", "p1", "[", "0", "]", "-", "p2", "[", "0", "]", "\n", "dy", "=", "p1", "[", "1", "]", "-", "p2", "[", "1", "]", "\n", "return", "math", ".", "sqrt", "(", "dx", "*", "dx", "+", "dy", "*", "dy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.Touchdown.TouchdownEnv.reset": [[86, 106], ["range", "range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "len", "states.append", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\n", "\t\t", "self", ".", "blue_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "self", ".", "epstep", "=", "0", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n", "\n", "", "return", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.Touchdown.TouchdownEnv.step": [[108, 147], ["range", "range", "len", "Touchdown.TouchdownEnv.step_player", "len", "states.append", "Touchdown.TouchdownEnv.dist", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.step_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.dist", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "\n", "\t\t", "self", ".", "epstep", "+=", "1", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "actions", ")", ")", ":", "\n", "\t\t\t", "self", ".", "step_player", "(", "actions", "[", "i", "]", ",", "i", ")", "\n", "\n", "#check for collisions", "\n", "", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t\t", "d", "=", "self", ".", "dist", "(", "bp", ",", "rp", ")", "\n", "if", "d", "<", "self", ".", "capture_radius", ":", "\n", "\t\t\t\t\t", "bp", "[", "0", "]", "=", "0.0", "\n", "bp", "[", "1", "]", "=", "self", ".", "start_pos", "\n", "rp", "[", "0", "]", "=", "0.0", "\n", "rp", "[", "1", "]", "=", "-", "self", ".", "start_pos", "\n", "\n", "#check for end-of-game", "\n", "", "", "", "done", "=", "False", "\n", "blue_reward", "=", "0.0", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "bp", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "+=", "1.0", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "rp", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "-=", "1.0", "\n", "\n", "", "", "done", "=", "done", "or", "self", ".", "epstep", ">=", "1500", "\n", "rewards", "=", "[", "blue_reward", "for", "p", "in", "self", ".", "blue_team", "]", "+", "[", "-", "blue_reward", "for", "p", "in", "self", ".", "red_team", "]", "\n", "infos", "=", "[", "{", "}", "for", "p", "in", "self", ".", "all_players", "]", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n", "\n", "", "return", "states", ",", "rewards", ",", "done", ",", "infos", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.Touchdown.TouchdownEnv.step_player": [[149, 207], ["None"], "methods", ["None"], ["", "def", "step_player", "(", "self", ",", "action", ",", "idx", ")", ":", "\n", "\n", "\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "#blue player", "\n", "\t\t\t", "if", "self", ".", "blue_actions", "==", "\"discrete\"", ":", "\n", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "self", ".", "player_movement", "\n", "\n", "", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "self", ".", "player_movement", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "self", ".", "player_movement", "\n", "\n", "#blue player cannot exceed y=1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "1.0", "\n", "\n", "", "", "else", ":", "\n", "#red player - actions are all reversed", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "#red player cannot preceed y=-1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "-", "1.0", "\n", "\n", "\n", "#check for left-right walls", "\n", "", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", ">", "1.0", ":", "\n", "\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "<", "-", "1.0", ":", "\n", "\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "-", "1.0", "\n", "\n", "\n", "", "self", ".", "blue_team", "=", "self", ".", "all_players", "[", ":", "self", ".", "team_size", "]", "\n", "self", ".", "red_team", "=", "self", ".", "all_players", "[", "self", ".", "team_size", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.Touchdown.TouchdownEnv.render": [[209, 233], ["numpy.flip", "cv2.namedWindow", "cv2.imshow", "cv2.waitKey", "numpy.ones", "int", "int", "cv2.circle", "int", "int", "cv2.circle"], "methods", ["None"], ["", "def", "render", "(", "self", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "ones", "(", "(", "300", ",", "300", ",", "3", ")", ",", "np", ".", "uint8", ")", "*", "255.0", "\n", "\n", "#draw all blue", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "(", "0", ",", "0", ",", "255.0", ")", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "(", "255.0", ",", "0", ",", "0", ")", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n", "#we actually are drawing in BGR, so flip last axis", "\n", "", "img", "=", "np", ".", "flip", "(", "img", ",", "axis", "=", "-", "1", ")", "\n", "\n", "cv2", ".", "namedWindow", "(", "\"img\"", ",", "cv2", ".", "WINDOW_NORMAL", ")", "\n", "cv2", ".", "imshow", "(", "\"img\"", ",", "img", ")", "\n", "cv2", ".", "waitKey", "(", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.Touchdown.TouchdownEnv.get_state_for_player": [[235, 250], ["Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player"], ["", "def", "get_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n", "", "elif", "self", ".", "blue_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "", "", "else", ":", "\n", "\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.Touchdown.TouchdownEnv.get_vector_state_for_player": [[252, 276], ["range", "numpy.asarray", "len"], "methods", ["None"], ["", "", "", "def", "get_vector_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "self_state", "=", "[", "]", "\n", "ally_states", "=", "[", "]", "\n", "enemy_states", "=", "[", "]", "\n", "\n", "for", "idx", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\n", "\t\t\t", "if", "idx", "==", "entity_idx", ":", "\n", "\t\t\t\t", "self_state", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "else", ":", "\n", "\t\t\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "", "else", ":", "\n", "\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "", "", "", "state", "=", "self_state", "+", "ally_states", "+", "enemy_states", "\n", "return", "np", ".", "asarray", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_sac.Touchdown.TouchdownEnv.get_image_state_for_player": [[278, 309], ["numpy.zeros", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "numpy.flipud", "numpy.fliplr"], "methods", ["None"], ["", "def", "get_image_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "zeros", "(", "(", "84", ",", "84", ",", "1", ")", ",", "np", ".", "int8", ")", "\n", "\n", "#draw all blue", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", "<", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", ">=", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#now re-draw this entity", "\n", "", "color", "=", "2.0", "\n", "p", "=", "self", ".", "all_players", "[", "entity_idx", "]", "\n", "pix_x", "=", "int", "(", "(", "p", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "p", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#flip", "\n", "if", "entity_idx", ">=", "self", ".", "team_size", ":", "\n", "\t\t\t", "img", "=", "np", ".", "flipud", "(", "img", ")", "\n", "img", "=", "np", ".", "fliplr", "(", "img", ")", "\n", "\n", "", "return", "img", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.atari_evo.make_env.make_env": [[7, 15], ["stable_baselines.common.atari_wrappers.wrap_deepmind", "single_agent_wrapper.seed", "single_agent_wrapper", "stable_baselines.common.atari_wrappers.make_atari", "mpi4py.MPI.COMM_WORLD.Get_rank"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed"], ["\t", "return", "TouchdownEnv", "(", "3", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "\n", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.atari_evo.plot_experiments.winning_policy": [[12, 23], ["PolicyRecord", "PolicyRecord", "min", "np.mean", "len"], "function", ["None"], ["def", "winning_policy", "(", "pool", ",", "logsdir", ")", ":", "\n", "\t", "best_pol", "=", "0", "\n", "best_score", "=", "-", "1000000", "\n", "for", "p", "in", "pool", ":", "\n", "\t\t", "pr", "=", "PolicyRecord", "(", "p", ",", "logsdir", ")", "\n", "length", "=", "min", "(", "len", "(", "pr", ".", "channels", "[", "\"main\"", "]", ".", "ep_results", ")", ",", "100", ")", "\n", "score", "=", "np", ".", "mean", "(", "pr", ".", "channels", "[", "\"main\"", "]", ".", "ep_results", "[", "-", "length", ":", "]", ")", "\n", "if", "score", ">", "best_score", ":", "\n", "\t\t\t", "best_pol", "=", "p", "\n", "best_score", "=", "score", "\n", "", "", "return", "PolicyRecord", "(", "best_pol", ",", "logsdir", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.Flatten.forward": [[13, 15], ["x.view", "x.size"], "methods", ["None"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.Model.__init__": [[18, 34], ["super().__init__", "my_policy.Model.__init__.make_model"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["\n", "\n", "class", "MyCustomPolicyRandom", "(", ")", ":", "\n", "\n", "# create our policy, tracking the environment and comms objects", "\n", "\t", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ")", ":", "\n", "\t\t", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "\n", "# the most rudimentary random agent", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.Model.forward": [[35, 37], ["my_policy.Model.policy", "my_policy.Model.logstd", "my_policy.Model.value"], "methods", ["None"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "\n", "#do an initial reset of the environment", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.__init__": [[46, 66], ["my_policy.PPOCuriosity.comm.Get_rank", "mpi4py.MPI.COMM_WORLD.Get_rank", "print", "torch.set_num_threads", "my_policy.Model", "torch.optim.Adam", "max", "my_policy.PPOCuriosity.model.parameters", "int", "my_policy.PPOCuriosity.comm.Get_size", "torch.get_num_threads", "my_policy.PPOCuriosity.comm.Get_size"], "methods", ["None"], ["_", ",", "_", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "a", ")", "\n", "\n", "if", "done", ":", "\n", "\t\t\t\t", "self", ".", "env", ".", "reset", "(", ")", "", "", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.run": [[67, 98], ["int", "my_policy.PPOCuriosity.load_from_dir", "my_policy.PPOCuriosity.rollouts", "my_policy.PPOCuriosity.save_to_dir", "my_policy.PPOCuriosity.mpi_sync_weights", "next", "my_policy.PPOCuriosity.mpi_gather_memory", "my_policy.PPOCuriosity.comm.Get_size", "my_policy.PPOCuriosity.train"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.load_from_dir", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.rollouts", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.save_to_dir", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.mpi_sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.mpi_gather_memory", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.train"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.load_from_dir": [[99, 102], ["os.path.exists", "my_policy.PPOCuriosity.model.load_state_dict", "torch.load"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.save_to_dir": [[103, 107], ["policy_record.save", "torch.save", "my_policy.PPOCuriosity.model.state_dict"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.mpi_sync_weights": [[108, 120], ["my_policy.PPOCuriosity.comm.bcast", "my_policy.PPOCuriosity.model.load_state_dict", "my_policy.PPOCuriosity.model.state_dict"], "methods", ["None"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.mpi_gather_memory": [[121, 136], ["memory.numpy", "my_policy.PPOCuriosity.comm.gather", "memory.clear", "enumerate", "type", "len", "my_policy.PPOCuriosity.comm.Get_size", "memory.store_numpy"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.clear", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.store_numpy"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.rollouts": [[137, 171], ["memory.Memory.Memory", "my_policy.PPOCuriosity.env.reset", "range", "torch.from_numpy().float", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal.sample", "my_policy.PPOCuriosity.env.step", "memory.Memory.Memory.store", "torch.no_grad", "my_policy.PPOCuriosity.model", "torch.exp", "torch.distributions.normal.Normal.sample.numpy", "memory.Memory.Memory", "my_policy.PPOCuriosity.env.reset", "torch.from_numpy", "torch.from_numpy().float.unsqueeze"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.maddpg.ReplayBuffer.store", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.train": [[172, 213], ["memory.get", "memory.get", "ppo.generalized_advantage_estimation", "ppo.discounted_reward", "torch.zeros", "torch.zeros", "torch.zeros", "range", "print", "print", "print", "print", "torch.utils.data.BatchSampler", "ppo.generalized_advantage_estimation.mean", "ppo.generalized_advantage_estimation.std", "torch.utils.data.SubsetRandomSampler", "my_policy.PPOCuriosity.model", "ppo.ppo_loss", "my_policy.PPOCuriosity.optimizer.zero_grad", "loss.backward", "my_policy.PPOCuriosity.optimizer.step", "range", "reward.sum().item", "torch.zeros.detach().numpy", "torch.zeros.detach().numpy", "torch.zeros.detach().numpy", "reward.sum", "torch.zeros.detach", "torch.zeros.detach", "torch.zeros.detach"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.get", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.get", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.ppo.generalized_advantage_estimation", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.ppo.discounted_reward", "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.ppo_loss", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.make_env.make_env": [[5, 8], ["Touchdown.TouchdownEnv"], "function", ["None"], ["# this will therefore expose 6 entities: blue 1-3 and red 1-3 (indexed 0-5)", "\n", "def", "make_env", "(", ")", ":", "\n", "\t", "return", "TouchdownEnv", "(", "3", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "\n", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.__init__": [[20, 78], ["range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.observation_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Box", "Touchdown.TouchdownEnv.observation_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Discrete", "Touchdown.TouchdownEnv.action_spaces.append", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "gym.spaces.Box", "len", "len"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "team_size", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", ":", "\n", "\n", "\t\t", "self", ".", "team_size", "=", "team_size", "\n", "self", ".", "blue_obs", "=", "blue_obs", "\n", "self", ".", "blue_actions", "=", "blue_actions", "\n", "self", ".", "red_obs", "=", "red_obs", "\n", "self", ".", "red_actions", "=", "red_actions", "\n", "\n", "self", ".", "start_pos", "=", "0.9", "\n", "self", ".", "capture_radius", "=", "0.1", "\n", "self", ".", "player_movement", "=", "0.1", "\n", "\n", "#track player movement in a normalized space", "\n", "self", ".", "blue_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "#obs spaces", "\n", "self", ".", "observation_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "blue_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "84", ",", "84", ",", "1", ")", ")", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "self", ".", "observation_spaces", ".", "append", "(", "Box", "(", "-", "10", ",", "10", ",", "(", "len", "(", "self", ".", "all_players", ")", "*", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "#action spaces", "\n", "", "", "self", ".", "action_spaces", "=", "[", "]", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "self", ".", "blue_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Discrete", "(", "5", ")", ")", "\n", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "action_spaces", ".", "append", "(", "Box", "(", "-", "1.0", ",", "1.0", ",", "(", "2", ",", ")", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.dist": [[79, 83], ["math.sqrt"], "methods", ["None"], ["\n", "", "", "", "def", "dist", "(", "self", ",", "p1", ",", "p2", ")", ":", "\n", "\t\t", "dx", "=", "p1", "[", "0", "]", "-", "p2", "[", "0", "]", "\n", "dy", "=", "p1", "[", "1", "]", "-", "p2", "[", "1", "]", "\n", "return", "math", ".", "sqrt", "(", "dx", "*", "dx", "+", "dy", "*", "dy", ")", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.reset": [[84, 104], ["range", "range", "range", "Touchdown.TouchdownEnv.blue_team.append", "Touchdown.TouchdownEnv.red_team.append", "len", "states.append", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["\n", "\n", "", "def", "reset", "(", "self", ")", ":", "\n", "\n", "\t\t", "self", ".", "blue_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "blue_team", ".", "append", "(", "[", "0.0", ",", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "red_team", "=", "[", "]", "\n", "for", "t", "in", "range", "(", "self", ".", "team_size", ")", ":", "\n", "\t\t\t", "self", ".", "red_team", ".", "append", "(", "[", "0.0", ",", "-", "self", ".", "start_pos", "]", ")", "\n", "\n", "", "self", ".", "all_players", "=", "self", ".", "blue_team", "+", "self", ".", "red_team", "\n", "\n", "self", ".", "epstep", "=", "0", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.step": [[105, 144], ["range", "range", "len", "Touchdown.TouchdownEnv.step_player", "len", "states.append", "Touchdown.TouchdownEnv.dist", "Touchdown.TouchdownEnv.get_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.step_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.dist", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player"], ["", "return", "states", "\n", "\n", "\n", "", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "\n", "\t\t", "self", ".", "epstep", "+=", "1", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "actions", ")", ")", ":", "\n", "\t\t\t", "self", ".", "step_player", "(", "actions", "[", "i", "]", ",", "i", ")", "\n", "\n", "#check for collisions", "\n", "", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t\t", "d", "=", "self", ".", "dist", "(", "bp", ",", "rp", ")", "\n", "if", "d", "<", "self", ".", "capture_radius", ":", "\n", "\t\t\t\t\t", "bp", "[", "0", "]", "=", "0.0", "\n", "bp", "[", "1", "]", "=", "self", ".", "start_pos", "\n", "rp", "[", "0", "]", "=", "0.0", "\n", "rp", "[", "1", "]", "=", "-", "self", ".", "start_pos", "\n", "\n", "#check for end-of-game", "\n", "", "", "", "done", "=", "False", "\n", "blue_reward", "=", "0.0", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "if", "bp", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "+=", "1.0", "\n", "", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "if", "rp", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "done", "=", "True", "\n", "blue_reward", "-=", "1.0", "\n", "\n", "", "", "done", "=", "done", "or", "self", ".", "epstep", ">=", "1500", "\n", "rewards", "=", "[", "blue_reward", "for", "p", "in", "self", ".", "blue_team", "]", "+", "[", "-", "blue_reward", "for", "p", "in", "self", ".", "red_team", "]", "\n", "infos", "=", "[", "{", "}", "for", "p", "in", "self", ".", "all_players", "]", "\n", "\n", "#get states and return", "\n", "states", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\t\t\t", "states", ".", "append", "(", "self", ".", "get_state_for_player", "(", "i", ")", ")", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.step_player": [[145, 201], ["None"], "methods", ["None"], ["\n", "", "return", "states", ",", "rewards", ",", "done", ",", "infos", "\n", "\n", "\n", "", "def", "step_player", "(", "self", ",", "action", ",", "idx", ")", ":", "\n", "\n", "\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "#blue player", "\n", "\t\t\t", "if", "self", ".", "blue_actions", "==", "\"discrete\"", ":", "\n", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "self", ".", "player_movement", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "self", ".", "player_movement", "\n", "\n", "", "", "elif", "self", ".", "blue_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "self", ".", "player_movement", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "self", ".", "player_movement", "\n", "\n", "#blue player cannot exceed y=1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", ">", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "1.0", "\n", "\n", "", "", "else", ":", "\n", "#red player - actions are all reversed", "\n", "\t\t\t", "if", "self", ".", "red_actions", "==", "\"discrete\"", ":", "\n", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "\n", "if", "action", "==", "1", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "2", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "3", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "(", "-", "self", ".", "player_movement", ")", "\n", "", "elif", "action", "==", "4", ":", "\n", "\t\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "-=", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "", "", "elif", "self", ".", "red_actions", "==", "\"continuous\"", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "+=", "action", "[", "0", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "+=", "action", "[", "1", "]", "*", "(", "-", "self", ".", "player_movement", ")", "\n", "\n", "#red player cannot preceed y=-1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "<", "-", "1.0", ":", "\n", "\t\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "1", "]", "=", "-", "1.0", "\n", "\n", "\n", "#check for left-right walls", "\n", "", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", ">", "1.0", ":", "\n", "\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "1.0", "\n", "", "if", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "<", "-", "1.0", ":", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.render": [[202, 226], ["numpy.flip", "cv2.namedWindow", "cv2.imshow", "cv2.waitKey", "numpy.ones", "int", "int", "cv2.circle", "int", "int", "cv2.circle"], "methods", ["None"], ["\t\t\t", "self", ".", "all_players", "[", "idx", "]", "[", "0", "]", "=", "-", "1.0", "\n", "\n", "\n", "", "self", ".", "blue_team", "=", "self", ".", "all_players", "[", ":", "self", ".", "team_size", "]", "\n", "self", ".", "red_team", "=", "self", ".", "all_players", "[", "self", ".", "team_size", ":", "]", "\n", "\n", "\n", "", "def", "render", "(", "self", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "ones", "(", "(", "300", ",", "300", ",", "3", ")", ",", "np", ".", "uint8", ")", "*", "255.0", "\n", "\n", "#draw all blue", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "(", "0", ",", "0", ",", "255.0", ")", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "(", "255.0", ",", "0", ",", "0", ")", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "300.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "10", ",", "color", ",", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_state_for_player": [[227, 242], ["Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_image_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player", "Touchdown.TouchdownEnv.get_vector_state_for_player"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player"], ["#we actually are drawing in BGR, so flip last axis", "\n", "", "img", "=", "np", ".", "flip", "(", "img", ",", "axis", "=", "-", "1", ")", "\n", "\n", "cv2", ".", "namedWindow", "(", "\"img\"", ",", "cv2", ".", "WINDOW_NORMAL", ")", "\n", "cv2", ".", "imshow", "(", "\"img\"", ",", "img", ")", "\n", "cv2", ".", "waitKey", "(", "1", ")", "\n", "\n", "\n", "", "def", "get_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t", "if", "self", ".", "blue_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n", "", "elif", "self", ".", "blue_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_vector_state_for_player": [[244, 268], ["range", "numpy.asarray", "len"], "methods", ["None"], ["\t\t\t", "if", "self", ".", "red_obs", "==", "\"image\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_image_state_for_player", "(", "entity_idx", ")", "\n", "", "elif", "self", ".", "red_obs", "==", "\"vector\"", ":", "\n", "\t\t\t\t", "return", "self", ".", "get_vector_state_for_player", "(", "entity_idx", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "raise", "ValueError", "\n", "\n", "# TODO: support vector states as well", "\n", "", "", "", "def", "get_vector_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\t\t", "self_state", "=", "[", "]", "\n", "ally_states", "=", "[", "]", "\n", "enemy_states", "=", "[", "]", "\n", "\n", "for", "idx", "in", "range", "(", "len", "(", "self", ".", "all_players", ")", ")", ":", "\n", "\n", "\t\t\t", "if", "idx", "==", "entity_idx", ":", "\n", "\t\t\t\t", "self_state", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "else", ":", "\n", "\t\t\t\t", "if", "entity_idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.Touchdown.TouchdownEnv.get_image_state_for_player": [[269, 300], ["numpy.zeros", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "int", "int", "cv2.circle", "numpy.flipud", "numpy.fliplr"], "methods", ["None"], ["\t\t\t\t\t", "if", "idx", "<", "self", ".", "team_size", ":", "\n", "\t\t\t\t\t\t", "enemy_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "ally_states", "+=", "self", ".", "all_players", "[", "idx", "]", "\n", "\n", "", "", "", "", "state", "=", "self_state", "+", "ally_states", "+", "enemy_states", "\n", "return", "np", ".", "asarray", "(", "state", ")", "\n", "\n", "\n", "", "def", "get_image_state_for_player", "(", "self", ",", "entity_idx", ")", ":", "\n", "\n", "\t\t", "img", "=", "np", ".", "zeros", "(", "(", "84", ",", "84", ",", "1", ")", ",", "np", ".", "int8", ")", "\n", "\n", "#draw all blue", "\n", "for", "bp", "in", "self", ".", "blue_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", "<", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "bp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "bp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#draw all red", "\n", "", "for", "rp", "in", "self", ".", "red_team", ":", "\n", "\t\t\t", "color", "=", "1.0", "if", "entity_idx", ">=", "self", ".", "team_size", "else", "-", "1.0", "\n", "pix_x", "=", "int", "(", "(", "rp", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "rp", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "cv2", ".", "circle", "(", "img", ",", "(", "pix_x", ",", "pix_y", ")", ",", "3", ",", "(", "color", ")", ",", "-", "1", ")", "\n", "\n", "#now re-draw this entity", "\n", "", "color", "=", "2.0", "\n", "p", "=", "self", ".", "all_players", "[", "entity_idx", "]", "\n", "pix_x", "=", "int", "(", "(", "p", "[", "0", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n", "pix_y", "=", "int", "(", "(", "p", "[", "1", "]", "+", "1.0", ")", "*", "0.5", "*", "84.0", ")", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.ppo.generalized_advantage_estimation": [[6, 18], ["torch.zeros_like", "reversed", "range", "reward.size"], "function", ["None"], ["def", "generalized_advantage_estimation", "(", "reward", ",", "value", ",", "done", ",", "gamma", "=", "0.99", ",", "lam", "=", "0.95", ")", ":", "\n", "    ", "last_part", "=", "0", "\n", "advantages", "=", "torch", ".", "zeros_like", "(", "reward", ")", "\n", "for", "t", "in", "reversed", "(", "range", "(", "reward", ".", "size", "(", "0", ")", ")", ")", ":", "\n", "        ", "if", "done", "[", "t", "]", ":", "\n", "            ", "delta", "=", "reward", "[", "t", "]", "-", "value", "[", "t", "]", "\n", "last_part", "=", "0", "\n", "", "else", ":", "\n", "            ", "delta", "=", "reward", "[", "t", "]", "+", "gamma", "*", "value", "[", "t", "+", "1", "]", "-", "value", "[", "t", "]", "\n", "", "advantages", "[", "t", "]", "=", "delta", "+", "gamma", "*", "lam", "*", "last_part", "\n", "last_part", "=", "advantages", "[", "t", "]", "\n", "", "return", "advantages", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.ppo.discounted_reward": [[20, 29], ["reward.size", "torch.zeros_like", "reversed", "range"], "function", ["None"], ["", "def", "discounted_reward", "(", "reward", ",", "done", ",", "gamma", "=", "0.99", ")", ":", "\n", "    ", "T", "=", "reward", ".", "size", "(", "0", ")", "\n", "discounted_rewards", "=", "torch", ".", "zeros_like", "(", "reward", ")", "\n", "for", "t", "in", "reversed", "(", "range", "(", "T", ")", ")", ":", "\n", "        ", "if", "done", "[", "t", "]", ":", "\n", "            ", "discounted_rewards", "[", "t", "]", "=", "reward", "[", "t", "]", "\n", "", "else", ":", "\n", "            ", "discounted_rewards", "[", "t", "]", "=", "reward", "[", "t", "]", "+", "gamma", "*", "discounted_rewards", "[", "t", "+", "1", "]", "\n", "", "", "return", "discounted_rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.ppo.ppo_loss": [[31, 45], ["torch.distributions.normal.Normal", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal.log_prob", "torch.exp", "torch.min", "torch.exp", "torch.exp", "torch.min.sum().mean", "torch.clamp", "torch.min.sum", "torch.exp"], "function", ["None"], ["", "def", "ppo_loss", "(", "old_means", ",", "old_log_stds", ",", "new_means", ",", "new_log_stds", ",", "actions", ",", "advantages", ",", "clip_param", "=", "0.2", ")", ":", "\n", "    ", "old_distribution", "=", "Normal", "(", "old_means", ",", "torch", ".", "exp", "(", "old_log_stds", ")", ")", "\n", "old_log_prob", "=", "old_distribution", ".", "log_prob", "(", "actions", ")", "\n", "\n", "new_distribution", "=", "Normal", "(", "new_means", ",", "torch", ".", "exp", "(", "new_log_stds", ")", ")", "\n", "new_log_prob", "=", "new_distribution", ".", "log_prob", "(", "actions", ")", "\n", "\n", "ratio", "=", "torch", ".", "exp", "(", "new_log_prob", "-", "old_log_prob", ")", "\n", "surrogate_loss", "=", "torch", ".", "min", "(", "advantages", "*", "ratio", ",", "advantages", "*", "torch", ".", "clamp", "(", "ratio", ",", "1.0", "-", "clip_param", ",", "1.0", "+", "clip_param", ")", ")", "\n", "policy_loss", "=", "-", "surrogate_loss", ".", "sum", "(", "-", "1", ")", ".", "mean", "(", ")", "\n", "\n", "entropy", "=", "-", "(", "new_log_prob", "*", "torch", ".", "exp", "(", "new_log_prob", ")", ")", ".", "sum", "(", "-", "1", ")", ".", "mean", "(", ")", "\n", "\n", "return", "policy_loss", ",", "entropy", "\n", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.__init__": [[7, 9], ["collections.defaultdict"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "data", "=", "defaultdict", "(", "list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.store": [[10, 13], ["kwargs.items", "memory.Memory.data[].append"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "        ", "for", "key", ",", "value", "in", "kwargs", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "data", "[", "key", "]", ".", "append", "(", "value", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.get": [[14, 16], ["memory.Memory.torchify"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.torchify"], ["", "", "def", "get", "(", "self", ",", "*", "args", ")", ":", "\n", "        ", "return", "[", "self", ".", "torchify", "(", "key", ")", "for", "key", "in", "args", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.torchify": [[17, 25], ["type", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor", "torch.stack", "torch.tensor"], "methods", ["None"], ["", "def", "torchify", "(", "self", ",", "key", ")", "->", "torch", ".", "Tensor", ":", "\n", "        ", "ty", "=", "type", "(", "self", ".", "data", "[", "key", "]", "[", "0", "]", ")", "\n", "if", "ty", "is", "bool", ":", "\n", "            ", "return", "torch", ".", "tensor", "(", "self", ".", "data", "[", "key", "]", ",", "dtype", "=", "torch", ".", "bool", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "elif", "ty", "is", "float", ":", "\n", "            ", "return", "torch", ".", "tensor", "(", "self", ".", "data", "[", "key", "]", ",", "dtype", "=", "torch", ".", "float", ")", ".", "unsqueeze", "(", "1", ")", "\n", "", "elif", "ty", "is", "torch", ".", "Tensor", ":", "\n", "            ", "return", "torch", ".", "stack", "(", "self", ".", "data", "[", "key", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.clear": [[26, 28], ["memory.Memory.data.clear"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.clear"], ["", "", "def", "clear", "(", "self", ")", ":", "\n", "        ", "self", ".", "data", ".", "clear", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy": [[29, 34], ["memory.Memory.data.items", "memory.Memory.torchify().detach().numpy", "memory.Memory.torchify().detach", "memory.Memory.torchify"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.torchify"], ["", "def", "numpy", "(", "self", ")", "->", "dict", ":", "\n", "        ", "data", "=", "{", "}", "\n", "for", "key", ",", "value", "in", "self", ".", "data", ".", "items", "(", ")", ":", "\n", "            ", "data", "[", "key", "]", "=", "self", ".", "torchify", "(", "key", ")", ".", "detach", "(", ")", ".", "numpy", "(", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.store_numpy": [[35, 39], ["data.items", "enumerate", "memory.Memory.data[].append", "torch.from_numpy"], "methods", ["None"], ["", "def", "store_numpy", "(", "self", ",", "data", ":", "dict", ")", ":", "\n", "        ", "for", "key", ",", "values", "in", "data", ".", "items", "(", ")", ":", "\n", "            ", "for", "i", ",", "value", "in", "enumerate", "(", "values", ")", ":", "\n", "                ", "self", ".", "data", "[", "key", "]", ".", "append", "(", "torch", ".", "from_numpy", "(", "value", ")", ")", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.make_env.make_env": [[7, 10], ["CoverEnv.CoverEnv"], "function", ["None"], ["\t", "return", "TouchdownEnv", "(", "3", ",", "blue_obs", "=", "\"image\"", ",", "blue_actions", "=", "\"discrete\"", ",", "\n", "red_obs", "=", "\"image\"", ",", "red_actions", "=", "\"discrete\"", ")", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.Player.__init__": [[12, 15], ["pygame_base.GameElement.GameElement.__init__"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["    ", "def", "__init__", "(", "self", ",", "color", "=", "[", "0", ",", "0", ",", "0", "]", ",", "centered", "=", "False", ")", ":", "\n", "        ", "w", "=", "0.075", "\n", "super", "(", ")", ".", "__init__", "(", "0", ",", "0", ",", "w", ",", "w", ",", "fill", "=", "color", ",", "stroke", "=", "color", ",", "centered", "=", "centered", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.Player.reset": [[16, 20], ["random.random", "random.random"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "x", ",", "self", ".", "y", "=", "random", ".", "random", "(", ")", ",", "random", ".", "random", "(", ")", "\n", "self", ".", "vx", "=", "0", "\n", "self", ".", "vy", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.Player.draw": [[21, 24], ["display.cvtX", "pygame.draw.circle", "display.cvtPoint"], "methods", ["None"], ["", "def", "draw", "(", "self", ",", "display", ")", ":", "\n", "        ", "r", "=", "display", ".", "cvtX", "(", "self", ".", "w", "*", "0.5", ")", "\n", "pygame", ".", "draw", ".", "circle", "(", "display", ".", "display", ",", "self", ".", "fill", ",", "display", ".", "cvtPoint", "(", "self", ".", "x", "+", "self", ".", "w", "*", "0.5", ",", "self", ".", "y", "+", "self", ".", "w", "*", "0.5", ")", ",", "r", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.Target.__init__": [[27, 32], ["pygame_base.GameElement.GameElement.__init__"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["    ", "def", "__init__", "(", "self", ",", "color", "=", "[", "0", ",", "0", ",", "0", "]", ",", "centered", "=", "False", ")", ":", "\n", "        ", "w", "=", "0.15", "\n", "super", "(", ")", ".", "__init__", "(", "0", ",", "0", ",", "w", ",", "w", ",", "fill", "=", "color", ",", "stroke", "=", "color", ",", "centered", "=", "centered", ")", "\n", "self", ".", "occupied", "=", "False", "\n", "self", ".", "collision_adjust", "=", "-", "0.05", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.Target.reset": [[33, 37], ["random.uniform", "random.uniform"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "x", ",", "self", ".", "y", "=", "random", ".", "uniform", "(", "0.0", ",", "1.0", "-", "self", ".", "w", ")", ",", "random", ".", "uniform", "(", "0.0", ",", "1.0", "-", "self", ".", "w", ")", "\n", "self", ".", "vx", "=", "0", "\n", "self", ".", "vy", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.Target.draw": [[38, 43], ["pygame.draw.rect", "pygame.draw.rect", "pygame.Rect", "pygame.Rect", "display.cvtRect", "display.cvtRect"], "methods", ["None"], ["", "def", "draw", "(", "self", ",", "display", ")", ":", "\n", "        ", "if", "not", "self", ".", "occupied", ":", "\n", "            ", "pygame", ".", "draw", ".", "rect", "(", "display", ".", "display", ",", "self", ".", "fill", ",", "pygame", ".", "Rect", "(", "*", "display", ".", "cvtRect", "(", "self", ".", "x", ",", "self", ".", "y", ",", "self", ".", "w", ",", "self", ".", "h", ")", ")", ")", "\n", "", "else", ":", "\n", "            ", "pygame", ".", "draw", ".", "rect", "(", "display", ".", "display", ",", "self", ".", "stroke", ",", "pygame", ".", "Rect", "(", "*", "display", ".", "cvtRect", "(", "self", ".", "x", ",", "self", ".", "y", ",", "self", ".", "w", ",", "self", ".", "h", ")", ")", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.Target.subtick": [[44, 57], ["isinstance"], "methods", ["None"], ["", "", "def", "subtick", "(", "self", ",", "game_elements", ")", ":", "\n", "\n", "        ", "self", ".", "occupied", "=", "False", "\n", "for", "e", "in", "game_elements", ":", "\n", "            ", "if", "isinstance", "(", "e", ",", "Player", ")", ":", "\n", "                ", "pcx", ",", "pcy", "=", "e", ".", "center", "\n", "if", "pcx", ">", "self", ".", "x", "and", "pcx", "<", "(", "self", ".", "x", "+", "self", ".", "w", ")", ":", "\n", "                    ", "if", "pcy", ">", "self", ".", "y", "and", "pcy", "<", "(", "self", ".", "y", "+", "self", ".", "h", ")", ":", "\n", "                        ", "self", ".", "occupied", "=", "True", "\n", "\n", "", "", "", "", "if", "self", ".", "occupied", ":", "\n", "            ", "return", "0.25", ",", "False", "\n", "", "return", "0.0", ",", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.CoverEnv.__init__": [[62, 72], ["pygame_base.GameEngine.GameEngine.__init__", "GameDisplay"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["    ", "def", "__init__", "(", "self", ",", "N", ",", "headless", "=", "False", ",", "draws", "=", "False", ")", ":", "\n", "\n", "        ", "def", "import_and_create_display", "(", ")", ":", "\n", "            ", "from", "pygame_base", ".", "GameDisplay", "import", "GameDisplay", "\n", "return", "GameDisplay", "(", ")", "\n", "\n", "", "self", ".", "N", "=", "N", "\n", "\n", "super", "(", ")", ".", "__init__", "(", "headless", "=", "headless", ",", "make_display_fn", "=", "import_and_create_display", ",", "\n", "step_limit", "=", "300", ",", "bg_color", "=", "[", "255", ",", "255", ",", "255", "]", ",", "draws", "=", "draws", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.CoverEnv.setup": [[74, 99], ["gym.spaces.Box", "gym.spaces.Box", "range", "range", "CoverEnv.Player", "CoverEnv.CoverEnv.players.append", "CoverEnv.CoverEnv.targets.append", "CoverEnv.Target"], "methods", ["None"], ["", "def", "setup", "(", "self", ")", ":", "\n", "        ", "self", ".", "action_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "-", "1.0", ",", "high", "=", "1.0", ",", "shape", "=", "(", "2", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "obspc", "=", "3", "*", "self", ".", "N", "+", "2", "*", "(", "self", ".", "N", "-", "1", ")", "\n", "self", ".", "observation_space", "=", "gym", ".", "spaces", ".", "Box", "(", "low", "=", "-", "1.0", ",", "high", "=", "1.0", ",", "shape", "=", "(", "obspc", ",", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "\n", "self", ".", "action_spaces", "=", "[", "self", ".", "action_space", "]", "*", "self", ".", "N", "\n", "self", ".", "observation_spaces", "=", "[", "self", ".", "observation_space", "]", "*", "self", ".", "N", "\n", "\n", "colors", "=", "[", "\n", "[", "255", ",", "0", ",", "0", "]", ",", "\n", "[", "0", ",", "255", ",", "0", "]", ",", "\n", "[", "0", ",", "0", ",", "255", "]", ",", "\n", "[", "255", ",", "255", ",", "0", "]", ",", "\n", "[", "255", ",", "0", ",", "255", "]", ",", "\n", "[", "0", ",", "255", ",", "255", "]", ",", "\n", "[", "0", ",", "0", ",", "0", "]", "\n", "]", "\n", "self", ".", "players", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "            ", "p", "=", "Player", "(", "color", "=", "colors", "[", "i", "]", ")", "\n", "self", ".", "players", ".", "append", "(", "p", ")", "\n", "\n", "", "self", ".", "targets", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "            ", "self", ".", "targets", ".", "append", "(", "Target", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.CoverEnv.apply_action": [[101, 105], ["range"], "methods", ["None"], ["", "", "def", "apply_action", "(", "self", ",", "action", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "            ", "self", ".", "players", "[", "i", "]", ".", "vx", "=", "action", "[", "i", "]", "[", "0", "]", "*", "0.06", "\n", "self", ".", "players", "[", "i", "]", ".", "vy", "=", "action", "[", "i", "]", "[", "1", "]", "*", "0.06", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.CoverEnv.poststep_hook": [[106, 110], ["numpy.clip", "numpy.clip"], "methods", ["None"], ["", "", "def", "poststep_hook", "(", "self", ")", ":", "\n", "        ", "for", "p", "in", "self", ".", "players", ":", "\n", "            ", "p", ".", "x", "=", "np", ".", "clip", "(", "p", ".", "x", ",", "0", ",", "1", ")", "\n", "p", ".", "y", "=", "np", ".", "clip", "(", "p", ".", "y", ",", "0", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.CoverEnv.reset_hook": [[112, 132], ["p.reset", "range", "t.reset", "range", "CoverEnv.CoverEnv.targets[].check_collision"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "", "def", "reset_hook", "(", "self", ")", ":", "\n", "\n", "        ", "for", "p", "in", "self", ".", "players", ":", "\n", "            ", "p", ".", "reset", "(", ")", "\n", "\n", "", "valid", "=", "False", "\n", "while", "not", "valid", ":", "\n", "            ", "for", "t", "in", "self", ".", "targets", ":", "\n", "                ", "t", ".", "reset", "(", ")", "\n", "", "valid", "=", "True", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "                ", "for", "j", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "                    ", "if", "i", "==", "j", ":", "\n", "                        ", "continue", "\n", "", "if", "self", ".", "targets", "[", "i", "]", ".", "check_collision", "(", "self", ".", "targets", "[", "j", "]", ")", ":", "\n", "                        ", "valid", "=", "False", "\n", "\n", "\n", "", "", "", "", "self", ".", "game_elements", "=", "self", ".", "targets", "+", "self", ".", "players", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.CoverEnv.process_state": [[135, 159], ["range", "range", "range", "states.append", "state.append", "state.append", "state.append", "state.append", "state.append"], "methods", ["None"], ["", "def", "process_state", "(", "self", ",", "imgstate", ")", ":", "\n", "        ", "states", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "            ", "origin", "=", "self", ".", "players", "[", "i", "]", "\n", "state", "=", "[", "]", "\n", "\n", "# other players", "\n", "for", "j", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "                ", "if", "i", "!=", "j", ":", "\n", "                    ", "other", "=", "self", ".", "players", "[", "j", "]", "\n", "state", ".", "append", "(", "other", ".", "center", "[", "0", "]", "-", "origin", ".", "center", "[", "0", "]", ")", "\n", "state", ".", "append", "(", "other", ".", "center", "[", "1", "]", "-", "origin", ".", "center", "[", "1", "]", ")", "\n", "\n", "# targets", "\n", "", "", "for", "j", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "                ", "other", "=", "self", ".", "targets", "[", "j", "]", "\n", "state", ".", "append", "(", "other", ".", "center", "[", "0", "]", "-", "origin", ".", "center", "[", "0", "]", ")", "\n", "state", ".", "append", "(", "other", ".", "center", "[", "1", "]", "-", "origin", ".", "center", "[", "1", "]", ")", "\n", "state", ".", "append", "(", "1.0", "if", "other", ".", "occupied", "else", "0.0", ")", "\n", "\n", "", "states", ".", "append", "(", "state", ")", "\n", "\n", "", "return", "states", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.CoverEnv.process_rewards": [[160, 162], ["None"], "methods", ["None"], ["", "def", "process_rewards", "(", "self", ",", "rewards", ")", ":", "\n", "        ", "return", "[", "rewards", "/", "self", ".", "N", "]", "*", "self", ".", "N", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.CoverEnv.CoverEnv.process_infos": [[163, 165], ["None"], "methods", ["None"], ["", "def", "process_infos", "(", "self", ",", "infos", ")", ":", "\n", "        ", "return", "[", "infos", "]", "*", "self", ".", "N", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.HPPOPolicy.__init__": [[28, 56], ["tensorflow.ConfigProto", "tensorflow.Session", "tensorflow.keras.backend.set_session", "stable_baselines.logger.configure", "hppo.BehaviorModel", "hppo.BehaviorModel", "hppo.BehaviorModel", "hppo.MetaAgent"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ",", "**", "kwargs", ")", ":", "\n", "\n", "# Pull params out of kwargs", "\n", "        ", "self", ".", "params", "=", "kwargs", "[", "'params'", "]", "\n", "\n", "config", "=", "tf", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "# dynamically grow the memory used on the GPU", "\n", "config", ".", "log_device_placement", "=", "True", "# to log device placement (on which device the operation ran)", "\n", "sess", "=", "tf", ".", "Session", "(", "config", "=", "config", ")", "\n", "set_session", "(", "sess", ")", "# set this TensorFlow session as the default session for Keras", "\n", "\n", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "\n", "# Setup tensorboard logger", "\n", "logger", ".", "configure", "(", "self", ".", "params", ".", "logDir", ",", "format_strs", "=", "[", "'tensorboard'", "]", ")", "\n", "\n", "state_size", "=", "env", ".", "observation_space", ".", "shape", "\n", "action_size_behavior", "=", "env", ".", "action_space", ".", "shape", "\n", "self", ".", "b_agent_attack", "=", "BehaviorModel", "(", "state_size", ",", "action_size_behavior", ",", "self", ".", "comm", ",", "label", "=", "'attack'", ")", "\n", "self", ".", "b_agent_evade", "=", "BehaviorModel", "(", "state_size", ",", "action_size_behavior", ",", "self", ".", "comm", ",", "label", "=", "'evade'", ")", "\n", "self", ".", "b_agent_transit", "=", "BehaviorModel", "(", "state_size", ",", "action_size_behavior", ",", "self", ".", "comm", ",", "label", "=", "'transit'", ")", "\n", "\n", "# Define meta agent", "\n", "self", ".", "m_agent", "=", "MetaAgent", "(", "state_size", ",", "[", "self", ".", "b_agent_attack", ",", "self", ".", "b_agent_evade", ",", "self", ".", "b_agent_transit", "]", ",", "self", ".", "comm", ")", "\n", "\n", "# constants", "\n", "self", ".", "discount_factor", "=", "0.99", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.HPPOPolicy.run": [[57, 144], ["int", "hppo.HPPOPolicy.b_agent_attack.sync_weights", "hppo.HPPOPolicy.b_agent_evade.sync_weights", "hppo.HPPOPolicy.b_agent_transit.sync_weights", "hppo.HPPOPolicy.m_agent.sync_weights", "hppo.HPPOPolicy.env.reset", "stable_baselines.logger.dumpkvs", "print", "hppo.HPPOPolicy.comm.Get_size", "hppo.HPPOPolicy.m_agent.get_action", "hppo.HPPOPolicy.env.step", "training_state[].append", "training_action[].append", "training_reward[].append", "training_next_state[].append", "training_done[].append", "enumerate", "stable_baselines.logger.logkv", "numpy.invert().astype", "arena5.algos.hppo.GAE.GAE", "stable_baselines.common.Dataset", "range", "training_state[].append", "training_action[].append", "training_reward[].append", "training_next_state[].append", "training_done[].append", "reward_sum.items", "dict", "enumerate", "numpy.invert", "stable_baselines.common.Dataset.iterate_once", "model.train", "reward.items", "numpy.asarray", "numpy.asarray", "numpy.asarray", "numpy.asarray", "len"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.MetaAgent.get_action", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.GAE.GAE", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.my_policy.PPOCuriosity.train"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "        ", "local_steps", "=", "int", "(", "num_steps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "\n", "steps", "=", "0", "\n", "\n", "# TODO: Add alpha annealing over num_steps", "\n", "\n", "while", "True", ":", "\n", "\n", "# sync weights", "\n", "            ", "self", ".", "b_agent_attack", ".", "sync_weights", "(", ")", "\n", "self", ".", "b_agent_evade", ".", "sync_weights", "(", ")", "\n", "self", ".", "b_agent_transit", ".", "sync_weights", "(", ")", "\n", "self", ".", "m_agent", ".", "sync_weights", "(", ")", "\n", "\n", "# create placeholders to store experience that we gather", "\n", "training_state", "=", "{", "\"meta\"", ":", "[", "]", ",", "\"attack\"", ":", "[", "]", ",", "\"evade\"", ":", "[", "]", ",", "\"transit\"", ":", "[", "]", "}", "\n", "training_action", "=", "{", "\"meta\"", ":", "[", "]", ",", "\"attack\"", ":", "[", "]", ",", "\"evade\"", ":", "[", "]", ",", "\"transit\"", ":", "[", "]", "}", "\n", "training_reward", "=", "{", "\"meta\"", ":", "[", "]", ",", "\"attack\"", ":", "[", "]", ",", "\"evade\"", ":", "[", "]", ",", "\"transit\"", ":", "[", "]", "}", "\n", "training_next_state", "=", "{", "\"meta\"", ":", "[", "]", ",", "\"attack\"", ":", "[", "]", ",", "\"evade\"", ":", "[", "]", ",", "\"transit\"", ":", "[", "]", "}", "\n", "training_done", "=", "{", "\"meta\"", ":", "[", "]", ",", "\"attack\"", ":", "[", "]", ",", "\"evade\"", ":", "[", "]", ",", "\"transit\"", ":", "[", "]", "}", "\n", "training_reward_sum_combined", "=", "0", "# keep track of combined reward over all episodes between training", "\n", "\n", "state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "reward_sum", "=", "{", "}", "\n", "done", "=", "False", "\n", "while", "not", "done", ":", "\n", "                ", "complete_action", ",", "distribution", ",", "beh_actions", ",", "label", "=", "self", ".", "m_agent", ".", "get_action", "(", "state", ")", "\n", "\n", "next_state", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "complete_action", ",", "dst", "=", "distribution", ",", "label", "=", "label", ")", "\n", "\n", "# Aggregate reward throughout the episode", "\n", "if", "not", "reward_sum", ":", "\n", "                    ", "reward_sum", "=", "reward", "\n", "", "else", ":", "\n", "                    ", "reward_sum", "=", "{", "k", ":", "reward_sum", "[", "k", "]", "+", "reward", "[", "k", "]", "for", "(", "k", ",", "v", ")", "in", "reward", ".", "items", "(", ")", "}", "\n", "", "training_reward_sum_combined", "+=", "reward", "[", "\"combined\"", "]", "\n", "\n", "training_state", "[", "\"meta\"", "]", ".", "append", "(", "state", ")", "\n", "training_action", "[", "\"meta\"", "]", ".", "append", "(", "distribution", ")", "\n", "training_reward", "[", "\"meta\"", "]", ".", "append", "(", "reward", "[", "\"combined\"", "]", ")", "\n", "training_next_state", "[", "\"meta\"", "]", ".", "append", "(", "next_state", ")", "\n", "training_done", "[", "\"meta\"", "]", ".", "append", "(", "done", ")", "\n", "\n", "for", "idx", ",", "label", "in", "enumerate", "(", "[", "'attack'", ",", "'evade'", ",", "'transit'", "]", ")", ":", "\n", "                    ", "training_state", "[", "label", "]", ".", "append", "(", "state", ")", "\n", "training_action", "[", "label", "]", ".", "append", "(", "beh_actions", "[", "idx", "]", ")", "\n", "training_reward", "[", "label", "]", ".", "append", "(", "reward", "[", "label", "]", ")", "\n", "training_next_state", "[", "label", "]", ".", "append", "(", "next_state", ")", "\n", "training_done", "[", "label", "]", ".", "append", "(", "done", ")", "\n", "\n", "", "state", "=", "next_state", "\n", "\n", "# #now we have batches of data: compute the values and advantages", "\n", "# training_value = {\"meta\": None, \"attack\": None, \"evade\": None, \"transit\": None}", "\n", "# training_advantages = {\"meta\": None, \"attack\": None, \"evade\": None, \"transit\": None}", "\n", "\n", "# log tensorboard", "\n", "", "{", "logger", ".", "logkv", "(", "k", ",", "v", ")", "for", "(", "k", ",", "v", ")", "in", "reward_sum", ".", "items", "(", ")", "}", "\n", "logger", ".", "dumpkvs", "(", ")", "\n", "\n", "# vcompute advantages and values", "\n", "models", "=", "[", "self", ".", "b_agent_attack", ",", "self", ".", "b_agent_evade", ",", "self", ".", "b_agent_transit", ",", "self", ".", "m_agent", "]", "\n", "for", "model", "in", "models", ":", "\n", "\n", "                ", "network", "=", "model", ".", "label", "\n", "\n", "states", "=", "training_state", "[", "network", "]", "\n", "actions", "=", "training_action", "[", "network", "]", "\n", "reward", "=", "training_reward", "[", "network", "]", "\n", "next_states", "=", "training_next_state", "[", "network", "]", "\n", "done", "=", "training_done", "[", "network", "]", "\n", "\n", "# Convert done bools to ints and invert", "\n", "done_int", "=", "np", ".", "invert", "(", "done", ")", ".", "astype", "(", "np", ".", "int", ")", "\n", "\n", "# Generalized advantage estimation (gets advantages to train on and value estimates)", "\n", "target", ",", "advantages", "=", "GAE", "(", "states", ",", "actions", ",", "reward", ",", "next_states", ",", "done_int", ",", "model", ".", "sample_value", ",", "T", "=", "128", ",", "y", "=", "0.99", ",", "lam", "=", "0.95", ",", "use_Q", "=", "False", ")", "\n", "\n", "# train this model", "\n", "dataset", "=", "Dataset", "(", "dict", "(", "ob", "=", "np", ".", "asarray", "(", "states", ")", ",", "ac", "=", "np", ".", "asarray", "(", "actions", ")", ",", "atarg", "=", "np", ".", "asarray", "(", "advantages", ")", ",", "vtarg", "=", "np", ".", "asarray", "(", "target", ")", ")", ",", "shuffle", "=", "True", ")", "\n", "\n", "for", "k", "in", "range", "(", "4", ")", ":", "\n", "                    ", "for", "i", ",", "batch", "in", "enumerate", "(", "dataset", ".", "iterate_once", "(", "len", "(", "states", ")", ")", ")", ":", "\n", "                        ", "model", ".", "train", "(", "batch", "[", "\"ob\"", "]", ",", "batch", "[", "\"ac\"", "]", ",", "batch", "[", "\"vtarg\"", "]", ",", "batch", "[", "\"atarg\"", "]", ",", "1.0", ")", "\n", "\n", "", "", "", "print", "(", "'FINISHED TRAINING EPISODE'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.BehaviorModel.__init__": [[256, 263], ["hppo.general_actor_critic"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.general_actor_critic"], ["    ", "def", "__init__", "(", "self", ",", "input_shape", ",", "output_shape", ",", "comm", ",", "label", "=", "'behavior'", ")", ":", "\n", "        ", "self", ".", "label", "=", "label", "\n", "self", ".", "input_shape", "=", "input_shape", "\n", "self", ".", "output_shape", "=", "output_shape", "\n", "\n", "# These are all methods!  Use them to interact with the ppo models.", "\n", "self", ".", "sync_weights", ",", "self", ".", "sample_action", ",", "self", ".", "sample_value", ",", "self", ".", "train", "=", "general_actor_critic", "(", "self", ".", "input_shape", ",", "self", ".", "output_shape", ",", "comm", ",", "label", "=", "self", ".", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.BehaviorModel.get_action": [[264, 267], ["hppo.BehaviorModel.sample_action", "numpy.asarray"], "methods", ["None"], ["", "def", "get_action", "(", "self", ",", "state", ")", ":", "\n", "        ", "action", "=", "self", ".", "sample_action", "(", "np", ".", "asarray", "(", "[", "state", "]", ")", ")", "[", "0", "]", "# batch dimension 1", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.MetaAgent.__init__": [[270, 276], ["len", "hppo.general_actor_critic"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.general_actor_critic"], ["    ", "def", "__init__", "(", "self", ",", "input_shape", ",", "behavior_primitive_mdls", ",", "comm", ")", ":", "\n", "        ", "self", ".", "label", "=", "\"meta\"", "\n", "self", ".", "behavior_primitive_mdls", "=", "behavior_primitive_mdls", "\n", "self", ".", "input_shape", "=", "input_shape", "\n", "self", ".", "output_shape", "=", "len", "(", "behavior_primitive_mdls", ")", "\n", "self", ".", "sync_weights", ",", "self", ".", "sample_action", ",", "self", ".", "sample_value", ",", "self", ".", "train", "=", "general_actor_critic", "(", "self", ".", "input_shape", ",", "self", ".", "output_shape", ",", "comm", ",", "label", "=", "self", ".", "label", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.MetaAgent.get_action": [[277, 294], ["numpy.array", "numpy.argmax", "numpy.tensordot", "hppo.MetaAgent.sample_action", "numpy.exp", "sum", "numpy.expand_dims", "numpy.asarray", "bm.get_action", "numpy.exp"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.MetaAgent.get_action"], ["", "def", "get_action", "(", "self", ",", "state", ")", ":", "\n", "        ", "meta_action", "=", "self", ".", "sample_action", "(", "np", ".", "asarray", "(", "[", "state", "]", ")", ")", "[", "0", "]", "\n", "beh_actions", "=", "np", ".", "array", "(", "[", "bm", ".", "get_action", "(", "state", ")", "for", "bm", "in", "self", ".", "behavior_primitive_mdls", "]", ")", "\n", "\n", "meta_action_softmax", "=", "np", ".", "exp", "(", "meta_action", ")", "/", "sum", "(", "np", ".", "exp", "(", "meta_action", ")", ")", "\n", "\n", "# Get the argmax of the softmax", "\n", "meta_action_argmax", "=", "np", ".", "argmax", "(", "meta_action_softmax", ",", "axis", "=", "-", "1", ")", "\n", "\n", "# Get label from argmax", "\n", "label", "=", "self", ".", "behavior_primitive_mdls", "[", "meta_action_argmax", "]", ".", "label", "\n", "\n", "# Should be doing a vectorized dot product", "\n", "complete_action", "=", "np", ".", "tensordot", "(", "beh_actions", ",", "np", ".", "expand_dims", "(", "meta_action_softmax", ",", "axis", "=", "0", ")", ",", "axes", "=", "[", "0", ",", "1", "]", ")", "\n", "# complete_action = np.tensordot(meta_action_softmax, beh_actions, axes=[0, 1])", "\n", "\n", "return", "complete_action", ",", "meta_action_softmax", ",", "beh_actions", ",", "label", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.general_actor_critic": [[146, 222], ["tensorflow.keras.backend.get_session", "numpy.random.seed", "tensorflow.set_random_seed", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "hppo.ppo_continuous_loss", "stable_baselines.common.tf_util.function", "tensorflow.global_variables_initializer", "K.get_session.run", "stable_baselines.common.mpi_adam.MpiAdam.sync", "hppo.general_actor_critic.sync_weights"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed", "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.ppo_continuous_loss", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.env_process.EnvironmentProcess.run", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights"], ["", "", "", "def", "general_actor_critic", "(", "input_shape_vec", ",", "act_output_shape", ",", "comm", ",", "learn_rate", "=", "[", "0.001", ",", "0.001", "]", ",", "trainable", "=", "True", ",", "label", "=", "\"\"", ")", ":", "\n", "\n", "    ", "sess", "=", "K", ".", "get_session", "(", ")", "\n", "np", ".", "random", ".", "seed", "(", "0", ")", "\n", "tf", ".", "set_random_seed", "(", "0", ")", "\n", "\n", "# network 1 (new policy)", "\n", "with", "tf", ".", "variable_scope", "(", "label", "+", "\"_pi_new\"", ",", "reuse", "=", "False", ")", ":", "\n", "        ", "inp", "=", "Input", "(", "shape", "=", "input_shape_vec", ")", "# [5,6,3]", "\n", "# rc_lyr = Lambda(lambda x:  ned_to_ripCoords_tf(x, 4000))(inp)", "\n", "trunk_x", "=", "Reshape", "(", "[", "input_shape_vec", "[", "0", "]", ",", "input_shape_vec", "[", "1", "]", "*", "3", "]", ")", "(", "inp", ")", "\n", "trunk_x", "=", "LSTM", "(", "128", ")", "(", "trunk_x", ")", "\n", "dist", ",", "sample_action_op", ",", "action_ph", ",", "value_output", "=", "ppo_continuous", "(", "3", ",", "trunk_x", ")", "\n", "\n", "# network 2 (old policy)", "\n", "", "with", "tf", ".", "variable_scope", "(", "label", "+", "\"_pi_old\"", ",", "reuse", "=", "False", ")", ":", "\n", "        ", "inp_old", "=", "Input", "(", "shape", "=", "input_shape_vec", ")", "# [5,6,3]", "\n", "# rc_lyr = Lambda(lambda x:  ned_to_ripCoords_tf(x, 4000))(inp_old)", "\n", "trunk_x", "=", "Reshape", "(", "[", "input_shape_vec", "[", "0", "]", ",", "input_shape_vec", "[", "1", "]", "*", "3", "]", ")", "(", "inp_old", ")", "\n", "trunk_x", "=", "LSTM", "(", "128", ")", "(", "trunk_x", ")", "\n", "dist_old", ",", "sample_action_op_old", ",", "action_ph_old", ",", "value_output_old", "=", "ppo_continuous", "(", "3", ",", "trunk_x", ")", "\n", "\n", "# additional placeholders", "\n", "", "adv_ph", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ",", "name", "=", "\"advantages_ph\"", ")", "\n", "alpha_ph", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "shape", "=", "(", ")", ",", "name", "=", "\"alpha_ph\"", ")", "\n", "vtarg", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", "]", ")", "# target value placeholder", "\n", "\n", "# loss", "\n", "loss", "=", "ppo_continuous_loss", "(", "dist", ",", "dist_old", ",", "value_output", ",", "action_ph", ",", "alpha_ph", ",", "adv_ph", ",", "vtarg", ")", "\n", "\n", "# gradient", "\n", "with", "tf", ".", "variable_scope", "(", "\"grad\"", ",", "reuse", "=", "False", ")", ":", "\n", "        ", "gradient", "=", "tf_util", ".", "flatgrad", "(", "loss", ",", "tf_util", ".", "get_trainable_vars", "(", "label", "+", "\"_pi_new\"", ")", ")", "\n", "adam", "=", "MpiAdam", "(", "tf_util", ".", "get_trainable_vars", "(", "label", "+", "\"_pi_new\"", ")", ",", "epsilon", "=", "0.00001", ",", "sess", "=", "sess", ",", "comm", "=", "comm", ")", "\n", "\n", "# method for sync'ing the two policies", "\n", "", "assign_old_eq_new", "=", "tf_util", ".", "function", "(", "[", "]", ",", "[", "]", ",", "updates", "=", "[", "tf", ".", "assign", "(", "oldv", ",", "newv", ")", "for", "(", "oldv", ",", "newv", ")", "in", "\n", "zipsame", "(", "tf_util", ".", "get_globals_vars", "(", "label", "+", "\"_pi_old\"", ")", ",", "tf_util", ".", "get_globals_vars", "(", "label", "+", "\"_pi_new\"", ")", ")", "]", ")", "\n", "\n", "# initialize all the things", "\n", "init_op", "=", "tf", ".", "global_variables_initializer", "(", ")", "\n", "sess", ".", "run", "(", "init_op", ")", "\n", "\n", "# methods for interacting with this model", "\n", "\n", "def", "sync_weights", "(", ")", ":", "\n", "        ", "assign_old_eq_new", "(", "sess", "=", "sess", ")", "\n", "\n", "", "def", "sample_action", "(", "states", ",", "logstd_override", "=", "None", ")", ":", "\n", "        ", "a", "=", "sess", ".", "run", "(", "sample_action_op", ",", "feed_dict", "=", "{", "inp", ":", "states", "}", ")", "\n", "return", "a", "\n", "\n", "", "def", "sample_value", "(", "states", ")", ":", "\n", "        ", "v", "=", "sess", ".", "run", "(", "value_output", ",", "feed_dict", "=", "{", "inp", ":", "states", "}", ")", "\n", "return", "v", "\n", "\n", "", "def", "train", "(", "states", ",", "actions", ",", "vtarget", ",", "advs", ",", "alpha", ")", ":", "\n", "        ", "alpha", "=", "max", "(", "alpha", ",", "0.0", ")", "\n", "adam_lr", "=", "learn_rate", "[", "0", "]", "\n", "\n", "g", "=", "sess", ".", "run", "(", "[", "gradient", "]", ",", "feed_dict", "=", "{", "\n", "inp", ":", "states", ",", "\n", "inp_old", ":", "states", ",", "\n", "action_ph", ":", "actions", ",", "\n", "adv_ph", ":", "advs", ",", "\n", "alpha_ph", ":", "alpha", ",", "\n", "vtarg", ":", "vtarget", "\n", "}", ")", "\n", "\n", "adam", ".", "update", "(", "g", "[", "0", "]", ",", "adam_lr", "*", "alpha", ")", "\n", "\n", "# initial sync", "\n", "", "adam", ".", "sync", "(", ")", "\n", "sync_weights", "(", ")", "\n", "\n", "return", "sync_weights", ",", "sample_action", ",", "sample_value", ",", "train", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.ppo_continuous": [[224, 240], ["tensorflow.placeholder", "tensorflow.get_variable", "tensorflow.concat", "stable_baselines.common.distributions.DiagGaussianProbabilityDistribution", "stable_baselines.common.distributions.DiagGaussianProbabilityDistribution.sample", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Dense", "tensorflow.zeros_initializer"], "function", ["None"], ["", "def", "ppo_continuous", "(", "num_actions", ",", "previous_layer", ")", ":", "\n", "\n", "# act distribution", "\n", "    ", "action_ph", "=", "tf", ".", "placeholder", "(", "tf", ".", "float32", ",", "[", "None", ",", "num_actions", "]", ",", "name", "=", "\"actions_ph\"", ")", "\n", "means", "=", "Dense", "(", "num_actions", ",", "activation", "=", "\"linear\"", ",", "kernel_initializer", "=", "ORTHO_01", ")", "(", "previous_layer", ")", "\n", "vlogstd", "=", "tf", ".", "get_variable", "(", "name", "=", "'pi/vlogstd'", ",", "shape", "=", "[", "1", ",", "num_actions", "]", ",", "initializer", "=", "tf", ".", "zeros_initializer", "(", ")", ")", "\n", "means_and_logstd", "=", "tf", ".", "concat", "(", "[", "means", ",", "means", "*", "0.0", "+", "vlogstd", "]", ",", "1", ")", "\n", "distribution", "=", "DiagGaussianProbabilityDistribution", "(", "means_and_logstd", ")", "\n", "\n", "# sample op", "\n", "sample_action_op", "=", "distribution", ".", "sample", "(", ")", "\n", "\n", "# value", "\n", "value_output", "=", "Dense", "(", "1", ")", "(", "previous_layer", ")", "\n", "\n", "return", "distribution", ",", "sample_action_op", ",", "action_ph", ",", "value_output", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.hppo.ppo_continuous_loss": [[242, 253], ["tensorflow.exp", "tensorflow.reduce_mean", "tensorflow.clip_by_value", "tensorflow.reduce_mean", "tensorflow.square", "new_dist.logp", "old_dist.logp", "tensorflow.minimum"], "function", ["None"], ["", "def", "ppo_continuous_loss", "(", "new_dist", ",", "old_dist", ",", "value_output", ",", "actions_ph", ",", "alpha_ph", ",", "adv_ph", ",", "val_ph", ",", "clipping_epsilon", "=", "0.2", ")", ":", "\n", "\n", "    ", "ratio", "=", "tf", ".", "exp", "(", "new_dist", ".", "logp", "(", "actions_ph", ")", "-", "old_dist", ".", "logp", "(", "actions_ph", ")", ")", "\n", "epsilon", "=", "clipping_epsilon", "*", "alpha_ph", "\n", "surr1", "=", "ratio", "*", "adv_ph", "\n", "surr2", "=", "tf", ".", "clip_by_value", "(", "ratio", ",", "1.0", "-", "epsilon", ",", "1.0", "+", "epsilon", ")", "*", "adv_ph", "\n", "ploss", "=", "-", "tf", ".", "reduce_mean", "(", "tf", ".", "minimum", "(", "surr1", ",", "surr2", ")", ")", "\n", "vloss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "value_output", "-", "val_ph", ")", ")", "\n", "loss", "=", "ploss", "+", "vloss", "\n", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.GAE.GAE": [[5, 53], ["numpy.asarray", "numpy.asarray", "numpy.asarray", "range", "numpy.squeeze", "numpy.squeeze", "value_sampler", "value_sampler", "len", "min", "range", "numpy.asarray", "numpy.sum", "np.squeeze.append", "np.squeeze.append", "numpy.asarray", "numpy.asarray", "len", "np.asarray.append", "numpy.asarray", "numpy.asarray"], "function", ["None"], ["def", "GAE", "(", "states", ",", "actions", ",", "rewards", ",", "nexts", ",", "dones", ",", "value_sampler", ",", "T", "=", "128", ",", "y", "=", "0.99", ",", "lam", "=", "0.95", ",", "use_Q", "=", "False", ")", ":", "\n", "\n", "    ", "r", "=", "np", ".", "asarray", "(", "rewards", ")", "\n", "d", "=", "np", ".", "asarray", "(", "dones", ")", "\n", "\n", "a", "=", "actions", "# want this to remain a list", "\n", "\n", "all_states", "=", "np", ".", "asarray", "(", "states", "+", "[", "nexts", "[", "-", "1", "]", "]", ")", "\n", "\n", "if", "use_Q", ":", "\n", "        ", "all_values", "=", "value_sampler", "(", "all_states", ",", "actions", ")", "\n", "", "else", ":", "\n", "        ", "all_values", "=", "value_sampler", "(", "all_states", ")", "\n", "", "V_s", "=", "all_values", "[", ":", "-", "1", "]", "\n", "V_ns", "=", "all_values", "[", "1", ":", "]", "\n", "\n", "val_target", "=", "[", "]", "\n", "adv_target", "=", "[", "]", "\n", "\n", "#find the target values and target advantages", "\n", "for", "i", "in", "range", "(", "len", "(", "states", ")", ")", ":", "\n", "\n", "        ", "remaining", "=", "len", "(", "states", ")", "-", "i", "# remaining experience, including this entry", "\n", "\n", "adv_returns", "=", "[", "]", "\n", "coef", "=", "1.0", "\n", "\n", "roll_length", "=", "min", "(", "T", ",", "remaining", ")", "\n", "for", "j", "in", "range", "(", "roll_length", ")", ":", "\n", "\n", "            ", "delta_t", "=", "r", "[", "i", "+", "j", "]", "+", "y", "*", "V_ns", "[", "i", "+", "j", "]", "*", "d", "[", "i", "+", "j", "]", "-", "V_s", "[", "i", "+", "j", "]", "\n", "adv_returns", ".", "append", "(", "coef", "*", "delta_t", ")", "\n", "coef", "*=", "(", "y", "*", "lam", ")", "\n", "\n", "#if done, end calculation", "\n", "if", "d", "[", "i", "+", "j", "]", "<", "0.1", ":", "# robust check for done==0.0", "\n", "                ", "break", "\n", "\n", "", "", "adv_returns", "=", "np", ".", "asarray", "(", "adv_returns", ")", "\n", "adv", "=", "np", ".", "sum", "(", "adv_returns", ")", "\n", "adv_target", ".", "append", "(", "np", ".", "asarray", "(", "adv", ")", ")", "\n", "val_target", ".", "append", "(", "np", ".", "asarray", "(", "adv", "+", "V_s", "[", "i", "]", ")", ")", "\n", "\n", "#convert to np arrays to return", "\n", "", "val_target", "=", "np", ".", "squeeze", "(", "np", ".", "asarray", "(", "val_target", ")", ")", "\n", "adv_target", "=", "np", ".", "squeeze", "(", "np", ".", "asarray", "(", "adv_target", ")", ")", "\n", "\n", "return", "val_target", ",", "adv_target", "\n", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.hppo.utils.ned_to_ripCoords_tf": [[6, 18], ["tensorflow.cast", "tensorflow.cast", "tensorflow.reduce_sum", "tensorflow.atan2", "tensorflow.atan2", "tensorflow.stack", "tensorflow.square", "tensorflow.square", "tensorflow.sqrt", "tensorflow.expand_dims", "tensorflow.clip_by_value", "tensorflow.sin", "tensorflow.cos", "tensorflow.sin", "tensorflow.cos", "tensorflow.sqrt"], "function", ["None"], ["def", "ned_to_ripCoords_tf", "(", "xyz", ",", "max_det", ")", ":", "\n", "    ", "xyz", "=", "tf", ".", "cast", "(", "xyz", ",", "tf", ".", "float32", ")", "\n", "max_det", "=", "tf", ".", "cast", "(", "max_det", ",", "tf", ".", "float32", ")", "\n", "xy", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "xyz", "[", ":", ",", ":", ",", ":", ",", ":", "1", "]", ")", ",", "axis", "=", "-", "1", ")", "\n", "r_tmp", "=", "xy", "+", "tf", ".", "square", "(", "xyz", "[", ":", ",", ":", ",", ":", ",", "2", "]", ")", "\n", "r_tmp", "=", "(", "max_det", "-", "tf", ".", "clip_by_value", "(", "tf", ".", "sqrt", "(", "r_tmp", ")", ",", "0", ",", "max_det", ")", ")", "/", "max_det", "\n", "thet_tmp", "=", "tf", ".", "atan2", "(", "tf", ".", "sqrt", "(", "xy", ")", ",", "xyz", "[", ":", ",", ":", ",", ":", ",", "2", "]", ")", "\n", "phi_tmp", "=", "tf", ".", "atan2", "(", "xyz", "[", ":", ",", ":", ",", ":", ",", "1", "]", ",", "xyz", "[", ":", ",", ":", ",", ":", ",", "0", "]", ")", "\n", "rip_crd", "=", "tf", ".", "stack", "(", "[", "tf", ".", "sin", "(", "thet_tmp", ")", ",", "tf", ".", "cos", "(", "thet_tmp", ")", ",", "tf", ".", "sin", "(", "phi_tmp", ")", ",", "tf", ".", "cos", "(", "phi_tmp", ")", "]", ",", "axis", "=", "-", "1", ")", "\n", "rip_crd", "=", "tf", ".", "expand_dims", "(", "r_tmp", ",", "axis", "=", "-", "1", ")", "*", "rip_crd", "\n", "\n", "return", "rip_crd", "\n", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.__init__": [[50, 92], ["stable_baselines.common.ActorCriticRLModel.__init__", "ppo1_mod.PPO1.setup_model"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.setup_model"], ["def", "__init__", "(", "self", ",", "policy", ",", "env", ",", "comm", ",", "gamma", "=", "0.99", ",", "timesteps_per_actorbatch", "=", "256", ",", "clip_param", "=", "0.2", ",", "entcoeff", "=", "0.01", ",", "\n", "optim_epochs", "=", "4", ",", "optim_stepsize", "=", "1e-3", ",", "optim_batchsize", "=", "64", ",", "lam", "=", "0.95", ",", "adam_epsilon", "=", "1e-5", ",", "\n", "schedule", "=", "'linear'", ",", "verbose", "=", "0", ",", "tensorboard_log", "=", "None", ",", "\n", "_init_setup_model", "=", "True", ",", "policy_kwargs", "=", "None", ",", "full_tensorboard_log", "=", "False", ",", "clip_rewards", "=", "False", ")", ":", "\n", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "policy", "=", "policy", ",", "env", "=", "env", ",", "verbose", "=", "verbose", ",", "requires_vec_env", "=", "False", ",", "\n", "_init_setup_model", "=", "_init_setup_model", ",", "policy_kwargs", "=", "policy_kwargs", ")", "\n", "\n", "self", ".", "gamma", "=", "gamma", "\n", "self", ".", "timesteps_per_actorbatch", "=", "timesteps_per_actorbatch", "\n", "self", ".", "clip_param", "=", "clip_param", "\n", "self", ".", "entcoeff", "=", "entcoeff", "\n", "self", ".", "optim_epochs", "=", "optim_epochs", "\n", "self", ".", "optim_stepsize", "=", "optim_stepsize", "\n", "self", ".", "optim_batchsize", "=", "optim_batchsize", "\n", "self", ".", "lam", "=", "lam", "\n", "self", ".", "adam_epsilon", "=", "adam_epsilon", "\n", "self", ".", "schedule", "=", "schedule", "\n", "self", ".", "tensorboard_log", "=", "tensorboard_log", "\n", "self", ".", "full_tensorboard_log", "=", "full_tensorboard_log", "\n", "\n", "self", ".", "graph", "=", "None", "\n", "self", ".", "sess", "=", "None", "\n", "self", ".", "policy_pi", "=", "None", "\n", "self", ".", "loss_names", "=", "None", "\n", "self", ".", "lossandgrad", "=", "None", "\n", "self", ".", "adam", "=", "None", "\n", "self", ".", "assign_old_eq_new", "=", "None", "\n", "self", ".", "compute_losses", "=", "None", "\n", "self", ".", "params", "=", "None", "\n", "self", ".", "step", "=", "None", "\n", "self", ".", "proba_step", "=", "None", "\n", "self", ".", "initial_state", "=", "None", "\n", "self", ".", "summary", "=", "None", "\n", "self", ".", "episode_reward", "=", "None", "\n", "\n", "self", ".", "clip_rewards", "=", "clip_rewards", "\n", "\n", "self", ".", "comm", "=", "comm", "\n", "\n", "if", "_init_setup_model", ":", "\n", "            ", "self", ".", "setup_model", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1._get_pretrain_placeholders": [[93, 99], ["policy.pdtype.sample_placeholder", "isinstance"], "methods", ["None"], ["", "", "def", "_get_pretrain_placeholders", "(", "self", ")", ":", "\n", "        ", "policy", "=", "self", ".", "policy_pi", "\n", "action_ph", "=", "policy", ".", "pdtype", ".", "sample_placeholder", "(", "[", "None", "]", ")", "\n", "if", "isinstance", "(", "self", ".", "action_space", ",", "gym", ".", "spaces", ".", "Discrete", ")", ":", "\n", "            ", "return", "policy", ".", "obs_ph", ",", "action_ph", ",", "policy", ".", "policy", "\n", "", "return", "policy", ".", "obs_ph", ",", "action_ph", ",", "policy", ".", "deterministic_action", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.setup_model": [[100, 197], ["stable_baselines.common.SetVerbosity", "tensorflow.Graph", "ppo1_mod.PPO1.graph.as_default", "stable_baselines.single_threaded_session", "ppo1_mod.PPO1.policy", "stable_baselines.initialize", "tensorflow.summary.merge_all", "stable_baselines.function", "stable_baselines.function", "tensorflow.variable_scope", "ppo1_mod.PPO1.policy", "tensorflow.variable_scope", "tensorflow.placeholder", "tensorflow.placeholder", "tensorflow.placeholder", "ppo1_mod.PPO1.policy_pi.pdtype.sample_placeholder", "ppo1_mod.PPO1.proba_distribution.kl", "ppo1_mod.PPO1.policy_pi.proba_distribution.entropy", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.exp", "tensorflow.reduce_mean", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "stable_baselines.get_trainable_vars", "stable_baselines.function", "tensorflow.variable_scope", "stable_baselines.common.mpi_adam.MpiAdam", "tensorflow.variable_scope", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.summary.scalar", "tensorflow.clip_by_value", "tensorflow.reduce_mean", "tensorflow.square", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.reduce_mean", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "tensorflow.summary.histogram", "stable_baselines.is_image", "ppo1_mod.PPO1.policy_pi.proba_distribution.logp", "ppo1_mod.PPO1.proba_distribution.logp", "tensorflow.minimum", "tensorflow.summary.image", "tensorflow.summary.histogram", "stable_baselines.flatgrad", "tensorflow.assign", "stable_baselines.common.zipsame", "stable_baselines.get_globals_vars", "stable_baselines.get_globals_vars"], "methods", ["None"], ["", "def", "setup_model", "(", "self", ")", ":", "\n", "        ", "with", "SetVerbosity", "(", "self", ".", "verbose", ")", ":", "\n", "\n", "            ", "self", ".", "graph", "=", "tf", ".", "Graph", "(", ")", "\n", "with", "self", ".", "graph", ".", "as_default", "(", ")", ":", "\n", "                ", "self", ".", "sess", "=", "tf_util", ".", "single_threaded_session", "(", "graph", "=", "self", ".", "graph", ")", "\n", "\n", "# Construct network for new policy", "\n", "self", ".", "policy_pi", "=", "self", ".", "policy", "(", "self", ".", "sess", ",", "self", ".", "observation_space", ",", "self", ".", "action_space", ",", "self", ".", "n_envs", ",", "1", ",", "\n", "None", ",", "reuse", "=", "False", ",", "**", "self", ".", "policy_kwargs", ")", "\n", "\n", "# Network for old policy", "\n", "with", "tf", ".", "variable_scope", "(", "\"oldpi\"", ",", "reuse", "=", "False", ")", ":", "\n", "                    ", "old_pi", "=", "self", ".", "policy", "(", "self", ".", "sess", ",", "self", ".", "observation_space", ",", "self", ".", "action_space", ",", "self", ".", "n_envs", ",", "1", ",", "\n", "None", ",", "reuse", "=", "False", ",", "**", "self", ".", "policy_kwargs", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"loss\"", ",", "reuse", "=", "False", ")", ":", "\n", "# Target advantage function (if applicable)", "\n", "                    ", "atarg", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "None", "]", ")", "\n", "\n", "# Empirical return", "\n", "ret", "=", "tf", ".", "placeholder", "(", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "None", "]", ")", "\n", "\n", "# learning rate multiplier, updated with schedule", "\n", "lrmult", "=", "tf", ".", "placeholder", "(", "name", "=", "'lrmult'", ",", "dtype", "=", "tf", ".", "float32", ",", "shape", "=", "[", "]", ")", "\n", "\n", "# Annealed cliping parameter epislon", "\n", "clip_param", "=", "self", ".", "clip_param", "*", "lrmult", "\n", "\n", "obs_ph", "=", "self", ".", "policy_pi", ".", "obs_ph", "\n", "action_ph", "=", "self", ".", "policy_pi", ".", "pdtype", ".", "sample_placeholder", "(", "[", "None", "]", ")", "\n", "\n", "kloldnew", "=", "old_pi", ".", "proba_distribution", ".", "kl", "(", "self", ".", "policy_pi", ".", "proba_distribution", ")", "\n", "ent", "=", "self", ".", "policy_pi", ".", "proba_distribution", ".", "entropy", "(", ")", "\n", "meankl", "=", "tf", ".", "reduce_mean", "(", "kloldnew", ")", "\n", "meanent", "=", "tf", ".", "reduce_mean", "(", "ent", ")", "\n", "pol_entpen", "=", "(", "-", "self", ".", "entcoeff", ")", "*", "meanent", "\n", "\n", "# pnew / pold", "\n", "ratio", "=", "tf", ".", "exp", "(", "self", ".", "policy_pi", ".", "proba_distribution", ".", "logp", "(", "action_ph", ")", "-", "\n", "old_pi", ".", "proba_distribution", ".", "logp", "(", "action_ph", ")", ")", "\n", "\n", "# surrogate from conservative policy iteration", "\n", "surr1", "=", "ratio", "*", "atarg", "\n", "surr2", "=", "tf", ".", "clip_by_value", "(", "ratio", ",", "1.0", "-", "clip_param", ",", "1.0", "+", "clip_param", ")", "*", "atarg", "\n", "\n", "# PPO's pessimistic surrogate (L^CLIP)", "\n", "pol_surr", "=", "-", "tf", ".", "reduce_mean", "(", "tf", ".", "minimum", "(", "surr1", ",", "surr2", ")", ")", "\n", "vf_loss", "=", "tf", ".", "reduce_mean", "(", "tf", ".", "square", "(", "self", ".", "policy_pi", ".", "value_flat", "-", "ret", ")", ")", "\n", "total_loss", "=", "pol_surr", "+", "pol_entpen", "+", "vf_loss", "\n", "losses", "=", "[", "pol_surr", ",", "pol_entpen", ",", "vf_loss", ",", "meankl", ",", "meanent", "]", "\n", "self", ".", "loss_names", "=", "[", "\"pol_surr\"", ",", "\"pol_entpen\"", ",", "\"vf_loss\"", ",", "\"kl\"", ",", "\"ent\"", "]", "\n", "\n", "tf", ".", "summary", ".", "scalar", "(", "'entropy_loss'", ",", "pol_entpen", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'policy_gradient_loss'", ",", "pol_surr", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'value_function_loss'", ",", "vf_loss", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'approximate_kullback-leibler'", ",", "meankl", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'clip_factor'", ",", "clip_param", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'loss'", ",", "total_loss", ")", "\n", "\n", "self", ".", "params", "=", "tf_util", ".", "get_trainable_vars", "(", "\"model\"", ")", "\n", "\n", "self", ".", "assign_old_eq_new", "=", "tf_util", ".", "function", "(", "\n", "[", "]", ",", "[", "]", ",", "updates", "=", "[", "tf", ".", "assign", "(", "oldv", ",", "newv", ")", "for", "(", "oldv", ",", "newv", ")", "in", "\n", "zipsame", "(", "tf_util", ".", "get_globals_vars", "(", "\"oldpi\"", ")", ",", "tf_util", ".", "get_globals_vars", "(", "\"model\"", ")", ")", "]", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"Adam_mpi\"", ",", "reuse", "=", "False", ")", ":", "\n", "                    ", "self", ".", "adam", "=", "MpiAdam", "(", "self", ".", "params", ",", "epsilon", "=", "self", ".", "adam_epsilon", ",", "sess", "=", "self", ".", "sess", ",", "comm", "=", "self", ".", "comm", ")", "\n", "\n", "", "with", "tf", ".", "variable_scope", "(", "\"input_info\"", ",", "reuse", "=", "False", ")", ":", "\n", "                    ", "tf", ".", "summary", ".", "scalar", "(", "'discounted_rewards'", ",", "tf", ".", "reduce_mean", "(", "ret", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'learning_rate'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "optim_stepsize", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'advantage'", ",", "tf", ".", "reduce_mean", "(", "atarg", ")", ")", "\n", "tf", ".", "summary", ".", "scalar", "(", "'clip_range'", ",", "tf", ".", "reduce_mean", "(", "self", ".", "clip_param", ")", ")", "\n", "\n", "if", "self", ".", "full_tensorboard_log", ":", "\n", "                        ", "tf", ".", "summary", ".", "histogram", "(", "'discounted_rewards'", ",", "ret", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'learning_rate'", ",", "self", ".", "optim_stepsize", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'advantage'", ",", "atarg", ")", "\n", "tf", ".", "summary", ".", "histogram", "(", "'clip_range'", ",", "self", ".", "clip_param", ")", "\n", "if", "tf_util", ".", "is_image", "(", "self", ".", "observation_space", ")", ":", "\n", "                            ", "tf", ".", "summary", ".", "image", "(", "'observation'", ",", "obs_ph", ")", "\n", "", "else", ":", "\n", "                            ", "tf", ".", "summary", ".", "histogram", "(", "'observation'", ",", "obs_ph", ")", "\n", "\n", "", "", "", "self", ".", "step", "=", "self", ".", "policy_pi", ".", "step", "\n", "self", ".", "proba_step", "=", "self", ".", "policy_pi", ".", "proba_step", "\n", "self", ".", "initial_state", "=", "self", ".", "policy_pi", ".", "initial_state", "\n", "\n", "tf_util", ".", "initialize", "(", "sess", "=", "self", ".", "sess", ")", "\n", "\n", "self", ".", "summary", "=", "tf", ".", "summary", ".", "merge_all", "(", ")", "\n", "\n", "self", ".", "lossandgrad", "=", "tf_util", ".", "function", "(", "[", "obs_ph", ",", "old_pi", ".", "obs_ph", ",", "action_ph", ",", "atarg", ",", "ret", ",", "lrmult", "]", ",", "\n", "[", "self", ".", "summary", ",", "tf_util", ".", "flatgrad", "(", "total_loss", ",", "self", ".", "params", ")", "]", "+", "losses", ")", "\n", "self", ".", "compute_losses", "=", "tf_util", ".", "function", "(", "[", "obs_ph", ",", "old_pi", ".", "obs_ph", ",", "action_ph", ",", "atarg", ",", "ret", ",", "lrmult", "]", ",", "\n", "losses", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.evaluate": [[199, 215], ["int", "ppo1_mod.PPO1.env.reset", "ppo1_mod.PPO1.policy_pi.step", "ppo1_mod.PPO1.env.step", "ppo1_mod.PPO1.comm.Get_size", "ppo1_mod.PPO1.reshape", "ppo1_mod.PPO1.env.reset"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "", "", "def", "evaluate", "(", "self", ",", "total_timesteps", ")", ":", "\n", "\n", "        ", "local_steps", "=", "int", "(", "total_timesteps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "steps", "=", "0", "\n", "observation", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "while", "steps", "<", "local_steps", ":", "\n", "\n", "            ", "action", ",", "_", ",", "_", ",", "_", "=", "self", ".", "policy_pi", ".", "step", "(", "observation", ".", "reshape", "(", "-", "1", ",", "*", "observation", ".", "shape", ")", ")", "\n", "\n", "#step environment", "\n", "observation", ",", "reward", ",", "done", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "if", "done", ":", "\n", "                ", "observation", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "", "steps", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.learn": [[218, 388], ["int", "ppo1_mod.PPO1._init_num_timesteps", "stable_baselines.common.SetVerbosity", "stable_baselines.common.TensorboardWriter", "ppo1_mod.PPO1._setup_learn", "issubclass", "ppo1_mod.PPO1.comm.Get_size", "ppo1_mod.PPO1.sess.as_default", "ppo1_mod.PPO1.adam.sync", "arena5.algos.ppo.seg_gen_limited.traj_segment_generator", "time.time", "collections.deque", "collections.deque", "numpy.zeros", "stable_baselines.logger.log", "stable_baselines.common.Dataset", "ppo1_mod.PPO1.assign_old_eq_new", "stable_baselines.logger.log", "stable_baselines.logger.log", "range", "stable_baselines.logger.log", "stable_baselines.common.Dataset.iterate_once", "stable_baselines.common.mpi_moments.mpi_moments", "stable_baselines.logger.log", "stable_baselines.common.zipsame", "stable_baselines.logger.record_tabular", "ppo1_mod.PPO1.comm.allgather", "map", "collections.deque.extend", "collections.deque.extend", "stable_baselines.logger.record_tabular", "len", "ppo1_mod.PPO1.comm.allreduce", "stable_baselines.logger.record_tabular", "stable_baselines.logger.record_tabular", "stable_baselines.logger.record_tabular", "float", "ppo1_mod.PPO1.comm.Get_size", "arena5.algos.ppo.seg_gen_limited.traj_segment_generator.__next__", "stable_baselines.trpo_mpi.utils.add_vtarg_and_adv", "arena5.algos.ppo.seg_gen_limited.traj_segment_generator.__next__", "stable_baselines.a2c.utils.total_episode_reward_logger", "atarg.std", "dict", "stable_baselines.common.fmt_row", "enumerate", "stable_baselines.logger.log", "ppo1_mod.PPO1.compute_losses", "losses.append", "stable_baselines.common.fmt_row", "stable_baselines.logger.record_tabular", "stable_baselines.common.explained_variance", "zip", "range", "policy_record.save", "len", "stable_baselines.logger.record_tabular", "stable_baselines.logger.record_tabular", "len", "stable_baselines.logger.dump_tabular", "callback", "max", "seg[].reshape", "seg[].reshape", "atarg.mean", "stable_baselines.common.Dataset.iterate_once", "ppo1_mod.PPO1.adam.update", "losses.append", "stable_baselines.common.fmt_row", "len", "policy_record.add_result", "numpy.mean", "numpy.mean", "time.time", "ppo1_mod.PPO1.comm.Get_rank", "locals", "globals", "int", "writer.add_summary", "ppo1_mod.PPO1.lossandgrad", "numpy.mean", "tensorflow.RunOptions", "tensorflow.RunMetadata", "ppo1_mod.PPO1.lossandgrad", "writer.add_run_metadata", "ppo1_mod.PPO1.lossandgrad", "float", "len"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.seg_gen_limited.traj_segment_generator", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.add_result"], ["", "", "def", "learn", "(", "self", ",", "total_timesteps", ",", "policy_record", "=", "None", ",", "callback", "=", "None", ",", "seed", "=", "None", ",", "log_interval", "=", "100", ",", "tb_log_name", "=", "\"PPO1\"", ",", "\n", "reset_num_timesteps", "=", "True", ")", ":", "\n", "\n", "#compute the number of local steps that are needed", "\n", "        ", "local_steps", "=", "int", "(", "total_timesteps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "\n", "new_tb_log", "=", "self", ".", "_init_num_timesteps", "(", "reset_num_timesteps", ")", "\n", "\n", "with", "SetVerbosity", "(", "self", ".", "verbose", ")", ",", "TensorboardWriter", "(", "self", ".", "graph", ",", "self", ".", "tensorboard_log", ",", "tb_log_name", ",", "new_tb_log", ")", "as", "writer", ":", "\n", "            ", "self", ".", "_setup_learn", "(", ")", "\n", "\n", "assert", "issubclass", "(", "self", ".", "policy", ",", "ActorCriticPolicy", ")", ",", "\"Error: the input policy for the PPO1 model must be \"", "\"an instance of common.policies.ActorCriticPolicy.\"", "\n", "\n", "with", "self", ".", "sess", ".", "as_default", "(", ")", ":", "\n", "                ", "self", ".", "adam", ".", "sync", "(", ")", "\n", "\n", "# Prepare for rollouts", "\n", "# Note we are using modified traj_gen", "\n", "seg_gen", "=", "traj_segment_generator", "(", "self", ".", "policy_pi", ",", "self", ".", "env", ",", "self", ".", "timesteps_per_actorbatch", ",", "local_steps", ",", "self", ".", "clip_rewards", ")", "\n", "\n", "episodes_so_far", "=", "0", "\n", "timesteps_so_far", "=", "0", "\n", "iters_so_far", "=", "0", "\n", "t_start", "=", "time", ".", "time", "(", ")", "\n", "\n", "# rolling buffer for episode lengths", "\n", "lenbuffer", "=", "deque", "(", "maxlen", "=", "100", ")", "\n", "# rolling buffer for episode rewards", "\n", "rewbuffer", "=", "deque", "(", "maxlen", "=", "100", ")", "\n", "\n", "self", ".", "episode_reward", "=", "np", ".", "zeros", "(", "(", "self", ".", "n_envs", ",", ")", ")", "\n", "\n", "while", "True", ":", "\n", "                    ", "if", "callback", "is", "not", "None", ":", "\n", "# Only stop training if return value is False, not when it is None. This is for backwards", "\n", "# compatibility with callbacks that have no return statement.", "\n", "                        ", "if", "callback", "(", "locals", "(", ")", ",", "globals", "(", ")", ")", "is", "False", ":", "\n", "                            ", "break", "\n", "", "", "if", "total_timesteps", "and", "timesteps_so_far", ">=", "total_timesteps", ":", "\n", "                        ", "break", "\n", "\n", "", "if", "self", ".", "schedule", "==", "'constant'", ":", "\n", "                        ", "cur_lrmult", "=", "1.0", "\n", "", "elif", "self", ".", "schedule", "==", "'linear'", ":", "\n", "                        ", "cur_lrmult", "=", "max", "(", "1.0", "-", "float", "(", "timesteps_so_far", ")", "/", "total_timesteps", ",", "0", ")", "\n", "", "else", ":", "\n", "                        ", "raise", "NotImplementedError", "\n", "\n", "", "logger", ".", "log", "(", "\"********** Iteration %i ************\"", "%", "iters_so_far", ")", "\n", "\n", "#check how many steps we have left", "\n", "steps_remaining", "=", "total_timesteps", "-", "timesteps_so_far", "\n", "local_steps_remaining", "=", "float", "(", "steps_remaining", ")", "/", "self", ".", "comm", ".", "Get_size", "(", ")", "\n", "\n", "if", "local_steps_remaining", ">", "self", ".", "timesteps_per_actorbatch", ":", "\n", "                        ", "seg", "=", "seg_gen", ".", "__next__", "(", ")", "\n", "add_vtarg_and_adv", "(", "seg", ",", "self", ".", "gamma", ",", "self", ".", "lam", ")", "\n", "", "else", ":", "\n", "# this is the final rollout and it is truncated.", "\n", "# ignore training on this data.", "\n", "                        ", "seg", "=", "seg_gen", ".", "__next__", "(", ")", "\n", "break", "\n", "\n", "# ob, ac, atarg, ret, td1ret = map(np.concatenate, (obs, acs, atargs, rets, td1rets))", "\n", "", "observations", ",", "actions", "=", "seg", "[", "\"observations\"", "]", ",", "seg", "[", "\"actions\"", "]", "\n", "atarg", ",", "tdlamret", "=", "seg", "[", "\"adv\"", "]", ",", "seg", "[", "\"tdlamret\"", "]", "\n", "\n", "# true_rew is the reward without discount", "\n", "if", "writer", "is", "not", "None", ":", "\n", "                        ", "self", ".", "episode_reward", "=", "total_episode_reward_logger", "(", "self", ".", "episode_reward", ",", "\n", "seg", "[", "\"true_rewards\"", "]", ".", "reshape", "(", "(", "self", ".", "n_envs", ",", "-", "1", ")", ")", ",", "\n", "seg", "[", "\"dones\"", "]", ".", "reshape", "(", "(", "self", ".", "n_envs", ",", "-", "1", ")", ")", ",", "\n", "writer", ",", "self", ".", "num_timesteps", ")", "\n", "\n", "# predicted value function before udpate", "\n", "", "vpredbefore", "=", "seg", "[", "\"vpred\"", "]", "\n", "\n", "# standardized advantage function estimate", "\n", "atarg", "=", "(", "atarg", "-", "atarg", ".", "mean", "(", ")", ")", "/", "atarg", ".", "std", "(", ")", "\n", "dataset", "=", "Dataset", "(", "dict", "(", "ob", "=", "observations", ",", "ac", "=", "actions", ",", "atarg", "=", "atarg", ",", "vtarg", "=", "tdlamret", ")", ",", "\n", "shuffle", "=", "not", "self", ".", "policy", ".", "recurrent", ")", "\n", "optim_batchsize", "=", "self", ".", "optim_batchsize", "or", "observations", ".", "shape", "[", "0", "]", "\n", "\n", "# set old parameter values to new parameter values", "\n", "self", ".", "assign_old_eq_new", "(", "sess", "=", "self", ".", "sess", ")", "\n", "logger", ".", "log", "(", "\"Optimizing...\"", ")", "\n", "logger", ".", "log", "(", "fmt_row", "(", "13", ",", "self", ".", "loss_names", ")", ")", "\n", "\n", "# Here we do a bunch of optimization epochs over the data", "\n", "for", "k", "in", "range", "(", "self", ".", "optim_epochs", ")", ":", "\n", "# list of tuples, each of which gives the loss for a minibatch", "\n", "                        ", "losses", "=", "[", "]", "\n", "for", "i", ",", "batch", "in", "enumerate", "(", "dataset", ".", "iterate_once", "(", "optim_batchsize", ")", ")", ":", "\n", "                            ", "steps", "=", "(", "self", ".", "num_timesteps", "+", "\n", "k", "*", "optim_batchsize", "+", "\n", "int", "(", "i", "*", "(", "optim_batchsize", "/", "len", "(", "dataset", ".", "data_map", ")", ")", ")", ")", "\n", "if", "writer", "is", "not", "None", ":", "\n", "# run loss backprop with summary, but once every 10 runs save the metadata", "\n", "# (memory, compute time, ...)", "\n", "                                ", "if", "self", ".", "full_tensorboard_log", "and", "(", "1", "+", "k", ")", "%", "10", "==", "0", ":", "\n", "                                    ", "run_options", "=", "tf", ".", "RunOptions", "(", "trace_level", "=", "tf", ".", "RunOptions", ".", "FULL_TRACE", ")", "\n", "run_metadata", "=", "tf", ".", "RunMetadata", "(", ")", "\n", "summary", ",", "grad", ",", "*", "newlosses", "=", "self", ".", "lossandgrad", "(", "batch", "[", "\"ob\"", "]", ",", "batch", "[", "\"ob\"", "]", ",", "batch", "[", "\"ac\"", "]", ",", "\n", "batch", "[", "\"atarg\"", "]", ",", "batch", "[", "\"vtarg\"", "]", ",", "\n", "cur_lrmult", ",", "sess", "=", "self", ".", "sess", ",", "\n", "options", "=", "run_options", ",", "\n", "run_metadata", "=", "run_metadata", ")", "\n", "writer", ".", "add_run_metadata", "(", "run_metadata", ",", "'step%d'", "%", "steps", ")", "\n", "", "else", ":", "\n", "                                    ", "summary", ",", "grad", ",", "*", "newlosses", "=", "self", ".", "lossandgrad", "(", "batch", "[", "\"ob\"", "]", ",", "batch", "[", "\"ob\"", "]", ",", "batch", "[", "\"ac\"", "]", ",", "\n", "batch", "[", "\"atarg\"", "]", ",", "batch", "[", "\"vtarg\"", "]", ",", "\n", "cur_lrmult", ",", "sess", "=", "self", ".", "sess", ")", "\n", "", "writer", ".", "add_summary", "(", "summary", ",", "steps", ")", "\n", "", "else", ":", "\n", "                                ", "_", ",", "grad", ",", "*", "newlosses", "=", "self", ".", "lossandgrad", "(", "batch", "[", "\"ob\"", "]", ",", "batch", "[", "\"ob\"", "]", ",", "batch", "[", "\"ac\"", "]", ",", "\n", "batch", "[", "\"atarg\"", "]", ",", "batch", "[", "\"vtarg\"", "]", ",", "cur_lrmult", ",", "\n", "sess", "=", "self", ".", "sess", ")", "\n", "\n", "", "self", ".", "adam", ".", "update", "(", "grad", ",", "self", ".", "optim_stepsize", "*", "cur_lrmult", ")", "\n", "losses", ".", "append", "(", "newlosses", ")", "\n", "", "logger", ".", "log", "(", "fmt_row", "(", "13", ",", "np", ".", "mean", "(", "losses", ",", "axis", "=", "0", ")", ")", ")", "\n", "\n", "", "logger", ".", "log", "(", "\"Evaluating losses...\"", ")", "\n", "losses", "=", "[", "]", "\n", "for", "batch", "in", "dataset", ".", "iterate_once", "(", "optim_batchsize", ")", ":", "\n", "                        ", "newlosses", "=", "self", ".", "compute_losses", "(", "batch", "[", "\"ob\"", "]", ",", "batch", "[", "\"ob\"", "]", ",", "batch", "[", "\"ac\"", "]", ",", "batch", "[", "\"atarg\"", "]", ",", "\n", "batch", "[", "\"vtarg\"", "]", ",", "cur_lrmult", ",", "sess", "=", "self", ".", "sess", ")", "\n", "losses", ".", "append", "(", "newlosses", ")", "\n", "\n", "", "mean_losses", ",", "_", ",", "_", "=", "mpi_moments", "(", "losses", ",", "axis", "=", "0", ",", "comm", "=", "self", ".", "comm", ")", "\n", "logger", ".", "log", "(", "fmt_row", "(", "13", ",", "mean_losses", ")", ")", "\n", "for", "(", "loss_val", ",", "name", ")", "in", "zipsame", "(", "mean_losses", ",", "self", ".", "loss_names", ")", ":", "\n", "                        ", "logger", ".", "record_tabular", "(", "\"loss_\"", "+", "name", ",", "loss_val", ")", "\n", "", "logger", ".", "record_tabular", "(", "\"ev_tdlam_before\"", ",", "explained_variance", "(", "vpredbefore", ",", "tdlamret", ")", ")", "\n", "\n", "# local values", "\n", "lrlocal", "=", "(", "seg", "[", "\"ep_lens\"", "]", ",", "seg", "[", "\"ep_rets\"", "]", ")", "\n", "\n", "# list of tuples", "\n", "listoflrpairs", "=", "self", ".", "comm", ".", "allgather", "(", "lrlocal", ")", "\n", "lens", ",", "rews", "=", "map", "(", "flatten_lists", ",", "zip", "(", "*", "listoflrpairs", ")", ")", "\n", "lenbuffer", ".", "extend", "(", "lens", ")", "\n", "rewbuffer", ".", "extend", "(", "rews", ")", "\n", "\n", "if", "policy_record", "is", "not", "None", ":", "#only true for root proc", "\n", "                        ", "for", "idx", "in", "range", "(", "len", "(", "lens", ")", ")", ":", "\n", "                            ", "policy_record", ".", "add_result", "(", "rews", "[", "idx", "]", ",", "lens", "[", "idx", "]", ")", "\n", "\n", "#save and plot", "\n", "", "policy_record", ".", "save", "(", ")", "\n", "\n", "", "if", "len", "(", "lenbuffer", ")", ">", "0", ":", "\n", "                        ", "logger", ".", "record_tabular", "(", "\"EpLenMean\"", ",", "np", ".", "mean", "(", "lenbuffer", ")", ")", "\n", "logger", ".", "record_tabular", "(", "\"EpRewMean\"", ",", "np", ".", "mean", "(", "rewbuffer", ")", ")", "\n", "\n", "", "logger", ".", "record_tabular", "(", "\"EpThisIter\"", ",", "len", "(", "lens", ")", ")", "\n", "episodes_so_far", "+=", "len", "(", "lens", ")", "\n", "current_it_timesteps", "=", "self", ".", "comm", ".", "allreduce", "(", "seg", "[", "\"total_timestep\"", "]", ")", "\n", "timesteps_so_far", "+=", "current_it_timesteps", "\n", "self", ".", "num_timesteps", "+=", "current_it_timesteps", "\n", "iters_so_far", "+=", "1", "\n", "logger", ".", "record_tabular", "(", "\"EpisodesSoFar\"", ",", "episodes_so_far", ")", "\n", "logger", ".", "record_tabular", "(", "\"TimestepsSoFar\"", ",", "self", ".", "num_timesteps", ")", "\n", "logger", ".", "record_tabular", "(", "\"TimeElapsed\"", ",", "time", ".", "time", "(", ")", "-", "t_start", ")", "\n", "if", "self", ".", "verbose", ">=", "1", "and", "self", ".", "comm", ".", "Get_rank", "(", ")", "==", "0", ":", "\n", "                        ", "logger", ".", "dump_tabular", "(", ")", "\n", "\n", "", "", "", "", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.save": [[389, 413], ["ppo1_mod.PPO1.get_parameters", "ppo1_mod.PPO1._save_to_file"], "methods", ["None"], ["", "def", "save", "(", "self", ",", "save_path", ",", "cloudpickle", "=", "False", ")", ":", "\n", "        ", "data", "=", "{", "\n", "\"gamma\"", ":", "self", ".", "gamma", ",", "\n", "\"timesteps_per_actorbatch\"", ":", "self", ".", "timesteps_per_actorbatch", ",", "\n", "\"clip_param\"", ":", "self", ".", "clip_param", ",", "\n", "\"entcoeff\"", ":", "self", ".", "entcoeff", ",", "\n", "\"optim_epochs\"", ":", "self", ".", "optim_epochs", ",", "\n", "\"optim_stepsize\"", ":", "self", ".", "optim_stepsize", ",", "\n", "\"optim_batchsize\"", ":", "self", ".", "optim_batchsize", ",", "\n", "\"lam\"", ":", "self", ".", "lam", ",", "\n", "\"adam_epsilon\"", ":", "self", ".", "adam_epsilon", ",", "\n", "\"schedule\"", ":", "self", ".", "schedule", ",", "\n", "\"verbose\"", ":", "self", ".", "verbose", ",", "\n", "\"policy\"", ":", "self", ".", "policy", ",", "\n", "\"observation_space\"", ":", "self", ".", "observation_space", ",", "\n", "\"action_space\"", ":", "self", ".", "action_space", ",", "\n", "\"n_envs\"", ":", "self", ".", "n_envs", ",", "\n", "\"_vectorize_action\"", ":", "self", ".", "_vectorize_action", ",", "\n", "\"policy_kwargs\"", ":", "self", ".", "policy_kwargs", "\n", "}", "\n", "\n", "params_to_save", "=", "self", ".", "get_parameters", "(", ")", "\n", "\n", "self", ".", "_save_to_file", "(", "save_path", ",", "data", "=", "data", ",", "params", "=", "params_to_save", ")", "#, cloudpickle=cloudpickle)", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.load": [[416, 435], ["cls._load_from_file", "cls", "cls.__dict__.update", "cls.__dict__.update", "cls.set_env", "cls.setup_model", "cls.load_parameters", "ValueError"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.setup_model"], ["", "@", "classmethod", "\n", "def", "load", "(", "cls", ",", "load_path", ",", "env", ",", "comm", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "data", ",", "params", "=", "cls", ".", "_load_from_file", "(", "load_path", ")", "\n", "\n", "if", "'policy_kwargs'", "in", "kwargs", "and", "kwargs", "[", "'policy_kwargs'", "]", "!=", "data", "[", "'policy_kwargs'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "\"The specified policy kwargs do not equal the stored policy kwargs. \"", "\n", "\"Stored kwargs: {}, specified kwargs: {}\"", ".", "format", "(", "data", "[", "'policy_kwargs'", "]", ",", "\n", "kwargs", "[", "'policy_kwargs'", "]", ")", ")", "\n", "\n", "", "model", "=", "cls", "(", "data", "[", "\"policy\"", "]", ",", "env", ",", "comm", ",", "_init_setup_model", "=", "False", ")", "\n", "model", ".", "__dict__", ".", "update", "(", "data", ")", "\n", "model", ".", "__dict__", ".", "update", "(", "kwargs", ")", "\n", "model", ".", "set_env", "(", "env", ")", "\n", "model", ".", "setup_model", "(", ")", "\n", "\n", "model", ".", "load_parameters", "(", "params", ")", "\n", "\n", "return", "model", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo_bkup.PPOPolicy.__init__": [[18, 39], ["arena5.algos.ppo.ppo1_mod.PPO1", "len", "len"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ",", "use_lstm", "=", "False", ",", "eval_mode", "=", "False", ")", ":", "\n", "\t\t", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "\n", "if", "use_lstm", ":", "\n", "\t\t\t", "if", "len", "(", "self", ".", "env", ".", "observation_space", ".", "shape", ")", ">", "2", ":", "\n", "\t\t\t\t", "pcy", "=", "CnnLstmPolicy", "\n", "", "else", ":", "\n", "\t\t\t\t", "pcy", "=", "MlpLstmPolicy", "\n", "\n", "", "", "else", ":", "\n", "\t\t\t", "if", "len", "(", "self", ".", "env", ".", "observation_space", ".", "shape", ")", ">", "1", ":", "\n", "\t\t\t\t", "pcy", "=", "CnnPolicy", "\n", "", "else", ":", "\n", "\t\t\t\t", "pcy", "=", "MlpPolicy", "\n", "\n", "", "", "self", ".", "model", "=", "PPO1", "(", "pcy", ",", "env", ",", "policy_comm", ",", "timesteps_per_actorbatch", "=", "128", ",", "clip_param", "=", "0.2", ",", "entcoeff", "=", "0.01", ",", "\n", "optim_epochs", "=", "4", ",", "optim_stepsize", "=", "1e-3", ",", "optim_batchsize", "=", "64", ",", "gamma", "=", "0.99", ",", "lam", "=", "0.95", ",", "schedule", "=", "'linear'", ",", "\n", "verbose", "=", "1", ")", "\n", "\n", "self", ".", "eval_mode", "=", "eval_mode", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo_bkup.PPOPolicy.run": [[41, 56], ["os.path.exists", "os.path.exists", "arena5.algos.ppo.ppo1_mod.PPO1.load", "print", "ppo_bkup.PPOPolicy.model.evaluate", "ppo_bkup.PPOPolicy.model.learn", "policy_record.save", "ppo_bkup.PPOPolicy.model.save"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.evaluate", "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.learn", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "\n", "\t\t", "if", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"/ppo_save.pkl\"", ")", "or", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"/ppo_save.zip\"", ")", ":", "\n", "\t\t\t", "self", ".", "model", "=", "PPO1", ".", "load", "(", "data_dir", "+", "\"ppo_save\"", ",", "self", ".", "env", ",", "self", ".", "comm", ")", "\n", "print", "(", "\"loaded model from saved file!\"", ")", "\n", "\n", "", "if", "self", ".", "eval_mode", ":", "\n", "\t\t\t", "self", ".", "model", ".", "evaluate", "(", "num_steps", ")", "\n", "", "else", ":", "\n", "\t\t\t", "self", ".", "model", ".", "learn", "(", "num_steps", ",", "policy_record", ")", "\n", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "\t\t\t\t", "policy_record", ".", "save", "(", ")", "\n", "\n", "self", ".", "model", ".", "save", "(", "policy_record", ".", "data_dir", "+", "\"ppo_save\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo_bkup.PPOLSTMPolicy": [[7, 9], ["ppo_bkup.PPOPolicy"], "function", ["None"], ["def", "PPOLSTMPolicy", "(", "env", ",", "policy_comm", ")", ":", "\n", "\t", "return", "PPOPolicy", "(", "env", ",", "policy_comm", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo_bkup.PPOLSTMPolicyEval": [[10, 12], ["ppo_bkup.PPOPolicy"], "function", ["None"], ["", "def", "PPOLSTMPolicyEval", "(", "env", ",", "policy_comm", ")", ":", "\n", "\t", "return", "PPOPolicy", "(", "env", ",", "policy_comm", ",", "True", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo_bkup.PPOPolicyEval": [[13, 15], ["ppo_bkup.PPOPolicy"], "function", ["None"], ["", "def", "PPOPolicyEval", "(", "env", ",", "policy_comm", ")", ":", "\n", "\t", "return", "PPOPolicy", "(", "env", ",", "policy_comm", ",", "False", ",", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.seg_gen_limited.traj_segment_generator": [[12, 136], ["env.action_space.sample", "env.reset", "numpy.array", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.array", "policy.step", "isinstance", "env.reset.reshape", "policy.step", "numpy.clip", "reward_giver.get_reward", "env.step", "env.step", "copy.deepcopy", "info.get", "ep_rets.append", "ep_true_rets.append", "ep_lens.append", "range", "range", "env.reset.reshape", "numpy.clip", "isinstance", "env.reset"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.get", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["def", "traj_segment_generator", "(", "policy", ",", "env", ",", "horizon", ",", "total_steps", ",", "clip_rewards", ",", "reward_giver", "=", "None", ",", "gail", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Compute target value using TD(lambda) estimator, and advantage with GAE(lambda)\n    :param policy: (MLPPolicy) the policy\n    :param env: (Gym Environment) the environment\n    :param horizon: (int) the number of timesteps to run per batch\n    :param reward_giver: (TransitionClassifier) the reward predicter from obsevation and action\n    :param gail: (bool) Whether we are using this generator for standard trpo or with gail\n    :return: (dict) generator that returns a dict with the following keys:\n        - observations: (np.ndarray) observations\n        - rewards: (numpy float) rewards (if gail is used it is the predicted reward)\n        - true_rewards: (numpy float) if gail is used it is the original reward\n        - vpred: (numpy float) action logits\n        - dones: (numpy bool) dones (is end of episode, used for logging)\n        - episode_starts: (numpy bool)\n            True if first timestep of an episode, used for GAE\n        - actions: (np.ndarray) actions\n        - nextvpred: (numpy float) next action logits\n        - ep_rets: (float) cumulated current episode reward\n        - ep_lens: (int) the length of the current episode\n        - ep_true_rets: (float) the real environment reward\n    \"\"\"", "\n", "# Check when using GAIL", "\n", "assert", "not", "(", "gail", "and", "reward_giver", "is", "None", ")", ",", "\"You must pass a reward giver when using GAIL\"", "\n", "\n", "# Initialize state variables", "\n", "step", "=", "0", "\n", "action", "=", "env", ".", "action_space", ".", "sample", "(", ")", "# not used, just so we have the datatype", "\n", "observation", "=", "env", ".", "reset", "(", ")", "\n", "\n", "cur_ep_ret", "=", "0", "# return in current episode", "\n", "current_it_len", "=", "0", "# len of current iteration", "\n", "current_ep_len", "=", "0", "# len of current episode", "\n", "cur_ep_true_ret", "=", "0", "\n", "ep_true_rets", "=", "[", "]", "\n", "ep_rets", "=", "[", "]", "# returns of completed episodes in this segment", "\n", "ep_lens", "=", "[", "]", "# Episode lengths", "\n", "\n", "# Initialize history arrays", "\n", "observations", "=", "np", ".", "array", "(", "[", "observation", "for", "_", "in", "range", "(", "horizon", ")", "]", ")", "\n", "true_rewards", "=", "np", ".", "zeros", "(", "horizon", ",", "'float32'", ")", "\n", "rewards", "=", "np", ".", "zeros", "(", "horizon", ",", "'float32'", ")", "\n", "vpreds", "=", "np", ".", "zeros", "(", "horizon", ",", "'float32'", ")", "\n", "episode_starts", "=", "np", ".", "zeros", "(", "horizon", ",", "'bool'", ")", "\n", "dones", "=", "np", ".", "zeros", "(", "horizon", ",", "'bool'", ")", "\n", "actions", "=", "np", ".", "array", "(", "[", "action", "for", "_", "in", "range", "(", "horizon", ")", "]", ")", "\n", "states", "=", "policy", ".", "initial_state", "\n", "episode_start", "=", "True", "# marks if we're on first timestep of an episode", "\n", "done", "=", "False", "\n", "\n", "while", "True", ":", "\n", "        ", "action", ",", "vpred", ",", "states", ",", "_", "=", "policy", ".", "step", "(", "observation", ".", "reshape", "(", "-", "1", ",", "*", "observation", ".", "shape", ")", ",", "states", ",", "done", ")", "\n", "# Slight weirdness here because we need value function at time T", "\n", "# before returning segment [0, T-1] so we get the correct", "\n", "# terminal value", "\n", "if", "step", ">", "0", "and", "(", "step", "%", "horizon", "==", "0", "or", "step", ">=", "total_steps", ")", ":", "\n", "            ", "yield", "{", "\n", "\"observations\"", ":", "observations", ",", "\n", "\"rewards\"", ":", "rewards", ",", "\n", "\"dones\"", ":", "dones", ",", "\n", "\"episode_starts\"", ":", "episode_starts", ",", "\n", "\"true_rewards\"", ":", "true_rewards", ",", "\n", "\"vpred\"", ":", "vpreds", ",", "\n", "\"actions\"", ":", "actions", ",", "\n", "\"nextvpred\"", ":", "vpred", "[", "0", "]", "*", "(", "1", "-", "episode_start", ")", ",", "\n", "\"ep_rets\"", ":", "ep_rets", ",", "\n", "\"ep_lens\"", ":", "ep_lens", ",", "\n", "\"ep_true_rets\"", ":", "ep_true_rets", ",", "\n", "\"total_timestep\"", ":", "current_it_len", "\n", "}", "\n", "_", ",", "vpred", ",", "_", ",", "_", "=", "policy", ".", "step", "(", "observation", ".", "reshape", "(", "-", "1", ",", "*", "observation", ".", "shape", ")", ")", "\n", "# Be careful!!! if you change the downstream algorithm to aggregate", "\n", "# several of these batches, then be sure to do a deepcopy", "\n", "ep_rets", "=", "[", "]", "\n", "ep_true_rets", "=", "[", "]", "\n", "ep_lens", "=", "[", "]", "\n", "# Reset current iteration length", "\n", "current_it_len", "=", "0", "\n", "", "i", "=", "step", "%", "horizon", "\n", "observations", "[", "i", "]", "=", "observation", "\n", "vpreds", "[", "i", "]", "=", "vpred", "[", "0", "]", "\n", "actions", "[", "i", "]", "=", "action", "[", "0", "]", "\n", "episode_starts", "[", "i", "]", "=", "episode_start", "\n", "\n", "clipped_action", "=", "action", "\n", "# Clip the actions to avoid out of bound error", "\n", "if", "isinstance", "(", "env", ".", "action_space", ",", "gym", ".", "spaces", ".", "Box", ")", ":", "\n", "            ", "clipped_action", "=", "np", ".", "clip", "(", "action", ",", "env", ".", "action_space", ".", "low", ",", "env", ".", "action_space", ".", "high", ")", "\n", "\n", "", "if", "gail", ":", "\n", "            ", "reward", "=", "reward_giver", ".", "get_reward", "(", "observation", ",", "clipped_action", "[", "0", "]", ")", "\n", "observation", ",", "true_reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "clipped_action", "[", "0", "]", ")", "\n", "", "else", ":", "\n", "            ", "observation", ",", "reward", ",", "done", ",", "info", "=", "env", ".", "step", "(", "clipped_action", "[", "0", "]", ")", "\n", "true_reward", "=", "copy", ".", "deepcopy", "(", "reward", ")", "\n", "if", "clip_rewards", ":", "\n", "                ", "reward", "=", "np", ".", "clip", "(", "reward", ",", "-", "1.0", ",", "1.0", ")", "\n", "\n", "", "", "rewards", "[", "i", "]", "=", "reward", "\n", "true_rewards", "[", "i", "]", "=", "true_reward", "\n", "dones", "[", "i", "]", "=", "done", "\n", "episode_start", "=", "done", "\n", "\n", "cur_ep_ret", "+=", "reward", "\n", "cur_ep_true_ret", "+=", "true_reward", "\n", "current_it_len", "+=", "1", "\n", "current_ep_len", "+=", "1", "\n", "if", "done", ":", "\n", "# Retrieve unnormalized reward if using Monitor wrapper", "\n", "            ", "maybe_ep_info", "=", "info", ".", "get", "(", "'episode'", ")", "\n", "if", "maybe_ep_info", "is", "not", "None", ":", "\n", "                ", "if", "not", "gail", ":", "\n", "                    ", "cur_ep_ret", "=", "maybe_ep_info", "[", "'r'", "]", "\n", "", "cur_ep_true_ret", "=", "maybe_ep_info", "[", "'r'", "]", "\n", "\n", "", "ep_rets", ".", "append", "(", "cur_ep_ret", ")", "\n", "ep_true_rets", ".", "append", "(", "cur_ep_true_ret", ")", "\n", "ep_lens", ".", "append", "(", "current_ep_len", ")", "\n", "cur_ep_ret", "=", "0", "\n", "cur_ep_true_ret", "=", "0", "\n", "current_ep_len", "=", "0", "\n", "if", "not", "isinstance", "(", "env", ",", "VecEnv", ")", ":", "\n", "                ", "observation", "=", "env", ".", "reset", "(", ")", "\n", "", "", "step", "+=", "1", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo.PPOPolicy.__init__": [[18, 55], ["arena5.algos.ppo.ppo1_mod.PPO1", "len", "len"], "methods", ["None"], ["\n", "\n", "", "def", "discounted_reward", "(", "reward", ",", "done", ",", "gamma", "=", "0.99", ")", ":", "\n", "    ", "T", "=", "reward", ".", "size", "(", "0", ")", "\n", "discounted_rewards", "=", "torch", ".", "zeros_like", "(", "reward", ")", "\n", "for", "t", "in", "reversed", "(", "range", "(", "T", ")", ")", ":", "\n", "        ", "if", "done", "[", "t", "]", ":", "\n", "            ", "discounted_rewards", "[", "t", "]", "=", "reward", "[", "t", "]", "\n", "", "else", ":", "\n", "            ", "discounted_rewards", "[", "t", "]", "=", "reward", "[", "t", "]", "+", "gamma", "*", "discounted_rewards", "[", "t", "+", "1", "]", "\n", "", "", "return", "discounted_rewards", "\n", "\n", "\n", "", "def", "ppo_loss", "(", "old_means", ",", "old_log_stds", ",", "new_means", ",", "new_log_stds", ",", "actions", ",", "advantages", ",", "clip_param", "=", "0.2", ")", ":", "\n", "    ", "old_distribution", "=", "Normal", "(", "old_means", ",", "torch", ".", "exp", "(", "old_log_stds", ")", ")", "\n", "old_log_prob", "=", "old_distribution", ".", "log_prob", "(", "actions", ")", "\n", "\n", "new_distribution", "=", "Normal", "(", "new_means", ",", "torch", ".", "exp", "(", "new_log_stds", ")", ")", "\n", "new_log_prob", "=", "new_distribution", ".", "log_prob", "(", "actions", ")", "\n", "\n", "ratio", "=", "torch", ".", "exp", "(", "new_log_prob", "-", "old_log_prob", ")", "\n", "surrogate_loss", "=", "torch", ".", "min", "(", "advantages", "*", "ratio", ",", "advantages", "*", "torch", ".", "clamp", "(", "ratio", ",", "1.0", "-", "clip_param", ",", "1.0", "+", "clip_param", ")", ")", "\n", "policy_loss", "=", "-", "surrogate_loss", ".", "sum", "(", "-", "1", ")", ".", "mean", "(", ")", "\n", "\n", "entropy", "=", "-", "(", "new_log_prob", "*", "torch", ".", "exp", "(", "new_log_prob", ")", ")", ".", "sum", "(", "-", "1", ")", ".", "mean", "(", ")", "\n", "\n", "return", "policy_loss", ",", "entropy", "\n", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo.PPOPolicy.run": [[57, 74], ["arena5.algos.ppo.ppo1_mod.PPO1.load", "ppo.PPOPolicy.model.evaluate", "ppo.PPOPolicy.model.learn", "os.path.exists", "os.path.exists", "arena5.algos.ppo.ppo1_mod.PPO1.load", "print", "policy_record.save", "ppo.PPOPolicy.model.save"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.evaluate", "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo1_mod.PPO1.learn", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo.PPOLSTMPolicy": [[7, 9], ["ppo.PPOPolicy"], "function", ["None"], ["    ", "last_part", "=", "0", "\n", "advantages", "=", "torch", ".", "zeros_like", "(", "reward", ")", "\n", "for", "t", "in", "reversed", "(", "range", "(", "reward", ".", "size", "(", "0", ")", ")", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo.PPOLSTMPolicyEval": [[10, 12], ["ppo.PPOPolicy"], "function", ["None"], ["        ", "if", "done", "[", "t", "]", ":", "\n", "            ", "delta", "=", "reward", "[", "t", "]", "-", "value", "[", "t", "]", "\n", "last_part", "=", "0", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ppo.ppo.PPOPolicyEval": [[13, 15], ["ppo.PPOPolicy"], "function", ["None"], ["", "else", ":", "\n", "            ", "delta", "=", "reward", "[", "t", "]", "+", "gamma", "*", "value", "[", "t", "+", "1", "]", "-", "value", "[", "t", "]", "\n", "", "advantages", "[", "t", "]", "=", "delta", "+", "gamma", "*", "lam", "*", "last_part", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_policy.DDPGPolicy.__init__": [[10, 14], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ",", "eval_mode", "=", "False", ")", ":", "\n", "        ", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "self", ".", "eval_mode", "=", "eval_mode", "\n", "# print(env.observation_spaces, env.action_spaces)", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_policy.DDPGPolicy.run": [[17, 27], ["arena5.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater", "int", "arena5.algos.ddpg.ddpg.ddpg", "ddpg_policy.DDPGPolicy.comm.Get_size", "dict"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg.ddpg"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "env", "=", "MPISynchronizedPRUpdater", "(", "self", ".", "env", ",", "self", ".", "comm", ",", "policy_record", ")", "\n", "local_steps", "=", "int", "(", "num_steps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "\n", "ddpg", "(", "lambda", ":", "self", ".", "env", ",", "self", ".", "comm", ",", "data_dir", ",", "policy_record", ",", "self", ".", "eval_mode", ",", "\n", "actor_critic", "=", "core", ".", "MLPActorCritic", ",", "\n", "ac_kwargs", "=", "dict", "(", "hidden_sizes", "=", "[", "256", ",", "256", "]", ")", ",", "\n", "gamma", "=", "0.99", ",", "seed", "=", "1337", ",", "steps_per_epoch", "=", "local_steps", ",", "epochs", "=", "1", ",", "\n", "logger_kwargs", "=", "None", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg.ReplayBuffer.__init__": [[18, 25], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "arena5.combined_shape", "arena5.combined_shape", "arena5.combined_shape"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape"], ["def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "size", ")", ":", "\n", "        ", "self", ".", "obs_buf", "=", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "obs_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "obs2_buf", "=", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "obs_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "act_buf", "=", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "act_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rew_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "ptr", ",", "self", ".", "size", ",", "self", ".", "max_size", "=", "0", ",", "0", ",", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg.ReplayBuffer.store": [[26, 34], ["min"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "obs", ",", "act", ",", "rew", ",", "next_obs", ",", "done", ")", ":", "\n", "        ", "self", ".", "obs_buf", "[", "self", ".", "ptr", "]", "=", "obs", "\n", "self", ".", "obs2_buf", "[", "self", ".", "ptr", "]", "=", "next_obs", "\n", "self", ".", "act_buf", "[", "self", ".", "ptr", "]", "=", "act", "\n", "self", ".", "rew_buf", "[", "self", ".", "ptr", "]", "=", "rew", "\n", "self", ".", "done_buf", "[", "self", ".", "ptr", "]", "=", "done", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg.ReplayBuffer.sample_batch": [[35, 43], ["numpy.random.randint", "dict", "torch.as_tensor", "dict.items"], "methods", ["None"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", "=", "32", ")", ":", "\n", "        ", "idxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "batch", "=", "dict", "(", "obs", "=", "self", ".", "obs_buf", "[", "idxs", "]", ",", "\n", "obs2", "=", "self", ".", "obs2_buf", "[", "idxs", "]", ",", "\n", "act", "=", "self", ".", "act_buf", "[", "idxs", "]", ",", "\n", "rew", "=", "self", ".", "rew_buf", "[", "idxs", "]", ",", "\n", "done", "=", "self", ".", "done_buf", "[", "idxs", "]", ")", "\n", "return", "{", "k", ":", "torch", ".", "as_tensor", "(", "v", ",", "dtype", "=", "torch", ".", "float32", ")", "for", "k", ",", "v", "in", "batch", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg.ddpg": [[46, 289], ["dict", "int", "dict", "torch.set_num_threads", "torch.manual_seed", "numpy.random.seed", "actor_critic", "copy.deepcopy", "arena5.core.mpi_pytorch_utils.sync_weights", "arena5.core.mpi_pytorch_utils.sync_weights", "arena5.core.mpi_pytorch_utils.sync_weights", "copy.deepcopy.parameters", "ddpg.ReplayBuffer", "tuple", "torch.optim.Adam", "torch.optim.Adam", "time.time", "range", "env_fn", "env_fn", "os.path.exists", "os.path.exists", "actor_critic.q.parameters", "actor_critic.pi.parameters", "copy.deepcopy.parameters", "actor_critic.q", "dict", "actor_critic.q", "actor_critic.pi.parameters", "actor_critic.q.parameters", "torch.optim.Adam.zero_grad", "ddpg.ddpg.compute_loss_q"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights"], ["", "", "def", "ddpg", "(", "env_fn", ",", "comm", ",", "data_dir", ",", "policy_record", "=", "None", ",", "eval_mode", "=", "False", ",", "\n", "actor_critic", "=", "core", ".", "MLPActorCritic", ",", "ac_kwargs", "=", "dict", "(", ")", ",", "seed", "=", "0", ",", "\n", "steps_per_epoch", "=", "4000", ",", "epochs", "=", "100", ",", "replay_size", "=", "int", "(", "1e6", ")", ",", "gamma", "=", "0.99", ",", "\n", "polyak", "=", "0.995", ",", "pi_lr", "=", "1e-3", ",", "q_lr", "=", "1e-3", ",", "batch_size", "=", "100", ",", "start_steps", "=", "10000", ",", "\n", "update_after", "=", "1000", ",", "update_every", "=", "50", ",", "act_noise", "=", "0.1", ",", "num_test_episodes", "=", "10", ",", "\n", "max_ep_len", "=", "1000", ",", "logger_kwargs", "=", "dict", "(", ")", ",", "save_freq", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Deep Deterministic Policy Gradient (DDPG)\n    Args:\n        env_fn : A function which creates a copy of the environment.\n            The environment must satisfy the OpenAI Gym API.\n        actor_critic: The constructor method for a PyTorch Module with an ``act`` \n            method, a ``pi`` module, and a ``q`` module. The ``act`` method and\n            ``pi`` module should accept batches of observations as inputs,\n            and ``q`` should accept a batch of observations and a batch of \n            actions as inputs. When called, these should return:\n            ===========  ================  ======================================\n            Call         Output Shape      Description\n            ===========  ================  ======================================\n            ``act``      (batch, act_dim)  | Numpy array of actions for each \n                                           | observation.\n            ``pi``       (batch, act_dim)  | Tensor containing actions from policy\n                                           | given observations.\n            ``q``        (batch,)          | Tensor containing the current estimate\n                                           | of Q* for the provided observations\n                                           | and actions. (Critical: make sure to\n                                           | flatten this!)\n            ===========  ================  ======================================\n        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object \n            you provided to DDPG.\n        seed (int): Seed for random number generators.\n        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n            for the agent and the environment in each epoch.\n        epochs (int): Number of epochs to run and train agent.\n        replay_size (int): Maximum length of replay buffer.\n        gamma (float): Discount factor. (Always between 0 and 1.)\n        polyak (float): Interpolation factor in polyak averaging for target \n            networks. Target networks are updated towards main networks \n            according to:\n            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n            close to 1.)\n        pi_lr (float): Learning rate for policy.\n        q_lr (float): Learning rate for Q-networks.\n        batch_size (int): Minibatch size for SGD.\n        start_steps (int): Number of steps for uniform-random action selection,\n            before running real policy. Helps exploration.\n        update_after (int): Number of env interactions to collect before\n            starting to do gradient descent updates. Ensures replay buffer\n            is full enough for useful updates.\n        update_every (int): Number of env interactions that should elapse\n            between gradient descent updates. Note: Regardless of how long \n            you wait between updates, the ratio of env steps to gradient steps \n            is locked to 1.\n        act_noise (float): Stddev for Gaussian exploration noise added to \n            policy at training time. (At test time, no noise is added.)\n        num_test_episodes (int): Number of episodes to test the deterministic\n            policy at the end of each epoch.\n        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n        logger_kwargs (dict): Keyword args for EpochLogger.\n        save_freq (int): How often (in terms of gap between epochs) to save\n            the current policy and value function.\n    \"\"\"", "\n", "\n", "os", ".", "environ", "[", "\"OMP_NUM_THREADS\"", "]", "=", "\"1\"", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "env", ",", "test_env", "=", "env_fn", "(", ")", ",", "env_fn", "(", ")", "\n", "obs_dim", "=", "env", ".", "observation_space", ".", "shape", "\n", "act_dim", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "\n", "# Action limit for clamping: critically, assumes all dimensions share the same bound!", "\n", "act_limit", "=", "env", ".", "action_space", ".", "high", "[", "0", "]", "\n", "\n", "# Create actor-critic module and target networks", "\n", "ac", "=", "actor_critic", "(", "env", ".", "observation_space", ",", "env", ".", "action_space", ",", "**", "ac_kwargs", ")", "\n", "ac_targ", "=", "deepcopy", "(", "ac", ")", "\n", "\n", "# if records exist, load weights", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"ac.pt\"", ")", ":", "\n", "            ", "ac", ".", "load_state_dict", "(", "torch", ".", "load", "(", "data_dir", "+", "\"ac.pt\"", ")", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"ac_targ.pt\"", ")", ":", "\n", "            ", "ac_targ", ".", "load_state_dict", "(", "torch", ".", "load", "(", "data_dir", "+", "\"ac_targ.pt\"", ")", ")", "\n", "\n", "# initial weight sync", "\n", "", "", "sync_weights", "(", "comm", ",", "ac", ".", "q", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac", ".", "pi", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac_targ", ".", "parameters", "(", ")", ")", "\n", "\n", "# Freeze target networks with respect to optimizers (only update via polyak averaging)", "\n", "for", "p", "in", "ac_targ", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "# Experience buffer", "\n", "", "replay_buffer", "=", "ReplayBuffer", "(", "obs_dim", "=", "obs_dim", ",", "act_dim", "=", "act_dim", ",", "size", "=", "replay_size", ")", "\n", "\n", "# Count variables (protip: try to get a feel for how different size networks behave!)", "\n", "var_counts", "=", "tuple", "(", "core", ".", "count_vars", "(", "module", ")", "for", "module", "in", "[", "ac", ".", "pi", ",", "ac", ".", "q", "]", ")", "\n", "\n", "# Set up function for computing DDPG Q-loss", "\n", "def", "compute_loss_q", "(", "data", ")", ":", "\n", "        ", "o", ",", "a", ",", "r", ",", "o2", ",", "d", "=", "data", "[", "'obs'", "]", ",", "data", "[", "'act'", "]", ",", "data", "[", "'rew'", "]", ",", "data", "[", "'obs2'", "]", ",", "data", "[", "'done'", "]", "\n", "\n", "q", "=", "ac", ".", "q", "(", "o", ",", "a", ")", "\n", "\n", "# Bellman backup for Q function", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_pi_targ", "=", "ac_targ", ".", "q", "(", "o2", ",", "ac_targ", ".", "pi", "(", "o2", ")", ")", "\n", "backup", "=", "r", "+", "gamma", "*", "(", "1", "-", "d", ")", "*", "q_pi_targ", "\n", "\n", "# MSE loss against Bellman backup", "\n", "", "loss_q", "=", "(", "(", "q", "-", "backup", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "\n", "# Useful info for logging", "\n", "loss_info", "=", "dict", "(", "QVals", "=", "q", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "return", "loss_q", ",", "loss_info", "\n", "\n", "# Set up function for computing DDPG pi loss", "\n", "", "def", "compute_loss_pi", "(", "data", ")", ":", "\n", "        ", "o", "=", "data", "[", "'obs'", "]", "\n", "q_pi", "=", "ac", ".", "q", "(", "o", ",", "ac", ".", "pi", "(", "o", ")", ")", "\n", "return", "-", "q_pi", ".", "mean", "(", ")", "\n", "\n", "# Set up optimizers for policy and q-function", "\n", "", "pi_optimizer", "=", "Adam", "(", "ac", ".", "pi", ".", "parameters", "(", ")", ",", "lr", "=", "pi_lr", ")", "\n", "q_optimizer", "=", "Adam", "(", "ac", ".", "q", ".", "parameters", "(", ")", ",", "lr", "=", "q_lr", ")", "\n", "\n", "\n", "def", "update", "(", "data", ")", ":", "\n", "# First run one gradient descent step for Q.", "\n", "        ", "q_optimizer", ".", "zero_grad", "(", ")", "\n", "loss_q", ",", "loss_info", "=", "compute_loss_q", "(", "data", ")", "\n", "loss_q", ".", "backward", "(", ")", "\n", "sync_grads", "(", "comm", ",", "ac", ".", "q", ".", "parameters", "(", ")", ")", "\n", "q_optimizer", ".", "step", "(", ")", "\n", "\n", "# Freeze Q-network so you don't waste computational effort ", "\n", "# computing gradients for it during the policy learning step.", "\n", "for", "p", "in", "ac", ".", "q", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "# Next run one gradient descent step for pi.", "\n", "", "pi_optimizer", ".", "zero_grad", "(", ")", "\n", "loss_pi", "=", "compute_loss_pi", "(", "data", ")", "\n", "loss_pi", ".", "backward", "(", ")", "\n", "sync_grads", "(", "comm", ",", "ac", ".", "pi", ".", "parameters", "(", ")", ")", "\n", "pi_optimizer", ".", "step", "(", ")", "\n", "\n", "# Unfreeze Q-network so you can optimize it at next DDPG step.", "\n", "for", "p", "in", "ac", ".", "q", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "True", "\n", "\n", "# Finally, update target networks by polyak averaging.", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "p", ",", "p_targ", "in", "zip", "(", "ac", ".", "parameters", "(", ")", ",", "ac_targ", ".", "parameters", "(", ")", ")", ":", "\n", "# NB: We use an in-place operations \"mul_\", \"add_\" to update target", "\n", "# params, as opposed to \"mul\" and \"add\", which would make new tensors.", "\n", "                ", "p_targ", ".", "data", ".", "mul_", "(", "polyak", ")", "\n", "p_targ", ".", "data", ".", "add_", "(", "(", "1", "-", "polyak", ")", "*", "p", ".", "data", ")", "\n", "\n", "\n", "# sync weights", "\n", "", "", "sync_weights", "(", "comm", ",", "ac", ".", "q", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac", ".", "pi", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac_targ", ".", "parameters", "(", ")", ")", "\n", "\n", "# save weights", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "            ", "torch", ".", "save", "(", "ac", ".", "state_dict", "(", ")", ",", "data_dir", "+", "\"ac.pt\"", ")", "\n", "torch", ".", "save", "(", "ac_targ", ".", "state_dict", "(", ")", ",", "data_dir", "+", "\"ac_targ.pt\"", ")", "\n", "\n", "\n", "", "", "def", "get_action", "(", "o", ",", "noise_scale", ")", ":", "\n", "        ", "a", "=", "ac", ".", "act", "(", "torch", ".", "as_tensor", "(", "o", ",", "dtype", "=", "torch", ".", "float32", ")", ")", "\n", "a", "+=", "noise_scale", "*", "np", ".", "random", ".", "randn", "(", "act_dim", ")", "\n", "return", "np", ".", "clip", "(", "a", ",", "-", "act_limit", ",", "act_limit", ")", "\n", "\n", "", "def", "test_agent", "(", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "num_test_episodes", ")", ":", "\n", "            ", "o", ",", "d", ",", "ep_ret", ",", "ep_len", "=", "test_env", ".", "reset", "(", ")", ",", "False", ",", "0", ",", "0", "\n", "while", "not", "(", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ")", ":", "\n", "# Take deterministic actions at test time (noise_scale=0)", "\n", "                ", "o", ",", "r", ",", "d", ",", "_", "=", "test_env", ".", "step", "(", "get_action", "(", "o", ",", "0", ")", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "", "logger", ".", "store", "(", "TestEpRet", "=", "ep_ret", ",", "TestEpLen", "=", "ep_len", ")", "\n", "\n", "# Prepare for interaction with environment", "\n", "", "", "total_steps", "=", "steps_per_epoch", "*", "epochs", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "o", ",", "ep_ret", ",", "ep_len", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", "\n", "\n", "# Main loop: collect experience in env and update/log each epoch", "\n", "for", "t", "in", "range", "(", "total_steps", ")", ":", "\n", "\n", "# Until start_steps have elapsed, randomly sample actions", "\n", "# from a uniform distribution for better exploration. Afterwards, ", "\n", "# use the learned policy (with some noise, via act_noise). ", "\n", "        ", "if", "eval_mode", ":", "\n", "            ", "a", "=", "get_action", "(", "o", ",", "0.0", ")", "\n", "", "else", ":", "\n", "            ", "if", "t", ">", "start_steps", ":", "\n", "                ", "a", "=", "get_action", "(", "o", ",", "act_noise", ")", "\n", "", "else", ":", "\n", "                ", "a", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "\n", "# Step the env", "\n", "", "", "o2", ",", "r", ",", "d", ",", "_", "=", "env", ".", "step", "(", "a", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "\n", "# Ignore the \"done\" signal if it comes from hitting the time", "\n", "# horizon (that is, when it's an artificial terminal signal", "\n", "# that isn't based on the agent's state)", "\n", "d", "=", "False", "if", "ep_len", "==", "max_ep_len", "else", "d", "\n", "\n", "# Store experience to replay buffer", "\n", "replay_buffer", ".", "store", "(", "o", ",", "a", ",", "r", ",", "o2", ",", "d", ")", "\n", "\n", "# Super critical, easy to overlook step: make sure to update ", "\n", "# most recent observation!", "\n", "o", "=", "o2", "\n", "\n", "# End of trajectory handling", "\n", "if", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ":", "\n", "            ", "o", ",", "ep_ret", ",", "ep_len", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", "\n", "\n", "# Update handling", "\n", "", "if", "not", "eval_mode", ":", "\n", "            ", "if", "t", ">=", "update_after", "and", "t", "%", "update_every", "==", "0", ":", "\n", "                ", "for", "_", "in", "range", "(", "update_every", ")", ":", "\n", "                    ", "batch", "=", "replay_buffer", ".", "sample_batch", "(", "batch_size", ")", "\n", "update", "(", "data", "=", "batch", ")", "\n", "\n", "# End of epoch handling", "\n", "", "", "", "if", "(", "t", "+", "1", ")", "%", "steps_per_epoch", "==", "0", ":", "\n", "            ", "epoch", "=", "(", "t", "+", "1", ")", "//", "steps_per_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_network_utils.MLPActor.__init__": [[26, 31], ["torch.Module.__init__", "ddpg_network_utils.mlp", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ",", "act_limit", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "pi_sizes", "=", "[", "obs_dim", "]", "+", "list", "(", "hidden_sizes", ")", "+", "[", "act_dim", "]", "\n", "self", ".", "pi", "=", "mlp", "(", "pi_sizes", ",", "activation", ",", "nn", ".", "Tanh", ")", "\n", "self", ".", "act_limit", "=", "act_limit", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_network_utils.MLPActor.forward": [[32, 35], ["ddpg_network_utils.MLPActor.pi"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ")", ":", "\n", "# Return output from network scaled to action space limits.", "\n", "        ", "return", "self", ".", "act_limit", "*", "self", ".", "pi", "(", "obs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_network_utils.MLPQFunction.__init__": [[38, 41], ["torch.Module.__init__", "ddpg_network_utils.mlp", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "q", "=", "mlp", "(", "[", "obs_dim", "+", "act_dim", "]", "+", "list", "(", "hidden_sizes", ")", "+", "[", "1", "]", ",", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_network_utils.MLPQFunction.forward": [[42, 45], ["ddpg_network_utils.MLPQFunction.q", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ",", "act", ")", ":", "\n", "        ", "q", "=", "self", ".", "q", "(", "torch", ".", "cat", "(", "[", "obs", ",", "act", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "return", "torch", ".", "squeeze", "(", "q", ",", "-", "1", ")", "# Critical to ensure q has right shape.", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_network_utils.MLPActorCritic.__init__": [[48, 59], ["torch.Module.__init__", "ddpg_network_utils.MLPActor", "ddpg_network_utils.MLPQFunction"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["    ", "def", "__init__", "(", "self", ",", "observation_space", ",", "action_space", ",", "hidden_sizes", "=", "(", "256", ",", "256", ")", ",", "\n", "activation", "=", "nn", ".", "ReLU", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "obs_dim", "=", "observation_space", ".", "shape", "[", "0", "]", "\n", "act_dim", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "act_limit", "=", "action_space", ".", "high", "[", "0", "]", "\n", "\n", "# build policy and value functions", "\n", "self", ".", "pi", "=", "MLPActor", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ",", "act_limit", ")", "\n", "self", ".", "q", "=", "MLPQFunction", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_network_utils.MLPActorCritic.act": [[60, 63], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ddpg_network_utils.MLPActorCritic.pi().numpy", "ddpg_network_utils.MLPActorCritic.pi"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy"], ["", "def", "act", "(", "self", ",", "obs", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "return", "self", ".", "pi", "(", "obs", ")", ".", "numpy", "(", ")", "", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_network_utils.combined_shape": [[9, 13], ["numpy.isscalar"], "function", ["None"], ["def", "combined_shape", "(", "length", ",", "shape", "=", "None", ")", ":", "\n", "    ", "if", "shape", "is", "None", ":", "\n", "        ", "return", "(", "length", ",", ")", "\n", "", "return", "(", "length", ",", "shape", ")", "if", "np", ".", "isscalar", "(", "shape", ")", "else", "(", "length", ",", "*", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_network_utils.mlp": [[14, 20], ["range", "torch.Sequential", "len", "torch.Linear", "act", "len", "torch.nn.Tanh"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPActorCritic.act"], ["", "def", "mlp", "(", "sizes", ",", "activation", ",", "output_activation", "=", "nn", ".", "Identity", ")", ":", "\n", "    ", "layers", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "sizes", ")", "-", "1", ")", ":", "\n", "        ", "act", "=", "activation", "if", "j", "<", "len", "(", "sizes", ")", "-", "2", "else", "output_activation", "\n", "layers", "+=", "[", "nn", ".", "Linear", "(", "sizes", "[", "j", "]", ",", "sizes", "[", "j", "+", "1", "]", ")", ",", "act", "(", ")", "]", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.ddpg.ddpg_network_utils.count_vars": [[21, 23], ["sum", "numpy.prod", "module.parameters"], "function", ["None"], ["", "def", "count_vars", "(", "module", ")", ":", "\n", "    ", "return", "sum", "(", "[", "np", ".", "prod", "(", "p", ".", "shape", ")", "for", "p", "in", "module", ".", "parameters", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.GymEnvironment.__init__": [[22, 31], ["agent.GymEnvironment.env.reset", "agent.GymEnvironment.env.seed"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed"], ["    ", "def", "__init__", "(", "self", ",", "agent", ",", "rank", ",", "env", ",", "seed", "=", "None", ")", ":", "\n", "        ", "self", ".", "done", "=", "False", "\n", "\n", "self", ".", "agent", "=", "agent", "\n", "self", ".", "rank", "=", "rank", "\n", "self", ".", "env", "=", "env", "\n", "if", "seed", "is", "not", "None", ":", "\n", "            ", "self", ".", "env", ".", "seed", "(", "seed", ")", "\n", "", "self", ".", "last_state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.GymEnvironment.reset": [[32, 36], ["agent.GymEnvironment.env.reset", "print"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "done", "=", "False", "\n", "self", ".", "last_state", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "print", "(", "'finished reset'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.GymEnvironment.step": [[37, 45], ["print", "agent.GymEnvironment.agent.decide", "agent.GymEnvironment.env.step", "agent.GymEnvironment.agent.reward", "print", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.decide", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.reward"], ["", "def", "step", "(", "self", ")", ":", "\n", "        ", "print", "(", "'stepping'", ")", "\n", "action", "=", "self", ".", "agent", ".", "decide", "(", "torch", ".", "tensor", "(", "self", ".", "last_state", ")", ".", "float", "(", ")", ")", "\n", "new_state", ",", "reward", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "self", ".", "last_state", "=", "new_state", "\n", "self", ".", "agent", ".", "reward", "(", "reward", ")", "\n", "self", ".", "done", "=", "done", "\n", "print", "(", "'done stepping'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.__init__": [[48, 54], ["agent.Replay"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "__iter", "=", "0", "\n", "self", ".", "last_value", "=", "0.0", "\n", "self", ".", "replay", "=", "Replay", "(", ")", "\n", "self", ".", "total_reward", "=", "0.0", "\n", "self", ".", "deterministic", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.decide": [[55, 66], ["agent.A2CAgent.replay.states.append", "agent.A2CAgent.forward", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "agent.A2CAgent.replay.actions.append", "value.item", "torch.distributions.Categorical.sample().item", "torch.distributions.Categorical.sample().item", "torch.distributions.Categorical.sample().item", "torch.distributions.Categorical.sample().item", "torch.distributions.Categorical.sample().item", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.argmax", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample", "torch.distributions.Categorical.sample"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPQFunction.forward"], ["", "def", "decide", "(", "self", ",", "x", ")", ":", "\n", "        ", "self", ".", "replay", ".", "states", ".", "append", "(", "x", ")", "\n", "probs", ",", "value", "=", "self", ".", "forward", "(", "x", ")", "\n", "\n", "distrib", "=", "Categorical", "(", "probs", ")", "\n", "action", "=", "distrib", ".", "sample", "(", ")", ".", "item", "(", ")", "if", "not", "self", ".", "deterministic", "else", "torch", ".", "argmax", "(", "probs", ")", "\n", "self", ".", "replay", ".", "actions", ".", "append", "(", "action", ")", "\n", "self", ".", "last_value", "=", "value", ".", "item", "(", ")", "\n", "self", ".", "__iter", "+=", "1", "\n", "self", ".", "replay", ".", "iter", "=", "self", ".", "__iter", "\n", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.reward": [[67, 70], ["agent.A2CAgent.replay.rewards.append"], "methods", ["None"], ["", "def", "reward", "(", "self", ",", "r", ")", ":", "\n", "        ", "self", ".", "replay", ".", "rewards", ".", "append", "(", "r", ")", "\n", "self", ".", "total_reward", "+=", "r", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.forward": [[71, 73], ["None"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.on_reset": [[74, 76], ["None"], "methods", ["None"], ["", "def", "on_reset", "(", "self", ",", "is_terminal", ")", ":", "\n", "        ", "raise", "NotImplementedError", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.set_replay_hidden": [[77, 79], ["None"], "methods", ["None"], ["", "def", "set_replay_hidden", "(", "self", ",", "hidden", ")", ":", "\n", "        ", "self", ".", "replay", ".", "hidden0", "=", "hidden", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.reset": [[80, 86], ["agent.Replay", "agent.A2CAgent.on_reset"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.Brain.on_reset"], ["", "def", "reset", "(", "self", ",", "new_episode", "=", "True", ")", ":", "\n", "        ", "if", "new_episode", ":", "\n", "            ", "self", ".", "__iter", "=", "0", "\n", "self", ".", "total_reward", "=", "0.0", "\n", "", "self", ".", "replay", "=", "Replay", "(", ")", "\n", "self", ".", "on_reset", "(", "new_episode", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.end_replay": [[87, 97], ["agent.A2CAgent.reset", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "end_replay", "(", "self", ",", "is_terminal", ")", ":", "\n", "        ", "replay", "=", "self", ".", "replay", "\n", "replay", ".", "is_terminal", "=", "is_terminal", "\n", "replay", ".", "iter", "=", "self", ".", "__iter", "\n", "replay", ".", "total_reward", "=", "self", ".", "total_reward", "\n", "if", "not", "is_terminal", ":", "\n", "            ", "self", ".", "reset", "(", "new_episode", "=", "False", ")", "\n", "", "if", "replay", ".", "hidden0", "is", "None", ":", "\n", "            ", "replay", ".", "hidden0", "=", "torch", ".", "zeros", "(", "1", ")", "\n", "", "return", "replay", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.Brain.__init__": [[99, 106], ["torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "torch.Module.__init__", "agent.A2CAgent.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "nn", ".", "Module", ".", "__init__", "(", "self", ")", "\n", "A2CAgent", ".", "__init__", "(", "self", ")", "\n", "\n", "self", ".", "affine1", "=", "nn", ".", "Linear", "(", "4", ",", "128", ")", "\n", "self", ".", "action_head", "=", "nn", ".", "Linear", "(", "128", ",", "2", ")", "\n", "self", ".", "value_head", "=", "nn", ".", "Linear", "(", "128", ",", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.Brain.forward": [[107, 112], ["torch.relu", "torch.relu", "torch.relu", "torch.relu", "torch.relu", "agent.Brain.action_head", "agent.Brain.value_head", "agent.Brain.affine1", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "F", ".", "relu", "(", "self", ".", "affine1", "(", "x", ")", ")", "\n", "action_scores", "=", "self", ".", "action_head", "(", "x", ")", "\n", "value", "=", "self", ".", "value_head", "(", "x", ")", "\n", "return", "F", ".", "softmax", "(", "action_scores", ",", "dim", "=", "-", "1", ")", ",", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.Brain.save": [[113, 117], ["open", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "open.close", "agent.Brain.state_dict"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.close"], ["", "def", "save", "(", "self", ",", "filename", ")", ":", "\n", "        ", "f", "=", "open", "(", "filename", ",", "\"wb\"", ")", "\n", "torch", ".", "save", "(", "self", ".", "state_dict", "(", ")", ",", "f", ")", "\n", "f", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.Brain.load": [[118, 121], ["torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "agent.Brain.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load"], ["", "def", "load", "(", "self", ",", "filename", ")", ":", "\n", "        ", "state_dict", "=", "torch", ".", "load", "(", "filename", ")", "\n", "self", ".", "load_state_dict", "(", "state_dict", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.Brain.on_reset": [[122, 124], ["None"], "methods", ["None"], ["", "def", "on_reset", "(", "self", ",", "new_episode", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.Replay.__init__": [[128, 136], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "hidden0", "=", "None", "\n", "self", ".", "states", "=", "[", "]", "\n", "self", ".", "actions", "=", "[", "]", "\n", "self", ".", "rewards", "=", "[", "]", "\n", "self", ".", "is_terminal", "=", "False", "\n", "self", ".", "iter", "=", "0", "\n", "self", ".", "total_reward", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.init_mpi_rng": [[138, 151], ["comm.Get_rank", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "int", "numpy.random.seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "torch.manual_seed", "int", "[].item", "time.time", "time.time", "time.time", "time.time", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint", "torch.randint"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed"], ["    ", "@", "staticmethod", "\n", "def", "init_mpi_rng", "(", "seed_base", "=", "None", ")", ":", "\n", "        ", "comm", "=", "MPI", ".", "COMM_WORLD", "\n", "rank", "=", "comm", ".", "Get_rank", "(", ")", "\n", "if", "seed_base", "is", "None", ":", "\n", "            ", "seed_base", "=", "int", "(", "time", ".", "time", "(", ")", ")", "\n", "", "torch", ".", "manual_seed", "(", "seed_base", ")", "\n", "\n", "seed", "=", "int", "(", "torch", ".", "randint", "(", "2", "**", "32", ",", "(", "rank", "+", "1", ",", ")", ")", "[", "rank", "]", ".", "item", "(", ")", ")", "\n", "seed", "=", "seed_base", "^", "seed", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "return", "seed", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.__init__": [[153, 176], ["agent.MPIA2C.init_mpi_rng", "mpi4py.MPI.COMM_WORLD.Get_rank", "agent.Brain", "agent.GymEnvironment", "torch.Adam", "torch.Adam", "torch.Adam", "torch.Adam", "torch.Adam", "print", "agent.MPIA2C.agent.parameters"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.init_mpi_rng"], ["", "def", "__init__", "(", "self", ",", "env", ",", "comm", ")", ":", "\n", "        ", "self", ".", "use_gpu", "=", "False", "\n", "\n", "self", ".", "seed", "=", "MPIA2C", ".", "init_mpi_rng", "(", "0", ")", "\n", "self", ".", "comm", "=", "comm", "\n", "self", ".", "rank", "=", "MPI", ".", "COMM_WORLD", ".", "Get_rank", "(", ")", "#was passed by parameter previously", "\n", "self", ".", "replays", "=", "[", "]", "\n", "self", ".", "episode", "=", "0", "\n", "self", ".", "max_train_iters", "=", "0", "\n", "self", ".", "agent", "=", "Brain", "(", ")", "\n", "self", ".", "env", "=", "GymEnvironment", "(", "self", ".", "agent", ",", "self", ".", "rank", ",", "env", ",", "self", ".", "seed", ")", "\n", "\n", "self", ".", "gamma", "=", "0.99", "\n", "self", ".", "steps_in_replay", "=", "1", "\n", "\n", "#self.optimizer = optimizer", "\n", "self", ".", "optimizer", "=", "optim", ".", "Adam", "(", "self", ".", "agent", ".", "parameters", "(", ")", ",", "lr", "=", "3e-3", ")", "\n", "self", ".", "ppo_clip", "=", "0.2", "\n", "self", ".", "ppo_iters", "=", "4", "\n", "\n", "self", ".", "on_batch_done", "=", "self", ".", "batch_done", "\n", "self", ".", "on_episodes_done", "=", "self", ".", "episodes_done", "\n", "print", "(", "'finished agent init'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.batch_done": [[178, 180], ["None"], "methods", ["None"], ["", "def", "batch_done", "(", "self", ",", "loss", ",", "batch", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.episodes_done": [[181, 203], ["torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "torch.tensor().float().mean().item", "time.time", "time.time", "time.time", "time.time", "print", "sys.stdout.flush", "agent.MPIA2C.agent.save", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "torch.tensor().float().mean", "time.time", "time.time", "time.time", "time.time", "print", "sys.exit", "round", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "torch.tensor().float", "round", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save"], ["", "def", "episodes_done", "(", "self", ",", "episode", ",", "batch_rewards", ",", "batch_iters", ")", ":", "\n", "        ", "total_reward_avg", "=", "torch", ".", "tensor", "(", "batch_rewards", ")", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "episode_reward", "=", "total_reward_avg", "\n", "iter_avg", "=", "torch", ".", "tensor", "(", "batch_iters", ")", ".", "float", "(", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "self", ".", "running_reward", "=", "self", ".", "running_reward", "*", "0.99", "+", "total_reward_avg", "*", "0.01", "\n", "self", ".", "running_iter", "=", "self", ".", "running_iter", "*", "0.99", "+", "iter_avg", "*", "0.01", "\n", "self", ".", "episode", "=", "episode", "\n", "\n", "if", "episode", "%", "self", ".", "log_interval", "==", "0", ":", "\n", "            ", "dt", "=", "time", ".", "time", "(", ")", "-", "self", ".", "lst_tick", "\n", "self", ".", "lst_tick", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "total_time", "+=", "dt", "\n", "\n", "print", "(", "'Episode {}\\tLast length: {:5}\\tAverage length: {:.2f}'", ".", "format", "(", "self", ".", "episode", ",", "round", "(", "self", ".", "episode_reward", ")", ",", "self", ".", "running_reward", ")", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "if", "self", ".", "running_reward", ">", "self", ".", "threshold", ":", "\n", "                ", "print", "(", "\"Solved! Running reward is now {} and \"", "\n", "\"the last episode runs to {} time steps!\"", ".", "format", "(", "self", ".", "running_reward", ",", "round", "(", "self", ".", "episode_reward", ")", ")", ")", "\n", "sys", ".", "exit", "(", "0", ")", "\n", "\n", "", "", "if", "episode", "%", "self", ".", "save_interval", "==", "0", ":", "\n", "            ", "self", ".", "agent", ".", "save", "(", "self", ".", "dst", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.ppo_loss": [[204, 214], ["torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "torch.min", "loss.sum", "mask.sum"], "methods", ["None"], ["", "", "def", "ppo_loss", "(", "self", ",", "probs", ",", "old_probs", ",", "pred_vals", ",", "true_vals", ",", "mask", ")", ":", "\n", "        ", "advantage", "=", "(", "true_vals", "-", "pred_vals", ")", ".", "detach", "(", ")", "\n", "prob_ratio", "=", "torch", ".", "exp", "(", "probs", "-", "old_probs", ")", "\n", "ploss_cpi", "=", "prob_ratio", "*", "advantage", "\n", "ploss_clip", "=", "torch", ".", "clamp", "(", "prob_ratio", ",", "1", "-", "self", ".", "ppo_clip", ",", "1", "+", "self", ".", "ppo_clip", ")", "*", "advantage", "\n", "ploss", "=", "-", "torch", ".", "min", "(", "ploss_cpi", ",", "ploss_clip", ")", "\n", "vloss", "=", "F", ".", "smooth_l1_loss", "(", "pred_vals", ",", "true_vals", ",", "reduce", "=", "False", ")", "\n", "loss", "=", "(", "ploss", "+", "vloss", ")", "*", "mask", "\n", "loss", "=", "loss", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.a2c_loss": [[215, 222], ["torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "torch.smooth_l1_loss", "loss.sum", "mask.sum"], "methods", ["None"], ["", "def", "a2c_loss", "(", "self", ",", "logprobs", ",", "pred_vals", ",", "true_vals", ",", "mask", ")", ":", "\n", "        ", "advantage", "=", "(", "true_vals", "-", "pred_vals", ")", ".", "detach", "(", ")", "\n", "ploss", "=", "-", "(", "logprobs", "*", "advantage", ")", "\n", "vloss", "=", "F", ".", "smooth_l1_loss", "(", "pred_vals", ",", "true_vals", ",", "reduce", "=", "False", ")", "\n", "loss", "=", "(", "ploss", "+", "vloss", ")", "*", "mask", "\n", "loss", "=", "loss", ".", "sum", "(", ")", "/", "mask", ".", "sum", "(", ")", "\n", "return", "loss", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.batch_replays": [[223, 285], ["len", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "torch.ones", "len", "sys.stdout.flush", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad.unsqueeze_", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad.unsqueeze_", "h0.unsqueeze_", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad", "torch.pad.unsqueeze_", "torch.split.cuda", "torch.split.cuda", "torch.split.cuda", "torch.split.cuda", "torch.split.cuda", "true_values.cuda.cuda.cuda", "torch.split.cuda", "torch.split.cuda", "torch.split.cuda", "torch.split.cuda", "torch.split.cuda", "hiddens.cuda.cuda.cuda", "mask.cuda.cuda.cuda", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "torch.split", "total_rewards.append", "iters.append", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "range"], "methods", ["None"], ["", "def", "batch_replays", "(", "self", ",", "replays", ",", "max_pad", ")", ":", "\n", "        ", "batch_size", "=", "len", "(", "replays", ")", "\n", "mask", "=", "torch", ".", "ones", "(", "batch_size", ",", "max_pad", ")", "\n", "\n", "init_done", "=", "False", "\n", "i", "=", "0", "\n", "\n", "states", "=", "None", "\n", "actions", "=", "None", "\n", "true_values", "=", "None", "\n", "hiddens", "=", "None", "\n", "\n", "total_rewards", "=", "[", "]", "\n", "iters", "=", "[", "]", "\n", "\n", "for", "r", "in", "replays", ":", "\n", "            ", "if", "r", ".", "is_terminal", ":", "\n", "                ", "total_rewards", ".", "append", "(", "r", ".", "total_reward", ")", "\n", "iters", ".", "append", "(", "r", ".", "iter", ")", "\n", "\n", "", "seq_len", "=", "len", "(", "r", ".", "rewards", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "if", "seq_len", "<", "max_pad", ":", "\n", "                ", "mask", "[", "i", ",", "range", "(", "seq_len", ",", "max_pad", ")", "]", "=", "0.0", "\n", "\n", "", "tv", "=", "torch", ".", "tensor", "(", "r", ".", "rewards", ")", "\n", "tv", "=", "F", ".", "pad", "(", "tv", ",", "(", "0", ",", "max_pad", "-", "seq_len", ")", ")", "\n", "tv", ".", "unsqueeze_", "(", "0", ")", "\n", "\n", "st", "=", "r", ".", "states", "\n", "st", "=", "F", ".", "pad", "(", "st", ",", "(", "0", ",", "0", ",", "0", ",", "max_pad", "-", "seq_len", ")", ")", "\n", "st", ".", "unsqueeze_", "(", "0", ")", "\n", "\n", "h0", "=", "r", ".", "hidden0", "\n", "h0", ".", "unsqueeze_", "(", "0", ")", "\n", "\n", "ac", "=", "r", ".", "actions", "\n", "ac", "=", "F", ".", "pad", "(", "ac", ",", "(", "0", ",", "max_pad", "-", "seq_len", ")", ")", "\n", "ac", ".", "unsqueeze_", "(", "0", ")", "\n", "\n", "actions", "=", "torch", ".", "cat", "(", "(", "actions", ",", "ac", ")", ")", "if", "init_done", "else", "ac", "\n", "true_values", "=", "torch", ".", "cat", "(", "(", "true_values", ",", "tv", ")", ")", "if", "init_done", "else", "tv", "\n", "states", "=", "torch", ".", "cat", "(", "(", "states", ",", "st", ")", ")", "if", "init_done", "else", "st", "\n", "hiddens", "=", "torch", ".", "cat", "(", "(", "hiddens", ",", "h0", ")", ")", "if", "init_done", "else", "h0", "\n", "\n", "init_done", "=", "True", "\n", "i", "+=", "1", "\n", "\n", "", "if", "self", ".", "use_gpu", ":", "\n", "            ", "actions", "=", "actions", ".", "cuda", "(", ")", "\n", "true_values", "=", "true_values", ".", "cuda", "(", ")", "\n", "states", "=", "states", ".", "cuda", "(", ")", "\n", "hiddens", "=", "hiddens", ".", "cuda", "(", ")", "\n", "mask", "=", "mask", ".", "cuda", "(", ")", "\n", "\n", "", "if", "batch_size", "==", "1", ":", "\n", "            ", "states", "=", "torch", ".", "split", "(", "states", ",", "1", ",", "dim", "=", "1", ")", "\n", "actions", "=", "torch", ".", "split", "(", "actions", ",", "1", ",", "dim", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "states", "=", "torch", ".", "split", "(", "states", ",", "1", ",", "dim", "=", "1", ")", "\n", "actions", "=", "torch", ".", "split", "(", "actions", ",", "1", ",", "dim", "=", "1", ")", "\n", "", "return", "states", ",", "actions", ",", "hiddens", ",", "true_values", ",", "mask", ",", "total_rewards", ",", "iters", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.master": [[286, 368], ["agent.MPIA2C.comm.barrier", "print", "agent.MPIA2C.agent.cuda", "agent.MPIA2C.comm.gather", "sys.stdout.flush", "len", "len", "agent.MPIA2C.batch_replays", "total_rewards_in_replays.extend", "iters_in_replays.extend", "range", "agent.MPIA2C.agent.reset", "agent.MPIA2C.comm.bcast", "agent.MPIA2C.on_batch_done", "zip", "true_values.squeeze_", "agent.MPIA2C.optimizer.zero_grad", "agent.MPIA2C.backward", "agent.MPIA2C.optimizer.step", "agent.MPIA2C.agent.state_dict", "len", "agent.MPIA2C.on_episodes_done", "max", "agent.MPIA2C.agent.forward", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "logprob_list.append", "pred_values_list.append", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "agent.MPIA2C.ppo_\u00dfloss", "agent.MPIA2C.a2c_loss", "s.view.view.squeeze_", "a.squeeze_", "s.view.view.view", "logprobs.detach", "s.view.view.size", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.batch_replays", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPQFunction.forward", "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.a2c_loss"], ["", "def", "master", "(", "self", ")", ":", "\n", "        ", "agents", "=", "self", ".", "comm", ".", "size", "-", "1", "\n", "total_rewards_in_replays", "=", "[", "]", "\n", "iters_in_replays", "=", "[", "]", "\n", "iter", "=", "0", "\n", "episodes", "=", "0", "\n", "\n", "if", "self", ".", "use_gpu", ":", "\n", "            ", "self", ".", "agent", ".", "cuda", "(", ")", "\n", "\n", "", "self", ".", "comm", ".", "barrier", "(", ")", "\n", "while", "iter", "<", "self", ".", "max_train_iters", ":", "\n", "            ", "replays", "=", "self", ".", "comm", ".", "gather", "(", "None", ",", "root", "=", "0", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "del", "replays", "[", "0", "]", "\n", "batch_size", "=", "len", "(", "replays", ")", "\n", "\n", "if", "batch_size", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "", "max_pad", "=", "len", "(", "max", "(", "replays", ",", "key", "=", "lambda", "r", ":", "len", "(", "r", ".", "rewards", ")", ")", ".", "rewards", ")", "\n", "\n", "# self.replays.extend(replays)", "\n", "states", ",", "actions", ",", "hiddens", ",", "true_values", ",", "mask", ",", "batch_total_rewards", ",", "batch_iters", "=", "self", ".", "batch_replays", "(", "replays", ",", "max_pad", ")", "\n", "\n", "total_rewards_in_replays", ".", "extend", "(", "batch_total_rewards", ")", "\n", "iters_in_replays", ".", "extend", "(", "batch_iters", ")", "\n", "\n", "old_logprobs", "=", "None", "\n", "loss", "=", "0.0", "\n", "for", "k", "in", "range", "(", "self", ".", "ppo_iters", ")", ":", "\n", "                ", "self", ".", "agent", ".", "hidden", "=", "hiddens", "\n", "logprob_list", "=", "[", "]", "\n", "pred_values_list", "=", "[", "]", "\n", "\n", "for", "s", ",", "a", "in", "zip", "(", "states", ",", "actions", ")", ":", "\n", "                    ", "if", "batch_size", ">", "1", ":", "\n", "                        ", "s", ".", "squeeze_", "(", ")", "\n", "a", ".", "squeeze_", "(", ")", "\n", "", "else", ":", "\n", "                        ", "s", "=", "s", ".", "view", "(", "-", "1", ",", "s", ".", "size", "(", "2", ")", ")", "\n", "\n", "", "prob", ",", "value", "=", "self", ".", "agent", ".", "forward", "(", "s", ")", "\n", "distrib", "=", "Categorical", "(", "prob", ")", "\n", "logprob", "=", "distrib", ".", "log_prob", "(", "a", ")", "\n", "\n", "logprob_list", ".", "append", "(", "logprob", ")", "\n", "pred_values_list", ".", "append", "(", "value", ")", "\n", "\n", "", "logprobs", "=", "torch", ".", "stack", "(", "logprob_list", ")", ".", "t", "(", ")", "if", "batch_size", ">", "1", "else", "torch", ".", "stack", "(", "logprob_list", ")", ".", "squeeze_", "(", ")", "\n", "\n", "pred_vals", "=", "torch", ".", "stack", "(", "pred_values_list", ")", ".", "squeeze", "(", ")", ".", "t", "(", ")", "if", "batch_size", ">", "1", "else", "torch", ".", "stack", "(", "\n", "pred_values_list", ")", ".", "squeeze_", "(", ")", "\n", "\n", "true_values", ".", "squeeze_", "(", ")", "\n", "\n", "if", "self", ".", "ppo_iters", ">", "1", ":", "\n", "                    ", "if", "old_logprobs", "is", "None", ":", "\n", "                        ", "old_logprobs", "=", "logprobs", ".", "detach", "(", ")", "\n", "", "loss", "=", "self", ".", "ppo_\u00dfloss", "(", "logprobs", ",", "old_logprobs", ",", "pred_vals", ",", "true_values", ",", "mask", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "self", ".", "a2c_loss", "(", "logprobs", ",", "pred_vals", ",", "true_values", ",", "mask", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "iter", "+=", "1", "\n", "self", ".", "agent", ".", "reset", "(", ")", "\n", "self", ".", "comm", ".", "bcast", "(", "self", ".", "agent", ".", "state_dict", "(", ")", ",", "root", "=", "0", ")", "\n", "\n", "self", ".", "on_batch_done", "(", "loss", ",", "iter", ")", "\n", "\n", "# keep track of full episodes", "\n", "if", "len", "(", "total_rewards_in_replays", ")", ">=", "agents", ":", "\n", "                ", "self", ".", "on_episodes_done", "(", "episodes", ",", "total_rewards_in_replays", ",", "iters_in_replays", ")", "\n", "total_rewards_in_replays", "=", "[", "]", "\n", "iters_in_replays", "=", "[", "]", "\n", "episodes", "+=", "1", "\n", "", "", "print", "(", "\"Done!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.slave": [[369, 397], ["agent.MPIA2C.comm.barrier", "print", "agent.MPIA2C.agent.reset", "env.reset", "env.step", "agent.MPIA2C.agent.end_replay", "agent.MPIA2C.discount_rewards", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "agent.MPIA2C.hidden0.squeeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "agent.MPIA2C.comm.gather", "agent.MPIA2C.comm.bcast", "agent.MPIA2C.agent.load_state_dict"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.end_replay", "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.discount_rewards"], ["", "def", "slave", "(", "self", ")", ":", "\n", "        ", "env", "=", "self", ".", "env", "\n", "batch", "=", "0", "\n", "\n", "self", ".", "comm", ".", "barrier", "(", ")", "\n", "while", "batch", "<", "self", ".", "max_train_iters", ":", "\n", "            ", "self", ".", "agent", ".", "reset", "(", ")", "\n", "env", ".", "reset", "(", ")", "\n", "steps", "=", "0", "\n", "while", "not", "env", ".", "done", ":", "\n", "                ", "env", ".", "step", "(", ")", "\n", "steps", "+=", "1", "\n", "if", "(", "steps", "%", "self", ".", "steps_in_replay", "==", "0", "and", "steps", "!=", "0", ")", "or", "env", ".", "done", ":", "\n", "                    ", "replay", "=", "self", ".", "agent", ".", "end_replay", "(", "env", ".", "done", ")", "\n", "replay", ".", "rewards", "=", "MPIA2C", ".", "discount_rewards", "(", "replay", ".", "rewards", ",", "self", ".", "gamma", ",", "env", ".", "done", ",", "self", ".", "agent", ".", "last_value", ")", "\n", "replay", ".", "states", "=", "torch", ".", "stack", "(", "replay", ".", "states", ")", "\n", "\n", "replay", ".", "hidden0", "=", "replay", ".", "hidden0", ".", "squeeze", "(", ")", "\n", "replay", ".", "actions", "=", "torch", ".", "tensor", "(", "replay", ".", "actions", ")", "\n", "self", ".", "comm", ".", "gather", "(", "replay", ",", "root", "=", "0", ")", "\n", "\n", "batch", "+=", "1", "\n", "\n", "new_weights", "=", "self", ".", "comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "self", ".", "agent", ".", "load_state_dict", "(", "new_weights", ")", "\n", "\n", "", "", "self", ".", "episode", "+=", "1", "\n", "", "print", "(", "\"Agent %d terminated\"", "%", "self", ".", "rank", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.run": [[399, 497], ["print", "agent.MPIA2C.comm.barrier", "agent.MPIA2C.agent.reset", "env.reset", "agent.MPIA2C.comm.gather", "sys.stdout.flush", "len", "len", "agent.MPIA2C.batch_replays", "total_rewards_in_replays.extend", "iters_in_replays.extend", "range", "agent.MPIA2C.agent.reset", "agent.MPIA2C.comm.bcast", "agent.MPIA2C.on_batch_done", "env.step", "zip", "true_values.squeeze_", "agent.MPIA2C.optimizer.zero_grad", "agent.MPIA2C.backward", "agent.MPIA2C.optimizer.step", "agent.MPIA2C.agent.state_dict", "len", "agent.MPIA2C.on_episodes_done", "agent.MPIA2C.agent.end_replay", "agent.MPIA2C.discount_rewards", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "agent.MPIA2C.hidden0.squeeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "agent.MPIA2C.comm.gather", "agent.MPIA2C.comm.bcast", "agent.MPIA2C.agent.load_state_dict", "max", "agent.MPIA2C.agent.forward", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "torch.distributions.Categorical.log_prob", "logprob_list.append", "pred_values_list.append", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().t", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze().t", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "torch.stack().squeeze_", "agent.MPIA2C.ppo_\u00dfloss", "agent.MPIA2C.a2c_loss", "s.view.view.squeeze_", "a.squeeze_", "s.view.view.view", "logprobs.detach", "s.view.view.size", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack().squeeze", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "len", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.batch_replays", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.A2CAgent.end_replay", "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.discount_rewards", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPQFunction.forward", "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.a2c_loss"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "        ", "print", "(", "'starting to run the agent'", ")", "\n", "env", "=", "self", ".", "env", "\n", "batch", "=", "0", "\n", "self", ".", "max_train_iters", "=", "num_steps", "\n", "self", ".", "comm", ".", "barrier", "(", ")", "\n", "while", "batch", "<", "self", ".", "max_train_iters", ":", "\n", "            ", "self", ".", "agent", ".", "reset", "(", ")", "\n", "env", ".", "reset", "(", ")", "\n", "steps", "=", "0", "\n", "while", "not", "env", ".", "done", ":", "\n", "                ", "env", ".", "step", "(", ")", "\n", "steps", "+=", "1", "\n", "if", "(", "steps", "%", "self", ".", "steps_in_replay", "==", "0", "and", "steps", "!=", "0", ")", "or", "env", ".", "done", ":", "\n", "                    ", "replay", "=", "self", ".", "agent", ".", "end_replay", "(", "env", ".", "done", ")", "\n", "replay", ".", "rewards", "=", "MPIA2C", ".", "discount_rewards", "(", "replay", ".", "rewards", ",", "self", ".", "gamma", ",", "env", ".", "done", ",", "self", ".", "agent", ".", "last_value", ")", "\n", "replay", ".", "states", "=", "torch", ".", "stack", "(", "replay", ".", "states", ")", "\n", "\n", "replay", ".", "hidden0", "=", "replay", ".", "hidden0", ".", "squeeze", "(", ")", "\n", "replay", ".", "actions", "=", "torch", ".", "tensor", "(", "replay", ".", "actions", ")", "\n", "self", ".", "comm", ".", "gather", "(", "replay", ",", "root", "=", "0", ")", "\n", "\n", "batch", "+=", "1", "\n", "\n", "new_weights", "=", "self", ".", "comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "self", ".", "agent", ".", "load_state_dict", "(", "new_weights", ")", "\n", "\n", "", "", "self", ".", "episode", "+=", "1", "\n", "#if self.rank == 0:", "\n", "replays", "=", "self", ".", "comm", ".", "gather", "(", "None", ",", "root", "=", "0", ")", "\n", "sys", ".", "stdout", ".", "flush", "(", ")", "\n", "del", "replays", "[", "0", "]", "\n", "batch_size", "=", "len", "(", "replays", ")", "\n", "\n", "if", "batch_size", "==", "0", ":", "\n", "                ", "break", "\n", "\n", "", "max_pad", "=", "len", "(", "max", "(", "replays", ",", "key", "=", "lambda", "r", ":", "len", "(", "r", ".", "rewards", ")", ")", ".", "rewards", ")", "\n", "\n", "# self.replays.extend(replays)", "\n", "states", ",", "actions", ",", "hiddens", ",", "true_values", ",", "mask", ",", "batch_total_rewards", ",", "batch_iters", "=", "self", ".", "batch_replays", "(", "replays", ",", "max_pad", ")", "\n", "\n", "total_rewards_in_replays", ".", "extend", "(", "batch_total_rewards", ")", "\n", "iters_in_replays", ".", "extend", "(", "batch_iters", ")", "\n", "\n", "old_logprobs", "=", "None", "\n", "loss", "=", "0.0", "\n", "for", "k", "in", "range", "(", "self", ".", "ppo_iters", ")", ":", "\n", "                ", "self", ".", "agent", ".", "hidden", "=", "hiddens", "\n", "logprob_list", "=", "[", "]", "\n", "pred_values_list", "=", "[", "]", "\n", "\n", "for", "s", ",", "a", "in", "zip", "(", "states", ",", "actions", ")", ":", "\n", "                    ", "if", "batch_size", ">", "1", ":", "\n", "                        ", "s", ".", "squeeze_", "(", ")", "\n", "a", ".", "squeeze_", "(", ")", "\n", "", "else", ":", "\n", "                        ", "s", "=", "s", ".", "view", "(", "-", "1", ",", "s", ".", "size", "(", "2", ")", ")", "\n", "\n", "", "prob", ",", "value", "=", "self", ".", "agent", ".", "forward", "(", "s", ")", "\n", "distrib", "=", "Categorical", "(", "prob", ")", "\n", "logprob", "=", "distrib", ".", "log_prob", "(", "a", ")", "\n", "\n", "logprob_list", ".", "append", "(", "logprob", ")", "\n", "pred_values_list", ".", "append", "(", "value", ")", "\n", "\n", "", "logprobs", "=", "torch", ".", "stack", "(", "logprob_list", ")", ".", "t", "(", ")", "if", "batch_size", ">", "1", "else", "torch", ".", "stack", "(", "logprob_list", ")", ".", "squeeze_", "(", ")", "\n", "\n", "pred_vals", "=", "torch", ".", "stack", "(", "pred_values_list", ")", ".", "squeeze", "(", ")", ".", "t", "(", ")", "if", "batch_size", ">", "1", "else", "torch", ".", "stack", "(", "\n", "pred_values_list", ")", ".", "squeeze_", "(", ")", "\n", "\n", "true_values", ".", "squeeze_", "(", ")", "\n", "\n", "if", "self", ".", "ppo_iters", ">", "1", ":", "\n", "                    ", "if", "old_logprobs", "is", "None", ":", "\n", "                        ", "old_logprobs", "=", "logprobs", ".", "detach", "(", ")", "\n", "", "loss", "=", "self", ".", "ppo_\u00dfloss", "(", "logprobs", ",", "old_logprobs", ",", "pred_vals", ",", "true_values", ",", "mask", ")", "\n", "", "else", ":", "\n", "                    ", "loss", "=", "self", ".", "a2c_loss", "(", "logprobs", ",", "pred_vals", ",", "true_values", ",", "mask", ")", "\n", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "\n", "", "iter", "+=", "1", "\n", "self", ".", "agent", ".", "reset", "(", ")", "\n", "self", ".", "comm", ".", "bcast", "(", "self", ".", "agent", ".", "state_dict", "(", ")", ",", "root", "=", "0", ")", "\n", "\n", "self", ".", "on_batch_done", "(", "loss", ",", "iter", ")", "\n", "\n", "# keep track of full episodes", "\n", "if", "len", "(", "total_rewards_in_replays", ")", ">=", "agents", ":", "\n", "                ", "self", ".", "on_episodes_done", "(", "episodes", ",", "total_rewards_in_replays", ",", "iters_in_replays", ")", "\n", "total_rewards_in_replays", "=", "[", "]", "\n", "iters_in_replays", "=", "[", "]", "\n", "episodes", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.torch_ppo.agent.MPIA2C.discount_rewards": [[511, 519], ["values.insert"], "methods", ["None"], ["", "", "", "@", "staticmethod", "\n", "def", "discount_rewards", "(", "rewards", ",", "gamma", ",", "is_terminal", ",", "last_value", ")", ":", "\n", "        ", "values", "=", "[", "]", "\n", "v", "=", "last_value", "if", "not", "is_terminal", "else", "0.0", "\n", "for", "r", "in", "rewards", "[", ":", ":", "-", "1", "]", ":", "\n", "            ", "v", "=", "r", "+", "gamma", "*", "v", "\n", "values", ".", "insert", "(", "0", ",", "v", ")", "\n", "", "return", "values", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.random.random_policy.RandomPolicy.__init__": [[9, 12], ["None"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ")", ":", "\n", "\t\t", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.random.random_policy.RandomPolicy.run": [[13, 33], ["arena5.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater", "random_policy.RandomPolicy.env.reset", "int", "range", "random_policy.RandomPolicy.env.action_space.sample", "random_policy.RandomPolicy.env.step", "random_policy.RandomPolicy.comm.Get_size", "random_policy.RandomPolicy.env.reset"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "\n", "\t\t", "self", ".", "env", "=", "MPISynchronizedPRUpdater", "(", "self", ".", "env", ",", "self", ".", "comm", ",", "policy_record", ")", "\n", "\n", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "# cumr = 0.0", "\n", "# ep_len = 0", "\n", "\n", "#since we do not synchronize step counts, get steps needed for individual worker:", "\n", "local_steps", "=", "int", "(", "num_steps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "\n", "for", "stp", "in", "range", "(", "local_steps", ")", ":", "\n", "\t\t\t", "a", "=", "self", ".", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "_", ",", "r", ",", "done", ",", "_", "=", "self", ".", "env", ".", "step", "(", "a", ")", "\n", "# cumr += r", "\n", "# ep_len += 1", "\n", "\n", "if", "done", ":", "\n", "\t\t\t\t", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.masac_policy.MASACPolicy.__init__": [[10, 15], ["print"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ",", "eval_mode", "=", "False", ")", ":", "\n", "        ", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "self", ".", "eval_mode", "=", "eval_mode", "\n", "print", "(", "env", ".", "observation_spaces", ",", "env", ".", "action_spaces", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.masac_policy.MASACPolicy.run": [[17, 27], ["arena5.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater", "int", "arena5.algos.masac.masac.masac", "masac_policy.MASACPolicy.comm.Get_size", "dict"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.masac.masac"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "env", "=", "MPISynchronizedPRUpdater", "(", "self", ".", "env", ",", "self", ".", "comm", ",", "policy_record", ",", "sum_over", "=", "True", ")", "\n", "local_steps", "=", "int", "(", "num_steps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "\n", "masac", "(", "lambda", ":", "self", ".", "env", ",", "self", ".", "comm", ",", "data_dir", ",", "policy_record", ",", "self", ".", "eval_mode", ",", "\n", "actor_critic", "=", "core", ".", "MLPActorCritic", ",", "\n", "ac_kwargs", "=", "dict", "(", "hidden_sizes", "=", "[", "256", ",", "256", "]", ")", ",", "\n", "gamma", "=", "0.99", ",", "seed", "=", "1337", ",", "steps_per_epoch", "=", "local_steps", ",", "epochs", "=", "1", ",", "\n", "logger_kwargs", "=", "None", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.SquashedGaussianMLPActor.__init__": [[34, 40], ["torch.Module.__init__", "network_utils.mlp", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ",", "act_limit", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "net", "=", "mlp", "(", "[", "obs_dim", "]", "+", "list", "(", "hidden_sizes", ")", ",", "activation", ",", "activation", ")", "\n", "self", ".", "mu_layer", "=", "nn", ".", "Linear", "(", "hidden_sizes", "[", "-", "1", "]", ",", "act_dim", ")", "\n", "self", ".", "log_std_layer", "=", "nn", ".", "Linear", "(", "hidden_sizes", "[", "-", "1", "]", ",", "act_dim", ")", "\n", "self", ".", "act_limit", "=", "act_limit", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.SquashedGaussianMLPActor.forward": [[41, 71], ["network_utils.SquashedGaussianMLPActor.net", "network_utils.SquashedGaussianMLPActor.mu_layer", "network_utils.SquashedGaussianMLPActor.log_std_layer", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob", "torch.softplus", "torch.softplus", "torch.softplus", "numpy.log"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ",", "deterministic", "=", "False", ",", "with_logprob", "=", "True", ")", ":", "\n", "        ", "net_out", "=", "self", ".", "net", "(", "obs", ")", "\n", "mu", "=", "self", ".", "mu_layer", "(", "net_out", ")", "\n", "log_std", "=", "self", ".", "log_std_layer", "(", "net_out", ")", "\n", "log_std", "=", "torch", ".", "clamp", "(", "log_std", ",", "LOG_STD_MIN", ",", "LOG_STD_MAX", ")", "\n", "std", "=", "torch", ".", "exp", "(", "log_std", ")", "\n", "\n", "# Pre-squash distribution and sample", "\n", "pi_distribution", "=", "Normal", "(", "mu", ",", "std", ")", "\n", "if", "deterministic", ":", "\n", "# Only used for evaluating policy at test time.", "\n", "            ", "pi_action", "=", "mu", "\n", "", "else", ":", "\n", "            ", "pi_action", "=", "pi_distribution", ".", "rsample", "(", ")", "\n", "\n", "", "if", "with_logprob", ":", "\n", "# Compute logprob from Gaussian, and then apply correction for Tanh squashing.", "\n", "# NOTE: The correction formula is a little bit magic. To get an understanding ", "\n", "# of where it comes from, check out the original SAC paper (arXiv 1801.01290) ", "\n", "# and look in appendix C. This is a more numerically-stable equivalent to Eq 21.", "\n", "# Try deriving it yourself as a (very difficult) exercise. :)", "\n", "            ", "logp_pi", "=", "pi_distribution", ".", "log_prob", "(", "pi_action", ")", ".", "sum", "(", "axis", "=", "-", "1", ")", "\n", "logp_pi", "-=", "(", "2", "*", "(", "np", ".", "log", "(", "2", ")", "-", "pi_action", "-", "F", ".", "softplus", "(", "-", "2", "*", "pi_action", ")", ")", ")", ".", "sum", "(", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "logp_pi", "=", "None", "\n", "\n", "", "pi_action", "=", "torch", ".", "tanh", "(", "pi_action", ")", "\n", "pi_action", "=", "self", ".", "act_limit", "*", "pi_action", "\n", "\n", "return", "pi_action", ",", "logp_pi", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.MLPQFunction.__init__": [[75, 78], ["torch.Module.__init__", "network_utils.mlp", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "q", "=", "mlp", "(", "[", "obs_dim", "+", "act_dim", "]", "+", "list", "(", "hidden_sizes", ")", "+", "[", "1", "]", ",", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.MLPQFunction.forward": [[79, 84], ["len", "network_utils.MLPQFunction.q", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ",", "act", ")", ":", "\n", "        ", "N", "=", "len", "(", "obs", ")", "\n", "all_inputs", "=", "[", "o", "for", "o", "in", "obs", "]", "+", "[", "a", "for", "a", "in", "act", "]", "\n", "q", "=", "self", ".", "q", "(", "torch", ".", "cat", "(", "all_inputs", ",", "dim", "=", "-", "1", ")", ")", "\n", "return", "torch", ".", "squeeze", "(", "q", ",", "-", "1", ")", "# Critical to ensure q has right shape.", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.MLPActorCritic.__init__": [[88, 109], ["torch.Module.__init__", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "len", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "network_utils.MLPQFunction", "network_utils.MLPQFunction", "network_utils.SquashedGaussianMLPActor", "network_utils.MLPActorCritic.pis.append"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["    ", "def", "__init__", "(", "self", ",", "observation_spaces", ",", "action_spaces", ",", "hidden_sizes", "=", "(", "256", ",", "256", ")", ",", "\n", "activation", "=", "nn", ".", "ReLU", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "os", ".", "environ", "[", "\"OMP_NUM_THREADS\"", "]", "=", "\"1\"", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "N", "=", "len", "(", "observation_spaces", ")", "\n", "obs_dim", "=", "observation_spaces", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "act_dim", "=", "action_spaces", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "act_limit", "=", "action_spaces", "[", "0", "]", ".", "high", "[", "0", "]", "\n", "\n", "# build policy and value functions", "\n", "self", ".", "pis", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "pi", "=", "SquashedGaussianMLPActor", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ",", "act_limit", ")", "\n", "self", ".", "pis", ".", "append", "(", "pi", ")", "\n", "", "self", ".", "pis", "=", "nn", ".", "ModuleList", "(", "self", ".", "pis", ")", "\n", "\n", "self", ".", "q1", "=", "MLPQFunction", "(", "obs_dim", "*", "N", ",", "act_dim", "*", "N", ",", "hidden_sizes", ",", "activation", ")", "\n", "self", ".", "q2", "=", "MLPQFunction", "(", "obs_dim", "*", "N", ",", "act_dim", "*", "N", ",", "hidden_sizes", ",", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.MLPActorCritic.act": [[111, 120], ["range", "len", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "actions.append", "a.numpy"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy"], ["", "def", "act", "(", "self", ",", "obs", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "actions", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "obs", ")", ")", ":", "\n", "\n", "            ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "a", ",", "_", "=", "self", ".", "pis", "[", "i", "]", "(", "obs", "[", "i", "]", ",", "deterministic", ",", "False", ")", "\n", "actions", ".", "append", "(", "a", ".", "numpy", "(", ")", ")", "\n", "\n", "", "", "return", "actions", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.MLPActorCritic.current_pi": [[121, 130], ["range", "len", "actions.append", "logprobs.append"], "methods", ["None"], ["", "def", "current_pi", "(", "self", ",", "obs", ")", ":", "\n", "        ", "actions", "=", "[", "]", "\n", "logprobs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "obs", ")", ")", ":", "\n", "            ", "a", ",", "lp", "=", "self", ".", "pis", "[", "i", "]", "(", "obs", "[", "i", "]", ")", "\n", "actions", ".", "append", "(", "a", ")", "\n", "logprobs", ".", "append", "(", "lp", ")", "\n", "\n", "", "return", "actions", ",", "logprobs", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.combined_shape": [[13, 17], ["numpy.isscalar"], "function", ["None"], ["def", "combined_shape", "(", "length", ",", "shape", "=", "None", ")", ":", "\n", "    ", "if", "shape", "is", "None", ":", "\n", "        ", "return", "(", "length", ",", ")", "\n", "", "return", "(", "length", ",", "shape", ")", "if", "np", ".", "isscalar", "(", "shape", ")", "else", "(", "length", ",", "*", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.mlp": [[18, 24], ["range", "torch.Sequential", "len", "torch.Linear", "act", "len"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPActorCritic.act"], ["", "def", "mlp", "(", "sizes", ",", "activation", ",", "output_activation", "=", "nn", ".", "Identity", ")", ":", "\n", "    ", "layers", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "sizes", ")", "-", "1", ")", ":", "\n", "        ", "act", "=", "activation", "if", "j", "<", "len", "(", "sizes", ")", "-", "2", "else", "output_activation", "\n", "layers", "+=", "[", "nn", ".", "Linear", "(", "sizes", "[", "j", "]", ",", "sizes", "[", "j", "+", "1", "]", ")", ",", "act", "(", ")", "]", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.count_vars": [[25, 27], ["sum", "numpy.prod", "module.parameters"], "function", ["None"], ["", "def", "count_vars", "(", "module", ")", ":", "\n", "    ", "return", "sum", "(", "[", "np", ".", "prod", "(", "p", ".", "shape", ")", "for", "p", "in", "module", ".", "parameters", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.masac.ReplayBuffer.__init__": [[20, 32], ["range", "numpy.zeros", "numpy.zeros", "masac.ReplayBuffer.obs_buf.append", "masac.ReplayBuffer.obs2_buf.append", "masac.ReplayBuffer.act_buf.append", "numpy.zeros", "numpy.zeros", "numpy.zeros", "arena5.combined_shape", "arena5.combined_shape", "arena5.combined_shape"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape"], ["def", "__init__", "(", "self", ",", "N", ",", "obs_dim", ",", "act_dim", ",", "size", ")", ":", "\n", "        ", "self", ".", "obs_buf", "=", "[", "]", "\n", "self", ".", "obs2_buf", "=", "[", "]", "\n", "self", ".", "act_buf", "=", "[", "]", "\n", "self", ".", "N", "=", "N", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "self", ".", "obs_buf", ".", "append", "(", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "obs_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "self", ".", "obs2_buf", ".", "append", "(", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "obs_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "self", ".", "act_buf", ".", "append", "(", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "act_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "", "self", ".", "rew_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "ptr", ",", "self", ".", "size", ",", "self", ".", "max_size", "=", "0", ",", "0", ",", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.masac.ReplayBuffer.store": [[33, 42], ["range", "min"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "obs", ",", "act", ",", "rew", ",", "next_obs", ",", "done", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "            ", "self", ".", "obs_buf", "[", "i", "]", "[", "self", ".", "ptr", "]", "=", "obs", "[", "i", "]", "\n", "self", ".", "obs2_buf", "[", "i", "]", "[", "self", ".", "ptr", "]", "=", "next_obs", "[", "i", "]", "\n", "self", ".", "act_buf", "[", "i", "]", "[", "self", ".", "ptr", "]", "=", "act", "[", "i", "]", "\n", "", "self", ".", "rew_buf", "[", "self", ".", "ptr", "]", "=", "rew", "\n", "self", ".", "done_buf", "[", "self", ".", "ptr", "]", "=", "done", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.masac.ReplayBuffer.sample_batch": [[43, 54], ["numpy.random.randint", "dict", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "range", "range", "range"], "methods", ["None"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", "=", "32", ")", ":", "\n", "        ", "idxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "batch", "=", "dict", "(", "\n", "obs", "=", "[", "torch", ".", "as_tensor", "(", "self", ".", "obs_buf", "[", "i", "]", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "for", "i", "in", "range", "(", "self", ".", "N", ")", "]", ",", "\n", "obs2", "=", "[", "torch", ".", "as_tensor", "(", "self", ".", "obs2_buf", "[", "i", "]", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "for", "i", "in", "range", "(", "self", ".", "N", ")", "]", ",", "\n", "act", "=", "[", "torch", ".", "as_tensor", "(", "self", ".", "act_buf", "[", "i", "]", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "for", "i", "in", "range", "(", "self", ".", "N", ")", "]", ",", "\n", "rew", "=", "torch", ".", "as_tensor", "(", "self", ".", "rew_buf", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", ",", "\n", "done", "=", "torch", ".", "as_tensor", "(", "self", ".", "done_buf", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", ")", "\n", "\n", "return", "batch", "#{k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.masac.masac": [[57, 369], ["dict", "int", "dict", "torch.set_num_threads", "torch.manual_seed", "numpy.random.seed", "len", "actor_critic", "copy.deepcopy", "arena5.core.mpi_pytorch_utils.sync_weights", "arena5.core.mpi_pytorch_utils.sync_weights", "copy.deepcopy.parameters", "itertools.chain", "masac.ReplayBuffer", "torch.optim.Adam", "time.time", "range", "env_fn", "env_fn", "os.path.exists", "os.path.exists", "actor_critic.parameters", "copy.deepcopy.parameters", "actor_critic.q1.parameters", "actor_critic.q2.parameters", "actor_critic.q1", "actor_critic.q2", "dict", "actor_critic.current_pi", "actor_critic.q1", "actor_critic.q2", "torch.min", "sum", "actor_critic.current_pi", "actor_critic.q1", "actor_critic.q2", "torch.min", "torch.optim.Adam", "torch.optim.Adam.zero_grad", "masac.masac.compute_loss_q"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.MLPActorCritic.current_pi", "home.repos.pwc.inspect_result.cgrivera_ai-arena.masac.network_utils.MLPActorCritic.current_pi"], ["", "", "def", "masac", "(", "env_fn", ",", "comm", ",", "data_dir", ",", "policy_record", "=", "None", ",", "eval_mode", "=", "False", ",", "\n", "actor_critic", "=", "core", ".", "MLPActorCritic", ",", "ac_kwargs", "=", "dict", "(", ")", ",", "seed", "=", "0", ",", "\n", "steps_per_epoch", "=", "4000", ",", "epochs", "=", "100", ",", "replay_size", "=", "int", "(", "1e6", ")", ",", "gamma", "=", "0.99", ",", "\n", "polyak", "=", "0.995", ",", "lr", "=", "1e-3", ",", "alpha", "=", "0.2", ",", "batch_size", "=", "100", ",", "start_steps", "=", "10000", ",", "\n", "update_after", "=", "1000", ",", "update_every", "=", "50", ",", "num_test_episodes", "=", "10", ",", "max_ep_len", "=", "1000", ",", "\n", "logger_kwargs", "=", "dict", "(", ")", ",", "save_freq", "=", "1", ")", ":", "\n", "\n", "\n", "    ", "\"\"\"\n    Soft Actor-Critic (SAC)\n\n\n    Args:\n        env_fn : A function which creates a copy of the environment.\n            The environment must satisfy the OpenAI Gym API.\n\n        actor_critic: The constructor method for a PyTorch Module with an ``act`` \n            method, a ``pi`` module, a ``q1`` module, and a ``q2`` module.\n            The ``act`` method and ``pi`` module should accept batches of \n            observations as inputs, and ``q1`` and ``q2`` should accept a batch \n            of observations and a batch of actions as inputs. When called, \n            ``act``, ``q1``, and ``q2`` should return:\n\n            ===========  ================  ======================================\n            Call         Output Shape      Description\n            ===========  ================  ======================================\n            ``act``      (batch, act_dim)  | Numpy array of actions for each \n                                           | observation.\n            ``q1``       (batch,)          | Tensor containing one current estimate\n                                           | of Q* for the provided observations\n                                           | and actions. (Critical: make sure to\n                                           | flatten this!)\n            ``q2``       (batch,)          | Tensor containing the other current \n                                           | estimate of Q* for the provided observations\n                                           | and actions. (Critical: make sure to\n                                           | flatten this!)\n            ===========  ================  ======================================\n\n            Calling ``pi`` should return:\n\n            ===========  ================  ======================================\n            Symbol       Shape             Description\n            ===========  ================  ======================================\n            ``a``        (batch, act_dim)  | Tensor containing actions from policy\n                                           | given observations.\n            ``logp_pi``  (batch,)          | Tensor containing log probabilities of\n                                           | actions in ``a``. Importantly: gradients\n                                           | should be able to flow back into ``a``.\n            ===========  ================  ======================================\n\n        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object \n            you provided to SAC.\n\n        seed (int): Seed for random number generators.\n\n        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n            for the agent and the environment in each epoch.\n\n        epochs (int): Number of epochs to run and train agent.\n\n        replay_size (int): Maximum length of replay buffer.\n\n        gamma (float): Discount factor. (Always between 0 and 1.)\n\n        polyak (float): Interpolation factor in polyak averaging for target \n            networks. Target networks are updated towards main networks \n            according to:\n\n            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n\n            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n            close to 1.)\n\n        lr (float): Learning rate (used for both policy and value learning).\n\n        alpha (float): Entropy regularization coefficient. (Equivalent to \n            inverse of reward scale in the original SAC paper.)\n\n        batch_size (int): Minibatch size for SGD.\n\n        start_steps (int): Number of steps for uniform-random action selection,\n            before running real policy. Helps exploration.\n\n        update_after (int): Number of env interactions to collect before\n            starting to do gradient descent updates. Ensures replay buffer\n            is full enough for useful updates.\n\n        update_every (int): Number of env interactions that should elapse\n            between gradient descent updates. Note: Regardless of how long \n            you wait between updates, the ratio of env steps to gradient steps \n            is locked to 1.\n\n        num_test_episodes (int): Number of episodes to test the deterministic\n            policy at the end of each epoch.\n\n        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n\n        logger_kwargs (dict): Keyword args for EpochLogger.\n\n        save_freq (int): How often (in terms of gap between epochs) to save\n            the current policy and value function.\n\n    \"\"\"", "\n", "\n", "os", ".", "environ", "[", "\"OMP_NUM_THREADS\"", "]", "=", "\"1\"", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "env", ",", "test_env", "=", "env_fn", "(", ")", ",", "env_fn", "(", ")", "\n", "N", "=", "len", "(", "env", ".", "observation_spaces", ")", "\n", "obs_dim", "=", "env", ".", "observation_spaces", "[", "0", "]", ".", "shape", "\n", "act_dim", "=", "env", ".", "action_spaces", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "\n", "# Action limit for clamping: critically, assumes all dimensions share the same bound!", "\n", "act_limit", "=", "env", ".", "action_spaces", "[", "0", "]", ".", "high", "[", "0", "]", "\n", "\n", "# Create actor-critic module and target networks", "\n", "ac", "=", "actor_critic", "(", "env", ".", "observation_spaces", ",", "env", ".", "action_spaces", ",", "**", "ac_kwargs", ")", "\n", "ac_targ", "=", "deepcopy", "(", "ac", ")", "\n", "\n", "# if records exist, load weights", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"ac.pt\"", ")", ":", "\n", "            ", "ac", ".", "load_state_dict", "(", "torch", ".", "load", "(", "data_dir", "+", "\"ac.pt\"", ")", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"ac_targ.pt\"", ")", ":", "\n", "            ", "ac_targ", ".", "load_state_dict", "(", "torch", ".", "load", "(", "data_dir", "+", "\"ac_targ.pt\"", ")", ")", "\n", "\n", "# initial weight sync", "\n", "", "", "sync_weights", "(", "comm", ",", "ac", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac_targ", ".", "parameters", "(", ")", ")", "\n", "\n", "# Freeze target networks with respect to optimizers (only update via polyak averaging)", "\n", "for", "p", "in", "ac_targ", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "# List of parameters for both Q-networks (save this for convenience)", "\n", "", "q_params", "=", "itertools", ".", "chain", "(", "ac", ".", "q1", ".", "parameters", "(", ")", ",", "ac", ".", "q2", ".", "parameters", "(", ")", ")", "\n", "\n", "# Experience buffer", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "N", ",", "obs_dim", "=", "obs_dim", ",", "act_dim", "=", "act_dim", ",", "size", "=", "replay_size", ")", "\n", "\n", "# Set up function for computing SAC Q-losses", "\n", "def", "compute_loss_q", "(", "data", ")", ":", "\n", "        ", "o", ",", "a", ",", "r", ",", "o2", ",", "d", "=", "data", "[", "'obs'", "]", ",", "data", "[", "'act'", "]", ",", "data", "[", "'rew'", "]", ",", "data", "[", "'obs2'", "]", ",", "data", "[", "'done'", "]", "\n", "\n", "q1", "=", "ac", ".", "q1", "(", "o", ",", "a", ")", "\n", "q2", "=", "ac", ".", "q2", "(", "o", ",", "a", ")", "\n", "\n", "# Bellman backup for Q functions", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Target actions come from *current* policy", "\n", "            ", "a2", ",", "logp_a2", "=", "ac", ".", "current_pi", "(", "o2", ")", "\n", "total_action_logprob", "=", "sum", "(", "logp_a2", ")", "\n", "\n", "# Target Q-values", "\n", "q1_pi_targ", "=", "ac_targ", ".", "q1", "(", "o2", ",", "a2", ")", "\n", "q2_pi_targ", "=", "ac_targ", ".", "q2", "(", "o2", ",", "a2", ")", "\n", "q_pi_targ", "=", "torch", ".", "min", "(", "q1_pi_targ", ",", "q2_pi_targ", ")", "\n", "backup", "=", "r", "+", "gamma", "*", "(", "1", "-", "d", ")", "*", "(", "q_pi_targ", "-", "alpha", "*", "total_action_logprob", ")", "\n", "\n", "# MSE loss against Bellman backup", "\n", "", "loss_q1", "=", "(", "(", "q1", "-", "backup", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "loss_q2", "=", "(", "(", "q2", "-", "backup", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "loss_q", "=", "loss_q1", "+", "loss_q2", "\n", "\n", "# Useful info for logging", "\n", "q_info", "=", "dict", "(", "Q1Vals", "=", "q1", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "Q2Vals", "=", "q2", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "return", "loss_q", ",", "q_info", "\n", "\n", "# Set up function for computing SAC pi loss", "\n", "", "def", "compute_losses_pi", "(", "data", ")", ":", "\n", "        ", "o", "=", "data", "[", "'obs'", "]", "\n", "pi", ",", "logp_pi", "=", "ac", ".", "current_pi", "(", "o", ")", "\n", "q1_pi", "=", "ac", ".", "q1", "(", "o", ",", "pi", ")", "\n", "q2_pi", "=", "ac", ".", "q2", "(", "o", ",", "pi", ")", "\n", "q_pi", "=", "torch", ".", "min", "(", "q1_pi", ",", "q2_pi", ")", "\n", "\n", "# Entropy-regularized policy loss", "\n", "losses_pi", "=", "[", "(", "alpha", "*", "logp_pi", "[", "i", "]", "-", "q_pi", ")", ".", "mean", "(", ")", "for", "i", "in", "range", "(", "N", ")", "]", "\n", "loss_pi", "=", "sum", "(", "losses_pi", ")", "\n", "\n", "return", "loss_pi", "\n", "\n", "", "def", "compute_losses_pi_ddpg", "(", "data", ")", ":", "\n", "        ", "o", "=", "data", "[", "'obs'", "]", "\n", "pi", ",", "logp_pi", "=", "ac", ".", "current_pi", "(", "o", ")", "\n", "q1_pi", "=", "ac", ".", "q1", "(", "o", ",", "pi", ")", "\n", "q2_pi", "=", "ac", ".", "q2", "(", "o", ",", "pi", ")", "\n", "q_pi", "=", "torch", ".", "min", "(", "q1_pi", ",", "q2_pi", ")", "\n", "losses_pi", "=", "(", "-", "q_pi", ")", ".", "mean", "(", ")", "\n", "return", "losses_pi", "\n", "\n", "# Set up optimizers for policy and q-function", "\n", "", "pi_optimizers", "=", "[", "Adam", "(", "ac", ".", "pis", "[", "i", "]", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "for", "i", "in", "range", "(", "N", ")", "]", "\n", "q_optimizer", "=", "Adam", "(", "q_params", ",", "lr", "=", "lr", ")", "\n", "\n", "\n", "def", "update", "(", "data", ")", ":", "\n", "\n", "# First run one gradient descent step for Q1 and Q2", "\n", "        ", "q_optimizer", ".", "zero_grad", "(", ")", "\n", "loss_q", ",", "q_info", "=", "compute_loss_q", "(", "data", ")", "\n", "loss_q", ".", "backward", "(", ")", "\n", "sync_grads", "(", "comm", ",", "q_params", ")", "\n", "q_optimizer", ".", "step", "(", ")", "\n", "\n", "# Freeze Q-networks so you don't waste computational effort ", "\n", "# computing gradients for them during the policy learning step.", "\n", "for", "p", "in", "q_params", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "# Next run one gradient descent step for each pi.", "\n", "", "[", "opt", ".", "zero_grad", "(", ")", "for", "opt", "in", "pi_optimizers", "]", "\n", "loss_pi", "=", "compute_losses_pi", "(", "data", ")", "\n", "loss_pi", ".", "backward", "(", ")", "\n", "[", "sync_grads", "(", "comm", ",", "ac", ".", "pis", "[", "i", "]", ".", "parameters", "(", ")", ")", "for", "i", "in", "range", "(", "N", ")", "]", "\n", "[", "opt", ".", "step", "(", ")", "for", "opt", "in", "pi_optimizers", "]", "\n", "\n", "# Unfreeze Q-networks so you can optimize it at next DDPG step.", "\n", "for", "p", "in", "q_params", ":", "\n", "            ", "p", ".", "requires_grad", "=", "True", "\n", "\n", "\n", "# Finally, update target networks by polyak averaging.", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "p", ",", "p_targ", "in", "zip", "(", "ac", ".", "parameters", "(", ")", ",", "ac_targ", ".", "parameters", "(", ")", ")", ":", "\n", "# NB: We use an in-place operations \"mul_\", \"add_\" to update target", "\n", "# params, as opposed to \"mul\" and \"add\", which would make new tensors.", "\n", "                ", "p_targ", ".", "data", ".", "mul_", "(", "polyak", ")", "\n", "p_targ", ".", "data", ".", "add_", "(", "(", "1", "-", "polyak", ")", "*", "p", ".", "data", ")", "\n", "\n", "\n", "# sync weights", "\n", "", "", "sync_weights", "(", "comm", ",", "q_params", ")", "\n", "[", "sync_weights", "(", "comm", ",", "pi", ".", "parameters", "(", ")", ")", "for", "pi", "in", "ac", ".", "pis", "]", "\n", "sync_weights", "(", "comm", ",", "ac_targ", ".", "parameters", "(", ")", ")", "\n", "\n", "# save weights", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "            ", "torch", ".", "save", "(", "ac", ".", "state_dict", "(", ")", ",", "data_dir", "+", "\"ac.pt\"", ")", "\n", "torch", ".", "save", "(", "ac_targ", ".", "state_dict", "(", ")", ",", "data_dir", "+", "\"ac_targ.pt\"", ")", "\n", "\n", "\n", "", "", "def", "get_action", "(", "o", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "return", "ac", ".", "act", "(", "torch", ".", "as_tensor", "(", "o", ",", "dtype", "=", "torch", ".", "float32", ")", ",", "\n", "deterministic", ")", "\n", "\n", "# def test_agent():", "\n", "#     for j in range(num_test_episodes):", "\n", "#         o, d, ep_ret, ep_len = test_env.reset(), False, 0, 0", "\n", "#         while not(d or (ep_len == max_ep_len)):", "\n", "#             # Take deterministic actions at test time ", "\n", "#             o, r, d, _ = test_env.step(get_action(o, True))", "\n", "#             ep_ret += r", "\n", "#             ep_len += 1", "\n", "\n", "# Prepare for interaction with environment", "\n", "", "total_steps", "=", "steps_per_epoch", "*", "epochs", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "o", ",", "ep_ret", ",", "ep_len", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", "\n", "\n", "# Main loop: collect experience in env and update/log each epoch", "\n", "for", "t", "in", "range", "(", "total_steps", ")", ":", "\n", "\n", "# Until start_steps have elapsed, randomly sample actions", "\n", "# from a uniform distribution for better exploration. Afterwards, ", "\n", "# use the learned policy. ", "\n", "        ", "if", "eval_mode", ":", "\n", "            ", "a", "=", "get_action", "(", "o", ",", "True", ")", "\n", "", "else", ":", "\n", "            ", "if", "t", ">", "start_steps", ":", "\n", "                ", "a", "=", "get_action", "(", "o", ")", "\n", "", "else", ":", "\n", "                ", "a", "=", "[", "asp", ".", "sample", "(", ")", "for", "asp", "in", "env", ".", "action_spaces", "]", "\n", "\n", "# Step the env", "\n", "", "", "o2", ",", "rs", ",", "d", ",", "_", "=", "env", ".", "step", "(", "a", ")", "\n", "r", "=", "sum", "(", "rs", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "\n", "# Ignore the \"done\" signal if it comes from hitting the time", "\n", "# horizon (that is, when it's an artificial terminal signal", "\n", "# that isn't based on the agent's state)", "\n", "d", "=", "False", "if", "ep_len", "==", "max_ep_len", "else", "d", "\n", "\n", "# Store experience to replay buffer", "\n", "replay_buffer", ".", "store", "(", "o", ",", "a", ",", "r", ",", "o2", ",", "d", ")", "\n", "\n", "# Super critical, easy to overlook step: make sure to update ", "\n", "# most recent observation!", "\n", "o", "=", "o2", "\n", "\n", "# End of trajectory handling", "\n", "if", "d", ":", "\n", "            ", "o", ",", "ep_ret", ",", "ep_len", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", "\n", "\n", "# Update handling (training only)", "\n", "", "if", "not", "eval_mode", ":", "\n", "            ", "if", "t", ">=", "update_after", "and", "t", "%", "update_every", "==", "0", ":", "\n", "                ", "for", "j", "in", "range", "(", "update_every", ")", ":", "\n", "                    ", "batch", "=", "replay_buffer", ".", "sample_batch", "(", "batch_size", ")", "\n", "update", "(", "data", "=", "batch", ")", "\n", "\n", "# End of epoch handling", "\n", "", "", "", "if", "(", "t", "+", "1", ")", "%", "steps_per_epoch", "==", "0", ":", "\n", "            ", "epoch", "=", "(", "t", "+", "1", ")", "//", "steps_per_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_policy.SACPolicy.__init__": [[10, 15], ["int"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ",", "eval_mode", "=", "False", ",", "worker_replay_size", "=", "int", "(", "1e5", ")", ")", ":", "\n", "        ", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "self", ".", "eval_mode", "=", "eval_mode", "\n", "self", ".", "worker_replay_size", "=", "worker_replay_size", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_policy.SACPolicy.run": [[17, 32], ["arena5.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater", "int", "print", "print", "arena5.algos.sac.sac.sac", "len", "sac_policy.SACPolicy.comm.Get_size", "dict"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac.sac"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "env", "=", "MPISynchronizedPRUpdater", "(", "self", ".", "env", ",", "self", ".", "comm", ",", "policy_record", ")", "\n", "local_steps", "=", "int", "(", "num_steps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "\n", "print", "(", "self", ".", "env", ".", "action_space", ",", "self", ".", "env", ".", "observation_space", ")", "\n", "policy", "=", "core", ".", "MLPActorCritic", "\n", "print", "(", "'observation space shape'", ",", "self", ".", "env", ".", "observation_space", ".", "shape", ")", "\n", "if", "len", "(", "self", ".", "env", ".", "observation_space", ".", "shape", ")", ">", "1", ":", "\n", "            ", "policy", "=", "core", ".", "ConvActorCritic", "\n", "", "sac", "(", "lambda", ":", "self", ".", "env", ",", "self", ".", "comm", ",", "data_dir", ",", "policy_record", ",", "self", ".", "eval_mode", ",", "\n", "actor_critic", "=", "policy", ",", "\n", "ac_kwargs", "=", "dict", "(", "hidden_sizes", "=", "[", "256", ",", "256", "]", ")", ",", "\n", "gamma", "=", "0.99", ",", "seed", "=", "1337", ",", "steps_per_epoch", "=", "local_steps", ",", "epochs", "=", "1", ",", "\n", "replay_size", "=", "self", ".", "worker_replay_size", ",", "logger_kwargs", "=", "None", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac.ReplayBuffer.__init__": [[20, 27], ["numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "numpy.zeros", "arena5.combined_shape", "arena5.combined_shape", "arena5.combined_shape"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape"], ["def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "size", ")", ":", "\n", "        ", "self", ".", "obs_buf", "=", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "obs_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "obs2_buf", "=", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "obs_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "act_buf", "=", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "act_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "rew_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "ptr", ",", "self", ".", "size", ",", "self", ".", "max_size", "=", "0", ",", "0", ",", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac.ReplayBuffer.store": [[28, 36], ["min"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "obs", ",", "act", ",", "rew", ",", "next_obs", ",", "done", ")", ":", "\n", "        ", "self", ".", "obs_buf", "[", "self", ".", "ptr", "]", "=", "obs", "\n", "self", ".", "obs2_buf", "[", "self", ".", "ptr", "]", "=", "next_obs", "\n", "self", ".", "act_buf", "[", "self", ".", "ptr", "]", "=", "act", "\n", "self", ".", "rew_buf", "[", "self", ".", "ptr", "]", "=", "rew", "\n", "self", ".", "done_buf", "[", "self", ".", "ptr", "]", "=", "done", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac.ReplayBuffer.sample_batch": [[37, 45], ["numpy.random.randint", "dict", "torch.as_tensor", "dict.items"], "methods", ["None"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", "=", "32", ")", ":", "\n", "        ", "idxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "batch", "=", "dict", "(", "obs", "=", "self", ".", "obs_buf", "[", "idxs", "]", ",", "\n", "obs2", "=", "self", ".", "obs2_buf", "[", "idxs", "]", ",", "\n", "act", "=", "self", ".", "act_buf", "[", "idxs", "]", ",", "\n", "rew", "=", "self", ".", "rew_buf", "[", "idxs", "]", ",", "\n", "done", "=", "self", ".", "done_buf", "[", "idxs", "]", ")", "\n", "return", "{", "k", ":", "torch", ".", "as_tensor", "(", "v", ",", "dtype", "=", "torch", ".", "float32", ")", "for", "k", ",", "v", "in", "batch", ".", "items", "(", ")", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac.sac": [[48, 355], ["dict", "int", "dict", "torch.set_num_threads", "torch.manual_seed", "numpy.random.seed", "actor_critic", "copy.deepcopy", "arena5.core.mpi_pytorch_utils.sync_weights", "arena5.core.mpi_pytorch_utils.sync_weights", "arena5.core.mpi_pytorch_utils.sync_weights", "arena5.core.mpi_pytorch_utils.sync_weights", "copy.deepcopy.parameters", "itertools.chain", "sac.ReplayBuffer", "tuple", "torch.optim.Adam", "torch.optim.Adam", "time.time", "range", "env_fn", "env_fn", "os.path.exists", "os.path.exists", "actor_critic.q1.parameters", "actor_critic.q2.parameters", "actor_critic.pi.parameters", "copy.deepcopy.parameters", "actor_critic.q1.parameters", "actor_critic.q2.parameters", "actor_critic.q1", "actor_critic.q2", "dict", "actor_critic.pi", "actor_critic.q1", "actor_critic.q2", "torch.min", "dict", "actor_critic.pi.parameters", "torch.optim.Adam.zero_grad", "sac.sac.compute_loss_q"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights"], ["", "", "def", "sac", "(", "env_fn", ",", "comm", ",", "data_dir", ",", "policy_record", "=", "None", ",", "eval_mode", "=", "False", ",", "\n", "actor_critic", "=", "core", ".", "MLPActorCritic", ",", "ac_kwargs", "=", "dict", "(", ")", ",", "seed", "=", "0", ",", "\n", "steps_per_epoch", "=", "4000", ",", "epochs", "=", "100", ",", "replay_size", "=", "int", "(", "1e6", ")", ",", "gamma", "=", "0.99", ",", "\n", "polyak", "=", "0.995", ",", "lr", "=", "1e-3", ",", "alpha", "=", "0.2", ",", "batch_size", "=", "100", ",", "start_steps", "=", "10000", ",", "\n", "update_after", "=", "1000", ",", "update_every", "=", "50", ",", "num_test_episodes", "=", "10", ",", "max_ep_len", "=", "1000", ",", "\n", "logger_kwargs", "=", "dict", "(", ")", ",", "save_freq", "=", "1", ")", ":", "\n", "\n", "\n", "    ", "\"\"\"\n    Soft Actor-Critic (SAC)\n\n\n    Args:\n        env_fn : A function which creates a copy of the environment.\n            The environment must satisfy the OpenAI Gym API.\n\n        actor_critic: The constructor method for a PyTorch Module with an ``act`` \n            method, a ``pi`` module, a ``q1`` module, and a ``q2`` module.\n            The ``act`` method and ``pi`` module should accept batches of \n            observations as inputs, and ``q1`` and ``q2`` should accept a batch \n            of observations and a batch of actions as inputs. When called, \n            ``act``, ``q1``, and ``q2`` should return:\n\n            ===========  ================  ======================================\n            Call         Output Shape      Description\n            ===========  ================  ======================================\n            ``act``      (batch, act_dim)  | Numpy array of actions for each \n                                           | observation.\n            ``q1``       (batch,)          | Tensor containing one current estimate\n                                           | of Q* for the provided observations\n                                           | and actions. (Critical: make sure to\n                                           | flatten this!)\n            ``q2``       (batch,)          | Tensor containing the other current \n                                           | estimate of Q* for the provided observations\n                                           | and actions. (Critical: make sure to\n                                           | flatten this!)\n            ===========  ================  ======================================\n\n            Calling ``pi`` should return:\n\n            ===========  ================  ======================================\n            Symbol       Shape             Description\n            ===========  ================  ======================================\n            ``a``        (batch, act_dim)  | Tensor containing actions from policy\n                                           | given observations.\n            ``logp_pi``  (batch,)          | Tensor containing log probabilities of\n                                           | actions in ``a``. Importantly: gradients\n                                           | should be able to flow back into ``a``.\n            ===========  ================  ======================================\n\n        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object \n            you provided to SAC.\n\n        seed (int): Seed for random number generators.\n\n        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n            for the agent and the environment in each epoch.\n\n        epochs (int): Number of epochs to run and train agent.\n\n        replay_size (int): Maximum length of replay buffer.\n\n        gamma (float): Discount factor. (Always between 0 and 1.)\n\n        polyak (float): Interpolation factor in polyak averaging for target \n            networks. Target networks are updated towards main networks \n            according to:\n\n            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n\n            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n            close to 1.)\n\n        lr (float): Learning rate (used for both policy and value learning).\n\n        alpha (float): Entropy regularization coefficient. (Equivalent to \n            inverse of reward scale in the original SAC paper.)\n\n        batch_size (int): Minibatch size for SGD.\n\n        start_steps (int): Number of steps for uniform-random action selection,\n            before running real policy. Helps exploration.\n\n        update_after (int): Number of env interactions to collect before\n            starting to do gradient descent updates. Ensures replay buffer\n            is full enough for useful updates.\n\n        update_every (int): Number of env interactions that should elapse\n            between gradient descent updates. Note: Regardless of how long \n            you wait between updates, the ratio of env steps to gradient steps \n            is locked to 1.\n\n        num_test_episodes (int): Number of episodes to test the deterministic\n            policy at the end of each epoch.\n\n        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n\n        logger_kwargs (dict): Keyword args for EpochLogger.\n\n        save_freq (int): How often (in terms of gap between epochs) to save\n            the current policy and value function.\n\n    \"\"\"", "\n", "\n", "os", ".", "environ", "[", "\"OMP_NUM_THREADS\"", "]", "=", "\"1\"", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "env", ",", "test_env", "=", "env_fn", "(", ")", ",", "env_fn", "(", ")", "\n", "obs_dim", "=", "env", ".", "observation_space", ".", "shape", "\n", "act_dim", "=", "env", ".", "action_space", ".", "shape", "[", "0", "]", "\n", "\n", "# Action limit for clamping: critically, assumes all dimensions share the same bound!", "\n", "act_limit", "=", "env", ".", "action_space", ".", "high", "[", "0", "]", "\n", "\n", "# Create actor-critic module and target networks", "\n", "ac", "=", "actor_critic", "(", "env", ".", "observation_space", ",", "env", ".", "action_space", ",", "**", "ac_kwargs", ")", "\n", "ac_targ", "=", "deepcopy", "(", "ac", ")", "\n", "\n", "# if records exist, load weights", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"ac.pt\"", ")", ":", "\n", "            ", "ac", ".", "load_state_dict", "(", "torch", ".", "load", "(", "data_dir", "+", "\"ac.pt\"", ")", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"ac_targ.pt\"", ")", ":", "\n", "            ", "ac_targ", ".", "load_state_dict", "(", "torch", ".", "load", "(", "data_dir", "+", "\"ac_targ.pt\"", ")", ")", "\n", "\n", "# initial weight sync", "\n", "", "", "sync_weights", "(", "comm", ",", "ac", ".", "q1", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac", ".", "q2", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac", ".", "pi", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac_targ", ".", "parameters", "(", ")", ")", "\n", "\n", "# Freeze target networks with respect to optimizers (only update via polyak averaging)", "\n", "for", "p", "in", "ac_targ", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "# List of parameters for both Q-networks (save this for convenience)", "\n", "", "q_params", "=", "itertools", ".", "chain", "(", "ac", ".", "q1", ".", "parameters", "(", ")", ",", "ac", ".", "q2", ".", "parameters", "(", ")", ")", "\n", "\n", "# Experience buffer", "\n", "replay_buffer", "=", "ReplayBuffer", "(", "obs_dim", "=", "obs_dim", ",", "act_dim", "=", "act_dim", ",", "size", "=", "replay_size", ")", "\n", "\n", "# Count variables (protip: try to get a feel for how different size networks behave!)", "\n", "var_counts", "=", "tuple", "(", "core", ".", "count_vars", "(", "module", ")", "for", "module", "in", "[", "ac", ".", "pi", ",", "ac", ".", "q1", ",", "ac", ".", "q2", "]", ")", "\n", "\n", "# Set up function for computing SAC Q-losses", "\n", "def", "compute_loss_q", "(", "data", ")", ":", "\n", "        ", "o", ",", "a", ",", "r", ",", "o2", ",", "d", "=", "data", "[", "'obs'", "]", ",", "data", "[", "'act'", "]", ",", "data", "[", "'rew'", "]", ",", "data", "[", "'obs2'", "]", ",", "data", "[", "'done'", "]", "\n", "\n", "q1", "=", "ac", ".", "q1", "(", "o", ",", "a", ")", "\n", "q2", "=", "ac", ".", "q2", "(", "o", ",", "a", ")", "\n", "\n", "# Bellman backup for Q functions", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "# Target actions come from *current* policy", "\n", "            ", "a2", ",", "logp_a2", "=", "ac", ".", "pi", "(", "o2", ")", "\n", "\n", "# Target Q-values", "\n", "q1_pi_targ", "=", "ac_targ", ".", "q1", "(", "o2", ",", "a2", ")", "\n", "q2_pi_targ", "=", "ac_targ", ".", "q2", "(", "o2", ",", "a2", ")", "\n", "q_pi_targ", "=", "torch", ".", "min", "(", "q1_pi_targ", ",", "q2_pi_targ", ")", "\n", "backup", "=", "r", "+", "gamma", "*", "(", "1", "-", "d", ")", "*", "(", "q_pi_targ", "-", "alpha", "*", "logp_a2", ")", "\n", "\n", "# MSE loss against Bellman backup", "\n", "", "loss_q1", "=", "(", "(", "q1", "-", "backup", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "loss_q2", "=", "(", "(", "q2", "-", "backup", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "loss_q", "=", "loss_q1", "+", "loss_q2", "\n", "\n", "# Useful info for logging", "\n", "q_info", "=", "dict", "(", "Q1Vals", "=", "q1", ".", "detach", "(", ")", ".", "numpy", "(", ")", ",", "\n", "Q2Vals", "=", "q2", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "return", "loss_q", ",", "q_info", "\n", "\n", "# Set up function for computing SAC pi loss", "\n", "", "def", "compute_loss_pi", "(", "data", ")", ":", "\n", "        ", "o", "=", "data", "[", "'obs'", "]", "\n", "pi", ",", "logp_pi", "=", "ac", ".", "pi", "(", "o", ")", "\n", "q1_pi", "=", "ac", ".", "q1", "(", "o", ",", "pi", ")", "\n", "q2_pi", "=", "ac", ".", "q2", "(", "o", ",", "pi", ")", "\n", "q_pi", "=", "torch", ".", "min", "(", "q1_pi", ",", "q2_pi", ")", "\n", "\n", "# Entropy-regularized policy loss", "\n", "loss_pi", "=", "(", "alpha", "*", "logp_pi", "-", "q_pi", ")", ".", "mean", "(", ")", "\n", "\n", "# Useful info for logging", "\n", "pi_info", "=", "dict", "(", "LogPi", "=", "logp_pi", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "return", "loss_pi", ",", "pi_info", "\n", "\n", "# Set up optimizers for policy and q-function", "\n", "", "pi_optimizer", "=", "Adam", "(", "ac", ".", "pi", ".", "parameters", "(", ")", ",", "lr", "=", "lr", ")", "\n", "q_optimizer", "=", "Adam", "(", "q_params", ",", "lr", "=", "lr", ")", "\n", "\n", "\n", "def", "update", "(", "data", ")", ":", "\n", "\n", "# First run one gradient descent step for Q1 and Q2", "\n", "        ", "q_optimizer", ".", "zero_grad", "(", ")", "\n", "loss_q", ",", "q_info", "=", "compute_loss_q", "(", "data", ")", "\n", "loss_q", ".", "backward", "(", ")", "\n", "sync_grads", "(", "comm", ",", "q_params", ")", "\n", "q_optimizer", ".", "step", "(", ")", "\n", "\n", "# Freeze Q-networks so you don't waste computational effort ", "\n", "# computing gradients for them during the policy learning step.", "\n", "for", "p", "in", "q_params", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "# Next run one gradient descent step for pi.", "\n", "", "pi_optimizer", ".", "zero_grad", "(", ")", "\n", "loss_pi", ",", "pi_info", "=", "compute_loss_pi", "(", "data", ")", "\n", "loss_pi", ".", "backward", "(", ")", "\n", "sync_grads", "(", "comm", ",", "ac", ".", "pi", ".", "parameters", "(", ")", ")", "\n", "pi_optimizer", ".", "step", "(", ")", "\n", "\n", "# Unfreeze Q-networks so you can optimize it at next DDPG step.", "\n", "for", "p", "in", "q_params", ":", "\n", "            ", "p", ".", "requires_grad", "=", "True", "\n", "\n", "\n", "# Finally, update target networks by polyak averaging.", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "p", ",", "p_targ", "in", "zip", "(", "ac", ".", "parameters", "(", ")", ",", "ac_targ", ".", "parameters", "(", ")", ")", ":", "\n", "# NB: We use an in-place operations \"mul_\", \"add_\" to update target", "\n", "# params, as opposed to \"mul\" and \"add\", which would make new tensors.", "\n", "                ", "p_targ", ".", "data", ".", "mul_", "(", "polyak", ")", "\n", "p_targ", ".", "data", ".", "add_", "(", "(", "1", "-", "polyak", ")", "*", "p", ".", "data", ")", "\n", "\n", "\n", "# sync weights", "\n", "", "", "sync_weights", "(", "comm", ",", "q_params", ")", "\n", "sync_weights", "(", "comm", ",", "ac", ".", "pi", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac_targ", ".", "parameters", "(", ")", ")", "\n", "\n", "# save weights", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "            ", "torch", ".", "save", "(", "ac", ".", "state_dict", "(", ")", ",", "data_dir", "+", "\"ac.pt\"", ")", "\n", "torch", ".", "save", "(", "ac_targ", ".", "state_dict", "(", ")", ",", "data_dir", "+", "\"ac_targ.pt\"", ")", "\n", "\n", "\n", "", "", "def", "get_action", "(", "o", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "return", "ac", ".", "act", "(", "torch", ".", "as_tensor", "(", "o", ",", "dtype", "=", "torch", ".", "float32", ")", ",", "\n", "deterministic", ")", "\n", "\n", "", "def", "test_agent", "(", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "num_test_episodes", ")", ":", "\n", "            ", "o", ",", "d", ",", "ep_ret", ",", "ep_len", "=", "test_env", ".", "reset", "(", ")", ",", "False", ",", "0", ",", "0", "\n", "while", "not", "(", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ")", ":", "\n", "# Take deterministic actions at test time ", "\n", "                ", "o", ",", "r", ",", "d", ",", "_", "=", "test_env", ".", "step", "(", "get_action", "(", "o", ",", "True", ")", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "\n", "# Prepare for interaction with environment", "\n", "", "", "", "total_steps", "=", "steps_per_epoch", "*", "epochs", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "o", ",", "ep_ret", ",", "ep_len", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", "\n", "\n", "# Main loop: collect experience in env and update/log each epoch", "\n", "for", "t", "in", "range", "(", "total_steps", ")", ":", "\n", "\n", "# Until start_steps have elapsed, randomly sample actions", "\n", "# from a uniform distribution for better exploration. Afterwards, ", "\n", "# use the learned policy. ", "\n", "        ", "if", "eval_mode", ":", "\n", "            ", "a", "=", "get_action", "(", "o", ",", "True", ")", "\n", "", "else", ":", "\n", "            ", "if", "t", ">", "start_steps", ":", "\n", "                ", "a", "=", "get_action", "(", "o", ")", "\n", "", "else", ":", "\n", "                ", "a", "=", "env", ".", "action_space", ".", "sample", "(", ")", "\n", "\n", "# Step the env", "\n", "", "", "o2", ",", "r", ",", "d", ",", "_", "=", "env", ".", "step", "(", "a", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "\n", "# Ignore the \"done\" signal if it comes from hitting the time", "\n", "# horizon (that is, when it's an artificial terminal signal", "\n", "# that isn't based on the agent's state)", "\n", "d", "=", "False", "if", "ep_len", "==", "max_ep_len", "else", "d", "\n", "\n", "# Store experience to replay buffer", "\n", "replay_buffer", ".", "store", "(", "o", ",", "a", ",", "r", ",", "o2", ",", "d", ")", "\n", "\n", "# Super critical, easy to overlook step: make sure to update ", "\n", "# most recent observation!", "\n", "o", "=", "o2", "\n", "\n", "# End of trajectory handling", "\n", "if", "d", ":", "\n", "            ", "o", ",", "ep_ret", ",", "ep_len", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", "\n", "\n", "# Update handling (training only)", "\n", "", "if", "not", "eval_mode", ":", "\n", "            ", "if", "t", ">=", "update_after", "and", "t", "%", "update_every", "==", "0", ":", "\n", "                ", "for", "j", "in", "range", "(", "update_every", ")", ":", "\n", "                    ", "batch", "=", "replay_buffer", ".", "sample_batch", "(", "batch_size", ")", "\n", "update", "(", "data", "=", "batch", ")", "\n", "\n", "# End of epoch handling", "\n", "", "", "", "if", "(", "t", "+", "1", ")", "%", "steps_per_epoch", "==", "0", ":", "\n", "            ", "epoch", "=", "(", "t", "+", "1", ")", "//", "steps_per_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.SquashedGaussianMLPActor.__init__": [[34, 40], ["torch.Module.__init__", "sac_network_utils.mlp", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ",", "act_limit", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "net", "=", "mlp", "(", "[", "obs_dim", "]", "+", "list", "(", "hidden_sizes", ")", ",", "activation", ",", "activation", ")", "\n", "self", ".", "mu_layer", "=", "nn", ".", "Linear", "(", "hidden_sizes", "[", "-", "1", "]", ",", "act_dim", ")", "\n", "self", ".", "log_std_layer", "=", "nn", ".", "Linear", "(", "hidden_sizes", "[", "-", "1", "]", ",", "act_dim", ")", "\n", "self", ".", "act_limit", "=", "act_limit", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.SquashedGaussianMLPActor.forward": [[41, 71], ["sac_network_utils.SquashedGaussianMLPActor.net", "sac_network_utils.SquashedGaussianMLPActor.mu_layer", "sac_network_utils.SquashedGaussianMLPActor.log_std_layer", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob", "torch.softplus", "torch.softplus", "torch.softplus", "numpy.log"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ",", "deterministic", "=", "False", ",", "with_logprob", "=", "True", ")", ":", "\n", "        ", "net_out", "=", "self", ".", "net", "(", "obs", ")", "\n", "mu", "=", "self", ".", "mu_layer", "(", "net_out", ")", "\n", "log_std", "=", "self", ".", "log_std_layer", "(", "net_out", ")", "\n", "log_std", "=", "torch", ".", "clamp", "(", "log_std", ",", "LOG_STD_MIN", ",", "LOG_STD_MAX", ")", "\n", "std", "=", "torch", ".", "exp", "(", "log_std", ")", "\n", "\n", "# Pre-squash distribution and sample", "\n", "pi_distribution", "=", "Normal", "(", "mu", ",", "std", ")", "\n", "if", "deterministic", ":", "\n", "# Only used for evaluating policy at test time.", "\n", "            ", "pi_action", "=", "mu", "\n", "", "else", ":", "\n", "            ", "pi_action", "=", "pi_distribution", ".", "rsample", "(", ")", "\n", "\n", "", "if", "with_logprob", ":", "\n", "# Compute logprob from Gaussian, and then apply correction for Tanh squashing.", "\n", "# NOTE: The correction formula is a little bit magic. To get an understanding ", "\n", "# of where it comes from, check out the original SAC paper (arXiv 1801.01290) ", "\n", "# and look in appendix C. This is a more numerically-stable equivalent to Eq 21.", "\n", "# Try deriving it yourself as a (very difficult) exercise. :)", "\n", "            ", "logp_pi", "=", "pi_distribution", ".", "log_prob", "(", "pi_action", ")", ".", "sum", "(", "axis", "=", "-", "1", ")", "\n", "logp_pi", "-=", "(", "2", "*", "(", "np", ".", "log", "(", "2", ")", "-", "pi_action", "-", "F", ".", "softplus", "(", "-", "2", "*", "pi_action", ")", ")", ")", ".", "sum", "(", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "logp_pi", "=", "None", "\n", "\n", "", "pi_action", "=", "torch", ".", "tanh", "(", "pi_action", ")", "\n", "pi_action", "=", "self", ".", "act_limit", "*", "pi_action", "\n", "\n", "return", "pi_action", ",", "logp_pi", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.MLPQFunction.__init__": [[75, 78], ["torch.Module.__init__", "sac_network_utils.mlp", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "q", "=", "mlp", "(", "[", "obs_dim", "+", "act_dim", "]", "+", "list", "(", "hidden_sizes", ")", "+", "[", "1", "]", ",", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.MLPQFunction.forward": [[79, 83], ["sac_network_utils.MLPQFunction.q", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ",", "act", ")", ":", "\n", "#obs = torch.flatten(obs,start_dim=1)", "\n", "        ", "q", "=", "self", ".", "q", "(", "torch", ".", "cat", "(", "[", "obs", ",", "act", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "return", "torch", ".", "squeeze", "(", "q", ",", "-", "1", ")", "# Critical to ensure q has right shape.", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.MLPActorCritic.__init__": [[86, 101], ["torch.Module.__init__", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "sac_network_utils.SquashedGaussianMLPActor", "sac_network_utils.MLPQFunction", "sac_network_utils.MLPQFunction"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["    ", "def", "__init__", "(", "self", ",", "observation_space", ",", "action_space", ",", "hidden_sizes", "=", "(", "256", ",", "256", ")", ",", "\n", "activation", "=", "nn", ".", "ReLU", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "os", ".", "environ", "[", "\"OMP_NUM_THREADS\"", "]", "=", "\"1\"", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "obs_dim", "=", "observation_space", ".", "shape", "[", "0", "]", "\n", "act_dim", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "act_limit", "=", "action_space", ".", "high", "[", "0", "]", "\n", "\n", "# build policy and value functions", "\n", "self", ".", "pi", "=", "SquashedGaussianMLPActor", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ",", "act_limit", ")", "\n", "self", ".", "q1", "=", "MLPQFunction", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", "\n", "self", ".", "q2", "=", "MLPQFunction", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.MLPActorCritic.act": [[102, 106], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sac_network_utils.MLPActorCritic.pi", "a.numpy"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy"], ["", "def", "act", "(", "self", ",", "obs", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "a", ",", "_", "=", "self", ".", "pi", "(", "obs", ",", "deterministic", ",", "False", ")", "\n", "return", "a", ".", "numpy", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.SquashedGaussianConvActor.__init__": [[112, 125], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "sac_network_utils.mlp", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "math.floor", "math.floor", "math.floor", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ",", "act_limit", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "sz", "=", "obs_dim", "[", "0", "]", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "obs_dim", "[", "2", "]", ",", "16", ",", "(", "5", ",", "5", ")", ",", "stride", "=", "(", "2", ",", "2", ")", ")", "\n", "sz", "=", "math", ".", "floor", "(", "(", "sz", "-", "5", ")", "/", "2.0", ")", "+", "1", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "16", ",", "64", ",", "(", "5", ",", "5", ")", ",", "stride", "=", "(", "2", ",", "2", ")", ")", "\n", "sz", "=", "math", ".", "floor", "(", "(", "sz", "-", "5", ")", "/", "2.0", ")", "+", "1", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "64", ",", "32", ",", "(", "5", ",", "5", ")", ",", "stride", "=", "(", "2", ",", "2", ")", ")", "\n", "sz", "=", "math", ".", "floor", "(", "(", "sz", "-", "5", ")", "/", "2.0", ")", "+", "1", "\n", "self", ".", "net", "=", "mlp", "(", "[", "sz", "*", "sz", "*", "32", "]", "+", "list", "(", "hidden_sizes", ")", ",", "activation", ",", "activation", ")", "\n", "self", ".", "mu_layer", "=", "nn", ".", "Linear", "(", "hidden_sizes", "[", "-", "1", "]", ",", "act_dim", ")", "\n", "self", ".", "log_std_layer", "=", "nn", ".", "Linear", "(", "hidden_sizes", "[", "-", "1", "]", ",", "act_dim", ")", "\n", "self", ".", "act_limit", "=", "act_limit", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.SquashedGaussianConvActor.forward": [[126, 161], ["obs.permute", "sac_network_utils.SquashedGaussianConvActor.conv1", "sac_network_utils.SquashedGaussianConvActor.conv2", "sac_network_utils.SquashedGaussianConvActor.conv3", "x.view.view.view", "sac_network_utils.SquashedGaussianConvActor.net", "sac_network_utils.SquashedGaussianConvActor.mu_layer", "sac_network_utils.SquashedGaussianConvActor.log_std_layer", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.distributions.normal.Normal", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "torch.tanh", "x.view.view.size", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.rsample", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob().sum", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob", "torch.distributions.normal.Normal.log_prob", "torch.softplus", "torch.softplus", "torch.softplus", "numpy.log"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ",", "deterministic", "=", "False", ",", "with_logprob", "=", "True", ")", ":", "\n", "        ", "x", "=", "obs", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "#channels first", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "net_out", "=", "self", ".", "net", "(", "x", ")", "\n", "mu", "=", "self", ".", "mu_layer", "(", "net_out", ")", "\n", "log_std", "=", "self", ".", "log_std_layer", "(", "net_out", ")", "\n", "log_std", "=", "torch", ".", "clamp", "(", "log_std", ",", "LOG_STD_MIN", ",", "LOG_STD_MAX", ")", "\n", "std", "=", "torch", ".", "exp", "(", "log_std", ")", "\n", "\n", "# Pre-squash distribution and sample", "\n", "pi_distribution", "=", "Normal", "(", "mu", ",", "std", ")", "\n", "if", "deterministic", ":", "\n", "# Only used for evaluating policy at test time.", "\n", "            ", "pi_action", "=", "mu", "\n", "", "else", ":", "\n", "            ", "pi_action", "=", "pi_distribution", ".", "rsample", "(", ")", "\n", "\n", "", "if", "with_logprob", ":", "\n", "# Compute logprob from Gaussian, and then apply correction for Tanh squashing.", "\n", "# NOTE: The correction formula is a little bit magic. To get an understanding ", "\n", "# of where it comes from, check out the original SAC paper (arXiv 1801.01290) ", "\n", "# and look in appendix C. This is a more numerically-stable equivalent to Eq 21.", "\n", "# Try deriving it yourself as a (very difficult) exercise. :)", "\n", "            ", "logp_pi", "=", "pi_distribution", ".", "log_prob", "(", "pi_action", ")", ".", "sum", "(", "axis", "=", "-", "1", ")", "\n", "logp_pi", "-=", "(", "2", "*", "(", "np", ".", "log", "(", "2", ")", "-", "pi_action", "-", "F", ".", "softplus", "(", "-", "2", "*", "pi_action", ")", ")", ")", ".", "sum", "(", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "logp_pi", "=", "None", "\n", "\n", "", "pi_action", "=", "torch", ".", "tanh", "(", "pi_action", ")", "\n", "pi_action", "=", "self", ".", "act_limit", "*", "pi_action", "\n", "\n", "return", "pi_action", ",", "logp_pi", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.ConvQFunction.__init__": [[165, 177], ["torch.Module.__init__", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "torch.Conv2d", "sac_network_utils.mlp", "math.floor", "math.floor", "math.floor", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "sz", "=", "obs_dim", "[", "0", "]", "\n", "\n", "self", ".", "conv1", "=", "nn", ".", "Conv2d", "(", "obs_dim", "[", "2", "]", ",", "16", ",", "(", "5", ",", "5", ")", ",", "stride", "=", "(", "2", ",", "2", ")", ")", "\n", "sz", "=", "math", ".", "floor", "(", "(", "sz", "-", "5", ")", "/", "2.0", ")", "+", "1", "\n", "self", ".", "conv2", "=", "nn", ".", "Conv2d", "(", "16", ",", "64", ",", "(", "5", ",", "5", ")", ",", "stride", "=", "(", "2", ",", "2", ")", ")", "\n", "sz", "=", "math", ".", "floor", "(", "(", "sz", "-", "5", ")", "/", "2.0", ")", "+", "1", "\n", "self", ".", "conv3", "=", "nn", ".", "Conv2d", "(", "64", ",", "32", ",", "(", "5", ",", "5", ")", ",", "stride", "=", "(", "2", ",", "2", ")", ")", "\n", "sz", "=", "math", ".", "floor", "(", "(", "sz", "-", "5", ")", "/", "2.0", ")", "+", "1", "\n", "\n", "self", ".", "q", "=", "mlp", "(", "[", "sz", "*", "sz", "*", "32", "+", "act_dim", "]", "+", "list", "(", "hidden_sizes", ")", "+", "[", "1", "]", ",", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.ConvQFunction.forward": [[178, 188], ["obs.permute", "sac_network_utils.ConvQFunction.conv1", "sac_network_utils.ConvQFunction.conv2", "sac_network_utils.ConvQFunction.conv3", "x.view.view.view", "sac_network_utils.ConvQFunction.q", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "obs.size", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "obs", ",", "act", ")", ":", "\n", "        ", "x", "=", "obs", ".", "permute", "(", "0", ",", "3", ",", "1", ",", "2", ")", "#channels first", "\n", "x", "=", "self", ".", "conv1", "(", "x", ")", "\n", "x", "=", "self", ".", "conv2", "(", "x", ")", "\n", "x", "=", "self", ".", "conv3", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "obs", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "\n", "#obs = torch.flatten(obs,start_dim=1)", "\n", "q", "=", "self", ".", "q", "(", "torch", ".", "cat", "(", "[", "x", ",", "act", "]", ",", "dim", "=", "-", "1", ")", ")", "\n", "return", "torch", ".", "squeeze", "(", "q", ",", "-", "1", ")", "# Critical to ensure q has right shape.", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.ConvActorCritic.__init__": [[191, 206], ["torch.Module.__init__", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "sac_network_utils.SquashedGaussianConvActor", "sac_network_utils.ConvQFunction", "sac_network_utils.ConvQFunction"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["    ", "def", "__init__", "(", "self", ",", "observation_space", ",", "action_space", ",", "hidden_sizes", "=", "(", "256", ",", "256", ")", ",", "\n", "activation", "=", "nn", ".", "ReLU", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "os", ".", "environ", "[", "\"OMP_NUM_THREADS\"", "]", "=", "\"1\"", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "obs_dim", "=", "observation_space", ".", "shape", "\n", "act_dim", "=", "action_space", ".", "shape", "[", "0", "]", "\n", "act_limit", "=", "action_space", ".", "high", "[", "0", "]", "\n", "\n", "# build policy and value functions", "\n", "self", ".", "pi", "=", "SquashedGaussianConvActor", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ",", "act_limit", ")", "\n", "self", ".", "q1", "=", "ConvQFunction", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", "\n", "self", ".", "q2", "=", "ConvQFunction", "(", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.ConvActorCritic.act": [[207, 211], ["torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "sac_network_utils.ConvActorCritic.pi", "a.numpy"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy"], ["", "def", "act", "(", "self", ",", "obs", ",", "deterministic", "=", "False", ")", ":", "\n", "        ", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "a", ",", "_", "=", "self", ".", "pi", "(", "obs", ",", "deterministic", ",", "False", ")", "\n", "return", "a", ".", "numpy", "(", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.combined_shape": [[13, 17], ["numpy.isscalar"], "function", ["None"], ["def", "combined_shape", "(", "length", ",", "shape", "=", "None", ")", ":", "\n", "    ", "if", "shape", "is", "None", ":", "\n", "        ", "return", "(", "length", ",", ")", "\n", "", "return", "(", "length", ",", "shape", ")", "if", "np", ".", "isscalar", "(", "shape", ")", "else", "(", "length", ",", "*", "shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.mlp": [[18, 24], ["range", "torch.Sequential", "len", "torch.Linear", "act", "len"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPActorCritic.act"], ["", "def", "mlp", "(", "sizes", ",", "activation", ",", "output_activation", "=", "nn", ".", "Identity", ")", ":", "\n", "    ", "layers", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "sizes", ")", "-", "1", ")", ":", "\n", "        ", "act", "=", "activation", "if", "j", "<", "len", "(", "sizes", ")", "-", "2", "else", "output_activation", "\n", "layers", "+=", "[", "nn", ".", "Linear", "(", "sizes", "[", "j", "]", ",", "sizes", "[", "j", "+", "1", "]", ")", ",", "act", "(", ")", "]", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.sac.sac_network_utils.count_vars": [[25, 27], ["sum", "numpy.prod", "module.parameters"], "function", ["None"], ["", "def", "count_vars", "(", "module", ")", ":", "\n", "    ", "return", "sum", "(", "[", "np", ".", "prod", "(", "p", ".", "shape", ")", "for", "p", "in", "module", ".", "parameters", "(", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_random.multiagent_random_policy.MARandomPolicy.__init__": [[13, 18], ["len"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ")", ":", "\n", "\t\t", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "\n", "self", ".", "num_entities", "=", "len", "(", "self", ".", "env", ".", "observation_spaces", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_random.multiagent_random_policy.MARandomPolicy.run": [[19, 60], ["multiagent_random_policy.MARandomPolicy.env.reset", "int", "range", "range", "multiagent_random_policy.MARandomPolicy.env.step", "policy_record.save", "multiagent_random_policy.MARandomPolicy.comm.Get_size", "multiagent_random_policy.MARandomPolicy.env.action_spaces[].sample", "actions.append", "multiagent_random_policy.MARandomPolicy.env.reset", "policy_record.add_result"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.add_result"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "\n", "# initial reset", "\n", "\t\t", "states", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "cumr", "=", "0.0", "\n", "ep_len", "=", "0", "\n", "\n", "# since we do not synchronize step counts, get steps needed for individual worker:", "\n", "# NOTE: one step consists of an action from all entities, so this is independent from ", "\n", "# the number of entities we control", "\n", "local_steps", "=", "int", "(", "num_steps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "\n", "for", "stp", "in", "range", "(", "local_steps", ")", ":", "\n", "\n", "# collect random aciton for each entity", "\n", "\t\t\t", "actions", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "self", ".", "num_entities", ")", ":", "\n", "\t\t\t\t", "a", "=", "self", ".", "env", ".", "action_spaces", "[", "i", "]", ".", "sample", "(", ")", "\n", "actions", ".", "append", "(", "a", ")", "\n", "\n", "# act", "\n", "", "states", ",", "rewards", ",", "done", ",", "infos", "=", "self", ".", "env", ".", "step", "(", "actions", ")", "\n", "\n", "# we will log the sum of all rewards but you could do whatever you want here", "\n", "for", "r", "in", "rewards", ":", "\n", "\t\t\t\t", "cumr", "+=", "r", "\n", "", "ep_len", "+=", "1", "\n", "\n", "# handle done", "\n", "if", "done", ":", "\n", "\t\t\t\t", "states", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "\t\t\t\t\t", "policy_record", ".", "add_result", "(", "cumr", ",", "ep_len", ")", "\n", "\n", "", "cumr", "=", "0.0", "\n", "ep_len", "=", "0", "\n", "\n", "", "", "if", "policy_record", "is", "not", "None", ":", "\n", "\t\t\t", "policy_record", ".", "save", "(", ")", "", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.maddpg_policy.MADDPGPolicy.__init__": [[10, 14], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "env", ",", "policy_comm", ",", "eval_mode", "=", "False", ")", ":", "\n", "        ", "self", ".", "env", "=", "env", "\n", "self", ".", "comm", "=", "policy_comm", "\n", "self", ".", "eval_mode", "=", "eval_mode", "\n", "# print(env.observation_spaces, env.action_spaces)", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.maddpg_policy.MADDPGPolicy.run": [[17, 27], ["arena5.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater", "int", "arena5.algos.maddpg.maddpg.maddpg", "maddpg_policy.MADDPGPolicy.comm.Get_size", "dict"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.maddpg.maddpg"], ["", "def", "run", "(", "self", ",", "num_steps", ",", "data_dir", ",", "policy_record", "=", "None", ")", ":", "\n", "\n", "        ", "self", ".", "env", "=", "MPISynchronizedPRUpdater", "(", "self", ".", "env", ",", "self", ".", "comm", ",", "policy_record", ",", "sum_over", "=", "True", ")", "\n", "local_steps", "=", "int", "(", "num_steps", "/", "self", ".", "comm", ".", "Get_size", "(", ")", ")", "\n", "\n", "maddpg", "(", "lambda", ":", "self", ".", "env", ",", "self", ".", "comm", ",", "data_dir", ",", "policy_record", ",", "self", ".", "eval_mode", ",", "\n", "actor_critic", "=", "core", ".", "MLPActorCritic", ",", "\n", "ac_kwargs", "=", "dict", "(", "hidden_sizes", "=", "[", "256", ",", "256", "]", ")", ",", "\n", "gamma", "=", "0.99", ",", "seed", "=", "1337", ",", "steps_per_epoch", "=", "local_steps", ",", "epochs", "=", "1", ",", "\n", "logger_kwargs", "=", "None", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPActor.__init__": [[27, 32], ["torch.Module.__init__", "network_utils.mlp", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["\n", "\n", "", "LOG_STD_MAX", "=", "2", "\n", "LOG_STD_MIN", "=", "-", "20", "\n", "\n", "class", "SquashedGaussianMLPActor", "(", "nn", ".", "Module", ")", ":", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPActor.forward": [[33, 36], ["network_utils.MLPActor.pi"], "methods", ["None"], ["\n", "    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ",", "act_limit", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "net", "=", "mlp", "(", "[", "obs_dim", "]", "+", "list", "(", "hidden_sizes", ")", ",", "activation", ",", "activation", ")", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPQFunction.__init__": [[39, 42], ["torch.Module.__init__", "network_utils.mlp", "list"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp"], ["self", ".", "act_limit", "=", "act_limit", "\n", "\n", "", "def", "forward", "(", "self", ",", "obs", ",", "deterministic", "=", "False", ",", "with_logprob", "=", "True", ")", ":", "\n", "        ", "net_out", "=", "self", ".", "net", "(", "obs", ")", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPQFunction.forward": [[43, 48], ["len", "network_utils.MLPQFunction.q", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.squeeze", "torch.cat", "torch.cat", "torch.cat", "torch.cat"], "methods", ["None"], ["mu", "=", "self", ".", "mu_layer", "(", "net_out", ")", "\n", "log_std", "=", "self", ".", "log_std_layer", "(", "net_out", ")", "\n", "log_std", "=", "torch", ".", "clamp", "(", "log_std", ",", "LOG_STD_MIN", ",", "LOG_STD_MAX", ")", "\n", "std", "=", "torch", ".", "exp", "(", "log_std", ")", "\n", "\n", "# Pre-squash distribution and sample", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPActorCritic.__init__": [[51, 77], ["torch.Module.__init__", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "torch.set_num_threads", "len", "network_utils.MLPQFunction", "network_utils.MLPActor", "torch.ModuleList", "torch.ModuleList", "range", "torch.ModuleList", "torch.ModuleList", "network_utils.MLPActor", "network_utils.MLPActorCritic.pis.append", "range"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__"], ["# Only used for evaluating policy at test time.", "\n", "            ", "pi_action", "=", "mu", "\n", "", "else", ":", "\n", "            ", "pi_action", "=", "pi_distribution", ".", "rsample", "(", ")", "\n", "\n", "", "if", "with_logprob", ":", "\n", "# Compute logprob from Gaussian, and then apply correction for Tanh squashing.", "\n", "# NOTE: The correction formula is a little bit magic. To get an understanding ", "\n", "# of where it comes from, check out the original SAC paper (arXiv 1801.01290) ", "\n", "# and look in appendix C. This is a more numerically-stable equivalent to Eq 21.", "\n", "# Try deriving it yourself as a (very difficult) exercise. :)", "\n", "            ", "logp_pi", "=", "pi_distribution", ".", "log_prob", "(", "pi_action", ")", ".", "sum", "(", "axis", "=", "-", "1", ")", "\n", "logp_pi", "-=", "(", "2", "*", "(", "np", ".", "log", "(", "2", ")", "-", "pi_action", "-", "F", ".", "softplus", "(", "-", "2", "*", "pi_action", ")", ")", ")", ".", "sum", "(", "axis", "=", "1", ")", "\n", "", "else", ":", "\n", "            ", "logp_pi", "=", "None", "\n", "\n", "", "pi_action", "=", "torch", ".", "tanh", "(", "pi_action", ")", "\n", "pi_action", "=", "self", ".", "act_limit", "*", "pi_action", "\n", "\n", "return", "pi_action", ",", "logp_pi", "\n", "\n", "\n", "", "", "class", "MLPQFunction", "(", "nn", ".", "Module", ")", ":", "\n", "\n", "    ", "def", "__init__", "(", "self", ",", "obs_dim", ",", "act_dim", ",", "hidden_sizes", ",", "activation", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "q", "=", "mlp", "(", "[", "obs_dim", "+", "act_dim", "]", "+", "list", "(", "hidden_sizes", ")", "+", "[", "1", "]", ",", "activation", ")", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPActorCritic.act": [[78, 85], ["range", "len", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.no_grad", "actions.append", "a.numpy"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy"], ["\n", "", "def", "forward", "(", "self", ",", "obs", ",", "act", ")", ":", "\n", "        ", "N", "=", "len", "(", "obs", ")", "\n", "all_inputs", "=", "[", "o", "for", "o", "in", "obs", "]", "+", "[", "a", "for", "a", "in", "act", "]", "\n", "q", "=", "self", ".", "q", "(", "torch", ".", "cat", "(", "all_inputs", ",", "dim", "=", "-", "1", ")", ")", "\n", "return", "torch", ".", "squeeze", "(", "q", ",", "-", "1", ")", "# Critical to ensure q has right shape.", "\n", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape": [[10, 14], ["numpy.isscalar"], "function", ["None"], ["import", "os", "\n", "\n", "\n", "def", "combined_shape", "(", "length", ",", "shape", "=", "None", ")", ":", "\n", "    ", "if", "shape", "is", "None", ":", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.mlp": [[15, 21], ["range", "torch.Sequential", "len", "torch.Linear", "act", "len", "torch.nn.Tanh"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.MLPActorCritic.act"], ["        ", "return", "(", "length", ",", ")", "\n", "", "return", "(", "length", ",", "shape", ")", "if", "np", ".", "isscalar", "(", "shape", ")", "else", "(", "length", ",", "*", "shape", ")", "\n", "\n", "", "def", "mlp", "(", "sizes", ",", "activation", ",", "output_activation", "=", "nn", ".", "Identity", ")", ":", "\n", "    ", "layers", "=", "[", "]", "\n", "for", "j", "in", "range", "(", "len", "(", "sizes", ")", "-", "1", ")", ":", "\n", "        ", "act", "=", "activation", "if", "j", "<", "len", "(", "sizes", ")", "-", "2", "else", "output_activation", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.count_vars": [[22, 24], ["sum", "numpy.prod", "module.parameters"], "function", ["None"], ["layers", "+=", "[", "nn", ".", "Linear", "(", "sizes", "[", "j", "]", ",", "sizes", "[", "j", "+", "1", "]", ")", ",", "act", "(", ")", "]", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.maddpg.ReplayBuffer.__init__": [[18, 30], ["range", "numpy.zeros", "numpy.zeros", "maddpg.ReplayBuffer.obs_buf.append", "maddpg.ReplayBuffer.obs2_buf.append", "maddpg.ReplayBuffer.act_buf.append", "numpy.zeros", "numpy.zeros", "numpy.zeros", "arena5.combined_shape", "arena5.combined_shape", "arena5.combined_shape"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape", "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.network_utils.combined_shape"], ["def", "__init__", "(", "self", ",", "N", ",", "obs_dim", ",", "act_dim", ",", "size", ")", ":", "\n", "        ", "self", ".", "obs_buf", "=", "[", "]", "\n", "self", ".", "obs2_buf", "=", "[", "]", "\n", "self", ".", "act_buf", "=", "[", "]", "\n", "self", ".", "N", "=", "N", "\n", "for", "i", "in", "range", "(", "N", ")", ":", "\n", "            ", "self", ".", "obs_buf", ".", "append", "(", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "obs_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "self", ".", "obs2_buf", ".", "append", "(", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "obs_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "self", ".", "act_buf", ".", "append", "(", "np", ".", "zeros", "(", "core", ".", "combined_shape", "(", "size", ",", "act_dim", ")", ",", "dtype", "=", "np", ".", "float32", ")", ")", "\n", "", "self", ".", "rew_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "done_buf", "=", "np", ".", "zeros", "(", "size", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "self", ".", "ptr", ",", "self", ".", "size", ",", "self", ".", "max_size", "=", "0", ",", "0", ",", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.maddpg.ReplayBuffer.store": [[31, 40], ["range", "min"], "methods", ["None"], ["", "def", "store", "(", "self", ",", "obs", ",", "act", ",", "rew", ",", "next_obs", ",", "done", ")", ":", "\n", "        ", "for", "i", "in", "range", "(", "self", ".", "N", ")", ":", "\n", "            ", "self", ".", "obs_buf", "[", "i", "]", "[", "self", ".", "ptr", "]", "=", "obs", "[", "i", "]", "\n", "self", ".", "obs2_buf", "[", "i", "]", "[", "self", ".", "ptr", "]", "=", "next_obs", "[", "i", "]", "\n", "self", ".", "act_buf", "[", "i", "]", "[", "self", ".", "ptr", "]", "=", "act", "[", "i", "]", "\n", "", "self", ".", "rew_buf", "[", "self", ".", "ptr", "]", "=", "rew", "\n", "self", ".", "done_buf", "[", "self", ".", "ptr", "]", "=", "done", "\n", "self", ".", "ptr", "=", "(", "self", ".", "ptr", "+", "1", ")", "%", "self", ".", "max_size", "\n", "self", ".", "size", "=", "min", "(", "self", ".", "size", "+", "1", ",", "self", ".", "max_size", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.maddpg.ReplayBuffer.sample_batch": [[41, 52], ["numpy.random.randint", "dict", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "torch.as_tensor", "range", "range", "range"], "methods", ["None"], ["", "def", "sample_batch", "(", "self", ",", "batch_size", "=", "32", ")", ":", "\n", "        ", "idxs", "=", "np", ".", "random", ".", "randint", "(", "0", ",", "self", ".", "size", ",", "size", "=", "batch_size", ")", "\n", "batch", "=", "dict", "(", "\n", "obs", "=", "[", "torch", ".", "as_tensor", "(", "self", ".", "obs_buf", "[", "i", "]", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "for", "i", "in", "range", "(", "self", ".", "N", ")", "]", ",", "\n", "obs2", "=", "[", "torch", ".", "as_tensor", "(", "self", ".", "obs2_buf", "[", "i", "]", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "for", "i", "in", "range", "(", "self", ".", "N", ")", "]", ",", "\n", "act", "=", "[", "torch", ".", "as_tensor", "(", "self", ".", "act_buf", "[", "i", "]", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "for", "i", "in", "range", "(", "self", ".", "N", ")", "]", ",", "\n", "rew", "=", "torch", ".", "as_tensor", "(", "self", ".", "rew_buf", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", ",", "\n", "done", "=", "torch", ".", "as_tensor", "(", "self", ".", "done_buf", "[", "idxs", "]", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", ")", "\n", "\n", "return", "batch", "#{k: torch.as_tensor(v, dtype=torch.float32) for k,v in batch.items()}", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.maddpg.maddpg.maddpg": [[55, 300], ["dict", "int", "dict", "torch.set_num_threads", "torch.manual_seed", "numpy.random.seed", "len", "actor_critic", "copy.deepcopy", "arena5.core.mpi_pytorch_utils.sync_weights", "arena5.core.mpi_pytorch_utils.sync_weights", "copy.deepcopy.parameters", "maddpg.ReplayBuffer", "torch.optim.Adam", "time.time", "range", "env_fn", "env_fn", "os.path.exists", "os.path.exists", "actor_critic.parameters", "copy.deepcopy.parameters", "actor_critic.q", "dict", "actor_critic.q", "torch.optim.Adam", "actor_critic.q.parameters", "torch.optim.Adam.zero_grad", "maddpg.maddpg.compute_loss_q"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights"], ["", "", "def", "maddpg", "(", "env_fn", ",", "comm", ",", "data_dir", ",", "policy_record", "=", "None", ",", "eval_mode", "=", "False", ",", "\n", "common_actor", "=", "False", ",", "\n", "actor_critic", "=", "core", ".", "MLPActorCritic", ",", "ac_kwargs", "=", "dict", "(", ")", ",", "seed", "=", "0", ",", "\n", "steps_per_epoch", "=", "4000", ",", "epochs", "=", "100", ",", "replay_size", "=", "int", "(", "1e6", ")", ",", "gamma", "=", "0.99", ",", "\n", "polyak", "=", "0.995", ",", "pi_lr", "=", "1e-3", ",", "q_lr", "=", "1e-3", ",", "batch_size", "=", "100", ",", "start_steps", "=", "10000", ",", "\n", "update_after", "=", "1000", ",", "update_every", "=", "50", ",", "act_noise", "=", "0.1", ",", "num_test_episodes", "=", "10", ",", "\n", "max_ep_len", "=", "1000", ",", "logger_kwargs", "=", "dict", "(", ")", ",", "save_freq", "=", "1", ")", ":", "\n", "    ", "\"\"\"\n    Deep Deterministic Policy Gradient (DDPG)\n    Args:\n        env_fn : A function which creates a copy of the environment.\n            The environment must satisfy the OpenAI Gym API.\n        actor_critic: The constructor method for a PyTorch Module with an ``act`` \n            method, a ``pi`` module, and a ``q`` module. The ``act`` method and\n            ``pi`` module should accept batches of observations as inputs,\n            and ``q`` should accept a batch of observations and a batch of \n            actions as inputs. When called, these should return:\n            ===========  ================  ======================================\n            Call         Output Shape      Description\n            ===========  ================  ======================================\n            ``act``      (batch, act_dim)  | Numpy array of actions for each \n                                           | observation.\n            ``pi``       (batch, act_dim)  | Tensor containing actions from policy\n                                           | given observations.\n            ``q``        (batch,)          | Tensor containing the current estimate\n                                           | of Q* for the provided observations\n                                           | and actions. (Critical: make sure to\n                                           | flatten this!)\n            ===========  ================  ======================================\n        ac_kwargs (dict): Any kwargs appropriate for the ActorCritic object \n            you provided to DDPG.\n        seed (int): Seed for random number generators.\n        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n            for the agent and the environment in each epoch.\n        epochs (int): Number of epochs to run and train agent.\n        replay_size (int): Maximum length of replay buffer.\n        gamma (float): Discount factor. (Always between 0 and 1.)\n        polyak (float): Interpolation factor in polyak averaging for target \n            networks. Target networks are updated towards main networks \n            according to:\n            .. math:: \\\\theta_{\\\\text{targ}} \\\\leftarrow \n                \\\\rho \\\\theta_{\\\\text{targ}} + (1-\\\\rho) \\\\theta\n            where :math:`\\\\rho` is polyak. (Always between 0 and 1, usually \n            close to 1.)\n        pi_lr (float): Learning rate for policy.\n        q_lr (float): Learning rate for Q-networks.\n        batch_size (int): Minibatch size for SGD.\n        start_steps (int): Number of steps for uniform-random action selection,\n            before running real policy. Helps exploration.\n        update_after (int): Number of env interactions to collect before\n            starting to do gradient descent updates. Ensures replay buffer\n            is full enough for useful updates.\n        update_every (int): Number of env interactions that should elapse\n            between gradient descent updates. Note: Regardless of how long \n            you wait between updates, the ratio of env steps to gradient steps \n            is locked to 1.\n        act_noise (float): Stddev for Gaussian exploration noise added to \n            policy at training time. (At test time, no noise is added.)\n        num_test_episodes (int): Number of episodes to test the deterministic\n            policy at the end of each epoch.\n        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n        logger_kwargs (dict): Keyword args for EpochLogger.\n        save_freq (int): How often (in terms of gap between epochs) to save\n            the current policy and value function.\n    \"\"\"", "\n", "\n", "os", ".", "environ", "[", "\"OMP_NUM_THREADS\"", "]", "=", "\"1\"", "\n", "torch", ".", "set_num_threads", "(", "1", ")", "\n", "\n", "torch", ".", "manual_seed", "(", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "\n", "env", ",", "test_env", "=", "env_fn", "(", ")", ",", "env_fn", "(", ")", "\n", "N", "=", "len", "(", "env", ".", "observation_spaces", ")", "\n", "obs_dim", "=", "env", ".", "observation_spaces", "[", "0", "]", ".", "shape", "\n", "act_dim", "=", "env", ".", "action_spaces", "[", "0", "]", ".", "shape", "[", "0", "]", "\n", "\n", "# Action limit for clamping: critically, assumes all dimensions share the same bound!", "\n", "act_limit", "=", "env", ".", "action_spaces", "[", "0", "]", ".", "high", "[", "0", "]", "\n", "\n", "# Create actor-critic module and target networks", "\n", "ac", "=", "actor_critic", "(", "env", ".", "observation_spaces", ",", "env", ".", "action_spaces", ",", "common_actor", ",", "**", "ac_kwargs", ")", "\n", "ac_targ", "=", "deepcopy", "(", "ac", ")", "\n", "\n", "# if records exist, load weights", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"ac.pt\"", ")", ":", "\n", "            ", "ac", ".", "load_state_dict", "(", "torch", ".", "load", "(", "data_dir", "+", "\"ac.pt\"", ")", ")", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "data_dir", "+", "\"ac_targ.pt\"", ")", ":", "\n", "            ", "ac_targ", ".", "load_state_dict", "(", "torch", ".", "load", "(", "data_dir", "+", "\"ac_targ.pt\"", ")", ")", "\n", "\n", "# initial weight sync", "\n", "", "", "sync_weights", "(", "comm", ",", "ac", ".", "parameters", "(", ")", ")", "\n", "sync_weights", "(", "comm", ",", "ac_targ", ".", "parameters", "(", ")", ")", "\n", "\n", "# for param in ac.named_parameters():", "\n", "#     print(param)", "\n", "\n", "# Freeze target networks with respect to optimizers (only update via polyak averaging)", "\n", "for", "p", "in", "ac_targ", ".", "parameters", "(", ")", ":", "\n", "        ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "# Experience buffer", "\n", "", "replay_buffer", "=", "ReplayBuffer", "(", "N", ",", "obs_dim", "=", "obs_dim", ",", "act_dim", "=", "act_dim", ",", "size", "=", "replay_size", ")", "\n", "\n", "# Set up function for computing DDPG Q-loss", "\n", "def", "compute_loss_q", "(", "data", ")", ":", "\n", "        ", "o", ",", "a", ",", "r", ",", "o2", ",", "d", "=", "data", "[", "'obs'", "]", ",", "data", "[", "'act'", "]", ",", "data", "[", "'rew'", "]", ",", "data", "[", "'obs2'", "]", ",", "data", "[", "'done'", "]", "\n", "\n", "q", "=", "ac", ".", "q", "(", "o", ",", "a", ")", "\n", "\n", "# Bellman backup for Q function", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "q_pi_targ", "=", "ac_targ", ".", "q", "(", "o2", ",", "[", "ac_targ", ".", "pis", "[", "i", "]", "(", "o2", "[", "i", "]", ")", "for", "i", "in", "range", "(", "N", ")", "]", ")", "\n", "backup", "=", "r", "+", "gamma", "*", "(", "1", "-", "d", ")", "*", "q_pi_targ", "\n", "\n", "# MSE loss against Bellman backup", "\n", "", "loss_q", "=", "(", "(", "q", "-", "backup", ")", "**", "2", ")", ".", "mean", "(", ")", "\n", "\n", "# Useful info for logging", "\n", "loss_info", "=", "dict", "(", "QVals", "=", "q", ".", "detach", "(", ")", ".", "numpy", "(", ")", ")", "\n", "\n", "return", "loss_q", ",", "loss_info", "\n", "\n", "# Set up function for computing DDPG pi loss", "\n", "", "def", "compute_loss_pi", "(", "data", ")", ":", "\n", "        ", "o", "=", "data", "[", "'obs'", "]", "\n", "q_pi", "=", "ac", ".", "q", "(", "o", ",", "[", "ac", ".", "pis", "[", "i", "]", "(", "o", "[", "i", "]", ")", "for", "i", "in", "range", "(", "N", ")", "]", ")", "\n", "return", "-", "q_pi", ".", "mean", "(", ")", "\n", "\n", "# Set up optimizers for policy and q-function", "\n", "", "pi_optimizers", "=", "[", "Adam", "(", "p", ".", "parameters", "(", ")", ",", "lr", "=", "pi_lr", ")", "for", "p", "in", "ac", ".", "unique_pis", "]", "\n", "q_optimizer", "=", "Adam", "(", "ac", ".", "q", ".", "parameters", "(", ")", ",", "lr", "=", "q_lr", ")", "\n", "\n", "\n", "def", "update", "(", "data", ")", ":", "\n", "# First run one gradient descent step for Q.", "\n", "        ", "q_optimizer", ".", "zero_grad", "(", ")", "\n", "loss_q", ",", "loss_info", "=", "compute_loss_q", "(", "data", ")", "\n", "loss_q", ".", "backward", "(", ")", "\n", "sync_grads", "(", "comm", ",", "ac", ".", "q", ".", "parameters", "(", ")", ")", "\n", "q_optimizer", ".", "step", "(", ")", "\n", "\n", "# Freeze Q-network so you don't waste computational effort ", "\n", "# computing gradients for it during the policy learning step.", "\n", "for", "p", "in", "ac", ".", "q", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "False", "\n", "\n", "# Next run one gradient descent step for pi.", "\n", "", "[", "opt", ".", "zero_grad", "(", ")", "for", "opt", "in", "pi_optimizers", "]", "\n", "loss_pi", "=", "compute_loss_pi", "(", "data", ")", "\n", "loss_pi", ".", "backward", "(", ")", "\n", "[", "sync_grads", "(", "comm", ",", "p", ".", "parameters", "(", ")", ")", "for", "p", "in", "ac", ".", "unique_pis", "]", "\n", "[", "opt", ".", "step", "(", ")", "for", "opt", "in", "pi_optimizers", "]", "\n", "\n", "# Unfreeze Q-network so you can optimize it at next DDPG step.", "\n", "for", "p", "in", "ac", ".", "q", ".", "parameters", "(", ")", ":", "\n", "            ", "p", ".", "requires_grad", "=", "True", "\n", "\n", "# Finally, update target networks by polyak averaging.", "\n", "", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "p", ",", "p_targ", "in", "zip", "(", "ac", ".", "parameters", "(", ")", ",", "ac_targ", ".", "parameters", "(", ")", ")", ":", "\n", "# NB: We use an in-place operations \"mul_\", \"add_\" to update target", "\n", "# params, as opposed to \"mul\" and \"add\", which would make new tensors.", "\n", "                ", "p_targ", ".", "data", ".", "mul_", "(", "polyak", ")", "\n", "p_targ", ".", "data", ".", "add_", "(", "(", "1", "-", "polyak", ")", "*", "p", ".", "data", ")", "\n", "\n", "\n", "# sync weights", "\n", "", "", "sync_weights", "(", "comm", ",", "ac", ".", "q", ".", "parameters", "(", ")", ")", "\n", "[", "sync_weights", "(", "comm", ",", "pi", ".", "parameters", "(", ")", ")", "for", "pi", "in", "ac", ".", "unique_pis", "]", "\n", "sync_weights", "(", "comm", ",", "ac_targ", ".", "parameters", "(", ")", ")", "\n", "\n", "# save weights", "\n", "if", "policy_record", "is", "not", "None", ":", "\n", "            ", "torch", ".", "save", "(", "ac", ".", "state_dict", "(", ")", ",", "data_dir", "+", "\"ac.pt\"", ")", "\n", "torch", ".", "save", "(", "ac_targ", ".", "state_dict", "(", ")", ",", "data_dir", "+", "\"ac_targ.pt\"", ")", "\n", "\n", "\n", "", "", "def", "get_action", "(", "o", ",", "noise_scale", ")", ":", "\n", "        ", "a", "=", "ac", ".", "act", "(", "torch", ".", "as_tensor", "(", "o", ",", "dtype", "=", "torch", ".", "float32", ")", ")", "\n", "a", "+=", "noise_scale", "*", "np", ".", "random", ".", "randn", "(", "act_dim", ")", "\n", "return", "np", ".", "clip", "(", "a", ",", "-", "act_limit", ",", "act_limit", ")", "\n", "\n", "", "def", "test_agent", "(", ")", ":", "\n", "        ", "for", "j", "in", "range", "(", "num_test_episodes", ")", ":", "\n", "            ", "o", ",", "d", ",", "ep_ret", ",", "ep_len", "=", "test_env", ".", "reset", "(", ")", ",", "False", ",", "0", ",", "0", "\n", "while", "not", "(", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ")", ":", "\n", "# Take deterministic actions at test time (noise_scale=0)", "\n", "                ", "o", ",", "r", ",", "d", ",", "_", "=", "test_env", ".", "step", "(", "get_action", "(", "o", ",", "0", ")", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "", "logger", ".", "store", "(", "TestEpRet", "=", "ep_ret", ",", "TestEpLen", "=", "ep_len", ")", "\n", "\n", "# Prepare for interaction with environment", "\n", "", "", "total_steps", "=", "steps_per_epoch", "*", "epochs", "\n", "start_time", "=", "time", ".", "time", "(", ")", "\n", "o", ",", "ep_ret", ",", "ep_len", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", "\n", "\n", "# Main loop: collect experience in env and update/log each epoch", "\n", "for", "t", "in", "range", "(", "total_steps", ")", ":", "\n", "\n", "# Until start_steps have elapsed, randomly sample actions", "\n", "# from a uniform distribution for better exploration. Afterwards, ", "\n", "# use the learned policy (with some noise, via act_noise). ", "\n", "        ", "if", "eval_mode", ":", "\n", "            ", "a", "=", "get_action", "(", "o", ",", "0.0", ")", "\n", "", "else", ":", "\n", "            ", "if", "t", ">", "start_steps", ":", "\n", "                ", "a", "=", "get_action", "(", "o", ",", "act_noise", ")", "\n", "", "else", ":", "\n", "                ", "a", "=", "[", "asp", ".", "sample", "(", ")", "for", "asp", "in", "env", ".", "action_spaces", "]", "\n", "\n", "# Step the env", "\n", "", "", "o2", ",", "rs", ",", "d", ",", "_", "=", "env", ".", "step", "(", "a", ")", "\n", "r", "=", "sum", "(", "rs", ")", "\n", "ep_ret", "+=", "r", "\n", "ep_len", "+=", "1", "\n", "\n", "# Ignore the \"done\" signal if it comes from hitting the time", "\n", "# horizon (that is, when it's an artificial terminal signal", "\n", "# that isn't based on the agent's state)", "\n", "d", "=", "False", "if", "ep_len", "==", "max_ep_len", "else", "d", "\n", "\n", "# Store experience to replay buffer", "\n", "replay_buffer", ".", "store", "(", "o", ",", "a", ",", "r", ",", "o2", ",", "d", ")", "\n", "\n", "# Super critical, easy to overlook step: make sure to update ", "\n", "# most recent observation!", "\n", "o", "=", "o2", "\n", "\n", "# End of trajectory handling", "\n", "if", "d", "or", "(", "ep_len", "==", "max_ep_len", ")", ":", "\n", "            ", "o", ",", "ep_ret", ",", "ep_len", "=", "env", ".", "reset", "(", ")", ",", "0", ",", "0", "\n", "\n", "# Update handling", "\n", "", "if", "not", "eval_mode", ":", "\n", "            ", "if", "t", ">=", "update_after", "and", "t", "%", "update_every", "==", "0", ":", "\n", "                ", "for", "_", "in", "range", "(", "update_every", ")", ":", "\n", "                    ", "batch", "=", "replay_buffer", ".", "sample_batch", "(", "batch_size", ")", "\n", "update", "(", "data", "=", "batch", ")", "\n", "\n", "# End of epoch handling", "\n", "", "", "", "if", "(", "t", "+", "1", ")", "%", "steps_per_epoch", "==", "0", ":", "\n", "            ", "epoch", "=", "(", "t", "+", "1", ")", "//", "steps_per_epoch", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.single_agent_wrapper.__init__": [[12, 17], ["None"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "\n", "\t\t", "self", ".", "env", "=", "env", "\n", "self", ".", "observation_spaces", "=", "[", "env", ".", "observation_space", "]", "\n", "self", ".", "action_spaces", "=", "[", "env", ".", "action_space", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.single_agent_wrapper.reset": [[18, 20], ["single_agent_wrappers.single_agent_wrapper.env.reset"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\t\t", "return", "[", "self", ".", "env", ".", "reset", "(", ")", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.single_agent_wrapper.step": [[21, 31], ["single_agent_wrappers.single_agent_wrapper.prepare_action", "single_agent_wrappers.single_agent_wrapper.env.step"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.single_agent_wrapper.prepare_action", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "\t\t", "action", "=", "self", ".", "prepare_action", "(", "action", ")", "\n", "s", ",", "r", ",", "d", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n", "# for i in range(4):", "\n", "# \tdisp = s[:,:,i]", "\n", "# \tcv2.imshow(\"state\"+str(i), disp)", "\n", "# \tcv2.waitKey(1)", "\n", "#mpi_print(r)", "\n", "return", "[", "s", "]", ",", "[", "r", "]", ",", "d", ",", "[", "info", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.single_agent_wrapper.prepare_action": [[32, 40], ["numpy.asarray", "isinstance", "int", "numpy.squeeze", "len"], "methods", ["None"], ["", "def", "prepare_action", "(", "self", ",", "action", ")", ":", "\n", "\t\t", "action", "=", "np", ".", "asarray", "(", "action", ")", "\n", "if", "isinstance", "(", "self", ".", "env", ".", "action_space", ",", "Discrete", ")", ":", "\n", "\t\t\t", "action", "=", "int", "(", "np", ".", "squeeze", "(", "action", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t", "while", "len", "(", "action", ".", "shape", ")", ">", "1", ":", "\n", "\t\t\t\t", "action", "=", "action", "[", "0", "]", "\n", "", "", "return", "action", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.single_agent_wrapper.render": [[41, 43], ["single_agent_wrappers.single_agent_wrapper.env.render"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.render"], ["", "def", "render", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.single_agent_wrapper.close": [[44, 46], ["single_agent_wrappers.single_agent_wrapper.env.close"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.FrameStackWrapper.__init__": [[53, 68], ["list", "gym.spaces.Box", "numpy.ndarray.flatten", "numpy.ndarray.flatten", "tuple"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "env", ",", "stack", ")", ":", "\n", "\n", "\t\t", "self", ".", "depth", "=", "stack", "\n", "self", ".", "state_buffer", "=", "[", "]", "\n", "\n", "low", "=", "np", ".", "ndarray", ".", "flatten", "(", "env", ".", "observation_space", ".", "low", ")", "[", "0", "]", "\n", "high", "=", "np", ".", "ndarray", ".", "flatten", "(", "env", ".", "observation_space", ".", "high", ")", "[", "0", "]", "\n", "dtype", "=", "env", ".", "observation_space", ".", "dtype", "\n", "\n", "shape", "=", "list", "(", "env", ".", "observation_space", ".", "shape", ")", "\n", "shape", "[", "-", "1", "]", "*=", "self", ".", "depth", "\n", "self", ".", "observation_space", "=", "Box", "(", "low", ",", "high", ",", "shape", "=", "tuple", "(", "shape", ")", ",", "dtype", "=", "dtype", ")", "\n", "self", ".", "action_space", "=", "env", ".", "action_space", "\n", "\n", "self", ".", "env", "=", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.FrameStackWrapper.reset": [[69, 75], ["single_agent_wrappers.FrameStackWrapper.env.reset", "single_agent_wrappers.FrameStackWrapper.compile_buffer", "len", "single_agent_wrappers.FrameStackWrapper.state_buffer.append"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset", "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.FrameStackWrapper.compile_buffer"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\t\t", "s", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "self", ".", "state_buffer", "=", "[", "]", "\n", "while", "len", "(", "self", ".", "state_buffer", ")", "<", "self", ".", "depth", ":", "\n", "\t\t\t", "self", ".", "state_buffer", ".", "append", "(", "s", ")", "\n", "", "return", "self", ".", "compile_buffer", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.FrameStackWrapper.step": [[76, 81], ["single_agent_wrappers.FrameStackWrapper.env.step", "single_agent_wrappers.FrameStackWrapper.state_buffer.append", "single_agent_wrappers.FrameStackWrapper.state_buffer.pop", "single_agent_wrappers.FrameStackWrapper.compile_buffer"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.FrameStackWrapper.compile_buffer"], ["", "def", "step", "(", "self", ",", "a", ")", ":", "\n", "\t\t", "ns", ",", "r", ",", "d", ",", "info", "=", "self", ".", "env", ".", "step", "(", "a", ")", "\n", "self", ".", "state_buffer", ".", "append", "(", "ns", ")", "\n", "self", ".", "state_buffer", ".", "pop", "(", "0", ")", "\n", "return", "self", ".", "compile_buffer", "(", ")", ",", "r", ",", "d", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.FrameStackWrapper.render": [[82, 84], ["single_agent_wrappers.FrameStackWrapper.env.render"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.render"], ["", "def", "render", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.FrameStackWrapper.close": [[85, 87], ["single_agent_wrappers.FrameStackWrapper.env.close"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.FrameStackWrapper.compile_buffer": [[88, 95], ["len", "numpy.asarray", "numpy.transpose", "numpy.concatenate", "numpy.asarray", "numpy.asarray"], "methods", ["None"], ["", "def", "compile_buffer", "(", "self", ")", ":", "\n", "\t\t", "if", "len", "(", "np", ".", "asarray", "(", "self", ".", "state_buffer", ")", ".", "shape", ")", "==", "3", ":", "\n", "\t\t\t", "final_state", "=", "np", ".", "asarray", "(", "self", ".", "state_buffer", ")", "\n", "final_state", "=", "np", ".", "transpose", "(", "final_state", ",", "(", "1", ",", "2", ",", "0", ")", ")", "\n", "return", "final_state", "\n", "", "else", ":", "\n", "\t\t\t", "return", "np", ".", "concatenate", "(", "[", "np", ".", "asarray", "(", "s", ")", "for", "s", "in", "self", ".", "state_buffer", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.GrayscaleWrapper.__init__": [[99, 110], ["list", "gym.spaces.Box", "numpy.ndarray.flatten", "numpy.ndarray.flatten", "tuple"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "env", ")", ":", "\n", "\t\t", "self", ".", "env", "=", "env", "\n", "\n", "low", "=", "np", ".", "ndarray", ".", "flatten", "(", "env", ".", "observation_space", ".", "low", ")", "[", "0", "]", "\n", "high", "=", "np", ".", "ndarray", ".", "flatten", "(", "env", ".", "observation_space", ".", "high", ")", "[", "0", "]", "\n", "dtype", "=", "env", ".", "observation_space", ".", "dtype", "\n", "\n", "shape", "=", "list", "(", "env", ".", "observation_space", ".", "shape", ")", "\n", "shape", "[", "-", "1", "]", "=", "1", "\n", "self", ".", "observation_space", "=", "Box", "(", "low", ",", "high", ",", "shape", "=", "tuple", "(", "shape", ")", ",", "dtype", "=", "dtype", ")", "\n", "self", ".", "action_space", "=", "env", ".", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.GrayscaleWrapper.reset": [[111, 113], ["single_agent_wrappers.GrayscaleWrapper.process_state", "single_agent_wrappers.GrayscaleWrapper.env.reset"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.process_state", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "process_state", "(", "self", ".", "env", ".", "reset", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.GrayscaleWrapper.step": [[114, 117], ["single_agent_wrappers.GrayscaleWrapper.env.step", "single_agent_wrappers.GrayscaleWrapper.process_state"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.process_state"], ["", "def", "step", "(", "self", ",", "a", ")", ":", "\n", "\t\t", "ns", ",", "r", ",", "d", ",", "info", "=", "self", ".", "env", ".", "step", "(", "a", ")", "\n", "return", "self", ".", "process_state", "(", "ns", ")", ",", "r", ",", "d", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.GrayscaleWrapper.render": [[118, 120], ["single_agent_wrappers.GrayscaleWrapper.env.render"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.render"], ["", "def", "render", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.GrayscaleWrapper.close": [[121, 123], ["single_agent_wrappers.GrayscaleWrapper.env.close"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.GrayscaleWrapper.process_state": [[124, 126], ["cv2.cvtColor().astype", "cv2.cvtColor"], "methods", ["None"], ["", "def", "process_state", "(", "self", ",", "s", ")", ":", "\n", "\t\t", "return", "cv2", ".", "cvtColor", "(", "s", ",", "cv2", ".", "COLOR_BGR2GRAY", ")", ".", "astype", "(", "np", ".", "float32", ")", "/", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.__init__": [[130, 144], ["list", "range", "gym.spaces.Box", "numpy.ndarray.flatten", "numpy.ndarray.flatten", "len", "tuple"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "env", ",", "square_size", ")", ":", "\n", "\t\t", "self", ".", "env", "=", "env", "\n", "self", ".", "sz", "=", "square_size", "\n", "\n", "low", "=", "np", ".", "ndarray", ".", "flatten", "(", "env", ".", "observation_space", ".", "low", ")", "[", "0", "]", "\n", "high", "=", "np", ".", "ndarray", ".", "flatten", "(", "env", ".", "observation_space", ".", "high", ")", "[", "0", "]", "\n", "dtype", "=", "env", ".", "observation_space", ".", "dtype", "\n", "\n", "shape", "=", "list", "(", "env", ".", "observation_space", ".", "shape", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "shape", ")", "-", "1", ")", ":", "\n", "\t\t\t", "shape", "[", "i", "]", "=", "square_size", "\n", "", "shape", "[", "-", "1", "]", "=", "1", "\n", "self", ".", "observation_space", "=", "Box", "(", "low", ",", "high", ",", "shape", "=", "tuple", "(", "shape", ")", ",", "dtype", "=", "dtype", ")", "\n", "self", ".", "action_space", "=", "env", ".", "action_space", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.reset": [[145, 147], ["single_agent_wrappers.ImageDownsizeWrapper.process_state", "single_agent_wrappers.ImageDownsizeWrapper.env.reset"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.process_state", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\t\t", "return", "self", ".", "process_state", "(", "self", ".", "env", ".", "reset", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.step": [[148, 151], ["single_agent_wrappers.ImageDownsizeWrapper.env.step", "single_agent_wrappers.ImageDownsizeWrapper.process_state"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.process_state"], ["", "def", "step", "(", "self", ",", "a", ")", ":", "\n", "\t\t", "ns", ",", "r", ",", "d", ",", "info", "=", "self", ".", "env", ".", "step", "(", "a", ")", "\n", "return", "self", ".", "process_state", "(", "ns", ")", ",", "r", ",", "d", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.render": [[152, 154], ["single_agent_wrappers.ImageDownsizeWrapper.env.render"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.render"], ["", "def", "render", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.close": [[155, 157], ["single_agent_wrappers.ImageDownsizeWrapper.env.close"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.single_agent_wrappers.ImageDownsizeWrapper.process_state": [[158, 160], ["cv2.resize"], "methods", ["None"], ["", "def", "process_state", "(", "self", ",", "s", ")", ":", "\n", "\t\t", "return", "cv2", ".", "resize", "(", "s", ",", "(", "self", ".", "sz", ",", "self", ".", "sz", ")", ")", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.__init__": [[7, 30], ["len"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "proxy_env", ",", "policy_comms", ",", "policy_record", "=", "None", ",", "save_every", "=", "20", ",", "sum_over", "=", "False", ",", "channel", "=", "\"main\"", ")", ":", "\n", "\n", "        ", "self", ".", "multiagent", "=", "proxy_env", ".", "is_multiagent", "\n", "if", "self", ".", "multiagent", ":", "\n", "            ", "self", ".", "num_agents", "=", "len", "(", "proxy_env", ".", "entity_idxs", ")", "\n", "self", ".", "observation_spaces", "=", "proxy_env", ".", "observation_spaces", "\n", "self", ".", "action_spaces", "=", "proxy_env", ".", "action_spaces", "\n", "", "else", ":", "\n", "            ", "self", ".", "num_agents", "=", "1", "\n", "self", ".", "observation_space", "=", "proxy_env", ".", "observation_space", "\n", "self", ".", "action_space", "=", "proxy_env", ".", "action_space", "\n", "\n", "", "self", ".", "sum_over", "=", "sum_over", "\n", "\n", "self", ".", "env", "=", "proxy_env", "\n", "self", ".", "comm", "=", "policy_comms", "\n", "self", ".", "ep_len", "=", "(", "[", "0.0", "]", "*", "self", ".", "num_agents", "if", "not", "self", ".", "sum_over", "else", "[", "0.0", "]", ")", "\n", "self", ".", "ep_rew", "=", "(", "[", "0.0", "]", "*", "self", ".", "num_agents", "if", "not", "self", ".", "sum_over", "else", "[", "0.0", "]", ")", "\n", "self", ".", "policy_record", "=", "policy_record", "\n", "self", ".", "channel", "=", "channel", "\n", "\n", "self", ".", "save_every", "=", "save_every", "\n", "self", ".", "episodes_until_save", "=", "self", ".", "save_every", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.reset": [[31, 35], ["mpi_logging_wrappers.MPISynchronizedPRUpdater.env.reset"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "reset", "(", "self", ")", ":", "\n", "        ", "self", ".", "ep_len", "=", "(", "[", "0.0", "]", "*", "self", ".", "num_agents", "if", "not", "self", ".", "sum_over", "else", "[", "0.0", "]", ")", "\n", "self", ".", "ep_rew", "=", "(", "[", "0.0", "]", "*", "self", ".", "num_agents", "if", "not", "self", ".", "sum_over", "else", "[", "0.0", "]", ")", "\n", "return", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.step": [[36, 72], ["mpi_logging_wrappers.MPISynchronizedPRUpdater.env.step", "mpi_logging_wrappers.MPISynchronizedPRUpdater.comm.allgather", "map", "zip", "range", "len", "sum", "range", "len", "mpi_logging_wrappers.MPISynchronizedPRUpdater.policy_record.add_result", "mpi_logging_wrappers.MPISynchronizedPRUpdater.policy_record.save", "len"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.add_result", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "        ", "s", ",", "r", ",", "d", ",", "info", "=", "self", ".", "env", ".", "step", "(", "action", ")", "\n", "\n", "#synchronize results ---------------------------------", "\n", "if", "self", ".", "multiagent", ":", "\n", "            ", "if", "self", ".", "sum_over", ":", "\n", "                ", "self", ".", "ep_len", "[", "0", "]", "+=", "1.0", "\n", "self", ".", "ep_rew", "[", "0", "]", "+=", "sum", "(", "r", ")", "\n", "", "else", ":", "\n", "                ", "for", "i", "in", "range", "(", "len", "(", "r", ")", ")", ":", "\n", "                    ", "self", ".", "ep_len", "[", "i", "]", "+=", "1.0", "\n", "self", ".", "ep_rew", "[", "i", "]", "+=", "r", "[", "i", "]", "\n", "", "", "", "else", ":", "\n", "            ", "self", ".", "ep_len", "[", "0", "]", "+=", "1.0", "\n", "self", ".", "ep_rew", "[", "0", "]", "+=", "r", "\n", "\n", "", "if", "d", ":", "\n", "            ", "lrlocal", "=", "(", "self", ".", "ep_len", ",", "self", ".", "ep_rew", ")", "\n", "", "else", ":", "\n", "            ", "lrlocal", "=", "(", "[", "]", ",", "[", "]", ")", "\n", "\n", "", "listoflrpairs", "=", "self", ".", "comm", ".", "allgather", "(", "lrlocal", ")", "\n", "lens", ",", "rews", "=", "map", "(", "self", ".", "flatten_lists", ",", "zip", "(", "*", "listoflrpairs", ")", ")", "\n", "\n", "if", "self", ".", "policy_record", "is", "not", "None", ":", "\n", "            ", "for", "idx", "in", "range", "(", "len", "(", "lens", ")", ")", ":", "\n", "                ", "self", ".", "policy_record", ".", "add_result", "(", "rews", "[", "idx", "]", ",", "lens", "[", "idx", "]", ",", "channel", "=", "self", ".", "channel", ")", "\n", "\n", "", "self", ".", "episodes_until_save", "-=", "len", "(", "lens", ")", "\n", "if", "self", ".", "episodes_until_save", "<=", "0", ":", "\n", "                ", "self", ".", "policy_record", ".", "save", "(", ")", "\n", "self", ".", "episodes_until_save", "=", "self", ".", "save_every", "\n", "\n", "# -----------------------------------------------------", "\n", "\n", "", "", "return", "s", ",", "r", ",", "d", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.render": [[73, 75], ["mpi_logging_wrappers.MPISynchronizedPRUpdater.env.render"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.render"], ["", "def", "render", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "render", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.close": [[76, 78], ["mpi_logging_wrappers.MPISynchronizedPRUpdater.env.close"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.flatten_lists": [[80, 87], ["None"], "methods", ["None"], ["", "def", "flatten_lists", "(", "self", ",", "listoflists", ")", ":", "\n", "        ", "\"\"\"\n        Flatten a python list of list\n        :param listoflists: (list(list))\n        :return: (list)\n        \"\"\"", "\n", "return", "[", "el", "for", "list_", "in", "listoflists", "for", "el", "in", "list_", "]", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.plot_utils.plot_policy_records": [[9, 93], ["hasattr", "enumerate", "ax.set_ylabel", "default_colors.append", "matplotlib.subplots", "enumerate", "ax.set_xlabel", "ax.set_xlabel", "filename.replace.replace", "filename.replace.replace", "matplotlib.savefig", "matplotlib.close", "plot_utils.randomRGBPure", "colors.append", "offsets.append", "range", "range", "ax.plot", "len", "len", "len", "rolling_avg_buffer.append", "float", "len", "rolling_avg_buffer.pop", "rolling_avg_results.append", "numpy.mean", "rolling_avg_steps.append", "rolling_avg_steps.append"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.close", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.plot_utils.randomRGBPure"], ["def", "plot_policy_records", "(", "records", ",", "windows", ",", "alphas", ",", "filename", ",", "colors", "=", "None", ",", "offsets", "=", "None", ",", "\n", "episodic", "=", "False", ",", "fig", "=", "None", ",", "ax", "=", "None", ",", "return_figure", "=", "False", ")", ":", "\n", "\n", "# get the main channels if these are record objects", "\n", "\t", "if", "hasattr", "(", "records", "[", "0", "]", ",", "\"channels\"", ")", ":", "\n", "\t\t", "records", "=", "[", "r", ".", "channels", "[", "\"main\"", "]", "for", "r", "in", "records", "]", "\n", "\n", "", "default_colors", "=", "[", "]", "\n", "hues", "=", "[", "1", ",", "4", ",", "7", ",", "10", ",", "3", ",", "6", ",", "9", ",", "12", ",", "2", ",", "5", ",", "8", ",", "11", "]", "\n", "for", "h", "in", "hues", ":", "\n", "\t\t", "default_colors", ".", "append", "(", "randomRGBPure", "(", "float", "(", "h", ")", "/", "12.0", ")", ")", "\n", "\n", "", "if", "colors", "is", "None", ":", "\n", "\t\t", "colors", "=", "[", "]", "\n", "idx", "=", "0", "\n", "for", "pr", "in", "records", ":", "\n", "\t\t\t", "colors", ".", "append", "(", "default_colors", "[", "idx", "]", ")", "\n", "idx", "+=", "1", "\n", "if", "idx", ">=", "len", "(", "default_colors", ")", ":", "\n", "\t\t\t\t", "idx", "=", "0", "\n", "\n", "", "", "", "if", "offsets", "is", "None", ":", "\n", "\t\t", "offsets", "=", "[", "]", "\n", "for", "pr", "in", "records", ":", "\n", "\t\t\t", "offsets", ".", "append", "(", "0", ")", "\n", "\n", "", "", "if", "fig", "is", "None", ":", "\n", "\t\t", "fig", ",", "ax", "=", "plt", ".", "subplots", "(", "1", ",", "1", ",", "figsize", "=", "(", "12", ",", "6", ")", ")", "\n", "\n", "", "for", "widx", ",", "windowsize", "in", "enumerate", "(", "windows", ")", ":", "\n", "\n", "\t\t", "alpha", "=", "alphas", "[", "widx", "]", "\n", "\n", "for", "pridx", ",", "pol", "in", "enumerate", "(", "records", ")", ":", "\n", "\n", "\t\t\t", "ylabel", "=", "pol", ".", "ylabel", "\n", "\n", "steps", "=", "pol", ".", "ep_cumlens", "\n", "results", "=", "pol", ".", "ep_results", "\n", "\n", "for", "k", "in", "range", "(", "len", "(", "steps", ")", ")", ":", "\n", "\t\t\t\t", "steps", "[", "k", "]", "+=", "offsets", "[", "pridx", "]", "\n", "\n", "", "rolling_avg_buffer", "=", "[", "]", "\n", "rolling_avg_results", "=", "[", "]", "\n", "rolling_avg_steps", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "steps", ")", ")", ":", "\n", "\t\t\t\t", "rolling_avg_buffer", ".", "append", "(", "results", "[", "i", "]", ")", "\n", "\n", "if", "len", "(", "rolling_avg_buffer", ")", ">", "windowsize", ":", "\n", "\t\t\t\t\t", "rolling_avg_buffer", ".", "pop", "(", "0", ")", "\n", "\n", "rolling_avg_results", ".", "append", "(", "np", ".", "mean", "(", "rolling_avg_buffer", ")", ")", "\n", "\n", "if", "episodic", ":", "\n", "\t\t\t\t\t\t", "rolling_avg_steps", ".", "append", "(", "i", "+", "1", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "rolling_avg_steps", ".", "append", "(", "steps", "[", "i", "]", ")", "\n", "\n", "#plot", "\n", "", "", "", "ax", ".", "plot", "(", "rolling_avg_steps", ",", "rolling_avg_results", ",", "color", "=", "colors", "[", "pridx", "]", ",", "alpha", "=", "alpha", ")", "\n", "\n", "", "", "if", "episodic", ":", "\n", "\t\t", "ax", ".", "set_xlabel", "(", "\"Episodes\"", ")", "\n", "", "else", ":", "\n", "\t\t", "ax", ".", "set_xlabel", "(", "\"Steps\"", ")", "\n", "", "ax", ".", "set_ylabel", "(", "ylabel", ")", "\n", "\n", "# change .png to _steps.png or _episodes.png", "\n", "if", "\".png\"", "not", "in", "filename", ":", "\n", "\t\t", "filename", "=", "filename", "+", "\".png\"", "\n", "\n", "", "if", "episodic", ":", "\n", "\t\t", "filename", "=", "filename", ".", "replace", "(", "\".png\"", ",", "\"_episodes.png\"", ")", "\n", "", "else", ":", "\n", "\t\t", "filename", "=", "filename", ".", "replace", "(", "\".png\"", ",", "\"_steps.png\"", ")", "\n", "\n", "", "if", "return_figure", ":", "\n", "\t\t", "return", "fig", ",", "ax", "\n", "", "else", ":", "\n", "\t\t", "plt", ".", "savefig", "(", "filename", ")", "\n", "#clear the figure so we dont plot on top of other plots", "\n", "plt", ".", "close", "(", "fig", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.plot_utils.randomRGBPure": [[95, 101], ["random.uniform", "random.uniform", "colorsys.hsv_to_rgb", "random.random"], "function", ["None"], ["", "", "def", "randomRGBPure", "(", "hue", "=", "None", ")", ":", "\n", "    ", "h", "=", "random", ".", "random", "(", ")", "if", "hue", "is", "None", "else", "hue", "\n", "s", "=", "random", ".", "uniform", "(", "0.8", ",", "0.9", ")", "\n", "v", "=", "random", ".", "uniform", "(", "0.8", ",", "0.9", ")", "\n", "r", ",", "g", ",", "b", "=", "colorsys", ".", "hsv_to_rgb", "(", "h", ",", "s", ",", "v", ")", "\n", "return", "(", "r", ",", "g", ",", "b", ")", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.RecordChannel.__init__": [[14, 26], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "data_dir", ",", "name", "=", "\"main\"", ",", "color", "=", "\"#eb0033\"", ",", "ylabel", "=", "\"Episodic Reward\"", ",", "\n", "windows", "=", "[", "20", ",", "50", ",", "100", "]", ",", "alphas", "=", "[", "0.1", ",", "0.3", ",", "1.0", "]", ")", ":", "\n", "\n", "        ", "self", ".", "data_dir", "=", "data_dir", "\n", "self", ".", "ep_results", "=", "[", "]", "\n", "self", ".", "ep_lengths", "=", "[", "]", "\n", "self", ".", "ep_cumlens", "=", "[", "]", "\n", "self", ".", "color", "=", "color", "\n", "self", ".", "name", "=", "name", "\n", "self", ".", "ylabel", "=", "ylabel", "\n", "self", ".", "windows", "=", "windows", "\n", "self", ".", "alphas", "=", "alphas", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.RecordChannel.save": [[27, 38], ["pickle.dump", "plot_policy_records", "plot_policy_records", "open"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.plot_utils.plot_policy_records", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.plot_utils.plot_policy_records"], ["", "def", "save", "(", "self", ")", ":", "\n", "        ", "data", "=", "[", "self", ".", "ep_results", ",", "self", ".", "ep_lengths", ",", "self", ".", "ep_cumlens", "]", "\n", "pickle", ".", "dump", "(", "data", ",", "open", "(", "self", ".", "data_dir", "+", "\"policy_record_\"", "+", "self", ".", "name", "+", "\".p\"", ",", "\"wb\"", ")", ")", "\n", "\n", "#save a plot also", "\n", "plot_policy_records", "(", "[", "self", "]", ",", "self", ".", "windows", ",", "self", ".", "alphas", ",", "\n", "self", ".", "data_dir", "+", "\"plot_\"", "+", "self", ".", "name", "+", "\".png\"", ",", "colors", "=", "[", "self", ".", "color", "]", ",", "\n", "episodic", "=", "False", ")", "\n", "plot_policy_records", "(", "[", "self", "]", ",", "self", ".", "windows", ",", "self", ".", "alphas", ",", "\n", "self", ".", "data_dir", "+", "\"plot_\"", "+", "self", ".", "name", "+", "\".png\"", ",", "colors", "=", "[", "self", ".", "color", "]", ",", "\n", "episodic", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.RecordChannel.load": [[39, 44], ["os.path.exists", "pickle.load", "open"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load"], ["", "def", "load", "(", "self", ")", ":", "\n", "        ", "path", "=", "self", ".", "data_dir", "+", "\"policy_record_\"", "+", "self", ".", "name", "+", "\".p\"", "\n", "if", "os", ".", "path", ".", "exists", "(", "path", ")", ":", "\n", "            ", "data", "=", "pickle", ".", "load", "(", "open", "(", "path", ",", "\"rb\"", ")", ")", "\n", "self", ".", "ep_results", ",", "self", ".", "ep_lengths", ",", "self", ".", "ep_cumlens", "=", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.RecordChannel.get_copy": [[46, 58], ["policy_record.RecordChannel"], "methods", ["None"], ["", "", "def", "get_copy", "(", "self", ")", ":", "\n", "        ", "rc", "=", "RecordChannel", "(", "self", ".", "data_dir", ",", "\n", "name", "=", "self", ".", "name", ",", "\n", "color", "=", "self", ".", "color", ",", "\n", "ylabel", "=", "self", ".", "ylabel", ",", "\n", "windows", "=", "self", ".", "windows", ",", "\n", "alphas", "=", "self", ".", "alphas", ")", "\n", "\n", "rc", ".", "ep_results", "=", "self", ".", "ep_results", "[", ":", "]", "\n", "rc", ".", "ep_lengths", "=", "self", ".", "ep_lengths", "[", ":", "]", "\n", "rc", ".", "ep_cumlens", "=", "self", ".", "ep_cumlens", "[", ":", "]", "\n", "return", "rc", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.__init__": [[63, 78], ["policy_record.get_dir_for_policy", "policy_record.RecordChannel", "os.path.exists", "os.makedirs", "policy_record.PolicyRecord.load"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.get_dir_for_policy", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load"], ["    ", "def", "__init__", "(", "self", ",", "policy_id", ",", "log_comms_dir", ",", "plot_color", "=", "\"#eb0033\"", ")", ":", "\n", "\n", "        ", "self", ".", "plot_color", "=", "plot_color", "\n", "\n", "self", ".", "log_comms_dir", "=", "log_comms_dir", "\n", "self", ".", "data_dir", "=", "get_dir_for_policy", "(", "policy_id", ",", "log_comms_dir", ")", "\n", "\n", "self", ".", "channels", "=", "{", "\n", "\"main\"", ":", "RecordChannel", "(", "self", ".", "data_dir", ",", "name", "=", "\"main\"", ")", "\n", "}", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "self", ".", "data_dir", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "self", ".", "data_dir", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "load", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.add_result": [[80, 93], ["ch.ep_results.append", "ch.ep_lengths.append", "policy_record.PolicyRecord.add_channel", "len", "ch.ep_cumlens.append", "ch.ep_cumlens.append"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.add_channel"], ["", "", "def", "add_result", "(", "self", ",", "total_reward", ",", "ep_len", ",", "channel", "=", "\"main\"", ")", ":", "\n", "\n", "        ", "if", "channel", "not", "in", "self", ".", "channels", ":", "\n", "            ", "self", ".", "add_channel", "(", "channel", ")", "\n", "\n", "", "ch", "=", "self", ".", "channels", "[", "channel", "]", "\n", "\n", "ch", ".", "ep_results", ".", "append", "(", "total_reward", ")", "\n", "ch", ".", "ep_lengths", ".", "append", "(", "ep_len", ")", "\n", "if", "len", "(", "ch", ".", "ep_cumlens", ")", "==", "0", ":", "\n", "            ", "ch", ".", "ep_cumlens", ".", "append", "(", "ep_len", ")", "\n", "", "else", ":", "\n", "            ", "ch", ".", "ep_cumlens", ".", "append", "(", "ch", ".", "ep_cumlens", "[", "-", "1", "]", "+", "ep_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.add_channel": [[95, 101], ["policy_record.RecordChannel", "policy_record.RecordChannel.load"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load"], ["", "", "def", "add_channel", "(", "self", ",", "channel_name", ",", "**", "kwargs", ")", ":", "\n", "\n", "        ", "if", "channel_name", "not", "in", "self", ".", "channels", ":", "\n", "            ", "ch", "=", "RecordChannel", "(", "self", ".", "data_dir", ",", "name", "=", "channel_name", ",", "**", "kwargs", ")", "\n", "ch", ".", "load", "(", ")", "\n", "self", ".", "channels", "[", "channel_name", "]", "=", "ch", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save": [[103, 106], ["policy_record.PolicyRecord.channels[].save"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.save"], ["", "", "def", "save", "(", "self", ")", ":", "\n", "        ", "for", "ch", "in", "self", ".", "channels", ":", "\n", "            ", "self", ".", "channels", "[", "ch", "]", ".", "save", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load": [[108, 111], ["policy_record.PolicyRecord.channels[].load"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load"], ["", "", "def", "load", "(", "self", ")", ":", "\n", "        ", "for", "ch", "in", "self", ".", "channels", ":", "\n", "            ", "self", ".", "channels", "[", "ch", "]", ".", "load", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.fork": [[113, 130], ["policy_record.PolicyRecord", "os.listdir", "policy_record.PolicyRecord.channels[].get_copy", "shutil.copy"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.RecordChannel.get_copy"], ["", "", "def", "fork", "(", "self", ",", "new_id", ")", ":", "\n", "        ", "new_pr", "=", "PolicyRecord", "(", "new_id", ",", "self", ".", "log_comms_dir", ")", "\n", "\n", "for", "ch", "in", "self", ".", "channels", ":", "\n", "            ", "new_ch", "=", "self", ".", "channels", "[", "ch", "]", ".", "get_copy", "(", ")", "\n", "new_pr", ".", "channels", "[", "ch", "]", "=", "new_ch", "\n", "\n", "# copy files over from existing log dir", "\n", "", "for", "f", "in", "os", ".", "listdir", "(", "self", ".", "data_dir", ")", ":", "\n", "\n", "# HACK: check for f not having any extension", "\n", "            ", "if", "\".\"", "not", "in", "f", ":", "\n", "                ", "continue", "\n", "\n", "", "copy", "(", "self", ".", "data_dir", "+", "f", ",", "new_pr", ".", "data_dir", ")", "\n", "\n", "", "return", "new_pr", "\n", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.get_dir_for_policy": [[9, 11], ["str"], "function", ["None"], ["def", "get_dir_for_policy", "(", "policy_id", ",", "log_comms_dir", ")", ":", "\n", "    ", "return", "log_comms_dir", "+", "\"policy_\"", "+", "str", "(", "policy_id", ")", "+", "\"/\"", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.gym_proxy_env.__init__": [[27, 34], ["None"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "entity_idx", ",", "obs_space", ",", "act_space", ",", "match_comm", ",", "match_root_rank", ")", ":", "\n", "\t\t", "self", ".", "observation_space", "=", "obs_space", "\n", "self", ".", "action_space", "=", "act_space", "\n", "self", ".", "comm", "=", "match_comm", "\n", "self", ".", "match_root_rank", "=", "match_root_rank", "\n", "self", ".", "entity_idx", "=", "entity_idx", "\n", "self", ".", "is_multiagent", "=", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.gym_proxy_env.seed": [[35, 37], ["None"], "methods", ["None"], ["", "def", "seed", "(", "self", ",", "sd", ")", ":", "\n", "\t\t", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.gym_proxy_env.reset": [[38, 42], ["proxy_env.gym_proxy_env.comm.bcast"], "methods", ["None"], ["", "def", "reset", "(", "self", ")", ":", "\n", "\t\t", "states", "=", "self", ".", "comm", ".", "bcast", "(", "None", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "state", "=", "states", "[", "self", ".", "entity_idx", "]", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.gym_proxy_env.step": [[43, 67], ["numpy.asarray", "numpy.squeeze.tolist", "proxy_env.gym_proxy_env.comm.gather", "proxy_env.gym_proxy_env.comm.bcast", "len", "numpy.expand_dims", "len", "numpy.squeeze"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "action", ")", ":", "\n", "\n", "#make np array if it is not already", "\n", "\t\t", "action", "=", "np", ".", "asarray", "(", "[", "action", "]", ")", "\n", "\n", "#make sure action is a 1D array", "\n", "while", "len", "(", "action", ".", "shape", ")", "<", "1", ":", "\n", "\t\t\t", "action", "=", "np", ".", "expand_dims", "(", "action", ",", "-", "1", ")", "\n", "", "while", "len", "(", "action", ".", "shape", ")", ">", "1", ":", "\n", "\t\t\t", "action", "=", "np", ".", "squeeze", "(", "action", ")", "\n", "\n", "#convert to list", "\n", "", "action", "=", "action", ".", "tolist", "(", ")", "\n", "\n", "#send actions to main env proc", "\n", "action_packet", "=", "[", "[", "self", ".", "entity_idx", "]", ",", "[", "action", "]", "]", "\n", "self", ".", "comm", ".", "gather", "(", "action_packet", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "\n", "#get resulting info", "\n", "result", "=", "self", ".", "comm", ".", "bcast", "(", "None", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "nss", ",", "rs", ",", "done", ",", "infos", "=", "result", "\n", "ns", ",", "r", ",", "info", "=", "nss", "[", "self", ".", "entity_idx", "]", ",", "rs", "[", "self", ".", "entity_idx", "]", ",", "infos", "[", "self", ".", "entity_idx", "]", "\n", "\n", "return", "ns", ",", "r", ",", "done", ",", "info", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.__init__": [[71, 78], ["None"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "entity_idxs", ",", "obs_spaces", ",", "act_spaces", ",", "match_comm", ",", "match_root_rank", ")", ":", "\n", "\t\t", "self", ".", "observation_spaces", "=", "obs_spaces", "\n", "self", ".", "action_spaces", "=", "act_spaces", "\n", "self", ".", "comm", "=", "match_comm", "\n", "self", ".", "match_root_rank", "=", "match_root_rank", "\n", "self", ".", "entity_idxs", "=", "entity_idxs", "\n", "self", ".", "is_multiagent", "=", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.seed": [[79, 81], ["None"], "methods", ["None"], ["", "def", "seed", "(", "self", ",", "sd", ")", ":", "\n", "\t\t", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset": [[82, 88], ["proxy_env.ma_proxy_env.comm.bcast", "ret_states.append"], "methods", ["None"], ["", "def", "reset", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "\t\t", "states", "=", "self", ".", "comm", ".", "bcast", "(", "None", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "ret_states", "=", "[", "]", "\n", "for", "idx", "in", "self", ".", "entity_idxs", ":", "\n", "\t\t\t", "ret_states", ".", "append", "(", "states", "[", "idx", "]", ")", "\n", "", "return", "ret_states", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step": [[89, 127], ["numpy.asarray().tolist", "proxy_env.ma_proxy_env.comm.gather", "proxy_env.ma_proxy_env.comm.bcast", "numpy.asarray", "numpy.squeeze.tolist", "fmtactions.append", "next_states.append", "rewards.append", "infos.append", "numpy.asarray", "len", "numpy.expand_dims", "len", "numpy.squeeze"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "actions", ")", ":", "\n", "\n", "#assume this contains properly formatted actions", "\n", "#and that it indeed contains more than one action", "\n", "#convert to a list in case it is a numpy array", "\n", "\t\t", "actions", "=", "np", ".", "asarray", "(", "actions", ")", ".", "tolist", "(", ")", "\n", "fmtactions", "=", "[", "]", "\n", "for", "a", "in", "actions", ":", "\n", "#make np array if it is not already", "\n", "\t\t\t", "action", "=", "np", ".", "asarray", "(", "[", "a", "]", ")", "\n", "\n", "#make sure action is a 1D array", "\n", "while", "len", "(", "action", ".", "shape", ")", "<", "1", ":", "\n", "\t\t\t\t", "action", "=", "np", ".", "expand_dims", "(", "action", ",", "-", "1", ")", "\n", "", "while", "len", "(", "action", ".", "shape", ")", ">", "1", ":", "\n", "\t\t\t\t", "action", "=", "np", ".", "squeeze", "(", "action", ")", "\n", "\n", "#convert to list", "\n", "", "action", "=", "action", ".", "tolist", "(", ")", "\n", "fmtactions", ".", "append", "(", "action", ")", "\n", "\n", "#send actions to main env proc", "\n", "", "action_packet", "=", "[", "self", ".", "entity_idxs", ",", "fmtactions", "]", "\n", "self", ".", "comm", ".", "gather", "(", "action_packet", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "\n", "#get resulting info", "\n", "result", "=", "self", ".", "comm", ".", "bcast", "(", "None", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "nss", ",", "rs", ",", "done", ",", "infs", "=", "result", "\n", "\n", "next_states", "=", "[", "]", "\n", "rewards", "=", "[", "]", "\n", "infos", "=", "[", "]", "\n", "for", "idx", "in", "self", ".", "entity_idxs", ":", "\n", "\t\t\t", "next_states", ".", "append", "(", "nss", "[", "idx", "]", ")", "\n", "rewards", ".", "append", "(", "rs", "[", "idx", "]", ")", "\n", "infos", ".", "append", "(", "infs", "[", "idx", "]", ")", "\n", "\n", "", "return", "next_states", ",", "rewards", ",", "done", ",", "infos", "", "", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.make_proxy_env": [[18, 23], ["len", "proxy_env.gym_proxy_env", "proxy_env.ma_proxy_env"], "function", ["None"], ["def", "make_proxy_env", "(", "entity_idxs", ",", "obs_spaces", ",", "act_spaces", ",", "match_comm", ",", "match_root_rank", ")", ":", "\n", "\t", "if", "len", "(", "entity_idxs", ")", "==", "1", ":", "\n", "\t\t", "return", "gym_proxy_env", "(", "entity_idxs", "[", "0", "]", ",", "obs_spaces", "[", "0", "]", ",", "act_spaces", "[", "0", "]", ",", "match_comm", ",", "match_root_rank", ")", "\n", "", "else", ":", "\n", "\t\t", "return", "ma_proxy_env", "(", "entity_idxs", ",", "obs_spaces", ",", "act_spaces", ",", "match_comm", ",", "match_root_rank", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.env_process.EnvironmentProcess.__init__": [[8, 23], ["isinstance", "arena5.core.utils.mpi_print", "sys.path.append", "make_env", "make_env_method"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.multiagent_navigation.make_env.make_env"], ["\t", "def", "__init__", "(", "self", ",", "make_env_method", ",", "global_comm", ",", "local_comm", ",", "match_root_rank", ",", "call_render", "=", "False", ",", "env_kwargs", "=", "{", "}", ")", ":", "\n", "\n", "\t\t", "if", "isinstance", "(", "make_env_method", ",", "str", ")", ":", "\n", "\t\t\t", "sys", ".", "path", ".", "append", "(", "make_env_method", ")", "\n", "from", "make_env", "import", "make_env", "\n", "self", ".", "env", "=", "make_env", "(", "**", "env_kwargs", ")", "\n", "", "else", ":", "\n", "\t\t\t", "self", ".", "env", "=", "make_env_method", "(", "**", "env_kwargs", ")", "\n", "\n", "", "mpi_print", "(", "\"made env\"", ")", "\n", "\n", "self", ".", "global_comm", "=", "global_comm", "\n", "self", ".", "local_comm", "=", "local_comm", "\n", "self", ".", "match_root_rank", "=", "match_root_rank", "\n", "self", ".", "call_render", "=", "call_render", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.env_process.EnvironmentProcess.proxy_sync": [[24, 27], ["env_process.EnvironmentProcess.env.reset", "env_process.EnvironmentProcess.local_comm.bcast"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "proxy_sync", "(", "self", ")", ":", "\n", "\t\t", "self", ".", "states", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "self", ".", "local_comm", ".", "bcast", "(", "self", ".", "states", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.env_process.EnvironmentProcess.run": [[28, 76], ["range", "hasattr", "env_process.EnvironmentProcess.local_comm.gather", "sorted", "env_process.EnvironmentProcess.env.step", "env_process.EnvironmentProcess.local_comm.bcast", "env_process.EnvironmentProcess.env.close", "range", "env_process.EnvironmentProcess.env.render", "env_process.EnvironmentProcess.env.reset", "env_process.EnvironmentProcess.local_comm.bcast", "len", "sorted.append"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.step", "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.close", "home.repos.pwc.inspect_result.cgrivera_ai-arena.wrappers.mpi_logging_wrappers.MPISynchronizedPRUpdater.render", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.ma_proxy_env.reset"], ["", "def", "run", "(", "self", ",", "num_steps", ")", ":", "\n", "\n", "#play steps", "\n", "\t\t", "for", "stp", "in", "range", "(", "num_steps", ")", ":", "\n", "\n", "#get all actions", "\n", "\t\t\t", "actions", "=", "[", "[", "-", "1", "]", ",", "[", "[", "0", "]", "]", "]", "\n", "actions", "=", "self", ".", "local_comm", ".", "gather", "(", "actions", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "\n", "#some entries may represent multiple entities- convert all to single entity", "\n", "entity_actions", "=", "[", "]", "\n", "for", "entry", "in", "actions", ":", "\n", "\t\t\t\t", "idxs", "=", "entry", "[", "0", "]", "\n", "actions_for_idxs", "=", "entry", "[", "1", "]", "\n", "\n", "for", "i", "in", "range", "(", "len", "(", "idxs", ")", ")", ":", "\n", "\t\t\t\t\t", "entity_actions", ".", "append", "(", "[", "idxs", "[", "i", "]", ",", "actions_for_idxs", "[", "i", "]", "]", ")", "\n", "\n", "#sort actions by entity id", "\n", "", "", "entity_actions", "=", "sorted", "(", "entity_actions", ",", "key", "=", "lambda", "x", ":", "x", "[", "0", "]", ")", "\n", "\n", "#discard the first action, which is a dummy provided by this processes", "\n", "entity_actions", "=", "[", "x", "[", "1", "]", "for", "x", "in", "entity_actions", "[", "1", ":", "]", "]", "\n", "\n", "#step", "\n", "new_states", ",", "rewards", ",", "done", ",", "infos", "=", "self", ".", "env", ".", "step", "(", "entity_actions", ")", "\n", "\n", "# RENDER HERE", "\n", "if", "self", ".", "call_render", ":", "\n", "\t\t\t\t", "self", ".", "env", ".", "render", "(", ")", "\n", "\n", "#send results back", "\n", "", "self", ".", "local_comm", ".", "bcast", "(", "[", "new_states", ",", "rewards", ",", "done", ",", "infos", "]", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "\n", "if", "done", ":", "\n", "\t\t\t\t", "self", ".", "states", "=", "self", ".", "env", ".", "reset", "(", ")", "\n", "\n", "#proxies will call reset -> respond with states", "\n", "self", ".", "local_comm", ".", "bcast", "(", "self", ".", "states", ",", "root", "=", "self", ".", "match_root_rank", ")", "\n", "\n", "", "else", ":", "\n", "\t\t\t\t", "self", ".", "states", "=", "new_states", "\n", "\n", "\n", "#mpi_print(\"env\", stp+1, \"/\", num_steps)", "\n", "\n", "", "", "if", "hasattr", "(", "self", ".", "env", ",", "\"close\"", ")", ":", "\n", "\t\t\t", "self", ".", "env", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.UserStem.__init__": [[31, 36], ["stems.UserStem.global_comm.Get_rank", "arena5.core.utils.mpi_print"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print"], ["\t", "def", "__init__", "(", "self", ",", "make_env_method", ",", "log_comms_dir", ",", "obs_spaces", ",", "act_spaces", ",", "additional_policies", ")", ":", "\n", "\n", "\t\t", "self", ".", "global_comm", "=", "MPI", ".", "COMM_WORLD", "\n", "self", ".", "global_rank", "=", "self", ".", "global_comm", ".", "Get_rank", "(", ")", "\n", "mpi_print", "(", "\"I am the user stem, at rank\"", ",", "self", ".", "global_rank", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.UserStem.kickoff": [[37, 108], ["arena5.core.utils.count_needed_procs", "mpi4py.MPI.COMM_WORLD.Get_size", "stems.UserStem.global_comm.bcast", "stems.UserStem.global_comm.bcast", "stems.UserStem.global_comm.bcast", "stems.UserStem.global_comm.bcast", "stems.UserStem.global_comm.bcast", "stems.UserStem.global_comm.bcast", "stems.UserStem.global_comm.bcast", "stems.UserStem.global_comm.bcast", "stems.UserStem.global_comm.Create", "stems.UserStem.global_comm.Create", "stems.UserStem.global_comm.gather", "arena5.core.utils.mpi_print", "RuntimeError", "arena5.core.utils.count_needed_procs", "isinstance", "arena5.core.utils.mpi_print", "arena5.core.utils.mpi_print", "arena5.core.utils.mpi_print", "arena5.core.utils.mpi_print", "arena5.core.utils.mpi_print", "stems.UserStem.global_comm.group.Excl", "stems.UserStem.global_comm.group.Excl", "range", "len", "range", "new_match_list.append", "range", "str", "new_emap_list.append", "str", "str", "str", "str"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.count_needed_procs", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.count_needed_procs", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print"], ["", "def", "kickoff", "(", "self", ",", "match_list", ",", "policy_types", ",", "steps_per_match", ",", "entity_remaps", "=", "[", "]", ",", "render", "=", "False", ",", "scale", "=", "False", ",", "\n", "policy_kwargs", "=", "{", "}", ",", "env_kwargs", "=", "{", "}", ",", "plot_colors", "=", "{", "}", ")", ":", "\n", "\n", "#make sure there are enough procs to run the matches", "\n", "\t\t", "min_procs", "=", "count_needed_procs", "(", "match_list", ")", "\n", "avail_procs", "=", "MPI", ".", "COMM_WORLD", ".", "Get_size", "(", ")", "\n", "if", "avail_procs", "<", "min_procs", ":", "\n", "\t\t\t", "raise", "RuntimeError", "(", "\"At least \"", "+", "str", "(", "min_procs", ")", "+", "\" processes are needed to run matches, but only \"", "+", "str", "(", "avail_procs", ")", "+", "\" exist.\"", ")", "\n", "\n", "# if scale is turned on, duplicate the match list until we run out of processes", "\n", "", "if", "scale", ":", "\n", "\t\t\t", "num_duplicates", "=", "(", "avail_procs", "-", "1", ")", "//", "(", "min_procs", "-", "1", ")", "\n", "new_match_list", "=", "[", "]", "\n", "for", "m", "in", "match_list", ":", "\n", "\t\t\t\t", "for", "d", "in", "range", "(", "num_duplicates", ")", ":", "\n", "\t\t\t\t\t", "new_match_list", ".", "append", "(", "m", ")", "\n", "", "", "match_list", "=", "new_match_list", "\n", "num_used_procs", "=", "count_needed_procs", "(", "match_list", ")", "\n", "num_unused_procs", "=", "avail_procs", "-", "num_used_procs", "\n", "\n", "if", "len", "(", "entity_remaps", ")", ">", "0", ":", "\n", "\t\t\t\t", "new_emap_list", "=", "[", "]", "\n", "for", "m", "in", "entity_remaps", ":", "\n", "\t\t\t\t\t", "for", "d", "in", "range", "(", "num_duplicates", ")", ":", "\n", "\t\t\t\t\t\t", "new_emap_list", ".", "append", "(", "m", ")", "\n", "", "", "entity_remaps", "=", "new_emap_list", "\n", "\n", "", "if", "isinstance", "(", "env_kwargs", ",", "list", ")", ":", "\n", "\t\t\t\t", "nkwargs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "num_duplicates", ")", ":", "\n", "\t\t\t\t\t", "nkwargs", "+=", "env_kwargs", "\n", "", "env_kwargs", "=", "nkwargs", "\n", "\n", "", "mpi_print", "(", "\"\\n================== SCALING REPORT ======================\"", ")", "\n", "mpi_print", "(", "\"AI Arena was able to duplicate the matches \"", "+", "str", "(", "num_duplicates", ")", "+", "\" times each.\"", ")", "\n", "mpi_print", "(", "\"There will be: \"", "+", "str", "(", "num_unused_procs", ")", "+", "\" unused processes.\"", ")", "\n", "mpi_print", "(", "\"You need to allocate: \"", "+", "str", "(", "min_procs", "-", "num_unused_procs", "-", "1", ")", "+", "\" more processes to duplicate again.\"", ")", "\n", "mpi_print", "(", "\"=========================================================\\n\"", ")", "\n", "\n", "#broadcast the match list", "\n", "", "self", ".", "global_comm", ".", "bcast", "(", "match_list", ",", "root", "=", "0", ")", "\n", "\n", "#broadcast the policy types", "\n", "self", ".", "global_comm", ".", "bcast", "(", "policy_types", ",", "root", "=", "0", ")", "\n", "\n", "#broadcast the number of steps to run each match", "\n", "self", ".", "global_comm", ".", "bcast", "(", "steps_per_match", ",", "root", "=", "0", ")", "\n", "\n", "#broadcast the entity index maps", "\n", "self", ".", "global_comm", ".", "bcast", "(", "entity_remaps", ",", "root", "=", "0", ")", "\n", "\n", "#broadcast if we will be calling render() on environments", "\n", "self", ".", "global_comm", ".", "bcast", "(", "render", ",", "root", "=", "0", ")", "\n", "\n", "#broadcast any policy kwargs", "\n", "self", ".", "global_comm", ".", "bcast", "(", "policy_kwargs", ",", "root", "=", "0", ")", "\n", "\n", "#broadcast any environment kwargs", "\n", "self", ".", "global_comm", ".", "bcast", "(", "env_kwargs", ",", "root", "=", "0", ")", "\n", "\n", "#broadcast any plotting colors", "\n", "self", ".", "global_comm", ".", "bcast", "(", "plot_colors", ",", "root", "=", "0", ")", "\n", "\n", "#create some unused groups - need to call this from root for it to work in other procs", "\n", "temp_match_group_comm", "=", "self", ".", "global_comm", ".", "Create", "(", "self", ".", "global_comm", ".", "group", ".", "Excl", "(", "[", "]", ")", ")", "\n", "temp_pol_group_comm", "=", "self", ".", "global_comm", ".", "Create", "(", "self", ".", "global_comm", ".", "group", ".", "Excl", "(", "[", "]", ")", ")", "\n", "\n", "# now wait for response from all the workers, gather dummy data as a syncing tool", "\n", "_", "=", "self", ".", "global_comm", ".", "gather", "(", "None", ",", "root", "=", "0", ")", "\n", "\n", "mpi_print", "(", "\"ROUND COMPLETE!\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.__init__": [[111, 120], ["stems.WorkerStem.global_comm.Get_rank"], "methods", ["None"], ["\t", "def", "__init__", "(", "self", ",", "make_env_method", ",", "log_comms_dir", ",", "obs_spaces", ",", "act_spaces", ",", "additional_policies", ")", ":", "\n", "\n", "\t\t", "self", ".", "global_comm", "=", "MPI", ".", "COMM_WORLD", "\n", "self", ".", "global_rank", "=", "self", ".", "global_comm", ".", "Get_rank", "(", ")", "\n", "self", ".", "make_env_method", "=", "make_env_method", "\n", "self", ".", "log_comms_dir", "=", "log_comms_dir", "\n", "self", ".", "obs_spaces", "=", "obs_spaces", "\n", "self", ".", "act_spaces", "=", "act_spaces", "\n", "self", ".", "additional_policies", "=", "additional_policies", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.loop": [[121, 338], ["stems.WorkerStem.global_comm.bcast", "stems.WorkerStem.global_comm.bcast", "stems.WorkerStem.global_comm.bcast", "stems.WorkerStem.global_comm.bcast", "stems.WorkerStem.global_comm.bcast", "arena5.core.utils.mpi_print", "stems.WorkerStem.global_comm.bcast", "stems.WorkerStem.global_comm.bcast", "stems.WorkerStem.global_comm.bcast", "enumerate", "range", "range", "stems.WorkerStem.global_comm.gather", "policies_flat.append", "isinstance", "entity_map.append", "match_num_flat.append", "stems.WorkerStem.global_comm.Get_size", "len", "excluded.append", "stems.WorkerStem.global_comm.Create", "stems.WorkerStem.gather", "stems.WorkerStem.global_comm.Create", "stems.WorkerStem.global_comm.Create", "arena5.core.utils.mpi_print", "arena5.core.env_process.EnvironmentProcess", "stems.WorkerStem.global_comm.Create", "stems.WorkerStem.process.proxy_sync", "stems.WorkerStem.process.run", "env_kwargs_map.append", "env_kwargs_map.append", "isinstance", "isinstance", "match_num_flat.append", "len", "unused_ranks.append", "excluded.append", "stems.WorkerStem.global_comm.group.Excl", "stems.WorkerStem.Get_rank", "stems.WorkerStem.Get_rank", "range", "stems.WorkerStem.bcast", "stems.WorkerStem.global_comm.group.Excl", "stems.WorkerStem.global_comm.group.Excl", "stems.WorkerStem.global_comm.group.Excl", "arena5.core.utils.mpi_print", "range", "arena5.core.utils.mpi_print", "stems.WorkerStem.global_comm.Create", "arena5.core.proxy_env.make_proxy_env", "available_policies.update", "arena5.core.policy_record.get_dir_for_policy", "arena5.core.utils.mpi_print", "policies_flat.append", "env_kwargs_map.append", "isinstance", "isinstance", "len", "len", "excluded.append", "stems.WorkerStem.global_comm.group.Excl", "stems.WorkerStem.Get_size", "policy_maker", "policy_maker", "stems.WorkerStem.Get_rank", "arena5.core.policy_record.PolicyRecord.load", "policy_maker.run", "policy_maker.run", "policies_flat.append", "env_kwargs_map.append", "ValueError", "len", "entity_map.append", "entity_map.append", "entity_map.append", "ValueError", "stems.WorkerStem.bcast", "excluded.append", "arena5.core.policy_record.PolicyRecord", "arena5.core.policy_record.PolicyRecord", "str", "len", "ents.append", "ents.append"], "methods", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.env_process.EnvironmentProcess.proxy_sync", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.env_process.EnvironmentProcess.run", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.proxy_env.make_proxy_env", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.get_dir_for_policy", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.policy_record.PolicyRecord.load", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.env_process.EnvironmentProcess.run", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.env_process.EnvironmentProcess.run"], ["", "def", "loop", "(", "self", ")", ":", "\n", "\n", "#receive the match list", "\n", "\t\t", "match_list", "=", "self", ".", "global_comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "\n", "#receive the policy types", "\n", "policy_types", "=", "self", ".", "global_comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "\n", "#receive the number of steps to run each match", "\n", "steps_per_match", "=", "self", ".", "global_comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "\n", "#receive the optional entity index maps for non-sequential entities ", "\n", "entity_remaps", "=", "self", ".", "global_comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "\n", "#receive if we will be calling render() on environments", "\n", "will_call_render", "=", "self", ".", "global_comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "mpi_print", "(", "\"will render:\"", ",", "will_call_render", ")", "\n", "\n", "#receive any policy kwargs", "\n", "policy_kwargs", "=", "self", ".", "global_comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "\n", "#receive any environment kwargs", "\n", "env_kwargs", "=", "self", ".", "global_comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "\n", "#receive any environment kwargs", "\n", "plot_colors", "=", "self", ".", "global_comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "\n", "#figure out mappings from ranks to policies, matches, entities", "\n", "policies_flat", "=", "[", "-", "2", "]", "#index=rank, entry=policy number, root proc=0, envs=-1", "\n", "match_num_flat", "=", "[", "-", "2", "]", "#index=rank, entry=match number, root proc=0", "\n", "entity_map", "=", "[", "[", "-", "1", "]", "]", "#index=rank, entry=[entity numbers], root proc=[-1], envs=[-1]", "\n", "env_kwargs_map", "=", "[", "None", "]", "\n", "\n", "#populate the above mappings", "\n", "for", "midx", ",", "m", "in", "enumerate", "(", "match_list", ")", ":", "\n", "\n", "# Create the rank --> policy mapping =======================================", "\n", "\t\t\t", "policies_flat", ".", "append", "(", "-", "1", ")", "#one environment for each match", "\n", "if", "isinstance", "(", "env_kwargs", ",", "list", ")", ":", "\n", "\t\t\t\t", "env_kwargs_map", ".", "append", "(", "env_kwargs", "[", "midx", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "env_kwargs_map", ".", "append", "(", "env_kwargs", ")", "\n", "\n", "", "for", "entry", "in", "m", ":", "\n", "\t\t\t\t", "if", "isinstance", "(", "entry", ",", "int", ")", ":", "\n", "#we have a single entity being controlled by a process", "\n", "\t\t\t\t\t", "policies_flat", ".", "append", "(", "entry", ")", "\n", "env_kwargs_map", ".", "append", "(", "None", ")", "\n", "", "elif", "isinstance", "(", "entry", ",", "list", ")", ":", "\n", "#we have a group of entities being controlled by a process", "\n", "\t\t\t\t\t", "policies_flat", ".", "append", "(", "entry", "[", "0", "]", ")", "\n", "env_kwargs_map", ".", "append", "(", "None", ")", "\n", "", "else", ":", "\n", "#we have an unknown specification", "\n", "\t\t\t\t\t", "raise", "ValueError", "(", "\"Match entry may contain only ints and lists of ints\"", ")", "\n", "\n", "# Create the rank --> entities mapping ======================================", "\n", "", "", "entity_map", ".", "append", "(", "[", "-", "1", "]", ")", "\n", "entity_nominal_num", "=", "0", "\n", "for", "entry", "in", "m", ":", "\n", "\t\t\t\t", "if", "isinstance", "(", "entry", ",", "int", ")", ":", "\n", "#we have a single entity being controlled by a process", "\n", "\t\t\t\t\t", "if", "len", "(", "entity_remaps", ")", ">", "0", ":", "\n", "\t\t\t\t\t\t", "emap", "=", "entity_remaps", "[", "midx", "]", "\n", "entity_map", ".", "append", "(", "[", "emap", "[", "entity_nominal_num", "]", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t", "entity_map", ".", "append", "(", "[", "entity_nominal_num", "]", ")", "\n", "", "entity_nominal_num", "+=", "1", "\n", "\n", "", "elif", "isinstance", "(", "entry", ",", "list", ")", ":", "\n", "#we have a group of entities being controlled by a process", "\n", "\t\t\t\t\t", "ents", "=", "[", "]", "\n", "for", "e", "in", "entry", ":", "\n", "\t\t\t\t\t\t", "if", "len", "(", "entity_remaps", ")", ">", "0", ":", "\n", "\t\t\t\t\t\t\t", "emap", "=", "entity_remaps", "[", "midx", "]", "\n", "ents", ".", "append", "(", "emap", "[", "entity_nominal_num", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t\t\t", "ents", ".", "append", "(", "entity_nominal_num", ")", "\n", "", "entity_nominal_num", "+=", "1", "\n", "", "entity_map", ".", "append", "(", "ents", ")", "\n", "\n", "", "else", ":", "\n", "#we have an unknown specification", "\n", "\t\t\t\t\t", "raise", "ValueError", "(", "\"Match entry may contain only ints and lists of ints\"", ")", "\n", "\n", "", "", "match_num_flat", ".", "append", "(", "midx", "+", "1", ")", "#env member", "\n", "for", "entry", "in", "m", ":", "\n", "\t\t\t\t", "match_num_flat", ".", "append", "(", "midx", "+", "1", ")", "#policy member", "\n", "\n", "#look for any unused ranks", "\n", "", "", "unused_ranks", "=", "[", "]", "\n", "for", "r", "in", "range", "(", "self", ".", "global_comm", ".", "Get_size", "(", ")", ")", ":", "\n", "\t\t\t", "if", "r", ">=", "len", "(", "policies_flat", ")", ":", "\n", "\t\t\t\t", "unused_ranks", ".", "append", "(", "r", ")", "\n", "\n", "# lookup the policy and match for this process specifically", "\n", "", "", "if", "self", ".", "global_rank", "not", "in", "unused_ranks", ":", "\n", "\t\t\t", "my_pol", "=", "policies_flat", "[", "self", ".", "global_rank", "]", "\n", "my_match", "=", "match_num_flat", "[", "self", ".", "global_rank", "]", "\n", "my_env_kwargs", "=", "env_kwargs_map", "[", "self", ".", "global_rank", "]", "\n", "", "else", ":", "\n", "#unused rank", "\n", "\t\t\t", "my_pol", "=", "-", "2", "\n", "my_match", "=", "-", "2", "\n", "\n", "# create a local comm between members of a single match", "\n", "", "excluded", "=", "[", "]", "\n", "for", "rank", "in", "range", "(", "len", "(", "match_num_flat", ")", ")", ":", "\n", "\t\t\t", "if", "match_num_flat", "[", "rank", "]", "!=", "my_match", ":", "\n", "\t\t\t\t", "excluded", ".", "append", "(", "rank", ")", "\n", "", "", "for", "rank", "in", "unused_ranks", ":", "\n", "\t\t\t", "excluded", ".", "append", "(", "rank", ")", "\n", "\n", "", "if", "self", ".", "global_rank", "not", "in", "unused_ranks", ":", "\n", "\t\t\t", "match_group_comm", "=", "self", ".", "global_comm", ".", "Create", "(", "self", ".", "global_comm", ".", "group", ".", "Excl", "(", "excluded", ")", ")", "\n", "\n", "#everyone in the match group agrees on which rank is the environment", "\n", "my_packet", "=", "[", "match_group_comm", ".", "Get_rank", "(", ")", ",", "my_pol", "==", "-", "1", "]", "#match rank and whether or not you are the environment", "\n", "\n", "#root 0 gathers and then bcasts to everyone in the match", "\n", "proc_info", "=", "match_group_comm", ".", "gather", "(", "my_packet", ",", "root", "=", "0", ")", "\n", "if", "match_group_comm", ".", "Get_rank", "(", ")", "==", "0", ":", "\n", "\t\t\t\t", "for", "rank", "in", "range", "(", "len", "(", "proc_info", ")", ")", ":", "\n", "\t\t\t\t\t", "if", "proc_info", "[", "rank", "]", "[", "1", "]", ":", "\n", "\t\t\t\t\t\t", "root_proc", "=", "proc_info", "[", "rank", "]", "[", "0", "]", "\n", "match_group_comm", ".", "bcast", "(", "root_proc", ",", "root", "=", "0", ")", "\n", "break", "\n", "", "", "", "else", ":", "\n", "\t\t\t\t", "root_proc", "=", "match_group_comm", ".", "bcast", "(", "None", ",", "root", "=", "0", ")", "\n", "", "", "else", ":", "\n", "\t\t\t", "unused_group_comm", "=", "self", ".", "global_comm", ".", "Create", "(", "self", ".", "global_comm", ".", "group", ".", "Excl", "(", "unused_ranks", ")", ")", "\n", "unused_group_comm", "=", "self", ".", "global_comm", ".", "Create", "(", "self", ".", "global_comm", ".", "group", ".", "Excl", "(", "unused_ranks", ")", ")", "\n", "\n", "# now all procs in the match should have the root process rank", "\n", "\n", "", "if", "my_pol", "==", "-", "1", ":", "\n", "\t\t\t", "mpi_print", "(", "\"I am an environment\"", ")", "\n", "self", ".", "process", "=", "EnvironmentProcess", "(", "self", ".", "make_env_method", ",", "self", ".", "global_comm", ",", "match_group_comm", ",", "root_proc", ",", "will_call_render", ",", "env_kwargs", "=", "my_env_kwargs", ")", "\n", "temp_pol_group_comm", "=", "self", ".", "global_comm", ".", "Create", "(", "self", ".", "global_comm", ".", "group", ".", "Excl", "(", "[", "]", ")", ")", "\n", "self", ".", "process", ".", "proxy_sync", "(", ")", "\n", "self", ".", "process", ".", "run", "(", "steps_per_match", ")", "\n", "\n", "del", "self", ".", "process", "\n", "\n", "", "elif", "my_pol", ">", "-", "1", ":", "\n", "\n", "#determine which entity/ies this proc controls", "\n", "\t\t\t", "my_entities", "=", "entity_map", "[", "self", ".", "global_rank", "]", "\n", "\n", "mpi_print", "(", "\"I am a worker for policy\"", ",", "my_pol", ",", "\"for entities\"", ",", "my_entities", ")", "\n", "\n", "#create a local comm among members of the same policy", "\n", "excluded", "=", "[", "]", "\n", "for", "rank", "in", "range", "(", "len", "(", "policies_flat", ")", ")", ":", "\n", "\t\t\t\t", "if", "policies_flat", "[", "rank", "]", "!=", "my_pol", ":", "\n", "\t\t\t\t\t", "excluded", ".", "append", "(", "rank", ")", "\n", "", "", "for", "rank", "in", "unused_ranks", ":", "\n", "\t\t\t\t", "excluded", ".", "append", "(", "rank", ")", "\n", "\n", "", "mpi_print", "(", "\"excluded ranks:\"", ",", "excluded", ")", "\n", "policy_group_comm", "=", "self", ".", "global_comm", ".", "Create", "(", "self", ".", "global_comm", ".", "group", ".", "Excl", "(", "excluded", ")", ")", "\n", "\n", "#calculate how many steps the policy needs to run for", "\n", "#for now this is (number pol workers) * (steps in match)", "\n", "steps_to_run", "=", "policy_group_comm", ".", "Get_size", "(", ")", "*", "steps_per_match", "\n", "\n", "#make a proxy environment", "\n", "obs_spaces", "=", "[", "self", ".", "obs_spaces", "[", "e", "]", "for", "e", "in", "my_entities", "]", "\n", "act_spaces", "=", "[", "self", ".", "act_spaces", "[", "e", "]", "for", "e", "in", "my_entities", "]", "\n", "proxyenv", "=", "make_proxy_env", "(", "my_entities", ",", "obs_spaces", ",", "act_spaces", ",", "match_group_comm", ",", "root_proc", ")", "\n", "\n", "#make the collection of policy options", "\n", "available_policies", "=", "{", "\n", "\"random\"", ":", "RandomPolicy", ",", "\n", "\"ppo\"", ":", "PPOPolicy", ",", "\n", "\"ppo-eval\"", ":", "PPOPolicyEval", ",", "\n", "\"ppo-lstm\"", ":", "PPOLSTMPolicy", ",", "\n", "\"ppo-lstm-eval\"", ":", "PPOLSTMPolicyEval", ",", "\n", "\"multiagent_random\"", ":", "MARandomPolicy", ",", "\n", "\"sac\"", ":", "SACPolicy", ",", "\n", "\"masac\"", ":", "MASACPolicy", ",", "\n", "\"ddpg\"", ":", "DDPGPolicy", ",", "\n", "\"maddpg\"", ":", "MADDPGPolicy", "\n", "}", "\n", "\n", "#add custom policies here", "\n", "available_policies", ".", "update", "(", "self", ".", "additional_policies", ")", "\n", "\n", "#make the policy", "\n", "pol_type", "=", "policy_types", "[", "my_pol", "]", "\n", "policy_maker", "=", "available_policies", "[", "pol_type", "]", "\n", "if", "my_pol", "in", "policy_kwargs", ":", "\n", "\t\t\t\t", "policy", "=", "policy_maker", "(", "proxyenv", ",", "policy_group_comm", ",", "**", "(", "policy_kwargs", "[", "my_pol", "]", ")", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "policy", "=", "policy_maker", "(", "proxyenv", ",", "policy_group_comm", ")", "\n", "\n", "# compute full log comms directory for this policy", "\n", "", "data_dir", "=", "get_dir_for_policy", "(", "my_pol", ",", "self", ".", "log_comms_dir", ")", "\n", "\n", "# create policy record for policy roots", "\n", "if", "policy_group_comm", ".", "Get_rank", "(", ")", "==", "0", ":", "\n", "\t\t\t\t", "if", "plot_colors", ":", "\n", "\t\t\t\t\t", "pr", "=", "PolicyRecord", "(", "my_pol", ",", "self", ".", "log_comms_dir", ",", "plot_colors", "[", "my_pol", "]", ")", "\n", "", "else", ":", "\n", "\t\t\t\t\t", "pr", "=", "PolicyRecord", "(", "my_pol", ",", "self", ".", "log_comms_dir", ")", "\n", "", "pr", ".", "load", "(", ")", "\n", "policy", ".", "run", "(", "steps_to_run", ",", "data_dir", ",", "pr", ")", "\n", "", "else", ":", "\n", "\t\t\t\t", "policy", ".", "run", "(", "steps_to_run", ",", "data_dir", ")", "\n", "\n", "\n", "", "", "else", ":", "\n", "\t\t\t", "mpi_print", "(", "\"Process with rank \"", "+", "str", "(", "self", ".", "global_rank", ")", "+", "\" is unused.\"", ")", "\n", "#this is an unused process :(", "\n", "\n", "# sync with main proc -> we are done working", "\n", "", "self", ".", "global_comm", ".", "gather", "(", "1", ",", "root", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.make_stem": [[18, 27], ["mpi4py.MPI.COMM_WORLD.Get_rank", "stems.UserStem", "stems.WorkerStem", "stems.WorkerStem.loop"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.stems.WorkerStem.loop"], ["def", "make_stem", "(", "make_env_method", ",", "log_comms_dir", ",", "obs_spaces", ",", "act_spaces", ",", "additional_policies", "=", "{", "}", ")", ":", "\n", "\t", "rank", "=", "MPI", ".", "COMM_WORLD", ".", "Get_rank", "(", ")", "\n", "if", "rank", "==", "0", ":", "\n", "# return an object to control arena", "\n", "\t\t", "return", "UserStem", "(", "make_env_method", ",", "log_comms_dir", ",", "obs_spaces", ",", "act_spaces", ",", "additional_policies", ")", "\n", "", "else", ":", "\n", "\t\t", "stem", "=", "WorkerStem", "(", "make_env_method", ",", "log_comms_dir", ",", "obs_spaces", ",", "act_spaces", ",", "additional_policies", ")", "\n", "while", "True", ":", "\n", "\t\t\t", "stem", ".", "loop", "(", ")", "#loop indefinitely", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_grads": [[9, 18], ["p.grad.numpy", "mpi_pytorch_utils.mpi_avg", "print"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_avg"], ["def", "sync_grads", "(", "comm", ",", "parameters", ")", ":", "\n", "    ", "for", "p", "in", "parameters", ":", "\n", "        ", "if", "p", ".", "grad", "is", "None", ":", "\n", "            ", "print", "(", "\"WARNING: network parameter gradient does not exist.\"", ")", "\n", "continue", "\n", "\n", "", "p_grad_numpy", "=", "p", ".", "grad", ".", "numpy", "(", ")", "\n", "avg_p_grad", "=", "mpi_avg", "(", "comm", ",", "p", ".", "grad", ")", "\n", "p_grad_numpy", "[", ":", "]", "=", "avg_p_grad", "[", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.sync_weights": [[20, 24], ["p.data.numpy", "comm.Bcast"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.touchdown_pytorch_ppo.memory.Memory.numpy"], ["", "", "def", "sync_weights", "(", "comm", ",", "parameters", ")", ":", "\n", "    ", "for", "p", "in", "parameters", ":", "\n", "        ", "p_numpy", "=", "p", ".", "data", ".", "numpy", "(", ")", "\n", "comm", ".", "Bcast", "(", "p_numpy", ",", "root", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_avg": [[26, 32], ["comm.Get_size", "numpy.asarray", "numpy.zeros_like", "comm.Allreduce"], "function", ["None"], ["", "", "def", "mpi_avg", "(", "comm", ",", "x", ")", ":", "\n", "    ", "num_procs", "=", "comm", ".", "Get_size", "(", ")", "\n", "x", "=", "np", ".", "asarray", "(", "x", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "buff", "=", "np", ".", "zeros_like", "(", "x", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "comm", ".", "Allreduce", "(", "x", ",", "buff", ",", "op", "=", "MPI", ".", "SUM", ")", "\n", "return", "buff", "/", "num_procs", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_op": [[37, 43], ["numpy.asarray", "numpy.zeros_like", "comm.Allreduce", "numpy.isscalar", "mpi4py.MPI.SUM"], "function", ["None"], ["", "def", "mpi_op", "(", "comm", ",", "x", ",", "op", ")", ":", "\n", "    ", "x", ",", "scalar", "=", "(", "[", "x", "]", ",", "True", ")", "if", "np", ".", "isscalar", "(", "x", ")", "else", "(", "x", ",", "False", ")", "\n", "x", "=", "np", ".", "asarray", "(", "x", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "buff", "=", "np", ".", "zeros_like", "(", "x", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "comm", ".", "Allreduce", "(", "x", ",", "buff", ",", "op", "=", "op", ")", "\n", "return", "buff", "[", "0", "]", "if", "scalar", "else", "buff", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_sum": [[44, 46], ["mpi_pytorch_utils.mpi_op"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_op"], ["", "def", "mpi_sum", "(", "comm", ",", "x", ")", ":", "\n", "    ", "return", "mpi_op", "(", "comm", ",", "x", ",", "MPI", ".", "SUM", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_statistics_scalar": [[48, 69], ["numpy.array", "mpi_pytorch_utils.mpi_sum", "mpi_pytorch_utils.mpi_sum", "numpy.sqrt", "numpy.sum", "mpi_pytorch_utils.mpi_op", "mpi_pytorch_utils.mpi_op", "numpy.sum", "len", "numpy.min", "numpy.max", "len", "len"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_sum", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_sum", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_op", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.mpi_pytorch_utils.mpi_op"], ["", "def", "mpi_statistics_scalar", "(", "comm", ",", "x", ",", "with_min_and_max", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Get mean/std and optional min/max of scalar x across MPI processes.\n    Args:\n        x: An array containing samples of the scalar to produce statistics\n            for.\n        with_min_and_max (bool): If true, return min and max of x in \n            addition to mean and std.\n    \"\"\"", "\n", "x", "=", "np", ".", "array", "(", "x", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "global_sum", ",", "global_n", "=", "mpi_sum", "(", "comm", ",", "[", "np", ".", "sum", "(", "x", ")", ",", "len", "(", "x", ")", "]", ")", "\n", "mean", "=", "global_sum", "/", "global_n", "\n", "\n", "global_sum_sq", "=", "mpi_sum", "(", "comm", ",", "np", ".", "sum", "(", "(", "x", "-", "mean", ")", "**", "2", ")", ")", "\n", "std", "=", "np", ".", "sqrt", "(", "global_sum_sq", "/", "global_n", ")", "# compute global std", "\n", "\n", "if", "with_min_and_max", ":", "\n", "        ", "global_min", "=", "mpi_op", "(", "comm", ",", "np", ".", "min", "(", "x", ")", "if", "len", "(", "x", ")", ">", "0", "else", "np", ".", "inf", ",", "op", "=", "MPI", ".", "MIN", ")", "\n", "global_max", "=", "mpi_op", "(", "comm", ",", "np", ".", "max", "(", "x", ")", "if", "len", "(", "x", ")", ">", "0", "else", "-", "np", ".", "inf", ",", "op", "=", "MPI", ".", "MAX", ")", "\n", "return", "mean", ",", "std", ",", "global_min", ",", "global_max", "\n", "", "return", "mean", ",", "std", "\n", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_lag": [[6, 11], ["float", "float", "time.sleep", "mpi4py.MPI.COMM_WORLD.Get_rank", "mpi4py.MPI.COMM_WORLD.Get_size"], "function", ["None"], ["def", "ned_to_ripCoords_tf", "(", "xyz", ",", "max_det", ")", ":", "\n", "    ", "xyz", "=", "tf", ".", "cast", "(", "xyz", ",", "tf", ".", "float32", ")", "\n", "max_det", "=", "tf", ".", "cast", "(", "max_det", ",", "tf", ".", "float32", ")", "\n", "xy", "=", "tf", ".", "reduce_sum", "(", "tf", ".", "square", "(", "xyz", "[", ":", ",", ":", ",", ":", ",", ":", "1", "]", ")", ",", "axis", "=", "-", "1", ")", "\n", "r_tmp", "=", "xy", "+", "tf", ".", "square", "(", "xyz", "[", ":", ",", ":", ",", ":", ",", "2", "]", ")", "\n", "r_tmp", "=", "(", "max_det", "-", "tf", ".", "clip_by_value", "(", "tf", ".", "sqrt", "(", "r_tmp", ")", ",", "0", ",", "max_det", ")", ")", "/", "max_det", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.mpi_print": [[12, 15], ["print", "sys.stdout.flush"], "function", ["None"], ["thet_tmp", "=", "tf", ".", "atan2", "(", "tf", ".", "sqrt", "(", "xy", ")", ",", "xyz", "[", ":", ",", ":", ",", ":", ",", "2", "]", ")", "\n", "phi_tmp", "=", "tf", ".", "atan2", "(", "xyz", "[", ":", ",", ":", ",", ":", ",", "1", "]", ",", "xyz", "[", ":", ",", ":", ",", ":", ",", "0", "]", ")", "\n", "rip_crd", "=", "tf", ".", "stack", "(", "[", "tf", ".", "sin", "(", "thet_tmp", ")", ",", "tf", ".", "cos", "(", "thet_tmp", ")", ",", "tf", ".", "sin", "(", "phi_tmp", ")", ",", "tf", ".", "cos", "(", "phi_tmp", ")", "]", ",", "axis", "=", "-", "1", ")", "\n", "rip_crd", "=", "tf", ".", "expand_dims", "(", "r_tmp", ",", "axis", "=", "-", "1", ")", "*", "rip_crd", "\n"]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.count_needed_procs": [[16, 28], ["None"], "function", ["None"], ["\n", "return", "rip_crd", "\n", "", ""]], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.count_number_scaled_matches": [[29, 34], ["utils.count_needed_procs", "mpi4py.MPI.COMM_WORLD.Get_size", "len"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.count_needed_procs"], []], "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.total_steps_to_match_steps": [[36, 42], ["utils.count_number_scaled_matches", "utils.count_needed_procs", "ValueError", "str"], "function", ["home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.count_number_scaled_matches", "home.repos.pwc.inspect_result.cgrivera_ai-arena.core.utils.count_needed_procs"], []]}