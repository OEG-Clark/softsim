{"home.repos.pwc.inspect_result.ahaliassos_lipforensics.None.evaluate.parse_args": [[20, 69], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.set_defaults", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.crop_mouths.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"DeepFake detector evaluation\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dataset\"", ",", "\n", "help", "=", "\"Dataset to evaluate on\"", ",", "\n", "type", "=", "str", ",", "\n", "choices", "=", "[", "\n", "\"FaceForensics++\"", ",", "\n", "\"Deepfakes\"", ",", "\n", "\"FaceSwap\"", ",", "\n", "\"Face2Face\"", ",", "\n", "\"NeuralTextures\"", ",", "\n", "\"FaceShifter\"", ",", "\n", "\"DeeperForensics\"", ",", "\n", "\"CelebDF\"", ",", "\n", "\"DFDC\"", ",", "\n", "]", ",", "\n", "default", "=", "\"FaceForensics++\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--compression\"", ",", "\n", "help", "=", "\"Video compression level for FaceForensics++\"", ",", "\n", "type", "=", "str", ",", "\n", "choices", "=", "[", "\"c0\"", ",", "\"c23\"", ",", "\"c40\"", "]", ",", "\n", "default", "=", "\"c23\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--grayscale\"", ",", "dest", "=", "\"grayscale\"", ",", "action", "=", "\"store_true\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--rgb\"", ",", "dest", "=", "\"grayscale\"", ",", "action", "=", "\"store_false\"", ")", "\n", "parser", ".", "set_defaults", "(", "grayscale", "=", "True", ")", "\n", "parser", ".", "add_argument", "(", "\"--frames_per_clip\"", ",", "default", "=", "25", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "default", "=", "32", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\"--device\"", ",", "help", "=", "\"Device to put tensors on\"", ",", "type", "=", "str", ",", "default", "=", "\"cuda:0\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "default", "=", "4", ",", "type", "=", "int", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--weights_forgery_path\"", ",", "\n", "help", "=", "\"Path to pretrained weights for forgery detection\"", ",", "\n", "type", "=", "str", ",", "\n", "default", "=", "\"./models/weights/lipforensics_ff.pth\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--split_path\"", ",", "help", "=", "\"Path to FF++ splits\"", ",", "type", "=", "str", ",", "default", "=", "\"./data/datasets/Forensics/splits/test.json\"", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dfdc_metadata_path\"", ",", "help", "=", "\"Path to DFDC metadata\"", ",", "type", "=", "str", ",", "default", "=", "\"./data/datasets/DFDC/metadata.json\"", "\n", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.None.evaluate.compute_video_level_auc": [[71, 89], ["torch.stack", "torch.stack", "sklearn.metrics.roc_curve", "sklearn.metrics.auc", "torch.stack.cpu().numpy", "torch.stack.cpu().numpy", "torch.mean", "torch.stack", "video_to_logits.keys", "video_to_logits.keys", "torch.stack.cpu", "torch.stack.cpu"], "function", ["None"], ["", "def", "compute_video_level_auc", "(", "video_to_logits", ",", "video_to_labels", ")", ":", "\n", "    ", "\"\"\" \"\n    Compute video-level area under ROC curve. Averages the logits across the video for non-overlapping clips.\n\n    Parameters\n    ----------\n    video_to_logits : dict\n        Maps video ids to list of logit values\n    video_to_labels : dict\n        Maps video ids to label\n    \"\"\"", "\n", "output_batch", "=", "torch", ".", "stack", "(", "\n", "[", "torch", ".", "mean", "(", "torch", ".", "stack", "(", "video_to_logits", "[", "video_id", "]", ")", ",", "0", ",", "keepdim", "=", "False", ")", "for", "video_id", "in", "video_to_logits", ".", "keys", "(", ")", "]", "\n", ")", "\n", "output_labels", "=", "torch", ".", "stack", "(", "[", "video_to_labels", "[", "video_id", "]", "for", "video_id", "in", "video_to_logits", ".", "keys", "(", ")", "]", ")", "\n", "\n", "fpr", ",", "tpr", ",", "_", "=", "metrics", ".", "roc_curve", "(", "output_labels", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ",", "output_batch", ".", "cpu", "(", ")", ".", "numpy", "(", ")", ")", "\n", "return", "metrics", ".", "auc", "(", "fpr", ",", "tpr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.None.evaluate.validate_video_level": [[91, 125], ["model.eval", "collections.defaultdict", "evaluate.compute_video_level_auc", "torch.no_grad", "tqdm.tqdm", "images.to.to", "labels.to.to", "model", "range", "len", "video_indices[].item", "video_to_logits[].append"], "function", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.None.evaluate.compute_video_level_auc"], ["", "def", "validate_video_level", "(", "model", ",", "loader", ",", "args", ")", ":", "\n", "    ", "\"\"\" \"\n    Evaluate model using video-level AUC score.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model instance\n    loader : torch.utils.data.DataLoader\n        Loader for forgery data\n    args\n        Options for evaluation\n    \"\"\"", "\n", "model", ".", "eval", "(", ")", "\n", "\n", "video_to_logits", "=", "defaultdict", "(", "list", ")", "\n", "video_to_labels", "=", "{", "}", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "data", "in", "tqdm", "(", "loader", ")", ":", "\n", "            ", "images", ",", "labels", ",", "video_indices", "=", "data", "\n", "images", "=", "images", ".", "to", "(", "args", ".", "device", ")", "\n", "labels", "=", "labels", ".", "to", "(", "args", ".", "device", ")", "\n", "\n", "# Forward", "\n", "logits", "=", "model", "(", "images", ",", "lengths", "=", "[", "args", ".", "frames_per_clip", "]", "*", "images", ".", "shape", "[", "0", "]", ")", "\n", "\n", "# Get maps from video ids to list of logits (representing outputs for clips) as well as to label", "\n", "for", "i", "in", "range", "(", "len", "(", "video_indices", ")", ")", ":", "\n", "                ", "video_id", "=", "video_indices", "[", "i", "]", ".", "item", "(", ")", "\n", "video_to_logits", "[", "video_id", "]", ".", "append", "(", "logits", "[", "i", "]", ")", "\n", "video_to_labels", "[", "video_id", "]", "=", "labels", "[", "i", "]", "\n", "\n", "", "", "", "auc_video", "=", "compute_video_level_auc", "(", "video_to_logits", ",", "video_to_labels", ")", "\n", "return", "auc_video", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.None.evaluate.main": [[127, 176], ["evaluate.parse_args", "models.spatiotemporal_net.get_model", "torchvision.transforms.Compose", "data.samplers.ConsecutiveClipSampler", "torch.utils.data.DataLoader", "evaluate.validate_video_level", "print", "pandas.read_json", "utils.get_files_from_split", "data.dataset_clips.ForensicsClips", "data.transforms.ToTensorVideo", "torchvision.transforms.CenterCrop", "data.transforms.NormalizeVideo", "data.dataset_clips.CelebDFClips", "data.dataset_clips.DFDCClips", "pandas.read_json"], "function", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.crop_mouths.parse_args", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.get_model", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.None.evaluate.validate_video_level", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.None.utils.get_files_from_split"], ["", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "\n", "model", "=", "get_model", "(", "weights_forgery_path", "=", "args", ".", "weights_forgery_path", ")", "\n", "\n", "# Get dataset", "\n", "transform", "=", "Compose", "(", "\n", "[", "ToTensorVideo", "(", ")", ",", "CenterCrop", "(", "(", "88", ",", "88", ")", ")", ",", "NormalizeVideo", "(", "(", "0.421", ",", ")", ",", "(", "0.165", ",", ")", ")", "]", "\n", ")", "\n", "if", "args", ".", "dataset", "in", "[", "\n", "\"FaceForensics++\"", ",", "\n", "\"Deepfakes\"", ",", "\n", "\"FaceSwap\"", ",", "\n", "\"Face2Face\"", ",", "\n", "\"NeuralTextures\"", ",", "\n", "\"FaceShifter\"", ",", "\n", "\"DeeperForensics\"", ",", "\n", "]", ":", "\n", "        ", "if", "args", ".", "dataset", "==", "\"FaceForensics++\"", ":", "\n", "            ", "fake_types", "=", "(", "\"Deepfakes\"", ",", "\"FaceSwap\"", ",", "\"Face2Face\"", ",", "\"NeuralTextures\"", ")", "\n", "", "else", ":", "\n", "            ", "fake_types", "=", "(", "args", ".", "dataset", ",", ")", "\n", "\n", "", "test_split", "=", "pd", ".", "read_json", "(", "args", ".", "split_path", ",", "dtype", "=", "False", ")", "\n", "test_files_real", ",", "test_files_fake", "=", "get_files_from_split", "(", "test_split", ")", "\n", "\n", "dataset", "=", "ForensicsClips", "(", "\n", "test_files_real", ",", "\n", "test_files_fake", ",", "\n", "args", ".", "frames_per_clip", ",", "\n", "grayscale", "=", "args", ".", "grayscale", ",", "\n", "compression", "=", "args", ".", "compression", ",", "\n", "fakes", "=", "fake_types", ",", "\n", "transform", "=", "transform", ",", "\n", "max_frames_per_video", "=", "110", ",", "\n", ")", "\n", "", "elif", "args", ".", "dataset", "==", "\"CelebDF\"", ":", "\n", "        ", "dataset", "=", "CelebDFClips", "(", "args", ".", "frames_per_clip", ",", "args", ".", "grayscale", ",", "transform", ")", "\n", "", "else", ":", "\n", "        ", "metadata", "=", "pd", ".", "read_json", "(", "args", ".", "dfdc_metadata_path", ")", ".", "T", "\n", "dataset", "=", "DFDCClips", "(", "args", ".", "frames_per_clip", ",", "metadata", ",", "args", ".", "grayscale", ",", "transform", ")", "\n", "\n", "# Get sampler that splits video into non-overlapping clips", "\n", "", "sampler", "=", "ConsecutiveClipSampler", "(", "dataset", ".", "clips_per_video", ")", "\n", "\n", "loader", "=", "DataLoader", "(", "dataset", ",", "batch_size", "=", "args", ".", "batch_size", ",", "sampler", "=", "sampler", ",", "num_workers", "=", "args", ".", "num_workers", ")", "\n", "\n", "auc", "=", "validate_video_level", "(", "model", ",", "loader", ",", "args", ")", "\n", "print", "(", "args", ".", "dataset", ",", "f\"AUC (video-level): {auc}\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.None.utils.get_files_from_split": [[4, 18], ["split[].astype().str.cat", "split[].astype().str.cat", "pandas.concat().to_list", "pandas.concat().to_list", "split[].astype", "split[].astype", "pandas.concat", "pandas.concat", "split[].astype", "split[].astype", "split[].astype", "split[].astype"], "function", ["None"], ["def", "get_files_from_split", "(", "split", ")", ":", "\n", "    ", "\"\"\" \"\n    Get filenames for real and fake samples\n\n    Parameters\n    ----------\n    split : pandas.DataFrame\n        DataFrame containing filenames\n    \"\"\"", "\n", "files_1", "=", "split", "[", "0", "]", ".", "astype", "(", "str", ")", ".", "str", ".", "cat", "(", "split", "[", "1", "]", ".", "astype", "(", "str", ")", ",", "sep", "=", "\"_\"", ")", "\n", "files_2", "=", "split", "[", "1", "]", ".", "astype", "(", "str", ")", ".", "str", ".", "cat", "(", "split", "[", "0", "]", ".", "astype", "(", "str", ")", ",", "sep", "=", "\"_\"", ")", "\n", "files_real", "=", "pd", ".", "concat", "(", "[", "split", "[", "0", "]", ".", "astype", "(", "str", ")", ",", "split", "[", "1", "]", ".", "astype", "(", "str", ")", "]", ")", ".", "to_list", "(", ")", "\n", "files_fake", "=", "pd", ".", "concat", "(", "[", "files_1", ",", "files_2", "]", ")", ".", "to_list", "(", ")", "\n", "return", "files_real", ",", "files_fake", "\n", "", ""]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.utils.warp_img": [[9, 29], ["skimage.transform.estimate_transform", "skimage.transform.warp", "warped.astype.astype"], "function", ["None"], ["\n", "files_1", "=", "split", "[", "0", "]", ".", "astype", "(", "str", ")", ".", "str", ".", "cat", "(", "split", "[", "1", "]", ".", "astype", "(", "str", ")", ",", "sep", "=", "\"_\"", ")", "\n", "files_2", "=", "split", "[", "1", "]", ".", "astype", "(", "str", ")", ".", "str", ".", "cat", "(", "split", "[", "0", "]", ".", "astype", "(", "str", ")", ",", "sep", "=", "\"_\"", ")", "\n", "files_real", "=", "pd", ".", "concat", "(", "[", "split", "[", "0", "]", ".", "astype", "(", "str", ")", ",", "split", "[", "1", "]", ".", "astype", "(", "str", ")", "]", ")", ".", "to_list", "(", ")", "\n", "files_fake", "=", "pd", ".", "concat", "(", "[", "files_1", ",", "files_2", "]", ")", ".", "to_list", "(", ")", "\n", "return", "files_real", ",", "files_fake", "\n", "", ""]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.utils.apply_transform": [[31, 48], ["skimage.transform.warp", "warped.astype.astype"], "function", ["None"], []], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.utils.cut_patch": [[50, 93], ["numpy.mean", "numpy.copy", "Exception", "Exception", "Exception", "Exception", "int", "int", "int", "int", "int", "round", "round", "round", "round", "round", "round", "round", "round"], "function", ["None"], []], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.crop_mouths.parse_args": [[38, 76], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.crop_mouths.parse_args"], ["def", "parse_args", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "\"Pre-processing\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--data-root\"", ",", "help", "=", "\"Root path of datasets\"", ",", "type", "=", "str", ",", "default", "=", "\"./data/datasets\"", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--dataset\"", ",", "\n", "help", "=", "\"Dataset to preprocess\"", ",", "\n", "type", "=", "str", ",", "\n", "choices", "=", "[", "\n", "\"all\"", ",", "\n", "\"FaceForensics++\"", ",", "\n", "\"RealFF\"", ",", "\n", "\"Deepfakes\"", ",", "\n", "\"FaceSwap\"", ",", "\n", "\"Face2Face\"", ",", "\n", "\"NeuralTextures\"", ",", "\n", "\"FaceShifter\"", ",", "\n", "\"DeeperForensics\"", ",", "\n", "\"CelebDF\"", ",", "\n", "\"DFDC\"", ",", "\n", "]", ",", "\n", "default", "=", "\"ff\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--compression\"", ",", "\n", "help", "=", "\"Video compression level for FaceForensics++\"", ",", "\n", "type", "=", "str", ",", "\n", "choices", "=", "[", "\"c0\"", ",", "\"c23\"", ",", "\"c40\"", "]", ",", "\n", "default", "=", "\"c23\"", ",", "\n", ")", "\n", "parser", ".", "add_argument", "(", "\"--mean-face\"", ",", "default", "=", "\"./preprocessing/20words_mean_face.npy\"", ",", "help", "=", "\"Mean face pathname\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--crop-width\"", ",", "default", "=", "96", ",", "type", "=", "int", ",", "help", "=", "\"Width of mouth ROIs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--crop-height\"", ",", "default", "=", "96", ",", "type", "=", "int", ",", "help", "=", "\"Height of mouth ROIs\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--start-idx\"", ",", "default", "=", "48", ",", "type", "=", "int", ",", "help", "=", "\"Start of landmark index for mouth\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--stop-idx\"", ",", "default", "=", "68", ",", "type", "=", "int", ",", "help", "=", "\"End of landmark index for mouth\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--window-margin\"", ",", "default", "=", "12", ",", "type", "=", "int", ",", "help", "=", "\"Window margin for smoothed_landmarks\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.crop_mouths.crop_video_and_save": [[78, 154], ["sorted", "enumerate", "os.path.exists", "os.makedirs", "os.listdir", "collections.deque", "collections.deque", "collections.deque", "numpy.load", "q_frames.append", "q_landmarks.append", "q_name.append", "q_frames.popleft", "q_name.popleft", "q_landmarks.popleft", "utils.apply_transform", "trans", "utils.cut_patch", "os.path.join", "PIL.Image.fromarray().save", "PIL.Image.open", "numpy.array", "os.path.join", "len", "numpy.mean", "q_landmarks.popleft", "q_frames.popleft", "q_name.popleft", "utils.warp_img", "trans", "utils.cut_patch", "os.path.join", "PIL.Image.fromarray().save", "os.path.join", "PIL.Image.fromarray", "PIL.Image.fromarray", "utils.cut_patch.astype", "utils.cut_patch.astype"], "function", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.utils.apply_transform", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.utils.cut_patch", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.utils.warp_img", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.utils.cut_patch"], ["", "def", "crop_video_and_save", "(", "video_path", ",", "landmarks_dir", ",", "target_dir", ",", "mean_face_landmarks", ",", "args", ")", ":", "\n", "    ", "\"\"\" \"\n    Align frames and crop mouths. The landmarks are smoothed over 12 frames to account for motion jitter, and each frame\n    is affine warped to the mean face via five landmarks (around the eyes and nose). The mouth is cropped in each frame\n    by resizing the image and then extracting a fixed 96 by 96 region centred around the mean mouth landmark.\n\n    Parameters\n    ----------\n    video_path : str\n        Path to video directory containing frames of faces\n    landmarks_dir : str\n        Path to directory of landmarks for each frame\n    target_dir : str\n        Path to target directory for cropped frames\n    mean_face_landmarks : numpy.array\n        Landmarks for the mean face of a dataset (in this case, the LRW dataset)\n    args\n        Further options\n    \"\"\"", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "target_dir", ")", ":", "\n", "        ", "os", ".", "makedirs", "(", "target_dir", ",", "exist_ok", "=", "True", ")", "\n", "", "frame_names", "=", "sorted", "(", "os", ".", "listdir", "(", "video_path", ")", ")", "\n", "\n", "q_frames", ",", "q_landmarks", ",", "q_name", "=", "deque", "(", ")", ",", "deque", "(", ")", ",", "deque", "(", ")", "\n", "for", "i", ",", "frame_name", "in", "enumerate", "(", "frame_names", ")", ":", "\n", "        ", "with", "Image", ".", "open", "(", "os", ".", "path", ".", "join", "(", "video_path", ",", "frame_name", ")", ")", "as", "pil_img", ":", "\n", "            ", "img", "=", "np", ".", "array", "(", "pil_img", ")", "\n", "", "landmarks", "=", "np", ".", "load", "(", "os", ".", "path", ".", "join", "(", "landmarks_dir", ",", "f\"{frame_name[:-4]}.npy\"", ")", ")", "\n", "\n", "# Add elements to the queues", "\n", "q_frames", ".", "append", "(", "img", ")", "\n", "q_landmarks", ".", "append", "(", "landmarks", ")", "\n", "q_name", ".", "append", "(", "frame_name", ")", "\n", "\n", "if", "len", "(", "q_frames", ")", "==", "args", ".", "window_margin", ":", "# Wait until queues are large enough", "\n", "            ", "smoothed_landmarks", "=", "np", ".", "mean", "(", "q_landmarks", ",", "axis", "=", "0", ")", "\n", "\n", "cur_landmarks", "=", "q_landmarks", ".", "popleft", "(", ")", "\n", "cur_frame", "=", "q_frames", ".", "popleft", "(", ")", "\n", "cur_name", "=", "q_name", ".", "popleft", "(", ")", "\n", "\n", "# Get aligned frame as well as affine transformation that produced it", "\n", "trans_frame", ",", "trans", "=", "warp_img", "(", "\n", "smoothed_landmarks", "[", "STABLE_POINTS", ",", ":", "]", ",", "mean_face_landmarks", "[", "STABLE_POINTS", ",", ":", "]", ",", "cur_frame", ",", "STD_SIZE", "\n", ")", "\n", "\n", "# Apply that affine transform to the landmarks", "\n", "trans_landmarks", "=", "trans", "(", "cur_landmarks", ")", "\n", "\n", "# Crop mouth region", "\n", "cropped_frame", "=", "cut_patch", "(", "\n", "trans_frame", ",", "\n", "trans_landmarks", "[", "args", ".", "start_idx", ":", "args", ".", "stop_idx", "]", ",", "\n", "args", ".", "crop_height", "//", "2", ",", "\n", "args", ".", "crop_width", "//", "2", ",", "\n", ")", "\n", "\n", "# Save image", "\n", "target_path", "=", "os", ".", "path", ".", "join", "(", "target_dir", ",", "cur_name", ")", "\n", "Image", ".", "fromarray", "(", "cropped_frame", ".", "astype", "(", "np", ".", "uint8", ")", ")", ".", "save", "(", "target_path", ")", "\n", "\n", "# Process remaining frames in the queue", "\n", "", "", "while", "q_frames", ":", "\n", "        ", "cur_frame", "=", "q_frames", ".", "popleft", "(", ")", "\n", "cur_name", "=", "q_name", ".", "popleft", "(", ")", "\n", "cur_landmarks", "=", "q_landmarks", ".", "popleft", "(", ")", "\n", "\n", "trans_frame", "=", "apply_transform", "(", "trans", ",", "cur_frame", ",", "STD_SIZE", ")", "\n", "trans_landmarks", "=", "trans", "(", "cur_landmarks", ")", "\n", "\n", "cropped_frame", "=", "cut_patch", "(", "\n", "trans_frame", ",", "trans_landmarks", "[", "args", ".", "start_idx", ":", "args", ".", "stop_idx", "]", ",", "args", ".", "crop_height", "//", "2", ",", "args", ".", "crop_width", "//", "2", "\n", ")", "\n", "\n", "target_path", "=", "os", ".", "path", ".", "join", "(", "target_dir", ",", "cur_name", ")", "\n", "Image", ".", "fromarray", "(", "cropped_frame", ".", "astype", "(", "np", ".", "uint8", ")", ")", ".", "save", "(", "target_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.crop_mouths.main": [[156, 194], ["crop_mouths.parse_args", "numpy.load", "os.path.join", "os.path.join", "os.path.join", "sorted", "print", "tqdm.tqdm", "os.listdir", "os.path.join", "os.path.join", "os.path.join", "crop_mouths.crop_video_and_save"], "function", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.crop_mouths.parse_args", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.preprocessing.crop_mouths.crop_video_and_save"], ["", "", "def", "main", "(", ")", ":", "\n", "    ", "args", "=", "parse_args", "(", ")", "\n", "\n", "mean_face_landmarks", "=", "np", ".", "load", "(", "args", ".", "mean_face", ")", "\n", "\n", "if", "args", ".", "dataset", "==", "\"all\"", ":", "\n", "        ", "datasets", "=", "[", "\n", "\"Forensics/RealFF\"", ",", "\n", "\"Forensics/Deepfakes\"", ",", "\n", "\"Forensics/FaceSwap\"", ",", "\n", "\"Forensics/Face2Face\"", ",", "\n", "\"Forensics/NeuralTextures\"", ",", "\n", "\"Forensics/FaceShifter\"", ",", "\n", "\"Forensics/DeeperForensics\"", ",", "\n", "\"CelebDF/RealCelebDF\"", ",", "\n", "\"CelebDF/FakeCelebDF\"", ",", "\n", "\"DFDC\"", ",", "\n", "]", "\n", "", "else", ":", "\n", "        ", "datasets", "=", "DATASETS", "[", "args", ".", "dataset", "]", "\n", "\n", "", "for", "dataset", "in", "datasets", ":", "\n", "        ", "compression", "=", "(", "\n", "args", ".", "compression", "if", "dataset", "not", "in", "(", "\"CelebDF/RealCelebDF\"", ",", "\"CelebDF/FakeCelebDF\"", ",", "\"DFDC\"", ")", "else", "\"\"", "\n", ")", "\n", "root", "=", "os", ".", "path", ".", "join", "(", "args", ".", "data_root", ",", "dataset", ",", "compression", ")", "\n", "videos_root", "=", "os", ".", "path", ".", "join", "(", "root", ",", "\"images\"", ")", "\n", "landmarks_root", "=", "os", ".", "path", ".", "join", "(", "root", ",", "\"landmarks\"", ")", "\n", "\n", "video_folders", "=", "sorted", "(", "os", ".", "listdir", "(", "videos_root", ")", ")", "\n", "\n", "print", "(", "f\"\\nProcessing {dataset}...\"", ")", "\n", "for", "video", "in", "tqdm", "(", "video_folders", ")", ":", "\n", "            ", "target_dir", "=", "os", ".", "path", ".", "join", "(", "root", ",", "\"cropped_mouths\"", ",", "video", ")", "\n", "video_dir", "=", "os", ".", "path", ".", "join", "(", "videos_root", ",", "video", ")", "\n", "landmarks_dir", "=", "os", ".", "path", ".", "join", "(", "landmarks_root", ",", "video", ")", "\n", "\n", "crop_video_and_save", "(", "video_dir", ",", "landmarks_dir", ",", "target_dir", ",", "mean_face_landmarks", ",", "args", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.BasicBlock.__init__": [[30, 53], ["torch.Module.__init__", "resnet.conv3x3", "torch.BatchNorm2d", "resnet.conv3x3", "torch.BatchNorm2d", "torch.ReLU", "torch.ReLU", "torch.PReLU", "torch.PReLU", "Exception"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__init__", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.conv3x3", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.conv3x3"], ["def", "__init__", "(", "self", ",", "inplanes", ",", "planes", ",", "stride", "=", "1", ",", "downsample", "=", "None", ",", "relu_type", "=", "\"relu\"", ")", ":", "\n", "        ", "super", "(", "BasicBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "relu_type", "in", "[", "\"relu\"", ",", "\"prelu\"", "]", "\n", "\n", "self", ".", "conv1", "=", "conv3x3", "(", "inplanes", ",", "planes", ",", "stride", ")", "\n", "self", ".", "bn1", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "\n", "if", "relu_type", "==", "\"relu\"", ":", "\n", "            ", "self", ".", "relu1", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "self", ".", "relu2", "=", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", "\n", "", "elif", "relu_type", "==", "\"prelu\"", ":", "\n", "            ", "self", ".", "relu1", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "planes", ")", "\n", "self", ".", "relu2", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "planes", ")", "\n", "", "else", ":", "\n", "            ", "raise", "Exception", "(", "\"relu type not implemented\"", ")", "\n", "# --------", "\n", "\n", "", "self", ".", "conv2", "=", "conv3x3", "(", "planes", ",", "planes", ")", "\n", "self", ".", "bn2", "=", "nn", ".", "BatchNorm2d", "(", "planes", ")", "\n", "\n", "self", ".", "downsample", "=", "downsample", "\n", "self", ".", "stride", "=", "stride", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.BasicBlock.forward": [[54, 68], ["resnet.BasicBlock.conv1", "resnet.BasicBlock.bn1", "resnet.BasicBlock.relu1", "resnet.BasicBlock.conv2", "resnet.BasicBlock.bn2", "resnet.BasicBlock.relu2", "resnet.BasicBlock.downsample"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "residual", "=", "x", "\n", "out", "=", "self", ".", "conv1", "(", "x", ")", "\n", "out", "=", "self", ".", "bn1", "(", "out", ")", "\n", "out", "=", "self", ".", "relu1", "(", "out", ")", "\n", "out", "=", "self", ".", "conv2", "(", "out", ")", "\n", "out", "=", "self", ".", "bn2", "(", "out", ")", "\n", "if", "self", ".", "downsample", "is", "not", "None", ":", "\n", "            ", "residual", "=", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "", "out", "+=", "residual", "\n", "out", "=", "self", ".", "relu2", "(", "out", ")", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.ResNet.__init__": [[71, 96], ["torch.Module.__init__", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "resnet.ResNet._make_layer", "torch.AdaptiveAvgPool2d", "resnet.ResNet.modules", "isinstance", "resnet.ResNet.modules", "m.weight.data.normal_", "isinstance", "isinstance", "math.sqrt", "m.weight.data.fill_", "m.bias.data.zero_", "m.bn2.weight.data.zero_"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__init__", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.ResNet._make_layer", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.ResNet._make_layer", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.ResNet._make_layer", "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.ResNet._make_layer"], ["    ", "def", "__init__", "(", "self", ",", "block", ",", "layers", ",", "relu_type", "=", "\"relu\"", ",", "gamma_zero", "=", "False", ",", "avg_pool_downsample", "=", "False", ")", ":", "\n", "        ", "self", ".", "inplanes", "=", "64", "\n", "self", ".", "relu_type", "=", "relu_type", "\n", "self", ".", "gamma_zero", "=", "gamma_zero", "\n", "self", ".", "downsample_block", "=", "downsample_basic_block_v2", "if", "avg_pool_downsample", "else", "downsample_basic_block", "\n", "super", "(", "ResNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "layer1", "=", "self", ".", "_make_layer", "(", "block", ",", "64", ",", "layers", "[", "0", "]", ")", "\n", "self", ".", "layer2", "=", "self", ".", "_make_layer", "(", "block", ",", "128", ",", "layers", "[", "1", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "layer3", "=", "self", ".", "_make_layer", "(", "block", ",", "256", ",", "layers", "[", "2", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "layer4", "=", "self", ".", "_make_layer", "(", "block", ",", "512", ",", "layers", "[", "3", "]", ",", "stride", "=", "2", ")", "\n", "self", ".", "avgpool", "=", "nn", ".", "AdaptiveAvgPool2d", "(", "1", ")", "\n", "\n", "# default init", "\n", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "            ", "if", "isinstance", "(", "m", ",", "nn", ".", "Conv2d", ")", ":", "\n", "                ", "n", "=", "m", ".", "kernel_size", "[", "0", "]", "*", "m", ".", "kernel_size", "[", "1", "]", "*", "m", ".", "out_channels", "\n", "m", ".", "weight", ".", "data", ".", "normal_", "(", "0", ",", "math", ".", "sqrt", "(", "2.0", "/", "n", ")", ")", "\n", "", "elif", "isinstance", "(", "m", ",", "nn", ".", "BatchNorm2d", ")", ":", "\n", "                ", "m", ".", "weight", ".", "data", ".", "fill_", "(", "1", ")", "\n", "m", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n", "", "", "if", "self", ".", "gamma_zero", ":", "\n", "            ", "for", "m", "in", "self", ".", "modules", "(", ")", ":", "\n", "                ", "if", "isinstance", "(", "m", ",", "BasicBlock", ")", ":", "\n", "                    ", "m", ".", "bn2", ".", "weight", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.ResNet._make_layer": [[97, 111], ["layers.append", "range", "torch.Sequential", "resnet.ResNet.downsample_block", "block", "layers.append", "block"], "methods", ["None"], ["", "", "", "", "def", "_make_layer", "(", "self", ",", "block", ",", "planes", ",", "blocks", ",", "stride", "=", "1", ")", ":", "\n", "        ", "downsample", "=", "None", "\n", "if", "stride", "!=", "1", "or", "self", ".", "inplanes", "!=", "planes", "*", "block", ".", "expansion", ":", "\n", "            ", "downsample", "=", "self", ".", "downsample_block", "(", "\n", "inplanes", "=", "self", ".", "inplanes", ",", "outplanes", "=", "planes", "*", "block", ".", "expansion", ",", "stride", "=", "stride", "\n", ")", "\n", "\n", "", "layers", "=", "[", "]", "\n", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "stride", ",", "downsample", ",", "relu_type", "=", "self", ".", "relu_type", ")", ")", "\n", "self", ".", "inplanes", "=", "planes", "*", "block", ".", "expansion", "\n", "for", "i", "in", "range", "(", "1", ",", "blocks", ")", ":", "\n", "            ", "layers", ".", "append", "(", "block", "(", "self", ".", "inplanes", ",", "planes", ",", "relu_type", "=", "self", ".", "relu_type", ")", ")", "\n", "\n", "", "return", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.ResNet.forward": [[112, 120], ["resnet.ResNet.layer1", "resnet.ResNet.layer2", "resnet.ResNet.layer3", "resnet.ResNet.layer4", "resnet.ResNet.avgpool", "x.view.view.view", "x.view.view.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "x", "=", "self", ".", "layer1", "(", "x", ")", "\n", "x", "=", "self", ".", "layer2", "(", "x", ")", "\n", "x", "=", "self", ".", "layer3", "(", "x", ")", "\n", "x", "=", "self", ".", "layer4", "(", "x", ")", "\n", "x", "=", "self", ".", "avgpool", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "x", ".", "size", "(", "0", ")", ",", "-", "1", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.conv3x3": [[8, 10], ["torch.Conv2d"], "function", ["None"], ["def", "conv3x3", "(", "in_planes", ",", "out_planes", ",", "stride", "=", "1", ")", ":", "\n", "    ", "return", "nn", ".", "Conv2d", "(", "in_planes", ",", "out_planes", ",", "kernel_size", "=", "3", ",", "stride", "=", "stride", ",", "padding", "=", "1", ",", "bias", "=", "False", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.downsample_basic_block": [[12, 16], ["torch.Sequential", "torch.Conv2d", "torch.BatchNorm2d"], "function", ["None"], ["", "def", "downsample_basic_block", "(", "inplanes", ",", "outplanes", ",", "stride", ")", ":", "\n", "    ", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv2d", "(", "inplanes", ",", "outplanes", ",", "kernel_size", "=", "1", ",", "stride", "=", "stride", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "outplanes", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.resnet.downsample_basic_block_v2": [[19, 24], ["torch.Sequential", "torch.AvgPool2d", "torch.Conv2d", "torch.BatchNorm2d"], "function", ["None"], ["", "def", "downsample_basic_block_v2", "(", "inplanes", ",", "outplanes", ",", "stride", ")", ":", "\n", "    ", "return", "nn", ".", "Sequential", "(", "\n", "nn", ".", "AvgPool2d", "(", "kernel_size", "=", "stride", ",", "stride", "=", "stride", ",", "ceil_mode", "=", "True", ",", "count_include_pad", "=", "False", ")", ",", "\n", "nn", ".", "Conv2d", "(", "inplanes", ",", "outplanes", ",", "kernel_size", "=", "1", ",", "stride", "=", "1", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm2d", "(", "outplanes", ")", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.tcn.Chomp1d.__init__": [[9, 15], ["torch.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "chomp_size", ",", "symm_chomp", ")", ":", "\n", "        ", "super", "(", "Chomp1d", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chomp_size", "=", "chomp_size", "\n", "self", ".", "symm_chomp", "=", "symm_chomp", "\n", "if", "self", ".", "symm_chomp", ":", "\n", "            ", "assert", "self", ".", "chomp_size", "%", "2", "==", "0", ",", "\"If symmetric chomp, chomp size needs to be even\"", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.tcn.Chomp1d.forward": [[16, 23], ["x[].contiguous", "x[].contiguous"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "chomp_size", "==", "0", ":", "\n", "            ", "return", "x", "\n", "", "if", "self", ".", "symm_chomp", ":", "\n", "            ", "return", "x", "[", ":", ",", ":", ",", "self", ".", "chomp_size", "//", "2", ":", "-", "self", ".", "chomp_size", "//", "2", "]", ".", "contiguous", "(", ")", "\n", "", "else", ":", "\n", "            ", "return", "x", "[", ":", ",", ":", ",", ":", "-", "self", ".", "chomp_size", "]", ".", "contiguous", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.tcn.ConvBatchChompRelu.__init__": [[26, 53], ["torch.Module.__init__", "torch.Sequential", "torch.Sequential", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "tcn.Chomp1d", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "tcn.Chomp1d", "torch.Conv1d", "torch.Conv1d", "torch.BatchNorm1d", "torch.BatchNorm1d", "torch.PReLU", "torch.PReLU", "torch.ReLU", "torch.ReLU", "torch.PReLU", "torch.PReLU", "torch.ReLU", "torch.ReLU", "torch.PReLU", "torch.PReLU", "torch.ReLU", "torch.ReLU"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "n_inputs", ",", "n_outputs", ",", "kernel_size", ",", "stride", ",", "dilation", ",", "padding", ",", "relu_type", ",", "dwpw", "=", "False", ")", ":", "\n", "        ", "super", "(", "ConvBatchChompRelu", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dwpw", "=", "dwpw", "\n", "if", "dwpw", ":", "\n", "            ", "self", ".", "conv", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv1d", "(", "\n", "n_inputs", ",", "\n", "n_inputs", ",", "\n", "kernel_size", ",", "\n", "stride", "=", "stride", ",", "\n", "padding", "=", "padding", ",", "\n", "dilation", "=", "dilation", ",", "\n", "groups", "=", "n_inputs", ",", "\n", "bias", "=", "False", ",", "\n", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "n_inputs", ")", ",", "\n", "Chomp1d", "(", "padding", ",", "True", ")", ",", "\n", "nn", ".", "PReLU", "(", "num_parameters", "=", "n_inputs", ")", "if", "relu_type", "==", "\"prelu\"", "else", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", "nn", ".", "Conv1d", "(", "n_inputs", ",", "n_outputs", ",", "1", ",", "1", ",", "0", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm1d", "(", "n_outputs", ")", ",", "\n", "nn", ".", "PReLU", "(", "num_parameters", "=", "n_outputs", ")", "if", "relu_type", "==", "\"prelu\"", "else", "nn", ".", "ReLU", "(", "inplace", "=", "True", ")", ",", "\n", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "conv", "=", "nn", ".", "Conv1d", "(", "n_inputs", ",", "n_outputs", ",", "kernel_size", ",", "stride", "=", "stride", ",", "padding", "=", "padding", ",", "dilation", "=", "dilation", ")", "\n", "self", ".", "batchnorm", "=", "nn", ".", "BatchNorm1d", "(", "n_outputs", ")", "\n", "self", ".", "chomp", "=", "Chomp1d", "(", "padding", ",", "True", ")", "\n", "self", ".", "non_lin", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "n_outputs", ")", "if", "relu_type", "==", "\"prelu\"", "else", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.tcn.ConvBatchChompRelu.forward": [[54, 62], ["tcn.ConvBatchChompRelu.conv", "tcn.ConvBatchChompRelu.conv", "tcn.ConvBatchChompRelu.batchnorm", "tcn.ConvBatchChompRelu.chomp", "tcn.ConvBatchChompRelu.non_lin"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "if", "self", ".", "dwpw", ":", "\n", "            ", "return", "self", ".", "conv", "(", "x", ")", "\n", "", "else", ":", "\n", "            ", "out", "=", "self", ".", "conv", "(", "x", ")", "\n", "out", "=", "self", ".", "batchnorm", "(", "out", ")", "\n", "out", "=", "self", ".", "chomp", "(", "out", ")", "\n", "return", "self", ".", "non_lin", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.tcn.MultibranchTemporalBlock.__init__": [[65, 97], ["torch.Module.__init__", "len", "enumerate", "torch.Dropout", "torch.Dropout", "enumerate", "torch.Dropout", "torch.Dropout", "tcn.ConvBatchChompRelu", "setattr", "tcn.ConvBatchChompRelu", "setattr", "torch.Conv1d", "torch.Conv1d", "torch.ReLU", "torch.ReLU", "torch.PReLU", "torch.PReLU"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__init__"], ["    ", "def", "__init__", "(", "\n", "self", ",", "n_inputs", ",", "n_outputs", ",", "kernel_sizes", ",", "stride", ",", "dilation", ",", "padding", ",", "dropout", "=", "0.2", ",", "relu_type", "=", "\"relu\"", ",", "dwpw", "=", "False", "\n", ")", ":", "\n", "        ", "super", "(", "MultibranchTemporalBlock", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "kernel_sizes", "=", "kernel_sizes", "\n", "self", ".", "num_kernels", "=", "len", "(", "kernel_sizes", ")", "\n", "self", ".", "n_outputs_branch", "=", "n_outputs", "//", "self", ".", "num_kernels", "\n", "assert", "n_outputs", "%", "self", ".", "num_kernels", "==", "0", ",", "\"Number of output channels needs to be divisible by number of kernels\"", "\n", "\n", "for", "k_idx", ",", "k", "in", "enumerate", "(", "self", ".", "kernel_sizes", ")", ":", "\n", "            ", "cbcr", "=", "ConvBatchChompRelu", "(", "\n", "n_inputs", ",", "self", ".", "n_outputs_branch", ",", "k", ",", "stride", ",", "dilation", ",", "padding", "[", "k_idx", "]", ",", "relu_type", ",", "dwpw", "=", "dwpw", "\n", ")", "\n", "setattr", "(", "self", ",", "\"cbcr0_{}\"", ".", "format", "(", "k_idx", ")", ",", "cbcr", ")", "\n", "", "self", ".", "dropout0", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "for", "k_idx", ",", "k", "in", "enumerate", "(", "self", ".", "kernel_sizes", ")", ":", "\n", "            ", "cbcr", "=", "ConvBatchChompRelu", "(", "\n", "n_outputs", ",", "self", ".", "n_outputs_branch", ",", "k", ",", "stride", ",", "dilation", ",", "padding", "[", "k_idx", "]", ",", "relu_type", ",", "dwpw", "=", "dwpw", "\n", ")", "\n", "setattr", "(", "self", ",", "\"cbcr1_{}\"", ".", "format", "(", "k_idx", ")", ",", "cbcr", ")", "\n", "", "self", ".", "dropout1", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n", "# Downsample?", "\n", "self", ".", "downsample", "=", "nn", ".", "Conv1d", "(", "n_inputs", ",", "n_outputs", ",", "1", ")", "if", "(", "n_inputs", "//", "self", ".", "num_kernels", ")", "!=", "n_outputs", "else", "None", "\n", "\n", "# Final relu", "\n", "if", "relu_type", "==", "\"relu\"", ":", "\n", "            ", "self", ".", "relu_final", "=", "nn", ".", "ReLU", "(", ")", "\n", "", "elif", "relu_type", "==", "\"prelu\"", ":", "\n", "            ", "self", ".", "relu_final", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "n_outputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.tcn.MultibranchTemporalBlock.forward": [[98, 120], ["range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "tcn.MultibranchTemporalBlock.dropout0", "range", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "tcn.MultibranchTemporalBlock.dropout1", "tcn.MultibranchTemporalBlock.relu_final", "getattr", "outputs.append", "getattr", "outputs.append", "tcn.MultibranchTemporalBlock.downsample", "getattr.", "getattr."], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# First multi-branch set of convolutions", "\n", "        ", "outputs", "=", "[", "]", "\n", "for", "k_idx", "in", "range", "(", "self", ".", "num_kernels", ")", ":", "\n", "            ", "branch_convs", "=", "getattr", "(", "self", ",", "\"cbcr0_{}\"", ".", "format", "(", "k_idx", ")", ")", "\n", "outputs", ".", "append", "(", "branch_convs", "(", "x", ")", ")", "\n", "", "out0", "=", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "\n", "out0", "=", "self", ".", "dropout0", "(", "out0", ")", "\n", "\n", "# Second multi-branch set of convolutions", "\n", "outputs", "=", "[", "]", "\n", "for", "k_idx", "in", "range", "(", "self", ".", "num_kernels", ")", ":", "\n", "            ", "branch_convs", "=", "getattr", "(", "self", ",", "\"cbcr1_{}\"", ".", "format", "(", "k_idx", ")", ")", "\n", "outputs", ".", "append", "(", "branch_convs", "(", "out0", ")", ")", "\n", "", "out1", "=", "torch", ".", "cat", "(", "outputs", ",", "1", ")", "\n", "out1", "=", "self", ".", "dropout1", "(", "out1", ")", "\n", "\n", "# Downsample?", "\n", "res", "=", "x", "if", "self", ".", "downsample", "is", "None", "else", "self", ".", "downsample", "(", "x", ")", "\n", "\n", "return", "self", ".", "relu_final", "(", "out1", "+", "res", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.tcn.MultibranchTemporalConvNet.__init__": [[123, 149], ["torch.Module.__init__", "len", "range", "torch.Sequential", "torch.Sequential", "layers.append", "tcn.MultibranchTemporalBlock"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_inputs", ",", "num_channels", ",", "tcn_options", ",", "dropout", "=", "0.2", ",", "relu_type", "=", "\"relu\"", ",", "dwpw", "=", "False", ")", ":", "\n", "        ", "super", "(", "MultibranchTemporalConvNet", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "ksizes", "=", "tcn_options", "[", "\"kernel_size\"", "]", "\n", "\n", "layers", "=", "[", "]", "\n", "num_levels", "=", "len", "(", "num_channels", ")", "\n", "for", "i", "in", "range", "(", "num_levels", ")", ":", "\n", "            ", "dilation_size", "=", "2", "**", "i", "\n", "in_channels", "=", "num_inputs", "if", "i", "==", "0", "else", "num_channels", "[", "i", "-", "1", "]", "\n", "out_channels", "=", "num_channels", "[", "i", "]", "\n", "padding", "=", "[", "(", "s", "-", "1", ")", "*", "dilation_size", "for", "s", "in", "self", ".", "ksizes", "]", "\n", "layers", ".", "append", "(", "\n", "MultibranchTemporalBlock", "(", "\n", "in_channels", ",", "\n", "out_channels", ",", "\n", "self", ".", "ksizes", ",", "\n", "stride", "=", "1", ",", "\n", "dilation", "=", "dilation_size", ",", "\n", "padding", "=", "padding", ",", "\n", "dropout", "=", "dropout", ",", "\n", "relu_type", "=", "relu_type", ",", "\n", "dwpw", "=", "dwpw", ",", "\n", ")", "\n", ")", "\n", "", "self", ".", "network", "=", "nn", ".", "Sequential", "(", "*", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.tcn.MultibranchTemporalConvNet.forward": [[150, 152], ["tcn.MultibranchTemporalConvNet.network"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "self", ".", "network", "(", "x", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.MultiscaleMultibranchTCN.__init__": [[64, 75], ["torch.Module.__init__", "len", "tcn.MultibranchTemporalConvNet", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_size", ",", "num_channels", ",", "num_classes", ",", "tcn_options", ",", "dropout", ",", "relu_type", ",", "dwpw", "=", "False", ")", ":", "\n", "        ", "super", "(", "MultiscaleMultibranchTCN", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "kernel_sizes", "=", "tcn_options", "[", "\"kernel_size\"", "]", "\n", "self", ".", "num_kernels", "=", "len", "(", "self", ".", "kernel_sizes", ")", "\n", "\n", "self", ".", "mb_ms_tcn", "=", "MultibranchTemporalConvNet", "(", "\n", "input_size", ",", "num_channels", ",", "tcn_options", ",", "dropout", "=", "dropout", ",", "relu_type", "=", "relu_type", ",", "dwpw", "=", "dwpw", "\n", ")", "\n", "self", ".", "tcn_output", "=", "nn", ".", "Linear", "(", "num_channels", "[", "-", "1", "]", ",", "num_classes", ")", "\n", "self", ".", "consensus_func", "=", "_average_batch", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.MultiscaleMultibranchTCN.forward": [[76, 81], ["x.transpose.transpose.transpose", "spatiotemporal_net.MultiscaleMultibranchTCN.mb_ms_tcn", "spatiotemporal_net.MultiscaleMultibranchTCN.consensus_func", "spatiotemporal_net.MultiscaleMultibranchTCN.tcn_output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "        ", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "out", "=", "self", ".", "mb_ms_tcn", "(", "x", ")", "\n", "out", "=", "self", ".", "consensus_func", "(", "out", ",", "lengths", ")", "\n", "return", "self", ".", "tcn_output", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.Lipreading.__init__": [[84, 106], ["torch.Module.__init__", "resnet.ResNet", "torch.Sequential", "torch.Sequential", "spatiotemporal_net.MultiscaleMultibranchTCN", "torch.PReLU", "torch.PReLU", "torch.ReLU", "torch.ReLU", "torch.Conv3d", "torch.Conv3d", "torch.BatchNorm3d", "torch.BatchNorm3d", "torch.MaxPool3d", "torch.MaxPool3d", "len"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_dim", "=", "256", ",", "num_classes", "=", "500", ",", "relu_type", "=", "\"prelu\"", ",", "tcn_options", "=", "{", "}", ")", ":", "\n", "        ", "super", "(", "Lipreading", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "frontend_nout", "=", "64", "\n", "self", ".", "backend_out", "=", "512", "\n", "self", ".", "trunk", "=", "ResNet", "(", "BasicBlock", ",", "[", "2", ",", "2", ",", "2", ",", "2", "]", ",", "relu_type", "=", "relu_type", ")", "\n", "\n", "frontend_relu", "=", "nn", ".", "PReLU", "(", "num_parameters", "=", "self", ".", "frontend_nout", ")", "if", "relu_type", "==", "\"prelu\"", "else", "nn", ".", "ReLU", "(", ")", "\n", "self", ".", "frontend3D", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Conv3d", "(", "1", ",", "self", ".", "frontend_nout", ",", "kernel_size", "=", "(", "5", ",", "7", ",", "7", ")", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "(", "2", ",", "3", ",", "3", ")", ",", "bias", "=", "False", ")", ",", "\n", "nn", ".", "BatchNorm3d", "(", "self", ".", "frontend_nout", ")", ",", "\n", "frontend_relu", ",", "\n", "nn", ".", "MaxPool3d", "(", "kernel_size", "=", "(", "1", ",", "3", ",", "3", ")", ",", "stride", "=", "(", "1", ",", "2", ",", "2", ")", ",", "padding", "=", "(", "0", ",", "1", ",", "1", ")", ")", ",", "\n", ")", "\n", "self", ".", "tcn", "=", "MultiscaleMultibranchTCN", "(", "\n", "input_size", "=", "self", ".", "backend_out", ",", "\n", "num_channels", "=", "[", "hidden_dim", "*", "len", "(", "tcn_options", "[", "\"kernel_size\"", "]", ")", "*", "tcn_options", "[", "\"width_mult\"", "]", "]", "\n", "*", "tcn_options", "[", "\"num_layers\"", "]", ",", "\n", "num_classes", "=", "num_classes", ",", "\n", "tcn_options", "=", "tcn_options", ",", "\n", "dropout", "=", "tcn_options", "[", "\"dropout\"", "]", ",", "\n", "relu_type", "=", "relu_type", ",", "\n", "dwpw", "=", "tcn_options", "[", "\"dwpw\"", "]", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.Lipreading.forward": [[108, 115], ["spatiotemporal_net.Lipreading.frontend3D", "spatiotemporal_net.reshape_tensor", "spatiotemporal_net.Lipreading.trunk", "x.view.view.view", "spatiotemporal_net.Lipreading.tcn", "x.view.view.size"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.reshape_tensor"], ["", "def", "forward", "(", "self", ",", "x", ",", "lengths", ")", ":", "\n", "        ", "x", "=", "self", ".", "frontend3D", "(", "x", ")", "\n", "t_new", "=", "x", ".", "shape", "[", "2", "]", "\n", "x", "=", "reshape_tensor", "(", "x", ")", "\n", "x", "=", "self", ".", "trunk", "(", "x", ")", "\n", "x", "=", "x", ".", "view", "(", "-", "1", ",", "t_new", ",", "x", ".", "size", "(", "1", ")", ")", "\n", "return", "self", ".", "tcn", "(", "x", ",", "lengths", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.load_json": [[12, 16], ["open", "json.load"], "function", ["None"], ["def", "load_json", "(", "json_fp", ")", ":", "\n", "    ", "with", "open", "(", "json_fp", ",", "\"r\"", ")", "as", "f", ":", "\n", "        ", "json_content", "=", "json", ".", "load", "(", "f", ")", "\n", "", "return", "json_content", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.get_model": [[18, 51], ["spatiotemporal_net.load_json", "spatiotemporal_net.Lipreading", "Lipreading.to", "torch.load", "torch.load", "Lipreading.load_state_dict", "print", "print", "storage.cuda"], "function", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.load_json"], ["", "def", "get_model", "(", "weights_forgery_path", "=", "None", ",", "device", "=", "\"cuda:0\"", ")", ":", "\n", "    ", "\"\"\" \"\n    Get Resnet+MS-TCN model, optionally with pre-trained weights\n\n    Parameters\n    ----------\n    weights_forgery_path : str\n        Path to file with network weights\n    device : str\n        Device to put model on\n    \"\"\"", "\n", "args_loaded", "=", "load_json", "(", "\"./models/configs/lrw_resnet18_mstcn.json\"", ")", "\n", "relu_type", "=", "args_loaded", "[", "\"relu_type\"", "]", "\n", "tcn_options", "=", "{", "\n", "\"num_layers\"", ":", "args_loaded", "[", "\"tcn_num_layers\"", "]", ",", "\n", "\"kernel_size\"", ":", "args_loaded", "[", "\"tcn_kernel_size\"", "]", ",", "\n", "\"dropout\"", ":", "args_loaded", "[", "\"tcn_dropout\"", "]", ",", "\n", "\"dwpw\"", ":", "args_loaded", "[", "\"tcn_dwpw\"", "]", ",", "\n", "\"width_mult\"", ":", "args_loaded", "[", "\"tcn_width_mult\"", "]", ",", "\n", "}", "\n", "\n", "model", "=", "Lipreading", "(", "num_classes", "=", "1", ",", "tcn_options", "=", "tcn_options", ",", "relu_type", "=", "relu_type", ")", "\n", "\n", "# load weights learned during face forgery detection", "\n", "if", "weights_forgery_path", "is", "not", "None", ":", "\n", "        ", "checkpoint_dict", "=", "torch", ".", "load", "(", "weights_forgery_path", ",", "map_location", "=", "lambda", "storage", ",", "loc", ":", "storage", ".", "cuda", "(", "device", ")", ")", "\n", "model", ".", "load_state_dict", "(", "checkpoint_dict", "[", "\"model\"", "]", ")", "\n", "print", "(", "\"Face forgery weights loaded.\"", ")", "\n", "", "else", ":", "\n", "        ", "print", "(", "\"Randomly initialised weights.\"", ")", "\n", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net.reshape_tensor": [[53, 57], ["x.transpose.transpose", "x.transpose.reshape"], "function", ["None"], ["", "def", "reshape_tensor", "(", "x", ")", ":", "\n", "    ", "n_batch", ",", "n_channels", ",", "s_time", ",", "sx", ",", "sy", "=", "x", ".", "shape", "\n", "x", "=", "x", ".", "transpose", "(", "1", ",", "2", ")", "\n", "return", "x", ".", "reshape", "(", "n_batch", "*", "s_time", ",", "n_channels", ",", "sx", ",", "sy", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.models.spatiotemporal_net._average_batch": [[59, 61], ["torch.stack", "torch.stack", "torch.mean", "torch.mean", "enumerate"], "function", ["None"], ["", "def", "_average_batch", "(", "x", ",", "lengths", ")", ":", "\n", "    ", "return", "torch", ".", "stack", "(", "[", "torch", ".", "mean", "(", "x", "[", "index", "]", "[", ":", ",", "0", ":", "i", "]", ",", "1", ")", "for", "index", ",", "i", "in", "enumerate", "(", "lengths", ")", "]", ",", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.transforms.NormalizeVideo.__init__": [[39, 42], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "mean", ",", "std", ")", ":", "\n", "        ", "self", ".", "mean", "=", "mean", "\n", "self", ".", "std", "=", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.transforms.NormalizeVideo.__call__": [[43, 45], ["transforms.normalize"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.transforms.normalize"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "return", "normalize", "(", "clip", ",", "self", ".", "mean", ",", "self", ".", "std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.transforms.ToTensorVideo.__init__": [[48, 50], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "pass", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.transforms.ToTensorVideo.__call__": [[51, 53], ["transforms.to_tensor"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.transforms.to_tensor"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "return", "to_tensor", "(", "clip", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.transforms.to_tensor": [[6, 16], ["clip.float().permute", "clip.float"], "function", ["None"], ["def", "to_tensor", "(", "clip", ")", ":", "\n", "    ", "\"\"\"\n    Cast tensor type to float, then permute dimensions from TxHxWxC to CxTxHxW, and finally divide by 255\n\n    Parameters\n    ----------\n    clip : torch.tensor\n        video clip\n    \"\"\"", "\n", "return", "clip", ".", "float", "(", ")", ".", "permute", "(", "3", ",", "0", ",", "1", ",", "2", ")", "/", "255.0", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.transforms.normalize": [[18, 36], ["clip.clone.clone", "torch.as_tensor", "torch.as_tensor", "clip.clone.sub_().div_", "clip.clone.sub_"], "function", ["None"], ["", "def", "normalize", "(", "clip", ",", "mean", ",", "std", ")", ":", "\n", "    ", "\"\"\"\n    Normalise clip by subtracting mean and dividing by standard deviation\n\n    Parameters\n    ----------\n    clip : torch.tensor\n        video clip\n    mean : tuple\n        Tuple of mean values for each channel\n    std : tuple\n        Tuple of standard deviation values for each channel\n    \"\"\"", "\n", "clip", "=", "clip", ".", "clone", "(", ")", "\n", "mean", "=", "torch", ".", "as_tensor", "(", "mean", ",", "dtype", "=", "clip", ".", "dtype", ",", "device", "=", "clip", ".", "device", ")", "\n", "std", "=", "torch", ".", "as_tensor", "(", "std", ",", "dtype", "=", "clip", ".", "dtype", ",", "device", "=", "clip", ".", "device", ")", "\n", "clip", ".", "sub_", "(", "mean", "[", ":", ",", "None", ",", "None", ",", "None", "]", ")", ".", "div_", "(", "std", "[", ":", ",", "None", ",", "None", ",", "None", "]", ")", "\n", "return", "clip", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.ForensicsClips.__init__": [[15, 60], ["torch.as_tensor", "torch.as_tensor.cumsum().tolist", "list", "os.path.join", "len", "sorted", "os.path.join", "min", "dataset_clips.ForensicsClips.clips_per_video.append", "dataset_clips.ForensicsClips.paths.append", "torch.as_tensor.cumsum", "sorted", "sorted", "len", "os.listdir", "os.listdir", "el.startswith", "sorted.append", "f.split"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "real_videos", ",", "\n", "fake_videos", ",", "\n", "frames_per_clip", ",", "\n", "fakes", "=", "(", "'Deepfakes'", ",", "'FaceSwap'", ",", "'Face2Face'", ",", "'NeuralTextures'", ")", ",", "\n", "compression", "=", "'c23'", ",", "\n", "grayscale", "=", "False", ",", "\n", "transform", "=", "None", ",", "\n", "max_frames_per_video", "=", "270", ",", "\n", ")", ":", "\n", "        ", "self", ".", "frames_per_clip", "=", "frames_per_clip", "\n", "self", ".", "videos_per_type", "=", "{", "}", "\n", "self", ".", "paths", "=", "[", "]", "\n", "self", ".", "grayscale", "=", "grayscale", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "clips_per_video", "=", "[", "]", "\n", "\n", "ds_types", "=", "[", "'RealFF'", "]", "+", "list", "(", "fakes", ")", "# Since we compute AUC, we need to include the Real dataset as well", "\n", "for", "ds_type", "in", "ds_types", ":", "\n", "\n", "# get list of video names", "\n", "            ", "video_paths", "=", "os", ".", "path", ".", "join", "(", "'./data/datasets/Forensics'", ",", "ds_type", ",", "compression", ",", "'cropped_mouths'", ")", "\n", "if", "ds_type", "==", "'RealFF'", ":", "\n", "                ", "videos", "=", "sorted", "(", "real_videos", ")", "\n", "", "elif", "ds_type", "==", "'DeeperForensics'", ":", "# Extra processing for DeeperForensics videos due to naming differences", "\n", "                ", "videos", "=", "[", "]", "\n", "for", "f", "in", "fake_videos", ":", "\n", "                    ", "for", "el", "in", "os", ".", "listdir", "(", "video_paths", ")", ":", "\n", "                        ", "if", "el", ".", "startswith", "(", "f", ".", "split", "(", "'_'", ")", "[", "0", "]", ")", ":", "\n", "                            ", "videos", ".", "append", "(", "el", ")", "\n", "", "", "", "videos", "=", "sorted", "(", "videos", ")", "\n", "", "else", ":", "\n", "                ", "videos", "=", "sorted", "(", "fake_videos", ")", "\n", "\n", "", "self", ".", "videos_per_type", "[", "ds_type", "]", "=", "len", "(", "videos", ")", "\n", "for", "video", "in", "videos", ":", "\n", "                ", "path", "=", "os", ".", "path", ".", "join", "(", "video_paths", ",", "video", ")", "\n", "num_frames", "=", "min", "(", "len", "(", "os", ".", "listdir", "(", "path", ")", ")", ",", "max_frames_per_video", ")", "\n", "num_clips", "=", "num_frames", "//", "frames_per_clip", "\n", "self", ".", "clips_per_video", ".", "append", "(", "num_clips", ")", "\n", "self", ".", "paths", ".", "append", "(", "path", ")", "\n", "\n", "", "", "clip_lengths", "=", "torch", ".", "as_tensor", "(", "self", ".", "clips_per_video", ")", "\n", "self", ".", "cumulative_sizes", "=", "clip_lengths", ".", "cumsum", "(", "0", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.ForensicsClips.__len__": [[61, 63], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cumulative_sizes", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.ForensicsClips.get_clip": [[64, 88], ["bisect.bisect_right", "sorted", "range", "numpy.stack", "os.listdir", "numpy.stack.append", "PIL.Image.open", "numpy.array", "os.path.join", "pil_img.convert.convert.convert"], "methods", ["None"], ["", "def", "get_clip", "(", "self", ",", "idx", ")", ":", "\n", "        ", "video_idx", "=", "bisect", ".", "bisect_right", "(", "self", ".", "cumulative_sizes", ",", "idx", ")", "\n", "if", "video_idx", "==", "0", ":", "\n", "            ", "clip_idx", "=", "idx", "\n", "", "else", ":", "\n", "            ", "clip_idx", "=", "idx", "-", "self", ".", "cumulative_sizes", "[", "video_idx", "-", "1", "]", "\n", "\n", "", "path", "=", "self", ".", "paths", "[", "video_idx", "]", "\n", "frames", "=", "sorted", "(", "os", ".", "listdir", "(", "path", ")", ")", "\n", "\n", "start_idx", "=", "clip_idx", "*", "self", ".", "frames_per_clip", "\n", "\n", "end_idx", "=", "start_idx", "+", "self", ".", "frames_per_clip", "\n", "\n", "sample", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "start_idx", ",", "end_idx", ",", "1", ")", ":", "\n", "            ", "with", "Image", ".", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "frames", "[", "idx", "]", ")", ")", "as", "pil_img", ":", "\n", "                ", "if", "self", ".", "grayscale", ":", "\n", "                    ", "pil_img", "=", "pil_img", ".", "convert", "(", "\"L\"", ")", "\n", "", "img", "=", "np", ".", "array", "(", "pil_img", ")", "\n", "", "sample", ".", "append", "(", "img", ")", "\n", "", "sample", "=", "np", ".", "stack", "(", "sample", ")", "\n", "\n", "return", "sample", ",", "video_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.ForensicsClips.__getitem__": [[89, 99], ["dataset_clips.ForensicsClips.get_clip", "torch.from_numpy", "torch.from_numpy().unsqueeze", "numpy.array", "dataset_clips.ForensicsClips.transform", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.DFDCClips.get_clip"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "sample", ",", "video_idx", "=", "self", ".", "get_clip", "(", "idx", ")", "\n", "\n", "label", "=", "0", "if", "video_idx", "<", "self", ".", "videos_per_type", "[", "'RealFF'", "]", "else", "1", "\n", "label", "=", "torch", ".", "from_numpy", "(", "np", ".", "array", "(", "label", ")", ")", "\n", "sample", "=", "torch", ".", "from_numpy", "(", "sample", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "sample", "=", "self", ".", "transform", "(", "sample", ")", "\n", "\n", "", "return", "sample", ",", "label", ",", "video_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.CelebDFClips.__init__": [[103, 131], ["torch.as_tensor", "torch.as_tensor.cumsum().tolist", "os.path.join", "sorted", "len", "os.listdir", "os.path.join", "len", "dataset_clips.CelebDFClips.clips_per_video.append", "dataset_clips.CelebDFClips.paths.append", "torch.as_tensor.cumsum", "os.listdir"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "frames_per_clip", ",", "\n", "grayscale", "=", "False", ",", "\n", "transform", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "frames_per_clip", "=", "frames_per_clip", "\n", "self", ".", "videos_per_type", "=", "{", "}", "\n", "self", ".", "paths", "=", "[", "]", "\n", "self", ".", "grayscale", "=", "grayscale", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "clips_per_video", "=", "[", "]", "\n", "\n", "ds_types", "=", "[", "'RealCelebDF'", ",", "'FakeCelebDF'", "]", "\n", "for", "ds_type", "in", "ds_types", ":", "\n", "            ", "video_paths", "=", "os", ".", "path", ".", "join", "(", "'./data'", ",", "'datasets'", ",", "'CelebDF'", ",", "ds_type", ",", "'cropped_mouths'", ")", "\n", "videos", "=", "sorted", "(", "os", ".", "listdir", "(", "video_paths", ")", ")", "\n", "\n", "self", ".", "videos_per_type", "[", "ds_type", "]", "=", "len", "(", "videos", ")", "\n", "for", "video", "in", "videos", ":", "\n", "                ", "path", "=", "os", ".", "path", ".", "join", "(", "video_paths", ",", "video", ")", "\n", "num_frames", "=", "len", "(", "os", ".", "listdir", "(", "path", ")", ")", "\n", "num_clips", "=", "num_frames", "//", "frames_per_clip", "\n", "self", ".", "clips_per_video", ".", "append", "(", "num_clips", ")", "\n", "self", ".", "paths", ".", "append", "(", "path", ")", "\n", "\n", "", "", "clip_lengths", "=", "torch", ".", "as_tensor", "(", "self", ".", "clips_per_video", ")", "\n", "self", ".", "cumulative_sizes", "=", "clip_lengths", ".", "cumsum", "(", "0", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.CelebDFClips.__len__": [[132, 134], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cumulative_sizes", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.CelebDFClips.get_clip": [[135, 160], ["bisect.bisect_right", "sorted", "range", "numpy.stack", "os.listdir", "numpy.stack.append", "PIL.Image.open", "numpy.array", "os.path.join", "pil_img.convert.convert.convert"], "methods", ["None"], ["", "def", "get_clip", "(", "self", ",", "idx", ")", ":", "\n", "        ", "video_idx", "=", "bisect", ".", "bisect_right", "(", "self", ".", "cumulative_sizes", ",", "idx", ")", "\n", "if", "video_idx", "==", "0", ":", "\n", "            ", "clip_idx", "=", "idx", "\n", "", "else", ":", "\n", "            ", "clip_idx", "=", "idx", "-", "self", ".", "cumulative_sizes", "[", "video_idx", "-", "1", "]", "\n", "\n", "", "path", "=", "self", ".", "paths", "[", "video_idx", "]", "\n", "frames", "=", "sorted", "(", "os", ".", "listdir", "(", "path", ")", ")", "\n", "\n", "start_idx", "=", "clip_idx", "*", "self", ".", "frames_per_clip", "\n", "\n", "end_idx", "=", "start_idx", "+", "self", ".", "frames_per_clip", "\n", "\n", "sample", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "start_idx", ",", "end_idx", ",", "1", ")", ":", "\n", "            ", "with", "Image", ".", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "frames", "[", "idx", "]", ")", ")", "as", "pil_img", ":", "\n", "                ", "if", "self", ".", "grayscale", ":", "\n", "                    ", "pil_img", "=", "pil_img", ".", "convert", "(", "\"L\"", ")", "\n", "", "img", "=", "np", ".", "array", "(", "pil_img", ")", "\n", "", "sample", ".", "append", "(", "img", ")", "\n", "\n", "", "sample", "=", "np", ".", "stack", "(", "sample", ")", "\n", "\n", "return", "sample", ",", "video_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.CelebDFClips.__getitem__": [[161, 172], ["dataset_clips.CelebDFClips.get_clip", "torch.tensor", "torch.from_numpy().unsqueeze", "dataset_clips.CelebDFClips.transform", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.DFDCClips.get_clip"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "sample", ",", "video_idx", "=", "self", ".", "get_clip", "(", "idx", ")", "\n", "\n", "label", "=", "0", "if", "video_idx", "<", "self", ".", "videos_per_type", "[", "'RealCelebDF'", "]", "else", "1", "\n", "label", "=", "torch", ".", "tensor", "(", "label", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "sample", "=", "torch", ".", "from_numpy", "(", "sample", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "sample", "=", "self", ".", "transform", "(", "sample", ")", "\n", "\n", "", "return", "sample", ",", "label", ",", "video_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.DFDCClips.__init__": [[176, 201], ["os.path.join", "sorted", "torch.as_tensor", "torch.as_tensor.cumsum().tolist", "os.listdir", "os.path.join", "len", "dataset_clips.DFDCClips.clips_per_video.append", "dataset_clips.DFDCClips.paths.append", "os.listdir", "torch.as_tensor.cumsum"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "\n", "frames_per_clip", ",", "\n", "metadata", ",", "\n", "grayscale", "=", "False", ",", "\n", "transform", "=", "None", ",", "\n", ")", ":", "\n", "        ", "self", ".", "frames_per_clip", "=", "frames_per_clip", "\n", "self", ".", "metadata", "=", "metadata", "\n", "self", ".", "paths", "=", "[", "]", "\n", "self", ".", "grayscale", "=", "grayscale", "\n", "self", ".", "transform", "=", "transform", "\n", "self", ".", "clips_per_video", "=", "[", "]", "\n", "\n", "video_paths", "=", "os", ".", "path", ".", "join", "(", "'./data'", ",", "'datasets'", ",", "'DFDC'", ",", "'cropped_mouths'", ")", "\n", "videos", "=", "sorted", "(", "os", ".", "listdir", "(", "video_paths", ")", ")", "\n", "for", "video", "in", "videos", ":", "\n", "            ", "path", "=", "os", ".", "path", ".", "join", "(", "video_paths", ",", "video", ")", "\n", "num_frames", "=", "len", "(", "os", ".", "listdir", "(", "path", ")", ")", "\n", "num_clips", "=", "num_frames", "//", "frames_per_clip", "\n", "self", ".", "clips_per_video", ".", "append", "(", "num_clips", ")", "\n", "self", ".", "paths", ".", "append", "(", "path", ")", "\n", "\n", "", "clip_lengths", "=", "torch", ".", "as_tensor", "(", "self", ".", "clips_per_video", ")", "\n", "self", ".", "cumulative_sizes", "=", "clip_lengths", ".", "cumsum", "(", "0", ")", ".", "tolist", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.DFDCClips.__len__": [[202, 204], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "cumulative_sizes", "[", "-", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.DFDCClips.get_clip": [[205, 231], ["bisect.bisect_right", "sorted", "range", "numpy.stack", "path.split", "os.listdir", "numpy.stack.append", "PIL.Image.open", "numpy.array", "os.path.join", "pil_img.convert.convert.convert"], "methods", ["None"], ["", "def", "get_clip", "(", "self", ",", "idx", ")", ":", "\n", "        ", "video_idx", "=", "bisect", ".", "bisect_right", "(", "self", ".", "cumulative_sizes", ",", "idx", ")", "\n", "if", "video_idx", "==", "0", ":", "\n", "            ", "clip_idx", "=", "idx", "\n", "", "else", ":", "\n", "            ", "clip_idx", "=", "idx", "-", "self", ".", "cumulative_sizes", "[", "video_idx", "-", "1", "]", "\n", "\n", "", "path", "=", "self", ".", "paths", "[", "video_idx", "]", "\n", "video_name", "=", "path", ".", "split", "(", "'/'", ")", "[", "-", "1", "]", "\n", "frames", "=", "sorted", "(", "os", ".", "listdir", "(", "path", ")", ")", "\n", "\n", "start_idx", "=", "clip_idx", "*", "self", ".", "frames_per_clip", "\n", "\n", "end_idx", "=", "start_idx", "+", "self", ".", "frames_per_clip", "\n", "\n", "sample", "=", "[", "]", "\n", "for", "idx", "in", "range", "(", "start_idx", ",", "end_idx", ",", "1", ")", ":", "\n", "            ", "with", "Image", ".", "open", "(", "os", ".", "path", ".", "join", "(", "path", ",", "frames", "[", "idx", "]", ")", ")", "as", "pil_img", ":", "\n", "                ", "if", "self", ".", "grayscale", ":", "\n", "                    ", "pil_img", "=", "pil_img", ".", "convert", "(", "\"L\"", ")", "\n", "", "img", "=", "np", ".", "array", "(", "pil_img", ")", "\n", "", "sample", ".", "append", "(", "img", ")", "\n", "\n", "", "sample", "=", "np", ".", "stack", "(", "sample", ")", "\n", "\n", "return", "sample", ",", "video_idx", ",", "video_name", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.DFDCClips.__getitem__": [[232, 243], ["dataset_clips.DFDCClips.get_clip", "torch.tensor", "torch.from_numpy().unsqueeze", "dataset_clips.DFDCClips.transform", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.dataset_clips.DFDCClips.get_clip"], ["", "def", "__getitem__", "(", "self", ",", "idx", ")", ":", "\n", "        ", "sample", ",", "video_idx", ",", "video_name", "=", "self", ".", "get_clip", "(", "idx", ")", "\n", "\n", "label", "=", "self", ".", "metadata", ".", "loc", "[", "f'{video_name}.mp4'", "]", "[", "'is_fake'", "]", "\n", "label", "=", "torch", ".", "tensor", "(", "label", ",", "dtype", "=", "torch", ".", "float32", ")", "\n", "\n", "sample", "=", "torch", ".", "from_numpy", "(", "sample", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "if", "self", ".", "transform", "is", "not", "None", ":", "\n", "            ", "sample", "=", "self", ".", "transform", "(", "sample", ")", "\n", "\n", "", "return", "sample", ",", "label", ",", "video_idx", "\n", "", "", ""]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__init__": [[10, 18], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "clips_per_video", ")", ":", "\n", "        ", "\"\"\" \"\n        Parameters\n        ----------\n        clips_per_video : list\n            Number of clips in each video\n        \"\"\"", "\n", "self", ".", "clips_per_video", "=", "clips_per_video", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__iter__": [[19, 29], ["torch.cat().tolist", "iter", "torch.cat().tolist.append", "torch.cat", "torch.arange"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "\"\"\"Sampler for consecutive non-overlapping clips in a video\"\"\"", "\n", "idxs", "=", "[", "]", "\n", "s", "=", "0", "\n", "for", "num_clips", "in", "self", ".", "clips_per_video", ":", "\n", "            ", "sampled", "=", "torch", ".", "arange", "(", "num_clips", ")", "[", ":", "num_clips", "]", "+", "s", "\n", "s", "+=", "num_clips", "\n", "idxs", ".", "append", "(", "sampled", ")", "\n", "", "idxs", "=", "torch", ".", "cat", "(", "idxs", ")", ".", "tolist", "(", ")", "\n", "return", "iter", "(", "idxs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.ahaliassos_lipforensics.data.samplers.ConsecutiveClipSampler.__len__": [[30, 32], ["sum"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "sum", "(", "num_clips", "for", "num_clips", "in", "self", ".", "clips_per_video", ")", "\n", "", "", ""]]}