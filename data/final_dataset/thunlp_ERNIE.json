{"home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.create_anchor.run_proc": [[17, 30], ["open", "tqdm.tqdm", "open.close", "open.write", "requests.get", "list", "[].items", "requests.get.json"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["def", "run_proc", "(", "idx", ",", "n", ",", "input_names", ")", ":", "\n", "    ", "folder", "=", "\"pretrain_data/anchor\"", "\n", "target", "=", "\"{}/{}\"", ".", "format", "(", "folder", ",", "idx", ")", "\n", "fout", "=", "open", "(", "target", "+", "\"_anchor2id\"", ",", "\"a+\"", ")", "\n", "for", "input_name", "in", "tqdm", "(", "input_names", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "entity_url", "=", "f\"https://en.wikipedia.org/w/api.php?action=query&prop=pageprops&ppprop=wikibase_item&redirects=1&titles={input_name}&format=json\"", "\n", "entity_info", "=", "requests", ".", "get", "(", "entity_url", ")", "\n", "id", "=", "list", "(", "entity_info", ".", "json", "(", ")", "[", "'query'", "]", "[", "'pages'", "]", ".", "items", "(", ")", ")", "[", "0", "]", "[", "1", "]", "[", "'pageprops'", "]", "[", "'wikibase_item'", "]", "\n", "", "except", ":", "\n", "            ", "id", "=", "'#UNK#'", "\n", "", "fout", ".", "write", "(", "f\"{input_name}\\t{id}\\n\"", ")", "\n", "", "fout", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.create_ids.run_proc": [[34, 97], ["range", "len", "open", "open", "open.close", "open.close", "print", "open", "open.close", "doc.strip.strip", "doc.strip.split", "nltk.tokenize.sent_tokenize", "open.write", "open.write", "x.split", "len", "len", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "len", "x.strip", "len", "ent_out.append", "ent_out.extend", "text_out.append", "text_out.extend", "sent.split", "result.append", "result.append", "new_text_out.append", "new_ent_out.append", "len", "len", "str", "str"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_tokens_to_ids"], ["", "", "def", "run_proc", "(", "idx", ",", "n", ",", "file_list", ")", ":", "\n", "    ", "folder", "=", "\"pretrain_data/raw\"", "\n", "for", "i", "in", "range", "(", "len", "(", "file_list", ")", ")", ":", "\n", "        ", "if", "i", "%", "n", "==", "idx", ":", "\n", "            ", "target", "=", "\"{}/{}\"", ".", "format", "(", "folder", ",", "i", ")", "\n", "fout_text", "=", "open", "(", "target", "+", "\"_token\"", ",", "\"w\"", ")", "\n", "fout_ent", "=", "open", "(", "target", "+", "\"_entity\"", ",", "\"w\"", ")", "\n", "input_names", "=", "file_list", "[", "i", "]", "\n", "for", "input_name", "in", "input_names", ":", "\n", "                ", "print", "(", "input_name", ")", "\n", "fin", "=", "open", "(", "input_name", ",", "\"r\"", ")", "\n", "\n", "for", "doc", "in", "fin", ":", "\n", "                    ", "doc", "=", "doc", ".", "strip", "(", ")", "\n", "segs", "=", "doc", ".", "split", "(", "\"[_end_]\"", ")", "\n", "content", "=", "segs", "[", "0", "]", "\n", "sentences", "=", "sent_tokenize", "(", "content", ")", "\n", "map_segs", "=", "segs", "[", "1", ":", "]", "\n", "maps", "=", "{", "}", "\n", "for", "x", "in", "map_segs", ":", "\n", "                        ", "v", "=", "x", ".", "split", "(", "\"[_map_]\"", ")", "\n", "if", "len", "(", "v", ")", "!=", "2", ":", "\n", "                            ", "continue", "\n", "", "if", "v", "[", "1", "]", "in", "d_ent", ":", "\n", "                            ", "maps", "[", "v", "[", "0", "]", "]", "=", "d_ent", "[", "v", "[", "1", "]", "]", "\n", "\n", "", "", "text_out", "=", "[", "len", "(", "sentences", ")", "]", "\n", "ent_out", "=", "[", "len", "(", "sentences", ")", "]", "\n", "\n", "for", "sent", "in", "sentences", ":", "\n", "                        ", "tokens", "=", "tokenizer", ".", "tokenize", "(", "sent", ")", "\n", "anchor_segs", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "sent", ".", "split", "(", "\"sepsepsep\"", ")", "]", "\n", "result", "=", "[", "]", "\n", "for", "x", "in", "anchor_segs", ":", "\n", "                            ", "if", "x", "in", "maps", ":", "\n", "                                ", "result", ".", "append", "(", "maps", "[", "x", "]", ")", "\n", "", "else", ":", "\n", "                                ", "result", ".", "append", "(", "\"#UNK#\"", ")", "\n", "", "", "cur_seg", "=", "0", "\n", "\n", "new_text_out", "=", "[", "]", "\n", "new_ent_out", "=", "[", "]", "\n", "\n", "for", "token", "in", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", ":", "\n", "                            ", "if", "token", "!=", "sep_id", ":", "\n", "                                ", "new_text_out", ".", "append", "(", "token", ")", "\n", "new_ent_out", ".", "append", "(", "result", "[", "cur_seg", "]", ")", "\n", "", "else", ":", "\n", "                                ", "cur_seg", "+=", "1", "\n", "\n", "", "", "if", "len", "(", "new_ent_out", ")", "!=", "0", ":", "\n", "                            ", "ent_out", ".", "append", "(", "len", "(", "new_ent_out", ")", ")", "\n", "ent_out", ".", "extend", "(", "new_ent_out", ")", "\n", "text_out", ".", "append", "(", "len", "(", "new_text_out", ")", ")", "\n", "text_out", ".", "extend", "(", "new_text_out", ")", "\n", "", "else", ":", "\n", "                            ", "text_out", "[", "0", "]", "-=", "1", "\n", "ent_out", "[", "0", "]", "-=", "1", "\n", "", "", "fout_ent", ".", "write", "(", "\"\\t\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "ent_out", "]", ")", "+", "\"\\n\"", ")", "\n", "fout_text", ".", "write", "(", "\"\\t\"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "text_out", "]", ")", "+", "\"\\n\"", ")", "\n", "", "fin", ".", "close", "(", ")", "\n", "", "fout_ent", ".", "close", "(", ")", "\n", "fout_text", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Template.parse": [[403, 418], ["WikiExtractor.Template", "WikiExtractor.findMatchingBraces", "Template.append", "Template.append", "Template.append", "WikiExtractor.TemplateText", "WikiExtractor.TemplateText", "WikiExtractor.TemplateArg"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.findMatchingBraces"], ["@", "classmethod", "\n", "def", "parse", "(", "cls", ",", "body", ")", ":", "\n", "        ", "tpl", "=", "Template", "(", ")", "\n", "# we must handle nesting, s.a.", "\n", "# {{{1|{{PAGENAME}}}", "\n", "# {{{italics|{{{italic|}}}", "\n", "# {{#if:{{{{{#if:{{{nominee|}}}|nominee|candidate}}|}}}|", "\n", "#", "\n", "start", "=", "0", "\n", "for", "s", ",", "e", "in", "findMatchingBraces", "(", "body", ",", "3", ")", ":", "\n", "            ", "tpl", ".", "append", "(", "TemplateText", "(", "body", "[", "start", ":", "s", "]", ")", ")", "\n", "tpl", ".", "append", "(", "TemplateArg", "(", "body", "[", "s", "+", "3", ":", "e", "-", "3", "]", ")", ")", "\n", "start", "=", "e", "\n", "", "tpl", ".", "append", "(", "TemplateText", "(", "body", "[", "start", ":", "]", ")", ")", "# leftover", "\n", "return", "tpl", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Template.subst": [[420, 442], ["tpl.subst"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.TemplateArg.subst"], ["", "def", "subst", "(", "self", ",", "params", ",", "extractor", ",", "depth", "=", "0", ")", ":", "\n", "# We perform parameter substitutions recursively.", "\n", "# We also limit the maximum number of iterations to avoid too long or", "\n", "# even endless loops (in case of malformed input).", "\n", "\n", "# :see: http://meta.wikimedia.org/wiki/Help:Expansion#Distinction_between_variables.2C_parser_functions.2C_and_templates", "\n", "#", "\n", "# Parameter values are assigned to parameters in two (?) passes.", "\n", "# Therefore a parameter name in a template can depend on the value of", "\n", "# another parameter of the same template, regardless of the order in", "\n", "# which they are specified in the template call, for example, using", "\n", "# Template:ppp containing \"{{{{{{p}}}}}}\", {{ppp|p=q|q=r}} and even", "\n", "# {{ppp|q=r|p=q}} gives r, but using Template:tvvv containing", "\n", "# \"{{{{{{{{{p}}}}}}}}}\", {{tvvv|p=q|q=r|r=s}} gives s.", "\n", "\n", "# logging.debug('&*ssubst tpl %d %s', extractor.frame.length, '', depth, self)", "\n", "\n", "        ", "if", "depth", ">", "extractor", ".", "maxParameterRecursionLevels", ":", "\n", "            ", "extractor", ".", "recursion_exceeded_3_errs", "+=", "1", "\n", "return", "''", "\n", "\n", "", "return", "''", ".", "join", "(", "[", "tpl", ".", "subst", "(", "params", ",", "extractor", ",", "depth", ")", "for", "tpl", "in", "self", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Template.__str__": [[443, 445], ["text_type"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "return", "''", ".", "join", "(", "[", "text_type", "(", "x", ")", "for", "x", "in", "self", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.TemplateText.subst": [[451, 453], ["None"], "methods", ["None"], ["def", "subst", "(", "self", ",", "params", ",", "extractor", ",", "depth", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.TemplateArg.__init__": [[461, 480], ["WikiExtractor.splitParts", "WikiExtractor.Template.parse", "len", "WikiExtractor.Template.parse"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.splitParts", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Template.parse", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Template.parse"], ["def", "__init__", "(", "self", ",", "parameter", ")", ":", "\n", "        ", "\"\"\"\n        :param parameter: the parts of a tplarg.\n        \"\"\"", "\n", "# the parameter name itself might contain templates, e.g.:", "\n", "#   appointe{{#if:{{{appointer14|}}}|r|d}}14|", "\n", "#   4|{{{{{subst|}}}CURRENTYEAR}}", "\n", "\n", "# any parts in a tplarg after the first (the parameter default) are", "\n", "# ignored, and an equals sign in the first part is treated as plain text.", "\n", "# logging.debug('TemplateArg %s', parameter)", "\n", "\n", "parts", "=", "splitParts", "(", "parameter", ")", "\n", "self", ".", "name", "=", "Template", ".", "parse", "(", "parts", "[", "0", "]", ")", "\n", "if", "len", "(", "parts", ")", ">", "1", ":", "\n", "# This parameter has a default value", "\n", "            ", "self", ".", "default", "=", "Template", ".", "parse", "(", "parts", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "default", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.TemplateArg.__str__": [[481, 486], ["None"], "methods", ["None"], ["", "", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "default", ":", "\n", "            ", "return", "'{{{%s|%s}}}'", "%", "(", "self", ".", "name", ",", "self", ".", "default", ")", "\n", "", "else", ":", "\n", "            ", "return", "'{{{%s}}}'", "%", "self", ".", "name", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.TemplateArg.subst": [[488, 506], ["WikiExtractor.TemplateArg.name.subst", "extractor.transform", "WikiExtractor.TemplateArg.default.subst", "extractor.transform"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.TemplateArg.subst", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.TemplateArg.subst", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform"], ["", "", "def", "subst", "(", "self", ",", "params", ",", "extractor", ",", "depth", ")", ":", "\n", "        ", "\"\"\"\n        Substitute value for this argument from dict :param params:\n        Use :param extractor: to evaluate expressions for name and default.\n        Limit substitution to the maximun :param depth:.\n        \"\"\"", "\n", "# the parameter name itself might contain templates, e.g.:", "\n", "# appointe{{#if:{{{appointer14|}}}|r|d}}14|", "\n", "paramName", "=", "self", ".", "name", ".", "subst", "(", "params", ",", "extractor", ",", "depth", "+", "1", ")", "\n", "paramName", "=", "extractor", ".", "transform", "(", "paramName", ")", "\n", "res", "=", "''", "\n", "if", "paramName", "in", "params", ":", "\n", "            ", "res", "=", "params", "[", "paramName", "]", "# use parameter value specified in template invocation", "\n", "", "elif", "self", ".", "default", ":", "# use the default value", "\n", "            ", "defaultValue", "=", "self", ".", "default", ".", "subst", "(", "params", ",", "extractor", ",", "depth", "+", "1", ")", "\n", "res", "=", "extractor", ".", "transform", "(", "defaultValue", ")", "\n", "# logging.debug('subst arg %d %s -> %s' % (depth, paramName, res))", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.__init__": [[510, 515], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "title", "=", "''", ",", "args", "=", "[", "]", ",", "prev", "=", "None", ")", ":", "\n", "        ", "self", ".", "title", "=", "title", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "prev", "=", "prev", "\n", "self", ".", "depth", "=", "prev", ".", "depth", "+", "1", "if", "prev", "else", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.push": [[517, 519], ["WikiExtractor.Frame"], "methods", ["None"], ["", "def", "push", "(", "self", ",", "title", ",", "args", ")", ":", "\n", "        ", "return", "Frame", "(", "title", ",", "args", ",", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop": [[521, 523], ["None"], "methods", ["None"], ["", "def", "pop", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "prev", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.__str__": [[525, 533], ["None"], "methods", ["None"], ["", "def", "__str__", "(", "self", ")", ":", "\n", "        ", "res", "=", "''", "\n", "prev", "=", "self", ".", "prev", "\n", "while", "prev", ":", "\n", "            ", "if", "res", ":", "res", "+=", "', '", "\n", "res", "+=", "'(%s, %s)'", "%", "(", "prev", ".", "title", ",", "prev", ".", "args", ")", "\n", "prev", "=", "prev", ".", "prev", "\n", "", "return", "'<Frame ['", "+", "res", "+", "']>'", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.__init__": [[542, 558], ["WikiExtractor.MagicWords", "WikiExtractor.Frame"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "id", ",", "revid", ",", "title", ",", "lines", ")", ":", "\n", "        ", "\"\"\"\n        :param id: id of page.\n        :param title: tutle of page.\n        :param lines: a list of lines.\n        \"\"\"", "\n", "self", ".", "id", "=", "id", "\n", "self", ".", "revid", "=", "revid", "\n", "self", ".", "title", "=", "title", "\n", "self", ".", "text", "=", "''", ".", "join", "(", "lines", ")", "\n", "self", ".", "magicWords", "=", "MagicWords", "(", ")", "\n", "self", ".", "frame", "=", "Frame", "(", ")", "\n", "self", ".", "recursion_exceeded_1_errs", "=", "0", "# template recursion within expand()", "\n", "self", ".", "recursion_exceeded_2_errs", "=", "0", "# template recursion within expandTemplate()", "\n", "self", ".", "recursion_exceeded_3_errs", "=", "0", "# parameter recursion", "\n", "self", ".", "template_title_errs", "=", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.write_output": [[559, 596], ["WikiExtractor.get_url", "json.dumps", "out.write", "out.write", "out.write", "out.write", "out_str.encode.encode.encode", "header.encode.encode.encode", "out.write", "out.write", "line.encode.encode.encode"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.get_url", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "def", "write_output", "(", "self", ",", "out", ",", "text", ")", ":", "\n", "        ", "\"\"\"\n        :param out: a memory file\n        :param text: the text of the page\n        \"\"\"", "\n", "url", "=", "get_url", "(", "self", ".", "id", ")", "\n", "if", "options", ".", "write_json", ":", "\n", "            ", "json_data", "=", "{", "\n", "'id'", ":", "self", ".", "id", ",", "\n", "'url'", ":", "url", ",", "\n", "'title'", ":", "self", ".", "title", ",", "\n", "'text'", ":", "\"\\n\"", ".", "join", "(", "text", ")", "\n", "}", "\n", "if", "options", ".", "print_revision", ":", "\n", "                ", "json_data", "[", "'revid'", "]", "=", "self", ".", "revid", "\n", "# We don't use json.dump(data, out) because we want to be", "\n", "# able to encode the string if the output is sys.stdout", "\n", "", "out_str", "=", "json", ".", "dumps", "(", "json_data", ",", "ensure_ascii", "=", "False", ")", "\n", "if", "out", "==", "sys", ".", "stdout", ":", "# option -a or -o -", "\n", "                ", "out_str", "=", "out_str", ".", "encode", "(", "'utf-8'", ")", "\n", "", "out", ".", "write", "(", "out_str", ")", "\n", "out", ".", "write", "(", "'\\n'", ")", "\n", "", "else", ":", "\n", "            ", "if", "options", ".", "print_revision", ":", "\n", "                ", "header", "=", "'<doc id=\"%s\" revid=\"%s\" url=\"%s\" title=\"%s\">\\n'", "%", "(", "self", ".", "id", ",", "self", ".", "revid", ",", "url", ",", "self", ".", "title", ")", "\n", "", "else", ":", "\n", "                ", "header", "=", "'<doc id=\"%s\" url=\"%s\" title=\"%s\">\\n'", "%", "(", "self", ".", "id", ",", "url", ",", "self", ".", "title", ")", "\n", "", "footer", "=", "\"\\n</doc>\\n\"", "\n", "if", "out", "==", "sys", ".", "stdout", ":", "# option -a or -o -", "\n", "                ", "header", "=", "header", ".", "encode", "(", "'utf-8'", ")", "\n", "", "out", ".", "write", "(", "header", ")", "\n", "for", "line", "in", "text", ":", "\n", "                ", "if", "out", "==", "sys", ".", "stdout", ":", "# option -a or -o -", "\n", "                    ", "line", "=", "line", ".", "encode", "(", "'utf-8'", ")", "\n", "", "out", ".", "write", "(", "line", ")", "\n", "out", ".", "write", "(", "'\\n'", ")", "\n", "", "out", ".", "write", "(", "footer", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.extract": [[597, 664], ["logging.info", "WikiExtractor.Extractor.title.find", "options.knownNamespaces.get", "pagename.rfind", "pagename.find", "time.strftime", "time.strftime", "time.strftime", "time.strftime", "time.strftime", "WikiExtractor.Extractor.transform", "WikiExtractor.Extractor.wiki2text", "WikiExtractor.compact", "WikiExtractor.Extractor.write_output", "any", "WikiExtractor.Extractor.clean", "sum", "logging.warn", "len"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.wiki2text", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.compact", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.write_output", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.clean"], ["", "", "def", "extract", "(", "self", ",", "out", ")", ":", "\n", "        ", "\"\"\"\n        :param out: a memory file.\n        \"\"\"", "\n", "logging", ".", "info", "(", "'%s\\t%s'", ",", "self", ".", "id", ",", "self", ".", "title", ")", "\n", "\n", "# Separate header from text with a newline.", "\n", "if", "options", ".", "toHTML", ":", "\n", "            ", "title_str", "=", "'<h1>'", "+", "self", ".", "title", "+", "'</h1>'", "\n", "", "else", ":", "\n", "            ", "title_str", "=", "self", ".", "title", "+", "'\\n'", "\n", "# https://www.mediawiki.org/wiki/Help:Magic_words", "\n", "", "colon", "=", "self", ".", "title", ".", "find", "(", "':'", ")", "\n", "if", "colon", "!=", "-", "1", ":", "\n", "            ", "ns", "=", "self", ".", "title", "[", ":", "colon", "]", "\n", "pagename", "=", "self", ".", "title", "[", "colon", "+", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "ns", "=", "''", "# Main", "\n", "pagename", "=", "self", ".", "title", "\n", "", "self", ".", "magicWords", "[", "'NAMESPACE'", "]", "=", "ns", "\n", "self", ".", "magicWords", "[", "'NAMESPACENUMBER'", "]", "=", "options", ".", "knownNamespaces", ".", "get", "(", "ns", ",", "'0'", ")", "\n", "self", ".", "magicWords", "[", "'PAGENAME'", "]", "=", "pagename", "\n", "self", ".", "magicWords", "[", "'FULLPAGENAME'", "]", "=", "self", ".", "title", "\n", "slash", "=", "pagename", ".", "rfind", "(", "'/'", ")", "\n", "if", "slash", "!=", "-", "1", ":", "\n", "            ", "self", ".", "magicWords", "[", "'BASEPAGENAME'", "]", "=", "pagename", "[", ":", "slash", "]", "\n", "self", ".", "magicWords", "[", "'SUBPAGENAME'", "]", "=", "pagename", "[", "slash", "+", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "magicWords", "[", "'BASEPAGENAME'", "]", "=", "pagename", "\n", "self", ".", "magicWords", "[", "'SUBPAGENAME'", "]", "=", "''", "\n", "", "slash", "=", "pagename", ".", "find", "(", "'/'", ")", "\n", "if", "slash", "!=", "-", "1", ":", "\n", "            ", "self", ".", "magicWords", "[", "'ROOTPAGENAME'", "]", "=", "pagename", "[", ":", "slash", "]", "\n", "", "else", ":", "\n", "            ", "self", ".", "magicWords", "[", "'ROOTPAGENAME'", "]", "=", "pagename", "\n", "", "self", ".", "magicWords", "[", "'CURRENTYEAR'", "]", "=", "time", ".", "strftime", "(", "'%Y'", ")", "\n", "self", ".", "magicWords", "[", "'CURRENTMONTH'", "]", "=", "time", ".", "strftime", "(", "'%m'", ")", "\n", "self", ".", "magicWords", "[", "'CURRENTDAY'", "]", "=", "time", ".", "strftime", "(", "'%d'", ")", "\n", "self", ".", "magicWords", "[", "'CURRENTHOUR'", "]", "=", "time", ".", "strftime", "(", "'%H'", ")", "\n", "self", ".", "magicWords", "[", "'CURRENTTIME'", "]", "=", "time", ".", "strftime", "(", "'%H:%M:%S'", ")", "\n", "text", "=", "self", ".", "text", "\n", "self", ".", "text", "=", "''", "# save memory", "\n", "#", "\n", "# @see https://doc.wikimedia.org/mediawiki-core/master/php/classParser.html", "\n", "# This does the equivalent of internalParse():", "\n", "#", "\n", "# $dom = $this->preprocessToDom( $text, $flag );", "\n", "# $text = $frame->expand( $dom );", "\n", "#", "\n", "text", "=", "self", ".", "transform", "(", "text", ")", "\n", "text", "=", "self", ".", "wiki2text", "(", "text", ")", "\n", "text", "=", "compact", "(", "self", ".", "clean", "(", "text", ")", ")", "\n", "# from zwChan", "\n", "text", "=", "[", "title_str", "]", "+", "text", "\n", "\n", "if", "sum", "(", "len", "(", "line", ")", "for", "line", "in", "text", ")", "<", "options", ".", "min_text_length", ":", "\n", "            ", "return", "\n", "\n", "", "self", ".", "write_output", "(", "out", ",", "text", ")", "\n", "\n", "errs", "=", "(", "self", ".", "template_title_errs", ",", "\n", "self", ".", "recursion_exceeded_1_errs", ",", "\n", "self", ".", "recursion_exceeded_2_errs", ",", "\n", "self", ".", "recursion_exceeded_3_errs", ")", "\n", "if", "any", "(", "errs", ")", ":", "\n", "            ", "logging", ".", "warn", "(", "\"Template errors in article '%s' (%s): title(%d) recursion(%d, %d, %d)\"", ",", "\n", "self", ".", "title", ",", "self", ".", "id", ",", "*", "errs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform": [[666, 680], ["nowiki.finditer", "WikiExtractor.Extractor.transform1", "m.end", "WikiExtractor.Extractor.transform1", "m.start", "m.end", "m.start"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform1", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform1"], ["", "", "def", "transform", "(", "self", ",", "wikitext", ")", ":", "\n", "        ", "\"\"\"\n        Transforms wiki markup.\n        @see https://www.mediawiki.org/wiki/Help:Formatting\n        \"\"\"", "\n", "# look for matching <nowiki>...</nowiki>", "\n", "res", "=", "''", "\n", "cur", "=", "0", "\n", "for", "m", "in", "nowiki", ".", "finditer", "(", "wikitext", ",", "cur", ")", ":", "\n", "            ", "res", "+=", "self", ".", "transform1", "(", "wikitext", "[", "cur", ":", "m", ".", "start", "(", ")", "]", ")", "+", "wikitext", "[", "m", ".", "start", "(", ")", ":", "m", ".", "end", "(", ")", "]", "\n", "cur", "=", "m", ".", "end", "(", ")", "\n", "# leftover", "\n", "", "res", "+=", "self", ".", "transform1", "(", "wikitext", "[", "cur", ":", "]", ")", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform1": [[682, 691], ["WikiExtractor.Extractor.expand", "WikiExtractor.dropNested"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.dropNested"], ["", "def", "transform1", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Transform text not containing <nowiki>\"\"\"", "\n", "if", "options", ".", "expand_templates", ":", "\n", "# expand templates", "\n", "# See: http://www.mediawiki.org/wiki/Help:Templates", "\n", "            ", "return", "self", ".", "expand", "(", "text", ")", "\n", "", "else", ":", "\n", "# Drop transclusions (template, parser functions)", "\n", "            ", "return", "dropNested", "(", "text", ",", "r'{{'", ",", "r'}}'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.wiki2text": [[693, 747], ["quote_quote.sub.replace().replace", "WikiExtractor.replaceInternalLinks", "WikiExtractor.replaceExternalLinks", "magicWordsRE.sub", "syntaxhighlight.finditer", "WikiExtractor.dropNested", "WikiExtractor.dropNested", "bold_italic.sub", "bold.sub", "italic.sub", "bold_italic.sub", "bold.sub", "italic_quote.sub", "italic.sub", "quote_quote.sub", "m.end", "WikiExtractor.unescape", "quote_quote.sub.replace", "WikiExtractor.unescape", "m.group", "m.start"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.replaceInternalLinks", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.replaceExternalLinks", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.dropNested", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.dropNested", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.unescape", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.unescape"], ["", "", "def", "wiki2text", "(", "self", ",", "text", ")", ":", "\n", "#", "\n", "# final part of internalParse().)", "\n", "#", "\n", "# $text = $this->doTableStuff( $text );", "\n", "# $text = preg_replace( '/(^|\\n)-----*/', '\\\\1<hr />', $text );", "\n", "# $text = $this->doDoubleUnderscore( $text );", "\n", "# $text = $this->doHeadings( $text );", "\n", "# $text = $this->replaceInternalLinks( $text );", "\n", "# $text = $this->doAllQuotes( $text );", "\n", "# $text = $this->replaceExternalLinks( $text );", "\n", "# $text = str_replace( self::MARKER_PREFIX . 'NOPARSE', '', $text );", "\n", "# $text = $this->doMagicLinks( $text );", "\n", "# $text = $this->formatHeadings( $text, $origText, $isMain );", "\n", "\n", "# Drop tables", "\n", "# first drop residual templates, or else empty parameter |} might look like end of table.", "\n", "        ", "if", "not", "options", ".", "keep_tables", ":", "\n", "            ", "text", "=", "dropNested", "(", "text", ",", "r'{{'", ",", "r'}}'", ")", "\n", "text", "=", "dropNested", "(", "text", ",", "r'{\\|'", ",", "r'\\|}'", ")", "\n", "\n", "# Handle bold/italic/quote", "\n", "", "if", "options", ".", "toHTML", ":", "\n", "            ", "text", "=", "bold_italic", ".", "sub", "(", "r'<b>\\1</b>'", ",", "text", ")", "\n", "text", "=", "bold", ".", "sub", "(", "r'<b>\\1</b>'", ",", "text", ")", "\n", "text", "=", "italic", ".", "sub", "(", "r'<i>\\1</i>'", ",", "text", ")", "\n", "", "else", ":", "\n", "            ", "text", "=", "bold_italic", ".", "sub", "(", "r'\\1'", ",", "text", ")", "\n", "text", "=", "bold", ".", "sub", "(", "r'\\1'", ",", "text", ")", "\n", "text", "=", "italic_quote", ".", "sub", "(", "r'\"\\1\"'", ",", "text", ")", "\n", "text", "=", "italic", ".", "sub", "(", "r'\"\\1\"'", ",", "text", ")", "\n", "text", "=", "quote_quote", ".", "sub", "(", "r'\"\\1\"'", ",", "text", ")", "\n", "# residuals of unbalanced quotes", "\n", "", "text", "=", "text", ".", "replace", "(", "\"'''\"", ",", "''", ")", ".", "replace", "(", "\"''\"", ",", "'\"'", ")", "\n", "\n", "# replace internal links", "\n", "text", "=", "replaceInternalLinks", "(", "text", ")", "\n", "\n", "# replace external links", "\n", "text", "=", "replaceExternalLinks", "(", "text", ")", "\n", "\n", "# drop MagicWords behavioral switches", "\n", "text", "=", "magicWordsRE", ".", "sub", "(", "''", ",", "text", ")", "\n", "\n", "# ############### Process HTML ###############", "\n", "\n", "# turn into HTML, except for the content of <syntaxhighlight>", "\n", "res", "=", "''", "\n", "cur", "=", "0", "\n", "for", "m", "in", "syntaxhighlight", ".", "finditer", "(", "text", ")", ":", "\n", "            ", "res", "+=", "unescape", "(", "text", "[", "cur", ":", "m", ".", "start", "(", ")", "]", ")", "+", "m", ".", "group", "(", "1", ")", "\n", "cur", "=", "m", ".", "end", "(", ")", "\n", "", "text", "=", "res", "+", "unescape", "(", "text", "[", "cur", ":", "]", ")", "\n", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.clean": [[749, 813], ["comment.finditer", "WikiExtractor.dropSpans", "text.replace.replace.replace().replace", "text.replace.replace.replace", "spaces.sub", "dots.sub", "re.sub", "re.sub", "re.sub", "text.replace.replace.replace().replace", "spans.append", "pattern.finditer", "left.finditer", "right.finditer", "WikiExtractor.dropNested", "WikiExtractor.unescape", "pattern.finditer", "re.sub", "re.sub", "text.replace.replace.replace", "text.replace.replace.replace", "cgi.escape", "spans.append", "spans.append", "spans.append", "text.replace.replace.replace", "text.replace.replace.replace", "text.replace.replace.replace", "m.start", "m.end", "match.group", "m.start", "m.end", "m.start", "m.end", "m.start", "m.end"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.dropSpans", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.dropNested", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.unescape"], ["", "def", "clean", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"\n        Removes irrelevant parts from :param: text.\n        \"\"\"", "\n", "\n", "# Collect spans", "\n", "spans", "=", "[", "]", "\n", "# Drop HTML comments", "\n", "for", "m", "in", "comment", ".", "finditer", "(", "text", ")", ":", "\n", "            ", "spans", ".", "append", "(", "(", "m", ".", "start", "(", ")", ",", "m", ".", "end", "(", ")", ")", ")", "\n", "\n", "# Drop self-closing tags", "\n", "", "for", "pattern", "in", "selfClosing_tag_patterns", ":", "\n", "            ", "for", "m", "in", "pattern", ".", "finditer", "(", "text", ")", ":", "\n", "                ", "spans", ".", "append", "(", "(", "m", ".", "start", "(", ")", ",", "m", ".", "end", "(", ")", ")", ")", "\n", "\n", "# Drop ignored tags", "\n", "", "", "for", "left", ",", "right", "in", "options", ".", "ignored_tag_patterns", ":", "\n", "            ", "for", "m", "in", "left", ".", "finditer", "(", "text", ")", ":", "\n", "                ", "spans", ".", "append", "(", "(", "m", ".", "start", "(", ")", ",", "m", ".", "end", "(", ")", ")", ")", "\n", "", "for", "m", "in", "right", ".", "finditer", "(", "text", ")", ":", "\n", "                ", "spans", ".", "append", "(", "(", "m", ".", "start", "(", ")", ",", "m", ".", "end", "(", ")", ")", ")", "\n", "\n", "# Bulk remove all spans", "\n", "", "", "text", "=", "dropSpans", "(", "spans", ",", "text", ")", "\n", "\n", "# Drop discarded elements", "\n", "for", "tag", "in", "options", ".", "discardElements", ":", "\n", "            ", "text", "=", "dropNested", "(", "text", ",", "r'<\\s*%s\\b[^>/]*>'", "%", "tag", ",", "r'<\\s*/\\s*%s>'", "%", "tag", ")", "\n", "\n", "", "if", "not", "options", ".", "toHTML", ":", "\n", "# Turn into text what is left (&amp;nbsp;) and <syntaxhighlight>", "\n", "            ", "text", "=", "unescape", "(", "text", ")", "\n", "\n", "# Expand placeholders", "\n", "", "for", "pattern", ",", "placeholder", "in", "placeholder_tag_patterns", ":", "\n", "            ", "index", "=", "1", "\n", "for", "match", "in", "pattern", ".", "finditer", "(", "text", ")", ":", "\n", "                ", "text", "=", "text", ".", "replace", "(", "match", ".", "group", "(", ")", ",", "'%s_%d'", "%", "(", "placeholder", ",", "index", ")", ")", "\n", "index", "+=", "1", "\n", "\n", "", "", "text", "=", "text", ".", "replace", "(", "'<<'", ",", "'\u00ab'", ")", ".", "replace", "(", "'>>'", ",", "'\u00bb'", ")", "\n", "\n", "#############################################", "\n", "\n", "# Cleanup text", "\n", "text", "=", "text", ".", "replace", "(", "'\\t'", ",", "' '", ")", "\n", "text", "=", "spaces", ".", "sub", "(", "' '", ",", "text", ")", "\n", "text", "=", "dots", ".", "sub", "(", "'...'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "' (,:\\.\\)\\]\u00bb)'", ",", "r'\\1'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "'(\\[\\(\u00ab) '", ",", "r'\\1'", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'\\n\\W+?\\n'", ",", "'\\n'", ",", "text", ",", "flags", "=", "re", ".", "U", ")", "# lines with only punctuations", "\n", "text", "=", "text", ".", "replace", "(", "',,'", ",", "','", ")", ".", "replace", "(", "',.'", ",", "'.'", ")", "\n", "if", "options", ".", "keep_tables", ":", "\n", "# the following regular expressions are used to remove the wikiml chartacters around table strucutures", "\n", "# yet keep the content. The order here is imporant so we remove certain markup like {| and then", "\n", "# then the future html attributes such as 'style'. Finally we drop the remaining '|-' that delimits cells.", "\n", "            ", "text", "=", "re", ".", "sub", "(", "r'!(?:\\s)?style=\\\"[a-z]+:(?:\\d+)%;\\\"'", ",", "r''", ",", "text", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'!(?:\\s)?style=\"[a-z]+:(?:\\d+)%;[a-z]+:(?:#)?(?:[0-9a-z]+)?\"'", ",", "r''", ",", "text", ")", "\n", "text", "=", "text", ".", "replace", "(", "'|-'", ",", "''", ")", "\n", "text", "=", "text", ".", "replace", "(", "'|'", ",", "''", ")", "\n", "", "if", "options", ".", "toHTML", ":", "\n", "            ", "text", "=", "cgi", ".", "escape", "(", "text", ")", "\n", "", "return", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand": [[825, 864], ["WikiExtractor.findMatchingBraces", "WikiExtractor.Extractor.expandTemplate"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.findMatchingBraces", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expandTemplate"], ["def", "expand", "(", "self", ",", "wikitext", ")", ":", "\n", "        ", "\"\"\"\n        :param wikitext: the text to be expanded.\n\n        Templates are frequently nested. Occasionally, parsing mistakes may\n        cause template insertion to enter an infinite loop, for instance when\n        trying to instantiate Template:Country\n\n        {{country_{{{1}}}|{{{2}}}|{{{2}}}|size={{{size|}}}|name={{{name|}}}}}\n\n        which is repeatedly trying to insert template 'country_', which is\n        again resolved to Template:Country. The straightforward solution of\n        keeping track of templates that were already inserted for the current\n        article would not work, because the same template may legally be used\n        more than once, with different parameters in different parts of the\n        article.  Therefore, we limit the number of iterations of nested\n        template inclusion.\n\n        \"\"\"", "\n", "# Test template expansion at:", "\n", "# https://en.wikipedia.org/wiki/Special:ExpandTemplates", "\n", "# https://it.wikipedia.org/wiki/Speciale:EspandiTemplate", "\n", "\n", "res", "=", "''", "\n", "if", "self", ".", "frame", ".", "depth", ">=", "self", ".", "maxTemplateRecursionLevels", ":", "\n", "            ", "self", ".", "recursion_exceeded_1_errs", "+=", "1", "\n", "return", "res", "\n", "\n", "# logging.debug('%*s<expand', self.frame.depth, '')", "\n", "\n", "", "cur", "=", "0", "\n", "# look for matching {{...}}", "\n", "for", "s", ",", "e", "in", "findMatchingBraces", "(", "wikitext", ",", "2", ")", ":", "\n", "            ", "res", "+=", "wikitext", "[", "cur", ":", "s", "]", "+", "self", ".", "expandTemplate", "(", "wikitext", "[", "s", "+", "2", ":", "e", "-", "2", "]", ")", "\n", "cur", "=", "e", "\n", "# leftover", "\n", "", "res", "+=", "wikitext", "[", "cur", ":", "]", "\n", "# logging.debug('%*sexpand> %s', self.frame.depth, '', res)", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.templateParams": [[866, 933], ["re.match", "re.match.group().strip", "re.match.group", "parameterValue.strip.strip.strip", "param.strip.strip.strip", "re.match.group", "str"], "methods", ["None"], ["", "def", "templateParams", "(", "self", ",", "parameters", ")", ":", "\n", "        ", "\"\"\"\n        Build a dictionary with positional or name key to expanded parameters.\n        :param parameters: the parts[1:] of a template, i.e. all except the title.\n        \"\"\"", "\n", "templateParams", "=", "{", "}", "\n", "\n", "if", "not", "parameters", ":", "\n", "            ", "return", "templateParams", "\n", "# logging.debug('%*s<templateParams: %s', self.frame.length, '', '|'.join(parameters))", "\n", "\n", "# Parameters can be either named or unnamed. In the latter case, their", "\n", "# name is defined by their ordinal position (1, 2, 3, ...).", "\n", "\n", "", "unnamedParameterCounter", "=", "0", "\n", "\n", "# It's legal for unnamed parameters to be skipped, in which case they", "\n", "# will get default values (if available) during actual instantiation.", "\n", "# That is {{template_name|a||c}} means parameter 1 gets", "\n", "# the value 'a', parameter 2 value is not defined, and parameter 3 gets", "\n", "# the value 'c'.  This case is correctly handled by function 'split',", "\n", "# and does not require any special handling.", "\n", "for", "param", "in", "parameters", ":", "\n", "# Spaces before or after a parameter value are normally ignored,", "\n", "# UNLESS the parameter contains a link (to prevent possible gluing", "\n", "# the link to the following text after template substitution)", "\n", "\n", "# Parameter values may contain \"=\" symbols, hence the parameter", "\n", "# name extends up to the first such symbol.", "\n", "\n", "# It is legal for a parameter to be specified several times, in", "\n", "# which case the last assignment takes precedence. Example:", "\n", "# \"{{t|a|b|c|2=B}}\" is equivalent to \"{{t|a|B|c}}\".", "\n", "# Therefore, we don't check if the parameter has been assigned a", "\n", "# value before, because anyway the last assignment should override", "\n", "# any previous ones.", "\n", "# FIXME: Don't use DOTALL here since parameters may be tags with", "\n", "# attributes, e.g. <div class=\"templatequotecite\">", "\n", "# Parameters may span several lines, like:", "\n", "# {{Reflist|colwidth=30em|refs=", "\n", "# &lt;ref name=&quot;Goode&quot;&gt;Title&lt;/ref&gt;", "\n", "\n", "# The '=' might occurr within an HTML attribute:", "\n", "#   \"&lt;ref name=value\"", "\n", "# but we stop at first.", "\n", "            ", "m", "=", "re", ".", "match", "(", "' *([^=]*?) *?=(.*)'", ",", "param", ",", "re", ".", "DOTALL", ")", "\n", "if", "m", ":", "\n", "# This is a named parameter.  This case also handles parameter", "\n", "# assignments like \"2=xxx\", where the number of an unnamed", "\n", "# parameter (\"2\") is specified explicitly - this is handled", "\n", "# transparently.", "\n", "\n", "                ", "parameterName", "=", "m", ".", "group", "(", "1", ")", ".", "strip", "(", ")", "\n", "parameterValue", "=", "m", ".", "group", "(", "2", ")", "\n", "\n", "if", "']]'", "not", "in", "parameterValue", ":", "# if the value does not contain a link, trim whitespace", "\n", "                    ", "parameterValue", "=", "parameterValue", ".", "strip", "(", ")", "\n", "", "templateParams", "[", "parameterName", "]", "=", "parameterValue", "\n", "", "else", ":", "\n", "# this is an unnamed parameter", "\n", "                ", "unnamedParameterCounter", "+=", "1", "\n", "\n", "if", "']]'", "not", "in", "param", ":", "# if the value does not contain a link, trim whitespace", "\n", "                    ", "param", "=", "param", ".", "strip", "(", ")", "\n", "", "templateParams", "[", "str", "(", "unnamedParameterCounter", ")", "]", "=", "param", "\n", "# logging.debug('%*stemplateParams> %s', self.frame.length, '', '|'.join(templateParams.values()))", "\n", "", "", "return", "templateParams", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expandTemplate": [[935, 1104], ["logging.debug", "WikiExtractor.splitParts", "parts[].strip", "WikiExtractor.Extractor.expand", "re.match", "re.sub.find", "WikiExtractor.fullyQualifiedTemplateTitle", "options.redirects.get", "logging.debug", "WikiExtractor.Extractor.templateParams", "WikiExtractor.Extractor.frame.push", "WikiExtractor.Template.parse", "WikiExtractor.Extractor.transform", "WikiExtractor.Extractor.frame.pop", "logging.debug", "re.sub", "logging.debug", "title[].strip", "WikiExtractor.callParserFunction", "logging.debug", "WikiExtractor.Template.parse", "logging.debug", "WikiExtractor.Extractor.transform"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.splitParts", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.fullyQualifiedTemplateTitle", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.templateParams", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.push", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Template.parse", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.callParserFunction", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Template.parse", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform"], ["", "def", "expandTemplate", "(", "self", ",", "body", ")", ":", "\n", "        ", "\"\"\"Expands template invocation.\n        :param body: the parts of a template.\n\n        :see http://meta.wikimedia.org/wiki/Help:Expansion for an explanation\n        of the process.\n\n        See in particular: Expansion of names and values\n        http://meta.wikimedia.org/wiki/Help:Expansion#Expansion_of_names_and_values\n\n        For most parser functions all names and values are expanded,\n        regardless of what is relevant for the result. The branching functions\n        (#if, #ifeq, #iferror, #ifexist, #ifexpr, #switch) are exceptions.\n\n        All names in a template call are expanded, and the titles of the\n        tplargs in the template body, after which it is determined which\n        values must be expanded, and for which tplargs in the template body\n        the first part (default) [sic in the original doc page].\n\n        In the case of a tplarg, any parts beyond the first are never\n        expanded.  The possible name and the value of the first part is\n        expanded if the title does not match a name in the template call.\n\n        :see code for braceSubstitution at\n        https://doc.wikimedia.org/mediawiki-core/master/php/html/Parser_8php_source.html#3397:\n\n        \"\"\"", "\n", "\n", "# template        = \"{{\" parts \"}}\"", "\n", "\n", "# Templates and tplargs are decomposed in the same way, with pipes as", "\n", "# separator, even though eventually any parts in a tplarg after the first", "\n", "# (the parameter default) are ignored, and an equals sign in the first", "\n", "# part is treated as plain text.", "\n", "# Pipes inside inner templates and tplargs, or inside double rectangular", "\n", "# brackets within the template or tplargs are not taken into account in", "\n", "# this decomposition.", "\n", "# The first part is called title, the other parts are simply called parts.", "\n", "\n", "# If a part has one or more equals signs in it, the first equals sign", "\n", "# determines the division into name = value. Equals signs inside inner", "\n", "# templates and tplargs, or inside double rectangular brackets within the", "\n", "# part are not taken into account in this decomposition. Parts without", "\n", "# equals sign are indexed 1, 2, .., given as attribute in the <name> tag.", "\n", "\n", "if", "self", ".", "frame", ".", "depth", ">=", "self", ".", "maxTemplateRecursionLevels", ":", "\n", "            ", "self", ".", "recursion_exceeded_2_errs", "+=", "1", "\n", "# logging.debug('%*sEXPAND> %s', self.frame.depth, '', body)", "\n", "return", "''", "\n", "\n", "", "logging", ".", "debug", "(", "'%*sEXPAND %s'", ",", "self", ".", "frame", ".", "depth", ",", "''", ",", "body", ")", "\n", "parts", "=", "splitParts", "(", "body", ")", "\n", "# title is the portion before the first |", "\n", "title", "=", "parts", "[", "0", "]", ".", "strip", "(", ")", "\n", "title", "=", "self", ".", "expand", "(", "title", ")", "\n", "\n", "# SUBST", "\n", "# Apply the template tag to parameters without", "\n", "# substituting into them, e.g.", "\n", "# {{subst:t|a{{{p|q}}}b}} gives the wikitext start-a{{{p|q}}}b-end", "\n", "# @see https://www.mediawiki.org/wiki/Manual:Substitution#Partial_substitution", "\n", "subst", "=", "False", "\n", "if", "re", ".", "match", "(", "substWords", ",", "title", ",", "re", ".", "IGNORECASE", ")", ":", "\n", "            ", "title", "=", "re", ".", "sub", "(", "substWords", ",", "''", ",", "title", ",", "1", ",", "re", ".", "IGNORECASE", ")", "\n", "subst", "=", "True", "\n", "\n", "", "if", "title", "in", "self", ".", "magicWords", ".", "values", ":", "\n", "            ", "ret", "=", "self", ".", "magicWords", "[", "title", "]", "\n", "logging", ".", "debug", "(", "'%*s<EXPAND %s %s'", ",", "self", ".", "frame", ".", "depth", ",", "''", ",", "title", ",", "ret", ")", "\n", "return", "ret", "\n", "\n", "# Parser functions.", "\n", "\n", "# For most parser functions all names and values are expanded,", "\n", "# regardless of what is relevant for the result. The branching", "\n", "# functions (#if, #ifeq, #iferror, #ifexist, #ifexpr, #switch) are", "\n", "# exceptions: for #if, #iferror, #ifexist, #ifexp, only the part that", "\n", "# is applicable is expanded; for #ifeq the first and the applicable", "\n", "# part are expanded; for #switch, expanded are the names up to and", "\n", "# including the match (or all if there is no match), and the value in", "\n", "# the case of a match or if there is no match, the default, if any.", "\n", "\n", "# The first argument is everything after the first colon.", "\n", "# It has been evaluated above.", "\n", "", "colon", "=", "title", ".", "find", "(", "':'", ")", "\n", "if", "colon", ">", "1", ":", "\n", "            ", "funct", "=", "title", "[", ":", "colon", "]", "\n", "parts", "[", "0", "]", "=", "title", "[", "colon", "+", "1", ":", "]", ".", "strip", "(", ")", "# side-effect (parts[0] not used later)", "\n", "# arguments after first are not evaluated", "\n", "ret", "=", "callParserFunction", "(", "funct", ",", "parts", ",", "self", ")", "\n", "logging", ".", "debug", "(", "'%*s<EXPAND %s %s'", ",", "self", ".", "frame", ".", "depth", ",", "''", ",", "funct", ",", "ret", ")", "\n", "return", "ret", "\n", "\n", "", "title", "=", "fullyQualifiedTemplateTitle", "(", "title", ")", "\n", "if", "not", "title", ":", "\n", "            ", "self", ".", "template_title_errs", "+=", "1", "\n", "return", "''", "\n", "\n", "", "redirected", "=", "options", ".", "redirects", ".", "get", "(", "title", ")", "\n", "if", "redirected", ":", "\n", "            ", "title", "=", "redirected", "\n", "\n", "# get the template", "\n", "", "if", "title", "in", "options", ".", "templateCache", ":", "\n", "            ", "template", "=", "options", ".", "templateCache", "[", "title", "]", "\n", "", "elif", "title", "in", "options", ".", "templates", ":", "\n", "            ", "template", "=", "Template", ".", "parse", "(", "options", ".", "templates", "[", "title", "]", ")", "\n", "# add it to cache", "\n", "options", ".", "templateCache", "[", "title", "]", "=", "template", "\n", "del", "options", ".", "templates", "[", "title", "]", "\n", "", "else", ":", "\n", "# The page being included could not be identified", "\n", "            ", "logging", ".", "debug", "(", "'%*s<EXPAND %s %s'", ",", "self", ".", "frame", ".", "depth", ",", "''", ",", "title", ",", "''", ")", "\n", "return", "''", "\n", "\n", "", "logging", ".", "debug", "(", "'%*sTEMPLATE %s: %s'", ",", "self", ".", "frame", ".", "depth", ",", "''", ",", "title", ",", "template", ")", "\n", "\n", "# tplarg          = \"{{{\" parts \"}}}\"", "\n", "# parts           = [ title *( \"|\" part ) ]", "\n", "# part            = ( part-name \"=\" part-value ) / ( part-value )", "\n", "# part-name       = wikitext-L3", "\n", "# part-value      = wikitext-L3", "\n", "# wikitext-L3     = literal / template / tplarg / link / comment /", "\n", "#                   line-eating-comment / unclosed-comment /", "\n", "#           \t    xmlish-element / *wikitext-L3", "\n", "\n", "# A tplarg may contain other parameters as well as templates, e.g.:", "\n", "#   {{{text|{{{quote|{{{1|{{error|Error: No text given}}}}}}}}}}}", "\n", "# hence no simple RE like this would work:", "\n", "#   '{{{((?:(?!{{{).)*?)}}}'", "\n", "# We must use full CF parsing.", "\n", "\n", "# the parameter name itself might be computed, e.g.:", "\n", "#   {{{appointe{{#if:{{{appointer14|}}}|r|d}}14|}}}", "\n", "\n", "# Because of the multiple uses of double-brace and triple-brace", "\n", "# syntax, expressions can sometimes be ambiguous.", "\n", "# Precedence rules specifed here:", "\n", "# http://www.mediawiki.org/wiki/Preprocessor_ABNF#Ideal_precedence", "\n", "# resolve ambiguities like this:", "\n", "#   {{{{ }}}} -> { {{{ }}} }", "\n", "#   {{{{{ }}}}} -> {{ {{{ }}} }}", "\n", "#", "\n", "# :see: https://en.wikipedia.org/wiki/Help:Template#Handling_parameters", "\n", "\n", "params", "=", "parts", "[", "1", ":", "]", "\n", "\n", "# Order of evaluation.", "\n", "# Template parameters are fully evaluated before they are passed to the template.", "\n", "# :see: https://www.mediawiki.org/wiki/Help:Templates#Order_of_evaluation", "\n", "if", "not", "subst", ":", "\n", "# Evaluate parameters, since they may contain templates, including", "\n", "# the symbol \"=\".", "\n", "# {{#ifexpr: {{{1}}} = 1 }}", "\n", "            ", "params", "=", "[", "self", ".", "transform", "(", "p", ")", "for", "p", "in", "params", "]", "\n", "\n", "# build a dict of name-values for the parameter values", "\n", "", "params", "=", "self", ".", "templateParams", "(", "params", ")", "\n", "\n", "# Perform parameter substitution.", "\n", "# Extend frame before subst, since there may be recursion in default", "\n", "# parameter value, e.g. {{OTRS|celebrative|date=April 2015}} in article", "\n", "# 21637542 in enwiki.", "\n", "self", ".", "frame", "=", "self", ".", "frame", ".", "push", "(", "title", ",", "params", ")", "\n", "instantiated", "=", "template", ".", "subst", "(", "params", ",", "self", ")", "\n", "value", "=", "self", ".", "transform", "(", "instantiated", ")", "\n", "self", ".", "frame", "=", "self", ".", "frame", ".", "pop", "(", ")", "\n", "logging", ".", "debug", "(", "'%*s<EXPAND %s %s'", ",", "self", ".", "frame", ".", "depth", ",", "''", ",", "title", ",", "value", ")", "\n", "return", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.MagicWords.__init__": [[1631, 1633], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "values", "=", "{", "'!'", ":", "'|'", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.MagicWords.__getitem__": [[1634, 1636], ["WikiExtractor.MagicWords.values.get"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "name", ")", ":", "\n", "        ", "return", "self", ".", "values", ".", "get", "(", "name", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.MagicWords.__setitem__": [[1637, 1639], ["None"], "methods", ["None"], ["", "def", "__setitem__", "(", "self", ",", "name", ",", "value", ")", ":", "\n", "        ", "self", ".", "values", "[", "name", "]", "=", "value", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Infix.__init__": [[1739, 1741], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "function", ")", ":", "\n", "        ", "self", ".", "function", "=", "function", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Infix.__ror__": [[1742, 1744], ["WikiExtractor.Infix", "WikiExtractor.Infix.function"], "methods", ["None"], ["", "def", "__ror__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "Infix", "(", "lambda", "x", ",", "self", "=", "self", ",", "other", "=", "other", ":", "self", ".", "function", "(", "other", ",", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Infix.__or__": [[1745, 1747], ["WikiExtractor.Infix.function"], "methods", ["None"], ["", "def", "__or__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "function", "(", "other", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Infix.__rlshift__": [[1748, 1750], ["WikiExtractor.Infix", "WikiExtractor.Infix.function"], "methods", ["None"], ["", "def", "__rlshift__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "Infix", "(", "lambda", "x", ",", "self", "=", "self", ",", "other", "=", "other", ":", "self", ".", "function", "(", "other", ",", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Infix.__rshift__": [[1751, 1753], ["WikiExtractor.Infix.function"], "methods", ["None"], ["", "def", "__rshift__", "(", "self", ",", "other", ")", ":", "\n", "        ", "return", "self", ".", "function", "(", "other", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Infix.__call__": [[1754, 1756], ["WikiExtractor.Infix.function"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "value1", ",", "value2", ")", ":", "\n", "        ", "return", "self", ".", "function", "(", "value1", ",", "value2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.NextFile.__init__": [[2673, 2677], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "path_name", ")", ":", "\n", "        ", "self", ".", "path_name", "=", "path_name", "\n", "self", ".", "dir_index", "=", "-", "1", "\n", "self", ".", "file_index", "=", "-", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.NextFile.__next__": [[2678, 2686], ["WikiExtractor.NextFile._dirname", "WikiExtractor.NextFile._filepath", "os.path.isdir", "os.makedirs"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.NextFile._dirname", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.NextFile._filepath"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "self", ".", "file_index", "=", "(", "self", ".", "file_index", "+", "1", ")", "%", "NextFile", ".", "filesPerDir", "\n", "if", "self", ".", "file_index", "==", "0", ":", "\n", "            ", "self", ".", "dir_index", "+=", "1", "\n", "", "dirname", "=", "self", ".", "_dirname", "(", ")", "\n", "if", "not", "os", ".", "path", ".", "isdir", "(", "dirname", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "dirname", ")", "\n", "", "return", "self", ".", "_filepath", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.NextFile._dirname": [[2689, 2693], ["os.path.join", "ord", "ord"], "methods", ["None"], ["def", "_dirname", "(", "self", ")", ":", "\n", "        ", "char1", "=", "self", ".", "dir_index", "%", "26", "\n", "char2", "=", "self", ".", "dir_index", "//", "26", "%", "26", "\n", "return", "os", ".", "path", ".", "join", "(", "self", ".", "path_name", ",", "'%c%c'", "%", "(", "ord", "(", "'A'", ")", "+", "char2", ",", "ord", "(", "'A'", ")", "+", "char1", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.NextFile._filepath": [[2694, 2696], ["WikiExtractor.NextFile._dirname"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.NextFile._dirname"], ["", "def", "_filepath", "(", "self", ")", ":", "\n", "        ", "return", "'%s/wiki_%02d'", "%", "(", "self", ".", "_dirname", "(", ")", ",", "self", ".", "file_index", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.__init__": [[2703, 2714], ["WikiExtractor.OutputSplitter.open", "next"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["def", "__init__", "(", "self", ",", "nextFile", ",", "max_file_size", "=", "0", ",", "compress", "=", "True", ")", ":", "\n", "        ", "\"\"\"\n        :param nextFile: a NextFile object from which to obtain filenames\n            to use.\n        :param max_file_size: the maximum size of each file.\n        :para compress: whether to write data with bzip compression.\n        \"\"\"", "\n", "self", ".", "nextFile", "=", "nextFile", "\n", "self", ".", "compress", "=", "compress", "\n", "self", ".", "max_file_size", "=", "max_file_size", "\n", "self", ".", "file", "=", "self", ".", "open", "(", "next", "(", "self", ".", "nextFile", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.reserve": [[2715, 2719], ["WikiExtractor.OutputSplitter.close", "WikiExtractor.OutputSplitter.open", "WikiExtractor.OutputSplitter.file.tell", "next"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "def", "reserve", "(", "self", ",", "size", ")", ":", "\n", "        ", "if", "self", ".", "file", ".", "tell", "(", ")", "+", "size", ">", "self", ".", "max_file_size", ":", "\n", "            ", "self", ".", "close", "(", ")", "\n", "self", ".", "file", "=", "self", ".", "open", "(", "next", "(", "self", ".", "nextFile", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write": [[2720, 2723], ["WikiExtractor.OutputSplitter.reserve", "WikiExtractor.OutputSplitter.file.write", "len"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.reserve", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "", "def", "write", "(", "self", ",", "data", ")", ":", "\n", "        ", "self", ".", "reserve", "(", "len", "(", "data", ")", ")", "\n", "self", ".", "file", ".", "write", "(", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close": [[2724, 2726], ["WikiExtractor.OutputSplitter.file.close"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close"], ["", "def", "close", "(", "self", ")", ":", "\n", "        ", "self", ".", "file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open": [[2727, 2732], ["bz2.BZ2File", "WikiExtractor.OutputSplitter.open"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "def", "open", "(", "self", ",", "filename", ")", ":", "\n", "        ", "if", "self", ".", "compress", ":", "\n", "            ", "return", "bz2", ".", "BZ2File", "(", "filename", "+", "'.bz2'", ",", "'w'", ")", "\n", "", "else", ":", "\n", "            ", "return", "open", "(", "filename", ",", "'wb'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.keepPage": [[220, 239], ["logging.debug", "logging.debug", "filter_disambig_page_pattern.match", "len", "len", "len", "len", "str", "str"], "function", ["None"], ["def", "keepPage", "(", "ns", ",", "catSet", ",", "page", ")", ":", "\n", "    ", "global", "g_page_articl_total", ",", "g_page_total", ",", "g_page_articl_used_total", "\n", "g_page_total", "+=", "1", "\n", "if", "ns", "!=", "'0'", ":", "# Aritcle", "\n", "        ", "return", "False", "\n", "# remove disambig pages if desired", "\n", "", "g_page_articl_total", "+=", "1", "\n", "if", "options", ".", "filter_disambig_pages", ":", "\n", "        ", "for", "line", "in", "page", ":", "\n", "            ", "if", "filter_disambig_page_pattern", ".", "match", "(", "line", ")", ":", "\n", "                ", "return", "False", "\n", "", "", "", "if", "len", "(", "options", ".", "filter_category_include", ")", ">", "0", "and", "len", "(", "options", ".", "filter_category_include", "&", "catSet", ")", "==", "0", ":", "\n", "        ", "logging", ".", "debug", "(", "\"***No include  \"", "+", "str", "(", "catSet", ")", ")", "\n", "return", "False", "\n", "", "if", "len", "(", "options", ".", "filter_category_exclude", ")", ">", "0", "and", "len", "(", "options", ".", "filter_category_exclude", "&", "catSet", ")", ">", "0", ":", "\n", "        ", "logging", ".", "debug", "(", "\"***Exclude  \"", "+", "str", "(", "catSet", ")", ")", "\n", "return", "False", "\n", "", "g_page_articl_used_total", "+=", "1", "\n", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.get_url": [[241, 243], ["None"], "function", ["None"], ["", "def", "get_url", "(", "uid", ")", ":", "\n", "    ", "return", "\"%s?curid=%s\"", "%", "(", "options", ".", "urlbase", ",", "uid", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.normalizeTitle": [[286, 322], ["ucfirst.strip", "re.sub", "re.match", "re.match.group", "re.match.group", "re.match.group", "WikiExtractor.normalizeNamespace", "WikiExtractor.ucfirst", "WikiExtractor.ucfirst", "WikiExtractor.ucfirst", "WikiExtractor.ucfirst"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.normalizeNamespace", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ucfirst", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ucfirst", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ucfirst", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ucfirst"], ["def", "normalizeTitle", "(", "title", ")", ":", "\n", "    ", "\"\"\"Normalize title\"\"\"", "\n", "# remove leading/trailing whitespace and underscores", "\n", "title", "=", "title", ".", "strip", "(", "' _'", ")", "\n", "# replace sequences of whitespace and underscore chars with a single space", "\n", "title", "=", "re", ".", "sub", "(", "r'[\\s_]+'", ",", "' '", ",", "title", ")", "\n", "\n", "m", "=", "re", ".", "match", "(", "r'([^:]*):(\\s*)(\\S(?:.*))'", ",", "title", ")", "\n", "if", "m", ":", "\n", "        ", "prefix", "=", "m", ".", "group", "(", "1", ")", "\n", "if", "m", ".", "group", "(", "2", ")", ":", "\n", "            ", "optionalWhitespace", "=", "' '", "\n", "", "else", ":", "\n", "            ", "optionalWhitespace", "=", "''", "\n", "", "rest", "=", "m", ".", "group", "(", "3", ")", "\n", "\n", "ns", "=", "normalizeNamespace", "(", "prefix", ")", "\n", "if", "ns", "in", "options", ".", "knownNamespaces", ":", "\n", "# If the prefix designates a known namespace, then it might be", "\n", "# followed by optional whitespace that should be removed to get", "\n", "# the canonical page name", "\n", "# (e.g., \"Category:  Births\" should become \"Category:Births\").", "\n", "            ", "title", "=", "ns", "+", "\":\"", "+", "ucfirst", "(", "rest", ")", "\n", "", "else", ":", "\n", "# No namespace, just capitalize first letter.", "\n", "# If the part before the colon is not a known namespace, then we", "\n", "# must not remove the space after the colon (if any), e.g.,", "\n", "# \"3001: The_Final_Odyssey\" != \"3001:The_Final_Odyssey\".", "\n", "# However, to get the canonical page name we must contract multiple", "\n", "# spaces into one, because", "\n", "# \"3001:   The_Final_Odyssey\" != \"3001: The_Final_Odyssey\".", "\n", "            ", "title", "=", "ucfirst", "(", "prefix", ")", "+", "\":\"", "+", "optionalWhitespace", "+", "ucfirst", "(", "rest", ")", "\n", "", "", "else", ":", "\n", "# no namespace, just capitalize first letter", "\n", "        ", "title", "=", "ucfirst", "(", "title", ")", "\n", "", "return", "title", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.unescape": [[324, 347], ["re.sub", "m.group", "m.group", "chr", "chr", "chr", "int", "int"], "function", ["None"], ["", "def", "unescape", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    Removes HTML or XML character references and entities from a text string.\n\n    :param text The HTML (or XML) source text.\n    :return The plain text, as a Unicode string, if necessary.\n    \"\"\"", "\n", "\n", "def", "fixup", "(", "m", ")", ":", "\n", "        ", "text", "=", "m", ".", "group", "(", "0", ")", "\n", "code", "=", "m", ".", "group", "(", "1", ")", "\n", "try", ":", "\n", "            ", "if", "text", "[", "1", "]", "==", "\"#\"", ":", "# character reference", "\n", "                ", "if", "text", "[", "2", "]", "==", "\"x\"", ":", "\n", "                    ", "return", "chr", "(", "int", "(", "code", "[", "1", ":", "]", ",", "16", ")", ")", "\n", "", "else", ":", "\n", "                    ", "return", "chr", "(", "int", "(", "code", ")", ")", "\n", "", "", "else", ":", "# named entity", "\n", "                ", "return", "chr", "(", "name2codepoint", "[", "code", "]", ")", "\n", "", "", "except", ":", "\n", "            ", "return", "text", "# leave as is", "\n", "\n", "", "", "return", "re", ".", "sub", "(", "\"&#?(\\w+);\"", ",", "fixup", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ignoreTag": [[358, 362], ["re.compile", "re.compile", "options.ignored_tag_patterns.append"], "function", ["None"], ["def", "ignoreTag", "(", "tag", ")", ":", "\n", "    ", "left", "=", "re", ".", "compile", "(", "r'<%s\\b.*?>'", "%", "tag", ",", "re", ".", "IGNORECASE", "|", "re", ".", "DOTALL", ")", "# both <ref> and <reference>", "\n", "right", "=", "re", ".", "compile", "(", "r'</\\s*%s>'", "%", "tag", ",", "re", ".", "IGNORECASE", ")", "\n", "options", ".", "ignored_tag_patterns", ".", "append", "(", "(", "left", ",", "right", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.splitParts": [[1110, 1181], ["WikiExtractor.findMatchingBraces", "paramsList[].split", "paramsList[].split", "len", "parameters.extend", "len", "parameters.extend"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.findMatchingBraces"], ["", "", "def", "splitParts", "(", "paramsList", ")", ":", "\n", "    ", "\"\"\"\n    :param paramsList: the parts of a template or tplarg.\n\n    Split template parameters at the separator \"|\".\n    separator \"=\".\n\n    Template parameters often contain URLs, internal links, text or even\n    template expressions, since we evaluate templates outside in.\n    This is required for cases like:\n      {{#if: {{{1}}} | {{lc:{{{1}}} | \"parameter missing\"}}\n    Parameters are separated by \"|\" symbols. However, we\n    cannot simply split the string on \"|\" symbols, since these\n    also appear inside templates and internal links, e.g.\n\n     {{if:|\n      |{{#if:the president|\n           |{{#if:|\n               [[Category:Hatnote templates|A{{PAGENAME}}]]\n            }}\n       }}\n     }}\n\n    We split parts at the \"|\" symbols that are not inside any pair\n    {{{...}}}, {{...}}, [[...]], {|...|}.\n    \"\"\"", "\n", "\n", "# Must consider '[' as normal in expansion of Template:EMedicine2:", "\n", "# #ifeq: ped|article|[http://emedicine.medscape.com/article/180-overview|[http://www.emedicine.com/ped/topic180.htm#{{#if: |section~}}", "\n", "# as part of:", "\n", "# {{#ifeq: ped|article|[http://emedicine.medscape.com/article/180-overview|[http://www.emedicine.com/ped/topic180.htm#{{#if: |section~}}}} ped/180{{#if: |~}}]", "\n", "\n", "# should handle both tpl arg like:", "\n", "#    4|{{{{{subst|}}}CURRENTYEAR}}", "\n", "# and tpl parameters like:", "\n", "#    ||[[Category:People|{{#if:A|A|{{PAGENAME}}}}]]", "\n", "\n", "sep", "=", "'|'", "\n", "parameters", "=", "[", "]", "\n", "cur", "=", "0", "\n", "\n", "for", "s", ",", "e", "in", "findMatchingBraces", "(", "paramsList", ")", ":", "\n", "        ", "par", "=", "paramsList", "[", "cur", ":", "s", "]", ".", "split", "(", "sep", ")", "\n", "if", "par", ":", "\n", "            ", "if", "parameters", ":", "\n", "# portion before | belongs to previous parameter", "\n", "                ", "parameters", "[", "-", "1", "]", "+=", "par", "[", "0", "]", "\n", "if", "len", "(", "par", ")", ">", "1", ":", "\n", "# rest are new parameters", "\n", "                    ", "parameters", ".", "extend", "(", "par", "[", "1", ":", "]", ")", "\n", "", "", "else", ":", "\n", "                ", "parameters", "=", "par", "\n", "", "", "elif", "not", "parameters", ":", "\n", "            ", "parameters", "=", "[", "''", "]", "# create first param", "\n", "# add span to last previous parameter", "\n", "", "parameters", "[", "-", "1", "]", "+=", "paramsList", "[", "s", ":", "e", "]", "\n", "cur", "=", "e", "\n", "# leftover", "\n", "", "par", "=", "paramsList", "[", "cur", ":", "]", ".", "split", "(", "sep", ")", "\n", "if", "par", ":", "\n", "        ", "if", "parameters", ":", "\n", "# portion before | belongs to previous parameter", "\n", "            ", "parameters", "[", "-", "1", "]", "+=", "par", "[", "0", "]", "\n", "if", "len", "(", "par", ")", ">", "1", ":", "\n", "# rest are new parameters", "\n", "                ", "parameters", ".", "extend", "(", "par", "[", "1", ":", "]", ")", "\n", "", "", "else", ":", "\n", "            ", "parameters", "=", "par", "\n", "\n", "# logging.debug('splitParts %s %s\\nparams: %s', sep, paramsList, text_type(parameters))", "\n", "", "", "return", "parameters", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.findMatchingBraces": [[1183, 1291], ["re.compile", "re.compile", "re.compile", "re.compile", "re.compile.search", "reOpen.search.end", "reOpen.search.end", "reOpen.search.start", "re.compile.search", "reNext.search.end", "reOpen.search.group", "reNext.search.group", "reNext.search.end", "reNext.search.start", "stack.append", "stack.pop", "stack.append", "stack.append", "reOpen.search.start", "len", "stack.pop", "stack.append", "reOpen.search.start"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "def", "findMatchingBraces", "(", "text", ",", "ldelim", "=", "0", ")", ":", "\n", "    ", "\"\"\"\n    :param ldelim: number of braces to match. 0 means match [[]], {{}} and {{{}}}.\n    \"\"\"", "\n", "# Parsing is done with respect to pairs of double braces {{..}} delimiting", "\n", "# a template, and pairs of triple braces {{{..}}} delimiting a tplarg.", "\n", "# If double opening braces are followed by triple closing braces or", "\n", "# conversely, this is taken as delimiting a template, with one left-over", "\n", "# brace outside it, taken as plain text. For any pattern of braces this", "\n", "# defines a set of templates and tplargs such that any two are either", "\n", "# separate or nested (not overlapping).", "\n", "\n", "# Unmatched double rectangular closing brackets can be in a template or", "\n", "# tplarg, but unmatched double rectangular opening brackets cannot.", "\n", "# Unmatched double or triple closing braces inside a pair of", "\n", "# double rectangular brackets are treated as plain text.", "\n", "# Other formulation: in ambiguity between template or tplarg on one hand,", "\n", "# and a link on the other hand, the structure with the rightmost opening", "\n", "# takes precedence, even if this is the opening of a link without any", "\n", "# closing, so not producing an actual link.", "\n", "\n", "# In the case of more than three opening braces the last three are assumed", "\n", "# to belong to a tplarg, unless there is no matching triple of closing", "\n", "# braces, in which case the last two opening braces are are assumed to", "\n", "# belong to a template.", "\n", "\n", "# We must skip individual { like in:", "\n", "#   {{#ifeq: {{padleft:|1|}} | { | | &nbsp;}}", "\n", "# We must resolve ambiguities like this:", "\n", "#   {{{{ }}}} -> { {{{ }}} }", "\n", "#   {{{{{ }}}}} -> {{ {{{ }}} }}", "\n", "#   {{#if:{{{{{#if:{{{nominee|}}}|nominee|candidate}}|}}}|...}}", "\n", "#   {{{!}} {{!}}}", "\n", "\n", "# Handle:", "\n", "#   {{{{{|safesubst:}}}#Invoke:String|replace|{{{1|{{{{{|safesubst:}}}PAGENAME}}}}}|%s+%([^%(]-%)$||plain=false}}", "\n", "# as well as expressions with stray }:", "\n", "#   {{{link|{{ucfirst:{{{1}}}}}} interchange}}}", "\n", "\n", "if", "ldelim", ":", "# 2-3", "\n", "        ", "reOpen", "=", "re", ".", "compile", "(", "'[{]{%d,}'", "%", "ldelim", ")", "# at least ldelim", "\n", "reNext", "=", "re", ".", "compile", "(", "'[{]{2,}|}{2,}'", ")", "# at least 2", "\n", "", "else", ":", "\n", "        ", "reOpen", "=", "re", ".", "compile", "(", "'{{2,}|\\[{2,}'", ")", "\n", "reNext", "=", "re", ".", "compile", "(", "'{{2,}|}{2,}|\\[{2,}|]{2,}'", ")", "# at least 2", "\n", "\n", "", "cur", "=", "0", "\n", "while", "True", ":", "\n", "        ", "m1", "=", "reOpen", ".", "search", "(", "text", ",", "cur", ")", "\n", "if", "not", "m1", ":", "\n", "            ", "return", "\n", "", "lmatch", "=", "m1", ".", "end", "(", ")", "-", "m1", ".", "start", "(", ")", "\n", "if", "m1", ".", "group", "(", ")", "[", "0", "]", "==", "'{'", ":", "\n", "            ", "stack", "=", "[", "lmatch", "]", "# stack of opening braces lengths", "\n", "", "else", ":", "\n", "            ", "stack", "=", "[", "-", "lmatch", "]", "# negative means [", "\n", "", "end", "=", "m1", ".", "end", "(", ")", "\n", "while", "True", ":", "\n", "            ", "m2", "=", "reNext", ".", "search", "(", "text", ",", "end", ")", "\n", "if", "not", "m2", ":", "\n", "                ", "return", "# unbalanced", "\n", "", "end", "=", "m2", ".", "end", "(", ")", "\n", "brac", "=", "m2", ".", "group", "(", ")", "[", "0", "]", "\n", "lmatch", "=", "m2", ".", "end", "(", ")", "-", "m2", ".", "start", "(", ")", "\n", "\n", "if", "brac", "==", "'{'", ":", "\n", "                ", "stack", ".", "append", "(", "lmatch", ")", "\n", "", "elif", "brac", "==", "'}'", ":", "\n", "                ", "while", "stack", ":", "\n", "                    ", "openCount", "=", "stack", ".", "pop", "(", ")", "# opening span", "\n", "if", "openCount", "==", "0", ":", "# illegal unmatched [[", "\n", "                        ", "continue", "\n", "", "if", "lmatch", ">=", "openCount", ":", "\n", "                        ", "lmatch", "-=", "openCount", "\n", "if", "lmatch", "<=", "1", ":", "# either close or stray }", "\n", "                            ", "break", "\n", "", "", "else", ":", "\n", "# put back unmatched", "\n", "                        ", "stack", ".", "append", "(", "openCount", "-", "lmatch", ")", "\n", "break", "\n", "", "", "if", "not", "stack", ":", "\n", "                    ", "yield", "m1", ".", "start", "(", ")", ",", "end", "-", "lmatch", "\n", "cur", "=", "end", "\n", "break", "\n", "", "elif", "len", "(", "stack", ")", "==", "1", "and", "0", "<", "stack", "[", "0", "]", "<", "ldelim", ":", "\n", "# ambiguous {{{{{ }}} }}", "\n", "#yield m1.start() + stack[0], end", "\n", "                    ", "cur", "=", "end", "\n", "break", "\n", "", "", "elif", "brac", "==", "'['", ":", "# [[", "\n", "                ", "stack", ".", "append", "(", "-", "lmatch", ")", "\n", "", "else", ":", "# ]]", "\n", "                ", "while", "stack", "and", "stack", "[", "-", "1", "]", "<", "0", ":", "# matching [[", "\n", "                    ", "openCount", "=", "-", "stack", ".", "pop", "(", ")", "\n", "if", "lmatch", ">=", "openCount", ":", "\n", "                        ", "lmatch", "-=", "openCount", "\n", "if", "lmatch", "<=", "1", ":", "# either close or stray ]", "\n", "                            ", "break", "\n", "", "", "else", ":", "\n", "# put back unmatched (negative)", "\n", "                        ", "stack", ".", "append", "(", "lmatch", "-", "openCount", ")", "\n", "break", "\n", "", "", "if", "not", "stack", ":", "\n", "                    ", "yield", "m1", ".", "start", "(", ")", ",", "end", "-", "lmatch", "\n", "cur", "=", "end", "\n", "break", "\n", "# unmatched ]] are discarded", "\n", "", "cur", "=", "end", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.findBalanced": [[1293, 1333], ["re.compile", "re.compile", "nextPat.search", "nextPat.search.group", "nextPat.search.end", "re.escape", "zip", "nextPat.search.start", "stack.append", "stack.pop", "nextPat.search.end", "nextPat.search.end"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "", "", "", "def", "findBalanced", "(", "text", ",", "openDelim", "=", "[", "'[['", "]", ",", "closeDelim", "=", "[", "']]'", "]", ")", ":", "\n", "    ", "\"\"\"\n    Assuming that text contains a properly balanced expression using\n    :param openDelim: as opening delimiters and\n    :param closeDelim: as closing delimiters.\n    :return: an iterator producing pairs (start, end) of start and end\n    positions in text containing a balanced expression.\n    \"\"\"", "\n", "openPat", "=", "'|'", ".", "join", "(", "[", "re", ".", "escape", "(", "x", ")", "for", "x", "in", "openDelim", "]", ")", "\n", "# pattern for delimiters expected after each opening delimiter", "\n", "afterPat", "=", "{", "o", ":", "re", ".", "compile", "(", "openPat", "+", "'|'", "+", "c", ",", "re", ".", "DOTALL", ")", "for", "o", ",", "c", "in", "zip", "(", "openDelim", ",", "closeDelim", ")", "}", "\n", "stack", "=", "[", "]", "\n", "start", "=", "0", "\n", "cur", "=", "0", "\n", "# end = len(text)", "\n", "startSet", "=", "False", "\n", "startPat", "=", "re", ".", "compile", "(", "openPat", ")", "\n", "nextPat", "=", "startPat", "\n", "while", "True", ":", "\n", "        ", "next", "=", "nextPat", ".", "search", "(", "text", ",", "cur", ")", "\n", "if", "not", "next", ":", "\n", "            ", "return", "\n", "", "if", "not", "startSet", ":", "\n", "            ", "start", "=", "next", ".", "start", "(", ")", "\n", "startSet", "=", "True", "\n", "", "delim", "=", "next", ".", "group", "(", "0", ")", "\n", "if", "delim", "in", "openDelim", ":", "\n", "            ", "stack", ".", "append", "(", "delim", ")", "\n", "nextPat", "=", "afterPat", "[", "delim", "]", "\n", "", "else", ":", "\n", "            ", "opening", "=", "stack", ".", "pop", "(", ")", "\n", "# assert opening == openDelim[closeDelim.index(next.group(0))]", "\n", "if", "stack", ":", "\n", "                ", "nextPat", "=", "afterPat", "[", "stack", "[", "-", "1", "]", "]", "\n", "", "else", ":", "\n", "                ", "yield", "start", ",", "next", ".", "end", "(", ")", "\n", "nextPat", "=", "startPat", "\n", "start", "=", "next", ".", "end", "(", ")", "\n", "startSet", "=", "False", "\n", "", "", "cur", "=", "next", ".", "end", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.if_empty": [[1341, 1382], ["None"], "function", ["None"], ["", "", "def", "if_empty", "(", "*", "rest", ")", ":", "\n", "    ", "\"\"\"\n    This implements If_empty from English Wikipedia module:\n\n       <title>Module:If empty</title>\n       <ns>828</ns>\n       <text>local p = {}\n\n    function p.main(frame)\n            local args = require('Module:Arguments').getArgs(frame, {wrappers = 'Template:If empty', removeBlanks = false})\n\n            -- For backwards compatibility reasons, the first 8 parameters can be unset instead of being blank,\n            -- even though there's really no legitimate use case for this. At some point, this will be removed.\n            local lowestNil = math.huge\n            for i = 8,1,-1 do\n                    if args[i] == nil then\n                            args[i] = ''\n                            lowestNil = i\n                    end\n            end\n\n            for k,v in ipairs(args) do\n                    if v ~= '' then\n                            if lowestNil &lt; k then\n                                    -- If any uses of this template depend on the behavior above, add them to a tracking category.\n                                    -- This is a rather fragile, convoluted, hacky way to do it, but it ensures that this module's output won't be modified\n                                    -- by it.\n                                    frame:extensionTag('ref', '[[Category:Instances of Template:If_empty missing arguments]]', {group = 'TrackingCategory'})\n                                    frame:extensionTag('references', '', {group = 'TrackingCategory'})\n                            end\n                            return v\n                    end\n            end\n    end\n\n    return p   </text>\n    \"\"\"", "\n", "for", "arg", "in", "rest", ":", "\n", "        ", "if", "arg", ":", "\n", "            ", "return", "arg", "\n", "", "", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.functionParams": [[1388, 1406], ["args.get", "args.get", "str"], "function", ["None"], ["", "def", "functionParams", "(", "args", ",", "vars", ")", ":", "\n", "    ", "\"\"\"\n    Build a dictionary of var/value from :param: args.\n    Parameters can be either named or unnamed. In the latter case, their\n    name is taken fron :param: vars.\n    \"\"\"", "\n", "params", "=", "{", "}", "\n", "index", "=", "1", "\n", "for", "var", "in", "vars", ":", "\n", "        ", "value", "=", "args", ".", "get", "(", "var", ")", "\n", "if", "value", "is", "None", ":", "\n", "            ", "value", "=", "args", ".", "get", "(", "str", "(", "index", ")", ")", "# positional argument", "\n", "if", "value", "is", "None", ":", "\n", "                ", "value", "=", "''", "\n", "", "else", ":", "\n", "                ", "index", "+=", "1", "\n", "", "", "params", "[", "var", "]", "=", "value", "\n", "", "return", "params", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.string_sub": [[1408, 1417], ["WikiExtractor.functionParams", "functionParams.get", "int", "int", "len", "functionParams.get", "functionParams.get"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.functionParams"], ["", "def", "string_sub", "(", "args", ")", ":", "\n", "    ", "params", "=", "functionParams", "(", "args", ",", "(", "'s'", ",", "'i'", ",", "'j'", ")", ")", "\n", "s", "=", "params", ".", "get", "(", "'s'", ",", "''", ")", "\n", "i", "=", "int", "(", "params", ".", "get", "(", "'i'", ",", "1", ")", "or", "1", ")", "# or handles case of '' value", "\n", "j", "=", "int", "(", "params", ".", "get", "(", "'j'", ",", "-", "1", ")", "or", "-", "1", ")", "\n", "if", "i", ">", "0", ":", "i", "-=", "1", "# lua is 1-based", "\n", "if", "j", "<", "0", ":", "j", "+=", "1", "\n", "if", "j", "==", "0", ":", "j", "=", "len", "(", "s", ")", "\n", "return", "s", "[", "i", ":", "j", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.string_sublength": [[1419, 1425], ["WikiExtractor.functionParams", "functionParams.get", "int", "int", "functionParams.get", "functionParams.get"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.functionParams"], ["", "def", "string_sublength", "(", "args", ")", ":", "\n", "    ", "params", "=", "functionParams", "(", "args", ",", "(", "'s'", ",", "'i'", ",", "'len'", ")", ")", "\n", "s", "=", "params", ".", "get", "(", "'s'", ",", "''", ")", "\n", "i", "=", "int", "(", "params", ".", "get", "(", "'i'", ",", "1", ")", "or", "1", ")", "-", "1", "# lua is 1-based", "\n", "len", "=", "int", "(", "params", ".", "get", "(", "'len'", ",", "1", ")", "or", "1", ")", "\n", "return", "s", "[", "i", ":", "i", "+", "len", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.string_len": [[1427, 1431], ["WikiExtractor.functionParams", "functionParams.get", "len"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.functionParams"], ["", "def", "string_len", "(", "args", ")", ":", "\n", "    ", "params", "=", "functionParams", "(", "args", ",", "(", "'s'", ")", ")", "\n", "s", "=", "params", ".", "get", "(", "'s'", ",", "''", ")", "\n", "return", "len", "(", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.string_find": [[1433, 1445], ["WikiExtractor.functionParams", "functionParams.get", "functionParams.get", "int", "int", "functionParams.get", "params.get.find", "functionParams.get", "re.compile().search", "re.compile"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.functionParams"], ["", "def", "string_find", "(", "args", ")", ":", "\n", "    ", "params", "=", "functionParams", "(", "args", ",", "(", "'source'", ",", "'target'", ",", "'start'", ",", "'plain'", ")", ")", "\n", "source", "=", "params", ".", "get", "(", "'source'", ",", "''", ")", "\n", "pattern", "=", "params", ".", "get", "(", "'target'", ",", "''", ")", "\n", "start", "=", "int", "(", "'0'", "+", "params", ".", "get", "(", "'start'", ",", "1", ")", ")", "-", "1", "# lua is 1-based", "\n", "plain", "=", "int", "(", "'0'", "+", "params", ".", "get", "(", "'plain'", ",", "1", ")", ")", "\n", "if", "source", "==", "''", "or", "pattern", "==", "''", ":", "\n", "        ", "return", "0", "\n", "", "if", "plain", ":", "\n", "        ", "return", "source", ".", "find", "(", "pattern", ",", "start", ")", "+", "1", "# lua is 1-based", "\n", "", "else", ":", "\n", "        ", "return", "(", "re", ".", "compile", "(", "pattern", ")", ".", "search", "(", "source", ",", "start", ")", "or", "-", "1", ")", "+", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.string_pos": [[1447, 1454], ["WikiExtractor.functionParams", "functionParams.get", "int", "functionParams.get"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.functionParams"], ["", "", "def", "string_pos", "(", "args", ")", ":", "\n", "    ", "params", "=", "functionParams", "(", "args", ",", "(", "'target'", ",", "'pos'", ")", ")", "\n", "target", "=", "params", ".", "get", "(", "'target'", ",", "''", ")", "\n", "pos", "=", "int", "(", "params", ".", "get", "(", "'pos'", ",", "1", ")", "or", "1", ")", "\n", "if", "pos", ">", "0", ":", "\n", "        ", "pos", "-=", "1", "# The first character has an index value of 1", "\n", "", "return", "target", "[", "pos", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.string_replace": [[1456, 1470], ["WikiExtractor.functionParams", "functionParams.get", "functionParams.get", "functionParams.get", "int", "int", "re.compile().sub", "functionParams.get", "functionParams.get", "params.get.replace", "params.get.replace", "re.compile"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.functionParams"], ["", "def", "string_replace", "(", "args", ")", ":", "\n", "    ", "params", "=", "functionParams", "(", "args", ",", "(", "'source'", ",", "'pattern'", ",", "'replace'", ",", "'count'", ",", "'plain'", ")", ")", "\n", "source", "=", "params", ".", "get", "(", "'source'", ",", "''", ")", "\n", "pattern", "=", "params", ".", "get", "(", "'pattern'", ",", "''", ")", "\n", "replace", "=", "params", ".", "get", "(", "'replace'", ",", "''", ")", "\n", "count", "=", "int", "(", "params", ".", "get", "(", "'count'", ",", "0", ")", "or", "0", ")", "\n", "plain", "=", "int", "(", "params", ".", "get", "(", "'plain'", ",", "1", ")", "or", "1", ")", "\n", "if", "plain", ":", "\n", "        ", "if", "count", ":", "\n", "            ", "return", "source", ".", "replace", "(", "pattern", ",", "replace", ",", "count", ")", "\n", "", "else", ":", "\n", "            ", "return", "source", ".", "replace", "(", "pattern", ",", "replace", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "re", ".", "compile", "(", "pattern", ")", ".", "sub", "(", "replace", ",", "source", ",", "count", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.string_rep": [[1472, 1477], ["WikiExtractor.functionParams", "functionParams.get", "int", "functionParams.get"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.functionParams"], ["", "", "def", "string_rep", "(", "args", ")", ":", "\n", "    ", "params", "=", "functionParams", "(", "args", ",", "(", "'s'", ")", ")", "\n", "source", "=", "params", ".", "get", "(", "'source'", ",", "''", ")", "\n", "count", "=", "int", "(", "params", ".", "get", "(", "'count'", ",", "'1'", ")", ")", "\n", "return", "source", "*", "count", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.roman_main": [[1485, 1510], ["int", "WikiExtractor.roman_main.toRoman"], "function", ["None"], ["", "def", "roman_main", "(", "args", ")", ":", "\n", "    ", "\"\"\"Convert first arg to roman numeral if <= 5000 else :return: second arg.\"\"\"", "\n", "num", "=", "int", "(", "float", "(", "args", ".", "get", "(", "'1'", ")", ")", ")", "\n", "\n", "# Return a message for numbers too big to be expressed in Roman numerals.", "\n", "if", "0", ">", "num", "or", "num", ">=", "5000", ":", "\n", "        ", "return", "args", ".", "get", "(", "'2'", ",", "'N/A'", ")", "\n", "\n", "", "def", "toRoman", "(", "n", ",", "romanNumeralMap", ")", ":", "\n", "        ", "\"\"\"convert integer to Roman numeral\"\"\"", "\n", "result", "=", "\"\"", "\n", "for", "integer", ",", "numeral", "in", "romanNumeralMap", ":", "\n", "            ", "while", "n", ">=", "integer", ":", "\n", "                ", "result", "+=", "numeral", "\n", "n", "-=", "integer", "\n", "", "", "return", "result", "\n", "\n", "# Find the Roman numerals for numbers 4999 or less.", "\n", "", "smallRomans", "=", "(", "\n", "(", "1000", ",", "\"M\"", ")", ",", "\n", "(", "900", ",", "\"CM\"", ")", ",", "(", "500", ",", "\"D\"", ")", ",", "(", "400", ",", "\"CD\"", ")", ",", "(", "100", ",", "\"C\"", ")", ",", "\n", "(", "90", ",", "\"XC\"", ")", ",", "(", "50", ",", "\"L\"", ")", ",", "(", "40", ",", "\"XL\"", ")", ",", "(", "10", ",", "\"X\"", ")", ",", "\n", "(", "9", ",", "\"IX\"", ")", ",", "(", "5", ",", "\"V\"", ")", ",", "(", "4", ",", "\"IV\"", ")", ",", "(", "1", ",", "\"I\"", ")", "\n", ")", "\n", "return", "toRoman", "(", "num", ",", "smallRomans", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ucfirst": [[1669, 1677], ["string[].upper"], "function", ["None"], ["def", "ucfirst", "(", "string", ")", ":", "\n", "    ", "\"\"\":return: a string with just its first character uppercase\n    We can't use title() since it coverts all words.\n    \"\"\"", "\n", "if", "string", ":", "\n", "        ", "return", "string", "[", "0", "]", ".", "upper", "(", ")", "+", "string", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "        ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.lcfirst": [[1679, 1688], ["len", "string.lower", "string[].lower"], "function", ["None"], ["", "", "def", "lcfirst", "(", "string", ")", ":", "\n", "    ", "\"\"\":return: a string with its first character lowercase\"\"\"", "\n", "if", "string", ":", "\n", "        ", "if", "len", "(", "string", ")", ">", "1", ":", "\n", "            ", "return", "string", "[", "0", "]", ".", "lower", "(", ")", "+", "string", "[", "1", ":", "]", "\n", "", "else", ":", "\n", "            ", "return", "string", ".", "lower", "(", ")", "\n", "", "", "else", ":", "\n", "        ", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.fullyQualifiedTemplateTitle": [[1690, 1721], ["templateTitle.startswith", "WikiExtractor.ucfirst", "re.match", "WikiExtractor.normalizeNamespace", "WikiExtractor.ucfirst", "re.match.group", "WikiExtractor.ucfirst", "re.match.group"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ucfirst", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.normalizeNamespace", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ucfirst", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ucfirst"], ["", "", "def", "fullyQualifiedTemplateTitle", "(", "templateTitle", ")", ":", "\n", "    ", "\"\"\"\n    Determine the namespace of the page being included through the template\n    mechanism\n    \"\"\"", "\n", "if", "templateTitle", ".", "startswith", "(", "':'", ")", ":", "\n", "# Leading colon by itself implies main namespace, so strip this colon", "\n", "        ", "return", "ucfirst", "(", "templateTitle", "[", "1", ":", "]", ")", "\n", "", "else", ":", "\n", "        ", "m", "=", "re", ".", "match", "(", "'([^:]*)(:.*)'", ",", "templateTitle", ")", "\n", "if", "m", ":", "\n", "# colon found but not in the first position - check if it", "\n", "# designates a known namespace", "\n", "            ", "prefix", "=", "normalizeNamespace", "(", "m", ".", "group", "(", "1", ")", ")", "\n", "if", "prefix", "in", "options", ".", "knownNamespaces", ":", "\n", "                ", "return", "prefix", "+", "ucfirst", "(", "m", ".", "group", "(", "2", ")", ")", "\n", "# The title of the page being included is NOT in the main namespace and", "\n", "# lacks any other explicit designation of the namespace - therefore, it", "\n", "# is resolved to the Template namespace (that's the default for the", "\n", "# template inclusion mechanism).", "\n", "\n", "# This is a defense against pages whose title only contains UTF-8 chars", "\n", "# that are reduced to an empty string. Right now I can think of one such", "\n", "# case - <C2><A0> which represents the non-breaking space.", "\n", "# In this particular case, this page is a redirect to [[Non-nreaking", "\n", "# space]], but having in the system a redirect page with an empty title", "\n", "# causes numerous problems, so we'll live happier without it.", "\n", "", "", "", "if", "templateTitle", ":", "\n", "        ", "return", "options", ".", "templatePrefix", "+", "ucfirst", "(", "templateTitle", ")", "\n", "", "else", ":", "\n", "        ", "return", "''", "# caller may log as error", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.normalizeNamespace": [[1723, 1725], ["WikiExtractor.ucfirst"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ucfirst"], ["", "", "def", "normalizeNamespace", "(", "ns", ")", ":", "\n", "    ", "return", "ucfirst", "(", "ns", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.sharp_expr": [[1764, 1775], ["extr.expand", "re.sub", "re.sub", "re.sub", "re.sub", "text_type", "eval"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand"], ["def", "sharp_expr", "(", "extr", ",", "expr", ")", ":", "\n", "    ", "\"\"\"Tries converting a lua expr into a Python expr.\"\"\"", "\n", "try", ":", "\n", "        ", "expr", "=", "extr", ".", "expand", "(", "expr", ")", "\n", "expr", "=", "re", ".", "sub", "(", "'(?<![!<>])='", ",", "'=='", ",", "expr", ")", "# negative lookbehind", "\n", "expr", "=", "re", ".", "sub", "(", "'mod'", ",", "'%'", ",", "expr", ")", "# no \\b here", "\n", "expr", "=", "re", ".", "sub", "(", "'\\bdiv\\b'", ",", "'/'", ",", "expr", ")", "\n", "expr", "=", "re", ".", "sub", "(", "'\\bround\\b'", ",", "'|ROUND|'", ",", "expr", ")", "\n", "return", "text_type", "(", "eval", "(", "expr", ")", ")", "\n", "", "except", ":", "\n", "        ", "return", "'<span class=\"error\">%s</span>'", "%", "expr", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.sharp_if": [[1777, 1789], ["testValue.strip", "extr.expand", "extr.expand.strip", "extr.expand", "valueIfFalse.strip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand"], ["", "", "def", "sharp_if", "(", "extr", ",", "testValue", ",", "valueIfTrue", ",", "valueIfFalse", "=", "None", ",", "*", "args", ")", ":", "\n", "# In theory, we should evaluate the first argument here,", "\n", "# but it was evaluated while evaluating part[0] in expandTemplate().", "\n", "    ", "if", "testValue", ".", "strip", "(", ")", ":", "\n", "# The {{#if:}} function is an if-then-else construct.", "\n", "# The applied condition is: \"The condition string is non-empty\".", "\n", "        ", "valueIfTrue", "=", "extr", ".", "expand", "(", "valueIfTrue", ".", "strip", "(", ")", ")", "# eval", "\n", "if", "valueIfTrue", ":", "\n", "            ", "return", "valueIfTrue", "\n", "", "", "elif", "valueIfFalse", ":", "\n", "        ", "return", "extr", ".", "expand", "(", "valueIfFalse", ".", "strip", "(", ")", ")", "# eval", "\n", "", "return", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.sharp_ifeq": [[1791, 1807], ["rvalue.strip.strip", "lvalue.strip", "extr.expand", "extr.expand", "valueIfTrue.strip", "valueIfFalse.strip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand"], ["", "def", "sharp_ifeq", "(", "extr", ",", "lvalue", ",", "rvalue", ",", "valueIfTrue", ",", "valueIfFalse", "=", "None", ",", "*", "args", ")", ":", "\n", "    ", "rvalue", "=", "rvalue", ".", "strip", "(", ")", "\n", "if", "rvalue", ":", "\n", "# lvalue is always evaluated", "\n", "        ", "if", "lvalue", ".", "strip", "(", ")", "==", "rvalue", ":", "\n", "# The {{#ifeq:}} function is an if-then-else construct. The", "\n", "# applied condition is \"is rvalue equal to lvalue\". Note that this", "\n", "# does only string comparison while MediaWiki implementation also", "\n", "# supports numerical comparissons.", "\n", "\n", "            ", "if", "valueIfTrue", ":", "\n", "                ", "return", "extr", ".", "expand", "(", "valueIfTrue", ".", "strip", "(", ")", ")", "\n", "", "", "else", ":", "\n", "            ", "if", "valueIfFalse", ":", "\n", "                ", "return", "extr", ".", "expand", "(", "valueIfFalse", ".", "strip", "(", ")", ")", "\n", "", "", "", "return", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.sharp_iferror": [[1809, 1816], ["re.match", "extr.expand", "then.strip", "test.strip", "extr.expand", "Else.strip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand"], ["", "def", "sharp_iferror", "(", "extr", ",", "test", ",", "then", "=", "''", ",", "Else", "=", "None", ",", "*", "args", ")", ":", "\n", "    ", "if", "re", ".", "match", "(", "'<(?:strong|span|p|div)\\s(?:[^\\s>]*\\s+)*?class=\"(?:[^\"\\s>]*\\s+)*?error(?:\\s[^\">]*)?\"'", ",", "test", ")", ":", "\n", "        ", "return", "extr", ".", "expand", "(", "then", ".", "strip", "(", ")", ")", "\n", "", "elif", "Else", "is", "None", ":", "\n", "        ", "return", "test", ".", "strip", "(", ")", "\n", "", "else", ":", "\n", "        ", "return", "extr", ".", "expand", "(", "Else", ".", "strip", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.sharp_switch": [[1818, 1860], ["primary.strip.strip", "param.split", "extr.expand", "pair[].strip", "len", "extr.expand", "pair[].strip", "v.strip", "extr.expand.split"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.expand"], ["", "", "def", "sharp_switch", "(", "extr", ",", "primary", ",", "*", "params", ")", ":", "\n", "# FIXME: we don't support numeric expressions in primary", "\n", "\n", "# {{#switch: comparison string", "\n", "#  | case1 = result1", "\n", "#  | case2", "\n", "#  | case4 = result2", "\n", "#  | 1 | case5 = result3", "\n", "#  | #default = result4", "\n", "# }}", "\n", "\n", "    ", "primary", "=", "primary", ".", "strip", "(", ")", "\n", "found", "=", "False", "# for fall through cases", "\n", "default", "=", "None", "\n", "rvalue", "=", "None", "\n", "lvalue", "=", "''", "\n", "for", "param", "in", "params", ":", "\n", "# handle cases like:", "\n", "#  #default = [http://www.perseus.tufts.edu/hopper/text?doc=Perseus...]", "\n", "        ", "pair", "=", "param", ".", "split", "(", "'='", ",", "1", ")", "\n", "lvalue", "=", "extr", ".", "expand", "(", "pair", "[", "0", "]", ".", "strip", "(", ")", ")", "\n", "rvalue", "=", "None", "\n", "if", "len", "(", "pair", ")", ">", "1", ":", "\n", "# got \"=\"", "\n", "            ", "rvalue", "=", "extr", ".", "expand", "(", "pair", "[", "1", "]", ".", "strip", "(", ")", ")", "\n", "# check for any of multiple values pipe separated", "\n", "if", "found", "or", "primary", "in", "[", "v", ".", "strip", "(", ")", "for", "v", "in", "lvalue", ".", "split", "(", "'|'", ")", "]", ":", "\n", "# Found a match, return now", "\n", "                ", "return", "rvalue", "\n", "", "elif", "lvalue", "==", "'#default'", ":", "\n", "                ", "default", "=", "rvalue", "\n", "", "rvalue", "=", "None", "# avoid defaulting to last case", "\n", "", "elif", "lvalue", "==", "primary", ":", "\n", "# If the value matches, set a flag and continue", "\n", "            ", "found", "=", "True", "\n", "# Default case", "\n", "# Check if the last item had no = sign, thus specifying the default case", "\n", "", "", "if", "rvalue", "is", "not", "None", ":", "\n", "        ", "return", "lvalue", "\n", "", "elif", "default", "is", "not", "None", ":", "\n", "        ", "return", "default", "\n", "", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.sharp_invoke": [[1863, 1870], ["modules.get", "modules.get.get", "text_type", "functions.get."], "function", ["None"], ["", "def", "sharp_invoke", "(", "module", ",", "function", ",", "args", ")", ":", "\n", "    ", "functions", "=", "modules", ".", "get", "(", "module", ")", "\n", "if", "functions", ":", "\n", "        ", "funct", "=", "functions", ".", "get", "(", "function", ")", "\n", "if", "funct", ":", "\n", "            ", "return", "text_type", "(", "funct", "(", "args", ")", ")", "\n", "", "", "return", "''", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.callParserFunction": [[1915, 1957], ["functionName.lower.lower", "logging.debug", "WikiExtractor.sharp_invoke", "logging.debug", "args[].strip", "args[].strip", "len", "WikiExtractor.fullyQualifiedTemplateTitle", "extractor.templateParams", "logging.warn", "extractor.transform"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.sharp_invoke", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.fullyQualifiedTemplateTitle", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.templateParams", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform"], ["def", "callParserFunction", "(", "functionName", ",", "args", ",", "extractor", ")", ":", "\n", "    ", "\"\"\"\n    Parser functions have similar syntax as templates, except that\n    the first argument is everything after the first colon.\n    :return: the result of the invocation, None in case of failure.\n\n    :param: args not yet expanded (see branching functions).\n    https://www.mediawiki.org/wiki/Help:Extension:ParserFunctions\n    \"\"\"", "\n", "\n", "try", ":", "\n", "# https://it.wikipedia.org/wiki/Template:Str_endswith has #Invoke", "\n", "        ", "functionName", "=", "functionName", ".", "lower", "(", ")", "\n", "if", "functionName", "==", "'#invoke'", ":", "\n", "            ", "module", ",", "fun", "=", "args", "[", "0", "]", ".", "strip", "(", ")", ",", "args", "[", "1", "]", ".", "strip", "(", ")", "\n", "logging", ".", "debug", "(", "'%*s#invoke %s %s %s'", ",", "extractor", ".", "frame", ".", "depth", ",", "''", ",", "module", ",", "fun", ",", "args", "[", "2", ":", "]", ")", "\n", "# special handling of frame", "\n", "if", "len", "(", "args", ")", "==", "2", ":", "\n", "# find parameters in frame whose title is the one of the original", "\n", "# template invocation", "\n", "                ", "templateTitle", "=", "fullyQualifiedTemplateTitle", "(", "module", ")", "\n", "if", "not", "templateTitle", ":", "\n", "                    ", "logging", ".", "warn", "(", "\"Template with empty title\"", ")", "\n", "", "params", "=", "None", "\n", "frame", "=", "extractor", ".", "frame", "\n", "while", "frame", ":", "\n", "                    ", "if", "frame", ".", "title", "==", "templateTitle", ":", "\n", "                        ", "params", "=", "frame", ".", "args", "\n", "break", "\n", "", "frame", "=", "frame", ".", "prev", "\n", "", "", "else", ":", "\n", "                ", "params", "=", "[", "extractor", ".", "transform", "(", "p", ")", "for", "p", "in", "args", "[", "2", ":", "]", "]", "# evaluates them", "\n", "params", "=", "extractor", ".", "templateParams", "(", "params", ")", "\n", "", "ret", "=", "sharp_invoke", "(", "module", ",", "fun", ",", "params", ")", "\n", "logging", ".", "debug", "(", "'%*s<#invoke %s %s %s'", ",", "extractor", ".", "frame", ".", "depth", ",", "''", ",", "module", ",", "fun", ",", "ret", ")", "\n", "return", "ret", "\n", "", "if", "functionName", "in", "parserFunctions", ":", "\n", "# branching functions use the extractor to selectively evaluate args", "\n", "            ", "return", "parserFunctions", "[", "functionName", "]", "(", "extractor", ",", "*", "args", ")", "\n", "", "", "except", ":", "\n", "        ", "return", "\"\"", "# FIXME: fix errors", "\n", "", "return", "\"\"", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.define_template": [[1977, 2025], ["re.match", "WikiExtractor.unescape", "comment.sub", "reNoinclude.sub", "re.sub", "re.sub", "re.finditer", "re.match.group", "re.match.group", "reIncludeonly.sub", "logging.warn"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.unescape"], ["def", "define_template", "(", "title", ",", "page", ")", ":", "\n", "    ", "\"\"\"\n    Adds a template defined in the :param page:.\n    @see https://en.wikipedia.org/wiki/Help:Template#Noinclude.2C_includeonly.2C_and_onlyinclude\n    \"\"\"", "\n", "# title = normalizeTitle(title)", "\n", "\n", "# sanity check (empty template, e.g. Template:Crude Oil Prices))", "\n", "if", "not", "page", ":", "return", "\n", "\n", "# check for redirects", "\n", "m", "=", "re", ".", "match", "(", "'#REDIRECT.*?\\[\\[([^\\]]*)]]'", ",", "page", "[", "0", "]", ",", "re", ".", "IGNORECASE", ")", "\n", "if", "m", ":", "\n", "        ", "options", ".", "redirects", "[", "title", "]", "=", "m", ".", "group", "(", "1", ")", "# normalizeTitle(m.group(1))", "\n", "return", "\n", "\n", "", "text", "=", "unescape", "(", "''", ".", "join", "(", "page", ")", ")", "\n", "\n", "# We're storing template text for future inclusion, therefore,", "\n", "# remove all <noinclude> text and keep all <includeonly> text", "\n", "# (but eliminate <includeonly> tags per se).", "\n", "# However, if <onlyinclude> ... </onlyinclude> parts are present,", "\n", "# then only keep them and discard the rest of the template body.", "\n", "# This is because using <onlyinclude> on a text fragment is", "\n", "# equivalent to enclosing it in <includeonly> tags **AND**", "\n", "# enclosing all the rest of the template body in <noinclude> tags.", "\n", "\n", "# remove comments", "\n", "text", "=", "comment", ".", "sub", "(", "''", ",", "text", ")", "\n", "\n", "# eliminate <noinclude> fragments", "\n", "text", "=", "reNoinclude", ".", "sub", "(", "''", ",", "text", ")", "\n", "# eliminate unterminated <noinclude> elements", "\n", "text", "=", "re", ".", "sub", "(", "r'<noinclude\\s*>.*$'", ",", "''", ",", "text", ",", "flags", "=", "re", ".", "DOTALL", ")", "\n", "text", "=", "re", ".", "sub", "(", "r'<noinclude/>'", ",", "''", ",", "text", ")", "\n", "\n", "onlyincludeAccumulator", "=", "''", "\n", "for", "m", "in", "re", ".", "finditer", "(", "'<onlyinclude>(.*?)</onlyinclude>'", ",", "text", ",", "re", ".", "DOTALL", ")", ":", "\n", "        ", "onlyincludeAccumulator", "+=", "m", ".", "group", "(", "1", ")", "\n", "", "if", "onlyincludeAccumulator", ":", "\n", "        ", "text", "=", "onlyincludeAccumulator", "\n", "", "else", ":", "\n", "        ", "text", "=", "reIncludeonly", ".", "sub", "(", "''", ",", "text", ")", "\n", "\n", "", "if", "text", ":", "\n", "        ", "if", "title", "in", "options", ".", "templates", ":", "\n", "            ", "logging", ".", "warn", "(", "'Redefining: %s'", ",", "title", ")", "\n", "", "options", ".", "templates", "[", "title", "]", "=", "text", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.dropNested": [[2029, 2080], ["re.compile", "re.compile", "re.compile.search", "re.compile.search", "WikiExtractor.dropSpans", "openRE.search.end", "re.compile.search", "openRE.search.end", "spans.append", "closeRE.search.end", "openRE.search.start", "re.compile.search", "closeRE.search.end", "re.compile.search", "spans.append", "re.compile.search", "closeRE.search.end", "openRE.search.start", "closeRE.search.end", "closeRE.search.end", "openRE.search.end", "openRE.search.start", "closeRE.search.end", "openRE.search.start"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.dropSpans"], ["", "", "def", "dropNested", "(", "text", ",", "openDelim", ",", "closeDelim", ")", ":", "\n", "    ", "\"\"\"\n    A matching function for nested expressions, e.g. namespaces and tables.\n    \"\"\"", "\n", "openRE", "=", "re", ".", "compile", "(", "openDelim", ",", "re", ".", "IGNORECASE", ")", "\n", "closeRE", "=", "re", ".", "compile", "(", "closeDelim", ",", "re", ".", "IGNORECASE", ")", "\n", "# partition text in separate blocks { } { }", "\n", "spans", "=", "[", "]", "# pairs (s, e) for each partition", "\n", "nest", "=", "0", "# nesting level", "\n", "start", "=", "openRE", ".", "search", "(", "text", ",", "0", ")", "\n", "if", "not", "start", ":", "\n", "        ", "return", "text", "\n", "", "end", "=", "closeRE", ".", "search", "(", "text", ",", "start", ".", "end", "(", ")", ")", "\n", "next", "=", "start", "\n", "while", "end", ":", "\n", "        ", "next", "=", "openRE", ".", "search", "(", "text", ",", "next", ".", "end", "(", ")", ")", "\n", "if", "not", "next", ":", "# termination", "\n", "            ", "while", "nest", ":", "# close all pending", "\n", "                ", "nest", "-=", "1", "\n", "end0", "=", "closeRE", ".", "search", "(", "text", ",", "end", ".", "end", "(", ")", ")", "\n", "if", "end0", ":", "\n", "                    ", "end", "=", "end0", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "", "", "spans", ".", "append", "(", "(", "start", ".", "start", "(", ")", ",", "end", ".", "end", "(", ")", ")", ")", "\n", "break", "\n", "", "while", "end", ".", "end", "(", ")", "<", "next", ".", "start", "(", ")", ":", "\n", "# { } {", "\n", "            ", "if", "nest", ":", "\n", "                ", "nest", "-=", "1", "\n", "# try closing more", "\n", "last", "=", "end", ".", "end", "(", ")", "\n", "end", "=", "closeRE", ".", "search", "(", "text", ",", "end", ".", "end", "(", ")", ")", "\n", "if", "not", "end", ":", "# unbalanced", "\n", "                    ", "if", "spans", ":", "\n", "                        ", "span", "=", "(", "spans", "[", "0", "]", "[", "0", "]", ",", "last", ")", "\n", "", "else", ":", "\n", "                        ", "span", "=", "(", "start", ".", "start", "(", ")", ",", "last", ")", "\n", "", "spans", "=", "[", "span", "]", "\n", "break", "\n", "", "", "else", ":", "\n", "                ", "spans", ".", "append", "(", "(", "start", ".", "start", "(", ")", ",", "end", ".", "end", "(", ")", ")", ")", "\n", "# advance start, find next close", "\n", "start", "=", "next", "\n", "end", "=", "closeRE", ".", "search", "(", "text", ",", "next", ".", "end", "(", ")", ")", "\n", "break", "# { }", "\n", "", "", "if", "next", "!=", "start", ":", "\n", "# { { }", "\n", "            ", "nest", "+=", "1", "\n", "# collect text outside partitions", "\n", "", "", "return", "dropSpans", "(", "spans", ",", "text", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.dropSpans": [[2082, 2096], ["spans.sort"], "function", ["None"], ["", "def", "dropSpans", "(", "spans", ",", "text", ")", ":", "\n", "    ", "\"\"\"\n    Drop from text the blocks identified in :param spans:, possibly nested.\n    \"\"\"", "\n", "spans", ".", "sort", "(", ")", "\n", "res", "=", "''", "\n", "offset", "=", "0", "\n", "for", "s", ",", "e", "in", "spans", ":", "\n", "        ", "if", "offset", "<=", "s", ":", "# handle nesting", "\n", "            ", "if", "offset", "<", "s", ":", "\n", "                ", "res", "+=", "text", "[", "offset", ":", "s", "]", "\n", "", "offset", "=", "e", "\n", "", "", "res", "+=", "text", "[", "offset", ":", "]", "\n", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.replaceInternalLinks": [[2105, 2145], ["WikiExtractor.findBalanced", "tailRE.match", "inner.find", "tailRE.match.group", "tailRE.match.end", "inner[].rstrip", "WikiExtractor.findBalanced", "inner[].strip", "inner.rfind", "WikiExtractor.makeInternalLink"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.findBalanced", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.findBalanced", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.makeInternalLink"], ["", "def", "replaceInternalLinks", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    Replaces internal links of the form:\n    [[title |...|label]]trail\n\n    with title concatenated with trail, when present, e.g. 's' for plural.\n\n    See https://www.mediawiki.org/wiki/Help:Links#Internal_links\n    \"\"\"", "\n", "# call this after removal of external links, so we need not worry about", "\n", "# triple closing ]]].", "\n", "cur", "=", "0", "\n", "res", "=", "''", "\n", "for", "s", ",", "e", "in", "findBalanced", "(", "text", ")", ":", "\n", "        ", "m", "=", "tailRE", ".", "match", "(", "text", ",", "e", ")", "\n", "if", "m", ":", "\n", "            ", "trail", "=", "m", ".", "group", "(", "0", ")", "\n", "end", "=", "m", ".", "end", "(", ")", "\n", "", "else", ":", "\n", "            ", "trail", "=", "''", "\n", "end", "=", "e", "\n", "", "inner", "=", "text", "[", "s", "+", "2", ":", "e", "-", "2", "]", "\n", "# find first |", "\n", "pipe", "=", "inner", ".", "find", "(", "'|'", ")", "\n", "if", "pipe", "<", "0", ":", "\n", "            ", "title", "=", "inner", "\n", "label", "=", "title", "\n", "", "else", ":", "\n", "            ", "title", "=", "inner", "[", ":", "pipe", "]", ".", "rstrip", "(", ")", "\n", "# find last |", "\n", "curp", "=", "pipe", "+", "1", "\n", "for", "s1", ",", "e1", "in", "findBalanced", "(", "inner", ")", ":", "\n", "                ", "last", "=", "inner", ".", "rfind", "(", "'|'", ",", "curp", ",", "s1", ")", "\n", "if", "last", ">=", "0", ":", "\n", "                    ", "pipe", "=", "last", "# advance", "\n", "", "curp", "=", "e1", "\n", "", "label", "=", "inner", "[", "pipe", "+", "1", ":", "]", ".", "strip", "(", ")", "\n", "", "res", "+=", "text", "[", "cur", ":", "s", "]", "+", "makeInternalLink", "(", "title", ",", "label", ")", "+", "trail", "\n", "cur", "=", "end", "\n", "", "return", "res", "+", "text", "[", "cur", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.makeInternalLink": [[2412, 2425], ["title.find", "title.find", "quote", "title.encode"], "function", ["None"], ["", "def", "makeInternalLink", "(", "title", ",", "label", ")", ":", "\n", "    ", "colon", "=", "title", ".", "find", "(", "':'", ")", "\n", "if", "colon", ">", "0", "and", "title", "[", ":", "colon", "]", "not", "in", "options", ".", "acceptedNamespaces", ":", "\n", "        ", "return", "''", "\n", "", "if", "colon", "==", "0", ":", "\n", "# drop also :File:", "\n", "        ", "colon2", "=", "title", ".", "find", "(", "':'", ",", "colon", "+", "1", ")", "\n", "if", "colon2", ">", "1", "and", "title", "[", "colon", "+", "1", ":", "colon2", "]", "not", "in", "options", ".", "acceptedNamespaces", ":", "\n", "            ", "return", "''", "\n", "", "", "if", "options", ".", "keepLinks", ":", "\n", "        ", "return", "'<a href=\"%s\">%s</a>'", "%", "(", "quote", "(", "title", ".", "encode", "(", "'utf-8'", ")", ")", ",", "label", ")", "\n", "", "else", ":", "\n", "        ", "return", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.replaceExternalLinks": [[2460, 2495], ["ExtLinkBracketedRegex.finditer", "EXT_IMAGE_REGEX.match.end", "EXT_IMAGE_REGEX.match.group", "EXT_IMAGE_REGEX.match.group", "EXT_IMAGE_REGEX.match", "WikiExtractor.makeExternalLink", "WikiExtractor.makeExternalImage", "EXT_IMAGE_REGEX.match.start"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.makeExternalLink", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.makeExternalImage"], ["def", "replaceExternalLinks", "(", "text", ")", ":", "\n", "    ", "\"\"\"\n    https://www.mediawiki.org/wiki/Help:Links#External_links\n    [URL anchor text]\n    \"\"\"", "\n", "s", "=", "''", "\n", "cur", "=", "0", "\n", "for", "m", "in", "ExtLinkBracketedRegex", ".", "finditer", "(", "text", ")", ":", "\n", "        ", "s", "+=", "text", "[", "cur", ":", "m", ".", "start", "(", ")", "]", "\n", "cur", "=", "m", ".", "end", "(", ")", "\n", "\n", "url", "=", "m", ".", "group", "(", "1", ")", "\n", "label", "=", "m", ".", "group", "(", "3", ")", "\n", "\n", "# # The characters '<' and '>' (which were escaped by", "\n", "# # removeHTMLtags()) should not be included in", "\n", "# # URLs, per RFC 2396.", "\n", "# m2 = re.search('&(lt|gt);', url)", "\n", "# if m2:", "\n", "#     link = url[m2.end():] + ' ' + link", "\n", "#     url = url[0:m2.end()]", "\n", "\n", "# If the link text is an image URL, replace it with an <img> tag", "\n", "# This happened by accident in the original parser, but some people used it extensively", "\n", "m", "=", "EXT_IMAGE_REGEX", ".", "match", "(", "label", ")", "\n", "if", "m", ":", "\n", "            ", "label", "=", "makeExternalImage", "(", "label", ")", "\n", "\n", "# Use the encoded URL", "\n", "# This means that users can paste URLs directly into the text", "\n", "# Funny characters like \u00f6 aren't valid in URLs anyway", "\n", "# This was changed in August 2004", "\n", "", "s", "+=", "makeExternalLink", "(", "url", ",", "label", ")", "# + trail", "\n", "\n", "", "return", "s", "+", "text", "[", "cur", ":", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.makeExternalLink": [[2497, 2503], ["quote", "url.encode"], "function", ["None"], ["", "def", "makeExternalLink", "(", "url", ",", "anchor", ")", ":", "\n", "    ", "\"\"\"Function applied to wikiLinks\"\"\"", "\n", "if", "options", ".", "keepLinks", ":", "\n", "        ", "return", "'<a href=\"%s\">%s</a>'", "%", "(", "quote", "(", "url", ".", "encode", "(", "'utf-8'", ")", ")", ",", "anchor", ")", "\n", "", "else", ":", "\n", "        ", "return", "anchor", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.makeExternalImage": [[2505, 2510], ["None"], "function", ["None"], ["", "", "def", "makeExternalImage", "(", "url", ",", "alt", "=", "''", ")", ":", "\n", "    ", "if", "options", ".", "keepLinks", ":", "\n", "        ", "return", "'<img src=\"%s\" alt=\"%s\">'", "%", "(", "url", ",", "alt", ")", "\n", "", "else", ":", "\n", "        ", "return", "alt", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.compact": [[2528, 2654], ["text.split", "section.match", "len", "section.match.group", "len", "list", "line[].strip.startswith", "page.append", "section.match.group", "page.append", "headers.keys", "reversed", "page.append", "page.append", "page.append", "zip_longest", "line[].strip", "len", "page.append", "listCount.append", "headers.clear", "page.append", "reversed", "page.append", "sorted", "page.append", "page.append", "len", "page.append", "page.append", "headers.items", "page.append", "line[].strip.strip", "headers.clear", "page.append", "len", "sorted", "headers.items", "page.append", "page.append"], "function", ["None"], ["def", "compact", "(", "text", ")", ":", "\n", "    ", "\"\"\"Deal with headers, lists, empty sections, residuals of tables.\n    :param text: convert to HTML.\n    \"\"\"", "\n", "\n", "page", "=", "[", "]", "# list of paragraph", "\n", "headers", "=", "{", "}", "# Headers for unfilled sections", "\n", "emptySection", "=", "False", "# empty sections are discarded", "\n", "listLevel", "=", "[", "]", "# nesting of lists", "\n", "listCount", "=", "[", "]", "# count of each list (it should be always in the same length of listLevel)", "\n", "for", "line", "in", "text", ".", "split", "(", "'\\n'", ")", ":", "\n", "        ", "if", "not", "line", ":", "# collapse empty lines", "\n", "# if there is an opening list, close it if we see an empty line", "\n", "            ", "if", "len", "(", "listLevel", ")", ":", "\n", "                ", "page", ".", "append", "(", "line", ")", "\n", "if", "options", ".", "toHTML", ":", "\n", "                    ", "for", "c", "in", "reversed", "(", "listLevel", ")", ":", "\n", "                        ", "page", ".", "append", "(", "listClose", "[", "c", "]", ")", "\n", "", "", "listLevel", "=", "[", "]", "\n", "listCount", "=", "[", "]", "\n", "emptySection", "=", "False", "\n", "", "elif", "page", "and", "page", "[", "-", "1", "]", ":", "\n", "                ", "page", ".", "append", "(", "''", ")", "\n", "", "continue", "\n", "# Handle section titles", "\n", "", "m", "=", "section", ".", "match", "(", "line", ")", "\n", "if", "m", ":", "\n", "            ", "title", "=", "m", ".", "group", "(", "2", ")", "\n", "lev", "=", "len", "(", "m", ".", "group", "(", "1", ")", ")", "# header level", "\n", "if", "options", ".", "toHTML", ":", "\n", "                ", "page", ".", "append", "(", "\"<h%d>%s</h%d>\"", "%", "(", "lev", ",", "title", ",", "lev", ")", ")", "\n", "", "if", "title", "and", "title", "[", "-", "1", "]", "not", "in", "'!?'", ":", "\n", "                ", "title", "+=", "'.'", "# terminate sentence.", "\n", "", "headers", "[", "lev", "]", "=", "title", "\n", "# drop previous headers", "\n", "for", "i", "in", "list", "(", "headers", ".", "keys", "(", ")", ")", ":", "\n", "                ", "if", "i", ">", "lev", ":", "\n", "                    ", "del", "headers", "[", "i", "]", "\n", "", "", "emptySection", "=", "True", "\n", "listLevel", "=", "[", "]", "\n", "listCount", "=", "[", "]", "\n", "continue", "\n", "# Handle page title", "\n", "", "elif", "line", ".", "startswith", "(", "'++'", ")", ":", "\n", "            ", "title", "=", "line", "[", "2", ":", "-", "2", "]", "\n", "if", "title", ":", "\n", "                ", "if", "title", "[", "-", "1", "]", "not", "in", "'!?'", ":", "\n", "                    ", "title", "+=", "'.'", "\n", "", "page", ".", "append", "(", "title", ")", "\n", "# handle indents", "\n", "", "", "elif", "line", "[", "0", "]", "==", "':'", ":", "\n", "# page.append(line.lstrip(':*#;'))", "\n", "            ", "continue", "\n", "# handle lists", "\n", "", "elif", "line", "[", "0", "]", "in", "'*#;:'", ":", "\n", "            ", "i", "=", "0", "\n", "# c: current level char", "\n", "# n: next level char", "\n", "for", "c", ",", "n", "in", "zip_longest", "(", "listLevel", ",", "line", ",", "fillvalue", "=", "''", ")", ":", "\n", "                ", "if", "not", "n", "or", "n", "not", "in", "'*#;:'", ":", "# shorter or different", "\n", "                    ", "if", "c", ":", "\n", "                        ", "if", "options", ".", "toHTML", ":", "\n", "                            ", "page", ".", "append", "(", "listClose", "[", "c", "]", ")", "\n", "", "listLevel", "=", "listLevel", "[", ":", "-", "1", "]", "\n", "listCount", "=", "listCount", "[", ":", "-", "1", "]", "\n", "continue", "\n", "", "else", ":", "\n", "                        ", "break", "\n", "# n != ''", "\n", "", "", "if", "c", "!=", "n", "and", "(", "not", "c", "or", "(", "c", "not", "in", "';:'", "and", "n", "not", "in", "';:'", ")", ")", ":", "\n", "                    ", "if", "c", ":", "\n", "# close level", "\n", "                        ", "if", "options", ".", "toHTML", ":", "\n", "                            ", "page", ".", "append", "(", "listClose", "[", "c", "]", ")", "\n", "", "listLevel", "=", "listLevel", "[", ":", "-", "1", "]", "\n", "listCount", "=", "listCount", "[", ":", "-", "1", "]", "\n", "", "listLevel", "+=", "n", "\n", "listCount", ".", "append", "(", "0", ")", "\n", "if", "options", ".", "toHTML", ":", "\n", "                        ", "page", ".", "append", "(", "listOpen", "[", "n", "]", ")", "\n", "", "", "i", "+=", "1", "\n", "", "n", "=", "line", "[", "i", "-", "1", "]", "# last list char", "\n", "line", "=", "line", "[", "i", ":", "]", ".", "strip", "(", ")", "\n", "if", "line", ":", "# FIXME: n is '\"'", "\n", "                ", "if", "options", ".", "keepLists", ":", "\n", "                    ", "if", "options", ".", "keepSections", ":", "\n", "# emit open sections", "\n", "                        ", "items", "=", "sorted", "(", "headers", ".", "items", "(", ")", ")", "\n", "for", "_", ",", "v", "in", "items", ":", "\n", "                            ", "page", ".", "append", "(", "\"Section::::\"", "+", "v", ")", "\n", "", "", "headers", ".", "clear", "(", ")", "\n", "# use item count for #-lines", "\n", "listCount", "[", "i", "-", "1", "]", "+=", "1", "\n", "bullet", "=", "'BULLET::::%d. '", "%", "listCount", "[", "i", "-", "1", "]", "if", "n", "==", "'#'", "else", "'BULLET::::- '", "\n", "page", ".", "append", "(", "'{0:{1}s}'", ".", "format", "(", "bullet", ",", "len", "(", "listLevel", ")", ")", "+", "line", ")", "\n", "", "elif", "options", ".", "toHTML", ":", "\n", "                    ", "if", "n", "not", "in", "listItem", ":", "\n", "                        ", "n", "=", "'*'", "\n", "", "page", ".", "append", "(", "listItem", "[", "n", "]", "%", "line", ")", "\n", "", "", "", "elif", "len", "(", "listLevel", ")", ":", "\n", "            ", "if", "options", ".", "toHTML", ":", "\n", "                ", "for", "c", "in", "reversed", "(", "listLevel", ")", ":", "\n", "                    ", "page", ".", "append", "(", "listClose", "[", "c", "]", ")", "\n", "", "", "listLevel", "=", "[", "]", "\n", "listCount", "=", "[", "]", "\n", "page", ".", "append", "(", "line", ")", "\n", "\n", "# Drop residuals of lists", "\n", "", "elif", "line", "[", "0", "]", "in", "'{|'", "or", "line", "[", "-", "1", "]", "==", "'}'", ":", "\n", "            ", "continue", "\n", "# Drop irrelevant lines", "\n", "", "elif", "(", "line", "[", "0", "]", "==", "'('", "and", "line", "[", "-", "1", "]", "==", "')'", ")", "or", "line", ".", "strip", "(", "'.-'", ")", "==", "''", ":", "\n", "            ", "continue", "\n", "", "elif", "len", "(", "headers", ")", ":", "\n", "            ", "if", "options", ".", "keepSections", ":", "\n", "                ", "items", "=", "sorted", "(", "headers", ".", "items", "(", ")", ")", "\n", "for", "i", ",", "v", "in", "items", ":", "\n", "                    ", "page", ".", "append", "(", "\"Section::::\"", "+", "v", ")", "\n", "", "", "headers", ".", "clear", "(", ")", "\n", "page", ".", "append", "(", "line", ")", "# first line", "\n", "emptySection", "=", "False", "\n", "", "elif", "not", "emptySection", ":", "\n", "# Drop preformatted", "\n", "            ", "if", "line", "[", "0", "]", "!=", "' '", ":", "# dangerous", "\n", "                ", "page", ".", "append", "(", "line", ")", "\n", "", "", "", "return", "page", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.handle_unicode": [[2656, 2660], ["int", "chr"], "function", ["None"], ["", "def", "handle_unicode", "(", "entity", ")", ":", "\n", "    ", "numeric_code", "=", "int", "(", "entity", "[", "2", ":", "-", "1", "]", ")", "\n", "if", "numeric_code", ">=", "0x10000", ":", "return", "''", "\n", "return", "chr", "(", "numeric_code", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.load_templates": [[2742, 2785], ["enumerate", "codecs.open", "WikiExtractor.pages_from", "codecs.open.close", "logging.info", "WikiExtractor.define_template", "logging.info", "len", "title.find", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write", "codecs.open.write"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.pages_from", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.define_template", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["def", "load_templates", "(", "file", ",", "output_file", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Load templates from :param file:.\n    :param output_file: file where to save templates and modules.\n    \"\"\"", "\n", "options", ".", "templatePrefix", "=", "options", ".", "templateNamespace", "+", "':'", "\n", "options", ".", "modulePrefix", "=", "options", ".", "moduleNamespace", "+", "':'", "\n", "\n", "if", "output_file", ":", "\n", "        ", "output", "=", "codecs", ".", "open", "(", "output_file", ",", "'wb'", ",", "'utf-8'", ")", "\n", "", "for", "page_count", ",", "page_data", "in", "enumerate", "(", "pages_from", "(", "file", ")", ")", ":", "\n", "        ", "id", ",", "revid", ",", "title", ",", "ns", ",", "catSet", ",", "page", "=", "page_data", "\n", "if", "not", "output_file", "and", "(", "not", "options", ".", "templateNamespace", "or", "\n", "not", "options", ".", "moduleNamespace", ")", ":", "# do not know it yet", "\n", "# reconstruct templateNamespace and moduleNamespace from the first title", "\n", "            ", "if", "ns", "in", "templateKeys", ":", "\n", "                ", "colon", "=", "title", ".", "find", "(", "':'", ")", "\n", "if", "colon", ">", "1", ":", "\n", "                    ", "if", "ns", "==", "'10'", ":", "\n", "                        ", "options", ".", "templateNamespace", "=", "title", "[", ":", "colon", "]", "\n", "options", ".", "templatePrefix", "=", "title", "[", ":", "colon", "+", "1", "]", "\n", "", "elif", "ns", "==", "'828'", ":", "\n", "                        ", "options", ".", "moduleNamespace", "=", "title", "[", ":", "colon", "]", "\n", "options", ".", "modulePrefix", "=", "title", "[", ":", "colon", "+", "1", "]", "\n", "", "", "", "", "if", "ns", "in", "templateKeys", ":", "\n", "            ", "text", "=", "''", ".", "join", "(", "page", ")", "\n", "define_template", "(", "title", ",", "text", ")", "\n", "# save templates and modules to file", "\n", "if", "output_file", ":", "\n", "                ", "output", ".", "write", "(", "'<page>\\n'", ")", "\n", "output", ".", "write", "(", "'   <title>%s</title>\\n'", "%", "title", ")", "\n", "output", ".", "write", "(", "'   <ns>%s</ns>\\n'", "%", "ns", ")", "\n", "output", ".", "write", "(", "'   <id>%s</id>\\n'", "%", "id", ")", "\n", "output", ".", "write", "(", "'   <text>'", ")", "\n", "for", "line", "in", "page", ":", "\n", "                    ", "output", ".", "write", "(", "line", ")", "\n", "", "output", ".", "write", "(", "'   </text>\\n'", ")", "\n", "output", ".", "write", "(", "'</page>\\n'", ")", "\n", "", "", "if", "page_count", "and", "page_count", "%", "100000", "==", "0", ":", "\n", "            ", "logging", ".", "info", "(", "\"Preprocessed %d pages\"", ",", "page_count", ")", "\n", "", "", "if", "output_file", ":", "\n", "        ", "output", ".", "close", "(", ")", "\n", "logging", ".", "info", "(", "\"Saved %d templates to '%s'\"", ",", "len", "(", "options", ".", "templates", ")", ",", "output_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.pages_from": [[2787, 2855], ["tagRE.search", "tagRE.search.group", "isinstance", "line.decode.decode", "set", "page.append", "line.decode.lstrip().startswith", "tagRE.search.group", "catRE.search", "tagRE.search.group", "line.decode.lstrip", "set.add", "tagRE.search.group", "catRE.search.group", "tagRE.search.group", "page.append", "tagRE.search.group", "tagRE.search.start", "tagRE.search.end", "page.append", "page.append", "tagRE.search.group", "tagRE.search.start"], "function", ["None"], ["", "", "def", "pages_from", "(", "input", ")", ":", "\n", "    ", "\"\"\"\n    Scans input extracting pages.\n    :return: (id, revid, title, namespace key, page), page is a list of lines.\n    \"\"\"", "\n", "# we collect individual lines, since str.join() is significantly faster", "\n", "# than concatenation", "\n", "page", "=", "[", "]", "\n", "id", "=", "None", "\n", "ns", "=", "'0'", "\n", "last_id", "=", "None", "\n", "revid", "=", "None", "\n", "inText", "=", "False", "\n", "redirect", "=", "False", "\n", "title", "=", "None", "\n", "for", "line", "in", "input", ":", "\n", "        ", "if", "not", "isinstance", "(", "line", ",", "text_type", ")", ":", "line", "=", "line", ".", "decode", "(", "'utf-8'", ")", "\n", "if", "'<'", "not", "in", "line", ":", "# faster than doing re.search()", "\n", "            ", "if", "inText", ":", "\n", "                ", "page", ".", "append", "(", "line", ")", "\n", "# extract categories", "\n", "if", "line", ".", "lstrip", "(", ")", ".", "startswith", "(", "'[[Category:'", ")", ":", "\n", "                    ", "mCat", "=", "catRE", ".", "search", "(", "line", ")", "\n", "if", "mCat", ":", "\n", "                        ", "catSet", ".", "add", "(", "mCat", ".", "group", "(", "1", ")", ")", "\n", "", "", "", "continue", "\n", "", "m", "=", "tagRE", ".", "search", "(", "line", ")", "\n", "if", "not", "m", ":", "\n", "            ", "continue", "\n", "", "tag", "=", "m", ".", "group", "(", "2", ")", "\n", "if", "tag", "==", "'page'", ":", "\n", "            ", "page", "=", "[", "]", "\n", "catSet", "=", "set", "(", ")", "\n", "redirect", "=", "False", "\n", "", "elif", "tag", "==", "'id'", "and", "not", "id", ":", "\n", "            ", "id", "=", "m", ".", "group", "(", "3", ")", "\n", "", "elif", "tag", "==", "'id'", "and", "id", ":", "\n", "            ", "revid", "=", "m", ".", "group", "(", "3", ")", "\n", "", "elif", "tag", "==", "'title'", ":", "\n", "            ", "title", "=", "m", ".", "group", "(", "3", ")", "\n", "", "elif", "tag", "==", "'ns'", ":", "\n", "            ", "ns", "=", "m", ".", "group", "(", "3", ")", "\n", "", "elif", "tag", "==", "'redirect'", ":", "\n", "            ", "redirect", "=", "True", "\n", "", "elif", "tag", "==", "'text'", ":", "\n", "            ", "if", "m", ".", "lastindex", "==", "3", "and", "line", "[", "m", ".", "start", "(", "3", ")", "-", "2", "]", "==", "'/'", ":", "# self closing", "\n", "# <text xml:space=\"preserve\" />", "\n", "                ", "continue", "\n", "", "inText", "=", "True", "\n", "line", "=", "line", "[", "m", ".", "start", "(", "3", ")", ":", "m", ".", "end", "(", "3", ")", "]", "\n", "page", ".", "append", "(", "line", ")", "\n", "if", "m", ".", "lastindex", "==", "4", ":", "# open-close", "\n", "                ", "inText", "=", "False", "\n", "", "", "elif", "tag", "==", "'/text'", ":", "\n", "            ", "if", "m", ".", "group", "(", "1", ")", ":", "\n", "                ", "page", ".", "append", "(", "m", ".", "group", "(", "1", ")", ")", "\n", "", "inText", "=", "False", "\n", "", "elif", "inText", ":", "\n", "            ", "page", ".", "append", "(", "line", ")", "\n", "", "elif", "tag", "==", "'/page'", ":", "\n", "            ", "if", "id", "!=", "last_id", "and", "not", "redirect", ":", "\n", "                ", "yield", "(", "id", ",", "revid", ",", "title", ",", "ns", ",", "catSet", ",", "page", ")", "\n", "last_id", "=", "id", "\n", "ns", "=", "'0'", "\n", "", "id", "=", "None", "\n", "revid", "=", "None", "\n", "title", "=", "None", "\n", "page", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.process_dump": [[2857, 3003], ["logging.info", "timeit.default_timer", "max", "multiprocessing.Queue", "multiprocessing.Value", "multiprocessing.Process", "multiprocessing.Process.start", "multiprocessing.Queue", "logging.info", "range", "WikiExtractor.pages_from", "fileinput.FileInput.close", "multiprocessing.Queue.put", "multiprocessing.Process.join", "logging.info", "logging.info", "fileinput.FileInput", "tagRE.search", "tagRE.search.group", "timeit.default_timer", "logging.info", "multiprocessing.Process", "multiprocessing.Process.start", "workers.append", "WikiExtractor.keepPage", "multiprocessing.Queue.put", "w.join", "timeit.default_timer", "isinstance", "line.decode.decode", "tagRE.search.group", "os.path.exists", "timeit.default_timer", "len", "multiprocessing.Queue.put", "keyRE.search", "re.search", "logging.info", "fileinput.FileInput", "WikiExtractor.load_templates", "fileinput.FileInput.close", "logging.info", "WikiExtractor.load_templates", "fileinput.FileInput.close", "fileinput.FileInput", "logging.info", "m.group.rfind", "tagRE.search.group", "re.search", "ValueError", "time.sleep", "keyRE.search.groups", "tagRE.search.group", "tagRE.search.group"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.pages_from", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.keepPage", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.load_templates", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.load_templates", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close"], ["", "", "", "def", "process_dump", "(", "input_file", ",", "template_file", ",", "out_file", ",", "file_size", ",", "file_compress", ",", "\n", "process_count", ")", ":", "\n", "    ", "\"\"\"\n    :param input_file: name of the wikipedia dump file; '-' to read from stdin\n    :param template_file: optional file with template definitions.\n    :param out_file: directory where to store extracted data, or '-' for stdout\n    :param file_size: max size of each extracted file, or None for no max (one file)\n    :param file_compress: whether to compress files with bzip.\n    :param process_count: number of extraction processes to spawn.\n    \"\"\"", "\n", "\n", "if", "input_file", "==", "'-'", ":", "\n", "        ", "input", "=", "sys", ".", "stdin", "\n", "", "else", ":", "\n", "        ", "input", "=", "fileinput", ".", "FileInput", "(", "input_file", ",", "openhook", "=", "fileinput", ".", "hook_compressed", ")", "\n", "\n", "# collect siteinfo", "\n", "", "for", "line", "in", "input", ":", "\n", "# When an input file is .bz2 or .gz, line can be a bytes even in Python 3.", "\n", "        ", "if", "not", "isinstance", "(", "line", ",", "text_type", ")", ":", "line", "=", "line", ".", "decode", "(", "'utf-8'", ")", "\n", "m", "=", "tagRE", ".", "search", "(", "line", ")", "\n", "if", "not", "m", ":", "\n", "            ", "continue", "\n", "", "tag", "=", "m", ".", "group", "(", "2", ")", "\n", "if", "tag", "==", "'base'", ":", "\n", "# discover urlbase from the xml dump file", "\n", "# /mediawiki/siteinfo/base", "\n", "            ", "base", "=", "m", ".", "group", "(", "3", ")", "\n", "options", ".", "urlbase", "=", "base", "[", ":", "base", ".", "rfind", "(", "\"/\"", ")", "]", "\n", "", "elif", "tag", "==", "'namespace'", ":", "\n", "            ", "mk", "=", "keyRE", ".", "search", "(", "line", ")", "\n", "if", "mk", ":", "\n", "                ", "nsid", "=", "''", ".", "join", "(", "mk", ".", "groups", "(", ")", ")", "\n", "", "else", ":", "\n", "                ", "nsid", "=", "''", "\n", "", "options", ".", "knownNamespaces", "[", "m", ".", "group", "(", "3", ")", "]", "=", "nsid", "\n", "if", "re", ".", "search", "(", "'key=\"10\"'", ",", "line", ")", ":", "\n", "                ", "options", ".", "templateNamespace", "=", "m", ".", "group", "(", "3", ")", "\n", "options", ".", "templatePrefix", "=", "options", ".", "templateNamespace", "+", "':'", "\n", "", "elif", "re", ".", "search", "(", "'key=\"828\"'", ",", "line", ")", ":", "\n", "                ", "options", ".", "moduleNamespace", "=", "m", ".", "group", "(", "3", ")", "\n", "options", ".", "modulePrefix", "=", "options", ".", "moduleNamespace", "+", "':'", "\n", "", "", "elif", "tag", "==", "'/siteinfo'", ":", "\n", "            ", "break", "\n", "\n", "", "", "if", "options", ".", "expand_templates", ":", "\n", "# preprocess", "\n", "        ", "template_load_start", "=", "default_timer", "(", ")", "\n", "if", "template_file", ":", "\n", "            ", "if", "os", ".", "path", ".", "exists", "(", "template_file", ")", ":", "\n", "                ", "logging", ".", "info", "(", "\"Loading template definitions from: %s\"", ",", "template_file", ")", "\n", "# can't use with here:", "\n", "file", "=", "fileinput", ".", "FileInput", "(", "template_file", ",", "\n", "openhook", "=", "fileinput", ".", "hook_compressed", ")", "\n", "load_templates", "(", "file", ")", "\n", "file", ".", "close", "(", ")", "\n", "", "else", ":", "\n", "                ", "if", "input_file", "==", "'-'", ":", "\n", "# can't scan then reset stdin; must error w/ suggestion to specify template_file", "\n", "                    ", "raise", "ValueError", "(", "\"to use templates with stdin dump, must supply explicit template-file\"", ")", "\n", "", "logging", ".", "info", "(", "\"Preprocessing '%s' to collect template definitions: this may take some time.\"", ",", "input_file", ")", "\n", "load_templates", "(", "input", ",", "template_file", ")", "\n", "input", ".", "close", "(", ")", "\n", "input", "=", "fileinput", ".", "FileInput", "(", "input_file", ",", "openhook", "=", "fileinput", ".", "hook_compressed", ")", "\n", "", "", "template_load_elapsed", "=", "default_timer", "(", ")", "-", "template_load_start", "\n", "logging", ".", "info", "(", "\"Loaded %d templates in %.1fs\"", ",", "len", "(", "options", ".", "templates", ")", ",", "template_load_elapsed", ")", "\n", "\n", "# process pages", "\n", "", "logging", ".", "info", "(", "\"Starting page extraction from %s.\"", ",", "input_file", ")", "\n", "extract_start", "=", "default_timer", "(", ")", "\n", "\n", "# Parallel Map/Reduce:", "\n", "# - pages to be processed are dispatched to workers", "\n", "# - a reduce process collects the results, sort them and print them.", "\n", "\n", "process_count", "=", "max", "(", "1", ",", "process_count", ")", "\n", "maxsize", "=", "10", "*", "process_count", "\n", "# output queue", "\n", "output_queue", "=", "Queue", "(", "maxsize", "=", "maxsize", ")", "\n", "\n", "if", "out_file", "==", "'-'", ":", "\n", "        ", "out_file", "=", "None", "\n", "\n", "", "worker_count", "=", "process_count", "\n", "\n", "# load balancing", "\n", "max_spool_length", "=", "10000", "\n", "spool_length", "=", "Value", "(", "'i'", ",", "0", ",", "lock", "=", "False", ")", "\n", "\n", "# reduce job that sorts and prints output", "\n", "reduce", "=", "Process", "(", "target", "=", "reduce_process", ",", "\n", "args", "=", "(", "options", ",", "output_queue", ",", "spool_length", ",", "\n", "out_file", ",", "file_size", ",", "file_compress", ")", ")", "\n", "reduce", ".", "start", "(", ")", "\n", "\n", "# initialize jobs queue", "\n", "jobs_queue", "=", "Queue", "(", "maxsize", "=", "maxsize", ")", "\n", "\n", "# start worker processes", "\n", "logging", ".", "info", "(", "\"Using %d extract processes.\"", ",", "worker_count", ")", "\n", "workers", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "worker_count", ")", ":", "\n", "        ", "extractor", "=", "Process", "(", "target", "=", "extract_process", ",", "\n", "args", "=", "(", "options", ",", "i", ",", "jobs_queue", ",", "output_queue", ")", ")", "\n", "extractor", ".", "daemon", "=", "True", "# only live while parent process lives", "\n", "extractor", ".", "start", "(", ")", "\n", "workers", ".", "append", "(", "extractor", ")", "\n", "\n", "# Mapper process", "\n", "", "page_num", "=", "0", "\n", "for", "page_data", "in", "pages_from", "(", "input", ")", ":", "\n", "        ", "id", ",", "revid", ",", "title", ",", "ns", ",", "catSet", ",", "page", "=", "page_data", "\n", "if", "keepPage", "(", "ns", ",", "catSet", ",", "page", ")", ":", "\n", "# slow down", "\n", "            ", "delay", "=", "0", "\n", "if", "spool_length", ".", "value", ">", "max_spool_length", ":", "\n", "# reduce to 10%", "\n", "                ", "while", "spool_length", ".", "value", ">", "max_spool_length", "/", "10", ":", "\n", "                    ", "time", ".", "sleep", "(", "10", ")", "\n", "delay", "+=", "10", "\n", "", "", "if", "delay", ":", "\n", "                ", "logging", ".", "info", "(", "'Delay %ds'", ",", "delay", ")", "\n", "", "job", "=", "(", "id", ",", "revid", ",", "title", ",", "page", ",", "page_num", ")", "\n", "jobs_queue", ".", "put", "(", "job", ")", "# goes to any available extract_process", "\n", "page_num", "+=", "1", "\n", "", "page", "=", "None", "# free memory", "\n", "\n", "", "input", ".", "close", "(", ")", "\n", "\n", "# signal termination", "\n", "for", "_", "in", "workers", ":", "\n", "        ", "jobs_queue", ".", "put", "(", "None", ")", "\n", "# wait for workers to terminate", "\n", "", "for", "w", "in", "workers", ":", "\n", "        ", "w", ".", "join", "(", ")", "\n", "\n", "# signal end of work to reduce process", "\n", "", "output_queue", ".", "put", "(", "None", ")", "\n", "# wait for it to finish", "\n", "reduce", ".", "join", "(", ")", "\n", "\n", "extract_duration", "=", "default_timer", "(", ")", "-", "extract_start", "\n", "extract_rate", "=", "page_num", "/", "extract_duration", "\n", "logging", ".", "info", "(", "\"Finished %d-process extraction of %d articles in %.1fs (%.1f art/s)\"", ",", "\n", "process_count", ",", "page_num", ",", "extract_duration", ",", "extract_rate", ")", "\n", "logging", ".", "info", "(", "\"total of page: %d, total of articl page: %d; total of used articl page: %d\"", "%", "(", "g_page_total", ",", "g_page_articl_total", ",", "g_page_articl_used_total", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.extract_process": [[3009, 3044], ["WikiExtractor.createLogger", "io.StringIO", "io.StringIO.close", "jobs_queue.get", "output_queue.put", "io.StringIO.truncate", "io.StringIO.seek", "logging.debug", "WikiExtractor.Extractor", "WikiExtractor.Extractor.extract", "io.StringIO.getvalue", "logging.exception"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.createLogger", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.extract"], ["", "def", "extract_process", "(", "opts", ",", "i", ",", "jobs_queue", ",", "output_queue", ")", ":", "\n", "    ", "\"\"\"Pull tuples of raw page content, do CPU/regex-heavy fixup, push finished text\n    :param i: process id.\n    :param jobs_queue: where to get jobs.\n    :param output_queue: where to queue extracted text for output.\n    \"\"\"", "\n", "\n", "global", "options", "\n", "options", "=", "opts", "\n", "\n", "createLogger", "(", "options", ".", "quiet", ",", "options", ".", "debug", ",", "options", ".", "log_file", ")", "\n", "\n", "out", "=", "StringIO", "(", ")", "# memory buffer", "\n", "\n", "\n", "while", "True", ":", "\n", "        ", "job", "=", "jobs_queue", ".", "get", "(", ")", "# job is (id, title, page, page_num)", "\n", "if", "job", ":", "\n", "            ", "id", ",", "revid", ",", "title", ",", "page", ",", "page_num", "=", "job", "\n", "try", ":", "\n", "                ", "e", "=", "Extractor", "(", "*", "job", "[", ":", "4", "]", ")", "# (id, revid, title, page)", "\n", "page", "=", "None", "# free memory", "\n", "e", ".", "extract", "(", "out", ")", "\n", "text", "=", "out", ".", "getvalue", "(", ")", "\n", "", "except", ":", "\n", "                ", "text", "=", "''", "\n", "logging", ".", "exception", "(", "'Processing page: %s %s'", ",", "id", ",", "title", ")", "\n", "\n", "", "output_queue", ".", "put", "(", "(", "page_num", ",", "text", ")", ")", "\n", "out", ".", "truncate", "(", "0", ")", "\n", "out", ".", "seek", "(", "0", ")", "\n", "", "else", ":", "\n", "            ", "logging", ".", "debug", "(", "'Quit extractor'", ")", "\n", "break", "\n", "", "", "out", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.reduce_process": [[3047, 3103], ["WikiExtractor.createLogger", "timeit.default_timer", "WikiExtractor.NextFile", "WikiExtractor.OutputSplitter", "WikiExtractor.OutputSplitter.close", "logging.warn", "WikiExtractor.OutputSplitter.write", "len", "output_queue.get", "len", "spool.pop().encode", "logging.info", "timeit.default_timer", "len", "logging.debug", "len", "spool.pop", "timeit.default_timer"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.createLogger", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["def", "reduce_process", "(", "opts", ",", "output_queue", ",", "spool_length", ",", "\n", "out_file", "=", "None", ",", "file_size", "=", "0", ",", "file_compress", "=", "True", ")", ":", "\n", "    ", "\"\"\"Pull finished article text, write series of files (or stdout)\n    :param opts: global parameters.\n    :param output_queue: text to be output.\n    :param spool_length: spool length.\n    :param out_file: filename where to print.\n    :param file_size: max file size.\n    :param file_compress: whether to compress output.\n    \"\"\"", "\n", "\n", "global", "options", "\n", "options", "=", "opts", "\n", "\n", "createLogger", "(", "options", ".", "quiet", ",", "options", ".", "debug", ",", "options", ".", "log_file", ")", "\n", "\n", "if", "out_file", ":", "\n", "        ", "nextFile", "=", "NextFile", "(", "out_file", ")", "\n", "output", "=", "OutputSplitter", "(", "nextFile", ",", "file_size", ",", "file_compress", ")", "\n", "", "else", ":", "\n", "        ", "output", "=", "sys", ".", "stdout", "if", "PY2", "else", "sys", ".", "stdout", ".", "buffer", "\n", "if", "file_compress", ":", "\n", "            ", "logging", ".", "warn", "(", "\"writing to stdout, so no output compression (use an external tool)\"", ")", "\n", "\n", "", "", "interval_start", "=", "default_timer", "(", ")", "\n", "# FIXME: use a heap", "\n", "spool", "=", "{", "}", "# collected pages", "\n", "next_page", "=", "0", "# sequence numbering of page", "\n", "while", "True", ":", "\n", "        ", "if", "next_page", "in", "spool", ":", "\n", "            ", "output", ".", "write", "(", "spool", ".", "pop", "(", "next_page", ")", ".", "encode", "(", "'utf-8'", ")", ")", "\n", "next_page", "+=", "1", "\n", "# tell mapper our load:", "\n", "spool_length", ".", "value", "=", "len", "(", "spool", ")", "\n", "# progress report", "\n", "if", "next_page", "%", "report_period", "==", "0", ":", "\n", "                ", "interval_rate", "=", "report_period", "/", "(", "default_timer", "(", ")", "-", "interval_start", ")", "\n", "logging", ".", "info", "(", "\"Extracted %d articles (%.1f art/s)\"", ",", "\n", "next_page", ",", "interval_rate", ")", "\n", "interval_start", "=", "default_timer", "(", ")", "\n", "", "", "else", ":", "\n", "# mapper puts None to signal finish", "\n", "            ", "pair", "=", "output_queue", ".", "get", "(", ")", "\n", "if", "not", "pair", ":", "\n", "                ", "break", "\n", "", "page_num", ",", "text", "=", "pair", "\n", "spool", "[", "page_num", "]", "=", "text", "\n", "# tell mapper our load:", "\n", "spool_length", ".", "value", "=", "len", "(", "spool", ")", "\n", "# FIXME: if an extractor dies, process stalls; the other processes", "\n", "# continue to produce pairs, filling up memory.", "\n", "if", "len", "(", "spool", ")", ">", "200", ":", "\n", "                ", "logging", ".", "debug", "(", "'Collected %d, waiting: %d, %d'", ",", "len", "(", "spool", ")", ",", "\n", "next_page", ",", "next_page", "==", "page_num", ")", "\n", "", "", "", "if", "output", "!=", "sys", ".", "stdout", ":", "\n", "        ", "output", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.main": [[3110, 3283], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "max", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument_group", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "parser.add_argument_group.add_argument", "argparse.ArgumentParser.parse_args", "logging.basicConfig", "WikiExtractor.createLogger", "WikiExtractor.process_dump", "set", "set", "WikiExtractor.ignoreTag", "set", "WikiExtractor.ignoreTag", "fileinput.FileInput", "WikiExtractor.pages_from", "fileinput.FileInput.close", "os.path.basename", "multiprocessing.cpu_count", "int", "ValueError", "logging.error", "parser.parse_args.namespaces.split", "parser.parse_args.ignored_tags.split", "parser.parse_args.discard_elements.split", "os.path.exists", "WikiExtractor.Extractor.extract", "os.path.isdir", "os.makedirs", "len", "open", "f.readlines", "logging.info", "logging.info", "logging.info", "logging.info", "parser.parse_args.bytes[].lower", "logging.error", "str", "str", "open", "WikiExtractor.load_templates", "WikiExtractor.Extractor", "str", "len", "str.strip", "str.startswith", "str.startswith", "print", "print", "len", "options.filter_category_exclude.add", "options.filter_category_include.add", "str.lstrip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.createLogger", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.process_dump", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ignoreTag", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.ignoreTag", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.pages_from", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.extract", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.load_templates"], ["def", "main", "(", ")", ":", "\n", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "prog", "=", "os", ".", "path", ".", "basename", "(", "sys", ".", "argv", "[", "0", "]", ")", ",", "\n", "formatter_class", "=", "argparse", ".", "RawDescriptionHelpFormatter", ",", "\n", "description", "=", "__doc__", ")", "\n", "parser", ".", "add_argument", "(", "\"input\"", ",", "\n", "help", "=", "\"XML wiki dump file\"", ")", "\n", "groupO", "=", "parser", ".", "add_argument_group", "(", "'Output'", ")", "\n", "groupO", ".", "add_argument", "(", "\"-o\"", ",", "\"--output\"", ",", "default", "=", "\"text\"", ",", "\n", "help", "=", "\"directory for extracted files (or '-' for dumping to stdout)\"", ")", "\n", "groupO", ".", "add_argument", "(", "\"-b\"", ",", "\"--bytes\"", ",", "default", "=", "\"1M\"", ",", "\n", "help", "=", "\"maximum bytes per output file (default %(default)s)\"", ",", "\n", "metavar", "=", "\"n[KMG]\"", ")", "\n", "groupO", ".", "add_argument", "(", "\"-c\"", ",", "\"--compress\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"compress output files using bzip\"", ")", "\n", "groupO", ".", "add_argument", "(", "\"--json\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"write output in json format instead of the default one\"", ")", "\n", "\n", "\n", "groupP", "=", "parser", ".", "add_argument_group", "(", "'Processing'", ")", "\n", "groupP", ".", "add_argument", "(", "\"--html\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"produce HTML output, subsumes --links\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"-l\"", ",", "\"--links\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"preserve links\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"-s\"", ",", "\"--sections\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"preserve sections\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--lists\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"preserve lists\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"-ns\"", ",", "\"--namespaces\"", ",", "default", "=", "\"\"", ",", "metavar", "=", "\"ns1,ns2\"", ",", "\n", "help", "=", "\"accepted namespaces in links\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--templates\"", ",", "\n", "help", "=", "\"use or create file containing templates\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--no_templates\"", ",", "action", "=", "\"store_false\"", ",", "\n", "help", "=", "\"Do not expand templates\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"-r\"", ",", "\"--revision\"", ",", "action", "=", "\"store_true\"", ",", "default", "=", "options", ".", "print_revision", ",", "\n", "help", "=", "\"Include the document revision id (default=%(default)s)\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--min_text_length\"", ",", "type", "=", "int", ",", "default", "=", "options", ".", "min_text_length", ",", "\n", "help", "=", "\"Minimum expanded text length required to write document (default=%(default)s)\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--filter_disambig_pages\"", ",", "action", "=", "\"store_true\"", ",", "default", "=", "options", ".", "filter_disambig_pages", ",", "\n", "help", "=", "\"Remove pages from output that contain disabmiguation markup (default=%(default)s)\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"-it\"", ",", "\"--ignored_tags\"", ",", "default", "=", "\"\"", ",", "metavar", "=", "\"abbr,b,big\"", ",", "\n", "help", "=", "\"comma separated list of tags that will be dropped, keeping their content\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"-de\"", ",", "\"--discard_elements\"", ",", "default", "=", "\"\"", ",", "metavar", "=", "\"gallery,timeline,noinclude\"", ",", "\n", "help", "=", "\"comma separated list of elements that will be removed from the article text\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--keep_tables\"", ",", "action", "=", "\"store_true\"", ",", "default", "=", "options", ".", "keep_tables", ",", "\n", "help", "=", "\"Preserve tables in the output article text (default=%(default)s)\"", ")", "\n", "default_process_count", "=", "max", "(", "1", ",", "cpu_count", "(", ")", "-", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--processes\"", ",", "type", "=", "int", ",", "default", "=", "default_process_count", ",", "\n", "help", "=", "\"Number of processes to use (default %(default)s)\"", ")", "\n", "\n", "groupS", "=", "parser", ".", "add_argument_group", "(", "'Special'", ")", "\n", "groupS", ".", "add_argument", "(", "\"-q\"", ",", "\"--quiet\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"suppress reporting progress info\"", ")", "\n", "groupS", ".", "add_argument", "(", "\"--debug\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"print debug info\"", ")", "\n", "groupS", ".", "add_argument", "(", "\"-a\"", ",", "\"--article\"", ",", "action", "=", "\"store_true\"", ",", "\n", "help", "=", "\"analyze a file containing a single article (debug option)\"", ")", "\n", "groupS", ".", "add_argument", "(", "\"--log_file\"", ",", "\n", "help", "=", "\"path to save the log info\"", ")", "\n", "groupS", ".", "add_argument", "(", "\"-v\"", ",", "\"--version\"", ",", "action", "=", "\"version\"", ",", "\n", "version", "=", "'%(prog)s '", "+", "version", ",", "\n", "help", "=", "\"print program version\"", ")", "\n", "groupP", ".", "add_argument", "(", "\"--filter_category\"", ",", "\n", "help", "=", "\"specify the file that listing the Categories you want to include or exclude. One line for\"", "\n", "\" one category. starting with: 1) '#' comment, ignored; 2) '^' exclude; Note: excluding has higher priority than including\"", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "options", ".", "keepLinks", "=", "args", ".", "links", "\n", "options", ".", "keepSections", "=", "args", ".", "sections", "\n", "options", ".", "keepLists", "=", "args", ".", "lists", "\n", "options", ".", "toHTML", "=", "args", ".", "html", "\n", "options", ".", "write_json", "=", "args", ".", "json", "\n", "options", ".", "print_revision", "=", "args", ".", "revision", "\n", "options", ".", "min_text_length", "=", "args", ".", "min_text_length", "\n", "if", "args", ".", "html", ":", "\n", "        ", "options", ".", "keepLinks", "=", "True", "\n", "\n", "", "options", ".", "expand_templates", "=", "args", ".", "no_templates", "\n", "options", ".", "filter_disambig_pages", "=", "args", ".", "filter_disambig_pages", "\n", "options", ".", "keep_tables", "=", "args", ".", "keep_tables", "\n", "\n", "try", ":", "\n", "        ", "power", "=", "'kmg'", ".", "find", "(", "args", ".", "bytes", "[", "-", "1", "]", ".", "lower", "(", ")", ")", "+", "1", "\n", "file_size", "=", "int", "(", "args", ".", "bytes", "[", ":", "-", "1", "]", ")", "*", "1024", "**", "power", "\n", "if", "file_size", "<", "minFileSize", ":", "\n", "            ", "raise", "ValueError", "(", ")", "\n", "", "", "except", "ValueError", ":", "\n", "        ", "logging", ".", "error", "(", "'Insufficient or invalid size: %s'", ",", "args", ".", "bytes", ")", "\n", "return", "\n", "\n", "", "if", "args", ".", "namespaces", ":", "\n", "        ", "options", ".", "acceptedNamespaces", "=", "set", "(", "args", ".", "namespaces", ".", "split", "(", "','", ")", ")", "\n", "\n", "# ignoredTags and discardElemets have default values already supplied, if passed in the defaults are overwritten", "\n", "", "if", "args", ".", "ignored_tags", ":", "\n", "        ", "ignoredTags", "=", "set", "(", "args", ".", "ignored_tags", ".", "split", "(", "','", ")", ")", "\n", "", "else", ":", "\n", "        ", "ignoredTags", "=", "[", "\n", "'abbr'", ",", "'b'", ",", "'big'", ",", "'blockquote'", ",", "'center'", ",", "'cite'", ",", "'em'", ",", "\n", "'font'", ",", "'h1'", ",", "'h2'", ",", "'h3'", ",", "'h4'", ",", "'hiero'", ",", "'i'", ",", "'kbd'", ",", "\n", "'p'", ",", "'plaintext'", ",", "'s'", ",", "'span'", ",", "'strike'", ",", "'strong'", ",", "\n", "'tt'", ",", "'u'", ",", "'var'", "\n", "]", "\n", "\n", "# 'a' tag is handled separately", "\n", "", "for", "tag", "in", "ignoredTags", ":", "\n", "        ", "ignoreTag", "(", "tag", ")", "\n", "\n", "", "if", "args", ".", "discard_elements", ":", "\n", "        ", "options", ".", "discardElements", "=", "set", "(", "args", ".", "discard_elements", ".", "split", "(", "','", ")", ")", "\n", "\n", "", "FORMAT", "=", "'%(levelname)s: %(message)s'", "\n", "logging", ".", "basicConfig", "(", "format", "=", "FORMAT", ")", "\n", "\n", "options", ".", "quiet", "=", "args", ".", "quiet", "\n", "options", ".", "debug", "=", "args", ".", "debug", "\n", "options", ".", "log_file", "=", "args", ".", "log_file", "\n", "createLogger", "(", "options", ".", "quiet", ",", "options", ".", "debug", ",", "options", ".", "log_file", ")", "\n", "\n", "input_file", "=", "args", ".", "input", "\n", "\n", "if", "not", "options", ".", "keepLinks", ":", "\n", "        ", "ignoreTag", "(", "'a'", ")", "\n", "\n", "# sharing cache of parser templates is too slow:", "\n", "# manager = Manager()", "\n", "# templateCache = manager.dict()", "\n", "\n", "", "if", "args", ".", "article", ":", "\n", "        ", "if", "args", ".", "templates", ":", "\n", "            ", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "templates", ")", ":", "\n", "                ", "with", "open", "(", "args", ".", "templates", ")", "as", "file", ":", "\n", "                    ", "load_templates", "(", "file", ")", "\n", "\n", "", "", "", "file", "=", "fileinput", ".", "FileInput", "(", "input_file", ",", "openhook", "=", "fileinput", ".", "hook_compressed", ")", "\n", "for", "page_data", "in", "pages_from", "(", "file", ")", ":", "\n", "            ", "id", ",", "revid", ",", "title", ",", "ns", ",", "catSet", ",", "page", "=", "page_data", "\n", "Extractor", "(", "id", ",", "revid", ",", "title", ",", "page", ")", ".", "extract", "(", "sys", ".", "stdout", ")", "\n", "", "file", ".", "close", "(", ")", "\n", "return", "\n", "\n", "", "output_path", "=", "args", ".", "output", "\n", "if", "output_path", "!=", "'-'", "and", "not", "os", ".", "path", ".", "isdir", "(", "output_path", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "os", ".", "makedirs", "(", "output_path", ")", "\n", "", "except", ":", "\n", "            ", "logging", ".", "error", "(", "'Could not create: %s'", ",", "output_path", ")", "\n", "return", "\n", "\n", "", "", "filter_category", "=", "args", ".", "filter_category", "\n", "if", "(", "filter_category", "!=", "None", "and", "len", "(", "filter_category", ")", ">", "0", ")", ":", "\n", "        ", "with", "open", "(", "filter_category", ")", "as", "f", ":", "\n", "            ", "error_cnt", "=", "0", "\n", "for", "line", "in", "f", ".", "readlines", "(", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "line", "=", "str", "(", "line", ".", "strip", "(", ")", ")", "\n", "if", "line", ".", "startswith", "(", "'#'", ")", "or", "len", "(", "line", ")", "==", "0", ":", "\n", "                        ", "continue", ";", "\n", "", "elif", "line", ".", "startswith", "(", "'^'", ")", ":", "\n", "                        ", "options", ".", "filter_category_exclude", ".", "add", "(", "line", ".", "lstrip", "(", "'^'", ")", ")", "\n", "", "else", ":", "\n", "                        ", "options", ".", "filter_category_include", ".", "add", "(", "line", ")", "\n", "", "", "except", "Exception", "as", "e", ":", "\n", "                    ", "error_cnt", "+=", "1", "\n", "print", "(", "u\"Category not in utf8, ignored. error cnt %d:\\t%s\"", "%", "(", "error_cnt", ",", "e", ")", ")", "\n", "print", "(", "line", ")", "\n", "", "", "logging", ".", "info", "(", "\"Excluding categories:\"", ",", ")", "\n", "logging", ".", "info", "(", "str", "(", "options", ".", "filter_category_exclude", ")", ")", "\n", "logging", ".", "info", "(", "\"Including categories:\"", ")", "\n", "logging", ".", "info", "(", "str", "(", "len", "(", "options", ".", "filter_category_include", ")", ")", ")", "\n", "\n", "", "", "process_dump", "(", "input_file", ",", "args", ".", "templates", ",", "output_path", ",", "file_size", ",", "\n", "args", ".", "compress", ",", "args", ".", "processes", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.createLogger": [[3284, 3294], ["logging.getLogger", "logging.getLogger.setLevel", "logging.getLogger.setLevel", "logging.FileHandler", "logging.getLogger.addHandler"], "function", ["None"], ["", "def", "createLogger", "(", "quiet", ",", "debug", ",", "log_file", ")", ":", "\n", "    ", "logger", "=", "logging", ".", "getLogger", "(", ")", "\n", "if", "not", "quiet", ":", "\n", "        ", "logger", ".", "setLevel", "(", "logging", ".", "INFO", ")", "\n", "", "if", "debug", ":", "\n", "        ", "logger", ".", "setLevel", "(", "logging", ".", "DEBUG", ")", "\n", "#print (log_file)", "\n", "", "if", "log_file", ":", "\n", "        ", "fileHandler", "=", "logging", ".", "FileHandler", "(", "log_file", ")", "\n", "logger", ".", "addHandler", "(", "fileHandler", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.FullTokenizer.__init__": [[165, 170], ["tokenization.load_vocab", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "tokenization.FullTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.FullTokenizer.tokenize": [[171, 178], ["tokenization.FullTokenizer.basic_tokenizer.tokenize", "tokenization.FullTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.FullTokenizer.convert_tokens_to_ids": [[179, 181], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.FullTokenizer.convert_ids_to_tokens": [[182, 184], ["tokenization.convert_by_vocab"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.BasicTokenizer.__init__": [[189, 196], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.BasicTokenizer.tokenize": [[197, 220], ["tokenization.convert_to_unicode", "tokenization.BasicTokenizer._clean_text", "tokenization.BasicTokenizer._tokenize_chinese_chars", "tokenization.whitespace_tokenize", "tokenization.whitespace_tokenize", "split_tokens.extend", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "tokenization.BasicTokenizer._run_split_on_punc"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._tokenize_chinese_chars", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.whitespace_tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.whitespace_tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._run_strip_accents", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._run_split_on_punc"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "text", "=", "self", ".", "_tokenize_chinese_chars", "(", "text", ")", "\n", "\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n", "\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.BasicTokenizer._run_strip_accents": [[221, 231], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.BasicTokenizer._run_split_on_punc": [[232, 251], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.BasicTokenizer._tokenize_chinese_chars": [[252, 264], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.BasicTokenizer._is_chinese_char": [[265, 286], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "    ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "      ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.BasicTokenizer._clean_text": [[287, 299], ["ord", "tokenization._is_whitespace", "tokenization._is_control", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_whitespace", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.WordpieceTokenizer.__init__": [[304, 308], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "200", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.WordpieceTokenizer.tokenize": [[309, 361], ["tokenization.convert_to_unicode", "tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_to_unicode", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n    This uses a greedy longest-match-first algorithm to perform tokenization\n    using the given vocabulary.\n\n    For example:\n      input = \"unaffable\"\n      output = [\"un\", \"##aff\", \"##able\"]\n\n    Args:\n      text: A single token or whitespace separated tokens. This should have\n        already been passed through `BasicTokenizer.\n\n    Returns:\n      A list of wordpiece tokens.\n    \"\"\"", "\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "      ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "        ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "          ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "            ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "            ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "          ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "        ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.validate_case_matches_checkpoint": [[28, 76], ["re.match", "re.match.group", "ValueError"], "function", ["None"], ["def", "validate_case_matches_checkpoint", "(", "do_lower_case", ",", "init_checkpoint", ")", ":", "\n", "  ", "\"\"\"Checks whether the casing config is consistent with the checkpoint name.\"\"\"", "\n", "\n", "# The casing has to be passed in by the user and there is no explicit check", "\n", "# as to whether it matches the checkpoint. The casing information probably", "\n", "# should have been stored in the bert_config.json file, but it's not, so", "\n", "# we have to heuristically detect it to validate.", "\n", "\n", "if", "not", "init_checkpoint", ":", "\n", "    ", "return", "\n", "\n", "", "m", "=", "re", ".", "match", "(", "\"^.*?([A-Za-z0-9_-]+)/bert_model.ckpt\"", ",", "init_checkpoint", ")", "\n", "if", "m", "is", "None", ":", "\n", "    ", "return", "\n", "\n", "", "model_name", "=", "m", ".", "group", "(", "1", ")", "\n", "\n", "lower_models", "=", "[", "\n", "\"uncased_L-24_H-1024_A-16\"", ",", "\"uncased_L-12_H-768_A-12\"", ",", "\n", "\"multilingual_L-12_H-768_A-12\"", ",", "\"chinese_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "cased_models", "=", "[", "\n", "\"cased_L-12_H-768_A-12\"", ",", "\"cased_L-24_H-1024_A-16\"", ",", "\n", "\"multi_cased_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "is_bad_config", "=", "False", "\n", "if", "model_name", "in", "lower_models", "and", "not", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"False\"", "\n", "case_name", "=", "\"lowercased\"", "\n", "opposite_flag", "=", "\"True\"", "\n", "\n", "", "if", "model_name", "in", "cased_models", "and", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"True\"", "\n", "case_name", "=", "\"cased\"", "\n", "opposite_flag", "=", "\"False\"", "\n", "\n", "", "if", "is_bad_config", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"", "\n", "\"However, `%s` seems to be a %s model, so you \"", "\n", "\"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"", "\n", "\"how the model was pre-training. If this error is wrong, please \"", "\n", "\"just comment out this check.\"", "%", "(", "actual_flag", ",", "init_checkpoint", ",", "\n", "model_name", ",", "case_name", ",", "opposite_flag", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_to_unicode": [[78, 96], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "text.decode", "isinstance", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "convert_to_unicode", "(", "text", ")", ":", "\n", "  ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.printable_text": [[98, 119], ["isinstance", "isinstance", "isinstance", "ValueError", "text.decode", "ValueError", "isinstance", "text.encode", "ValueError", "type", "type"], "function", ["None"], ["", "", "def", "printable_text", "(", "text", ")", ":", "\n", "  ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.load_vocab": [[121, 135], ["collections.OrderedDict", "tensorflow.gfile.GFile", "tokenization.convert_to_unicode", "token.strip.strip", "reader.readline"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_to_unicode"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "  ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "        ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "vocab", "[", "\"sepsepsep\"", "]", "=", "index", "\n", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_by_vocab": [[137, 143], ["output.append"], "function", ["None"], ["", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "  ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "    ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_tokens_to_ids": [[145, 147], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_by_vocab"], ["", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_ids_to_tokens": [[149, 151], ["tokenization.convert_by_vocab"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.convert_by_vocab"], ["", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization.whitespace_tokenize": [[153, 160], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization._is_whitespace": [[363, 373], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization._is_control": [[375, 385], ["unicodedata.category"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "in", "(", "\"Cc\"", ",", "\"Cf\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.tokenization._is_punctuation": [[387, 401], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "", "", ""]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.create_insts.run_proc": [[16, 23], ["range", "len", "file_list[].replace", "print", "subprocess.run", "command.format().split", "command.format"], "function", ["None"], ["def", "run_proc", "(", "idx", ",", "n", ",", "file_list", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "len", "(", "file_list", ")", ")", ":", "\n", "        ", "if", "i", "%", "n", "==", "idx", ":", "\n", "            ", "target", "=", "file_list", "[", "i", "]", ".", "replace", "(", "\"raw\"", ",", "\"data\"", ")", "\n", "print", "(", "file_list", "[", "i", "]", ")", "\n", "command", "=", "\"python3 code/create_instances.py --input_file_prefix {} --output_file {} --vocab_file ernie_base/vocab.txt --dupe_factor 1 --max_seq_length 256 --max_predictions_per_seq 40\"", "\n", "subprocess", ".", "run", "(", "command", ".", "format", "(", "file_list", "[", "i", "]", ",", "target", ")", ".", "split", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.extract.run_proc": [[17, 48], ["input_name.replace", "bs4.BeautifulSoup", "bs4.BeautifulSoup.find_all", "logging.info", "os.path.exists", "os.makedirs", "os.path.exists", "open", "open", "input_name.replace.split", "doc.get_text", "x.strip", "fout.write", "doc.get_text.split", "logging.warning", "x.get_text().strip", "x.get", "doc.find_all", "str", "x.get_text"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "", "def", "run_proc", "(", "idx", ",", "n", ",", "file_list", ")", ":", "\n", "    ", "for", "input_name", "in", "file_list", "[", "idx", ":", ":", "n", "]", ":", "\n", "        ", "target", "=", "input_name", ".", "replace", "(", "input_folder", ",", "\"pretrain_data/ann\"", ")", "\n", "folder", "=", "'/'", ".", "join", "(", "target", ".", "split", "(", "'/'", ")", "[", ":", "-", "1", "]", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "folder", ")", ":", "\n", "            ", "os", ".", "makedirs", "(", "folder", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "target", ")", ":", "\n", "# Already done parsing", "\n", "            ", "continue", "\n", "\n", "", "soup", "=", "BeautifulSoup", "(", "open", "(", "input_name", ")", ",", "features", "=", "\"html.parser\"", ")", "\n", "docs", "=", "soup", ".", "find_all", "(", "'doc'", ")", "\n", "\n", "with", "open", "(", "target", ",", "'w'", ")", "as", "fout", ":", "\n", "            ", "for", "doc", "in", "docs", ":", "\n", "                ", "content", "=", "doc", ".", "get_text", "(", "\" sepsepsep \"", ")", "\n", "while", "content", "[", "0", "]", "==", "\"\\n\"", ":", "\n", "                    ", "content", "=", "content", "[", "1", ":", "]", "\n", "", "content", "=", "[", "x", ".", "strip", "(", ")", "for", "x", "in", "content", ".", "split", "(", "\"\\n\"", ")", "]", "\n", "content", "=", "\"\"", ".", "join", "(", "content", "[", "1", ":", "]", ")", "\n", "\n", "try", ":", "\n", "                    ", "lookup", "=", "[", "(", "x", ".", "get_text", "(", ")", ".", "strip", "(", ")", ",", "x", ".", "get", "(", "'href'", ")", ")", "\n", "for", "x", "in", "doc", ".", "find_all", "(", "\"a\"", ")", "]", "\n", "lookup", "=", "\"[_end_]\"", ".", "join", "(", "\n", "[", "\"[_map_]\"", ".", "join", "(", "x", ")", "for", "x", "in", "lookup", "]", ")", "\n", "fout", ".", "write", "(", "content", "+", "\"[_end_]\"", "+", "lookup", "+", "\"\\n\"", ")", "\n", "", "except", "Exception", "as", "e", ":", "\n", "                    ", "logging", ".", "warning", "(", "\n", "'Error {} when parsing file {}'", ".", "format", "(", "str", "(", "e", ")", ",", "input_name", ")", ")", "\n", "", "", "", "logging", ".", "info", "(", "'Finished {}'", ".", "format", "(", "target", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.utils.get_all_anchors_name": [[7, 33], ["os.walk", "tqdm.trange", "print", "json.dump", "len", "open", "open.readlines", "open.close", "len", "list", "open", "file_list.append", "doc.strip.strip", "doc.strip.split", "anchors.keys", "os.path.join", "x.split", "len", "anchors.get"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["def", "get_all_anchors_name", "(", ")", ":", "\n", "    ", "input_folder", "=", "\"pretrain_data/ann\"", "\n", "file_list", "=", "[", "]", "\n", "for", "path", ",", "_", ",", "filenames", "in", "os", ".", "walk", "(", "input_folder", ")", ":", "\n", "        ", "for", "filename", "in", "filenames", ":", "\n", "            ", "file_list", ".", "append", "(", "os", ".", "path", ".", "join", "(", "path", ",", "filename", ")", ")", "\n", "\n", "", "", "anchors", "=", "{", "}", "\n", "for", "i", "in", "trange", "(", "len", "(", "file_list", ")", ")", ":", "\n", "        ", "input_name", "=", "file_list", "[", "i", "]", "\n", "fin", "=", "open", "(", "input_name", ",", "\"r\"", ")", "\n", "for", "doc", "in", "fin", ".", "readlines", "(", ")", ":", "\n", "            ", "doc", "=", "doc", ".", "strip", "(", ")", "\n", "segs", "=", "doc", ".", "split", "(", "\"[_end_]\"", ")", "\n", "map_segs", "=", "segs", "[", "1", ":", "]", "\n", "maps", "=", "{", "}", "\n", "for", "x", "in", "map_segs", ":", "\n", "                ", "v", "=", "x", ".", "split", "(", "\"[_map_]\"", ")", "\n", "if", "len", "(", "v", ")", "!=", "2", ":", "\n", "                    ", "continue", "\n", "", "if", "anchors", ".", "get", "(", "v", "[", "1", "]", ",", "-", "1", ")", "!=", "-", "1", ":", "\n", "                    ", "continue", "\n", "", "anchors", "[", "v", "[", "1", "]", "]", "=", "1", "\n", "", "", "fin", ".", "close", "(", ")", "\n", "", "print", "(", "len", "(", "anchors", ")", ")", "\n", "json", ".", "dump", "(", "list", "(", "anchors", ".", "keys", "(", ")", ")", ",", "open", "(", "\"pretrain_data/all_anchors_name.json\"", ",", "'w'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.utils.aggregate_anchor2id": [[34, 42], ["open", "os.listdir", "open.close", "open", "open.write", "open.close", "os.path.join", "open.read"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close"], ["", "def", "aggregate_anchor2id", "(", ")", ":", "\n", "    ", "fout", "=", "open", "(", "\"anchor2id.txt\"", ",", "'w'", ")", "\n", "files", "=", "os", ".", "listdir", "(", "\"pretrain_data/anchor\"", ")", "\n", "for", "file", "in", "files", ":", "\n", "        ", "f", "=", "open", "(", "os", ".", "path", ".", "join", "(", "\"pretrain_data/anchor\"", ",", "file", ")", ")", "\n", "fout", ".", "write", "(", "f", ".", "read", "(", ")", ")", "\n", "f", ".", "close", "(", ")", "\n", "", "fout", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.InputExample.__init__": [[50, 66], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.InputFeatures.__init__": [[71, 78], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "labels", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "input_ent", "=", "input_ent", "\n", "self", ".", "ent_mask", "=", "ent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.DataProcessor.get_train_examples": [[83, 86], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.DataProcessor.get_dev_examples": [[87, 90], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.DataProcessor.get_labels": [[91, 94], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.DataProcessor._read_json": [[95, 99], ["open", "simplejson.load"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "@", "classmethod", "\n", "def", "_read_json", "(", "cls", ",", "input_file", ")", ":", "\n", "        ", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "return", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.TypingProcessor.get_train_examples": [[104, 119], ["logger.info", "run_typing.TypingProcessor._create_examples", "d.items", "run_typing.TypingProcessor._read_json", "list", "os.path.join", "os.path.join", "d.keys", "len"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "logger", ".", "info", "(", "\"LOOKING AT {}\"", ".", "format", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ")", "\n", "examples", "=", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ",", "\"train\"", ")", "\n", "d", "=", "{", "}", "\n", "for", "e", "in", "examples", ":", "\n", "            ", "for", "l", "in", "e", ".", "label", ":", "\n", "                ", "if", "l", "in", "d", ":", "\n", "                    ", "d", "[", "l", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "d", "[", "l", "]", "=", "1", "\n", "", "", "", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", ":", "\n", "            ", "d", "[", "k", "]", "=", "(", "len", "(", "examples", ")", "-", "v", ")", "*", "1.", "/", "v", "\n", "", "return", "examples", ",", "list", "(", "d", ".", "keys", "(", ")", ")", ",", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.TypingProcessor.get_dev_examples": [[120, 124], ["run_typing.TypingProcessor._create_examples", "run_typing.TypingProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.json\"", ")", ")", ",", "\"dev\"", ")", "\n", "", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.TypingProcessor.get_test_examples": [[124, 128], ["run_typing.TypingProcessor._create_examples", "run_typing.TypingProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test.json\"", ")", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.TypingProcessor.get_labels": [[130, 133], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.TypingProcessor._create_examples": [[134, 147], ["enumerate", "examples.append", "run_typing.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "i", "\n", "text_a", "=", "(", "line", "[", "'sent'", "]", ",", "[", "[", "\"SPAN\"", ",", "line", "[", "\"start\"", "]", ",", "line", "[", "\"end\"", "]", "]", "]", ")", "\n", "text_b", "=", "line", "[", "'ents'", "]", "\n", "label", "=", "line", "[", "'labels'", "]", "\n", "#if guid != 51:", "\n", "#    continue", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.convert_examples_to_features": [[149, 263], ["enumerate", "open", "fin.readline", "tokenizer_label.tokenize", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "line.strip().split", "int", "len", "len", "print", "print", "print", "print", "exit", "sum", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "run_typing.InputFeatures", "span_mask.append", "span_mask.append", "input_ent.append", "ent_mask.append", "input_ent.append", "ent_mask.append", "len", "len", "line.strip", "str", "str", "zip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_tokens_to_ids"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer_label", ",", "tokenizer", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "entity2id", "=", "{", "}", "\n", "with", "open", "(", "\"kg_embed/entity2id.txt\"", ")", "as", "fin", ":", "\n", "        ", "fin", ".", "readline", "(", ")", "\n", "for", "line", "in", "fin", ":", "\n", "            ", "qid", ",", "eid", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "entity2id", "[", "qid", "]", "=", "int", "(", "eid", ")", "\n", "\n", "", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "ex_text_a", "=", "example", ".", "text_a", "[", "0", "]", "\n", "h", "=", "example", ".", "text_a", "[", "1", "]", "[", "0", "]", "\n", "ex_text_a", "=", "ex_text_a", "[", ":", "h", "[", "1", "]", "]", "+", "\"\u3002 \"", "+", "ex_text_a", "[", "h", "[", "1", "]", ":", "h", "[", "2", "]", "]", "+", "\" \u3002\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "]", "\n", "begin", ",", "end", "=", "h", "[", "1", ":", "3", "]", "\n", "h", "[", "1", "]", "+=", "2", "\n", "h", "[", "2", "]", "+=", "2", "\n", "tokens_a", ",", "entities_a", "=", "tokenizer_label", ".", "tokenize", "(", "ex_text_a", ",", "[", "h", "]", ")", "\n", "# change begin pos", "\n", "ent_pos", "=", "[", "x", "for", "x", "in", "example", ".", "text_b", "if", "x", "[", "-", "1", "]", ">", "threshold", "]", "\n", "for", "x", "in", "ent_pos", ":", "\n", "            ", "if", "x", "[", "1", "]", ">", "end", ":", "\n", "                ", "x", "[", "1", "]", "+=", "4", "\n", "", "elif", "x", "[", "1", "]", ">=", "begin", ":", "\n", "                ", "x", "[", "1", "]", "+=", "2", "\n", "", "", "_", ",", "entities", "=", "tokenizer", ".", "tokenize", "(", "ex_text_a", ",", "ent_pos", ")", "\n", "if", "h", "[", "1", "]", "==", "h", "[", "2", "]", ":", "\n", "            ", "continue", "\n", "", "mark", "=", "False", "\n", "tokens_b", "=", "None", "\n", "for", "e", "in", "entities_a", ":", "\n", "            ", "if", "e", "!=", "\"UNK\"", ":", "\n", "                ", "mark", "=", "True", "\n", "", "", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "            ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities_a", "=", "entities_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities", "=", "entities", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "=", "[", "\"UNK\"", "]", "+", "entities_a", "+", "[", "\"UNK\"", "]", "\n", "real_ents", "=", "[", "\"UNK\"", "]", "+", "entities", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "span_mask", "=", "[", "]", "\n", "for", "ent", "in", "ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", ":", "\n", "                ", "span_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "span_mask", ".", "append", "(", "0", ")", "\n", "\n", "", "", "input_ent", "=", "[", "]", "\n", "ent_mask", "=", "[", "]", "\n", "for", "ent", "in", "real_ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", "and", "ent", "in", "entity2id", ":", "\n", "                ", "input_ent", ".", "append", "(", "entity2id", "[", "ent", "]", ")", "\n", "ent_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "input_ent", ".", "append", "(", "-", "1", ")", "\n", "ent_mask", ".", "append", "(", "0", ")", "\n", "", "", "ent_mask", "[", "0", "]", "=", "1", "\n", "\n", "if", "not", "mark", ":", "\n", "            ", "print", "(", "example", ".", "guid", ")", "\n", "print", "(", "example", ".", "text_a", "[", "0", "]", ")", "\n", "print", "(", "example", ".", "text_a", "[", "0", "]", "[", "example", ".", "text_a", "[", "1", "]", "[", "0", "]", "[", "1", "]", ":", "example", ".", "text_a", "[", "1", "]", "[", "0", "]", "[", "2", "]", "]", ")", "\n", "print", "(", "ents", ")", "\n", "exit", "(", "1", ")", "\n", "", "if", "sum", "(", "span_mask", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "padding_", "=", "[", "-", "1", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "ent_mask", "+=", "padding", "\n", "input_ent", "+=", "padding_", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "ent_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_ent", ")", "==", "max_seq_length", "\n", "\n", "labels", "=", "[", "0", "]", "*", "len", "(", "label_map", ")", "\n", "for", "l", "in", "example", ".", "label", ":", "\n", "            ", "l", "=", "label_map", "[", "l", "]", "\n", "labels", "[", "l", "]", "=", "1", "\n", "", "if", "ex_index", "<", "10", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"Entity: %s\"", "%", "example", ".", "text_a", "[", "1", "]", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "zip", "(", "tokens", ",", "ents", ")", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s %s\"", "%", "(", "example", ".", "label", ",", "labels", ")", ")", "\n", "logger", ".", "info", "(", "real_ents", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "input_ent", "=", "input_ent", ",", "\n", "ent_mask", "=", "ent_mask", ",", "\n", "labels", "=", "labels", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing._truncate_seq_pair": [[265, 282], ["len", "len", "len", "len", "tokens_a.pop", "ents_a.pop", "tokens_b.pop", "ents_b.pop"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "ents_a", ",", "ents_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "ents_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "ents_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.accuracy": [[283, 301], ["zip", "max", "range", "y1.append", "y2.append", "len", "set", "set", "yy1.append", "yy2.append"], "function", ["None"], ["", "", "", "def", "accuracy", "(", "out", ",", "l", ")", ":", "\n", "    ", "cnt", "=", "0", "\n", "y1", "=", "[", "]", "\n", "y2", "=", "[", "]", "\n", "for", "x1", ",", "x2", "in", "zip", "(", "out", ",", "l", ")", ":", "\n", "        ", "yy1", "=", "[", "]", "\n", "yy2", "=", "[", "]", "\n", "top", "=", "max", "(", "x1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "x1", ")", ")", ":", "\n", "#if x1[i] > 0 or x1[i] == top:", "\n", "            ", "if", "x1", "[", "i", "]", ">", "0", ":", "\n", "                ", "yy1", ".", "append", "(", "i", ")", "\n", "", "if", "x2", "[", "i", "]", ">", "0", ":", "\n", "                ", "yy2", ".", "append", "(", "i", ")", "\n", "", "", "y1", ".", "append", "(", "yy1", ")", "\n", "y2", ".", "append", "(", "yy2", ")", "\n", "cnt", "+=", "set", "(", "yy1", ")", "==", "set", "(", "yy2", ")", "\n", "", "return", "cnt", ",", "y1", ",", "y2", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.warmup_linear": [[302, 306], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_typing.main": [[307, 571], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "os.makedirs", "run_typing.TypingProcessor", "knowledge_bert.typing.BertTokenizer.from_pretrained", "knowledge_bert.typing.BertTokenizer.from_pretrained", "run_typing.TypingProcessor.get_train_examples", "sorted", "int", "knowledge_bert.modeling.BertForEntityTyping.from_pretrained", "torch.nn.DataParallel.to", "list", "vecs.append", "torch.FloatTensor", "torch.nn.Embedding.from_pretrained", "logger.info", "exit", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "os.path.exists", "os.listdir", "ValueError", "S.append", "torch.nn.DataParallel.half", "DDP", "torch.nn.DataParallel.named_parameters", "FusedAdam", "knowledge_bert.optimization.BertAdam", "open", "run_typing.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "os.path.join", "open", "torch.nn.DataParallel.train", "tqdm.trange", "bool", "len", "torch.nn.DataParallel", "torch.distributed.get_world_size", "FP16_Optimizer", "FP16_Optimizer", "line.strip().split", "vecs.append", "str", "len", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "int", "enumerate", "os.path.join", "torch.save", "s.append", "s.append", "ImportError", "any", "ImportError", "float", "torch.nn.Embedding.from_pretrained.weight.size", "tqdm.tqdm", "tuple", "torch.nn.Embedding.from_pretrained.to", "torch.nn.DataParallel.", "open.write", "loss.mean.item", "input_ids.size", "hasattr", "model_to_save.state_dict", "torch.cuda.is_available", "len", "any", "line.strip", "embed().to.half", "labels.half", "loss.mean.mean", "FP16_Optimizer.backward", "loss.mean.backward", "FP16_Optimizer.step", "FP16_Optimizer.zero_grad", "any", "torch.nn.Embedding.from_pretrained.", "run_typing.warmup_linear", "os.path.join", "torch.save", "t.to", "enumerate", "loss.mean.item", "hasattr", "model_to_save.state_dict"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_train_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.BertAdam.step", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.warmup_linear", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.state_dict"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ernie_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Ernie pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold'", ",", "type", "=", "float", ",", "default", "=", ".3", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "and", "args", ".", "do_train", ":", "\n", "        ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "processor", "=", "TypingProcessor", "(", ")", "\n", "\n", "tokenizer_label", "=", "BertTokenizer_label", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "train_examples", ",", "label_list", ",", "d", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "label_list", "=", "sorted", "(", "label_list", ")", "\n", "#class_weight = [min(d[x], 100) for x in label_list]", "\n", "#logger.info(class_weight)", "\n", "S", "=", "[", "]", "\n", "for", "l", "in", "label_list", ":", "\n", "        ", "s", "=", "[", "]", "\n", "for", "ll", "in", "label_list", ":", "\n", "            ", "if", "ll", "in", "l", ":", "\n", "                ", "s", ".", "append", "(", "1.", ")", "\n", "", "else", ":", "\n", "                ", "s", ".", "append", "(", "0.", ")", "\n", "", "", "S", ".", "append", "(", "s", ")", "\n", "", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", ")", "\n", "\n", "# Prepare model", "\n", "model", ",", "_", "=", "BertForEntityTyping", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "\n", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "/", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ",", "\n", "num_labels", "=", "len", "(", "label_list", ")", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_grad", "=", "[", "'bert.encoder.layer.11.output.dense_ent'", ",", "'bert.encoder.layer.11.output.LayerNorm_ent'", "]", "\n", "param_optimizer", "=", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_grad", ")", "]", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "t_total", "=", "num_train_steps", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "t_total", "=", "t_total", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "t_total", ")", "\n", "\n", "\n", "", "global_step", "=", "0", "\n", "\n", "vecs", "=", "[", "]", "\n", "vecs", ".", "append", "(", "[", "0", "]", "*", "100", ")", "\n", "with", "open", "(", "\"kg_embed/entity2vec.vec\"", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "vec", "=", "[", "float", "(", "x", ")", "for", "x", "in", "vec", "]", "\n", "vecs", ".", "append", "(", "vec", ")", "\n", "", "", "embed", "=", "torch", ".", "FloatTensor", "(", "vecs", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embed", ")", "\n", "logger", ".", "info", "(", "\"Shape of entity embedding: \"", "+", "str", "(", "embed", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "del", "vecs", "\n", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_features", "=", "convert_examples_to_features", "(", "\n", "train_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer_label", ",", "tokenizer", ",", "args", ".", "threshold", ")", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_ent", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ent", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ent_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "labels", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_input_ent", ",", "all_ent_mask", ",", "all_labels", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "output_loss_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"loss\"", ")", "\n", "loss_fout", "=", "open", "(", "output_loss_file", ",", "'w'", ")", "\n", "model", ".", "train", "(", ")", "\n", "for", "epoch", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "\n", "nb_tr_examples", ",", "nb_tr_steps", "=", "0", ",", "0", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", ")", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "if", "i", "!=", "3", "else", "t", "for", "i", ",", "t", "in", "enumerate", "(", "batch", ")", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "labels", "=", "batch", "\n", "input_ent", "=", "embed", "(", "input_ent", "+", "1", ")", ".", "to", "(", "device", ")", "\n", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ".", "half", "(", ")", ",", "ent_mask", ",", "labels", ".", "half", "(", ")", ")", "\n", "#loss = model(input_ids, segment_ids, input_mask, input_ent, ent_mask, labels)", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "loss_fout", ".", "write", "(", "\"{}\\n\"", ".", "format", "(", "loss", ".", "item", "(", ")", "*", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "                    ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                        ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "if", "global_step", "%", "150", "==", "0", "and", "global_step", ">", "0", ":", "\n", "                        ", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"pytorch_model.bin_{}\"", ".", "format", "(", "global_step", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "", "", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"pytorch_model.bin_{}\"", ".", "format", "(", "epoch", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "", "", "exit", "(", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.CountingIterator.__init__": [[27, 31], ["iter"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ")", ":", "\n", "        ", "self", ".", "iterable", "=", "iterable", "\n", "self", ".", "count", "=", "0", "\n", "self", ".", "itr", "=", "iter", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.CountingIterator.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "iterable", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.CountingIterator.__iter__": [[35, 39], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "for", "x", "in", "self", ".", "iterable", ":", "\n", "            ", "self", ".", "count", "+=", "1", "\n", "yield", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.CountingIterator.__next__": [[40, 42], ["next"], "methods", ["None"], ["", "", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "itr", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.CountingIterator.has_next": [[43, 46], ["len"], "methods", ["None"], ["", "def", "has_next", "(", "self", ")", ":", "\n", "        ", "\"\"\"Whether the iterator has been exhausted.\"\"\"", "\n", "return", "self", ".", "count", "<", "len", "(", "self", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.CountingIterator.skip": [[47, 51], ["next", "itertools.islice"], "methods", ["None"], ["", "def", "skip", "(", "self", ",", "num_to_skip", ")", ":", "\n", "        ", "\"\"\"Fast-forward the iterator by skipping *num_to_skip* elements.\"\"\"", "\n", "next", "(", "itertools", ".", "islice", "(", "self", ".", "itr", ",", "num_to_skip", ",", "num_to_skip", ")", ",", "None", ")", "\n", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.__init__": [[77, 91], ["isinstance", "tuple", "hasattr"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "dataset", ",", "collate_fn", ",", "batch_sampler", ",", "seed", "=", "1", ",", "num_shards", "=", "1", ",", "shard_id", "=", "0", ")", ":", "\n", "        ", "assert", "isinstance", "(", "dataset", ",", "torch", ".", "utils", ".", "data", ".", "Dataset", ")", "\n", "self", ".", "dataset", "=", "dataset", "\n", "self", ".", "collate_fn", "=", "collate_fn", "\n", "self", ".", "frozen_batches", "=", "tuple", "(", "batch_sampler", ")", "\n", "self", ".", "seed", "=", "seed", "\n", "self", ".", "num_shards", "=", "num_shards", "\n", "self", ".", "shard_id", "=", "shard_id", "\n", "\n", "self", ".", "epoch", "=", "0", "\n", "self", ".", "_cur_epoch_itr", "=", "None", "\n", "self", ".", "_next_epoch_itr", "=", "None", "\n", "self", ".", "_supports_prefetch", "=", "(", "\n", "hasattr", "(", "dataset", ",", "'supports_prefetch'", ")", "and", "dataset", ".", "supports_prefetch", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.__len__": [[93, 95], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "frozen_batches", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.next_epoch_itr": [[96, 115], ["iterators.EpochBatchIterator._get_iterator_for_epoch"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator._get_iterator_for_epoch"], ["", "def", "next_epoch_itr", "(", "self", ",", "shuffle", "=", "True", ",", "fix_batches_to_gpus", "=", "False", ")", ":", "\n", "        ", "\"\"\"Return a new iterator over the dataset.\n\n        Args:\n            shuffle (bool, optional): shuffle batches before returning the\n                iterator. Default: ``True``\n            fix_batches_to_gpus: ensure that batches are always\n                allocated to the same shards across epochs. Requires\n                that :attr:`dataset` supports prefetching. Default:\n                ``False``\n        \"\"\"", "\n", "if", "self", ".", "_next_epoch_itr", "is", "not", "None", ":", "\n", "            ", "self", ".", "_cur_epoch_itr", "=", "self", ".", "_next_epoch_itr", "\n", "self", ".", "_next_epoch_itr", "=", "None", "\n", "", "else", ":", "\n", "            ", "self", ".", "epoch", "+=", "1", "\n", "self", ".", "_cur_epoch_itr", "=", "self", ".", "_get_iterator_for_epoch", "(", "\n", "self", ".", "epoch", ",", "shuffle", ",", "fix_batches_to_gpus", "=", "fix_batches_to_gpus", ")", "\n", "", "return", "self", ".", "_cur_epoch_itr", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.end_of_epoch": [[116, 119], ["iterators.EpochBatchIterator._cur_epoch_itr.has_next"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.CountingIterator.has_next"], ["", "def", "end_of_epoch", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns whether the most recent epoch iterator has been exhausted\"\"\"", "\n", "return", "not", "self", ".", "_cur_epoch_itr", ".", "has_next", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.iterations_in_epoch": [[120, 128], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "iterations_in_epoch", "(", "self", ")", ":", "\n", "        ", "\"\"\"The number of consumed batches in the current epoch.\"\"\"", "\n", "if", "self", ".", "_cur_epoch_itr", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_cur_epoch_itr", ".", "count", "\n", "", "elif", "self", ".", "_next_epoch_itr", "is", "not", "None", ":", "\n", "            ", "return", "self", ".", "_next_epoch_itr", ".", "count", "\n", "", "return", "0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.state_dict": [[129, 134], ["None"], "methods", ["None"], ["", "def", "state_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Returns a dictionary containing a whole state of the iterator.\"\"\"", "\n", "return", "{", "\n", "'epoch'", ":", "self", ".", "epoch", ",", "\n", "'iterations_in_epoch'", ":", "self", ".", "iterations_in_epoch", ",", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.load_state_dict": [[136, 145], ["state_dict.get", "iterators.EpochBatchIterator._get_iterator_for_epoch", "state_dict.get", "len", "iterators.EpochBatchIterator.skip"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator._get_iterator_for_epoch", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.CountingIterator.skip"], ["", "def", "load_state_dict", "(", "self", ",", "state_dict", ")", ":", "\n", "        ", "\"\"\"Copies the state of the iterator from the given *state_dict*.\"\"\"", "\n", "self", ".", "epoch", "=", "state_dict", "[", "'epoch'", "]", "\n", "itr_pos", "=", "state_dict", ".", "get", "(", "'iterations_in_epoch'", ",", "0", ")", "\n", "if", "itr_pos", ">", "0", ":", "\n", "# fast-forward epoch iterator", "\n", "            ", "itr", "=", "self", ".", "_get_iterator_for_epoch", "(", "self", ".", "epoch", ",", "state_dict", ".", "get", "(", "'shuffle'", ",", "True", ")", ")", "\n", "if", "itr_pos", "<", "len", "(", "itr", ")", ":", "\n", "                ", "self", ".", "_next_epoch_itr", "=", "itr", ".", "skip", "(", "itr_pos", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator._get_iterator_for_epoch": [[146, 180], ["iterators.CountingIterator", "list", "iterators.EpochBatchIterator.dataset.prefetch", "iterators.ShardedIterator", "torch.utils.data.DataLoader", "data_utils.numpy_seed", "numpy.random.shuffle", "iterators.EpochBatchIterator._get_iterator_for_epoch.shuffle_batches"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedCachedDataset.prefetch", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.data_utils.numpy_seed"], ["", "", "", "def", "_get_iterator_for_epoch", "(", "self", ",", "epoch", ",", "shuffle", ",", "fix_batches_to_gpus", "=", "False", ")", ":", "\n", "\n", "        ", "def", "shuffle_batches", "(", "batches", ",", "seed", ")", ":", "\n", "# set seed based on the seed and epoch number so that we get", "\n", "# reproducible results when resuming from checkpoints", "\n", "            ", "with", "data_utils", ".", "numpy_seed", "(", "seed", ")", ":", "\n", "                ", "np", ".", "random", ".", "shuffle", "(", "batches", ")", "\n", "", "return", "batches", "\n", "\n", "", "if", "self", ".", "_supports_prefetch", ":", "\n", "            ", "batches", "=", "self", ".", "frozen_batches", "\n", "\n", "if", "shuffle", "and", "not", "fix_batches_to_gpus", ":", "\n", "                ", "batches", "=", "shuffle_batches", "(", "list", "(", "batches", ")", ",", "self", ".", "seed", "+", "epoch", ")", "\n", "\n", "", "batches", "=", "list", "(", "ShardedIterator", "(", "\n", "batches", ",", "self", ".", "num_shards", ",", "self", ".", "shard_id", ",", "fill_value", "=", "[", "]", ")", ")", "\n", "\n", "self", ".", "dataset", ".", "prefetch", "(", "[", "i", "for", "s", "in", "batches", "for", "i", "in", "s", "]", ")", "\n", "\n", "if", "shuffle", "and", "fix_batches_to_gpus", ":", "\n", "                ", "batches", "=", "shuffle_batches", "(", "batches", ",", "self", ".", "seed", "+", "epoch", "+", "self", ".", "shard_id", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "if", "shuffle", ":", "\n", "                ", "batches", "=", "shuffle_batches", "(", "list", "(", "self", ".", "frozen_batches", ")", ",", "self", ".", "seed", "+", "epoch", ")", "\n", "", "else", ":", "\n", "                ", "batches", "=", "self", ".", "frozen_batches", "\n", "", "batches", "=", "ShardedIterator", "(", "batches", ",", "self", ".", "num_shards", ",", "self", ".", "shard_id", ",", "fill_value", "=", "[", "]", ")", "\n", "\n", "", "return", "CountingIterator", "(", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "\n", "self", ".", "dataset", ",", "\n", "collate_fn", "=", "self", ".", "collate_fn", ",", "\n", "batch_sampler", "=", "batches", ",", "\n", ")", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.GroupedIterator.__init__": [[191, 195], ["int", "iter", "math.ceil", "len", "float"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ",", "chunk_size", ")", ":", "\n", "        ", "self", ".", "_len", "=", "int", "(", "math", ".", "ceil", "(", "len", "(", "iterable", ")", "/", "float", "(", "chunk_size", ")", ")", ")", "\n", "self", ".", "itr", "=", "iter", "(", "iterable", ")", "\n", "self", ".", "chunk_size", "=", "chunk_size", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.GroupedIterator.__len__": [[196, 198], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_len", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.GroupedIterator.__iter__": [[199, 201], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.GroupedIterator.__next__": [[202, 211], ["range", "chunk.append", "next", "len"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "chunk", "=", "[", "]", "\n", "try", ":", "\n", "            ", "for", "_", "in", "range", "(", "self", ".", "chunk_size", ")", ":", "\n", "                ", "chunk", ".", "append", "(", "next", "(", "self", ".", "itr", ")", ")", "\n", "", "", "except", "StopIteration", "as", "e", ":", "\n", "            ", "if", "len", "(", "chunk", ")", "==", "0", ":", "\n", "                ", "raise", "e", "\n", "", "", "return", "chunk", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.ShardedIterator.__init__": [[224, 236], ["itertools.zip_longest", "ValueError", "len", "range", "itertools.islice", "len", "len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "iterable", ",", "num_shards", ",", "shard_id", ",", "fill_value", "=", "None", ")", ":", "\n", "        ", "if", "shard_id", "<", "0", "or", "shard_id", ">=", "num_shards", ":", "\n", "            ", "raise", "ValueError", "(", "'shard_id must be between 0 and num_shards'", ")", "\n", "\n", "", "self", ".", "_sharded_len", "=", "len", "(", "iterable", ")", "//", "num_shards", "\n", "if", "len", "(", "iterable", ")", "%", "num_shards", ">", "0", ":", "\n", "            ", "self", ".", "_sharded_len", "+=", "1", "\n", "\n", "", "self", ".", "itr", "=", "itertools", ".", "zip_longest", "(", "\n", "range", "(", "self", ".", "_sharded_len", ")", ",", "\n", "itertools", ".", "islice", "(", "iterable", ",", "shard_id", ",", "len", "(", "iterable", ")", ",", "num_shards", ")", ",", "\n", "fillvalue", "=", "fill_value", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.ShardedIterator.__len__": [[238, 240], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "_sharded_len", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.ShardedIterator.__iter__": [[241, 243], ["None"], "methods", ["None"], ["", "def", "__iter__", "(", "self", ")", ":", "\n", "        ", "return", "self", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.ShardedIterator.__next__": [[244, 246], ["next"], "methods", ["None"], ["", "def", "__next__", "(", "self", ")", ":", "\n", "        ", "return", "next", "(", "self", ".", "itr", ")", "[", "1", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.data_utils.infer_language_pair": [[14, 22], ["os.listdir", "filename.split", "parts[].split", "len", "len", "parts[].split"], "function", ["None"], ["def", "infer_language_pair", "(", "path", ")", ":", "\n", "    ", "\"\"\"Infer language pair from filename: <split>.<lang1>-<lang2>.(...).idx\"\"\"", "\n", "src", ",", "dst", "=", "None", ",", "None", "\n", "for", "filename", "in", "os", ".", "listdir", "(", "path", ")", ":", "\n", "        ", "parts", "=", "filename", ".", "split", "(", "'.'", ")", "\n", "if", "len", "(", "parts", ")", ">=", "3", "and", "len", "(", "parts", "[", "1", "]", ".", "split", "(", "'-'", ")", ")", "==", "2", ":", "\n", "            ", "return", "parts", "[", "1", "]", ".", "split", "(", "'-'", ")", "\n", "", "", "return", "src", ",", "dst", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.data_utils.collate_tokens": [[24, 41], ["max", "values[].new().fill_", "enumerate", "data_utils.collate_tokens.copy_tensor"], "function", ["None"], ["", "def", "collate_tokens", "(", "values", ",", "pad_idx", ",", "eos_idx", ",", "left_pad", ",", "move_eos_to_beginning", "=", "False", ")", ":", "\n", "    ", "\"\"\"Convert a list of 1d tensors into a padded 2d tensor.\"\"\"", "\n", "size", "=", "max", "(", "v", ".", "size", "(", "0", ")", "for", "v", "in", "values", ")", "\n", "res", "=", "values", "[", "0", "]", ".", "new", "(", "len", "(", "values", ")", ",", "size", ")", ".", "fill_", "(", "pad_idx", ")", "\n", "\n", "def", "copy_tensor", "(", "src", ",", "dst", ")", ":", "\n", "        ", "assert", "dst", ".", "numel", "(", ")", "==", "src", ".", "numel", "(", ")", "\n", "if", "move_eos_to_beginning", ":", "\n", "            ", "assert", "src", "[", "-", "1", "]", "==", "eos_idx", "\n", "dst", "[", "0", "]", "=", "eos_idx", "\n", "dst", "[", "1", ":", "]", "=", "src", "[", ":", "-", "1", "]", "\n", "", "else", ":", "\n", "            ", "dst", ".", "copy_", "(", "src", ")", "\n", "\n", "", "", "for", "i", ",", "v", "in", "enumerate", "(", "values", ")", ":", "\n", "        ", "copy_tensor", "(", "v", ",", "res", "[", "i", "]", "[", "size", "-", "len", "(", "v", ")", ":", "]", "if", "left_pad", "else", "res", "[", "i", "]", "[", ":", "len", "(", "v", ")", "]", ")", "\n", "", "return", "res", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.data_utils.numpy_seed": [[43, 56], ["numpy.random.get_state", "numpy.random.seed", "numpy.random.set_state"], "function", ["None"], ["", "@", "contextlib", ".", "contextmanager", "\n", "def", "numpy_seed", "(", "seed", ")", ":", "\n", "    ", "\"\"\"Context manager which seeds the NumPy PRNG with the specified seed and\n    restores the state afterward\"\"\"", "\n", "if", "seed", "is", "None", ":", "\n", "        ", "yield", "\n", "return", "\n", "", "state", "=", "np", ".", "random", ".", "get_state", "(", ")", "\n", "np", ".", "random", ".", "seed", "(", "seed", ")", "\n", "try", ":", "\n", "        ", "yield", "\n", "", "finally", ":", "\n", "        ", "np", ".", "random", ".", "set_state", "(", "state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.data_utils.collect_filtered": [[58, 73], ["function", "filtered.append"], "function", ["None"], ["", "", "def", "collect_filtered", "(", "function", ",", "iterable", ",", "filtered", ")", ":", "\n", "    ", "\"\"\"\n    Similar to :func:`filter` but collects filtered elements in ``filtered``.\n\n    Args:\n        function (callable): function that returns ``False`` for elements that\n            should be filtered\n        iterable (iterable): iterable to filter\n        filtered (list): list to store filtered elements\n    \"\"\"", "\n", "for", "el", "in", "iterable", ":", "\n", "        ", "if", "function", "(", "el", ")", ":", "\n", "            ", "yield", "el", "\n", "", "else", ":", "\n", "            ", "filtered", ".", "append", "(", "el", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.data_utils.filter_by_size": [[75, 117], ["data_utils.collect_filtered", "len", "print", "isinstance", "isinstance", "isinstance", "Exception", "size_fn", "size_fn", "isinstance", "all", "all", "len", "len", "set", "set", "size_fn", "max_positions.keys", "size_fn.keys", "zip", "size_fn"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.data_utils.collect_filtered"], ["", "", "", "def", "filter_by_size", "(", "indices", ",", "size_fn", ",", "max_positions", ",", "raise_exception", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Filter indices based on their size.\n\n    Args:\n        indices (List[int]): ordered list of dataset indices\n        size_fn (callable): function that returns the size of a given index\n        max_positions (tuple): filter elements larger than this size.\n            Comparisons are done component-wise.\n        raise_exception (bool, optional): if ``True``, raise an exception\n            if any elements are filtered. Default: ``False``\n    \"\"\"", "\n", "def", "check_size", "(", "idx", ")", ":", "\n", "        ", "if", "isinstance", "(", "max_positions", ",", "float", ")", "or", "isinstance", "(", "max_positions", ",", "int", ")", ":", "\n", "            ", "return", "size_fn", "(", "idx", ")", "<=", "max_positions", "\n", "", "elif", "isinstance", "(", "max_positions", ",", "dict", ")", ":", "\n", "            ", "idx_size", "=", "size_fn", "(", "idx", ")", "\n", "assert", "isinstance", "(", "idx_size", ",", "dict", ")", "\n", "intersect_keys", "=", "set", "(", "max_positions", ".", "keys", "(", ")", ")", "&", "set", "(", "idx_size", ".", "keys", "(", ")", ")", "\n", "return", "all", "(", "\n", "idx_size", "[", "key", "]", "<=", "max_positions", "[", "key", "]", "for", "key", "in", "intersect_keys", "\n", ")", "\n", "", "else", ":", "\n", "            ", "return", "all", "(", "a", "is", "None", "or", "b", "is", "None", "or", "a", "<=", "b", "\n", "for", "a", ",", "b", "in", "zip", "(", "size_fn", "(", "idx", ")", ",", "max_positions", ")", ")", "\n", "\n", "", "", "ignored", "=", "[", "]", "\n", "itr", "=", "collect_filtered", "(", "check_size", ",", "indices", ",", "ignored", ")", "\n", "\n", "for", "idx", "in", "itr", ":", "\n", "        ", "if", "len", "(", "ignored", ")", ">", "0", "and", "raise_exception", ":", "\n", "            ", "raise", "Exception", "(", "(", "\n", "'Size of sample #{} is invalid (={}) since max_positions={}, '", "\n", "'skip this example with --skip-invalid-size-inputs-valid-test'", "\n", ")", ".", "format", "(", "ignored", "[", "0", "]", ",", "size_fn", "(", "ignored", "[", "0", "]", ")", ",", "max_positions", ")", ")", "\n", "", "yield", "idx", "\n", "\n", "", "if", "len", "(", "ignored", ")", ">", "0", ":", "\n", "        ", "print", "(", "(", "\n", "'| WARNING: {} samples have invalid sizes and will be skipped, '", "\n", "'max_positions={}, first few sample ids={}'", "\n", ")", ".", "format", "(", "len", "(", "ignored", ")", ",", "max_positions", ",", "ignored", "[", ":", "10", "]", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.data_utils.batch_by_size": [[119, 173], ["float", "float", "sample_lens.append", "max", "data_utils.batch_by_size.is_batch_full"], "function", ["None"], ["", "", "def", "batch_by_size", "(", "\n", "indices", ",", "num_tokens_fn", ",", "max_tokens", "=", "None", ",", "max_sentences", "=", "None", ",", "\n", "required_batch_size_multiple", "=", "1", ",", "\n", ")", ":", "\n", "    ", "\"\"\"\n    Yield mini-batches of indices bucketed by size. Batches may contain\n    sequences of different lengths.\n\n    Args:\n        indices (List[int]): ordered list of dataset indices\n        num_tokens_fn (callable): function that returns the number of tokens at\n            a given index\n        max_tokens (int, optional): max number of tokens in each batch.\n            Default: ``None``\n        max_sentences (int, optional): max number of sentences in each\n            batch. Default: ``None``\n        required_batch_size_multiple (int, optional): require batch size to\n            be a multiple of N. Default: ``1``\n    \"\"\"", "\n", "max_tokens", "=", "max_tokens", "if", "max_tokens", "is", "not", "None", "else", "float", "(", "'Inf'", ")", "\n", "max_sentences", "=", "max_sentences", "if", "max_sentences", "is", "not", "None", "else", "float", "(", "'Inf'", ")", "\n", "bsz_mult", "=", "required_batch_size_multiple", "\n", "\n", "batch", "=", "[", "]", "\n", "\n", "def", "is_batch_full", "(", "num_tokens", ")", ":", "\n", "        ", "if", "len", "(", "batch", ")", "==", "0", ":", "\n", "            ", "return", "False", "\n", "", "if", "len", "(", "batch", ")", "==", "max_sentences", ":", "\n", "            ", "return", "True", "\n", "", "if", "num_tokens", ">", "max_tokens", ":", "\n", "            ", "return", "True", "\n", "", "return", "False", "\n", "\n", "", "sample_len", "=", "0", "\n", "sample_lens", "=", "[", "]", "\n", "for", "idx", "in", "indices", ":", "\n", "        ", "sample_lens", ".", "append", "(", "num_tokens_fn", "(", "idx", ")", ")", "\n", "sample_len", "=", "max", "(", "sample_len", ",", "sample_lens", "[", "-", "1", "]", ")", "\n", "num_tokens", "=", "(", "len", "(", "batch", ")", "+", "1", ")", "*", "sample_len", "\n", "if", "is_batch_full", "(", "num_tokens", ")", ":", "\n", "            ", "mod_len", "=", "max", "(", "\n", "bsz_mult", "*", "(", "len", "(", "batch", ")", "//", "bsz_mult", ")", ",", "\n", "len", "(", "batch", ")", "%", "bsz_mult", ",", "\n", ")", "\n", "yield", "batch", "[", ":", "mod_len", "]", "\n", "batch", "=", "batch", "[", "mod_len", ":", "]", "\n", "sample_lens", "=", "sample_lens", "[", "mod_len", ":", "]", "\n", "sample_len", "=", "max", "(", "sample_lens", ")", "if", "len", "(", "sample_lens", ")", ">", "0", "else", "0", "\n", "\n", "", "batch", ".", "append", "(", "idx", ")", "\n", "\n", "", "if", "len", "(", "batch", ")", ">", "0", ":", "\n", "        ", "yield", "batch", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.example.get_ents": [[29, 37], ["ann.get_annotations", "ents.append"], "function", ["None"], ["", "", "def", "get_ents", "(", "ann", ")", ":", "\n", "    ", "ents", "=", "[", "]", "\n", "# Keep annotations with a score higher than 0.3", "\n", "for", "a", "in", "ann", ".", "get_annotations", "(", "0.3", ")", ":", "\n", "        ", "if", "a", ".", "entity_title", "not", "in", "ent_map", ":", "\n", "            ", "continue", "\n", "", "ents", ".", "append", "(", "[", "ent_map", "[", "a", ".", "entity_title", "]", ",", "a", ".", "begin", ",", "a", ".", "end", ",", "a", ".", "score", "]", ")", "\n", "", "return", "ents", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.__init__": [[54, 61], ["super().__init__", "indexed_dataset.IndexedDataset.read_index", "indexed_dataset.IndexedDataset.read_data"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.read_index", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.read_data"], ["def", "__init__", "(", "self", ",", "path", ",", "fix_lua_indexing", "=", "False", ",", "read_data", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "fix_lua_indexing", "=", "fix_lua_indexing", "\n", "self", ".", "read_index", "(", "path", ")", "\n", "self", ".", "data_file", "=", "None", "\n", "if", "read_data", ":", "\n", "            ", "self", ".", "read_data", "(", "path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.read_index": [[62, 74], ["open", "f.read", "f.read", "struct.unpack", "struct.unpack", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.read_longs", "indexed_dataset.index_file_path", "struct.unpack", "f.read", "f.read"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.read_longs", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.index_file_path"], ["", "", "def", "read_index", "(", "self", ",", "path", ")", ":", "\n", "        ", "with", "open", "(", "index_file_path", "(", "path", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "magic", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "magic", "==", "b'TNTIDX\\x00\\x00'", "\n", "version", "=", "f", ".", "read", "(", "8", ")", "\n", "assert", "struct", ".", "unpack", "(", "'<Q'", ",", "version", ")", "==", "(", "1", ",", ")", "\n", "code", ",", "self", ".", "element_size", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "dtype", "=", "dtypes", "[", "code", "]", "\n", "self", ".", "size", ",", "self", ".", "s", "=", "struct", ".", "unpack", "(", "'<QQ'", ",", "f", ".", "read", "(", "16", ")", ")", "\n", "self", ".", "dim_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "size", "+", "1", ")", "\n", "self", ".", "data_offsets", "=", "read_longs", "(", "f", ",", "self", ".", "size", "+", "1", ")", "\n", "self", ".", "sizes", "=", "read_longs", "(", "f", ",", "self", ".", "s", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.read_data": [[75, 77], ["open", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.data_file_path"], ["", "", "def", "read_data", "(", "self", ",", "path", ")", ":", "\n", "        ", "self", ".", "data_file", "=", "open", "(", "data_file_path", "(", "path", ")", ",", "'rb'", ",", "buffering", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.check_index": [[78, 81], ["IndexError"], "methods", ["None"], ["", "def", "check_index", "(", "self", ",", "i", ")", ":", "\n", "        ", "if", "i", "<", "0", "or", "i", ">=", "self", ".", "size", ":", "\n", "            ", "raise", "IndexError", "(", "'index out of range'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.__del__": [[82, 85], ["indexed_dataset.IndexedDataset.data_file.close"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close"], ["", "", "def", "__del__", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "data_file", ":", "\n", "            ", "self", ".", "data_file", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.__getitem__": [[86, 97], ["indexed_dataset.IndexedDataset.check_index", "numpy.empty", "indexed_dataset.IndexedDataset.data_file.seek", "indexed_dataset.IndexedDataset.data_file.readinto"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.check_index"], ["", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "i", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n", "# item = torch.from_numpy(a).long()", "\n", "item", "=", "a", "\n", "if", "self", ".", "fix_lua_indexing", ":", "\n", "            ", "item", "-=", "1", "# subtract 1 for 0-based indexing", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.__len__": [[98, 100], ["None"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "self", ".", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists": [[101, 106], ["os.path.exists", "os.path.exists", "indexed_dataset.index_file_path", "indexed_dataset.data_file_path"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.index_file_path", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.data_file_path"], ["", "@", "staticmethod", "\n", "def", "exists", "(", "path", ")", ":", "\n", "        ", "return", "(", "\n", "os", ".", "path", ".", "exists", "(", "index_file_path", "(", "path", ")", ")", "and", "\n", "os", ".", "path", ".", "exists", "(", "data_file_path", "(", "path", ")", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedCachedDataset.__init__": [[111, 115], ["indexed_dataset.IndexedDataset.__init__"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "path", ",", "fix_lua_indexing", "=", "False", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "path", ",", "fix_lua_indexing", ",", "True", ")", "\n", "self", ".", "cache", "=", "None", "\n", "self", ".", "cache_index", "=", "{", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedCachedDataset.supports_prefetch": [[116, 119], ["None"], "methods", ["None"], ["", "@", "property", "\n", "def", "supports_prefetch", "(", "self", ")", ":", "\n", "        ", "return", "True", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedCachedDataset.prefetch": [[120, 137], ["all", "sorted", "numpy.empty", "indexed_dataset.IndexedCachedDataset.cache_index.clear", "set", "indexed_dataset.IndexedCachedDataset.data_file.seek", "indexed_dataset.IndexedCachedDataset.data_file.readinto"], "methods", ["None"], ["", "def", "prefetch", "(", "self", ",", "indices", ")", ":", "\n", "        ", "if", "all", "(", "i", "in", "self", ".", "cache_index", "for", "i", "in", "indices", ")", ":", "\n", "            ", "return", "\n", "", "indices", "=", "sorted", "(", "set", "(", "indices", ")", ")", "\n", "total_size", "=", "0", "\n", "for", "i", "in", "indices", ":", "\n", "            ", "total_size", "+=", "self", ".", "data_offsets", "[", "i", "+", "1", "]", "-", "self", ".", "data_offsets", "[", "i", "]", "\n", "", "self", ".", "cache", "=", "np", ".", "empty", "(", "total_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "ptx", "=", "0", "\n", "self", ".", "cache_index", ".", "clear", "(", ")", "\n", "for", "i", "in", "indices", ":", "\n", "            ", "self", ".", "cache_index", "[", "i", "]", "=", "ptx", "\n", "size", "=", "self", ".", "data_offsets", "[", "i", "+", "1", "]", "-", "self", ".", "data_offsets", "[", "i", "]", "\n", "a", "=", "self", ".", "cache", "[", "ptx", ":", "ptx", "+", "size", "]", "\n", "self", ".", "data_file", ".", "seek", "(", "self", ".", "data_offsets", "[", "i", "]", "*", "self", ".", "element_size", ")", "\n", "self", ".", "data_file", ".", "readinto", "(", "a", ")", "\n", "ptx", "+=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedCachedDataset.__getitem__": [[138, 148], ["indexed_dataset.IndexedCachedDataset.check_index", "numpy.empty", "numpy.copyto", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy().long", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.check_index"], ["", "", "def", "__getitem__", "(", "self", ",", "i", ")", ":", "\n", "        ", "self", ".", "check_index", "(", "i", ")", "\n", "tensor_size", "=", "self", ".", "sizes", "[", "self", ".", "dim_offsets", "[", "i", "]", ":", "self", ".", "dim_offsets", "[", "i", "+", "1", "]", "]", "\n", "a", "=", "np", ".", "empty", "(", "tensor_size", ",", "dtype", "=", "self", ".", "dtype", ")", "\n", "ptx", "=", "self", ".", "cache_index", "[", "i", "]", "\n", "np", ".", "copyto", "(", "a", ",", "self", ".", "cache", "[", "ptx", ":", "ptx", "+", "a", ".", "size", "]", ")", "\n", "item", "=", "torch", ".", "from_numpy", "(", "a", ")", ".", "long", "(", ")", "\n", "if", "self", ".", "fix_lua_indexing", ":", "\n", "            ", "item", "-=", "1", "# subtract 1 for 0-based indexing", "\n", "", "return", "item", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDatasetBuilder.__init__": [[160, 167], ["open"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["def", "__init__", "(", "self", ",", "out_file", ",", "dtype", "=", "np", ".", "int32", ")", ":", "\n", "        ", "self", ".", "out_file", "=", "open", "(", "out_file", ",", "'wb'", ")", "\n", "self", ".", "dtype", "=", "dtype", "\n", "self", ".", "data_offsets", "=", "[", "0", "]", "\n", "self", ".", "dim_offsets", "=", "[", "0", "]", "\n", "self", ".", "sizes", "=", "[", "]", "\n", "self", ".", "element_size", "=", "self", ".", "element_sizes", "[", "self", ".", "dtype", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDatasetBuilder.add_item": [[168, 175], ["indexed_dataset.IndexedDatasetBuilder.out_file.write", "indexed_dataset.IndexedDatasetBuilder.data_offsets.append", "tensor.size", "indexed_dataset.IndexedDatasetBuilder.dim_offsets.append", "numpy.array", "indexed_dataset.IndexedDatasetBuilder.sizes.append", "len", "tensor.numpy", "tensor.size"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "def", "add_item", "(", "self", ",", "tensor", ")", ":", "\n", "# +1 for Lua compatibility", "\n", "        ", "bytes", "=", "self", ".", "out_file", ".", "write", "(", "np", ".", "array", "(", "tensor", ".", "numpy", "(", ")", "+", "1", ",", "dtype", "=", "self", ".", "dtype", ")", ")", "\n", "self", ".", "data_offsets", ".", "append", "(", "self", ".", "data_offsets", "[", "-", "1", "]", "+", "bytes", "/", "self", ".", "element_size", ")", "\n", "for", "s", "in", "tensor", ".", "size", "(", ")", ":", "\n", "            ", "self", ".", "sizes", ".", "append", "(", "s", ")", "\n", "", "self", ".", "dim_offsets", ".", "append", "(", "self", ".", "dim_offsets", "[", "-", "1", "]", "+", "len", "(", "tensor", ".", "size", "(", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDatasetBuilder.merge_file_": [[176, 195], ["indexed_dataset.IndexedDataset", "indexed_dataset.IndexedDatasetBuilder.sizes.extend", "indexed_dataset.IndexedDatasetBuilder.data_offsets.append", "indexed_dataset.IndexedDatasetBuilder.dim_offsets.append", "open", "indexed_dataset.data_file_path", "f.read", "indexed_dataset.IndexedDatasetBuilder.out_file.write"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.data_file_path", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "def", "merge_file_", "(", "self", ",", "another_file", ")", ":", "\n", "        ", "index", "=", "IndexedDataset", "(", "another_file", ",", "read_data", "=", "False", ")", "\n", "assert", "index", ".", "dtype", "==", "self", ".", "dtype", "\n", "\n", "begin", "=", "self", ".", "data_offsets", "[", "-", "1", "]", "\n", "for", "offset", "in", "index", ".", "data_offsets", "[", "1", ":", "]", ":", "\n", "            ", "self", ".", "data_offsets", ".", "append", "(", "begin", "+", "offset", ")", "\n", "", "self", ".", "sizes", ".", "extend", "(", "index", ".", "sizes", ")", "\n", "begin", "=", "self", ".", "dim_offsets", "[", "-", "1", "]", "\n", "for", "dim_offset", "in", "index", ".", "dim_offsets", "[", "1", ":", "]", ":", "\n", "            ", "self", ".", "dim_offsets", ".", "append", "(", "begin", "+", "dim_offset", ")", "\n", "\n", "", "with", "open", "(", "data_file_path", "(", "another_file", ")", ",", "'rb'", ")", "as", "f", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "data", "=", "f", ".", "read", "(", "1024", ")", "\n", "if", "data", ":", "\n", "                    ", "self", ".", "out_file", ".", "write", "(", "data", ")", "\n", "", "else", ":", "\n", "                    ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDatasetBuilder.finalize": [[196, 207], ["indexed_dataset.IndexedDatasetBuilder.out_file.close", "open", "open.write", "open.write", "open.write", "open.write", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "indexed_dataset.write_longs", "open.close", "struct.pack", "struct.pack", "struct.pack", "indexed_dataset.code", "len", "len"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.write_longs", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.code"], ["", "", "", "", "def", "finalize", "(", "self", ",", "index_file", ")", ":", "\n", "        ", "self", ".", "out_file", ".", "close", "(", ")", "\n", "index", "=", "open", "(", "index_file", ",", "'wb'", ")", "\n", "index", ".", "write", "(", "b'TNTIDX\\x00\\x00'", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<Q'", ",", "1", ")", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<QQ'", ",", "code", "(", "self", ".", "dtype", ")", ",", "self", ".", "element_size", ")", ")", "\n", "index", ".", "write", "(", "struct", ".", "pack", "(", "'<QQ'", ",", "len", "(", "self", ".", "data_offsets", ")", "-", "1", ",", "len", "(", "self", ".", "sizes", ")", ")", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "dim_offsets", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "data_offsets", ")", "\n", "write_longs", "(", "index", ",", "self", ".", "sizes", ")", "\n", "index", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.read_longs": [[16, 20], ["numpy.empty", "f.readinto"], "function", ["None"], ["def", "read_longs", "(", "f", ",", "n", ")", ":", "\n", "    ", "a", "=", "np", ".", "empty", "(", "n", ",", "dtype", "=", "np", ".", "int64", ")", "\n", "f", ".", "readinto", "(", "a", ")", "\n", "return", "a", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.write_longs": [[22, 24], ["f.write", "numpy.array"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "def", "write_longs", "(", "f", ",", "a", ")", ":", "\n", "    ", "f", ".", "write", "(", "np", ".", "array", "(", "a", ",", "dtype", "=", "np", ".", "int64", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.code": [[37, 41], ["dtypes.keys"], "function", ["None"], ["def", "code", "(", "dtype", ")", ":", "\n", "    ", "for", "k", "in", "dtypes", ".", "keys", "(", ")", ":", "\n", "        ", "if", "dtypes", "[", "k", "]", "==", "dtype", ":", "\n", "            ", "return", "k", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.index_file_path": [[43, 45], ["None"], "function", ["None"], ["", "", "", "def", "index_file_path", "(", "prefix_path", ")", ":", "\n", "    ", "return", "prefix_path", "+", "'.idx'", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.data_file_path": [[47, 49], ["None"], "function", ["None"], ["", "def", "data_file_path", "(", "prefix_path", ")", ":", "\n", "    ", "return", "prefix_path", "+", "'.bin'", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.InputExample.__init__": [[48, 64], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.InputFeatures.__init__": [[69, 76], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "label_id", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "input_ent", "=", "input_ent", "\n", "self", ".", "ent_mask", "=", "ent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.DataProcessor.get_train_examples": [[81, 84], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.DataProcessor.get_dev_examples": [[85, 88], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.DataProcessor.get_labels": [[89, 92], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.DataProcessor._read_json": [[93, 97], ["open", "simplejson.loads", "f.read"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "@", "classmethod", "\n", "def", "_read_json", "(", "cls", ",", "input_file", ")", ":", "\n", "        ", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "return", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.FewrelProcessor.get_train_examples": [[101, 107], ["eval_fewrel.FewrelProcessor._create_examples", "set", "eval_fewrel.FewrelProcessor._read_json", "list", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "examples", "=", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ",", "\"train\"", ")", "\n", "labels", "=", "set", "(", "[", "x", ".", "label", "for", "x", "in", "examples", "]", ")", "\n", "return", "examples", ",", "list", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.FewrelProcessor.get_dev_examples": [[108, 112], ["eval_fewrel.FewrelProcessor._create_examples", "eval_fewrel.FewrelProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.json\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.FewrelProcessor.get_test_examples": [[113, 117], ["eval_fewrel.FewrelProcessor._create_examples", "eval_fewrel.FewrelProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test.json\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.FewrelProcessor.get_labels": [[119, 122], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Useless\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.FewrelProcessor._create_examples": [[123, 137], ["enumerate", "examples.append", "eval_fewrel.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "for", "x", "in", "line", "[", "'ents'", "]", ":", "\n", "                ", "if", "x", "[", "1", "]", "==", "1", ":", "\n", "                    ", "x", "[", "1", "]", "=", "0", "\n", "#print(line['text'][x[1]:x[2]].encode(\"utf-8\"))", "\n", "", "", "text_a", "=", "(", "line", "[", "'text'", "]", ",", "line", "[", "'ents'", "]", ")", "\n", "label", "=", "line", "[", "'label'", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.convert_examples_to_features": [[139, 269], ["sorted", "enumerate", "open", "fin.readline", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "line.strip().split", "int", "len", "tokenizer.tokenize", "eval_fewrel._truncate_seq_pair", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "eval_fewrel.InputFeatures", "len", "input_ent.append", "ent_mask.append", "input_ent.append", "ent_mask.append", "len", "len", "line.strip", "len", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred._truncate_seq_pair"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "label_list", "=", "sorted", "(", "label_list", ")", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "entity2id", "=", "{", "}", "\n", "with", "open", "(", "\"kg_embed/entity2id.txt\"", ")", "as", "fin", ":", "\n", "        ", "fin", ".", "readline", "(", ")", "\n", "for", "line", "in", "fin", ":", "\n", "            ", "qid", ",", "eid", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "entity2id", "[", "qid", "]", "=", "int", "(", "eid", ")", "\n", "\n", "", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "ex_text_a", "=", "example", ".", "text_a", "[", "0", "]", "\n", "h", ",", "t", "=", "example", ".", "text_a", "[", "1", "]", "\n", "h_name", "=", "ex_text_a", "[", "h", "[", "1", "]", ":", "h", "[", "2", "]", "]", "\n", "t_name", "=", "ex_text_a", "[", "t", "[", "1", "]", ":", "t", "[", "2", "]", "]", "\n", "if", "h", "[", "1", "]", "<", "t", "[", "1", "]", ":", "\n", "            ", "ex_text_a", "=", "ex_text_a", "[", ":", "h", "[", "1", "]", "]", "+", "\"# \"", "+", "h_name", "+", "\" #\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "t", "[", "1", "]", "]", "+", "\"$ \"", "+", "t_name", "+", "\" $\"", "+", "ex_text_a", "[", "t", "[", "2", "]", ":", "]", "\n", "", "else", ":", "\n", "            ", "ex_text_a", "=", "ex_text_a", "[", ":", "t", "[", "1", "]", "]", "+", "\"$ \"", "+", "t_name", "+", "\" $\"", "+", "ex_text_a", "[", "t", "[", "2", "]", ":", "h", "[", "1", "]", "]", "+", "\"# \"", "+", "h_name", "+", "\" #\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "]", "\n", "\n", "", "if", "h", "[", "1", "]", "<", "t", "[", "1", "]", ":", "\n", "            ", "h", "[", "1", "]", "+=", "2", "\n", "h", "[", "2", "]", "+=", "2", "\n", "t", "[", "1", "]", "+=", "6", "\n", "t", "[", "2", "]", "+=", "6", "\n", "", "else", ":", "\n", "            ", "h", "[", "1", "]", "+=", "6", "\n", "h", "[", "2", "]", "+=", "6", "\n", "t", "[", "1", "]", "+=", "2", "\n", "t", "[", "2", "]", "+=", "2", "\n", "", "tokens_a", ",", "entities_a", "=", "tokenizer", ".", "tokenize", "(", "ex_text_a", ",", "[", "h", ",", "t", "]", ")", "\n", "assert", "len", "(", "[", "x", "for", "x", "in", "entities_a", "if", "x", "!=", "\"UNK\"", "]", ")", "==", "2", "\n", "\n", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "            ", "tokens_b", ",", "entities_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", "[", "0", "]", ",", "[", "x", "for", "x", "in", "example", ".", "text_b", "[", "1", "]", "if", "x", "[", "-", "1", "]", ">", "threshold", "]", ")", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "entities_a", ",", "entities_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities_a", "=", "entities_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "=", "[", "\"UNK\"", "]", "+", "entities_a", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "tokens", "+=", "tokens_b", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "+=", "entities_b", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "+=", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_ent", "=", "[", "]", "\n", "ent_mask", "=", "[", "]", "\n", "for", "ent", "in", "ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", "and", "ent", "in", "entity2id", ":", "\n", "                ", "input_ent", ".", "append", "(", "entity2id", "[", "ent", "]", ")", "\n", "ent_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "input_ent", ".", "append", "(", "-", "1", ")", "\n", "ent_mask", ".", "append", "(", "0", ")", "\n", "", "", "ent_mask", "[", "0", "]", "=", "1", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "padding_", "=", "[", "-", "1", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "input_ent", "+=", "padding_", "\n", "ent_mask", "+=", "padding", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_ent", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "ent_mask", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "if", "ex_index", "<", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"ents: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "ents", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "input_ent", "=", "input_ent", ",", "\n", "ent_mask", "=", "ent_mask", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel._truncate_seq_pair": [[271, 288], ["len", "len", "len", "len", "tokens_a.pop", "ents_a.pop", "tokens_b.pop", "ents_b.pop"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "ents_a", ",", "ents_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "ents_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "ents_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.accuracy": [[289, 292], ["numpy.argmax", "numpy.sum"], "function", ["None"], ["", "", "", "def", "accuracy", "(", "out", ",", "labels", ")", ":", "\n", "    ", "outputs", "=", "np", ".", "argmax", "(", "out", ",", "axis", "=", "1", ")", "\n", "return", "np", ".", "sum", "(", "outputs", "==", "labels", ")", ",", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.warmup_linear": [[293, 297], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_fewrel.main": [[298, 532], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "processors", "knowledge_bert.tokenization.BertTokenizer.from_pretrained", "processors.get_train_examples", "vecs.append", "torch.FloatTensor", "torch.nn.Embedding.from_pretrained", "logger.info", "os.listdir", "processors.get_dev_examples", "eval_fewrel.convert_examples_to_features", "processors.get_test_examples", "eval_fewrel.convert_examples_to_features", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "open", "file_mark.append", "file_mark.append", "print", "os.path.join", "torch.load", "knowledge_bert.modeling.BertForSequenceClassification.from_pretrained", "model.to", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "open", "open", "model.eval", "bool", "line.strip().split", "vecs.append", "str", "len", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "torch.nn.Embedding.from_pretrained.", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "input_ent.to.to", "ent_mask.to.to", "label_ids.to().numpy.to", "model.detach().cpu().numpy", "label_ids.to().numpy.to().numpy", "eval_fewrel.accuracy", "zip", "model.mean().item", "input_ids.to.size", "open", "logger.info", "sorted", "float", "torch.nn.Embedding.from_pretrained.weight.size", "len", "torch.no_grad", "model", "model", "open.write", "open.write", "result.keys", "logger.info", "writer.write", "torch.cuda.is_available", "line.strip", "model.detach().cpu", "label_ids.to().numpy.to", "model.mean", "str", "x.split", "x.split", "x.split", "x.split", "x.split", "x.split", "model.detach", "str"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_train_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_dev_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_test_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.accuracy", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ernie_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Ernie pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold'", ",", "type", "=", "float", ",", "default", "=", ".3", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "processors", "=", "FewrelProcessor", "\n", "\n", "num_labels_task", "=", "80", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "processor", "=", "processors", "(", ")", "\n", "num_labels", "=", "num_labels_task", "\n", "label_list", "=", "None", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "train_examples", ",", "label_list", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "vecs", "=", "[", "]", "\n", "vecs", ".", "append", "(", "[", "0", "]", "*", "100", ")", "\n", "with", "open", "(", "\"kg_embed/entity2vec.vec\"", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "vec", "=", "[", "float", "(", "x", ")", "for", "x", "in", "vec", "]", "\n", "vecs", ".", "append", "(", "vec", ")", "\n", "", "", "embed", "=", "torch", ".", "FloatTensor", "(", "vecs", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embed", ")", "\n", "#embed = torch.nn.Embedding(5041175, 100)", "\n", "\n", "logger", ".", "info", "(", "\"Shape of entity embedding: \"", "+", "str", "(", "embed", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "del", "vecs", "\n", "\n", "filenames", "=", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "\n", "filenames", "=", "[", "x", "for", "x", "in", "filenames", "if", "\"pytorch_model.bin_\"", "in", "x", "]", "\n", "\n", "file_mark", "=", "[", "]", "\n", "for", "x", "in", "filenames", ":", "\n", "        ", "file_mark", ".", "append", "(", "[", "x", ",", "True", "]", ")", "\n", "file_mark", ".", "append", "(", "[", "x", ",", "False", "]", ")", "\n", "\n", "", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ")", "\n", "dev", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ",", "args", ".", "threshold", ")", "\n", "eval_examples", "=", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ")", "\n", "test", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ",", "args", ".", "threshold", ")", "\n", "\n", "for", "x", ",", "mark", "in", "file_mark", ":", "\n", "        ", "print", "(", "x", ",", "mark", ")", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "x", ")", "\n", "model_state_dict", "=", "torch", ".", "load", "(", "output_model_file", ")", "\n", "model", ",", "_", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "state_dict", "=", "model_state_dict", ",", "num_labels", "=", "len", "(", "label_list", ")", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "mark", ":", "\n", "            ", "eval_features", "=", "dev", "\n", "", "else", ":", "\n", "            ", "eval_features", "=", "test", "\n", "", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "# zeros = [0 for _ in range(args.max_seq_length)]", "\n", "# zeros_ent = [0 for _ in range(100)]", "\n", "# zeros_ent = [zeros_ent for _ in range(args.max_seq_length)]", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ent", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent_masks", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ent_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_ent", ",", "all_ent_masks", ",", "all_label_ids", ")", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "if", "mark", ":", "\n", "            ", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "output_file_pred", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_pred_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "output_file_glod", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_gold_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"test_results_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "output_file_pred", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"test_pred_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "output_file_glod", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"test_gold_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "\n", "", "fpred", "=", "open", "(", "output_file_pred", ",", "\"w\"", ")", "\n", "fgold", "=", "open", "(", "output_file_glod", ",", "\"w\"", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", ",", "eval_accuracy", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "label_ids", "in", "eval_dataloader", ":", "\n", "            ", "input_ent", "=", "embed", "(", "input_ent", "+", "1", ")", "# -1 -> 0", "\n", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "input_ent", "=", "input_ent", ".", "to", "(", "device", ")", "\n", "ent_mask", "=", "ent_mask", ".", "to", "(", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "tmp_eval_loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ",", "ent_mask", ",", "label_ids", ")", "\n", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ",", "ent_mask", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "tmp_eval_accuracy", ",", "pred", "=", "accuracy", "(", "logits", ",", "label_ids", ")", "\n", "for", "a", ",", "b", "in", "zip", "(", "pred", ",", "label_ids", ")", ":", "\n", "                ", "fgold", ".", "write", "(", "\"{}\\n\"", ".", "format", "(", "b", ")", ")", "\n", "fpred", ".", "write", "(", "\"{}\\n\"", ".", "format", "(", "a", ")", ")", "\n", "\n", "", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "eval_accuracy", "+=", "tmp_eval_accuracy", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "eval_accuracy", "=", "eval_accuracy", "/", "nb_eval_examples", "\n", "\n", "result", "=", "{", "'eval_loss'", ":", "eval_loss", ",", "\n", "'eval_accuracy'", ":", "eval_accuracy", "\n", "}", "\n", "\n", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.InputExample.__init__": [[50, 66], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.InputFeatures.__init__": [[71, 78], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "labels", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "input_ent", "=", "input_ent", "\n", "self", ".", "ent_mask", "=", "ent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.DataProcessor.get_train_examples": [[83, 86], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.DataProcessor.get_dev_examples": [[87, 90], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.DataProcessor.get_labels": [[91, 94], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.DataProcessor._read_json": [[95, 99], ["open", "simplejson.load"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "@", "classmethod", "\n", "def", "_read_json", "(", "cls", ",", "input_file", ")", ":", "\n", "        ", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "return", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.TypingProcessor.get_train_examples": [[104, 119], ["logger.info", "eval_figer.TypingProcessor._create_examples", "d.items", "eval_figer.TypingProcessor._read_json", "list", "os.path.join", "os.path.join", "d.keys", "len"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "logger", ".", "info", "(", "\"LOOKING AT {}\"", ".", "format", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ")", "\n", "examples", "=", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ",", "\"train\"", ")", "\n", "d", "=", "{", "}", "\n", "for", "e", "in", "examples", ":", "\n", "            ", "for", "l", "in", "e", ".", "label", ":", "\n", "                ", "if", "l", "in", "d", ":", "\n", "                    ", "d", "[", "l", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "d", "[", "l", "]", "=", "1", "\n", "", "", "", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", ":", "\n", "            ", "d", "[", "k", "]", "=", "(", "len", "(", "examples", ")", "-", "v", ")", "*", "1.", "/", "v", "\n", "", "return", "examples", ",", "list", "(", "d", ".", "keys", "(", ")", ")", ",", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.TypingProcessor.get_dev_examples": [[120, 124], ["eval_figer.TypingProcessor._create_examples", "eval_figer.TypingProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.json\"", ")", ")", ",", "\"dev\"", ")", "\n", "", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.TypingProcessor.get_test_examples": [[124, 128], ["eval_figer.TypingProcessor._create_examples", "eval_figer.TypingProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test.json\"", ")", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.TypingProcessor.get_label": [[130, 139], ["os.path.join", "open", "line.split", "v.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "def", "get_label", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"label.dict\"", ")", "\n", "v", "=", "[", "]", "\n", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "vec", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "v", ".", "append", "(", "vec", "[", "1", "]", ")", "\n", "", "", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.TypingProcessor._create_examples": [[140, 153], ["enumerate", "examples.append", "eval_figer.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "i", "\n", "text_a", "=", "(", "line", "[", "'sent'", "]", ",", "[", "[", "\"SPAN\"", ",", "line", "[", "\"start\"", "]", ",", "line", "[", "\"end\"", "]", "]", "]", ")", "\n", "text_b", "=", "line", "[", "'ents'", "]", "\n", "label", "=", "line", "[", "'labels'", "]", "\n", "#if guid != 51:", "\n", "#    continue", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.convert_examples_to_features": [[155, 273], ["enumerate", "open", "fin.readline", "tokenizer_label.tokenize", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "line.strip().split", "int", "len", "len", "print", "print", "print", "print", "exit", "sum", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "eval_figer.InputFeatures", "span_mask.append", "span_mask.append", "input_ent.append", "ent_mask.append", "input_ent.append", "ent_mask.append", "len", "len", "line.strip", "str", "str", "zip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_tokens_to_ids"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer_label", ",", "tokenizer", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "entity2id", "=", "{", "}", "\n", "with", "open", "(", "\"kg_embed/entity2id.txt\"", ")", "as", "fin", ":", "\n", "        ", "fin", ".", "readline", "(", ")", "\n", "for", "line", "in", "fin", ":", "\n", "            ", "qid", ",", "eid", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "entity2id", "[", "qid", "]", "=", "int", "(", "eid", ")", "\n", "\n", "", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "ex_text_a", "=", "example", ".", "text_a", "[", "0", "]", "\n", "h", "=", "example", ".", "text_a", "[", "1", "]", "[", "0", "]", "\n", "ex_text_a", "=", "ex_text_a", "[", ":", "h", "[", "1", "]", "]", "+", "\"\u3002 \"", "+", "ex_text_a", "[", "h", "[", "1", "]", ":", "h", "[", "2", "]", "]", "+", "\" \u3002\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "]", "\n", "begin", ",", "end", "=", "h", "[", "1", ":", "3", "]", "\n", "h", "[", "1", "]", "+=", "2", "\n", "h", "[", "2", "]", "+=", "2", "\n", "tokens_a", ",", "entities_a", "=", "tokenizer_label", ".", "tokenize", "(", "ex_text_a", ",", "[", "h", "]", ")", "\n", "# change begin pos", "\n", "ent_pos", "=", "[", "x", "for", "x", "in", "example", ".", "text_b", "if", "x", "[", "-", "1", "]", ">", "threshold", "]", "\n", "for", "x", "in", "ent_pos", ":", "\n", "            ", "if", "x", "[", "1", "]", ">", "end", ":", "\n", "                ", "x", "[", "1", "]", "+=", "4", "\n", "", "elif", "x", "[", "1", "]", ">=", "begin", ":", "\n", "                ", "x", "[", "1", "]", "+=", "2", "\n", "", "", "_", ",", "entities", "=", "tokenizer", ".", "tokenize", "(", "ex_text_a", ",", "ent_pos", ")", "\n", "if", "h", "[", "1", "]", "==", "h", "[", "2", "]", ":", "\n", "            ", "continue", "\n", "", "mark", "=", "False", "\n", "tokens_b", "=", "None", "\n", "for", "e", "in", "entities_a", ":", "\n", "            ", "if", "e", "!=", "\"UNK\"", ":", "\n", "                ", "mark", "=", "True", "\n", "", "", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "            ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities_a", "=", "entities_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities", "=", "entities", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "=", "[", "\"UNK\"", "]", "+", "entities_a", "+", "[", "\"UNK\"", "]", "\n", "real_ents", "=", "[", "\"UNK\"", "]", "+", "entities", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "span_mask", "=", "[", "]", "\n", "for", "ent", "in", "ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", ":", "\n", "                ", "span_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "span_mask", ".", "append", "(", "0", ")", "\n", "\n", "", "", "input_ent", "=", "[", "]", "\n", "ent_mask", "=", "[", "]", "\n", "for", "ent", "in", "real_ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", "and", "ent", "in", "entity2id", ":", "\n", "                ", "input_ent", ".", "append", "(", "entity2id", "[", "ent", "]", ")", "\n", "ent_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "input_ent", ".", "append", "(", "-", "1", ")", "\n", "ent_mask", ".", "append", "(", "0", ")", "\n", "", "", "ent_mask", "[", "0", "]", "=", "1", "\n", "\n", "if", "not", "mark", ":", "\n", "            ", "print", "(", "example", ".", "guid", ")", "\n", "print", "(", "example", ".", "text_a", "[", "0", "]", ")", "\n", "print", "(", "example", ".", "text_a", "[", "0", "]", "[", "example", ".", "text_a", "[", "1", "]", "[", "0", "]", "[", "1", "]", ":", "example", ".", "text_a", "[", "1", "]", "[", "0", "]", "[", "2", "]", "]", ")", "\n", "print", "(", "ents", ")", "\n", "exit", "(", "1", ")", "\n", "", "if", "sum", "(", "span_mask", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "padding_", "=", "[", "-", "1", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "ent_mask", "+=", "padding", "\n", "input_ent", "+=", "padding_", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "ent_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_ent", ")", "==", "max_seq_length", "\n", "\n", "labels", "=", "[", "0", "]", "*", "len", "(", "label_map", ")", "\n", "for", "l", "in", "example", ".", "label", ":", "\n", "            ", "l", "=", "label_map", "[", "l", "]", "\n", "labels", "[", "l", "]", "=", "1", "\n", "", "if", "ex_index", "<", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"Entity: %s\"", "%", "example", ".", "text_a", "[", "1", "]", ")", "\n", "#logger.info(\"Entity: %s\" % example.text_a[0][example.text_a[1][1]:example.text_a[1][2]])", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "zip", "(", "tokens", ",", "ents", ")", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "#logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))", "\n", "#logger.info(", "\n", "#        \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))", "\n", "logger", ".", "info", "(", "\"label: %s %s\"", "%", "(", "example", ".", "label", ",", "labels", ")", ")", "\n", "logger", ".", "info", "(", "real_ents", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "input_ent", "=", "input_ent", ",", "\n", "ent_mask", "=", "ent_mask", ",", "\n", "labels", "=", "labels", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer._truncate_seq_pair": [[275, 292], ["len", "len", "len", "len", "tokens_a.pop", "ents_a.pop", "tokens_b.pop", "ents_b.pop"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "ents_a", ",", "ents_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "ents_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "ents_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.accuracy": [[293, 310], ["zip", "max", "range", "y1.append", "y2.append", "len", "set", "set", "yy1.append", "yy2.append"], "function", ["None"], ["", "", "", "def", "accuracy", "(", "out", ",", "l", ")", ":", "\n", "    ", "cnt", "=", "0", "\n", "y1", "=", "[", "]", "\n", "y2", "=", "[", "]", "\n", "for", "x1", ",", "x2", "in", "zip", "(", "out", ",", "l", ")", ":", "\n", "        ", "yy1", "=", "[", "]", "\n", "yy2", "=", "[", "]", "\n", "top", "=", "max", "(", "x1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "x1", ")", ")", ":", "\n", "            ", "if", "x1", "[", "i", "]", ">", "0", "or", "x1", "[", "i", "]", "==", "top", ":", "\n", "                ", "yy1", ".", "append", "(", "i", ")", "\n", "", "if", "x2", "[", "i", "]", ">", "0", ":", "\n", "                ", "yy2", ".", "append", "(", "i", ")", "\n", "", "", "y1", ".", "append", "(", "yy1", ")", "\n", "y2", ".", "append", "(", "yy2", ")", "\n", "cnt", "+=", "set", "(", "yy1", ")", "==", "set", "(", "yy2", ")", "\n", "", "return", "cnt", ",", "y1", ",", "y2", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.warmup_linear": [[311, 315], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_figer.main": [[316, 578], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "eval_figer.TypingProcessor", "knowledge_bert.typing.BertTokenizer.from_pretrained", "knowledge_bert.typing.BertTokenizer.from_pretrained", "eval_figer.TypingProcessor.get_train_examples", "sorted", "vecs.append", "torch.FloatTensor", "torch.nn.Embedding.from_pretrained", "logger.info", "os.listdir", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "S.append", "open", "file_mark.append", "file_mark.append", "print", "os.path.join", "torch.load", "knowledge_bert.modeling.BertForEntityTyping.from_pretrained", "model.to", "eval_figer.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "model.eval", "bool", "line.strip().split", "vecs.append", "str", "eval_figer.TypingProcessor.get_dev_examples", "eval_figer.TypingProcessor.get_test_examples", "len", "torch.nn.Embedding.from_pretrained.", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "input_ent.to.to", "ent_mask.to.to", "labels.to().numpy.to", "model.detach().cpu().numpy", "labels.to().numpy.to().numpy", "eval_figer.accuracy", "pred.extend", "true.extend", "model.mean().item", "input_ids.to.size", "len", "zip", "zip", "os.path.join", "os.path.join", "open", "logger.info", "sorted", "s.append", "s.append", "float", "torch.nn.Embedding.from_pretrained.weight.size", "len", "torch.no_grad", "model", "model", "float", "len", "eval_figer.main.f1"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_train_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_dev_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_test_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.accuracy", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ernie_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Ernie pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold'", ",", "type", "=", "float", ",", "default", "=", ".3", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "processor", "=", "TypingProcessor", "(", ")", "\n", "\n", "tokenizer_label", "=", "BertTokenizer_label", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "_", ",", "label_list", ",", "_", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "label_list", "=", "sorted", "(", "label_list", ")", "\n", "#class_weight = [min(d[x], 100) for x in label_list]", "\n", "#logger.info(class_weight)", "\n", "S", "=", "[", "]", "\n", "for", "l", "in", "label_list", ":", "\n", "        ", "s", "=", "[", "]", "\n", "for", "ll", "in", "label_list", ":", "\n", "            ", "if", "ll", "in", "l", ":", "\n", "                ", "s", ".", "append", "(", "1.", ")", "\n", "", "else", ":", "\n", "                ", "s", ".", "append", "(", "0.", ")", "\n", "", "", "S", ".", "append", "(", "s", ")", "\n", "\n", "", "vecs", "=", "[", "]", "\n", "vecs", ".", "append", "(", "[", "0", "]", "*", "100", ")", "\n", "with", "open", "(", "\"kg_embed/entity2vec.vec\"", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "vec", "=", "[", "float", "(", "x", ")", "for", "x", "in", "vec", "]", "\n", "vecs", ".", "append", "(", "vec", ")", "\n", "", "", "embed", "=", "torch", ".", "FloatTensor", "(", "vecs", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embed", ")", "\n", "logger", ".", "info", "(", "\"Shape of entity embedding: \"", "+", "str", "(", "embed", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "del", "vecs", "\n", "\n", "filenames", "=", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "\n", "filenames", "=", "[", "x", "for", "x", "in", "filenames", "if", "\"pytorch_model.bin_\"", "in", "x", "]", "\n", "\n", "file_mark", "=", "[", "]", "\n", "for", "x", "in", "filenames", ":", "\n", "        ", "file_mark", ".", "append", "(", "[", "x", ",", "True", "]", ")", "\n", "file_mark", ".", "append", "(", "[", "x", ",", "False", "]", ")", "\n", "\n", "", "for", "x", ",", "mark", "in", "file_mark", ":", "\n", "        ", "print", "(", "x", ",", "mark", ")", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "x", ")", "\n", "model_state_dict", "=", "torch", ".", "load", "(", "output_model_file", ")", "\n", "model", ",", "_", "=", "BertForEntityTyping", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "state_dict", "=", "model_state_dict", ",", "num_labels", "=", "len", "(", "label_list", ")", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "mark", ":", "\n", "            ", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ")", "\n", "", "else", ":", "\n", "            ", "eval_examples", "=", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ")", "\n", "", "eval_features", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer_label", ",", "tokenizer", ",", "args", ".", "threshold", ")", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "# zeros = [0 for _ in range(args.max_seq_length)]", "\n", "# zeros_ent = [0 for _ in range(100)]", "\n", "# zeros_ent = [zeros_ent for _ in range(args.max_seq_length)]", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_ent", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ent", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ent_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "labels", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_input_ent", ",", "all_ent_mask", ",", "all_labels", ")", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", ",", "eval_accuracy", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "pred", "=", "[", "]", "\n", "true", "=", "[", "]", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "labels", "in", "eval_dataloader", ":", "\n", "            ", "input_ent", "=", "embed", "(", "input_ent", "+", "1", ")", "\n", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "input_ent", "=", "input_ent", ".", "to", "(", "device", ")", "\n", "ent_mask", "=", "ent_mask", ".", "to", "(", "device", ")", "\n", "labels", "=", "labels", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "tmp_eval_loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ",", "ent_mask", ",", "labels", ")", "\n", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ",", "ent_mask", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "labels", "=", "labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "tmp_eval_accuracy", ",", "tmp_pred", ",", "tmp_true", "=", "accuracy", "(", "logits", ",", "labels", ")", "\n", "pred", ".", "extend", "(", "tmp_pred", ")", "\n", "true", ".", "extend", "(", "tmp_true", ")", "\n", "\n", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "eval_accuracy", "+=", "tmp_eval_accuracy", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "eval_accuracy", "=", "eval_accuracy", "/", "nb_eval_examples", "\n", "\n", "def", "f1", "(", "p", ",", "r", ")", ":", "\n", "            ", "if", "r", "==", "0.", ":", "\n", "                ", "return", "0.", "\n", "", "return", "2", "*", "p", "*", "r", "/", "float", "(", "p", "+", "r", ")", "\n", "", "def", "loose_macro", "(", "true", ",", "pred", ")", ":", "\n", "            ", "num_entities", "=", "len", "(", "true", ")", "\n", "p", "=", "0.", "\n", "r", "=", "0.", "\n", "for", "true_labels", ",", "predicted_labels", "in", "zip", "(", "true", ",", "pred", ")", ":", "\n", "                ", "if", "len", "(", "predicted_labels", ")", ">", "0", ":", "\n", "                    ", "p", "+=", "len", "(", "set", "(", "predicted_labels", ")", ".", "intersection", "(", "set", "(", "true_labels", ")", ")", ")", "/", "float", "(", "len", "(", "predicted_labels", ")", ")", "\n", "", "if", "len", "(", "true_labels", ")", ":", "\n", "                    ", "r", "+=", "len", "(", "set", "(", "predicted_labels", ")", ".", "intersection", "(", "set", "(", "true_labels", ")", ")", ")", "/", "float", "(", "len", "(", "true_labels", ")", ")", "\n", "", "", "precision", "=", "p", "/", "num_entities", "\n", "recall", "=", "r", "/", "num_entities", "\n", "return", "precision", ",", "recall", ",", "f1", "(", "precision", ",", "recall", ")", "\n", "", "def", "loose_micro", "(", "true", ",", "pred", ")", ":", "\n", "            ", "num_predicted_labels", "=", "0.", "\n", "num_true_labels", "=", "0.", "\n", "num_correct_labels", "=", "0.", "\n", "for", "true_labels", ",", "predicted_labels", "in", "zip", "(", "true", ",", "pred", ")", ":", "\n", "                ", "num_predicted_labels", "+=", "len", "(", "predicted_labels", ")", "\n", "num_true_labels", "+=", "len", "(", "true_labels", ")", "\n", "num_correct_labels", "+=", "len", "(", "set", "(", "predicted_labels", ")", ".", "intersection", "(", "set", "(", "true_labels", ")", ")", ")", "\n", "", "if", "num_predicted_labels", ">", "0", ":", "\n", "                ", "precision", "=", "num_correct_labels", "/", "num_predicted_labels", "\n", "", "else", ":", "\n", "                ", "precision", "=", "0.", "\n", "", "recall", "=", "num_correct_labels", "/", "num_true_labels", "\n", "return", "precision", ",", "recall", ",", "f1", "(", "precision", ",", "recall", ")", "\n", "\n", "", "if", "False", ":", "\n", "            ", "result", "=", "{", "'eval_loss'", ":", "eval_loss", ",", "\n", "'eval_accuracy'", ":", "eval_accuracy", ",", "\n", "'macro'", ":", "loose_macro", "(", "true", ",", "pred", ")", ",", "\n", "'micro'", ":", "loose_micro", "(", "true", ",", "pred", ")", "\n", "}", "\n", "", "else", ":", "\n", "            ", "result", "=", "{", "'eval_loss'", ":", "eval_loss", ",", "\n", "'eval_accuracy'", ":", "eval_accuracy", ",", "\n", "'macro'", ":", "loose_macro", "(", "true", ",", "pred", ")", ",", "\n", "'micro'", ":", "loose_micro", "(", "true", ",", "pred", ")", "\n", "}", "\n", "\n", "", "if", "mark", ":", "\n", "            ", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"test_results_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.InputExample.__init__": [[49, 65], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.InputFeatures.__init__": [[70, 77], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "label_id", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "input_ent", "=", "input_ent", "\n", "self", ".", "ent_mask", "=", "ent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.DataProcessor.get_train_examples": [[82, 85], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.DataProcessor.get_dev_examples": [[86, 89], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.DataProcessor.get_labels": [[90, 93], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.DataProcessor._read_json": [[94, 98], ["open", "simplejson.loads", "f.read"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "@", "classmethod", "\n", "def", "_read_json", "(", "cls", ",", "input_file", ")", ":", "\n", "        ", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "return", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.TacredProcessor.get_train_examples": [[102, 108], ["run_tacred.TacredProcessor._create_examples", "set", "run_tacred.TacredProcessor._read_json", "list", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "examples", "=", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ",", "\"train\"", ")", "\n", "labels", "=", "set", "(", "[", "x", ".", "label", "for", "x", "in", "examples", "]", ")", "\n", "return", "examples", ",", "list", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.TacredProcessor.get_dev_examples": [[109, 113], ["run_tacred.TacredProcessor._create_examples", "run_tacred.TacredProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.json\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.TacredProcessor.get_labels": [[114, 117], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Useless\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.TacredProcessor._create_examples": [[118, 132], ["enumerate", "examples.append", "run_tacred.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "for", "x", "in", "line", "[", "'ents'", "]", ":", "\n", "                ", "if", "x", "[", "1", "]", "==", "1", ":", "\n", "                    ", "x", "[", "1", "]", "=", "0", "\n", "#print(line['text'][x[1]:x[2]].encode(\"utf-8\"))", "\n", "", "", "text_a", "=", "(", "line", "[", "'text'", "]", ",", "line", "[", "'ents'", "]", ")", "\n", "label", "=", "line", "[", "'label'", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "line", "[", "'ann'", "]", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.convert_examples_to_features": [[134, 283], ["sorted", "enumerate", "open", "fin.readline", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "line.strip().split", "int", "tokenizer.tokenize", "run_tacred._truncate_seq_pair", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "run_tacred.InputFeatures", "len", "input_ent.append", "ent_mask.append", "input_ent.append", "ent_mask.append", "len", "len", "line.strip", "len", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred._truncate_seq_pair"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "label_list", "=", "sorted", "(", "label_list", ")", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "entity2id", "=", "{", "}", "\n", "with", "open", "(", "\"kg_embed/entity2id.txt\"", ")", "as", "fin", ":", "\n", "        ", "fin", ".", "readline", "(", ")", "\n", "for", "line", "in", "fin", ":", "\n", "            ", "qid", ",", "eid", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "entity2id", "[", "qid", "]", "=", "int", "(", "eid", ")", "\n", "\n", "", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "\n", "        ", "ex_text_a", "=", "example", ".", "text_a", "[", "0", "]", "\n", "h", ",", "t", "=", "example", ".", "text_a", "[", "1", "]", "\n", "h_name", "=", "ex_text_a", "[", "h", "[", "1", "]", ":", "h", "[", "2", "]", "]", "\n", "t_name", "=", "ex_text_a", "[", "t", "[", "1", "]", ":", "t", "[", "2", "]", "]", "\n", "#ex_text_a = ex_text_a.replace(h_name, \"# \"+h_name+\" #\", 1)", "\n", "#ex_text_a = ex_text_a.replace(t_name, \"$ \"+t_name+\" $\", 1)", "\n", "# Add [HD] and [TL], which are \"#\" and \"$\" respectively.", "\n", "if", "h", "[", "1", "]", "<", "t", "[", "1", "]", ":", "\n", "            ", "ex_text_a", "=", "ex_text_a", "[", ":", "h", "[", "1", "]", "]", "+", "\"# \"", "+", "h_name", "+", "\" #\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "t", "[", "1", "]", "]", "+", "\"$ \"", "+", "t_name", "+", "\" $\"", "+", "ex_text_a", "[", "t", "[", "2", "]", ":", "]", "\n", "", "else", ":", "\n", "            ", "ex_text_a", "=", "ex_text_a", "[", ":", "t", "[", "1", "]", "]", "+", "\"$ \"", "+", "t_name", "+", "\" $\"", "+", "ex_text_a", "[", "t", "[", "2", "]", ":", "h", "[", "1", "]", "]", "+", "\"# \"", "+", "h_name", "+", "\" #\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "]", "\n", "\n", "", "ent_pos", "=", "[", "x", "for", "x", "in", "example", ".", "text_b", "if", "x", "[", "-", "1", "]", ">", "threshold", "]", "\n", "for", "x", "in", "ent_pos", ":", "\n", "            ", "cnt", "=", "0", "\n", "if", "x", "[", "1", "]", ">", "h", "[", "2", "]", ":", "\n", "                ", "cnt", "+=", "2", "\n", "", "if", "x", "[", "1", "]", ">=", "h", "[", "1", "]", ":", "\n", "                ", "cnt", "+=", "2", "\n", "", "if", "x", "[", "1", "]", ">=", "t", "[", "1", "]", ":", "\n", "                ", "cnt", "+=", "2", "\n", "", "if", "x", "[", "1", "]", ">", "t", "[", "2", "]", ":", "\n", "                ", "cnt", "+=", "2", "\n", "", "x", "[", "1", "]", "+=", "cnt", "\n", "x", "[", "2", "]", "+=", "cnt", "\n", "", "tokens_a", ",", "entities_a", "=", "tokenizer", ".", "tokenize", "(", "ex_text_a", ",", "ent_pos", ")", "\n", "'''\n        cnt = 0\n        for x in entities_a:\n            if x != \"UNK\":\n                cnt += 1\n        if cnt != len(ent_pos) and ent_pos[0][0] != 'Q46809':\n            print(cnt, len(ent_pos))\n            print(ex_text_a)\n            print(ent_pos)\n            for x in ent_pos:\n                print(ex_text_a[x[1]:x[2]])\n            exit(1)\n        '''", "\n", "\n", "tokens_b", "=", "None", "\n", "if", "False", ":", "\n", "            ", "tokens_b", ",", "entities_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", "[", "0", "]", ",", "[", "x", "for", "x", "in", "example", ".", "text_b", "[", "1", "]", "if", "x", "[", "-", "1", "]", ">", "threshold", "]", ")", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "entities_a", ",", "entities_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities_a", "=", "entities_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "=", "[", "\"UNK\"", "]", "+", "entities_a", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "tokens", "+=", "tokens_b", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "+=", "entities_b", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "+=", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_ent", "=", "[", "]", "\n", "ent_mask", "=", "[", "]", "\n", "for", "ent", "in", "ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", "and", "ent", "in", "entity2id", ":", "\n", "                ", "input_ent", ".", "append", "(", "entity2id", "[", "ent", "]", ")", "\n", "ent_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "input_ent", ".", "append", "(", "-", "1", ")", "\n", "ent_mask", ".", "append", "(", "0", ")", "\n", "", "", "ent_mask", "[", "0", "]", "=", "1", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "padding_", "=", "[", "-", "1", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "input_ent", "+=", "padding_", "\n", "ent_mask", "+=", "padding", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_ent", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "ent_mask", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "if", "ex_index", "<", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"ents: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "ents", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "input_ent", "=", "input_ent", ",", "\n", "ent_mask", "=", "ent_mask", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred._truncate_seq_pair": [[285, 302], ["len", "len", "len", "len", "tokens_a.pop", "ents_a.pop", "tokens_b.pop", "ents_b.pop"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "ents_a", ",", "ents_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "ents_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "ents_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.accuracy": [[303, 306], ["numpy.argmax", "numpy.sum"], "function", ["None"], ["", "", "", "def", "accuracy", "(", "out", ",", "labels", ")", ":", "\n", "    ", "outputs", "=", "np", ".", "argmax", "(", "out", ",", "axis", "=", "1", ")", "\n", "return", "np", ".", "sum", "(", "outputs", "==", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.warmup_linear": [[307, 311], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_tacred.main": [[312, 573], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "os.makedirs", "processors", "knowledge_bert.tokenization.BertTokenizer.from_pretrained", "processors.get_train_examples", "len", "int", "knowledge_bert.modeling.BertForSequenceClassification.from_pretrained", "torch.nn.DataParallel.to", "list", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "os.path.exists", "os.listdir", "ValueError", "torch.nn.DataParallel.half", "DDP", "torch.nn.DataParallel.named_parameters", "FusedAdam", "knowledge_bert.optimization.BertAdam", "run_tacred.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "logger.info", "vecs.append", "torch.FloatTensor", "torch.nn.Embedding.from_pretrained", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "os.path.join", "open", "torch.nn.DataParallel.train", "tqdm.trange", "os.path.join", "torch.save", "bool", "torch.nn.DataParallel", "torch.distributed.get_world_size", "FP16_Optimizer", "FP16_Optimizer", "len", "open", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "int", "enumerate", "os.path.join", "torch.save", "hasattr", "model_to_save.state_dict", "ImportError", "any", "ImportError", "line.strip().split", "vecs.append", "str", "tqdm.tqdm", "tuple", "torch.nn.Embedding.from_pretrained.to", "torch.nn.DataParallel.", "open.write", "loss.mean.item", "input_ids.size", "hasattr", "model_to_save.state_dict", "torch.cuda.is_available", "len", "any", "float", "torch.nn.Embedding.from_pretrained.weight.size", "embed().to.half", "loss.mean.mean", "FP16_Optimizer.backward", "loss.mean.backward", "FP16_Optimizer.step", "FP16_Optimizer.zero_grad", "any", "line.strip", "torch.nn.Embedding.from_pretrained.", "loss.mean.item", "run_tacred.warmup_linear", "t.to", "enumerate"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_train_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.BertAdam.step", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.warmup_linear"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ernie_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Ernie pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold'", ",", "type", "=", "float", ",", "default", "=", ".3", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "processors", "=", "TacredProcessor", "\n", "\n", "num_labels_task", "=", "80", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "and", "args", ".", "do_train", ":", "\n", "        ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "\n", "processor", "=", "processors", "(", ")", "\n", "label_list", "=", "None", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "train_examples", ",", "label_list", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "num_labels", "=", "len", "(", "label_list", ")", "\n", "\n", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", ")", "\n", "\n", "# Prepare model", "\n", "model", ",", "_", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "\n", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "/", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_grad", "=", "[", "'bert.encoder.layer.11.output.dense_ent'", ",", "'bert.encoder.layer.11.output.LayerNorm_ent'", "]", "\n", "param_optimizer", "=", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_grad", ")", "]", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "t_total", "=", "num_train_steps", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "t_total", "=", "t_total", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "True", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "t_total", ")", "\n", "", "global_step", "=", "0", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_features", "=", "convert_examples_to_features", "(", "\n", "train_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ",", "args", ".", "threshold", ")", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "\n", "vecs", "=", "[", "]", "\n", "vecs", ".", "append", "(", "[", "0", "]", "*", "100", ")", "\n", "with", "open", "(", "\"kg_embed/entity2vec.vec\"", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "vec", "=", "[", "float", "(", "x", ")", "for", "x", "in", "vec", "]", "\n", "vecs", ".", "append", "(", "vec", ")", "\n", "", "", "embed", "=", "torch", ".", "FloatTensor", "(", "vecs", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embed", ")", "\n", "#embed = torch.nn.Embedding(5041175, 100)", "\n", "\n", "logger", ".", "info", "(", "\"Shape of entity embedding: \"", "+", "str", "(", "embed", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "del", "vecs", "\n", "\n", "# zeros = [0 for _ in range(args.max_seq_length)]", "\n", "# zeros_ent = [0 for _ in range(100)]", "\n", "# zeros_ent = [zeros_ent for _ in range(args.max_seq_length)]", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ent", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent_masks", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ent_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_ent", ",", "all_ent_masks", ",", "all_label_ids", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "output_loss_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"loss\"", ")", "\n", "loss_fout", "=", "open", "(", "output_loss_file", ",", "'w'", ")", "\n", "model", ".", "train", "(", ")", "\n", "for", "_", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "\n", "nb_tr_examples", ",", "nb_tr_steps", "=", "0", ",", "0", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", ")", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "if", "i", "!=", "3", "else", "t", "for", "i", ",", "t", "in", "enumerate", "(", "batch", ")", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "label_ids", "=", "batch", "\n", "input_ent", "=", "embed", "(", "input_ent", "+", "1", ")", ".", "to", "(", "device", ")", "# -1 -> 0", "\n", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ".", "half", "(", ")", ",", "ent_mask", ",", "label_ids", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "loss_fout", ".", "write", "(", "\"{}\\n\"", ".", "format", "(", "loss", ".", "item", "(", ")", ")", ")", "\n", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "                    ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                        ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"pytorch_model.bin_{}\"", ".", "format", "(", "global_step", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n", "# Save a trained model", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"pytorch_model.bin\"", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.InputExample.__init__": [[50, 66], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.InputFeatures.__init__": [[71, 78], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "labels", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "labels", "=", "labels", "\n", "self", ".", "input_ent", "=", "input_ent", "\n", "self", ".", "ent_mask", "=", "ent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.DataProcessor.get_train_examples": [[83, 86], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.DataProcessor.get_dev_examples": [[87, 90], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.DataProcessor.get_labels": [[91, 94], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.DataProcessor._read_json": [[95, 99], ["open", "simplejson.load"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "@", "classmethod", "\n", "def", "_read_json", "(", "cls", ",", "input_file", ")", ":", "\n", "        ", "with", "open", "(", "input_file", ",", "\"r\"", ")", "as", "f", ":", "\n", "            ", "return", "json", ".", "load", "(", "f", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.TypingProcessor.get_train_examples": [[104, 119], ["logger.info", "eval_typing.TypingProcessor._create_examples", "d.items", "eval_typing.TypingProcessor._read_json", "list", "os.path.join", "os.path.join", "d.keys", "len"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "logger", ".", "info", "(", "\"LOOKING AT {}\"", ".", "format", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ")", "\n", "examples", "=", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ",", "\"train\"", ")", "\n", "d", "=", "{", "}", "\n", "for", "e", "in", "examples", ":", "\n", "            ", "for", "l", "in", "e", ".", "label", ":", "\n", "                ", "if", "l", "in", "d", ":", "\n", "                    ", "d", "[", "l", "]", "+=", "1", "\n", "", "else", ":", "\n", "                    ", "d", "[", "l", "]", "=", "1", "\n", "", "", "", "for", "k", ",", "v", "in", "d", ".", "items", "(", ")", ":", "\n", "            ", "d", "[", "k", "]", "=", "(", "len", "(", "examples", ")", "-", "v", ")", "*", "1.", "/", "v", "\n", "", "return", "examples", ",", "list", "(", "d", ".", "keys", "(", ")", ")", ",", "d", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.TypingProcessor.get_dev_examples": [[120, 124], ["eval_typing.TypingProcessor._create_examples", "eval_typing.TypingProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.json\"", ")", ")", ",", "\"dev\"", ")", "\n", "", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.TypingProcessor.get_test_examples": [[124, 128], ["eval_typing.TypingProcessor._create_examples", "eval_typing.TypingProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test.json\"", ")", ")", ",", "\"test\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.TypingProcessor.get_label": [[130, 139], ["os.path.join", "open", "line.split", "v.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "def", "get_label", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "filename", "=", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"label.dict\"", ")", "\n", "v", "=", "[", "]", "\n", "with", "open", "(", "filename", ",", "\"r\"", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "vec", "=", "line", ".", "split", "(", "\"\\t\"", ")", "\n", "v", ".", "append", "(", "vec", "[", "1", "]", ")", "\n", "", "", "return", "v", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.TypingProcessor._create_examples": [[140, 153], ["enumerate", "examples.append", "eval_typing.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "i", "\n", "text_a", "=", "(", "line", "[", "'sent'", "]", ",", "[", "[", "\"SPAN\"", ",", "line", "[", "\"start\"", "]", ",", "line", "[", "\"end\"", "]", "]", "]", ")", "\n", "text_b", "=", "line", "[", "'ents'", "]", "\n", "label", "=", "line", "[", "'labels'", "]", "\n", "#if guid != 51:", "\n", "#    continue", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "text_b", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.convert_examples_to_features": [[155, 273], ["enumerate", "open", "fin.readline", "tokenizer_label.tokenize", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "line.strip().split", "int", "len", "len", "print", "print", "print", "print", "exit", "sum", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "eval_typing.InputFeatures", "span_mask.append", "span_mask.append", "input_ent.append", "ent_mask.append", "input_ent.append", "ent_mask.append", "len", "len", "line.strip", "str", "str", "zip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_tokens_to_ids"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer_label", ",", "tokenizer", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "entity2id", "=", "{", "}", "\n", "with", "open", "(", "\"kg_embed/entity2id.txt\"", ")", "as", "fin", ":", "\n", "        ", "fin", ".", "readline", "(", ")", "\n", "for", "line", "in", "fin", ":", "\n", "            ", "qid", ",", "eid", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "entity2id", "[", "qid", "]", "=", "int", "(", "eid", ")", "\n", "\n", "", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "ex_text_a", "=", "example", ".", "text_a", "[", "0", "]", "\n", "h", "=", "example", ".", "text_a", "[", "1", "]", "[", "0", "]", "\n", "ex_text_a", "=", "ex_text_a", "[", ":", "h", "[", "1", "]", "]", "+", "\"\u3002 \"", "+", "ex_text_a", "[", "h", "[", "1", "]", ":", "h", "[", "2", "]", "]", "+", "\" \u3002\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "]", "\n", "begin", ",", "end", "=", "h", "[", "1", ":", "3", "]", "\n", "h", "[", "1", "]", "+=", "2", "\n", "h", "[", "2", "]", "+=", "2", "\n", "tokens_a", ",", "entities_a", "=", "tokenizer_label", ".", "tokenize", "(", "ex_text_a", ",", "[", "h", "]", ")", "\n", "# change begin pos", "\n", "ent_pos", "=", "[", "x", "for", "x", "in", "example", ".", "text_b", "if", "x", "[", "-", "1", "]", ">", "threshold", "]", "\n", "for", "x", "in", "ent_pos", ":", "\n", "            ", "if", "x", "[", "1", "]", ">", "end", ":", "\n", "                ", "x", "[", "1", "]", "+=", "4", "\n", "", "elif", "x", "[", "1", "]", ">=", "begin", ":", "\n", "                ", "x", "[", "1", "]", "+=", "2", "\n", "", "", "_", ",", "entities", "=", "tokenizer", ".", "tokenize", "(", "ex_text_a", ",", "ent_pos", ")", "\n", "if", "h", "[", "1", "]", "==", "h", "[", "2", "]", ":", "\n", "            ", "continue", "\n", "", "mark", "=", "False", "\n", "tokens_b", "=", "None", "\n", "for", "e", "in", "entities_a", ":", "\n", "            ", "if", "e", "!=", "\"UNK\"", ":", "\n", "                ", "mark", "=", "True", "\n", "", "", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "            ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities_a", "=", "entities_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities", "=", "entities", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "=", "[", "\"UNK\"", "]", "+", "entities_a", "+", "[", "\"UNK\"", "]", "\n", "real_ents", "=", "[", "\"UNK\"", "]", "+", "entities", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "span_mask", "=", "[", "]", "\n", "for", "ent", "in", "ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", ":", "\n", "                ", "span_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "span_mask", ".", "append", "(", "0", ")", "\n", "\n", "", "", "input_ent", "=", "[", "]", "\n", "ent_mask", "=", "[", "]", "\n", "for", "ent", "in", "real_ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", "and", "ent", "in", "entity2id", ":", "\n", "                ", "input_ent", ".", "append", "(", "entity2id", "[", "ent", "]", ")", "\n", "ent_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "input_ent", ".", "append", "(", "-", "1", ")", "\n", "ent_mask", ".", "append", "(", "0", ")", "\n", "", "", "ent_mask", "[", "0", "]", "=", "1", "\n", "\n", "if", "not", "mark", ":", "\n", "            ", "print", "(", "example", ".", "guid", ")", "\n", "print", "(", "example", ".", "text_a", "[", "0", "]", ")", "\n", "print", "(", "example", ".", "text_a", "[", "0", "]", "[", "example", ".", "text_a", "[", "1", "]", "[", "0", "]", "[", "1", "]", ":", "example", ".", "text_a", "[", "1", "]", "[", "0", "]", "[", "2", "]", "]", ")", "\n", "print", "(", "ents", ")", "\n", "exit", "(", "1", ")", "\n", "", "if", "sum", "(", "span_mask", ")", "==", "0", ":", "\n", "            ", "continue", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "padding_", "=", "[", "-", "1", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "ent_mask", "+=", "padding", "\n", "input_ent", "+=", "padding_", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "ent_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_ent", ")", "==", "max_seq_length", "\n", "\n", "labels", "=", "[", "0", "]", "*", "len", "(", "label_map", ")", "\n", "for", "l", "in", "example", ".", "label", ":", "\n", "            ", "l", "=", "label_map", "[", "l", "]", "\n", "labels", "[", "l", "]", "=", "1", "\n", "", "if", "ex_index", "<", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"Entity: %s\"", "%", "example", ".", "text_a", "[", "1", "]", ")", "\n", "#logger.info(\"Entity: %s\" % example.text_a[0][example.text_a[1][1]:example.text_a[1][2]])", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "zip", "(", "tokens", ",", "ents", ")", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "#logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))", "\n", "#logger.info(", "\n", "#        \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))", "\n", "logger", ".", "info", "(", "\"label: %s %s\"", "%", "(", "example", ".", "label", ",", "labels", ")", ")", "\n", "logger", ".", "info", "(", "real_ents", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "input_ent", "=", "input_ent", ",", "\n", "ent_mask", "=", "ent_mask", ",", "\n", "labels", "=", "labels", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing._truncate_seq_pair": [[275, 292], ["len", "len", "len", "len", "tokens_a.pop", "ents_a.pop", "tokens_b.pop", "ents_b.pop"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "ents_a", ",", "ents_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "ents_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "ents_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.accuracy": [[293, 310], ["zip", "max", "range", "y1.append", "y2.append", "len", "set", "set", "yy1.append", "yy2.append"], "function", ["None"], ["", "", "", "def", "accuracy", "(", "out", ",", "l", ")", ":", "\n", "    ", "cnt", "=", "0", "\n", "y1", "=", "[", "]", "\n", "y2", "=", "[", "]", "\n", "for", "x1", ",", "x2", "in", "zip", "(", "out", ",", "l", ")", ":", "\n", "        ", "yy1", "=", "[", "]", "\n", "yy2", "=", "[", "]", "\n", "top", "=", "max", "(", "x1", ")", "\n", "for", "i", "in", "range", "(", "len", "(", "x1", ")", ")", ":", "\n", "            ", "if", "x1", "[", "i", "]", ">", "0", ":", "\n", "                ", "yy1", ".", "append", "(", "i", ")", "\n", "", "if", "x2", "[", "i", "]", ">", "0", ":", "\n", "                ", "yy2", ".", "append", "(", "i", ")", "\n", "", "", "y1", ".", "append", "(", "yy1", ")", "\n", "y2", ".", "append", "(", "yy2", ")", "\n", "cnt", "+=", "set", "(", "yy1", ")", "==", "set", "(", "yy2", ")", "\n", "", "return", "cnt", ",", "y1", ",", "y2", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.warmup_linear": [[311, 315], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_typing.main": [[316, 572], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "eval_typing.TypingProcessor", "knowledge_bert.typing.BertTokenizer.from_pretrained", "knowledge_bert.typing.BertTokenizer.from_pretrained", "eval_typing.TypingProcessor.get_train_examples", "sorted", "vecs.append", "torch.FloatTensor", "torch.nn.Embedding.from_pretrained", "logger.info", "os.listdir", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "S.append", "open", "file_mark.append", "file_mark.append", "print", "os.path.join", "torch.load", "knowledge_bert.modeling.BertForEntityTyping.from_pretrained", "model.to", "eval_typing.convert_examples_to_features", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "model.eval", "bool", "line.strip().split", "vecs.append", "str", "eval_typing.TypingProcessor.get_dev_examples", "eval_typing.TypingProcessor.get_test_examples", "len", "torch.nn.Embedding.from_pretrained.", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "input_ent.to.to", "ent_mask.to.to", "labels.to().numpy.to", "model.detach().cpu().numpy", "labels.to().numpy.to().numpy", "eval_typing.accuracy", "pred.extend", "true.extend", "model.mean().item", "input_ids.to.size", "len", "zip", "zip", "eval_typing.main.loose_macro"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_train_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_dev_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_test_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.accuracy"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ernie_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Ernie pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold'", ",", "type", "=", "float", ",", "default", "=", ".3", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "processor", "=", "TypingProcessor", "(", ")", "\n", "\n", "tokenizer_label", "=", "BertTokenizer_label", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "_", ",", "label_list", ",", "_", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "label_list", "=", "sorted", "(", "label_list", ")", "\n", "#class_weight = [min(d[x], 100) for x in label_list]", "\n", "#logger.info(class_weight)", "\n", "S", "=", "[", "]", "\n", "for", "l", "in", "label_list", ":", "\n", "        ", "s", "=", "[", "]", "\n", "for", "ll", "in", "label_list", ":", "\n", "            ", "if", "ll", "in", "l", ":", "\n", "                ", "s", ".", "append", "(", "1.", ")", "\n", "", "else", ":", "\n", "                ", "s", ".", "append", "(", "0.", ")", "\n", "", "", "S", ".", "append", "(", "s", ")", "\n", "\n", "", "vecs", "=", "[", "]", "\n", "vecs", ".", "append", "(", "[", "0", "]", "*", "100", ")", "\n", "with", "open", "(", "\"kg_embed/entity2vec.vec\"", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "vec", "=", "[", "float", "(", "x", ")", "for", "x", "in", "vec", "]", "\n", "vecs", ".", "append", "(", "vec", ")", "\n", "", "", "embed", "=", "torch", ".", "FloatTensor", "(", "vecs", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embed", ")", "\n", "logger", ".", "info", "(", "\"Shape of entity embedding: \"", "+", "str", "(", "embed", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "del", "vecs", "\n", "\n", "filenames", "=", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "\n", "filenames", "=", "[", "x", "for", "x", "in", "filenames", "if", "\"pytorch_model.bin_\"", "in", "x", "]", "\n", "\n", "file_mark", "=", "[", "]", "\n", "for", "x", "in", "filenames", ":", "\n", "        ", "file_mark", ".", "append", "(", "[", "x", ",", "True", "]", ")", "\n", "file_mark", ".", "append", "(", "[", "x", ",", "False", "]", ")", "\n", "\n", "", "for", "x", ",", "mark", "in", "file_mark", ":", "\n", "        ", "print", "(", "x", ",", "mark", ")", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "x", ")", "\n", "model_state_dict", "=", "torch", ".", "load", "(", "output_model_file", ")", "\n", "model", ",", "_", "=", "BertForEntityTyping", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "state_dict", "=", "model_state_dict", ",", "num_labels", "=", "len", "(", "label_list", ")", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "mark", ":", "\n", "            ", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ")", "\n", "", "else", ":", "\n", "            ", "eval_examples", "=", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ")", "\n", "", "eval_features", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer_label", ",", "tokenizer", ",", "args", ".", "threshold", ")", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "# zeros = [0 for _ in range(args.max_seq_length)]", "\n", "# zeros_ent = [0 for _ in range(100)]", "\n", "# zeros_ent = [zeros_ent for _ in range(args.max_seq_length)]", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_ent", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ent", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ent_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_labels", "=", "torch", ".", "tensor", "(", "[", "f", ".", "labels", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_input_ent", ",", "all_ent_mask", ",", "all_labels", ")", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", ",", "eval_accuracy", "=", "0", ",", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "pred", "=", "[", "]", "\n", "true", "=", "[", "]", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "labels", "in", "eval_dataloader", ":", "\n", "            ", "input_ent", "=", "embed", "(", "input_ent", "+", "1", ")", "\n", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "input_ent", "=", "input_ent", ".", "to", "(", "device", ")", "\n", "ent_mask", "=", "ent_mask", ".", "to", "(", "device", ")", "\n", "labels", "=", "labels", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "tmp_eval_loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ",", "ent_mask", ",", "labels", ")", "\n", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ",", "ent_mask", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "labels", "=", "labels", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "tmp_eval_accuracy", ",", "tmp_pred", ",", "tmp_true", "=", "accuracy", "(", "logits", ",", "labels", ")", "\n", "pred", ".", "extend", "(", "tmp_pred", ")", "\n", "true", ".", "extend", "(", "tmp_true", ")", "\n", "\n", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "eval_accuracy", "+=", "tmp_eval_accuracy", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "eval_accuracy", "=", "eval_accuracy", "/", "nb_eval_examples", "\n", "\n", "def", "f1", "(", "p", ",", "r", ")", ":", "\n", "            ", "if", "r", "==", "0.", ":", "\n", "                ", "return", "0.", "\n", "", "return", "2", "*", "p", "*", "r", "/", "float", "(", "p", "+", "r", ")", "\n", "", "def", "loose_macro", "(", "true", ",", "pred", ")", ":", "\n", "            ", "num_entities", "=", "len", "(", "true", ")", "\n", "p", "=", "0.", "\n", "r", "=", "0.", "\n", "for", "true_labels", ",", "predicted_labels", "in", "zip", "(", "true", ",", "pred", ")", ":", "\n", "                ", "if", "len", "(", "predicted_labels", ")", ">", "0", ":", "\n", "                    ", "p", "+=", "len", "(", "set", "(", "predicted_labels", ")", ".", "intersection", "(", "set", "(", "true_labels", ")", ")", ")", "/", "float", "(", "len", "(", "predicted_labels", ")", ")", "\n", "", "if", "len", "(", "true_labels", ")", ":", "\n", "                    ", "r", "+=", "len", "(", "set", "(", "predicted_labels", ")", ".", "intersection", "(", "set", "(", "true_labels", ")", ")", ")", "/", "float", "(", "len", "(", "true_labels", ")", ")", "\n", "", "", "precision", "=", "p", "/", "num_entities", "\n", "recall", "=", "r", "/", "num_entities", "\n", "return", "precision", ",", "recall", ",", "f1", "(", "precision", ",", "recall", ")", "\n", "", "def", "loose_micro", "(", "true", ",", "pred", ")", ":", "\n", "            ", "num_predicted_labels", "=", "0.", "\n", "num_true_labels", "=", "0.", "\n", "num_correct_labels", "=", "0.", "\n", "for", "true_labels", ",", "predicted_labels", "in", "zip", "(", "true", ",", "pred", ")", ":", "\n", "                ", "num_predicted_labels", "+=", "len", "(", "predicted_labels", ")", "\n", "num_true_labels", "+=", "len", "(", "true_labels", ")", "\n", "num_correct_labels", "+=", "len", "(", "set", "(", "predicted_labels", ")", ".", "intersection", "(", "set", "(", "true_labels", ")", ")", ")", "\n", "", "if", "num_predicted_labels", ">", "0", ":", "\n", "                ", "precision", "=", "num_correct_labels", "/", "num_predicted_labels", "\n", "", "else", ":", "\n", "                ", "precision", "=", "0.", "\n", "", "recall", "=", "num_correct_labels", "/", "num_true_labels", "\n", "return", "precision", ",", "recall", ",", "f1", "(", "precision", ",", "recall", ")", "\n", "\n", "\n", "", "result", "=", "{", "'eval_loss'", ":", "eval_loss", ",", "\n", "'eval_accuracy'", ":", "eval_accuracy", ",", "\n", "'macro'", ":", "loose_macro", "(", "true", ",", "pred", ")", ",", "\n", "'micro'", ":", "loose_micro", "(", "true", ",", "pred", ")", "\n", "}", "\n", "\n", "if", "mark", ":", "\n", "            ", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"eval_results_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"test_results_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_pretrain.accuracy": [[44, 47], ["numpy.argmax", "numpy.sum"], "function", ["None"], ["def", "accuracy", "(", "out", ",", "labels", ")", ":", "\n", "    ", "outputs", "=", "np", ".", "argmax", "(", "out", ",", "axis", "=", "1", ")", "\n", "return", "np", ".", "sum", "(", "outputs", "==", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_pretrain.warmup_linear": [[48, 52], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_pretrain.main": [[53, 356], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "os.makedirs", "parser.parse_args.task_name.lower", "vecs.append", "torch.FloatTensor", "torch.nn.Embedding.from_pretrained", "logger.info", "knowledge_bert.modeling.BertForPreTraining.from_pretrained", "torch.nn.DataParallel.to", "list", "os.path.join", "torch.save", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "os.path.exists", "os.listdir", "ValueError", "open", "indexed_dataset.IndexedDataset", "BatchSampler", "iterators.EpochBatchIterator", "int", "torch.nn.DataParallel.half", "DDP", "torch.nn.DataParallel.named_parameters", "torch.LongTensor.replace", "FusedAdam", "knowledge_bert.optimization.BertAdam", "logger.info", "logger.info", "logger.info", "logger.info", "torch.nn.DataParallel.train", "open", "tqdm.trange", "open.close", "hasattr", "model_to_save.state_dict", "bool", "line.strip().split", "vecs.append", "str", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "torch.LongTensor", "numpy.unique", "torch.nn.Embedding.from_pretrained.", "ent_candidate.repeat.repeat", "enumerate", "entity_idx.clone", "ent_labels.apply_.apply_", "entity_idx.apply_", "torch.nn.Embedding.from_pretrained.", "entity_idx.clone", "entity_idx.clone.apply_", "torch.nn.DataParallel", "torch.distributed.get_world_size", "FP16_Optimizer", "FP16_Optimizer", "len", "os.path.join", "int", "enumerate", "float", "torch.nn.Embedding.from_pretrained.weight.size", "entity_idx.numpy", "torch.LongTensor", "dd.append", "len", "ImportError", "any", "ImportError", "tqdm.tqdm", "tuple", "open.write", "loss.mean.item", "input_ids.size", "torch.cuda.is_available", "line.strip", "random.uniform", "any", "datetime.datetime.now", "iterators.EpochBatchIterator.next_epoch_itr", "torch.nn.DataParallel.", "torch.nn.DataParallel.", "loss.mean.mean", "original_loss.mean.mean", "FP16_Optimizer.backward", "loss.mean.backward", "FP16_Optimizer.step", "FP16_Optimizer.zero_grad", "len", "any", "t.to", "input_ent.half", "ent_candidate.repeat.half", "original_loss.mean.item", "run_pretrain.warmup_linear", "loss.mean.item", "random.randint"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.next_epoch_itr", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.BertAdam.step", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.warmup_linear"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--bert_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Bert pre-trained model selected in the list: bert-base-uncased, \"", "\n", "\"bert-large-uncased, bert-base-cased, bert-base-multilingual, bert-base-chinese.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--task_name\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The name of the task to train.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", ":", "\n", "        ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "task_name", "=", "args", ".", "task_name", ".", "lower", "(", ")", "\n", "\n", "vecs", "=", "[", "]", "\n", "vecs", ".", "append", "(", "[", "0", "]", "*", "100", ")", "# CLS", "\n", "with", "open", "(", "\"kg_embed/entity2vec.vec\"", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "vec", "=", "[", "float", "(", "x", ")", "for", "x", "in", "vec", "]", "\n", "vecs", ".", "append", "(", "vec", ")", "\n", "", "", "embed", "=", "torch", ".", "FloatTensor", "(", "vecs", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embed", ")", "\n", "#embed = torch.nn.Embedding(5041175, 100)", "\n", "\n", "logger", ".", "info", "(", "\"Shape of entity embedding: \"", "+", "str", "(", "embed", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "del", "vecs", "\n", "\n", "train_data", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "if", "args", ".", "do_train", ":", "\n", "# TODO", "\n", "        ", "import", "indexed_dataset", "\n", "from", "torch", ".", "utils", ".", "data", "import", "TensorDataset", ",", "DataLoader", ",", "RandomSampler", ",", "SequentialSampler", ",", "BatchSampler", "\n", "import", "iterators", "\n", "#train_data = indexed_dataset.IndexedCachedDataset(args.data_dir)", "\n", "train_data", "=", "indexed_dataset", ".", "IndexedDataset", "(", "args", ".", "data_dir", ",", "fix_lua_indexing", "=", "True", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_sampler", "=", "BatchSampler", "(", "train_sampler", ",", "args", ".", "train_batch_size", ",", "True", ")", "\n", "def", "collate_fn", "(", "x", ")", ":", "\n", "            ", "x", "=", "torch", ".", "LongTensor", "(", "[", "xx", "for", "xx", "in", "x", "]", ")", "\n", "\n", "entity_idx", "=", "x", "[", ":", ",", "4", "*", "args", ".", "max_seq_length", ":", "5", "*", "args", ".", "max_seq_length", "]", "\n", "# Build candidate", "\n", "uniq_idx", "=", "np", ".", "unique", "(", "entity_idx", ".", "numpy", "(", ")", ")", "\n", "ent_candidate", "=", "embed", "(", "torch", ".", "LongTensor", "(", "uniq_idx", "+", "1", ")", ")", "\n", "ent_candidate", "=", "ent_candidate", ".", "repeat", "(", "[", "n_gpu", ",", "1", "]", ")", "\n", "# build entity labels", "\n", "d", "=", "{", "}", "\n", "dd", "=", "[", "]", "\n", "for", "i", ",", "idx", "in", "enumerate", "(", "uniq_idx", ")", ":", "\n", "                ", "d", "[", "idx", "]", "=", "i", "\n", "dd", ".", "append", "(", "idx", ")", "\n", "", "ent_size", "=", "len", "(", "uniq_idx", ")", "-", "1", "\n", "def", "map", "(", "x", ")", ":", "\n", "                ", "if", "x", "==", "-", "1", ":", "\n", "                    ", "return", "-", "1", "\n", "", "else", ":", "\n", "                    ", "rnd", "=", "random", ".", "uniform", "(", "0", ",", "1", ")", "\n", "if", "rnd", "<", "0.05", ":", "\n", "                        ", "return", "dd", "[", "random", ".", "randint", "(", "1", ",", "ent_size", ")", "]", "\n", "", "elif", "rnd", "<", "0.2", ":", "\n", "                        ", "return", "-", "1", "\n", "", "else", ":", "\n", "                        ", "return", "x", "\n", "", "", "", "ent_labels", "=", "entity_idx", ".", "clone", "(", ")", "\n", "d", "[", "-", "1", "]", "=", "-", "1", "\n", "ent_labels", "=", "ent_labels", ".", "apply_", "(", "lambda", "x", ":", "d", "[", "x", "]", ")", "\n", "\n", "entity_idx", ".", "apply_", "(", "map", ")", "\n", "ent_emb", "=", "embed", "(", "entity_idx", "+", "1", ")", "\n", "mask", "=", "entity_idx", ".", "clone", "(", ")", "\n", "mask", ".", "apply_", "(", "lambda", "x", ":", "0", "if", "x", "==", "-", "1", "else", "1", ")", "\n", "mask", "[", ":", ",", "0", "]", "=", "1", "\n", "\n", "return", "x", "[", ":", ",", ":", "args", ".", "max_seq_length", "]", ",", "x", "[", ":", ",", "args", ".", "max_seq_length", ":", "2", "*", "args", ".", "max_seq_length", "]", ",", "x", "[", ":", ",", "2", "*", "args", ".", "max_seq_length", ":", "3", "*", "args", ".", "max_seq_length", "]", ",", "x", "[", ":", ",", "3", "*", "args", ".", "max_seq_length", ":", "4", "*", "args", ".", "max_seq_length", "]", ",", "ent_emb", ",", "mask", ",", "x", "[", ":", ",", "6", "*", "args", ".", "max_seq_length", ":", "]", ",", "ent_candidate", ",", "ent_labels", "\n", "", "train_iterator", "=", "iterators", ".", "EpochBatchIterator", "(", "train_data", ",", "collate_fn", ",", "train_sampler", ")", "\n", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_data", ")", "/", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", ")", "\n", "\n", "# Prepare model", "\n", "", "model", ",", "missing_keys", "=", "BertForPreTraining", ".", "from_pretrained", "(", "args", ".", "bert_model", ",", "\n", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "/", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_linear", "=", "[", "'layer.2.output.dense_ent'", ",", "'layer.2.intermediate.dense_1'", ",", "'bert.encoder.layer.2.intermediate.dense_1_ent'", ",", "'layer.2.output.LayerNorm_ent'", "]", "\n", "no_linear", "=", "[", "x", ".", "replace", "(", "'2'", ",", "'11'", ")", "for", "x", "in", "no_linear", "]", "\n", "param_optimizer", "=", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nl", "in", "n", "for", "nl", "in", "no_linear", ")", "]", "\n", "#param_optimizer = [(n, p) for n, p in param_optimizer if not any(nl in n for nl in missing_keys)]", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", ",", "'LayerNorm_ent.bias'", ",", "'LayerNorm_ent.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "t_total", "=", "num_train_steps", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "t_total", "=", "t_total", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "#logger.info(dir(optimizer))", "\n", "#op_path = os.path.join(args.bert_model, \"pytorch_op.bin\")", "\n", "#optimizer.load_state_dict(torch.load(op_path))", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "t_total", ")", "\n", "\n", "", "global_step", "=", "0", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_data", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "model", ".", "train", "(", ")", "\n", "import", "datetime", "\n", "fout", "=", "open", "(", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"loss.{}\"", ".", "format", "(", "datetime", ".", "datetime", ".", "now", "(", ")", ")", ")", ",", "'w'", ")", "\n", "for", "_", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "\n", "nb_tr_examples", ",", "nb_tr_steps", "=", "0", ",", "0", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_iterator", ".", "next_epoch_itr", "(", ")", ",", "desc", "=", "\"Iteration\"", ")", ")", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "for", "t", "in", "batch", ")", "\n", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "masked_lm_labels", ",", "input_ent", ",", "ent_mask", ",", "next_sentence_label", ",", "ent_candidate", ",", "ent_labels", "=", "batch", "\n", "if", "args", ".", "fp16", ":", "\n", "                    ", "loss", ",", "original_loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "masked_lm_labels", ",", "input_ent", ".", "half", "(", ")", ",", "ent_mask", ",", "next_sentence_label", ",", "ent_candidate", ".", "half", "(", ")", ",", "ent_labels", ")", "\n", "", "else", ":", "\n", "                    ", "loss", ",", "original_loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "masked_lm_labels", ",", "input_ent", ",", "ent_mask", ",", "next_sentence_label", ",", "ent_candidate", ",", "ent_labels", ")", "\n", "\n", "\n", "", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "original_loss", "=", "original_loss", ".", "mean", "(", ")", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "fout", ".", "write", "(", "\"{} {}\\n\"", ".", "format", "(", "loss", ".", "item", "(", ")", "*", "args", ".", "gradient_accumulation_steps", ",", "original_loss", ".", "item", "(", ")", ")", ")", "\n", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "                    ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                        ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "#if global_step % 1000 == 0:", "\n", "#    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self", "\n", "#    output_model_file = os.path.join(args.output_dir, \"pytorch_model.bin_{}\".format(global_step))", "\n", "#    torch.save(model_to_save.state_dict(), output_model_file)", "\n", "", "", "", "fout", ".", "close", "(", ")", "\n", "\n", "# Save a trained model", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"pytorch_model.bin\"", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.InputExample.__init__": [[48, 64], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.InputFeatures.__init__": [[69, 76], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "label_id", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "input_ent", "=", "input_ent", "\n", "self", ".", "ent_mask", "=", "ent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.DataProcessor.get_train_examples": [[81, 84], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.DataProcessor.get_dev_examples": [[85, 88], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.DataProcessor.get_labels": [[89, 92], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.DataProcessor._read_json": [[93, 97], ["open", "simplejson.loads", "f.read"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "@", "classmethod", "\n", "def", "_read_json", "(", "cls", ",", "input_file", ")", ":", "\n", "        ", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "return", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.FewrelProcessor.get_train_examples": [[101, 107], ["run_fewrel.FewrelProcessor._create_examples", "set", "run_fewrel.FewrelProcessor._read_json", "list", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "examples", "=", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ",", "\"train\"", ")", "\n", "labels", "=", "set", "(", "[", "x", ".", "label", "for", "x", "in", "examples", "]", ")", "\n", "return", "examples", ",", "list", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.FewrelProcessor.get_dev_examples": [[108, 112], ["run_fewrel.FewrelProcessor._create_examples", "run_fewrel.FewrelProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.json\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.FewrelProcessor.get_labels": [[113, 116], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Useless\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.FewrelProcessor._create_examples": [[117, 130], ["enumerate", "examples.append", "run_fewrel.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "for", "x", "in", "line", "[", "'ents'", "]", ":", "\n", "                ", "if", "x", "[", "1", "]", "==", "1", ":", "\n", "                    ", "x", "[", "1", "]", "=", "0", "\n", "", "", "text_a", "=", "(", "line", "[", "'text'", "]", ",", "line", "[", "'ents'", "]", ")", "\n", "label", "=", "line", "[", "'label'", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.convert_examples_to_features": [[132, 265], ["sorted", "enumerate", "open", "fin.readline", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "line.strip().split", "int", "len", "print", "exit", "tokenizer.tokenize", "run_fewrel._truncate_seq_pair", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "run_fewrel.InputFeatures", "len", "len", "input_ent.append", "ent_mask.append", "input_ent.append", "ent_mask.append", "len", "len", "line.strip", "len", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred._truncate_seq_pair"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "label_list", "=", "sorted", "(", "label_list", ")", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "entity2id", "=", "{", "}", "\n", "with", "open", "(", "\"kg_embed/entity2id.txt\"", ")", "as", "fin", ":", "\n", "        ", "fin", ".", "readline", "(", ")", "\n", "for", "line", "in", "fin", ":", "\n", "            ", "qid", ",", "eid", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "entity2id", "[", "qid", "]", "=", "int", "(", "eid", ")", "\n", "\n", "", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "ex_text_a", "=", "example", ".", "text_a", "[", "0", "]", "\n", "h", ",", "t", "=", "example", ".", "text_a", "[", "1", "]", "\n", "h_name", "=", "ex_text_a", "[", "h", "[", "1", "]", ":", "h", "[", "2", "]", "]", "\n", "t_name", "=", "ex_text_a", "[", "t", "[", "1", "]", ":", "t", "[", "2", "]", "]", "\n", "# Add [HD] and [TL], which are \"#\" and \"$\" respectively.", "\n", "if", "h", "[", "1", "]", "<", "t", "[", "1", "]", ":", "\n", "            ", "ex_text_a", "=", "ex_text_a", "[", ":", "h", "[", "1", "]", "]", "+", "\"# \"", "+", "h_name", "+", "\" #\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "t", "[", "1", "]", "]", "+", "\"$ \"", "+", "t_name", "+", "\" $\"", "+", "ex_text_a", "[", "t", "[", "2", "]", ":", "]", "\n", "", "else", ":", "\n", "            ", "ex_text_a", "=", "ex_text_a", "[", ":", "t", "[", "1", "]", "]", "+", "\"$ \"", "+", "t_name", "+", "\" $\"", "+", "ex_text_a", "[", "t", "[", "2", "]", ":", "h", "[", "1", "]", "]", "+", "\"# \"", "+", "h_name", "+", "\" #\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "]", "\n", "\n", "", "if", "h", "[", "1", "]", "<", "t", "[", "1", "]", ":", "\n", "            ", "h", "[", "1", "]", "+=", "2", "\n", "h", "[", "2", "]", "+=", "2", "\n", "t", "[", "1", "]", "+=", "6", "\n", "t", "[", "2", "]", "+=", "6", "\n", "", "else", ":", "\n", "            ", "h", "[", "1", "]", "+=", "6", "\n", "h", "[", "2", "]", "+=", "6", "\n", "t", "[", "1", "]", "+=", "2", "\n", "t", "[", "2", "]", "+=", "2", "\n", "", "tokens_a", ",", "entities_a", "=", "tokenizer", ".", "tokenize", "(", "ex_text_a", ",", "[", "h", ",", "t", "]", ")", "\n", "if", "len", "(", "[", "x", "for", "x", "in", "entities_a", "if", "x", "!=", "\"UNK\"", "]", ")", "!=", "2", ":", "\n", "            ", "print", "(", "entities_a", ",", "len", "(", "[", "x", "for", "x", "in", "entities_a", "if", "x", "[", "0", "]", "!=", "\"UNK\"", "]", ")", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "tokens_b", "=", "None", "\n", "if", "example", ".", "text_b", ":", "\n", "            ", "tokens_b", ",", "entities_b", "=", "tokenizer", ".", "tokenize", "(", "example", ".", "text_b", "[", "0", "]", ",", "[", "x", "for", "x", "in", "example", ".", "text_b", "[", "1", "]", "if", "x", "[", "-", "1", "]", ">", "threshold", "]", ")", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "entities_a", ",", "entities_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities_a", "=", "entities_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "=", "[", "\"UNK\"", "]", "+", "entities_a", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "tokens", "+=", "tokens_b", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "+=", "entities_b", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "+=", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_ent", "=", "[", "]", "\n", "ent_mask", "=", "[", "]", "\n", "for", "ent", "in", "ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", "and", "ent", "in", "entity2id", ":", "\n", "                ", "input_ent", ".", "append", "(", "entity2id", "[", "ent", "]", ")", "\n", "ent_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "input_ent", ".", "append", "(", "-", "1", ")", "\n", "ent_mask", ".", "append", "(", "0", ")", "\n", "", "", "ent_mask", "[", "0", "]", "=", "1", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "padding_", "=", "[", "-", "1", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "input_ent", "+=", "padding_", "\n", "ent_mask", "+=", "padding", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_ent", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "ent_mask", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "if", "ex_index", "<", "5", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"ents: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "ents", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "input_ent", "=", "input_ent", ",", "\n", "ent_mask", "=", "ent_mask", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel._truncate_seq_pair": [[267, 284], ["len", "len", "len", "len", "tokens_a.pop", "ents_a.pop", "tokens_b.pop", "ents_b.pop"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "ents_a", ",", "ents_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "ents_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "ents_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.accuracy": [[285, 288], ["numpy.argmax", "numpy.sum"], "function", ["None"], ["", "", "", "def", "accuracy", "(", "out", ",", "labels", ")", ":", "\n", "    ", "outputs", "=", "np", ".", "argmax", "(", "out", ",", "axis", "=", "1", ")", "\n", "return", "np", ".", "sum", "(", "outputs", "==", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.warmup_linear": [[289, 293], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.run_fewrel.main": [[294, 552], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "int", "random.seed", "numpy.random.seed", "torch.manual_seed", "os.makedirs", "processors", "knowledge_bert.tokenization.BertTokenizer.from_pretrained", "processors.get_train_examples", "int", "knowledge_bert.modeling.BertForSequenceClassification.from_pretrained", "torch.nn.DataParallel.to", "list", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "os.path.exists", "os.listdir", "ValueError", "torch.nn.DataParallel.half", "DDP", "torch.nn.DataParallel.named_parameters", "FusedAdam", "knowledge_bert.optimization.BertAdam", "run_fewrel.convert_examples_to_features", "vecs.append", "torch.FloatTensor", "torch.nn.Embedding.from_pretrained", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.DataLoader", "os.path.join", "open", "torch.nn.DataParallel.train", "tqdm.trange", "os.path.join", "torch.save", "bool", "torch.nn.DataParallel", "torch.distributed.get_world_size", "FP16_Optimizer", "FP16_Optimizer", "open", "len", "torch.utils.data.RandomSampler", "torch.utils.data.distributed.DistributedSampler", "int", "enumerate", "os.path.join", "torch.save", "hasattr", "model_to_save.state_dict", "ImportError", "any", "ImportError", "line.strip().split", "vecs.append", "str", "tqdm.tqdm", "tuple", "torch.nn.Embedding.from_pretrained.to", "torch.nn.DataParallel.", "open.write", "loss.mean.item", "input_ids.size", "hasattr", "model_to_save.state_dict", "torch.cuda.is_available", "len", "any", "float", "torch.nn.Embedding.from_pretrained.weight.size", "embed().to.half", "loss.mean.mean", "FP16_Optimizer.backward", "loss.mean.backward", "FP16_Optimizer.step", "FP16_Optimizer.zero_grad", "any", "line.strip", "torch.nn.Embedding.from_pretrained.", "loss.mean.item", "run_fewrel.warmup_linear", "t.to", "enumerate"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_train_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.state_dict", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.BertAdam.step", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.warmup_linear"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "## Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ernie_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Ernie pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "## Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold'", ",", "type", "=", "float", ",", "default", "=", ".3", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "processors", "=", "FewrelProcessor", "\n", "\n", "num_labels_task", "=", "80", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "args", ".", "train_batch_size", "=", "int", "(", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", ")", "\n", "\n", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", ":", "\n", "        ", "raise", "ValueError", "(", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "if", "os", ".", "path", ".", "exists", "(", "args", ".", "output_dir", ")", "and", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "and", "args", ".", "do_train", ":", "\n", "        ", "raise", "ValueError", "(", "\"Output directory ({}) already exists and is not empty.\"", ".", "format", "(", "args", ".", "output_dir", ")", ")", "\n", "", "os", ".", "makedirs", "(", "args", ".", "output_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "\n", "processor", "=", "processors", "(", ")", "\n", "num_labels", "=", "num_labels_task", "\n", "label_list", "=", "None", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "train_examples", ",", "label_list", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "num_train_steps", "=", "int", "(", "\n", "len", "(", "train_examples", ")", "/", "args", ".", "train_batch_size", "/", "args", ".", "gradient_accumulation_steps", "*", "args", ".", "num_train_epochs", ")", "\n", "\n", "# Prepare model", "\n", "model", ",", "_", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "ernie_model", ",", "\n", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "/", "'distributed_{}'", ".", "format", "(", "args", ".", "local_rank", ")", ",", "\n", "num_labels", "=", "num_labels", ")", "\n", "if", "args", ".", "fp16", ":", "\n", "        ", "model", ".", "half", "(", ")", "\n", "", "model", ".", "to", "(", "device", ")", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "parallel", "import", "DistributedDataParallel", "as", "DDP", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "model", "=", "DDP", "(", "model", ")", "\n", "", "elif", "n_gpu", ">", "1", ":", "\n", "        ", "model", "=", "torch", ".", "nn", ".", "DataParallel", "(", "model", ")", "\n", "\n", "# Prepare optimizer", "\n", "", "param_optimizer", "=", "list", "(", "model", ".", "named_parameters", "(", ")", ")", "\n", "no_grad", "=", "[", "'bert.encoder.layer.11.output.dense_ent'", ",", "'bert.encoder.layer.11.output.LayerNorm_ent'", "]", "\n", "param_optimizer", "=", "[", "(", "n", ",", "p", ")", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_grad", ")", "]", "\n", "no_decay", "=", "[", "'bias'", ",", "'LayerNorm.bias'", ",", "'LayerNorm.weight'", "]", "\n", "optimizer_grouped_parameters", "=", "[", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "not", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.01", "}", ",", "\n", "{", "'params'", ":", "[", "p", "for", "n", ",", "p", "in", "param_optimizer", "if", "any", "(", "nd", "in", "n", "for", "nd", "in", "no_decay", ")", "]", ",", "'weight_decay'", ":", "0.0", "}", "\n", "]", "\n", "t_total", "=", "num_train_steps", "\n", "if", "args", ".", "local_rank", "!=", "-", "1", ":", "\n", "        ", "t_total", "=", "t_total", "//", "torch", ".", "distributed", ".", "get_world_size", "(", ")", "\n", "", "if", "args", ".", "fp16", ":", "\n", "        ", "try", ":", "\n", "            ", "from", "apex", ".", "optimizers", "import", "FP16_Optimizer", "\n", "from", "apex", ".", "optimizers", "import", "FusedAdam", "\n", "", "except", "ImportError", ":", "\n", "            ", "raise", "ImportError", "(", "\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\"", ")", "\n", "\n", "", "optimizer", "=", "FusedAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "bias_correction", "=", "False", ",", "\n", "max_grad_norm", "=", "1.0", ")", "\n", "if", "args", ".", "loss_scale", "==", "0", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "dynamic_loss_scale", "=", "True", ")", "\n", "", "else", ":", "\n", "            ", "optimizer", "=", "FP16_Optimizer", "(", "optimizer", ",", "static_loss_scale", "=", "args", ".", "loss_scale", ")", "\n", "\n", "", "", "else", ":", "\n", "        ", "optimizer", "=", "BertAdam", "(", "optimizer_grouped_parameters", ",", "\n", "lr", "=", "args", ".", "learning_rate", ",", "\n", "warmup", "=", "args", ".", "warmup_proportion", ",", "\n", "t_total", "=", "t_total", ")", "\n", "", "global_step", "=", "0", "\n", "if", "args", ".", "do_train", ":", "\n", "        ", "train_features", "=", "convert_examples_to_features", "(", "\n", "train_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ",", "args", ".", "threshold", ")", "\n", "\n", "vecs", "=", "[", "]", "\n", "vecs", ".", "append", "(", "[", "0", "]", "*", "100", ")", "\n", "with", "open", "(", "\"kg_embed/entity2vec.vec\"", ",", "'r'", ")", "as", "fin", ":", "\n", "            ", "for", "line", "in", "fin", ":", "\n", "                ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "vec", "=", "[", "float", "(", "x", ")", "for", "x", "in", "vec", "]", "\n", "vecs", ".", "append", "(", "vec", ")", "\n", "", "", "embed", "=", "torch", ".", "FloatTensor", "(", "vecs", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embed", ")", "\n", "#embed = torch.nn.Embedding(5041175, 100)", "\n", "\n", "logger", ".", "info", "(", "\"Shape of entity embedding: \"", "+", "str", "(", "embed", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "del", "vecs", "\n", "\n", "\n", "logger", ".", "info", "(", "\"***** Running training *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "train_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "train_batch_size", ")", "\n", "logger", ".", "info", "(", "\"  Num steps = %d\"", ",", "num_train_steps", ")", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "segment_ids", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "[", "f", ".", "label_id", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent", "=", "torch", ".", "tensor", "(", "[", "f", ".", "input_ent", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent_masks", "=", "torch", ".", "tensor", "(", "[", "f", ".", "ent_mask", "for", "f", "in", "train_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "train_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "all_segment_ids", ",", "all_ent", ",", "all_ent_masks", ",", "all_label_ids", ")", "\n", "if", "args", ".", "local_rank", "==", "-", "1", ":", "\n", "            ", "train_sampler", "=", "RandomSampler", "(", "train_data", ")", "\n", "", "else", ":", "\n", "            ", "train_sampler", "=", "DistributedSampler", "(", "train_data", ")", "\n", "", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "sampler", "=", "train_sampler", ",", "batch_size", "=", "args", ".", "train_batch_size", ")", "\n", "\n", "output_loss_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"loss\"", ")", "\n", "loss_fout", "=", "open", "(", "output_loss_file", ",", "'w'", ")", "\n", "model", ".", "train", "(", ")", "\n", "for", "_", "in", "trange", "(", "int", "(", "args", ".", "num_train_epochs", ")", ",", "desc", "=", "\"Epoch\"", ")", ":", "\n", "            ", "tr_loss", "=", "0", "\n", "nb_tr_examples", ",", "nb_tr_steps", "=", "0", ",", "0", "\n", "for", "step", ",", "batch", "in", "enumerate", "(", "tqdm", "(", "train_dataloader", ",", "desc", "=", "\"Iteration\"", ")", ")", ":", "\n", "                ", "batch", "=", "tuple", "(", "t", ".", "to", "(", "device", ")", "if", "i", "!=", "3", "else", "t", "for", "i", ",", "t", "in", "enumerate", "(", "batch", ")", ")", "\n", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "label_ids", "=", "batch", "\n", "input_ent", "=", "embed", "(", "input_ent", "+", "1", ")", ".", "to", "(", "device", ")", "# -1 -> 0", "\n", "loss", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ".", "half", "(", ")", ",", "ent_mask", ",", "label_ids", ")", "\n", "if", "n_gpu", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", ".", "mean", "(", ")", "# mean() to average on multi-gpu.", "\n", "", "if", "args", ".", "gradient_accumulation_steps", ">", "1", ":", "\n", "                    ", "loss", "=", "loss", "/", "args", ".", "gradient_accumulation_steps", "\n", "\n", "", "if", "args", ".", "fp16", ":", "\n", "                    ", "optimizer", ".", "backward", "(", "loss", ")", "\n", "", "else", ":", "\n", "                    ", "loss", ".", "backward", "(", ")", "\n", "\n", "", "loss_fout", ".", "write", "(", "\"{}\\n\"", ".", "format", "(", "loss", ".", "item", "(", ")", ")", ")", "\n", "tr_loss", "+=", "loss", ".", "item", "(", ")", "\n", "nb_tr_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_tr_steps", "+=", "1", "\n", "if", "(", "step", "+", "1", ")", "%", "args", ".", "gradient_accumulation_steps", "==", "0", ":", "\n", "# modify learning rate with special warm up BERT uses", "\n", "                    ", "lr_this_step", "=", "args", ".", "learning_rate", "*", "warmup_linear", "(", "global_step", "/", "t_total", ",", "args", ".", "warmup_proportion", ")", "\n", "for", "param_group", "in", "optimizer", ".", "param_groups", ":", "\n", "                        ", "param_group", "[", "'lr'", "]", "=", "lr_this_step", "\n", "", "optimizer", ".", "step", "(", ")", "\n", "optimizer", ".", "zero_grad", "(", ")", "\n", "global_step", "+=", "1", "\n", "", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"pytorch_model.bin_{}\"", ".", "format", "(", "global_step", ")", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n", "# Save a trained model", "\n", "", "model_to_save", "=", "model", ".", "module", "if", "hasattr", "(", "model", ",", "'module'", ")", "else", "model", "# Only save the model it-self", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "\"pytorch_model.bin\"", ")", "\n", "torch", ".", "save", "(", "model_to_save", ".", "state_dict", "(", ")", ",", "output_model_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.score.parse_arguments": [[13, 19], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args"], "function", ["None"], ["def", "parse_arguments", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Score a prediction file using the gold labels.'", ")", "\n", "parser", ".", "add_argument", "(", "'gold_file'", ",", "help", "=", "'The gold relation file; one relation per line'", ")", "\n", "parser", ".", "add_argument", "(", "'pred_file'", ",", "help", "=", "'A prediction file; one relation per line, in the same order as the gold file.'", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.score.score": [[20, 97], ["collections.Counter", "collections.Counter", "collections.Counter", "range", "print", "print", "print", "len", "print", "collections.Counter.keys", "sorted", "sorted", "print", "print", "sum", "sum", "max", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "collections.Counter.values", "float", "float", "collections.Counter.values", "float", "float", "len", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sys.stdout.write", "sum", "sum", "sum", "sum", "float", "float", "float", "float", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "collections.Counter.values", "str"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "def", "score", "(", "key", ",", "prediction", ",", "verbose", "=", "False", ")", ":", "\n", "    ", "correct_by_relation", "=", "Counter", "(", ")", "\n", "guessed_by_relation", "=", "Counter", "(", ")", "\n", "gold_by_relation", "=", "Counter", "(", ")", "\n", "\n", "# Loop over the data to compute a score", "\n", "for", "row", "in", "range", "(", "len", "(", "key", ")", ")", ":", "\n", "        ", "gold", "=", "key", "[", "row", "]", "\n", "guess", "=", "prediction", "[", "row", "]", "\n", "\n", "if", "gold", "==", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "pass", "\n", "", "elif", "gold", "==", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "==", "NO_RELATION", ":", "\n", "            ", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "", "elif", "gold", "!=", "NO_RELATION", "and", "guess", "!=", "NO_RELATION", ":", "\n", "            ", "guessed_by_relation", "[", "guess", "]", "+=", "1", "\n", "gold_by_relation", "[", "gold", "]", "+=", "1", "\n", "if", "gold", "==", "guess", ":", "\n", "                ", "correct_by_relation", "[", "guess", "]", "+=", "1", "\n", "\n", "# Print verbose information", "\n", "", "", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Per-relation statistics:\"", ")", "\n", "relations", "=", "gold_by_relation", ".", "keys", "(", ")", "\n", "longest_relation", "=", "0", "\n", "for", "relation", "in", "sorted", "(", "relations", ")", ":", "\n", "            ", "longest_relation", "=", "max", "(", "len", "(", "relation", ")", ",", "longest_relation", ")", "\n", "", "for", "relation", "in", "sorted", "(", "relations", ")", ":", "\n", "# (compute the score)", "\n", "            ", "correct", "=", "correct_by_relation", "[", "relation", "]", "\n", "guessed", "=", "guessed_by_relation", "[", "relation", "]", "\n", "gold", "=", "gold_by_relation", "[", "relation", "]", "\n", "prec", "=", "1.0", "\n", "if", "guessed", ">", "0", ":", "\n", "                ", "prec", "=", "float", "(", "correct", ")", "/", "float", "(", "guessed", ")", "\n", "", "recall", "=", "0.0", "\n", "if", "gold", ">", "0", ":", "\n", "                ", "recall", "=", "float", "(", "correct", ")", "/", "float", "(", "gold", ")", "\n", "", "f1", "=", "0.0", "\n", "if", "prec", "+", "recall", ">", "0", ":", "\n", "                ", "f1", "=", "2.0", "*", "prec", "*", "recall", "/", "(", "prec", "+", "recall", ")", "\n", "# (print the score)", "\n", "", "sys", ".", "stdout", ".", "write", "(", "(", "\"{:<\"", "+", "str", "(", "longest_relation", ")", "+", "\"}\"", ")", ".", "format", "(", "relation", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  P: \"", ")", "\n", "if", "prec", "<", "0.1", ":", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "if", "prec", "<", "1.0", ":", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"{:.2%}\"", ".", "format", "(", "prec", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  R: \"", ")", "\n", "if", "recall", "<", "0.1", ":", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "if", "recall", "<", "1.0", ":", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"{:.2%}\"", ".", "format", "(", "recall", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  F1: \"", ")", "\n", "if", "f1", "<", "0.1", ":", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "if", "f1", "<", "1.0", ":", "sys", ".", "stdout", ".", "write", "(", "' '", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"{:.2%}\"", ".", "format", "(", "f1", ")", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"  #: %d\"", "%", "gold", ")", "\n", "sys", ".", "stdout", ".", "write", "(", "\"\\n\"", ")", "\n", "", "print", "(", "\"\"", ")", "\n", "\n", "# Print the aggregate score", "\n", "", "if", "verbose", ":", "\n", "        ", "print", "(", "\"Final Score:\"", ")", "\n", "", "prec_micro", "=", "1.0", "\n", "if", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "prec_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "sum", "(", "guessed_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "recall_micro", "=", "0.0", "\n", "if", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ">", "0", ":", "\n", "        ", "recall_micro", "=", "float", "(", "sum", "(", "correct_by_relation", ".", "values", "(", ")", ")", ")", "/", "float", "(", "sum", "(", "gold_by_relation", ".", "values", "(", ")", ")", ")", "\n", "", "f1_micro", "=", "0.0", "\n", "if", "prec_micro", "+", "recall_micro", ">", "0.0", ":", "\n", "        ", "f1_micro", "=", "2.0", "*", "prec_micro", "*", "recall_micro", "/", "(", "prec_micro", "+", "recall_micro", ")", "\n", "", "print", "(", "\"Precision (micro): {:.3%}\"", ".", "format", "(", "prec_micro", ")", ")", "\n", "print", "(", "\"   Recall (micro): {:.3%}\"", ".", "format", "(", "recall_micro", ")", ")", "\n", "print", "(", "\"       F1 (micro): {:.3%}\"", ".", "format", "(", "f1_micro", ")", ")", "\n", "return", "prec_micro", ",", "recall_micro", ",", "f1_micro", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.InputExample.__init__": [[49, 65], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "guid", ",", "text_a", ",", "text_b", "=", "None", ",", "label", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs a InputExample.\n\n        Args:\n            guid: Unique id for the example.\n            text_a: string. The untokenized text of the first sequence. For single\n            sequence tasks, only this sequence must be specified.\n            text_b: (Optional) string. The untokenized text of the second sequence.\n            Only must be specified for sequence pair tasks.\n            label: (Optional) string. The label of the example. This should be\n            specified for train and dev examples, but not for test examples.\n        \"\"\"", "\n", "self", ".", "guid", "=", "guid", "\n", "self", ".", "text_a", "=", "text_a", "\n", "self", ".", "text_b", "=", "text_b", "\n", "self", ".", "label", "=", "label", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.InputFeatures.__init__": [[70, 77], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "label_id", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "input_ids", "\n", "self", ".", "input_mask", "=", "input_mask", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "label_id", "=", "label_id", "\n", "self", ".", "input_ent", "=", "input_ent", "\n", "self", ".", "ent_mask", "=", "ent_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor.get_train_examples": [[82, 85], ["NotImplementedError"], "methods", ["None"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor.get_dev_examples": [[86, 89], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor.get_labels": [[90, 93], ["NotImplementedError"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Gets the list of labels for this data set.\"\"\"", "\n", "raise", "NotImplementedError", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json": [[94, 98], ["open", "simplejson.loads", "f.read"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "@", "classmethod", "\n", "def", "_read_json", "(", "cls", ",", "input_file", ")", ":", "\n", "        ", "with", "open", "(", "input_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "f", ":", "\n", "            ", "return", "json", ".", "loads", "(", "f", ".", "read", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_train_examples": [[103, 109], ["eval_tacred.TacredProcessor._create_examples", "set", "eval_tacred.TacredProcessor._read_json", "list", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["def", "get_train_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "examples", "=", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"train.json\"", ")", ")", ",", "\"train\"", ")", "\n", "labels", "=", "set", "(", "[", "x", ".", "label", "for", "x", "in", "examples", "]", ")", "\n", "return", "examples", ",", "list", "(", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_dev_examples": [[110, 114], ["eval_tacred.TacredProcessor._create_examples", "eval_tacred.TacredProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_dev_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"dev.json\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_test_examples": [[115, 119], ["eval_tacred.TacredProcessor._create_examples", "eval_tacred.TacredProcessor._read_json", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.DataProcessor._read_json"], ["", "def", "get_test_examples", "(", "self", ",", "data_dir", ")", ":", "\n", "        ", "\"\"\"See base class.\"\"\"", "\n", "return", "self", ".", "_create_examples", "(", "\n", "self", ".", "_read_json", "(", "os", ".", "path", ".", "join", "(", "data_dir", ",", "\"test.json\"", ")", ")", ",", "\"dev\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_labels": [[120, 123], ["None"], "methods", ["None"], ["", "def", "get_labels", "(", "self", ")", ":", "\n", "        ", "\"\"\"Useless\"\"\"", "\n", "return", "[", "\"0\"", ",", "\"1\"", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor._create_examples": [[124, 138], ["enumerate", "examples.append", "eval_tacred.InputExample"], "methods", ["None"], ["", "def", "_create_examples", "(", "self", ",", "lines", ",", "set_type", ")", ":", "\n", "        ", "\"\"\"Creates examples for the training and dev sets.\"\"\"", "\n", "examples", "=", "[", "]", "\n", "for", "(", "i", ",", "line", ")", "in", "enumerate", "(", "lines", ")", ":", "\n", "            ", "guid", "=", "\"%s-%s\"", "%", "(", "set_type", ",", "i", ")", "\n", "for", "x", "in", "line", "[", "'ents'", "]", ":", "\n", "                ", "if", "x", "[", "1", "]", "==", "1", ":", "\n", "                    ", "x", "[", "1", "]", "=", "0", "\n", "# print(line['text'][x[1]:x[2]].encode(\"utf-8\"))", "\n", "", "", "text_a", "=", "(", "line", "[", "'text'", "]", ",", "line", "[", "'ents'", "]", ")", "\n", "label", "=", "line", "[", "'label'", "]", "\n", "examples", ".", "append", "(", "\n", "InputExample", "(", "guid", "=", "guid", ",", "text_a", "=", "text_a", ",", "text_b", "=", "line", "[", "'ann'", "]", ",", "label", "=", "label", ")", ")", "\n", "", "return", "examples", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features": [[140, 278], ["sorted", "enumerate", "open", "fin.readline", "tokenizer.tokenize", "tokenizer.convert_tokens_to_ids", "features.append", "enumerate", "line.strip().split", "int", "tokenizer.tokenize", "eval_tacred._truncate_seq_pair", "len", "len", "len", "len", "len", "len", "len", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "logger.info", "eval_tacred.InputFeatures", "len", "input_ent.append", "ent_mask.append", "input_ent.append", "ent_mask.append", "len", "len", "line.strip", "len", "str", "str", "str", "str", "str"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_tokens_to_ids", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred._truncate_seq_pair"], ["", "", "def", "convert_examples_to_features", "(", "examples", ",", "label_list", ",", "max_seq_length", ",", "tokenizer", ",", "threshold", ")", ":", "\n", "    ", "\"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"", "\n", "\n", "label_list", "=", "sorted", "(", "label_list", ")", "\n", "label_map", "=", "{", "label", ":", "i", "for", "i", ",", "label", "in", "enumerate", "(", "label_list", ")", "}", "\n", "\n", "entity2id", "=", "{", "}", "\n", "with", "open", "(", "\"kg_embed/entity2id.txt\"", ")", "as", "fin", ":", "\n", "        ", "fin", ".", "readline", "(", ")", "\n", "for", "line", "in", "fin", ":", "\n", "            ", "qid", ",", "eid", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "entity2id", "[", "qid", "]", "=", "int", "(", "eid", ")", "\n", "\n", "", "", "features", "=", "[", "]", "\n", "for", "(", "ex_index", ",", "example", ")", "in", "enumerate", "(", "examples", ")", ":", "\n", "        ", "ex_text_a", "=", "example", ".", "text_a", "[", "0", "]", "\n", "h", ",", "t", "=", "example", ".", "text_a", "[", "1", "]", "\n", "h_name", "=", "ex_text_a", "[", "h", "[", "1", "]", ":", "h", "[", "2", "]", "]", "\n", "t_name", "=", "ex_text_a", "[", "t", "[", "1", "]", ":", "t", "[", "2", "]", "]", "\n", "if", "h", "[", "1", "]", "<", "t", "[", "1", "]", ":", "\n", "            ", "ex_text_a", "=", "ex_text_a", "[", ":", "h", "[", "1", "]", "]", "+", "\"# \"", "+", "h_name", "+", "\" #\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "t", "[", "1", "]", "]", "+", "\"$ \"", "+", "t_name", "+", "\" $\"", "+", "ex_text_a", "[", "t", "[", "2", "]", ":", "]", "\n", "", "else", ":", "\n", "            ", "ex_text_a", "=", "ex_text_a", "[", ":", "t", "[", "1", "]", "]", "+", "\"$ \"", "+", "t_name", "+", "\" $\"", "+", "ex_text_a", "[", "t", "[", "2", "]", ":", "h", "[", "1", "]", "]", "+", "\"# \"", "+", "h_name", "+", "\" #\"", "+", "ex_text_a", "[", "h", "[", "2", "]", ":", "]", "\n", "\n", "", "ent_pos", "=", "[", "x", "for", "x", "in", "example", ".", "text_b", "if", "x", "[", "-", "1", "]", ">", "threshold", "]", "\n", "for", "x", "in", "ent_pos", ":", "\n", "            ", "cnt", "=", "0", "\n", "if", "x", "[", "1", "]", ">", "h", "[", "2", "]", ":", "\n", "                ", "cnt", "+=", "2", "\n", "", "if", "x", "[", "1", "]", ">=", "h", "[", "1", "]", ":", "\n", "                ", "cnt", "+=", "2", "\n", "", "if", "x", "[", "1", "]", ">=", "t", "[", "1", "]", ":", "\n", "                ", "cnt", "+=", "2", "\n", "", "if", "x", "[", "1", "]", ">", "t", "[", "2", "]", ":", "\n", "                ", "cnt", "+=", "2", "\n", "", "x", "[", "1", "]", "+=", "cnt", "\n", "x", "[", "2", "]", "+=", "cnt", "\n", "", "tokens_a", ",", "entities_a", "=", "tokenizer", ".", "tokenize", "(", "ex_text_a", ",", "ent_pos", ")", "\n", "\n", "tokens_b", "=", "None", "\n", "if", "False", ":", "\n", "            ", "tokens_b", ",", "entities_b", "=", "tokenizer", ".", "tokenize", "(", "\n", "example", ".", "text_b", "[", "0", "]", ",", "[", "x", "for", "x", "in", "example", ".", "text_b", "[", "1", "]", "if", "x", "[", "-", "1", "]", ">", "threshold", "]", ")", "\n", "# Modifies `tokens_a` and `tokens_b` in place so that the total", "\n", "# length is less than the specified length.", "\n", "# Account for [CLS], [SEP], [SEP] with \"- 3\"", "\n", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "entities_a", ",", "\n", "entities_b", ",", "max_seq_length", "-", "3", ")", "\n", "", "else", ":", "\n", "# Account for [CLS] and [SEP] with \"- 2\"", "\n", "            ", "if", "len", "(", "tokens_a", ")", ">", "max_seq_length", "-", "2", ":", "\n", "                ", "tokens_a", "=", "tokens_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "entities_a", "=", "entities_a", "[", ":", "(", "max_seq_length", "-", "2", ")", "]", "\n", "\n", "# The convention in BERT is:", "\n", "# (a) For sequence pairs:", "\n", "#  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]", "\n", "#  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1", "\n", "# (b) For single sequences:", "\n", "#  tokens:   [CLS] the dog is hairy . [SEP]", "\n", "#  type_ids: 0   0   0   0  0     0 0", "\n", "#", "\n", "# Where \"type_ids\" are used to indicate whether this is the first", "\n", "# sequence or the second sequence. The embedding vectors for `type=0` and", "\n", "# `type=1` were learned during pre-training and are added to the wordpiece", "\n", "# embedding vector (and position vector). This is not *strictly* necessary", "\n", "# since the [SEP] token unambigiously separates the sequences, but it makes", "\n", "# it easier for the model to learn the concept of sequences.", "\n", "#", "\n", "# For classification tasks, the first vector (corresponding to [CLS]) is", "\n", "# used as as the \"sentence vector\". Note that this only makes sense because", "\n", "# the entire model is fine-tuned.", "\n", "", "", "tokens", "=", "[", "\"[CLS]\"", "]", "+", "tokens_a", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "=", "[", "\"UNK\"", "]", "+", "entities_a", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "=", "[", "0", "]", "*", "len", "(", "tokens", ")", "\n", "\n", "if", "tokens_b", ":", "\n", "            ", "tokens", "+=", "tokens_b", "+", "[", "\"[SEP]\"", "]", "\n", "ents", "+=", "entities_b", "+", "[", "\"UNK\"", "]", "\n", "segment_ids", "+=", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "", "input_ids", "=", "tokenizer", ".", "convert_tokens_to_ids", "(", "tokens", ")", "\n", "input_ent", "=", "[", "]", "\n", "ent_mask", "=", "[", "]", "\n", "for", "ent", "in", "ents", ":", "\n", "            ", "if", "ent", "!=", "\"UNK\"", "and", "ent", "in", "entity2id", ":", "\n", "                ", "input_ent", ".", "append", "(", "entity2id", "[", "ent", "]", ")", "\n", "ent_mask", ".", "append", "(", "1", ")", "\n", "", "else", ":", "\n", "                ", "input_ent", ".", "append", "(", "-", "1", ")", "\n", "ent_mask", ".", "append", "(", "0", ")", "\n", "", "", "ent_mask", "[", "0", "]", "=", "1", "\n", "\n", "# The mask has 1 for real tokens and 0 for padding tokens. Only real", "\n", "# tokens are attended to.", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "\n", "# Zero-pad up to the sequence length.", "\n", "padding", "=", "[", "0", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "padding_", "=", "[", "-", "1", "]", "*", "(", "max_seq_length", "-", "len", "(", "input_ids", ")", ")", "\n", "input_ids", "+=", "padding", "\n", "input_mask", "+=", "padding", "\n", "segment_ids", "+=", "padding", "\n", "input_ent", "+=", "padding_", "\n", "ent_mask", "+=", "padding", "\n", "\n", "assert", "len", "(", "input_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_mask", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "segment_ids", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "input_ent", ")", "==", "max_seq_length", "\n", "assert", "len", "(", "ent_mask", ")", "==", "max_seq_length", "\n", "\n", "label_id", "=", "label_map", "[", "example", ".", "label", "]", "\n", "if", "ex_index", "<", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "logger", ".", "info", "(", "\"guid: %s\"", "%", "(", "example", ".", "guid", ")", ")", "\n", "logger", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "tokens", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"ents: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "ents", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_ids: %s\"", "%", "\n", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"input_mask: %s\"", "%", "\n", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "logger", ".", "info", "(", "\n", "\"segment_ids: %s\"", "%", "\" \"", ".", "join", "(", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "logger", ".", "info", "(", "\"label: %s (id = %d)\"", "%", "(", "example", ".", "label", ",", "label_id", ")", ")", "\n", "\n", "", "features", ".", "append", "(", "\n", "InputFeatures", "(", "input_ids", "=", "input_ids", ",", "\n", "input_mask", "=", "input_mask", ",", "\n", "segment_ids", "=", "segment_ids", ",", "\n", "input_ent", "=", "input_ent", ",", "\n", "ent_mask", "=", "ent_mask", ",", "\n", "label_id", "=", "label_id", ")", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred._truncate_seq_pair": [[280, 297], ["len", "len", "len", "len", "tokens_a.pop", "ents_a.pop", "tokens_b.pop", "ents_b.pop"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "def", "_truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "ents_a", ",", "ents_b", ",", "max_length", ")", ":", "\n", "    ", "\"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"", "\n", "\n", "# This is a simple heuristic which will always truncate the longer sequence", "\n", "# one token at a time. This makes more sense than truncating an equal percent", "\n", "# of tokens from each, since if one sequence is very short then each token", "\n", "# that's truncated likely contains more information than a longer sequence.", "\n", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_length", ":", "\n", "            ", "break", "\n", "", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", ":", "\n", "            ", "tokens_a", ".", "pop", "(", ")", "\n", "ents_a", ".", "pop", "(", ")", "\n", "", "else", ":", "\n", "            ", "tokens_b", ".", "pop", "(", ")", "\n", "ents_b", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.warmup_linear": [[299, 303], ["None"], "function", ["None"], ["", "", "", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.eval_result": [[305, 337], ["len", "range", "float", "float", "float", "float", "float", "float"], "function", ["None"], ["", "def", "eval_result", "(", "pred_result", ",", "labels", ",", "na_id", ")", ":", "\n", "    ", "correct", "=", "0", "\n", "total", "=", "len", "(", "labels", ")", "\n", "correct_positive", "=", "0", "\n", "pred_positive", "=", "0", "\n", "gold_positive", "=", "0", "\n", "\n", "for", "i", "in", "range", "(", "total", ")", ":", "\n", "        ", "if", "labels", "[", "i", "]", "==", "pred_result", "[", "i", "]", ":", "\n", "            ", "correct", "+=", "1", "\n", "if", "labels", "[", "i", "]", "!=", "na_id", ":", "\n", "                ", "correct_positive", "+=", "1", "\n", "", "", "if", "labels", "[", "i", "]", "!=", "na_id", ":", "\n", "            ", "gold_positive", "+=", "1", "\n", "", "if", "pred_result", "[", "i", "]", "!=", "na_id", ":", "\n", "            ", "pred_positive", "+=", "1", "\n", "", "", "acc", "=", "float", "(", "correct", ")", "/", "float", "(", "total", ")", "\n", "try", ":", "\n", "        ", "micro_p", "=", "float", "(", "correct_positive", ")", "/", "float", "(", "pred_positive", ")", "\n", "", "except", ":", "\n", "        ", "micro_p", "=", "0", "\n", "", "try", ":", "\n", "        ", "micro_r", "=", "float", "(", "correct_positive", ")", "/", "float", "(", "gold_positive", ")", "\n", "", "except", ":", "\n", "        ", "micro_r", "=", "0", "\n", "", "try", ":", "\n", "        ", "micro_f1", "=", "2", "*", "micro_p", "*", "micro_r", "/", "(", "micro_p", "+", "micro_r", ")", "\n", "", "except", ":", "\n", "        ", "micro_f1", "=", "0", "\n", "", "result", "=", "{", "'acc'", ":", "acc", ",", "'micro_p'", ":", "micro_p", ",", "\n", "'micro_r'", ":", "micro_r", ",", "'micro_f1'", ":", "micro_f1", "}", "\n", "return", "result", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.main": [[339, 595], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "logger.info", "random.seed", "numpy.random.seed", "torch.manual_seed", "processors", "knowledge_bert.tokenization.BertTokenizer.from_pretrained", "processors.get_train_examples", "sorted", "vecs.append", "torch.tensor", "torch.nn.Embedding.from_pretrained", "logger.info", "os.listdir", "processors.get_dev_examples", "eval_tacred.convert_examples_to_features", "processors.get_test_examples", "eval_tacred.convert_examples_to_features", "torch.device", "torch.cuda.device_count", "torch.cuda.set_device", "torch.device", "torch.distributed.init_process_group", "ValueError", "torch.cuda.manual_seed_all", "ValueError", "open", "file_mark.append", "file_mark.append", "print", "os.path.join", "torch.load", "knowledge_bert.modeling.BertForSequenceClassification.from_pretrained", "model.to", "open", "open", "logger.info", "logger.info", "logger.info", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.utils.data.TensorDataset", "torch.utils.data.SequentialSampler", "torch.utils.data.DataLoader", "model.eval", "eval_tacred.eval_result", "bool", "line.strip().split", "vecs.append", "str", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "len", "torch.nn.Embedding.from_pretrained.", "input_ids.to.to", "input_mask.to.to", "segment_ids.to.to", "input_ent.to.to", "ent_mask.to.to", "label_ids.to().numpy.to", "model.detach().cpu().numpy", "label_ids.to().numpy.to().numpy", "numpy.argmax", "zip", "model.mean().item", "input_ids.to.size", "sorted.index", "os.path.join", "os.path.join", "open", "logger.info", "sorted", "float", "len", "torch.nn.Embedding.from_pretrained.weight.size", "len", "torch.no_grad", "model", "model", "pred_all.append", "label_all.append", "open.write", "open.write", "eval_result.keys", "logger.info", "writer.write", "torch.cuda.is_available", "line.strip", "len", "model.detach().cpu", "label_ids.to().numpy.to", "model.mean", "str", "x.split", "x.split", "x.split", "x.split", "x.split", "x.split", "range", "model.detach", "str"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_train_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_dev_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.TacredProcessor.get_test_examples", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.convert_examples_to_features", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.eval_tacred.eval_result", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "def", "main", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "# Required parameters", "\n", "parser", ".", "add_argument", "(", "\"--data_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The input data dir. Should contain the .tsv files (or other data files) for the task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--ernie_model\"", ",", "default", "=", "None", ",", "type", "=", "str", ",", "required", "=", "True", ",", "\n", "help", "=", "\"Ernie pre-trained model\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "\n", "default", "=", "None", ",", "\n", "type", "=", "str", ",", "\n", "required", "=", "True", ",", "\n", "help", "=", "\"The output directory where the model predictions and checkpoints will be written.\"", ")", "\n", "\n", "# Other parameters", "\n", "parser", ".", "add_argument", "(", "\"--max_seq_length\"", ",", "\n", "default", "=", "128", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"The maximum total input sequence length after WordPiece tokenization. \\n\"", "\n", "\"Sequences longer than this will be truncated, and sequences shorter \\n\"", "\n", "\"than this will be padded.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_train\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_eval\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to run eval on the dev set.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--do_lower_case\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Set this flag if you are using an uncased model.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--train_batch_size\"", ",", "\n", "default", "=", "32", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--eval_batch_size\"", ",", "\n", "default", "=", "8", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"Total batch size for eval.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\n", "default", "=", "5e-5", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"The initial learning rate for Adam.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_train_epochs\"", ",", "\n", "default", "=", "3.0", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Total number of training epochs to perform.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--warmup_proportion\"", ",", "\n", "default", "=", "0.1", ",", "\n", "type", "=", "float", ",", "\n", "help", "=", "\"Proportion of training to perform linear learning rate warmup for. \"", "\n", "\"E.g., 0.1 = 10%% of training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--no_cuda\"", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether not to use CUDA when available\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--local_rank\"", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "-", "1", ",", "\n", "help", "=", "\"local_rank for distributed training on gpus\"", ")", "\n", "parser", ".", "add_argument", "(", "'--seed'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "42", ",", "\n", "help", "=", "\"random seed for initialization\"", ")", "\n", "parser", ".", "add_argument", "(", "'--gradient_accumulation_steps'", ",", "\n", "type", "=", "int", ",", "\n", "default", "=", "1", ",", "\n", "help", "=", "\"Number of updates steps to accumulate before performing a backward/update pass.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--fp16'", ",", "\n", "default", "=", "False", ",", "\n", "action", "=", "'store_true'", ",", "\n", "help", "=", "\"Whether to use 16-bit float precision instead of 32-bit\"", ")", "\n", "parser", ".", "add_argument", "(", "'--loss_scale'", ",", "\n", "type", "=", "float", ",", "default", "=", "0", ",", "\n", "help", "=", "\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"", "\n", "\"0 (default value): dynamic loss scaling.\\n\"", "\n", "\"Positive power of 2: static loss scaling value.\\n\"", ")", "\n", "parser", ".", "add_argument", "(", "'--threshold'", ",", "type", "=", "float", ",", "default", "=", ".3", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "processors", "=", "TacredProcessor", "\n", "\n", "num_labels_task", "=", "42", "\n", "\n", "if", "args", ".", "local_rank", "==", "-", "1", "or", "args", ".", "no_cuda", ":", "\n", "        ", "device", "=", "torch", ".", "device", "(", "\n", "\"cuda\"", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "and", "not", "args", ".", "no_cuda", "else", "\"cpu\"", ")", "\n", "n_gpu", "=", "torch", ".", "cuda", ".", "device_count", "(", ")", "\n", "", "else", ":", "\n", "        ", "torch", ".", "cuda", ".", "set_device", "(", "args", ".", "local_rank", ")", "\n", "device", "=", "torch", ".", "device", "(", "\"cuda\"", ",", "args", ".", "local_rank", ")", "\n", "n_gpu", "=", "1", "\n", "# Initializes the distributed backend which will take care of sychronizing nodes/GPUs", "\n", "torch", ".", "distributed", ".", "init_process_group", "(", "backend", "=", "'nccl'", ")", "\n", "", "logger", ".", "info", "(", "\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\"", ".", "format", "(", "\n", "device", ",", "n_gpu", ",", "bool", "(", "args", ".", "local_rank", "!=", "-", "1", ")", ",", "args", ".", "fp16", ")", ")", "\n", "\n", "if", "args", ".", "gradient_accumulation_steps", "<", "1", ":", "\n", "        ", "raise", "ValueError", "(", "\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\"", ".", "format", "(", "\n", "args", ".", "gradient_accumulation_steps", ")", ")", "\n", "\n", "", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "np", ".", "random", ".", "seed", "(", "args", ".", "seed", ")", "\n", "torch", ".", "manual_seed", "(", "args", ".", "seed", ")", "\n", "if", "n_gpu", ">", "0", ":", "\n", "        ", "torch", ".", "cuda", ".", "manual_seed_all", "(", "args", ".", "seed", ")", "\n", "\n", "", "if", "not", "args", ".", "do_train", "and", "not", "args", ".", "do_eval", ":", "\n", "        ", "raise", "ValueError", "(", "\n", "\"At least one of `do_train` or `do_eval` must be True.\"", ")", "\n", "\n", "", "processor", "=", "processors", "(", ")", "\n", "num_labels", "=", "num_labels_task", "\n", "label_list", "=", "None", "\n", "\n", "tokenizer", "=", "BertTokenizer", ".", "from_pretrained", "(", "\n", "args", ".", "ernie_model", ",", "do_lower_case", "=", "args", ".", "do_lower_case", ")", "\n", "\n", "train_examples", "=", "None", "\n", "num_train_steps", "=", "None", "\n", "train_examples", ",", "label_list", "=", "processor", ".", "get_train_examples", "(", "args", ".", "data_dir", ")", "\n", "label_list", "=", "sorted", "(", "label_list", ")", "\n", "vecs", "=", "[", "]", "\n", "vecs", ".", "append", "(", "[", "0", "]", "*", "100", ")", "\n", "with", "open", "(", "\"kg_embed/entity2vec.vec\"", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "vec", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "vec", "=", "[", "float", "(", "x", ")", "for", "x", "in", "vec", "]", "\n", "if", "len", "(", "vec", ")", "!=", "100", ":", "\n", "                ", "diff", "=", "100", "-", "len", "(", "vec", ")", "\n", "vec", "=", "vec", "+", "[", "0", "for", "_", "in", "range", "(", "diff", ")", "]", "\n", "", "vecs", ".", "append", "(", "vec", ")", "\n", "", "", "embed", "=", "torch", ".", "tensor", "(", "vecs", ",", "dtype", "=", "torch", ".", "float", ")", "\n", "embed", "=", "torch", ".", "nn", ".", "Embedding", ".", "from_pretrained", "(", "embed", ")", "\n", "\n", "logger", ".", "info", "(", "\"Shape of entity embedding: \"", "+", "str", "(", "embed", ".", "weight", ".", "size", "(", ")", ")", ")", "\n", "del", "vecs", "\n", "\n", "filenames", "=", "os", ".", "listdir", "(", "args", ".", "output_dir", ")", "\n", "filenames", "=", "[", "x", "for", "x", "in", "filenames", "if", "\"pytorch_model.bin_\"", "in", "x", "]", "\n", "\n", "file_mark", "=", "[", "]", "\n", "for", "x", "in", "filenames", ":", "\n", "        ", "file_mark", ".", "append", "(", "[", "x", ",", "True", "]", ")", "\n", "file_mark", ".", "append", "(", "[", "x", ",", "False", "]", ")", "\n", "\n", "", "eval_examples", "=", "processor", ".", "get_dev_examples", "(", "args", ".", "data_dir", ")", "\n", "dev", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ",", "args", ".", "threshold", ")", "\n", "eval_examples", "=", "processor", ".", "get_test_examples", "(", "args", ".", "data_dir", ")", "\n", "test", "=", "convert_examples_to_features", "(", "\n", "eval_examples", ",", "label_list", ",", "args", ".", "max_seq_length", ",", "tokenizer", ",", "args", ".", "threshold", ")", "\n", "\n", "for", "x", ",", "mark", "in", "file_mark", ":", "\n", "        ", "print", "(", "x", ",", "mark", ")", "\n", "output_model_file", "=", "os", ".", "path", ".", "join", "(", "args", ".", "output_dir", ",", "x", ")", "\n", "model_state_dict", "=", "torch", ".", "load", "(", "output_model_file", ")", "\n", "model", ",", "_", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "\n", "args", ".", "ernie_model", ",", "state_dict", "=", "model_state_dict", ",", "num_labels", "=", "len", "(", "label_list", ")", ")", "\n", "model", ".", "to", "(", "device", ")", "\n", "\n", "if", "mark", ":", "\n", "            ", "eval_features", "=", "dev", "\n", "output_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"eval_pred_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "output_file_", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"eval_gold_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "eval_features", "=", "test", "\n", "output_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"test_pred_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "output_file_", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"test_gold_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "", "fpred", "=", "open", "(", "output_file", ",", "\"w\"", ")", "\n", "fgold", "=", "open", "(", "output_file_", ",", "\"w\"", ")", "\n", "\n", "logger", ".", "info", "(", "\"***** Running evaluation *****\"", ")", "\n", "logger", ".", "info", "(", "\"  Num examples = %d\"", ",", "len", "(", "eval_examples", ")", ")", "\n", "logger", ".", "info", "(", "\"  Batch size = %d\"", ",", "args", ".", "eval_batch_size", ")", "\n", "# zeros = [0 for _ in range(args.max_seq_length)]", "\n", "# zeros_ent = [0 for _ in range(100)]", "\n", "# zeros_ent = [zeros_ent for _ in range(args.max_seq_length)]", "\n", "all_input_ids", "=", "torch", ".", "tensor", "(", "\n", "[", "f", ".", "input_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_input_mask", "=", "torch", ".", "tensor", "(", "\n", "[", "f", ".", "input_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_segment_ids", "=", "torch", ".", "tensor", "(", "\n", "[", "f", ".", "segment_ids", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_label_ids", "=", "torch", ".", "tensor", "(", "\n", "[", "f", ".", "label_id", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent", "=", "torch", ".", "tensor", "(", "\n", "[", "f", ".", "input_ent", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "all_ent_masks", "=", "torch", ".", "tensor", "(", "\n", "[", "f", ".", "ent_mask", "for", "f", "in", "eval_features", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "eval_data", "=", "TensorDataset", "(", "all_input_ids", ",", "all_input_mask", ",", "\n", "all_segment_ids", ",", "all_ent", ",", "all_ent_masks", ",", "all_label_ids", ")", "\n", "# Run prediction for full data", "\n", "eval_sampler", "=", "SequentialSampler", "(", "eval_data", ")", "\n", "eval_dataloader", "=", "DataLoader", "(", "\n", "eval_data", ",", "sampler", "=", "eval_sampler", ",", "batch_size", "=", "args", ".", "eval_batch_size", ")", "\n", "\n", "model", ".", "eval", "(", ")", "\n", "eval_loss", "=", "0", "\n", "nb_eval_steps", ",", "nb_eval_examples", "=", "0", ",", "0", "\n", "pred_all", ",", "label_all", "=", "[", "]", ",", "[", "]", "\n", "for", "input_ids", ",", "input_mask", ",", "segment_ids", ",", "input_ent", ",", "ent_mask", ",", "label_ids", "in", "eval_dataloader", ":", "\n", "            ", "input_ent", "=", "embed", "(", "input_ent", "+", "1", ")", "# -1 -> 0", "\n", "input_ids", "=", "input_ids", ".", "to", "(", "device", ")", "\n", "input_mask", "=", "input_mask", ".", "to", "(", "device", ")", "\n", "segment_ids", "=", "segment_ids", ".", "to", "(", "device", ")", "\n", "input_ent", "=", "input_ent", ".", "to", "(", "device", ")", "\n", "ent_mask", "=", "ent_mask", ".", "to", "(", "device", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "device", ")", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "                ", "tmp_eval_loss", "=", "model", "(", "\n", "input_ids", ",", "segment_ids", ",", "input_mask", ",", "input_ent", ",", "ent_mask", ",", "label_ids", ")", "\n", "logits", "=", "model", "(", "input_ids", ",", "segment_ids", ",", "\n", "input_mask", ",", "input_ent", ",", "ent_mask", ")", "\n", "\n", "", "logits", "=", "logits", ".", "detach", "(", ")", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "label_ids", "=", "label_ids", ".", "to", "(", "'cpu'", ")", ".", "numpy", "(", ")", "\n", "pred", "=", "np", ".", "argmax", "(", "logits", ",", "axis", "=", "1", ")", "\n", "for", "a", ",", "b", "in", "zip", "(", "pred", ",", "label_ids", ")", ":", "\n", "                ", "pred_all", ".", "append", "(", "a", ")", "\n", "label_all", ".", "append", "(", "b", ")", "\n", "fgold", ".", "write", "(", "\"{}\\n\"", ".", "format", "(", "label_list", "[", "b", "]", ")", ")", "\n", "fpred", ".", "write", "(", "\"{}\\n\"", ".", "format", "(", "label_list", "[", "a", "]", ")", ")", "\n", "\n", "", "eval_loss", "+=", "tmp_eval_loss", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "nb_eval_examples", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "nb_eval_steps", "+=", "1", "\n", "\n", "", "eval_loss", "=", "eval_loss", "/", "nb_eval_steps", "\n", "\n", "result", "=", "eval_result", "(", "pred_all", ",", "label_all", ",", "label_list", ".", "index", "(", "\"NA\"", ")", ")", "\n", "\n", "if", "mark", ":", "\n", "            ", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"eval_results_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "", "else", ":", "\n", "            ", "output_eval_file", "=", "os", ".", "path", ".", "join", "(", "\n", "args", ".", "output_dir", ",", "\"test_results_{}.txt\"", ".", "format", "(", "x", ".", "split", "(", "\"_\"", ")", "[", "-", "1", "]", ")", ")", "\n", "\n", "", "with", "open", "(", "output_eval_file", ",", "\"w\"", ")", "as", "writer", ":", "\n", "            ", "logger", ".", "info", "(", "\"***** Eval results *****\"", ")", "\n", "for", "key", "in", "sorted", "(", "result", ".", "keys", "(", ")", ")", ":", "\n", "                ", "logger", ".", "info", "(", "\"  %s = %s\"", ",", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", "\n", "writer", ".", "write", "(", "\"%s = %s\\n\"", "%", "(", "key", ",", "str", "(", "result", "[", "key", "]", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.TrainingInstance.__init__": [[36, 43], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "tokens", ",", "segment_ids", ",", "masked_lm_positions", ",", "masked_lm_labels", ",", "\n", "is_random_next", ")", ":", "\n", "        ", "self", ".", "input_ids", "=", "tokens", "\n", "self", ".", "segment_ids", "=", "segment_ids", "\n", "self", ".", "is_random_next", "=", "is_random_next", "\n", "self", ".", "masked_lm_positions", "=", "masked_lm_positions", "\n", "self", ".", "masked_lm_labels", "=", "masked_lm_labels", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.create_training_instances": [[44, 101], ["indexed_dataset.IndexedDatasetBuilder", "rng.randint", "rng.seed", "rng.shuffle", "rng.seed", "rng.shuffle", "range", "indexed_dataset.IndexedDatasetBuilder.finalize", "open", "fin.readline", "tensorflow.gfile.GFile", "range", "fin.readline", "fin.readline.strip().split", "int", "tensorflow.gfile.GFile", "len", "create_instances.create_instances_from_document", "reader.readline", "reader_ent.readline", "reader_ent.readline.strip().split", "enumerate", "fin.readline.strip", "int", "all_documents.append", "all_documents_ent.append", "reader.readline.strip().split", "reader_ent.readline.strip", "int", "reader.readline.strip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.create_instances_from_document"], ["", "", "def", "create_training_instances", "(", "input_file", ",", "output_file", ",", "max_seq_length", ",", "\n", "dupe_factor", ",", "short_seq_prob", ",", "masked_lm_prob", ",", "\n", "max_predictions_per_seq", ",", "rng", ")", ":", "\n", "\n", "    ", "ds", "=", "indexed_dataset", ".", "IndexedDatasetBuilder", "(", "output_file", "+", "\".bin\"", ")", "\n", "\n", "# read entity mapping", "\n", "with", "open", "(", "\"kg_embed/entity2id.txt\"", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "d", "=", "{", "}", "\n", "fin", ".", "readline", "(", ")", "\n", "while", "1", ":", "\n", "            ", "l", "=", "fin", ".", "readline", "(", ")", "\n", "if", "l", "==", "\"\"", ":", "\n", "                ", "break", "\n", "", "ent", ",", "idx", "=", "l", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "d", "[", "ent", "]", "=", "int", "(", "idx", ")", "\n", "\n", "", "", "all_documents", "=", "[", "]", "\n", "all_documents_ent", "=", "[", "]", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "input_file", "+", "\"_token\"", ",", "\"r\"", ")", "as", "reader", ":", "\n", "        ", "with", "tf", ".", "gfile", ".", "GFile", "(", "input_file", "+", "\"_entity\"", ",", "\"r\"", ")", "as", "reader_ent", ":", "\n", "            ", "while", "True", ":", "\n", "                ", "line", "=", "reader", ".", "readline", "(", ")", "\n", "line_ent", "=", "reader_ent", ".", "readline", "(", ")", "\n", "# if len(all_documents) > 10:", "\n", "#     break", "\n", "if", "not", "line", ":", "\n", "                    ", "break", "\n", "", "line", "=", "[", "int", "(", "x", ")", "for", "x", "in", "line", ".", "strip", "(", ")", ".", "split", "(", ")", "]", "\n", "vec", "=", "line_ent", ".", "strip", "(", ")", ".", "split", "(", ")", "\n", "for", "i", ",", "x", "in", "enumerate", "(", "vec", ")", ":", "\n", "                    ", "if", "x", "==", "\"#UNK#\"", ":", "\n", "                        ", "vec", "[", "i", "]", "=", "-", "1", "\n", "", "elif", "x", "[", "0", "]", "==", "\"Q\"", ":", "\n", "                        ", "if", "x", "in", "d", ":", "\n", "                            ", "vec", "[", "i", "]", "=", "d", "[", "x", "]", "\n", "if", "i", "!=", "0", "and", "vec", "[", "i", "]", "==", "vec", "[", "i", "-", "1", "]", ":", "\n", "                                ", "vec", "[", "i", "]", "=", "-", "1", "# Q123 Q123 Q123 -> d[Q123] -1 -1", "\n", "", "", "else", ":", "\n", "                            ", "vec", "[", "i", "]", "=", "-", "1", "\n", "", "", "else", ":", "\n", "                        ", "vec", "[", "i", "]", "=", "int", "(", "x", ")", "\n", "", "", "if", "line", "[", "0", "]", "!=", "0", ":", "\n", "                    ", "all_documents", ".", "append", "(", "line", ")", "\n", "all_documents_ent", ".", "append", "(", "vec", ")", "\n", "", "", "", "", "seed", "=", "rng", ".", "randint", "(", "0", ",", "100", ")", "\n", "rng", ".", "seed", "(", "seed", ")", "\n", "rng", ".", "shuffle", "(", "all_documents", ")", "\n", "rng", ".", "seed", "(", "seed", ")", "\n", "rng", ".", "shuffle", "(", "all_documents_ent", ")", "\n", "for", "_", "in", "range", "(", "dupe_factor", ")", ":", "\n", "        ", "for", "document_index", "in", "range", "(", "len", "(", "all_documents", ")", ")", ":", "\n", "            ", "create_instances_from_document", "(", "\n", "ds", ",", "all_documents", ",", "all_documents_ent", ",", "document_index", ",", "max_seq_length", ",", "short_seq_prob", ",", "\n", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "rng", ")", "\n", "\n", "", "", "ds", ".", "finalize", "(", "output_file", "+", "\".idx\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.jump_in_document": [[102, 108], ["None"], "function", ["None"], ["", "def", "jump_in_document", "(", "document", ",", "i", ")", ":", "\n", "    ", "pos", "=", "1", "\n", "while", "i", ">", "0", ":", "\n", "        ", "pos", "=", "pos", "+", "1", "+", "document", "[", "pos", "]", "\n", "i", "-=", "1", "\n", "", "return", "pos", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.create_instances_from_document": [[109, 210], ["rng.random", "rng.randint", "current_chunk.append", "create_instances.jump_in_document", "create_instances.truncate_seq_pair", "create_instances.create_masked_lm_predictions", "list", "len", "rng.randint", "create_instances.jump_in_document", "tokens_a.extend", "entity_a.extend", "range", "rng.randint", "range", "len", "len", "len", "len", "len", "len", "len", "input_ids.extend", "input_mask.extend", "segment_ids.extend", "entity.extend", "numpy.ones", "len", "ds.add_item", "len", "rng.random", "len", "rng.randint", "create_instances.jump_in_document", "tokens_b.extend", "entity_b.extend", "len", "create_instances.jump_in_document", "tokens_b.extend", "entity_b.extend", "len", "len", "torch.IntTensor", "len", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.jump_in_document", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.truncate_seq_pair", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.create_masked_lm_predictions", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.jump_in_document", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDatasetBuilder.add_item", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.jump_in_document", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.jump_in_document"], ["", "def", "create_instances_from_document", "(", "\n", "ds", ",", "all_documents", ",", "all_documents_ent", ",", "document_index", ",", "max_seq_length", ",", "short_seq_prob", ",", "\n", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "rng", ")", ":", "\n", "    ", "document", "=", "all_documents", "[", "document_index", "]", "\n", "document_ent", "=", "all_documents_ent", "[", "document_index", "]", "\n", "# Account for [CLS], [SEP], [SEP]", "\n", "max_num_tokens", "=", "max_seq_length", "-", "3", "\n", "target_seq_length", "=", "max_num_tokens", "\n", "if", "rng", ".", "random", "(", ")", "<", "short_seq_prob", ":", "\n", "        ", "target_seq_length", "=", "rng", ".", "randint", "(", "2", ",", "max_num_tokens", ")", "\n", "", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "i", "=", "0", "\n", "while", "i", "<", "document", "[", "0", "]", ":", "\n", "        ", "current_chunk", ".", "append", "(", "i", ")", "\n", "current_length", "+=", "document", "[", "jump_in_document", "(", "document", ",", "i", ")", "]", "\n", "if", "i", "==", "document", "[", "0", "]", "-", "1", "or", "current_length", ">=", "target_seq_length", ":", "\n", "            ", "if", "current_chunk", ":", "\n", "# `a_end` is how many segments from `current_chunk` go into the `A`", "\n", "                ", "a_end", "=", "1", "\n", "if", "len", "(", "current_chunk", ")", ">=", "2", ":", "\n", "                    ", "a_end", "=", "rng", ".", "randint", "(", "1", ",", "len", "(", "current_chunk", ")", "-", "1", ")", "\n", "", "tokens_a", "=", "[", "]", "\n", "entity_a", "=", "[", "]", "\n", "for", "j", "in", "current_chunk", "[", ":", "a_end", "]", ":", "\n", "                    ", "pos", "=", "jump_in_document", "(", "document", ",", "j", ")", "\n", "tokens_a", ".", "extend", "(", "document", "[", "pos", "+", "1", ":", "pos", "+", "1", "+", "document", "[", "pos", "]", "]", ")", "\n", "entity_a", ".", "extend", "(", "document_ent", "[", "pos", "+", "1", ":", "pos", "+", "1", "+", "document", "[", "pos", "]", "]", ")", "\n", "\n", "", "tokens_b", "=", "[", "]", "\n", "entity_b", "=", "[", "]", "\n", "# Random next", "\n", "is_random_next", "=", "False", "\n", "if", "len", "(", "current_chunk", ")", "==", "1", "or", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                    ", "is_random_next", "=", "True", "\n", "target_b_length", "=", "target_seq_length", "-", "len", "(", "tokens_a", ")", "\n", "\n", "for", "_", "in", "range", "(", "10", ")", ":", "\n", "                        ", "random_document_index", "=", "rng", ".", "randint", "(", "0", ",", "len", "(", "all_documents", ")", "-", "1", ")", "\n", "if", "random_document_index", "!=", "document_index", ":", "\n", "                            ", "break", "\n", "\n", "", "", "random_document", "=", "all_documents", "[", "random_document_index", "]", "\n", "random_document_ent", "=", "all_documents_ent", "[", "random_document_index", "]", "\n", "random_start", "=", "rng", ".", "randint", "(", "0", ",", "random_document", "[", "0", "]", "-", "1", ")", "\n", "for", "j", "in", "range", "(", "random_start", ",", "random_document", "[", "0", "]", ")", ":", "\n", "                        ", "pos", "=", "jump_in_document", "(", "random_document", ",", "j", ")", "\n", "tokens_b", ".", "extend", "(", "random_document", "[", "pos", "+", "1", ":", "pos", "+", "1", "+", "random_document", "[", "pos", "]", "]", ")", "\n", "entity_b", ".", "extend", "(", "random_document_ent", "[", "pos", "+", "1", ":", "pos", "+", "1", "+", "random_document", "[", "pos", "]", "]", ")", "\n", "if", "len", "(", "tokens_b", ")", ">=", "target_b_length", ":", "\n", "                            ", "break", "\n", "\n", "", "", "num_unused_segments", "=", "len", "(", "current_chunk", ")", "-", "a_end", "\n", "i", "-=", "num_unused_segments", "\n", "", "else", ":", "\n", "                    ", "is_random_next", "=", "False", "\n", "for", "j", "in", "current_chunk", "[", "a_end", ":", "]", ":", "\n", "                        ", "pos", "=", "jump_in_document", "(", "document", ",", "j", ")", "\n", "tokens_b", ".", "extend", "(", "document", "[", "pos", "+", "1", ":", "pos", "+", "1", "+", "document", "[", "pos", "]", "]", ")", "\n", "entity_b", ".", "extend", "(", "document_ent", "[", "pos", "+", "1", ":", "pos", "+", "1", "+", "document", "[", "pos", "]", "]", ")", "\n", "\n", "", "", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "entity_a", ",", "entity_b", ",", "max_num_tokens", ",", "rng", ")", "\n", "\n", "assert", "len", "(", "tokens_a", ")", ">=", "1", "\n", "assert", "len", "(", "tokens_b", ")", ">=", "1", "\n", "\n", "tokens", "=", "[", "101", "]", "+", "tokens_a", "+", "[", "102", "]", "+", "tokens_b", "+", "[", "102", "]", "\n", "entity", "=", "[", "-", "1", "]", "+", "entity_a", "+", "[", "-", "1", "]", "+", "entity_b", "+", "[", "-", "1", "]", "\n", "\n", "assert", "len", "(", "tokens", ")", "==", "len", "(", "entity", ")", "\n", "segment_ids", "=", "[", "0", "]", "*", "(", "len", "(", "tokens_a", ")", "+", "2", ")", "+", "[", "1", "]", "*", "(", "len", "(", "tokens_b", ")", "+", "1", ")", "\n", "\n", "(", "tokens", ",", "masked_lm_positions", ",", "\n", "masked_lm_ids", ")", "=", "create_masked_lm_predictions", "(", "\n", "tokens", ",", "masked_lm_prob", ",", "max_predictions_per_seq", ",", "rng", ")", "\n", "\n", "input_ids", "=", "tokens", "\n", "input_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "assert", "len", "(", "input_ids", ")", "<=", "max_seq_length", "\n", "if", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "                    ", "rest", "=", "max_seq_length", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "rest", ")", "\n", "input_mask", ".", "extend", "(", "[", "0", "]", "*", "rest", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "rest", ")", "\n", "entity", ".", "extend", "(", "[", "-", "1", "]", "*", "rest", ")", "\n", "", "entity_mask", "=", "[", "1", "if", "x", "!=", "-", "1", "else", "0", "for", "x", "in", "entity", "]", "\n", "\n", "masked_lm_labels", "=", "np", ".", "ones", "(", "len", "(", "input_ids", ")", ",", "dtype", "=", "int", ")", "*", "-", "1", "\n", "masked_lm_labels", "[", "masked_lm_positions", "]", "=", "masked_lm_ids", "\n", "masked_lm_labels", "=", "list", "(", "masked_lm_labels", ")", "\n", "#masked_lm_labels[0] = -1", "\n", "\n", "next_sentence_label", "=", "1", "if", "is_random_next", "else", "0", "\n", "\n", "if", "len", "(", "[", "x", "for", "x", "in", "entity", "if", "x", ">", "-", "1", "]", ")", ">=", "5", ":", "\n", "                    ", "ds", ".", "add_item", "(", "torch", ".", "IntTensor", "(", "input_ids", "+", "input_mask", "+", "segment_ids", "+", "masked_lm_labels", "+", "entity", "+", "entity_mask", "+", "[", "next_sentence_label", "]", ")", ")", "\n", "\n", "\n", "", "", "current_chunk", "=", "[", "]", "\n", "current_length", "=", "0", "\n", "", "i", "+=", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.create_masked_lm_predictions": [[211, 249], ["enumerate", "rng.shuffle", "list", "min", "set", "sorted", "cand_indexes.append", "max", "set.add", "sorted.append", "masked_lm_positions.append", "masked_lm_labels.append", "int", "len", "rng.random", "MaskedLmInstance", "round", "rng.random", "rng.randint", "len"], "function", ["None"], ["", "", "def", "create_masked_lm_predictions", "(", "tokens", ",", "masked_lm_prob", ",", "\n", "max_predictions_per_seq", ",", "rng", ")", ":", "\n", "    ", "cand_indexes", "=", "[", "]", "\n", "for", "(", "i", ",", "token", ")", "in", "enumerate", "(", "tokens", ")", ":", "\n", "        ", "if", "token", "==", "101", "or", "token", "==", "102", ":", "\n", "            ", "continue", "\n", "", "cand_indexes", ".", "append", "(", "i", ")", "\n", "\n", "", "rng", ".", "shuffle", "(", "cand_indexes", ")", "\n", "output_tokens", "=", "list", "(", "tokens", ")", "\n", "num_to_predict", "=", "min", "(", "max_predictions_per_seq", ",", "\n", "max", "(", "1", ",", "int", "(", "round", "(", "len", "(", "tokens", ")", "*", "masked_lm_prob", ")", ")", ")", ")", "\n", "\n", "masked_lms", "=", "[", "]", "\n", "covered_indexes", "=", "set", "(", ")", "\n", "for", "index", "in", "cand_indexes", ":", "\n", "        ", "if", "len", "(", "masked_lms", ")", ">=", "num_to_predict", ":", "\n", "            ", "break", "\n", "", "if", "index", "in", "covered_indexes", ":", "\n", "            ", "continue", "\n", "", "covered_indexes", ".", "add", "(", "index", ")", "\n", "masked_token", "=", "None", "\n", "if", "rng", ".", "random", "(", ")", "<", "0.8", ":", "\n", "            ", "masked_token", "=", "103", "# [MASK]", "\n", "", "else", ":", "\n", "            ", "if", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "                ", "masked_token", "=", "tokens", "[", "index", "]", "\n", "", "else", ":", "\n", "                 ", "masked_token", "=", "rng", ".", "randint", "(", "0", ",", "vocab_words_size", "-", "1", ")", "\n", "", "", "output_tokens", "[", "index", "]", "=", "masked_token", "\n", "masked_lms", ".", "append", "(", "MaskedLmInstance", "(", "index", "=", "index", ",", "label", "=", "tokens", "[", "index", "]", ")", ")", "\n", "", "masked_lms", "=", "sorted", "(", "masked_lms", ",", "key", "=", "lambda", "x", ":", "x", ".", "index", ")", "\n", "masked_lm_positions", "=", "[", "]", "\n", "masked_lm_labels", "=", "[", "]", "\n", "for", "p", "in", "masked_lms", ":", "\n", "        ", "masked_lm_positions", ".", "append", "(", "p", ".", "index", ")", "\n", "masked_lm_labels", ".", "append", "(", "p", ".", "label", ")", "\n", "", "return", "(", "output_tokens", ",", "masked_lm_positions", ",", "masked_lm_labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.truncate_seq_pair": [[252, 266], ["len", "len", "len", "rng.random", "trunc_tokens.pop", "trunc_entity.pop", "len", "len", "len", "len"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop"], ["", "def", "truncate_seq_pair", "(", "tokens_a", ",", "tokens_b", ",", "entity_a", ",", "entity_b", ",", "max_num_tokens", ",", "rng", ")", ":", "\n", "    ", "while", "True", ":", "\n", "        ", "total_length", "=", "len", "(", "tokens_a", ")", "+", "len", "(", "tokens_b", ")", "\n", "if", "total_length", "<=", "max_num_tokens", ":", "\n", "            ", "break", "\n", "", "trunc_tokens", "=", "tokens_a", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", "else", "tokens_b", "\n", "trunc_entity", "=", "entity_a", "if", "len", "(", "tokens_a", ")", ">", "len", "(", "tokens_b", ")", "else", "entity_b", "\n", "assert", "len", "(", "trunc_tokens", ")", ">=", "1", "\n", "if", "rng", ".", "random", "(", ")", "<", "0.5", ":", "\n", "            ", "del", "trunc_tokens", "[", "0", "]", "\n", "del", "trunc_entity", "[", "0", "]", "\n", "", "else", ":", "\n", "            ", "trunc_tokens", ".", "pop", "(", ")", "\n", "trunc_entity", ".", "pop", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.write_instance_to_example_files": [[268, 317], ["indexed_dataset.IndexedDatasetBuilder", "enumerate", "indexed_dataset.IndexedDatasetBuilder.finalize", "tensorflow.gfile.GFile", "list", "list", "list", "list", "list", "indexed_dataset.IndexedDatasetBuilder.add_item", "vocab_words.append", "len", "len", "len", "list.extend", "input_mask.extend", "list.extend", "numpy.ones", "torch.IntTensor", "tensorflow.logging.info", "tensorflow.logging.info", "list", "enumerate", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "tensorflow.logging.info", "line.strip", "len", "len", "str", "str"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDatasetBuilder.finalize", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDatasetBuilder.add_item"], ["", "", "", "def", "write_instance_to_example_files", "(", "instances", ",", "max_seq_length", ",", "\n", "max_predictions_per_seq", ",", "output_file", ",", "vocab_file", ")", ":", "\n", "# read vocab", "\n", "    ", "vocab_words", "=", "[", "]", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "'r'", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "vocab_words", ".", "append", "(", "line", ".", "strip", "(", ")", ")", "\n", "\n", "", "", "ds", "=", "indexed_dataset", ".", "IndexedDatasetBuilder", "(", "output_file", "+", "\".bin\"", ")", "\n", "for", "(", "inst_index", ",", "instance", ")", "in", "enumerate", "(", "instances", ")", ":", "\n", "        ", "input_mask", "=", "[", "1", "]", "*", "len", "(", "instance", ".", "input_ids", ")", "\n", "segment_ids", "=", "list", "(", "instance", ".", "segment_ids", ")", "\n", "input_ids", "=", "list", "(", "instance", ".", "input_ids", ")", "\n", "assert", "len", "(", "input_ids", ")", "<=", "max_seq_length", "\n", "if", "len", "(", "input_ids", ")", "<", "max_seq_length", ":", "\n", "            ", "rest", "=", "max_seq_length", "-", "len", "(", "input_ids", ")", "\n", "input_ids", ".", "extend", "(", "[", "0", "]", "*", "rest", ")", "\n", "input_mask", ".", "extend", "(", "[", "0", "]", "*", "rest", ")", "\n", "segment_ids", ".", "extend", "(", "[", "0", "]", "*", "rest", ")", "\n", "\n", "", "masked_lm_positions", "=", "list", "(", "instance", ".", "masked_lm_positions", ")", "\n", "masked_lm_ids", "=", "list", "(", "instance", ".", "masked_lm_labels", ")", "\n", "masked_lm_labels", "=", "np", ".", "ones", "(", "len", "(", "input_ids", ")", ",", "dtype", "=", "int", ")", "*", "-", "1", "\n", "masked_lm_labels", "[", "masked_lm_positions", "]", "=", "masked_lm_ids", "\n", "masked_lm_labels", "=", "list", "(", "masked_lm_labels", ")", "\n", "masked_lm_labels", "[", "0", "]", "=", "-", "1", "\n", "\n", "next_sentence_label", "=", "1", "if", "instance", ".", "is_random_next", "else", "0", "\n", "\n", "ds", ".", "add_item", "(", "torch", ".", "IntTensor", "(", "input_ids", "+", "input_mask", "+", "segment_ids", "+", "masked_lm_labels", "+", "[", "next_sentence_label", "]", ")", ")", "\n", "\n", "if", "inst_index", "<", "20", ":", "\n", "            ", "tf", ".", "logging", ".", "info", "(", "\"*** Example ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "vocab_words", "[", "x", "]", "for", "x", "in", "instance", ".", "input_ids", "]", ")", ")", "\n", "\n", "unmask", "=", "list", "(", "instance", ".", "input_ids", ")", "\n", "for", "i", ",", "x", "in", "enumerate", "(", "masked_lm_labels", ")", ":", "\n", "                ", "if", "x", "!=", "-", "1", ":", "\n", "                    ", "unmask", "[", "i", "]", "=", "x", "\n", "", "", "tf", ".", "logging", ".", "info", "(", "\"unmask_tokens: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "vocab_words", "[", "x", "]", "for", "x", "in", "unmask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"input_mask: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "input_mask", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"segment: %s\"", "%", "\" \"", ".", "join", "(", "\n", "[", "str", "(", "x", ")", "for", "x", "in", "segment_ids", "]", ")", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"next_sentence: %d\"", "%", "next_sentence_label", ")", "\n", "\n", "", "", "ds", ".", "finalize", "(", "output_file", "+", "\".idx\"", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.main": [[319, 330], ["tensorflow.logging.set_verbosity", "tensorflow.logging.info", "tensorflow.logging.info", "random.Random", "create_instances.create_training_instances"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.create_instances.create_training_instances"], ["", "def", "main", "(", "_", ")", ":", "\n", "    ", "tf", ".", "logging", ".", "set_verbosity", "(", "tf", ".", "logging", ".", "INFO", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"*** Reading from input files ***\"", ")", "\n", "tf", ".", "logging", ".", "info", "(", "\"%s\"", ",", "FLAGS", ".", "input_file_prefix", ")", "\n", "\n", "rng", "=", "random", ".", "Random", "(", "FLAGS", ".", "random_seed", ")", "\n", "\n", "create_training_instances", "(", "\n", "FLAGS", ".", "input_file_prefix", ",", "FLAGS", ".", "output_file", ",", "FLAGS", ".", "max_seq_length", ",", "FLAGS", ".", "dupe_factor", ",", "\n", "FLAGS", ".", "short_seq_prob", ",", "FLAGS", ".", "masked_lm_prob", ",", "FLAGS", ".", "max_predictions_per_seq", ",", "\n", "rng", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.__init__": [[69, 126], ["isinstance", "json.loads.items", "isinstance", "open", "json.loads", "ValueError", "reader.read"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "768", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ",", "\n", "layer_types", "=", "None", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "self", ".", "layer_types", "=", "layer_types", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.from_dict": [[128, 135], ["modeling.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.from_json_file": [[136, 142], ["cls.from_dict", "open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.from_dict", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.__repr__": [[143, 145], ["str", "modeling.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.to_dict": [[146, 150], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.to_json_string": [[151, 154], ["json.dumps", "modeling.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertEmbeddings.__init__": [[177, 187], ["torch.nn.Module.__init__", "torch.nn.Embedding", "torch.nn.Embedding", "torch.nn.Embedding", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEmbeddings", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "word_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "position_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "max_position_embeddings", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "token_type_embeddings", "=", "nn", ".", "Embedding", "(", "config", ".", "type_vocab_size", ",", "config", ".", "hidden_size", ")", "\n", "\n", "# self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load", "\n", "# any TensorFlow checkpoint file", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertEmbeddings.forward": [[188, 203], ["input_ids.size", "torch.arange", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze().expand_as", "modeling.BertEmbeddings.word_embeddings", "modeling.BertEmbeddings.position_embeddings", "modeling.BertEmbeddings.token_type_embeddings", "modeling.BertEmbeddings.LayerNorm", "modeling.BertEmbeddings.dropout", "torch.zeros_like", "position_ids.unsqueeze().expand_as.unsqueeze().expand_as.unsqueeze"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "seq_length", "=", "input_ids", ".", "size", "(", "1", ")", "\n", "position_ids", "=", "torch", ".", "arange", "(", "seq_length", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "input_ids", ".", "device", ")", "\n", "position_ids", "=", "position_ids", ".", "unsqueeze", "(", "0", ")", ".", "expand_as", "(", "input_ids", ")", "\n", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "", "words_embeddings", "=", "self", ".", "word_embeddings", "(", "input_ids", ")", "\n", "position_embeddings", "=", "self", ".", "position_embeddings", "(", "position_ids", ")", "\n", "token_type_embeddings", "=", "self", ".", "token_type_embeddings", "(", "token_type_ids", ")", "\n", "\n", "embeddings", "=", "words_embeddings", "+", "position_embeddings", "+", "token_type_embeddings", "\n", "embeddings", "=", "self", ".", "LayerNorm", "(", "embeddings", ")", "\n", "embeddings", "=", "self", ".", "dropout", "(", "embeddings", ")", "\n", "return", "embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertSelfAttention.__init__": [[206, 221], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "config", ".", "hidden_size", "%", "config", ".", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "config", ".", "hidden_size", ",", "config", ".", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "config", ".", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "config", ".", "hidden_size", "/", "config", ".", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "self", ".", "query", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "attention_probs_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertSelfAttention.transpose_for_scores": [[222, 226], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertSelfAttention.forward": [[227, 254], ["modeling.BertSelfAttention.query", "modeling.BertSelfAttention.key", "modeling.BertSelfAttention.value", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "modeling.BertSelfAttention.transpose_for_scores", "torch.matmul", "modeling.BertSelfAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "modeling.BertSelfAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertSelfAttention.transpose_for_scores", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertSelfAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "hidden_states", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "hidden_states", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertSelfOutput.__init__": [[257, 262], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertSelfOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertSelfOutput.forward": [[263, 268], ["modeling.BertSelfOutput.dense", "modeling.BertSelfOutput.dropout", "modeling.BertSelfOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertAttention.__init__": [[271, 282], ["torch.nn.Module.__init__", "modeling.BertSelfAttention", "modeling.BertSelfOutput", "copy.deepcopy", "modeling.BertSelfAttention", "modeling.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n", "config_ent", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "config_ent", ".", "hidden_size", "=", "100", "\n", "config_ent", ".", "num_attention_heads", "=", "4", "\n", "\n", "self", ".", "self_ent", "=", "BertSelfAttention", "(", "config_ent", ")", "\n", "self", ".", "output_ent", "=", "BertSelfOutput", "(", "config_ent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertAttention.forward": [[284, 290], ["modeling.BertAttention.self", "modeling.BertAttention.self_ent", "modeling.BertAttention.output", "modeling.BertAttention.output_ent"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ",", "input_tensor_ent", ",", "attention_mask_ent", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ")", "\n", "self_output_ent", "=", "self", ".", "self_ent", "(", "input_tensor_ent", ",", "attention_mask_ent", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "attention_output_ent", "=", "self", ".", "output_ent", "(", "self_output_ent", ",", "input_tensor_ent", ")", "\n", "return", "attention_output", ",", "attention_output_ent", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertAttention_simple.__init__": [[292, 296], ["torch.nn.Module.__init__", "modeling.BertSelfAttention", "modeling.BertSelfOutput"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertAttention_simple", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertSelfAttention", "(", "config", ")", "\n", "self", ".", "output", "=", "BertSelfOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertAttention_simple.forward": [[297, 301], ["modeling.BertAttention_simple.self", "modeling.BertAttention_simple.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertIntermediate.__init__": [[303, 313], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "self", ".", "dense_ent", "=", "nn", ".", "Linear", "(", "100", ",", "config", ".", "intermediate_size", ")", "\n", "\n", "#self.dense_1 = nn.Linear(config.hidden_size, 200)", "\n", "#self.dense_1_ent = nn.Linear(100, 200)", "\n", "\n", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertIntermediate.forward": [[314, 325], ["modeling.BertIntermediate.dense", "modeling.BertIntermediate.dense_ent", "modeling.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "hidden_states_ent", ")", ":", "\n", "        ", "hidden_states_", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states_ent_", "=", "self", ".", "dense_ent", "(", "hidden_states_ent", ")", "\n", "\n", "#hidden_states_1 = self.dense_1(hidden_states)", "\n", "#hidden_states_ent_1 = self.dense_1_ent(hidden_states_ent)", "\n", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states_", "+", "hidden_states_ent_", ")", "\n", "#hidden_states_ent = self.intermediate_act_fn(hidden_states_1+hidden_states_ent_1)", "\n", "\n", "return", "hidden_states", "#, hidden_states_ent", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertIntermediate_simple.__init__": [[327, 332], ["torch.nn.Module.__init__", "torch.nn.Linear", "isinstance"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertIntermediate_simple", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "intermediate_size", ")", "\n", "self", ".", "intermediate_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertIntermediate_simple.forward": [[333, 337], ["modeling.BertIntermediate_simple.dense", "modeling.BertIntermediate_simple.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertOutput.__init__": [[339, 346], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "BertLayerNorm", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "dense_ent", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "100", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "LayerNorm_ent", "=", "BertLayerNorm", "(", "100", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertOutput.forward": [[347, 357], ["modeling.BertOutput.dense", "modeling.BertOutput.dropout", "modeling.BertOutput.LayerNorm", "modeling.BertOutput.dense_ent", "modeling.BertOutput.dropout", "modeling.BertOutput.LayerNorm_ent"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states_", ",", "input_tensor", ",", "input_tensor_ent", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states_", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "\n", "hidden_states_ent", "=", "self", ".", "dense_ent", "(", "hidden_states_", ")", "\n", "hidden_states_ent", "=", "self", ".", "dropout", "(", "hidden_states_ent", ")", "\n", "hidden_states_ent", "=", "self", ".", "LayerNorm_ent", "(", "hidden_states_ent", "+", "input_tensor_ent", ")", "\n", "\n", "return", "hidden_states", ",", "hidden_states_ent", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertOutput_simple.__init__": [[360, 365], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOutput_simple", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "intermediate_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertOutput_simple.forward": [[366, 372], ["modeling.BertOutput_simple.dense", "modeling.BertOutput_simple.dropout", "modeling.BertOutput_simple.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertLayerMix.__init__": [[374, 379], ["torch.nn.Module.__init__", "modeling.BertAttention_simple", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayerMix", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention_simple", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertLayerMix.forward": [[380, 387], ["modeling.BertLayerMix.attention", "modeling.BertLayerMix.intermediate", "modeling.BertLayerMix.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "hidden_states_ent", ",", "attention_mask_ent", ",", "ent_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "attention_output_ent", "=", "hidden_states_ent", "*", "ent_mask", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ",", "attention_output_ent", ")", "\n", "layer_output", ",", "layer_output_ent", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ",", "attention_output_ent", ")", "\n", "# layer_output_ent = layer_output_ent * ent_mask", "\n", "return", "layer_output", ",", "layer_output_ent", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertLayer.__init__": [[389, 394], ["torch.nn.Module.__init__", "modeling.BertAttention", "modeling.BertIntermediate", "modeling.BertOutput"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertLayer.forward": [[395, 402], ["modeling.BertLayer.attention", "modeling.BertLayer.intermediate", "modeling.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "hidden_states_ent", ",", "attention_mask_ent", ",", "ent_mask", ")", ":", "\n", "        ", "attention_output", ",", "attention_output_ent", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ",", "hidden_states_ent", ",", "attention_mask_ent", ")", "\n", "attention_output_ent", "=", "attention_output_ent", "*", "ent_mask", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ",", "attention_output_ent", ")", "\n", "layer_output", ",", "layer_output_ent", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ",", "attention_output_ent", ")", "\n", "# layer_output_ent = layer_output_ent * ent_mask", "\n", "return", "layer_output", ",", "layer_output_ent", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertLayer_simple.__init__": [[404, 409], ["torch.nn.Module.__init__", "modeling.BertAttention_simple", "modeling.BertIntermediate_simple", "modeling.BertOutput_simple"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertLayer_simple", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertAttention_simple", "(", "config", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate_simple", "(", "config", ")", "\n", "self", ".", "output", "=", "BertOutput_simple", "(", "config", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertLayer_simple.forward": [[410, 415], ["modeling.BertLayer_simple.attention", "modeling.BertLayer_simple.intermediate", "modeling.BertLayer_simple.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "hidden_states_ent", ",", "attention_mask_ent", ",", "ent_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", ",", "hidden_states_ent", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertEncoder.__init__": [[418, 434], ["torch.nn.Module.__init__", "modeling.BertLayer", "modeling.BertLayer_simple", "modeling.BertLayerMix", "range", "torch.nn.ModuleList", "layers.append", "layers.append", "layers.append", "layers.append", "len", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy", "copy.deepcopy"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder", ",", "self", ")", ".", "__init__", "(", ")", "\n", "layer", "=", "BertLayer", "(", "config", ")", "\n", "layer_simple", "=", "BertLayer_simple", "(", "config", ")", "\n", "layer_mix", "=", "BertLayerMix", "(", "config", ")", "\n", "layers", "=", "[", "]", "\n", "for", "t", "in", "config", ".", "layer_types", ":", "\n", "            ", "if", "t", "==", "\"sim\"", ":", "\n", "                ", "layers", ".", "append", "(", "copy", ".", "deepcopy", "(", "layer_simple", ")", ")", "\n", "", "if", "t", "==", "\"norm\"", ":", "\n", "                ", "layers", ".", "append", "(", "copy", ".", "deepcopy", "(", "layer", ")", ")", "\n", "", "if", "t", "==", "\"mix\"", ":", "\n", "                ", "layers", ".", "append", "(", "copy", ".", "deepcopy", "(", "layer_mix", ")", ")", "\n", "", "", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", "-", "len", "(", "layers", ")", ")", ":", "\n", "            ", "layers", ".", "append", "(", "copy", ".", "deepcopy", "(", "layer_simple", ")", ")", "\n", "", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertEncoder.forward": [[435, 450], ["ent_mask.to().unsqueeze.to().unsqueeze.to().unsqueeze", "layer_module", "all_encoder_layers.append", "ent_mask.to().unsqueeze.to().unsqueeze.to", "all_encoder_layers.append", "next", "modeling.BertEncoder.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ",", "hidden_states_ent", ",", "attention_mask_ent", ",", "ent_mask", ",", "output_all_encoded_layers", "=", "True", ")", ":", "\n", "        ", "all_encoder_layers", "=", "[", "]", "\n", "ent_mask", "=", "ent_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# if self.training:", "\n", "#     ent_mask = ent_mask.half().unsqueeze(-1)", "\n", "# else:", "\n", "#     ent_mask = ent_mask.float().unsqueeze(-1)", "\n", "# ent_mask = ent_mask.float().unsqueeze(-1)", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "hidden_states", ",", "hidden_states_ent", "=", "layer_module", "(", "hidden_states", ",", "attention_mask", ",", "hidden_states_ent", ",", "attention_mask_ent", ",", "ent_mask", ")", "\n", "if", "output_all_encoded_layers", ":", "\n", "                ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "all_encoder_layers", ".", "append", "(", "hidden_states", ")", "\n", "", "return", "all_encoder_layers", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertPooler.__init__": [[453, 457], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertPooler.forward": [[458, 465], ["modeling.BertPooler.dense", "modeling.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertPredictionHeadTransform.__init__": [[468, 474], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "isinstance"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertPredictionHeadTransform", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "768", ",", "config", ".", "hidden_size", ")", "\n", "self", ".", "transform_act_fn", "=", "ACT2FN", "[", "config", ".", "hidden_act", "]", "if", "isinstance", "(", "config", ".", "hidden_act", ",", "str", ")", "else", "config", ".", "hidden_act", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "config", ".", "hidden_size", ",", "eps", "=", "1e-12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertPredictionHeadTransform.forward": [[475, 480], ["modeling.BertPredictionHeadTransform.dense", "modeling.BertPredictionHeadTransform.transform_act_fn", "modeling.BertPredictionHeadTransform.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "transform_act_fn", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertLMPredictionHead.__init__": [[483, 494], ["torch.nn.Module.__init__", "modeling.BertPredictionHeadTransform", "torch.nn.Linear", "torch.nn.Parameter", "bert_model_embedding_weights.size", "bert_model_embedding_weights.size", "torch.zeros", "bert_model_embedding_weights.size"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertLMPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config", ")", "\n", "\n", "# The output weights are the same as the input embeddings, but there is", "\n", "# an output-only bias for each token.", "\n", "self", ".", "decoder", "=", "nn", ".", "Linear", "(", "bert_model_embedding_weights", ".", "size", "(", "1", ")", ",", "\n", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ",", "\n", "bias", "=", "False", ")", "\n", "self", ".", "decoder", ".", "weight", "=", "bert_model_embedding_weights", "\n", "self", ".", "bias", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "bert_model_embedding_weights", ".", "size", "(", "0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertLMPredictionHead.forward": [[495, 499], ["modeling.BertLMPredictionHead.transform", "modeling.BertLMPredictionHead.decoder"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "decoder", "(", "hidden_states", ")", "+", "self", ".", "bias", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertEntPredictionHead.__init__": [[501, 506], ["torch.nn.Module.__init__", "copy.deepcopy", "modeling.BertPredictionHeadTransform"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEntPredictionHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "config_ent", "=", "copy", ".", "deepcopy", "(", "config", ")", "\n", "config_ent", ".", "hidden_size", "=", "100", "\n", "self", ".", "transform", "=", "BertPredictionHeadTransform", "(", "config_ent", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertEntPredictionHead.forward": [[507, 514], ["modeling.BertEntPredictionHead.transform", "torch.squeeze", "torch.matmul", "torch.squeeze.t"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Extractor.transform"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "candidate", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "transform", "(", "hidden_states", ")", "\n", "candidate", "=", "torch", ".", "squeeze", "(", "candidate", ",", "0", ")", "\n", "# hidden_states [batch_size, max_seq, dim]", "\n", "# candidate [entity_num_in_the_batch, dim]", "\n", "# return [batch_size, max_seq, entity_num_in_the_batch]", "\n", "return", "torch", ".", "matmul", "(", "hidden_states", ",", "candidate", ".", "t", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertOnlyMLMHead.__init__": [[517, 520], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertOnlyMLMHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertOnlyMLMHead.forward": [[521, 524], ["modeling.BertOnlyMLMHead.predictions"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertOnlyNSPHead.__init__": [[527, 530], ["torch.nn.Module.__init__", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertOnlyNSPHead", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertOnlyNSPHead.forward": [[531, 534], ["modeling.BertOnlyNSPHead.seq_relationship"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "pooled_output", ")", ":", "\n", "        ", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertPreTrainingHeads.__init__": [[537, 542], ["torch.nn.Module.__init__", "modeling.BertLMPredictionHead", "modeling.BertEntPredictionHead", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "bert_model_embedding_weights", ")", ":", "\n", "        ", "super", "(", "BertPreTrainingHeads", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "predictions", "=", "BertLMPredictionHead", "(", "config", ",", "bert_model_embedding_weights", ")", "\n", "self", ".", "predictions_ent", "=", "BertEntPredictionHead", "(", "config", ")", "\n", "self", ".", "seq_relationship", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertPreTrainingHeads.forward": [[543, 548], ["modeling.BertPreTrainingHeads.predictions", "modeling.BertPreTrainingHeads.seq_relationship", "modeling.BertPreTrainingHeads.predictions_ent"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "sequence_output", ",", "pooled_output", ",", "candidate", ")", ":", "\n", "        ", "prediction_scores", "=", "self", ".", "predictions", "(", "sequence_output", ")", "\n", "seq_relationship_score", "=", "self", ".", "seq_relationship", "(", "pooled_output", ")", "\n", "prediction_scores_ent", "=", "self", ".", "predictions_ent", "(", "sequence_output", ",", "candidate", ")", "\n", "return", "prediction_scores", ",", "seq_relationship_score", ",", "prediction_scores_ent", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.PreTrainedBertModel.__init__": [[554, 564], ["torch.nn.Module.__init__", "isinstance", "ValueError"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "super", "(", "PreTrainedBertModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "if", "not", "isinstance", "(", "config", ",", "BertConfig", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"", "\n", "\"To create a model from a Google pretrained model use \"", "\n", "\"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "\n", "self", ".", "__class__", ".", "__name__", ",", "self", ".", "__class__", ".", "__name__", "\n", ")", ")", "\n", "", "self", ".", "config", "=", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.PreTrainedBertModel.init_bert_weights": [[565, 577], ["isinstance", "module.weight.data.normal_", "isinstance", "isinstance", "module.bias.data.zero_", "module.bias.data.zero_", "module.weight.data.fill_"], "methods", ["None"], ["", "def", "init_bert_weights", "(", "self", ",", "module", ")", ":", "\n", "        ", "\"\"\" Initialize the weights.\n        \"\"\"", "\n", "if", "isinstance", "(", "module", ",", "(", "nn", ".", "Linear", ",", "nn", ".", "Embedding", ")", ")", ":", "\n", "# Slightly different from the TF version which uses truncated_normal for initialization", "\n", "# cf https://github.com/pytorch/pytorch/pull/5617", "\n", "            ", "module", ".", "weight", ".", "data", ".", "normal_", "(", "mean", "=", "0.0", ",", "std", "=", "self", ".", "config", ".", "initializer_range", ")", "\n", "", "elif", "isinstance", "(", "module", ",", "BertLayerNorm", ")", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "module", ".", "weight", ".", "data", ".", "fill_", "(", "1.0", ")", "\n", "", "if", "isinstance", "(", "module", ",", "nn", ".", "Linear", ")", "and", "module", ".", "bias", "is", "not", "None", ":", "\n", "            ", "module", ".", "bias", ".", "data", ".", "zero_", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.PreTrainedBertModel.from_pretrained": [[578, 683], ["os.path.isdir", "os.path.join", "modeling.BertConfig.from_json_file", "logger.info", "cls", "torch.load.keys", "zip", "getattr", "torch.load.copy", "modeling.PreTrainedBertModel.from_pretrained.load"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.from_json_file"], ["", "", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "state_dict", "=", "None", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file or a pytorch state dict.\n        Download and cache the pre-trained model file if needed.\n\n        Params:\n            pretrained_model_name: either:\n                - a str with the name of a pre-trained model to load selected in the list of:\n                    . `bert-base-uncased`\n                    . `bert-large-uncased`\n                    . `bert-base-cased`\n                    . `bert-base-multilingual`\n                    . `bert-base-chinese`\n                - a path or url to a pretrained model archive containing:\n                    . `bert_config.json` a configuration file for the model\n                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n            *inputs, **kwargs: additional input for the specific Bert class\n                (ex: num_labels for BertForSequenceClassification)\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_MODEL_ARCHIVE_MAP", ":", "\n", "            ", "archive_file", "=", "PRETRAINED_MODEL_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "archive_file", "=", "pretrained_model_name", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_archive_file", "=", "cached_path", "(", "archive_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_MODEL_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "archive_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_archive_file", "==", "archive_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {}\"", ".", "format", "(", "archive_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading archive file {} from cache at {}\"", ".", "format", "(", "\n", "archive_file", ",", "resolved_archive_file", ")", ")", "\n", "", "tempdir", "=", "None", "\n", "if", "os", ".", "path", ".", "isdir", "(", "resolved_archive_file", ")", ":", "\n", "            ", "serialization_dir", "=", "resolved_archive_file", "\n", "", "else", ":", "\n", "# Extract archive to temp dir", "\n", "            ", "tempdir", "=", "tempfile", ".", "mkdtemp", "(", ")", "\n", "logger", ".", "info", "(", "\"extracting archive file {} to temp dir {}\"", ".", "format", "(", "\n", "resolved_archive_file", ",", "tempdir", ")", ")", "\n", "with", "tarfile", ".", "open", "(", "resolved_archive_file", ",", "'r:gz'", ")", "as", "archive", ":", "\n", "                ", "archive", ".", "extractall", "(", "tempdir", ")", "\n", "", "serialization_dir", "=", "tempdir", "\n", "# Load config", "\n", "", "config_file", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "CONFIG_NAME", ")", "\n", "config", "=", "BertConfig", ".", "from_json_file", "(", "config_file", ")", "\n", "logger", ".", "info", "(", "\"Model config {}\"", ".", "format", "(", "config", ")", ")", "\n", "# Instantiate model.", "\n", "model", "=", "cls", "(", "config", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "if", "state_dict", "is", "None", ":", "\n", "            ", "weights_path", "=", "os", ".", "path", ".", "join", "(", "serialization_dir", ",", "WEIGHTS_NAME", ")", "\n", "state_dict", "=", "torch", ".", "load", "(", "weights_path", ")", "\n", "\n", "", "old_keys", "=", "[", "]", "\n", "new_keys", "=", "[", "]", "\n", "for", "key", "in", "state_dict", ".", "keys", "(", ")", ":", "\n", "            ", "new_key", "=", "None", "\n", "if", "'gamma'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'gamma'", ",", "'weight'", ")", "\n", "", "if", "'beta'", "in", "key", ":", "\n", "                ", "new_key", "=", "key", ".", "replace", "(", "'beta'", ",", "'bias'", ")", "\n", "", "if", "new_key", ":", "\n", "                ", "old_keys", ".", "append", "(", "key", ")", "\n", "new_keys", ".", "append", "(", "new_key", ")", "\n", "", "", "for", "old_key", ",", "new_key", "in", "zip", "(", "old_keys", ",", "new_keys", ")", ":", "\n", "            ", "state_dict", "[", "new_key", "]", "=", "state_dict", ".", "pop", "(", "old_key", ")", "\n", "\n", "", "missing_keys", "=", "[", "]", "\n", "unexpected_keys", "=", "[", "]", "\n", "error_msgs", "=", "[", "]", "\n", "# copy state_dict so _load_from_state_dict can modify it", "\n", "metadata", "=", "getattr", "(", "state_dict", ",", "'_metadata'", ",", "None", ")", "\n", "state_dict", "=", "state_dict", ".", "copy", "(", ")", "\n", "if", "metadata", "is", "not", "None", ":", "\n", "            ", "state_dict", ".", "_metadata", "=", "metadata", "\n", "\n", "", "def", "load", "(", "module", ",", "prefix", "=", "''", ")", ":", "\n", "            ", "local_metadata", "=", "{", "}", "if", "metadata", "is", "None", "else", "metadata", ".", "get", "(", "prefix", "[", ":", "-", "1", "]", ",", "{", "}", ")", "\n", "module", ".", "_load_from_state_dict", "(", "\n", "state_dict", ",", "prefix", ",", "local_metadata", ",", "True", ",", "missing_keys", ",", "unexpected_keys", ",", "error_msgs", ")", "\n", "for", "name", ",", "child", "in", "module", ".", "_modules", ".", "items", "(", ")", ":", "\n", "                ", "if", "child", "is", "not", "None", ":", "\n", "                    ", "load", "(", "child", ",", "prefix", "+", "name", "+", "'.'", ")", "\n", "", "", "", "load", "(", "model", ",", "prefix", "=", "''", "if", "hasattr", "(", "model", ",", "'bert'", ")", "else", "'bert.'", ")", "\n", "if", "len", "(", "missing_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights of {} not initialized from pretrained model: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "missing_keys", ")", ")", "\n", "", "if", "len", "(", "unexpected_keys", ")", ">", "0", ":", "\n", "            ", "logger", ".", "info", "(", "\"Weights from pretrained model not used in {}: {}\"", ".", "format", "(", "\n", "model", ".", "__class__", ".", "__name__", ",", "unexpected_keys", ")", ")", "\n", "", "if", "tempdir", ":", "\n", "# Clean up temp dir", "\n", "            ", "shutil", ".", "rmtree", "(", "tempdir", ")", "\n", "", "return", "model", ",", "missing_keys", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertModel.__init__": [[729, 735], ["modeling.PreTrainedBertModel.__init__", "modeling.BertEmbeddings", "modeling.BertEncoder", "modeling.BertPooler", "modeling.BertModel.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertModel.forward": [[736, 772], ["torch.ones_like.unsqueeze().unsqueeze", "ent_mask.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "extended_ent_mask.to.to.to", "modeling.BertModel.embeddings", "modeling.BertModel.encoder", "modeling.BertModel.pooler", "torch.ones_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "ent_mask.unsqueeze", "next", "next", "modeling.BertModel.parameters", "modeling.BertModel.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "input_ent", "=", "None", ",", "ent_mask", "=", "None", ",", "output_all_encoded_layers", "=", "True", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "extended_ent_mask", "=", "ent_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "extended_ent_mask", "=", "extended_ent_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_ent_mask", "=", "(", "1.0", "-", "extended_ent_mask", ")", "*", "-", "10000.0", "\n", "\n", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "encoded_layers", "=", "self", ".", "encoder", "(", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "input_ent", ",", "\n", "extended_ent_mask", ",", "\n", "ent_mask", ",", "\n", "output_all_encoded_layers", "=", "output_all_encoded_layers", ")", "\n", "sequence_output", "=", "encoded_layers", "[", "-", "1", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "if", "not", "output_all_encoded_layers", ":", "\n", "            ", "encoded_layers", "=", "encoded_layers", "[", "-", "1", "]", "\n", "", "return", "encoded_layers", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForPreTraining.__init__": [[824, 829], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertPreTrainingHeads", "modeling.BertForPreTraining.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForPreTraining", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertPreTrainingHeads", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForPreTraining.forward": [[830, 847], ["modeling.BertForPreTraining.bert", "modeling.BertForPreTraining.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "prediction_scores.view", "masked_lm_labels.view", "seq_relationship_score.view", "next_sentence_label.view", "prediction_scores_ent.view", "ent_labels.view", "candidate.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ",", "\n", "input_ent", "=", "None", ",", "ent_mask", "=", "None", ",", "next_sentence_label", "=", "None", ",", "candidate", "=", "None", ",", "ent_labels", "=", "None", ")", ":", "\n", "# the id in ent_labels should be consistent with the order of candidate.", "\n", "        ", "sequence_output", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "input_ent", ",", "ent_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", ",", "seq_relationship_score", ",", "prediction_scores_ent", "=", "self", ".", "cls", "(", "sequence_output", ",", "pooled_output", ",", "candidate", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", "and", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "ent_ae_loss", "=", "loss_fct", "(", "prediction_scores_ent", ".", "view", "(", "-", "1", ",", "candidate", ".", "size", "(", ")", "[", "0", "]", ")", ",", "ent_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "total_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "+", "ent_ae_loss", "\n", "original_loss", "=", "masked_lm_loss", "+", "next_sentence_loss", "\n", "return", "total_loss", ",", "original_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", ",", "seq_relationship_score", ",", "prediction_scores_ent", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForMaskedLM.__init__": [[891, 896], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertOnlyMLMHead", "modeling.BertForMaskedLM.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForMaskedLM", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyMLMHead", "(", "config", ",", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForMaskedLM.forward": [[897, 908], ["modeling.BertForMaskedLM.bert", "modeling.BertForMaskedLM.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForMaskedLM.view", "masked_lm_labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "input_ents", ",", "ent_mask", "=", "None", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "masked_lm_labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "input_ents", ",", "ent_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "prediction_scores", "=", "self", ".", "cls", "(", "sequence_output", ")", "\n", "\n", "if", "masked_lm_labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "masked_lm_loss", "=", "loss_fct", "(", "prediction_scores", ".", "view", "(", "-", "1", ",", "self", ".", "config", ".", "vocab_size", ")", ",", "masked_lm_labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "masked_lm_loss", "\n", "", "else", ":", "\n", "            ", "return", "prediction_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForNextSentencePrediction.__init__": [[953, 958], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "modeling.BertOnlyNSPHead", "modeling.BertForNextSentencePrediction.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForNextSentencePrediction", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "cls", "=", "BertOnlyNSPHead", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForNextSentencePrediction.forward": [[959, 970], ["modeling.BertForNextSentencePrediction.bert", "modeling.BertForNextSentencePrediction.cls", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForNextSentencePrediction.view", "next_sentence_label.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "next_sentence_label", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "\n", "output_all_encoded_layers", "=", "False", ")", "\n", "seq_relationship_score", "=", "self", ".", "cls", "(", "pooled_output", ")", "\n", "\n", "if", "next_sentence_label", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "-", "1", ")", "\n", "next_sentence_loss", "=", "loss_fct", "(", "seq_relationship_score", ".", "view", "(", "-", "1", ",", "2", ")", ",", "next_sentence_label", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "next_sentence_loss", "\n", "", "else", ":", "\n", "            ", "return", "seq_relationship_score", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForEntityTyping.__init__": [[974, 981], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForEntityTyping.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForEntityTyping", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "typing", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ",", "False", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForEntityTyping.forward": [[982, 993], ["modeling.BertForEntityTyping.bert", "modeling.BertForEntityTyping.dropout", "modeling.BertForEntityTyping.typing", "torch.nn.BCEWithLogitsLoss", "torch.nn.BCEWithLogitsLoss.", "modeling.BertForEntityTyping.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "input_ent", "=", "None", ",", "ent_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "input_ent", ",", "ent_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "typing", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "BCEWithLogitsLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForSTSB.__init__": [[995, 1005], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForSTSB.apply", "torch.nn.LogSoftmax", "torch.nn.Softmax"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForSTSB", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "2", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n", "self", ".", "m", "=", "torch", ".", "nn", ".", "LogSoftmax", "(", "-", "1", ")", "\n", "self", ".", "mm", "=", "torch", ".", "nn", ".", "Softmax", "(", "-", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForSTSB.forward": [[1006, 1021], ["modeling.BertForSTSB.bert", "modeling.BertForSTSB.dropout", "modeling.BertForSTSB.classifier", "modeling.BertForSTSB.m", "torch.mean", "modeling.BertForSTSB.mm", "torch.sum"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "input_ent", "=", "None", ",", "ent_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "input_ent", ",", "ent_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "probs", "=", "self", ".", "m", "(", "logits", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "#loss_fct = CrossEntropyLoss()", "\n", "#loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))", "\n", "            ", "per_example_loss", "=", "-", "torch", ".", "sum", "(", "labels", "*", "probs", ",", "-", "1", ")", "\n", "loss", "=", "torch", ".", "mean", "(", "per_example_loss", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "mm", "(", "logits", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForSequenceClassification.__init__": [[1068, 1075], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForSequenceClassification.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForSequenceClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForSequenceClassification.forward": [[1076, 1087], ["modeling.BertForSequenceClassification.bert", "modeling.BertForSequenceClassification.dropout", "modeling.BertForSequenceClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForSequenceClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "input_ent", "=", "None", ",", "ent_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "input_ent", ",", "ent_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForNQ.__init__": [[1092, 1099], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForNQ.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_choices", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForNQ", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_choices", "=", "num_choices", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForNQ.forward": [[1100, 1127], ["input_ids.view", "token_type_ids.view", "attention_mask.view", "input_ent.view", "ent_mask.view", "modeling.BertForNQ.bert", "modeling.BertForNQ.dropout", "modeling.BertForNQ.classifier", "modeling.BertForNQ.view", "torch.zeros().cuda", "input_ids.size", "token_type_ids.size", "attention_mask.size", "input_ent.size", "input_ent.size", "ent_mask.size", "torch.cat", "torch.FloatTensor().cuda", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.zeros", "torch.FloatTensor"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "input_ent", "=", "None", ",", "ent_mask", "=", "None", ",", "choice_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "#if choice_mask==None:", "\n", "#    _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, input_ent, ent_mask, output_all_encoded_layers=False)", "\n", "#    pooled_output = self.dropout(pooled_output)", "\n", "#    logits = self.classifier(pooled_output)", "\n", "#    return logits", "\n", "\n", "        ", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_input_ent", "=", "input_ent", ".", "view", "(", "-", "1", ",", "input_ent", ".", "size", "(", "-", "2", ")", ",", "input_ent", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_ent_mask", "=", "ent_mask", ".", "view", "(", "-", "1", ",", "ent_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "flat_input_ids", ",", "flat_token_type_ids", ",", "flat_attention_mask", ",", "flat_input_ent", ",", "flat_ent_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_choices", ")", "\n", "\n", "null_socre", "=", "torch", ".", "zeros", "(", "[", "labels", ".", "shape", "[", "0", "]", ",", "1", "]", ")", ".", "cuda", "(", ")", "\n", "reshaped_logits", "=", "torch", ".", "cat", "(", "[", "null_socre", ",", "reshaped_logits", "]", ",", "-", "1", ")", "+", "choice_mask", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "weight", "=", "torch", ".", "FloatTensor", "(", "[", "0.3", "]", "+", "[", "1", "]", "*", "16", ")", ".", "cuda", "(", ")", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "weight", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", "+", "1", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "reshaped_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForMultipleChoice.__init__": [[1172, 1179], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForMultipleChoice.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_choices", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForMultipleChoice", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_choices", "=", "num_choices", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "1", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForMultipleChoice.forward": [[1180, 1195], ["input_ids.view", "token_type_ids.view", "attention_mask.view", "modeling.BertForMultipleChoice.bert", "modeling.BertForMultipleChoice.dropout", "modeling.BertForMultipleChoice.classifier", "modeling.BertForMultipleChoice.view", "input_ids.size", "token_type_ids.size", "attention_mask.size", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss."], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "flat_input_ids", "=", "input_ids", ".", "view", "(", "-", "1", ",", "input_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_token_type_ids", "=", "token_type_ids", ".", "view", "(", "-", "1", ",", "token_type_ids", ".", "size", "(", "-", "1", ")", ")", "\n", "flat_attention_mask", "=", "attention_mask", ".", "view", "(", "-", "1", ",", "attention_mask", ".", "size", "(", "-", "1", ")", ")", "\n", "_", ",", "pooled_output", "=", "self", ".", "bert", "(", "flat_input_ids", ",", "flat_token_type_ids", ",", "flat_attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "reshaped_logits", "=", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_choices", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "reshaped_logits", ",", "labels", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "reshaped_logits", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForTokenClassification.__init__": [[1242, 1249], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Dropout", "torch.nn.Linear", "modeling.BertForTokenClassification.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ",", "num_labels", "=", "2", ")", ":", "\n", "        ", "super", "(", "BertForTokenClassification", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "num_labels", "=", "num_labels", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "num_labels", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForTokenClassification.forward": [[1250, 1261], ["modeling.BertForTokenClassification.bert", "modeling.BertForTokenClassification.dropout", "modeling.BertForTokenClassification.classifier", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "modeling.BertForTokenClassification.view", "labels.view"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "labels", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "sequence_output", "=", "self", ".", "dropout", "(", "sequence_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "sequence_output", ")", "\n", "\n", "if", "labels", "is", "not", "None", ":", "\n", "            ", "loss_fct", "=", "CrossEntropyLoss", "(", ")", "\n", "loss", "=", "loss_fct", "(", "logits", ".", "view", "(", "-", "1", ",", "self", ".", "num_labels", ")", ",", "labels", ".", "view", "(", "-", "1", ")", ")", "\n", "return", "loss", "\n", "", "else", ":", "\n", "            ", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForQuestionAnswering.__init__": [[1318, 1325], ["modeling.PreTrainedBertModel.__init__", "modeling.BertModel", "torch.nn.Linear", "modeling.BertForQuestionAnswering.apply"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertForQuestionAnswering", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "bert", "=", "BertModel", "(", "config", ")", "\n", "# TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version", "\n", "# self.dropout = nn.Dropout(config.hidden_dropout_prob)", "\n", "self", ".", "qa_outputs", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "2", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertForQuestionAnswering.forward": [[1326, 1351], ["modeling.BertForQuestionAnswering.bert", "modeling.BertForQuestionAnswering.qa_outputs", "modeling.BertForQuestionAnswering.split", "start_logits.squeeze.squeeze.squeeze", "end_logits.squeeze.squeeze.squeeze", "start_logits.squeeze.squeeze.size", "start_positions.squeeze.squeeze.clamp_", "end_positions.squeeze.squeeze.clamp_", "torch.nn.CrossEntropyLoss", "torch.nn.CrossEntropyLoss.", "torch.nn.CrossEntropyLoss.", "len", "start_positions.squeeze.squeeze.squeeze", "len", "end_positions.squeeze.squeeze.squeeze", "start_positions.squeeze.squeeze.size", "end_positions.squeeze.squeeze.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "input_ent", "=", "None", ",", "ent_mask", "=", "None", ",", "start_positions", "=", "None", ",", "end_positions", "=", "None", ")", ":", "\n", "        ", "sequence_output", ",", "_", "=", "self", ".", "bert", "(", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "input_ent", ",", "ent_mask", ",", "output_all_encoded_layers", "=", "False", ")", "\n", "logits", "=", "self", ".", "qa_outputs", "(", "sequence_output", ")", "\n", "start_logits", ",", "end_logits", "=", "logits", ".", "split", "(", "1", ",", "dim", "=", "-", "1", ")", "\n", "start_logits", "=", "start_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "end_logits", "=", "end_logits", ".", "squeeze", "(", "-", "1", ")", "\n", "\n", "if", "start_positions", "is", "not", "None", "and", "end_positions", "is", "not", "None", ":", "\n", "# If we are on multi-GPU, split add a dimension", "\n", "            ", "if", "len", "(", "start_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "start_positions", "=", "start_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "", "if", "len", "(", "end_positions", ".", "size", "(", ")", ")", ">", "1", ":", "\n", "                ", "end_positions", "=", "end_positions", ".", "squeeze", "(", "-", "1", ")", "\n", "# sometimes the start/end positions are outside our model inputs, we ignore these terms", "\n", "", "ignored_index", "=", "start_logits", ".", "size", "(", "1", ")", "\n", "start_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "end_positions", ".", "clamp_", "(", "0", ",", "ignored_index", ")", "\n", "\n", "loss_fct", "=", "CrossEntropyLoss", "(", "ignore_index", "=", "ignored_index", ")", "\n", "start_loss", "=", "loss_fct", "(", "start_logits", ",", "start_positions", ")", "\n", "end_loss", "=", "loss_fct", "(", "end_logits", ",", "end_positions", ")", "\n", "total_loss", "=", "(", "start_loss", "+", "end_loss", ")", "/", "2", "\n", "return", "total_loss", "\n", "", "else", ":", "\n", "            ", "return", "start_logits", ",", "end_logits", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.gelu": [[51, 57], ["torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.swish": [[59, 61], ["torch.sigmoid"], "function", ["None"], ["", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch": [[30, 88], ["os.path.abspath", "os.path.abspath", "print", "tensorflow.train.list_variables", "modeling.BertConfig.from_json_file", "print", "modeling.BertForPreTraining", "zip", "print", "torch.save", "print", "tensorflow.train.load_variable", "names.append", "arrays.append", "name.split.split", "any", "print", "torch.from_numpy", "modeling.BertForPreTraining.state_dict", "str", "print", "re.fullmatch", "getattr", "re.split", "getattr", "len", "int", "numpy.transpose", "getattr", "getattr", "getattr"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.modeling.BertConfig.from_json_file", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.iterators.EpochBatchIterator.state_dict"], ["def", "convert_tf_checkpoint_to_pytorch", "(", "tf_checkpoint_path", ",", "bert_config_file", ",", "pytorch_dump_path", ")", ":", "\n", "    ", "config_path", "=", "os", ".", "path", ".", "abspath", "(", "bert_config_file", ")", "\n", "tf_path", "=", "os", ".", "path", ".", "abspath", "(", "tf_checkpoint_path", ")", "\n", "print", "(", "\"Converting TensorFlow checkpoint from {} with config at {}\"", ".", "format", "(", "tf_path", ",", "config_path", ")", ")", "\n", "# Load weights from TF model", "\n", "init_vars", "=", "tf", ".", "train", ".", "list_variables", "(", "tf_path", ")", "\n", "names", "=", "[", "]", "\n", "arrays", "=", "[", "]", "\n", "for", "name", ",", "shape", "in", "init_vars", ":", "\n", "        ", "print", "(", "\"Loading TF weight {} with shape {}\"", ".", "format", "(", "name", ",", "shape", ")", ")", "\n", "array", "=", "tf", ".", "train", ".", "load_variable", "(", "tf_path", ",", "name", ")", "\n", "names", ".", "append", "(", "name", ")", "\n", "arrays", ".", "append", "(", "array", ")", "\n", "\n", "# Initialise PyTorch model", "\n", "", "config", "=", "BertConfig", ".", "from_json_file", "(", "bert_config_file", ")", "\n", "print", "(", "\"Building PyTorch model from configuration: {}\"", ".", "format", "(", "str", "(", "config", ")", ")", ")", "\n", "model", "=", "BertForPreTraining", "(", "config", ")", "\n", "\n", "for", "name", ",", "array", "in", "zip", "(", "names", ",", "arrays", ")", ":", "\n", "        ", "name", "=", "name", ".", "split", "(", "'/'", ")", "\n", "# adam_v and adam_m are variables used in AdamWeightDecayOptimizer to calculated m and v", "\n", "# which are not required for using pretrained model", "\n", "if", "any", "(", "n", "in", "[", "\"adam_v\"", ",", "\"adam_m\"", "]", "for", "n", "in", "name", ")", ":", "\n", "            ", "print", "(", "\"Skipping {}\"", ".", "format", "(", "\"/\"", ".", "join", "(", "name", ")", ")", ")", "\n", "continue", "\n", "", "pointer", "=", "model", "\n", "for", "m_name", "in", "name", ":", "\n", "            ", "if", "re", ".", "fullmatch", "(", "r'[A-Za-z]+_\\d+'", ",", "m_name", ")", ":", "\n", "                ", "l", "=", "re", ".", "split", "(", "r'_(\\d+)'", ",", "m_name", ")", "\n", "", "else", ":", "\n", "                ", "l", "=", "[", "m_name", "]", "\n", "", "if", "l", "[", "0", "]", "==", "'kernel'", "or", "l", "[", "0", "]", "==", "'gamma'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_bias'", "or", "l", "[", "0", "]", "==", "'beta'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'bias'", ")", "\n", "", "elif", "l", "[", "0", "]", "==", "'output_weights'", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "else", ":", "\n", "                ", "pointer", "=", "getattr", "(", "pointer", ",", "l", "[", "0", "]", ")", "\n", "", "if", "len", "(", "l", ")", ">=", "2", ":", "\n", "                ", "num", "=", "int", "(", "l", "[", "1", "]", ")", "\n", "pointer", "=", "pointer", "[", "num", "]", "\n", "", "", "if", "m_name", "[", "-", "11", ":", "]", "==", "'_embeddings'", ":", "\n", "            ", "pointer", "=", "getattr", "(", "pointer", ",", "'weight'", ")", "\n", "", "elif", "m_name", "==", "'kernel'", ":", "\n", "            ", "array", "=", "np", ".", "transpose", "(", "array", ")", "\n", "", "try", ":", "\n", "            ", "assert", "pointer", ".", "shape", "==", "array", ".", "shape", "\n", "", "except", "AssertionError", "as", "e", ":", "\n", "            ", "e", ".", "args", "+=", "(", "pointer", ".", "shape", ",", "array", ".", "shape", ")", "\n", "raise", "\n", "", "print", "(", "\"Initialize PyTorch weight {}\"", ".", "format", "(", "name", ")", ")", "\n", "pointer", ".", "data", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "\n", "# Save pytorch-model", "\n", "", "print", "(", "\"Save PyTorch model to {}\"", ".", "format", "(", "pytorch_dump_path", ")", ")", "\n", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "pytorch_dump_path", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.BertAdam.__init__": [[59, 78], ["dict", "torch.optim.Optimizer.__init__", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__"], ["def", "__init__", "(", "self", ",", "params", ",", "lr", "=", "required", ",", "warmup", "=", "-", "1", ",", "t_total", "=", "-", "1", ",", "schedule", "=", "'warmup_linear'", ",", "\n", "b1", "=", "0.9", ",", "b2", "=", "0.999", ",", "e", "=", "1e-6", ",", "weight_decay", "=", "0.01", ",", "\n", "max_grad_norm", "=", "1.0", ")", ":", "\n", "        ", "if", "lr", "is", "not", "required", "and", "lr", "<", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid learning rate: {} - should be >= 0.0\"", ".", "format", "(", "lr", ")", ")", "\n", "", "if", "schedule", "not", "in", "SCHEDULES", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid schedule parameter: {}\"", ".", "format", "(", "schedule", ")", ")", "\n", "", "if", "not", "0.0", "<=", "warmup", "<", "1.0", "and", "not", "warmup", "==", "-", "1", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid warmup: {} - should be in [0.0, 1.0[ or -1\"", ".", "format", "(", "warmup", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b1", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b1 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b1", ")", ")", "\n", "", "if", "not", "0.0", "<=", "b2", "<", "1.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid b2 parameter: {} - should be in [0.0, 1.0[\"", ".", "format", "(", "b2", ")", ")", "\n", "", "if", "not", "e", ">=", "0.0", ":", "\n", "            ", "raise", "ValueError", "(", "\"Invalid epsilon value: {} - should be >= 0.0\"", ".", "format", "(", "e", ")", ")", "\n", "", "defaults", "=", "dict", "(", "lr", "=", "lr", ",", "schedule", "=", "schedule", ",", "warmup", "=", "warmup", ",", "t_total", "=", "t_total", ",", "\n", "b1", "=", "b1", ",", "b2", "=", "b2", ",", "e", "=", "e", ",", "weight_decay", "=", "weight_decay", ",", "\n", "max_grad_norm", "=", "max_grad_norm", ")", "\n", "super", "(", "BertAdam", ",", "self", ")", ".", "__init__", "(", "params", ",", "defaults", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.BertAdam.get_lr": [[79, 93], ["lr.append", "len", "schedule_fct"], "methods", ["None"], ["", "def", "get_lr", "(", "self", ")", ":", "\n", "        ", "lr", "=", "[", "]", "\n", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "return", "[", "0", "]", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "", "lr", ".", "append", "(", "lr_scheduled", ")", "\n", "", "", "return", "lr", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.BertAdam.step": [[94, 163], ["closure", "next_m.mul_().add_", "next_v.mul_().addcmul_", "p.data.add_", "RuntimeError", "len", "torch.zeros_like", "torch.zeros_like", "torch.nn.utils.clip_grad_norm_", "next_m.mul_", "next_v.mul_", "next_v.sqrt", "schedule_fct"], "methods", ["None"], ["", "def", "step", "(", "self", ",", "closure", "=", "None", ")", ":", "\n", "        ", "\"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"", "\n", "loss", "=", "None", "\n", "if", "closure", "is", "not", "None", ":", "\n", "            ", "loss", "=", "closure", "(", ")", "\n", "\n", "", "for", "group", "in", "self", ".", "param_groups", ":", "\n", "            ", "for", "p", "in", "group", "[", "'params'", "]", ":", "\n", "                ", "if", "p", ".", "grad", "is", "None", ":", "\n", "                    ", "continue", "\n", "", "grad", "=", "p", ".", "grad", ".", "data", "\n", "if", "grad", ".", "is_sparse", ":", "\n", "                    ", "raise", "RuntimeError", "(", "'Adam does not support sparse gradients, please consider SparseAdam instead'", ")", "\n", "\n", "", "state", "=", "self", ".", "state", "[", "p", "]", "\n", "\n", "# State initialization", "\n", "if", "len", "(", "state", ")", "==", "0", ":", "\n", "                    ", "state", "[", "'step'", "]", "=", "0", "\n", "# Exponential moving average of gradient values", "\n", "state", "[", "'next_m'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "# Exponential moving average of squared gradient values", "\n", "state", "[", "'next_v'", "]", "=", "torch", ".", "zeros_like", "(", "p", ".", "data", ")", "\n", "\n", "", "next_m", ",", "next_v", "=", "state", "[", "'next_m'", "]", ",", "state", "[", "'next_v'", "]", "\n", "beta1", ",", "beta2", "=", "group", "[", "'b1'", "]", ",", "group", "[", "'b2'", "]", "\n", "\n", "# Add grad clipping", "\n", "if", "group", "[", "'max_grad_norm'", "]", ">", "0", ":", "\n", "                    ", "clip_grad_norm_", "(", "p", ",", "group", "[", "'max_grad_norm'", "]", ")", "\n", "\n", "# Decay the first and second moment running average coefficient", "\n", "# In-place operations to update the averages at the same time", "\n", "", "next_m", ".", "mul_", "(", "beta1", ")", ".", "add_", "(", "1", "-", "beta1", ",", "grad", ")", "\n", "next_v", ".", "mul_", "(", "beta2", ")", ".", "addcmul_", "(", "1", "-", "beta2", ",", "grad", ",", "grad", ")", "\n", "update", "=", "next_m", "/", "(", "next_v", ".", "sqrt", "(", ")", "+", "group", "[", "'e'", "]", ")", "\n", "\n", "# Just adding the square of the weights to the loss function is *not*", "\n", "# the correct way of using L2 regularization/weight decay with Adam,", "\n", "# since that will interact with the m and v parameters in strange ways.", "\n", "#", "\n", "# Instead we want to decay the weights in a manner that doesn't interact", "\n", "# with the m/v parameters. This is equivalent to adding the square", "\n", "# of the weights to the loss with plain (non-momentum) SGD.", "\n", "if", "group", "[", "'weight_decay'", "]", ">", "0.0", ":", "\n", "                    ", "update", "+=", "group", "[", "'weight_decay'", "]", "*", "p", ".", "data", "\n", "\n", "", "if", "group", "[", "'t_total'", "]", "!=", "-", "1", ":", "\n", "                    ", "schedule_fct", "=", "SCHEDULES", "[", "group", "[", "'schedule'", "]", "]", "\n", "lr_scheduled", "=", "group", "[", "'lr'", "]", "*", "schedule_fct", "(", "state", "[", "'step'", "]", "/", "group", "[", "'t_total'", "]", ",", "group", "[", "'warmup'", "]", ")", "\n", "", "else", ":", "\n", "                    ", "lr_scheduled", "=", "group", "[", "'lr'", "]", "\n", "\n", "", "update_with_lr", "=", "lr_scheduled", "*", "update", "\n", "p", ".", "data", ".", "add_", "(", "-", "update_with_lr", ")", "\n", "\n", "state", "[", "'step'", "]", "+=", "1", "\n", "\n", "# step_size = lr_scheduled * math.sqrt(bias_correction2) / bias_correction1", "\n", "# No bias correction", "\n", "# bias_correction1 = 1 - beta1 ** state['step']", "\n", "# bias_correction2 = 1 - beta2 ** state['step']", "\n", "\n", "", "", "return", "loss", "\n", "", "", ""]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.warmup_cosine": [[23, 27], ["torch.cos"], "function", ["None"], ["def", "warmup_cosine", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "0.5", "*", "(", "1.0", "+", "torch", ".", "cos", "(", "math", ".", "pi", "*", "x", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.warmup_constant": [[28, 32], ["None"], "function", ["None"], ["", "def", "warmup_constant", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.optimization.warmup_linear": [[33, 37], ["None"], "function", ["None"], ["", "def", "warmup_linear", "(", "x", ",", "warmup", "=", "0.002", ")", ":", "\n", "    ", "if", "x", "<", "warmup", ":", "\n", "        ", "return", "x", "/", "warmup", "\n", "", "return", "1.0", "-", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.url_to_filename": [[30, 46], ["url.encode", "hashlib.sha256", "hashlib.sha256.hexdigest", "etag.encode", "hashlib.sha256", "hashlib.sha256.hexdigest"], "function", ["None"], ["def", "url_to_filename", "(", "url", ":", "str", ",", "etag", ":", "str", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Convert `url` into a hashed filename in a repeatable way.\n    If `etag` is specified, append its hash to the url's, delimited\n    by a period.\n    \"\"\"", "\n", "url_bytes", "=", "url", ".", "encode", "(", "'utf-8'", ")", "\n", "url_hash", "=", "sha256", "(", "url_bytes", ")", "\n", "filename", "=", "url_hash", ".", "hexdigest", "(", ")", "\n", "\n", "if", "etag", ":", "\n", "        ", "etag_bytes", "=", "etag", ".", "encode", "(", "'utf-8'", ")", "\n", "etag_hash", "=", "sha256", "(", "etag_bytes", ")", "\n", "filename", "+=", "'.'", "+", "etag_hash", ".", "hexdigest", "(", ")", "\n", "\n", "", "return", "filename", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.filename_to_url": [[48, 72], ["isinstance", "os.path.join", "str", "os.path.exists", "FileNotFoundError", "os.path.exists", "FileNotFoundError", "open", "json.load"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "def", "filename_to_url", "(", "filename", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"\n    Return the url and etag (which may be ``None``) stored for `filename`.\n    Raise ``FileNotFoundError`` if `filename` or its stored metadata do not exist.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "cache_path", ")", ")", "\n", "\n", "", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "meta_path", ")", ":", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "meta_path", ")", ")", "\n", "\n", "", "with", "open", "(", "meta_path", ")", "as", "meta_file", ":", "\n", "        ", "metadata", "=", "json", ".", "load", "(", "meta_file", ")", "\n", "", "url", "=", "metadata", "[", "'url'", "]", "\n", "etag", "=", "metadata", "[", "'etag'", "]", "\n", "\n", "return", "url", ",", "etag", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.cached_path": [[74, 102], ["isinstance", "isinstance", "urllib.parse.urlparse", "str", "str", "file_utils.get_from_cache", "os.path.exists", "FileNotFoundError", "ValueError"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.get_from_cache", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists"], ["", "def", "cached_path", "(", "url_or_filename", ":", "Union", "[", "str", ",", "Path", "]", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given something that might be a URL (or might be a local path),\n    determine which. If it's a URL, download the file and cache it, and\n    return the path to the cached file. If it's already a local path,\n    make sure the file exists and then return the path.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "url_or_filename", ",", "Path", ")", ":", "\n", "        ", "url_or_filename", "=", "str", "(", "url_or_filename", ")", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "parsed", "=", "urlparse", "(", "url_or_filename", ")", "\n", "\n", "if", "parsed", ".", "scheme", "in", "(", "'http'", ",", "'https'", ",", "'s3'", ")", ":", "\n", "# URL, so get it from the cache (downloading if necessary)", "\n", "        ", "return", "get_from_cache", "(", "url_or_filename", ",", "cache_dir", ")", "\n", "", "elif", "os", ".", "path", ".", "exists", "(", "url_or_filename", ")", ":", "\n", "# File, and it exists.", "\n", "        ", "return", "url_or_filename", "\n", "", "elif", "parsed", ".", "scheme", "==", "''", ":", "\n", "# File, but it doesn't exist.", "\n", "        ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "", "else", ":", "\n", "# Something unknown", "\n", "        ", "raise", "ValueError", "(", "\"unable to parse {} as a URL or as a local path\"", ".", "format", "(", "url_or_filename", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.split_s3_path": [[104, 115], ["urllib.parse.urlparse", "s3_path.startswith", "ValueError"], "function", ["None"], ["", "", "def", "split_s3_path", "(", "url", ":", "str", ")", "->", "Tuple", "[", "str", ",", "str", "]", ":", "\n", "    ", "\"\"\"Split a full s3 path into the bucket name and path.\"\"\"", "\n", "parsed", "=", "urlparse", "(", "url", ")", "\n", "if", "not", "parsed", ".", "netloc", "or", "not", "parsed", ".", "path", ":", "\n", "        ", "raise", "ValueError", "(", "\"bad s3 path {}\"", ".", "format", "(", "url", ")", ")", "\n", "", "bucket_name", "=", "parsed", ".", "netloc", "\n", "s3_path", "=", "parsed", ".", "path", "\n", "# Remove '/' at beginning of path.", "\n", "if", "s3_path", ".", "startswith", "(", "\"/\"", ")", ":", "\n", "        ", "s3_path", "=", "s3_path", "[", "1", ":", "]", "\n", "", "return", "bucket_name", ",", "s3_path", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.s3_request": [[117, 134], ["functools.wraps", "func", "int", "FileNotFoundError"], "function", ["None"], ["", "def", "s3_request", "(", "func", ":", "Callable", ")", ":", "\n", "    ", "\"\"\"\n    Wrapper function for s3 requests in order to create more helpful error\n    messages.\n    \"\"\"", "\n", "\n", "@", "wraps", "(", "func", ")", "\n", "def", "wrapper", "(", "url", ":", "str", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "        ", "try", ":", "\n", "            ", "return", "func", "(", "url", ",", "*", "args", ",", "**", "kwargs", ")", "\n", "", "except", "ClientError", "as", "exc", ":", "\n", "            ", "if", "int", "(", "exc", ".", "response", "[", "\"Error\"", "]", "[", "\"Code\"", "]", ")", "==", "404", ":", "\n", "                ", "raise", "FileNotFoundError", "(", "\"file {} not found\"", ".", "format", "(", "url", ")", ")", "\n", "", "else", ":", "\n", "                ", "raise", "\n", "\n", "", "", "", "return", "wrapper", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.s3_etag": [[136, 143], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Object"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_etag", "(", "url", ":", "str", ")", "->", "Optional", "[", "str", "]", ":", "\n", "    ", "\"\"\"Check ETag on S3 object.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_object", "=", "s3_resource", ".", "Object", "(", "bucket_name", ",", "s3_path", ")", "\n", "return", "s3_object", ".", "e_tag", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.s3_get": [[145, 151], ["boto3.resource", "file_utils.split_s3_path", "boto3.resource.Bucket().download_fileobj", "boto3.resource.Bucket"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.split_s3_path"], ["", "@", "s3_request", "\n", "def", "s3_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "\"\"\"Pull a file directly from S3.\"\"\"", "\n", "s3_resource", "=", "boto3", ".", "resource", "(", "\"s3\"", ")", "\n", "bucket_name", ",", "s3_path", "=", "split_s3_path", "(", "url", ")", "\n", "s3_resource", ".", "Bucket", "(", "bucket_name", ")", ".", "download_fileobj", "(", "s3_path", ",", "temp_file", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.http_get": [[153, 163], ["requests.get", "requests.get.headers.get", "tqdm.tqdm", "requests.get.iter_content", "tqdm.tqdm.close", "int", "tqdm.tqdm.update", "temp_file.write", "len"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.close", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.write"], ["", "def", "http_get", "(", "url", ":", "str", ",", "temp_file", ":", "IO", ")", "->", "None", ":", "\n", "    ", "req", "=", "requests", ".", "get", "(", "url", ",", "stream", "=", "True", ")", "\n", "content_length", "=", "req", ".", "headers", ".", "get", "(", "'Content-Length'", ")", "\n", "total", "=", "int", "(", "content_length", ")", "if", "content_length", "is", "not", "None", "else", "None", "\n", "progress", "=", "tqdm", "(", "unit", "=", "\"B\"", ",", "total", "=", "total", ")", "\n", "for", "chunk", "in", "req", ".", "iter_content", "(", "chunk_size", "=", "1024", ")", ":", "\n", "        ", "if", "chunk", ":", "# filter out keep-alive new chunks", "\n", "            ", "progress", ".", "update", "(", "len", "(", "chunk", ")", ")", "\n", "temp_file", ".", "write", "(", "chunk", ")", "\n", "", "", "progress", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.get_from_cache": [[165, 222], ["isinstance", "os.makedirs", "url.startswith", "file_utils.url_to_filename", "os.path.join", "str", "file_utils.s3_etag", "requests.head", "requests.head.headers.get", "os.path.exists", "IOError", "tempfile.NamedTemporaryFile", "logger.info", "url.startswith", "temp_file.flush", "temp_file.seek", "logger.info", "logger.info", "logger.info", "file_utils.s3_get", "file_utils.http_get", "open", "shutil.copyfileobj", "open", "json.dump"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.url_to_filename", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.s3_etag", "home.repos.pwc.inspect_result.thunlp_ERNIE.code.indexed_dataset.IndexedDataset.exists", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.s3_get", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.http_get", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "def", "get_from_cache", "(", "url", ":", "str", ",", "cache_dir", ":", "Union", "[", "str", ",", "Path", "]", "=", "None", ")", "->", "str", ":", "\n", "    ", "\"\"\"\n    Given a URL, look for the corresponding dataset in the local cache.\n    If it's not there, download it. Then return the path to the cached file.\n    \"\"\"", "\n", "if", "cache_dir", "is", "None", ":", "\n", "        ", "cache_dir", "=", "PYTORCH_PRETRAINED_BERT_CACHE", "\n", "", "if", "isinstance", "(", "cache_dir", ",", "Path", ")", ":", "\n", "        ", "cache_dir", "=", "str", "(", "cache_dir", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "cache_dir", ",", "exist_ok", "=", "True", ")", "\n", "\n", "# Get eTag to add to filename, if it exists.", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "        ", "etag", "=", "s3_etag", "(", "url", ")", "\n", "", "else", ":", "\n", "        ", "response", "=", "requests", ".", "head", "(", "url", ",", "allow_redirects", "=", "True", ")", "\n", "if", "response", ".", "status_code", "!=", "200", ":", "\n", "            ", "raise", "IOError", "(", "\"HEAD request failed for url {} with status code {}\"", "\n", ".", "format", "(", "url", ",", "response", ".", "status_code", ")", ")", "\n", "", "etag", "=", "response", ".", "headers", ".", "get", "(", "\"ETag\"", ")", "\n", "\n", "", "filename", "=", "url_to_filename", "(", "url", ",", "etag", ")", "\n", "\n", "# get cache path to put the file", "\n", "cache_path", "=", "os", ".", "path", ".", "join", "(", "cache_dir", ",", "filename", ")", "\n", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "cache_path", ")", ":", "\n", "# Download to temporary file, then copy to cache dir once finished.", "\n", "# Otherwise you get corrupt cache entries if the download gets interrupted.", "\n", "        ", "with", "tempfile", ".", "NamedTemporaryFile", "(", ")", "as", "temp_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"%s not found in cache, downloading to %s\"", ",", "url", ",", "temp_file", ".", "name", ")", "\n", "\n", "# GET file object", "\n", "if", "url", ".", "startswith", "(", "\"s3://\"", ")", ":", "\n", "                ", "s3_get", "(", "url", ",", "temp_file", ")", "\n", "", "else", ":", "\n", "                ", "http_get", "(", "url", ",", "temp_file", ")", "\n", "\n", "# we are copying the file before closing it, so flush to avoid truncation", "\n", "", "temp_file", ".", "flush", "(", ")", "\n", "# shutil.copyfileobj() starts at the current position, so go to the start", "\n", "temp_file", ".", "seek", "(", "0", ")", "\n", "\n", "logger", ".", "info", "(", "\"copying %s to cache at %s\"", ",", "temp_file", ".", "name", ",", "cache_path", ")", "\n", "with", "open", "(", "cache_path", ",", "'wb'", ")", "as", "cache_file", ":", "\n", "                ", "shutil", ".", "copyfileobj", "(", "temp_file", ",", "cache_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"creating metadata file for %s\"", ",", "cache_path", ")", "\n", "meta", "=", "{", "'url'", ":", "url", ",", "'etag'", ":", "etag", "}", "\n", "meta_path", "=", "cache_path", "+", "'.json'", "\n", "with", "open", "(", "meta_path", ",", "'w'", ")", "as", "meta_file", ":", "\n", "                ", "json", ".", "dump", "(", "meta", ",", "meta_file", ")", "\n", "\n", "", "logger", ".", "info", "(", "\"removing temp file %s\"", ",", "temp_file", ".", "name", ")", "\n", "\n", "", "", "return", "cache_path", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.read_set_from_file": [[224, 234], ["set", "open", "set.add", "line.rstrip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["", "def", "read_set_from_file", "(", "filename", ":", "str", ")", "->", "Set", "[", "str", "]", ":", "\n", "    ", "'''\n    Extract a de-duped collection (set) of text from a file.\n    Expected file format is one item per line.\n    '''", "\n", "collection", "=", "set", "(", ")", "\n", "with", "open", "(", "filename", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "file_", ":", "\n", "        ", "for", "line", "in", "file_", ":", "\n", "            ", "collection", ".", "add", "(", "line", ".", "rstrip", "(", ")", ")", "\n", "", "", "return", "collection", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.get_file_extension": [[236, 240], ["os.path.splitext", "ext.lower"], "function", ["None"], ["", "def", "get_file_extension", "(", "path", ":", "str", ",", "dot", "=", "True", ",", "lower", ":", "bool", "=", "True", ")", ":", "\n", "    ", "ext", "=", "os", ".", "path", ".", "splitext", "(", "path", ")", "[", "1", "]", "\n", "ext", "=", "ext", "if", "dot", "else", "ext", "[", "1", ":", "]", "\n", "return", "ext", ".", "lower", "(", ")", "if", "lower", "else", "ext", "\n", "", ""]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BertTokenizer.__init__": [[121, 132], ["tokenization.load_vocab", "collections.OrderedDict", "tokenization.BasicTokenizer", "tokenization.WordpieceTokenizer", "os.path.isfile", "ValueError", "int", "tokenization.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.load_vocab"], ["", "", "def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "  ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "tf", ".", "gfile", ".", "GFile", "(", "vocab_file", ",", "\"r\"", ")", "as", "reader", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "token", "=", "convert_to_unicode", "(", "reader", ".", "readline", "(", ")", ")", "\n", "if", "not", "token", ":", "\n", "        ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BertTokenizer.tokenize": [[133, 146], ["tokenization.BertTokenizer.basic_tokenizer.tokenize", "tokenization.BertTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append", "split_ents.append", "split_ents.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize"], ["", "", "vocab", "[", "\"sepsepsep\"", "]", "=", "index", "\n", "return", "vocab", "\n", "\n", "\n", "", "def", "convert_by_vocab", "(", "vocab", ",", "items", ")", ":", "\n", "  ", "\"\"\"Converts a sequence of [tokens|ids] using the vocab.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "item", "in", "items", ":", "\n", "    ", "output", ".", "append", "(", "vocab", "[", "item", "]", ")", "\n", "", "return", "output", "\n", "\n", "\n", "", "def", "convert_tokens_to_ids", "(", "vocab", ",", "tokens", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "vocab", ",", "tokens", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BertTokenizer.convert_tokens_to_ids": [[147, 159], ["ids.append", "len", "ValueError", "len"], "methods", ["None"], ["\n", "\n", "", "def", "convert_ids_to_tokens", "(", "inv_vocab", ",", "ids", ")", ":", "\n", "  ", "return", "convert_by_vocab", "(", "inv_vocab", ",", "ids", ")", "\n", "\n", "\n", "", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "  ", "\"\"\"Runs basic whitespace cleaning and splitting on a piece of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BertTokenizer.convert_ids_to_tokens": [[160, 166], ["tokens.append"], "methods", ["None"], ["\n", "\n", "", "class", "FullTokenizer", "(", "object", ")", ":", "\n", "  ", "\"\"\"Runs end-to-end tokenziation.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BertTokenizer.from_pretrained": [[167, 204], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.cached_path"], ["self", ".", "inv_vocab", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "self", ".", "vocab", ".", "items", "(", ")", "}", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ")", ":", "\n", "      ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "        ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "\n", "", "", "return", "split_tokens", "\n", "\n", "", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "vocab", ",", "tokens", ")", "\n", "\n", "", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "    ", "return", "convert_by_vocab", "(", "self", ".", "inv_vocab", ",", "ids", ")", "\n", "\n", "\n", "", "", "class", "BasicTokenizer", "(", "object", ")", ":", "\n", "  ", "\"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "    ", "\"\"\"Constructs a BasicTokenizer.\n\n    Args:\n      do_lower_case: Whether to lower case the input.\n    \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "text", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BasicTokenizer.__init__": [[209, 216], ["None"], "methods", ["None"], ["\n", "orig_tokens", "=", "whitespace_tokenize", "(", "text", ")", "\n", "split_tokens", "=", "[", "]", "\n", "for", "token", "in", "orig_tokens", ":", "\n", "      ", "if", "self", ".", "do_lower_case", ":", "\n", "        ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "split_tokens", ".", "extend", "(", "self", ".", "_run_split_on_punc", "(", "token", ")", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BasicTokenizer.tokenize": [[217, 248], ["tokenization.BasicTokenizer._clean_text", "tokenization.whitespace_tokenize_ent", "zip", "len", "enumerate", "tokenization.BasicTokenizer._run_split_on_punc", "split_tokens.extend", "split_ents.extend", "sum", "sum", "tokenization.BasicTokenizer.lower", "tokenization.BasicTokenizer._run_strip_accents", "len"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.whitespace_tokenize_ent", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._run_split_on_punc", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._run_strip_accents"], ["\n", "", "output_tokens", "=", "whitespace_tokenize", "(", "\" \"", ".", "join", "(", "split_tokens", ")", ")", "\n", "return", "output_tokens", "\n", "\n", "", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "        ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "      ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "        ", "if", "start_new_word", ":", "\n", "          ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BasicTokenizer._run_strip_accents": [[249, 259], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n", "", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BasicTokenizer._run_split_on_punc": [[260, 279], ["list", "len", "tokenization._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_punctuation"], ["output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "    ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BasicTokenizer._tokenize_chinese_chars": [[280, 292], ["ord", "tokenization.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._is_chinese_char"], ["(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "      ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n", "", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "    ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "      ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BasicTokenizer._is_chinese_char": [[293, 314], ["None"], "methods", ["None"], ["        ", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "        ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "        ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n", "\n", "", "", "class", "WordpieceTokenizer", "(", "object", ")", ":", "\n", "  ", "\"\"\"Runs WordPiece tokenziation.\"\"\"", "\n", "\n", "def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "200", ")", ":", "\n", "    ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n", "", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "    "]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.BasicTokenizer._clean_text": [[315, 329], ["enumerate", "ord", "tokenization._is_whitespace", "tokenization._is_control", "drop_idx.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_whitespace", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_control"], ["\n", "\n", "text", "=", "convert_to_unicode", "(", "text", ")", "\n", "\n", "output_tokens", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.WordpieceTokenizer.__init__": [[334, 338], ["None"], "methods", ["None"], ["continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.WordpieceTokenizer.tokenize": [[339, 389], ["tokenization.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.whitespace_tokenize"], ["while", "start", "<", "len", "(", "chars", ")", ":", "\n", "        ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "          ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "            ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "            ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "          ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "        ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "        ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n", "\n", "", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n", "\n", "", "def", "_is_control", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "    ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "in", "(", "\"Cc\"", ",", "\"Cf\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "\n", "\n", "\n", "", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "  ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.load_vocab": [[51, 64], ["collections.OrderedDict", "open", "reader.readline", "token.strip.strip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["\"cased_L-12_H-768_A-12\"", ",", "\"cased_L-24_H-1024_A-16\"", ",", "\n", "\"multi_cased_L-12_H-768_A-12\"", "\n", "]", "\n", "\n", "is_bad_config", "=", "False", "\n", "if", "model_name", "in", "lower_models", "and", "not", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"False\"", "\n", "case_name", "=", "\"lowercased\"", "\n", "opposite_flag", "=", "\"True\"", "\n", "\n", "", "if", "model_name", "in", "cased_models", "and", "do_lower_case", ":", "\n", "    ", "is_bad_config", "=", "True", "\n", "actual_flag", "=", "\"True\"", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.whitespace_tokenize": [[66, 73], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["opposite_flag", "=", "\"False\"", "\n", "\n", "", "if", "is_bad_config", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "\"You passed in `--do_lower_case=%s` with `--init_checkpoint=%s`. \"", "\n", "\"However, `%s` seems to be a %s model, so you \"", "\n", "\"should pass in `--do_lower_case=%s` so that the fine-tuning matches \"", "\n", "\"how the model was pre-training. If this error is wrong, please \"", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization.whitespace_tokenize_ent": [[74, 117], ["text.find", "zip", "text.find", "dd.items", "tokens.append", "entities.append", "len", "dd.items", "tokens.append", "entities.append", "len"], "function", ["None"], ["\"just comment out this check.\"", "%", "(", "actual_flag", ",", "init_checkpoint", ",", "\n", "model_name", ",", "case_name", ",", "opposite_flag", ")", ")", "\n", "\n", "\n", "", "", "def", "convert_to_unicode", "(", "text", ")", ":", "\n", "  ", "\"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n", "    ", "raise", "ValueError", "(", "\"Not running on Python2 or Python 3?\"", ")", "\n", "\n", "\n", "", "", "def", "printable_text", "(", "text", ")", ":", "\n", "  ", "\"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"", "\n", "\n", "# These functions want `str` for both Python2 and Python3, but in one case", "\n", "# it's a Unicode string and in the other it's a byte string.", "\n", "if", "six", ".", "PY3", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "bytes", ")", ":", "\n", "      ", "return", "text", ".", "decode", "(", "\"utf-8\"", ",", "\"ignore\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "elif", "six", ".", "PY2", ":", "\n", "    ", "if", "isinstance", "(", "text", ",", "str", ")", ":", "\n", "      ", "return", "text", "\n", "", "elif", "isinstance", "(", "text", ",", "unicode", ")", ":", "\n", "      ", "return", "text", ".", "encode", "(", "\"utf-8\"", ")", "\n", "", "else", ":", "\n", "      ", "raise", "ValueError", "(", "\"Unsupported string type: %s\"", "%", "(", "type", "(", "text", ")", ")", ")", "\n", "", "", "else", ":", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization._is_whitespace": [[391, 401], ["unicodedata.category"], "function", ["None"], ["# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "    ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "    ", "return", "True", "\n", "", "return", "False", "", "", ""]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization._is_control": [[403, 413], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], []], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.tokenization._is_punctuation": [[415, 429], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], []], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.__main__.main": [[2, 20], ["len", "print", "sys.argv.pop", "sys.argv.pop", "sys.argv.pop", "convert_tf_checkpoint_to_pytorch", "print"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.Frame.pop", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.convert_tf_checkpoint_to_pytorch.convert_tf_checkpoint_to_pytorch"], ["def", "main", "(", ")", ":", "\n", "    ", "import", "sys", "\n", "try", ":", "\n", "        ", "from", ".", "convert_tf_checkpoint_to_pytorch", "import", "convert_tf_checkpoint_to_pytorch", "\n", "", "except", "ModuleNotFoundError", ":", "\n", "        ", "print", "(", "\"pytorch_pretrained_bert can only be used from the commandline to convert TensorFlow models in PyTorch, \"", "\n", "\"In that case, it requires TensorFlow to be installed. Please see \"", "\n", "\"https://www.tensorflow.org/install/ for installation instructions.\"", ")", "\n", "raise", "\n", "\n", "", "if", "len", "(", "sys", ".", "argv", ")", "!=", "5", ":", "\n", "# pylint: disable=line-too-long", "\n", "        ", "print", "(", "\"Should be used as `pytorch_pretrained_bert convert_tf_checkpoint_to_pytorch TF_CHECKPOINT TF_CONFIG PYTORCH_DUMP_OUTPUT`\"", ")", "\n", "", "else", ":", "\n", "        ", "PYTORCH_DUMP_OUTPUT", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "TF_CONFIG", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "TF_CHECKPOINT", "=", "sys", ".", "argv", ".", "pop", "(", ")", "\n", "convert_tf_checkpoint_to_pytorch", "(", "TF_CHECKPOINT", ",", "TF_CONFIG", ",", "PYTORCH_DUMP_OUTPUT", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.__init__": [[121, 132], ["typing.load_vocab", "collections.OrderedDict", "typing.BasicTokenizer", "typing.WordpieceTokenizer", "os.path.isfile", "ValueError", "int", "typing.BertTokenizer.vocab.items"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.load_vocab"], ["def", "__init__", "(", "self", ",", "vocab_file", ",", "do_lower_case", "=", "True", ",", "max_len", "=", "None", ")", ":", "\n", "        ", "if", "not", "os", ".", "path", ".", "isfile", "(", "vocab_file", ")", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Can't find a vocabulary file at path '{}'. To load the vocabulary from a Google pretrained \"", "\n", "\"model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "self", ".", "vocab", "=", "load_vocab", "(", "vocab_file", ")", "\n", "self", ".", "ids_to_tokens", "=", "collections", ".", "OrderedDict", "(", "\n", "[", "(", "ids", ",", "tok", ")", "for", "tok", ",", "ids", "in", "self", ".", "vocab", ".", "items", "(", ")", "]", ")", "\n", "self", ".", "basic_tokenizer", "=", "BasicTokenizer", "(", "do_lower_case", "=", "do_lower_case", ")", "\n", "self", ".", "wordpiece_tokenizer", "=", "WordpieceTokenizer", "(", "vocab", "=", "self", ".", "vocab", ")", "\n", "self", ".", "max_len", "=", "max_len", "if", "max_len", "is", "not", "None", "else", "int", "(", "1e12", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.tokenize": [[133, 141], ["typing.BertTokenizer.basic_tokenizer.tokenize", "typing.BertTokenizer.wordpiece_tokenizer.tokenize", "split_tokens.append", "split_ents.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "ents", ")", ":", "\n", "        ", "split_tokens", "=", "[", "]", "\n", "split_ents", "=", "[", "]", "\n", "for", "token", ",", "ent", "in", "self", ".", "basic_tokenizer", ".", "tokenize", "(", "text", ",", "ents", ")", ":", "\n", "            ", "for", "sub_token", "in", "self", ".", "wordpiece_tokenizer", ".", "tokenize", "(", "token", ")", ":", "\n", "                ", "split_tokens", ".", "append", "(", "sub_token", ")", "\n", "split_ents", ".", "append", "(", "ent", ")", "\n", "", "", "return", "split_tokens", ",", "split_ents", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_tokens_to_ids": [[142, 154], ["ids.append", "len", "ValueError", "len"], "methods", ["None"], ["", "def", "convert_tokens_to_ids", "(", "self", ",", "tokens", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"", "\n", "ids", "=", "[", "]", "\n", "for", "token", "in", "tokens", ":", "\n", "            ", "ids", ".", "append", "(", "self", ".", "vocab", "[", "token", "]", ")", "\n", "", "if", "len", "(", "ids", ")", ">", "self", ".", "max_len", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"Token indices sequence length is longer than the specified maximum \"", "\n", "\" sequence length for this BERT model ({} > {}). Running this\"", "\n", "\" sequence through BERT will result in indexing errors\"", ".", "format", "(", "len", "(", "ids", ")", ",", "self", ".", "max_len", ")", "\n", ")", "\n", "", "return", "ids", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.convert_ids_to_tokens": [[155, 161], ["tokens.append"], "methods", ["None"], ["", "def", "convert_ids_to_tokens", "(", "self", ",", "ids", ")", ":", "\n", "        ", "\"\"\"Converts a sequence of ids in wordpiece tokens using the vocab.\"\"\"", "\n", "tokens", "=", "[", "]", "\n", "for", "i", "in", "ids", ":", "\n", "            ", "tokens", ".", "append", "(", "self", ".", "ids_to_tokens", "[", "i", "]", ")", "\n", "", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BertTokenizer.from_pretrained": [[162, 199], ["os.path.isdir", "cls", "os.path.join", "file_utils.cached_path", "logger.info", "logger.info", "min", "logger.error", "kwargs.get", "int", "PRETRAINED_VOCAB_ARCHIVE_MAP.keys"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.file_utils.cached_path"], ["", "@", "classmethod", "\n", "def", "from_pretrained", "(", "cls", ",", "pretrained_model_name", ",", "cache_dir", "=", "None", ",", "*", "inputs", ",", "**", "kwargs", ")", ":", "\n", "        ", "\"\"\"\n        Instantiate a PreTrainedBertModel from a pre-trained model file.\n        Download and cache the pre-trained model file if needed.\n        \"\"\"", "\n", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_ARCHIVE_MAP", ":", "\n", "            ", "vocab_file", "=", "PRETRAINED_VOCAB_ARCHIVE_MAP", "[", "pretrained_model_name", "]", "\n", "", "else", ":", "\n", "            ", "vocab_file", "=", "pretrained_model_name", "\n", "", "if", "os", ".", "path", ".", "isdir", "(", "vocab_file", ")", ":", "\n", "            ", "vocab_file", "=", "os", ".", "path", ".", "join", "(", "vocab_file", ",", "VOCAB_NAME", ")", "\n", "# redirect to the cache, if necessary", "\n", "", "try", ":", "\n", "            ", "resolved_vocab_file", "=", "cached_path", "(", "vocab_file", ",", "cache_dir", "=", "cache_dir", ")", "\n", "", "except", "FileNotFoundError", ":", "\n", "            ", "logger", ".", "error", "(", "\n", "\"Model name '{}' was not found in model name list ({}). \"", "\n", "\"We assumed '{}' was a path or url but couldn't find any file \"", "\n", "\"associated to this path or url.\"", ".", "format", "(", "\n", "pretrained_model_name", ",", "\n", "', '", ".", "join", "(", "PRETRAINED_VOCAB_ARCHIVE_MAP", ".", "keys", "(", ")", ")", ",", "\n", "vocab_file", ")", ")", "\n", "return", "None", "\n", "", "if", "resolved_vocab_file", "==", "vocab_file", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {}\"", ".", "format", "(", "vocab_file", ")", ")", "\n", "", "else", ":", "\n", "            ", "logger", ".", "info", "(", "\"loading vocabulary file {} from cache at {}\"", ".", "format", "(", "\n", "vocab_file", ",", "resolved_vocab_file", ")", ")", "\n", "", "if", "pretrained_model_name", "in", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", ":", "\n", "# if we're using a pretrained model, ensure the tokenizer wont index sequences longer", "\n", "# than the number of positional embeddings", "\n", "            ", "max_len", "=", "PRETRAINED_VOCAB_POSITIONAL_EMBEDDINGS_SIZE_MAP", "[", "pretrained_model_name", "]", "\n", "kwargs", "[", "'max_len'", "]", "=", "min", "(", "kwargs", ".", "get", "(", "'max_len'", ",", "int", "(", "1e12", ")", ")", ",", "max_len", ")", "\n", "# Instantiate tokenizer.", "\n", "", "tokenizer", "=", "cls", "(", "resolved_vocab_file", ",", "*", "inputs", ",", "**", "kwargs", ")", "\n", "return", "tokenizer", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer.__init__": [[204, 211], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "do_lower_case", "=", "True", ")", ":", "\n", "        ", "\"\"\"Constructs a BasicTokenizer.\n\n        Args:\n          do_lower_case: Whether to lower case the input.\n        \"\"\"", "\n", "self", ".", "do_lower_case", "=", "do_lower_case", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer.tokenize": [[212, 243], ["typing.BasicTokenizer._clean_text", "typing.whitespace_tokenize_ent", "zip", "len", "enumerate", "typing.BasicTokenizer._run_split_on_punc", "split_tokens.extend", "split_ents.extend", "sum", "sum", "typing.BasicTokenizer.lower", "typing.BasicTokenizer._run_strip_accents", "len"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._clean_text", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.whitespace_tokenize_ent", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._run_split_on_punc", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._run_strip_accents"], ["", "def", "tokenize", "(", "self", ",", "text", ",", "ents", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text.\"\"\"", "\n", "text", ",", "drop_idx", "=", "self", ".", "_clean_text", "(", "text", ")", "\n", "# update ents", "\n", "if", "len", "(", "drop_idx", ")", ">", "0", ":", "\n", "            ", "for", "i", ",", "ent", "in", "enumerate", "(", "ents", ")", ":", "\n", "                ", "cnt", "=", "sum", "(", "[", "True", "if", "j", "<", "ent", "[", "1", "]", "else", "False", "for", "j", "in", "drop_idx", "]", ")", "\n", "ent", "[", "1", "]", "-=", "cnt", "\n", "cnt", "=", "sum", "(", "[", "True", "if", "j", "<", "ent", "[", "2", "]", "else", "False", "for", "j", "in", "drop_idx", "]", ")", "\n", "ent", "[", "2", "]", "-=", "cnt", "\n", "# This was added on November 1st, 2018 for the multilingual and Chinese", "\n", "# models. This is also applied to the English models now, but it doesn't", "\n", "# matter since the English models were not trained on any Chinese data", "\n", "# and generally don't have any Chinese data in them (there are Chinese", "\n", "# characters in the vocabulary because Wikipedia does have some Chinese", "\n", "# words in the English Wikipedia.).", "\n", "", "", "orig_tokens", "=", "whitespace_tokenize_ent", "(", "text", ",", "ents", ")", "\n", "split_tokens", "=", "[", "]", "\n", "split_ents", "=", "[", "]", "\n", "for", "token", ",", "ent", "in", "orig_tokens", ":", "\n", "            ", "if", "self", ".", "do_lower_case", ":", "\n", "                ", "token", "=", "token", ".", "lower", "(", ")", "\n", "token", "=", "self", ".", "_run_strip_accents", "(", "token", ")", "\n", "", "cur", "=", "self", ".", "_run_split_on_punc", "(", "token", ")", "\n", "split_tokens", ".", "extend", "(", "cur", ")", "\n", "split_ents", ".", "extend", "(", "[", "ent", "]", "+", "[", "\"UNK\"", "]", "*", "(", "len", "(", "cur", ")", "-", "1", ")", ")", "\n", "\n", "#output_tokens = whitespace_tokenize(\" \".join(split_tokens))", "\n", "#assert len(output_tokens) == len(split_ents)", "\n", "", "output_tokens", "=", "split_tokens", "\n", "return", "zip", "(", "output_tokens", ",", "split_ents", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._run_strip_accents": [[244, 254], ["unicodedata.normalize", "unicodedata.category", "output.append"], "methods", ["None"], ["", "def", "_run_strip_accents", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Strips accents from a piece of text.\"\"\"", "\n", "text", "=", "unicodedata", ".", "normalize", "(", "\"NFD\"", ",", "text", ")", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Mn\"", ":", "\n", "                ", "continue", "\n", "", "output", ".", "append", "(", "char", ")", "\n", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._run_split_on_punc": [[255, 274], ["list", "len", "typing._is_punctuation", "output.append", "output[].append", "output.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_punctuation"], ["", "def", "_run_split_on_punc", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Splits punctuation on a piece of text.\"\"\"", "\n", "chars", "=", "list", "(", "text", ")", "\n", "i", "=", "0", "\n", "start_new_word", "=", "True", "\n", "output", "=", "[", "]", "\n", "while", "i", "<", "len", "(", "chars", ")", ":", "\n", "            ", "char", "=", "chars", "[", "i", "]", "\n", "if", "_is_punctuation", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "[", "char", "]", ")", "\n", "start_new_word", "=", "True", "\n", "", "else", ":", "\n", "                ", "if", "start_new_word", ":", "\n", "                    ", "output", ".", "append", "(", "[", "]", ")", "\n", "", "start_new_word", "=", "False", "\n", "output", "[", "-", "1", "]", ".", "append", "(", "char", ")", "\n", "", "i", "+=", "1", "\n", "\n", "", "return", "[", "\"\"", ".", "join", "(", "x", ")", "for", "x", "in", "output", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._tokenize_chinese_chars": [[275, 287], ["ord", "typing.BasicTokenizer._is_chinese_char", "output.append", "output.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._is_chinese_char"], ["", "def", "_tokenize_chinese_chars", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Adds whitespace around any CJK character.\"\"\"", "\n", "output", "=", "[", "]", "\n", "for", "char", "in", "text", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "self", ".", "_is_chinese_char", "(", "cp", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "output", ".", "append", "(", "char", ")", "\n", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._is_chinese_char": [[288, 309], ["None"], "methods", ["None"], ["", "def", "_is_chinese_char", "(", "self", ",", "cp", ")", ":", "\n", "        ", "\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"", "\n", "# This defines a \"chinese character\" as anything in the CJK Unicode block:", "\n", "#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)", "\n", "#", "\n", "# Note that the CJK Unicode block is NOT all Japanese and Korean characters,", "\n", "# despite its name. The modern Korean Hangul alphabet is a different block,", "\n", "# as is Japanese Hiragana and Katakana. Those alphabets are used to write", "\n", "# space-separated words, so they are not treated specially and handled", "\n", "# like the all of the other languages.", "\n", "if", "(", "(", "cp", ">=", "0x4E00", "and", "cp", "<=", "0x9FFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x3400", "and", "cp", "<=", "0x4DBF", ")", "or", "#", "\n", "(", "cp", ">=", "0x20000", "and", "cp", "<=", "0x2A6DF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2A700", "and", "cp", "<=", "0x2B73F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B740", "and", "cp", "<=", "0x2B81F", ")", "or", "#", "\n", "(", "cp", ">=", "0x2B820", "and", "cp", "<=", "0x2CEAF", ")", "or", "\n", "(", "cp", ">=", "0xF900", "and", "cp", "<=", "0xFAFF", ")", "or", "#", "\n", "(", "cp", ">=", "0x2F800", "and", "cp", "<=", "0x2FA1F", ")", ")", ":", "#", "\n", "            ", "return", "True", "\n", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.BasicTokenizer._clean_text": [[310, 324], ["enumerate", "ord", "typing._is_whitespace", "typing._is_control", "drop_idx.append", "output.append", "output.append"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_whitespace", "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_control"], ["", "def", "_clean_text", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"", "\n", "output", "=", "[", "]", "\n", "drop_idx", "=", "[", "]", "\n", "for", "i", ",", "char", "in", "enumerate", "(", "text", ")", ":", "\n", "            ", "cp", "=", "ord", "(", "char", ")", "\n", "if", "cp", "==", "0", "or", "cp", "==", "0xfffd", "or", "_is_control", "(", "char", ")", ":", "\n", "                ", "drop_idx", ".", "append", "(", "i", ")", "\n", "continue", "\n", "", "if", "_is_whitespace", "(", "char", ")", ":", "\n", "                ", "output", ".", "append", "(", "\" \"", ")", "\n", "", "else", ":", "\n", "                ", "output", ".", "append", "(", "char", ")", "\n", "", "", "return", "\"\"", ".", "join", "(", "output", ")", ",", "drop_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.__init__": [[329, 333], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "vocab", ",", "unk_token", "=", "\"[UNK]\"", ",", "max_input_chars_per_word", "=", "100", ")", ":", "\n", "        ", "self", ".", "vocab", "=", "vocab", "\n", "self", ".", "unk_token", "=", "unk_token", "\n", "self", ".", "max_input_chars_per_word", "=", "max_input_chars_per_word", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.WordpieceTokenizer.tokenize": [[334, 384], ["typing.whitespace_tokenize", "list", "len", "output_tokens.append", "len", "len", "sub_tokens.append", "output_tokens.append", "output_tokens.extend"], "methods", ["home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.whitespace_tokenize"], ["", "def", "tokenize", "(", "self", ",", "text", ")", ":", "\n", "        ", "\"\"\"Tokenizes a piece of text into its word pieces.\n\n        This uses a greedy longest-match-first algorithm to perform tokenization\n        using the given vocabulary.\n\n        For example:\n          input = \"unaffable\"\n          output = [\"un\", \"##aff\", \"##able\"]\n\n        Args:\n          text: A single token or whitespace separated tokens. This should have\n            already been passed through `BasicTokenizer`.\n\n        Returns:\n          A list of wordpiece tokens.\n        \"\"\"", "\n", "\n", "output_tokens", "=", "[", "]", "\n", "for", "token", "in", "whitespace_tokenize", "(", "text", ")", ":", "\n", "            ", "chars", "=", "list", "(", "token", ")", "\n", "if", "len", "(", "chars", ")", ">", "self", ".", "max_input_chars_per_word", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "continue", "\n", "\n", "", "is_bad", "=", "False", "\n", "start", "=", "0", "\n", "sub_tokens", "=", "[", "]", "\n", "while", "start", "<", "len", "(", "chars", ")", ":", "\n", "                ", "end", "=", "len", "(", "chars", ")", "\n", "cur_substr", "=", "None", "\n", "while", "start", "<", "end", ":", "\n", "                    ", "substr", "=", "\"\"", ".", "join", "(", "chars", "[", "start", ":", "end", "]", ")", "\n", "if", "start", ">", "0", ":", "\n", "                        ", "substr", "=", "\"##\"", "+", "substr", "\n", "", "if", "substr", "in", "self", ".", "vocab", ":", "\n", "                        ", "cur_substr", "=", "substr", "\n", "break", "\n", "", "end", "-=", "1", "\n", "", "if", "cur_substr", "is", "None", ":", "\n", "                    ", "is_bad", "=", "True", "\n", "break", "\n", "", "sub_tokens", ".", "append", "(", "cur_substr", ")", "\n", "start", "=", "end", "\n", "\n", "", "if", "is_bad", ":", "\n", "                ", "output_tokens", ".", "append", "(", "self", ".", "unk_token", ")", "\n", "", "else", ":", "\n", "                ", "output_tokens", ".", "extend", "(", "sub_tokens", ")", "\n", "", "", "return", "output_tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.load_vocab": [[51, 64], ["collections.OrderedDict", "open", "reader.readline", "token.strip.strip"], "function", ["home.repos.pwc.inspect_result.thunlp_ERNIE.pretrain_data.WikiExtractor.OutputSplitter.open"], ["def", "load_vocab", "(", "vocab_file", ")", ":", "\n", "    ", "\"\"\"Loads a vocabulary file into a dictionary.\"\"\"", "\n", "vocab", "=", "collections", ".", "OrderedDict", "(", ")", "\n", "index", "=", "0", "\n", "with", "open", "(", "vocab_file", ",", "\"r\"", ",", "encoding", "=", "\"utf-8\"", ")", "as", "reader", ":", "\n", "        ", "while", "True", ":", "\n", "            ", "token", "=", "reader", ".", "readline", "(", ")", "\n", "if", "not", "token", ":", "\n", "                ", "break", "\n", "", "token", "=", "token", ".", "strip", "(", ")", "\n", "vocab", "[", "token", "]", "=", "index", "\n", "index", "+=", "1", "\n", "", "", "return", "vocab", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.whitespace_tokenize": [[66, 73], ["text.strip.strip", "text.strip.split"], "function", ["None"], ["", "def", "whitespace_tokenize", "(", "text", ")", ":", "\n", "    ", "\"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"", "\n", "text", "=", "text", ".", "strip", "(", ")", "\n", "if", "not", "text", ":", "\n", "        ", "return", "[", "]", "\n", "", "tokens", "=", "text", ".", "split", "(", ")", "\n", "return", "tokens", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing.whitespace_tokenize_ent": [[74, 117], ["text.find", "zip", "text.find", "dd.items", "tokens.append", "entities.append", "len", "dd.items", "tokens.append", "entities.append", "len"], "function", ["None"], ["", "def", "whitespace_tokenize_ent", "(", "text", ",", "ents", ")", ":", "\n", "\n", "#print(\"Ent\", text[ents[0][1]:ents[0][2]], ents[0][1], ents[0][2])", "\n", "\n", "#text = text.strip()", "\n", "  ", "if", "not", "text", ":", "\n", "    ", "return", "[", "]", "\n", "\n", "", "dd", "=", "{", "}", "\n", "for", "ent", "in", "ents", ":", "\n", "    ", "dd", "[", "ent", "[", "0", "]", "]", "=", "(", "ent", "[", "1", "]", ",", "ent", "[", "2", "]", ")", "\n", "\n", "", "begin", "=", "0", "\n", "tokens", "=", "[", "]", "\n", "entities", "=", "[", "]", "\n", "while", "begin", "<", "len", "(", "text", ")", "and", "text", "[", "begin", "]", "==", "\" \"", ":", "\n", "      ", "begin", "+=", "1", "\n", "", "pos", "=", "text", ".", "find", "(", "\" \"", ",", "begin", ")", "\n", "while", "pos", "!=", "-", "1", ":", "\n", "    ", "if", "text", "[", "begin", ":", "pos", "]", "!=", "\" \"", ":", "\n", "      ", "entity", "=", "\"UNK\"", "\n", "for", "k", ",", "v", "in", "dd", ".", "items", "(", ")", ":", "\n", "        ", "if", "begin", ">=", "v", "[", "0", "]", "and", "begin", "<", "v", "[", "1", "]", ":", "\n", "#if begin == k:", "\n", "          ", "entity", "=", "k", "\n", "break", "\n", "", "", "tokens", ".", "append", "(", "text", "[", "begin", ":", "pos", "]", ")", "\n", "entities", ".", "append", "(", "entity", ")", "\n", "", "begin", "=", "pos", "\n", "while", "begin", "<", "len", "(", "text", ")", "and", "text", "[", "begin", "]", "==", "\" \"", ":", "\n", "        ", "begin", "+=", "1", "\n", "", "pos", "=", "text", ".", "find", "(", "\" \"", ",", "begin", ")", "\n", "", "if", "text", "[", "begin", ":", "]", "!=", "\" \"", ":", "\n", "    ", "entity", "=", "\"UNK\"", "\n", "for", "k", ",", "v", "in", "dd", ".", "items", "(", ")", ":", "\n", "      ", "if", "begin", ">=", "v", "[", "0", "]", "and", "begin", "<", "v", "[", "1", "]", ":", "\n", "#if begin == k:", "\n", "        ", "entity", "=", "k", "\n", "break", "\n", "", "", "tokens", ".", "append", "(", "text", "[", "begin", ":", "]", ")", "\n", "entities", ".", "append", "(", "entity", ")", "\n", "#print(tokens)", "\n", "", "return", "zip", "(", "tokens", ",", "entities", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_whitespace": [[386, 396], ["unicodedata.category"], "function", ["None"], ["", "", "def", "_is_whitespace", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a whitespace character.\"\"\"", "\n", "# \\t, \\n, and \\r are technically contorl characters but we treat them", "\n", "# as whitespace since they are generally considered as such.", "\n", "if", "char", "==", "\" \"", "or", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", "==", "\"Zs\"", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_control": [[398, 408], ["unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_control", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a control character.\"\"\"", "\n", "# These are technically control characters but we count them as whitespace", "\n", "# characters.", "\n", "if", "char", "==", "\"\\t\"", "or", "char", "==", "\"\\n\"", "or", "char", "==", "\"\\r\"", ":", "\n", "        ", "return", "False", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"C\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.thunlp_ERNIE.knowledge_bert.typing._is_punctuation": [[410, 424], ["ord", "unicodedata.category", "unicodedata.category.startswith"], "function", ["None"], ["", "def", "_is_punctuation", "(", "char", ")", ":", "\n", "    ", "\"\"\"Checks whether `chars` is a punctuation character.\"\"\"", "\n", "cp", "=", "ord", "(", "char", ")", "\n", "# We treat all non-letter/number ASCII as punctuation.", "\n", "# Characters such as \"^\", \"$\", and \"`\" are not in the Unicode", "\n", "# Punctuation class but we treat them as punctuation anyways, for", "\n", "# consistency.", "\n", "if", "(", "(", "cp", ">=", "33", "and", "cp", "<=", "47", ")", "or", "(", "cp", ">=", "58", "and", "cp", "<=", "64", ")", "or", "\n", "(", "cp", ">=", "91", "and", "cp", "<=", "96", ")", "or", "(", "cp", ">=", "123", "and", "cp", "<=", "126", ")", ")", ":", "\n", "        ", "return", "True", "\n", "", "cat", "=", "unicodedata", ".", "category", "(", "char", ")", "\n", "if", "cat", ".", "startswith", "(", "\"P\"", ")", ":", "\n", "        ", "return", "True", "\n", "", "return", "False", "\n", "", ""]]}