{"home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.None.inference.Config.__init__": [[9, 52], ["torchvision.transforms.Compose", "torch.nn.GLU", "torchvision.transforms.Resize", "torchvision.transforms.RandomCrop", "torchvision.transforms.RandomHorizontalFlip", "inference.Config.trained_model.split"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "max_epoch", "=", "50", "\n", "self", ".", "batch_size", "=", "1", "\n", "self", ".", "encoder_name", "=", "'resnet101'", "\n", "self", ".", "kernel_size", "=", "3", "\n", "self", ".", "num_layers", "=", "6", "\n", "self", ".", "channels", "=", "300", "\n", "self", ".", "prediction_dim", "=", "1024", "\n", "# ak_token: 9489", "\n", "self", ".", "voc_size", "=", "9489", "\n", "self", ".", "attention_tracker", "=", "False", "\n", "self", ".", "width", "=", "224", "\n", "self", ".", "height", "=", "224", "\n", "self", ".", "is_gpu", "=", "True", "\n", "self", ".", "is_train", "=", "True", "\n", "self", ".", "shuffle", "=", "True", "\n", "self", ".", "num_workers", "=", "1", "\n", "self", ".", "transformer", "=", "tv", ".", "transforms", ".", "Compose", "(", "\n", "[", "\n", "tv", ".", "transforms", ".", "Resize", "(", "(", "self", ".", "width", "+", "32", ",", "self", ".", "height", "+", "32", ")", ")", ",", "\n", "tv", ".", "transforms", ".", "RandomCrop", "(", "self", ".", "width", ",", "self", ".", "height", ")", ",", "\n", "tv", ".", "transforms", ".", "RandomHorizontalFlip", "(", ")", "\n", "]", "\n", ")", "\n", "self", ".", "is_dotatt", "=", "True", "\n", "self", ".", "image_dir", "=", "''", "## dir of images", "\n", "self", ".", "train_ann_file", "=", "'data/files/captions_train2014.json'", "\n", "self", ".", "val_ann_file", "=", "'data/files/captions_val2014.json'", "\n", "self", ".", "split_file", "=", "'data/files/caption_id.pkl'", "\n", "self", ".", "vocab_file", "=", "'data/files/vocab.pkl'", "\n", "self", ".", "f", "=", "torch", ".", "nn", ".", "GLU", "(", "1", ")", "\n", "self", ".", "keep_prob", "=", "0.5", "\n", "if", "self", ".", "is_dotatt", ":", "\n", "            ", "string", "=", "(", "'%s_numl%s_kz%s_dotatt_visualization%s'", ")", "%", "(", "self", ".", "encoder_name", ",", "self", ".", "num_layers", ",", "self", ".", "kernel_size", ",", "self", ".", "keep_prob", ")", "\n", "self", ".", "trained_model", "=", "'trained_models/'", "+", "string", "+", "'.pth'", "\n", "", "else", ":", "\n", "            ", "string", "=", "(", "'%s_numl%s_kz%s_nnatt_keepprob%s'", ")", "%", "(", "self", ".", "encoder_name", ",", "self", ".", "num_layers", ",", "self", ".", "kernel_size", ",", "self", ".", "keep_prob", ")", "\n", "self", ".", "trained_model", "=", "'trained_models/'", "+", "string", "+", "'.pth'", "\n", "", "self", ".", "log_file", "=", "'logs/'", "+", "self", ".", "trained_model", ".", "split", "(", "'/'", ")", "[", "1", "]", "+", "'.txt'", "\n", "self", ".", "result_file", "=", "'results/captions_val2014_'", "+", "string", "+", "'_results.json'", "\n", "self", ".", "annfile", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.None.inference.HierConfig.__init__": [[55, 98], ["torchvision.transforms.Compose", "torchvision.transforms.Resize", "torchvision.transforms.RandomCrop", "torchvision.transforms.RandomHorizontalFlip", "inference.HierConfig.trained_model.split"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "max_epoch", "=", "50", "\n", "self", ".", "batch_size", "=", "1", "\n", "self", ".", "encoder_name", "=", "'resnet101'", "\n", "self", ".", "kernel_size", "=", "3", "\n", "self", ".", "num_layers", "=", "6", "\n", "self", ".", "channels", "=", "300", "\n", "self", ".", "hier_att_hidden_size", "=", "512", "\n", "self", ".", "hier_att_lang_hidden_size", "=", "512", "\n", "self", ".", "prediction_dim", "=", "1024", "\n", "self", ".", "voc_size", "=", "9489", "\n", "self", ".", "width", "=", "224", "\n", "self", ".", "height", "=", "224", "\n", "self", ".", "is_gpu", "=", "True", "\n", "self", ".", "is_train", "=", "True", "\n", "self", ".", "shuffle", "=", "True", "\n", "self", ".", "num_workers", "=", "1", "\n", "self", ".", "transformer", "=", "tv", ".", "transforms", ".", "Compose", "(", "\n", "[", "\n", "tv", ".", "transforms", ".", "Resize", "(", "(", "self", ".", "width", "+", "32", ",", "self", ".", "height", "+", "32", ")", ")", ",", "\n", "tv", ".", "transforms", ".", "RandomCrop", "(", "self", ".", "width", ",", "self", ".", "height", ")", ",", "\n", "tv", ".", "transforms", ".", "RandomHorizontalFlip", "(", ")", "\n", "]", "\n", ")", "\n", "self", ".", "is_dotatt", "=", "True", "\n", "self", ".", "image_dir", "=", "''", "##dir of images", "\n", "self", ".", "train_ann_file", "=", "'data/files/captions_train2014.json'", "\n", "self", ".", "val_ann_file", "=", "'data/files/captions_val2014.json'", "\n", "self", ".", "split_file", "=", "'data/files/caption_id.pkl'", "\n", "self", ".", "vocab_file", "=", "'data/files/vocab.pkl'", "\n", "self", ".", "keep_prob", "=", "0.5", "\n", "if", "self", ".", "is_dotatt", ":", "\n", "            ", "string", "=", "(", "'%s_numl%s_kz%s_hierdotatt_visualization%s'", ")", "%", "(", "self", ".", "encoder_name", ",", "self", ".", "num_layers", ",", "self", ".", "kernel_size", ",", "self", ".", "keep_prob", ")", "\n", "self", ".", "trained_model", "=", "'trained_models/'", "+", "string", "+", "'.pth'", "\n", "", "else", ":", "\n", "            ", "string", "=", "(", "'%s_numl%s_kz%s_hiernnatt_keepprob%s'", ")", "%", "(", "self", ".", "encoder_name", ",", "self", ".", "num_layers", ",", "self", ".", "kernel_size", ",", "self", ".", "keep_prob", ")", "\n", "self", ".", "trained_model", "=", "'trained_models/'", "+", "string", "+", "'.pth'", "\n", "", "self", ".", "log_file", "=", "'logs/'", "+", "self", ".", "trained_model", ".", "split", "(", "'/'", ")", "[", "1", "]", "+", "'.txt'", "\n", "self", ".", "result_file", "=", "'results/captions_val2014_'", "+", "string", "+", "'_results.json'", "\n", "self", ".", "annfile", "=", "None", "\n", "self", ".", "weight_decay", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.None.train.Config.__init__": [[9, 53], ["torchvision.transforms.Compose", "torch.nn.GLU", "torchvision.transforms.Resize", "torchvision.transforms.RandomCrop", "torchvision.transforms.RandomHorizontalFlip", "train.Config.trained_model.split"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "max_epoch", "=", "50", "\n", "self", ".", "batch_size", "=", "32", "\n", "self", ".", "encoder_name", "=", "'vgg16'", "\n", "self", ".", "kernel_size", "=", "3", "\n", "self", ".", "num_layers", "=", "6", "\n", "self", ".", "channels", "=", "300", "\n", "self", ".", "prediction_dim", "=", "4096", "\n", "# ak_token: 9489", "\n", "self", ".", "voc_size", "=", "9489", "\n", "self", ".", "attention_tracker", "=", "False", "\n", "self", ".", "width", "=", "224", "\n", "self", ".", "height", "=", "224", "\n", "self", ".", "is_gpu", "=", "True", "\n", "self", ".", "is_train", "=", "True", "\n", "self", ".", "shuffle", "=", "True", "\n", "self", ".", "num_workers", "=", "8", "\n", "self", ".", "transformer", "=", "tv", ".", "transforms", ".", "Compose", "(", "\n", "[", "\n", "tv", ".", "transforms", ".", "Resize", "(", "(", "self", ".", "width", "+", "32", ",", "self", ".", "height", "+", "32", ")", ")", ",", "\n", "tv", ".", "transforms", ".", "RandomCrop", "(", "self", ".", "width", ",", "self", ".", "height", ")", ",", "\n", "tv", ".", "transforms", ".", "RandomHorizontalFlip", "(", ")", "\n", "]", "\n", ")", "\n", "\n", "self", ".", "is_dotatt", "=", "True", "\n", "self", ".", "image_dir", "=", "''", "# dir of images", "\n", "self", ".", "train_ann_file", "=", "'data/files/captions_train2014.json'", "\n", "self", ".", "val_ann_file", "=", "'data/files/captions_val2014.json'", "\n", "self", ".", "split_file", "=", "'data/files/caption_id.pkl'", "\n", "self", ".", "vocab_file", "=", "'data/files/vocab.pkl'", "\n", "self", ".", "f", "=", "torch", ".", "nn", ".", "GLU", "(", "1", ")", "\n", "self", ".", "keep_prob", "=", "0.5", "\n", "if", "self", ".", "is_dotatt", ":", "\n", "            ", "string", "=", "(", "'%s_numl%s_kz%s_dotatt_keepprob%s'", ")", "%", "(", "self", ".", "encoder_name", ",", "self", ".", "num_layers", ",", "self", ".", "kernel_size", ",", "self", ".", "keep_prob", ")", "\n", "self", ".", "trained_model", "=", "'trained_models/'", "+", "string", "+", "'.pth'", "\n", "", "else", ":", "\n", "            ", "string", "=", "(", "'%s_numl%s_kz%s_nnatt_keepprob%s'", ")", "%", "(", "self", ".", "encoder_name", ",", "self", ".", "num_layers", ",", "self", ".", "kernel_size", ",", "self", ".", "keep_prob", ")", "\n", "self", ".", "trained_model", "=", "'trained_models/'", "+", "string", "+", "'.pth'", "\n", "", "self", ".", "log_file", "=", "'logs/'", "+", "self", ".", "trained_model", ".", "split", "(", "'/'", ")", "[", "1", "]", "+", "'.txt'", "\n", "self", ".", "result_file", "=", "'results/captions_val2014_'", "+", "string", "+", "'_results.json'", "\n", "self", ".", "annfile", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.None.train.HierConfig.__init__": [[56, 100], ["torchvision.transforms.Compose", "torchvision.transforms.Resize", "torchvision.transforms.RandomCrop", "torchvision.transforms.RandomHorizontalFlip", "train.HierConfig.trained_model.split"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "max_epoch", "=", "50", "\n", "self", ".", "batch_size", "=", "24", "\n", "self", ".", "encoder_name", "=", "'resnet101'", "\n", "self", ".", "kernel_size", "=", "3", "\n", "self", ".", "num_layers", "=", "6", "\n", "self", ".", "channels", "=", "300", "\n", "self", ".", "hier_att_hidden_size", "=", "512", "\n", "self", ".", "hier_att_lang_hidden_size", "=", "512", "\n", "self", ".", "prediction_dim", "=", "4096", "\n", "self", ".", "voc_size", "=", "9489", "\n", "self", ".", "width", "=", "224", "\n", "self", ".", "height", "=", "224", "\n", "self", ".", "is_gpu", "=", "True", "\n", "self", ".", "is_train", "=", "True", "\n", "self", ".", "shuffle", "=", "True", "\n", "self", ".", "num_workers", "=", "8", "\n", "self", ".", "transformer", "=", "tv", ".", "transforms", ".", "Compose", "(", "\n", "[", "\n", "tv", ".", "transforms", ".", "Resize", "(", "(", "self", ".", "width", "+", "32", ",", "self", ".", "height", "+", "32", ")", ")", ",", "\n", "tv", ".", "transforms", ".", "RandomCrop", "(", "self", ".", "width", ",", "self", ".", "height", ")", ",", "\n", "tv", ".", "transforms", ".", "RandomHorizontalFlip", "(", ")", "\n", "]", "\n", ")", "\n", "\n", "self", ".", "is_dotatt", "=", "True", "\n", "self", ".", "image_dir", "=", "''", "## dir of images", "\n", "self", ".", "train_ann_file", "=", "'data/files/captions_train2014.json'", "\n", "self", ".", "val_ann_file", "=", "'data/files/captions_val2014.json'", "\n", "self", ".", "split_file", "=", "'data/files/caption_id.pkl'", "\n", "self", ".", "vocab_file", "=", "'data/files/vocab.pkl'", "\n", "self", ".", "keep_prob", "=", "0.5", "\n", "if", "self", ".", "is_dotatt", ":", "\n", "            ", "string", "=", "(", "'%s_conv3x_numl%s_kz%s_hierdotatt_keepprob%s'", ")", "%", "(", "self", ".", "encoder_name", ",", "self", ".", "num_layers", ",", "self", ".", "kernel_size", ",", "self", ".", "keep_prob", ")", "\n", "self", ".", "trained_model", "=", "'trained_models/'", "+", "string", "+", "'.pth'", "\n", "", "else", ":", "\n", "            ", "string", "=", "(", "'%s_conv3x_numl%s_kz%s_hiernnatt_keepprob%s'", ")", "%", "(", "self", ".", "encoder_name", ",", "self", ".", "num_layers", ",", "self", ".", "kernel_size", ",", "self", ".", "keep_prob", ")", "\n", "self", ".", "trained_model", "=", "'trained_models/'", "+", "string", "+", "'.pth'", "\n", "", "self", ".", "log_file", "=", "'logs/'", "+", "self", ".", "trained_model", ".", "split", "(", "'/'", ")", "[", "1", "]", "+", "'.txt'", "\n", "self", ".", "result_file", "=", "'results/captions_val2014_'", "+", "string", "+", "'_results.json'", "\n", "self", ".", "annfile", "=", "None", "\n", "self", ".", "weight_decay", "=", "0.0", "\n", "\n"]], "home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.coco_eval.demo.scores": [[12, 42], ["pycocotools.coco.COCO", "pycocotools.coco.COCO.loadRes", "pycocoevalcap.eval.COCOEvalCap", "coco.loadRes.getImgIds", "pycocoevalcap.eval.COCOEvalCap.evaluate", "format"], "function", ["None"], ["def", "scores", "(", "ann_file", "=", "None", ",", "res_file", "=", "None", ")", ":", "\n", "    ", "encoder", ".", "FLOAT_REPR", "=", "lambda", "o", ":", "format", "(", "o", ",", "'.3f'", ")", "\n", "# set up file names and pathes", "\n", "\n", "dataDir", "=", "'.'", "\n", "dataType", "=", "'val2014'", "\n", "algName", "=", "'fakecap'", "\n", "annFile", "=", "'%s/annotations/captions_%s_coco.json'", "%", "(", "dataDir", ",", "dataType", ")", "\n", "subtypes", "=", "[", "'results'", ",", "'evalImgs'", ",", "'eval'", "]", "\n", "[", "resFile", ",", "evalImgsFile", ",", "evalFile", "]", "=", "[", "'%s/results/captions_%s_%s_%s.json'", "%", "(", "dataDir", ",", "dataType", ",", "algName", ",", "subtype", ")", "for", "subtype", "in", "subtypes", "]", "\n", "\n", "if", "res_file", "is", "not", "None", ":", "\n", "        ", "resFile", "=", "res_file", "\n", "\n", "# create coco object and cocoRes object", "\n", "", "coco", "=", "COCO", "(", "annFile", ")", "\n", "cocoRes", "=", "coco", ".", "loadRes", "(", "resFile", ")", "\n", "# create cocoEval object by taking coco and cocoRes", "\n", "cocoEval", "=", "COCOEvalCap", "(", "coco", ",", "cocoRes", ")", "\n", "\n", "# evaluate on a subset of images by setting", "\n", "# cocoEval.params['image_id'] = cocoRes.getImgIds()", "\n", "# please remove this line when evaluating the full validation set", "\n", "cocoEval", ".", "params", "[", "'image_id'", "]", "=", "cocoRes", ".", "getImgIds", "(", ")", "\n", "\n", "# evaluate results", "\n", "cocoEval", ".", "evaluate", "(", ")", "\n", "\n", "return", "cocoEval", ".", "eval", "\n", "\n"]], "home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.data.ak_data_loader.CocoDataset.__init__": [[12, 20], ["torchvision.ToTensor", "torchvision.Normalize"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "image_path", ",", "split", ",", "width", "=", "224", ",", "height", "=", "224", ",", "transformer", "=", "None", ")", ":", "\n", "        ", "self", ".", "_image_path", "=", "image_path", "# dir of images", "\n", "self", ".", "_split", "=", "split", "\n", "self", ".", "_width", "=", "width", "\n", "self", ".", "_height", "=", "height", "\n", "self", ".", "_transformer", "=", "transformer", "# torch image transformer, e.g. crop, color distortion", "\n", "self", ".", "_totensor", "=", "transforms", ".", "ToTensor", "(", ")", "\n", "self", ".", "_normalizer", "=", "transforms", ".", "Normalize", "(", "mean", "=", "(", "0.485", ",", "0.456", ",", "0.406", ")", ",", "std", "=", "(", "0.229", ",", "0.224", ",", "0.225", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.data.ak_data_loader.CocoDataset.__getitem__": [[21, 41], ["os.path.join", "PIL.Image.open().convert", "ak_data_loader.CocoDataset._totensor", "ak_data_loader.CocoDataset._normalizer", "ak_data_loader.CocoDataset._transformer", "image.resize.resize.resize", "torch.IntTensor", "torch.IntTensor", "torch.IntTensor", "torch.IntTensor", "torch.IntTensor", "torch.IntTensor", "torch.IntTensor", "torch.IntTensor", "PIL.Image.open"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "\"\"\"\n        1. read one data from file (e.g. images)\n        2. preprocess the data (e.g. crop, resize)\n        3. return one data pair (e.g. image and label)\n        :param index:\n        :return:\n        \"\"\"", "\n", "image_file", "=", "os", ".", "path", ".", "join", "(", "self", ".", "_image_path", ",", "self", ".", "_split", "[", "index", "]", "[", "'filename'", "]", ")", "\n", "image_id", "=", "self", ".", "_split", "[", "index", "]", "[", "'image_id'", "]", "\n", "caption", "=", "self", ".", "_split", "[", "index", "]", "[", "'caption_id'", "]", "\n", "image", "=", "Image", ".", "open", "(", "image_file", ")", ".", "convert", "(", "'RGB'", ")", "\n", "if", "self", ".", "_transformer", "is", "not", "None", ":", "\n", "            ", "image", "=", "self", ".", "_transformer", "(", "image", ")", "\n", "", "else", ":", "\n", "            ", "image", "=", "image", ".", "resize", "(", "(", "self", ".", "_width", ",", "self", ".", "_height", ")", ",", "resample", "=", "Image", ".", "BILINEAR", ")", "\n", "", "image", "=", "self", ".", "_totensor", "(", "image", ")", "\n", "image", "=", "self", ".", "_normalizer", "(", "image", ")", "\n", "\n", "return", "torch", ".", "IntTensor", "(", "[", "image_id", "]", ")", ",", "image", ",", "torch", ".", "IntTensor", "(", "caption", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.data.ak_data_loader.CocoDataset.__len__": [[42, 44], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "_split", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.data.ak_data_loader.collate_fn": [[46, 77], ["zip", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.zeros", "torch.zeros", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "torch.zeros().long", "enumerate", "len", "len", "torch.ones", "torch.ones", "max", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "len", "len", "max", "max"], "function", ["None"], ["", "", "def", "collate_fn", "(", "data", ")", ":", "\n", "    ", "\"\"\"\n    creat mini-batch tensors from the list of tuples (image, caption)\n    :param data:\n    list of tuples (image_id, image, caption)\n    -image: torch tensor of shape (3, width, height)\n    -caption: torch tensor of shape (?)\n    -image_id: torch tensor of shape (1)\n    :return:\n    images: torch tensor of shape (batch_size, 3, width, height)\n    inputs: torch tensor of shape (batch_size, length)\n    targets: torch tensor of shape (batch_size, length)\n    mask: torch tensor of shape (batch_size, length)\n    image_ids: torch tensor of shape (batch_size, 1)\n    \"\"\"", "\n", "image_ids", ",", "images", ",", "captions", "=", "zip", "(", "*", "data", ")", "\n", "\n", "image_ids", "=", "torch", ".", "stack", "(", "image_ids", ",", "0", ")", "# (batch_size, 3, width, height)", "\n", "images", "=", "torch", ".", "stack", "(", "images", ",", "0", ")", "# (batch_size, 1)", "\n", "\n", "lengths", "=", "[", "len", "(", "cap", ")", "for", "cap", "in", "captions", "]", "\n", "mask", "=", "torch", ".", "zeros", "(", "len", "(", "captions", ")", ",", "max", "(", "lengths", ")", "-", "1", ")", "\n", "inputs", "=", "torch", ".", "zeros", "(", "len", "(", "captions", ")", ",", "max", "(", "lengths", ")", "-", "1", ")", ".", "long", "(", ")", "\n", "targets", "=", "torch", ".", "zeros", "(", "len", "(", "captions", ")", ",", "max", "(", "lengths", ")", "-", "1", ")", ".", "long", "(", ")", "\n", "for", "i", ",", "cap", "in", "enumerate", "(", "captions", ")", ":", "\n", "        ", "end", "=", "lengths", "[", "i", "]", "-", "1", "\n", "inputs", "[", "i", ",", ":", "end", "]", "=", "cap", "[", ":", "end", "]", "\n", "targets", "[", "i", ",", ":", "end", "]", "=", "cap", "[", "1", ":", "]", "\n", "mask", "[", "i", ",", ":", "end", "]", "=", "torch", ".", "ones", "(", "end", ")", "\n", "\n", "", "return", "image_ids", ",", "images", ",", "inputs", ",", "targets", ",", "mask", "\n", "\n"]], "home.repos.pwc.inspect_result.qingzwang_GHA-ImageCaptioning.data.ak_data_loader.get_data_loader": [[79, 104], ["isinstance", "ak_data_loader.CocoDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "open", "pickle.load", "image_cap.extend"], "function", ["None"], ["", "def", "get_data_loader", "(", "image_dir", ",", "split_file", ",", "\n", "transformer", ",", "split_key", "=", "'train'", ",", "width", "=", "224", ",", "height", "=", "224", ",", "\n", "batch_size", "=", "10", ",", "num_workers", "=", "6", ",", "shuffle", "=", "True", ")", ":", "\n", "\n", "    ", "with", "open", "(", "split_file", ",", "'r'", ")", "as", "f", ":", "\n", "        ", "split", "=", "pickle", ".", "load", "(", "f", ")", "\n", "", "if", "isinstance", "(", "split_key", ",", "list", ")", ":", "\n", "        ", "image_cap", "=", "[", "]", "\n", "for", "key", "in", "split_key", ":", "\n", "            ", "image_cap", ".", "extend", "(", "split", "[", "key", "]", ")", "\n", "", "", "else", ":", "\n", "        ", "image_cap", "=", "split", "[", "split_key", "]", "\n", "", "coco", "=", "CocoDataset", "(", "\n", "image_path", "=", "image_dir", ",", "\n", "split", "=", "image_cap", ",", "\n", "transformer", "=", "transformer", ",", "\n", "width", "=", "width", ",", "\n", "height", "=", "height", ")", "\n", "data_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "dataset", "=", "coco", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "shuffle", "=", "shuffle", ",", "\n", "num_workers", "=", "num_workers", ",", "\n", "collate_fn", "=", "collate_fn", ")", "\n", "\n", "return", "data_loader", "\n", "", ""]]}