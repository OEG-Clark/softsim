{"home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl._compute_q_v": [[41, 64], ["numpy.max", "numpy.absolute", "numpy.einsum", "numpy.min"], "function", ["None"], ["def", "_compute_q_v", "(", "\n", "reward_matrix", ",", "gamma", ",", "transition_matrix", ",", "values", ",", "affordance_mask", "=", "None", ")", ":", "\n", "  ", "\"\"\"Computes Q-value.\"\"\"", "\n", "\n", "# All transitions out of the goal state should be masked out since you cannot", "\n", "# actually _start_ your trajectories here and the environment terminates once", "\n", "# you get here.", "\n", "# TODO(zaf): Do this in a nicer way by doing a transition_matrix.copy() or", "\n", "# doing this masking somewhere else.", "\n", "transition_matrix", "[", "env_utils", ".", "GOAL_STATES", ",", ":", "]", "=", "0", "\n", "\n", "q_values", "=", "reward_matrix", "+", "gamma", "*", "np", ".", "einsum", "(", "\n", "'ijk,k->ij'", ",", "transition_matrix", ",", "values", ")", "\n", "\n", "if", "affordance_mask", "is", "not", "None", ":", "\n", "# Set Q-values that are unaffordable to the worst q-value.", "\n", "    ", "q_values", "=", "(", "\n", "q_values", "*", "affordance_mask", "+", "(", "1", "-", "affordance_mask", ")", "*", "np", ".", "min", "(", "q_values", ")", ")", "\n", "\n", "", "values_new", "=", "np", ".", "max", "(", "q_values", ",", "axis", "=", "1", ")", "\n", "value_diff", "=", "np", ".", "absolute", "(", "values", "-", "values_new", ")", "\n", "\n", "return", "q_values", ",", "values_new", ",", "value_diff", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.extract_greedy_policy": [[66, 94], ["numpy.random.default_rng", "rl._compute_q_v", "numpy.apply_along_axis", "affordances_fn", "np.random.default_rng.choice", "numpy.isclose", "numpy.eye", "numpy.flatnonzero", "q_values.max"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl._compute_q_v"], ["", "def", "extract_greedy_policy", "(", "\n", "reward_matrix", ":", "np", ".", "ndarray", ",", "\n", "transition_matrix", ":", "np", ".", "ndarray", ",", "\n", "values", ":", "np", ".", "ndarray", ",", "\n", "gamma", ":", "Union", "[", "float", ",", "np", ".", "ndarray", "]", "=", "DEFAULT_GAMMA", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "affordances_fn", ":", "Optional", "[", "affordances", ".", "AffordancesFn", "]", "=", "None", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "  ", "\"\"\"Returns a table containing the best greedy actions to take.\"\"\"", "\n", "rng", "=", "np", ".", "random", ".", "default_rng", "(", "seed", ")", "\n", "\n", "if", "affordances_fn", "is", "not", "None", ":", "\n", "    ", "affordances_mask", "=", "affordances_fn", "(", ")", "\n", "", "else", ":", "\n", "    ", "affordances_mask", "=", "None", "\n", "", "q_values", ",", "_", ",", "_", "=", "_compute_q_v", "(", "\n", "reward_matrix", ",", "gamma", ",", "transition_matrix", ",", "values", ",", "affordances_mask", ")", "\n", "\n", "# Use \"random\" argmax with stochastic tie-breaking:", "\n", "rargmax", "=", "lambda", "arr", ":", "rng", ".", "choice", "(", "np", ".", "flatnonzero", "(", "arr", ")", ")", "\n", "best_actions", "=", "np", ".", "apply_along_axis", "(", "\n", "rargmax", ",", "1", ",", "np", ".", "isclose", "(", "q_values", ",", "q_values", ".", "max", "(", "-", "1", ",", "keepdims", "=", "True", ")", ")", ")", "\n", "num_states", ",", "num_actions", "=", "reward_matrix", ".", "shape", "\n", "del", "num_states", "\n", "pi", "=", "np", ".", "eye", "(", "num_actions", ")", "[", "best_actions", "]", "\n", "assert", "pi", ".", "shape", "==", "reward_matrix", ".", "shape", "\n", "\n", "return", "pi", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.value_iteration": [[96, 155], ["datetime.datetime.now", "numpy.zeros", "range", "absl.logging.info", "ValueError", "affordances_fn", "rl._compute_q_v", "numpy.all", "datetime.datetime.now", "numpy.mean", "writer.write", "absl.logging.debug", "numpy.max", "numpy.min", "numpy.mean", "numpy.mean", "numpy.mean"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl._compute_q_v"], ["", "def", "value_iteration", "(", "\n", "reward_matrix", ":", "np", ".", "ndarray", ",", "\n", "transition_matrix", ":", "np", ".", "ndarray", ",", "\n", "gamma", ":", "Union", "[", "float", ",", "np", ".", "ndarray", "]", "=", "DEFAULT_GAMMA", ",", "\n", "stopping_threshold", ":", "float", "=", "0.0001", ",", "\n", "max_iterations", ":", "int", "=", "100", ",", "\n", "affordances_fn", ":", "Optional", "[", "affordances", ".", "AffordancesFn", "]", "=", "None", ",", "\n", "writer", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "datetime", ".", "timedelta", ",", "int", "]", ":", "\n", "  ", "\"\"\"Obtains the optimal policy for an MDP using value iteration.\n\n  Args:\n    reward_matrix: Array of shape |S| x |A| determiniting rewards for a\n      transition.\n    transition_matrix: Array of shape |S| x |A| x |S| determining the\n      probability of transitioning from (s, a) to s'.\n    gamma: Discount factor. If this is a matrix, it must be of shape |S| x |A|.\n    stopping_threshold: The minimum change in the values needed to prevent the\n      algorithm from stopping early.\n    max_iterations: The maximum number of iterations to run value iteration for.\n    affordances_fn: A function that returns the list of affordances and a mask.\n    writer: Writer to write data.\n\n  Returns:\n    The values (V_pi of shape |S|) at the end of value iteration.\n    The amount of time value iteration was run for.\n    The number of iterations value iteration ran for before exiting.\n  \"\"\"", "\n", "start_time", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "\n", "num_states", ",", "num_actions", ",", "_", "=", "transition_matrix", ".", "shape", "\n", "if", "reward_matrix", ".", "shape", "!=", "(", "num_states", ",", "num_actions", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "f'Reward matrix ({reward_matrix.shape})has an incompatible shape to '", "\n", "f'transition matrix ({transition_matrix.shape})'", ")", "\n", "\n", "", "values", "=", "np", ".", "zeros", "(", "num_states", ")", "\n", "if", "affordances_fn", "is", "not", "None", ":", "\n", "# Cache the mask so we don't repeatedly call it.", "\n", "    ", "affordances_mask", "=", "affordances_fn", "(", ")", "\n", "", "else", ":", "\n", "    ", "affordances_mask", "=", "None", "\n", "", "for", "i", "in", "range", "(", "max_iterations", ")", ":", "\n", "    ", "_", ",", "values", ",", "value_diff", "=", "_compute_q_v", "(", "\n", "reward_matrix", ",", "gamma", ",", "transition_matrix", ",", "values", ",", "affordances_mask", ")", "\n", "if", "writer", "is", "not", "None", ":", "\n", "      ", "writer", ".", "write", "(", "{", "\n", "'iteration'", ":", "i", ",", "'max_value'", ":", "np", ".", "max", "(", "values", ")", ",", "\n", "'min_value'", ":", "np", ".", "min", "(", "values", ")", ",", "'mean_value'", ":", "np", ".", "mean", "(", "values", ")", ",", "\n", "'mean_diff'", ":", "np", ".", "mean", "(", "value_diff", ")", ",", "'max_diff'", ":", "np", ".", "mean", "(", "value_diff", ")", ",", "\n", "}", ")", "\n", "", "if", "np", ".", "all", "(", "value_diff", "<", "stopping_threshold", ")", ":", "\n", "      ", "logging", ".", "debug", "(", "'Terminating value iteration: stopping threshold reached.'", ")", "\n", "break", "\n", "\n", "", "", "elapsed", "=", "datetime", ".", "datetime", ".", "now", "(", ")", "-", "start_time", "\n", "logging", ".", "info", "(", "\n", "'Value iteration completed. Value Diff: %s, iterations: %s, time : %s'", ",", "\n", "np", ".", "mean", "(", "value_diff", ")", ",", "i", ",", "elapsed", ")", "\n", "return", "values", ",", "elapsed", ",", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.run_policy_in_env": [[157, 247], ["affordances_option_models.env_utils.make_taxi_environment", "env_utils.make_taxi_environment.seed", "range", "absl.logging.debug", "env_utils.make_taxi_environment.reset", "range", "trajectories.append", "lengths.append", "rewards.append", "min", "max", "min", "max", "len", "len", "len", "absl.logging.debug", "env_utils.make_taxi_environment.reset", "policy", "env_utils.make_taxi_environment.step", "absl.logging.debug", "transitions.append", "termination_fn", "sum", "len", "affordances_option_models.env_utils.int_to_state_fn", "affordances_option_models.env_utils.int_to_state_fn", "rl.Transition"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.make_taxi_environment", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.int_to_state_fn", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.int_to_state_fn"], ["", "def", "run_policy_in_env", "(", "\n", "policy", ":", "Callable", "[", "[", "int", "]", ",", "int", "]", ",", "\n", "num_episodes", ":", "int", "=", "1000", ",", "\n", "max_steps_per_episode", ":", "int", "=", "1000", ",", "\n", "initial_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "termination_fn", ":", "Callable", "[", "[", "Transition", "]", ",", "bool", "]", "=", "lambda", "t", ":", "t", ".", "done", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "Trajectory", "]", ",", "List", "[", "int", "]", ",", "List", "[", "float", "]", "]", ":", "\n", "  ", "\"\"\"Executes policy in the environment.\"\"\"", "\n", "\n", "env", "=", "env_utils", ".", "make_taxi_environment", "(", ")", "\n", "env", ".", "seed", "(", "seed", ")", "\n", "total_steps", ",", "total_pickups", ",", "total_illegal", ",", "total_reward", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "\n", "trajectories", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "rewards", "=", "[", "]", "\n", "\n", "for", "_", "in", "range", "(", "num_episodes", ")", ":", "\n", "    ", "episode_reward", ",", "episode_length", ",", "reward", "=", "0", ",", "0", ",", "0", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "if", "initial_state", "is", "not", "None", ":", "\n", "      ", "env", ".", "s", "=", "initial_state", "\n", "state", "=", "env", ".", "s", "\n", "logging", ".", "debug", "(", "'State set to %s'", ",", "env", ".", "s", ")", "\n", "", "else", ":", "\n", "      ", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "", "transitions", "=", "[", "]", "\n", "for", "_", "in", "range", "(", "max_steps_per_episode", ")", ":", "\n", "      ", "action", "=", "policy", "(", "state", ")", "\n", "new_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "logging", ".", "debug", "(", "\n", "(", "'New transition: \\n\\t'", "\n", "'State @ t = %s,\\n\\t'", "\n", "'action = %s,\\n\\t'", "\n", "'State @ t+1 = %s,\\n\\t'", "\n", "'reward = %s'", ")", ",", "\n", "env_utils", ".", "int_to_state_fn", "(", "state", ")", ",", "\n", "action", ",", "\n", "env_utils", ".", "int_to_state_fn", "(", "new_state", ")", ",", "\n", "reward", ")", "\n", "\n", "if", "reward", "==", "20", ":", "\n", "        ", "total_pickups", "+=", "1", "\n", "assert", "done", ",", "'Episode should terminate when pickup is successful.'", "\n", "", "if", "reward", "==", "-", "10", ":", "\n", "        ", "total_illegal", "+=", "1", "\n", "\n", "", "transitions", ".", "append", "(", "\n", "Transition", "(", "state", ",", "action", ",", "reward", ",", "new_state", ",", "done", ")", ")", "\n", "state", "=", "new_state", "\n", "\n", "total_steps", "+=", "1", "\n", "total_reward", "+=", "reward", "\n", "episode_reward", "+=", "reward", "\n", "episode_length", "+=", "1", "\n", "\n", "if", "termination_fn", "(", "transitions", "[", "-", "1", "]", ")", ":", "break", "\n", "\n", "", "trajectories", ".", "append", "(", "transitions", ")", "\n", "lengths", ".", "append", "(", "episode_length", ")", "\n", "rewards", ".", "append", "(", "episode_reward", ")", "\n", "\n", "", "logging", ".", "debug", "(", "\n", "(", "'Results average over %d episodes.\\n\\t'", "\n", "'Average timesteps per episode: %s\\n\\t'", "\n", "'Average illegal pickups/drops per step: %s\\n\\t'", "\n", "'Average successful pickups per step: %s\\n\\t'", "\n", "'Average reward per step: %s\\n\\t'", "\n", "'Average episode reward: %s\\n\\t'", "\n", "'Min/Max episode reward: %s/%s\\n\\t'", "\n", "'Min/Max episode length: %s/%s\\n\\t'", ")", ",", "\n", "num_episodes", ",", "\n", "total_steps", "/", "num_episodes", ",", "\n", "total_illegal", "/", "total_steps", ",", "\n", "total_pickups", "/", "total_steps", ",", "\n", "total_reward", "/", "total_steps", ",", "\n", "sum", "(", "rewards", ")", "/", "len", "(", "rewards", ")", ",", "\n", "min", "(", "rewards", ")", ",", "\n", "max", "(", "rewards", ")", ",", "\n", "min", "(", "lengths", ")", ",", "\n", "max", "(", "lengths", ")", ",", "\n", ")", "\n", "\n", "assert", "len", "(", "lengths", ")", "==", "num_episodes", "\n", "assert", "len", "(", "rewards", ")", "==", "num_episodes", "\n", "assert", "len", "(", "trajectories", ")", "==", "num_episodes", "\n", "return", "trajectories", ",", "lengths", ",", "rewards", "\n", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.affordances._compute_affordance_mask": [[31, 45], ["absl.logging.log_every_n", "numpy.zeros().astype", "ValueError", "len", "numpy.all", "ValueError", "numpy.zeros", "tuple", "zip", "np.zeros().astype.sum", "len"], "function", ["None"], ["def", "_compute_affordance_mask", "(", "affordances", ":", "AffordancesList", ")", "->", "np", ".", "ndarray", ":", "\n", "  ", "\"\"\"Computes the affordances mask and does some error checking.\"\"\"", "\n", "if", "not", "affordances", ":", "\n", "    ", "raise", "ValueError", "(", "'List of affordances cannot be empty.'", ")", "\n", "", "logging", ".", "log_every_n", "(", "\n", "logging", ".", "INFO", ",", "'Number of affordances: %s'", ",", "10", ",", "len", "(", "affordances", ")", ")", "\n", "\n", "affs", "=", "np", ".", "zeros", "(", "\n", "(", "env_utils", ".", "NUM_STATES", ",", "len", "(", "definitions", ".", "Options", ")", ")", ")", ".", "astype", "(", "np", ".", "float", ")", "\n", "affs", "[", "tuple", "(", "zip", "(", "*", "affordances", ")", ")", "]", "=", "1.0", "\n", "\n", "if", "not", "np", ".", "all", "(", "affs", ".", "sum", "(", "1", ")", ">=", "1", ")", ":", "\n", "    ", "raise", "ValueError", "(", "'All states must have at least one option affordable.'", ")", "\n", "", "return", "affs", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.affordances._all_affs": [[47, 54], ["range", "affordances.append"], "function", ["None"], ["", "def", "_all_affs", "(", ")", "->", "AffordancesList", ":", "\n", "  ", "\"\"\"Returns all states + options.\"\"\"", "\n", "affordances", "=", "[", "]", "\n", "for", "state", "in", "range", "(", "env_utils", ".", "NUM_STATES", ")", ":", "\n", "    ", "for", "option", "in", "definitions", ".", "Options", ":", "\n", "      ", "affordances", ".", "append", "(", "(", "state", ",", "option", ".", "value", "-", "1", ")", ")", "\n", "", "", "return", "affordances", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.affordances._pickup_drop_affs": [[56, 69], ["range", "affordances.append"], "function", ["None"], ["", "def", "_pickup_drop_affs", "(", ")", "->", "AffordancesList", ":", "\n", "  ", "\"\"\"Returns all pickup and drop options.\"\"\"", "\n", "affordances", "=", "[", "]", "\n", "for", "state", "in", "range", "(", "env_utils", ".", "NUM_STATES", ")", ":", "\n", "    ", "for", "option", "in", "definitions", ".", "Options", ":", "\n", "      ", "if", "option", "in", "definitions", ".", "OptionsAny", ":", "\n", "# Skip options that do \"any\".", "\n", "        ", "continue", "\n", "", "affordances", ".", "append", "(", "\n", "# -1 from o.value since Options starts idx at 1 and matrix at 0.", "\n", "(", "state", ",", "option", ".", "value", "-", "1", ")", "\n", ")", "\n", "", "", "return", "affordances", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.affordances._relevant_pickup_drop_affs": [[71, 86], ["range", "int", "affordances_option_models.env_utils.grid_cell_to_xy", "affordances.append", "option.name.replace().split", "option.name.replace"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.grid_cell_to_xy"], ["", "def", "_relevant_pickup_drop_affs", "(", ")", "->", "AffordancesList", ":", "\n", "  ", "\"\"\"Returns only pickup and drop options that are relevant to the 4 corners.\"\"\"", "\n", "affordances", "=", "[", "]", "\n", "for", "state", "in", "range", "(", "env_utils", ".", "NUM_STATES", ")", ":", "\n", "    ", "for", "option", "in", "definitions", ".", "Options", ":", "\n", "      ", "if", "option", "in", "definitions", ".", "OptionsAny", ":", "\n", "# Skip options that do \"any\".", "\n", "        ", "continue", "\n", "", "gridcell", "=", "int", "(", "option", ".", "name", ".", "replace", "(", "'GoTo'", ",", "''", ")", ".", "split", "(", "'_'", ")", "[", "0", "]", ")", "\n", "target_location", "=", "env_utils", ".", "grid_cell_to_xy", "(", "gridcell", ")", "\n", "\n", "# The option goes to relevant corners of the world.", "\n", "if", "target_location", "in", "env_utils", ".", "LOCATION_TO_COLOR_MAPPING", ":", "\n", "        ", "affordances", ".", "append", "(", "(", "state", ",", "option", ".", "value", "-", "1", ")", ")", "\n", "", "", "", "return", "affordances", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.affordances.get_heuristic_affordances_by_name": [[94, 100], ["affordances._compute_affordance_mask"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.affordances._compute_affordance_mask"], ["def", "get_heuristic_affordances_by_name", "(", "affordances_name", ":", "str", ")", "->", "AffordancesFn", ":", "\n", "  ", "affordances", "=", "ALL_AFFORDANCES", "[", "affordances_name", "]", "(", ")", "\n", "mask", "=", "_compute_affordance_mask", "(", "affordances", ")", "\n", "def", "_affordance_function", "(", ")", ":", "\n", "    ", "return", "mask", "\n", "", "return", "_affordance_function", "\n", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.networks.OneHotAndConcatLayer.__init__": [[25, 29], ["super().__init__"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.__init__"], ["def", "__init__", "(", "self", ",", "depth1", ",", "depth2", ")", ":", "\n", "    ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_depth1", "=", "depth1", "\n", "self", ".", "_depth2", "=", "depth2", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.networks.OneHotAndConcatLayer.__call__": [[30, 35], ["tensorflow.concat", "tensorflow.squeeze", "tensorflow.squeeze", "tensorflow.one_hot", "tensorflow.one_hot"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "x", ",", "y", ")", ":", "\n", "    ", "x", ",", "y", "=", "tf", ".", "squeeze", "(", "x", ",", "1", ")", ",", "tf", ".", "squeeze", "(", "y", ",", "1", ")", "\n", "return", "tf", ".", "concat", "(", "\n", "(", "tf", ".", "one_hot", "(", "x", ",", "self", ".", "_depth1", ")", ",", "tf", ".", "one_hot", "(", "y", ",", "self", ".", "_depth2", ")", ")", ",", "\n", "axis", "=", "1", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.networks.BaseNetwork.__init__": [[40, 61], ["super().__init__", "networks.OneHotAndConcatLayer", "tensorflow.keras.layers.Dense", "tensorflow.keras.layers.Dense"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "depth1", ":", "int", ",", "\n", "depth2", ":", "int", ",", "\n", "num_outputs", ":", "int", ",", "\n", "num_hiddens", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "final_with_bias", ":", "bool", "=", "True", ",", "\n", "kernel_initializer", ":", "str", "=", "'glorot_uniform'", ",", "# Default from Keras.", "\n", ")", ":", "\n", "    ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_concat_layer", "=", "OneHotAndConcatLayer", "(", "depth1", ",", "depth2", ")", "\n", "if", "num_hiddens", "is", "None", "or", "num_hiddens", "<=", "0", ":", "\n", "      ", "self", ".", "_dense_layer", "=", "None", "\n", "", "else", ":", "\n", "      ", "self", ".", "_dense_layer", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "num_hiddens", ",", "activation", "=", "tf", ".", "keras", ".", "activations", ".", "relu", ")", "\n", "", "self", ".", "_out", "=", "tf", ".", "keras", ".", "layers", ".", "Dense", "(", "\n", "num_outputs", ",", "\n", "activation", "=", "tf", ".", "keras", ".", "activations", ".", "linear", ",", "\n", "use_bias", "=", "final_with_bias", ",", "\n", "kernel_initializer", "=", "kernel_initializer", ",", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.networks.BaseNetwork.__call__": [[63, 68], ["networks.BaseNetwork._concat_layer", "networks.BaseNetwork._out", "networks.BaseNetwork._dense_layer"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "state", ",", "option", ")", ":", "\n", "    ", "x", "=", "self", ".", "_concat_layer", "(", "state", ",", "option", ")", "\n", "if", "self", ".", "_dense_layer", "is", "not", "None", ":", "\n", "      ", "x", "=", "self", ".", "_dense_layer", "(", "x", ")", "\n", "", "return", "self", ".", "_out", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.networks.IndependentTransitionModel.__init__": [[73, 85], ["super().__init__", "networks.BaseNetwork", "networks.BaseNetwork", "networks.BaseNetwork"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_states", ":", "int", ",", "\n", "num_options", ":", "int", ",", "\n", "num_hiddens", ":", "Optional", "[", "int", "]", "=", "None", ")", ":", "\n", "    ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_logits_layer", "=", "BaseNetwork", "(", "\n", "num_states", ",", "num_options", ",", "num_states", ",", "num_hiddens", ")", "\n", "self", ".", "_length_layer", "=", "BaseNetwork", "(", "\n", "num_states", ",", "num_options", ",", "1", ",", "num_hiddens", ")", "\n", "self", ".", "_reward_layer", "=", "BaseNetwork", "(", "\n", "num_states", ",", "num_options", ",", "1", ",", "num_hiddens", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.networks.IndependentTransitionModel.__call__": [[86, 91], ["tensorflow_probability.distributions.Categorical", "networks.IndependentTransitionModel._reward_layer", "networks.IndependentTransitionModel._logits_layer", "tensorflow.keras.activations.relu", "networks.IndependentTransitionModel._length_layer"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "state", ",", "option", ")", ":", "\n", "    ", "dist", "=", "tfp", ".", "distributions", ".", "Categorical", "(", "self", ".", "_logits_layer", "(", "state", ",", "option", ")", ")", "\n", "length", "=", "tf", ".", "keras", ".", "activations", ".", "relu", "(", "self", ".", "_length_layer", "(", "state", ",", "option", ")", ")", "+", "1", "\n", "rewards", "=", "self", ".", "_reward_layer", "(", "state", ",", "option", ")", "\n", "return", "dist", ",", "length", ",", "rewards", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.networks.AffordanceNetwork.__init__": [[96, 109], ["super().__init__", "networks.BaseNetwork"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.__init__"], ["def", "__init__", "(", "\n", "self", ",", "\n", "num_states", ":", "int", ",", "\n", "num_options", ":", "int", ",", "\n", "num_intents", ":", "int", ",", "\n", "num_hiddens", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "# Used to rescale the logits so there is a bias towards a certain value.", "\n", "shift_constant", ":", "float", "=", "2.0", ",", "\n", ")", ":", "\n", "    ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "_main_layer", "=", "BaseNetwork", "(", "\n", "num_states", ",", "num_options", ",", "num_intents", ",", "num_hiddens", ")", "\n", "self", ".", "_shift_constant", "=", "shift_constant", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.networks.AffordanceNetwork.__call__": [[110, 114], ["networks.AffordanceNetwork._main_layer", "tensorflow.keras.activations.sigmoid"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "state", ",", "option", ")", ":", "\n", "    ", "x", "=", "self", ".", "_main_layer", "(", "state", ",", "option", ")", "\n", "x", "=", "tf", ".", "keras", ".", "activations", ".", "sigmoid", "(", "x", "+", "self", ".", "_shift_constant", ")", "\n", "return", "x", "\n", "", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.intent_utils.is_intent_completed": [[26, 81], ["affordances_option_models.env_utils.int_to_state_fn", "affordances_option_models.env_utils.LOCATION_TO_COLOR_MAPPING.get", "ValueError"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.int_to_state_fn"], ["def", "is_intent_completed", "(", "\n", "s_i", ":", "int", ",", "\n", "option_id", ":", "option_utils", ".", "Options", ",", "\n", "s_f", ":", "int", ",", "\n", "intent_id", ":", "Intents", ",", "\n", ")", "->", "IntentStatus", ":", "\n", "  ", "\"\"\"Determines if a (state, option, state) transition completes an intent.\n\n  Args:\n    s_i: (Unused) The integer representing the taxi state.\n    option_id: (Unused) The integer representation of the option.\n    s_f: The integer representing the taxi state.\n    intent_id: The intent to check the completion for.\n\n  Returns:\n    Status of the intent.\n  \"\"\"", "\n", "del", "s_i", ",", "option_id", "# Unused.", "\n", "\n", "final_taxi_state", "=", "env_utils", ".", "int_to_state_fn", "(", "s_f", ")", "\n", "\n", "if", "intent_id", "not", "in", "Intents", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "f'Unknown intent_id={intent_id}. See {Intents} for valid intents.'", ")", "\n", "\n", "# Obtain which color is reached at this taxi location.", "\n", "", "color_reached", "=", "env_utils", ".", "LOCATION_TO_COLOR_MAPPING", ".", "get", "(", "\n", "(", "final_taxi_state", ".", "row", ",", "final_taxi_state", ".", "col", ")", ",", "None", ")", "\n", "\n", "# Determine if the passenger is inside the car.", "\n", "passenger_inside_car", "=", "(", "\n", "final_taxi_state", ".", "passenger_status", "==", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", "\n", ")", "\n", "\n", "if", "color_reached", "is", "None", ":", "\n", "# No color was reached so the intent could not have been completed.", "\n", "    ", "return", "IntentStatus", ".", "incomplete", "\n", "\n", "# At this color, the current intent cannot be completed.", "\n", "", "if", "intent_id", "not", "in", "definitions", ".", "COLOR_TO_INTENT_MAPPING", "[", "color_reached", "]", ":", "\n", "    ", "return", "IntentStatus", ".", "incomplete", "\n", "\n", "# This intent is supposed to have the passenger inside the car.", "\n", "", "if", "(", "intent_id", "in", "definitions", ".", "IntentsWithPassengersInside", "and", "\n", "passenger_inside_car", ")", ":", "\n", "    ", "return", "IntentStatus", ".", "complete", "\n", "\n", "# This intent is supposed to have the passenger outside the car.", "\n", "", "if", "(", "intent_id", "in", "definitions", ".", "IntentsWithPassengersOutside", "and", "\n", "not", "passenger_inside_car", ")", ":", "\n", "    ", "if", "final_taxi_state", ".", "passenger_status", "==", "color_reached", ".", "value", ":", "\n", "# Color must match the passenger status.", "\n", "      ", "return", "IntentStatus", ".", "complete", "\n", "\n", "", "", "return", "IntentStatus", ".", "incomplete", "\n", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl_test.RlTest.test_value_iteration_policy_evaluation": [[25, 60], ["affordances_option_models.env_utils.get_transition_and_reward_matrices", "affordances_option_models.rl.value_iteration", "affordances_option_models.rl.extract_greedy_policy", "affordances_option_models.rl.run_policy_in_env", "rl_test.RlTest.assertLessEqual", "affordances_option_models.rl.value_iteration", "affordances_option_models.rl.extract_greedy_policy", "affordances_option_models.rl.run_policy_in_env", "rl_test.RlTest.assertLess", "rl_test.RlTest.assertGreater", "rl_test.RlTest.assertGreater", "numpy.argmax", "sum", "sum", "numpy.argmax", "sum", "sum"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.get_transition_and_reward_matrices", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.value_iteration", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.extract_greedy_policy", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.run_policy_in_env", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.value_iteration", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.extract_greedy_policy", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.run_policy_in_env"], ["  ", "def", "test_value_iteration_policy_evaluation", "(", "self", ")", ":", "\n", "    ", "\"\"\"Integration test for RL components.\"\"\"", "\n", "# Obtain a bad policy in the environment.", "\n", "_", ",", "P_matrix", ",", "R_matrix", "=", "env_utils", ".", "get_transition_and_reward_matrices", "(", ")", "# pylint: disable=invalid-name", "\n", "bad_values", ",", "_", ",", "_", "=", "rl", ".", "value_iteration", "(", "R_matrix", ",", "P_matrix", ",", "max_iterations", "=", "1", ")", "\n", "pi_bad", "=", "rl", ".", "extract_greedy_policy", "(", "R_matrix", ",", "P_matrix", ",", "bad_values", ",", "seed", "=", "1", ")", "\n", "\n", "_", ",", "lengths", ",", "rewards", "=", "rl", ".", "run_policy_in_env", "(", "\n", "lambda", "s", ":", "np", ".", "argmax", "(", "pi_bad", "[", "s", "]", ")", ",", "\n", "num_episodes", "=", "100", ",", "\n", "max_steps_per_episode", "=", "1000", ",", "\n", "seed", "=", "1", ")", "\n", "reward_bad", "=", "rewards", "[", "-", "1", "]", "\n", "self", ".", "assertLessEqual", "(", "\n", "sum", "(", "rewards", ")", "/", "sum", "(", "lengths", ")", ",", "-", "1.0", ",", "\n", "msg", "=", "'Avg reward per step should be bad for the untrained policy.'", ")", "\n", "\n", "# Obtain a good policy in the environment.", "\n", "bad_values", ",", "_", ",", "num_iterations", "=", "rl", ".", "value_iteration", "(", "\n", "R_matrix", ",", "P_matrix", ",", "max_iterations", "=", "10000", ")", "\n", "pi_good", "=", "rl", ".", "extract_greedy_policy", "(", "R_matrix", ",", "P_matrix", ",", "bad_values", ",", "seed", "=", "1", ")", "\n", "\n", "_", ",", "lengths", ",", "rewards", "=", "rl", ".", "run_policy_in_env", "(", "\n", "lambda", "s", ":", "np", ".", "argmax", "(", "pi_good", "[", "s", "]", ")", ",", "\n", "num_episodes", "=", "100", ",", "\n", "max_steps_per_episode", "=", "1000", ",", "\n", "seed", "=", "1", ")", "\n", "reward_good", "=", "rewards", "[", "-", "1", "]", "\n", "self", ".", "assertLess", "(", "\n", "num_iterations", ",", "20", ",", "\n", "msg", "=", "'Value iteration should take <= 20 iterations to converge.'", ")", "\n", "self", ".", "assertGreater", "(", "reward_good", ",", "reward_bad", ")", "\n", "self", ".", "assertGreater", "(", "\n", "sum", "(", "rewards", ")", "/", "sum", "(", "lengths", ")", ",", "0", ",", "\n", "msg", "=", "'Avg reward per step should be > zero for the trained policy.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.hrl.run_hrl_policy_in_env": [[38, 166], ["affordances_option_models.env_utils.make_taxi_environment", "env_utils.make_taxi_environment.seed", "collections.Counter", "collections.Counter", "range", "absl.logging.info", "env_utils.make_taxi_environment.reset", "trajectories.append", "lengths.append", "rewards.append", "options_per_episode.append", "min", "max", "min", "max", "sum", "min", "max", "min", "max", "sum", "sum", "absl.logging.debug", "env_utils.make_taxi_environment.reset", "policy_over_options", "collections.Counter.update", "range", "option_rewards.append", "collections.Counter.update", "option_lengths.append", "sum", "len", "sum", "len", "sum", "len", "sum", "len", "collections.Counter.values", "option_policy", "env_utils.make_taxi_environment.step", "absl.logging.debug", "transitions.append", "option_term_fn", "absl.logging.debug", "sum", "collections.Counter.most_common", "collections.Counter.most_common", "affordances_option_models.env_utils.int_to_state_fn", "affordances_option_models.env_utils.int_to_state_fn", "hrl.TransitionWithOption", "absl.logging.debug", "absl.logging.debug", "affordances_option_models.rl.Transition"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.make_taxi_environment", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.int_to_state_fn", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.int_to_state_fn"], ["def", "run_hrl_policy_in_env", "(", "\n", "option_policy", ":", "Callable", "[", "[", "int", ",", "Options", "]", ",", "int", "]", ",", "\n", "policy_over_options", ":", "Callable", "[", "[", "int", "]", ",", "Options", "]", ",", "\n", "option_term_fn", ":", "Callable", "[", "[", "TransitionWithOption", "]", ",", "bool", "]", ",", "\n", "max_option_length", ":", "int", ",", "\n", "num_episodes", ":", "int", "=", "1000", ",", "\n", "max_steps_per_episode", ":", "int", "=", "1000", ",", "\n", "initial_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "List", "[", "TrajectoryWithOption", "]", ",", "List", "[", "int", "]", ",", "List", "[", "float", "]", ",", "Statistics", "]", ":", "\n", "  ", "\"\"\"Executes policy in the environment.\"\"\"", "\n", "\n", "env", "=", "env_utils", ".", "make_taxi_environment", "(", ")", "\n", "env", ".", "seed", "(", "seed", ")", "\n", "\n", "trajectories", "=", "[", "]", "\n", "lengths", "=", "[", "]", "\n", "rewards", "=", "[", "]", "\n", "option_lengths", "=", "[", "]", "\n", "option_rewards", "=", "[", "]", "\n", "options_per_episode", "=", "[", "]", "\n", "total_steps", ",", "total_pickups", ",", "total_illegal", ",", "total_reward", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "option_counts", "=", "collections", ".", "Counter", "(", ")", "\n", "per_option_rewards", "=", "collections", ".", "Counter", "(", ")", "\n", "\n", "for", "_", "in", "range", "(", "num_episodes", ")", ":", "\n", "    ", "episode_reward", ",", "episode_length", ",", "reward", ",", "num_options", "=", "0", ",", "0", ",", "0", ",", "0", "\n", "state", "=", "env", ".", "reset", "(", ")", "\n", "if", "initial_state", "is", "not", "None", ":", "\n", "      ", "env", ".", "s", "=", "initial_state", "\n", "state", "=", "env", ".", "s", "\n", "logging", ".", "debug", "(", "'State set to %s'", ",", "env", ".", "s", ")", "\n", "", "else", ":", "\n", "      ", "state", "=", "env", ".", "reset", "(", ")", "\n", "\n", "", "transitions", "=", "[", "]", "\n", "\n", "while", "True", ":", "# Main rollout loop.", "\n", "# Step 1: Decide which option to execute.", "\n", "      ", "option_id", "=", "policy_over_options", "(", "state", ")", "\n", "option_counts", ".", "update", "(", "{", "option_id", ":", "1", "}", ")", "\n", "num_options", "+=", "1", "\n", "option_reward", "=", "0", "\n", "for", "i", "in", "range", "(", "max_option_length", ")", ":", "\n", "# Execute the option in the environment.", "\n", "        ", "action", "=", "option_policy", "(", "state", ",", "option_id", ")", "\n", "new_state", ",", "reward", ",", "done", ",", "_", "=", "env", ".", "step", "(", "action", ")", "\n", "\n", "logging", ".", "debug", "(", "\n", "(", "'New transition: \\n\\t'", "\n", "'State @ t = %s,\\n\\t'", "\n", "'action = %s,\\n\\t'", "\n", "'option= %s\\n\\t'", "\n", "'State @ t+1 = %s,\\n\\t'", "\n", "'reward = %s'", ")", ",", "\n", "env_utils", ".", "int_to_state_fn", "(", "state", ")", ",", "\n", "action", ",", "\n", "option_id", ",", "\n", "env_utils", ".", "int_to_state_fn", "(", "new_state", ")", ",", "\n", "reward", ")", "\n", "\n", "if", "reward", "==", "20", ":", "\n", "          ", "total_pickups", "+=", "1", "\n", "assert", "done", ",", "'Episode should terminate when pickup is successful.'", "\n", "", "if", "reward", "==", "-", "10", ":", "\n", "          ", "total_illegal", "+=", "1", "\n", "\n", "", "transitions", ".", "append", "(", "\n", "TransitionWithOption", "(", "\n", "rl", ".", "Transition", "(", "state", ",", "action", ",", "reward", ",", "new_state", ",", "done", ")", ",", "\n", "option_id", ")", ")", "\n", "state", "=", "new_state", "\n", "\n", "total_steps", "+=", "1", "\n", "total_reward", "+=", "reward", "\n", "episode_reward", "+=", "reward", "\n", "episode_length", "+=", "1", "\n", "option_reward", "+=", "reward", "\n", "\n", "if", "option_term_fn", "(", "transitions", "[", "-", "1", "]", ")", ":", "\n", "          ", "logging", ".", "debug", "(", "'Option terminated. Option length =%d'", ",", "i", ")", "\n", "break", "\n", "", "if", "episode_length", ">", "max_steps_per_episode", ":", "\n", "          ", "logging", ".", "debug", "(", "'Episode too long'", ")", "\n", "break", "\n", "\n", "", "", "option_rewards", ".", "append", "(", "option_reward", ")", "\n", "per_option_rewards", ".", "update", "(", "{", "option_id", ":", "option_reward", "}", ")", "\n", "option_lengths", ".", "append", "(", "i", "+", "1", ")", "\n", "if", "done", "or", "episode_length", ">", "max_steps_per_episode", ":", "\n", "        ", "logging", ".", "debug", "(", "'Episode terminated. Length=%d'", ",", "episode_length", ")", "\n", "break", "\n", "", "", "trajectories", ".", "append", "(", "transitions", ")", "\n", "lengths", ".", "append", "(", "episode_length", ")", "\n", "rewards", ".", "append", "(", "episode_reward", ")", "\n", "options_per_episode", ".", "append", "(", "num_options", ")", "\n", "\n", "", "statistics", "=", "{", "\n", "'num_episodes'", ":", "num_episodes", ",", "\n", "'avg_num_steps_per_episode'", ":", "total_steps", "/", "num_episodes", ",", "\n", "'avg_num_illegal_per_step'", ":", "total_illegal", "/", "total_steps", ",", "\n", "'avg_success_per_step'", ":", "total_pickups", "/", "total_steps", ",", "\n", "'avg_reward_per_step'", ":", "total_reward", "/", "total_steps", ",", "\n", "'prop_success'", ":", "total_pickups", "/", "num_episodes", ",", "\n", "'prop_illegal'", ":", "total_illegal", "/", "num_episodes", ",", "\n", "'avg_episode_reward'", ":", "sum", "(", "rewards", ")", "/", "len", "(", "rewards", ")", ",", "\n", "'min_episode_reward'", ":", "min", "(", "rewards", ")", ",", "\n", "'max_episode_reward'", ":", "max", "(", "rewards", ")", ",", "\n", "'min_episode_length'", ":", "min", "(", "lengths", ")", ",", "\n", "'max_episode_length'", ":", "max", "(", "lengths", ")", ",", "\n", "'avg_num_options_per_episode'", ":", "(", "\n", "sum", "(", "options_per_episode", ")", "/", "len", "(", "options_per_episode", ")", ")", ",", "\n", "'total_options_executed'", ":", "sum", "(", "options_per_episode", ")", ",", "\n", "'total_steps'", ":", "total_steps", ",", "\n", "'avg_option_length'", ":", "sum", "(", "option_lengths", ")", "/", "len", "(", "option_lengths", ")", ",", "\n", "'min_option_length'", ":", "min", "(", "option_lengths", ")", ",", "\n", "'max_option_length'", ":", "max", "(", "option_lengths", ")", ",", "\n", "'avg_option_reward'", ":", "sum", "(", "option_rewards", ")", "/", "len", "(", "option_rewards", ")", ",", "\n", "'min_option_reward'", ":", "min", "(", "option_rewards", ")", ",", "\n", "'max_option_reward'", ":", "max", "(", "option_rewards", ")", ",", "\n", "'most_common_options'", ":", "{", "k", ".", "name", ":", "v", "/", "sum", "(", "options_per_episode", ")", "\n", "for", "k", ",", "v", "in", "option_counts", ".", "most_common", "(", "10", ")", "}", ",", "\n", "'most_common_option_reward'", ":", "{", "k", ".", "name", ":", "(", "per_option_rewards", "[", "k", "]", "/", "v", ")", "\n", "for", "k", ",", "v", "in", "option_counts", ".", "most_common", "(", "10", ")", "}", ",", "\n", "}", "\n", "logging", ".", "info", "(", "statistics", ")", "\n", "assert", "sum", "(", "option_counts", ".", "values", "(", ")", ")", "==", "sum", "(", "options_per_episode", ")", "\n", "return", "trajectories", ",", "lengths", ",", "rewards", ",", "statistics", "\n", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_options.make_writer": [[49, 66], ["absl.logging.info", "enumerate", "absl.logging.info", "queue.close", "absl.logging.info", "program_stopper", "str", "queue.enqueue_task", "absl.logging.info"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.close", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.enqueue_task"], ["def", "make_writer", "(", "program_stopper", ")", ":", "\n", "  ", "\"\"\"Creates a writer node to write all options to a queue..\"\"\"", "\n", "def", "writer", "(", "queue", ")", ":", "\n", "    ", "logging", ".", "info", "(", "'Writer has started.'", ")", "\n", "future_to_task_key", "=", "{", "}", "\n", "for", "task_key", ",", "option", "in", "enumerate", "(", "option_utils", ".", "Options", ")", ":", "\n", "      ", "task_key", "=", "str", "(", "task_key", ")", "\n", "task_parameters", "=", "{", "'option'", ":", "option", "}", "\n", "future", "=", "queue", ".", "enqueue_task", "(", "task_key", ",", "task_parameters", ")", "\n", "logging", ".", "info", "(", "'Adding task %s: %s'", ",", "task_key", ",", "task_parameters", ")", "\n", "future_to_task_key", "[", "future", "]", "=", "task_key", "\n", "\n", "", "logging", ".", "info", "(", "'All options added to the queue. Waiting for results...'", ")", "\n", "queue", ".", "close", "(", ")", "\n", "logging", ".", "info", "(", "'All results received. Done.'", ")", "\n", "program_stopper", "(", "mark_as_completed", "=", "True", ")", "\n", "", "return", "writer", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_options.make_consumer": [[68, 113], ["os.path.join", "absl.logging.info", "os.path.exists", "os.makedirs", "absl.logging.info", "time.sleep", "absl.logging.info", "queue.closed", "time.sleep", "queue.get_task", "absl.logging.info", "affordances_option_models.option_utils.learn_option_policy", "absl.logging.info", "absl.logging.info", "queue.set_result", "ValueError", "open", "numpy.save", "absl.logging.info"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.closed", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.get_task", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.learn_option_policy", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.set_result"], ["", "def", "make_consumer", "(", "gamma", ",", "max_iterations", ",", "topic_name", ",", "save_path", ")", ":", "\n", "  ", "\"\"\"Makes the function that consumes the queue.\"\"\"", "\n", "save_path", "=", "os", ".", "path", ".", "join", "(", "\n", "save_path", ",", "\n", "f'gamma{gamma}'", ",", "\n", "f'max_iterations{max_iterations}'", ",", "\n", "'options'", ")", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_path", ")", ":", "\n", "    ", "os", ".", "makedirs", "(", "save_path", ")", "\n", "", "logging", ".", "info", "(", "'Saving to folder: %s'", ",", "save_path", ")", "\n", "\n", "def", "consumer", "(", "queue", ")", ":", "\n", "    ", "logging", ".", "info", "(", "'Starting consumer.'", ")", "\n", "time", ".", "sleep", "(", "5.0", ")", "# Wait until the writer adds all the tasks.", "\n", "while", "not", "queue", ".", "closed", "(", ")", ":", "\n", "      ", "try", ":", "\n", "        ", "time", ".", "sleep", "(", "1.0", ")", "\n", "task_key", ",", "task_params", "=", "queue", ".", "get_task", "(", "topic_name", ")", "\n", "logging", ".", "info", "(", "'Task obtained: %s with params: %s'", ",", "task_key", ",", "task_params", ")", "\n", "option", "=", "task_params", "[", "'option'", "]", "\n", "if", "option", "not", "in", "option_utils", ".", "Options", ":", "\n", "          ", "raise", "ValueError", "(", "\n", "f'Got the option: {option}. Expected: {option_utils.Options}'", ")", "\n", "\n", "", "option_policy", ",", "num_iters", "=", "option_utils", ".", "learn_option_policy", "(", "\n", "option", ",", "\n", "gamma", "=", "gamma", ",", "\n", "stopping_threshold", "=", "1e-5", ",", "\n", "max_iterations", "=", "max_iterations", ",", "\n", "seed", "=", "_SEED", ")", "\n", "logging", ".", "info", "(", "\n", "'Option was learned in %s iterations. Saving to disk.'", ",", "num_iters", ")", "\n", "\n", "option_save_path", "=", "f'{save_path}/{option.name}.npz'", "\n", "with", "open", "(", "option_save_path", ",", "'wb'", ")", "as", "fout", ":", "\n", "          ", "np", ".", "save", "(", "fout", ",", "option_policy", ",", "allow_pickle", "=", "False", ")", "\n", "", "logging", ".", "info", "(", "'Saved option to %s'", ",", "option_save_path", ")", "\n", "\n", "queue", ".", "set_result", "(", "\n", "topic_name", ",", "task_key", ",", "{", "'option'", ":", "option", ",", "'learned'", ":", "True", "}", ")", "\n", "", "except", "task_queue", ".", "QueueClosedError", ":", "\n", "        ", "logging", ".", "info", "(", "'Queue is empty, ending early!'", ")", "\n", "break", "\n", "", "", "logging", ".", "info", "(", "'Queue is empty. Closing consumer.'", ")", "\n", "", "return", "consumer", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_options._make_program": [[115, 149], ["launchpad.Program", "launchpad.make_program_stopper", "lp.Program.group", "affordances_option_models.task_queue.TaskQueueNode", "lp.Program.add_node", "task_queue.TaskQueueNode.register_handle", "lp.Program.group", "lp_learn_options.make_writer", "lp.Program.add_node", "lp.Program.group", "range", "task_queue.TaskQueueNode.make_node", "launchpad.PyNode", "len", "ValueError", "lp.Program.add_node", "task_queue.TaskQueueNode.writer", "launchpad.PyNode", "lp_learn_options.make_consumer", "task_queue.TaskQueueNode.reader"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.register_handle", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_options.make_writer", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.make_node", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.writer", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_options.make_consumer", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.reader"], ["", "def", "_make_program", "(", "gamma", ",", "max_iterations", ",", "save_path", ",", "num_consumers", "=", "1", ")", ":", "\n", "  ", "\"\"\"Creates the launchpad program.\"\"\"", "\n", "program", "=", "lp", ".", "Program", "(", "'option_learning'", ")", "\n", "program_stopper", "=", "lp", ".", "make_program_stopper", "(", "FLAGS", ".", "lp_launch_type", ")", "\n", "topic_name", "=", "'default'", "\n", "\n", "##############################", "\n", "#       Task Queue           #", "\n", "##############################", "\n", "with", "program", ".", "group", "(", "'queue'", ")", ":", "\n", "    ", "queue", "=", "task_queue", ".", "TaskQueueNode", "(", ")", "\n", "queue_handle", "=", "program", ".", "add_node", "(", "queue", ".", "make_node", "(", ")", ")", "\n", "queue", ".", "register_handle", "(", "queue_handle", ")", "\n", "\n", "##############################", "\n", "#     Problems creator       #", "\n", "##############################", "\n", "", "with", "program", ".", "group", "(", "'writer'", ")", ":", "\n", "    ", "write_to_queue", "=", "make_writer", "(", "program_stopper", ")", "\n", "program", ".", "add_node", "(", "\n", "lp", ".", "PyNode", "(", "write_to_queue", ",", "queue", ".", "writer", "(", ")", ")", ")", "\n", "\n", "##############################", "\n", "#     Problems Solver        #", "\n", "##############################", "\n", "\n", "", "with", "program", ".", "group", "(", "'consumer'", ")", ":", "\n", "    ", "if", "num_consumers", ">", "len", "(", "option_utils", ".", "Options", ")", ":", "\n", "      ", "raise", "ValueError", "(", "'Cannot have more consumers than options!'", ")", "\n", "", "for", "_", "in", "range", "(", "num_consumers", ")", ":", "\n", "      ", "program", ".", "add_node", "(", "lp", ".", "PyNode", "(", "make_consumer", "(", "\n", "gamma", ",", "max_iterations", ",", "topic_name", ",", "save_path", ")", ",", "queue", ".", "reader", "(", ")", ")", ")", "\n", "\n", "", "", "return", "program", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_options.main": [[151, 160], ["lp_learn_options._make_program", "launchpad.launch"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options._make_program"], ["", "def", "main", "(", "_", ")", ":", "\n", "\n", "  ", "program", "=", "_make_program", "(", "\n", "FLAGS", ".", "gamma", ",", "\n", "FLAGS", ".", "max_iterations", ",", "\n", "FLAGS", ".", "save_path", ",", "\n", "FLAGS", ".", "num_consumers", ")", "\n", "\n", "lp", ".", "launch", "(", "program", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Trainer.__init__": [[289, 357], ["tensorflow.random.set_seed", "affordances_option_models.networks.IndependentTransitionModel", "tensorflow.keras.optimizers.Adam", "affordances_option_models.training.get_training_steps", "absl.logging.info", "affordances_option_models.networks.AffordanceNetwork", "tensorflow.keras.optimizers.Adam", "affordances_option_models.affordances.get_heuristic_affordances_by_name", "absl.logging.info"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.training.get_training_steps", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.affordances.get_heuristic_affordances_by_name"], ["def", "__init__", "(", "\n", "self", ",", "*", ",", "\n", "num_states", ":", "int", ",", "\n", "num_options", ":", "int", ",", "\n", "hidden_dims", ":", "int", ",", "\n", "model_learning_rate", ":", "float", ",", "\n", "affordances_name", ":", "str", ",", "\n", "stop_after_steps", ":", "int", ",", "\n", "queue", ",", "\n", "num_intents", ":", "int", "=", "8", ",", "\n", "topic_name", "=", "'default'", ",", "\n", "save_path", ":", "str", "=", "'~'", ",", "\n", "affordances_threshold", ":", "float", "=", "0.5", ",", "\n", "seed", ":", "int", "=", "0", ",", "\n", "save_every", ":", "int", "=", "-", "1", ",", "\n", "use_learned_affordances", ":", "bool", "=", "False", ",", "\n", "writer", "=", "None", ",", "\n", "program_stopper", "=", "None", ")", ":", "\n", "    ", "self", ".", "_program_stopper", "=", "program_stopper", "\n", "tf", ".", "random", ".", "set_seed", "(", "seed", ")", "\n", "self", ".", "_model_network", "=", "networks", ".", "IndependentTransitionModel", "(", "\n", "num_states", ",", "\n", "num_options", ",", "\n", "hidden_dims", ")", "\n", "self", ".", "_model_optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "\n", "learning_rate", "=", "model_learning_rate", ")", "\n", "\n", "if", "use_learned_affordances", ":", "\n", "# When learning affordances, we learn a specialized affordance network", "\n", "# that gives the affordances to the model to mask the updates.", "\n", "      ", "self", ".", "_affordance_network", "=", "networks", ".", "AffordanceNetwork", "(", "\n", "num_states", ",", "num_options", ",", "num_intents", ",", "hidden_dims", ")", "\n", "self", ".", "_affordance_optimizer", "=", "tf", ".", "keras", ".", "optimizers", ".", "Adam", "(", "\n", "learning_rate", "=", "model_learning_rate", ")", "\n", "heuristic_affordance_fn", "=", "None", "\n", "", "else", ":", "\n", "# When using heuristic affordances, no affordance network is created", "\n", "# and instead we rely on a heuristic affordance function that provides", "\n", "# the relevant affordances to the model update.", "\n", "      ", "self", ".", "_affordance_network", "=", "None", "\n", "self", ".", "_affordance_optimizer", "=", "None", "\n", "heuristic_affordance_fn", "=", "affordances", ".", "get_heuristic_affordances_by_name", "(", "\n", "affordances_name", ")", "\n", "\n", "", "self", ".", "_affordances_threshold", "=", "affordances_threshold", "\n", "self", ".", "_model_train_step", ",", "self", ".", "_affordance_train_step", "=", "(", "\n", "training", ".", "get_training_steps", "(", "\n", "model_network", "=", "self", ".", "_model_network", ",", "\n", "model_optimizer", "=", "self", ".", "_model_optimizer", ",", "\n", "affordance_network", "=", "self", ".", "_affordance_network", ",", "\n", "affordance_optimizer", "=", "self", ".", "_affordance_optimizer", ",", "\n", "heuristic_affordance_fn", "=", "heuristic_affordance_fn", ",", "\n", "use_learned_affordances", "=", "use_learned_affordances", ",", "\n", "affordance_mask_threshold", "=", "affordances_threshold", ",", "\n", ")", "\n", ")", "\n", "self", ".", "_queue", "=", "queue", "\n", "if", "stop_after_steps", "<", "0", ":", "\n", "      ", "stop_after_steps", "=", "np", ".", "inf", "\n", "logging", ".", "info", "(", "'Setting stop after steps to inifnity.'", ")", "\n", "", "self", ".", "_stop_after_steps", "=", "stop_after_steps", "\n", "logging", ".", "info", "(", "'Training will proceed for %s steps.'", ",", "self", ".", "_stop_after_steps", ")", "\n", "self", ".", "_save_dir", "=", "save_path", "\n", "self", ".", "_writer", "=", "writer", "\n", "self", ".", "_topic_name", "=", "topic_name", "\n", "self", ".", "_total_steps", "=", "0", "\n", "self", ".", "_last_save", "=", "0", "\n", "self", ".", "_save_every", "=", "save_every", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Trainer.run": [[358, 410], ["time.sleep", "custom_nodes._save_models_and_weights", "custom_nodes.Trainer._queue.empty", "time.time", "custom_nodes.Trainer._queue.get_task", "time.time", "len", "affordances_option_models.training.prepare_data", "custom_nodes.Trainer._model_train_step", "custom_nodes.Trainer._affordance_train_step", "logging_dict.update", "logging_dict.update", "time.time", "custom_nodes.Trainer._writer.write", "custom_nodes._save_models_and_weights", "absl.logging.info", "time.time", "v.numpy().item", "v.numpy().item", "custom_nodes.Trainer._program_stopper", "custom_nodes.Trainer.items", "custom_nodes.Trainer.items", "v.numpy", "v.numpy"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._save_models_and_weights", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.empty", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.get_task", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.training.prepare_data", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._save_models_and_weights"], ["", "def", "run", "(", "self", ")", ":", "\n", "    ", "\"\"\"Runs training loop.\"\"\"", "\n", "count", "=", "0", "\n", "total_trajectories", "=", "0", "\n", "time", ".", "sleep", "(", "5", ")", "# Give time for things to fire up.", "\n", "_save_models_and_weights", "(", "\n", "total_trajectories", ",", "self", ".", "_total_steps", ",", "self", ".", "_model_network", ",", "\n", "self", ".", "_save_dir", ",", "affordance_network", "=", "self", ".", "_affordance_network", ")", "\n", "while", "not", "self", ".", "_queue", ".", "empty", "(", ")", ":", "\n", "      ", "try", ":", "\n", "        ", "running_time", "=", "time", ".", "time", "(", ")", "\n", "_", ",", "result", "=", "self", ".", "_queue", ".", "get_task", "(", "self", ".", "_topic_name", ")", "\n", "queue_get_time", "=", "time", ".", "time", "(", ")", "-", "running_time", "\n", "running_time", "=", "time", ".", "time", "(", ")", "\n", "total_trajectories", "+=", "len", "(", "result", "[", "'data'", "]", ")", "\n", "data", "=", "training", ".", "prepare_data", "(", "result", "[", "'data'", "]", ")", "\n", "model_losses", "=", "self", ".", "_model_train_step", "(", "data", ")", "\n", "affordance_losses", "=", "self", ".", "_affordance_train_step", "(", "data", ")", "\n", "self", ".", "_total_steps", "+=", "result", "[", "'total_steps'", "]", "\n", "\n", "# Log important statistics.", "\n", "logging_dict", "=", "{", "\n", "'total_steps'", ":", "self", ".", "_total_steps", ",", "\n", "'updates'", ":", "count", ",", "\n", "'total_trajectories'", ":", "total_trajectories", ",", "\n", "'step_time'", ":", "time", ".", "time", "(", ")", "-", "running_time", ",", "\n", "'collection_time'", ":", "result", "[", "'collection_time'", "]", ",", "\n", "'queue_put_time'", ":", "result", "[", "'queue_put_time'", "]", ",", "\n", "'queue_get_time'", ":", "queue_get_time", ",", "\n", "}", "\n", "logging_dict", ".", "update", "(", "\n", "{", "k", ":", "v", ".", "numpy", "(", ")", ".", "item", "(", ")", "for", "k", ",", "v", "in", "model_losses", ".", "items", "(", ")", "}", ")", "\n", "logging_dict", ".", "update", "(", "\n", "{", "k", ":", "v", ".", "numpy", "(", ")", ".", "item", "(", ")", "for", "k", ",", "v", "in", "affordance_losses", ".", "items", "(", ")", "}", ")", "\n", "if", "self", ".", "_writer", ":", "self", ".", "_writer", ".", "write", "(", "logging_dict", ")", "\n", "\n", "count", "+=", "1", "\n", "\n", "if", "self", ".", "_total_steps", "-", "self", ".", "_last_save", ">", "self", ".", "_save_every", ":", "\n", "          ", "_save_models_and_weights", "(", "\n", "total_trajectories", ",", "self", ".", "_total_steps", ",", "self", ".", "_model_network", ",", "\n", "self", ".", "_save_dir", ",", "self", ".", "_affordance_network", ")", "\n", "self", ".", "_last_save", "=", "self", ".", "_total_steps", "\n", "\n", "", "if", "self", ".", "_total_steps", ">", "self", ".", "_stop_after_steps", ":", "\n", "          ", "logging", ".", "info", "(", "'Training completed after %s/%s steps.'", ",", "\n", "self", ".", "_total_steps", ",", "self", ".", "_stop_after_steps", ")", "\n", "if", "self", ".", "_program_stopper", "is", "not", "None", ":", "\n", "            ", "self", ".", "_program_stopper", "(", "mark_as_completed", "=", "True", ")", "\n", "", "return", "\n", "", "", "except", "task_queue", ".", "QueueClosedErrors", ":", "\n", "        ", "break", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Trainer.get_option_model_table": [[411, 416], ["absl.logging.info", "custom_nodes._make_option_model_table"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._make_option_model_table"], ["", "", "", "def", "get_option_model_table", "(", "self", ")", ":", "\n", "    ", "logging", ".", "info", "(", "'Get option model has been requested!'", ")", "\n", "return", "(", "\n", "_make_option_model_table", "(", "self", ".", "_model_network", ")", ",", "\n", "self", ".", "_total_steps", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Trainer.get_affordance_table": [[417, 422], ["absl.logging.info", "custom_nodes._make_affordance_table"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._make_affordance_table"], ["", "def", "get_affordance_table", "(", "self", ")", ":", "\n", "    ", "logging", ".", "info", "(", "'Affordances requested.'", ")", "\n", "return", "{", "\n", "'affordances'", ":", "_make_affordance_table", "(", "\n", "self", ".", "_affordance_network", ",", "self", ".", "_affordances_threshold", ")", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Evaluation.__init__": [[430, 456], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "*", ",", "\n", "path_to_options", ":", "str", ",", "\n", "affordances_name", ":", "str", ",", "\n", "gamma", ":", "float", ",", "\n", "max_iterations", ":", "int", ",", "\n", "trainer_node", ":", "Optional", "[", "Trainer", "]", "=", "None", ",", "\n", "path_to_option_model", ":", "Optional", "[", "str", "]", "=", "None", ",", "\n", "writer", "=", "None", ",", "\n", "vi_writer", "=", "None", ",", "\n", "save_every", ":", "int", "=", "1", ",", "\n", "save_path", ":", "str", "=", "'~'", ",", "\n", "num_eval_episodes", ":", "int", "=", "1000", ",", "\n", ")", ":", "\n", "    ", "self", ".", "_trainer_node", "=", "trainer_node", "\n", "self", ".", "_path_to_options", "=", "path_to_options", "\n", "self", ".", "_path_to_option_model", "=", "path_to_option_model", "\n", "self", ".", "_affordances_name", "=", "affordances_name", "\n", "self", ".", "_writer", "=", "writer", "\n", "self", ".", "_vi_writer", "=", "vi_writer", "\n", "self", ".", "_max_iterations", "=", "max_iterations", "\n", "self", ".", "_gamma", "=", "gamma", "\n", "self", ".", "_save_every", "=", "save_every", "\n", "self", ".", "_last_save", "=", "None", "\n", "self", ".", "_save_dir", "=", "save_path", "\n", "self", ".", "_num_eval_episodes", "=", "num_eval_episodes", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Evaluation._get_latest_options": [[457, 459], ["custom_nodes._load_options"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._load_options"], ["", "def", "_get_latest_options", "(", "self", ")", ":", "\n", "    ", "return", "_load_options", "(", "self", ".", "_path_to_options", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Evaluation._get_latest_option_model": [[460, 471], ["absl.logging.info", "custom_nodes.Evaluation._trainer_node.get_option_model_table", "RuntimeError", "_np_load().item", "custom_nodes._np_load"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Trainer.get_option_model_table", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_load"], ["", "def", "_get_latest_option_model", "(", "self", ")", ":", "\n", "    ", "\"\"\"Returns latest option model from relevant source.\"\"\"", "\n", "if", "self", ".", "_path_to_option_model", "is", "None", ":", "\n", "      ", "logging", ".", "info", "(", "'Getting option model from trainer node.'", ")", "\n", "if", "self", ".", "_trainer_node", "is", "None", ":", "\n", "        ", "raise", "RuntimeError", "(", "\n", "'Cannot get latest option model if both path to option model and'", "\n", "' trainer node is None.'", ")", "\n", "", "return", "self", ".", "_trainer_node", ".", "get_option_model_table", "(", ")", "\n", "", "else", ":", "\n", "      ", "return", "_np_load", "(", "self", ".", "_path_to_option_model", ")", ".", "item", "(", ")", ",", "1", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Evaluation._run_evaluation": [[472, 548], ["absl.logging.info", "affordances_fn", "affordances_option_models.option_utils.learn_policy_over_options", "absl.logging.info", "absl.logging.info", "numpy.argmax", "numpy.all", "ValueError", "numpy.argmax", "affordances_option_models.option_utils.Options", "affordances_option_models.option_utils.check_option_termination", "numpy.count_nonzero", "absl.logging.info", "affordances_option_models.hrl.run_hrl_policy_in_env", "all_statistics.update", "absl.logging.info", "custom_nodes._save_hrl_tables", "option_model_table[].copy", "policy_over_options_table.sum", "rollout_statistics.items"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.learn_policy_over_options", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.check_option_termination", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.hrl.run_hrl_policy_in_env", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._save_hrl_tables"], ["", "", "def", "_run_evaluation", "(", "\n", "self", ",", "\n", "option_policy_table", ",", "option_model_table", ",", "affordances_fn", ",", "total_steps", "=", "0", "\n", ")", ":", "\n", "    ", "\"\"\"Runs evaluation on a single set of tables.\"\"\"", "\n", "logging", ".", "info", "(", "'Running value iteration.'", ")", "\n", "if", "self", ".", "_last_save", "is", "None", ":", "\n", "      ", "self", ".", "_last_save", "=", "total_steps", "\n", "\n", "", "affordances_mask", "=", "affordances_fn", "(", ")", "\n", "\n", "policy_over_options_table", ",", "num_iters", "=", "option_utils", ".", "learn_policy_over_options", "(", "\n", "option_reward", "=", "option_model_table", "[", "'rewards'", "]", ",", "\n", "option_transition", "=", "option_model_table", "[", "'transitions'", "]", ".", "copy", "(", ")", ",", "\n", "option_length", "=", "option_model_table", "[", "'lengths'", "]", ",", "\n", "stopping_threshold", "=", "1e-8", ",", "\n", "gamma", "=", "self", ".", "_gamma", ",", "\n", "affordances_fn", "=", "affordances_fn", ",", "\n", "max_iterations", "=", "self", ".", "_max_iterations", ",", "\n", "seed", "=", "self", ".", "_EVAL_NODE_SEED", ",", "\n", "writer", "=", "self", ".", "_vi_writer", ")", "\n", "logging", ".", "info", "(", "'value iteration completed in %d steps'", ",", "num_iters", ")", "\n", "def", "option_policy", "(", "state", ":", "int", ",", "option_id", ":", "option_utils", ".", "Options", ")", "->", "int", ":", "\n", "      ", "action_probabilities", "=", "option_policy_table", "[", "option_id", "]", "[", "state", "]", "\n", "return", "np", ".", "argmax", "(", "action_probabilities", ")", "\n", "\n", "", "if", "not", "np", ".", "all", "(", "policy_over_options_table", ".", "sum", "(", "1", ")", ">", "0", ")", ":", "\n", "# Note that we do not actually check if this is a stochastic policy matrix", "\n", "# since the masking is not guaranteed to result in a probability matrix.", "\n", "# We probably want to do something like set logits to -inf and then", "\n", "# softmax, but since we do not actually use the proabalistic policy and", "\n", "# only the greedy, then this works equivalently.", "\n", "      ", "raise", "ValueError", "(", "'At least one option should be affordable!'", ")", "\n", "\n", "", "def", "policy_over_options", "(", "state", ":", "int", ")", "->", "option_utils", ".", "Options", ":", "\n", "      ", "option_int", "=", "np", ".", "argmax", "(", "policy_over_options_table", "[", "state", "]", ")", "\n", "# Options are indexed from 1, the model starts at 0.", "\n", "return", "option_utils", ".", "Options", "(", "option_int", "+", "1", ")", "\n", "\n", "", "def", "option_term_fn", "(", "transition", ":", "hrl", ".", "TransitionWithOption", ")", "->", "bool", ":", "\n", "      ", "rl_transition", "=", "transition", ".", "transition", "\n", "env_done", "=", "rl_transition", ".", "done", "\n", "option_done", "=", "option_utils", ".", "check_option_termination", "(", "\n", "rl_transition", ".", "s_t", ",", "\n", "rl_transition", ".", "a_t", ",", "\n", "transition", ".", "option_id", ")", "\n", "return", "option_done", "or", "env_done", "\n", "\n", "# Verification of learned policy.", "\n", "", "all_statistics", "=", "{", "\n", "'num_iters'", ":", "num_iters", ",", "\n", "'total_steps'", ":", "total_steps", ",", "\n", "'affordance_set_size'", ":", "np", ".", "count_nonzero", "(", "affordances_mask", ")", ",", "\n", "}", "\n", "for", "option_length", "in", "self", ".", "_OPTION_LENGTHS_TO_EVAL", ":", "\n", "      ", "logging", ".", "info", "(", "'running policy with option length = %s'", ",", "option_length", ")", "\n", "_", ",", "_", ",", "_", ",", "rollout_statistics", "=", "hrl", ".", "run_hrl_policy_in_env", "(", "\n", "option_policy", "=", "option_policy", ",", "\n", "policy_over_options", "=", "policy_over_options", ",", "\n", "option_term_fn", "=", "option_term_fn", ",", "\n", "max_option_length", "=", "option_length", ",", "\n", "num_episodes", "=", "self", ".", "_num_eval_episodes", ",", "\n", "seed", "=", "self", ".", "_EVAL_NODE_SEED", ",", "\n", "max_steps_per_episode", "=", "100", ",", "\n", ")", "\n", "all_statistics", ".", "update", "(", "\n", "{", "f'{k}_{option_length}'", ":", "v", "for", "k", ",", "v", "in", "rollout_statistics", ".", "items", "(", ")", "}", ")", "\n", "", "if", "total_steps", "-", "self", ".", "_last_save", ">", "self", ".", "_save_every", ":", "\n", "      ", "logging", ".", "info", "(", "'Saving HRL tables.'", ")", "\n", "_save_hrl_tables", "(", "\n", "total_steps", ",", "policy_over_options_table", ",", "option_policy_table", ",", "\n", "self", ".", "_save_dir", ",", "self", ".", "_affordances_name", ",", "\n", "affordances_table", "=", "affordances_mask", ")", "\n", "self", ".", "_last_save", "=", "total_steps", "\n", "", "logging", ".", "info", "(", "'Steps since last save: %s'", ",", "total_steps", "-", "self", ".", "_last_save", ")", "\n", "return", "all_statistics", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Evaluation.run": [[549, 568], ["absl.logging.info", "time.sleep", "custom_nodes._get_affordances_function", "time.sleep", "absl.logging.info", "custom_nodes.Evaluation._get_latest_options", "custom_nodes.Evaluation._get_latest_option_model", "absl.logging.info", "custom_nodes.Evaluation._run_evaluation", "custom_nodes.Evaluation._writer.write"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._get_affordances_function", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Evaluation._get_latest_options", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Evaluation._get_latest_option_model", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Evaluation._run_evaluation"], ["", "def", "run", "(", "self", ")", ":", "\n", "    ", "\"\"\"Runs value iteration and evaluation for the option model.\"\"\"", "\n", "logging", ".", "info", "(", "'Starting evaluation node.'", ")", "\n", "time", ".", "sleep", "(", "10", ")", "# Wait a while so network can get created etc.", "\n", "affordances_fn", "=", "_get_affordances_function", "(", "\n", "self", ".", "_affordances_name", ",", "self", ".", "_trainer_node", ")", "\n", "while", "True", ":", "\n", "      ", "time", ".", "sleep", "(", "1", ")", "\n", "logging", ".", "info", "(", "'Obtaining the latest tables.'", ")", "\n", "option_policy_table", "=", "self", ".", "_get_latest_options", "(", ")", "\n", "option_model_table", ",", "total_steps", "=", "self", ".", "_get_latest_option_model", "(", ")", "\n", "logging", ".", "info", "(", "'Running an evaluation'", ")", "\n", "all_statistics", "=", "self", ".", "_run_evaluation", "(", "\n", "option_policy_table", ",", "option_model_table", ",", "affordances_fn", ",", "\n", "total_steps", "=", "total_steps", ")", "\n", "if", "not", "all_statistics", ":", "continue", "\n", "all_statistics", "[", "'eval_affordances_name'", "]", "=", "self", ".", "_affordances_name", "\n", "if", "self", ".", "_writer", "is", "not", "None", ":", "\n", "        ", "self", ".", "_writer", ".", "write", "(", "all_statistics", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Rollout.__init__": [[573, 593], ["None"], "methods", ["None"], ["def", "__init__", "(", "\n", "self", ",", "*", ",", "\n", "global_seed", ":", "int", ",", "\n", "max_option_length", ":", "int", ",", "\n", "batch_size", ":", "int", ",", "\n", "path_to_options", ":", "str", ",", "\n", "affordances_name", ":", "str", ",", "\n", "queue_writer", "=", "None", ",", "\n", "trainer_node", "=", "None", ",", "\n", ")", ":", "\n", "    ", "self", ".", "_global_seed", "=", "global_seed", "\n", "self", ".", "_max_option_length", "=", "max_option_length", "\n", "self", ".", "_batch_size", "=", "batch_size", "\n", "self", ".", "_path_to_options", "=", "path_to_options", "\n", "self", ".", "_affordances_name", "=", "affordances_name", "\n", "self", ".", "_queue_writer", "=", "queue_writer", "\n", "if", "affordances_name", "==", "'learned'", ":", "\n", "      ", "self", ".", "_trainer_node", "=", "trainer_node", "\n", "", "else", ":", "\n", "      ", "self", ".", "_trainer_node", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Rollout._get_option_table": [[594, 596], ["custom_nodes._load_options"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._load_options"], ["", "", "def", "_get_option_table", "(", "self", ")", ":", "\n", "    ", "return", "_load_options", "(", "self", ".", "_path_to_options", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Rollout.run": [[597, 645], ["absl.logging.info", "custom_nodes.Rollout._get_option_table", "custom_nodes._get_affordances_function", "absl.logging.info", "absl.logging.info", "itertools.count", "time.sleep", "str", "time.time", "_get_affordances_function.", "affordances_option_models.data.get_trajectories", "absl.logging.info", "time.time", "time.time", "custom_nodes.Rollout._queue_writer.enqueue_task", "absl.logging.info", "absl.logging.info", "time.time"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Rollout._get_option_table", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._get_affordances_function", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.data.get_trajectories", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.enqueue_task"], ["", "def", "run", "(", "self", ")", ":", "\n", "    ", "\"\"\"Runs the rollout node to collect data.\"\"\"", "\n", "logging", ".", "info", "(", "'Welcome to the rollout node.'", ")", "\n", "option_policy_table", "=", "self", ".", "_get_option_table", "(", ")", "\n", "\n", "affordances_fn", "=", "_get_affordances_function", "(", "\n", "self", ".", "_affordances_name", ",", "self", ".", "_trainer_node", ")", "\n", "\n", "logging", ".", "info", "(", "'Using affordances %s in rollout node'", ",", "self", ".", "_affordances_name", ")", "\n", "logging", ".", "info", "(", "'Now collecting data.'", ")", "\n", "queue_put_time", "=", "0", "\n", "for", "i", "in", "itertools", ".", "count", "(", ")", ":", "\n", "      ", "try", ":", "\n", "        ", "time", ".", "sleep", "(", "0.5", ")", "\n", "rollout_seed", "=", "self", ".", "_global_seed", "+", "i", "\n", "task_key", "=", "str", "(", "rollout_seed", ")", "\n", "running_time", "=", "time", ".", "time", "(", ")", "\n", "affordances_mask", "=", "affordances_fn", "(", ")", "\n", "data", ",", "total_steps", "=", "data_tools", ".", "get_trajectories", "(", "\n", "num_trajectories", "=", "self", ".", "_batch_size", ",", "\n", "max_trajectory_length", "=", "self", ".", "_max_option_length", ",", "\n", "option_policies", "=", "option_policy_table", ",", "\n", "affordances_mask", "=", "affordances_mask", ",", "\n", "initial_state", "=", "None", ",", "\n", "uniform_random_initial_state", "=", "True", ",", "\n", "seed", "=", "rollout_seed", ")", "\n", "collection_time", "=", "time", ".", "time", "(", ")", "-", "running_time", "\n", "logging", ".", "info", "(", "\n", "'Collected %s trajectories (total_steps=%s) in %s seconds'", ",", "\n", "self", ".", "_batch_size", ",", "\n", "total_steps", ",", "\n", "collection_time", ")", "\n", "if", "self", ".", "_queue_writer", "is", "not", "None", ":", "\n", "          ", "running_time", "=", "time", ".", "time", "(", ")", "\n", "self", ".", "_queue_writer", ".", "enqueue_task", "(", "task_key", ",", "{", "\n", "'rollout_seed'", ":", "rollout_seed", ",", "\n", "'data'", ":", "data", ",", "\n", "'total_steps'", ":", "total_steps", ",", "\n", "'collection_time'", ":", "collection_time", ",", "\n", "'queue_put_time'", ":", "queue_put_time", ",", "\n", "}", ")", "\n", "queue_put_time", "=", "time", ".", "time", "(", ")", "-", "running_time", "\n", "", "else", ":", "\n", "          ", "logging", ".", "info", "(", "'Data was collected but no queue to put it into.'", ")", "\n", "break", "\n", "", "", "except", "task_queue", ".", "QueueClosedErrors", ":", "\n", "        ", "logging", ".", "info", "(", "'Queue is empty, ending early!'", ")", "\n", "break", "\n", "", "", "", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_save": [[54, 59], ["open", "numpy.save"], "function", ["None"], ["def", "_np_save", "(", "\n", "save_path", ":", "str", ",", "array", ":", "Union", "[", "np", ".", "ndarray", ",", "Dict", "[", "Any", ",", "np", ".", "ndarray", "]", "]", ")", ":", "\n", "  ", "\"\"\"Saves a numpy array.\"\"\"", "\n", "with", "open", "(", "save_path", ",", "'wb'", ")", "as", "fout", ":", "\n", "    ", "np", ".", "save", "(", "fout", ",", "array", ",", "allow_pickle", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_load": [[61, 65], ["open", "numpy.load"], "function", ["None"], ["", "", "def", "_np_load", "(", "load_path", ":", "str", ")", "->", "np", ".", "ndarray", ":", "\n", "  ", "\"\"\"Loads a numpy array.\"\"\"", "\n", "with", "open", "(", "load_path", ",", "'rb'", ")", "as", "fout", ":", "\n", "    ", "return", "np", ".", "load", "(", "fout", ",", "allow_pickle", "=", "True", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._load_options": [[67, 88], ["functools.lru_cache", "os.path.join", "absl.logging.log_every_n_seconds", "custom_nodes._np_load", "absl.logging.info", "affordances_option_models.option_utils.Options"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_load"], ["", "", "@", "functools", ".", "lru_cache", "(", "maxsize", "=", "1", ")", "\n", "def", "_load_options", "(", "\n", "path_to_options", ":", "str", ",", "debugging", ":", "bool", "=", "False", "\n", ")", "->", "Dict", "[", "definitions", ".", "Options", ",", "np", ".", "ndarray", "]", ":", "\n", "  ", "\"\"\"Loads options into a table.\"\"\"", "\n", "option_policy_table", "=", "{", "}", "\n", "for", "option_id", "in", "option_utils", ".", "Options", ":", "\n", "    ", "load_path", "=", "os", ".", "path", ".", "join", "(", "path_to_options", ",", "option_id", ".", "name", "+", "'.npz'", ")", "\n", "if", "debugging", "and", "option_id", ".", "value", ">", "1", ":", "\n", "# When debugging, we are just looking for if the code runs and there are", "\n", "# no issues. Since this is usually done locally, we bypass this by", "\n", "# re-using the table loaded for the first option.", "\n", "      ", "option_policy_table", "[", "option_id", "]", "=", "(", "\n", "option_policy_table", "[", "option_utils", ".", "Options", "(", "1", ")", "]", ")", "\n", "logging", ".", "log_every_n_seconds", "(", "logging", ".", "WARNING", ",", "'Debugging is on'", ",", "10", ")", "\n", "continue", "\n", "", "else", ":", "\n", "      ", "option_policy_table", "[", "option_id", "]", "=", "_np_load", "(", "load_path", ")", "\n", "logging", ".", "info", "(", "\n", "'Successfully loaded option: %s from %s'", ",", "option_id", ",", "load_path", ")", "\n", "", "", "return", "option_policy_table", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._make_option_model_table": [[90, 139], ["absl.logging.info", "len", "numpy.zeros", "numpy.zeros", "numpy.zeros", "list", "tensorflow.expand_dims", "tensorflow.expand_dims", "model_network", "hasattr", "o_length.numpy().squeeze().round", "o_reward.numpy().squeeze().round", "absl.logging.info", "zip", "tensorflow.constant", "tensorflow.constant", "transition_probs.probs_parameter().numpy", "tensorflow.math.softmax().numpy", "o_length.numpy().squeeze", "o_reward.numpy().squeeze", "list", "transition_probs.probs_parameter", "tensorflow.math.softmax", "itertools.product", "o_length.numpy", "o_reward.numpy", "range", "range"], "function", ["None"], ["", "def", "_make_option_model_table", "(", "\n", "model_network", ":", "tf", ".", "keras", ".", "Model", ")", "->", "Dict", "[", "str", ",", "np", ".", "ndarray", "]", ":", "\n", "  ", "\"\"\"Creates option model table to be used in value iteration.\n\n  Args:\n    model_network: A neural network that acts as a model that accepts states and\n      options as tf.Tensors   and returns a distribution over transitions\n      (as a tensorflow_probability distribution), option lengths and option\n      rewards.\n\n  Returns:\n    A dictionary with the following entries:\n      - transitions: The |S| x |A| x |S| option transition table.\n      - rewards: The |S| x |O| table that specifies how much reward is obtained\n          for state-option pair.\n      - lengths: The |S| x |O| table that specifies the length of execution of\n          each state-option pair..\n  \"\"\"", "\n", "# We create a tabular option model here for every state and option pair in", "\n", "# the environment. This can then be plugged into value iteration or other", "\n", "# planning algorithm to obtain a policy over options.", "\n", "logging", ".", "info", "(", "'Creating option model table.'", ")", "\n", "num_states", "=", "env_utils", ".", "NUM_STATES", "\n", "num_options", "=", "len", "(", "option_utils", ".", "Options", ")", "\n", "option_transition_table", "=", "np", ".", "zeros", "(", "\n", "(", "num_states", ",", "num_options", ",", "num_states", ")", ",", "dtype", "=", "np", ".", "float", ")", "\n", "option_reward_table", "=", "np", ".", "zeros", "(", "\n", "(", "num_states", ",", "num_options", ")", ",", "dtype", "=", "np", ".", "float", ")", "\n", "option_length_table", "=", "np", ".", "zeros", "(", "\n", "(", "num_states", ",", "num_options", ")", ",", "dtype", "=", "np", ".", "float", ")", "\n", "\n", "# Get s_t, o_t pairs for every entry in the above matrices.", "\n", "s_t", ",", "o_t", "=", "list", "(", "zip", "(", "*", "list", "(", "\n", "itertools", ".", "product", "(", "range", "(", "num_states", ")", ",", "range", "(", "num_options", ")", ")", ")", ")", ")", "\n", "s_t_tf", "=", "tf", ".", "expand_dims", "(", "tf", ".", "constant", "(", "s_t", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "-", "1", ")", "\n", "o_t_tf", "=", "tf", ".", "expand_dims", "(", "tf", ".", "constant", "(", "o_t", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "-", "1", ")", "\n", "transition_probs", ",", "o_length", ",", "o_reward", "=", "model_network", "(", "s_t_tf", ",", "o_t_tf", ")", "\n", "if", "hasattr", "(", "transition_probs", ",", "'probs_parameter'", ")", ":", "\n", "    ", "s_tp1", "=", "transition_probs", ".", "probs_parameter", "(", ")", ".", "numpy", "(", ")", "\n", "", "else", ":", "\n", "    ", "s_tp1", "=", "tf", ".", "math", ".", "softmax", "(", "transition_probs", ".", "logits", ")", ".", "numpy", "(", ")", "\n", "", "option_transition_table", "[", "s_t", ",", "o_t", ",", ":", "]", "=", "s_tp1", "\n", "option_length_table", "[", "s_t", ",", "o_t", "]", "=", "o_length", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", ".", "round", "(", "1", ")", "\n", "option_reward_table", "[", "s_t", ",", "o_t", "]", "=", "o_reward", ".", "numpy", "(", ")", ".", "squeeze", "(", ")", ".", "round", "(", "1", ")", "\n", "logging", ".", "info", "(", "'Option model table created.'", ")", "\n", "return", "{", "\n", "'transitions'", ":", "option_transition_table", ",", "\n", "'rewards'", ":", "option_reward_table", ",", "\n", "'lengths'", ":", "option_length_table", "\n", "}", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._make_affordance_table": [[142, 186], ["absl.logging.info", "len", "len", "list", "tensorflow.expand_dims", "tensorflow.expand_dims", "affordance_network().numpy", "np.logical_or().astype.reshape", "numpy.max", "numpy.argmax", "numpy.logical_or().astype", "absl.logging.info", "zip", "tensorflow.constant", "tensorflow.constant", "numpy.eye", "affordance_network", "numpy.logical_or", "list", "itertools.product", "range", "range"], "function", ["None"], ["", "def", "_make_affordance_table", "(", "\n", "affordance_network", ":", "tf", ".", "keras", ".", "Model", ",", "\n", "affordance_mask_threshold", ":", "float", ",", "\n", ")", "->", "np", ".", "ndarray", ":", "\n", "  ", "\"\"\"Creates an affordance to be used in value iteration.\n\n  Args:\n    affordance_network: A neural network that takes in a tf.Tensor of states\n      and options and returns a tf.Tensor of probabilities for every\n      intent if it is affordable. Please refer to the inline comments for more\n      details or the paper.\n    affordance_mask_threshold: The threshold at which an affordance value\n      between 0-1 is converted into a binary value representing the affordance.\n\n  Returns:\n    A table of shape |S| x |O| x |I| indicating for each state-option pair\n      which intents are affordable.\n  \"\"\"", "\n", "logging", ".", "info", "(", "'Creating affordance table.'", ")", "\n", "num_states", "=", "env_utils", ".", "NUM_STATES", "\n", "num_options", "=", "len", "(", "option_utils", ".", "Options", ")", "\n", "num_intents", "=", "len", "(", "definitions", ".", "Intents", ")", "\n", "# Get s_t, o_t pairs for every entry in the affordance matrix.", "\n", "s_t", ",", "o_t", "=", "list", "(", "zip", "(", "*", "list", "(", "\n", "itertools", ".", "product", "(", "range", "(", "num_states", ")", ",", "range", "(", "num_options", ")", ")", ")", ")", ")", "\n", "s_t_tf", "=", "tf", ".", "expand_dims", "(", "tf", ".", "constant", "(", "s_t", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "-", "1", ")", "\n", "o_t_tf", "=", "tf", ".", "expand_dims", "(", "tf", ".", "constant", "(", "o_t", ",", "dtype", "=", "tf", ".", "int32", ")", ",", "-", "1", ")", "\n", "affs", "=", "affordance_network", "(", "s_t_tf", ",", "o_t_tf", ")", ".", "numpy", "(", ")", "# (|S|x|O|, |I|)", "\n", "affs", "=", "affs", ".", "reshape", "(", "(", "num_states", ",", "num_options", ",", "num_intents", ")", ")", "# (|S|, |O|, |I|)", "\n", "affs_maxed", "=", "np", ".", "max", "(", "affs", ",", "2", ")", "# (|S|, |O|)", "\n", "\n", "# All options that are above the threshold should be affordable.", "\n", "affs_masked", "=", "affs_maxed", ">", "affordance_mask_threshold", "# (|S|, |O|)", "\n", "\n", "# If there are any states with no options affordable above the threshold, we", "\n", "# make sure that the most affordable option is available. This ensures that", "\n", "# for every (state, option) pair, at least one option is affordable.", "\n", "affs_maxed_idx", "=", "np", ".", "argmax", "(", "affs_maxed", ",", "1", ")", "# (|S|, )", "\n", "affs_maxed", "=", "np", ".", "eye", "(", "num_options", ")", "[", "affs_maxed_idx", "]", "# (|S|, |O|)", "\n", "\n", "# We take the OR to combine the two.", "\n", "affs", "=", "np", ".", "logical_or", "(", "affs_maxed", ",", "affs_masked", ")", ".", "astype", "(", "np", ".", "float32", ")", "# (|S|, |O|)", "\n", "logging", ".", "info", "(", "'Affordance table created.'", ")", "\n", "return", "affs", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._save_hrl_tables": [[188, 220], ["os.path.join", "os.path.exists", "os.makedirs", "absl.logging.info", "custom_nodes._np_save", "absl.logging.info", "absl.logging.info", "custom_nodes._np_save", "absl.logging.info", "os.path.exists", "custom_nodes._np_save", "shutil.rmtree"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_save", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_save", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_save"], ["", "def", "_save_hrl_tables", "(", "\n", "total_steps", ":", "int", ",", "\n", "policy_over_options_table", ":", "np", ".", "ndarray", ",", "\n", "option_policy_table", ":", "Dict", "[", "int", ",", "np", ".", "ndarray", "]", ",", "\n", "save_dir", ":", "str", ",", "\n", "affordances_name", ":", "str", ",", "\n", "affordances_table", ":", "np", ".", "ndarray", ",", "\n", ")", ":", "\n", "  ", "\"\"\"Saves tables for HRL evaluation.\"\"\"", "\n", "save_path", "=", "f'{save_dir}/option_policy_table.npz'", "\n", "if", "not", "os", ".", "path", ".", "exists", "(", "save_path", ")", ":", "\n", "    ", "_np_save", "(", "save_path", ",", "option_policy_table", ")", "\n", "\n", "", "save_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "save_dir", ",", "\n", "f'hrl__affordances_{affordances_name}__numsteps_{total_steps}'", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "\n", "# Overrwrite the previously saved checkpoint.", "\n", "    ", "shutil", ".", "rmtree", "(", "save_dir", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "\n", "save_path", "=", "f'{save_dir}/option_model_table.npz'", "\n", "logging", ".", "info", "(", "'Saving options to %s.'", ",", "save_path", ")", "\n", "_np_save", "(", "save_path", ",", "policy_over_options_table", ")", "\n", "logging", ".", "info", "(", "'Successfully saved option model.'", ")", "\n", "\n", "save_path", "=", "f'{save_dir}/affordances_mask_table.npz'", "\n", "logging", ".", "info", "(", "'Saving affordances to %s.'", ",", "save_path", ")", "\n", "_np_save", "(", "save_path", ",", "affordances_table", ")", "\n", "logging", ".", "info", "(", "'Successfully saved affordances.'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._save_models_and_weights": [[222, 257], ["os.path.join", "os.path.exists", "os.makedirs", "custom_nodes._make_option_model_table", "absl.logging.info", "custom_nodes._np_save", "absl.logging.info", "absl.logging.info", "custom_nodes._np_save", "absl.logging.info", "shutil.rmtree", "model_network.get_weights", "absl.logging.info", "custom_nodes._np_save", "absl.logging.info", "affordance_network.get_weights"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._make_option_model_table", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_save", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_save", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._np_save"], ["", "def", "_save_models_and_weights", "(", "\n", "num_trajectories", ":", "int", ",", "\n", "num_steps", ":", "int", ",", "\n", "model_network", ":", "tf", ".", "keras", ".", "Model", ",", "\n", "save_dir", ":", "str", ",", "\n", "affordance_network", ":", "Optional", "[", "tf", ".", "keras", ".", "Model", "]", "=", "None", ",", "\n", ")", ":", "\n", "  ", "\"\"\"Saves models and weights to disk.\"\"\"", "\n", "\n", "save_dir", "=", "os", ".", "path", ".", "join", "(", "\n", "save_dir", ",", "\n", "f'model_numtrajectories_{num_trajectories}__numsteps_{num_steps}'", ")", "\n", "\n", "if", "os", ".", "path", ".", "exists", "(", "save_dir", ")", ":", "\n", "# Overrwrite the previously saved checkpoint.", "\n", "    ", "shutil", ".", "rmtree", "(", "save_dir", ")", "\n", "\n", "", "os", ".", "makedirs", "(", "save_dir", ")", "\n", "\n", "option_model_table", "=", "_make_option_model_table", "(", "model_network", ")", "\n", "save_path", "=", "f'{save_dir}/option_model_table.npz'", "\n", "logging", ".", "info", "(", "'Creating and saving option model to %s.'", ",", "save_path", ")", "\n", "_np_save", "(", "save_path", ",", "option_model_table", ")", "\n", "logging", ".", "info", "(", "'Successfully saved option model.'", ")", "\n", "\n", "save_path", "=", "f'{save_dir}/option_model_weights.npz'", "\n", "logging", ".", "info", "(", "'Saving weights to %s.'", ",", "save_path", ")", "\n", "_np_save", "(", "save_path", ",", "model_network", ".", "get_weights", "(", ")", ")", "\n", "logging", ".", "info", "(", "'Successfully saved weights'", ")", "\n", "\n", "if", "affordance_network", "is", "not", "None", ":", "\n", "    ", "save_path", "=", "f'{save_dir}/affordances_weights.npz'", "\n", "logging", ".", "info", "(", "'Saving weights to %s.'", ",", "save_path", ")", "\n", "_np_save", "(", "save_path", ",", "affordance_network", ".", "get_weights", "(", ")", ")", "\n", "logging", ".", "info", "(", "'Successfully saved weights'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes._get_affordances_function": [[259, 284], ["affordances_option_models.affordances.get_heuristic_affordances_by_name", "functools.lru_cache", "trainer_node.get_affordance_table"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.affordances.get_heuristic_affordances_by_name", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.custom_nodes.Trainer.get_affordance_table"], ["", "", "def", "_get_affordances_function", "(", "\n", "affordances_name", ":", "str", ",", "\n", "trainer_node", ":", "Optional", "[", "'Trainer'", "]", ",", "\n", ")", "->", "affordances", ".", "AffordancesFn", ":", "\n", "  ", "\"\"\"Wraps heuristic and learned affordance setup to make them interoperable.\n\n  Args:\n    affordances_name: The name of the affordances to load. Supports\n      `everything`, `only_pickup_drop`, `only_relevant_pickup_drop`, `learned`.\n    trainer_node: If `affordances_name == \"learned\"` then a trainer_node is\n      queried to obtain the latest affordance table.\n\n  Returns:\n    An affordance function that when called returns the relevant affordance\n      table of shape |S| x |O| indicating which options are available in\n      every state.\n  \"\"\"", "\n", "if", "affordances_name", "==", "'learned'", "and", "trainer_node", "is", "not", "None", ":", "\n", "    ", "def", "aff_fn", "(", ")", ":", "\n", "      ", "return", "trainer_node", ".", "get_affordance_table", "(", ")", "[", "'affordances'", "]", "\n", "", "", "else", ":", "\n", "    ", "aff_fn", "=", "affordances", ".", "get_heuristic_affordances_by_name", "(", "affordances_name", ")", "\n", "aff_fn", "=", "functools", ".", "lru_cache", "(", ")", "(", "aff_fn", ")", "\n", "\n", "", "return", "aff_fn", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.data.get_trajectories": [[43, 159], ["numpy.random.default_rng", "range", "numpy.isclose", "np.random.default_rng.choice", "float", "affordances_option_models.option_utils.Options", "absl.logging.debug", "affordances_option_models.rl.run_policy_in_env", "total_steps.append", "sum", "len", "absl.logging.debug", "data.append", "sum", "np.isclose.max", "numpy.flatnonzero", "np.random.default_rng.integers", "absl.logging.debug", "np.random.default_rng.integers", "np.random.default_rng.choice", "data.get_trajectories.rargmax"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.run_policy_in_env"], ["", "def", "get_trajectories", "(", "\n", "option_policies", ":", "Dict", "[", "option_utils", ".", "Options", ",", "np", ".", "ndarray", "]", ",", "\n", "num_trajectories", ":", "int", "=", "1", ",", "\n", "max_trajectory_length", ":", "int", "=", "12", ",", "\n", "affordances_mask", ":", "Optional", "[", "np", ".", "ndarray", "]", "=", "None", ",", "\n", "uniform_random_initial_state", ":", "bool", "=", "False", ",", "\n", "initial_state", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ")", "->", "Tuple", "[", "List", "[", "OptionTransition", "]", ",", "int", "]", ":", "\n", "  ", "\"\"\"Samples trajectory transitions by executing options in an environment.\n\n  Options are sampled uniformly from the `option_policies` table. They are then\n  rolled out in the environment for `max_trajectory_length` steps. Statistics\n  are computed about the Option execution and returned for model learning. The\n  initial state can be sampled randomly.\n\n  Args:\n    option_policies: A dictionary mapping option_id to a numpy table\n      representing the optimal low level policy that maximizes that option.\n    num_trajectories: The total number of trajectories to sample.\n    max_trajectory_length: The maximum length of the trajectory.\n    affordances_mask: Mask for sampling over the affordances.\n    uniform_random_initial_state: Each episode can start uniformly randomly in\n      the environment (we do not use the internal initial state distribution,\n      via reset() to sample starting states).\n    initial_state: Initial state for the rollouts.\n    seed: seed for randomness\n  Returns:\n    1. Trajectories collected from the environment when executing an option from\n       a state. They are stored as `OptionTransition` which contains metadata\n       needed to learn a model.\n    2. An integer representing the total steps taken in the environment.\n  \"\"\"", "\n", "rng", "=", "np", ".", "random", ".", "default_rng", "(", "seed", ")", "\n", "def", "rargmax", "(", "arr", ")", ":", "\n", "    ", "\"\"\"Random argmax with stochastic tie-breaking.\"\"\"", "\n", "arr", "=", "np", ".", "isclose", "(", "arr", ",", "arr", ".", "max", "(", "-", "1", ",", "keepdims", "=", "True", ")", ")", "\n", "return", "rng", ".", "choice", "(", "np", ".", "flatnonzero", "(", "arr", ")", ")", "\n", "", "max_trajectory_length", "=", "max_trajectory_length", "or", "float", "(", "'inf'", ")", "\n", "data", "=", "[", "]", "\n", "total_steps", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "num_trajectories", ")", ":", "\n", "    ", "if", "uniform_random_initial_state", ":", "\n", "      ", "initial_state", "=", "rng", ".", "integers", "(", "0", ",", "env_utils", ".", "NUM_STATES", ")", "\n", "logging", ".", "debug", "(", "'Initial state set to %s'", ",", "initial_state", ")", "\n", "", "elif", "initial_state", "is", "None", ":", "\n", "      ", "raise", "ValueError", "(", "\n", "'Initial state cannot be None if uniform_random_initial_state=False'", ")", "\n", "\n", "# Pick an option according to the relevant distribution.", "\n", "", "if", "affordances_mask", "is", "None", ":", "\n", "# Select a random option.", "\n", "      ", "option_id", "=", "rng", ".", "integers", "(", "1", ",", "len", "(", "option_utils", ".", "Options", ")", ")", "\n", "", "else", ":", "\n", "      ", "possible_options", "=", "np", ".", "where", "(", "affordances_mask", "[", "initial_state", "]", ">", "0", ")", "[", "0", "]", "\n", "option_id", "=", "rng", ".", "choice", "(", "possible_options", ")", "\n", "\n", "# +1 since Options enumeration starts at 1 instead of 0.", "\n", "", "option_id", "=", "option_utils", ".", "Options", "(", "option_id", "+", "1", ")", "\n", "logging", ".", "debug", "(", "'Selected option: %s'", ",", "option_id", ")", "\n", "\n", "def", "option_policy", "(", "x", ")", ":", "\n", "      ", "\"\"\"Executes the relevant low level option policy.\"\"\"", "\n", "q_values", "=", "option_policies", "[", "option_id", "]", "[", "x", "]", "# pylint: disable=cell-var-from-loop", "\n", "return", "rargmax", "(", "q_values", ")", "\n", "\n", "", "def", "termination_fn", "(", "transition", ":", "rl", ".", "Transition", ")", ":", "\n", "      ", "\"\"\"Determines if any given transition terminates the option.\"\"\"", "\n", "return", "transition", ".", "done", "or", "option_utils", ".", "check_option_termination", "(", "\n", "transition", ".", "s_t", ",", "transition", ".", "a_t", ",", "option_id", ")", "# pylint: disable=cell-var-from-loop", "\n", "\n", "# Do a rollout with the selected option.", "\n", "", "trajectories", ",", "steps_per_trajectory", ",", "rewards", "=", "rl", ".", "run_policy_in_env", "(", "\n", "option_policy", ",", "\n", "num_episodes", "=", "1", ",", "\n", "initial_state", "=", "initial_state", ",", "\n", "max_steps_per_episode", "=", "max_trajectory_length", ",", "\n", "termination_fn", "=", "termination_fn", ",", "\n", "seed", "=", "seed", "+", "i", "if", "seed", "is", "not", "None", "else", "None", ",", "\n", ")", "\n", "\n", "assert", "len", "(", "trajectories", ")", "==", "1", "\n", "total_steps", ".", "append", "(", "steps_per_trajectory", "[", "0", "]", ")", "\n", "trajectory", "=", "trajectories", "[", "0", "]", "\n", "\n", "first_transition", "=", "trajectory", "[", "0", "]", "\n", "final_transition", "=", "trajectory", "[", "-", "1", "]", "\n", "\n", "# Collect indications for every intent whether it was completed.", "\n", "all_intents", "=", "[", "]", "\n", "for", "intent_id", "in", "intent_utils", ".", "Intents", ":", "\n", "      ", "intent_completed", "=", "intent_utils", ".", "is_intent_completed", "(", "\n", "first_transition", ".", "s_t", ",", "\n", "option_id", ",", "\n", "final_transition", ".", "s_tp1", ",", "\n", "intent_id", "=", "intent_id", ")", "\n", "all_intents", ".", "append", "(", "intent_completed", ")", "\n", "\n", "# Since we get -1 reward per step, this sum doesn't need to be discounted.", "\n", "", "option_reward", "=", "sum", "(", "rewards", ")", "\n", "option_length", "=", "len", "(", "trajectory", ")", "\n", "logging", ".", "debug", "(", "\n", "'Option Rollout: option_id=%s, Option length = %s, option reward = %s'", ",", "\n", "option_id", ",", "option_length", ",", "option_reward", ")", "\n", "\n", "data", ".", "append", "(", "\n", "OptionTransition", "(", "\n", "first_transition", ".", "s_t", ",", "\n", "option_id", ".", "value", "-", "1", ",", "# Option labels start from 1. We reindex to 0.", "\n", "option_length", ",", "\n", "option_reward", ",", "\n", "final_transition", ".", "s_tp1", ",", "\n", "IntentCompletionIndicator", "(", "tuple", "(", "all_intents", ")", ")", ",", "\n", ")", ")", "\n", "\n", "", "return", "data", ",", "sum", "(", "total_steps", ")", "\n", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.check_option_termination": [[34, 94], ["affordances_option_models.env_utils.int_to_state_fn", "affordances_option_models.env_utils.int_to_state_fn", "int", "affordances_option_models.env_utils.grid_cell_to_xy", "ValueError", "option.name.replace().split", "Options.__members__.values", "option.name.replace"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.int_to_state_fn", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.int_to_state_fn", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.grid_cell_to_xy"], ["def", "check_option_termination", "(", "\n", "s_t", ":", "int", ",", "\n", "a_t", ":", "int", ",", "\n", "option", ":", "Options", ")", "->", "bool", ":", "\n", "  ", "\"\"\"Given an (s, a) transition, determines if an option_id terminates in P.\n\n  Args:\n    s_t: The state at time t.\n    a_t: The action at time t.\n    option: The option you want to check completion for.\n\n  The following termination conditions apply:\n\n  GoToXX_Drop:\n    - Action must be DROP.\n    - Grid cell of s_tp1 must match the grid cell XX.\n    - Passenger must be inside the taxi.\n\n  GoToXX_Pickup:\n    - Action must be PICKUP.\n    - Grid cell of s_tp1 must match the grid cell XX.\n    - Passenger must be outside the taxi (doesn't matter where exactly).\n\n  GoToXX_Any:\n    - Grid cell of s_tp1 must match the grid cell XX.\n\n  Returns:\n    boolean indicating if the option terminates in this transition.\n  \"\"\"", "\n", "if", "option", "not", "in", "Options", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "f'Unknown Option {option}. Valid: {Options.__members__.values()}'", ")", "\n", "", "_", ",", "s_tp1", ",", "_", "=", "_TRANSITION_DICT", "[", "s_t", "]", "[", "a_t", "]", "[", "0", "]", "\n", "_", ",", "_", ",", "passenger_state", ",", "_", "=", "env_utils", ".", "int_to_state_fn", "(", "s_t", ")", "\n", "taxi_row", ",", "taxi_col", ",", "_", ",", "_", "=", "env_utils", ".", "int_to_state_fn", "(", "s_tp1", ")", "\n", "\n", "if", "option", "in", "OptionsDropping", ":", "\n", "# Option is supposed to drop off a passenger so action must be dropping.", "\n", "    ", "if", "a_t", "!=", "definitions", ".", "ActionMap", ".", "DROP", ":", "\n", "      ", "return", "False", "\n", "# If passenger was not in the car, this option cannot terminate.", "\n", "", "if", "passenger_state", "!=", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ":", "\n", "      ", "return", "False", "\n", "\n", "", "", "if", "option", "in", "OptionsPicking", ":", "\n", "# Option is supposed to pick up a passenger so action must be picking.", "\n", "    ", "if", "a_t", "!=", "definitions", ".", "ActionMap", ".", "PICKUP", ":", "\n", "      ", "return", "False", "\n", "# If the passenger is in the car, then picking up is not possible.", "\n", "", "if", "passenger_state", "==", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ":", "\n", "      ", "return", "False", "\n", "\n", "# Now check if the option \"go to\" location matches the current taxi position.", "\n", "# Options are named \"GoToXX_??\" where XX us the grid index.", "\n", "", "", "grid_idx", "=", "int", "(", "option", ".", "name", ".", "replace", "(", "'GoTo'", ",", "''", ")", ".", "split", "(", "'_'", ")", "[", "0", "]", ")", "\n", "grid_row", ",", "grid_col", "=", "env_utils", ".", "grid_cell_to_xy", "(", "grid_idx", ")", "\n", "if", "(", "taxi_row", ",", "taxi_col", ")", "==", "(", "grid_row", ",", "grid_col", ")", ":", "\n", "    ", "return", "True", "\n", "", "else", ":", "\n", "    ", "return", "False", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.compute_per_step_matrices_for_option_learning": [[96, 125], ["affordances_option_models.env_utils.make_taxi_environment", "numpy.full", "numpy.ones", "range", "range", "option_utils.check_option_termination"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.make_taxi_environment", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.check_option_termination"], ["", "", "def", "compute_per_step_matrices_for_option_learning", "(", "\n", "option", ":", "Options", ",", "\n", "r_option_completion", ":", "float", "=", "1.0", ",", "\n", "r_other", ":", "float", "=", "0.0", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "  ", "\"\"\"Computes per-step matrices needed to learn option polices.\n\n  Args:\n    option: The option for which you want to compute the matrices for.\n    r_option_completion: The reward for successful completion of the option.\n    r_other: The reward for all other steps of the option.\n\n  Returns:\n    1. A matrix containing the per-step rewards for the requested option.\n    2. A matrix containing the termination mask for the transition matrix. e.g.\n       if the entry (s, a) has a 1, transitions can take place. If it has a zero\n       it terminates.\n  \"\"\"", "\n", "taxienv", "=", "env_utils", ".", "make_taxi_environment", "(", ")", "\n", "num_states", ",", "num_actions", "=", "taxienv", ".", "nS", ",", "taxienv", ".", "nA", "\n", "option_step_reward", "=", "np", ".", "full", "(", "(", "num_states", ",", "num_actions", ")", ",", "r_other", ")", "\n", "option_transition_mask", "=", "np", ".", "ones", "(", "(", "num_states", ",", "num_actions", ")", ",", "dtype", "=", "np", ".", "float", ")", "\n", "for", "s", "in", "range", "(", "num_states", ")", ":", "\n", "    ", "for", "a", "in", "range", "(", "num_actions", ")", ":", "\n", "      ", "if", "check_option_termination", "(", "s", ",", "a", ",", "option", ")", ":", "\n", "        ", "option_step_reward", "[", "s", ",", "a", "]", "=", "r_option_completion", "\n", "option_transition_mask", "[", "s", ",", "a", "]", "=", "0.0", "# No possible transitions from here.", "\n", "\n", "", "", "", "return", "option_step_reward", ",", "option_transition_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.learn_option_policy": [[127, 164], ["option_utils.compute_per_step_matrices_for_option_learning", "numpy.expand_dims", "affordances_option_models.rl.value_iteration", "affordances_option_models.rl.extract_greedy_policy"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.compute_per_step_matrices_for_option_learning", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.value_iteration", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.extract_greedy_policy"], ["", "def", "learn_option_policy", "(", "\n", "option", ":", "Options", ",", "\n", "gamma", ":", "float", "=", "rl", ".", "DEFAULT_GAMMA", ",", "\n", "stopping_threshold", ":", "float", "=", "0.0001", ",", "\n", "max_iterations", ":", "int", "=", "10000", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "int", "]", ":", "\n", "  ", "\"\"\"Learns the low level policy for an option.\n\n  Args:\n    option: The option for which to learn the policy.\n    gamma: Discount factor in VI.\n    stopping_threshold: Stop if the change in value is less than this value.\n    max_iterations: Maximum number of iterations to run VI.\n    seed: For tie-breaking.\n\n  Returns:\n    pi_star: Low level policy, |S| x |A| that achieves the desired option.\n  \"\"\"", "\n", "r_option", ",", "b_option", "=", "compute_per_step_matrices_for_option_learning", "(", "option", ")", "\n", "# The transition matrix must be masked to take into account when an option", "\n", "# will terminate. For example, if the option termination condition is to go to", "\n", "# state X, then it must not be able to go to any other state after. The", "\n", "# b_option matrix contains this information and we expand dimensions to", "\n", "# automatically broadcast and mask the usual transition matrix.", "\n", "b_option", "=", "np", ".", "expand_dims", "(", "b_option", ",", "-", "1", ")", "\n", "masked_transition_matrix", "=", "b_option", "*", "_TRANSITION_MATRIX", "\n", "V_star", ",", "_", ",", "num_iters", "=", "rl", ".", "value_iteration", "(", "# pylint: disable=invalid-name", "\n", "reward_matrix", "=", "r_option", ",", "\n", "transition_matrix", "=", "masked_transition_matrix", ",", "\n", "max_iterations", "=", "max_iterations", ",", "\n", "stopping_threshold", "=", "stopping_threshold", ",", "\n", "gamma", "=", "gamma", ")", "\n", "pi_star", "=", "rl", ".", "extract_greedy_policy", "(", "\n", "r_option", ",", "masked_transition_matrix", ",", "V_star", ",", "gamma", "=", "gamma", ",", "seed", "=", "seed", ")", "\n", "\n", "return", "pi_star", ",", "num_iters", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.learn_policy_over_options": [[166, 235], ["numpy.any", "affordances_option_models.rl.value_iteration", "affordances_option_models.rl.extract_greedy_policy", "np.clip.min", "absl.logging.error", "numpy.clip", "ValueError", "ValueError", "ValueError", "ValueError", "option_transition.sum().round", "numpy.all", "numpy.all", "numpy.where", "option_transition.sum"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.value_iteration", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.rl.extract_greedy_policy"], ["", "def", "learn_policy_over_options", "(", "\n", "option_reward", ":", "np", ".", "ndarray", ",", "\n", "option_transition", ":", "np", ".", "ndarray", ",", "\n", "option_length", ":", "np", ".", "ndarray", ",", "\n", "gamma", ":", "float", "=", "rl", ".", "DEFAULT_GAMMA", ",", "\n", "stopping_threshold", ":", "float", "=", "0.0001", ",", "\n", "max_iterations", ":", "int", "=", "10000", ",", "\n", "seed", ":", "Optional", "[", "int", "]", "=", "None", ",", "\n", "affordances_fn", ":", "Optional", "[", "affordances", ".", "AffordancesFn", "]", "=", "None", ",", "\n", "writer", "=", "None", ",", "\n", ")", "->", "Tuple", "[", "np", ".", "ndarray", ",", "int", "]", ":", "\n", "  ", "\"\"\"Learns the policy over option policies.\n\n  Args:\n    option_reward: Reward matrix of shape |S| x |O| that determines the\n      environment reward for every state option pair.\n    option_transition: Transition matrix of shape |S| x |O| x |S| that\n      determines the transition state after executing an option in a state.\n    option_length: Length matrix of shape |S| x |O| that determines the\n      Length of execution for every state option pair.\n    gamma: Discount factor in VI.\n    stopping_threshold: Stop if the change in value is less than this value.\n    max_iterations: Maximum number of iterations to run VI.\n    seed: For tie-breaking.\n    affordances_fn: Affordances and relevant masking for the bellman update.\n    writer: An optional writer to save data.\n\n  Returns:\n    pi_star: Policy over options, |S| x |O|.\n  \"\"\"", "\n", "if", "option_length", ".", "min", "(", ")", "<", "1", ":", "\n", "    ", "logging", ".", "error", "(", "\n", "(", "'At least one option has a length < 1 at %s (values=%s). Clipping has '", "\n", "'occurred.'", ")", ",", "\n", "np", ".", "where", "(", "option_length", "<", "1", ")", "[", "0", "]", ",", "\n", "option_length", "[", "option_length", "<", "1", "]", ")", "\n", "option_length", "=", "np", ".", "clip", "(", "option_length", ",", "1", ",", "100", ")", "\n", "", "if", "np", ".", "any", "(", "option_transition", ".", "sum", "(", "-", "1", ")", ".", "round", "(", "2", ")", ">", "1", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "'At least one probability distribution from a (state, option) pair '", "\n", "'had a sum > 1.'", ")", "\n", "", "if", "not", "(", "np", ".", "all", "(", "option_transition", "<=", "1", ")", "and", "np", ".", "all", "(", "option_transition", ">=", "0", ")", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "'At least one transitition probability is not between (0, 1).'", ")", "\n", "\n", "", "gamma", "=", "gamma", "**", "option_length", "\n", "num_states", ",", "num_options", "=", "option_reward", ".", "shape", "\n", "if", "option_transition", ".", "shape", "!=", "(", "num_states", ",", "num_options", ",", "num_states", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "f'Option transition matrix has shape {option_transition.shape}. '", "\n", "f'Expected {(num_states, num_options, num_states)}'", ")", "\n", "", "if", "gamma", ".", "shape", "!=", "(", "num_states", ",", "num_options", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "f'gamma matrix has shape {gamma.shape}. '", "\n", "f'Expected {(num_states, num_options)}'", ")", "\n", "\n", "", "V_star", ",", "_", ",", "num_iters", "=", "rl", ".", "value_iteration", "(", "# pylint: disable=invalid-name", "\n", "reward_matrix", "=", "option_reward", ",", "\n", "transition_matrix", "=", "option_transition", ",", "\n", "max_iterations", "=", "max_iterations", ",", "\n", "stopping_threshold", "=", "stopping_threshold", ",", "\n", "affordances_fn", "=", "affordances_fn", ",", "\n", "gamma", "=", "gamma", ",", "\n", "writer", "=", "writer", ")", "\n", "pi_star", "=", "rl", ".", "extract_greedy_policy", "(", "\n", "option_reward", ",", "option_transition", ",", "V_star", ",", "gamma", "=", "gamma", ",", "seed", "=", "seed", ",", "\n", "affordances_fn", "=", "affordances_fn", ")", "\n", "\n", "return", "pi_star", ",", "num_iters", "\n", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.intent_utils_test.IntentUtilsTest.test_is_intent_completed": [[29, 120], ["absl.testing.parameterized.named_parameters", "affordances_option_models.env_utils.state_to_int_fn", "intent_utils_test.IntentUtilsTest.assertEqual", "affordances_option_models.env_utils.TaxiState", "affordances_option_models.intent_utils.is_intent_completed"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.state_to_int_fn", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.intent_utils.is_intent_completed"], ["  ", "@", "parameterized", ".", "named_parameters", "(", "\n", "{", "\n", "'testcase_name'", ":", "'Matches passenger state and taxi location'", ",", "\n", "'intent_id'", ":", "Intents", ".", "R_in", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'status'", ":", "IntentStatus", ".", "complete", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Does not matches passenger state and taxi location'", ",", "\n", "'intent_id'", ":", "Intents", ".", "G_in", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'status'", ":", "IntentStatus", ".", "incomplete", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Matches taxi location but not pass state'", ",", "\n", "'intent_id'", ":", "Intents", ".", "R_out", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'status'", ":", "IntentStatus", ".", "incomplete", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Matches pass state but not location'", ",", "\n", "'intent_id'", ":", "Intents", ".", "R_in", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "1", ",", "\n", "'col'", ":", "0", ",", "\n", "'status'", ":", "IntentStatus", ".", "incomplete", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Matches pass state outside @ location 1.'", ",", "\n", "'intent_id'", ":", "Intents", ".", "R_out", ",", "\n", "'passenger_status'", ":", "0", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'status'", ":", "IntentStatus", ".", "complete", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Matches pass state outside @ location 2.'", ",", "\n", "'intent_id'", ":", "Intents", ".", "B_out", ",", "\n", "'passenger_status'", ":", "3", ",", "\n", "'row'", ":", "4", ",", "\n", "'col'", ":", "3", ",", "\n", "'status'", ":", "IntentStatus", ".", "complete", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Matches pass state outside but wrong location.'", ",", "\n", "'intent_id'", ":", "Intents", ".", "R_out", ",", "\n", "'passenger_status'", ":", "2", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'status'", ":", "IntentStatus", ".", "incomplete", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Does not match pass state outside @ location.'", ",", "\n", "'intent_id'", ":", "Intents", ".", "G_out", ",", "\n", "'passenger_status'", ":", "1", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'status'", ":", "IntentStatus", ".", "incomplete", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Random location + passenger inside, incomplete 1.'", ",", "\n", "'intent_id'", ":", "Intents", ".", "G_out", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "2", ",", "\n", "'col'", ":", "2", ",", "\n", "'status'", ":", "IntentStatus", ".", "incomplete", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Random location + passenger inside, incomplete 2.'", ",", "\n", "'intent_id'", ":", "Intents", ".", "G_in", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "2", ",", "\n", "'col'", ":", "2", ",", "\n", "'status'", ":", "IntentStatus", ".", "incomplete", ",", "\n", "}", ",", "\n", ")", "\n", "def", "test_is_intent_completed", "(", "\n", "self", ",", "row", ",", "col", ",", "passenger_status", ",", "intent_id", ",", "status", ")", ":", "\n", "\n", "    ", "taxi_state", "=", "env_utils", ".", "state_to_int_fn", "(", "\n", "env_utils", ".", "TaxiState", "(", "row", ",", "col", ",", "passenger_status", ",", "0", ")", ")", "\n", "\n", "self", ".", "assertEqual", "(", "\n", "intent_utils", ".", "is_intent_completed", "(", "None", ",", "None", ",", "taxi_state", ",", "intent_id", ")", ",", "\n", "status", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options.make_trainer_node": [[73, 113], ["absl.logging.info", "acme.utils.loggers.make_default_logger", "acme.utils.loggers.GatedFilter.periodic", "affordances_option_models.custom_nodes.Trainer", "len"], "function", ["None"], ["def", "make_trainer_node", "(", "\n", "model_learning_rate", ":", "float", ",", "\n", "stop_after_steps", ":", "int", ",", "\n", "hidden_dims", ":", "int", ",", "\n", "program_stopper", ",", "\n", "affordances_name", ":", "str", ",", "\n", "seed", ":", "int", ",", "\n", "save_every", ":", "int", ",", "\n", "save_path", ":", "str", ",", "\n", "affordances_threshold", ":", "float", "=", "0.5", ",", "\n", "topic_name", ":", "str", "=", "'default'", ",", "\n", ")", ":", "\n", "  ", "\"\"\"Creates a training node to learn the models.\"\"\"", "\n", "def", "trainer_node", "(", "queue", ")", ":", "\n", "\n", "    ", "logging", ".", "info", "(", "'Beginning training...'", ")", "\n", "\n", "log_writer", "=", "loggers", ".", "make_default_logger", "(", "\n", "'experiment'", ",", "time_delta", "=", "0", ",", "asynchronous", "=", "True", ")", "\n", "log_writer", "=", "loggers", ".", "GatedFilter", ".", "periodic", "(", "log_writer", ",", "10", ")", "\n", "\n", "trainer", "=", "custom_nodes", ".", "Trainer", "(", "\n", "num_states", "=", "env_utils", ".", "NUM_STATES", ",", "\n", "num_options", "=", "len", "(", "option_utils", ".", "Options", ")", ",", "\n", "hidden_dims", "=", "hidden_dims", ",", "\n", "stop_after_steps", "=", "stop_after_steps", ",", "\n", "model_learning_rate", "=", "model_learning_rate", ",", "\n", "affordances_name", "=", "affordances_name", ",", "\n", "affordances_threshold", "=", "affordances_threshold", ",", "\n", "use_learned_affordances", "=", "affordances_name", "==", "'learned'", ",", "\n", "topic_name", "=", "topic_name", ",", "\n", "queue", "=", "queue", ",", "\n", "save_path", "=", "save_path", ",", "\n", "save_every", "=", "save_every", ",", "\n", "seed", "=", "seed", ",", "\n", "program_stopper", "=", "program_stopper", ",", "\n", "writer", "=", "log_writer", ")", "\n", "return", "trainer", "\n", "\n", "", "return", "trainer_node", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options.make_evaluation_node": [[115, 144], ["FLAGS.lp_launch_type.startswith", "absl.logging.info", "acme.utils.loggers.make_default_logger", "acme.utils.loggers.GatedFilter.periodic", "affordances_option_models.custom_nodes.Evaluation"], "function", ["None"], ["", "def", "make_evaluation_node", "(", "\n", "path_to_options", ",", "\n", "gamma", ",", "\n", "max_iterations", ",", "\n", "affordances_name", ":", "str", ",", "\n", "save_path", ":", "str", ",", "\n", "save_every", ":", "int", ",", "\n", ")", ":", "\n", "  ", "\"\"\"Creates a training node to learn the models.\"\"\"", "\n", "num_eval_episodes", "=", "1", "if", "FLAGS", ".", "lp_launch_type", ".", "startswith", "(", "'test'", ")", "else", "1000", "\n", "def", "evaluation_node", "(", "trainer_node", ")", ":", "\n", "    ", "logging", ".", "info", "(", "'Beginning evaluation node...'", ")", "\n", "\n", "log_writer", "=", "loggers", ".", "make_default_logger", "(", "\n", "f'evaluation_{affordances_name}'", ",", "time_delta", "=", "0", ",", "asynchronous", "=", "True", ")", "\n", "log_writer", "=", "loggers", ".", "GatedFilter", ".", "periodic", "(", "log_writer", ",", "10", ")", "\n", "\n", "evaluation", "=", "custom_nodes", ".", "Evaluation", "(", "\n", "path_to_options", "=", "path_to_options", ",", "\n", "affordances_name", "=", "affordances_name", ",", "\n", "gamma", "=", "gamma", ",", "\n", "max_iterations", "=", "max_iterations", ",", "\n", "trainer_node", "=", "trainer_node", ",", "\n", "save_path", "=", "save_path", ",", "\n", "save_every", "=", "save_every", ",", "\n", "num_eval_episodes", "=", "num_eval_episodes", ",", "\n", "writer", "=", "log_writer", ")", "\n", "return", "evaluation", "\n", "", "return", "evaluation_node", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options._make_program": [[146, 239], ["launchpad.Program", "launchpad.make_program_stopper", "lp.Program.group", "affordances_option_models.task_queue.TaskQueueNode", "lp.Program.add_node", "task_queue.TaskQueueNode.register_handle", "lp.Program.group", "launchpad.CourierNode", "lp.Program.add_node", "lp.Program.group", "ALL_AFFORDANCE_TYPES.copy", "lp.Program.group", "range", "task_queue.TaskQueueNode.make_node", "lp_learn_model_from_options.make_trainer_node", "task_queue.TaskQueueNode.reader", "ALL_AFFORDANCE_TYPES.copy.remove", "launchpad.CourierNode", "lp.Program.add_node", "launchpad.CourierNode", "lp.Program.add_node", "lp_learn_model_from_options.make_evaluation_node", "task_queue.TaskQueueNode.writer"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.register_handle", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.make_node", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options.make_trainer_node", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.reader", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options.make_evaluation_node", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.writer"], ["", "def", "_make_program", "(", "model_learning_rate", ":", "float", ",", "\n", "stop_after_steps", ":", "int", ",", "\n", "batch_size", ":", "int", ",", "\n", "path_to_options", ":", "str", ",", "\n", "max_option_length", ":", "int", ",", "\n", "affordances_name", ":", "str", ",", "\n", "use_affordances_rollout_node", ":", "bool", ",", "\n", "hidden_dims", ":", "int", ",", "\n", "save_path", ":", "str", ",", "\n", "max_iterations_for_value_iter", ":", "int", ",", "\n", "seed", ":", "int", ",", "\n", "affordances_threshold", ":", "float", "=", "0.5", ",", "\n", "num_rollout_nodes", "=", "1", ")", ":", "\n", "  ", "\"\"\"Creates the launchpad program.\"\"\"", "\n", "program", "=", "lp", ".", "Program", "(", "'model_learning'", ")", "\n", "program_stopper", "=", "lp", ".", "make_program_stopper", "(", "FLAGS", ".", "lp_launch_type", ")", "\n", "topic_name", "=", "'default'", "\n", "\n", "##############################", "\n", "#       Task Queue           #", "\n", "##############################", "\n", "with", "program", ".", "group", "(", "'queue'", ")", ":", "\n", "    ", "queue", "=", "task_queue", ".", "TaskQueueNode", "(", ")", "\n", "queue_handle", "=", "program", ".", "add_node", "(", "queue", ".", "make_node", "(", ")", ")", "\n", "queue", ".", "register_handle", "(", "queue_handle", ")", "\n", "\n", "##############################", "\n", "#     Training node          #", "\n", "##############################", "\n", "", "with", "program", ".", "group", "(", "'trainer'", ")", ":", "\n", "    ", "trainer_node", "=", "lp", ".", "CourierNode", "(", "\n", "make_trainer_node", "(", "\n", "model_learning_rate", "=", "model_learning_rate", ",", "\n", "stop_after_steps", "=", "stop_after_steps", ",", "\n", "hidden_dims", "=", "hidden_dims", ",", "\n", "program_stopper", "=", "program_stopper", ",", "\n", "affordances_name", "=", "affordances_name", ",", "\n", "affordances_threshold", "=", "affordances_threshold", ",", "\n", "topic_name", "=", "topic_name", ",", "\n", "save_every", "=", "200000", ",", "\n", "save_path", "=", "save_path", ",", "\n", "seed", "=", "_GLOBAL_SEED", "*", "seed", ",", "\n", ")", ",", "\n", "queue", ".", "reader", "(", ")", ")", "\n", "trainer_node", "=", "program", ".", "add_node", "(", "trainer_node", ")", "\n", "\n", "##############################", "\n", "#     Evaluation node        #", "\n", "##############################", "\n", "", "with", "program", ".", "group", "(", "'evaluator'", ")", ":", "\n", "\n", "    ", "affordance_types", "=", "ALL_AFFORDANCE_TYPES", ".", "copy", "(", ")", "\n", "if", "affordances_name", "==", "'learned'", ":", "\n", "# If the affordances are learned online, do not use heuristic affordances.", "\n", "      ", "affordance_types", "=", "[", "'learned'", "]", "\n", "", "else", ":", "\n", "      ", "affordance_types", ".", "remove", "(", "'learned'", ")", "\n", "\n", "", "for", "evaluation_affordance_name", "in", "affordance_types", ":", "\n", "      ", "evaluation_node", "=", "lp", ".", "CourierNode", "(", "\n", "make_evaluation_node", "(", "\n", "path_to_options", "=", "path_to_options", ",", "\n", "gamma", "=", "0.99", ",", "\n", "max_iterations", "=", "max_iterations_for_value_iter", ",", "\n", "affordances_name", "=", "evaluation_affordance_name", ",", "\n", "save_path", "=", "save_path", ",", "\n", "save_every", "=", "200000", ",", "\n", ")", ",", "\n", "trainer_node", ")", "\n", "program", ".", "add_node", "(", "evaluation_node", ")", "\n", "\n", "##############################", "\n", "#     Problems Solver        #", "\n", "##############################", "\n", "", "", "if", "use_affordances_rollout_node", ":", "\n", "    ", "rollout_node_affordances", "=", "affordances_name", "\n", "", "else", ":", "\n", "    ", "rollout_node_affordances", "=", "'everything'", "\n", "\n", "", "with", "program", ".", "group", "(", "'rollouts'", ")", ":", "\n", "    ", "for", "i", "in", "range", "(", "num_rollout_nodes", ")", ":", "\n", "      ", "rollout_node", "=", "lp", ".", "CourierNode", "(", "\n", "custom_nodes", ".", "Rollout", ",", "\n", "global_seed", "=", "seed", "*", "_GLOBAL_SEED", "+", "i", ",", "\n", "batch_size", "=", "batch_size", ",", "\n", "path_to_options", "=", "path_to_options", ",", "\n", "affordances_name", "=", "rollout_node_affordances", ",", "\n", "max_option_length", "=", "max_option_length", ",", "\n", "queue_writer", "=", "queue", ".", "writer", "(", ")", ",", "\n", "trainer_node", "=", "trainer_node", ")", "\n", "program", ".", "add_node", "(", "rollout_node", ")", "\n", "\n", "", "", "return", "program", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options.get_config": [[241, 253], ["None"], "function", ["None"], ["", "def", "get_config", "(", ")", ":", "\n", "  ", "\"\"\"Reproduces results in the paper.\"\"\"", "\n", "base_config", "=", "{", "\n", "'batch_size'", ":", "100", ",", "\n", "'model_learning_rate'", ":", "1e-4", ",", "\n", "'max_option_length'", ":", "100", ",", "\n", "'hidden_dims'", ":", "0", ",", "\n", "'stop_after_steps'", ":", "FLAGS", ".", "total_steps", ",", "\n", "'use_affordances_rollout_node'", ":", "True", ",", "\n", "'max_iterations_for_value_iter'", ":", "1", ",", "\n", "}", "\n", "return", "base_config", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options.main": [[255, 276], ["lp_learn_model_from_options.get_config", "lp_learn_model_from_options._make_program", "launchpad.launch", "ValueError", "ValueError"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options.get_config", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.lp_learn_model_from_options._make_program"], ["", "def", "main", "(", "_", ")", ":", "\n", "\n", "  ", "if", "(", "FLAGS", ".", "affordances_name", "==", "'learned'", "and", "\n", "FLAGS", ".", "affordances_threshold", "is", "None", ")", ":", "\n", "    ", "raise", "ValueError", "(", "\n", "'When affordances are learned, an affordance threshold must be given.'", ")", "\n", "", "if", "FLAGS", ".", "affordances_threshold", "is", "not", "None", ":", "\n", "    ", "if", "not", "0", "<=", "FLAGS", ".", "affordances_threshold", "<=", "1", ":", "\n", "      ", "raise", "ValueError", "(", "'Affordance threshold must be between 0 and 1.'", ")", "\n", "\n", "", "", "program_config", "=", "get_config", "(", ")", "\n", "program", "=", "_make_program", "(", "\n", "path_to_options", "=", "FLAGS", ".", "path_to_options", ",", "\n", "num_rollout_nodes", "=", "FLAGS", ".", "num_rollout_nodes", ",", "\n", "affordances_name", "=", "FLAGS", ".", "affordances_name", ",", "\n", "affordances_threshold", "=", "FLAGS", ".", "affordances_threshold", ",", "\n", "save_path", "=", "FLAGS", ".", "save_path", ",", "\n", "seed", "=", "FLAGS", ".", "seed", ",", "\n", "**", "program_config", ")", "\n", "\n", "lp", ".", "launch", "(", "program", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.__init__": [[35, 37], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ")", ":", "\n", "    ", "self", ".", "_handle", "=", "None", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.make_node": [[38, 40], ["launchpad.CourierNode"], "methods", ["None"], ["", "def", "make_node", "(", "self", ")", ":", "\n", "    ", "return", "lp", ".", "CourierNode", "(", "_QueueImplementation", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.register_handle": [[41, 43], ["None"], "methods", ["None"], ["", "def", "register_handle", "(", "self", ",", "queue_handle", ")", ":", "\n", "    ", "self", ".", "_handle", "=", "queue_handle", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.reader": [[44, 46], ["None"], "methods", ["None"], ["", "def", "reader", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_handle", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue.TaskQueueNode.writer": [[47, 49], ["None"], "methods", ["None"], ["", "def", "writer", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_handle", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.__init__": [[54, 59], ["queue.Queue", "absl.logging.info", "int"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "**", "kwargs", ")", ":", "\n", "    ", "del", "kwargs", "# Unused.", "\n", "self", ".", "_queue", "=", "queue", ".", "Queue", "(", "maxsize", "=", "int", "(", "1e9", ")", ")", "\n", "self", ".", "_closed", "=", "False", "\n", "logging", ".", "info", "(", "'Queue created!'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.enqueue_task": [[60, 68], ["data.copy.copy.copy", "task_queue._QueueImplementation._queue.put_nowait", "absl.logging.log_every_n_seconds", "absl.logging.log_every_n_seconds", "task_queue._QueueImplementation._queue.qsize"], "methods", ["None"], ["", "def", "enqueue_task", "(", "self", ",", "task_key", ",", "data", ")", ":", "\n", "    ", "data", "=", "data", ".", "copy", "(", ")", "\n", "data", "[", "'task_key'", "]", "=", "task_key", "\n", "self", ".", "_queue", ".", "put_nowait", "(", "data", ")", "\n", "logging", ".", "log_every_n_seconds", "(", "\n", "logging", ".", "INFO", ",", "'Current queue size is %d'", ",", "\n", "60", ",", "self", ".", "_queue", ".", "qsize", "(", ")", ")", "\n", "logging", ".", "log_every_n_seconds", "(", "logging", ".", "INFO", ",", "'[PUT] Data = %s'", ",", "15", ",", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.get_task": [[69, 75], ["task_queue._QueueImplementation._queue.get", "data.pop", "absl.logging.log_every_n_seconds"], "methods", ["None"], ["", "def", "get_task", "(", "self", ",", "topic_name", ")", ":", "\n", "    ", "del", "topic_name", "\n", "data", ":", "Dict", "[", "str", ",", "Any", "]", "=", "self", ".", "_queue", ".", "get", "(", ")", "\n", "task_key", "=", "data", ".", "pop", "(", "'task_key'", ")", "\n", "logging", ".", "log_every_n_seconds", "(", "logging", ".", "INFO", ",", "'[GET] Data = %s'", ",", "15", ",", "data", ")", "\n", "return", "task_key", ",", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.set_result": [[76, 80], ["task_queue._QueueImplementation._queue.task_done", "absl.logging.log_every_n_seconds"], "methods", ["None"], ["", "def", "set_result", "(", "self", ",", "*", "args", ",", "**", "kwargs", ")", ":", "\n", "    ", "self", ".", "_queue", ".", "task_done", "(", ")", "\n", "logging", ".", "log_every_n_seconds", "(", "\n", "logging", ".", "INFO", ",", "'Task result was set %s %s'", ",", "600", ",", "args", ",", "kwargs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.closed": [[81, 83], ["None"], "methods", ["None"], ["", "def", "closed", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_closed", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.empty": [[84, 86], ["task_queue._QueueImplementation._queue.empty"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.empty"], ["", "def", "empty", "(", "self", ")", ":", "\n", "    ", "return", "self", ".", "_queue", ".", "empty", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.close": [[87, 93], ["task_queue._QueueImplementation._queue.empty", "time.sleep"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.task_queue._QueueImplementation.empty"], ["", "def", "close", "(", "self", ")", ":", "\n", "    ", "while", "True", ":", "\n", "      ", "if", "self", ".", "_queue", ".", "empty", "(", ")", ":", "\n", "        ", "self", ".", "_closed", "=", "True", "\n", "break", "\n", "", "time", ".", "sleep", "(", "5", ")", "\n", "", "", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.training.prepare_data": [[29, 51], ["list", "zip", "tensorflow.reshape", "tensorflow.stack"], "function", ["None"], ["def", "prepare_data", "(", "data", ":", "List", "[", "Tuple", "[", "Any", ",", "...", "]", "]", ")", "->", "List", "[", "tf", ".", "Tensor", "]", ":", "\n", "  ", "r\"\"\"Prepares the trajectory data ready for tensorflow.\n\n  This function unpacks transition data and stacks them suitable for input\n  to a neural network. The idea is:\n  1. Convert the transition tuples into lists containing the entries of the\n     tuples.\n  2. Stack the elements for tensorflow and reshape the lists so that they are\n     of shape (batch_size, 1). Note: That if the transition contains a tuple,\n     it will be flattened such that the shape will be (None, 1).\n\n  Args:\n    data: A list of tuples with the transition data.\n\n  Returns:\n    A list of `tf.Tensor`s that are suitable for a neural network.\n  \"\"\"", "\n", "# Transpose data from [(x1, y1), (x2, y2), ...]  into", "\n", "# ([x1, x2, ...], [y1, y2, ...]).", "\n", "transposed_data", "=", "list", "(", "zip", "(", "*", "data", ")", ")", "\n", "# Convert inner lists into Tensors by stacking and reshaping them.", "\n", "return", "[", "tf", ".", "reshape", "(", "tf", ".", "stack", "(", "x", ")", ",", "(", "-", "1", ",", "1", ")", ")", "for", "x", "in", "transposed_data", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.training._both_are_none_or_both_are_given": [[53, 55], ["None"], "function", ["None"], ["", "def", "_both_are_none_or_both_are_given", "(", "entry1", ",", "entry2", ")", ":", "\n", "  ", "return", "(", "entry1", "is", "None", ")", "==", "(", "entry2", "is", "None", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.training.get_training_steps": [[57, 187], ["absl.logging.info", "tensorflow.function", "tensorflow.function", "training._both_are_none_or_both_are_given", "ValueError", "tape.gradient", "affordance_optimizer.apply_gradients", "dict", "heuristic_affordance_fn", "heuristic_affordance_fn.astype", "tape.gradient", "model_optimizer.apply_gradients", "dict", "dict", "tensorflow.GradientTape", "affordance_network", "tensorflow.reshape", "tensorflow.reshape", "tensorflow.keras.losses.binary_crossentropy", "tensorflow.reduce_mean", "ValueError", "zip", "tensorflow.GradientTape", "tensorflow.math.reduce_sum", "model_network", "transition_model.log_prob", "tensorflow.debugging.assert_shapes", "tensorflow.debugging.assert_shapes", "zip", "tensorflow.concat", "tensorflow.gather_nd", "tensorflow.squeeze", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.reduce_sum", "tensorflow.constant", "affordance_network", "tensorflow.math.greater_equal", "tensorflow.reduce_any", "tensorflow.stop_gradient", "tensorflow.ones_like", "tensorflow.keras.losses.mean_squared_error", "tensorflow.keras.losses.mean_squared_error", "tensorflow.cast", "tensorflow.squeeze"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.training._both_are_none_or_both_are_given"], ["", "def", "get_training_steps", "(", "\n", "model_network", ":", "tf", ".", "keras", ".", "Model", ",", "# pytype: disable=attribute-error", "\n", "model_optimizer", ":", "tf", ".", "keras", ".", "optimizers", ".", "Optimizer", ",", "# pytype: disable=attribute-error", "\n", "affordance_network", ":", "Optional", "[", "tf", ".", "keras", ".", "Model", "]", "=", "None", ",", "# pytype: disable=attribute-error", "\n", "affordance_optimizer", ":", "Optional", "[", "tf", ".", "keras", ".", "optimizers", ".", "Optimizer", "]", "=", "None", ",", "# pytype: disable=attribute-error", "\n", "heuristic_affordance_fn", ":", "Optional", "[", "affordances", ".", "AffordancesFn", "]", "=", "None", ",", "\n", "affordance_mask_threshold", ":", "float", "=", "0.5", ",", "\n", "use_learned_affordances", ":", "bool", "=", "False", ",", "\n", ")", "->", "Tuple", "[", "OptimizationStep", ",", "OptimizationStep", "]", ":", "\n", "  ", "\"\"\"Returns (optimized) training steps.\"\"\"", "\n", "\n", "# Error checking to make sure the correct combinations of model/affordance", "\n", "# nets and optimizers are given or none at all.", "\n", "if", "not", "_both_are_none_or_both_are_given", "(", "\n", "affordance_network", ",", "affordance_optimizer", ")", ":", "\n", "    ", "raise", "ValueError", "(", "'Both affordance network and optimizer have to be given.'", ")", "\n", "", "else", ":", "\n", "    ", "use_affordances", "=", "affordance_network", "is", "not", "None", "\n", "\n", "# User friendly print outs indicate what is happening.", "\n", "", "logging", ".", "info", "(", "'Using model? True. Using affordances? %s.'", ",", "use_affordances", ")", "\n", "\n", "def", "_train_step_affordances", "(", "trajectory", ")", ":", "\n", "    ", "\"\"\"Train the affordances network.\"\"\"", "\n", "if", "affordance_network", "is", "None", ":", "return", "dict", "(", "\n", "total_affordance_loss", "=", "tf", ".", "constant", "(", "0.0", ")", ")", "\n", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "\n", "      ", "s_t", ",", "o_t", ",", "_", ",", "_", ",", "_", ",", "achieved_intent", "=", "trajectory", "\n", "\n", "predicted_intent", "=", "affordance_network", "(", "s_t", ",", "o_t", ")", "\n", "achieved_intent", "=", "tf", ".", "reshape", "(", "achieved_intent", ",", "(", "-", "1", ",", "1", ")", ")", "\n", "predicted_intent", "=", "tf", ".", "reshape", "(", "predicted_intent", ",", "(", "-", "1", ",", "1", ")", ")", "\n", "\n", "loss", "=", "tf", ".", "keras", ".", "losses", ".", "binary_crossentropy", "(", "# pytype: disable=attribute-error", "\n", "achieved_intent", ",", "predicted_intent", ")", "\n", "total_loss", "=", "tf", ".", "reduce_mean", "(", "loss", ")", "\n", "", "grads", "=", "tape", ".", "gradient", "(", "total_loss", ",", "affordance_network", ".", "trainable_variables", ")", "\n", "if", "affordance_optimizer", "is", "None", ":", "\n", "      ", "raise", "ValueError", "(", "'Please provide an affordance optimizer.'", ")", "\n", "", "affordance_optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "grads", ",", "affordance_network", ".", "trainable_variables", ")", ")", "\n", "\n", "return", "dict", "(", "total_affordance_loss", "=", "total_loss", ")", "\n", "\n", "", "if", "heuristic_affordance_fn", "is", "not", "None", "and", "not", "use_learned_affordances", ":", "\n", "    ", "affs_matrix", "=", "heuristic_affordance_fn", "(", ")", "\n", "heuristic_affs_matrix", "=", "affs_matrix", ".", "astype", "(", "np", ".", "float32", ")", "\n", "", "else", ":", "\n", "    ", "heuristic_affs_matrix", "=", "None", "\n", "\n", "", "def", "_train_step_model", "(", "trajectory", ")", ":", "\n", "    ", "\"\"\"Train model network.\"\"\"", "\n", "with", "tf", ".", "GradientTape", "(", ")", "as", "tape", ":", "\n", "      ", "s_t", ",", "o_t", ",", "target_lengths", ",", "target_rewards", ",", "s_tp1", ",", "_", "=", "trajectory", "\n", "\n", "# Here we compute the mask for each element in the batch. For each", "\n", "# (state, option) pair in the batch, the mask is 1 if it is part of the", "\n", "# affordance set. We then use this mask to zero out unaffordable states", "\n", "# and options so the loss not given any weight for those transitions.", "\n", "if", "heuristic_affs_matrix", "is", "not", "None", ":", "\n", "# Creates an index tensor indicating which state and options are in the", "\n", "# batch.", "\n", "        ", "idx", "=", "tf", ".", "concat", "(", "[", "s_t", ",", "o_t", "]", ",", "axis", "=", "1", ")", "\n", "# The affs_matrix is of shape |S| x |O| with 1's where that tuple is", "\n", "# affordable.  The `gather_nd` operation picks out entries of that", "\n", "# matrix corresponding to the indices in the batch.", "\n", "mask", "=", "tf", ".", "gather_nd", "(", "heuristic_affs_matrix", ",", "idx", ")", "\n", "", "elif", "affordance_network", "is", "not", "None", ":", "\n", "# Use affordance network to output whether an intent can be completed at", "\n", "# each state action pair.", "\n", "        ", "affordances_predictions", "=", "affordance_network", "(", "s_t", ",", "o_t", ")", "\n", "# Use the threshold to convert this into a binary mask.", "\n", "masks_per_intent", "=", "tf", ".", "math", ".", "greater_equal", "(", "\n", "affordances_predictions", ",", "affordance_mask_threshold", ")", "\n", "# Reduce so that we output a single value determining if _any_ intent", "\n", "# can be completed from here.", "\n", "mask", "=", "tf", ".", "reduce_any", "(", "masks_per_intent", ",", "1", ")", "\n", "# Prevent gradient from flowing through to the affordance network.", "\n", "# Technically i do not think this is possible but just in case.", "\n", "mask", "=", "tf", ".", "stop_gradient", "(", "tf", ".", "cast", "(", "mask", ",", "tf", ".", "float32", ")", ")", "\n", "", "else", ":", "\n", "# By default everything is affordable.", "\n", "        ", "mask", "=", "tf", ".", "ones_like", "(", "tf", ".", "squeeze", "(", "s_t", ")", ",", "dtype", "=", "tf", ".", "float32", ")", "\n", "# The mask is a vector of length batch size with 1's indicating which", "\n", "# examples should be included in the loss. We take the sum of the mask", "\n", "# here to obtains the number of examples that are to be incldued. This", "\n", "# variable is used to divide the loss instead of taking a generic mean.", "\n", "", "num_examples", "=", "tf", ".", "math", ".", "reduce_sum", "(", "mask", ")", "\n", "\n", "transition_model", ",", "lengths", ",", "rewards", "=", "model_network", "(", "s_t", ",", "o_t", ")", "\n", "log_probs", "=", "transition_model", ".", "log_prob", "(", "tf", ".", "squeeze", "(", "s_tp1", ")", ")", "\n", "tf", ".", "debugging", ".", "assert_shapes", "(", "[", "\n", "(", "log_probs", ",", "(", "None", ",", ")", ")", "# Prevent silent broadcasting errors.", "\n", "]", ")", "\n", "\n", "# Negate log_prob here because we want to maximize this via minimization.", "\n", "transition_loss", "=", "-", "log_probs", "*", "mask", "\n", "# pytype: disable=attribute-error", "\n", "lengths_loss", "=", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "mean_squared_error", "(", "\n", "target_lengths", ",", "lengths", ")", "*", "mask", "\n", "rewards_loss", "=", "0.5", "*", "tf", ".", "keras", ".", "losses", ".", "mean_squared_error", "(", "\n", "target_rewards", ",", "rewards", ")", "*", "mask", "\n", "# pytype: enable=attribute-error", "\n", "\n", "tf", ".", "debugging", ".", "assert_shapes", "(", "[", "\n", "(", "transition_loss", ",", "(", "'B'", ",", ")", ")", ",", "\n", "(", "lengths_loss", ",", "(", "'B'", ",", ")", ")", ",", "\n", "(", "mask", ",", "(", "'B'", ",", ")", ")", ",", "\n", "(", "rewards_loss", ",", "(", "'B'", ",", ")", ")", ",", "\n", "]", ")", "\n", "\n", "transition_loss", "=", "tf", ".", "reduce_sum", "(", "transition_loss", ")", "/", "num_examples", "\n", "lengths_loss", "=", "tf", ".", "reduce_sum", "(", "lengths_loss", ")", "/", "num_examples", "\n", "rewards_loss", "=", "tf", ".", "reduce_sum", "(", "rewards_loss", ")", "/", "num_examples", "\n", "total_loss", "=", "rewards_loss", "+", "transition_loss", "+", "lengths_loss", "\n", "\n", "", "grads", "=", "tape", ".", "gradient", "(", "total_loss", ",", "model_network", ".", "trainable_variables", ")", "\n", "model_optimizer", ".", "apply_gradients", "(", "\n", "zip", "(", "grads", ",", "model_network", ".", "trainable_variables", ")", ")", "\n", "return", "dict", "(", "\n", "total_model_loss", "=", "total_loss", ",", "\n", "transition_loss", "=", "transition_loss", ",", "\n", "rewards_loss", "=", "rewards_loss", ",", "\n", "lengths_loss", "=", "lengths_loss", ")", "\n", "\n", "# Optimize training step execution by compiling them using tf.function.", "\n", "", "_train_step_affordances", "=", "tf", ".", "function", "(", "_train_step_affordances", ")", "# pylint: disable=invalid-name", "\n", "_train_step_model", "=", "tf", ".", "function", "(", "_train_step_model", ")", "# pylint: disable=invalid-name", "\n", "\n", "return", "_train_step_model", ",", "_train_step_affordances", "\n", "", ""]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils_test.OptionUtilsTest.test_number_of_options": [[33, 38], ["option_utils_test.OptionUtilsTest.assertLen", "option_utils_test.OptionUtilsTest.assertLen", "option_utils_test.OptionUtilsTest.assertLen", "option_utils_test.OptionUtilsTest.assertLen"], "methods", ["None"], ["  ", "def", "test_number_of_options", "(", "self", ")", ":", "\n", "    ", "self", ".", "assertLen", "(", "option_utils", ".", "Options", ",", "75", ")", "\n", "self", ".", "assertLen", "(", "option_utils", ".", "OptionsDropping", ",", "25", ")", "\n", "self", ".", "assertLen", "(", "option_utils", ".", "OptionsPicking", ",", "25", ")", "\n", "self", ".", "assertLen", "(", "option_utils", ".", "OptionsAny", ",", "25", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils_test.OptionUtilsTest.test_check_option_termination": [[39, 205], ["absl.testing.parameterized.named_parameters", "affordances_option_models.env_utils.state_to_int_fn", "option_utils_test.OptionUtilsTest.assertEqual", "affordances_option_models.env_utils.TaxiState", "affordances_option_models.option_utils.check_option_termination"], "methods", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.state_to_int_fn", "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.option_utils.check_option_termination"], ["", "@", "parameterized", ".", "named_parameters", "(", "\n", "#   GoToXX_Any:", "\n", "#     - Grid cell of s_tp1 must match the grid cell XX.", "\n", "{", "\n", "'testcase_name'", ":", "'GoTo 0 passenger inside. Dropping'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Any", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "DROP", ",", "\n", "'outcome'", ":", "True", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'GoTo 0 passenger inside. Picking'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Any", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "PICKUP", ",", "\n", "'outcome'", ":", "True", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'GoTo 0 passenger outside. Picking'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Any", ",", "\n", "'passenger_status'", ":", "2", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "PICKUP", ",", "\n", "'outcome'", ":", "True", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'GoTo 3 from 2 + East succeeds.'", ",", "\n", "'option'", ":", "Options", ".", "GoTo3_Any", ",", "\n", "'passenger_status'", ":", "2", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "2", ",", "\n", "'action'", ":", "ActionMap", ".", "EAST", ",", "\n", "'outcome'", ":", "True", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'GoTo (1, 3) from (0, 3) + South succeeds.'", ",", "\n", "'option'", ":", "Options", ".", "GoTo8_Any", ",", "\n", "'passenger_status'", ":", "2", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "3", ",", "\n", "'action'", ":", "ActionMap", ".", "SOUTH", ",", "\n", "'outcome'", ":", "True", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'GoTo (1, 3) from (0, 3) + EAST Fails.'", ",", "\n", "'option'", ":", "Options", ".", "GoTo8_Any", ",", "\n", "'passenger_status'", ":", "2", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "3", ",", "\n", "'action'", ":", "ActionMap", ".", "EAST", ",", "\n", "'outcome'", ":", "False", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'GoTo 2 from 2 + East fails.'", ",", "\n", "'option'", ":", "Options", ".", "GoTo2_Any", ",", "\n", "'passenger_status'", ":", "2", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "2", ",", "\n", "'action'", ":", "ActionMap", ".", "EAST", ",", "\n", "'outcome'", ":", "False", ",", "\n", "}", ",", "\n", "# GoToXX_Drop:", "\n", "#   - Action must be DROP.", "\n", "#   - Grid cell of s_tp1 must match the grid cell XX.", "\n", "#   - Passenger must be inside the taxi.", "\n", "{", "\n", "'testcase_name'", ":", "'Drop passenger in taxi at 0'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Drop", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "DROP", ",", "\n", "'outcome'", ":", "True", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Fail to drop passenger @ 0 (not in vehicle) at 0'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Drop", ",", "\n", "'passenger_status'", ":", "0", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "DROP", ",", "\n", "'outcome'", ":", "False", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Fail to drop passenger @ 2 (not in vehicle) at 0'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Drop", ",", "\n", "'passenger_status'", ":", "2", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "DROP", ",", "\n", "'outcome'", ":", "False", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Drop passenger in vehicle at (0, 2)'", ",", "\n", "'option'", ":", "Options", ".", "GoTo2_Drop", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "2", ",", "\n", "'action'", ":", "ActionMap", ".", "DROP", ",", "\n", "'outcome'", ":", "True", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "\n", "'Fail Drop passenger in vehicle at (0, 1) when at (0, 2)'", ",", "\n", "'option'", ":", "Options", ".", "GoTo1_Drop", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "2", ",", "\n", "'action'", ":", "ActionMap", ".", "DROP", ",", "\n", "'outcome'", ":", "False", ",", "\n", "}", ",", "\n", "#   GoToXX_Pickup:", "\n", "#     - Action must be PICKUP.", "\n", "#     - Grid cell of s_tp1 must match the grid cell XX.", "\n", "#     - Passenger must be outside the taxi (doesn't matter where exactly).", "\n", "{", "\n", "'testcase_name'", ":", "'Cannot pickup when action is move.'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Pickup", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "WEST", ",", "\n", "'outcome'", ":", "False", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Fail to pickup passenger already inside.'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Pickup", ",", "\n", "'passenger_status'", ":", "env_utils", ".", "PASSENGER_INSIDE_CAR_STATUS", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "PICKUP", ",", "\n", "'outcome'", ":", "False", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Try to pickup passenger @ 2 at 0'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Pickup", ",", "\n", "'passenger_status'", ":", "2", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "PICKUP", ",", "\n", "'outcome'", ":", "True", ",", "\n", "}", ",", "\n", "{", "\n", "'testcase_name'", ":", "'Try to pickup passenger @ 0 at 0'", ",", "\n", "'option'", ":", "Options", ".", "GoTo0_Pickup", ",", "\n", "'passenger_status'", ":", "0", ",", "\n", "'row'", ":", "0", ",", "\n", "'col'", ":", "0", ",", "\n", "'action'", ":", "ActionMap", ".", "PICKUP", ",", "\n", "'outcome'", ":", "True", ",", "\n", "}", ",", "\n", ")", "\n", "def", "test_check_option_termination", "(", "\n", "self", ",", "row", ",", "col", ",", "passenger_status", ",", "action", ",", "option", ",", "outcome", ")", ":", "\n", "\n", "    ", "taxi_state", "=", "env_utils", ".", "state_to_int_fn", "(", "\n", "env_utils", ".", "TaxiState", "(", "row", ",", "col", ",", "passenger_status", ",", "0", ")", ")", "\n", "\n", "self", ".", "assertEqual", "(", "\n", "option_utils", ".", "check_option_termination", "(", "taxi_state", ",", "action", ",", "option", ")", ",", "\n", "outcome", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.TaxiState.validate": [[52, 61], ["ValueError", "ValueError", "ValueError", "ValueError"], "methods", ["None"], ["def", "validate", "(", "self", ")", ":", "\n", "    ", "if", "self", ".", "passenger_status", ">", "PASSENGER_INSIDE_CAR_STATUS", ":", "\n", "      ", "raise", "ValueError", "(", "'Passenger is in undefined location.'", ")", "\n", "", "if", "self", ".", "destination", ">", "3", ":", "\n", "      ", "raise", "ValueError", "(", "'Only 4 possible destinations are valid.'", ")", "\n", "", "if", "not", "0", "<=", "self", ".", "row", "<=", "4", ":", "\n", "      ", "raise", "ValueError", "(", "'Row must be between (0, 4)'", ")", "\n", "", "if", "not", "0", "<=", "self", ".", "col", "<=", "4", ":", "\n", "      ", "raise", "ValueError", "(", "'Col must be between (0, 4)'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.make_taxi_environment": [[63, 65], ["gym.make"], "function", ["None"], ["", "", "", "def", "make_taxi_environment", "(", ")", ":", "\n", "  ", "return", "gym", ".", "make", "(", "'Taxi-v2'", ")", ".", "env", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.state_to_int_fn": [[72, 76], ["taxi_state.validate", "_GLOBAL_ENV.encode"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.TaxiState.validate"], ["def", "state_to_int_fn", "(", "taxi_state", ":", "TaxiState", ")", "->", "int", ":", "\n", "  ", "\"\"\"Converts a readable state in the environment to the integer state.\"\"\"", "\n", "taxi_state", ".", "validate", "(", ")", "\n", "return", "_GLOBAL_ENV", ".", "encode", "(", "*", "taxi_state", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.int_to_state_fn": [[78, 83], ["env_utils.TaxiState", "env_utils.TaxiState.validate", "_GLOBAL_ENV.decode"], "function", ["home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.TaxiState.validate"], ["", "def", "int_to_state_fn", "(", "x", ":", "int", ")", "->", "TaxiState", ":", "\n", "  ", "\"\"\"Converts an integer representation of state into a human readable one.\"\"\"", "\n", "state", "=", "TaxiState", "(", "*", "_GLOBAL_ENV", ".", "decode", "(", "x", ")", ")", "\n", "state", ".", "validate", "(", ")", "\n", "return", "state", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.grid_cell_to_xy": [[96, 104], ["ValueError"], "function", ["None"], ["def", "grid_cell_to_xy", "(", "pos", ":", "int", ",", "grid_size", ":", "int", "=", "5", ")", "->", "Tuple", "[", "int", ",", "int", "]", ":", "\n", "  ", "\"\"\"Converts an integer from 0-24 into an (x, y) position.\"\"\"", "\n", "num_cells", "=", "grid_size", "*", "grid_size", "-", "1", "\n", "if", "not", "0", "<=", "pos", "<=", "num_cells", ":", "\n", "    ", "raise", "ValueError", "(", "f'Grid cell does not exist in grid of size {grid_size}'", ")", "\n", "", "x", "=", "pos", "//", "grid_size", "\n", "y", "=", "pos", "%", "grid_size", "\n", "return", "(", "x", ",", "y", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.deepmind_affordances_option_models.None.env_utils.get_transition_and_reward_matrices": [[106, 137], ["numpy.zeros", "numpy.zeros", "P.items", "range", "_GLOBAL_ENV.P.items", "a2d.items"], "function", ["None"], ["", "def", "get_transition_and_reward_matrices", "(", ")", "->", "Tuple", "[", "Any", ",", "np", ".", "ndarray", ",", "np", ".", "ndarray", "]", ":", "\n", "  ", "\"\"\"Obtains transition and reward matrices for taxi as numpy arrays.\n\n  Use these quantities to do value iteration and obtain the best possible\n  flat policy.\n\n  Returns:\n    P: The internal dictionary representation of the transition matrix as given\n      by Gym.\n    P_matrix: A |S| x |A| x |S| probability transition matrix where P[s, a, s']\n      represents the probability of transitioning from state s, to s' by taking\n      action a.\n    R_matrix: A |S| x |A| matrix representing where R[s, a] represents the\n      reward obtained by taking action a from state s.\n  \"\"\"", "\n", "num_states", "=", "_GLOBAL_ENV", ".", "nS", "\n", "num_actions", "=", "_GLOBAL_ENV", ".", "nA", "\n", "# pylint: disable=invalid-name", "\n", "P", "=", "{", "\n", "s", ":", "{", "a", ":", "[", "tup", "[", ":", "3", "]", "for", "tup", "in", "tups", "]", "for", "(", "a", ",", "tups", ")", "in", "a2d", ".", "items", "(", ")", "\n", "}", "for", "(", "s", ",", "a2d", ")", "in", "_GLOBAL_ENV", ".", "P", ".", "items", "(", ")", "\n", "}", "\n", "P_matrix", "=", "np", ".", "zeros", "(", "(", "num_states", ",", "num_actions", ",", "num_states", ")", ",", "dtype", "=", "np", ".", "float32", ")", "\n", "R_matrix", "=", "np", ".", "zeros", "(", "(", "num_states", ",", "num_actions", ")", ")", "\n", "# pylint: enable=invalid-name", "\n", "for", "(", "s", ",", "transition", ")", "in", "P", ".", "items", "(", ")", ":", "\n", "    ", "for", "a", "in", "range", "(", "num_actions", ")", ":", "\n", "      ", "prob", ",", "sprime", ",", "reward", "=", "transition", "[", "a", "]", "[", "0", "]", "\n", "P_matrix", "[", "s", ",", "a", ",", "sprime", "]", "=", "prob", "\n", "R_matrix", "[", "s", ",", "a", "]", "=", "reward", "\n", "", "", "return", "P", ",", "P_matrix", ",", "R_matrix", "\n", "", ""]]}