{"home.repos.pwc.inspect_result.Anirudh257_strm.None.model.PositionalEncoding.__init__": [[24, 39], ["torch.Module.__init__", "torch.Dropout", "torch.Dropout", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.arange().unsqueeze", "torch.exp", "torch.exp", "torch.exp", "torch.exp", "pe.unsqueeze.unsqueeze.unsqueeze", "model.PositionalEncoding.register_buffer", "torch.sin", "torch.sin", "torch.sin", "torch.sin", "torch.cos", "torch.cos", "torch.cos", "torch.cos", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "math.log"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__"], ["def", "__init__", "(", "self", ",", "d_model", ",", "dropout", ",", "max_len", "=", "5000", ",", "pe_scale_factor", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "PositionalEncoding", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "self", ".", "pe_scale_factor", "=", "pe_scale_factor", "\n", "# Compute the positional encodings once in log space.", "\n", "# pe is of shape max_len(5000) x 2048(last layer of FC)", "\n", "pe", "=", "torch", ".", "zeros", "(", "max_len", ",", "d_model", ")", "\n", "# position is of shape 5000 x 1", "\n", "position", "=", "torch", ".", "arange", "(", "0", ",", "max_len", ")", ".", "unsqueeze", "(", "1", ")", "\n", "div_term", "=", "torch", ".", "exp", "(", "torch", ".", "arange", "(", "0", ",", "d_model", ",", "2", ")", "*", "-", "(", "math", ".", "log", "(", "10000.0", ")", "/", "d_model", ")", ")", "\n", "pe", "[", ":", ",", "0", ":", ":", "2", "]", "=", "torch", ".", "sin", "(", "position", "*", "div_term", ")", "*", "self", ".", "pe_scale_factor", "\n", "pe", "[", ":", ",", "1", ":", ":", "2", "]", "=", "torch", ".", "cos", "(", "position", "*", "div_term", ")", "*", "self", ".", "pe_scale_factor", "\n", "# pe contains a vector of shape 1 x 5000 x 2048", "\n", "pe", "=", "pe", ".", "unsqueeze", "(", "0", ")", "\n", "self", ".", "register_buffer", "(", "'pe'", ",", "pe", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.PositionalEncoding.forward": [[40, 43], ["model.PositionalEncoding.dropout", "torch.autograd.Variable", "torch.autograd.Variable", "x.size"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "       ", "x", "=", "x", "+", "Variable", "(", "self", ".", "pe", "[", ":", ",", ":", "x", ".", "size", "(", "1", ")", "]", ",", "requires_grad", "=", "False", ")", "\n", "return", "self", ".", "dropout", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.DistanceLoss.__init__": [[46, 64], ["torch.Module.__init__", "int", "torch.Dropout", "torch.Dropout", "itertools.combinations", "len", "torch.Linear", "torch.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "range", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__"], ["def", "__init__", "(", "self", ",", "args", ",", "temporal_set_size", "=", "3", ")", ":", "\n", "        ", "super", "(", "DistanceLoss", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "temporal_set_size", "=", "temporal_set_size", "\n", "\n", "max_len", "=", "int", "(", "self", ".", "args", ".", "seq_len", "*", "1.5", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "p", "=", "0.1", ")", "\n", "\n", "# generate all ordered tuples corresponding to the temporal set size 2 or 3.", "\n", "frame_idxs", "=", "[", "i", "for", "i", "in", "range", "(", "self", ".", "args", ".", "seq_len", ")", "]", "\n", "frame_combinations", "=", "combinations", "(", "frame_idxs", ",", "temporal_set_size", ")", "\n", "self", ".", "tuples", "=", "[", "torch", ".", "tensor", "(", "comb", ")", ".", "cuda", "(", ")", "for", "comb", "in", "frame_combinations", "]", "\n", "self", ".", "tuples_len", "=", "len", "(", "self", ".", "tuples", ")", "# 28 for tempset_2", "\n", "\n", "# nn.Linear(4096, 1024)", "\n", "self", ".", "clsW", "=", "nn", ".", "Linear", "(", "self", ".", "args", ".", "trans_linear_in_dim", "*", "self", ".", "temporal_set_size", ",", "self", ".", "args", ".", "trans_linear_in_dim", "//", "2", ")", "\n", "self", ".", "relu", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.DistanceLoss.forward": [[66, 126], ["model.DistanceLoss.dropout", "model.DistanceLoss.dropout", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack().to", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "support_labels.to.to.to", "torch.unique", "torch.unique", "torch.unique", "torch.unique", "model.DistanceLoss.clsW", "model.DistanceLoss.relu", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.stack.view", "torch.stack.view", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "class_k.view.view.view", "model.DistanceLoss.clsW", "model.DistanceLoss.relu", "torch.cdist", "torch.cdist", "torch.cdist", "torch.cdist", "[].reshape", "[].reshape.mean", "c.long", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "model.DistanceLoss._extract_class_indices", "class_k.view.view.to", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.cdist.min", "torch.cdist.min"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.model.TemporalCrossTransformer._extract_class_indices"], ["", "def", "forward", "(", "self", ",", "support_set", ",", "support_labels", ",", "queries", ")", ":", "\n", "# support_set : 25 x 8 x 2048, support_labels: 25, queries: 20 x 8 x 2048", "\n", "        ", "n_queries", "=", "queries", ".", "shape", "[", "0", "]", "#20", "\n", "n_support", "=", "support_set", ".", "shape", "[", "0", "]", "#25", "\n", "\n", "# Add a dropout before creating tuples", "\n", "support_set", "=", "self", ".", "dropout", "(", "support_set", ")", "# 25 x 8 x 2048", "\n", "queries", "=", "self", ".", "dropout", "(", "queries", ")", "# 20 x 8 x 2048", "\n", "\n", "# construct new queries and support set made of tuples of images after pe", "\n", "# Support set s = number of tuples(28 for 2/56 for 3) stacked in a list form containing elements of form 25 x 4096(2 x 2048 - (2 frames stacked))", "\n", "s", "=", "[", "torch", ".", "index_select", "(", "support_set", ",", "-", "2", ",", "p", ")", ".", "reshape", "(", "n_support", ",", "-", "1", ")", "for", "p", "in", "self", ".", "tuples", "]", "\n", "q", "=", "[", "torch", ".", "index_select", "(", "queries", ",", "-", "2", ",", "p", ")", ".", "reshape", "(", "n_queries", ",", "-", "1", ")", "for", "p", "in", "self", ".", "tuples", "]", "\n", "\n", "support_set", "=", "torch", ".", "stack", "(", "s", ",", "dim", "=", "-", "2", ")", ".", "to", "(", "device", ")", "# 25 x 28 x 4096", "\n", "queries", "=", "torch", ".", "stack", "(", "q", ",", "dim", "=", "-", "2", ")", "# 20 x 28 x 4096", "\n", "support_labels", "=", "support_labels", ".", "to", "(", "device", ")", "\n", "unique_labels", "=", "torch", ".", "unique", "(", "support_labels", ")", "# 5", "\n", "\n", "query_embed", "=", "self", ".", "clsW", "(", "queries", ".", "view", "(", "-", "1", ",", "self", ".", "args", ".", "trans_linear_in_dim", "*", "self", ".", "temporal_set_size", ")", ")", "# 560[20x28] x 1024", "\n", "\n", "# Add relu after clsW", "\n", "query_embed", "=", "self", ".", "relu", "(", "query_embed", ")", "# 560 x 1024        ", "\n", "\n", "# init tensor to hold distances between every support tuple and every target tuple. It is of shape 20  x 5", "\n", "'''\n            4-queries * 5 classes x 5(5 classes) and store this in a logit vector\n        '''", "\n", "dist_all", "=", "torch", ".", "zeros", "(", "n_queries", ",", "self", ".", "args", ".", "way", ")", "# 20 x 5", "\n", "\n", "for", "label_idx", ",", "c", "in", "enumerate", "(", "unique_labels", ")", ":", "\n", "# Select keys corresponding to this class from the support set tuples", "\n", "            ", "class_k", "=", "torch", ".", "index_select", "(", "support_set", ",", "0", ",", "self", ".", "_extract_class_indices", "(", "support_labels", ",", "c", ")", ")", "# 5 x 28 x 4096", "\n", "\n", "# Reshaping the selected keys", "\n", "class_k", "=", "class_k", ".", "view", "(", "-", "1", ",", "self", ".", "args", ".", "trans_linear_in_dim", "*", "self", ".", "temporal_set_size", ")", "# 140 x 4096", "\n", "\n", "# Get the support set projection from the current class", "\n", "support_embed", "=", "self", ".", "clsW", "(", "class_k", ".", "to", "(", "queries", ".", "device", ")", ")", "# 140[5 x 28] x1024", "\n", "\n", "# Add relu after clsW", "\n", "support_embed", "=", "self", ".", "relu", "(", "support_embed", ")", "# 140 x 1024", "\n", "\n", "# Calculate p-norm distance between the query embedding and the support set embedding", "\n", "distmat", "=", "torch", ".", "cdist", "(", "query_embed", ",", "support_embed", ")", "# 560[20 x 28] x 140[28 x 5]", "\n", "\n", "# Across the 140 tuples compared against, get the minimum distance for each of the 560 queries", "\n", "min_dist", "=", "distmat", ".", "min", "(", "dim", "=", "1", ")", "[", "0", "]", ".", "reshape", "(", "n_queries", ",", "self", ".", "tuples_len", ")", "# 20[5-way x 4-queries] x 28", "\n", "\n", "# Average across the 28 tuples", "\n", "query_dist", "=", "min_dist", ".", "mean", "(", "dim", "=", "1", ")", "# 20", "\n", "\n", "# Make it negative as this has to be reduced.", "\n", "distance", "=", "-", "1.0", "*", "query_dist", "\n", "c_idx", "=", "c", ".", "long", "(", ")", "\n", "dist_all", "[", ":", ",", "c_idx", "]", "=", "distance", "# Insert into the required location.", "\n", "\n", "", "return_dict", "=", "{", "'logits'", ":", "dist_all", "}", "\n", "\n", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.DistanceLoss._extract_class_indices": [[127, 138], ["torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_extract_class_indices", "(", "labels", ",", "which_class", ")", ":", "\n", "        ", "\"\"\"\n        Helper method to extract the indices of elements which have the specified label.\n        :param labels: (torch.tensor) Labels of the context set.\n        :param which_class: Label for which indices are extracted.\n        :return: (torch.tensor) Indices in the form of a mask that indicate the locations of the specified label.\n        \"\"\"", "\n", "class_mask", "=", "torch", ".", "eq", "(", "labels", ",", "which_class", ")", "# binary mask of labels equal to which_class", "\n", "class_mask_indices", "=", "torch", ".", "nonzero", "(", "class_mask", ")", "# indices of labels equal to which class", "\n", "return", "torch", ".", "reshape", "(", "class_mask_indices", ",", "(", "-", "1", ",", ")", ")", "# reshape to be a 1D vector", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.TemporalCrossTransformer.__init__": [[141, 163], ["torch.Module.__init__", "int", "model.PositionalEncoding", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.LayerNorm", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Softmax", "itertools.combinations", "len", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "torch.tensor().cuda", "range", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "temporal_set_size", "=", "3", ")", ":", "\n", "        ", "super", "(", "TemporalCrossTransformer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "args", "=", "args", "\n", "self", ".", "temporal_set_size", "=", "temporal_set_size", "\n", "\n", "max_len", "=", "int", "(", "self", ".", "args", ".", "seq_len", "*", "1.5", ")", "\n", "self", ".", "pe", "=", "PositionalEncoding", "(", "self", ".", "args", ".", "trans_linear_in_dim", ",", "self", ".", "args", ".", "trans_dropout", ",", "max_len", "=", "max_len", ")", "\n", "\n", "self", ".", "k_linear", "=", "nn", ".", "Linear", "(", "self", ".", "args", ".", "trans_linear_in_dim", "*", "temporal_set_size", ",", "self", ".", "args", ".", "trans_linear_out_dim", ")", "#.cuda()", "\n", "self", ".", "v_linear", "=", "nn", ".", "Linear", "(", "self", ".", "args", ".", "trans_linear_in_dim", "*", "temporal_set_size", ",", "self", ".", "args", ".", "trans_linear_out_dim", ")", "#.cuda()", "\n", "\n", "self", ".", "norm_k", "=", "nn", ".", "LayerNorm", "(", "self", ".", "args", ".", "trans_linear_out_dim", ")", "\n", "self", ".", "norm_v", "=", "nn", ".", "LayerNorm", "(", "self", ".", "args", ".", "trans_linear_out_dim", ")", "\n", "\n", "self", ".", "class_softmax", "=", "torch", ".", "nn", ".", "Softmax", "(", "dim", "=", "1", ")", "\n", "\n", "# generate all ordered tuples corresponding to the temporal set size 2 or 3.", "\n", "frame_idxs", "=", "[", "i", "for", "i", "in", "range", "(", "self", ".", "args", ".", "seq_len", ")", "]", "\n", "frame_combinations", "=", "combinations", "(", "frame_idxs", ",", "temporal_set_size", ")", "\n", "self", ".", "tuples", "=", "[", "torch", ".", "tensor", "(", "comb", ")", ".", "cuda", "(", ")", "for", "comb", "in", "frame_combinations", "]", "\n", "self", ".", "tuples_len", "=", "len", "(", "self", ".", "tuples", ")", "#28", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.TemporalCrossTransformer.forward": [[164, 242], ["model.TemporalCrossTransformer.pe", "model.TemporalCrossTransformer.pe", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "model.TemporalCrossTransformer.k_linear", "model.TemporalCrossTransformer.k_linear", "model.TemporalCrossTransformer.v_linear", "model.TemporalCrossTransformer.v_linear", "model.TemporalCrossTransformer.norm_k().to", "model.TemporalCrossTransformer.norm_k().to", "support_labels.to.to.to", "model.TemporalCrossTransformer.to", "model.TemporalCrossTransformer.to", "torch.unique", "torch.unique", "torch.unique", "torch.unique", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "enumerate", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select().reshape", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "class_scores.permute.permute.permute", "class_scores.permute.permute.reshape", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "class_scores.permute.permute.reshape", "class_scores.permute.permute.permute", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.sum().to", "torch.sum().to", "torch.sum().to", "torch.sum().to", "torch.div", "torch.div", "torch.div", "torch.div", "c.long", "model.TemporalCrossTransformer.norm_k", "model.TemporalCrossTransformer.norm_k", "model.TemporalCrossTransformer._extract_class_indices", "model.TemporalCrossTransformer._extract_class_indices", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "math.sqrt", "model.TemporalCrossTransformer.class_softmax", "torch.norm", "torch.norm", "torch.norm", "torch.norm", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "torch.index_select", "model.TemporalCrossTransformer.unsqueeze", "torch.index_select.transpose", "torch.index_select.transpose", "range", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.model.TemporalCrossTransformer._extract_class_indices", "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.TemporalCrossTransformer._extract_class_indices"], ["", "def", "forward", "(", "self", ",", "support_set", ",", "support_labels", ",", "queries", ")", ":", "\n", "# support_set : 25 x 8 x 2048, support_labels: 25, queries: 20 x 8 x 2048", "\n", "        ", "n_queries", "=", "queries", ".", "shape", "[", "0", "]", "#20", "\n", "n_support", "=", "support_set", ".", "shape", "[", "0", "]", "#25", "\n", "\n", "# static pe after adding the position embedding", "\n", "support_set", "=", "self", ".", "pe", "(", "support_set", ")", "# Support set is of shape 25 x 8 x 2048 -> 25 x 8 x 2048", "\n", "queries", "=", "self", ".", "pe", "(", "queries", ")", "# Queries is of shape 20 x 8 x 2048 -> 20 x 8 x 2048", "\n", "\n", "# construct new queries and support set made of tuples of images after pe", "\n", "# Support set s = number of tuples(28 for 2/56 for 3) stacked in a list form containing elements of form 25 x 4096(2 x 2048 - (2 frames stacked))", "\n", "s", "=", "[", "torch", ".", "index_select", "(", "support_set", ",", "-", "2", ",", "p", ")", ".", "reshape", "(", "n_support", ",", "-", "1", ")", "for", "p", "in", "self", ".", "tuples", "]", "\n", "q", "=", "[", "torch", ".", "index_select", "(", "queries", ",", "-", "2", ",", "p", ")", ".", "reshape", "(", "n_queries", ",", "-", "1", ")", "for", "p", "in", "self", ".", "tuples", "]", "\n", "\n", "support_set", "=", "torch", ".", "stack", "(", "s", ",", "dim", "=", "-", "2", ")", "# 25 x 28 x 4096", "\n", "queries", "=", "torch", ".", "stack", "(", "q", ",", "dim", "=", "-", "2", ")", "# 20 x 28 x 4096", "\n", "\n", "# apply linear maps for performing self-normalization in the next step and the key map's output", "\n", "'''\n            support_set_ks is of shape 25 x 28 x 1152, where 1152 is the dimension of the key = query head. converting the 5-way*5-shot x 28(tuples).\n            query_set_ks is of shape 20 x 28 x 1152 covering 4 query/sample*5-way x 28(number of tuples)\n        '''", "\n", "support_set_ks", "=", "self", ".", "k_linear", "(", "support_set", ")", "# 25 x 28 x 1152", "\n", "queries_ks", "=", "self", ".", "k_linear", "(", "queries", ")", "# 20 x 28 x 1152", "\n", "support_set_vs", "=", "self", ".", "v_linear", "(", "support_set", ")", "# 25 x 28 x 1152", "\n", "queries_vs", "=", "self", ".", "v_linear", "(", "queries", ")", "# 20 x 28 x 1152", "\n", "\n", "# apply norms where necessary", "\n", "mh_support_set_ks", "=", "self", ".", "norm_k", "(", "support_set_ks", ")", ".", "to", "(", "device", ")", "# 25 x 28 x 1152", "\n", "mh_queries_ks", "=", "self", ".", "norm_k", "(", "queries_ks", ")", ".", "to", "(", "device", ")", "# 20 x 28 x 1152", "\n", "support_labels", "=", "support_labels", ".", "to", "(", "device", ")", "\n", "mh_support_set_vs", "=", "support_set_vs", ".", "to", "(", "device", ")", "# 25 x 28 x 1152", "\n", "mh_queries_vs", "=", "queries_vs", ".", "to", "(", "device", ")", "# 20 x 28 x 1152", "\n", "\n", "unique_labels", "=", "torch", ".", "unique", "(", "support_labels", ")", "# 5", "\n", "\n", "# init tensor to hold distances between every support tuple and every target tuple. It is of shape 20  x 5", "\n", "'''\n            4-queries * 5 classes x 5(5 classes) and store this in a logit vector\n        '''", "\n", "all_distances_tensor", "=", "torch", ".", "zeros", "(", "n_queries", ",", "self", ".", "args", ".", "way", ")", "# 20 x 5", "\n", "\n", "for", "label_idx", ",", "c", "in", "enumerate", "(", "unique_labels", ")", ":", "\n", "\n", "# select keys and values for just this class ", "\n", "            ", "class_k", "=", "torch", ".", "index_select", "(", "mh_support_set_ks", ",", "0", ",", "self", ".", "_extract_class_indices", "(", "support_labels", ",", "c", ")", ")", "# 5 x 28 x 1152", "\n", "class_v", "=", "torch", ".", "index_select", "(", "mh_support_set_vs", ",", "0", ",", "self", ".", "_extract_class_indices", "(", "support_labels", ",", "c", ")", ")", "# 5 x 28 x 1152", "\n", "k_bs", "=", "class_k", ".", "shape", "[", "0", "]", "# 5", "\n", "\n", "class_scores", "=", "torch", ".", "matmul", "(", "mh_queries_ks", ".", "unsqueeze", "(", "1", ")", ",", "class_k", ".", "transpose", "(", "-", "2", ",", "-", "1", ")", ")", "/", "math", ".", "sqrt", "(", "self", ".", "args", ".", "trans_linear_out_dim", ")", "# 20 x 5 x 28 x 28", "\n", "\n", "# reshape etc. to apply a softmax for each query tuple", "\n", "class_scores", "=", "class_scores", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "# 20 x 28 x 5 x 28 ", "\n", "\n", "# [For the 20 queries' 28 tuple pairs, find the best match against the 5 selected support samples from the same class", "\n", "class_scores", "=", "class_scores", ".", "reshape", "(", "n_queries", ",", "self", ".", "tuples_len", ",", "-", "1", ")", "# 20 x 28 x 140", "\n", "class_scores", "=", "[", "self", ".", "class_softmax", "(", "class_scores", "[", "i", "]", ")", "for", "i", "in", "range", "(", "n_queries", ")", "]", "# list(20) x 28 x 140", "\n", "class_scores", "=", "torch", ".", "cat", "(", "class_scores", ")", "# 560 x 140 - concatenate all the scores for the tuples", "\n", "class_scores", "=", "class_scores", ".", "reshape", "(", "n_queries", ",", "self", ".", "tuples_len", ",", "-", "1", ",", "self", ".", "tuples_len", ")", "# 20 x 28 x 5 x 28", "\n", "class_scores", "=", "class_scores", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "# 20 x 5 x 28 x 28", "\n", "\n", "# get query specific class prototype         ", "\n", "query_prototype", "=", "torch", ".", "matmul", "(", "class_scores", ",", "class_v", ")", "# 20 x 5 x 28 x 1152 ", "\n", "query_prototype", "=", "torch", ".", "sum", "(", "query_prototype", ",", "dim", "=", "1", ")", ".", "to", "(", "device", ")", "# 20 x 28 x 1152 -> Sum across all the support set values of the corres. class", "\n", "\n", "# calculate distances from queries to query-specific class prototypes", "\n", "diff", "=", "mh_queries_vs", "-", "query_prototype", "# 20 x 28 x 1152", "\n", "norm_sq", "=", "torch", ".", "norm", "(", "diff", ",", "dim", "=", "[", "-", "2", ",", "-", "1", "]", ")", "**", "2", "# 20 ", "\n", "distance", "=", "torch", ".", "div", "(", "norm_sq", ",", "self", ".", "tuples_len", ")", "# 20", "\n", "\n", "# multiply by -1 to get logits", "\n", "distance", "=", "distance", "*", "-", "1", "\n", "c_idx", "=", "c", ".", "long", "(", ")", "\n", "all_distances_tensor", "[", ":", ",", "c_idx", "]", "=", "distance", "# 20", "\n", "\n", "", "return_dict", "=", "{", "'logits'", ":", "all_distances_tensor", "}", "\n", "\n", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.TemporalCrossTransformer._extract_class_indices": [[243, 254], ["torch.eq", "torch.eq", "torch.eq", "torch.eq", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.nonzero", "torch.reshape", "torch.reshape", "torch.reshape", "torch.reshape"], "methods", ["None"], ["", "@", "staticmethod", "\n", "def", "_extract_class_indices", "(", "labels", ",", "which_class", ")", ":", "\n", "        ", "\"\"\"\n        Helper method to extract the indices of elements which have the specified label.\n        :param labels: (torch.tensor) Labels of the context set.\n        :param which_class: Label for which indices are extracted.\n        :return: (torch.tensor) Indices in the form of a mask that indicate the locations of the specified label.\n        \"\"\"", "\n", "class_mask", "=", "torch", ".", "eq", "(", "labels", ",", "which_class", ")", "# binary mask of labels equal to which_class", "\n", "class_mask_indices", "=", "torch", ".", "nonzero", "(", "class_mask", ")", "# indices of labels equal to which class", "\n", "return", "torch", ".", "reshape", "(", "class_mask_indices", ",", "(", "-", "1", ",", ")", ")", "# reshape to be a 1D vector", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.Token_Perceptron.__init__": [[259, 265], ["super().__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__"], ["def", "__init__", "(", "self", ",", "in_dim", ")", ":", "\n", "        ", "super", "(", "Token_Perceptron", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# in_dim 8", "\n", "self", ".", "inp_fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "in_dim", ")", "\n", "self", ".", "out_fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "in_dim", ")", "\n", "self", ".", "relu", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.Token_Perceptron.forward": [[266, 278], ["model.Token_Perceptron.inp_fc", "model.Token_Perceptron.relu", "model.Token_Perceptron.out_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "# Applying the linear layer on the input", "\n", "        ", "output", "=", "self", ".", "inp_fc", "(", "x", ")", "# B x 2048 x 8", "\n", "\n", "# Apply the relu non-linearity", "\n", "output", "=", "self", ".", "relu", "(", "output", ")", "# B x 2048 x 8", "\n", "\n", "# Apply the 2nd linear layer", "\n", "output", "=", "self", ".", "out_fc", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.Bottleneck_Perceptron_2_layer.__init__": [[283, 289], ["super().__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__"], ["def", "__init__", "(", "self", ",", "in_dim", ")", ":", "\n", "# in_dim 2048", "\n", "        ", "super", "(", "Bottleneck_Perceptron_2_layer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inp_fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "in_dim", ")", "\n", "self", ".", "out_fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "in_dim", ")", "\n", "self", ".", "relu", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.Bottleneck_Perceptron_2_layer.forward": [[290, 295], ["model.Bottleneck_Perceptron_2_layer.relu", "model.Bottleneck_Perceptron_2_layer.out_fc", "model.Bottleneck_Perceptron_2_layer.inp_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "output", "=", "self", ".", "relu", "(", "self", ".", "inp_fc", "(", "x", ")", ")", "\n", "output", "=", "self", ".", "out_fc", "(", "output", ")", "\n", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.Bottleneck_Perceptron_3_layer_res.__init__": [[300, 307], ["super().__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.ReLU"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__"], ["def", "__init__", "(", "self", ",", "in_dim", ")", ":", "\n", "# in_dim 2048", "\n", "        ", "super", "(", "Bottleneck_Perceptron_3_layer_res", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "inp_fc", "=", "nn", ".", "Linear", "(", "in_dim", ",", "in_dim", "//", "2", ")", "\n", "self", ".", "hid_fc", "=", "nn", ".", "Linear", "(", "in_dim", "//", "2", ",", "in_dim", "//", "2", ")", "\n", "self", ".", "out_fc", "=", "nn", ".", "Linear", "(", "in_dim", "//", "2", ",", "in_dim", ")", "\n", "self", ".", "relu", "=", "torch", ".", "nn", ".", "ReLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.Bottleneck_Perceptron_3_layer_res.forward": [[308, 314], ["model.Bottleneck_Perceptron_3_layer_res.relu", "model.Bottleneck_Perceptron_3_layer_res.relu", "model.Bottleneck_Perceptron_3_layer_res.out_fc", "model.Bottleneck_Perceptron_3_layer_res.inp_fc", "model.Bottleneck_Perceptron_3_layer_res.hid_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "output", "=", "self", ".", "relu", "(", "self", ".", "inp_fc", "(", "x", ")", ")", "\n", "output", "=", "self", ".", "relu", "(", "self", ".", "hid_fc", "(", "output", ")", ")", "\n", "output", "=", "self", ".", "out_fc", "(", "output", ")", "\n", "\n", "return", "output", "+", "x", "# Residual output", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.Self_Attn_Bot.__init__": [[319, 333], ["torch.Module.__init__", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Softmax", "torch.Softmax", "torch.Parameter", "torch.Parameter", "model.Bottleneck_Perceptron_3_layer_res", "int", "model.PositionalEncoding", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__"], ["def", "__init__", "(", "self", ",", "in_dim", ",", "seq_len", ")", ":", "\n", "        ", "super", "(", "Self_Attn_Bot", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "chanel_in", "=", "in_dim", "# 2048", "\n", "\n", "# Using Linear projections for Key, Query and Value vectors", "\n", "self", ".", "key_proj", "=", "nn", ".", "Linear", "(", "in_dim", ",", "in_dim", ")", "\n", "self", ".", "query_proj", "=", "nn", ".", "Linear", "(", "in_dim", ",", "in_dim", ")", "\n", "self", ".", "value_conv", "=", "nn", ".", "Linear", "(", "in_dim", ",", "in_dim", ")", "\n", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "#", "\n", "self", ".", "gamma", "=", "nn", ".", "Parameter", "(", "torch", ".", "zeros", "(", "1", ")", ")", "\n", "self", ".", "Bot_MLP", "=", "Bottleneck_Perceptron_3_layer_res", "(", "in_dim", ")", "\n", "max_len", "=", "int", "(", "seq_len", "*", "1.5", ")", "\n", "self", ".", "pe", "=", "PositionalEncoding", "(", "in_dim", ",", "0.1", ",", "max_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.Self_Attn_Bot.forward": [[334, 377], ["model.Self_Attn_Bot.pe", "model.Self_Attn_Bot.size", "model.Self_Attn_Bot.query_proj", "model.Self_Attn_Bot.key_proj().permute", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "model.Self_Attn_Bot.softmax", "model.Self_Attn_Bot.value_conv().permute", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "model.Self_Attn_Bot.permute", "model.Self_Attn_Bot.Bot_MLP", "model.Self_Attn_Bot.permute", "model.Self_Attn_Bot.key_proj", "model.Self_Attn_Bot.value_conv"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "\"\"\"\n            inputs :\n                x : input feature maps( B X C X W )[B x 16 x 2048]\n            returns :\n                out : self attention value + input feature \n                attention: B X N X N (N is Width)\n        \"\"\"", "\n", "\n", "# Add a position embedding to the 16 patches", "\n", "x", "=", "self", ".", "pe", "(", "x", ")", "# B x 16 x 2048", "\n", "\n", "m_batchsize", ",", "C", ",", "width", "=", "x", ".", "size", "(", ")", "# m = 200/160, C = 2048, width = 16", "\n", "\n", "# Save residual for later use", "\n", "residual", "=", "x", "# B x 16 x 2048", "\n", "\n", "# Perform query projection", "\n", "proj_query", "=", "self", ".", "query_proj", "(", "x", ")", "# B x 16 x 2048", "\n", "\n", "# Perform Key projection", "\n", "proj_key", "=", "self", ".", "key_proj", "(", "x", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# B x 2048  x 16", "\n", "\n", "energy", "=", "torch", ".", "bmm", "(", "proj_query", ",", "proj_key", ")", "# transpose check B x 16 x 16", "\n", "attention", "=", "self", ".", "softmax", "(", "energy", ")", "#  B x 16 x 16", "\n", "\n", "# Get the entire value in 2048 dimension ", "\n", "proj_value", "=", "self", ".", "value_conv", "(", "x", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# B x 2048 x 16", "\n", "\n", "# Element-wise multiplication of projected value and attention: shape is x B x C x N: 1 x 2048 x 8", "\n", "out", "=", "torch", ".", "bmm", "(", "proj_value", ",", "attention", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", "# B x 2048 x 16", "\n", "\n", "# Reshaping before passing through MLP", "\n", "out", "=", "out", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# B x 16 x 2048", "\n", "\n", "# Passing via gamma attention", "\n", "out", "=", "self", ".", "gamma", "*", "out", "+", "residual", "# B x 16 x 2048", "\n", "\n", "# Pass it via a 3-layer Bottleneck MLP with Residual Layer defined within MLP", "\n", "out", "=", "self", ".", "Bot_MLP", "(", "out", ")", "# B x 16 x 2048", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.MLP_Mix_Enrich.__init__": [[382, 390], ["torch.Module.__init__", "model.Token_Perceptron", "model.Bottleneck_Perceptron_2_layer", "int", "model.PositionalEncoding"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__"], ["def", "__init__", "(", "self", ",", "in_dim", ",", "seq_len", ")", ":", "\n", "        ", "super", "(", "MLP_Mix_Enrich", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# in_dim = 2048", "\n", "self", ".", "Tok_MLP", "=", "Token_Perceptron", "(", "seq_len", ")", "# seq_len = 8 frames", "\n", "self", ".", "Bot_MLP", "=", "Bottleneck_Perceptron_2_layer", "(", "in_dim", ")", "\n", "\n", "max_len", "=", "int", "(", "seq_len", "*", "1.5", ")", "# seq_len = 8", "\n", "self", ".", "pe", "=", "PositionalEncoding", "(", "in_dim", ",", "0.1", ",", "max_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.MLP_Mix_Enrich.forward": [[391, 416], ["model.MLP_Mix_Enrich.pe", "model.MLP_Mix_Enrich.Tok_MLP().permute", "model.MLP_Mix_Enrich.Bot_MLP", "model.MLP_Mix_Enrich.Tok_MLP", "model.MLP_Mix_Enrich.permute"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n            inputs :\n                x : input feature maps( B X C X W ) # B(25/20) x 8 x 2048\n            returns :\n                out : self MLP-enriched value + input feature \n        \"\"\"", "\n", "\n", "# Add a position embedding to the 8 frames", "\n", "x", "=", "self", ".", "pe", "(", "x", ")", "# B x 8 x 2048", "\n", "\n", "# Store the residual for use later", "\n", "residual1", "=", "x", "# B x 8 x 2048", "\n", "\n", "# Pass it via a 2-layer Token MLP followed by Residual Layer", "\n", "# Permuted before passing into the MLP: B x 2048 x 8 ", "\n", "out", "=", "self", ".", "Tok_MLP", "(", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ")", ")", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "+", "residual1", "# B x 8 x 2048", "\n", "\n", "# Storing a residual ", "\n", "residual2", "=", "out", "# B x 8 x 2048", "\n", "\n", "# Pass it via 2-layer Bottleneck MLP defined on Channel(2048) features", "\n", "out", "=", "self", ".", "Bot_MLP", "(", "out", ")", "+", "residual2", "# B x 8 x 2048", "\n", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.CNN_STRM.__init__": [[423, 454], ["torch.Module.__init__", "model.CNN_STRM.train", "torch.Sequential", "torch.Sequential", "torch.AdaptiveMaxPool2d", "torch.AdaptiveMaxPool2d", "torch.ModuleList", "torch.ModuleList", "model.Self_Attn_Bot", "model.MLP_Mix_Enrich", "torchvision.resnet18", "model.DistanceLoss", "torchvision.resnet34", "model.TemporalCrossTransformer", "torchvision.resnet50", "list", "torchvision.resnet50.children"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__"], ["def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "CNN_STRM", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "train", "(", ")", "\n", "self", ".", "args", "=", "args", "\n", "\n", "# Using ResNet Backbone", "\n", "if", "self", ".", "args", ".", "method", "==", "\"resnet18\"", ":", "\n", "            ", "resnet", "=", "models", ".", "resnet18", "(", "pretrained", "=", "True", ")", "\n", "", "elif", "self", ".", "args", ".", "method", "==", "\"resnet34\"", ":", "\n", "            ", "resnet", "=", "models", ".", "resnet34", "(", "pretrained", "=", "True", ")", "\n", "", "elif", "self", ".", "args", ".", "method", "==", "\"resnet50\"", ":", "\n", "            ", "resnet", "=", "models", ".", "resnet50", "(", "pretrained", "=", "True", ")", "\n", "\n", "", "last_layer_idx", "=", "-", "2", "\n", "self", ".", "resnet", "=", "nn", ".", "Sequential", "(", "*", "list", "(", "resnet", ".", "children", "(", ")", ")", "[", ":", "last_layer_idx", "]", ")", "\n", "self", ".", "num_patches", "=", "16", "\n", "\n", "self", ".", "adap_max", "=", "nn", ".", "AdaptiveMaxPool2d", "(", "(", "4", ",", "4", ")", ")", "\n", "\n", "# Temporal Cross Transformer for modelling temporal relations", "\n", "self", ".", "transformers", "=", "nn", ".", "ModuleList", "(", "[", "TemporalCrossTransformer", "(", "args", ",", "s", ")", "for", "s", "in", "args", ".", "temp_set", "]", ")", "\n", "\n", "# New-distance metric for post patch-level enriched features", "\n", "self", ".", "new_dist_loss_post_pat", "=", "[", "DistanceLoss", "(", "args", ",", "s", ")", "for", "s", "in", "args", ".", "temp_set", "]", "\n", "\n", "# Linear-based patch-level attention over the 16 patches", "\n", "self", ".", "attn_pat", "=", "Self_Attn_Bot", "(", "self", ".", "args", ".", "trans_linear_in_dim", ",", "self", ".", "num_patches", ")", "\n", "\n", "# MLP-mixing frame-level enrichment over the 8 frames.", "\n", "self", ".", "fr_enrich", "=", "MLP_Mix_Enrich", "(", "self", ".", "args", ".", "trans_linear_in_dim", ",", "self", ".", "args", ".", "seq_len", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.CNN_STRM.forward": [[455, 517], ["model.CNN_STRM.resnet", "model.CNN_STRM.resnet", "model.CNN_STRM.adap_max", "model.CNN_STRM.adap_max", "context_features.reshape.reshape.reshape", "target_features.reshape.reshape.reshape", "context_features.reshape.reshape.permute", "target_features.reshape.reshape.permute", "model.CNN_STRM.attn_pat", "model.CNN_STRM.attn_pat", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "context_features.reshape.reshape.reshape", "target_features.reshape.reshape.reshape", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "model.CNN_STRM.fr_enrich", "model.CNN_STRM.fr_enrich", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.mean", "torch.mean", "torch.mean", "torch.mean", "utils.split_first_dim_linear", "utils.split_first_dim_linear", "n", "t"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.split_first_dim_linear", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.split_first_dim_linear"], ["", "def", "forward", "(", "self", ",", "context_images", ",", "context_labels", ",", "target_images", ")", ":", "\n", "\n", "        ", "'''\n            context_features/target_features is of shape (num_images x 2048) [final Resnet FC layer] after squeezing\n        '''", "\n", "'''\n            context_images: 200 x 3 x 224 x 224, target_images = 160 x 3 x 224 x 224\n        '''", "\n", "context_features", "=", "self", ".", "resnet", "(", "context_images", ")", "# 200 x 2048 x 7 x 7", "\n", "target_features", "=", "self", ".", "resnet", "(", "target_images", ")", "# 160 x 2048 x 7 x 7", "\n", "\n", "# Decrease to 4 x 4 = 16 patches", "\n", "context_features", "=", "self", ".", "adap_max", "(", "context_features", ")", "# 200 x 2048 x 4 x 4", "\n", "target_features", "=", "self", ".", "adap_max", "(", "target_features", ")", "# 160 x 2048 x 4 x 4", "\n", "\n", "# Reshape before averaging across all the patches", "\n", "context_features", "=", "context_features", ".", "reshape", "(", "-", "1", ",", "self", ".", "args", ".", "trans_linear_in_dim", ",", "self", ".", "num_patches", ")", "# 200 x 2048 x 16", "\n", "target_features", "=", "target_features", ".", "reshape", "(", "-", "1", ",", "self", ".", "args", ".", "trans_linear_in_dim", ",", "self", ".", "num_patches", ")", "# 160 x 2048 x 16       ", "\n", "\n", "# Permute before passing to the self-attention layer", "\n", "context_features", "=", "context_features", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# 200 x 16 x 2048", "\n", "target_features", "=", "target_features", ".", "permute", "(", "0", ",", "2", ",", "1", ")", "# 160 x 16 x 2048", "\n", "\n", "# Performing self-attention across the 16 patches", "\n", "context_features", "=", "self", ".", "attn_pat", "(", "context_features", ")", "# 200 x 16 x 2048 ", "\n", "target_features", "=", "self", ".", "attn_pat", "(", "target_features", ")", "# 160 x 16 x 2048", "\n", "\n", "# Average across the patches ", "\n", "context_features", "=", "torch", ".", "mean", "(", "context_features", ",", "dim", "=", "1", ")", "# 200 x 2048", "\n", "target_features", "=", "torch", ".", "mean", "(", "target_features", ",", "dim", "=", "1", ")", "# 160 x 2048", "\n", "\n", "# Reshaping before passing to the Cross-Transformer and computing the distance after patch-enrichment as well", "\n", "context_features", "=", "context_features", ".", "reshape", "(", "-", "1", ",", "self", ".", "args", ".", "seq_len", ",", "self", ".", "args", ".", "trans_linear_in_dim", ")", "# 25 x 8 x 2048", "\n", "target_features", "=", "target_features", ".", "reshape", "(", "-", "1", ",", "self", ".", "args", ".", "seq_len", ",", "self", ".", "args", ".", "trans_linear_in_dim", ")", "# 20 x 8 x 2048", "\n", "\n", "# Compute logits using the new loss before applying frame-level attention", "\n", "all_logits_post_pat", "=", "[", "n", "(", "context_features", ",", "context_labels", ",", "target_features", ")", "[", "'logits'", "]", "for", "n", "in", "self", ".", "new_dist_loss_post_pat", "]", "\n", "all_logits_post_pat", "=", "torch", ".", "stack", "(", "all_logits_post_pat", ",", "dim", "=", "-", "1", ")", "# 20 x 5 x 1[number of timesteps] 20 - 5 x 4[5-way x 4 queries/class]", "\n", "\n", "# Combing the patch and frame-level logits", "\n", "sample_logits_post_pat", "=", "all_logits_post_pat", "\n", "sample_logits_post_pat", "=", "torch", ".", "mean", "(", "sample_logits_post_pat", ",", "dim", "=", "[", "-", "1", "]", ")", "# 20 x 5", "\n", "\n", "# Perform self-attention across the 8 frames", "\n", "context_features_fr", "=", "self", ".", "fr_enrich", "(", "context_features", ")", "# 25 x 8 x 2048", "\n", "target_features_fr", "=", "self", ".", "fr_enrich", "(", "target_features", ")", "# 20 x 8 x 2048", "\n", "\n", "'''\n            For different temporal lengths(2, 3, ...) get the final logits and perform mean.\n        '''", "\n", "\n", "# Frame-level logits", "\n", "all_logits_fr", "=", "[", "t", "(", "context_features_fr", ",", "context_labels", ",", "target_features_fr", ")", "[", "'logits'", "]", "for", "t", "in", "self", ".", "transformers", "]", "\n", "all_logits_fr", "=", "torch", ".", "stack", "(", "all_logits_fr", ",", "dim", "=", "-", "1", ")", "# 20 x 5 x 1[number of timesteps] 20 - 5 x 4[5-way x 4 queries/class]", "\n", "\n", "sample_logits_fr", "=", "all_logits_fr", "\n", "sample_logits_fr", "=", "torch", ".", "mean", "(", "sample_logits_fr", ",", "dim", "=", "[", "-", "1", "]", ")", "# 20 x 5", "\n", "\n", "return_dict", "=", "{", "'logits'", ":", "split_first_dim_linear", "(", "sample_logits_fr", ",", "[", "NUM_SAMPLES", ",", "target_features", ".", "shape", "[", "0", "]", "]", ")", ",", "\n", "'logits_post_pat'", ":", "split_first_dim_linear", "(", "sample_logits_post_pat", ",", "[", "NUM_SAMPLES", ",", "target_features", ".", "shape", "[", "0", "]", "]", ")", "}", "\n", "\n", "return", "return_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.model.CNN_STRM.distribute_model": [[518, 535], ["model.CNN_STRM.resnet.cuda", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "model.CNN_STRM.transformers.cuda", "model.CNN_STRM.attn_pat.cuda", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "model.CNN_STRM.fr_enrich.cuda", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "torch.nn.DataParallel", "n.cuda", "range", "range", "range"], "methods", ["None"], ["", "def", "distribute_model", "(", "self", ")", ":", "\n", "        ", "\"\"\"\n        Distributes the CNNs over multiple GPUs.\n        :return: Nothing\n        \"\"\"", "\n", "if", "self", ".", "args", ".", "num_gpus", ">", "1", ":", "\n", "            ", "self", ".", "resnet", ".", "cuda", "(", "0", ")", "\n", "self", ".", "resnet", "=", "torch", ".", "nn", ".", "DataParallel", "(", "self", ".", "resnet", ",", "device_ids", "=", "[", "i", "for", "i", "in", "range", "(", "0", ",", "self", ".", "args", ".", "num_gpus", ")", "]", ")", "\n", "\n", "self", ".", "transformers", ".", "cuda", "(", "0", ")", "\n", "self", ".", "new_dist_loss_post_pat", "=", "[", "n", ".", "cuda", "(", "0", ")", "for", "n", "in", "self", ".", "new_dist_loss_post_pat", "]", "\n", "\n", "self", ".", "attn_pat", ".", "cuda", "(", "0", ")", "\n", "self", ".", "attn_pat", "=", "torch", ".", "nn", ".", "DataParallel", "(", "self", ".", "attn_pat", ",", "device_ids", "=", "[", "i", "for", "i", "in", "range", "(", "0", ",", "self", ".", "args", ".", "num_gpus", ")", "]", ")", "\n", "\n", "self", ".", "fr_enrich", ".", "cuda", "(", "0", ")", "\n", "self", ".", "fr_enrich", "=", "torch", ".", "nn", ".", "DataParallel", "(", "self", ".", "fr_enrich", ",", "device_ids", "=", "[", "i", "for", "i", "in", "range", "(", "0", ",", "self", ".", "args", ".", "num_gpus", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.__init__": [[54, 87], ["run.Learner.parse_command_line", "utils.get_log_files", "utils.print_and_log", "utils.print_and_log", "torch.device", "run.Learner.init_model", "run.Learner.init_data", "video_reader.VideoDataset", "torch.utils.data.DataLoader", "utils.TestAccuracies", "torch.optim.lr_scheduler.MultiStepLR", "run.Learner.optimizer.zero_grad", "torch.optim.Adam", "run.Learner.load_checkpoint", "torch.cuda.is_available", "run.Learner.model.parameters", "torch.optim.SGD", "run.Learner.model.parameters"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.parse_command_line", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.get_log_files", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.print_and_log", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.print_and_log", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.init_model", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.init_data", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.load_checkpoint"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "args", "=", "self", ".", "parse_command_line", "(", ")", "\n", "\n", "self", ".", "checkpoint_dir", ",", "self", ".", "logfile", ",", "self", ".", "checkpoint_path_validation", ",", "self", ".", "checkpoint_path_final", "=", "get_log_files", "(", "self", ".", "args", ".", "checkpoint_dir", ",", "self", ".", "args", ".", "resume_from_checkpoint", ",", "False", ")", "\n", "\n", "print_and_log", "(", "self", ".", "logfile", ",", "\"Options: %s\\n\"", "%", "self", ".", "args", ")", "\n", "print_and_log", "(", "self", ".", "logfile", ",", "\"Checkpoint Directory: %s\\n\"", "%", "self", ".", "checkpoint_dir", ")", "\n", "\n", "\n", "gpu_device", "=", "'cuda'", "\n", "self", ".", "device", "=", "torch", ".", "device", "(", "gpu_device", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", ")", "\n", "self", ".", "model", "=", "self", ".", "init_model", "(", ")", "\n", "self", ".", "train_set", ",", "self", ".", "validation_set", ",", "self", ".", "test_set", "=", "self", ".", "init_data", "(", ")", "\n", "\n", "self", ".", "vd", "=", "video_reader", ".", "VideoDataset", "(", "self", ".", "args", ")", "\n", "self", ".", "video_loader", "=", "torch", ".", "utils", ".", "data", ".", "DataLoader", "(", "self", ".", "vd", ",", "batch_size", "=", "1", ",", "num_workers", "=", "self", ".", "args", ".", "num_workers", ")", "\n", "\n", "self", ".", "loss", "=", "loss", "\n", "self", ".", "accuracy_fn", "=", "aggregate_accuracy", "\n", "\n", "if", "self", ".", "args", ".", "opt", "==", "\"adam\"", ":", "\n", "            ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "Adam", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "args", ".", "learning_rate", ")", "\n", "", "elif", "self", ".", "args", ".", "opt", "==", "\"sgd\"", ":", "\n", "            ", "self", ".", "optimizer", "=", "torch", ".", "optim", ".", "SGD", "(", "self", ".", "model", ".", "parameters", "(", ")", ",", "lr", "=", "self", ".", "args", ".", "learning_rate", ")", "\n", "", "self", ".", "test_accuracies", "=", "TestAccuracies", "(", "self", ".", "test_set", ")", "\n", "\n", "self", ".", "scheduler", "=", "MultiStepLR", "(", "self", ".", "optimizer", ",", "milestones", "=", "self", ".", "args", ".", "sch", ",", "gamma", "=", "0.1", ")", "\n", "\n", "self", ".", "start_iteration", "=", "0", "\n", "if", "self", ".", "args", ".", "resume_from_checkpoint", ":", "\n", "            ", "self", ".", "load_checkpoint", "(", ")", "\n", "", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.init_model": [[88, 94], ["model.to.CNN_STRM", "model.to.to.to", "model.to.to.distribute_model"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.model.CNN_STRM.distribute_model"], ["", "def", "init_model", "(", "self", ")", ":", "\n", "        ", "model", "=", "CNN_STRM", "(", "self", ".", "args", ")", "\n", "model", "=", "model", ".", "to", "(", "self", ".", "device", ")", "\n", "if", "self", ".", "args", ".", "num_gpus", ">", "1", ":", "\n", "            ", "model", ".", "distribute_model", "(", ")", "\n", "", "return", "model", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.init_data": [[95, 100], ["None"], "methods", ["None"], ["", "def", "init_data", "(", "self", ")", ":", "\n", "        ", "train_set", "=", "[", "self", ".", "args", ".", "dataset", "]", "\n", "validation_set", "=", "[", "self", ".", "args", ".", "dataset", "]", "\n", "test_set", "=", "[", "self", ".", "args", ".", "dataset", "]", "\n", "return", "train_set", ",", "validation_set", ",", "test_set", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.parse_command_line": [[105, 179], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "exit", "os.path.join", "os.path.join", "open", "pickle.dump", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print"], ["def", "parse_command_line", "(", "self", ")", ":", "\n", "        ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "choices", "=", "[", "\"ssv2\"", ",", "\"kinetics\"", ",", "\"hmdb\"", ",", "\"ucf\"", "]", ",", "default", "=", "\"ssv2\"", ",", "help", "=", "\"Dataset to use.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--learning_rate\"", ",", "\"-lr\"", ",", "type", "=", "float", ",", "default", "=", "0.001", ",", "help", "=", "\"Learning rate.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tasks_per_batch\"", ",", "type", "=", "int", ",", "default", "=", "16", ",", "help", "=", "\"Number of tasks between parameter optimizations.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--checkpoint_dir\"", ",", "\"-c\"", ",", "default", "=", "None", ",", "help", "=", "\"Directory to save checkpoint to.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--test_model_path\"", ",", "\"-m\"", ",", "default", "=", "None", ",", "help", "=", "\"Path to model to load and test.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--training_iterations\"", ",", "\"-i\"", ",", "type", "=", "int", ",", "default", "=", "100020", ",", "help", "=", "\"Number of meta-training iterations.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--resume_from_checkpoint\"", ",", "\"-r\"", ",", "dest", "=", "\"resume_from_checkpoint\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Restart from latest checkpoint.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--way\"", ",", "type", "=", "int", ",", "default", "=", "5", ",", "help", "=", "\"Way of each task.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--shot\"", ",", "type", "=", "int", ",", "default", "=", "5", ",", "help", "=", "\"Shots per class.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--query_per_class\"", ",", "type", "=", "int", ",", "default", "=", "5", ",", "help", "=", "\"Target samples (i.e. queries) per class used for training.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--query_per_class_test\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Target samples (i.e. queries) per class used for testing.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--test_iters'", ",", "nargs", "=", "'+'", ",", "type", "=", "int", ",", "help", "=", "'iterations to test at. Default is for ssv2 otam split.'", ",", "default", "=", "[", "75000", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_test_tasks\"", ",", "type", "=", "int", ",", "default", "=", "10000", ",", "help", "=", "\"number of random tasks to test on.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--print_freq\"", ",", "type", "=", "int", ",", "default", "=", "1000", ",", "help", "=", "\"print and log every n iterations.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--seq_len\"", ",", "type", "=", "int", ",", "default", "=", "8", ",", "help", "=", "\"Frames per video.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_workers\"", ",", "type", "=", "int", ",", "default", "=", "10", ",", "help", "=", "\"Num dataloader workers.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--method\"", ",", "choices", "=", "[", "\"resnet18\"", ",", "\"resnet34\"", ",", "\"resnet50\"", "]", ",", "default", "=", "\"resnet50\"", ",", "help", "=", "\"method\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--trans_linear_out_dim\"", ",", "type", "=", "int", ",", "default", "=", "1152", ",", "help", "=", "\"Transformer linear_out_dim\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--opt\"", ",", "choices", "=", "[", "\"adam\"", ",", "\"sgd\"", "]", ",", "default", "=", "\"sgd\"", ",", "help", "=", "\"Optimizer\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--trans_dropout\"", ",", "type", "=", "int", ",", "default", "=", "0.1", ",", "help", "=", "\"Transformer dropout\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_freq\"", ",", "type", "=", "int", ",", "default", "=", "5000", ",", "help", "=", "\"Number of iterations between checkpoint saves.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--img_size\"", ",", "type", "=", "int", ",", "default", "=", "224", ",", "help", "=", "\"Input image size to the CNN after cropping.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--temp_set'", ",", "nargs", "=", "'+'", ",", "type", "=", "int", ",", "help", "=", "'cardinalities e.g. 2,3 is pairs and triples'", ",", "default", "=", "[", "2", ",", "3", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--scratch\"", ",", "choices", "=", "[", "\"bc\"", ",", "\"bp\"", ",", "\"new\"", "]", ",", "default", "=", "\"bp\"", ",", "help", "=", "\"directory containing dataset, splits, and checkpoint saves.\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_gpus\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "help", "=", "\"Number of GPUs to split the ResNet over\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--debug_loader\"", ",", "default", "=", "False", ",", "action", "=", "\"store_true\"", ",", "help", "=", "\"Load 1 vid per class for debugging\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--split\"", ",", "type", "=", "int", ",", "default", "=", "7", ",", "help", "=", "\"Dataset split.\"", ")", "\n", "parser", ".", "add_argument", "(", "'--sch'", ",", "nargs", "=", "'+'", ",", "type", "=", "int", ",", "help", "=", "'iters to drop learning rate'", ",", "default", "=", "[", "1000000", "]", ")", "\n", "parser", ".", "add_argument", "(", "\"--test_model_only\"", ",", "type", "=", "bool", ",", "default", "=", "False", ",", "help", "=", "\"Only testing the model from the given checkpoint\"", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "\n", "if", "args", ".", "scratch", "==", "\"bc\"", ":", "\n", "            ", "args", ".", "scratch", "=", "\"/mnt/storage/home2/tp8961/scratch\"", "\n", "", "elif", "args", ".", "scratch", "==", "\"bp\"", ":", "\n", "            ", "args", ".", "num_gpus", "=", "4", "\n", "# this is low becuase of RAM constraints for the data loader", "\n", "args", ".", "num_workers", "=", "3", "\n", "args", ".", "scratch", "=", "\"/work/tp8961\"", "\n", "", "elif", "args", ".", "scratch", "==", "\"new\"", ":", "\n", "            ", "args", ".", "scratch", "=", "\"./imp_datasets/\"", "\n", "\n", "", "if", "args", ".", "checkpoint_dir", "==", "None", ":", "\n", "            ", "print", "(", "\"need to specify a checkpoint dir\"", ")", "\n", "exit", "(", "1", ")", "\n", "\n", "", "if", "(", "args", ".", "method", "==", "\"resnet50\"", ")", "or", "(", "args", ".", "method", "==", "\"resnet34\"", ")", ":", "\n", "            ", "args", ".", "img_size", "=", "224", "\n", "", "if", "args", ".", "method", "==", "\"resnet50\"", ":", "\n", "            ", "args", ".", "trans_linear_in_dim", "=", "2048", "\n", "", "else", ":", "\n", "            ", "args", ".", "trans_linear_in_dim", "=", "512", "\n", "\n", "", "if", "args", ".", "dataset", "==", "\"ssv2\"", ":", "\n", "            ", "args", ".", "traintestlist", "=", "os", ".", "path", ".", "join", "(", "args", ".", "scratch", ",", "\"video_datasets/splits/somethingsomethingv2TrainTestlist\"", ")", "\n", "args", ".", "path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "scratch", ",", "\"video_datasets/data/somethingsomethingv2_256x256q5_7l8.zip\"", ")", "\n", "", "elif", "args", ".", "dataset", "==", "\"kinetics\"", ":", "\n", "            ", "args", ".", "traintestlist", "=", "os", ".", "path", ".", "join", "(", "args", ".", "scratch", ",", "\"video_datasets/splits/kineticsTrainTestlist\"", ")", "\n", "args", ".", "path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "scratch", ",", "\"video_datasets/data/kinetics_256q5_1.zip\"", ")", "\n", "", "elif", "args", ".", "dataset", "==", "\"ucf\"", ":", "\n", "            ", "args", ".", "traintestlist", "=", "os", ".", "path", ".", "join", "(", "args", ".", "scratch", ",", "\"video_datasets/splits/ucfTrainTestlist\"", ")", "\n", "args", ".", "path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "scratch", ",", "\"video_datasets/data/UCF-101_320.zip\"", ")", "\n", "", "elif", "args", ".", "dataset", "==", "\"hmdb\"", ":", "\n", "            ", "args", ".", "traintestlist", "=", "os", ".", "path", ".", "join", "(", "args", ".", "scratch", ",", "\"video_datasets/splits/hmdb51TrainTestlist\"", ")", "\n", "args", ".", "path", "=", "os", ".", "path", ".", "join", "(", "args", ".", "scratch", ",", "\"video_datasets/data/hmdb51_256q5.zip\"", ")", "\n", "# args.path = os.path.join(args.scratch, \"video_datasets/data/hmdb51_jpegs_256.zip\")", "\n", "\n", "", "with", "open", "(", "\"args.pkl\"", ",", "\"wb\"", ")", "as", "f", ":", "\n", "            ", "pickle", ".", "dump", "(", "args", ",", "f", ",", "pickle", ".", "HIGHEST_PROTOCOL", ")", "\n", "\n", "", "return", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.run": [[180, 239], ["tensorflow.compat.v1.ConfigProto", "run.Learner.logfile.close", "tensorflow.compat.v1.Session", "torch.save", "print", "run.Learner.load_checkpoint", "run.Learner.test", "print", "torch.set_grad_enabled", "run.Learner.train_task", "train_accuracies.append", "losses.append", "run.Learner.scheduler.step", "run.Learner.model.state_dict", "run.Learner.optimizer.step", "run.Learner.optimizer.zero_grad", "utils.print_and_log", "train_logger.info", "torch.Tensor().mean().item", "torch.Tensor().mean().item", "run.Learner.save_checkpoint", "run.Learner.test", "print", "run.Learner.test_accuracies.print", "torch.Tensor().mean().item", "torch.Tensor().mean().item", "torch.Tensor().mean().item", "torch.Tensor().mean().item", "torch.Tensor().mean", "torch.Tensor().mean", "torch.Tensor().mean", "torch.Tensor().mean", "torch.Tensor().mean", "torch.Tensor().mean", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor", "torch.Tensor"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.load_checkpoint", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.test", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.train_task", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.print_and_log", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.save_checkpoint", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.test", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print"], ["", "def", "run", "(", "self", ")", ":", "\n", "        ", "config", "=", "tf", ".", "compat", ".", "v1", ".", "ConfigProto", "(", ")", "\n", "config", ".", "gpu_options", ".", "allow_growth", "=", "True", "\n", "with", "tf", ".", "compat", ".", "v1", ".", "Session", "(", "config", "=", "config", ")", "as", "session", ":", "\n", "                ", "train_accuracies", "=", "[", "]", "\n", "losses", "=", "[", "]", "\n", "total_iterations", "=", "self", ".", "args", ".", "training_iterations", "\n", "\n", "iteration", "=", "self", ".", "start_iteration", "\n", "\n", "if", "self", ".", "args", ".", "test_model_only", ":", "\n", "                    ", "print", "(", "\"Model being tested at path: \"", "+", "self", ".", "args", ".", "test_model_path", ")", "\n", "self", ".", "load_checkpoint", "(", ")", "\n", "accuracy_dict", "=", "self", ".", "test", "(", "session", ",", "1", ")", "\n", "print", "(", "accuracy_dict", ")", "\n", "\n", "\n", "", "for", "task_dict", "in", "self", ".", "video_loader", ":", "\n", "                    ", "if", "iteration", ">=", "total_iterations", ":", "\n", "                        ", "break", "\n", "", "iteration", "+=", "1", "\n", "torch", ".", "set_grad_enabled", "(", "True", ")", "\n", "\n", "task_loss", ",", "task_accuracy", "=", "self", ".", "train_task", "(", "task_dict", ")", "\n", "train_accuracies", ".", "append", "(", "task_accuracy", ")", "\n", "losses", ".", "append", "(", "task_loss", ")", "\n", "\n", "# optimize", "\n", "if", "(", "(", "iteration", "+", "1", ")", "%", "self", ".", "args", ".", "tasks_per_batch", "==", "0", ")", "or", "(", "iteration", "==", "(", "total_iterations", "-", "1", ")", ")", ":", "\n", "                        ", "self", ".", "optimizer", ".", "step", "(", ")", "\n", "self", ".", "optimizer", ".", "zero_grad", "(", ")", "\n", "", "self", ".", "scheduler", ".", "step", "(", ")", "\n", "if", "(", "iteration", "+", "1", ")", "%", "self", ".", "args", ".", "print_freq", "==", "0", ":", "\n", "# print training stats", "\n", "                        ", "print_and_log", "(", "self", ".", "logfile", ",", "'Task [{}/{}], Train Loss: {:.7f}, Train Accuracy: {:.7f}'", "\n", ".", "format", "(", "iteration", "+", "1", ",", "total_iterations", ",", "torch", ".", "Tensor", "(", "losses", ")", ".", "mean", "(", ")", ".", "item", "(", ")", ",", "\n", "torch", ".", "Tensor", "(", "train_accuracies", ")", ".", "mean", "(", ")", ".", "item", "(", ")", ")", ")", "\n", "train_logger", ".", "info", "(", "\"For Task: {0}, the training loss is {1} and Training Accuracy is {2}\"", ".", "format", "(", "iteration", "+", "1", ",", "torch", ".", "Tensor", "(", "losses", ")", ".", "mean", "(", ")", ".", "item", "(", ")", ",", "\n", "torch", ".", "Tensor", "(", "train_accuracies", ")", ".", "mean", "(", ")", ".", "item", "(", ")", ")", ")", "\n", "\n", "avg_train_acc", "=", "torch", ".", "Tensor", "(", "train_accuracies", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "avg_train_loss", "=", "torch", ".", "Tensor", "(", "losses", ")", ".", "mean", "(", ")", ".", "item", "(", ")", "\n", "\n", "train_accuracies", "=", "[", "]", "\n", "losses", "=", "[", "]", "\n", "\n", "", "if", "(", "(", "iteration", "+", "1", ")", "%", "self", ".", "args", ".", "save_freq", "==", "0", ")", "and", "(", "iteration", "+", "1", ")", "!=", "total_iterations", ":", "\n", "                        ", "self", ".", "save_checkpoint", "(", "iteration", "+", "1", ")", "\n", "\n", "\n", "", "if", "(", "(", "iteration", "+", "1", ")", "in", "self", ".", "args", ".", "test_iters", ")", "and", "(", "iteration", "+", "1", ")", "!=", "total_iterations", ":", "\n", "                        ", "accuracy_dict", "=", "self", ".", "test", "(", "session", ",", "iteration", "+", "1", ")", "\n", "print", "(", "accuracy_dict", ")", "\n", "self", ".", "test_accuracies", ".", "print", "(", "self", ".", "logfile", ",", "accuracy_dict", ")", "\n", "\n", "# save the final model", "\n", "", "", "torch", ".", "save", "(", "self", ".", "model", ".", "state_dict", "(", ")", ",", "self", ".", "checkpoint_path_final", ")", "\n", "\n", "", "self", ".", "logfile", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.train_task": [[240, 269], ["run.Learner.prepare_task", "context_images.to.to.to", "context_labels.to.to.to", "target_images.to.to.to", "run.Learner.model", "model_dict[].to", "model_dict[].to", "target_labels.to.to.to", "run.Learner.accuracy_fn", "task_loss.backward", "run.Learner.loss", "run.Learner.loss"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.prepare_task", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.loss", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.loss"], ["", "def", "train_task", "(", "self", ",", "task_dict", ")", ":", "\n", "        ", "context_images", ",", "target_images", ",", "context_labels", ",", "target_labels", ",", "real_target_labels", ",", "batch_class_list", "=", "self", ".", "prepare_task", "(", "task_dict", ")", "\n", "\n", "context_images", "=", "context_images", ".", "to", "(", "self", ".", "device", ")", "\n", "context_labels", "=", "context_labels", ".", "to", "(", "self", ".", "device", ")", "\n", "target_images", "=", "target_images", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "model_dict", "=", "self", ".", "model", "(", "context_images", ",", "context_labels", ",", "target_images", ")", "\n", "target_logits", "=", "model_dict", "[", "'logits'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Target logits after applying query-distance-based similarity metric on patch-level enriched features", "\n", "target_logits_post_pat", "=", "model_dict", "[", "'logits_post_pat'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "target_labels", "=", "target_labels", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "task_loss", "=", "self", ".", "loss", "(", "target_logits", ",", "target_labels", ",", "self", ".", "device", ")", "/", "self", ".", "args", ".", "tasks_per_batch", "\n", "task_loss_post_pat", "=", "self", ".", "loss", "(", "target_logits_post_pat", ",", "target_labels", ",", "self", ".", "device", ")", "/", "self", ".", "args", ".", "tasks_per_batch", "\n", "\n", "# Joint loss", "\n", "task_loss", "=", "task_loss", "+", "0.1", "*", "task_loss_post_pat", "\n", "\n", "# Add the logits before computing the accuracy", "\n", "target_logits", "=", "target_logits", "+", "0.1", "*", "target_logits_post_pat", "\n", "\n", "task_accuracy", "=", "self", ".", "accuracy_fn", "(", "target_logits", ",", "target_labels", ")", "\n", "\n", "task_loss", ".", "backward", "(", "retain_graph", "=", "False", ")", "\n", "\n", "return", "task_loss", ",", "task_accuracy", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.test": [[270, 324], ["run.Learner.model.eval", "run.Learner.model.train", "torch.no_grad", "numpy.array().mean", "eval_logger.info", "run.Learner.prepare_task", "run.Learner.model", "model_dict[].to", "model_dict[].to", "target_labels.to.to.to", "run.Learner.accuracy_fn", "eval_logger.info", "losses.append", "accuracies.append", "numpy.array().mean", "numpy.sqrt", "run.Learner.loss", "run.Learner.loss", "numpy.array().mean.item", "run.Learner.item", "numpy.array().std", "len", "numpy.array", "numpy.array().mean.item", "run.Learner.item", "numpy.array", "numpy.array"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.prepare_task", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.loss", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.loss"], ["", "def", "test", "(", "self", ",", "session", ",", "num_episode", ")", ":", "\n", "        ", "self", ".", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "\n", "                ", "self", ".", "video_loader", ".", "dataset", ".", "train", "=", "False", "\n", "accuracy_dict", "=", "{", "}", "\n", "accuracies", "=", "[", "]", "\n", "losses", "=", "[", "]", "\n", "iteration", "=", "0", "\n", "item", "=", "self", ".", "args", ".", "dataset", "\n", "for", "task_dict", "in", "self", ".", "video_loader", ":", "\n", "                    ", "if", "iteration", ">=", "self", ".", "args", ".", "num_test_tasks", ":", "\n", "                        ", "break", "\n", "", "iteration", "+=", "1", "\n", "\n", "context_images", ",", "target_images", ",", "context_labels", ",", "target_labels", ",", "real_target_labels", ",", "batch_class_list", "=", "self", ".", "prepare_task", "(", "task_dict", ")", "\n", "model_dict", "=", "self", ".", "model", "(", "context_images", ",", "context_labels", ",", "target_images", ")", "\n", "target_logits", "=", "model_dict", "[", "'logits'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Target logits after applying query-distance-based similarity metric on patch-level enriched features   ", "\n", "target_logits_post_pat", "=", "model_dict", "[", "'logits_post_pat'", "]", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "target_labels", "=", "target_labels", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "# Add the logits before computing the accuracy", "\n", "target_logits", "=", "target_logits", "+", "0.1", "*", "target_logits_post_pat", "\n", "\n", "accuracy", "=", "self", ".", "accuracy_fn", "(", "target_logits", ",", "target_labels", ")", "\n", "\n", "loss", "=", "self", ".", "loss", "(", "target_logits", ",", "target_labels", ",", "self", ".", "device", ")", "/", "self", ".", "args", ".", "num_test_tasks", "\n", "\n", "# Loss using the new distance metric after  patch-level enrichment", "\n", "loss_post_pat", "=", "self", ".", "loss", "(", "target_logits_post_pat", ",", "target_labels", ",", "self", ".", "device", ")", "/", "self", ".", "args", ".", "num_test_tasks", "\n", "\n", "# Joint loss", "\n", "loss", "=", "loss", "+", "0.1", "*", "loss_post_pat", "\n", "\n", "eval_logger", ".", "info", "(", "\"For Task: {0}, the testing loss is {1} and Testing Accuracy is {2}\"", ".", "format", "(", "iteration", "+", "1", ",", "loss", ".", "item", "(", ")", ",", "\n", "accuracy", ".", "item", "(", ")", ")", ")", "\n", "losses", ".", "append", "(", "loss", ".", "item", "(", ")", ")", "\n", "accuracies", ".", "append", "(", "accuracy", ".", "item", "(", ")", ")", "\n", "del", "target_logits", "\n", "del", "target_logits_post_pat", "\n", "\n", "", "accuracy", "=", "np", ".", "array", "(", "accuracies", ")", ".", "mean", "(", ")", "*", "100.0", "\n", "confidence", "=", "(", "196.0", "*", "np", ".", "array", "(", "accuracies", ")", ".", "std", "(", ")", ")", "/", "np", ".", "sqrt", "(", "len", "(", "accuracies", ")", ")", "\n", "loss", "=", "np", ".", "array", "(", "losses", ")", ".", "mean", "(", ")", "\n", "accuracy_dict", "[", "item", "]", "=", "{", "\"accuracy\"", ":", "accuracy", ",", "\"confidence\"", ":", "confidence", ",", "\"loss\"", ":", "loss", "}", "\n", "eval_logger", ".", "info", "(", "\"For Task: {0}, the testing loss is {1} and Testing Accuracy is {2}\"", ".", "format", "(", "num_episode", ",", "loss", ",", "accuracy", ")", ")", "\n", "\n", "self", ".", "video_loader", ".", "dataset", ".", "train", "=", "True", "\n", "", "self", ".", "model", ".", "train", "(", ")", "\n", "\n", "return", "accuracy_dict", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.prepare_task": [[326, 339], ["context_labels.to.to.to", "target_labels.type().to.type().to.type().to", "context_images.to.to.to", "target_images.to.to.to", "target_labels.type().to.type().to.type"], "methods", ["None"], ["", "def", "prepare_task", "(", "self", ",", "task_dict", ",", "images_to_device", "=", "True", ")", ":", "\n", "        ", "context_images", ",", "context_labels", "=", "task_dict", "[", "'support_set'", "]", "[", "0", "]", ",", "task_dict", "[", "'support_labels'", "]", "[", "0", "]", "\n", "target_images", ",", "target_labels", "=", "task_dict", "[", "'target_set'", "]", "[", "0", "]", ",", "task_dict", "[", "'target_labels'", "]", "[", "0", "]", "\n", "real_target_labels", "=", "task_dict", "[", "'real_target_labels'", "]", "[", "0", "]", "\n", "batch_class_list", "=", "task_dict", "[", "'batch_class_list'", "]", "[", "0", "]", "\n", "\n", "if", "images_to_device", ":", "\n", "            ", "context_images", "=", "context_images", ".", "to", "(", "self", ".", "device", ")", "\n", "target_images", "=", "target_images", ".", "to", "(", "self", ".", "device", ")", "\n", "", "context_labels", "=", "context_labels", ".", "to", "(", "self", ".", "device", ")", "\n", "target_labels", "=", "target_labels", ".", "type", "(", "torch", ".", "LongTensor", ")", ".", "to", "(", "self", ".", "device", ")", "\n", "\n", "return", "context_images", ",", "target_images", ",", "context_labels", ",", "target_labels", ",", "real_target_labels", ",", "batch_class_list", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.shuffle": [[340, 346], ["numpy.random.permutation"], "methods", ["None"], ["", "def", "shuffle", "(", "self", ",", "images", ",", "labels", ")", ":", "\n", "        ", "\"\"\"\n        Return shuffled data.\n        \"\"\"", "\n", "permutation", "=", "np", ".", "random", ".", "permutation", "(", "images", ".", "shape", "[", "0", "]", ")", "\n", "return", "images", "[", "permutation", "]", ",", "labels", "[", "permutation", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.save_checkpoint": [[348, 356], ["torch.save", "torch.save", "run.Learner.model.state_dict", "run.Learner.optimizer.state_dict", "run.Learner.scheduler.state_dict", "os.path.join", "os.path.join"], "methods", ["None"], ["", "def", "save_checkpoint", "(", "self", ",", "iteration", ")", ":", "\n", "        ", "d", "=", "{", "'iteration'", ":", "iteration", ",", "\n", "'model_state_dict'", ":", "self", ".", "model", ".", "state_dict", "(", ")", ",", "\n", "'optimizer_state_dict'", ":", "self", ".", "optimizer", ".", "state_dict", "(", ")", ",", "\n", "'scheduler'", ":", "self", ".", "scheduler", ".", "state_dict", "(", ")", "}", "\n", "\n", "torch", ".", "save", "(", "d", ",", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "'checkpoint{}.pt'", ".", "format", "(", "iteration", ")", ")", ")", "\n", "torch", ".", "save", "(", "d", ",", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "'checkpoint.pt'", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.load_checkpoint": [[357, 366], ["run.Learner.model.load_state_dict", "run.Learner.optimizer.load_state_dict", "run.Learner.scheduler.load_state_dict", "torch.load", "torch.load", "os.path.join"], "methods", ["None"], ["", "def", "load_checkpoint", "(", "self", ")", ":", "\n", "        ", "if", "self", ".", "args", ".", "test_model_only", ":", "\n", "            ", "checkpoint", "=", "torch", ".", "load", "(", "self", ".", "args", ".", "test_model_path", ")", "\n", "", "else", ":", "\n", "           ", "checkpoint", "=", "torch", ".", "load", "(", "os", ".", "path", ".", "join", "(", "self", ".", "checkpoint_dir", ",", "'checkpoint.pt'", ")", ")", "\n", "", "self", ".", "start_iteration", "=", "checkpoint", "[", "'iteration'", "]", "\n", "self", ".", "model", ".", "load_state_dict", "(", "checkpoint", "[", "'model_state_dict'", "]", ")", "\n", "self", ".", "optimizer", ".", "load_state_dict", "(", "checkpoint", "[", "'optimizer_state_dict'", "]", ")", "\n", "self", ".", "scheduler", ".", "load_state_dict", "(", "checkpoint", "[", "'scheduler'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.setup_logger": [[21, 30], ["logging.FileHandler", "logging.FileHandler.setFormatter", "logging.getLogger", "logging.getLogger.setLevel", "logging.getLogger.addHandler"], "function", ["None"], ["def", "setup_logger", "(", "name", ",", "log_file", ",", "level", "=", "logging", ".", "INFO", ")", ":", "\n", "    ", "handler", "=", "logging", ".", "FileHandler", "(", "log_file", ")", "\n", "handler", ".", "setFormatter", "(", "formatter", ")", "\n", "\n", "logger", "=", "logging", ".", "getLogger", "(", "name", ")", "\n", "logger", ".", "setLevel", "(", "level", ")", "\n", "logger", ".", "addHandler", "(", "handler", ")", "\n", "\n", "return", "logger", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.main": [[48, 51], ["run.Learner", "Learner.run"], "function", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.run"], ["def", "main", "(", ")", ":", "\n", "    ", "learner", "=", "Learner", "(", ")", "\n", "learner", ".", "run", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.__init__": [[17, 20], ["len"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "validation_datasets", ")", ":", "\n", "        ", "self", ".", "datasets", "=", "validation_datasets", "\n", "self", ".", "dataset_count", "=", "len", "(", "self", ".", "datasets", ")", "\n", "#        self.current_best_accuracy_dict = {}", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print": [[39, 46], ["utils.print_and_log", "utils.print_and_log", "utils.print_and_log", "utils.print_and_log"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.print_and_log", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.print_and_log", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.print_and_log", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.print_and_log"], ["", "def", "print", "(", "self", ",", "logfile", ",", "accuracy_dict", ")", ":", "\n", "        ", "print_and_log", "(", "logfile", ",", "\"\"", ")", "# add a blank line", "\n", "print_and_log", "(", "logfile", ",", "\"Test Accuracies:\"", ")", "\n", "for", "dataset", "in", "self", ".", "datasets", ":", "\n", "            ", "print_and_log", "(", "logfile", ",", "\"{0:}: {1:.1f}+/-{2:.1f}\"", ".", "format", "(", "dataset", ",", "accuracy_dict", "[", "dataset", "]", "[", "\"accuracy\"", "]", ",", "\n", "accuracy_dict", "[", "dataset", "]", "[", "\"confidence\"", "]", ")", ")", "\n", "", "print_and_log", "(", "logfile", ",", "\"\"", ")", "# add a blank line", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.verify_checkpoint_dir": [[51, 71], ["os.path.join", "os.path.exists", "os.path.exists", "print", "sys.exit", "os.path.isfile", "print", "sys.exit", "print", "print", "print", "sys.exit"], "function", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print"], ["", "", "def", "verify_checkpoint_dir", "(", "checkpoint_dir", ",", "resume", ",", "test_mode", ")", ":", "\n", "    ", "if", "resume", ":", "# verify that the checkpoint directory and file exists", "\n", "        ", "if", "not", "os", ".", "path", ".", "exists", "(", "checkpoint_dir", ")", ":", "\n", "            ", "print", "(", "\"Can't resume for checkpoint. Checkpoint directory ({}) does not exist.\"", ".", "format", "(", "checkpoint_dir", ")", ",", "flush", "=", "True", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n", "", "checkpoint_file", "=", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'checkpoint.pt'", ")", "\n", "if", "not", "os", ".", "path", ".", "isfile", "(", "checkpoint_file", ")", ":", "\n", "            ", "print", "(", "\"Can't resume for checkpoint. Checkpoint file ({}) does not exist.\"", ".", "format", "(", "checkpoint_file", ")", ",", "flush", "=", "True", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "#elif test_mode:", "\n", "#    if not os.path.exists(checkpoint_dir):", "\n", "#        print(\"Can't test. Checkpoint directory ({}) does not exist.\".format(checkpoint_dir), flush=True)", "\n", "#        sys.exit()", "\n", "", "", "else", ":", "\n", "        ", "if", "os", ".", "path", ".", "exists", "(", "checkpoint_dir", ")", ":", "\n", "            ", "print", "(", "\"Checkpoint directory ({}) already exits.\"", ".", "format", "(", "checkpoint_dir", ")", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"If starting a new training run, specify a directory that does not already exist.\"", ",", "flush", "=", "True", ")", "\n", "print", "(", "\"If you want to resume a training run, specify the -r option on the command line.\"", ",", "flush", "=", "True", ")", "\n", "sys", ".", "exit", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.print_and_log": [[73, 79], ["print", "log_file.write"], "function", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print"], ["", "", "", "def", "print_and_log", "(", "log_file", ",", "message", ")", ":", "\n", "    ", "\"\"\"\n    Helper function to print to the screen and the cnaps_layer_log.txt file.\n    \"\"\"", "\n", "print", "(", "message", ",", "flush", "=", "True", ")", "\n", "log_file", ".", "write", "(", "message", "+", "'\\n'", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.get_log_files": [[81, 99], ["utils.verify_checkpoint_dir", "os.path.join", "os.path.join", "os.path.join", "os.path.isfile", "os.makedirs", "open", "open"], "function", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.verify_checkpoint_dir"], ["", "def", "get_log_files", "(", "checkpoint_dir", ",", "resume", ",", "test_mode", ")", ":", "\n", "    ", "\"\"\"\n    Function that takes a path to a checkpoint directory and returns a reference to a logfile and paths to the\n    fully trained model and the model with the best validation score.\n    \"\"\"", "\n", "verify_checkpoint_dir", "(", "checkpoint_dir", ",", "resume", ",", "test_mode", ")", "\n", "#if not test_mode and not resume:", "\n", "if", "not", "resume", ":", "\n", "        ", "os", ".", "makedirs", "(", "checkpoint_dir", ")", "\n", "", "checkpoint_path_validation", "=", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'best_validation.pt'", ")", "\n", "checkpoint_path_final", "=", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'fully_trained.pt'", ")", "\n", "logfile_path", "=", "os", ".", "path", ".", "join", "(", "checkpoint_dir", ",", "'log.txt'", ")", "\n", "if", "os", ".", "path", ".", "isfile", "(", "logfile_path", ")", ":", "\n", "        ", "logfile", "=", "open", "(", "logfile_path", ",", "\"a\"", ",", "buffering", "=", "1", ")", "\n", "", "else", ":", "\n", "        ", "logfile", "=", "open", "(", "logfile_path", ",", "\"w\"", ",", "buffering", "=", "1", ")", "\n", "\n", "", "return", "checkpoint_dir", ",", "logfile", ",", "checkpoint_path_validation", ",", "checkpoint_path_final", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.stack_first_dim": [[101, 110], ["x.size", "x.view", "len"], "function", ["None"], ["", "def", "stack_first_dim", "(", "x", ")", ":", "\n", "    ", "\"\"\"\n    Method to combine the first two dimension of an array\n    \"\"\"", "\n", "x_shape", "=", "x", ".", "size", "(", ")", "\n", "new_shape", "=", "[", "x_shape", "[", "0", "]", "*", "x_shape", "[", "1", "]", "]", "\n", "if", "len", "(", "x_shape", ")", ">", "2", ":", "\n", "        ", "new_shape", "+=", "x_shape", "[", "2", ":", "]", "\n", "", "return", "x", ".", "view", "(", "new_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.split_first_dim_linear": [[112, 121], ["x.size", "x.view", "len"], "function", ["None"], ["", "def", "split_first_dim_linear", "(", "x", ",", "first_two_dims", ")", ":", "\n", "    ", "\"\"\"\n    Undo the stacking operation\n    \"\"\"", "\n", "x_shape", "=", "x", ".", "size", "(", ")", "\n", "new_shape", "=", "first_two_dims", "\n", "if", "len", "(", "x_shape", ")", ">", "1", ":", "\n", "        ", "new_shape", "+=", "[", "x_shape", "[", "-", "1", "]", "]", "\n", "", "return", "x", ".", "view", "(", "new_shape", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.sample_normal": [[123, 134], ["torch.distributions.Normal", "torch.distributions.Normal", "torch.distributions.Normal.rsample", "mean.repeat", "var.repeat", "len", "mean.size"], "function", ["None"], ["", "def", "sample_normal", "(", "mean", ",", "var", ",", "num_samples", ")", ":", "\n", "    ", "\"\"\"\n    Generate samples from a reparameterized normal distribution\n    :param mean: tensor - mean parameter of the distribution\n    :param var: tensor - variance of the distribution\n    :param num_samples: np scalar - number of samples to generate\n    :return: tensor - samples from distribution of size numSamples x dim(mean)\n    \"\"\"", "\n", "sample_shape", "=", "[", "num_samples", "]", "+", "len", "(", "mean", ".", "size", "(", ")", ")", "*", "[", "1", "]", "\n", "normal_distribution", "=", "torch", ".", "distributions", ".", "Normal", "(", "mean", ".", "repeat", "(", "sample_shape", ")", ",", "var", ".", "repeat", "(", "sample_shape", ")", ")", "\n", "return", "normal_distribution", ".", "rsample", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.loss": [[136, 149], ["test_logits_sample.size", "torch.tensor", "torch.tensor", "torch.empty", "torch.empty", "range", "torch.logsumexp", "torch.logsumexp", "torch.log", "torch.log", "torch.sum", "torch.sum", "torch.cross_entropy"], "function", ["None"], ["", "def", "loss", "(", "test_logits_sample", ",", "test_labels", ",", "device", ")", ":", "\n", "    ", "\"\"\"\n    Compute the classification loss.\n    \"\"\"", "\n", "size", "=", "test_logits_sample", ".", "size", "(", ")", "\n", "sample_count", "=", "size", "[", "0", "]", "# scalar for the loop counter", "\n", "num_samples", "=", "torch", ".", "tensor", "(", "[", "sample_count", "]", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ",", "requires_grad", "=", "False", ")", "\n", "\n", "log_py", "=", "torch", ".", "empty", "(", "size", "=", "(", "size", "[", "0", "]", ",", "size", "[", "1", "]", ")", ",", "dtype", "=", "torch", ".", "float", ",", "device", "=", "device", ")", "\n", "for", "sample", "in", "range", "(", "sample_count", ")", ":", "\n", "        ", "log_py", "[", "sample", "]", "=", "-", "F", ".", "cross_entropy", "(", "test_logits_sample", "[", "sample", "]", ",", "test_labels", ",", "reduction", "=", "'none'", ")", "\n", "", "score", "=", "torch", ".", "logsumexp", "(", "log_py", ",", "dim", "=", "0", ")", "-", "torch", ".", "log", "(", "num_samples", ")", "\n", "return", "-", "torch", ".", "sum", "(", "score", ",", "dim", "=", "0", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.aggregate_accuracy": [[151, 157], ["torch.logsumexp", "torch.logsumexp", "torch.mean", "torch.mean", "torch.eq().float", "torch.eq().float", "torch.eq", "torch.eq", "torch.argmax", "torch.argmax"], "function", ["None"], ["", "def", "aggregate_accuracy", "(", "test_logits_sample", ",", "test_labels", ")", ":", "\n", "    ", "\"\"\"\n    Compute classification accuracy.\n    \"\"\"", "\n", "averaged_predictions", "=", "torch", ".", "logsumexp", "(", "test_logits_sample", ",", "dim", "=", "0", ")", "\n", "return", "torch", ".", "mean", "(", "torch", ".", "eq", "(", "test_labels", ",", "torch", ".", "argmax", "(", "averaged_predictions", ",", "dim", "=", "-", "1", ")", ")", ".", "float", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.task_confusion": [[158, 162], ["torch.argmax", "torch.argmax", "torch.logsumexp", "torch.logsumexp"], "function", ["None"], ["", "def", "task_confusion", "(", "test_logits", ",", "test_labels", ",", "real_test_labels", ",", "batch_class_list", ")", ":", "\n", "    ", "preds", "=", "torch", ".", "argmax", "(", "torch", ".", "logsumexp", "(", "test_logits", ",", "dim", "=", "0", ")", ",", "dim", "=", "-", "1", ")", "\n", "real_preds", "=", "batch_class_list", "[", "preds", "]", "\n", "return", "real_preds", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.linear_classifier": [[163, 168], ["torch.linear"], "function", ["None"], ["", "def", "linear_classifier", "(", "x", ",", "param_dict", ")", ":", "\n", "    ", "\"\"\"\n    Classifier.\n    \"\"\"", "\n", "return", "F", ".", "linear", "(", "x", ",", "param_dict", "[", "'weight_mean'", "]", ",", "param_dict", "[", "'bias_mean'", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.__init__": [[18, 21], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ")", ":", "\n", "        ", "self", ".", "gt_a_list", "=", "[", "]", "\n", "self", ".", "videos", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.add_vid": [[22, 25], ["video_reader.Split.videos.append", "video_reader.Split.gt_a_list.append"], "methods", ["None"], ["", "def", "add_vid", "(", "self", ",", "paths", ",", "gt_a", ")", ":", "\n", "        ", "self", ".", "videos", ".", "append", "(", "paths", ")", "\n", "self", ".", "gt_a_list", ".", "append", "(", "gt_a", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.get_rand_vid": [[26, 36], ["range", "numpy.random.choice", "len", "match_idxs.append"], "methods", ["None"], ["", "def", "get_rand_vid", "(", "self", ",", "label", ",", "idx", "=", "-", "1", ")", ":", "\n", "        ", "match_idxs", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len", "(", "self", ".", "gt_a_list", ")", ")", ":", "\n", "            ", "if", "label", "==", "self", ".", "gt_a_list", "[", "i", "]", ":", "\n", "                ", "match_idxs", ".", "append", "(", "i", ")", "\n", "\n", "", "", "if", "idx", "!=", "-", "1", ":", "\n", "            ", "return", "self", ".", "videos", "[", "match_idxs", "[", "idx", "]", "]", ",", "match_idxs", "[", "idx", "]", "\n", "", "random_idx", "=", "np", ".", "random", ".", "choice", "(", "match_idxs", ")", "\n", "return", "self", ".", "videos", "[", "random_idx", "]", ",", "random_idx", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.get_num_videos_for_class": [[37, 39], ["len"], "methods", ["None"], ["", "def", "get_num_videos_for_class", "(", "self", ",", "label", ")", ":", "\n", "        ", "return", "len", "(", "[", "gt", "for", "gt", "in", "self", ".", "gt_a_list", "if", "gt", "==", "label", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.get_unique_classes": [[40, 42], ["list", "set"], "methods", ["None"], ["", "def", "get_unique_classes", "(", "self", ")", ":", "\n", "        ", "return", "list", "(", "set", "(", "self", ".", "gt_a_list", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.get_max_video_len": [[43, 50], ["len"], "methods", ["None"], ["", "def", "get_max_video_len", "(", "self", ")", ":", "\n", "        ", "max_len", "=", "0", "\n", "for", "v", "in", "self", ".", "videos", ":", "\n", "            ", "l", "=", "len", "(", "v", ")", "\n", "if", "l", ">", "max_len", ":", "\n", "                ", "max_len", "=", "l", "\n", "", "", "return", "max_len", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.__len__": [[51, 53], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "gt_a_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.__init__": [[56, 78], ["torchvision.transforms.ToTensor", "video_reader.Split", "video_reader.Split", "video_reader.VideoDataset.setup_transforms", "video_reader.VideoDataset._select_fold", "video_reader.VideoDataset.read_dir"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.setup_transforms", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset._select_fold", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.read_dir"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "self", ".", "args", "=", "args", "\n", "self", ".", "get_item_counter", "=", "0", "\n", "\n", "self", ".", "data_dir", "=", "args", ".", "path", "\n", "self", ".", "seq_len", "=", "args", ".", "seq_len", "\n", "self", ".", "train", "=", "True", "\n", "self", ".", "tensor_transform", "=", "transforms", ".", "ToTensor", "(", ")", "\n", "self", ".", "img_size", "=", "args", ".", "img_size", "\n", "\n", "self", ".", "annotation_path", "=", "args", ".", "traintestlist", "\n", "\n", "self", ".", "way", "=", "args", ".", "way", "\n", "self", ".", "shot", "=", "args", ".", "shot", "\n", "self", ".", "query_per_class", "=", "args", ".", "query_per_class", "\n", "\n", "self", ".", "train_split", "=", "Split", "(", ")", "\n", "self", ".", "test_split", "=", "Split", "(", ")", "\n", "\n", "self", ".", "setup_transforms", "(", ")", "\n", "self", ".", "_select_fold", "(", ")", "\n", "self", ".", "read_dir", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.setup_transforms": [[80, 101], ["video_transform_list.append", "video_transform_list.append", "video_test_list.append", "videotransforms.video_transforms.Compose", "videotransforms.video_transforms.Compose", "video_transform_list.append", "video_test_list.append", "videotransforms.video_transforms.RandomHorizontalFlip", "videotransforms.video_transforms.RandomCrop", "videotransforms.video_transforms.CenterCrop", "videotransforms.video_transforms.Resize", "videotransforms.video_transforms.Resize", "video_transform_list.append", "video_test_list.append", "print", "exit", "videotransforms.video_transforms.Resize", "videotransforms.video_transforms.Resize"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print"], ["def", "setup_transforms", "(", "self", ")", ":", "\n", "        ", "video_transform_list", "=", "[", "]", "\n", "video_test_list", "=", "[", "]", "\n", "\n", "if", "self", ".", "img_size", "==", "84", ":", "\n", "            ", "video_transform_list", ".", "append", "(", "Resize", "(", "96", ")", ")", "\n", "video_test_list", ".", "append", "(", "Resize", "(", "96", ")", ")", "\n", "", "elif", "self", ".", "img_size", "==", "224", ":", "\n", "            ", "video_transform_list", ".", "append", "(", "Resize", "(", "256", ")", ")", "\n", "video_test_list", ".", "append", "(", "Resize", "(", "256", ")", ")", "\n", "", "else", ":", "\n", "            ", "print", "(", "\"img size transforms not setup\"", ")", "\n", "exit", "(", "1", ")", "\n", "", "video_transform_list", ".", "append", "(", "RandomHorizontalFlip", "(", ")", ")", "\n", "video_transform_list", ".", "append", "(", "RandomCrop", "(", "self", ".", "img_size", ")", ")", "\n", "\n", "video_test_list", ".", "append", "(", "CenterCrop", "(", "self", ".", "img_size", ")", ")", "\n", "\n", "self", ".", "transform", "=", "{", "}", "\n", "self", ".", "transform", "[", "\"train\"", "]", "=", "Compose", "(", "video_transform_list", ")", "\n", "self", ".", "transform", "[", "\"test\"", "]", "=", "Compose", "(", "video_test_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.read_dir": [[104, 183], ["video_reader.VideoDataset.data_dir.endswith", "print", "print", "os.path.join", "open().read", "zipfile.ZipFile", "list", "list", "os.listdir.sort", "list", "os.listdir.sort", "img_list.sort", "video_reader.VideoDataset.get_train_or_test_db", "video_reader.VideoDataset.get_train_or_test_db", "os.listdir", "os.listdir.sort", "io.BytesIO", "set", "set", "set", "insert_frames.append", "video_reader.VideoDataset.add_vid", "os.listdir", "os.listdir.sort", "len", "len", "open", "enumerate", "enumerate", "video_reader.VideoDataset.zfile.namelist", "img_path.split", "len", "os.path.join", "video_reader.VideoDataset.get_train_or_test_db", "os.listdir", "os.listdir.sort", "paths.sort", "os.listdir.index", "video_reader.VideoDataset.add_vid", "len", "video_reader.VideoDataset.get_train_or_test_db", "os.path.join", "len", "os.path.join", "video_reader.VideoDataset.zfile.namelist", "x.split", "x.split", "last_video_folder.lower", "video_reader.VideoDataset.add_vid", "len", "len", "x.split", "x.split"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print", "home.repos.pwc.inspect_result.Anirudh257_strm.None.utils.TestAccuracies.print", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_train_or_test_db", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_train_or_test_db", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.add_vid", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_train_or_test_db", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.add_vid", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_train_or_test_db", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.add_vid"], ["def", "read_dir", "(", "self", ")", ":", "\n", "# load zipfile into memory", "\n", "        ", "if", "self", ".", "data_dir", ".", "endswith", "(", "'.zip'", ")", ":", "\n", "            ", "self", ".", "zip", "=", "True", "\n", "zip_fn", "=", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ")", "\n", "self", ".", "mem", "=", "open", "(", "zip_fn", ",", "'rb'", ")", ".", "read", "(", ")", "\n", "self", ".", "zfile", "=", "zipfile", ".", "ZipFile", "(", "io", ".", "BytesIO", "(", "self", ".", "mem", ")", ")", "\n", "", "else", ":", "\n", "            ", "self", ".", "zip", "=", "False", "\n", "\n", "# go through zip and populate splits with frame locations and action groundtruths", "\n", "", "if", "self", ".", "zip", ":", "\n", "\n", "# When using 'png' based datasets like kinetics, replace 'jpg' to 'png'", "\n", "            ", "dir_list", "=", "list", "(", "set", "(", "[", "x", "for", "x", "in", "self", ".", "zfile", ".", "namelist", "(", ")", "if", "'.jpg'", "not", "in", "x", "]", ")", ")", "\n", "\n", "class_folders", "=", "list", "(", "set", "(", "[", "x", ".", "split", "(", "os", ".", "sep", ")", "[", "-", "3", "]", "for", "x", "in", "dir_list", "if", "len", "(", "x", ".", "split", "(", "os", ".", "sep", ")", ")", ">", "2", "]", ")", ")", "\n", "class_folders", ".", "sort", "(", ")", "\n", "self", ".", "class_folders", "=", "class_folders", "\n", "video_folders", "=", "list", "(", "set", "(", "[", "x", ".", "split", "(", "os", ".", "sep", ")", "[", "-", "2", "]", "for", "x", "in", "dir_list", "if", "len", "(", "x", ".", "split", "(", "os", ".", "sep", ")", ")", ">", "3", "]", ")", ")", "\n", "video_folders", ".", "sort", "(", ")", "\n", "self", ".", "video_folders", "=", "video_folders", "\n", "\n", "class_folders_indexes", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "enumerate", "(", "self", ".", "class_folders", ")", "}", "\n", "video_folders_indexes", "=", "{", "v", ":", "k", "for", "k", ",", "v", "in", "enumerate", "(", "self", ".", "video_folders", ")", "}", "\n", "\n", "img_list", "=", "[", "x", "for", "x", "in", "self", ".", "zfile", ".", "namelist", "(", ")", "if", "'.jpg'", "in", "x", "]", "\n", "img_list", ".", "sort", "(", ")", "\n", "\n", "c", "=", "self", ".", "get_train_or_test_db", "(", "video_folders", "[", "0", "]", ")", "\n", "\n", "last_video_folder", "=", "None", "\n", "last_video_class", "=", "-", "1", "\n", "insert_frames", "=", "[", "]", "\n", "for", "img_path", "in", "img_list", ":", "\n", "\n", "                ", "class_folder", ",", "video_folder", ",", "jpg", "=", "img_path", ".", "split", "(", "os", ".", "sep", ")", "[", "-", "3", ":", "]", "\n", "\n", "if", "video_folder", "!=", "last_video_folder", ":", "\n", "                    ", "if", "len", "(", "insert_frames", ")", ">=", "self", ".", "seq_len", ":", "\n", "                        ", "c", "=", "self", ".", "get_train_or_test_db", "(", "last_video_folder", ".", "lower", "(", ")", ")", "\n", "if", "c", "!=", "None", ":", "\n", "                            ", "c", ".", "add_vid", "(", "insert_frames", ",", "last_video_class", ")", "\n", "", "else", ":", "\n", "                            ", "pass", "\n", "", "", "insert_frames", "=", "[", "]", "\n", "class_id", "=", "class_folders_indexes", "[", "class_folder", "]", "\n", "vid_id", "=", "video_folders_indexes", "[", "video_folder", "]", "\n", "\n", "", "insert_frames", ".", "append", "(", "img_path", ")", "\n", "last_video_folder", "=", "video_folder", "\n", "last_video_class", "=", "class_id", "\n", "\n", "", "c", "=", "self", ".", "get_train_or_test_db", "(", "last_video_folder", ")", "\n", "if", "c", "!=", "None", "and", "len", "(", "insert_frames", ")", ">=", "self", ".", "seq_len", ":", "\n", "                ", "c", ".", "add_vid", "(", "insert_frames", ",", "last_video_class", ")", "\n", "", "", "else", ":", "\n", "            ", "class_folders", "=", "os", ".", "listdir", "(", "self", ".", "data_dir", ")", "\n", "class_folders", ".", "sort", "(", ")", "\n", "self", ".", "class_folders", "=", "class_folders", "\n", "for", "class_folder", "in", "class_folders", ":", "\n", "                ", "video_folders", "=", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "class_folder", ")", ")", "\n", "video_folders", ".", "sort", "(", ")", "\n", "if", "self", ".", "args", ".", "debug_loader", ":", "\n", "                    ", "video_folders", "=", "video_folders", "[", "0", ":", "1", "]", "\n", "", "for", "video_folder", "in", "video_folders", ":", "\n", "                    ", "c", "=", "self", ".", "get_train_or_test_db", "(", "video_folder", ")", "\n", "if", "c", "==", "None", ":", "\n", "                        ", "continue", "\n", "", "imgs", "=", "os", ".", "listdir", "(", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "class_folder", ",", "video_folder", ")", ")", "\n", "if", "len", "(", "imgs", ")", "<", "self", ".", "seq_len", ":", "\n", "                        ", "continue", "\n", "", "imgs", ".", "sort", "(", ")", "\n", "paths", "=", "[", "os", ".", "path", ".", "join", "(", "self", ".", "data_dir", ",", "class_folder", ",", "video_folder", ",", "img", ")", "for", "img", "in", "imgs", "]", "\n", "paths", ".", "sort", "(", ")", "\n", "class_id", "=", "class_folders", ".", "index", "(", "class_folder", ")", "\n", "c", ".", "add_vid", "(", "paths", ",", "class_id", ")", "\n", "", "", "", "print", "(", "\"loaded {}\"", ".", "format", "(", "self", ".", "data_dir", ")", ")", "\n", "print", "(", "\"train: {}, test: {}\"", ".", "format", "(", "len", "(", "self", ".", "train_split", ")", ",", "len", "(", "self", ".", "test_split", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_train_or_test_db": [[185, 199], ["None"], "methods", ["None"], ["def", "get_train_or_test_db", "(", "self", ",", "split", "=", "None", ")", ":", "\n", "        ", "if", "split", "is", "None", ":", "\n", "            ", "get_train_split", "=", "self", ".", "train", "\n", "", "else", ":", "\n", "            ", "if", "split", "in", "self", ".", "train_test_lists", "[", "\"train\"", "]", ":", "\n", "                ", "get_train_split", "=", "True", "\n", "", "elif", "split", "in", "self", ".", "train_test_lists", "[", "\"test\"", "]", ":", "\n", "                ", "get_train_split", "=", "False", "\n", "", "else", ":", "\n", "                ", "return", "None", "\n", "", "", "if", "get_train_split", ":", "\n", "            ", "return", "self", ".", "train_split", "\n", "", "else", ":", "\n", "            ", "return", "self", ".", "test_split", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset._select_fold": [[201, 219], ["os.path.join", "open", "fid.readlines", "selected_files.extend", "x.replace().lower", "x.strip().split", "os.path.splitext", "x.replace", "x.strip", "os.path.split"], "methods", ["None"], ["def", "_select_fold", "(", "self", ")", ":", "\n", "        ", "lists", "=", "{", "}", "\n", "for", "name", "in", "[", "\"train\"", ",", "\"test\"", "]", ":", "\n", "            ", "fname", "=", "\"{}list{:02d}.txt\"", ".", "format", "(", "name", ",", "self", ".", "args", ".", "split", ")", "\n", "f", "=", "os", ".", "path", ".", "join", "(", "self", ".", "annotation_path", ",", "fname", ")", "\n", "selected_files", "=", "[", "]", "\n", "with", "open", "(", "f", ",", "\"r\"", ")", "as", "fid", ":", "\n", "                ", "data", "=", "fid", ".", "readlines", "(", ")", "\n", "data", "=", "[", "x", ".", "replace", "(", "' '", ",", "'_'", ")", ".", "lower", "(", ")", "for", "x", "in", "data", "]", "\n", "data", "=", "[", "x", ".", "strip", "(", ")", ".", "split", "(", "\" \"", ")", "[", "0", "]", "for", "x", "in", "data", "]", "\n", "data", "=", "[", "os", ".", "path", ".", "splitext", "(", "os", ".", "path", ".", "split", "(", "x", ")", "[", "1", "]", ")", "[", "0", "]", "for", "x", "in", "data", "]", "\n", "\n", "#                 if \"kinetics\" in self.args.path:", "\n", "#                     data = [x[0:11] for x in data]", "\n", "\n", "selected_files", ".", "extend", "(", "data", ")", "\n", "", "lists", "[", "name", "]", "=", "selected_files", "\n", "", "self", ".", "train_test_lists", "=", "lists", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.__len__": [[221, 225], ["video_reader.VideoDataset.get_train_or_test_db", "len"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_train_or_test_db"], ["def", "__len__", "(", "self", ")", ":", "\n", "        ", "c", "=", "self", ".", "get_train_or_test_db", "(", ")", "\n", "return", "1000000", "\n", "return", "len", "(", "c", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_split_class_list": [[227, 232], ["video_reader.VideoDataset.get_train_or_test_db", "list", "list.sort", "set"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_train_or_test_db"], ["def", "get_split_class_list", "(", "self", ")", ":", "\n", "        ", "c", "=", "self", ".", "get_train_or_test_db", "(", ")", "\n", "classes", "=", "list", "(", "set", "(", "c", ".", "gt_a_list", ")", ")", "\n", "classes", ".", "sort", "(", ")", "\n", "return", "classes", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.read_single_image": [[234, 244], ["video_reader.VideoDataset.zfile.open", "PIL.Image.open", "i.load", "PIL.Image.open", "i.load"], "methods", ["None"], ["def", "read_single_image", "(", "self", ",", "path", ")", ":", "\n", "        ", "if", "self", ".", "zip", ":", "\n", "            ", "with", "self", ".", "zfile", ".", "open", "(", "path", ",", "'r'", ")", "as", "f", ":", "\n", "                ", "with", "Image", ".", "open", "(", "f", ")", "as", "i", ":", "\n", "                    ", "i", ".", "load", "(", ")", "\n", "return", "i", "\n", "", "", "", "else", ":", "\n", "            ", "with", "Image", ".", "open", "(", "path", ")", "as", "i", ":", "\n", "                ", "i", ".", "load", "(", ")", "\n", "return", "i", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_seq": [[246, 288], ["video_reader.VideoDataset.get_train_or_test_db", "video_reader.VideoDataset.get_rand_vid", "len", "numpy.linspace", "video_reader.VideoDataset.read_single_image", "torch.stack", "int", "int", "int", "video_reader.VideoDataset.tensor_transform", "range", "min", "random.randint", "random.randint", "random.randint", "transform"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_train_or_test_db", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.get_rand_vid", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.read_single_image"], ["def", "get_seq", "(", "self", ",", "label", ",", "idx", "=", "-", "1", ")", ":", "\n", "        ", "c", "=", "self", ".", "get_train_or_test_db", "(", ")", "\n", "paths", ",", "vid_id", "=", "c", ".", "get_rand_vid", "(", "label", ",", "idx", ")", "\n", "n_frames", "=", "len", "(", "paths", ")", "\n", "if", "n_frames", "==", "self", ".", "args", ".", "seq_len", ":", "\n", "            ", "idxs", "=", "[", "int", "(", "f", ")", "for", "f", "in", "range", "(", "n_frames", ")", "]", "\n", "", "else", ":", "\n", "            ", "if", "self", ".", "train", ":", "\n", "                ", "excess_frames", "=", "n_frames", "-", "self", ".", "seq_len", "\n", "excess_pad", "=", "int", "(", "min", "(", "5", ",", "excess_frames", "/", "2", ")", ")", "\n", "if", "excess_pad", "<", "1", ":", "\n", "                    ", "start", "=", "0", "\n", "end", "=", "n_frames", "-", "1", "\n", "", "else", ":", "\n", "                    ", "start", "=", "random", ".", "randint", "(", "0", ",", "excess_pad", ")", "\n", "end", "=", "random", ".", "randint", "(", "n_frames", "-", "1", "-", "excess_pad", ",", "n_frames", "-", "1", ")", "\n", "", "", "else", ":", "\n", "                ", "start", "=", "1", "\n", "end", "=", "n_frames", "-", "2", "\n", "\n", "", "if", "end", "-", "start", "<", "self", ".", "seq_len", ":", "\n", "                ", "end", "=", "n_frames", "-", "1", "\n", "start", "=", "0", "\n", "", "else", ":", "\n", "                ", "pass", "\n", "\n", "", "idx_f", "=", "np", ".", "linspace", "(", "start", ",", "end", ",", "num", "=", "self", ".", "seq_len", ")", "\n", "idxs", "=", "[", "int", "(", "f", ")", "for", "f", "in", "idx_f", "]", "\n", "\n", "if", "self", ".", "seq_len", "==", "1", ":", "\n", "                ", "idxs", "=", "[", "random", ".", "randint", "(", "start", ",", "end", "-", "1", ")", "]", "\n", "\n", "", "", "imgs", "=", "[", "self", ".", "read_single_image", "(", "paths", "[", "i", "]", ")", "for", "i", "in", "idxs", "]", "\n", "if", "(", "self", ".", "transform", "is", "not", "None", ")", ":", "\n", "            ", "if", "self", ".", "train", ":", "\n", "                ", "transform", "=", "self", ".", "transform", "[", "\"train\"", "]", "\n", "", "else", ":", "\n", "                ", "transform", "=", "self", ".", "transform", "[", "\"test\"", "]", "\n", "\n", "", "imgs", "=", "[", "self", ".", "tensor_transform", "(", "v", ")", "for", "v", "in", "transform", "(", "imgs", ")", "]", "\n", "imgs", "=", "torch", ".", "stack", "(", "imgs", ")", "\n", "", "return", "imgs", ",", "vid_id", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.__getitem__": [[291, 342], ["video_reader.VideoDataset.get_train_or_test_db", "video_reader.VideoDataset.get_unique_classes", "random.sample", "enumerate", "list", "random.shuffle", "zip", "list", "random.shuffle", "zip", "torch.cat", "torch.cat", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "torch.FloatTensor", "video_reader.VideoDataset.get_num_videos_for_class", "random.sample", "zip", "zip", "video_reader.VideoDataset.get_seq", "torch.cat.append", "torch.FloatTensor.append", "video_reader.VideoDataset.get_seq", "torch.cat.append", "torch.FloatTensor.append", "torch.FloatTensor.append", "range"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_train_or_test_db", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.get_unique_classes", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.shuffle", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.shuffle", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.Split.get_num_videos_for_class", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_seq", "home.repos.pwc.inspect_result.Anirudh257_strm.None.video_reader.VideoDataset.get_seq"], ["def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "\n", "#select classes to use for this task", "\n", "        ", "c", "=", "self", ".", "get_train_or_test_db", "(", ")", "\n", "classes", "=", "c", ".", "get_unique_classes", "(", ")", "\n", "batch_classes", "=", "random", ".", "sample", "(", "classes", ",", "self", ".", "way", ")", "\n", "\n", "if", "self", ".", "train", ":", "\n", "            ", "n_queries", "=", "self", ".", "args", ".", "query_per_class", "\n", "", "else", ":", "\n", "            ", "n_queries", "=", "self", ".", "args", ".", "query_per_class_test", "\n", "\n", "", "support_set", "=", "[", "]", "\n", "support_labels", "=", "[", "]", "\n", "target_set", "=", "[", "]", "\n", "target_labels", "=", "[", "]", "\n", "real_support_labels", "=", "[", "]", "\n", "real_target_labels", "=", "[", "]", "\n", "\n", "for", "bl", ",", "bc", "in", "enumerate", "(", "batch_classes", ")", ":", "\n", "\n", "#select shots from the chosen classes", "\n", "            ", "n_total", "=", "c", ".", "get_num_videos_for_class", "(", "bc", ")", "\n", "idxs", "=", "random", ".", "sample", "(", "[", "i", "for", "i", "in", "range", "(", "n_total", ")", "]", ",", "self", ".", "args", ".", "shot", "+", "n_queries", ")", "\n", "\n", "for", "idx", "in", "idxs", "[", "0", ":", "self", ".", "args", ".", "shot", "]", ":", "\n", "                ", "vid", ",", "vid_id", "=", "self", ".", "get_seq", "(", "bc", ",", "idx", ")", "\n", "support_set", ".", "append", "(", "vid", ")", "\n", "support_labels", ".", "append", "(", "bl", ")", "\n", "", "for", "idx", "in", "idxs", "[", "self", ".", "args", ".", "shot", ":", "]", ":", "\n", "                ", "vid", ",", "vid_id", "=", "self", ".", "get_seq", "(", "bc", ",", "idx", ")", "\n", "target_set", ".", "append", "(", "vid", ")", "\n", "target_labels", ".", "append", "(", "bl", ")", "\n", "real_target_labels", ".", "append", "(", "bc", ")", "\n", "\n", "", "", "s", "=", "list", "(", "zip", "(", "support_set", ",", "support_labels", ")", ")", "\n", "random", ".", "shuffle", "(", "s", ")", "\n", "support_set", ",", "support_labels", "=", "zip", "(", "*", "s", ")", "\n", "\n", "t", "=", "list", "(", "zip", "(", "target_set", ",", "target_labels", ",", "real_target_labels", ")", ")", "\n", "random", ".", "shuffle", "(", "t", ")", "\n", "target_set", ",", "target_labels", ",", "real_target_labels", "=", "zip", "(", "*", "t", ")", "\n", "\n", "support_set", "=", "torch", ".", "cat", "(", "support_set", ")", "\n", "target_set", "=", "torch", ".", "cat", "(", "target_set", ")", "\n", "support_labels", "=", "torch", ".", "FloatTensor", "(", "support_labels", ")", "\n", "target_labels", "=", "torch", ".", "FloatTensor", "(", "target_labels", ")", "\n", "real_target_labels", "=", "torch", ".", "FloatTensor", "(", "real_target_labels", ")", "\n", "batch_classes", "=", "torch", ".", "FloatTensor", "(", "batch_classes", ")", "\n", "\n", "return", "{", "\"support_set\"", ":", "support_set", ",", "\"support_labels\"", ":", "support_labels", ",", "\"target_set\"", ":", "target_set", ",", "\"target_labels\"", ":", "target_labels", ",", "\"real_target_labels\"", ":", "real_target_labels", ",", "\"batch_class_list\"", ":", "batch_classes", "}", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.stack_transforms.ToStackedTensor.__init__": [[14, 16], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "channel_nb", "=", "3", ")", ":", "\n", "        ", "self", ".", "channel_nb", "=", "channel_nb", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.stack_transforms.ToStackedTensor.__call__": [[17, 50], ["isinstance", "numpy.zeros", "enumerate", "torch.from_numpy", "torch.from_numpy.float().div", "isinstance", "isinstance", "videotransforms.utils.images.convert_img", "TypeError", "int", "int", "isinstance", "torch.from_numpy.float", "len", "numpy.array", "TypeError", "type", "type"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.utils.images.convert_img"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args: \n            clip (list of numpy.ndarray or PIL.Image.Image): clip \n            (list of images) to be converted to tensor.\n        \"\"\"", "\n", "# Retrieve shape", "\n", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "h", ",", "w", ",", "ch", "=", "clip", "[", "0", "]", ".", "shape", "\n", "assert", "ch", "==", "self", ".", "channel_nb", ",", "'got {} channels instead of 3'", ".", "format", "(", "\n", "ch", ")", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "            ", "w", ",", "h", "=", "clip", "[", "0", "]", ".", "size", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image\\\n            but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "\n", "", "np_clip", "=", "np", ".", "zeros", "(", "[", "self", ".", "channel_nb", "*", "len", "(", "clip", ")", ",", "int", "(", "h", ")", ",", "int", "(", "w", ")", "]", ")", "\n", "\n", "# Convert", "\n", "for", "img_idx", ",", "img", "in", "enumerate", "(", "clip", ")", ":", "\n", "            ", "if", "isinstance", "(", "img", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "pass", "\n", "", "elif", "isinstance", "(", "img", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "                ", "img", "=", "np", ".", "array", "(", "img", ",", "copy", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image\\\n                but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "", "img", "=", "imageutils", ".", "convert_img", "(", "img", ")", "\n", "np_clip", "[", "img_idx", "*", "self", ".", "channel_nb", ":", "(", "\n", "img_idx", "+", "1", ")", "*", "self", ".", "channel_nb", ",", ":", ",", ":", "]", "=", "img", "\n", "", "tensor_clip", "=", "torch", ".", "from_numpy", "(", "np_clip", ")", "\n", "return", "tensor_clip", ".", "float", "(", ")", ".", "div", "(", "255", ")", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.volume_transforms.ClipToTensor.__init__": [[13, 17], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "channel_nb", "=", "3", ",", "div_255", "=", "True", ",", "numpy", "=", "False", ")", ":", "\n", "        ", "self", ".", "channel_nb", "=", "channel_nb", "\n", "self", ".", "div_255", "=", "div_255", "\n", "self", ".", "numpy", "=", "numpy", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.volume_transforms.ClipToTensor.__call__": [[18, 60], ["isinstance", "numpy.zeros", "enumerate", "isinstance", "isinstance", "videotransforms.utils.images.convert_img", "torch.from_numpy", "TypeError", "len", "int", "int", "isinstance", "isinstance", "tensor_clip.div.div.float", "tensor_clip.div.div.div", "numpy.array", "TypeError", "type", "type"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.utils.images.convert_img"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args: clip (list of numpy.ndarray): clip (list of images)\n        to be converted to tensor.\n        \"\"\"", "\n", "# Retrieve shape", "\n", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "h", ",", "w", ",", "ch", "=", "clip", "[", "0", "]", ".", "shape", "\n", "assert", "ch", "==", "self", ".", "channel_nb", ",", "'Got {0} instead of 3 channels'", ".", "format", "(", "\n", "ch", ")", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "Image", ".", "Image", ")", ":", "\n", "            ", "w", ",", "h", "=", "clip", "[", "0", "]", ".", "size", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image\\\n            but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "\n", "", "np_clip", "=", "np", ".", "zeros", "(", "[", "self", ".", "channel_nb", ",", "len", "(", "clip", ")", ",", "int", "(", "h", ")", ",", "int", "(", "w", ")", "]", ")", "\n", "\n", "# Convert", "\n", "for", "img_idx", ",", "img", "in", "enumerate", "(", "clip", ")", ":", "\n", "            ", "if", "isinstance", "(", "img", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "pass", "\n", "", "elif", "isinstance", "(", "img", ",", "Image", ".", "Image", ")", ":", "\n", "                ", "img", "=", "np", ".", "array", "(", "img", ",", "copy", "=", "False", ")", "\n", "", "else", ":", "\n", "                ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image\\\n                but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "", "img", "=", "imageutils", ".", "convert_img", "(", "img", ")", "\n", "np_clip", "[", ":", ",", "img_idx", ",", ":", ",", ":", "]", "=", "img", "\n", "", "if", "self", ".", "numpy", ":", "\n", "            ", "if", "self", ".", "div_255", ":", "\n", "                ", "np_clip", "=", "np_clip", "/", "255", "\n", "", "return", "np_clip", "\n", "\n", "", "else", ":", "\n", "            ", "tensor_clip", "=", "torch", ".", "from_numpy", "(", "np_clip", ")", "\n", "\n", "if", "not", "isinstance", "(", "tensor_clip", ",", "torch", ".", "FloatTensor", ")", ":", "\n", "                ", "tensor_clip", "=", "tensor_clip", ".", "float", "(", ")", "\n", "", "if", "self", ".", "div_255", ":", "\n", "                ", "tensor_clip", "=", "tensor_clip", ".", "div", "(", "255", ")", "\n", "", "return", "tensor_clip", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.volume_transforms.ToTensor.__call__": [[66, 69], ["torch.from_numpy"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "array", ")", ":", "\n", "        ", "tensor", "=", "torch", ".", "from_numpy", "(", "array", ")", "\n", "return", "tensor", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.Compose.__init__": [[23, 25], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "transforms", ")", ":", "\n", "        ", "self", ".", "transforms", "=", "transforms", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.Compose.__call__": [[26, 30], ["t"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "for", "t", "in", "self", ".", "transforms", ":", "\n", "            ", "clip", "=", "t", "(", "clip", ")", "\n", "", "return", "clip", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.RandomHorizontalFlip.__call__": [[37, 57], ["random.random", "isinstance", "isinstance", "numpy.fliplr", "TypeError", "img.transpose", "type"], "methods", ["None"], ["def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n\n        Returns:\n        PIL.Image or numpy.ndarray: Randomly flipped clip\n        \"\"\"", "\n", "if", "random", ".", "random", "(", ")", "<", "0.5", ":", "\n", "            ", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "                ", "return", "[", "np", ".", "fliplr", "(", "img", ")", "for", "img", "in", "clip", "]", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "                ", "return", "[", "\n", "img", ".", "transpose", "(", "PIL", ".", "Image", ".", "FLIP_LEFT_RIGHT", ")", "for", "img", "in", "clip", "\n", "]", "\n", "", "else", ":", "\n", "                ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image'", "+", "\n", "' but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "", "", "return", "clip", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.RandomResize.__init__": [[71, 74], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "ratio", "=", "(", "3.", "/", "4.", ",", "4.", "/", "3.", ")", ",", "interpolation", "=", "'nearest'", ")", ":", "\n", "        ", "self", ".", "ratio", "=", "ratio", "\n", "self", ".", "interpolation", "=", "interpolation", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.RandomResize.__call__": [[75, 89], ["random.uniform", "isinstance", "int", "int", "functional.resize_clip", "isinstance"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.resize_clip"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "scaling_factor", "=", "random", ".", "uniform", "(", "self", ".", "ratio", "[", "0", "]", ",", "self", ".", "ratio", "[", "1", "]", ")", "\n", "\n", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "im_h", ",", "im_w", ",", "im_c", "=", "clip", "[", "0", "]", ".", "shape", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "            ", "im_w", ",", "im_h", "=", "clip", "[", "0", "]", ".", "size", "\n", "\n", "", "new_w", "=", "int", "(", "im_w", "*", "scaling_factor", ")", "\n", "new_h", "=", "int", "(", "im_h", "*", "scaling_factor", ")", "\n", "new_size", "=", "(", "new_w", ",", "new_h", ")", "\n", "resized", "=", "F", ".", "resize_clip", "(", "\n", "clip", ",", "new_size", ",", "interpolation", "=", "self", ".", "interpolation", ")", "\n", "return", "resized", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.Resize.__init__": [[103, 106], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "size", ",", "interpolation", "=", "'nearest'", ")", ":", "\n", "        ", "self", ".", "size", "=", "size", "\n", "self", ".", "interpolation", "=", "interpolation", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.Resize.__call__": [[107, 111], ["functional.resize_clip"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.resize_clip"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "resized", "=", "F", ".", "resize_clip", "(", "\n", "clip", ",", "self", ".", "size", ",", "interpolation", "=", "self", ".", "interpolation", ")", "\n", "return", "resized", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.RandomCrop.__init__": [[121, 126], ["isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "if", "isinstance", "(", "size", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "size", "=", "(", "size", ",", "size", ")", "\n", "\n", "", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.RandomCrop.__call__": [[127, 157], ["isinstance", "random.randint", "random.randint", "functional.crop_clip", "isinstance", "ValueError", "TypeError", "type"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.crop_clip"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"", "\n", "h", ",", "w", "=", "self", ".", "size", "\n", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "im_h", ",", "im_w", ",", "im_c", "=", "clip", "[", "0", "]", ".", "shape", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "            ", "im_w", ",", "im_h", "=", "clip", "[", "0", "]", ".", "size", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image'", "+", "\n", "'but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "", "if", "w", ">", "im_w", "or", "h", ">", "im_h", ":", "\n", "            ", "error_msg", "=", "(", "\n", "'Initial image size should be larger then '", "\n", "'cropped size but got cropped sizes : ({w}, {h}) while '", "\n", "'initial image is ({im_w}, {im_h})'", ".", "format", "(", "\n", "im_w", "=", "im_w", ",", "im_h", "=", "im_h", ",", "w", "=", "w", ",", "h", "=", "h", ")", ")", "\n", "raise", "ValueError", "(", "error_msg", ")", "\n", "\n", "", "x1", "=", "random", ".", "randint", "(", "0", ",", "im_w", "-", "w", ")", "\n", "y1", "=", "random", ".", "randint", "(", "0", ",", "im_h", "-", "h", ")", "\n", "cropped", "=", "F", ".", "crop_clip", "(", "clip", ",", "y1", ",", "x1", ",", "h", ",", "w", ")", "\n", "\n", "return", "cropped", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.RandomRotation.__init__": [[170, 182], ["isinstance", "ValueError", "len", "ValueError"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "degrees", ")", ":", "\n", "        ", "if", "isinstance", "(", "degrees", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "if", "degrees", "<", "0", ":", "\n", "                ", "raise", "ValueError", "(", "'If degrees is a single number,'", "\n", "'must be positive'", ")", "\n", "", "degrees", "=", "(", "-", "degrees", ",", "degrees", ")", "\n", "", "else", ":", "\n", "            ", "if", "len", "(", "degrees", ")", "!=", "2", ":", "\n", "                ", "raise", "ValueError", "(", "'If degrees is a sequence,'", "\n", "'it must be of len 2.'", ")", "\n", "\n", "", "", "self", ".", "degrees", "=", "degrees", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.RandomRotation.__call__": [[183, 202], ["random.uniform", "isinstance", "isinstance", "scipy.misc.imrotate", "TypeError", "img.rotate", "type"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"", "\n", "angle", "=", "random", ".", "uniform", "(", "self", ".", "degrees", "[", "0", "]", ",", "self", ".", "degrees", "[", "1", "]", ")", "\n", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "rotated", "=", "[", "scipy", ".", "misc", ".", "imrotate", "(", "img", ",", "angle", ")", "for", "img", "in", "clip", "]", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "            ", "rotated", "=", "[", "img", ".", "rotate", "(", "angle", ")", "for", "img", "in", "clip", "]", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image'", "+", "\n", "'but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "\n", "", "return", "rotated", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.CenterCrop.__init__": [[212, 217], ["isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "if", "isinstance", "(", "size", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "size", "=", "(", "size", ",", "size", ")", "\n", "\n", "", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.CenterCrop.__call__": [[218, 248], ["isinstance", "int", "int", "functional.crop_clip", "isinstance", "ValueError", "round", "round", "TypeError", "type"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.crop_clip"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"", "\n", "h", ",", "w", "=", "self", ".", "size", "\n", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "im_h", ",", "im_w", ",", "im_c", "=", "clip", "[", "0", "]", ".", "shape", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "            ", "im_w", ",", "im_h", "=", "clip", "[", "0", "]", ".", "size", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image'", "+", "\n", "'but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "", "if", "w", ">", "im_w", "or", "h", ">", "im_h", ":", "\n", "            ", "error_msg", "=", "(", "\n", "'Initial image size should be larger then '", "\n", "'cropped size but got cropped sizes : ({w}, {h}) while '", "\n", "'initial image is ({im_w}, {im_h})'", ".", "format", "(", "\n", "im_w", "=", "im_w", ",", "im_h", "=", "im_h", ",", "w", "=", "w", ",", "h", "=", "h", ")", ")", "\n", "raise", "ValueError", "(", "error_msg", ")", "\n", "\n", "", "x1", "=", "int", "(", "round", "(", "(", "im_w", "-", "w", ")", "/", "2.", ")", ")", "\n", "y1", "=", "int", "(", "round", "(", "(", "im_h", "-", "h", ")", "/", "2.", ")", ")", "\n", "cropped", "=", "F", ".", "crop_clip", "(", "clip", ",", "y1", ",", "x1", ",", "h", ",", "w", ")", "\n", "\n", "return", "cropped", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.TenCrop.__init__": [[257, 262], ["isinstance"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "if", "isinstance", "(", "size", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "size", "=", "(", "size", ",", "size", ")", "\n", "\n", "", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.TenCrop.__call__": [[263, 319], ["isinstance", "isinstance", "int", "int", "all_x.append", "all_y.append", "all_x.append", "all_y.append", "all_x.append", "all_y.append", "all_x.append", "all_y.append", "cropped.extend", "isinstance", "ValueError", "isinstance", "round", "round", "functional.crop_clip", "functional.crop_clip", "TypeError", "numpy.fliplr", "TypeError", "zip", "zip", "img.transpose", "type", "type"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.crop_clip", "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.crop_clip"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n        img (PIL.Image or numpy.ndarray): List of images to be cropped\n        in format (h, w, c) in numpy.ndarray\n\n        Returns:\n        PIL.Image or numpy.ndarray: Cropped list of images\n        \"\"\"", "\n", "h", ",", "w", "=", "self", ".", "size", "\n", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "im_h", ",", "im_w", ",", "im_c", "=", "clip", "[", "0", "]", ".", "shape", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "            ", "im_w", ",", "im_h", "=", "clip", "[", "0", "]", ".", "size", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image'", "+", "\n", "'but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "", "if", "w", ">", "im_w", "or", "h", ">", "im_h", ":", "\n", "            ", "error_msg", "=", "(", "\n", "'Initial image size should be larger then '", "\n", "'cropped size but got cropped sizes : ({w}, {h}) while '", "\n", "'initial image is ({im_w}, {im_h})'", ".", "format", "(", "\n", "im_w", "=", "im_w", ",", "im_h", "=", "im_h", ",", "w", "=", "w", ",", "h", "=", "h", ")", ")", "\n", "raise", "ValueError", "(", "error_msg", ")", "\n", "\n", "", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "flip_clip", "=", "[", "np", ".", "fliplr", "(", "img", ")", "for", "img", "in", "clip", "]", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "            ", "flip_clip", "=", "[", "img", ".", "transpose", "(", "PIL", ".", "Image", ".", "FLIP_LEFT_RIGHT", ")", "for", "img", "in", "clip", "]", "\n", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image'", "+", "\n", "' but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "\n", "", "x1", "=", "int", "(", "round", "(", "(", "im_w", "-", "w", ")", "/", "2.", ")", ")", "\n", "y1", "=", "int", "(", "round", "(", "(", "im_h", "-", "h", ")", "/", "2.", ")", ")", "\n", "\n", "all_x", "=", "[", "x1", "]", "\n", "all_y", "=", "[", "y1", "]", "\n", "\n", "all_x", ".", "append", "(", "0", ")", "\n", "all_y", ".", "append", "(", "0", ")", "\n", "all_x", ".", "append", "(", "im_w", "-", "w", ")", "\n", "all_y", ".", "append", "(", "0", ")", "\n", "all_x", ".", "append", "(", "0", ")", "\n", "all_y", ".", "append", "(", "im_h", "-", "h", ")", "\n", "all_x", ".", "append", "(", "im_w", "-", "w", ")", "\n", "all_y", ".", "append", "(", "im_h", "-", "h", ")", "\n", "\n", "\n", "#cropped = F.crop_clip(clip, y1, x1, h, w)", "\n", "cropped", "=", "[", "F", ".", "crop_clip", "(", "clip", ",", "y", ",", "x", ",", "h", ",", "w", ")", "for", "x", ",", "y", "in", "zip", "(", "all_x", ",", "all_y", ")", "]", "\n", "flip_cropped", "=", "[", "F", ".", "crop_clip", "(", "flip_clip", ",", "y", ",", "x", ",", "h", ",", "w", ")", "for", "x", ",", "y", "in", "zip", "(", "all_x", ",", "all_y", ")", "]", "\n", "\n", "cropped", ".", "extend", "(", "flip_cropped", ")", "\n", "\n", "return", "cropped", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.ColorJitter.__init__": [[335, 340], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "brightness", "=", "0", ",", "contrast", "=", "0", ",", "saturation", "=", "0", ",", "hue", "=", "0", ")", ":", "\n", "        ", "self", ".", "brightness", "=", "brightness", "\n", "self", ".", "contrast", "=", "contrast", "\n", "self", ".", "saturation", "=", "saturation", "\n", "self", ".", "hue", "=", "hue", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.ColorJitter.get_params": [[341, 365], ["random.uniform", "random.uniform", "random.uniform", "random.uniform", "max", "max", "max"], "methods", ["None"], ["", "def", "get_params", "(", "self", ",", "brightness", ",", "contrast", ",", "saturation", ",", "hue", ")", ":", "\n", "        ", "if", "brightness", ">", "0", ":", "\n", "            ", "brightness_factor", "=", "random", ".", "uniform", "(", "\n", "max", "(", "0", ",", "1", "-", "brightness", ")", ",", "1", "+", "brightness", ")", "\n", "", "else", ":", "\n", "            ", "brightness_factor", "=", "None", "\n", "\n", "", "if", "contrast", ">", "0", ":", "\n", "            ", "contrast_factor", "=", "random", ".", "uniform", "(", "\n", "max", "(", "0", ",", "1", "-", "contrast", ")", ",", "1", "+", "contrast", ")", "\n", "", "else", ":", "\n", "            ", "contrast_factor", "=", "None", "\n", "\n", "", "if", "saturation", ">", "0", ":", "\n", "            ", "saturation_factor", "=", "random", ".", "uniform", "(", "\n", "max", "(", "0", ",", "1", "-", "saturation", ")", ",", "1", "+", "saturation", ")", "\n", "", "else", ":", "\n", "            ", "saturation_factor", "=", "None", "\n", "\n", "", "if", "hue", ">", "0", ":", "\n", "            ", "hue_factor", "=", "random", ".", "uniform", "(", "-", "hue", ",", "hue", ")", "\n", "", "else", ":", "\n", "            ", "hue_factor", "=", "None", "\n", "", "return", "brightness_factor", ",", "contrast_factor", ",", "saturation_factor", ",", "hue_factor", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.ColorJitter.__call__": [[366, 404], ["isinstance", "TypeError", "isinstance", "video_transforms.ColorJitter.get_params", "random.shuffle", "TypeError", "img_transforms.append", "img_transforms.append", "img_transforms.append", "img_transforms.append", "jittered_clip.append", "func", "torchvision.transforms.functional.adjust_brightness", "torchvision.transforms.functional.adjust_saturation", "torchvision.transforms.functional.adjust_hue", "torchvision.transforms.functional.adjust_contrast", "type"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.video_transforms.ColorJitter.get_params", "home.repos.pwc.inspect_result.Anirudh257_strm.None.run.Learner.shuffle"], ["", "def", "__call__", "(", "self", ",", "clip", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n        clip (list): list of PIL.Image\n\n        Returns:\n        list PIL.Image : list of transformed PIL.Image\n        \"\"\"", "\n", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "            ", "raise", "TypeError", "(", "\n", "'Color jitter not yet implemented for numpy arrays'", ")", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "            ", "brightness", ",", "contrast", ",", "saturation", ",", "hue", "=", "self", ".", "get_params", "(", "\n", "self", ".", "brightness", ",", "self", ".", "contrast", ",", "self", ".", "saturation", ",", "self", ".", "hue", ")", "\n", "\n", "# Create img transform function sequence", "\n", "img_transforms", "=", "[", "]", "\n", "if", "brightness", "is", "not", "None", ":", "\n", "                ", "img_transforms", ".", "append", "(", "lambda", "img", ":", "torchvision", ".", "transforms", ".", "functional", ".", "adjust_brightness", "(", "img", ",", "brightness", ")", ")", "\n", "", "if", "saturation", "is", "not", "None", ":", "\n", "                ", "img_transforms", ".", "append", "(", "lambda", "img", ":", "torchvision", ".", "transforms", ".", "functional", ".", "adjust_saturation", "(", "img", ",", "saturation", ")", ")", "\n", "", "if", "hue", "is", "not", "None", ":", "\n", "                ", "img_transforms", ".", "append", "(", "lambda", "img", ":", "torchvision", ".", "transforms", ".", "functional", ".", "adjust_hue", "(", "img", ",", "hue", ")", ")", "\n", "", "if", "contrast", "is", "not", "None", ":", "\n", "                ", "img_transforms", ".", "append", "(", "lambda", "img", ":", "torchvision", ".", "transforms", ".", "functional", ".", "adjust_contrast", "(", "img", ",", "contrast", ")", ")", "\n", "", "random", ".", "shuffle", "(", "img_transforms", ")", "\n", "\n", "# Apply to all images", "\n", "jittered_clip", "=", "[", "]", "\n", "for", "img", "in", "clip", ":", "\n", "                ", "for", "func", "in", "img_transforms", ":", "\n", "                    ", "jittered_img", "=", "func", "(", "img", ")", "\n", "", "jittered_clip", ".", "append", "(", "jittered_img", ")", "\n", "\n", "", "", "else", ":", "\n", "            ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image'", "+", "\n", "'but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "", "return", "jittered_clip", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.crop_clip": [[10, 22], ["isinstance", "isinstance", "TypeError", "img.crop", "type"], "function", ["None"], ["def", "crop_clip", "(", "clip", ",", "min_h", ",", "min_w", ",", "h", ",", "w", ")", ":", "\n", "    ", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "        ", "cropped", "=", "[", "img", "[", "min_h", ":", "min_h", "+", "h", ",", "min_w", ":", "min_w", "+", "w", ",", ":", "]", "for", "img", "in", "clip", "]", "\n", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "        ", "cropped", "=", "[", "\n", "img", ".", "crop", "(", "(", "min_w", ",", "min_h", ",", "min_w", "+", "w", ",", "min_h", "+", "h", ")", ")", "for", "img", "in", "clip", "\n", "]", "\n", "", "else", ":", "\n", "        ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image'", "+", "\n", "'but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "", "return", "cropped", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.resize_clip": [[24, 64], ["isinstance", "isinstance", "isinstance", "TypeError", "functional.get_resize_sizes", "img.resize", "type"], "function", ["home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.get_resize_sizes"], ["", "def", "resize_clip", "(", "clip", ",", "size", ",", "interpolation", "=", "'bilinear'", ")", ":", "\n", "    ", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n", "#        if isinstance(size, numbers.Number):", "\n", "#            im_h, im_w, im_c = clip[0].shape", "\n", "#            # Min spatial dim already matches minimal size", "\n", "#            if (im_w <= im_h and im_w == size) or (im_h <= im_w", "\n", "#                                                   and im_h == size):", "\n", "#                return clip", "\n", "#            new_h, new_w = get_resize_sizes(im_h, im_w, size)", "\n", "#            size = (new_w, new_h)", "\n", "#        else:", "\n", "#            size = size[1], size[0]", "\n", "#        if interpolation == 'bilinear':", "\n", "#            np_inter = cv2.INTER_LINEAR", "\n", "#        else:", "\n", "#            np_inter = cv2.INTER_NEAREST", "\n", "#        scaled = [", "\n", "#            cv2.resize(img, size, interpolation=np_inter) for img in clip", "\n", "#        ]", "\n", "        ", "raise", "NotImplementedError", "\n", "", "elif", "isinstance", "(", "clip", "[", "0", "]", ",", "PIL", ".", "Image", ".", "Image", ")", ":", "\n", "        ", "if", "isinstance", "(", "size", ",", "numbers", ".", "Number", ")", ":", "\n", "            ", "im_w", ",", "im_h", "=", "clip", "[", "0", "]", ".", "size", "\n", "# Min spatial dim already matches minimal size", "\n", "if", "(", "im_w", "<=", "im_h", "and", "im_w", "==", "size", ")", "or", "(", "im_h", "<=", "im_w", "\n", "and", "im_h", "==", "size", ")", ":", "\n", "                ", "return", "clip", "\n", "", "new_h", ",", "new_w", "=", "get_resize_sizes", "(", "im_h", ",", "im_w", ",", "size", ")", "\n", "size", "=", "(", "new_w", ",", "new_h", ")", "\n", "", "else", ":", "\n", "            ", "size", "=", "size", "[", "1", "]", ",", "size", "[", "0", "]", "\n", "", "if", "interpolation", "==", "'bilinear'", ":", "\n", "            ", "pil_inter", "=", "PIL", ".", "Image", ".", "NEAREST", "\n", "", "else", ":", "\n", "            ", "pil_inter", "=", "PIL", ".", "Image", ".", "BILINEAR", "\n", "", "scaled", "=", "[", "img", ".", "resize", "(", "size", ",", "pil_inter", ")", "for", "img", "in", "clip", "]", "\n", "", "else", ":", "\n", "        ", "raise", "TypeError", "(", "'Expected numpy.ndarray or PIL.Image'", "+", "\n", "'but got list of {0}'", ".", "format", "(", "type", "(", "clip", "[", "0", "]", ")", ")", ")", "\n", "", "return", "scaled", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.functional.get_resize_sizes": [[66, 74], ["int", "int"], "function", ["None"], ["", "def", "get_resize_sizes", "(", "im_h", ",", "im_w", ",", "size", ")", ":", "\n", "    ", "if", "im_w", "<", "im_h", ":", "\n", "        ", "ow", "=", "size", "\n", "oh", "=", "int", "(", "size", "*", "im_h", "/", "im_w", ")", "\n", "", "else", ":", "\n", "        ", "oh", "=", "size", "\n", "ow", "=", "int", "(", "size", "*", "im_w", "/", "im_h", ")", "\n", "", "return", "oh", ",", "ow", "\n", "", ""]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.Normalize.__init__": [[17, 20], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "mean", ",", "std", ")", ":", "\n", "        ", "self", ".", "mean", "=", "mean", "\n", "self", ".", "std", "=", "std", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.Normalize.__call__": [[21, 31], ["videotransforms.utils.functional.normalize"], "methods", ["home.repos.pwc.inspect_result.Anirudh257_strm.utils.functional.normalize"], ["", "def", "__call__", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            tensor (Tensor): Tensor of stacked images or image\n            of size (C, H, W) to be normalized\n\n        Returns:\n            Tensor: Normalized stack of image of image\n        \"\"\"", "\n", "return", "F", ".", "normalize", "(", "tensor", ",", "self", ".", "mean", ",", "self", ".", "std", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__init__": [[38, 44], ["None"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "size", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            size (tuple): in format (height, width)\n        \"\"\"", "\n", "self", ".", "size", "=", "size", "\n", "\n"]], "home.repos.pwc.inspect_result.Anirudh257_strm.videotransforms.tensor_transforms.SpatialRandomCrop.__call__": [[45, 60], ["random.randint", "random.randint", "ValueError"], "methods", ["None"], ["", "def", "__call__", "(", "self", ",", "tensor", ")", ":", "\n", "        ", "h", ",", "w", "=", "self", ".", "size", "\n", "_", ",", "_", ",", "tensor_h", ",", "tensor_w", "=", "tensor", ".", "shape", "\n", "\n", "if", "w", ">", "tensor_w", "or", "h", ">", "tensor_h", ":", "\n", "            ", "error_msg", "=", "(", "\n", "'Initial tensor spatial size should be larger then '", "\n", "'cropped size but got cropped sizes : ({w}, {h}) while '", "\n", "'initial tensor is ({t_w}, {t_h})'", ".", "format", "(", "\n", "t_w", "=", "tensor_w", ",", "t_h", "=", "tensor_h", ",", "w", "=", "w", ",", "h", "=", "h", ")", ")", "\n", "raise", "ValueError", "(", "error_msg", ")", "\n", "", "x1", "=", "random", ".", "randint", "(", "0", ",", "tensor_w", "-", "w", ")", "\n", "y1", "=", "random", ".", "randint", "(", "0", ",", "tensor_h", "-", "h", ")", "\n", "cropped", "=", "tensor", "[", ":", ",", ":", ",", "y1", ":", "y1", "+", "h", ",", "x1", ":", "x1", "+", "h", "]", "\n", "return", "cropped", "\n", "", "", ""]], "home.repos.pwc.inspect_result.Anirudh257_strm.utils.images.convert_img": [[4, 12], ["len", "np.expand_dims.transpose", "len", "numpy.expand_dims"], "function", ["None"], ["def", "convert_img", "(", "img", ")", ":", "\n", "    ", "\"\"\"Converts (H, W, C) numpy.ndarray to (C, W, H) format\n    \"\"\"", "\n", "if", "len", "(", "img", ".", "shape", ")", "==", "3", ":", "\n", "        ", "img", "=", "img", ".", "transpose", "(", "2", ",", "0", ",", "1", ")", "\n", "", "if", "len", "(", "img", ".", "shape", ")", "==", "2", ":", "\n", "        ", "img", "=", "np", ".", "expand_dims", "(", "img", ",", "0", ")", "\n", "", "return", "img", "\n", "", ""]], "home.repos.pwc.inspect_result.Anirudh257_strm.utils.functional.normalize": [[1, 11], ["tensor.sub_().div_", "tensor.sub_"], "function", ["None"], ["import", "numbers", "\n", "\n", "#import cv2", "\n", "import", "numpy", "as", "np", "\n", "import", "PIL", "\n", "#from skimage.transform import resize", "\n", "import", "torchvision", "\n", "\n", "\n", "def", "crop_clip", "(", "clip", ",", "min_h", ",", "min_w", ",", "h", ",", "w", ")", ":", "\n", "    ", "if", "isinstance", "(", "clip", "[", "0", "]", ",", "np", ".", "ndarray", ")", ":", "\n"]]}