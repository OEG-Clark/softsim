{"home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.Feature.__init__": [[145, 157], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "seq", ",", "label", ")", ":", "\n", "        ", "self", ".", "label", "=", "label", "\n", "self", ".", "seq_a", "=", "seq", "[", "0", "]", "\n", "self", ".", "seq_b", "=", "seq", "[", "1", "]", "\n", "self", ".", "final_adverse", "=", "seq", "\n", "self", ".", "ori_acc", "=", "None", "\n", "self", ".", "att_acc", "=", "1", "\n", "self", ".", "query", "=", "0", "\n", "self", ".", "change", "=", "0", "\n", "self", ".", "success", "=", "0", "\n", "self", ".", "sim", "=", "0.0", "\n", "self", ".", "changes", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.BERTDataset.__init__": [[187, 191], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "inputs", ",", "masks", ",", "segs", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "masks", "=", "masks", "\n", "self", ".", "segs", "=", "segs", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.BERTDataset.__getitem__": [[192, 197], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "input_ids", "=", "torch", ".", "tensor", "(", "self", ".", "inputs", "[", "index", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "self", ".", "masks", "[", "index", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "self", ".", "segs", "[", "index", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.BERTDataset.__len__": [[198, 200], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.get_sim_embed": [[56, 69], ["numpy.load", "open", "line.split", "len", "len"], "function", ["None"], ["def", "get_sim_embed", "(", "embed_path", ",", "sim_path", ")", ":", "\n", "    ", "id2word", "=", "{", "}", "\n", "word2id", "=", "{", "}", "\n", "\n", "with", "open", "(", "embed_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "ifile", ":", "\n", "        ", "for", "line", "in", "ifile", ":", "\n", "            ", "word", "=", "line", ".", "split", "(", ")", "[", "0", "]", "\n", "if", "word", "not", "in", "id2word", ":", "\n", "                ", "id2word", "[", "len", "(", "id2word", ")", "]", "=", "word", "\n", "word2id", "[", "word", "]", "=", "len", "(", "id2word", ")", "-", "1", "\n", "\n", "", "", "", "cos_sim", "=", "np", ".", "load", "(", "sim_path", ")", "\n", "return", "cos_sim", ",", "word2id", ",", "id2word", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.get_data_cls": [[71, 81], ["enumerate", "open().readlines", "line.strip().split", "int", "features.append", "open", "line.strip"], "function", ["None"], ["", "def", "get_data_cls", "(", "data_path", ")", ":", "\n", "    ", "lines", "=", "open", "(", "data_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", ".", "readlines", "(", ")", "[", "1", ":", "]", "\n", "features", "=", "[", "]", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "        ", "split", "=", "line", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "label", "=", "int", "(", "split", "[", "-", "1", "]", ")", "\n", "seq", "=", "split", "[", "0", "]", "\n", "\n", "features", ".", "append", "(", "[", "seq", ",", "label", "]", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.clean_str": [[83, 102], ["re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.strip", "re.sub.strip().lower", "re.sub.strip"], "function", ["None"], ["", "def", "clean_str", "(", "string", ",", "TREC", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Every dataset is lower cased except for TREC\n    \"\"\"", "\n", "string", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9(),!?\\'\\`]\"", ",", "\" \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'s\"", ",", "\" \\'s\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ve\"", ",", "\" \\'ve\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"n\\'t\"", ",", "\" n\\'t\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'re\"", ",", "\" \\'re\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'d\"", ",", "\" \\'d\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ll\"", ",", "\" \\'ll\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\",\"", ",", "\" , \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"!\"", ",", "\" ! \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\(\"", ",", "\" \\( \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\)\"", ",", "\" \\) \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\?\"", ",", "\" \\? \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", "if", "TREC", "else", "string", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.read_corpus": [[104, 129], ["zip", "open", "list", "random.shuffle", "labels.append", "data.append", "range", "line.partition", "int", "line.partition", "text.lower.lower", "len", "int", "bert_pair_robust.clean_str", "text.lower.strip", "text.lower.strip"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.clean_str"], ["", "def", "read_corpus", "(", "path", ",", "clean", "=", "True", ",", "MR", "=", "True", ",", "encoding", "=", "'utf8'", ",", "shuffle", "=", "False", ",", "lower", "=", "True", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "encoding", "=", "encoding", ")", "as", "fin", ":", "\n", "        ", "for", "line", "in", "fin", ":", "\n", "            ", "if", "MR", ":", "\n", "                ", "label", ",", "sep", ",", "text", "=", "line", ".", "partition", "(", "' '", ")", "\n", "label", "=", "int", "(", "label", ")", "\n", "", "else", ":", "\n", "                ", "label", ",", "sep", ",", "text", "=", "line", ".", "partition", "(", "','", ")", "\n", "label", "=", "int", "(", "label", ")", "-", "1", "\n", "", "if", "clean", ":", "\n", "                ", "text", "=", "clean_str", "(", "text", ".", "strip", "(", ")", ")", "if", "clean", "else", "text", ".", "strip", "(", ")", "\n", "", "if", "lower", ":", "\n", "                ", "text", "=", "text", ".", "lower", "(", ")", "\n", "", "labels", ".", "append", "(", "label", ")", "\n", "data", ".", "append", "(", "text", ")", "\n", "\n", "", "", "if", "shuffle", ":", "\n", "        ", "perm", "=", "list", "(", "range", "(", "len", "(", "data", ")", ")", ")", "\n", "random", ".", "shuffle", "(", "perm", ")", "\n", "data", "=", "[", "data", "[", "i", "]", "for", "i", "in", "perm", "]", "\n", "labels", "=", "[", "labels", "[", "i", "]", "for", "i", "in", "perm", "]", "\n", "\n", "", "return", "zip", "(", "data", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.read_nli_csv": [[130, 141], ["zip", "open", "enumerate", "line.strip().split", "labels.append", "data.append", "sentence1.lower", "sentence2.lower", "int", "line.strip"], "function", ["None"], ["", "def", "read_nli_csv", "(", "path", ",", "encoding", "=", "'utf-8'", ",", "lower", "=", "True", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "labels", "=", "[", "]", "\n", "with", "open", "(", "path", ",", "'r'", ",", "encoding", "=", "encoding", ")", "as", "fin", ":", "\n", "        ", "for", "i", ",", "line", "in", "enumerate", "(", "fin", ")", ":", "\n", "            ", "label", ",", "sentence1", ",", "sentence2", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "sentence1", ",", "sentence2", "=", "sentence1", ".", "lower", "(", ")", ",", "sentence2", ".", "lower", "(", ")", "\n", "labels", ".", "append", "(", "int", "(", "label", ")", ")", "\n", "data", ".", "append", "(", "[", "sentence1", ",", "sentence2", "]", ")", "\n", "\n", "", "", "return", "zip", "(", "data", ",", "labels", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust._tokenize": [[159, 173], ["seq.replace().lower.replace().lower", "seq.replace().lower.split", "tokenizer.tokenize", "keys.append", "len", "seq.replace().lower.replace", "len"], "function", ["None"], ["", "", "def", "_tokenize", "(", "seq", ",", "tokenizer", ")", ":", "\n", "    ", "seq", "=", "seq", ".", "replace", "(", "'\\n'", ",", "''", ")", ".", "lower", "(", ")", "\n", "words", "=", "seq", ".", "split", "(", "' '", ")", "\n", "\n", "sub_words", "=", "[", "]", "\n", "keys", "=", "[", "]", "\n", "index", "=", "0", "\n", "for", "word", "in", "words", ":", "\n", "        ", "sub", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "sub_words", "+=", "sub", "\n", "keys", ".", "append", "(", "[", "index", ",", "index", "+", "len", "(", "sub", ")", "]", ")", "\n", "index", "+=", "len", "(", "sub", ")", "\n", "\n", "", "return", "words", ",", "sub_words", ",", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust._get_masked": [[175, 185], ["len", "range", "masked_ids.append", "masked_words.append"], "function", ["None"], ["", "def", "_get_masked", "(", "words", ",", "filter_words", ",", "keys", ",", "max_length", ")", ":", "\n", "    ", "len_text", "=", "len", "(", "words", ")", "\n", "masked_words", "=", "[", "]", "\n", "masked_ids", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len_text", "-", "1", ")", ":", "\n", "        ", "if", "words", "[", "i", "]", "not", "in", "filter_words", "and", "keys", "[", "i", "]", "[", "1", "]", "<", "max_length", "-", "1", ":", "\n", "            ", "masked_ids", ".", "append", "(", "i", ")", "\n", "masked_words", ".", "append", "(", "words", "[", "0", ":", "i", "]", "+", "[", "'[UNK]'", "]", "+", "words", "[", "i", "+", "1", ":", "]", ")", "\n", "# list of words", "\n", "", "", "return", "masked_ids", ",", "masked_words", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.get_important_scores": [[201, 237], ["bert_pair_robust._get_masked", "bert_pair_robust.BERTDataset", "torch.utils.data.DataLoader", "torch.cat", "torch.cat", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "tokenizer.encode_plus", "all_input_ids.append", "all_masks.append", "all_segs.append", "torch.softmax.append", "len", "len", "tgt_model", "masked_input.cuda", "attn_mask.cuda", "seg_ids.cuda", "torch.index_select", "torch.index_select", "torch.softmax.max"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust._get_masked"], ["", "", "def", "get_important_scores", "(", "seq_a", ",", "words_b", ",", "tgt_model", ",", "orig_prob", ",", "orig_label", ",", "orig_probs", ",", "\n", "tokenizer", ",", "batch_size", ",", "max_length", ",", "filter_words", ",", "keys", ")", ":", "\n", "    ", "masked_ids", ",", "masked_words", "=", "_get_masked", "(", "words_b", ",", "filter_words", ",", "keys", ",", "max_length", ")", "# mask each words (not subwords!)", "\n", "texts", "=", "[", "' '", ".", "join", "(", "words", ")", "for", "words", "in", "masked_words", "]", "# list of text of masked words", "\n", "all_input_ids", "=", "[", "]", "\n", "all_masks", "=", "[", "]", "\n", "all_segs", "=", "[", "]", "\n", "for", "text", "in", "texts", ":", "\n", "        ", "inputs", "=", "tokenizer", ".", "encode_plus", "(", "seq_a", ",", "text", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "max_length", ",", "truncation", "=", "True", ")", "\n", "input_ids", ",", "token_type_ids", "=", "inputs", "[", "\"input_ids\"", "]", ",", "inputs", "[", "\"token_type_ids\"", "]", "\n", "attention_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "padding_length", "=", "max_length", "-", "len", "(", "input_ids", ")", "\n", "input_ids", "=", "input_ids", "+", "(", "padding_length", "*", "[", "0", "]", ")", "\n", "token_type_ids", "=", "token_type_ids", "+", "(", "padding_length", "*", "[", "0", "]", ")", "\n", "attention_mask", "=", "attention_mask", "+", "(", "padding_length", "*", "[", "0", "]", ")", "\n", "all_input_ids", ".", "append", "(", "input_ids", ")", "\n", "all_masks", ".", "append", "(", "attention_mask", ")", "\n", "all_segs", ".", "append", "(", "token_type_ids", ")", "\n", "\n", "", "eval_data", "=", "BERTDataset", "(", "all_input_ids", ",", "all_masks", ",", "all_segs", ")", "\n", "# Run prediction for full data", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "batch_size", ")", "\n", "leave_1_probs", "=", "[", "]", "\n", "for", "batch", "in", "eval_dataloader", ":", "\n", "        ", "masked_input", ",", "seg_ids", ",", "attn_mask", "=", "batch", "\n", "leave_1_prob_batch", "=", "tgt_model", "(", "masked_input", ".", "cuda", "(", ")", ",", "attn_mask", ".", "cuda", "(", ")", ",", "seg_ids", ".", "cuda", "(", ")", ")", "[", "0", "]", "# B num-label", "\n", "#leave_1_prob_batch = tgt_model(masked_input.cuda(), attn_mask.cuda())[0]", "\n", "leave_1_probs", ".", "append", "(", "leave_1_prob_batch", ")", "\n", "", "leave_1_probs", "=", "torch", ".", "cat", "(", "leave_1_probs", ",", "dim", "=", "0", ")", "# words, num-label", "\n", "leave_1_probs", "=", "torch", ".", "softmax", "(", "leave_1_probs", ",", "-", "1", ")", "#", "\n", "leave_1_probs_argmax", "=", "torch", ".", "argmax", "(", "leave_1_probs", ",", "dim", "=", "-", "1", ")", "\n", "import_scores", "=", "(", "orig_prob", "-", "leave_1_probs", "[", ":", ",", "orig_label", "]", "+", "(", "leave_1_probs_argmax", "!=", "orig_label", ")", ".", "float", "(", ")", "\n", "*", "(", "leave_1_probs", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", "-", "torch", ".", "index_select", "(", "orig_probs", ",", "0", ",", "leave_1_probs_argmax", ")", ")", "\n", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "\n", "return", "import_scores", ",", "masked_ids", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.get_substitues": [[239, 261], ["substitutes.size", "zip", "get_bpe_substitues.append", "bert_pair_robust.get_bpe_substitues", "tokenizer._convert_id_to_token", "int"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_bpe_substitues"], ["", "def", "get_substitues", "(", "substitutes", ",", "tokenizer", ",", "mlm_model", ",", "use_bpe", ",", "substitutes_score", "=", "None", ",", "threshold", "=", "3.0", ")", ":", "\n", "# substitues L,k", "\n", "# from this matrix to recover a word", "\n", "    ", "words", "=", "[", "]", "\n", "sub_len", ",", "k", "=", "substitutes", ".", "size", "(", ")", "# sub-len, k", "\n", "\n", "if", "sub_len", "==", "0", ":", "\n", "        ", "return", "words", "\n", "\n", "", "elif", "sub_len", "==", "1", ":", "\n", "        ", "for", "(", "i", ",", "j", ")", "in", "zip", "(", "substitutes", "[", "0", "]", ",", "substitutes_score", "[", "0", "]", ")", ":", "\n", "            ", "if", "threshold", "!=", "0", "and", "j", "<", "threshold", ":", "\n", "                ", "break", "\n", "", "words", ".", "append", "(", "tokenizer", ".", "_convert_id_to_token", "(", "int", "(", "i", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "use_bpe", "==", "1", ":", "\n", "            ", "words", "=", "get_bpe_substitues", "(", "substitutes", ",", "tokenizer", ",", "mlm_model", ")", "\n", "", "else", ":", "\n", "            ", "return", "words", "\n", "#", "\n", "# print(words)", "\n", "", "", "return", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.get_bpe_substitues": [[263, 301], ["range", "torch.CrossEntropyLoss", "torch.tensor", "torch.tensor", "all_substitutes[].to", "all_substitutes[].to.size", "nn.CrossEntropyLoss.", "torch.exp", "torch.exp", "torch.sort", "torch.sort", "substitutes.size", "mlm_model", "word_predictions.view", "all_substitutes[].to.view", "torch.mean", "torch.mean", "tokenizer.convert_tokens_to_string", "final_words.append", "len", "torch.exp.view", "tokenizer._convert_id_to_token", "int", "int", "lev_i.append", "int"], "function", ["None"], ["", "def", "get_bpe_substitues", "(", "substitutes", ",", "tokenizer", ",", "mlm_model", ")", ":", "\n", "# substitutes L, k", "\n", "\n", "    ", "substitutes", "=", "substitutes", "[", "0", ":", "12", ",", "0", ":", "4", "]", "# maximum BPE candidates", "\n", "\n", "# find all possible candidates", "\n", "\n", "all_substitutes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "substitutes", ".", "size", "(", "0", ")", ")", ":", "\n", "        ", "if", "len", "(", "all_substitutes", ")", "==", "0", ":", "\n", "            ", "lev_i", "=", "substitutes", "[", "i", "]", "\n", "all_substitutes", "=", "[", "[", "int", "(", "c", ")", "]", "for", "c", "in", "lev_i", "]", "\n", "", "else", ":", "\n", "            ", "lev_i", "=", "[", "]", "\n", "for", "all_sub", "in", "all_substitutes", ":", "\n", "                ", "for", "j", "in", "substitutes", "[", "i", "]", ":", "\n", "                    ", "lev_i", ".", "append", "(", "all_sub", "+", "[", "int", "(", "j", ")", "]", ")", "\n", "", "", "all_substitutes", "=", "lev_i", "\n", "\n", "# all substitutes  list of list of token-id (all candidates)", "\n", "", "", "c_loss", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "word_list", "=", "[", "]", "\n", "# all_substitutes = all_substitutes[:24]", "\n", "all_substitutes", "=", "torch", ".", "tensor", "(", "all_substitutes", ")", "# [ N, L ]", "\n", "all_substitutes", "=", "all_substitutes", "[", ":", "24", "]", ".", "to", "(", "'cuda'", ")", "\n", "# print(substitutes.size(), all_substitutes.size())", "\n", "N", ",", "L", "=", "all_substitutes", ".", "size", "(", ")", "\n", "word_predictions", "=", "mlm_model", "(", "all_substitutes", ")", "[", "0", "]", "# N L vocab-size", "\n", "ppl", "=", "c_loss", "(", "word_predictions", ".", "view", "(", "N", "*", "L", ",", "-", "1", ")", ",", "all_substitutes", ".", "view", "(", "-", "1", ")", ")", "# [ N*L ]", "\n", "ppl", "=", "torch", ".", "exp", "(", "torch", ".", "mean", "(", "ppl", ".", "view", "(", "N", ",", "L", ")", ",", "dim", "=", "-", "1", ")", ")", "# N", "\n", "_", ",", "word_list", "=", "torch", ".", "sort", "(", "ppl", ")", "\n", "word_list", "=", "[", "all_substitutes", "[", "i", "]", "for", "i", "in", "word_list", "]", "\n", "final_words", "=", "[", "]", "\n", "for", "word", "in", "word_list", ":", "\n", "        ", "tokens", "=", "[", "tokenizer", ".", "_convert_id_to_token", "(", "int", "(", "i", ")", ")", "for", "i", "in", "word", "]", "\n", "text", "=", "tokenizer", ".", "convert_tokens_to_string", "(", "tokens", ")", "\n", "final_words", ".", "append", "(", "text", ")", "\n", "", "return", "final_words", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.attack": [[302, 420], ["bert_pair_robust._tokenize", "bert_pair_robust._tokenize", "tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor().unsqueeze().to.size", "[].squeeze", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "torch.softmax.max", "torch.tensor", "torch.tensor", "[].squeeze", "torch.topk", "torch.topk", "bert_pair_robust.get_important_scores", "int", "sorted", "copy.deepcopy", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "zip", "bert_pair_robust.get_substitues", "tokenizer.convert_tokens_to_string", "len", "tokenizer.convert_tokens_to_ids", "int", "tokenizer.convert_tokens_to_string", "tokenizer.encode_plus", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "[].squeeze", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "feature.changes.append", "tgt_model", "mlm_model", "feature.changes.append", "torch.tensor().unsqueeze().to.unsqueeze().to", "torch.tensor().unsqueeze().to.unsqueeze().to", "torch.tensor().unsqueeze().to.unsqueeze().to", "torch.tensor.to", "len", "len", "len", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "tgt_model", "torch.tensor().unsqueeze().to.unsqueeze", "torch.tensor().unsqueeze().to.unsqueeze", "torch.tensor().unsqueeze().to.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust._tokenize", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust._tokenize", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_important_scores", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_substitues"], ["", "def", "attack", "(", "feature", ",", "tgt_model", ",", "mlm_model", ",", "tokenizer", ",", "k", ",", "batch_size", ",", "max_length", "=", "512", ",", "cos_mat", "=", "None", ",", "w2i", "=", "{", "}", ",", "i2w", "=", "{", "}", ",", "\n", "use_bpe", "=", "1", ",", "threshold_pred_score", "=", "0.3", ")", ":", "\n", "# MLM-process", "\n", "    ", "words", ",", "sub_words_b", ",", "keys", "=", "_tokenize", "(", "feature", ".", "seq_b", ",", "tokenizer", ")", "\n", "_", ",", "sub_words_a", ",", "_", "=", "_tokenize", "(", "feature", ".", "seq_a", ",", "tokenizer", ")", "\n", "\n", "# original label", "\n", "\n", "inputs", "=", "tokenizer", ".", "encode_plus", "(", "feature", ".", "seq_a", ",", "feature", ".", "seq_b", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "max_length", ",", "truncation", "=", "True", ")", "\n", "input_ids", ",", "token_type_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", ")", ",", "torch", ".", "tensor", "(", "inputs", "[", "\"token_type_ids\"", "]", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "input_ids", ")", ")", "\n", "seq_len", "=", "input_ids", ".", "size", "(", "0", ")", "\n", "orig_probs", "=", "tgt_model", "(", "input_ids", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", ",", "\n", "attention_mask", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", ",", "\n", "token_type_ids", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", "\n", ")", "[", "0", "]", ".", "squeeze", "(", ")", "\n", "orig_probs", "=", "torch", ".", "softmax", "(", "orig_probs", ",", "-", "1", ")", "\n", "orig_label", "=", "torch", ".", "argmax", "(", "orig_probs", ")", "\n", "\n", "current_prob", "=", "orig_probs", ".", "max", "(", ")", "\n", "\n", "if", "orig_label", "!=", "feature", ".", "label", ":", "\n", "        ", "feature", ".", "ori_acc", "=", "0", "\n", "feature", ".", "att_acc", "=", "0", "\n", "feature", ".", "success", "=", "-", "1", "\n", "return", "feature", "\n", "", "else", ":", "\n", "        ", "feature", ".", "ori_acc", "=", "1", "\n", "\n", "", "sub_words_b", "=", "[", "'[CLS]'", "]", "+", "sub_words_b", "[", ":", "max_length", "-", "2", "]", "+", "[", "'[SEP]'", "]", "\n", "input_ids_", "=", "torch", ".", "tensor", "(", "[", "tokenizer", ".", "convert_tokens_to_ids", "(", "sub_words_b", ")", "]", ")", "\n", "word_predictions", "=", "mlm_model", "(", "input_ids_", ".", "to", "(", "'cuda'", ")", ")", "[", "0", "]", ".", "squeeze", "(", ")", "# seq-len(sub) vocab", "\n", "word_pred_scores_all", ",", "word_predictions", "=", "torch", ".", "topk", "(", "word_predictions", ",", "k", ",", "-", "1", ")", "# seq-len k", "\n", "\n", "word_predictions", "=", "word_predictions", "[", "1", ":", "len", "(", "sub_words_b", ")", "+", "1", ",", ":", "]", "\n", "word_pred_scores_all", "=", "word_pred_scores_all", "[", "1", ":", "len", "(", "sub_words_b", ")", "+", "1", ",", ":", "]", "\n", "\n", "important_scores", ",", "masked_ids", "=", "get_important_scores", "(", "feature", ".", "seq_a", ",", "words", ",", "tgt_model", ",", "current_prob", ",", "orig_label", ",", "orig_probs", ",", "\n", "tokenizer", ",", "batch_size", ",", "max_length", ",", "filter_words", ",", "keys", ")", "\n", "\n", "feature", ".", "query", "+=", "int", "(", "len", "(", "words", ")", ")", "\n", "list_of_index", "=", "sorted", "(", "zip", "(", "masked_ids", ",", "important_scores", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "# print(list_of_index)", "\n", "final_words", "=", "copy", ".", "deepcopy", "(", "words", ")", "\n", "\n", "count", "=", "0", "\n", "\n", "for", "top_index", "in", "list_of_index", ":", "\n", "        ", "if", "feature", ".", "change", ">", "int", "(", "0.4", "*", "(", "len", "(", "words", ")", ")", ")", ":", "\n", "            ", "feature", ".", "success", "=", "-", "2", "# exceed", "\n", "return", "feature", "\n", "\n", "", "tgt_word", "=", "words", "[", "top_index", "[", "0", "]", "]", "\n", "\n", "substitutes", "=", "word_predictions", "[", "keys", "[", "top_index", "[", "0", "]", "]", "[", "0", "]", ":", "keys", "[", "top_index", "[", "0", "]", "]", "[", "1", "]", "]", "# L, k", "\n", "word_pred_scores", "=", "word_pred_scores_all", "[", "keys", "[", "top_index", "[", "0", "]", "]", "[", "0", "]", ":", "keys", "[", "top_index", "[", "0", "]", "]", "[", "1", "]", "]", "\n", "\n", "substitutes", "=", "get_substitues", "(", "substitutes", ",", "tokenizer", ",", "mlm_model", ",", "use_bpe", ",", "word_pred_scores", ",", "threshold_pred_score", ")", "\n", "\n", "most_gap", "=", "0.0", "\n", "candidate", "=", "None", "\n", "\n", "sub_count", "=", "0", "\n", "\n", "for", "substitute_", "in", "substitutes", ":", "\n", "            ", "substitute", "=", "substitute_", "\n", "\n", "if", "substitute", "==", "tgt_word", ":", "\n", "                ", "continue", "# filter out original word", "\n", "", "if", "'##'", "in", "substitute", ":", "\n", "                ", "continue", "# filter out sub-word", "\n", "\n", "", "if", "substitute", "in", "filter_words", ":", "\n", "                ", "continue", "\n", "", "if", "substitute", "in", "w2i", "and", "tgt_word", "in", "w2i", ":", "\n", "                ", "if", "cos_mat", "[", "w2i", "[", "substitute", "]", "]", "[", "w2i", "[", "tgt_word", "]", "]", "<", "0.7", ":", "\n", "                    ", "continue", "\n", "", "", "temp_replace", "=", "final_words", "\n", "temp_replace", "[", "top_index", "[", "0", "]", "]", "=", "substitute", "\n", "temp_text", "=", "tokenizer", ".", "convert_tokens_to_string", "(", "temp_replace", ")", "\n", "inputs", "=", "tokenizer", ".", "encode_plus", "(", "feature", ".", "seq_a", ",", "temp_text", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "max_length", ",", "\n", "truncation", "=", "True", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"token_type_ids\"", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", "\n", "temp_prob", "=", "tgt_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", ".", "squeeze", "(", ")", "\n", "#temp_prob = tgt_model(input_ids, attention_mask)[0].squeeze()", "\n", "feature", ".", "query", "+=", "1", "\n", "temp_prob", "=", "torch", ".", "softmax", "(", "temp_prob", ",", "-", "1", ")", "\n", "temp_label", "=", "torch", ".", "argmax", "(", "temp_prob", ")", "\n", "\n", "sub_count", "+=", "1", "\n", "\n", "if", "temp_label", "!=", "orig_label", ":", "\n", "                ", "feature", ".", "change", "+=", "1", "\n", "final_words", "[", "top_index", "[", "0", "]", "]", "=", "substitute", "\n", "feature", ".", "changes", ".", "append", "(", "[", "keys", "[", "top_index", "[", "0", "]", "]", "[", "0", "]", ",", "substitute", ",", "tgt_word", "]", ")", "\n", "feature", ".", "final_adverse", "=", "(", "feature", ".", "seq_a", ",", "temp_text", ")", "\n", "feature", ".", "success", "=", "4", "\n", "feature", ".", "att_acc", "=", "0", "\n", "return", "feature", "\n", "", "else", ":", "\n", "                ", "label_prob", "=", "temp_prob", "[", "orig_label", "]", "\n", "gap", "=", "current_prob", "-", "label_prob", "\n", "if", "gap", ">", "most_gap", ":", "\n", "                    ", "most_gap", "=", "gap", "\n", "candidate", "=", "substitute", "\n", "\n", "", "", "", "if", "most_gap", ">", "0", ":", "\n", "            ", "feature", ".", "change", "+=", "1", "\n", "feature", ".", "changes", ".", "append", "(", "[", "keys", "[", "top_index", "[", "0", "]", "]", "[", "0", "]", ",", "candidate", ",", "tgt_word", "]", ")", "\n", "current_prob", "=", "current_prob", "-", "most_gap", "\n", "final_words", "[", "top_index", "[", "0", "]", "]", "=", "candidate", "\n", "count", "+=", "1", "\n", "\n", "", "", "feature", ".", "final_adverse", "=", "(", "feature", ".", "seq_a", ",", "tokenizer", ".", "convert_tokens_to_string", "(", "final_words", ")", ")", "\n", "feature", ".", "success", "=", "2", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.save_to_original_BERT": [[421, 433], ["print", "torch.load", "torch.load", "model.load_state_dict"], "function", ["None"], ["", "def", "save_to_original_BERT", "(", "model", ",", "save_file", ")", ":", "\n", "    ", "print", "(", "'model reload started!!'", ")", "\n", "paras", "=", "torch", ".", "load", "(", "save_file", ")", "\n", "'''\n    paras_new = {}\n    for ele in paras:\n        if 'module.' in ele:\n            paras_new[ele[7:]] = paras[ele]\n        else:\n            paras_new[ele] = paras[ele]\n    '''", "\n", "model", ".", "load_state_dict", "(", "paras", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.run_attack": [[437, 534], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "str", "str", "str", "str", "bert_pair_robust.read_nli_csv", "print", "transformers.BertTokenizer.from_pretrained", "transformers.BertConfig.from_pretrained", "transformers.BertForMaskedLM.from_pretrained", "BertForMaskedLM.from_pretrained.to", "transformers.BertConfig.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained", "bert_pair_robust.save_to_original_BERT", "BertForSequenceClassification.from_pretrained.to", "print", "print", "open", "bert_pair_robust.get_sim_embed", "os.path.join", "torch.no_grad", "torch.no_grad", "enumerate", "print", "bert_pair_robust.Feature", "print", "ori_acc.append", "att_acc.append", "features_output.append", "bert_pair_robust.attack", "q_num.append", "perturb.append", "open.write", "print", "print", "parser.parse_args.data_path.split", "print", "print", "sum", "len", "sum", "len", "sum", "len", "sum", "len", "len", "len", "attack.seq_a.strip().split", "attack.seq_b.strip().split", "attack.seq_a.strip", "attack.seq_b.strip", "attack.seq_b.strip", "attack.seq_a.strip", "str"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.bert_pair_robust.read_nli_csv", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.save_to_original_BERT", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_sim_embed", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.attack"], ["", "def", "run_attack", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_path\"", ",", "type", "=", "str", ",", "default", "=", "\"Robust/qnli.txt\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/qnli_smart_vda.pt\"", ",", "help", "=", "\"xxx classifier\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "type", "=", "str", ",", "default", "=", "\"data_defense\"", ",", "help", "=", "\"train file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_sim_mat\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'whether use cosine_similarity to filter out atonyms'", ")", "\n", "parser", ".", "add_argument", "(", "\"--start\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"start step, for multi-thread process\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--end\"", ",", "type", "=", "int", ",", "default", "=", "2000", ",", "help", "=", "\"end step, for multi-thread process\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_bpe\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--k\"", ",", "type", "=", "int", ",", "default", "=", "48", ")", "\n", "parser", ".", "add_argument", "(", "\"--threshold_pred_score\"", ",", "type", "=", "float", ",", "default", "=", "0", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "data_path", "=", "str", "(", "args", ".", "data_path", ")", "\n", "mlm_path", "=", "str", "(", "args", ".", "mlm_path", ")", "\n", "tgt_path", "=", "str", "(", "args", ".", "tgt_path", ")", "\n", "output_dir", "=", "str", "(", "args", ".", "output_dir", ")", "\n", "\n", "features", "=", "read_nli_csv", "(", "args", ".", "data_path", ")", "\n", "\n", "\n", "# features = get_data_cls(data_path)", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "use_bpe", "=", "args", ".", "use_bpe", "\n", "k", "=", "args", ".", "k", "\n", "threshold_pred_score", "=", "args", ".", "threshold_pred_score", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "mlm_path", ")", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "mlm_path", ",", "config", "=", "config_atk", ")", "\n", "mlm_model", ".", "to", "(", "'cuda'", ")", "\n", "\n", "config_tgt", "=", "BertConfig", ".", "from_pretrained", "(", "mlm_path", ",", "num_labels", "=", "num_label", ")", "\n", "tgt_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "mlm_path", ",", "config", "=", "config_tgt", ")", "\n", "save_to_original_BERT", "(", "tgt_model", ",", "tgt_path", ")", "\n", "tgt_model", ".", "to", "(", "'cuda'", ")", "\n", "\n", "print", "(", "'loading sim-embed'", ")", "\n", "\n", "if", "args", ".", "use_sim_mat", "==", "1", ":", "\n", "        ", "cos_mat", ",", "w2i", ",", "i2w", "=", "get_sim_embed", "(", "'counter-fitted-vectors.txt'", ",", "'cos_sim_counter_fitting.npy'", ")", "\n", "", "else", ":", "\n", "        ", "cos_mat", ",", "w2i", ",", "i2w", "=", "None", ",", "{", "}", ",", "{", "}", "\n", "\n", "", "print", "(", "'finish get-sim-embed'", ")", "\n", "features_output", "=", "[", "]", "\n", "out_f", "=", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "args", ".", "data_path", ".", "split", "(", "'/'", ")", "[", "-", "2", "]", "+", "'_adversaries_test_new.txt'", ")", ",", "'w'", ")", "\n", "\n", "ori_acc", "=", "[", "]", "\n", "att_acc", "=", "[", "]", "\n", "q_num", "=", "[", "]", "\n", "perturb", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "index", ",", "feature", "in", "enumerate", "(", "features", ")", ":", "\n", "# print(feature)", "\n", "            ", "seq_a", ",", "label", "=", "feature", "\n", "feat", "=", "Feature", "(", "seq_a", ",", "label", ")", "\n", "print", "(", "'\\r number {:d} '", ".", "format", "(", "index", ")", "+", "tgt_path", ",", "end", "=", "''", ")", "\n", "# print(feat.seq[:100], feat.label)", "\n", "\n", "try", ":", "\n", "                ", "feat", "=", "attack", "(", "feat", ",", "tgt_model", ",", "mlm_model", ",", "tokenizer_tgt", ",", "k", ",", "batch_size", "=", "32", ",", "max_length", "=", "256", ",", "\n", "cos_mat", "=", "cos_mat", ",", "w2i", "=", "w2i", ",", "i2w", "=", "i2w", ",", "use_bpe", "=", "use_bpe", ",", "\n", "threshold_pred_score", "=", "threshold_pred_score", ")", "\n", "", "except", ":", "\n", "                ", "print", "(", "feature", ")", "\n", "print", "(", ")", "\n", "\n", "# print(feat.changes, feat.change, feat.query, feat.success)", "\n", "", "ori_acc", ".", "append", "(", "feat", ".", "ori_acc", ")", "\n", "att_acc", ".", "append", "(", "feat", ".", "att_acc", ")", "\n", "if", "feat", ".", "ori_acc", "==", "1", ":", "\n", "                ", "q_num", ".", "append", "(", "feat", ".", "query", ")", "\n", "perturb", ".", "append", "(", "feat", ".", "change", "/", "(", "len", "(", "feat", ".", "seq_a", ".", "strip", "(", ")", ".", "split", "(", ")", ")", "+", "len", "(", "feat", ".", "seq_b", ".", "strip", "(", ")", ".", "split", "(", ")", ")", ")", ")", "\n", "\n", "", "if", "True", ":", "#feat.success >= 2:", "\n", "                ", "new_line", "=", "str", "(", "feat", ".", "label", ")", "+", "'\\t'", "+", "feat", ".", "seq_a", ".", "strip", "(", ")", "+", "'\\t'", "+", "feat", ".", "seq_b", ".", "strip", "(", ")", "+", "'\\t'", "+", "feat", ".", "final_adverse", "[", "0", "]", "+", "'\\t'", "+", "feat", ".", "final_adverse", "[", "1", "]", "+", "'\\n'", "\n", "out_f", ".", "write", "(", "new_line", ")", "\n", "print", "(", "'success'", ",", "end", "=", "''", ")", "\n", "", "else", ":", "\n", "                ", "print", "(", "'failed'", ",", "end", "=", "''", ")", "\n", "", "features_output", ".", "append", "(", "feat", ")", "\n", "\n", "", "print", "(", "'original accuracy is %f, attack accuracy is %f, query num is %f, perturb rate is %f'", "\n", "%", "(", "sum", "(", "ori_acc", ")", "/", "len", "(", "ori_acc", ")", ",", "sum", "(", "att_acc", ")", "/", "len", "(", "att_acc", ")", ",", "sum", "(", "q_num", ")", "/", "len", "(", "q_num", ")", ",", "sum", "(", "perturb", ")", "/", "len", "(", "perturb", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.EmbAdapterDataset.__init__": [[16, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "data", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "args", ".", "max_length", "\n", "self", ".", "pad_token", "=", "0", "\n", "self", ".", "mask_ids", "=", "103", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.EmbAdapterDataset.__getitem__": [[23, 31], ["text_classifier_smart.EmbAdapterDataset.tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "x", ",", "y", "=", "self", ".", "data", "[", "index", "]", "\n", "inputs", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "x", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "max_length", ",", "truncation", "=", "True", ")", "\n", "padding_length", "=", "self", ".", "max_length", "-", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", "+", "padding_length", "*", "[", "self", ".", "pad_token", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "self", ".", "max_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.EmbAdapterDataset.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.FreeLB.__init__": [[36, 45], ["torch.Module.__init__", "transformers.BertConfig.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained().cuda", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.KLDivLoss", "torch.KLDivLoss", "transformers.BertForSequenceClassification.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "nclasses", ")", ":", "\n", "        ", "super", "(", "FreeLB", ",", "self", ")", ".", "__init__", "(", ")", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "num_labels", "=", "nclasses", ")", "\n", "self", ".", "tgt_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "config", "=", "config_atk", ")", ".", "cuda", "(", ")", "\n", "self", ".", "variance", "=", "args", ".", "variation", "\n", "self", ".", "variance_vda", "=", "args", ".", "variation_vda", "\n", "self", ".", "step", "=", "args", ".", "step", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "kl_criterion", "=", "nn", ".", "KLDivLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.FreeLB.forward": [[46, 48], ["text_classifier_smart.FreeLB.tgt_model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", ":", "\n", "        ", "return", "self", ".", "tgt_model", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.FreeLB.output_with_emb": [[49, 64], ["text_classifier_smart.FreeLB.tgt_model.bert.encoder", "text_classifier_smart.FreeLB.tgt_model.dropout", "text_classifier_smart.FreeLB.tgt_model.classifier", "text_classifier_smart.FreeLB.tgt_model.bert.pooler"], "methods", ["None"], ["", "def", "output_with_emb", "(", "self", ",", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "encoder_extended_attention_mask", ")", ":", "\n", "        ", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.FreeLB.adv_train": [[65, 130], ["input_ids.size", "text_classifier_smart.FreeLB.tgt_model.bert.get_extended_attention_mask", "text_classifier_smart.FreeLB.tgt_model.bert.get_head_mask", "text_classifier_smart.FreeLB.tgt_model.bert.embeddings", "text_classifier_smart.FreeLB.output_with_emb", "text_classifier_smart.FreeLB.output_with_emb", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "text_classifier_smart.FreeLB.output_with_emb", "text_classifier_smart.FreeLB.output_with_emb", "text_classifier_smart.FreeLB.criterion", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_classifier_smart.FreeLB.tgt_model.bert.embeddings", "text_classifier_smart.FreeLB.output_with_emb", "total_loss.backward", "optim.step", "scheduler.step", "text_classifier_smart.FreeLB.tgt_model.zero_grad", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "text_classifier_smart.FreeLB.cpu().item", "n_loss.cpu().item", "vda_loss.cpu().item", "text_classifier_smart.FreeLB.size", "text_classifier_smart.FreeLB.kl_criterion", "text_classifier_smart.FreeLB.kl_criterion", "text_classifier_smart.FreeLB.kl_criterion", "text_classifier_smart.FreeLB.kl_criterion", "torch.softmax.size", "torch.softmax.size", "torch.clamp.cuda", "torch.clamp.cuda", "text_classifier_smart.FreeLB.kl_criterion", "text_classifier_smart.FreeLB.kl_criterion", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "text_classifier_smart.FreeLB.cpu", "n_loss.cpu", "vda_loss.cpu", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb"], ["", "def", "adv_train", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ",", "y", ",", "optim", ",", "scheduler", ",", "mlm_model", ")", ":", "\n", "        ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_masks", ",", "\n", "input_shape", ",", "device", ")", "\n", "\n", "encoder_extended_attention_mask", "=", "None", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", ")", "\n", "\n", "noise", "=", "torch", ".", "randn", "(", "embedding_output", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "self", ".", "variance", "\n", "noise", ".", "requires_grad", "=", "True", "\n", "\n", "n_embedding_output", "=", "embedding_output", "+", "noise", "\n", "n_logits", "=", "self", ".", "output_with_emb", "(", "n_embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "logits", "=", "self", ".", "output_with_emb", "(", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "\n", "n_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "n_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "n_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "noise_grad", "=", "torch", ".", "autograd", ".", "grad", "(", "n_loss", ",", "noise", ")", "[", "0", "]", "\n", "noise", "=", "noise", "+", "(", "noise_grad", "/", "torch", ".", "norm", "(", "noise_grad", ")", ")", ".", "mul_", "(", "1e-3", ")", "\n", "noise", "=", "torch", ".", "clamp", "(", "noise", ",", "-", "self", ".", "variance", ",", "self", ".", "variance", ")", "\n", "#update the noise finish", "\n", "\n", "n_embedding_output", "=", "embedding_output", "+", "noise", "\n", "n_logits", "=", "self", ".", "output_with_emb", "(", "n_embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "logits", "=", "self", ".", "output_with_emb", "(", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "n_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "n_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "n_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "logits", ",", "y", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "self", ".", "variance_vda", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ".", "cuda", "(", ")", ",", "-", "1", ")", "# [B, Len, V]", "\n", "word_embs", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "vda_logits", "=", "self", ".", "output_with_emb", "(", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "vda_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "vda_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "vda_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "\n", "total_loss", "=", "loss", "+", "n_loss", "+", "vda_loss", "\n", "total_loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "#print({name:param.grad for name, param in self.tgt_model.classifier.named_parameters()})", "\n", "scheduler", ".", "step", "(", ")", "\n", "self", ".", "tgt_model", ".", "zero_grad", "(", ")", "\n", "\n", "return", "loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "n_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "vda_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.FreeLB.defense": [[131, 182], ["input_ids.size", "text_classifier_smart.FreeLB.tgt_model.bert.get_extended_attention_mask", "text_classifier_smart.FreeLB.tgt_model.bert.get_head_mask", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_classifier_smart.FreeLB.tgt_model.bert.embeddings", "text_classifier_smart.FreeLB.tgt_model.bert.encoder", "text_classifier_smart.FreeLB.tgt_model.classifier", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "text_classifier_smart.FreeLB.tgt_model.bert.pooler", "torch.softmax.size", "torch.softmax.size", "noise.cuda"], "methods", ["None"], ["", "def", "defense", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "mlm_model", ")", ":", "\n", "        ", "output_attentions", "=", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_hidden_states", ")", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "\n", "input_shape", ",", "device", ")", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ")", "*", "self", ".", "variance_vda", "\n", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ".", "cuda", "(", ")", ",", "-", "1", ")", "# [B, Len, V]", "\n", "\n", "word_embs", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "# logits = self.output_linear(prompt_output[0])", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.reader_data": [[183, 195], ["open", "line.strip().split", "output.append", "int", "line.strip", "int"], "function", ["None"], ["", "", "def", "reader_data", "(", "filename", ",", "add", "=", "True", ")", ":", "\n", "#return [[ori_text, adv_text, label], ... ,[]]", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "output", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "label", ",", "text", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "add", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "-", "1", "\n", "", "else", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "\n", "", "output", ".", "append", "(", "[", "text", ",", "label", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smart.run": [[197, 298], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "text_classifier_smart.reader_data", "text_classifier_smart.reader_data", "text_classifier_smart.reader_data", "print", "transformers.BertForMaskedLM.from_pretrained().cuda", "transformers.BertTokenizer.from_pretrained", "text_classifier_smart.EmbAdapterDataset", "text_classifier_smart.EmbAdapterDataset", "text_classifier_smart.EmbAdapterDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "text_classifier_smart.FreeLB", "model.cuda.cuda", "model.cuda.parameters", "transformers.AdamW", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup", "range", "str", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "model.cuda.train", "tqdm.tqdm", "print", "model.cuda.eval", "print", "print", "transformers.BertForMaskedLM.from_pretrained", "len", "model.cuda.zero_grad", "model.cuda.adv_train", "total_loss.append", "torch.no_grad", "torch.no_grad", "torch.save", "torch.save", "print", "torch.no_grad", "torch.no_grad", "ele.cuda", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "model.cuda.tgt_model.state_dict", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "ele.cuda", "model.cuda.", "ele.cuda", "model.cuda.", "sum", "len", "sum", "len", "sum", "len", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.adv_train"], ["", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "type", "=", "str", ",", "default", "=", "\"mr\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../TextFooler/target_models/mr1\"", ",", "\n", "help", "=", "\"xxx classifier\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/mr_smart_vda.pt\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "16", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_warmup\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--variation\"", ",", "type", "=", "float", ",", "default", "=", "0.02", ")", "\n", "parser", ".", "add_argument", "(", "\"--variation_vda\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--step\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "data_path", "=", "'data/'", "+", "str", "(", "args", ".", "dataset", ")", "\n", "if", "args", ".", "dataset", "==", "'ag'", "or", "args", ".", "dataset", "==", "'yelp'", ":", "\n", "        ", "add", "=", "True", "\n", "", "else", ":", "\n", "        ", "add", "=", "False", "\n", "", "train_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'train.txt'", ")", ",", "add", ")", "\n", "dev_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'dev.txt'", ")", ",", "add", ")", "\n", "test_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'test.txt'", ")", ",", "add", ")", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "args", ".", "mlm_path", ")", ".", "cuda", "(", ")", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "train_data", "=", "EmbAdapterDataset", "(", "args", ",", "train_features", ",", "tokenizer_tgt", ")", "\n", "dev_data", "=", "EmbAdapterDataset", "(", "args", ",", "dev_features", ",", "tokenizer_tgt", ")", "\n", "test_data", "=", "EmbAdapterDataset", "(", "args", ",", "test_features", ",", "tokenizer_tgt", ")", "\n", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "dev_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "test_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "print", "(", "'start building model'", ")", "\n", "model", "=", "FreeLB", "(", "args", ",", "args", ".", "num_label", ")", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "params", "=", "model", ".", "parameters", "(", ")", "\n", "#need_grad = lambda x: x.requires_grad", "\n", "optimizer", "=", "AdamW", "(", "\n", "params", ",", "\n", "lr", "=", "args", ".", "lr", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.01", ",", "\n", ")", "\n", "total_num", "=", "len", "(", "train_data", ")", "//", "args", ".", "batch_size", "*", "args", ".", "max_epoch", "\n", "scheduler", "=", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "num_warmup", "*", "total_num", ",", "\n", "num_training_steps", "=", "total_num", ")", "\n", "best_ratio", "=", "0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "max_epoch", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "total_loss", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "train_dataloader", ")", ":", "\n", "            ", "model", ".", "zero_grad", "(", ")", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "#origin_emb = model.produce_emb(input_ids, attention_mask, token_type_ids)", "\n", "loss", ",", "kl_loss", ",", "vda_loss", "=", "model", ".", "adv_train", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "y", ",", "optimizer", ",", "scheduler", ",", "mlm_model", ")", "\n", "total_loss", ".", "append", "(", "[", "loss", ",", "kl_loss", ",", "vda_loss", "]", ")", "\n", "\n", "", "print", "(", "'Epoch %d, the training ce loss is %f, the kl loss is %f, vda loss is %f, the total number is %f'", "\n", "%", "(", "epoch", ",", "sum", "(", "[", "ele", "[", "0", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "\n", "sum", "(", "[", "ele", "[", "1", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "sum", "(", "[", "ele", "[", "2", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "dev_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "if", "best_ratio", "<", "attack_ratio", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "tgt_model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", ")", "\n", "print", "(", "'--------save once-----------'", ")", "\n", "best_ratio", "=", "attack_ratio", "\n", "", "print", "(", "'The dev set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "test_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "print", "(", "'The test set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier.EmbAdapterDataset.__init__": [[16, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "data", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "args", ".", "max_length", "\n", "self", ".", "pad_token", "=", "0", "\n", "self", ".", "mask_ids", "=", "103", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier.EmbAdapterDataset.__getitem__": [[23, 31], ["text_classifier.EmbAdapterDataset.tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "x", ",", "y", "=", "self", ".", "data", "[", "index", "]", "\n", "inputs", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "x", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "max_length", ",", "truncation", "=", "True", ")", "\n", "padding_length", "=", "self", ".", "max_length", "-", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", "+", "padding_length", "*", "[", "self", ".", "pad_token", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "self", ".", "max_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier.EmbAdapterDataset.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier.EmbAdapterModel.__init__": [[36, 45], ["torch.Module.__init__", "transformers.BertConfig.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "EmbAdapterModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "masked_id", "=", "103", "\n", "self", ".", "pad_id", "=", "0", "\n", "self", ".", "variance", "=", "args", ".", "variance", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "num_labels", "=", "args", ".", "num_label", ")", "\n", "#config_atk.attention_probs_dropout_prob=0", "\n", "#config_atk.hidden_dropout_prob=0", "\n", "self", ".", "tgt_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "config", "=", "config_atk", ")", "\n", "#save_to_original_BERT(self.tgt_model, args.tgt_path)", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier.EmbAdapterModel.forward": [[48, 50], ["text_classifier.EmbAdapterModel.tgt_model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", ":", "\n", "        ", "return", "self", ".", "tgt_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier.EmbAdapterModel.produce_emb": [[51, 59], ["text_classifier.EmbAdapterModel.tgt_model.bert.embeddings"], "methods", ["None"], ["", "def", "produce_emb", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier.EmbAdapterModel.defense": [[60, 112], ["input_ids.size", "text_classifier.EmbAdapterModel.tgt_model.bert.get_extended_attention_mask", "text_classifier.EmbAdapterModel.tgt_model.bert.get_head_mask", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_classifier.EmbAdapterModel.tgt_model.bert.embeddings", "text_classifier.EmbAdapterModel.tgt_model.bert.encoder", "text_classifier.EmbAdapterModel.tgt_model.classifier", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "text_classifier.EmbAdapterModel.tgt_model.bert.pooler", "torch.softmax.size", "torch.softmax.size", "noise.cuda"], "methods", ["None"], ["", "def", "defense", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "mlm_model", ")", ":", "\n", "        ", "output_attentions", "=", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_hidden_states", ")", "\n", "return_dict", "=", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "use_return_dict", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "self", ".", "variance", "\n", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ".", "cuda", "(", ")", ",", "-", "1", ")", "#[B, Len, V]", "\n", "\n", "word_embs", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "#logits = self.output_linear(prompt_output[0])", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier.reader_data": [[113, 125], ["open", "line.strip().split", "output.append", "int", "line.strip", "int"], "function", ["None"], ["", "", "def", "reader_data", "(", "filename", ",", "add", "=", "True", ")", ":", "\n", "#return [[ori_text, adv_text, label], ... ,[]]", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "output", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "label", ",", "text", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "add", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "-", "1", "\n", "", "else", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "\n", "", "output", ".", "append", "(", "[", "text", ",", "label", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier.run": [[127, 242], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "text_classifier.reader_data", "text_classifier.reader_data", "text_classifier.reader_data", "print", "transformers.BertTokenizer.from_pretrained", "text_classifier.EmbAdapterDataset", "text_classifier.EmbAdapterDataset", "text_classifier.EmbAdapterDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "transformers.BertForMaskedLM.from_pretrained().cuda", "text_classifier.EmbAdapterModel", "model.cuda.cuda", "torch.CrossEntropyLoss", "torch.KLDivLoss", "model.cuda.parameters", "transformers.AdamW", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup", "range", "str", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "model.cuda.train", "tqdm.tqdm", "print", "model.cuda.eval", "print", "print", "transformers.BertForMaskedLM.from_pretrained", "len", "model.cuda.zero_grad", "model.cuda.defense", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "nn.CrossEntropyLoss.", "total_loss.append", "transformers.AdamW.step", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup.step", "torch.no_grad", "torch.no_grad", "torch.save", "torch.save", "print", "torch.no_grad", "torch.no_grad", "ele.cuda", "model.cuda.", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "model.cuda.tgt_model.state_dict", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "torch.sum().cpu", "torch.sum().cpu", "nn.KLDivLoss.", "nn.KLDivLoss.", "criterion.cpu().item", "kl_loss.cpu().item", "ele.cuda", "model.cuda.", "ele.cuda", "model.cuda.", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "sum", "len", "sum", "len", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "criterion.cpu", "kl_loss.cpu", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.defense"], ["", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "type", "=", "str", ",", "default", "=", "\"yelp\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../TextFooler/target_models/mr1\"", ",", "\n", "help", "=", "\"xxx classifier\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/yelp_vda.pt\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "type", "=", "int", ",", "default", "=", "512", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_warmup\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--variance\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "data_path", "=", "'data/'", "+", "str", "(", "args", ".", "dataset", ")", "\n", "if", "args", ".", "dataset", "==", "'ag'", "or", "args", ".", "dataset", "==", "'yelp'", ":", "\n", "        ", "add", "=", "True", "\n", "", "else", ":", "\n", "        ", "add", "=", "False", "\n", "", "train_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'train.txt'", ")", ",", "add", ")", "\n", "dev_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'dev.txt'", ")", ",", "add", ")", "\n", "test_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'test.txt'", ")", ",", "add", ")", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "train_data", "=", "EmbAdapterDataset", "(", "args", ",", "train_features", ",", "tokenizer_tgt", ")", "\n", "dev_data", "=", "EmbAdapterDataset", "(", "args", ",", "dev_features", ",", "tokenizer_tgt", ")", "\n", "test_data", "=", "EmbAdapterDataset", "(", "args", ",", "test_features", ",", "tokenizer_tgt", ")", "\n", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "dev_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "test_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "print", "(", "'start building model'", ")", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "args", ".", "mlm_path", ")", ".", "cuda", "(", ")", "\n", "model", "=", "EmbAdapterModel", "(", "args", ")", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "kl_criterion", "=", "nn", ".", "KLDivLoss", "(", ")", "\n", "\n", "params", "=", "model", ".", "parameters", "(", ")", "\n", "#need_grad = lambda x: x.requires_grad", "\n", "optimizer", "=", "AdamW", "(", "\n", "params", ",", "\n", "lr", "=", "args", ".", "lr", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.01", ",", "\n", ")", "\n", "total_num", "=", "len", "(", "train_data", ")", "//", "args", ".", "batch_size", "*", "args", ".", "max_epoch", "\n", "scheduler", "=", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "num_warmup", "*", "total_num", ",", "\n", "num_training_steps", "=", "total_num", ")", "\n", "best_ratio", "=", "0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "max_epoch", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "total_loss", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "train_dataloader", ")", ":", "\n", "            ", "model", ".", "zero_grad", "(", ")", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "#origin_emb = model.produce_emb(input_ids, attention_mask, token_type_ids)", "\n", "probs", "=", "model", ".", "defense", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "mlm_model", ")", "# , token_type_ids)[0]", "\n", "ori_probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "\n", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "\n", "kl_loss", "=", "0.5", "*", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "probs", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "ori_probs", ",", "-", "1", ")", ")", "+", "0.5", "*", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "ori_probs", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "probs", ",", "-", "1", ")", ")", "\n", "\n", "loss", "=", "criterion", "(", "ori_probs", ",", "y", ")", "\n", "total_loss", ".", "append", "(", "[", "loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "kl_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", "]", ")", "\n", "(", "loss", "+", "kl_loss", ")", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "", "print", "(", "'Epoch %d, the training ce loss is %f, kl loss %f, success attack ratio is %f, the total number is %f'", "\n", "%", "(", "epoch", ",", "sum", "(", "[", "ele", "[", "0", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "sum", "(", "[", "ele", "[", "1", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "dev_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "\n", "", "", "if", "best_ratio", "<", "attack_ratio", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "tgt_model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", ")", "\n", "print", "(", "'--------save once-----------'", ")", "\n", "best_ratio", "=", "attack_ratio", "\n", "", "print", "(", "'The dev set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "test_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "print", "(", "'The test set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.EmbAdapterDataset.__init__": [[20, 26], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "data", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "args", ".", "max_length", "\n", "self", ".", "pad_token", "=", "0", "\n", "self", ".", "mask_ids", "=", "103", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.EmbAdapterDataset.__getitem__": [[27, 35], ["text_classifier_smix.EmbAdapterDataset.tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "x", ",", "y", "=", "self", ".", "data", "[", "index", "]", "\n", "inputs", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "x", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "max_length", ",", "truncation", "=", "True", ")", "\n", "padding_length", "=", "self", ".", "max_length", "-", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", "+", "padding_length", "*", "[", "self", ".", "pad_token", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "self", ".", "max_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.EmbAdapterDataset.__len__": [[36, 38], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.BertEncoder4SentMix.__init__": [[40, 48], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformers.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder4SentMix", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# self.output_attentions = config.output_attentions", "\n", "# self.output_hidden_states = config.output_hidden_states", "\n", "self", ".", "output_attentions", "=", "False", "\n", "self", ".", "output_hidden_states", "=", "True", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "\n", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.BertEncoder4SentMix.forward": [[49, 109], ["enumerate", "layer_module", "layer_module", "layer_module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "hidden_states2", "=", "None", ",", "l", "=", "None", ",", "mix_layer", "=", "1000", ",", "attention_mask", "=", "None", ",", "\n", "attention_mask2", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "\n", "all_attentions", "=", "(", ")", "\n", "\n", "# Perform mix at till the mix_layer", "\n", "## mix_layer == -1: mixup at embedding layer", "\n", "if", "mix_layer", "==", "-", "1", ":", "\n", "            ", "if", "hidden_states2", "is", "not", "None", ":", "\n", "                ", "hidden_states", "=", "l", "*", "hidden_states", "+", "(", "1", "-", "l", ")", "*", "hidden_states2", "\n", "\n", "", "", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "i", "<=", "mix_layer", ":", "\n", "\n", "                ", "if", "self", ".", "output_hidden_states", ":", "\n", "                    ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "head_mask", "[", "i", "]", ")", "\n", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "if", "self", ".", "output_attentions", ":", "\n", "                    ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "", "if", "hidden_states2", "is", "not", "None", ":", "\n", "                    ", "layer_outputs2", "=", "layer_module", "(", "\n", "hidden_states2", ",", "attention_mask2", ",", "head_mask", "[", "i", "]", ")", "\n", "hidden_states2", "=", "layer_outputs2", "[", "0", "]", "\n", "\n", "", "", "if", "i", "==", "mix_layer", ":", "\n", "                ", "if", "hidden_states2", "is", "not", "None", ":", "\n", "# hidden_states = l * hidden_states + (1-l)*hidden_states2", "\n", "# attention_mask = attention_mask.long() | attention_mask2.long()", "\n", "# sentMix: (bsz, len, hid)", "\n", "                    ", "hidden_states", "[", ":", ",", "0", ",", ":", "]", "=", "l", "*", "hidden_states", "[", ":", ",", "0", ",", ":", "]", "+", "(", "1", "-", "l", ")", "*", "hidden_states2", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "", "", "if", "i", ">", "mix_layer", ":", "\n", "                ", "if", "self", ".", "output_hidden_states", ":", "\n", "                    ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "head_mask", "[", "i", "]", ")", "\n", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "if", "self", ".", "output_attentions", ":", "\n", "                    ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "# Add last layer", "\n", "", "", "", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_hidden_states", ",", ")", "\n", "", "if", "self", ".", "output_attentions", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_attentions", ",", ")", "\n", "# last-layer hidden state, (all hidden states), (all attentions)", "\n", "# print (len(outputs))", "\n", "# print (len(outputs[1])) ##hidden states: 13", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.BertModel4SentMix.__init__": [[111, 118], ["transformers.BertPreTrainedModel.__init__", "transformers.models.bert.modeling_bert.BertEmbeddings", "text_classifier_smix.BertEncoder4SentMix", "transformers.models.bert.modeling_bert.BertPooler", "text_classifier_smix.BertModel4SentMix.init_weights"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel4SentMix", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder4SentMix", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.BertModel4SentMix._resize_token_embeddings": [[119, 125], ["text_classifier_smix.BertModel4SentMix._get_resized_embeddings"], "methods", ["None"], ["", "def", "_resize_token_embeddings", "(", "self", ",", "new_num_tokens", ")", ":", "\n", "        ", "old_embeddings", "=", "self", ".", "embeddings", ".", "word_embeddings", "\n", "new_embeddings", "=", "self", ".", "_get_resized_embeddings", "(", "\n", "old_embeddings", ",", "new_num_tokens", ")", "\n", "self", ".", "embeddings", ".", "word_embeddings", "=", "new_embeddings", "\n", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.BertModel4SentMix._prune_heads": [[126, 133], ["heads_to_prune.items", "text_classifier_smix.BertModel4SentMix.encoder.layer[].attention.prune_heads"], "methods", ["None"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.BertModel4SentMix.forward": [[134, 198], ["input_ids.size", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "text_classifier_smix.BertModel4SentMix.embeddings", "text_classifier_smix.BertModel4SentMix.pooler", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask2.to.to.to", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.to", "text_classifier_smix.BertModel4SentMix.embeddings", "text_classifier_smix.BertModel4SentMix.encoder", "text_classifier_smix.BertModel4SentMix.encoder", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.expand", "next", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "text_classifier_smix.BertModel4SentMix.parameters", "next", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "next", "text_classifier_smix.BertModel4SentMix.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "text_classifier_smix.BertModel4SentMix.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "input_ids2", "=", "None", ",", "attention_mask2", "=", "None", ",", "l", "=", "None", ",", "mix_layer", "=", "1000", ",", "\n", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "head_mask", "=", "None", ",", "inputs_embeds", "=", "None", ")", ":", "\n", "\n", "        ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "if", "input_ids2", "is", "not", "None", ":", "\n", "                ", "attention_mask2", "=", "torch", ".", "ones_like", "(", "input_ids2", ",", "device", "=", "device", ")", "\n", "", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ",", "device", "=", "device", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "if", "input_ids2", "is", "not", "None", ":", "\n", "                ", "token_type_ids2", "=", "torch", ".", "zeros_like", "(", "input_ids2", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "if", "input_ids2", "is", "not", "None", ":", "\n", "            ", "extended_attention_mask2", "=", "attention_mask2", ".", "unsqueeze", "(", "\n", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "extended_attention_mask2", "=", "extended_attention_mask2", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask2", "=", "(", "1.0", "-", "extended_attention_mask2", ")", "*", "-", "10000.0", "\n", "\n", "", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "if", "head_mask", ".", "dim", "(", ")", "==", "1", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "\n", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "head_mask", "=", "head_mask", ".", "expand", "(", "\n", "self", ".", "config", ".", "num_hidden_layers", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "", "elif", "head_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "# We can specify head_mask for each layer", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# switch to fload if need + fp16 compatibility", "\n", "", "head_mask", "=", "head_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "\n", "", "else", ":", "\n", "            ", "head_mask", "=", "[", "None", "]", "*", "self", ".", "config", ".", "num_hidden_layers", "\n", "\n", "", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "position_ids", "=", "position_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "\n", "if", "input_ids2", "is", "not", "None", ":", "\n", "            ", "embedding_output2", "=", "self", ".", "embeddings", "(", "\n", "input_ids2", ",", "position_ids", "=", "position_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "\n", "", "if", "input_ids2", "is", "not", "None", ":", "\n", "            ", "encoder_outputs", "=", "self", ".", "encoder", "(", "embedding_output", ",", "embedding_output2", ",", "l", ",", "mix_layer", ",", "\n", "extended_attention_mask", ",", "extended_attention_mask2", ",", "head_mask", "=", "head_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "attention_mask", "=", "extended_attention_mask", ",", "head_mask", "=", "head_mask", ")", "\n", "\n", "", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "\n", "# add hidden_states and attentions if they are here", "\n", "outputs", "=", "(", "sequence_output", ",", "pooled_output", ",", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "# sequence_output, pooled_output, (hidden_states), (attentions)", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.SentMix.__init__": [[201, 212], ["transformers.BertPreTrainedModel.__init__", "text_classifier_smix.BertModel4SentMix", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.KLDivLoss", "torch.KLDivLoss", "torch.KLDivLoss", "text_classifier_smix.SentMix.init_weights"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "SentMix", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "bert", "=", "BertModel4SentMix", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "num_labels", ")", "\n", "\n", "self", ".", "kl_criterion", "=", "nn", ".", "KLDivLoss", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.SentMix.forward": [[213, 233], ["text_classifier_smix.SentMix.dropout", "text_classifier_smix.SentMix.classifier", "text_classifier_smix.SentMix.bert", "text_classifier_smix.SentMix.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "attention_mask", ",", "x2", "=", "None", ",", "attention_mask2", "=", "None", ",", "l", "=", "None", ",", "mix_layer", "=", "1000", ",", "inputs_embeds", "=", "None", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "if", "x2", "is", "not", "None", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "x", ",", "attention_mask", ",", "x2", ",", "attention_mask", ",", "l", ",", "mix_layer", ",", "token_type_ids", "=", "token_type_ids", ",", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "\n", "# pooled_output = torch.mean(outputs[0], 1)", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "x", ",", "attention_mask", ",", "token_type_ids", "=", "token_type_ids", ",", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "\n", "# pooled_output = torch.mean(outputs[0], 1)", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "\n", "", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "# sequence_output = outputs[0]", "\n", "# logits = self.classifier(sequence_output)", "\n", "\n", "return", "logits", ",", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.SentMix.output_with_emb": [[234, 245], ["text_classifier_smix.SentMix.bert.encoder", "text_classifier_smix.SentMix.dropout", "text_classifier_smix.SentMix.classifier", "text_classifier_smix.SentMix.bert.pooler"], "methods", ["None"], ["", "def", "output_with_emb", "(", "self", ",", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "encoder_extended_attention_mask", ")", ":", "\n", "        ", "encoder_outputs", "=", "self", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.SentMix.adv_train": [[246, 305], ["input_ids.size", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "numpy.random.beta", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.kl_div", "torch.kl_div", "torch.kl_div", "input_ids.size", "text_classifier_smix.SentMix.bert.get_extended_attention_mask", "text_classifier_smix.SentMix.bert.get_head_mask", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_classifier_smix.SentMix.bert.embeddings", "text_classifier_smix.SentMix.output_with_emb", "total_loss.backward", "optim.step", "scheduler.step", "text_classifier_smix.SentMix.zero_grad", "text_classifier_smix.SentMix.forward", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "y.view", "torch.zeros().cuda().scatter_.view", "torch.zeros().cuda().scatter_.view", "torch.zeros().cuda().scatter_.view", "numpy.random.choice", "text_classifier_smix.SentMix.forward", "torch.softmax.log", "torch.softmax.log", "torch.softmax.log", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "L_ori.cpu().item", "torch.kl_div.cpu().item", "vda_loss.cpu().item", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.softmax.size", "torch.softmax.size", "torch.softmax.size", "noise.cuda", "text_classifier_smix.SentMix.kl_criterion", "text_classifier_smix.SentMix.kl_criterion", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "L_ori.cpu", "torch.kl_div.cpu", "vda_loss.cpu", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "logits.size", "logits.size"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.forward", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.forward"], ["", "def", "adv_train", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ",", "y", ",", "optim", ",", "scheduler", ",", "mlm_model", ",", "args", ")", ":", "\n", "        ", "logits", "=", "self", ".", "forward", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", "=", "token_type_ids", ")", "[", "0", "]", "\n", "L_ori", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "(", "logits", ",", "y", ")", "\n", "\n", "batch_size", "=", "input_ids", ".", "size", "(", "0", ")", "\n", "idx", "=", "torch", ".", "randperm", "(", "batch_size", ")", "\n", "input_ids_2", "=", "input_ids", "[", "idx", "]", "\n", "labels_2", "=", "y", "[", "idx", "]", "\n", "attention_mask_2", "=", "attention_masks", "[", "idx", "]", "\n", "## convert the labels to one-hot", "\n", "labels", "=", "torch", ".", "zeros", "(", "batch_size", ",", "logits", ".", "size", "(", "-", "1", ")", ")", ".", "cuda", "(", ")", ".", "scatter_", "(", "\n", "1", ",", "y", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", "\n", ")", "\n", "labels_2", "=", "torch", ".", "zeros", "(", "batch_size", ",", "logits", ".", "size", "(", "-", "1", ")", ")", ".", "cuda", "(", ")", ".", "scatter_", "(", "\n", "1", ",", "labels_2", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", "\n", ")", "\n", "l", "=", "np", ".", "random", ".", "beta", "(", "args", ".", "alpha", ",", "args", ".", "alpha", ")", "\n", "# l = max(l, 1-l) ## not needed when only using labeled examples", "\n", "mixed_labels", "=", "l", "*", "labels", "+", "(", "1", "-", "l", ")", "*", "labels_2", "\n", "\n", "mix_layer", "=", "np", ".", "random", ".", "choice", "(", "args", ".", "mix_layers_set", ",", "1", ")", "[", "0", "]", "\n", "mix_layer", "=", "mix_layer", "-", "1", "\n", "\n", "logits", "=", "self", ".", "forward", "(", "input_ids", ",", "attention_masks", ",", "input_ids_2", ",", "attention_mask_2", ",", "l", ",", "mix_layer", ",", "token_type_ids", "=", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "torch", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "# (bsz, num_labels)", "\n", "L_mix", "=", "F", ".", "kl_div", "(", "probs", ".", "log", "(", ")", ",", "mixed_labels", ",", "None", ",", "None", ",", "'batchmean'", ")", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_masks", ",", "input_shape", ",", "device", ")", "\n", "\n", "encoder_extended_attention_mask", "=", "None", "\n", "head_mask", "=", "self", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "args", ".", "variance", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ".", "cuda", "(", ")", ",", "-", "1", ")", "# [B, Len, V]", "\n", "word_embs", "=", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "embedding_output", "=", "self", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "vda_logits", "=", "self", ".", "output_with_emb", "(", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "vda_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "vda_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "vda_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "\n", "total_loss", "=", "L_ori", "+", "L_mix", "#+vda_loss", "\n", "total_loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "#print({name:param.grad for name, param in self.tgt_model.classifier.named_parameters()})", "\n", "scheduler", ".", "step", "(", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "\n", "return", "L_ori", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "L_mix", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "vda_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.reader_data": [[307, 319], ["open", "line.strip().split", "output.append", "int", "line.strip", "int"], "function", ["None"], ["", "", "def", "reader_data", "(", "filename", ",", "add", "=", "True", ")", ":", "\n", "#return [[ori_text, adv_text, label], ... ,[]]", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "output", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "label", ",", "text", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "add", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "-", "1", "\n", "", "else", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "\n", "", "output", ".", "append", "(", "[", "text", ",", "label", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_smix.run": [[321, 433], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "text_classifier_smix.reader_data", "text_classifier_smix.reader_data", "text_classifier_smix.reader_data", "print", "transformers.BertForMaskedLM.from_pretrained().cuda", "transformers.BertTokenizer.from_pretrained", "text_classifier_smix.EmbAdapterDataset", "text_classifier_smix.EmbAdapterDataset", "text_classifier_smix.EmbAdapterDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "transformers.BertConfig.from_pretrained", "SentMix.from_pretrained", "model.cuda.cuda", "model.cuda.parameters", "transformers.AdamW", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup", "range", "str", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "model.cuda.train", "tqdm.tqdm", "print", "model.cuda.eval", "print", "print", "transformers.BertForMaskedLM.from_pretrained", "len", "model.cuda.zero_grad", "model.cuda.adv_train", "total_loss.append", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.save", "torch.save", "torch.save", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ele.cuda", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "model.cuda.state_dict", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "ele.cuda", "model.cuda.", "ele.cuda", "model.cuda.", "sum", "len", "sum", "len", "sum", "len", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.adv_train"], ["", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "type", "=", "str", ",", "default", "=", "\"mr\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../TextFooler/target_models/mr\"", ",", "\n", "help", "=", "\"xxx classifier\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/mr_smix.pt\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_warmup\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--variance\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--step\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--alpha\"", ",", "type", "=", "float", ",", "default", "=", "0.2", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--mix-layers-set\"", ",", "\n", "nargs", "=", "'+'", ",", "\n", "default", "=", "[", "7", ",", "9", ",", "12", "]", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"define mix layer set\"", "\n", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "data_path", "=", "'data/'", "+", "str", "(", "args", ".", "dataset", ")", "\n", "if", "args", ".", "dataset", "==", "'ag'", "or", "args", ".", "dataset", "==", "'yelp'", ":", "\n", "        ", "add", "=", "True", "\n", "", "else", ":", "\n", "        ", "add", "=", "False", "\n", "", "train_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'train.txt'", ")", ",", "add", ")", "\n", "dev_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'dev.txt'", ")", ",", "add", ")", "\n", "test_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'test.txt'", ")", ",", "add", ")", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "args", ".", "mlm_path", ")", ".", "cuda", "(", ")", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "train_data", "=", "EmbAdapterDataset", "(", "args", ",", "train_features", ",", "tokenizer_tgt", ")", "\n", "dev_data", "=", "EmbAdapterDataset", "(", "args", ",", "dev_features", ",", "tokenizer_tgt", ")", "\n", "test_data", "=", "EmbAdapterDataset", "(", "args", ",", "test_features", ",", "tokenizer_tgt", ")", "\n", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "dev_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "test_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "print", "(", "'start building model'", ")", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "num_labels", "=", "args", ".", "num_label", ")", "\n", "model", "=", "SentMix", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "config", "=", "config_atk", ")", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "params", "=", "model", ".", "parameters", "(", ")", "\n", "#need_grad = lambda x: x.requires_grad", "\n", "optimizer", "=", "AdamW", "(", "\n", "params", ",", "\n", "lr", "=", "args", ".", "lr", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.01", ",", "\n", ")", "\n", "total_num", "=", "len", "(", "train_data", ")", "//", "args", ".", "batch_size", "*", "args", ".", "max_epoch", "\n", "scheduler", "=", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "num_warmup", "*", "total_num", ",", "\n", "num_training_steps", "=", "total_num", ")", "\n", "best_ratio", "=", "0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "max_epoch", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "total_loss", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "train_dataloader", ")", ":", "\n", "            ", "model", ".", "zero_grad", "(", ")", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "labels", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "#origin_emb = model.produce_emb(input_ids, attention_mask, token_type_ids)", "\n", "loss", ",", "kl_loss", ",", "vda_loss", "=", "model", ".", "adv_train", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "labels", ",", "\n", "optimizer", ",", "\n", "scheduler", ",", "mlm_model", ",", "args", ")", "\n", "total_loss", ".", "append", "(", "[", "loss", ",", "kl_loss", ",", "vda_loss", "]", ")", "\n", "\n", "", "print", "(", "'Epoch %d, the training ce loss is %f, mix loss is %f, vda loss is %f, the total number is %f'", "\n", "%", "(", "epoch", ",", "sum", "(", "[", "ele", "[", "0", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "\n", "sum", "(", "[", "ele", "[", "1", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "\n", "sum", "(", "[", "ele", "[", "2", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "dev_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "if", "best_ratio", "<", "attack_ratio", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", ")", "\n", "print", "(", "'--------save once-----------'", ")", "\n", "best_ratio", "=", "attack_ratio", "\n", "", "print", "(", "'The dev set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "test_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "print", "(", "'The test set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.EmbAdapterDataset.__init__": [[16, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "data", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "args", ".", "max_length", "\n", "self", ".", "pad_token", "=", "0", "\n", "self", ".", "mask_ids", "=", "103", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.EmbAdapterDataset.__getitem__": [[23, 32], ["text_pair_classifier_smart.EmbAdapterDataset.tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "x1", ",", "x2", ",", "y", "=", "self", ".", "data", "[", "index", "]", "\n", "inputs", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "x1", ",", "x2", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "max_length", ",", "\n", "truncation", "=", "True", ")", "\n", "padding_length", "=", "self", ".", "max_length", "-", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", "+", "padding_length", "*", "[", "self", ".", "pad_token", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"token_type_ids\"", "]", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.EmbAdapterDataset.__len__": [[33, 35], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.FreeLB.__init__": [[37, 46], ["torch.Module.__init__", "transformers.BertConfig.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained().cuda", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.KLDivLoss", "torch.KLDivLoss", "transformers.BertForSequenceClassification.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "nclasses", ")", ":", "\n", "        ", "super", "(", "FreeLB", ",", "self", ")", ".", "__init__", "(", ")", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "num_labels", "=", "nclasses", ")", "\n", "self", ".", "tgt_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "config", "=", "config_atk", ")", ".", "cuda", "(", ")", "\n", "self", ".", "variance", "=", "args", ".", "variation", "\n", "self", ".", "variance_vda", "=", "args", ".", "variation_vda", "\n", "self", ".", "step", "=", "args", ".", "step", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "kl_criterion", "=", "nn", ".", "KLDivLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.FreeLB.forward": [[47, 49], ["text_pair_classifier_smart.FreeLB.tgt_model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", ":", "\n", "        ", "return", "self", ".", "tgt_model", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.FreeLB.output_with_emb": [[50, 65], ["text_pair_classifier_smart.FreeLB.tgt_model.bert.encoder", "text_pair_classifier_smart.FreeLB.tgt_model.dropout", "text_pair_classifier_smart.FreeLB.tgt_model.classifier", "text_pair_classifier_smart.FreeLB.tgt_model.bert.pooler"], "methods", ["None"], ["", "def", "output_with_emb", "(", "self", ",", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "encoder_extended_attention_mask", ")", ":", "\n", "        ", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.FreeLB.adv_train": [[66, 132], ["input_ids.size", "text_pair_classifier_smart.FreeLB.tgt_model.bert.get_extended_attention_mask", "text_pair_classifier_smart.FreeLB.tgt_model.bert.get_head_mask", "text_pair_classifier_smart.FreeLB.tgt_model.bert.embeddings", "torch.clamp.cuda", "torch.clamp.cuda", "text_pair_classifier_smart.FreeLB.output_with_emb", "text_pair_classifier_smart.FreeLB.output_with_emb", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "text_pair_classifier_smart.FreeLB.output_with_emb", "text_pair_classifier_smart.FreeLB.output_with_emb", "text_pair_classifier_smart.FreeLB.criterion", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_pair_classifier_smart.FreeLB.tgt_model.bert.embeddings", "text_pair_classifier_smart.FreeLB.output_with_emb", "total_loss.backward", "optim.step", "scheduler.step", "text_pair_classifier_smart.FreeLB.tgt_model.zero_grad", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "text_pair_classifier_smart.FreeLB.cpu().item", "n_loss.cpu().item", "vda_loss.cpu().item", "text_pair_classifier_smart.FreeLB.size", "text_pair_classifier_smart.FreeLB.kl_criterion", "text_pair_classifier_smart.FreeLB.kl_criterion", "text_pair_classifier_smart.FreeLB.kl_criterion", "text_pair_classifier_smart.FreeLB.kl_criterion", "torch.softmax.size", "torch.softmax.size", "torch.clamp.cuda", "torch.clamp.cuda", "text_pair_classifier_smart.FreeLB.kl_criterion", "text_pair_classifier_smart.FreeLB.kl_criterion", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "text_pair_classifier_smart.FreeLB.cpu", "n_loss.cpu", "vda_loss.cpu", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb"], ["", "def", "adv_train", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ",", "y", ",", "optim", ",", "scheduler", ",", "mlm_model", ")", ":", "\n", "        ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_masks", ",", "\n", "input_shape", ",", "device", ")", "\n", "\n", "encoder_extended_attention_mask", "=", "None", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", ")", "\n", "\n", "noise", "=", "torch", ".", "randn", "(", "embedding_output", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "self", ".", "variance", "\n", "noise", "=", "noise", ".", "cuda", "(", ")", "\n", "noise", ".", "requires_grad", "=", "True", "\n", "\n", "n_embedding_output", "=", "embedding_output", "+", "noise", "\n", "n_logits", "=", "self", ".", "output_with_emb", "(", "n_embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "logits", "=", "self", ".", "output_with_emb", "(", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "\n", "n_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "n_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "n_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "noise_grad", "=", "torch", ".", "autograd", ".", "grad", "(", "n_loss", ",", "noise", ")", "[", "0", "]", "\n", "noise", "=", "noise", "+", "(", "noise_grad", "/", "torch", ".", "norm", "(", "noise_grad", ")", ")", ".", "mul_", "(", "1e-3", ")", "\n", "noise", "=", "torch", ".", "clamp", "(", "noise", ",", "-", "self", ".", "variance", ",", "self", ".", "variance", ")", "\n", "#update the noise finish", "\n", "\n", "n_embedding_output", "=", "embedding_output", "+", "noise", "\n", "n_logits", "=", "self", ".", "output_with_emb", "(", "n_embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "logits", "=", "self", ".", "output_with_emb", "(", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "n_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "n_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "n_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "loss", "=", "self", ".", "criterion", "(", "logits", ",", "y", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "self", ".", "variance_vda", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ".", "cuda", "(", ")", ",", "-", "1", ")", "# [B, Len, V]", "\n", "word_embs", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "vda_logits", "=", "self", ".", "output_with_emb", "(", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "vda_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "vda_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "vda_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "\n", "total_loss", "=", "loss", "+", "n_loss", "+", "vda_loss", "\n", "total_loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "#print({name:param.grad for name, param in self.tgt_model.classifier.named_parameters()})", "\n", "scheduler", ".", "step", "(", ")", "\n", "self", ".", "tgt_model", ".", "zero_grad", "(", ")", "\n", "\n", "return", "loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "n_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "vda_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.FreeLB.defense": [[133, 184], ["input_ids.size", "text_pair_classifier_smart.FreeLB.tgt_model.bert.get_extended_attention_mask", "text_pair_classifier_smart.FreeLB.tgt_model.bert.get_head_mask", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_pair_classifier_smart.FreeLB.tgt_model.bert.embeddings", "text_pair_classifier_smart.FreeLB.tgt_model.bert.encoder", "text_pair_classifier_smart.FreeLB.tgt_model.classifier", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "text_pair_classifier_smart.FreeLB.tgt_model.bert.pooler", "torch.softmax.size", "torch.softmax.size", "noise.cuda"], "methods", ["None"], ["", "def", "defense", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "mlm_model", ")", ":", "\n", "        ", "output_attentions", "=", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_hidden_states", ")", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "\n", "input_shape", ",", "device", ")", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ")", "*", "self", ".", "variance_vda", "\n", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ".", "cuda", "(", ")", ",", "-", "1", ")", "# [B, Len, V]", "\n", "\n", "word_embs", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "# logits = self.output_linear(prompt_output[0])", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.reader_data": [[185, 194], ["open", "line.strip().split", "int", "output.append", "line.strip"], "function", ["None"], ["", "", "def", "reader_data", "(", "filename", ",", "add", "=", "True", ")", ":", "\n", "#return [[ori_text, adv_text, label], ... ,[]]", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "output", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "label", ",", "text1", ",", "text2", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "label", "=", "int", "(", "label", ")", "\n", "output", ".", "append", "(", "[", "text1", ",", "text2", ",", "label", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smart.run": [[196, 293], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "text_pair_classifier_smart.reader_data", "text_pair_classifier_smart.reader_data", "text_pair_classifier_smart.reader_data", "print", "transformers.BertForMaskedLM.from_pretrained().cuda", "transformers.BertTokenizer.from_pretrained", "text_pair_classifier_smart.EmbAdapterDataset", "text_pair_classifier_smart.EmbAdapterDataset", "text_pair_classifier_smart.EmbAdapterDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "text_pair_classifier_smart.FreeLB", "model.cuda.cuda", "model.cuda.parameters", "transformers.AdamW", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup", "range", "str", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "model.cuda.train", "tqdm.tqdm", "print", "model.cuda.eval", "print", "print", "transformers.BertForMaskedLM.from_pretrained", "len", "model.cuda.zero_grad", "model.cuda.adv_train", "total_loss.append", "torch.no_grad", "torch.no_grad", "torch.save", "torch.save", "print", "torch.no_grad", "torch.no_grad", "ele.cuda", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "model.cuda.tgt_model.state_dict", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "ele.cuda", "model.cuda.", "ele.cuda", "model.cuda.", "sum", "len", "sum", "len", "sum", "len", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.adv_train"], ["", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "type", "=", "str", ",", "default", "=", "\"qnli\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../TextFooler/target_models/mrpc\"", ",", "\n", "help", "=", "\"xxx classifier\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/qnli_smart_vda.pt\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "16", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_warmup\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--variation\"", ",", "type", "=", "float", ",", "default", "=", "0.02", ")", "\n", "parser", ".", "add_argument", "(", "\"--variation_vda\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--step\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "data_path", "=", "'data/'", "+", "str", "(", "args", ".", "dataset", ")", "\n", "train_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'train.txt'", ")", ")", "\n", "dev_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'dev.txt'", ")", ")", "\n", "test_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'test.txt'", ")", ")", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "args", ".", "mlm_path", ")", ".", "cuda", "(", ")", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "train_data", "=", "EmbAdapterDataset", "(", "args", ",", "train_features", ",", "tokenizer_tgt", ")", "\n", "dev_data", "=", "EmbAdapterDataset", "(", "args", ",", "dev_features", ",", "tokenizer_tgt", ")", "\n", "test_data", "=", "EmbAdapterDataset", "(", "args", ",", "test_features", ",", "tokenizer_tgt", ")", "\n", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "dev_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "test_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "print", "(", "'start building model'", ")", "\n", "model", "=", "FreeLB", "(", "args", ",", "args", ".", "num_label", ")", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "params", "=", "model", ".", "parameters", "(", ")", "\n", "#need_grad = lambda x: x.requires_grad", "\n", "optimizer", "=", "AdamW", "(", "\n", "params", ",", "\n", "lr", "=", "args", ".", "lr", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.01", ",", "\n", ")", "\n", "total_num", "=", "len", "(", "train_data", ")", "//", "args", ".", "batch_size", "*", "args", ".", "max_epoch", "\n", "scheduler", "=", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "num_warmup", "*", "total_num", ",", "\n", "num_training_steps", "=", "total_num", ")", "\n", "best_ratio", "=", "0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "max_epoch", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "total_loss", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "train_dataloader", ")", ":", "\n", "            ", "model", ".", "zero_grad", "(", ")", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "#origin_emb = model.produce_emb(input_ids, attention_mask, token_type_ids)", "\n", "loss", ",", "kl_loss", ",", "vda_loss", "=", "model", ".", "adv_train", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "y", ",", "optimizer", ",", "scheduler", ",", "mlm_model", ")", "\n", "total_loss", ".", "append", "(", "[", "loss", ",", "kl_loss", ",", "vda_loss", "]", ")", "\n", "\n", "", "print", "(", "'Epoch %d, the training ce loss is %f, the kl loss is %f, vda loss is %f, the total number is %f'", "\n", "%", "(", "epoch", ",", "sum", "(", "[", "ele", "[", "0", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "\n", "sum", "(", "[", "ele", "[", "1", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "sum", "(", "[", "ele", "[", "2", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "dev_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "if", "best_ratio", "<", "attack_ratio", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "tgt_model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", ")", "\n", "print", "(", "'--------save once-----------'", ")", "\n", "best_ratio", "=", "attack_ratio", "\n", "", "print", "(", "'The dev set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "test_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "print", "(", "'The test set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.Feature.__init__": [[118, 129], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "seq_a", ",", "label", ")", ":", "\n", "        ", "self", ".", "label", "=", "label", "\n", "self", ".", "seq", "=", "seq_a", "\n", "self", ".", "final_adverse", "=", "seq_a", "\n", "self", ".", "query", "=", "0", "\n", "self", ".", "change", "=", "0", "\n", "self", ".", "success", "=", "0", "\n", "self", ".", "sim", "=", "0.0", "\n", "self", ".", "ori_acc", "=", "None", "\n", "self", ".", "att_acc", "=", "1", "\n", "self", ".", "changes", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.BERTDataset.__init__": [[154, 158], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "inputs", ",", "masks", ",", "segs", ")", ":", "\n", "        ", "self", ".", "inputs", "=", "inputs", "\n", "self", ".", "masks", "=", "masks", "\n", "self", ".", "segs", "=", "segs", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.BERTDataset.__getitem__": [[159, 164], ["torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "input_ids", "=", "torch", ".", "tensor", "(", "self", ".", "inputs", "[", "index", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "self", ".", "masks", "[", "index", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "self", ".", "segs", "[", "index", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.BERTDataset.__len__": [[165, 167], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "inputs", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_sim_embed": [[55, 68], ["numpy.load", "open", "line.split", "len", "len"], "function", ["None"], ["def", "get_sim_embed", "(", "embed_path", ",", "sim_path", ")", ":", "\n", "    ", "id2word", "=", "{", "}", "\n", "word2id", "=", "{", "}", "\n", "\n", "with", "open", "(", "embed_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", "as", "ifile", ":", "\n", "        ", "for", "line", "in", "ifile", ":", "\n", "            ", "word", "=", "line", ".", "split", "(", ")", "[", "0", "]", "\n", "if", "word", "not", "in", "id2word", ":", "\n", "                ", "id2word", "[", "len", "(", "id2word", ")", "]", "=", "word", "\n", "word2id", "[", "word", "]", "=", "len", "(", "id2word", ")", "-", "1", "\n", "\n", "", "", "", "cos_sim", "=", "np", ".", "load", "(", "sim_path", ")", "\n", "return", "cos_sim", ",", "word2id", ",", "id2word", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_data_cls": [[70, 80], ["enumerate", "open().readlines", "line.strip().split", "int", "features.append", "open", "line.strip"], "function", ["None"], ["", "def", "get_data_cls", "(", "data_path", ")", ":", "\n", "    ", "lines", "=", "open", "(", "data_path", ",", "'r'", ",", "encoding", "=", "'utf-8'", ")", ".", "readlines", "(", ")", "[", "1", ":", "]", "\n", "features", "=", "[", "]", "\n", "for", "i", ",", "line", "in", "enumerate", "(", "lines", ")", ":", "\n", "        ", "split", "=", "line", ".", "strip", "(", "'\\n'", ")", ".", "split", "(", "'\\t'", ")", "\n", "label", "=", "int", "(", "split", "[", "-", "1", "]", ")", "\n", "seq", "=", "split", "[", "0", "]", "\n", "\n", "features", ".", "append", "(", "[", "seq", ",", "label", "]", ")", "\n", "", "return", "features", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.clean_str": [[82, 101], ["re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub", "re.sub.strip", "re.sub.strip().lower", "re.sub.strip"], "function", ["None"], ["", "def", "clean_str", "(", "string", ",", "TREC", "=", "False", ")", ":", "\n", "    ", "\"\"\"\n    Tokenization/string cleaning for all datasets except for SST.\n    Every dataset is lower cased except for TREC\n    \"\"\"", "\n", "string", "=", "re", ".", "sub", "(", "r\"[^A-Za-z0-9(),!?\\'\\`]\"", ",", "\" \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'s\"", ",", "\" \\'s\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ve\"", ",", "\" \\'ve\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"n\\'t\"", ",", "\" n\\'t\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'re\"", ",", "\" \\'re\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'d\"", ",", "\" \\'d\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\'ll\"", ",", "\" \\'ll\"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\",\"", ",", "\" , \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"!\"", ",", "\" ! \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\(\"", ",", "\" \\( \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\)\"", ",", "\" \\) \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\?\"", ",", "\" \\? \"", ",", "string", ")", "\n", "string", "=", "re", ".", "sub", "(", "r\"\\s{2,}\"", ",", "\" \"", ",", "string", ")", "\n", "return", "string", ".", "strip", "(", ")", "if", "TREC", "else", "string", ".", "strip", "(", ")", ".", "lower", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.reader_data": [[102, 116], ["open", "line.strip().split", "output.append", "int", "line.strip", "int"], "function", ["None"], ["", "def", "reader_data", "(", "filename", ",", "add", "=", "True", ")", ":", "\n", "#return [[ori_text, adv_text, label], ... ,[]]", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "output", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "label", ",", "text", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "add", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "-", "1", "\n", "", "else", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "\n", "", "output", ".", "append", "(", "[", "text", ",", "label", "]", ")", "\n", "", "if", "'imdb'", "in", "filename", "or", "'yelp'", "in", "filename", ":", "\n", "        ", "output", "=", "output", "[", ":", "300", "]", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust._tokenize": [[130, 144], ["seq.replace().lower.replace().lower", "seq.replace().lower.split", "tokenizer.tokenize", "keys.append", "len", "seq.replace().lower.replace", "len"], "function", ["None"], ["", "", "def", "_tokenize", "(", "seq", ",", "tokenizer", ")", ":", "\n", "    ", "seq", "=", "seq", ".", "replace", "(", "'\\n'", ",", "''", ")", ".", "lower", "(", ")", "\n", "words", "=", "seq", ".", "split", "(", "' '", ")", "\n", "\n", "sub_words", "=", "[", "]", "\n", "keys", "=", "[", "]", "\n", "index", "=", "0", "\n", "for", "word", "in", "words", ":", "\n", "        ", "sub", "=", "tokenizer", ".", "tokenize", "(", "word", ")", "\n", "sub_words", "+=", "sub", "\n", "keys", ".", "append", "(", "[", "index", ",", "index", "+", "len", "(", "sub", ")", "]", ")", "\n", "index", "+=", "len", "(", "sub", ")", "\n", "\n", "", "return", "words", ",", "sub_words", ",", "keys", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust._get_masked": [[145, 152], ["len", "range", "masked_words.append"], "function", ["None"], ["", "def", "_get_masked", "(", "words", ")", ":", "\n", "    ", "len_text", "=", "len", "(", "words", ")", "\n", "masked_words", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "len_text", "-", "1", ")", ":", "\n", "        ", "masked_words", ".", "append", "(", "words", "[", "0", ":", "i", "]", "+", "[", "'[UNK]'", "]", "+", "words", "[", "i", "+", "1", ":", "]", ")", "\n", "# list of words", "\n", "", "return", "masked_words", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_important_scores": [[168, 203], ["bert_robust._get_masked", "bert_robust.BERTDataset", "torch.utils.data.DataLoader", "torch.cat", "torch.cat", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "tokenizer.encode_plus", "all_input_ids.append", "all_masks.append", "all_segs.append", "torch.softmax.append", "len", "len", "ele.cuda", "tgt_model", "torch.index_select", "torch.index_select", "torch.softmax.max"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust._get_masked"], ["", "", "def", "get_important_scores", "(", "words", ",", "tgt_model", ",", "orig_prob", ",", "orig_label", ",", "orig_probs", ",", "tokenizer", ",", "batch_size", ",", "max_length", ")", ":", "\n", "    ", "masked_words", "=", "_get_masked", "(", "words", ")", "# mask each words (not subwords!)", "\n", "texts", "=", "[", "' '", ".", "join", "(", "words", ")", "for", "words", "in", "masked_words", "]", "# list of text of masked words", "\n", "all_input_ids", "=", "[", "]", "\n", "all_masks", "=", "[", "]", "\n", "all_segs", "=", "[", "]", "\n", "for", "text", "in", "texts", ":", "\n", "        ", "inputs", "=", "tokenizer", ".", "encode_plus", "(", "text", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "max_length", ",", "truncation", "=", "True", ")", "\n", "input_ids", ",", "token_type_ids", "=", "inputs", "[", "\"input_ids\"", "]", ",", "inputs", "[", "\"token_type_ids\"", "]", "\n", "attention_mask", "=", "[", "1", "]", "*", "len", "(", "input_ids", ")", "\n", "padding_length", "=", "max_length", "-", "len", "(", "input_ids", ")", "\n", "input_ids", "=", "input_ids", "+", "(", "padding_length", "*", "[", "0", "]", ")", "\n", "token_type_ids", "=", "token_type_ids", "+", "(", "padding_length", "*", "[", "0", "]", ")", "\n", "attention_mask", "=", "attention_mask", "+", "(", "padding_length", "*", "[", "0", "]", ")", "\n", "all_input_ids", ".", "append", "(", "input_ids", ")", "\n", "all_masks", ".", "append", "(", "attention_mask", ")", "\n", "all_segs", ".", "append", "(", "token_type_ids", ")", "\n", "\n", "", "eval_data", "=", "BERTDataset", "(", "all_input_ids", ",", "all_masks", ",", "all_segs", ")", "\n", "# Run prediction for full data", "\n", "eval_dataloader", "=", "DataLoader", "(", "eval_data", ",", "batch_size", "=", "batch_size", ")", "\n", "leave_1_probs", "=", "[", "]", "\n", "for", "batch", "in", "eval_dataloader", ":", "\n", "        ", "masked_input", ",", "seg_token", ",", "attn_mask", "=", "(", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", ")", "\n", "leave_1_prob_batch", "=", "tgt_model", "(", "masked_input", ",", "attn_mask", ",", "seg_token", ")", "[", "0", "]", "# B num-label", "\n", "leave_1_probs", ".", "append", "(", "leave_1_prob_batch", ")", "\n", "", "leave_1_probs", "=", "torch", ".", "cat", "(", "leave_1_probs", ",", "dim", "=", "0", ")", "# words, num-label", "\n", "leave_1_probs", "=", "torch", ".", "softmax", "(", "leave_1_probs", ",", "-", "1", ")", "#", "\n", "leave_1_probs_argmax", "=", "torch", ".", "argmax", "(", "leave_1_probs", ",", "dim", "=", "-", "1", ")", "\n", "import_scores", "=", "(", "orig_prob", "\n", "-", "leave_1_probs", "[", ":", ",", "orig_label", "]", "\n", "+", "(", "leave_1_probs_argmax", "!=", "orig_label", ")", ".", "float", "(", ")", "\n", "*", "(", "leave_1_probs", ".", "max", "(", "dim", "=", "-", "1", ")", "[", "0", "]", "-", "torch", ".", "index_select", "(", "orig_probs", ",", "0", ",", "leave_1_probs_argmax", ")", ")", "\n", ")", ".", "data", ".", "cpu", "(", ")", ".", "numpy", "(", ")", "\n", "return", "import_scores", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_substitues": [[205, 227], ["substitutes.size", "zip", "get_bpe_substitues.append", "bert_robust.get_bpe_substitues", "tokenizer._convert_id_to_token", "int"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_bpe_substitues"], ["", "def", "get_substitues", "(", "substitutes", ",", "tokenizer", ",", "mlm_model", ",", "use_bpe", ",", "substitutes_score", "=", "None", ",", "threshold", "=", "3.0", ")", ":", "\n", "# substitues L,k", "\n", "# from this matrix to recover a word", "\n", "    ", "words", "=", "[", "]", "\n", "sub_len", ",", "k", "=", "substitutes", ".", "size", "(", ")", "# sub-len, k", "\n", "\n", "if", "sub_len", "==", "0", ":", "\n", "        ", "return", "words", "\n", "\n", "", "elif", "sub_len", "==", "1", ":", "\n", "        ", "for", "(", "i", ",", "j", ")", "in", "zip", "(", "substitutes", "[", "0", "]", ",", "substitutes_score", "[", "0", "]", ")", ":", "\n", "            ", "if", "threshold", "!=", "0", "and", "j", "<", "threshold", ":", "\n", "                ", "break", "\n", "", "words", ".", "append", "(", "tokenizer", ".", "_convert_id_to_token", "(", "int", "(", "i", ")", ")", ")", "\n", "", "", "else", ":", "\n", "        ", "if", "use_bpe", "==", "1", ":", "\n", "            ", "words", "=", "get_bpe_substitues", "(", "substitutes", ",", "tokenizer", ",", "mlm_model", ")", "\n", "", "else", ":", "\n", "            ", "return", "words", "\n", "#", "\n", "# print(words)", "\n", "", "", "return", "words", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_bpe_substitues": [[229, 267], ["range", "torch.CrossEntropyLoss", "torch.tensor", "torch.tensor", "all_substitutes[].to", "all_substitutes[].to.size", "nn.CrossEntropyLoss.", "torch.exp", "torch.exp", "torch.sort", "torch.sort", "substitutes.size", "mlm_model", "word_predictions.view", "all_substitutes[].to.view", "torch.mean", "torch.mean", "tokenizer.convert_tokens_to_string", "final_words.append", "len", "torch.exp.view", "tokenizer._convert_id_to_token", "int", "int", "lev_i.append", "int"], "function", ["None"], ["", "def", "get_bpe_substitues", "(", "substitutes", ",", "tokenizer", ",", "mlm_model", ")", ":", "\n", "# substitutes L, k", "\n", "\n", "    ", "substitutes", "=", "substitutes", "[", "0", ":", "12", ",", "0", ":", "4", "]", "# maximum BPE candidates", "\n", "\n", "# find all possible candidates", "\n", "\n", "all_substitutes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "substitutes", ".", "size", "(", "0", ")", ")", ":", "\n", "        ", "if", "len", "(", "all_substitutes", ")", "==", "0", ":", "\n", "            ", "lev_i", "=", "substitutes", "[", "i", "]", "\n", "all_substitutes", "=", "[", "[", "int", "(", "c", ")", "]", "for", "c", "in", "lev_i", "]", "\n", "", "else", ":", "\n", "            ", "lev_i", "=", "[", "]", "\n", "for", "all_sub", "in", "all_substitutes", ":", "\n", "                ", "for", "j", "in", "substitutes", "[", "i", "]", ":", "\n", "                    ", "lev_i", ".", "append", "(", "all_sub", "+", "[", "int", "(", "j", ")", "]", ")", "\n", "", "", "all_substitutes", "=", "lev_i", "\n", "\n", "# all substitutes  list of list of token-id (all candidates)", "\n", "", "", "c_loss", "=", "nn", ".", "CrossEntropyLoss", "(", "reduction", "=", "'none'", ")", "\n", "word_list", "=", "[", "]", "\n", "# all_substitutes = all_substitutes[:24]", "\n", "all_substitutes", "=", "torch", ".", "tensor", "(", "all_substitutes", ")", "# [ N, L ]", "\n", "all_substitutes", "=", "all_substitutes", "[", ":", "24", "]", ".", "to", "(", "'cuda'", ")", "\n", "# print(substitutes.size(), all_substitutes.size())", "\n", "N", ",", "L", "=", "all_substitutes", ".", "size", "(", ")", "\n", "word_predictions", "=", "mlm_model", "(", "all_substitutes", ")", "[", "0", "]", "# N L vocab-size", "\n", "ppl", "=", "c_loss", "(", "word_predictions", ".", "view", "(", "N", "*", "L", ",", "-", "1", ")", ",", "all_substitutes", ".", "view", "(", "-", "1", ")", ")", "# [ N*L ]", "\n", "ppl", "=", "torch", ".", "exp", "(", "torch", ".", "mean", "(", "ppl", ".", "view", "(", "N", ",", "L", ")", ",", "dim", "=", "-", "1", ")", ")", "# N", "\n", "_", ",", "word_list", "=", "torch", ".", "sort", "(", "ppl", ")", "\n", "word_list", "=", "[", "all_substitutes", "[", "i", "]", "for", "i", "in", "word_list", "]", "\n", "final_words", "=", "[", "]", "\n", "for", "word", "in", "word_list", ":", "\n", "        ", "tokens", "=", "[", "tokenizer", ".", "_convert_id_to_token", "(", "int", "(", "i", ")", ")", "for", "i", "in", "word", "]", "\n", "text", "=", "tokenizer", ".", "convert_tokens_to_string", "(", "tokens", ")", "\n", "final_words", ".", "append", "(", "text", ")", "\n", "", "return", "final_words", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.attack": [[269, 381], ["bert_robust._tokenize", "tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor().unsqueeze().to.size", "[].squeeze", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "torch.softmax.max", "torch.tensor", "torch.tensor", "[].squeeze", "torch.topk", "torch.topk", "bert_robust.get_important_scores", "int", "sorted", "copy.deepcopy", "tokenizer.convert_tokens_to_string", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "enumerate", "bert_robust.get_substitues", "len", "tokenizer.convert_tokens_to_ids", "int", "tokenizer.convert_tokens_to_string", "tokenizer.encode_plus", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "torch.tensor().unsqueeze().to", "[].squeeze", "torch.softmax", "torch.softmax", "torch.argmax", "torch.argmax", "feature.changes.append", "tgt_model", "mlm_model", "feature.changes.append", "torch.tensor().unsqueeze().to.unsqueeze().to", "torch.tensor().unsqueeze().to.unsqueeze().to", "torch.tensor().unsqueeze().to.unsqueeze().to", "torch.tensor.to", "len", "len", "len", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "torch.tensor().unsqueeze", "tgt_model", "torch.tensor().unsqueeze().to.unsqueeze", "torch.tensor().unsqueeze().to.unsqueeze", "torch.tensor().unsqueeze().to.unsqueeze", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust._tokenize", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_important_scores", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_substitues"], ["", "def", "attack", "(", "feature", ",", "tgt_model", ",", "mlm_model", ",", "tokenizer", ",", "k", ",", "batch_size", ",", "max_length", "=", "512", ",", "cos_mat", "=", "None", ",", "w2i", "=", "{", "}", ",", "i2w", "=", "{", "}", ",", "\n", "use_bpe", "=", "1", ",", "threshold_pred_score", "=", "0.3", ")", ":", "\n", "# MLM-process", "\n", "    ", "words", ",", "sub_words", ",", "keys", "=", "_tokenize", "(", "feature", ".", "seq", ",", "tokenizer", ")", "\n", "\n", "# original label", "\n", "inputs", "=", "tokenizer", ".", "encode_plus", "(", "feature", ".", "seq", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "max_length", ",", "truncation", "=", "True", ")", "\n", "input_ids", ",", "token_type_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", ")", ",", "torch", ".", "tensor", "(", "inputs", "[", "\"token_type_ids\"", "]", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "input_ids", ")", ")", "\n", "seq_len", "=", "input_ids", ".", "size", "(", "0", ")", "\n", "orig_probs", "=", "tgt_model", "(", "input_ids", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", ",", "\n", "attention_mask", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", ",", "\n", "token_type_ids", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", "\n", ")", "[", "0", "]", ".", "squeeze", "(", ")", "\n", "orig_probs", "=", "torch", ".", "softmax", "(", "orig_probs", ",", "-", "1", ")", "\n", "orig_label", "=", "torch", ".", "argmax", "(", "orig_probs", ")", "\n", "current_prob", "=", "orig_probs", ".", "max", "(", ")", "\n", "\n", "if", "orig_label", "!=", "feature", ".", "label", ":", "\n", "        ", "feature", ".", "ori_acc", "=", "0", "\n", "feature", ".", "att_acc", "=", "0", "\n", "feature", ".", "success", "=", "-", "1", "\n", "return", "feature", "\n", "", "else", ":", "\n", "        ", "feature", ".", "ori_acc", "=", "1", "\n", "\n", "", "sub_words", "=", "[", "'[CLS]'", "]", "+", "sub_words", "[", ":", "max_length", "-", "2", "]", "+", "[", "'[SEP]'", "]", "\n", "\n", "input_ids_", "=", "torch", ".", "tensor", "(", "[", "tokenizer", ".", "convert_tokens_to_ids", "(", "sub_words", ")", "]", ")", "\n", "word_predictions", "=", "mlm_model", "(", "input_ids_", ".", "to", "(", "'cuda'", ")", ")", "[", "0", "]", ".", "squeeze", "(", ")", "# seq-len(sub) vocab", "\n", "word_pred_scores_all", ",", "word_predictions", "=", "torch", ".", "topk", "(", "word_predictions", ",", "k", ",", "-", "1", ")", "# seq-len k", "\n", "\n", "word_predictions", "=", "word_predictions", "[", "1", ":", "len", "(", "sub_words", ")", "+", "1", ",", ":", "]", "\n", "word_pred_scores_all", "=", "word_pred_scores_all", "[", "1", ":", "len", "(", "sub_words", ")", "+", "1", ",", ":", "]", "\n", "\n", "important_scores", "=", "get_important_scores", "(", "words", ",", "tgt_model", ",", "current_prob", ",", "orig_label", ",", "orig_probs", ",", "\n", "tokenizer", ",", "batch_size", ",", "max_length", ")", "\n", "feature", ".", "query", "+=", "int", "(", "len", "(", "words", ")", ")", "\n", "list_of_index", "=", "sorted", "(", "enumerate", "(", "important_scores", ")", ",", "key", "=", "lambda", "x", ":", "x", "[", "1", "]", ",", "reverse", "=", "True", ")", "\n", "# print(list_of_index)", "\n", "final_words", "=", "copy", ".", "deepcopy", "(", "words", ")", "\n", "\n", "for", "top_index", "in", "list_of_index", ":", "\n", "        ", "if", "feature", ".", "change", ">", "int", "(", "0.4", "*", "(", "len", "(", "words", ")", ")", ")", ":", "\n", "            ", "feature", ".", "success", "=", "-", "2", "# exceed", "\n", "return", "feature", "\n", "\n", "", "tgt_word", "=", "words", "[", "top_index", "[", "0", "]", "]", "\n", "if", "tgt_word", "in", "filter_words", ":", "\n", "            ", "continue", "\n", "", "if", "keys", "[", "top_index", "[", "0", "]", "]", "[", "0", "]", ">", "max_length", "-", "2", ":", "\n", "            ", "continue", "\n", "\n", "", "substitutes", "=", "word_predictions", "[", "keys", "[", "top_index", "[", "0", "]", "]", "[", "0", "]", ":", "keys", "[", "top_index", "[", "0", "]", "]", "[", "1", "]", "]", "# L, k", "\n", "word_pred_scores", "=", "word_pred_scores_all", "[", "keys", "[", "top_index", "[", "0", "]", "]", "[", "0", "]", ":", "keys", "[", "top_index", "[", "0", "]", "]", "[", "1", "]", "]", "\n", "\n", "substitutes", "=", "get_substitues", "(", "substitutes", ",", "tokenizer", ",", "mlm_model", ",", "use_bpe", ",", "word_pred_scores", ",", "threshold_pred_score", ")", "\n", "\n", "most_gap", "=", "0.0", "\n", "candidate", "=", "None", "\n", "\n", "for", "substitute_", "in", "substitutes", ":", "\n", "            ", "substitute", "=", "substitute_", "\n", "\n", "if", "substitute", "==", "tgt_word", ":", "\n", "                ", "continue", "# filter out original word", "\n", "", "if", "'##'", "in", "substitute", ":", "\n", "                ", "continue", "# filter out sub-word", "\n", "\n", "", "if", "substitute", "in", "filter_words", ":", "\n", "                ", "continue", "\n", "", "if", "substitute", "in", "w2i", "and", "tgt_word", "in", "w2i", ":", "\n", "                ", "if", "cos_mat", "[", "w2i", "[", "substitute", "]", "]", "[", "w2i", "[", "tgt_word", "]", "]", "<", "0.7", ":", "\n", "                    ", "continue", "\n", "", "", "temp_replace", "=", "final_words", "\n", "temp_replace", "[", "top_index", "[", "0", "]", "]", "=", "substitute", "\n", "temp_text", "=", "tokenizer", ".", "convert_tokens_to_string", "(", "temp_replace", ")", "\n", "inputs", "=", "tokenizer", ".", "encode_plus", "(", "temp_text", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "max_length", ",", "\n", "truncation", "=", "True", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"token_type_ids\"", "]", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "input_ids", ")", ")", ".", "unsqueeze", "(", "0", ")", ".", "to", "(", "'cuda'", ")", "\n", "\n", "temp_prob", "=", "tgt_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", ".", "squeeze", "(", ")", "\n", "feature", ".", "query", "+=", "1", "\n", "temp_prob", "=", "torch", ".", "softmax", "(", "temp_prob", ",", "-", "1", ")", "\n", "temp_label", "=", "torch", ".", "argmax", "(", "temp_prob", ")", "\n", "\n", "if", "temp_label", "!=", "orig_label", ":", "\n", "                ", "feature", ".", "change", "+=", "1", "\n", "final_words", "[", "top_index", "[", "0", "]", "]", "=", "substitute", "\n", "feature", ".", "changes", ".", "append", "(", "[", "keys", "[", "top_index", "[", "0", "]", "]", "[", "0", "]", ",", "substitute", ",", "tgt_word", "]", ")", "\n", "feature", ".", "final_adverse", "=", "temp_text", "\n", "feature", ".", "success", "=", "4", "\n", "feature", ".", "att_acc", "=", "0", "\n", "return", "feature", "\n", "", "else", ":", "\n", "                ", "label_prob", "=", "temp_prob", "[", "orig_label", "]", "\n", "gap", "=", "current_prob", "-", "label_prob", "\n", "if", "gap", ">", "most_gap", ":", "\n", "                    ", "most_gap", "=", "gap", "\n", "candidate", "=", "substitute", "\n", "\n", "", "", "", "if", "most_gap", ">", "0", ":", "\n", "            ", "feature", ".", "change", "+=", "1", "\n", "feature", ".", "changes", ".", "append", "(", "[", "keys", "[", "top_index", "[", "0", "]", "]", "[", "0", "]", ",", "candidate", ",", "tgt_word", "]", ")", "\n", "current_prob", "=", "current_prob", "-", "most_gap", "\n", "final_words", "[", "top_index", "[", "0", "]", "]", "=", "candidate", "\n", "\n", "", "", "feature", ".", "final_adverse", "=", "(", "tokenizer", ".", "convert_tokens_to_string", "(", "final_words", ")", ")", "\n", "feature", ".", "success", "=", "2", "\n", "return", "feature", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.save_to_original_BERT": [[382, 396], ["print", "torch.load", "torch.load", "model.load_state_dict", "print"], "function", ["None"], ["", "def", "save_to_original_BERT", "(", "model", ",", "save_file", ")", ":", "\n", "    ", "print", "(", "'model reload started!!'", ")", "\n", "paras", "=", "torch", ".", "load", "(", "save_file", ")", "\n", "\n", "paras_new", "=", "{", "}", "\n", "for", "ele", "in", "paras", ":", "\n", "#if 'module.' in ele:\t#'tgt_model.' in ele:", "\n", "        ", "if", "'tgt_model.'", "in", "ele", ":", "\n", "            ", "paras_new", "[", "ele", "[", "10", ":", "]", "]", "=", "paras", "[", "ele", "]", "\n", "", "else", ":", "\n", "            ", "paras_new", "[", "ele", "]", "=", "paras", "[", "ele", "]", "\n", "\n", "", "", "model", ".", "load_state_dict", "(", "paras_new", ")", "\n", "print", "(", "model", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.run_attack": [[397, 489], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "str", "str", "str", "str", "argparse.ArgumentParser.parse_args", "bert_robust.reader_data", "print", "transformers.BertTokenizer.from_pretrained", "transformers.BertConfig.from_pretrained", "transformers.BertForMaskedLM.from_pretrained", "BertForMaskedLM.from_pretrained.to", "transformers.BertConfig.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained", "bert_robust.save_to_original_BERT", "BertForSequenceClassification.from_pretrained.to", "print", "print", "open", "bert_robust.get_sim_embed", "os.path.join", "torch.no_grad", "torch.no_grad", "enumerate", "print", "bert_robust.Feature", "print", "bert_robust.attack", "ori_acc.append", "att_acc.append", "open.write", "print", "features_output.append", "q_num.append", "perturb.append", "parser.parse_args.data_path.split", "attack.final_adverse.strip", "len", "sum", "len", "sum", "len", "sum", "len", "sum", "len", "attack.seq.strip().split", "attack.seq.strip", "str", "attack.seq.strip"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.save_to_original_BERT", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.get_sim_embed", "home.repos.pwc.inspect_result.rucaibox_vda.None.bert_robust.attack"], ["", "def", "run_attack", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--data_path\"", ",", "type", "=", "str", ",", "default", "=", "\"Robust/yelp.txt\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/yelp_vda.pt\"", ",", "help", "=", "\"xxx classifier\"", ")", "\n", "\n", "parser", ".", "add_argument", "(", "\"--output_dir\"", ",", "type", "=", "str", ",", "default", "=", "\"data_defense\"", ",", "help", "=", "\"train file\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_sim_mat\"", ",", "type", "=", "int", ",", "default", "=", "1", ",", "\n", "help", "=", "'whether use cosine_similarity to filter out atonyms'", ")", "\n", "parser", ".", "add_argument", "(", "\"--start\"", ",", "type", "=", "int", ",", "default", "=", "0", ",", "help", "=", "\"start step, for multi-thread process\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--end\"", ",", "type", "=", "int", ",", "default", "=", "2000", ",", "help", "=", "\"end step, for multi-thread process\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--use_bpe\"", ",", "type", "=", "int", ",", "default", "=", "1", ")", "\n", "parser", ".", "add_argument", "(", "\"--k\"", ",", "type", "=", "int", ",", "default", "=", "48", ")", "\n", "parser", ".", "add_argument", "(", "\"--threshold_pred_score\"", ",", "type", "=", "float", ",", "default", "=", "0", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "type", "=", "int", ",", "default", "=", "512", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "data_path", "=", "str", "(", "args", ".", "data_path", ")", "\n", "mlm_path", "=", "str", "(", "args", ".", "mlm_path", ")", "\n", "tgt_path", "=", "str", "(", "args", ".", "tgt_path", ")", "\n", "output_dir", "=", "str", "(", "args", ".", "output_dir", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "if", "'ag'", "in", "data_path", "or", "'yelp'", "in", "data_path", ":", "\n", "        ", "add", "=", "True", "\n", "", "else", ":", "\n", "        ", "add", "=", "False", "\n", "", "features", "=", "reader_data", "(", "data_path", ",", "add", ")", "\n", "\n", "# features = get_data_cls(data_path)", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "use_bpe", "=", "args", ".", "use_bpe", "\n", "k", "=", "args", ".", "k", "\n", "threshold_pred_score", "=", "args", ".", "threshold_pred_score", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "mlm_path", ")", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "mlm_path", ",", "config", "=", "config_atk", ")", "\n", "mlm_model", ".", "to", "(", "'cuda'", ")", "\n", "\n", "config_tgt", "=", "BertConfig", ".", "from_pretrained", "(", "mlm_path", ",", "num_labels", "=", "num_label", ")", "\n", "tgt_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "mlm_path", ",", "config", "=", "config_tgt", ")", "\n", "#tgt_model.load_state_dict(torch.load(tgt_path).state_dict())", "\n", "save_to_original_BERT", "(", "tgt_model", ",", "tgt_path", ")", "\n", "tgt_model", ".", "to", "(", "'cuda'", ")", "\n", "\n", "print", "(", "'loading sim-embed'", ")", "\n", "\n", "if", "args", ".", "use_sim_mat", "==", "1", ":", "\n", "        ", "cos_mat", ",", "w2i", ",", "i2w", "=", "get_sim_embed", "(", "'counter-fitted-vectors.txt'", ",", "'cos_sim_counter_fitting.npy'", ")", "\n", "", "else", ":", "\n", "        ", "cos_mat", ",", "w2i", ",", "i2w", "=", "None", ",", "{", "}", ",", "{", "}", "\n", "\n", "", "print", "(", "'finish get-sim-embed'", ")", "\n", "features_output", "=", "[", "]", "\n", "out_f", "=", "open", "(", "os", ".", "path", ".", "join", "(", "output_dir", ",", "args", ".", "data_path", ".", "split", "(", "'/'", ")", "[", "-", "2", "]", "+", "'_adversaries.txt'", ")", ",", "'w'", ")", "\n", "\n", "ori_acc", "=", "[", "]", "\n", "att_acc", "=", "[", "]", "\n", "q_num", "=", "[", "]", "\n", "perturb", "=", "[", "]", "\n", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "        ", "for", "index", ",", "feature", "in", "enumerate", "(", "features", ")", ":", "\n", "# print(feature)", "\n", "            ", "seq_a", ",", "label", "=", "feature", "\n", "feat", "=", "Feature", "(", "seq_a", ",", "label", ")", "\n", "print", "(", "'\\r number {:d} '", ".", "format", "(", "index", ")", "+", "tgt_path", ",", "end", "=", "''", ")", "\n", "# print(feat.seq[:100], feat.label)", "\n", "feat", "=", "attack", "(", "feat", ",", "tgt_model", ",", "mlm_model", ",", "tokenizer_tgt", ",", "k", ",", "batch_size", "=", "32", ",", "max_length", "=", "512", ",", "\n", "cos_mat", "=", "cos_mat", ",", "w2i", "=", "w2i", ",", "i2w", "=", "i2w", ",", "use_bpe", "=", "use_bpe", ",", "threshold_pred_score", "=", "threshold_pred_score", ")", "\n", "\n", "# print(feat.changes, feat.change, feat.query, feat.success)", "\n", "ori_acc", ".", "append", "(", "feat", ".", "ori_acc", ")", "\n", "att_acc", ".", "append", "(", "feat", ".", "att_acc", ")", "\n", "if", "feat", ".", "ori_acc", "==", "1", ":", "\n", "                ", "q_num", ".", "append", "(", "feat", ".", "query", ")", "\n", "perturb", ".", "append", "(", "feat", ".", "change", "/", "(", "len", "(", "feat", ".", "seq", ".", "strip", "(", ")", ".", "split", "(", ")", ")", ")", ")", "\n", "\n", "# print(feat.changes, feat.change, feat.query, feat.success)", "\n", "", "new_line", "=", "str", "(", "feat", ".", "label", ")", "+", "'\\t'", "+", "feat", ".", "seq", ".", "strip", "(", ")", "+", "'\\t'", "+", "feat", ".", "final_adverse", ".", "strip", "(", ")", "+", "'\\n'", "\n", "out_f", ".", "write", "(", "new_line", ")", "\n", "print", "(", "'success'", ",", "end", "=", "''", ")", "\n", "features_output", ".", "append", "(", "feat", ")", "\n", "", "print", "(", "'original accuracy is %f, attack accuracy is %f, query num is %f, perturb rate is %f'", "\n", "%", "(", "sum", "(", "ori_acc", ")", "/", "len", "(", "ori_acc", ")", ",", "sum", "(", "att_acc", ")", "/", "len", "(", "att_acc", ")", ",", "sum", "(", "q_num", ")", "/", "len", "(", "q_num", ")", ",", "sum", "(", "perturb", ")", "/", "len", "(", "perturb", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_freelb.EmbAdapterDataset.__init__": [[16, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "data", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "args", ".", "max_length", "\n", "self", ".", "pad_token", "=", "0", "\n", "self", ".", "mask_ids", "=", "103", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_freelb.EmbAdapterDataset.__getitem__": [[23, 31], ["text_classifier_freelb.EmbAdapterDataset.tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "x", ",", "y", "=", "self", ".", "data", "[", "index", "]", "\n", "inputs", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "x", ",", "None", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "max_length", ",", "truncation", "=", "True", ")", "\n", "padding_length", "=", "self", ".", "max_length", "-", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", "+", "padding_length", "*", "[", "self", ".", "pad_token", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "self", ".", "max_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_freelb.EmbAdapterDataset.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_freelb.FreeLB.__init__": [[36, 45], ["torch.Module.__init__", "transformers.BertConfig.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained().cuda", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.KLDivLoss", "torch.KLDivLoss", "transformers.BertForSequenceClassification.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "nclasses", ")", ":", "\n", "        ", "super", "(", "FreeLB", ",", "self", ")", ".", "__init__", "(", ")", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "num_labels", "=", "nclasses", ")", "\n", "self", ".", "tgt_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "config", "=", "config_atk", ")", ".", "cuda", "(", ")", "\n", "self", ".", "variance", "=", "args", ".", "variation", "\n", "self", ".", "variance_vda", "=", "args", ".", "variation_vda", "\n", "self", ".", "step", "=", "args", ".", "step", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "kl_criterion", "=", "nn", ".", "KLDivLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_freelb.FreeLB.forward": [[46, 48], ["text_classifier_freelb.FreeLB.tgt_model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", ":", "\n", "        ", "return", "self", ".", "tgt_model", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_freelb.FreeLB.defense": [[49, 102], ["input_ids.size", "text_classifier_freelb.FreeLB.tgt_model.bert.get_extended_attention_mask", "text_classifier_freelb.FreeLB.tgt_model.bert.get_head_mask", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_classifier_freelb.FreeLB.tgt_model.bert.embeddings", "text_classifier_freelb.FreeLB.tgt_model.bert.encoder", "text_classifier_freelb.FreeLB.tgt_model.classifier", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "text_classifier_freelb.FreeLB.tgt_model.bert.pooler", "torch.softmax.size", "torch.softmax.size", "noise.cuda"], "methods", ["None"], ["", "def", "defense", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "mlm_model", ")", ":", "\n", "        ", "output_attentions", "=", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_hidden_states", ")", "\n", "return_dict", "=", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "use_return_dict", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "\n", "input_shape", ",", "device", ")", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "self", ".", "variance_vda", "\n", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ".", "cuda", "(", ")", ",", "-", "1", ")", "# [B, Len, V]", "\n", "\n", "word_embs", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", "return_dict", "=", "return_dict", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "# logits = self.output_linear(prompt_output[0])", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_freelb.FreeLB.adv_train": [[103, 162], ["text_classifier_freelb.FreeLB.defense", "input_ids.size", "text_classifier_freelb.FreeLB.tgt_model.bert.get_extended_attention_mask", "text_classifier_freelb.FreeLB.tgt_model.bert.get_head_mask", "text_classifier_freelb.FreeLB.tgt_model.bert.embeddings", "range", "optim.step", "scheduler.step", "text_classifier_freelb.FreeLB.tgt_model.zero_grad", "text_classifier_freelb.FreeLB.tgt_model.bert.encoder", "text_classifier_freelb.FreeLB.tgt_model.dropout", "text_classifier_freelb.FreeLB.tgt_model.classifier", "text_classifier_freelb.FreeLB.criterion", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "output_losses.append", "sum", "kl_loss.cpu().item", "text_classifier_freelb.FreeLB.tgt_model.bert.pooler", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "text_classifier_freelb.FreeLB.cpu().item", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "text_classifier_freelb.FreeLB.kl_criterion", "text_classifier_freelb.FreeLB.kl_criterion", "kl_loss.cpu", "text_classifier_freelb.FreeLB.size", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "text_classifier_freelb.FreeLB.cpu", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.defense"], ["", "def", "adv_train", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ",", "y", ",", "optim", ",", "scheduler", ",", "mlm_model", ")", ":", "\n", "        ", "noise_logits", "=", "self", ".", "defense", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ",", "mlm_model", ")", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_masks", ",", "\n", "input_shape", ",", "device", ")", "\n", "\n", "encoder_extended_attention_mask", "=", "None", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", ")", "\n", "\n", "noise", "=", "(", "torch", ".", "rand", "(", "embedding_output", ".", "size", "(", ")", ",", "device", "=", "device", ")", "-", "0.5", ")", "*", "2", "*", "self", ".", "variance", "\n", "noise", ".", "requires_grad", "=", "True", "\n", "\n", "output_losses", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "step", ")", ":", "\n", "            ", "embedding_output", "=", "embedding_output", "+", "noise", "\n", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "loss", "=", "self", ".", "criterion", "(", "logits", ",", "y", ")", "\n", "kl_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "noise_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "noise_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "(", "loss", "+", "kl_loss", ")", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "noise_grad", "=", "torch", ".", "autograd", ".", "grad", "(", "loss", ",", "noise", ")", "[", "0", "]", "\n", "noise", "=", "noise", "+", "(", "noise_grad", "/", "torch", ".", "norm", "(", "noise_grad", ")", ")", ".", "mul_", "(", "0.1", ")", "\n", "noise", "=", "torch", ".", "clamp", "(", "noise", ",", "-", "1.0", "*", "self", ".", "variance", ",", "self", ".", "variance", ")", "\n", "#print(loss)", "\n", "#if i==0:", "\n", "#    loss.backward(retain_graph=True)", "\n", "#else:", "\n", "#    loss.backward()", "\n", "#print(torch.sum(noise))", "\n", "output_losses", ".", "append", "(", "loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "optim", ".", "step", "(", ")", "\n", "#print({name:param.grad for name, param in self.tgt_model.classifier.named_parameters()})", "\n", "scheduler", ".", "step", "(", ")", "\n", "self", ".", "tgt_model", ".", "zero_grad", "(", ")", "\n", "\n", "return", "sum", "(", "output_losses", ")", ",", "kl_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_freelb.reader_data": [[163, 175], ["open", "line.strip().split", "output.append", "int", "line.strip", "int"], "function", ["None"], ["", "", "def", "reader_data", "(", "filename", ",", "add", "=", "True", ")", ":", "\n", "#return [[ori_text, adv_text, label], ... ,[]]", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "output", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "label", ",", "text", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "if", "add", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "-", "1", "\n", "", "else", ":", "\n", "            ", "label", "=", "int", "(", "label", ")", "\n", "", "output", ".", "append", "(", "[", "text", ",", "label", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_classifier_freelb.run": [[177, 278], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "text_classifier_freelb.reader_data", "text_classifier_freelb.reader_data", "text_classifier_freelb.reader_data", "print", "transformers.BertForMaskedLM.from_pretrained().cuda", "transformers.BertTokenizer.from_pretrained", "text_classifier_freelb.EmbAdapterDataset", "text_classifier_freelb.EmbAdapterDataset", "text_classifier_freelb.EmbAdapterDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "text_classifier_freelb.FreeLB", "model.cuda.cuda", "model.cuda.parameters", "transformers.AdamW", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup", "range", "str", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "model.cuda.train", "tqdm.tqdm", "print", "model.cuda.eval", "print", "print", "transformers.BertForMaskedLM.from_pretrained", "len", "model.cuda.zero_grad", "model.cuda.adv_train", "total_loss.append", "torch.no_grad", "torch.no_grad", "torch.save", "torch.save", "print", "torch.no_grad", "torch.no_grad", "ele.cuda", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "model.cuda.tgt_model.state_dict", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "ele.cuda", "model.cuda.", "ele.cuda", "model.cuda.", "sum", "len", "sum", "len", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.adv_train"], ["", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "type", "=", "str", ",", "default", "=", "\"ag\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../TextFooler/target_models/mr1\"", ",", "\n", "help", "=", "\"xxx classifier\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/ag_freelb_vda.pt\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "16", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "4", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_warmup\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--variation\"", ",", "type", "=", "float", ",", "default", "=", "0.02", ")", "\n", "parser", ".", "add_argument", "(", "\"--variation_vda\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--step\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "data_path", "=", "'data/'", "+", "str", "(", "args", ".", "dataset", ")", "\n", "if", "args", ".", "dataset", "==", "'ag'", "or", "args", ".", "dataset", "==", "'yelp'", ":", "\n", "        ", "add", "=", "True", "\n", "", "else", ":", "\n", "        ", "add", "=", "False", "\n", "", "train_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'train.txt'", ")", ",", "add", ")", "\n", "dev_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'dev.txt'", ")", ",", "add", ")", "\n", "test_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'test.txt'", ")", ",", "add", ")", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "args", ".", "mlm_path", ")", ".", "cuda", "(", ")", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "train_data", "=", "EmbAdapterDataset", "(", "args", ",", "train_features", ",", "tokenizer_tgt", ")", "\n", "dev_data", "=", "EmbAdapterDataset", "(", "args", ",", "dev_features", ",", "tokenizer_tgt", ")", "\n", "test_data", "=", "EmbAdapterDataset", "(", "args", ",", "test_features", ",", "tokenizer_tgt", ")", "\n", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "dev_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "test_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "print", "(", "'start building model'", ")", "\n", "model", "=", "FreeLB", "(", "args", ",", "args", ".", "num_label", ")", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "params", "=", "model", ".", "parameters", "(", ")", "\n", "#need_grad = lambda x: x.requires_grad", "\n", "optimizer", "=", "AdamW", "(", "\n", "params", ",", "\n", "lr", "=", "args", ".", "lr", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.01", ",", "\n", ")", "\n", "total_num", "=", "len", "(", "train_data", ")", "//", "args", ".", "batch_size", "*", "args", ".", "max_epoch", "\n", "scheduler", "=", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "num_warmup", "*", "total_num", ",", "\n", "num_training_steps", "=", "total_num", ")", "\n", "best_ratio", "=", "0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "max_epoch", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "total_loss", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "train_dataloader", ")", ":", "\n", "            ", "model", ".", "zero_grad", "(", ")", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "#origin_emb = model.produce_emb(input_ids, attention_mask, token_type_ids)", "\n", "loss", ",", "kl_loss", "=", "model", ".", "adv_train", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "y", ",", "optimizer", ",", "scheduler", ",", "mlm_model", ")", "\n", "total_loss", ".", "append", "(", "[", "loss", ",", "kl_loss", "]", ")", "\n", "\n", "", "print", "(", "'Epoch %d, the training ce loss is %f, the kl loss is %f, the total number is %f'", "\n", "%", "(", "epoch", ",", "sum", "(", "[", "ele", "[", "0", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "\n", "sum", "(", "[", "ele", "[", "1", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "dev_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "if", "best_ratio", "<", "attack_ratio", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "tgt_model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", ")", "\n", "print", "(", "'--------save once-----------'", ")", "\n", "best_ratio", "=", "attack_ratio", "\n", "", "print", "(", "'The dev set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "test_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "print", "(", "'The test set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.EmbAdapterDataset.__init__": [[20, 26], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "data", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "args", ".", "max_length", "\n", "self", ".", "pad_token", "=", "0", "\n", "self", ".", "mask_ids", "=", "103", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.EmbAdapterDataset.__getitem__": [[27, 36], ["text_pair_classifier_smix.EmbAdapterDataset.tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "x1", ",", "x2", ",", "y", "=", "self", ".", "data", "[", "index", "]", "\n", "inputs", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "x1", ",", "x2", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "max_length", ",", "\n", "truncation", "=", "True", ")", "\n", "padding_length", "=", "self", ".", "max_length", "-", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", "+", "padding_length", "*", "[", "self", ".", "pad_token", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"token_type_ids\"", "]", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.EmbAdapterDataset.__len__": [[37, 39], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.BertEncoder4SentMix.__init__": [[41, 49], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "transformers.BertLayer", "range"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertEncoder4SentMix", ",", "self", ")", ".", "__init__", "(", ")", "\n", "# self.output_attentions = config.output_attentions", "\n", "# self.output_hidden_states = config.output_hidden_states", "\n", "self", ".", "output_attentions", "=", "False", "\n", "self", ".", "output_hidden_states", "=", "True", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "[", "BertLayer", "(", "config", ")", "\n", "for", "_", "in", "range", "(", "config", ".", "num_hidden_layers", ")", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.BertEncoder4SentMix.forward": [[50, 110], ["enumerate", "layer_module", "layer_module", "layer_module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "hidden_states2", "=", "None", ",", "l", "=", "None", ",", "mix_layer", "=", "1000", ",", "attention_mask", "=", "None", ",", "\n", "attention_mask2", "=", "None", ",", "head_mask", "=", "None", ")", ":", "\n", "        ", "all_hidden_states", "=", "(", ")", "\n", "all_attentions", "=", "(", ")", "\n", "\n", "# Perform mix at till the mix_layer", "\n", "## mix_layer == -1: mixup at embedding layer", "\n", "if", "mix_layer", "==", "-", "1", ":", "\n", "            ", "if", "hidden_states2", "is", "not", "None", ":", "\n", "                ", "hidden_states", "=", "l", "*", "hidden_states", "+", "(", "1", "-", "l", ")", "*", "hidden_states2", "\n", "\n", "", "", "for", "i", ",", "layer_module", "in", "enumerate", "(", "self", ".", "layer", ")", ":", "\n", "            ", "if", "i", "<=", "mix_layer", ":", "\n", "\n", "                ", "if", "self", ".", "output_hidden_states", ":", "\n", "                    ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "head_mask", "[", "i", "]", ")", "\n", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "if", "self", ".", "output_attentions", ":", "\n", "                    ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "", "if", "hidden_states2", "is", "not", "None", ":", "\n", "                    ", "layer_outputs2", "=", "layer_module", "(", "\n", "hidden_states2", ",", "attention_mask2", ",", "head_mask", "[", "i", "]", ")", "\n", "hidden_states2", "=", "layer_outputs2", "[", "0", "]", "\n", "\n", "", "", "if", "i", "==", "mix_layer", ":", "\n", "                ", "if", "hidden_states2", "is", "not", "None", ":", "\n", "# hidden_states = l * hidden_states + (1-l)*hidden_states2", "\n", "# attention_mask = attention_mask.long() | attention_mask2.long()", "\n", "# sentMix: (bsz, len, hid)", "\n", "                    ", "hidden_states", "[", ":", ",", "0", ",", ":", "]", "=", "l", "*", "hidden_states", "[", ":", ",", "0", ",", ":", "]", "+", "(", "1", "-", "l", ")", "*", "hidden_states2", "[", ":", ",", "0", ",", ":", "]", "\n", "\n", "", "", "if", "i", ">", "mix_layer", ":", "\n", "                ", "if", "self", ".", "output_hidden_states", ":", "\n", "                    ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "layer_outputs", "=", "layer_module", "(", "\n", "hidden_states", ",", "attention_mask", ",", "head_mask", "[", "i", "]", ")", "\n", "hidden_states", "=", "layer_outputs", "[", "0", "]", "\n", "\n", "if", "self", ".", "output_attentions", ":", "\n", "                    ", "all_attentions", "=", "all_attentions", "+", "(", "layer_outputs", "[", "1", "]", ",", ")", "\n", "\n", "# Add last layer", "\n", "", "", "", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "all_hidden_states", "=", "all_hidden_states", "+", "(", "hidden_states", ",", ")", "\n", "\n", "", "outputs", "=", "(", "hidden_states", ",", ")", "\n", "if", "self", ".", "output_hidden_states", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_hidden_states", ",", ")", "\n", "", "if", "self", ".", "output_attentions", ":", "\n", "            ", "outputs", "=", "outputs", "+", "(", "all_attentions", ",", ")", "\n", "# last-layer hidden state, (all hidden states), (all attentions)", "\n", "# print (len(outputs))", "\n", "# print (len(outputs[1])) ##hidden states: 13", "\n", "", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.BertModel4SentMix.__init__": [[112, 119], ["transformers.BertPreTrainedModel.__init__", "transformers.models.bert.modeling_bert.BertEmbeddings", "text_pair_classifier_smix.BertEncoder4SentMix", "transformers.models.bert.modeling_bert.BertPooler", "text_pair_classifier_smix.BertModel4SentMix.init_weights"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "BertModel4SentMix", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "BertEncoder4SentMix", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.BertModel4SentMix._resize_token_embeddings": [[120, 126], ["text_pair_classifier_smix.BertModel4SentMix._get_resized_embeddings"], "methods", ["None"], ["", "def", "_resize_token_embeddings", "(", "self", ",", "new_num_tokens", ")", ":", "\n", "        ", "old_embeddings", "=", "self", ".", "embeddings", ".", "word_embeddings", "\n", "new_embeddings", "=", "self", ".", "_get_resized_embeddings", "(", "\n", "old_embeddings", ",", "new_num_tokens", ")", "\n", "self", ".", "embeddings", ".", "word_embeddings", "=", "new_embeddings", "\n", "return", "self", ".", "embeddings", ".", "word_embeddings", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.BertModel4SentMix._prune_heads": [[127, 134], ["heads_to_prune.items", "text_pair_classifier_smix.BertModel4SentMix.encoder.layer[].attention.prune_heads"], "methods", ["None"], ["", "def", "_prune_heads", "(", "self", ",", "heads_to_prune", ")", ":", "\n", "        ", "\"\"\" Prunes heads of the model.\n            heads_to_prune: dict of {layer_num: list of heads to prune in this layer}\n            See base class PreTrainedModel\n        \"\"\"", "\n", "for", "layer", ",", "heads", "in", "heads_to_prune", ".", "items", "(", ")", ":", "\n", "            ", "self", ".", "encoder", ".", "layer", "[", "layer", "]", ".", "attention", ".", "prune_heads", "(", "heads", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.BertModel4SentMix.forward": [[135, 199], ["input_ids.size", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "text_pair_classifier_smix.BertModel4SentMix.embeddings", "text_pair_classifier_smix.BertModel4SentMix.pooler", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask2.to.to.to", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.to", "text_pair_classifier_smix.BertModel4SentMix.embeddings", "text_pair_classifier_smix.BertModel4SentMix.encoder", "text_pair_classifier_smix.BertModel4SentMix.encoder", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.ones_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.zeros_like", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.expand", "next", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "torch.ones_like.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.dim", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "text_pair_classifier_smix.BertModel4SentMix.parameters", "next", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze", "next", "text_pair_classifier_smix.BertModel4SentMix.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "text_pair_classifier_smix.BertModel4SentMix.parameters", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze", "head_mask.unsqueeze().unsqueeze().unsqueeze.unsqueeze().unsqueeze().unsqueeze.unsqueeze"], "methods", ["None"], ["", "", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "input_ids2", "=", "None", ",", "attention_mask2", "=", "None", ",", "l", "=", "None", ",", "mix_layer", "=", "1000", ",", "\n", "token_type_ids", "=", "None", ",", "position_ids", "=", "None", ",", "head_mask", "=", "None", ",", "inputs_embeds", "=", "None", ")", ":", "\n", "\n", "        ", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "if", "attention_mask", "is", "None", ":", "\n", "            ", "if", "input_ids2", "is", "not", "None", ":", "\n", "                ", "attention_mask2", "=", "torch", ".", "ones_like", "(", "input_ids2", ",", "device", "=", "device", ")", "\n", "", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ",", "device", "=", "device", ")", "\n", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "if", "input_ids2", "is", "not", "None", ":", "\n", "                ", "token_type_ids2", "=", "torch", ".", "zeros_like", "(", "input_ids2", ",", "dtype", "=", "torch", ".", "long", ",", "device", "=", "device", ")", "\n", "\n", "", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "if", "input_ids2", "is", "not", "None", ":", "\n", "            ", "extended_attention_mask2", "=", "attention_mask2", ".", "unsqueeze", "(", "\n", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "extended_attention_mask2", "=", "extended_attention_mask2", ".", "to", "(", "\n", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask2", "=", "(", "1.0", "-", "extended_attention_mask2", ")", "*", "-", "10000.0", "\n", "\n", "", "if", "head_mask", "is", "not", "None", ":", "\n", "            ", "if", "head_mask", ".", "dim", "(", ")", "==", "1", ":", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "0", ")", ".", "unsqueeze", "(", "\n", "0", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "head_mask", "=", "head_mask", ".", "expand", "(", "\n", "self", ".", "config", ".", "num_hidden_layers", ",", "-", "1", ",", "-", "1", ",", "-", "1", ",", "-", "1", ")", "\n", "", "elif", "head_mask", ".", "dim", "(", ")", "==", "2", ":", "\n", "# We can specify head_mask for each layer", "\n", "                ", "head_mask", "=", "head_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", ".", "unsqueeze", "(", "-", "1", ")", "\n", "# switch to fload if need + fp16 compatibility", "\n", "", "head_mask", "=", "head_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "\n", "", "else", ":", "\n", "            ", "head_mask", "=", "[", "None", "]", "*", "self", ".", "config", ".", "num_hidden_layers", "\n", "\n", "", "embedding_output", "=", "self", ".", "embeddings", "(", "\n", "input_ids", ",", "position_ids", "=", "position_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "\n", "if", "input_ids2", "is", "not", "None", ":", "\n", "            ", "embedding_output2", "=", "self", ".", "embeddings", "(", "\n", "input_ids2", ",", "position_ids", "=", "position_ids", ",", "token_type_ids", "=", "token_type_ids", ")", "\n", "\n", "", "if", "input_ids2", "is", "not", "None", ":", "\n", "            ", "encoder_outputs", "=", "self", ".", "encoder", "(", "embedding_output", ",", "embedding_output2", ",", "l", ",", "mix_layer", ",", "\n", "extended_attention_mask", ",", "extended_attention_mask2", ",", "head_mask", "=", "head_mask", ")", "\n", "", "else", ":", "\n", "            ", "encoder_outputs", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "attention_mask", "=", "extended_attention_mask", ",", "head_mask", "=", "head_mask", ")", "\n", "\n", "", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "sequence_output", ")", "\n", "\n", "# add hidden_states and attentions if they are here", "\n", "outputs", "=", "(", "sequence_output", ",", "pooled_output", ",", ")", "+", "encoder_outputs", "[", "1", ":", "]", "\n", "# sequence_output, pooled_output, (hidden_states), (attentions)", "\n", "return", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.__init__": [[202, 213], ["transformers.BertPreTrainedModel.__init__", "text_pair_classifier_smix.BertModel4SentMix", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.Linear", "torch.Linear", "torch.Linear", "torch.KLDivLoss", "torch.KLDivLoss", "torch.KLDivLoss", "text_pair_classifier_smix.SentMix.init_weights"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", "SentMix", ",", "self", ")", ".", "__init__", "(", "config", ")", "\n", "\n", "self", ".", "num_labels", "=", "config", ".", "num_labels", "\n", "self", ".", "bert", "=", "BertModel4SentMix", "(", "config", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "config", ".", "hidden_dropout_prob", ")", "\n", "self", ".", "classifier", "=", "nn", ".", "Linear", "(", "config", ".", "hidden_size", ",", "self", ".", "num_labels", ")", "\n", "\n", "self", ".", "kl_criterion", "=", "nn", ".", "KLDivLoss", "(", ")", "\n", "\n", "self", ".", "init_weights", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.forward": [[214, 234], ["text_pair_classifier_smix.SentMix.dropout", "text_pair_classifier_smix.SentMix.classifier", "text_pair_classifier_smix.SentMix.bert", "text_pair_classifier_smix.SentMix.bert"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ",", "attention_mask", ",", "x2", "=", "None", ",", "attention_mask2", "=", "None", ",", "l", "=", "None", ",", "mix_layer", "=", "1000", ",", "inputs_embeds", "=", "None", ",", "token_type_ids", "=", "None", ")", ":", "\n", "        ", "if", "x2", "is", "not", "None", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "x", ",", "attention_mask", ",", "x2", ",", "attention_mask", ",", "l", ",", "mix_layer", ",", "token_type_ids", "=", "token_type_ids", ",", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "\n", "# pooled_output = torch.mean(outputs[0], 1)", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "", "else", ":", "\n", "            ", "outputs", "=", "self", ".", "bert", "(", "x", ",", "attention_mask", ",", "token_type_ids", "=", "token_type_ids", ",", "inputs_embeds", "=", "inputs_embeds", ")", "\n", "\n", "# pooled_output = torch.mean(outputs[0], 1)", "\n", "pooled_output", "=", "outputs", "[", "1", "]", "\n", "\n", "\n", "", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "# sequence_output = outputs[0]", "\n", "# logits = self.classifier(sequence_output)", "\n", "\n", "return", "logits", ",", "outputs", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb": [[235, 246], ["text_pair_classifier_smix.SentMix.bert.encoder", "text_pair_classifier_smix.SentMix.dropout", "text_pair_classifier_smix.SentMix.classifier", "text_pair_classifier_smix.SentMix.bert.pooler"], "methods", ["None"], ["", "def", "output_with_emb", "(", "self", ",", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "encoder_extended_attention_mask", ")", ":", "\n", "        ", "encoder_outputs", "=", "self", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "pooled_output", "=", "self", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "classifier", "(", "pooled_output", ")", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.adv_train": [[247, 306], ["input_ids.size", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.randperm", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "torch.zeros().cuda().scatter_", "numpy.random.beta", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.kl_div", "torch.kl_div", "torch.kl_div", "input_ids.size", "text_pair_classifier_smix.SentMix.bert.get_extended_attention_mask", "text_pair_classifier_smix.SentMix.bert.get_head_mask", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_pair_classifier_smix.SentMix.bert.embeddings", "text_pair_classifier_smix.SentMix.output_with_emb", "total_loss.backward", "optim.step", "scheduler.step", "text_pair_classifier_smix.SentMix.zero_grad", "text_pair_classifier_smix.SentMix.forward", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "y.view", "torch.zeros().cuda().scatter_.view", "torch.zeros().cuda().scatter_.view", "torch.zeros().cuda().scatter_.view", "numpy.random.choice", "text_pair_classifier_smix.SentMix.forward", "torch.softmax.log", "torch.softmax.log", "torch.softmax.log", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "L_ori.cpu().item", "torch.kl_div.cpu().item", "vda_loss.cpu().item", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.softmax.size", "torch.softmax.size", "torch.softmax.size", "noise.cuda", "text_pair_classifier_smix.SentMix.kl_criterion", "text_pair_classifier_smix.SentMix.kl_criterion", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "L_ori.cpu", "torch.kl_div.cpu", "vda_loss.cpu", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "logits.size", "logits.size"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.SentMix.output_with_emb", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.forward", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.forward"], ["", "def", "adv_train", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ",", "y", ",", "optim", ",", "scheduler", ",", "mlm_model", ",", "args", ")", ":", "\n", "        ", "logits", "=", "self", ".", "forward", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", "=", "token_type_ids", ")", "[", "0", "]", "\n", "L_ori", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "(", "logits", ",", "y", ")", "\n", "\n", "batch_size", "=", "input_ids", ".", "size", "(", "0", ")", "\n", "idx", "=", "torch", ".", "randperm", "(", "batch_size", ")", "\n", "input_ids_2", "=", "input_ids", "[", "idx", "]", "\n", "labels_2", "=", "y", "[", "idx", "]", "\n", "attention_mask_2", "=", "attention_masks", "[", "idx", "]", "\n", "## convert the labels to one-hot", "\n", "labels", "=", "torch", ".", "zeros", "(", "batch_size", ",", "logits", ".", "size", "(", "-", "1", ")", ")", ".", "cuda", "(", ")", ".", "scatter_", "(", "\n", "1", ",", "y", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", "\n", ")", "\n", "labels_2", "=", "torch", ".", "zeros", "(", "batch_size", ",", "logits", ".", "size", "(", "-", "1", ")", ")", ".", "cuda", "(", ")", ".", "scatter_", "(", "\n", "1", ",", "labels_2", ".", "view", "(", "-", "1", ",", "1", ")", ",", "1", "\n", ")", "\n", "l", "=", "np", ".", "random", ".", "beta", "(", "args", ".", "alpha", ",", "args", ".", "alpha", ")", "\n", "# l = max(l, 1-l) ## not needed when only using labeled examples", "\n", "mixed_labels", "=", "l", "*", "labels", "+", "(", "1", "-", "l", ")", "*", "labels_2", "\n", "\n", "mix_layer", "=", "np", ".", "random", ".", "choice", "(", "args", ".", "mix_layers_set", ",", "1", ")", "[", "0", "]", "\n", "mix_layer", "=", "mix_layer", "-", "1", "\n", "\n", "logits", "=", "self", ".", "forward", "(", "input_ids", ",", "attention_masks", ",", "input_ids_2", ",", "attention_mask_2", ",", "l", ",", "mix_layer", ",", "token_type_ids", "=", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "torch", ".", "softmax", "(", "logits", ",", "dim", "=", "1", ")", "# (bsz, num_labels)", "\n", "L_mix", "=", "F", ".", "kl_div", "(", "probs", ".", "log", "(", ")", ",", "mixed_labels", ",", "None", ",", "None", ",", "'batchmean'", ")", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_masks", ",", "input_shape", ",", "device", ")", "\n", "\n", "encoder_extended_attention_mask", "=", "None", "\n", "head_mask", "=", "self", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "args", ".", "variance", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ".", "cuda", "(", ")", ",", "-", "1", ")", "# [B, Len, V]", "\n", "word_embs", "=", "self", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "embedding_output", "=", "self", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "vda_logits", "=", "self", ".", "output_with_emb", "(", "embedding_output", ",", "extended_attention_mask", ",", "head_mask", ",", "\n", "encoder_extended_attention_mask", ")", "\n", "vda_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "vda_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "vda_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "\n", "total_loss", "=", "L_ori", "+", "L_mix", "+", "vda_loss", "\n", "total_loss", ".", "backward", "(", ")", "\n", "optim", ".", "step", "(", ")", "\n", "#print({name:param.grad for name, param in self.tgt_model.classifier.named_parameters()})", "\n", "scheduler", ".", "step", "(", ")", "\n", "self", ".", "zero_grad", "(", ")", "\n", "\n", "return", "L_ori", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "L_mix", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "vda_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.reader_data": [[308, 317], ["open", "line.strip().split", "int", "output.append", "line.strip"], "function", ["None"], ["", "", "def", "reader_data", "(", "filename", ",", "add", "=", "True", ")", ":", "\n", "#return [[ori_text, adv_text, label], ... ,[]]", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "output", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "label", ",", "text1", ",", "text2", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "label", "=", "int", "(", "label", ")", "\n", "output", ".", "append", "(", "[", "text1", ",", "text2", ",", "label", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_smix.run": [[319, 427], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "text_pair_classifier_smix.reader_data", "text_pair_classifier_smix.reader_data", "text_pair_classifier_smix.reader_data", "print", "transformers.BertForMaskedLM.from_pretrained().cuda", "transformers.BertTokenizer.from_pretrained", "text_pair_classifier_smix.EmbAdapterDataset", "text_pair_classifier_smix.EmbAdapterDataset", "text_pair_classifier_smix.EmbAdapterDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "transformers.BertConfig.from_pretrained", "SentMix.from_pretrained", "model.cuda.cuda", "model.cuda.parameters", "transformers.AdamW", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup", "range", "str", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "model.cuda.train", "tqdm.tqdm", "print", "model.cuda.eval", "print", "print", "transformers.BertForMaskedLM.from_pretrained", "len", "model.cuda.zero_grad", "model.cuda.adv_train", "total_loss.append", "torch.no_grad", "torch.no_grad", "torch.no_grad", "torch.save", "torch.save", "torch.save", "print", "torch.no_grad", "torch.no_grad", "torch.no_grad", "ele.cuda", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "model.cuda.state_dict", "torch.argmax", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "ele.cuda", "model.cuda.", "ele.cuda", "model.cuda.", "sum", "len", "sum", "len", "sum", "len", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.adv_train"], ["", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "type", "=", "str", ",", "default", "=", "\"qnli\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../TextFooler/target_models/mrpc\"", ",", "\n", "help", "=", "\"xxx classifier\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/qnli_smix_vda.pt\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "30", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "12", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_warmup\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--variance\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--step\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--alpha\"", ",", "type", "=", "float", ",", "default", "=", "0.2", ")", "\n", "parser", ".", "add_argument", "(", "\n", "\"--mix-layers-set\"", ",", "\n", "nargs", "=", "'+'", ",", "\n", "default", "=", "[", "7", ",", "9", ",", "12", "]", ",", "\n", "type", "=", "int", ",", "\n", "help", "=", "\"define mix layer set\"", "\n", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "data_path", "=", "'data/'", "+", "str", "(", "args", ".", "dataset", ")", "\n", "train_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'train.txt'", ")", ")", "\n", "dev_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'dev.txt'", ")", ")", "\n", "test_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'test.txt'", ")", ")", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "args", ".", "mlm_path", ")", ".", "cuda", "(", ")", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "train_data", "=", "EmbAdapterDataset", "(", "args", ",", "train_features", ",", "tokenizer_tgt", ")", "\n", "dev_data", "=", "EmbAdapterDataset", "(", "args", ",", "dev_features", ",", "tokenizer_tgt", ")", "\n", "test_data", "=", "EmbAdapterDataset", "(", "args", ",", "test_features", ",", "tokenizer_tgt", ")", "\n", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "dev_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "test_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "print", "(", "'start building model'", ")", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "num_labels", "=", "args", ".", "num_label", ")", "\n", "model", "=", "SentMix", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "config", "=", "config_atk", ")", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "params", "=", "model", ".", "parameters", "(", ")", "\n", "#need_grad = lambda x: x.requires_grad", "\n", "optimizer", "=", "AdamW", "(", "\n", "params", ",", "\n", "lr", "=", "args", ".", "lr", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.01", ",", "\n", ")", "\n", "total_num", "=", "len", "(", "train_data", ")", "//", "args", ".", "batch_size", "*", "args", ".", "max_epoch", "\n", "scheduler", "=", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "num_warmup", "*", "total_num", ",", "\n", "num_training_steps", "=", "total_num", ")", "\n", "best_ratio", "=", "0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "max_epoch", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "total_loss", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "train_dataloader", ")", ":", "\n", "            ", "model", ".", "zero_grad", "(", ")", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "labels", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "#origin_emb = model.produce_emb(input_ids, attention_mask, token_type_ids)", "\n", "loss", ",", "kl_loss", ",", "vda_loss", "=", "model", ".", "adv_train", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "labels", ",", "\n", "optimizer", ",", "\n", "scheduler", ",", "mlm_model", ",", "args", ")", "\n", "total_loss", ".", "append", "(", "[", "loss", ",", "kl_loss", ",", "vda_loss", "]", ")", "\n", "\n", "", "print", "(", "'Epoch %d, the training ce loss is %f, mix loss is %f, vda loss is %f, the total number is %f'", "\n", "%", "(", "epoch", ",", "sum", "(", "[", "ele", "[", "0", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "\n", "sum", "(", "[", "ele", "[", "1", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "\n", "sum", "(", "[", "ele", "[", "2", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "dev_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "if", "best_ratio", "<", "attack_ratio", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", ")", "\n", "print", "(", "'--------save once-----------'", ")", "\n", "best_ratio", "=", "attack_ratio", "\n", "", "print", "(", "'The dev set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "test_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "print", "(", "'The test set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.EmbAdapterDataset.__init__": [[16, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "data", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "args", ".", "max_length", "\n", "self", ".", "pad_token", "=", "0", "\n", "self", ".", "mask_ids", "=", "103", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.EmbAdapterDataset.__getitem__": [[23, 31], ["text_pair_classifier_freelb.EmbAdapterDataset.tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "x1", ",", "x2", ",", "y", "=", "self", ".", "data", "[", "index", "]", "\n", "inputs", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "x1", ",", "x2", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "max_length", ",", "truncation", "=", "True", ")", "\n", "padding_length", "=", "self", ".", "max_length", "-", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", "+", "padding_length", "*", "[", "self", ".", "pad_token", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"token_type_ids\"", "]", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.EmbAdapterDataset.__len__": [[32, 34], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.__init__": [[36, 45], ["torch.Module.__init__", "transformers.BertConfig.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained().cuda", "torch.CrossEntropyLoss", "torch.CrossEntropyLoss", "torch.KLDivLoss", "torch.KLDivLoss", "transformers.BertForSequenceClassification.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "nclasses", ")", ":", "\n", "        ", "super", "(", "FreeLB", ",", "self", ")", ".", "__init__", "(", ")", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "num_labels", "=", "nclasses", ")", "\n", "self", ".", "tgt_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "config", "=", "config_atk", ")", ".", "cuda", "(", ")", "\n", "self", ".", "variance", "=", "args", ".", "variation", "\n", "self", ".", "variance_vda", "=", "args", ".", "variation_vda", "\n", "self", ".", "step", "=", "args", ".", "step", "\n", "self", ".", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "self", ".", "kl_criterion", "=", "nn", ".", "KLDivLoss", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.forward": [[46, 48], ["text_pair_classifier_freelb.FreeLB.tgt_model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", ":", "\n", "        ", "return", "self", ".", "tgt_model", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.defense": [[49, 100], ["input_ids.size", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.get_extended_attention_mask", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.get_head_mask", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.embeddings", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.encoder", "text_pair_classifier_freelb.FreeLB.tgt_model.classifier", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.pooler", "torch.softmax.size", "torch.softmax.size", "noise.cuda"], "methods", ["None"], ["", "def", "defense", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "mlm_model", ")", ":", "\n", "        ", "output_attentions", "=", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_hidden_states", ")", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "\n", "input_shape", ",", "device", ")", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "self", ".", "variance_vda", "\n", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ".", "cuda", "(", ")", ",", "-", "1", ")", "# [B, Len, V]", "\n", "\n", "word_embs", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "# logits = self.output_linear(prompt_output[0])", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.adv_train": [[101, 161], ["text_pair_classifier_freelb.FreeLB.defense", "input_ids.size", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.get_extended_attention_mask", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.get_head_mask", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.embeddings", "torch.clamp.cuda", "torch.clamp.cuda", "range", "optim.step", "scheduler.step", "text_pair_classifier_freelb.FreeLB.tgt_model.zero_grad", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.encoder", "text_pair_classifier_freelb.FreeLB.tgt_model.dropout", "text_pair_classifier_freelb.FreeLB.tgt_model.classifier", "text_pair_classifier_freelb.FreeLB.criterion", "torch.clamp", "torch.clamp", "torch.clamp", "torch.clamp", "output_losses.append", "sum", "kl_loss.cpu().item", "text_pair_classifier_freelb.FreeLB.tgt_model.bert.pooler", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "torch.autograd.grad", "text_pair_classifier_freelb.FreeLB.cpu().item", "torch.rand", "torch.rand", "torch.rand", "torch.rand", "text_pair_classifier_freelb.FreeLB.kl_criterion", "text_pair_classifier_freelb.FreeLB.kl_criterion", "kl_loss.cpu", "text_pair_classifier_freelb.FreeLB.size", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "text_pair_classifier_freelb.FreeLB.cpu", "torch.norm", "torch.norm", "torch.norm", "torch.norm"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.defense"], ["", "def", "adv_train", "(", "self", ",", "input_ids", ",", "attention_masks", ",", "token_type_ids", ",", "y", ",", "optim", ",", "scheduler", ",", "mlm_model", ")", ":", "\n", "        ", "noise_logits", "=", "self", ".", "defense", "(", "input_ids", ",", "attention_masks", ",", "token_type_ids", ",", "mlm_model", ")", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "device", "=", "input_ids", ".", "device", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_masks", ",", "\n", "input_shape", ",", "device", ")", "\n", "\n", "encoder_extended_attention_mask", "=", "None", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", ")", "\n", "\n", "noise", "=", "(", "torch", ".", "rand", "(", "embedding_output", ".", "size", "(", ")", ",", "device", "=", "device", ")", "-", "0.5", ")", "*", "2", "*", "self", ".", "variance", "\n", "noise", "=", "noise", ".", "cuda", "(", ")", "\n", "noise", ".", "requires_grad", "=", "True", "\n", "\n", "output_losses", "=", "[", "]", "\n", "\n", "for", "i", "in", "range", "(", "self", ".", "step", ")", ":", "\n", "            ", "embedding_output", "=", "embedding_output", "+", "noise", "\n", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "encoder_extended_attention_mask", ",", "\n", "output_attentions", "=", "None", ",", "\n", "output_hidden_states", "=", "None", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "dropout", "(", "pooled_output", ")", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "loss", "=", "self", ".", "criterion", "(", "logits", ",", "y", ")", "\n", "kl_loss", "=", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "noise_logits", ",", "-", "1", ")", ")", "+", "0.5", "*", "self", ".", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "noise_logits", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "logits", ",", "-", "1", ")", ")", "\n", "(", "loss", "+", "kl_loss", ")", ".", "backward", "(", "retain_graph", "=", "True", ")", "\n", "noise_grad", "=", "torch", ".", "autograd", ".", "grad", "(", "loss", ",", "noise", ")", "[", "0", "]", "\n", "noise", "=", "noise", "+", "(", "noise_grad", "/", "torch", ".", "norm", "(", "noise_grad", ")", ")", ".", "mul_", "(", "0.1", ")", "\n", "noise", "=", "torch", ".", "clamp", "(", "noise", ",", "-", "1.0", "*", "self", ".", "variance", ",", "self", ".", "variance", ")", "\n", "#print(loss)", "\n", "#if i==0:", "\n", "#    loss.backward(retain_graph=True)", "\n", "#else:", "\n", "#    loss.backward()", "\n", "#print(torch.sum(noise))", "\n", "output_losses", ".", "append", "(", "loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ")", "\n", "\n", "", "optim", ".", "step", "(", ")", "\n", "#print({name:param.grad for name, param in self.tgt_model.classifier.named_parameters()})", "\n", "scheduler", ".", "step", "(", ")", "\n", "self", ".", "tgt_model", ".", "zero_grad", "(", ")", "\n", "\n", "return", "sum", "(", "output_losses", ")", ",", "kl_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.reader_data": [[162, 171], ["open", "line.strip().split", "int", "output.append", "line.strip"], "function", ["None"], ["", "", "def", "reader_data", "(", "filename", ",", "add", "=", "True", ")", ":", "\n", "#return [[ori_text, adv_text, label], ... ,[]]", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "output", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "label", ",", "text1", ",", "text2", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "label", "=", "int", "(", "label", ")", "\n", "output", ".", "append", "(", "[", "text1", ",", "text2", ",", "label", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.run": [[173, 270], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "text_pair_classifier_freelb.reader_data", "text_pair_classifier_freelb.reader_data", "text_pair_classifier_freelb.reader_data", "print", "transformers.BertForMaskedLM.from_pretrained().cuda", "transformers.BertTokenizer.from_pretrained", "text_pair_classifier_freelb.EmbAdapterDataset", "text_pair_classifier_freelb.EmbAdapterDataset", "text_pair_classifier_freelb.EmbAdapterDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "text_pair_classifier_freelb.FreeLB", "model.cuda.cuda", "model.cuda.parameters", "transformers.AdamW", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup", "range", "str", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "model.cuda.train", "tqdm.tqdm", "print", "model.cuda.eval", "print", "print", "transformers.BertForMaskedLM.from_pretrained", "len", "model.cuda.zero_grad", "model.cuda.adv_train", "total_loss.append", "torch.no_grad", "torch.no_grad", "torch.save", "torch.save", "print", "torch.no_grad", "torch.no_grad", "ele.cuda", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "model.cuda.tgt_model.state_dict", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "ele.cuda", "model.cuda.", "ele.cuda", "model.cuda.", "sum", "len", "sum", "len", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier_freelb.FreeLB.adv_train"], ["", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "type", "=", "str", ",", "default", "=", "\"qnli\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../TextFooler/target_models/mrpc\"", ",", "\n", "help", "=", "\"xxx classifier\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/qnli_freelb_vda.pt\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "16", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_warmup\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--variation\"", ",", "type", "=", "float", ",", "default", "=", "0.02", ")", "\n", "parser", ".", "add_argument", "(", "\"--variation_vda\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--step\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "data_path", "=", "'data/'", "+", "str", "(", "args", ".", "dataset", ")", "\n", "train_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'train.txt'", ")", ")", "\n", "dev_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'dev.txt'", ")", ")", "\n", "test_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'test.txt'", ")", ")", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "args", ".", "mlm_path", ")", ".", "cuda", "(", ")", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "train_data", "=", "EmbAdapterDataset", "(", "args", ",", "train_features", ",", "tokenizer_tgt", ")", "\n", "dev_data", "=", "EmbAdapterDataset", "(", "args", ",", "dev_features", ",", "tokenizer_tgt", ")", "\n", "test_data", "=", "EmbAdapterDataset", "(", "args", ",", "test_features", ",", "tokenizer_tgt", ")", "\n", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "dev_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "test_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "print", "(", "'start building model'", ")", "\n", "model", "=", "FreeLB", "(", "args", ",", "args", ".", "num_label", ")", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "params", "=", "model", ".", "parameters", "(", ")", "\n", "#need_grad = lambda x: x.requires_grad", "\n", "optimizer", "=", "AdamW", "(", "\n", "params", ",", "\n", "lr", "=", "args", ".", "lr", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.01", ",", "\n", ")", "\n", "total_num", "=", "len", "(", "train_data", ")", "//", "args", ".", "batch_size", "*", "args", ".", "max_epoch", "\n", "scheduler", "=", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "num_warmup", "*", "total_num", ",", "\n", "num_training_steps", "=", "total_num", ")", "\n", "best_ratio", "=", "0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "max_epoch", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "total_loss", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "train_dataloader", ")", ":", "\n", "            ", "model", ".", "zero_grad", "(", ")", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "#origin_emb = model.produce_emb(input_ids, attention_mask, token_type_ids)", "\n", "loss", ",", "kl_loss", "=", "model", ".", "adv_train", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "y", ",", "optimizer", ",", "scheduler", ",", "mlm_model", ")", "\n", "total_loss", ".", "append", "(", "[", "loss", ",", "kl_loss", "]", ")", "\n", "\n", "", "print", "(", "'Epoch %d, the training ce loss is %f, the kl loss is %f, the total number is %f'", "\n", "%", "(", "epoch", ",", "sum", "(", "[", "ele", "[", "0", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "\n", "sum", "(", "[", "ele", "[", "1", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "dev_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "if", "best_ratio", "<", "attack_ratio", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "tgt_model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", ")", "\n", "print", "(", "'--------save once-----------'", ")", "\n", "best_ratio", "=", "attack_ratio", "\n", "", "print", "(", "'The dev set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "test_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "print", "(", "'The test set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterDataset.__init__": [[16, 22], ["None"], "methods", ["None"], ["    ", "def", "__init__", "(", "self", ",", "args", ",", "data", ",", "tokenizer", ")", ":", "\n", "        ", "self", ".", "data", "=", "data", "\n", "self", ".", "tokenizer", "=", "tokenizer", "\n", "self", ".", "max_length", "=", "args", ".", "max_length", "\n", "self", ".", "pad_token", "=", "0", "\n", "self", ".", "mask_ids", "=", "103", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterDataset.__getitem__": [[23, 32], ["text_pair_classifier.EmbAdapterDataset.tokenizer.encode_plus", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "len", "len"], "methods", ["None"], ["", "def", "__getitem__", "(", "self", ",", "index", ")", ":", "\n", "        ", "x1", ",", "x2", ",", "y", "=", "self", ".", "data", "[", "index", "]", "\n", "inputs", "=", "self", ".", "tokenizer", ".", "encode_plus", "(", "x1", ",", "x2", ",", "add_special_tokens", "=", "True", ",", "max_length", "=", "self", ".", "max_length", ",", "\n", "truncation", "=", "True", ")", "\n", "padding_length", "=", "self", ".", "max_length", "-", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "\n", "input_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"input_ids\"", "]", "+", "padding_length", "*", "[", "self", ".", "pad_token", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "token_type_ids", "=", "torch", ".", "tensor", "(", "inputs", "[", "\"token_type_ids\"", "]", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "attention_mask", "=", "torch", ".", "tensor", "(", "[", "1", "]", "*", "len", "(", "inputs", "[", "\"input_ids\"", "]", ")", "+", "padding_length", "*", "[", "0", "]", ",", "dtype", "=", "torch", ".", "long", ")", "\n", "return", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterDataset.__len__": [[33, 35], ["len"], "methods", ["None"], ["", "def", "__len__", "(", "self", ")", ":", "\n", "        ", "return", "len", "(", "self", ".", "data", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__": [[37, 46], ["torch.Module.__init__", "transformers.BertConfig.from_pretrained", "transformers.BertForSequenceClassification.from_pretrained"], "methods", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.__init__"], ["    ", "def", "__init__", "(", "self", ",", "args", ")", ":", "\n", "        ", "super", "(", "EmbAdapterModel", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "masked_id", "=", "103", "\n", "self", ".", "pad_id", "=", "0", "\n", "self", ".", "variance", "=", "args", ".", "variance", "\n", "config_atk", "=", "BertConfig", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "num_labels", "=", "args", ".", "num_label", ")", "\n", "#config_atk.attention_probs_dropout_prob=0", "\n", "#config_atk.hidden_dropout_prob=0", "\n", "self", ".", "tgt_model", "=", "BertForSequenceClassification", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "config", "=", "config_atk", ")", "\n", "#save_to_original_BERT(self.tgt_model, args.tgt_path)", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.forward": [[49, 51], ["text_pair_classifier.EmbAdapterModel.tgt_model"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", ":", "\n", "        ", "return", "self", ".", "tgt_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.produce_emb": [[52, 60], ["text_pair_classifier.EmbAdapterModel.tgt_model.bert.embeddings"], "methods", ["None"], ["", "def", "produce_emb", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", ":", "\n", "        ", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "None", ",", "\n", ")", "\n", "return", "embedding_output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.defense": [[61, 111], ["input_ids.size", "text_pair_classifier.EmbAdapterModel.tgt_model.bert.get_extended_attention_mask", "text_pair_classifier.EmbAdapterModel.tgt_model.bert.get_head_mask", "torch.softmax", "torch.softmax", "torch.softmax", "torch.softmax", "torch.matmul", "torch.matmul", "torch.matmul", "torch.matmul", "text_pair_classifier.EmbAdapterModel.tgt_model.bert.embeddings", "text_pair_classifier.EmbAdapterModel.tgt_model.bert.encoder", "text_pair_classifier.EmbAdapterModel.tgt_model.classifier", "mlm_model", "torch.sum", "torch.sum", "torch.sum", "torch.sum", "torch.randn", "torch.randn", "torch.randn", "torch.randn", "text_pair_classifier.EmbAdapterModel.tgt_model.bert.pooler", "torch.softmax.size", "torch.softmax.size"], "methods", ["None"], ["", "def", "defense", "(", "self", ",", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "mlm_model", ")", ":", "\n", "        ", "output_attentions", "=", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_attentions", "\n", "output_hidden_states", "=", "(", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "output_hidden_states", ")", "\n", "\n", "input_shape", "=", "input_ids", ".", "size", "(", ")", "\n", "\n", "device", "=", "input_ids", ".", "device", "\n", "\n", "# We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]", "\n", "# ourselves in which case we just need to make it broadcastable to all heads.", "\n", "extended_attention_mask", ":", "torch", ".", "Tensor", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_extended_attention_mask", "(", "attention_mask", ",", "input_shape", ",", "device", ")", "\n", "# Prepare head mask if needed", "\n", "# 1.0 in head_mask indicate we keep the head", "\n", "# attention_probs has shape bsz x n_heads x N x N", "\n", "# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]", "\n", "# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]", "\n", "head_mask", "=", "self", ".", "tgt_model", ".", "bert", ".", "get_head_mask", "(", "None", ",", "self", ".", "tgt_model", ".", "bert", ".", "config", ".", "num_hidden_layers", ")", "\n", "\n", "probs", "=", "mlm_model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "\n", "probs", "=", "probs", "/", "torch", ".", "sum", "(", "probs", ",", "-", "1", ",", "keepdim", "=", "True", ")", "\n", "noise", "=", "torch", ".", "randn", "(", "probs", ".", "size", "(", ")", ",", "device", "=", "device", ")", "*", "self", ".", "variance", "\n", "\n", "probs", "=", "torch", ".", "softmax", "(", "probs", "+", "noise", ",", "-", "1", ")", "#[B, Len, V]", "\n", "\n", "word_embs", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", ".", "word_embeddings", ".", "weight", "\n", "input_embeds", "=", "torch", ".", "matmul", "(", "probs", ",", "word_embs", ")", "\n", "\n", "embedding_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "embeddings", "(", "\n", "input_ids", "=", "input_ids", ",", "\n", "position_ids", "=", "None", ",", "\n", "token_type_ids", "=", "token_type_ids", ",", "\n", "inputs_embeds", "=", "input_embeds", ",", "\n", ")", "\n", "encoder_outputs", "=", "self", ".", "tgt_model", ".", "bert", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "attention_mask", "=", "extended_attention_mask", ",", "\n", "head_mask", "=", "head_mask", ",", "\n", "encoder_hidden_states", "=", "None", ",", "\n", "encoder_attention_mask", "=", "None", ",", "\n", "output_attentions", "=", "output_attentions", ",", "\n", "output_hidden_states", "=", "output_hidden_states", ",", "\n", ")", "\n", "sequence_output", "=", "encoder_outputs", "[", "0", "]", "\n", "\n", "pooled_output", "=", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "(", "sequence_output", ")", "if", "self", ".", "tgt_model", ".", "bert", ".", "pooler", "is", "not", "None", "else", "None", "\n", "\n", "#logits = self.output_linear(prompt_output[0])", "\n", "logits", "=", "self", ".", "tgt_model", ".", "classifier", "(", "pooled_output", ")", "\n", "\n", "return", "logits", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data": [[112, 121], ["open", "line.strip().split", "int", "output.append", "line.strip"], "function", ["None"], ["", "", "def", "reader_data", "(", "filename", ",", "add", "=", "True", ")", ":", "\n", "#return [[ori_text, adv_text, label], ... ,[]]", "\n", "    ", "f", "=", "open", "(", "filename", ",", "encoding", "=", "'utf-8'", ")", "\n", "output", "=", "[", "]", "\n", "for", "line", "in", "f", ":", "\n", "        ", "label", ",", "text1", ",", "text2", "=", "line", ".", "strip", "(", ")", ".", "split", "(", "'\\t'", ")", "\n", "label", "=", "int", "(", "label", ")", "\n", "output", ".", "append", "(", "[", "text1", ",", "text2", ",", "label", "]", ")", "\n", "", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.run": [[123, 233], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "text_pair_classifier.reader_data", "text_pair_classifier.reader_data", "text_pair_classifier.reader_data", "print", "transformers.BertTokenizer.from_pretrained", "text_pair_classifier.EmbAdapterDataset", "text_pair_classifier.EmbAdapterDataset", "text_pair_classifier.EmbAdapterDataset", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "torch.utils.data.DataLoader", "print", "transformers.BertForMaskedLM.from_pretrained().cuda", "text_pair_classifier.EmbAdapterModel", "model.cuda.cuda", "torch.CrossEntropyLoss", "torch.KLDivLoss", "model.cuda.parameters", "transformers.AdamW", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup", "range", "str", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "os.path.join", "model.cuda.train", "tqdm.tqdm", "print", "model.cuda.eval", "print", "print", "transformers.BertForMaskedLM.from_pretrained", "len", "model.cuda.zero_grad", "model.cuda.defense", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "nn.CrossEntropyLoss.", "total_loss.append", "transformers.AdamW.step", "transformers.get_cosine_with_hard_restarts_schedule_with_warmup.step", "torch.no_grad", "torch.no_grad", "torch.save", "torch.save", "print", "torch.no_grad", "torch.no_grad", "ele.cuda", "model.cuda.", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "model.cuda.tgt_model.state_dict", "torch.argmax", "torch.argmax", "torch.sum().cpu().item", "torch.sum().cpu().item", "input_ids.size", "torch.sum().cpu", "torch.sum().cpu", "nn.KLDivLoss.", "nn.KLDivLoss.", "criterion.cpu().item", "kl_loss.cpu().item", "ele.cuda", "model.cuda.", "ele.cuda", "model.cuda.", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "torch.log_softmax", "torch.log_softmax", "torch.softmax", "torch.softmax", "sum", "len", "sum", "len", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum().cpu", "torch.sum", "torch.sum", "criterion.cpu", "kl_loss.cpu", "torch.sum", "torch.sum", "torch.sum", "torch.sum"], "function", ["home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.reader_data", "home.repos.pwc.inspect_result.rucaibox_vda.None.text_pair_classifier.EmbAdapterModel.defense"], ["", "def", "run", "(", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", ")", "\n", "parser", ".", "add_argument", "(", "\"--dataset\"", ",", "type", "=", "str", ",", "default", "=", "\"qnli\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--mlm_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../bert_file\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--tgt_path\"", ",", "type", "=", "str", ",", "default", "=", "\"../TextFooler/target_models/mrpc\"", ",", "\n", "help", "=", "\"xxx classifier\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--save_path\"", ",", "type", "=", "str", ",", "default", "=", "\"saved/qnli_vda.pt\"", ",", "help", "=", "\"xxx mlm\"", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_length\"", ",", "type", "=", "int", ",", "default", "=", "256", ")", "\n", "parser", ".", "add_argument", "(", "\"--max_epoch\"", ",", "type", "=", "int", ",", "default", "=", "10", ")", "\n", "parser", ".", "add_argument", "(", "\"--batch_size\"", ",", "type", "=", "int", ",", "default", "=", "20", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_label\"", ",", "type", "=", "int", ",", "default", "=", "2", ")", "\n", "parser", ".", "add_argument", "(", "\"--lr\"", ",", "type", "=", "float", ",", "default", "=", "1e-5", ")", "\n", "parser", ".", "add_argument", "(", "\"--num_warmup\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "parser", ".", "add_argument", "(", "\"--variance\"", ",", "type", "=", "float", ",", "default", "=", "0.05", ")", "\n", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "data_path", "=", "'data/'", "+", "str", "(", "args", ".", "dataset", ")", "\n", "train_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'train.txt'", ")", ")", "\n", "dev_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'dev.txt'", ")", ")", "\n", "test_features", "=", "reader_data", "(", "os", ".", "path", ".", "join", "(", "data_path", ",", "'test.txt'", ")", ")", "\n", "\n", "num_label", "=", "args", ".", "num_label", "\n", "\n", "print", "(", "'start process'", ")", "\n", "\n", "# tokenizer_mlm = BertTokenizer.from_pretrained(mlm_path, do_lower_case=True)", "\n", "tokenizer_tgt", "=", "BertTokenizer", ".", "from_pretrained", "(", "args", ".", "mlm_path", ",", "do_lower_case", "=", "True", ")", "\n", "\n", "train_data", "=", "EmbAdapterDataset", "(", "args", ",", "train_features", ",", "tokenizer_tgt", ")", "\n", "dev_data", "=", "EmbAdapterDataset", "(", "args", ",", "dev_features", ",", "tokenizer_tgt", ")", "\n", "test_data", "=", "EmbAdapterDataset", "(", "args", ",", "test_features", ",", "tokenizer_tgt", ")", "\n", "\n", "train_dataloader", "=", "DataLoader", "(", "train_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "dev_dataloader", "=", "DataLoader", "(", "dev_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "test_dataloader", "=", "DataLoader", "(", "test_data", ",", "batch_size", "=", "args", ".", "batch_size", ")", "\n", "\n", "print", "(", "'start building model'", ")", "\n", "mlm_model", "=", "BertForMaskedLM", ".", "from_pretrained", "(", "args", ".", "mlm_path", ")", ".", "cuda", "(", ")", "\n", "model", "=", "EmbAdapterModel", "(", "args", ")", "\n", "model", "=", "model", ".", "cuda", "(", ")", "\n", "\n", "criterion", "=", "nn", ".", "CrossEntropyLoss", "(", ")", "\n", "kl_criterion", "=", "nn", ".", "KLDivLoss", "(", ")", "\n", "\n", "params", "=", "model", ".", "parameters", "(", ")", "\n", "#need_grad = lambda x: x.requires_grad", "\n", "optimizer", "=", "AdamW", "(", "\n", "params", ",", "\n", "lr", "=", "args", ".", "lr", ",", "eps", "=", "1e-8", ",", "weight_decay", "=", "0.01", ",", "\n", ")", "\n", "total_num", "=", "len", "(", "train_data", ")", "//", "args", ".", "batch_size", "*", "args", ".", "max_epoch", "\n", "scheduler", "=", "get_cosine_with_hard_restarts_schedule_with_warmup", "(", "optimizer", ",", "num_warmup_steps", "=", "args", ".", "num_warmup", "*", "total_num", ",", "\n", "num_training_steps", "=", "total_num", ")", "\n", "best_ratio", "=", "0", "\n", "for", "epoch", "in", "range", "(", "args", ".", "max_epoch", ")", ":", "\n", "        ", "model", ".", "train", "(", ")", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "total_loss", "=", "[", "]", "\n", "for", "batch", "in", "tqdm", "(", "train_dataloader", ")", ":", "\n", "            ", "model", ".", "zero_grad", "(", ")", "\n", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "#origin_emb = model.produce_emb(input_ids, attention_mask, token_type_ids)", "\n", "probs", "=", "model", ".", "defense", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ",", "mlm_model", ")", "# , token_type_ids)[0]", "\n", "ori_probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "\n", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "\n", "kl_loss", "=", "0.5", "*", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "probs", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "ori_probs", ",", "-", "1", ")", ")", "+", "0.5", "*", "kl_criterion", "(", "torch", ".", "log_softmax", "(", "ori_probs", ",", "-", "1", ")", ",", "torch", ".", "softmax", "(", "probs", ",", "-", "1", ")", ")", "\n", "\n", "loss", "=", "criterion", "(", "ori_probs", ",", "y", ")", "\n", "total_loss", ".", "append", "(", "[", "loss", ".", "cpu", "(", ")", ".", "item", "(", ")", ",", "kl_loss", ".", "cpu", "(", ")", ".", "item", "(", ")", "]", ")", "\n", "(", "loss", "+", "kl_loss", ")", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "scheduler", ".", "step", "(", ")", "\n", "", "print", "(", "'Epoch %d, the training ce loss is %f, kl loss %f, success attack ratio is %f, the total number is %f'", "\n", "%", "(", "epoch", ",", "sum", "(", "[", "ele", "[", "0", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "sum", "(", "[", "ele", "[", "1", "]", "for", "ele", "in", "total_loss", "]", ")", "/", "len", "(", "total_loss", ")", ",", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "model", ".", "eval", "(", ")", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "dev_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "if", "best_ratio", "<", "attack_ratio", ":", "\n", "            ", "torch", ".", "save", "(", "model", ".", "tgt_model", ".", "state_dict", "(", ")", ",", "args", ".", "save_path", ")", "\n", "print", "(", "'--------save once-----------'", ")", "\n", "best_ratio", "=", "attack_ratio", "\n", "", "print", "(", "'The dev set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n", "attack_ratio", "=", "0", "\n", "total_num", "=", "0", "\n", "with", "torch", ".", "no_grad", "(", ")", ":", "\n", "            ", "for", "batch", "in", "test_dataloader", ":", "\n", "                ", "input_ids", ",", "token_type_ids", ",", "attention_mask", ",", "y", "=", "[", "ele", ".", "cuda", "(", ")", "for", "ele", "in", "batch", "]", "\n", "probs", "=", "model", "(", "input_ids", ",", "attention_mask", ",", "token_type_ids", ")", "[", "0", "]", "# , token_type_ids)[0]", "\n", "argmax_probs", "=", "torch", ".", "argmax", "(", "probs", ",", "dim", "=", "-", "1", ")", "\n", "success_num", "=", "torch", ".", "sum", "(", "(", "argmax_probs", "==", "y", ")", ".", "float", "(", ")", ")", ".", "cpu", "(", ")", ".", "item", "(", ")", "\n", "attack_ratio", "+=", "success_num", "\n", "total_num", "+=", "input_ids", ".", "size", "(", "0", ")", "\n", "", "", "print", "(", "'The test set defense success attack ratio is %f, the total number is %f'", "%", "(", "attack_ratio", "/", "total_num", ",", "total_num", ")", ")", "\n", "\n"]]}