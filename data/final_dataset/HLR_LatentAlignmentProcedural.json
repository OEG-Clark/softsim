{"home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.parse_arguments": [[30, 81], ["argparse.ArgumentParser", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.add_argument", "argparse.ArgumentParser.parse_args", "print", "print"], "function", ["None"], ["def", "parse_arguments", "(", "mode", "=", "\"train\"", ",", "number", "=", "200", ",", "_set", "=", "\"train\"", ",", "load", "=", "False", ",", "iteration", "=", "1", ",", "cuda", "=", "0", ",", "path", "=", "\"saves/\"", ",", "log", "=", "\"saves/log.txt\"", ",", "architecture", "=", "1", ",", "embedding_type", "=", "1", ",", "loss_mode", "=", "\"all\"", ",", "learning_rate", "=", "0.1", ",", "score_mode", "=", "\"max\"", ",", "max_pool", "=", "True", ")", ":", "\n", "    ", "parser", "=", "argparse", ".", "ArgumentParser", "(", "description", "=", "'Getting the arguments passed'", ")", "\n", "parser", ".", "add_argument", "(", "'-m'", ",", "'--mode'", ",", "help", "=", "'The mode of program'", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-n'", ",", "'--number'", ",", "help", "=", "'Number of examples'", ",", "type", "=", "int", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-i'", ",", "'--iteration'", ",", "help", "=", "'Number of iterations'", ",", "type", "=", "int", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-s'", ",", "'--set'", ",", "help", "=", "'Working on which set'", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-l'", ",", "'--load'", ",", "help", "=", "'Load or not'", ",", "type", "=", "bool", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-c'", ",", "'--cuda'", ",", "help", "=", "'Cuda option'", ",", "type", "=", "int", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-p'", ",", "'--path'", ",", "help", "=", "'Save and Load path'", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-f'", ",", "'--file'", ",", "help", "=", "'Log file name'", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-a'", ",", "'--architecture'", ",", "help", "=", "'Specify Architecture'", ",", "type", "=", "int", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-e'", ",", "'--embedding'", ",", "help", "=", "'Embedding'", ",", "type", "=", "int", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-o'", ",", "'--loss'", ",", "help", "=", "'Loss mode'", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-r'", ",", "'--rate'", ",", "help", "=", "'Learning rate'", ",", "type", "=", "float", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-y'", ",", "'--score'", ",", "help", "=", "'Aggregating Scores Mode'", ",", "type", "=", "str", ",", "required", "=", "False", ")", "\n", "parser", ".", "add_argument", "(", "'-x'", ",", "'--maxpool'", ",", "help", "=", "'Using customized maxpool'", ",", "type", "=", "bool", ",", "required", "=", "False", ",", "default", "=", "True", ")", "\n", "args", "=", "parser", ".", "parse_args", "(", ")", "\n", "print", "(", "args", ")", "\n", "if", "args", ".", "mode", "and", "args", ".", "mode", "in", "[", "\"train\"", ",", "\"test\"", "]", ":", "\n", "        ", "mode", "=", "args", ".", "mode", "\n", "", "if", "args", ".", "number", ":", "\n", "        ", "number", "=", "args", ".", "number", "\n", "", "if", "args", ".", "set", "and", "args", ".", "set", "in", "[", "\"test\"", ",", "\"train\"", ",", "\"valid\"", "]", ":", "\n", "        ", "_set", "=", "args", ".", "set", "\n", "", "if", "args", ".", "load", ":", "\n", "        ", "load", "=", "args", ".", "load", "\n", "", "if", "args", ".", "iteration", ":", "\n", "        ", "iteration", "=", "args", ".", "iteration", "\n", "", "if", "args", ".", "cuda", "in", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", ":", "\n", "        ", "cuda", "=", "args", ".", "cuda", "\n", "", "elif", "args", ".", "cuda", "and", "args", ".", "cuda", "==", "-", "1", ":", "\n", "        ", "cuda", "=", "-", "1", "\n", "", "if", "args", ".", "path", ":", "\n", "        ", "path", "=", "args", ".", "path", "\n", "", "if", "args", ".", "file", ":", "\n", "        ", "log", "=", "args", ".", "file", "\n", "", "if", "args", ".", "architecture", ":", "\n", "        ", "architecture", "=", "args", ".", "architecture", "\n", "", "if", "args", ".", "embedding", ":", "\n", "        ", "embedding_type", "=", "args", ".", "embedding", "\n", "", "if", "args", ".", "loss", "and", "args", ".", "loss", "in", "[", "\"random\"", ",", "\"all\"", ",", "\"one\"", "]", ":", "\n", "        ", "loss_mode", "=", "args", ".", "loss", "\n", "", "if", "args", ".", "rate", ":", "\n", "        ", "learning_rate", "=", "args", ".", "rate", "\n", "", "if", "args", ".", "maxpool", ":", "\n", "        ", "max_pool", "=", "args", ".", "maxpool", "\n", "", "if", "args", ".", "score", "and", "args", ".", "score", "in", "[", "\"max\"", ",", "\"mean\"", "]", ":", "\n", "        ", "score_mode", "=", "args", ".", "score", "\n", "", "print", "(", "mode", ",", "number", ",", "_set", ",", "load", ",", "iteration", ",", "cuda", ",", "path", ",", "log", ",", "architecture", ",", "embedding_type", ",", "loss_mode", ",", "learning_rate", ",", "score_mode", ",", "max_pool", ")", "\n", "\n", "return", "mode", ",", "number", ",", "_set", ",", "load", ",", "iteration", ",", "cuda", ",", "path", ",", "log", ",", "architecture", ",", "embedding_type", ",", "loss_mode", ",", "learning_rate", ",", "score_mode", ",", "max_pool", ",", "args", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.read_data": [[83, 103], ["json.loads", "print", "print", "print", "print", "print", "print", "print", "print", "print", "print", "open", "myfile.read", "len", "len", "len", "len", "len"], "function", ["None"], ["", "def", "read_data", "(", "file", "=", "\"train.json\"", ")", ":", "\n", "    ", "with", "open", "(", "file", ",", "'r'", ")", "as", "myfile", ":", "\n", "        ", "data", "=", "myfile", ".", "read", "(", ")", "\n", "# parse file", "\n", "", "info", "=", "json", ".", "loads", "(", "data", ")", "\n", "visual_coherence", "=", "[", "data", "for", "data", "in", "info", "[", "'data'", "]", "if", "data", "[", "'task'", "]", "==", "\"visual_coherence\"", "]", "\n", "textual_cloze", "=", "[", "data", "for", "data", "in", "info", "[", "'data'", "]", "if", "data", "[", "'task'", "]", "==", "\"textual_cloze\"", "]", "\n", "visual_ordering", "=", "[", "data", "for", "data", "in", "info", "[", "'data'", "]", "if", "data", "[", "'task'", "]", "==", "\"visual_ordering\"", "]", "\n", "visual_cloze", "=", "[", "data", "for", "data", "in", "info", "[", "'data'", "]", "if", "data", "[", "'task'", "]", "==", "\"visual_cloze\"", "]", "\n", "print", "(", "\"size of task textual_cloze\"", ")", "\n", "print", "(", "len", "(", "textual_cloze", ")", ")", "\n", "print", "(", "\"size of task visual_cloze\"", ")", "\n", "print", "(", "len", "(", "visual_cloze", ")", ")", "\n", "print", "(", "\"size of task visual_coherence\"", ")", "\n", "print", "(", "len", "(", "visual_coherence", ")", ")", "\n", "print", "(", "\"size of task visual_ordering\"", ")", "\n", "print", "(", "len", "(", "visual_ordering", ")", ")", "\n", "print", "(", "\"size of whole set\"", ")", "\n", "print", "(", "len", "(", "info", "[", "'data'", "]", ")", ")", "\n", "return", "info", ",", "visual_cloze", ",", "visual_coherence", ",", "visual_ordering", ",", "textual_cloze", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.prepare_data": [[104, 114], ["main.read_data", "main.read_data", "main.read_data"], "function", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.read_data", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.read_data", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.read_data"], ["", "def", "prepare_data", "(", "_set", "=", "\"train\"", ")", ":", "\n", "    ", "if", "_set", "==", "\"train\"", ":", "\n", "        ", "train", ",", "train_visual_cloze", ",", "train_visual_coherence", ",", "train_visual_ordering", ",", "train_textual_cloze", "=", "read_data", "(", "file", "=", "\"train.json\"", ")", "\n", "return", "train", ",", "train_visual_cloze", ",", "train_visual_coherence", ",", "train_visual_ordering", ",", "train_textual_cloze", "\n", "", "elif", "_set", "==", "\"valid\"", ":", "\n", "        ", "valid", ",", "val_visual_cloze", ",", "val_visual_coherence", ",", "val_visual_ordering", ",", "val_textual_cloze", "=", "read_data", "(", "file", "=", "\"val.json\"", ")", "\n", "return", "valid", ",", "val_visual_cloze", ",", "val_visual_coherence", ",", "val_visual_ordering", ",", "val_textual_cloze", "\n", "", "elif", "_set", "==", "\"test\"", ":", "\n", "        ", "test", ",", "test_visual_cloze", ",", "test_visual_coherence", ",", "test_visual_ordering", ",", "test_textual_cloze", "=", "read_data", "(", "file", "=", "\"test.json\"", ")", "\n", "return", "test", ",", "test_visual_cloze", ",", "test_visual_coherence", ",", "test_visual_ordering", ",", "test_textual_cloze", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.sentence_split": [[117, 126], ["nlp.annotate", "list", "list.append", "item.lower"], "function", ["None"], ["", "", "def", "sentence_split", "(", "text", ",", "properties", "=", "{", "'annotators'", ":", "'ssplit'", ",", "'outputFormat'", ":", "'json'", "}", ")", ":", "\n", "    ", "\"\"\"Split sentence using Stanford NLP\"\"\"", "\n", "annotated", "=", "nlp", ".", "annotate", "(", "text", ",", "properties", ")", "\n", "sentence_split", "=", "list", "(", ")", "\n", "for", "sentence", "in", "annotated", "[", "'sentences'", "]", ":", "\n", "        ", "s", "=", "[", "t", "[", "'word'", "]", "for", "t", "in", "sentence", "[", "'tokens'", "]", "]", "\n", "k", "=", "[", "item", ".", "lower", "(", ")", "for", "item", "in", "s", "if", "item", "not", "in", "[", "\",\"", ",", "\".\"", ",", "'...'", ",", "'..'", "]", "]", "\n", "sentence_split", ".", "append", "(", "\" \"", ".", "join", "(", "k", ")", ")", "\n", "", "return", "sentence_split", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.preprocess": [[127, 142], ["text.replace().replace.replace().replace", "main.sentence_split", "sentence.lower", "input_str.split.strip", "input_str.split.replace().replace().replace", "input_str.split.split", "str.maketrans", "results.append", "text.replace().replace.replace", "w.translate", "input_str.split.replace().replace", "input_str.split.replace"], "function", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.sentence_split"], ["", "def", "preprocess", "(", "text", ")", ":", "\n", "    ", "text", "=", "text", ".", "replace", "(", "\"\\'\\'\"", ",", "\"\"", ")", ".", "replace", "(", "\".\"", ",", "\". \"", ")", "\n", "sentences", "=", "sentence_split", "(", "text", ")", "\n", "results", "=", "[", "]", "\n", "for", "sentence", "in", "sentences", ":", "\n", "        ", "input_str", "=", "sentence", ".", "lower", "(", ")", "\n", "input_str", "=", "input_str", ".", "strip", "(", ")", "\n", "input_str", "=", "input_str", ".", "replace", "(", "\",\"", ",", "\" ,\"", ")", ".", "replace", "(", "\"-rrb-\"", ",", "\")\"", ")", ".", "replace", "(", "\"-lrb-\"", ",", "\"(\"", ")", "\n", "input_str", "=", "input_str", ".", "split", "(", ")", "\n", "punc", "=", "'!\"#$%&*+,/:;<=>?@[\\]^_`{|}~'", "\n", "table", "=", "str", ".", "maketrans", "(", "''", ",", "''", ",", "punc", ")", "\n", "stripped", "=", "[", "w", ".", "translate", "(", "table", ")", "for", "w", "in", "input_str", "]", "\n", "stripped", "=", "[", "w", "for", "w", "in", "stripped", "if", "w", "]", "\n", "results", ".", "append", "(", "\" \"", ".", "join", "(", "stripped", ")", ")", "\n", "", "return", "results", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.embedding": [[144, 150], ["flair.data.Sentence", "embedder.embed", "torch.stack", "torch.stack", "torch.stack", "torch.stack"], "function", ["None"], ["", "def", "embedding", "(", "text", ",", "embedder", ")", ":", "\n", "#    print(text)", "\n", "    ", "sentence", "=", "Sentence", "(", "text", ")", "\n", "embedder", ".", "embed", "(", "sentence", ")", "\n", "#    print(sentence)", "\n", "return", "torch", ".", "stack", "(", "[", "w", ".", "embedding", "for", "w", "in", "sentence", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.get_vector": [[152, 163], ["PIL.Image.open", "torch.autograd.Variable", "model", "model.reshape", "img.convert.convert", "normalize().unsqueeze", "normalize", "to_tensor", "scaler"], "function", ["None"], ["", "def", "get_vector", "(", "model", ",", "image_name", ")", ":", "\n", "# 1. Load the image with Pillow library", "\n", "    ", "img", "=", "Image", ".", "open", "(", "image_name", ")", "\n", "if", "img", ".", "mode", "!=", "\"RGB\"", ":", "\n", "        ", "img", "=", "img", ".", "convert", "(", "\"RGB\"", ")", "\n", "# 2. Create a PyTorch Variable with the transformed image", "\n", "", "t_img", "=", "Variable", "(", "normalize", "(", "to_tensor", "(", "scaler", "(", "img", ")", ")", ")", ".", "unsqueeze", "(", "0", ")", ")", "\n", "# 6. Run the model on our transformed image", "\n", "data", "=", "model", "(", "t_img", ")", "\n", "# 8. Return the feature vector", "\n", "return", "data", ".", "reshape", "(", "2048", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.prepare_language": [[165, 171], ["main.embedding", "data.to.unsqueeze", "data.to.to"], "function", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.embedding"], ["", "def", "prepare_language", "(", "text", ",", "embedder", ",", "cuda_option", ")", ":", "\n", "    ", "data", "=", "embedding", "(", "text", ",", "embedder", ")", "\n", "data", "=", "data", ".", "unsqueeze", "(", "0", ")", "\n", "if", "is_cuda", ":", "\n", "        ", "data", "=", "data", ".", "to", "(", "device", "=", "cuda_option", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.prepare_images": [[173, 182], ["torch.stack", "torch.stack", "torch.stack", "torch.stack", "data.to.unsqueeze", "data.to.append", "data.to.to", "main.get_vector"], "function", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.get_vector"], ["", "def", "prepare_images", "(", "images", ",", "cuda_option", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "for", "image", "in", "images", ":", "\n", "        ", "data", ".", "append", "(", "get_vector", "(", "resnet", ",", "image", ")", ")", "\n", "", "data", "=", "torch", ".", "stack", "(", "data", ")", "\n", "data", "=", "data", ".", "unsqueeze", "(", "0", ")", "\n", "if", "is_cuda", ":", "\n", "        ", "data", "=", "data", ".", "to", "(", "device", "=", "cuda_option", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.prepare_answer": [[184, 201], ["torch.stack", "torch.stack", "torch.stack", "torch.stack", "data.to.unsqueeze", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "data.to.append", "data.to.to", "main.embedding", "position.cuda.cuda"], "function", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.embedding"], ["", "def", "prepare_answer", "(", "texts", ",", "embedder", ",", "cuda_option", ")", ":", "\n", "    ", "data", "=", "[", "]", "\n", "for", "text", "in", "texts", ":", "\n", "        ", "embed", "=", "embedding", "(", "text", "[", "0", "]", ",", "embedder", ")", "[", "-", "1", "]", "\n", "position", "=", "torch", ".", "zeros", "(", "4", ")", "\n", "position", "[", "text", "[", "1", "]", "]", "=", "1", "\n", "if", "is_cuda", ":", "\n", "            ", "position", "=", "position", ".", "cuda", "(", "cuda_option", ")", "\n", "", "result", "=", "torch", ".", "cat", "(", "(", "embed", ",", "position", ")", ",", "0", ")", "\n", "#         result = result.squeeze(0)", "\n", "data", ".", "append", "(", "result", ")", "\n", "\n", "", "data", "=", "torch", ".", "stack", "(", "data", ")", "\n", "data", "=", "data", ".", "unsqueeze", "(", "0", ")", "\n", "if", "is_cuda", ":", "\n", "        ", "data", "=", "data", ".", "to", "(", "device", "=", "cuda_option", ")", "\n", "", "return", "data", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.execute": [[203, 467], ["tqdm.tqdm", "torch.SGD", "range", "open", "print", "open.write", "tqdm.tqdm", "print", "print", "open.write", "open.close", "list", "list", "range", "print", "print", "range", "torch.stack.append", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "answerTransformer", "print", "open.write", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "list", "list", "imageTransformer.parameters", "list", "list", "LSTM_Lang.parameters", "list", "list", "LSTM_Answer.zero_grad", "LSTM_Img.zero_grad", "LSTM_Lang.zero_grad", "contextTransformer.zero_grad", "answerTransformer.zero_grad", "imageTransformer.zero_grad", "range", "range", "answers.append", "len", "torch.stack.append", "range", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "torch.mm", "torch.mm", "torch.mm", "torch.mm", "torch.max", "torch.max", "torch.max", "torch.max", "results[].item", "range", "range", "print", "print", "print", "LSTM_Answer.state_dict", "LSTM_Img.state_dict", "LSTM_Lang.state_dict", "contextTransformer.state_dict", "answerTransformer.state_dict", "imageTransformer.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "answerTransformer.parameters", "contextTransformer.parameters", "LSTM_Answer.parameters", "LSTM_Img.parameters", "multicoder.parameters", "textTransformer.parameters", "str", "multicoder.zero_grad", "textTransformer.zero_grad", "len", "len", "question.append", "print", "print", "len", "main.preprocess", "torch.stack.append", "b_norm.transpose", "torch.mm.clone", "range", "indexes.sort", "torch.mm.max", "print_results_list.append", "numpy.argmax", "torch.tensor().topk", "torch.tensor().topk", "torch.tensor().topk", "torch.tensor().topk", "print", "open.close", "print", "multicoder.state_dict", "textTransformer.state_dict", "LSTM_Answer.state_dict", "LSTM_Img.state_dict", "LSTM_Lang.state_dict", "contextTransformer.state_dict", "answerTransformer.state_dict", "imageTransformer.state_dict", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "torch.save", "str", "im_tensor.append", "img_data.append", "img_data.append", "LSTM_Answer", "_list.insert", "print", "LSTM_Answer", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "contextTransformer", "answerTransformer.norm", "torch.stack.norm", "final_results.clone.max", "v.flatten().max", "indexes.append", "torch.arange().long", "torch.arange().long", "torch.arange().long", "torch.arange().long", "index1.append", "index2.append", "results[].item", "results[].item", "random.choice", "range", "loss.item", "loss.backward", "optim.SGD.step", "str", "multicoder.state_dict", "textTransformer.state_dict", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.from_numpy().to", "torch.stack", "torch.stack", "torch.stack", "torch.stack", "main.prepare_answer", "main.prepare_answer", "LSTM_Answer", "print", "print", "len", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "textTransformer", "len", "len", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "contextTransformer", "contextTransformer", "operator.itemgetter", "torch.tensor", "torch.tensor", "torch.tensor", "torch.tensor", "max", "max", "max", "str", "str", "str", "str", "str", "str", "main.prepare_answer", "main.prepare_language", "LSTM_Lang", "len", "multicoder", "multicoder", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "torch.zeros().cuda", "LSTM_Lang", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "torch.cat", "v.flatten", "i1.item", "i[].item", "torch.arange", "torch.arange", "torch.arange", "torch.arange", "str", "str", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "torch.from_numpy", "len", "textTransformer.append", "textTransformer.unsqueeze", "LSTM_Img", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "textTransformer.unsqueeze", "final_results.clone.size", "i[].item", "i1.item", "prepare_language.squeeze", "img_data[].unsqueeze", "textTransformer.unsqueeze", "img_data[].unsqueeze", "LSTM_Lang", "LSTM_Img", "textTransformer.unsqueeze", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "LSTM_Lang", "i1.item", "i1.item"], "function", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.preprocess", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.prepare_answer", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.prepare_answer", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.prepare_answer", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.prepare_language"], ["", "def", "execute", "(", "_m", ",", "_n", ",", "_s", ",", "_iteration", ",", "_d", ",", "base_image_path", ",", "log_file", ",", "cuda_option", ",", "save_path", ",", "loss_mode", ",", "learning_rate", ",", "score_mode", ",", "max_pool", ")", ":", "\n", "#set the parameters and optimizer for training", "\n", "    ", "if", "_m", "==", "\"train\"", ":", "\n", "        ", "params", "=", "list", "(", "answerTransformer", ".", "parameters", "(", ")", ")", "+", "list", "(", "contextTransformer", ".", "parameters", "(", ")", ")", "+", "list", "(", "imageTransformer", ".", "parameters", "(", ")", ")", "\n", "params", "+=", "list", "(", "LSTM_Answer", ".", "parameters", "(", ")", ")", "+", "list", "(", "LSTM_Img", ".", "parameters", "(", ")", ")", "+", "list", "(", "LSTM_Lang", ".", "parameters", "(", ")", ")", "\n", "if", "architecture", "==", "8", ":", "\n", "            ", "params", "+=", "list", "(", "multicoder", ".", "parameters", "(", ")", ")", "+", "list", "(", "textTransformer", ".", "parameters", "(", ")", ")", "\n", "\n", "", "optimizer", "=", "optim", ".", "SGD", "(", "params", ",", "lr", "=", "learning_rate", ",", "momentum", "=", "0.9", ")", "\n", "", "for", "it", "in", "tqdm", "(", "range", "(", "_iteration", ")", ")", ":", "\n", "        ", "logger", "=", "open", "(", "log_file", ",", "\"a+\"", ")", "\n", "print", "(", "\"-----------------\"", ",", "file", "=", "logger", ")", "\n", "logger", ".", "write", "(", "\"Start of the iteration \"", "+", "str", "(", "it", ")", "+", "\". \\n\"", ")", "\n", "total_loss", "=", "0", "\n", "number_true", "=", "0", "\n", "p2", "=", "0", "\n", "passed", "=", "0", "\n", "for", "ind", "in", "tqdm", "(", "range", "(", "0", ",", "_n", ")", ")", ":", "\n", "            ", "print", "(", "\"sample number: \"", ",", "ind", ",", "file", "=", "logger", ")", "\n", "sample", "=", "_d", "[", "ind", "]", "\n", "print", "(", "\"sample id: \"", ",", "sample", "[", "'recipe_id'", "]", ",", "file", "=", "logger", ")", "\n", "if", "_m", "==", "\"train\"", ":", "\n", "#zero grad all the learners", "\n", "                ", "LSTM_Answer", ".", "zero_grad", "(", ")", "\n", "LSTM_Img", ".", "zero_grad", "(", ")", "\n", "LSTM_Lang", ".", "zero_grad", "(", ")", "\n", "contextTransformer", ".", "zero_grad", "(", ")", "\n", "answerTransformer", ".", "zero_grad", "(", ")", "\n", "imageTransformer", ".", "zero_grad", "(", ")", "\n", "if", "architecture", "==", "8", ":", "\n", "                    ", "multicoder", ".", "zero_grad", "(", ")", "\n", "textTransformer", ".", "zero_grad", "(", ")", "\n", "\n", "#Add after having the images", "\n", "#prepare the images", "\n", "", "", "if", "architecture", "==", "7", "or", "architecture", "==", "8", ":", "\n", "                ", "images_list", "=", "[", "image", "[", "'images'", "]", "for", "image", "in", "sample", "[", "'context'", "]", "]", "\n", "img_data", "=", "[", "]", "\n", "for", "info", "in", "range", "(", "len", "(", "images_list", ")", ")", ":", "\n", "                    ", "im_tensor", "=", "[", "]", "\n", "check", "=", "False", "\n", "for", "item", "in", "images_list", "[", "info", "]", ":", "\n", "                        ", "_id", "=", "images_id", "[", "item", "]", "\n", "check", "=", "True", "\n", "im_tensor", ".", "append", "(", "torch", ".", "from_numpy", "(", "images_representation", "[", "_id", "]", ")", ".", "to", "(", "cuda_option", ")", ")", "\n", "", "if", "check", ":", "\n", "#                         print(\"here\")", "\n", "                        ", "img_data", ".", "append", "(", "torch", ".", "stack", "(", "im_tensor", ")", ")", "\n", "", "else", ":", "\n", "                        ", "img_data", ".", "append", "(", "[", "]", ")", "\n", "\n", "#prepare the instructions", "\n", "", "", "", "instructions", "=", "[", "text", "[", "'body'", "]", "for", "text", "in", "sample", "[", "'context'", "]", "]", "\n", "\n", "#prepare Question", "\n", "placeholder", "=", "0", "\n", "g_placeholder", "=", "0", "\n", "question", "=", "[", "]", "\n", "try", ":", "\n", "                ", "for", "q", "in", "range", "(", "len", "(", "sample", "[", "'question'", "]", ")", ")", ":", "\n", "                    ", "if", "sample", "[", "'question'", "]", "[", "q", "]", "==", "\"@placeholder\"", ":", "\n", "                        ", "placeholder", "=", "1", "\n", "g_placeholder", "=", "q", "\n", "continue", "\n", "", "question", ".", "append", "(", "[", "sample", "[", "'question'", "]", "[", "q", "]", ",", "q", "]", ")", "\n", "", "question_result", "=", "LSTM_Answer", "(", "prepare_answer", "(", "texts", "=", "question", ",", "embedder", "=", "selected_embedding", ",", "cuda_option", "=", "cuda_option", ")", ")", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "", "except", ":", "\n", "                ", "print", "(", "q", ")", "\n", "print", "(", "sample", "[", "'question'", "]", ")", "\n", "raise", "\n", "\n", "#prepare answers pairs", "\n", "", "answers", "=", "[", "]", "\n", "for", "item", "in", "sample", "[", "'choice_list'", "]", ":", "\n", "                ", "answers", ".", "append", "(", "[", "item", ",", "g_placeholder", "]", ")", "\n", "", "correct_answer", "=", "[", "sample", "[", "'choice_list'", "]", "[", "sample", "[", "'answer'", "]", "]", ",", "g_placeholder", "]", "\n", "del", "answers", "[", "sample", "[", "'answer'", "]", "]", "\n", "_list", "=", "[", "]", "\n", "for", "_it", "in", "range", "(", "len", "(", "answers", ")", ")", ":", "\n", "                ", "try", ":", "\n", "                    ", "if", "answers", "[", "_it", "]", "[", "0", "]", "==", "\"\"", "or", "answers", "[", "_it", "]", "[", "0", "]", "==", "\" \"", ":", "\n", "                        ", "_list", ".", "insert", "(", "0", ",", "_it", ")", "\n", "", "", "except", ":", "\n", "                    ", "print", "(", "_it", ")", "\n", "raise", "\n", "", "", "for", "item", "in", "_list", ":", "\n", "                ", "del", "answers", "[", "item", "]", "\n", "", "answers_results", "=", "[", "]", "\n", "answers_results", ".", "append", "(", "LSTM_Answer", "(", "prepare_answer", "(", "texts", "=", "[", "correct_answer", "]", ",", "embedder", "=", "selected_embedding", ",", "cuda_option", "=", "cuda_option", ")", ")", "[", "-", "1", "]", "[", "-", "1", "]", ")", "\n", "for", "answer", "in", "answers", ":", "\n", "                 ", "answers_results", ".", "append", "(", "LSTM_Answer", "(", "prepare_answer", "(", "texts", "=", "[", "answer", "]", ",", "embedder", "=", "selected_embedding", ",", "cuda_option", "=", "cuda_option", ")", ")", "[", "-", "1", "]", "[", "-", "1", "]", ")", "\n", "", "answers_results", "=", "torch", ".", "stack", "(", "answers_results", ")", "\n", "answer_results", "=", "answerTransformer", "(", "answers_results", ")", "\n", "\n", "results", "=", "[", "]", "\n", "try", ":", "\n", "                ", "for", "_it", "in", "range", "(", "len", "(", "instructions", ")", ")", ":", "\n", "                    ", "sentences", "=", "preprocess", "(", "instructions", "[", "_it", "]", ")", "\n", "#                     sentences_result = torch.zeros(2048).cuda(cuda_option)", "\n", "if", "architecture", "==", "7", "or", "architecture", "==", "8", "or", "architecture", "==", "9", ":", "\n", "                        ", "all_text", "=", "[", "]", "\n", "", "try", ":", "\n", "                        ", "for", "sentence", "in", "sentences", ":", "\n", "                            ", "if", "sentence", "!=", "\"\"", "and", "sentence", "!=", "\" \"", "and", "len", "(", "sentence", ")", ">", "3", ":", "\n", "                                ", "_input", "=", "prepare_language", "(", "text", "=", "sentence", ",", "embedder", "=", "selected_embedding", ",", "cuda_option", "=", "cuda_option", ")", "\n", "if", "architecture", "==", "7", "or", "architecture", "==", "8", "or", "architecture", "==", "9", ":", "\n", "                                    ", "all_text", ".", "append", "(", "_input", ".", "squeeze", "(", "0", ")", ")", "\n", "", "", "", "", "except", ":", "\n", "                        ", "print", "(", "sample", ")", "\n", "print", "(", "sentences", ")", "\n", "raise", "\n", "", "if", "architecture", "==", "7", ":", "\n", "                        ", "if", "not", "len", "(", "all_text", ")", ":", "\n", "                            ", "continue", "\n", "", "all_text", "=", "torch", ".", "cat", "(", "all_text", ",", "0", ")", "\n", "sentences_result", "=", "LSTM_Lang", "(", "all_text", ".", "unsqueeze", "(", "0", ")", ")", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "if", "len", "(", "img_data", "[", "_it", "]", ")", ":", "\n", "                            ", "image_result", "=", "LSTM_Img", "(", "img_data", "[", "_it", "]", ".", "unsqueeze", "(", "0", ")", ")", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                            ", "image_result", "=", "torch", ".", "zeros", "(", "2048", ")", ".", "cuda", "(", "cuda_option", ")", "\n", "", "", "elif", "architecture", "==", "8", ":", "\n", "                        ", "if", "not", "len", "(", "all_text", ")", ":", "\n", "                            ", "continue", "\n", "", "all_text", "=", "torch", ".", "cat", "(", "all_text", ",", "0", ")", "\n", "all_text", "=", "textTransformer", "(", "all_text", ")", "\n", "#Add after having the images", "\n", "if", "len", "(", "img_data", "[", "_it", "]", ")", ":", "\n", "#                         print(\"all text shape is: \", all_text.unsqueeze(0).shape)", "\n", "#                         print(\"images shape is: \", img_data[_it].unsqueeze(0).shape)", "\n", "                            ", "all_text", ",", "vision_input", "=", "multicoder", "(", "lang_feats", "=", "all_text", ".", "unsqueeze", "(", "0", ")", ",", "visn_feats", "=", "img_data", "[", "_it", "]", ".", "unsqueeze", "(", "0", ")", ",", "visn_attention_mask", "=", "None", ",", "lang_attention_mask", "=", "None", ")", "\n", "sentences_result", "=", "LSTM_Lang", "(", "all_text", ")", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "image_result", "=", "LSTM_Img", "(", "vision_input", ")", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "", "else", ":", "\n", "                            ", "all_text", ",", "vision_input", "=", "multicoder", "(", "lang_feats", "=", "all_text", ".", "unsqueeze", "(", "0", ")", ",", "visn_feats", "=", "None", ",", "visn_attention_mask", "=", "None", ",", "lang_attention_mask", "=", "None", ")", "\n", "image_result", "=", "torch", ".", "zeros", "(", "2048", ")", ".", "cuda", "(", "cuda_option", ")", "\n", "sentences_result", "=", "LSTM_Lang", "(", "all_text", ")", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "\n", "", "", "if", "architecture", "==", "9", ":", "\n", "                        ", "if", "not", "len", "(", "all_text", ")", ":", "\n", "                            ", "continue", "\n", "", "all_text", "=", "torch", ".", "cat", "(", "all_text", ",", "0", ")", "\n", "sentences_result", "=", "LSTM_Lang", "(", "all_text", ".", "unsqueeze", "(", "0", ")", ")", "[", "-", "1", "]", "[", "-", "1", "]", "\n", "value", "=", "contextTransformer", "(", "torch", ".", "cat", "(", "(", "sentences_result", ",", "question_result", ")", ")", ")", "\n", "", "elif", "architecture", "==", "8", "or", "architecture", "==", "7", ":", "\n", "                        ", "value", "=", "contextTransformer", "(", "torch", ".", "cat", "(", "(", "sentences_result", ",", "question_result", ",", "image_result", ")", ")", ")", "\n", "", "else", ":", "\n", "                        ", "value", "=", "contextTransformer", "(", "torch", ".", "cat", "(", "(", "sentences_result", ",", "question_result", ")", ")", ")", "\n", "", "results", ".", "append", "(", "value", ")", "\n", "\n", "", "results", "=", "torch", ".", "stack", "(", "results", ")", "\n", "a_norm", "=", "answer_results", "/", "answer_results", ".", "norm", "(", "dim", "=", "1", ")", "[", ":", ",", "None", "]", "\n", "b_norm", "=", "results", "/", "results", ".", "norm", "(", "dim", "=", "1", ")", "[", ":", ",", "None", "]", "\n", "final_results", "=", "torch", ".", "mm", "(", "a_norm", ",", "b_norm", ".", "transpose", "(", "0", ",", "1", ")", ")", "\n", "#                 print(\"final results is: \", final_results, file=logger)", "\n", "if", "max_pool", ":", "\n", "                    ", "r11", "=", "final_results", ".", "clone", "(", ")", "\n", "indexes", "=", "[", "]", "\n", "for", "i", "in", "range", "(", "4", ")", ":", "\n", "                        ", "v", ",", "i", "=", "r11", ".", "max", "(", "1", ")", "\n", "v1", ",", "i1", "=", "v", ".", "flatten", "(", ")", ".", "max", "(", "0", ")", "\n", "indexes", ".", "append", "(", "(", "i1", ".", "item", "(", ")", ",", "i", "[", "i1", ".", "item", "(", ")", "]", ".", "item", "(", ")", ")", ")", "\n", "j", "=", "torch", ".", "arange", "(", "r11", ".", "size", "(", "0", ")", ")", ".", "long", "(", ")", "\n", "r11", "[", "j", ",", "i", "[", "i1", ".", "item", "(", ")", "]", ".", "item", "(", ")", "]", "=", "-", "100000000000", "\n", "r11", "[", "i1", ".", "item", "(", ")", "]", "[", ":", "]", "=", "-", "100000000000", "\n", "", "indexes", ".", "sort", "(", "key", "=", "operator", ".", "itemgetter", "(", "0", ")", ")", "\n", "index0", "=", "indexes", "[", "0", "]", "[", "1", "]", "\n", "index1", "=", "[", "]", "\n", "index2", "=", "[", "]", "\n", "for", "item", "in", "indexes", ":", "\n", "                        ", "index1", ".", "append", "(", "item", "[", "0", "]", ")", "\n", "index2", ".", "append", "(", "item", "[", "1", "]", ")", "\n", "", "results", "=", "final_results", "[", "index1", ",", "index2", "]", "\n", "", "else", ":", "\n", "                    ", "results", ",", "indexes", "=", "final_results", ".", "max", "(", "1", ")", "\n", "index0", "=", "indexes", "[", "0", "]", "\n", "#                 print(\"the matching maxes are: \", results, file=logger)", "\n", "", "most", ",", "index_most", "=", "torch", ".", "max", "(", "results", ",", "0", ")", "\n", "print_results", "=", "{", "}", "\n", "print_results", "[", "sample", "[", "'answer'", "]", "]", "=", "results", "[", "0", "]", ".", "item", "(", ")", "\n", "for", "tt", "in", "range", "(", "4", ")", ":", "\n", "                    ", "if", "tt", "==", "sample", "[", "'answer'", "]", ":", "\n", "                        ", "continue", "\n", "", "if", "tt", "<", "sample", "[", "'answer'", "]", ":", "\n", "                        ", "print_results", "[", "tt", "]", "=", "results", "[", "tt", "+", "1", "]", ".", "item", "(", ")", "\n", "", "else", ":", "\n", "                        ", "print_results", "[", "tt", "]", "=", "results", "[", "tt", "]", ".", "item", "(", ")", "\n", "", "", "print_results_list", "=", "[", "]", "\n", "for", "tt", "in", "range", "(", "4", ")", ":", "\n", "                    ", "print_results_list", ".", "append", "(", "print_results", "[", "tt", "]", ")", "\n", "", "print", "(", "\"the matching result is: \"", ",", "print_results_list", ",", "file", "=", "logger", ")", "\n", "print", "(", "\"the predicted answer: \"", ",", "np", ".", "argmax", "(", "print_results_list", ")", ",", "file", "=", "logger", ")", "\n", "print", "(", "\"The answer is: \"", ",", "sample", "[", "'answer'", "]", ",", "file", "=", "logger", ")", "\n", "checking_p2", "=", "torch", ".", "tensor", "(", "print_results_list", ")", ".", "topk", "(", "2", ")", "[", "1", "]", "\n", "if", "sample", "[", "'answer'", "]", "in", "checking_p2", ":", "\n", "                    ", "p2", "+=", "1", "\n", "", "if", "index_most", "==", "0", ":", "\n", "                    ", "number_true", "+=", "1", "\n", "print", "(", "\"correct number: \"", ",", "number_true", ",", "file", "=", "logger", ")", "\n", "\n", "", "if", "_m", "==", "\"train\"", ":", "\n", "                    ", "if", "loss_mode", "==", "\"one\"", ":", "\n", "#                     loss = 1 - results[0]", "\n", "#                     ri = random.choice([1, 2, 3])", "\n", "#                     for ind in range(final_results[ri].shape[0]):", "\n", "#                         loss += max(0, final_results[ri][ind] - 0.2)", "\n", "                        ", "keys", "=", "[", "1", ",", "2", ",", "3", "]", "\n", "keys", "=", "[", "key", "for", "key", "in", "keys", "if", "key", "<", "final_results", ".", "shape", "[", "0", "]", "]", "\n", "ri", "=", "random", ".", "choice", "(", "keys", ")", "\n", "loss", "=", "0", "\n", "for", "key", "in", "keys", ":", "\n", "                            ", "loss", "+=", "max", "(", "0", ",", "final_results", "[", "key", "]", "[", "index0", "]", "-", "results", "[", "0", "]", "+", "0.1", ")", "\n", "", "for", "ind", "in", "range", "(", "final_results", "[", "ri", "]", ".", "shape", "[", "0", "]", ")", ":", "\n", "                            ", "loss", "+=", "max", "(", "0", ",", "results", "[", "ri", "]", "-", "results", "[", "0", "]", "+", "0.1", ")", "\n", "# print(loss)", "\n", "", "", "else", ":", "\n", "                        ", "loss", "=", "1", "-", "results", "[", "0", "]", "\n", "keys", "=", "[", "1", ",", "2", ",", "3", "]", "\n", "keys", "=", "[", "key", "for", "key", "in", "keys", "if", "key", "<", "final_results", ".", "shape", "[", "0", "]", "]", "\n", "for", "_it", "in", "keys", ":", "\n", "                            ", "loss", "+=", "max", "(", "0", ",", "results", "[", "_it", "]", "-", "0.1", ")", "\n", "\n", "#                     print(\"the loss of this item is: \", loss, file=logger)", "\n", "", "", "if", "loss", "!=", "0", ":", "\n", "                        ", "total_loss", "+=", "loss", ".", "item", "(", ")", "\n", "loss", ".", "backward", "(", ")", "\n", "optimizer", ".", "step", "(", ")", "\n", "", "", "", "except", "KeyboardInterrupt", ":", "\n", "                ", "logger", ".", "close", "(", ")", "\n", "raise", "\n", "", "except", ":", "\n", "                ", "raise", "\n", "print", "(", "\"GPU PASS\"", ")", "\n", "passed", "+=", "1", "\n", "\n", "", "", "if", "_m", "==", "\"train\"", ":", "\n", "            ", "print", "(", "\"total loss is\"", ",", "total_loss", ")", "\n", "logger", ".", "write", "(", "\"The loss of this iteration is \"", "+", "str", "(", "total_loss", ")", "+", "\". \\n\"", ")", "\n", "torch", ".", "save", "(", "LSTM_Answer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"ANSWERL\"", ")", "\n", "torch", ".", "save", "(", "LSTM_Img", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"IMGL\"", ")", "\n", "torch", ".", "save", "(", "LSTM_Lang", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"LANGL\"", ")", "\n", "torch", ".", "save", "(", "contextTransformer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"ContextT\"", ")", "\n", "torch", ".", "save", "(", "answerTransformer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"AnswerT\"", ")", "\n", "torch", ".", "save", "(", "imageTransformer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"ImgT\"", ")", "\n", "\n", "if", "architecture", "==", "8", ":", "\n", "                ", "torch", ".", "save", "(", "multicoder", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"MultiCoder\"", ")", "\n", "torch", ".", "save", "(", "textTransformer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"TextT\"", ")", "\n", "\n", "", "if", "it", "%", "3", "==", "0", ":", "\n", "                ", "torch", ".", "save", "(", "LSTM_Answer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"step\"", "+", "str", "(", "it", ")", "+", "\"_ANSWERL\"", ")", "\n", "torch", ".", "save", "(", "LSTM_Img", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"step\"", "+", "str", "(", "it", ")", "+", "\"_IMGL\"", ")", "\n", "torch", ".", "save", "(", "LSTM_Lang", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"step\"", "+", "str", "(", "it", ")", "+", "\"_LANGL\"", ")", "\n", "torch", ".", "save", "(", "contextTransformer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"step\"", "+", "str", "(", "it", ")", "+", "\"_ContextT\"", ")", "\n", "torch", ".", "save", "(", "answerTransformer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"step\"", "+", "str", "(", "it", ")", "+", "\"_AnswerT\"", ")", "\n", "torch", ".", "save", "(", "imageTransformer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"step\"", "+", "str", "(", "it", ")", "+", "\"_ImgT\"", ")", "\n", "\n", "if", "architecture", "==", "8", ":", "\n", "                    ", "torch", ".", "save", "(", "multicoder", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"step\"", "+", "str", "(", "it", ")", "+", "\"_MultiCoder\"", ")", "\n", "torch", ".", "save", "(", "textTransformer", ".", "state_dict", "(", ")", ",", "save_path", "+", "\"step\"", "+", "str", "(", "it", ")", "+", "\"_TextT\"", ")", "\n", "\n", "", "", "", "print", "(", "\"The ratio of being correct is: \"", ",", "number_true", "/", "(", "_n", "-", "passed", ")", ")", "\n", "print", "(", "\"The ratio of p2 correct is: \"", ",", "p2", "/", "(", "_n", "-", "passed", ")", ",", "file", "=", "logger", ")", "\n", "logger", ".", "write", "(", "\"The accuracy is \"", "+", "str", "(", "(", "number_true", "/", "(", "_n", "-", "passed", ")", ")", ")", "+", "\". \\n\"", ")", "\n", "logger", ".", "close", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.main": [[469, 541], ["str", "open", "open.write", "open.close", "open", "open.write", "open.close", "main.prepare_data", "main.execute", "open", "json.dump", "LSTM_Lang.cuda", "LSTM_Img.cuda", "LSTM_Answer.cuda", "contextTransformer.cuda", "answerTransformer.cuda", "imageTransformer.cuda", "torch.device", "torch.device", "torch.device", "torch.device", "LSTM_Lang.load_state_dict", "LSTM_Img.load_state_dict", "LSTM_Answer.load_state_dict", "contextTransformer.load_state_dict", "answerTransformer.load_state_dict", "imageTransformer.load_state_dict", "LSTM_Lang.train", "LSTM_Img.train", "LSTM_Answer.train", "contextTransformer.train", "answerTransformer.train", "imageTransformer.train", "multicoder.cuda", "textTransformer.cuda", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "multicoder.load_state_dict", "textTransformer.load_state_dict", "multicoder.train", "textTransformer.train", "LSTM_Lang.eval", "LSTM_Img.eval", "LSTM_Answer.eval", "contextTransformer.eval", "answerTransformer.eval", "imageTransformer.eval", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "torch.load", "multicoder.eval", "textTransformer.eval"], "function", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.prepare_data", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.None.main.execute"], ["", "", "def", "main", "(", "mode", ",", "number", ",", "_set", ",", "load", ",", "iteration", ",", "cuda_option", ",", "save_path", ",", "log_file", ",", "architecture", ",", "loss_mode", ",", "learning_rate", ",", "score_mode", ",", "max_pool", ",", "args", ")", ":", "\n", "#get the arguments", "\n", "#     mode, number, _set, load, iteration, cuda_option, save_path, log_file, architecture, args = parse_arguments()", "\n", "    ", "save_path", "=", "str", "(", "save_path", ")", "\n", "logger", "=", "open", "(", "log_file", ",", "\"a+\"", ")", "\n", "logger", ".", "write", "(", "\"\\n --------------- \\n Start of the model execution. \\n\"", ")", "\n", "logger", ".", "close", "(", ")", "\n", "with", "open", "(", "log_file", ",", "'a+'", ")", "as", "f", ":", "\n", "        ", "json", ".", "dump", "(", "args", ".", "__dict__", ",", "f", ",", "indent", "=", "2", ")", "\n", "", "logger", "=", "open", "(", "log_file", ",", "\"a+\"", ")", "\n", "logger", ".", "write", "(", "\"\\n start the training \\n\"", ")", "\n", "logger", ".", "close", "(", ")", "\n", "#prepare the data", "\n", "data", ",", "data_vcl", ",", "data_vc", ",", "data_vo", ",", "data_tc", "=", "prepare_data", "(", "_set", ")", "\n", "\n", "#Transfer to cuda", "\n", "if", "cuda_option", "in", "[", "0", ",", "1", ",", "2", ",", "3", ",", "4", ",", "5", ",", "6", "]", "and", "is_cuda", ":", "\n", "        ", "LSTM_Lang", ".", "cuda", "(", "cuda_option", ")", "\n", "LSTM_Img", ".", "cuda", "(", "cuda_option", ")", "\n", "LSTM_Answer", ".", "cuda", "(", "cuda_option", ")", "\n", "contextTransformer", ".", "cuda", "(", "cuda_option", ")", "\n", "answerTransformer", ".", "cuda", "(", "cuda_option", ")", "\n", "imageTransformer", ".", "cuda", "(", "cuda_option", ")", "\n", "flair", ".", "device", "=", "torch", ".", "device", "(", "cuda_option", ")", "\n", "if", "architecture", "==", "8", ":", "\n", "            ", "multicoder", ".", "cuda", "(", "cuda_option", ")", "\n", "textTransformer", ".", "cuda", "(", "cuda_option", ")", "\n", "\n", "#check for the loading parameters", "\n", "", "", "if", "load", ":", "\n", "        ", "LSTM_Lang", ".", "load_state_dict", "(", "torch", ".", "load", "(", "save_path", "+", "\"LANGL\"", ")", ")", "\n", "LSTM_Img", ".", "load_state_dict", "(", "torch", ".", "load", "(", "save_path", "+", "\"IMGL\"", ")", ")", "\n", "LSTM_Answer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "save_path", "+", "\"ANSWERL\"", ")", ")", "\n", "contextTransformer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "save_path", "+", "\"ContextT\"", ")", ")", "\n", "answerTransformer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "save_path", "+", "\"AnswerT\"", ")", ")", "\n", "imageTransformer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "save_path", "+", "\"ImgT\"", ")", ")", "\n", "if", "architecture", "==", "8", ":", "\n", "            ", "multicoder", ".", "load_state_dict", "(", "torch", ".", "load", "(", "save_path", "+", "\"MultiCoder\"", ")", ")", "\n", "textTransformer", ".", "load_state_dict", "(", "torch", ".", "load", "(", "save_path", "+", "\"TextT\"", ")", ")", "\n", "\n", "#set to training", "\n", "", "", "if", "mode", "==", "\"train\"", "and", "load", ":", "\n", "        ", "LSTM_Lang", ".", "train", "(", ")", "\n", "LSTM_Img", ".", "train", "(", ")", "\n", "LSTM_Answer", ".", "train", "(", ")", "\n", "contextTransformer", ".", "train", "(", ")", "\n", "answerTransformer", ".", "train", "(", ")", "\n", "imageTransformer", ".", "train", "(", ")", "\n", "if", "architecture", "==", "8", ":", "\n", "            ", "multicoder", ".", "train", "(", ")", "\n", "textTransformer", ".", "train", "(", ")", "\n", "#set to the testing", "\n", "", "", "elif", "mode", "==", "\"test\"", "and", "load", ":", "\n", "        ", "LSTM_Lang", ".", "eval", "(", ")", "\n", "LSTM_Img", ".", "eval", "(", ")", "\n", "LSTM_Answer", ".", "eval", "(", ")", "\n", "contextTransformer", ".", "eval", "(", ")", "\n", "answerTransformer", ".", "eval", "(", ")", "\n", "imageTransformer", ".", "eval", "(", ")", "\n", "if", "architecture", "==", "8", ":", "\n", "            ", "multicoder", ".", "eval", "(", ")", "\n", "textTransformer", ".", "eval", "(", ")", "\n", "\n", "#define the base address for images", "\n", "", "", "if", "_set", "==", "\"train\"", ":", "\n", "        ", "base_image_path", "=", "'images-qa/train/images-qa/'", "\n", "", "elif", "_set", "==", "\"test\"", ":", "\n", "        ", "base_image_path", "=", "'images-qa/test/images-qa/'", "\n", "", "elif", "_set", "==", "\"valid\"", ":", "\n", "        ", "base_image_path", "=", "'images-qa/val/images-qa/'", "\n", "\n", "", "execute", "(", "mode", ",", "number", ",", "_set", ",", "iteration", ",", "data_tc", ",", "base_image_path", ",", "log_file", ",", "cuda_option", ",", "save_path", ",", "loss_mode", ",", "learning_rate", ",", "score_mode", ",", "max_pool", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.single_transformer.SelfAttentionWide.__init__": [[8, 26], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "emb", ",", "heads", "=", "8", ",", "mask", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param emb:\n        :param heads:\n        :param mask:\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "emb", "=", "emb", "\n", "self", ".", "heads", "=", "heads", "\n", "self", ".", "mask", "=", "mask", "\n", "\n", "self", ".", "tokeys", "=", "nn", ".", "Linear", "(", "emb", ",", "emb", "*", "heads", ",", "bias", "=", "False", ")", "\n", "self", ".", "toqueries", "=", "nn", ".", "Linear", "(", "emb", ",", "emb", "*", "heads", ",", "bias", "=", "False", ")", "\n", "self", ".", "tovalues", "=", "nn", ".", "Linear", "(", "emb", ",", "emb", "*", "heads", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "unifyheads", "=", "nn", ".", "Linear", "(", "heads", "*", "emb", ",", "emb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.single_transformer.SelfAttentionWide.forward": [[27, 67], ["x.size", "single_transformer.SelfAttentionWide.tokeys().view", "single_transformer.SelfAttentionWide.toqueries().view", "single_transformer.SelfAttentionWide.tovalues().view", "keys.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "queries.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "values.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.softmax", "torch.softmax", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "out.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "single_transformer.SelfAttentionWide.unifyheads", "keys.transpose().contiguous().view.transpose().contiguous().view.transpose", "torch.softmax.size", "util.mask_", "single_transformer.SelfAttentionWide.tokeys", "single_transformer.SelfAttentionWide.toqueries", "single_transformer.SelfAttentionWide.tovalues", "keys.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "queries.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "values.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "out.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "float", "keys.transpose().contiguous().view.transpose().contiguous().view.transpose", "queries.transpose().contiguous().view.transpose().contiguous().view.transpose", "values.transpose().contiguous().view.transpose().contiguous().view.transpose", "out.transpose().contiguous().view.transpose().contiguous().view.transpose"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.util.mask_"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "b", ",", "t", ",", "e", "=", "x", ".", "size", "(", ")", "\n", "h", "=", "self", ".", "heads", "\n", "assert", "e", "==", "self", ".", "emb", ",", "f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'", "\n", "\n", "keys", "=", "self", ".", "tokeys", "(", "x", ")", ".", "view", "(", "b", ",", "t", ",", "h", ",", "e", ")", "\n", "queries", "=", "self", ".", "toqueries", "(", "x", ")", ".", "view", "(", "b", ",", "t", ",", "h", ",", "e", ")", "\n", "values", "=", "self", ".", "tovalues", "(", "x", ")", ".", "view", "(", "b", ",", "t", ",", "h", ",", "e", ")", "\n", "\n", "# compute scaled dot-product self-attention", "\n", "\n", "# - fold heads into the batch dimension", "\n", "keys", "=", "keys", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "b", "*", "h", ",", "t", ",", "e", ")", "\n", "queries", "=", "queries", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "b", "*", "h", ",", "t", ",", "e", ")", "\n", "values", "=", "values", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "b", "*", "h", ",", "t", ",", "e", ")", "\n", "\n", "queries", "=", "queries", "/", "(", "e", "**", "(", "1", "/", "4", ")", ")", "\n", "keys", "=", "keys", "/", "(", "e", "**", "(", "1", "/", "4", ")", ")", "\n", "# - Instead of dividing the dot products by sqrt(e), we scale the keys and values.", "\n", "#   This should be more memory efficient", "\n", "\n", "# - get dot product of queries and keys, and scale", "\n", "dot", "=", "torch", ".", "bmm", "(", "queries", ",", "keys", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "assert", "dot", ".", "size", "(", ")", "==", "(", "b", "*", "h", ",", "t", ",", "t", ")", "\n", "\n", "if", "self", ".", "mask", ":", "# mask out the upper half of the dot matrix, excluding the diagonal", "\n", "            ", "mask_", "(", "dot", ",", "maskval", "=", "float", "(", "'-inf'", ")", ",", "mask_diagonal", "=", "False", ")", "\n", "\n", "", "dot", "=", "F", ".", "softmax", "(", "dot", ",", "dim", "=", "2", ")", "\n", "# - dot now has row-wise self-attention probabilities", "\n", "\n", "# apply the self attention to the values", "\n", "out", "=", "torch", ".", "bmm", "(", "dot", ",", "values", ")", ".", "view", "(", "b", ",", "h", ",", "t", ",", "e", ")", "\n", "\n", "# swap h, t back, unify heads", "\n", "out", "=", "out", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "b", ",", "t", ",", "h", "*", "e", ")", "\n", "\n", "return", "self", ".", "unifyheads", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.single_transformer.SelfAttentionNarrow.__init__": [[70, 93], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "emb", ",", "heads", "=", "8", ",", "mask", "=", "False", ")", ":", "\n", "        ", "\"\"\"\n        :param emb:\n        :param heads:\n        :param mask:\n        \"\"\"", "\n", "\n", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "assert", "emb", "%", "heads", "==", "0", ",", "f'Embedding dimension ({emb}) should be divisible by nr. of heads ({heads})'", "\n", "\n", "self", ".", "emb", "=", "emb", "\n", "self", ".", "heads", "=", "heads", "\n", "self", ".", "mask", "=", "mask", "\n", "\n", "s", "=", "emb", "//", "heads", "\n", "# - We will break the embedding into `heads` chunks and feed each to a different attention head", "\n", "\n", "self", ".", "tokeys", "=", "nn", ".", "Linear", "(", "s", ",", "s", ",", "bias", "=", "False", ")", "\n", "self", ".", "toqueries", "=", "nn", ".", "Linear", "(", "s", ",", "s", ",", "bias", "=", "False", ")", "\n", "self", ".", "tovalues", "=", "nn", ".", "Linear", "(", "s", ",", "s", ",", "bias", "=", "False", ")", "\n", "\n", "self", ".", "unifyheads", "=", "nn", ".", "Linear", "(", "heads", "*", "s", ",", "emb", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.single_transformer.SelfAttentionNarrow.forward": [[94, 141], ["x.view.view.size", "x.view.view.view", "single_transformer.SelfAttentionNarrow.tokeys", "single_transformer.SelfAttentionNarrow.toqueries", "single_transformer.SelfAttentionNarrow.tovalues", "keys.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "queries.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "values.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "torch.softmax", "torch.softmax", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "torch.bmm().view", "out.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous().view", "single_transformer.SelfAttentionNarrow.unifyheads", "keys.transpose().contiguous().view.transpose().contiguous().view.size", "queries.transpose().contiguous().view.transpose().contiguous().view.size", "values.transpose().contiguous().view.transpose().contiguous().view.size", "keys.transpose().contiguous().view.transpose().contiguous().view.transpose", "torch.softmax.size", "util.mask_", "keys.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "queries.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "values.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "out.transpose().contiguous().view.transpose().contiguous().view.transpose().contiguous", "float", "keys.transpose().contiguous().view.transpose().contiguous().view.transpose", "queries.transpose().contiguous().view.transpose().contiguous().view.transpose", "values.transpose().contiguous().view.transpose().contiguous().view.transpose", "out.transpose().contiguous().view.transpose().contiguous().view.transpose"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.util.mask_"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "b", ",", "t", ",", "e", "=", "x", ".", "size", "(", ")", "\n", "h", "=", "self", ".", "heads", "\n", "assert", "e", "==", "self", ".", "emb", ",", "f'Input embedding dim ({e}) should match layer embedding dim ({self.emb})'", "\n", "\n", "s", "=", "e", "//", "h", "\n", "x", "=", "x", ".", "view", "(", "b", ",", "t", ",", "h", ",", "s", ")", "\n", "\n", "keys", "=", "self", ".", "tokeys", "(", "x", ")", "\n", "queries", "=", "self", ".", "toqueries", "(", "x", ")", "\n", "values", "=", "self", ".", "tovalues", "(", "x", ")", "\n", "\n", "assert", "keys", ".", "size", "(", ")", "==", "(", "b", ",", "t", ",", "h", ",", "s", ")", "\n", "assert", "queries", ".", "size", "(", ")", "==", "(", "b", ",", "t", ",", "h", ",", "s", ")", "\n", "assert", "values", ".", "size", "(", ")", "==", "(", "b", ",", "t", ",", "h", ",", "s", ")", "\n", "\n", "# Compute scaled dot-product self-attention", "\n", "\n", "# - fold heads into the batch dimension", "\n", "keys", "=", "keys", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "b", "*", "h", ",", "t", ",", "s", ")", "\n", "queries", "=", "queries", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "b", "*", "h", ",", "t", ",", "s", ")", "\n", "values", "=", "values", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "b", "*", "h", ",", "t", ",", "s", ")", "\n", "\n", "queries", "=", "queries", "/", "(", "e", "**", "(", "1", "/", "4", ")", ")", "\n", "keys", "=", "keys", "/", "(", "e", "**", "(", "1", "/", "4", ")", ")", "\n", "# - Instead of dividing the dot products by sqrt(e), we scale the keys and values.", "\n", "#   This should be more memory efficient", "\n", "\n", "# - get dot product of queries and keys, and scale", "\n", "dot", "=", "torch", ".", "bmm", "(", "queries", ",", "keys", ".", "transpose", "(", "1", ",", "2", ")", ")", "\n", "\n", "assert", "dot", ".", "size", "(", ")", "==", "(", "b", "*", "h", ",", "t", ",", "t", ")", "\n", "\n", "if", "self", ".", "mask", ":", "# mask out the upper half of the dot matrix, excluding the diagonal", "\n", "            ", "mask_", "(", "dot", ",", "maskval", "=", "float", "(", "'-inf'", ")", ",", "mask_diagonal", "=", "False", ")", "\n", "\n", "", "dot", "=", "F", ".", "softmax", "(", "dot", ",", "dim", "=", "2", ")", "\n", "# - dot now has row-wise self-attention probabilities", "\n", "\n", "# apply the self attention to the values", "\n", "out", "=", "torch", ".", "bmm", "(", "dot", ",", "values", ")", ".", "view", "(", "b", ",", "h", ",", "t", ",", "s", ")", "\n", "\n", "# swap h, t back, unify heads", "\n", "out", "=", "out", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ".", "view", "(", "b", ",", "t", ",", "s", "*", "h", ")", "\n", "\n", "return", "self", ".", "unifyheads", "(", "out", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.single_transformer.TransformerBlock.__init__": [[144, 161], ["torch.nn.Module.__init__", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.LayerNorm", "torch.nn.Sequential", "torch.nn.Sequential", "torch.nn.Dropout", "torch.nn.Dropout", "single_transformer.SelfAttentionWide", "single_transformer.SelfAttentionNarrow", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.ReLU", "torch.nn.ReLU", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "emb", ",", "heads", ",", "mask", "=", "False", ",", "ff_hidden_mult", "=", "4", ",", "dropout", "=", "0.0", ",", "wide", "=", "True", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "attention", "=", "SelfAttentionWide", "(", "emb", ",", "heads", "=", "heads", ",", "mask", "=", "mask", ")", "if", "wide", "else", "SelfAttentionNarrow", "(", "emb", ",", "heads", "=", "heads", ",", "mask", "=", "mask", ")", "\n", "self", ".", "mask", "=", "mask", "\n", "\n", "self", ".", "norm1", "=", "nn", ".", "LayerNorm", "(", "emb", ")", "\n", "self", ".", "norm2", "=", "nn", ".", "LayerNorm", "(", "emb", ")", "\n", "\n", "self", ".", "ff", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "emb", ",", "ff_hidden_mult", "*", "emb", ")", ",", "\n", "nn", ".", "ReLU", "(", ")", ",", "\n", "nn", ".", "Linear", "(", "ff_hidden_mult", "*", "emb", ",", "emb", ")", "\n", ")", "\n", "\n", "self", ".", "do", "=", "nn", ".", "Dropout", "(", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.single_transformer.TransformerBlock.forward": [[162, 177], ["single_transformer.TransformerBlock.attention", "single_transformer.TransformerBlock.norm1", "single_transformer.TransformerBlock.do", "single_transformer.TransformerBlock.ff", "single_transformer.TransformerBlock.norm2", "single_transformer.TransformerBlock.do"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "\n", "        ", "attended", "=", "self", ".", "attention", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "norm1", "(", "attended", "+", "x", ")", "\n", "\n", "x", "=", "self", ".", "do", "(", "x", ")", "\n", "\n", "fedforward", "=", "self", ".", "ff", "(", "x", ")", "\n", "\n", "x", "=", "self", ".", "norm2", "(", "fedforward", "+", "x", ")", "\n", "\n", "x", "=", "self", ".", "do", "(", "x", ")", "\n", "\n", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.single_transformer.AttentionWide.__init__": [[205, 218], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Softmax", "torch.nn.Softmax", "torch.nn.Tanh", "torch.nn.Tanh", "ValueError", "torch.nn.Linear", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["def", "__init__", "(", "self", ",", "dimensions", ",", "attention_type", "=", "'dot'", ")", ":", "\n", "        ", "super", "(", "AttentionWide", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "if", "attention_type", "not", "in", "[", "'dot'", ",", "'general'", "]", ":", "\n", "            ", "raise", "ValueError", "(", "'Invalid attention type selected.'", ")", "\n", "\n", "", "self", ".", "attention_type", "=", "attention_type", "\n", "if", "self", ".", "attention_type", "==", "'general'", ":", "\n", "            ", "self", ".", "linear_in", "=", "nn", ".", "Linear", "(", "dimensions", ",", "dimensions", ",", "bias", "=", "False", ")", "\n", "\n", "", "self", ".", "linear_out", "=", "nn", ".", "Linear", "(", "dimensions", "*", "2", ",", "dimensions", ",", "bias", "=", "False", ")", "\n", "self", ".", "softmax", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "\n", "self", ".", "tanh", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.single_transformer.AttentionWide.forward": [[219, 267], ["query.reshape.reshape.size", "context.size", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "attention_scores.view.view.view", "single_transformer.AttentionWide.softmax", "attention_weights.view.view.view", "torch.bmm", "torch.bmm", "torch.bmm", "torch.bmm", "query.reshape.reshape.reshape", "single_transformer.AttentionWide.linear_in", "query.reshape.reshape.reshape", "context.transpose().contiguous", "context.transpose"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "query", ",", "context", ")", ":", "\n", "        ", "\"\"\"\n        Args:\n            query (:class:`torch.FloatTensor` [batch size, output length, dimensions]): Sequence of\n                queries to query the context.\n            context (:class:`torch.FloatTensor` [batch size, query length, dimensions]): Data\n                overwhich to apply the attention mechanism.\n\n        Returns:\n            :class:`tuple` with `output` and `weights`:\n            * **output** (:class:`torch.LongTensor` [batch size, output length, dimensions]):\n              Tensor containing the attended features.\n            * **weights** (:class:`torch.FloatTensor` [batch size, output length, query length]):\n              Tensor containing attention weights.\n        \"\"\"", "\n", "batch_size", ",", "output_len", ",", "dimensions", "=", "query", ".", "size", "(", ")", "\n", "query_len", "=", "context", ".", "size", "(", "1", ")", "\n", "\n", "if", "self", ".", "attention_type", "==", "\"general\"", ":", "\n", "            ", "query", "=", "query", ".", "reshape", "(", "batch_size", "*", "output_len", ",", "dimensions", ")", "\n", "query", "=", "self", ".", "linear_in", "(", "query", ")", "\n", "query", "=", "query", ".", "reshape", "(", "batch_size", ",", "output_len", ",", "dimensions", ")", "\n", "\n", "# TODO: Include mask on PADDING_INDEX?", "\n", "\n", "# (batch_size, output_len, dimensions) * (batch_size, query_len, dimensions) ->", "\n", "# (batch_size, output_len, query_len)", "\n", "", "attention_scores", "=", "torch", ".", "bmm", "(", "query", ",", "context", ".", "transpose", "(", "1", ",", "2", ")", ".", "contiguous", "(", ")", ")", "\n", "\n", "# Compute weights across every context sequence", "\n", "attention_scores", "=", "attention_scores", ".", "view", "(", "batch_size", "*", "output_len", ",", "query_len", ")", "\n", "attention_weights", "=", "self", ".", "softmax", "(", "attention_scores", ")", "\n", "attention_weights", "=", "attention_weights", ".", "view", "(", "batch_size", ",", "output_len", ",", "query_len", ")", "\n", "\n", "# (batch_size, output_len, query_len) * (batch_size, query_len, dimensions) ->", "\n", "# (batch_size, output_len, dimensions)", "\n", "mix", "=", "torch", ".", "bmm", "(", "attention_weights", ",", "context", ")", "\n", "\n", "# concat -> (batch_size * output_len, 2*dimensions)", "\n", "#         combined = torch.cat((mix, query), dim=2)", "\n", "#         combined = combined.view(batch_size * output_len, 2 * dimensions)", "\n", "\n", "# Apply linear_out on every 2nd dimension of concat", "\n", "# output -> (batch_size, output_len, dimensions)", "\n", "#         output = self.linear_out(combined).view(batch_size, output_len, dimensions)", "\n", "#         output = self.tanh(output)", "\n", "\n", "return", "mix", ",", "attention_weights", "", "", "", ""]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.LSTMFlair.__init__": [[13, 23], ["torch.Module.__init__", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.cuda.is_available", "torch.LSTM", "torch.LSTM", "torch.LSTM"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "input_dim", ",", "hidden_dim", ",", "batch_size", ",", "num_layers", "=", "1", ",", "bidirectional", "=", "False", ")", ":", "\n", "        ", "super", "(", "LSTMFlair", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "input_dim", "=", "input_dim", "\n", "self", ".", "hidden_dim", "=", "hidden_dim", "\n", "self", ".", "batch_size", "=", "batch_size", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "self", ".", "bidirectional", "=", "bidirectional", "\n", "self", ".", "is_cuda", "=", "torch", ".", "cuda", ".", "is_available", "(", ")", "\n", "# Define the LSTM layer", "\n", "self", ".", "lstm", "=", "nn", ".", "LSTM", "(", "input_size", "=", "self", ".", "input_dim", ",", "hidden_size", "=", "self", ".", "hidden_dim", ",", "num_layers", "=", "self", ".", "num_layers", ",", "bidirectional", "=", "bidirectional", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.LSTMFlair.init_hidden": [[24, 31], ["torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros", "torch.zeros"], "methods", ["None"], ["", "def", "init_hidden", "(", "self", ")", ":", "\n", "# This is what we'll initialise our hidden state as", "\n", "        ", "i", "=", "1", "\n", "if", "self", ".", "bidirectional", ":", "\n", "            ", "i", "=", "2", "\n", "", "return", "(", "torch", ".", "zeros", "(", "self", ".", "num_layers", "*", "i", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_dim", ")", ",", "\n", "torch", ".", "zeros", "(", "self", ".", "num_layers", "*", "i", ",", "self", ".", "batch_size", ",", "self", ".", "hidden_dim", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.LSTMFlair.forward": [[32, 35], ["torchvision.LSTMFlair.lstm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input", ")", ":", "\n", "        ", "lstm_out", ",", "share_hidden", "=", "self", ".", "lstm", "(", "input", ")", "\n", "return", "lstm_out", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.FullyConnected.__init__": [[38, 47], ["torch.Module.__init__", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "layers_list.append", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dims", ",", "layers", "=", "1", ")", ":", "\n", "        ", "super", "(", "FullyConnected", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dims", "=", "dims", "\n", "\n", "# Define the fully connected layer", "\n", "layers_list", "=", "[", "]", "\n", "for", "layer", "in", "range", "(", "layers", ")", ":", "\n", "            ", "layers_list", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "dims", "[", "layer", "]", ",", "self", ".", "dims", "[", "layer", "+", "1", "]", ")", ")", "\n", "", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "layers_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.FullyConnected.forward": [[48, 55], ["range", "len", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "_input", ")", ":", "\n", "        ", "fc_result", "=", "_input", "\n", "for", "layer", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "            ", "fc_result", "=", "self", ".", "layers", "[", "layer", "]", "(", "fc_result", ")", "\n", "if", "layer", "!=", "len", "(", "self", ".", "layers", ")", "-", "1", ":", "\n", "                ", "fc_result", "=", "F", ".", "leaky_relu", "(", "fc_result", ")", "\n", "", "", "return", "fc_result", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.FCDecider.__init__": [[58, 67], ["torch.Module.__init__", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "layers_list.append", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dims", ",", "layers", "=", "1", ")", ":", "\n", "        ", "super", "(", "FCDecider", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dims", "=", "dims", "\n", "\n", "# Define the fully connected layer", "\n", "layers_list", "=", "[", "]", "\n", "for", "layer", "in", "range", "(", "layers", ")", ":", "\n", "            ", "layers_list", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "dims", "[", "layer", "]", ",", "self", ".", "dims", "[", "layer", "+", "1", "]", ")", ")", "\n", "", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "layers_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.FCDecider.forward": [[68, 76], ["range", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "len", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "_input", ")", ":", "\n", "        ", "fc_result", "=", "_input", "\n", "for", "layer", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "            ", "fc_result", "=", "self", ".", "layers", "[", "layer", "]", "(", "fc_result", ")", "\n", "if", "layer", "!=", "len", "(", "self", ".", "layers", ")", "-", "1", ":", "\n", "                ", "fc_result", "=", "F", ".", "leaky_relu", "(", "fc_result", ")", "\n", "", "", "fc_result", "=", "torch", ".", "sigmoid", "(", "fc_result", ")", "\n", "return", "fc_result", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.DropFullyConnected.__init__": [[79, 89], ["torch.Module.__init__", "range", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Dropout", "torch.Dropout", "torch.Dropout", "layers_list.append", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dims", ",", "layers", "=", "1", ",", "dropout", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "DropFullyConnected", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dims", "=", "dims", "\n", "\n", "# Define the fully connected layer", "\n", "layers_list", "=", "[", "]", "\n", "for", "layer", "in", "range", "(", "layers", ")", ":", "\n", "            ", "layers_list", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "dims", "[", "layer", "]", ",", "self", ".", "dims", "[", "layer", "+", "1", "]", ")", ")", "\n", "", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "layers_list", ")", "\n", "self", ".", "drop_layer", "=", "nn", ".", "Dropout", "(", "p", "=", "dropout", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.DropFullyConnected.forward": [[90, 98], ["range", "len", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "torchvision.DropFullyConnected.drop_layer", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "_input", ")", ":", "\n", "        ", "fc_result", "=", "_input", "\n", "for", "layer", "in", "range", "(", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "            ", "fc_result", "=", "self", ".", "layers", "[", "layer", "]", "(", "fc_result", ")", "\n", "if", "layer", "!=", "len", "(", "self", ".", "layers", ")", "-", "1", ":", "\n", "                ", "fc_result", "=", "F", ".", "leaky_relu", "(", "fc_result", ")", "\n", "fc_result", "=", "self", ".", "drop_layer", "(", "fc_result", ")", "\n", "", "", "return", "fc_result", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.ResidualFullyConnected.__init__": [[101, 111], ["torch.Module.__init__", "range", "torch.Dropout", "torch.Dropout", "torch.Dropout", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "layers_list.append", "torch.Linear", "torch.Linear", "torch.Linear"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "dims", ",", "layers", "=", "1", ",", "p", "=", "0.1", ")", ":", "\n", "        ", "super", "(", "ResidualFullyConnected", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dims", "=", "dims", "\n", "\n", "# Define the fully connected layer", "\n", "layers_list", "=", "[", "]", "\n", "for", "layer", "in", "range", "(", "layers", ")", ":", "\n", "            ", "layers_list", ".", "append", "(", "nn", ".", "Linear", "(", "self", ".", "dims", "[", "layer", "]", ",", "self", ".", "dims", "[", "layer", "+", "1", "]", ")", ")", "\n", "", "self", ".", "drop_layer", "=", "nn", ".", "Dropout", "(", "p", "=", "p", ")", "\n", "self", ".", "layers", "=", "nn", ".", "ModuleList", "(", "layers_list", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.ResidualFullyConnected.forward": [[112, 127], ["torchvision.ResidualFullyConnected.drop_layer", "range", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "len", "torch.leaky_relu", "torch.leaky_relu", "torch.leaky_relu", "len"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "_input", ")", ":", "\n", "        ", "fc_result", "=", "_input", "\n", "_input", "=", "self", ".", "drop_layer", "(", "_input", ")", "\n", "out", "=", "self", ".", "layers", "[", "0", "]", "(", "_input", ")", "\n", "if", "0", "!=", "len", "(", "self", ".", "layers", ")", "-", "1", ":", "\n", "            ", "out", "=", "F", ".", "leaky_relu", "(", "out", ")", "\n", "", "for", "layer", "in", "range", "(", "1", ",", "len", "(", "self", ".", "layers", ")", ")", ":", "\n", "            ", "fc_result", "=", "out", "\n", "if", "out", ".", "shape", "==", "_input", ".", "shape", ":", "\n", "                ", "out", "=", "self", ".", "layers", "[", "layer", "]", "(", "out", "+", "_input", ")", "\n", "", "else", ":", "\n", "                ", "out", "=", "self", ".", "layers", "[", "layer", "]", "(", "out", ")", "\n", "", "out", "=", "F", ".", "leaky_relu", "(", "out", ")", "\n", "_input", "=", "fc_result", "\n", "", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.Highway.__init__": [[130, 143], ["torch.Module.__init__", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.ModuleList", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "torch.Linear", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "num_layers", ",", "f", ")", ":", "\n", "\n", "        ", "super", "(", "Highway", ",", "self", ")", ".", "__init__", "(", ")", "\n", "\n", "self", ".", "num_layers", "=", "num_layers", "\n", "\n", "self", ".", "nonlinear", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "size", ",", "size", ")", "for", "_", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "\n", "self", ".", "linear", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "size", ",", "size", ")", "for", "_", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "\n", "self", ".", "gate", "=", "nn", ".", "ModuleList", "(", "[", "nn", ".", "Linear", "(", "size", ",", "size", ")", "for", "_", "in", "range", "(", "num_layers", ")", "]", ")", "\n", "\n", "self", ".", "f", "=", "f", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.Highway.forward": [[144, 162], ["range", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torch.sigmoid", "torchvision.Highway.f"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "\"\"\"\n            :param x: tensor with shape of [batch_size, size]\n            :return: tensor with shape of [batch_size, size]\n            applies \u03c3(x) \u2a00 (f(G(x))) + (1 - \u03c3(x)) \u2a00 (Q(x)) transformation | G and Q is affine transformation,\n            f is non-linear transformation, \u03c3(x) is affine transformation with sigmoid non-linearition\n            and \u2a00 is element-wise multiplication\n            \"\"\"", "\n", "\n", "for", "layer", "in", "range", "(", "self", ".", "num_layers", ")", ":", "\n", "            ", "gate", "=", "torch", ".", "sigmoid", "(", "self", ".", "gate", "[", "layer", "]", "(", "x", ")", ")", "\n", "\n", "nonlinear", "=", "self", ".", "f", "(", "self", ".", "nonlinear", "[", "layer", "]", "(", "x", ")", ")", "\n", "linear", "=", "self", ".", "linear", "[", "layer", "]", "(", "x", ")", "\n", "\n", "x", "=", "gate", "*", "nonlinear", "+", "(", "1", "-", "gate", ")", "*", "linear", "\n", "\n", "", "return", "x", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.HighwayResidualFC.__init__": [[165, 169], ["torch.Module.__init__", "torchvision.Highway", "torchvision.ResidualFullyConnected"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "num_layers", ",", "f", ",", "dims", ",", "layers", "=", "1", ")", ":", "\n", "        ", "super", "(", "HighwayResidualFC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hgw", "=", "Highway", "(", "size", ",", "num_layers", ",", "f", ")", "\n", "self", ".", "rfc", "=", "ResidualFullyConnected", "(", "dims", ",", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.HighwayResidualFC.forward": [[170, 174], ["torchvision.HighwayResidualFC.hgw", "torchvision.HighwayResidualFC.rfc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "_input", ")", ":", "\n", "        ", "out", "=", "self", ".", "hgw", "(", "_input", ")", "\n", "out", "=", "self", ".", "rfc", "(", "_input", ")", "\n", "return", "out", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.HighwayFC.__init__": [[177, 181], ["torch.Module.__init__", "torchvision.Highway", "torchvision.FullyConnected"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "size", ",", "num_layers", ",", "f", ",", "dims", ",", "layers", "=", "1", ")", ":", "\n", "        ", "super", "(", "HighwayFC", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "hgw", "=", "Highway", "(", "size", ",", "num_layers", ",", "f", ")", "\n", "self", ".", "rfc", "=", "FullyConnected", "(", "dims", ",", "layers", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.models.HighwayFC.forward": [[182, 186], ["torchvision.HighwayFC.hgw", "torchvision.HighwayFC.rfc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "_input", ")", ":", "\n", "        ", "out", "=", "self", ".", "hgw", "(", "_input", ")", "\n", "out", "=", "self", ".", "rfc", "(", "_input", ")", "\n", "return", "out", "\n", "", "", ""]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.GeLU.__init__": [[49, 51], ["torch.nn.Module.__init__"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["def", "__init__", "(", "self", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.GeLU.forward": [[52, 54], ["bertModel.gelu"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.gelu"], ["", "def", "forward", "(", "self", ",", "x", ")", ":", "\n", "        ", "return", "gelu", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertConfig.__init__": [[66, 121], ["isinstance", "json.loads.items", "isinstance", "isinstance", "io.open", "json.loads", "ValueError", "reader.read"], "methods", ["None"], ["def", "__init__", "(", "self", ",", "\n", "vocab_size_or_config_json_file", ",", "\n", "hidden_size", "=", "3072", ",", "\n", "num_hidden_layers", "=", "12", ",", "\n", "num_attention_heads", "=", "12", ",", "\n", "intermediate_size", "=", "3072", ",", "\n", "hidden_act", "=", "\"gelu\"", ",", "\n", "hidden_dropout_prob", "=", "0.1", ",", "\n", "attention_probs_dropout_prob", "=", "0.1", ",", "\n", "max_position_embeddings", "=", "512", ",", "\n", "type_vocab_size", "=", "2", ",", "\n", "initializer_range", "=", "0.02", ")", ":", "\n", "        ", "\"\"\"Constructs BertConfig.\n        Args:\n            vocab_size_or_config_json_file: Vocabulary size of `inputs_ids` in `BertModel`.\n            hidden_size: Size of the encoder layers and the pooler layer.\n            num_hidden_layers: Number of hidden layers in the Transformer encoder.\n            num_attention_heads: Number of attention heads for each attention layer in\n                the Transformer encoder.\n            intermediate_size: The size of the \"intermediate\" (i.e., feed-forward)\n                layer in the Transformer encoder.\n            hidden_act: The non-linear activation function (function or string) in the\n                encoder and pooler. If string, \"gelu\", \"relu\" and \"swish\" are supported.\n            hidden_dropout_prob: The dropout probabilitiy for all fully connected\n                layers in the embeddings, encoder, and pooler.\n            attention_probs_dropout_prob: The dropout ratio for the attention\n                probabilities.\n            max_position_embeddings: The maximum sequence length that this model might\n                ever be used with. Typically set this to something large just in case\n                (e.g., 512 or 1024 or 2048).\n            type_vocab_size: The vocabulary size of the `token_type_ids` passed into\n                `BertModel`.\n            initializer_range: The sttdev of the truncated_normal_initializer for\n                initializing all weight matrices.\n        \"\"\"", "\n", "if", "isinstance", "(", "vocab_size_or_config_json_file", ",", "str", ")", "or", "(", "sys", ".", "version_info", "[", "0", "]", "==", "2", "\n", "and", "isinstance", "(", "vocab_size_or_config_json_file", ",", "unicode", ")", ")", ":", "\n", "            ", "with", "open", "(", "vocab_size_or_config_json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "                ", "json_config", "=", "json", ".", "loads", "(", "reader", ".", "read", "(", ")", ")", "\n", "", "for", "key", ",", "value", "in", "json_config", ".", "items", "(", ")", ":", "\n", "                ", "self", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "", "elif", "isinstance", "(", "vocab_size_or_config_json_file", ",", "int", ")", ":", "\n", "            ", "self", ".", "vocab_size", "=", "vocab_size_or_config_json_file", "\n", "self", ".", "hidden_size", "=", "hidden_size", "\n", "self", ".", "num_hidden_layers", "=", "num_hidden_layers", "\n", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "hidden_act", "=", "hidden_act", "\n", "self", ".", "intermediate_size", "=", "intermediate_size", "\n", "self", ".", "hidden_dropout_prob", "=", "hidden_dropout_prob", "\n", "self", ".", "attention_probs_dropout_prob", "=", "attention_probs_dropout_prob", "\n", "self", ".", "max_position_embeddings", "=", "max_position_embeddings", "\n", "self", ".", "type_vocab_size", "=", "type_vocab_size", "\n", "self", ".", "initializer_range", "=", "initializer_range", "\n", "", "else", ":", "\n", "            ", "raise", "ValueError", "(", "\"First argument must be either a vocabulary size (int)\"", "\n", "\"or the path to a pretrained model config file (str)\"", ")", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertConfig.from_dict": [[123, 130], ["bertModel.BertConfig", "json_object.items"], "methods", ["None"], ["", "", "@", "classmethod", "\n", "def", "from_dict", "(", "cls", ",", "json_object", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a Python dictionary of parameters.\"\"\"", "\n", "config", "=", "BertConfig", "(", "vocab_size_or_config_json_file", "=", "-", "1", ")", "\n", "for", "key", ",", "value", "in", "json_object", ".", "items", "(", ")", ":", "\n", "            ", "config", ".", "__dict__", "[", "key", "]", "=", "value", "\n", "", "return", "config", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertConfig.from_json_file": [[131, 137], ["cls.from_dict", "io.open", "reader.read", "json.loads"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertConfig.from_dict"], ["", "@", "classmethod", "\n", "def", "from_json_file", "(", "cls", ",", "json_file", ")", ":", "\n", "        ", "\"\"\"Constructs a `BertConfig` from a json file of parameters.\"\"\"", "\n", "with", "open", "(", "json_file", ",", "\"r\"", ",", "encoding", "=", "'utf-8'", ")", "as", "reader", ":", "\n", "            ", "text", "=", "reader", ".", "read", "(", ")", "\n", "", "return", "cls", ".", "from_dict", "(", "json", ".", "loads", "(", "text", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertConfig.__repr__": [[138, 140], ["str", "bertModel.BertConfig.to_json_string"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertConfig.to_json_string"], ["", "def", "__repr__", "(", "self", ")", ":", "\n", "        ", "return", "str", "(", "self", ".", "to_json_string", "(", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertConfig.to_dict": [[141, 145], ["copy.deepcopy"], "methods", ["None"], ["", "def", "to_dict", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a Python dictionary.\"\"\"", "\n", "output", "=", "copy", ".", "deepcopy", "(", "self", ".", "__dict__", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertConfig.to_json_string": [[146, 149], ["json.dumps", "bertModel.BertConfig.to_dict"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertConfig.to_dict"], ["", "def", "to_json_string", "(", "self", ")", ":", "\n", "        ", "\"\"\"Serializes this instance to a JSON string.\"\"\"", "\n", "return", "json", ".", "dumps", "(", "self", ".", "to_dict", "(", ")", ",", "indent", "=", "2", ",", "sort_keys", "=", "True", ")", "+", "\"\\n\"", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertAttention.__init__": [[152, 170], ["torch.nn.Module.__init__", "int", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Linear", "torch.nn.Dropout", "ValueError"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_attention_heads", "=", "8", ",", "hidden_size", "=", "3072", ",", "drop", "=", "0.0", ",", "ctx_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "if", "hidden_size", "%", "num_attention_heads", "!=", "0", ":", "\n", "            ", "raise", "ValueError", "(", "\n", "\"The hidden size (%d) is not a multiple of the number of attention \"", "\n", "\"heads (%d)\"", "%", "(", "hidden_size", ",", "num_attention_heads", ")", ")", "\n", "", "self", ".", "num_attention_heads", "=", "num_attention_heads", "\n", "self", ".", "attention_head_size", "=", "int", "(", "hidden_size", "/", "num_attention_heads", ")", "\n", "self", ".", "all_head_size", "=", "self", ".", "num_attention_heads", "*", "self", ".", "attention_head_size", "\n", "\n", "# visual_dim = 2048", "\n", "if", "ctx_dim", "is", "None", ":", "\n", "            ", "ctx_dim", "=", "hidden_size", "\n", "", "self", ".", "query", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "key", "=", "nn", ".", "Linear", "(", "ctx_dim", ",", "self", ".", "all_head_size", ")", "\n", "self", ".", "value", "=", "nn", ".", "Linear", "(", "ctx_dim", ",", "self", ".", "all_head_size", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertAttention.transpose_for_scores": [[171, 175], ["x.view.view.view", "x.view.view.permute", "x.view.view.size"], "methods", ["None"], ["", "def", "transpose_for_scores", "(", "self", ",", "x", ")", ":", "\n", "        ", "new_x_shape", "=", "x", ".", "size", "(", ")", "[", ":", "-", "1", "]", "+", "(", "self", ".", "num_attention_heads", ",", "self", ".", "attention_head_size", ")", "\n", "x", "=", "x", ".", "view", "(", "*", "new_x_shape", ")", "\n", "return", "x", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertAttention.forward": [[176, 204], ["bertModel.BertAttention.query", "bertModel.BertAttention.key", "bertModel.BertAttention.value", "bertModel.BertAttention.transpose_for_scores", "bertModel.BertAttention.transpose_for_scores", "bertModel.BertAttention.transpose_for_scores", "torch.matmul", "bertModel.BertAttention.dropout", "torch.matmul", "context_layer.view.view.permute().contiguous", "context_layer.view.view.view", "bertModel.BertAttention.transpose", "math.sqrt", "torch.nn.Softmax", "context_layer.view.view.permute", "context_layer.view.view.size"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertAttention.transpose_for_scores", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertAttention.transpose_for_scores", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertAttention.transpose_for_scores"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "context", ",", "attention_mask", "=", "None", ")", ":", "\n", "        ", "mixed_query_layer", "=", "self", ".", "query", "(", "hidden_states", ")", "\n", "mixed_key_layer", "=", "self", ".", "key", "(", "context", ")", "\n", "mixed_value_layer", "=", "self", ".", "value", "(", "context", ")", "\n", "\n", "query_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_query_layer", ")", "\n", "key_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_key_layer", ")", "\n", "value_layer", "=", "self", ".", "transpose_for_scores", "(", "mixed_value_layer", ")", "\n", "\n", "# Take the dot product between \"query\" and \"key\" to get the raw attention scores.", "\n", "attention_scores", "=", "torch", ".", "matmul", "(", "query_layer", ",", "key_layer", ".", "transpose", "(", "-", "1", ",", "-", "2", ")", ")", "\n", "attention_scores", "=", "attention_scores", "/", "math", ".", "sqrt", "(", "self", ".", "attention_head_size", ")", "\n", "# Apply the attention mask is (precomputed for all layers in BertModel forward() function)", "\n", "if", "attention_mask", "is", "not", "None", ":", "\n", "            ", "attention_scores", "=", "attention_scores", "+", "attention_mask", "\n", "\n", "# Normalize the attention scores to probabilities.", "\n", "", "attention_probs", "=", "nn", ".", "Softmax", "(", "dim", "=", "-", "1", ")", "(", "attention_scores", ")", "\n", "\n", "# This is actually dropping out entire tokens to attend to, which might", "\n", "# seem a bit unusual, but is taken from the original Transformer paper.", "\n", "attention_probs", "=", "self", ".", "dropout", "(", "attention_probs", ")", "\n", "\n", "context_layer", "=", "torch", ".", "matmul", "(", "attention_probs", ",", "value_layer", ")", "\n", "context_layer", "=", "context_layer", ".", "permute", "(", "0", ",", "2", ",", "1", ",", "3", ")", ".", "contiguous", "(", ")", "\n", "new_context_layer_shape", "=", "context_layer", ".", "size", "(", ")", "[", ":", "-", "2", "]", "+", "(", "self", ".", "all_head_size", ",", ")", "\n", "context_layer", "=", "context_layer", ".", "view", "(", "*", "new_context_layer_shape", ")", "\n", "return", "context_layer", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertAttOutput.__init__": [[207, 212], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", "=", "3072", ",", "drop", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "BertAttOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "hidden_size", ",", "eps", "=", "1e-8", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertAttOutput.forward": [[213, 218], ["bertModel.BertAttOutput.dense", "bertModel.BertAttOutput.dropout", "bertModel.BertAttOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertCrossattLayer.__init__": [[221, 225], ["torch.nn.Module.__init__", "bertModel.BertAttention", "bertModel.BertAttOutput"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_attention_heads", "=", "8", ",", "hidden_size", "=", "3072", ",", "drop", "=", "0.0", ",", "ctx_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "self", ".", "att", "=", "BertAttention", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ")", "\n", "self", ".", "output", "=", "BertAttOutput", "(", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertCrossattLayer.forward": [[226, 230], ["bertModel.BertCrossattLayer.att", "bertModel.BertCrossattLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "ctx_tensor", ",", "ctx_att_mask", "=", "None", ")", ":", "\n", "        ", "output", "=", "self", ".", "att", "(", "input_tensor", ",", "ctx_tensor", ",", "ctx_att_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertSelfattLayer.__init__": [[233, 237], ["torch.nn.Module.__init__", "bertModel.BertAttention", "bertModel.BertAttOutput"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_attention_heads", "=", "8", ",", "hidden_size", "=", "3072", ",", "drop", "=", "0.0", ",", "ctx_dim", "=", "None", ")", ":", "\n", "        ", "super", "(", "BertSelfattLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "self", "=", "BertAttention", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ")", "\n", "self", ".", "output", "=", "BertAttOutput", "(", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertSelfattLayer.forward": [[238, 243], ["bertModel.BertSelfattLayer.self", "bertModel.BertSelfattLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_tensor", ",", "attention_mask", ")", ":", "\n", "# Self attention attends to itself, thus keys and querys are the same (input_tensor).", "\n", "        ", "self_output", "=", "self", ".", "self", "(", "input_tensor", ",", "input_tensor", ",", "attention_mask", ")", "\n", "attention_output", "=", "self", ".", "output", "(", "self_output", ",", "input_tensor", ")", "\n", "return", "attention_output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertIntermediate.__init__": [[246, 250], ["torch.nn.Module.__init__", "torch.nn.Linear", "bertModel.GeLU"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", "=", "3072", ",", "intermediate_size", "=", "2048", ")", ":", "\n", "        ", "super", "(", "BertIntermediate", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "intermediate_size", ")", "\n", "self", ".", "intermediate_act_fn", "=", "GeLU", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertIntermediate.forward": [[251, 255], ["bertModel.BertIntermediate.dense", "bertModel.BertIntermediate.intermediate_act_fn"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "intermediate_act_fn", "(", "hidden_states", ")", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertOutput.__init__": [[258, 263], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", "=", "3072", ",", "intermediate_size", "=", "2048", ",", "drop", "=", "0.0", ")", ":", "\n", "        ", "super", "(", "BertOutput", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "intermediate_size", ",", "hidden_size", ")", "\n", "self", ".", "LayerNorm", "=", "BertLayerNorm", "(", "hidden_size", ",", "eps", "=", "1e-8", ")", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertOutput.forward": [[264, 270], ["bertModel.BertOutput.dense", "bertModel.BertOutput.dropout", "bertModel.BertOutput.LayerNorm"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "input_tensor", ")", ":", "\n", "        ", "hidden_states", "=", "self", ".", "dense", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "dropout", "(", "hidden_states", ")", "\n", "hidden_states", "=", "self", ".", "LayerNorm", "(", "hidden_states", "+", "input_tensor", ")", "\n", "#         hidden_states = hidden_states + input_tensor", "\n", "return", "hidden_states", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertLayer.__init__": [[273, 278], ["torch.nn.Module.__init__", "bertModel.BertSelfattLayer", "bertModel.BertIntermediate", "bertModel.BertOutput"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_attention_heads", "=", "2", ",", "hidden_size", "=", "3072", ",", "drop", "=", "0.0", ",", "intermediate_size", "=", "2048", ")", ":", "\n", "        ", "super", "(", "BertLayer", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "attention", "=", "BertSelfattLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ")", "\n", "self", ".", "intermediate", "=", "BertIntermediate", "(", "hidden_size", "=", "hidden_size", ",", "intermediate_size", "=", "intermediate_size", ")", "\n", "self", ".", "output", "=", "BertOutput", "(", "hidden_size", "=", "hidden_size", ",", "intermediate_size", "=", "intermediate_size", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertLayer.forward": [[279, 284], ["bertModel.BertLayer.attention", "bertModel.BertLayer.intermediate", "bertModel.BertLayer.output"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ",", "attention_mask", ")", ":", "\n", "        ", "attention_output", "=", "self", ".", "attention", "(", "hidden_states", ",", "attention_mask", ")", "\n", "intermediate_output", "=", "self", ".", "intermediate", "(", "attention_output", ")", "\n", "layer_output", "=", "self", ".", "output", "(", "intermediate_output", ",", "attention_output", ")", "\n", "return", "layer_output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTXLayer.__init__": [[294, 308], ["torch.nn.Module.__init__", "bertModel.BertCrossattLayer", "bertModel.BertSelfattLayer", "bertModel.BertSelfattLayer", "bertModel.BertIntermediate", "bertModel.BertOutput", "bertModel.BertIntermediate", "bertModel.BertOutput"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "num_attention_heads", "=", "8", ",", "hidden_size", "=", "3072", ",", "drop", "=", "0.0", ",", "intermediate_size", "=", "2048", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "# The cross-attention Layer", "\n", "self", ".", "visual_attention", "=", "BertCrossattLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ")", "\n", "\n", "# Self-attention Layers", "\n", "self", ".", "lang_self_att", "=", "BertSelfattLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ")", "\n", "self", ".", "visn_self_att", "=", "BertSelfattLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ")", "\n", "\n", "# Intermediate and Output Layers (FFNs)", "\n", "self", ".", "lang_inter", "=", "BertIntermediate", "(", "hidden_size", "=", "hidden_size", ",", "intermediate_size", "=", "intermediate_size", ")", "\n", "self", ".", "lang_output", "=", "BertOutput", "(", "hidden_size", "=", "hidden_size", ",", "intermediate_size", "=", "intermediate_size", ",", "drop", "=", "drop", ")", "\n", "self", ".", "visn_inter", "=", "BertIntermediate", "(", "hidden_size", "=", "hidden_size", ",", "intermediate_size", "=", "intermediate_size", ")", "\n", "self", ".", "visn_output", "=", "BertOutput", "(", "hidden_size", "=", "hidden_size", ",", "intermediate_size", "=", "intermediate_size", ",", "drop", "=", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTXLayer.cross_att": [[309, 314], ["bertModel.LXRTXLayer.visual_attention", "bertModel.LXRTXLayer.visual_attention"], "methods", ["None"], ["", "def", "cross_att", "(", "self", ",", "lang_input", ",", "lang_attention_mask", ",", "visn_input", ",", "visn_attention_mask", ")", ":", "\n", "# Cross Attention", "\n", "        ", "lang_att_output", "=", "self", ".", "visual_attention", "(", "lang_input", ",", "visn_input", ",", "ctx_att_mask", "=", "visn_attention_mask", ")", "\n", "visn_att_output", "=", "self", ".", "visual_attention", "(", "visn_input", ",", "lang_input", ",", "ctx_att_mask", "=", "lang_attention_mask", ")", "\n", "return", "lang_att_output", ",", "visn_att_output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTXLayer.self_att": [[315, 320], ["bertModel.LXRTXLayer.lang_self_att", "bertModel.LXRTXLayer.visn_self_att"], "methods", ["None"], ["", "def", "self_att", "(", "self", ",", "lang_input", ",", "lang_attention_mask", ",", "visn_input", ",", "visn_attention_mask", ")", ":", "\n", "# Self Attention", "\n", "        ", "lang_att_output", "=", "self", ".", "lang_self_att", "(", "lang_input", ",", "lang_attention_mask", ")", "\n", "visn_att_output", "=", "self", ".", "visn_self_att", "(", "visn_input", ",", "visn_attention_mask", ")", "\n", "return", "lang_att_output", ",", "visn_att_output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTXLayer.output_fc": [[321, 330], ["bertModel.LXRTXLayer.lang_inter", "bertModel.LXRTXLayer.visn_inter", "bertModel.LXRTXLayer.lang_output", "bertModel.LXRTXLayer.visn_output"], "methods", ["None"], ["", "def", "output_fc", "(", "self", ",", "lang_input", ",", "visn_input", ")", ":", "\n", "# FC layers", "\n", "        ", "lang_inter_output", "=", "self", ".", "lang_inter", "(", "lang_input", ")", "\n", "visn_inter_output", "=", "self", ".", "visn_inter", "(", "visn_input", ")", "\n", "\n", "# Layer output", "\n", "lang_output", "=", "self", ".", "lang_output", "(", "lang_inter_output", ",", "lang_input", ")", "\n", "visn_output", "=", "self", ".", "visn_output", "(", "visn_inter_output", ",", "visn_input", ")", "\n", "return", "lang_output", ",", "visn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTXLayer.forward": [[331, 343], ["bertModel.LXRTXLayer.cross_att", "bertModel.LXRTXLayer.self_att", "bertModel.LXRTXLayer.output_fc"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTXLayer.cross_att", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTXLayer.self_att", "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTXLayer.output_fc"], ["", "def", "forward", "(", "self", ",", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", ")", ":", "\n", "        ", "lang_att_output", "=", "lang_feats", "\n", "visn_att_output", "=", "visn_feats", "\n", "\n", "lang_att_output", ",", "visn_att_output", "=", "self", ".", "cross_att", "(", "lang_att_output", ",", "lang_attention_mask", ",", "\n", "visn_att_output", ",", "visn_attention_mask", ")", "\n", "lang_att_output", ",", "visn_att_output", "=", "self", ".", "self_att", "(", "lang_att_output", ",", "lang_attention_mask", ",", "\n", "visn_att_output", ",", "visn_attention_mask", ")", "\n", "lang_output", ",", "visn_output", "=", "self", ".", "output_fc", "(", "lang_att_output", ",", "visn_att_output", ")", "\n", "\n", "return", "lang_output", ",", "visn_output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.VisualFeatEncoder.__init__": [[346, 360], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "visual_feat_dim", ",", "visual_pos_dim", ",", "hidden_size", ",", "drop", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "feat_dim", "=", "visual_feat_dim", "\n", "pos_dim", "=", "visual_pos_dim", "\n", "\n", "# Object feature encoding", "\n", "self", ".", "visn_fc", "=", "nn", ".", "Linear", "(", "feat_dim", ",", "hidden_size", ")", "\n", "self", ".", "visn_layer_norm", "=", "BertLayerNorm", "(", "hidden_size", ",", "eps", "=", "1e-9", ")", "\n", "\n", "# Box position encoding", "\n", "self", ".", "box_fc", "=", "nn", ".", "Linear", "(", "pos_dim", ",", "hidden_size", ")", "\n", "self", ".", "box_layer_norm", "=", "BertLayerNorm", "(", "hidden_size", ",", "eps", "=", "1e-9", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.VisualFeatEncoder.forward": [[361, 372], ["bertModel.VisualFeatEncoder.visn_fc", "bertModel.VisualFeatEncoder.visn_layer_norm", "bertModel.VisualFeatEncoder.box_fc", "bertModel.VisualFeatEncoder.box_layer_norm", "bertModel.VisualFeatEncoder.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "vis_input", ")", ":", "\n", "        ", "feats", ",", "boxes", "=", "vis_input", "\n", "\n", "x", "=", "self", ".", "visn_fc", "(", "feats", ")", "\n", "x", "=", "self", ".", "visn_layer_norm", "(", "x", ")", "\n", "y", "=", "self", ".", "box_fc", "(", "boxes", ")", "\n", "y", "=", "self", ".", "box_layer_norm", "(", "y", ")", "\n", "output", "=", "(", "x", "+", "y", ")", "/", "2", "\n", "\n", "output", "=", "self", ".", "dropout", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTEncoder.__init__": [[375, 401], ["torch.nn.Module.__init__", "bertModel.VisualFeatEncoder", "print", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "bertModel.BertLayer", "bertModel.LXRTXLayer", "bertModel.BertLayer", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "visual_feat_dim", "=", "2048", ",", "visual_pos_dim", "=", "8", ",", "drop", "=", "0.0", ",", "l_layers", "=", "9", ",", "x_layers", "=", "5", ",", "r_layers", "=", "5", ",", "num_attention_heads", "=", "4", ",", "hidden_size", "=", "3072", ",", "intermediate_size", "=", "2048", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Obj-level image embedding layer", "\n", "self", ".", "visn_fc", "=", "VisualFeatEncoder", "(", "visual_feat_dim", "=", "visual_feat_dim", ",", "visual_pos_dim", "=", "visual_pos_dim", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "0.0", ")", "\n", "\n", "# Number of layers", "\n", "self", ".", "num_l_layers", "=", "l_layers", "\n", "self", ".", "num_x_layers", "=", "x_layers", "\n", "self", ".", "num_r_layers", "=", "r_layers", "\n", "print", "(", "\"LXRT encoder with %d l_layers, %d x_layers, and %d r_layers.\"", "%", "\n", "(", "self", ".", "num_l_layers", ",", "self", ".", "num_x_layers", ",", "self", ".", "num_r_layers", ")", ")", "\n", "\n", "# Layers", "\n", "# Using self.layer instead of self.l_layer to support loading BERT weights.", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "\n", "[", "BertLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ",", "intermediate_size", "=", "intermediate_size", ")", "for", "_", "in", "range", "(", "self", ".", "num_l_layers", ")", "]", "\n", ")", "\n", "self", ".", "x_layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "LXRTXLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ",", "intermediate_size", "=", "intermediate_size", ")", "for", "_", "in", "range", "(", "self", ".", "num_x_layers", ")", "]", "\n", ")", "\n", "self", ".", "r_layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "BertLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ",", "intermediate_size", "=", "intermediate_size", ")", "for", "_", "in", "range", "(", "self", ".", "num_r_layers", ")", "]", "\n", ")", "\n", "\n", "self", ".", "cuda_list", "=", "[", "]", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTEncoder.transform_cuda": [[403, 430], ["bertModel.LXRTEncoder.visn_fc.to", "range", "range", "range", "len", "len", "layer_module.to", "len", "layer_module.to", "layer_module.to", "layer_module.to", "layer_module.to", "len", "len"], "methods", ["None"], ["", "def", "transform_cuda", "(", "self", ",", "cuda_list", ")", ":", "\n", "\n", "        ", "self", ".", "cuda_list", "=", "cuda_list", "\n", "\n", "self", ".", "visn_fc", ".", "to", "(", "cuda_list", "[", "0", "]", ")", "\n", "# Run language layers", "\n", "for", "layer_module", "in", "range", "(", "len", "(", "self", ".", "layer", ")", ")", ":", "\n", "            ", "if", "layer_module", "<", "len", "(", "self", ".", "layer", ")", "/", "2", ":", "\n", "                ", "layer_module", "=", "self", ".", "layer", "[", "layer_module", "]", "\n", "layer_module", ".", "to", "(", "cuda_list", "[", "1", "]", ")", "\n", "", "else", ":", "\n", "                ", "layer_module", "=", "self", ".", "layer", "[", "layer_module", "]", "\n", "layer_module", ".", "to", "(", "cuda_list", "[", "2", "]", ")", "\n", "\n", "# Run relational layers", "\n", "", "", "for", "layer_module", "in", "range", "(", "len", "(", "self", ".", "r_layers", ")", ")", ":", "\n", "            ", "layer_module", "=", "self", ".", "r_layers", "[", "layer_module", "]", "\n", "layer_module", ".", "to", "(", "cuda_list", "[", "0", "]", ")", "\n", "\n", "# Run cross-modality layers", "\n", "", "for", "layer_module", "in", "range", "(", "len", "(", "self", ".", "x_layers", ")", ")", ":", "\n", "            ", "if", "layer_module", "<", "len", "(", "self", ".", "x_layers", ")", "/", "2", ":", "\n", "                ", "layer_module", "=", "self", ".", "x_layers", "[", "layer_module", "]", "\n", "layer_module", ".", "to", "(", "cuda_list", "[", "3", "]", ")", "\n", "", "else", ":", "\n", "                ", "layer_module", "=", "self", ".", "x_layers", "[", "layer_module", "]", "\n", "layer_module", ".", "to", "(", "cuda_list", "[", "2", "]", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTEncoder.forward": [[432, 484], ["bertModel.LXRTEncoder.visn_fc", "lang_feats.to.to.to", "visn_feats.to.to.to", "print", "print", "layer_module", "print", "print", "print", "layer_module", "layer_module", "layer_module", "layer_module", "len", "lang_feats.to.to.to", "len", "lang_feats.to.to.to", "visn_feats.to.to.to"], "methods", ["None"], ["", "", "", "def", "forward", "(", "self", ",", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", "=", "None", ",", "logger", "=", "False", ")", ":", "\n", "# Run visual embedding layer", "\n", "# Note: Word embedding layer was executed outside this module.", "\n", "#       Keep this design to allow loading BERT weights.", "\n", "        ", "visn_feats", "=", "self", ".", "visn_fc", "(", "visn_feats", ")", "\n", "\n", "# Run language layers", "\n", "count", "=", "0", "\n", "check", "=", "False", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "            ", "if", "count", "<", "len", "(", "self", ".", "layer", ")", "/", "2", ":", "\n", "                ", "lang_feats", "=", "layer_module", "(", "lang_feats", ",", "lang_attention_mask", ")", "\n", "", "else", ":", "\n", "                ", "if", "not", "check", ":", "\n", "                    ", "lang_feats", "=", "lang_feats", ".", "to", "(", "self", ".", "cuda_list", "[", "2", "]", ")", "\n", "check", "=", "True", "\n", "", "lang_feats", "=", "layer_module", "(", "lang_feats", ",", "lang_attention_mask", ")", "\n", "", "count", "+=", "1", "\n", "", "if", "logger", ":", "\n", "            ", "print", "(", "\"the language info output of self-attention shape is: \"", ",", "lang_feats", ".", "shape", ",", "file", "=", "logger", ")", "\n", "print", "(", "\"the language info output of self-attention is: \"", ",", "lang_feats", ",", "file", "=", "logger", ")", "\n", "\n", "# Run relational layers", "\n", "", "for", "layer_module", "in", "self", ".", "r_layers", ":", "\n", "            ", "visn_feats", "=", "layer_module", "(", "visn_feats", ",", "visn_attention_mask", ")", "\n", "\n", "", "if", "logger", ":", "\n", "            ", "print", "(", "\"the Vision info output of self-attention shape is: \"", ",", "visn_feats", ".", "shape", ",", "file", "=", "logger", ")", "\n", "print", "(", "\"the Vision info output of self-attention is: \"", ",", "visn_feats", ",", "file", "=", "logger", ")", "\n", "\n", "", "lang_feats", "=", "lang_feats", ".", "to", "(", "self", ".", "cuda_list", "[", "3", "]", ")", "\n", "visn_feats", "=", "visn_feats", ".", "to", "(", "self", ".", "cuda_list", "[", "3", "]", ")", "\n", "# Run cross-modality layers", "\n", "count", "=", "0", "\n", "check", "=", "False", "\n", "for", "layer_module", "in", "self", ".", "x_layers", ":", "\n", "            ", "if", "count", "<", "len", "(", "self", ".", "x_layers", ")", "/", "2", ":", "\n", "                ", "lang_feats", ",", "visn_feats", "=", "layer_module", "(", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", ")", "\n", "", "else", ":", "\n", "                ", "if", "not", "check", ":", "\n", "                    ", "lang_feats", "=", "lang_feats", ".", "to", "(", "self", ".", "cuda_list", "[", "2", "]", ")", "\n", "visn_feats", "=", "visn_feats", ".", "to", "(", "self", ".", "cuda_list", "[", "2", "]", ")", "\n", "check", "=", "True", "\n", "", "lang_feats", ",", "visn_feats", "=", "layer_module", "(", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", ")", "\n", "", "count", "+=", "1", "\n", "\n", "", "if", "logger", ":", "\n", "            ", "print", "(", "\"the Lang info output of cross_attention is: \"", ",", "lang_feats", ",", "file", "=", "logger", ")", "\n", "", "return", "lang_feats", ",", "visn_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertPooler.__init__": [[487, 491], ["torch.nn.Module.__init__", "torch.nn.Linear", "torch.nn.Tanh"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "hidden_size", ")", ":", "\n", "        ", "super", "(", "BertPooler", ",", "self", ")", ".", "__init__", "(", ")", "\n", "self", ".", "dense", "=", "nn", ".", "Linear", "(", "hidden_size", ",", "hidden_size", ")", "\n", "self", ".", "activation", "=", "nn", ".", "Tanh", "(", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertPooler.forward": [[492, 499], ["bertModel.BertPooler.dense", "bertModel.BertPooler.activation"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "# We \"pool\" the model by simply taking the hidden state corresponding", "\n", "# to the first token.", "\n", "        ", "first_token_tensor", "=", "hidden_states", "[", ":", ",", "0", "]", "\n", "pooled_output", "=", "self", ".", "dense", "(", "first_token_tensor", ")", "\n", "pooled_output", "=", "self", ".", "activation", "(", "pooled_output", ")", "\n", "return", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertVisualAnswerHead.__init__": [[502, 510], ["torch.nn.Module.__init__", "torch.nn.Sequential", "torch.nn.Linear", "bertModel.GeLU", "BertLayerNorm", "torch.nn.Linear"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "config", ",", "num_answers", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "hid_dim", "=", "config", ".", "hidden_size", "\n", "self", ".", "logit_fc", "=", "nn", ".", "Sequential", "(", "\n", "nn", ".", "Linear", "(", "hid_dim", ",", "hid_dim", "*", "2", ")", ",", "\n", "GeLU", "(", ")", ",", "\n", "BertLayerNorm", "(", "hid_dim", "*", "2", ",", "eps", "=", "1e-12", ")", ",", "\n", "nn", ".", "Linear", "(", "hid_dim", "*", "2", ",", "num_answers", ")", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.BertVisualAnswerHead.forward": [[512, 514], ["bertModel.BertVisualAnswerHead.logit_fc"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "hidden_states", ")", ":", "\n", "        ", "return", "self", ".", "logit_fc", "(", "hidden_states", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTModel.__init__": [[521, 527], ["torch.nn.Module.__init__", "BertEmbeddings", "bertModel.LXRTEncoder", "bertModel.BertPooler", "bertModel.LXRTModel.apply"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["def", "__init__", "(", "self", ",", "config", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", "config", ")", "\n", "self", ".", "embeddings", "=", "BertEmbeddings", "(", "config", ")", "\n", "self", ".", "encoder", "=", "LXRTEncoder", "(", "config", ")", "\n", "self", ".", "pooler", "=", "BertPooler", "(", "config", ")", "\n", "self", ".", "apply", "(", "self", ".", "init_bert_weights", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.LXRTModel.forward": [[528, 570], ["torch.ones_like.unsqueeze().unsqueeze", "extended_attention_mask.to.to.to", "bertModel.LXRTModel.embeddings", "bertModel.LXRTModel.encoder", "bertModel.LXRTModel.pooler", "torch.ones_like", "torch.zeros_like", "visual_attention_mask.unsqueeze().unsqueeze", "extended_visual_attention_mask.to.to.to", "torch.ones_like.unsqueeze", "next", "visual_attention_mask.unsqueeze", "bertModel.LXRTModel.parameters", "next", "bertModel.LXRTModel.parameters"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "input_ids", ",", "token_type_ids", "=", "None", ",", "attention_mask", "=", "None", ",", "\n", "visual_feats", "=", "None", ",", "visual_attention_mask", "=", "None", ")", ":", "\n", "        ", "if", "attention_mask", "is", "None", ":", "\n", "            ", "attention_mask", "=", "torch", ".", "ones_like", "(", "input_ids", ")", "\n", "", "if", "token_type_ids", "is", "None", ":", "\n", "            ", "token_type_ids", "=", "torch", ".", "zeros_like", "(", "input_ids", ")", "\n", "\n", "# We create a 3D attention mask from a 2D tensor mask.", "\n", "# Sizes are [batch_size, 1, 1, to_seq_length]", "\n", "# So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]", "\n", "# this attention mask is more simple than the triangular masking of causal attention", "\n", "# used in OpenAI GPT, we just need to prepare the broadcast dimension here.", "\n", "", "extended_attention_mask", "=", "attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "\n", "# Since attention_mask is 1.0 for positions we want to attend and 0.0 for", "\n", "# masked positions, this operation will create a tensor which is 0.0 for", "\n", "# positions we want to attend and -10000.0 for masked positions.", "\n", "# Since we are adding it to the raw scores before the softmax, this is", "\n", "# effectively the same as removing these entirely.", "\n", "extended_attention_mask", "=", "extended_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_attention_mask", "=", "(", "1.0", "-", "extended_attention_mask", ")", "*", "-", "10000.0", "\n", "\n", "# Process the visual attention mask", "\n", "if", "visual_attention_mask", "is", "not", "None", ":", "\n", "            ", "extended_visual_attention_mask", "=", "visual_attention_mask", ".", "unsqueeze", "(", "1", ")", ".", "unsqueeze", "(", "2", ")", "\n", "extended_visual_attention_mask", "=", "extended_visual_attention_mask", ".", "to", "(", "dtype", "=", "next", "(", "self", ".", "parameters", "(", ")", ")", ".", "dtype", ")", "# fp16 compatibility", "\n", "extended_visual_attention_mask", "=", "(", "1.0", "-", "extended_visual_attention_mask", ")", "*", "-", "10000.0", "\n", "", "else", ":", "\n", "            ", "extended_visual_attention_mask", "=", "None", "\n", "\n", "# Positional Word Embeddings", "\n", "", "embedding_output", "=", "self", ".", "embeddings", "(", "input_ids", ",", "token_type_ids", ")", "\n", "\n", "# Run LXRT backbone", "\n", "lang_feats", ",", "visn_feats", "=", "self", ".", "encoder", "(", "\n", "embedding_output", ",", "\n", "extended_attention_mask", ",", "\n", "visn_feats", "=", "visual_feats", ",", "\n", "visn_attention_mask", "=", "extended_visual_attention_mask", ")", "\n", "pooled_output", "=", "self", ".", "pooler", "(", "lang_feats", ")", "\n", "\n", "return", "(", "lang_feats", ",", "visn_feats", ")", ",", "pooled_output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosVisualFeatEncoder.__init__": [[574, 583], ["torch.nn.Module.__init__", "torch.nn.Linear", "BertLayerNorm", "torch.nn.Dropout"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "visual_feat_dim", ",", "hidden_size", ",", "drop", "=", "0.0", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "feat_dim", "=", "visual_feat_dim", "\n", "\n", "# Object feature encoding", "\n", "self", ".", "visn_fc", "=", "nn", ".", "Linear", "(", "feat_dim", ",", "hidden_size", ")", "\n", "self", ".", "visn_layer_norm", "=", "BertLayerNorm", "(", "hidden_size", ",", "eps", "=", "1e-9", ")", "\n", "\n", "self", ".", "dropout", "=", "nn", ".", "Dropout", "(", "drop", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosVisualFeatEncoder.forward": [[584, 593], ["bertModel.NoPosVisualFeatEncoder.visn_fc", "bertModel.NoPosVisualFeatEncoder.dropout"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "vis_input", ")", ":", "\n", "        ", "feats", "=", "vis_input", "\n", "\n", "x", "=", "self", ".", "visn_fc", "(", "feats", ")", "\n", "#         x = self.visn_layer_norm(x)", "\n", "output", "=", "x", "\n", "\n", "output", "=", "self", ".", "dropout", "(", "output", ")", "\n", "return", "output", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__": [[596, 619], ["torch.nn.Module.__init__", "bertModel.NoPosVisualFeatEncoder", "print", "torch.nn.ModuleList", "torch.nn.ModuleList", "torch.nn.ModuleList", "bertModel.BertLayer", "bertModel.LXRTXLayer", "bertModel.BertLayer", "range", "range", "range"], "methods", ["home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.__init__"], ["    ", "def", "__init__", "(", "self", ",", "visual_feat_dim", "=", "2048", ",", "drop", "=", "0.0", ",", "l_layers", "=", "9", ",", "x_layers", "=", "5", ",", "r_layers", "=", "5", ",", "num_attention_heads", "=", "4", ",", "hidden_size", "=", "3072", ",", "intermediate_size", "=", "2048", ")", ":", "\n", "        ", "super", "(", ")", ".", "__init__", "(", ")", "\n", "\n", "# Obj-level image embedding layer", "\n", "self", ".", "visn_fc", "=", "NoPosVisualFeatEncoder", "(", "visual_feat_dim", "=", "visual_feat_dim", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "0.0", ")", "\n", "\n", "# Number of layers", "\n", "self", ".", "num_l_layers", "=", "l_layers", "\n", "self", ".", "num_x_layers", "=", "x_layers", "\n", "self", ".", "num_r_layers", "=", "r_layers", "\n", "print", "(", "\"LXRT encoder with %d l_layers, %d x_layers, and %d r_layers.\"", "%", "\n", "(", "self", ".", "num_l_layers", ",", "self", ".", "num_x_layers", ",", "self", ".", "num_r_layers", ")", ")", "\n", "\n", "# Layers", "\n", "# Using self.layer instead of self.l_layer to support loading BERT weights.", "\n", "self", ".", "layer", "=", "nn", ".", "ModuleList", "(", "\n", "[", "BertLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ",", "intermediate_size", "=", "intermediate_size", ")", "for", "_", "in", "range", "(", "self", ".", "num_l_layers", ")", "]", "\n", ")", "\n", "self", ".", "x_layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "LXRTXLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ",", "intermediate_size", "=", "intermediate_size", ")", "for", "_", "in", "range", "(", "self", ".", "num_x_layers", ")", "]", "\n", ")", "\n", "self", ".", "r_layers", "=", "nn", ".", "ModuleList", "(", "\n", "[", "BertLayer", "(", "num_attention_heads", "=", "num_attention_heads", ",", "hidden_size", "=", "hidden_size", ",", "drop", "=", "drop", ",", "intermediate_size", "=", "intermediate_size", ")", "for", "_", "in", "range", "(", "self", ".", "num_r_layers", ")", "]", "\n", ")", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.NoPosLXRTEncoder.forward": [[622, 649], ["bertModel.NoPosLXRTEncoder.visn_fc", "layer_module", "layer_module", "layer_module", "layer_module"], "methods", ["None"], ["", "def", "forward", "(", "self", ",", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", "=", "None", ",", "logger", "=", "False", ")", ":", "\n", "# Run visual embedding layer", "\n", "# Note: Word embedding layer was executed outside this module.", "\n", "#       Keep this design to allow loading BERT weights.", "\n", "\n", "        ", "if", "visn_feats", "!=", "None", ":", "\n", "            ", "visn_feats", "=", "self", ".", "visn_fc", "(", "visn_feats", ")", "\n", "\n", "# Run language layers", "\n", "count", "=", "0", "\n", "check", "=", "False", "\n", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "                ", "lang_feats", "=", "layer_module", "(", "lang_feats", ",", "lang_attention_mask", ")", "\n", "\n", "# Run relational layers", "\n", "", "for", "layer_module", "in", "self", ".", "r_layers", ":", "\n", "                ", "visn_feats", "=", "layer_module", "(", "visn_feats", ",", "visn_attention_mask", ")", "\n", "\n", "", "for", "layer_module", "in", "self", ".", "x_layers", ":", "\n", "                ", "lang_feats", ",", "visn_feats", "=", "layer_module", "(", "lang_feats", ",", "lang_attention_mask", ",", "\n", "visn_feats", ",", "visn_attention_mask", ")", "\n", "", "", "else", ":", "\n", "            ", "for", "layer_module", "in", "self", ".", "layer", ":", "\n", "                ", "lang_feats", "=", "layer_module", "(", "lang_feats", ",", "lang_attention_mask", ")", "\n", "\n", "", "", "return", "lang_feats", ",", "visn_feats", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.gelu": [[34, 41], ["torch.erf", "math.sqrt"], "function", ["None"], ["def", "gelu", "(", "x", ")", ":", "\n", "    ", "\"\"\"Implementation of the gelu activation function.\n        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n        Also see https://arxiv.org/abs/1606.08415\n    \"\"\"", "\n", "return", "x", "*", "0.5", "*", "(", "1.0", "+", "torch", ".", "erf", "(", "x", "/", "math", ".", "sqrt", "(", "2.0", ")", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.bertModel.swish": [[56, 58], ["torch.sigmoid"], "function", ["None"], ["", "", "def", "swish", "(", "x", ")", ":", "\n", "    ", "return", "x", "*", "torch", ".", "sigmoid", "(", "x", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.util.mask_": [[3, 16], ["matrices.size", "torch.triu_indices"], "function", ["None"], ["def", "mask_", "(", "matrices", ",", "maskval", "=", "0.0", ",", "mask_diagonal", "=", "True", ")", ":", "\n", "    ", "\"\"\"\n    Masks out all values in the given batch of matrices where i <= j holds,\n    i < j if mask_diagonal is false\n    In place operation\n    :param tns:\n    :return:\n    \"\"\"", "\n", "\n", "b", ",", "h", ",", "w", "=", "matrices", ".", "size", "(", ")", "\n", "\n", "indices", "=", "torch", ".", "triu_indices", "(", "h", ",", "w", ",", "offset", "=", "0", "if", "mask_diagonal", "else", "1", ")", "\n", "matrices", "[", ":", ",", "indices", "[", "0", "]", ",", "indices", "[", "1", "]", "]", "=", "maskval", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.util.d": [[17, 27], ["torch.cuda.is_available"], "function", ["None"], ["", "def", "d", "(", "tensor", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    Returns a device string either for the best available device,\n    or for the device corresponding to the argument\n    :param tensor:\n    :return:\n    \"\"\"", "\n", "if", "tensor", "is", "None", ":", "\n", "        ", "return", "'cuda'", "if", "torch", ".", "cuda", ".", "is_available", "(", ")", "else", "'cpu'", "\n", "", "return", "'cuda'", "if", "tensor", ".", "is_cuda", "else", "'cpu'", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.util.here": [[28, 36], ["os.path.abspath", "os.path.abspath", "os.path.join", "os.path.join", "os.path.dirname", "os.path.dirname"], "function", ["None"], ["", "def", "here", "(", "subpath", "=", "None", ")", ":", "\n", "    ", "\"\"\"\n    :return: the path in which the package resides (the directory containing the 'former' dir)\n    \"\"\"", "\n", "if", "subpath", "is", "None", ":", "\n", "        ", "return", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "'../..'", ")", ")", "\n", "\n", "", "return", "os", ".", "path", ".", "abspath", "(", "os", ".", "path", ".", "join", "(", "os", ".", "path", ".", "dirname", "(", "__file__", ")", ",", "'../..'", ",", "subpath", ")", ")", "\n", "\n"]], "home.repos.pwc.inspect_result.HLR_LatentAlignmentProcedural.src.util.contains_nan": [[37, 39], ["bool"], "function", ["None"], ["", "def", "contains_nan", "(", "tensor", ")", ":", "\n", "    ", "return", "bool", "(", "(", "tensor", "!=", "tensor", ")", ".", "sum", "(", ")", ">", "0", ")", "\n", "", ""]]}